/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/23 21:33:46 visual_prompt]: Rank of current process: 0. World size: 1
[10/23 21:33:50 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/23 21:33:50 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/23 21:33:50 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/23 21:33:50 visual_prompt]: Training with config:
[10/23 21:33:50 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr50.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/23 21:33:50 visual_prompt]: Loading training data...
[10/23 21:33:50 visual_prompt]: Constructing mammo-cbis dataset train...
[10/23 21:33:50 visual_prompt]: Loading validation data...
[10/23 21:33:50 visual_prompt]: Constructing mammo-cbis dataset val...
[10/23 21:33:50 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/23 21:34:08 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/23 21:34:08 visual_prompt]: tuned percent:0.522
[10/23 21:34:08 visual_prompt]: Device used for model: 0
[10/23 21:34:08 visual_prompt]: Setting up Evaluator...
[10/23 21:34:08 visual_prompt]: Setting up Trainer...
[10/23 21:34:08 visual_prompt]: 	Setting up the optimizer...
[10/23 21:34:08 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/23 21:35:17 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6543 s / batch. (data: 8.27e-04). ETA=20:05:05, max mem: 15.9 GB 
[10/23 21:36:21 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6328 s / batch. (data: 4.27e-04). ETA=19:24:20, max mem: 15.9 GB 
[10/23 21:37:25 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6339 s / batch. (data: 7.39e-04). ETA=19:25:16, max mem: 15.9 GB 
[10/23 21:38:28 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6218 s / batch. (data: 8.06e-04). ETA=19:02:00, max mem: 15.9 GB 
[10/23 21:39:32 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6281 s / batch. (data: 3.03e-04). ETA=19:12:30, max mem: 15.9 GB 
[10/23 21:40:35 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6194 s / batch. (data: 3.01e-04). ETA=18:55:30, max mem: 15.9 GB 
[10/23 21:41:38 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6297 s / batch. (data: 7.91e-04). ETA=19:13:26, max mem: 15.9 GB 
[10/23 21:42:42 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6317 s / batch. (data: 8.40e-04). ETA=19:15:59, max mem: 15.9 GB 
[10/23 21:43:45 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6316 s / batch. (data: 7.39e-04). ETA=19:14:49, max mem: 15.9 GB 
[10/23 21:44:48 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6201 s / batch. (data: 3.04e-04). ETA=18:52:39, max mem: 15.9 GB 
[10/23 21:45:51 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6189 s / batch. (data: 3.35e-04). ETA=18:49:33, max mem: 15.9 GB 
[10/23 21:45:55 visual_prompt]: Epoch 1 / 100: avg data time: 6.87e-03, avg batch time: 0.6390, average train loss: 1.4028
[10/23 21:46:46 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2260 s / batch. (data: 4.15e-05)max mem: 15.88805 GB 
[10/23 21:46:57 visual_prompt]: Inference (val):avg data time: 9.77e-05, avg batch time: 0.2333, average loss: 1.3505
[10/23 21:46:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/23 21:46:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/23 21:48:01 visual_prompt]: 	Training 100/1106. train loss: 27.4343,	0.6360 s / batch. (data: 8.01e-04). ETA=19:19:34, max mem: 15.9 GB 
[10/23 21:49:04 visual_prompt]: 	Training 200/1106. train loss: 53.8110,	0.6141 s / batch. (data: 3.31e-04). ETA=18:38:32, max mem: 15.9 GB 
[10/23 21:50:07 visual_prompt]: 	Training 300/1106. train loss: 21.1374,	0.6430 s / batch. (data: 7.75e-04). ETA=19:30:07, max mem: 15.9 GB 
[10/23 21:51:10 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6338 s / batch. (data: 8.96e-04). ETA=19:12:25, max mem: 15.9 GB 
[10/23 21:52:14 visual_prompt]: 	Training 500/1106. train loss: 21.3293,	0.6303 s / batch. (data: 3.17e-04). ETA=19:05:04, max mem: 15.9 GB 
[10/23 21:53:17 visual_prompt]: 	Training 600/1106. train loss: 14.1258,	0.6400 s / batch. (data: 3.25e-04). ETA=19:21:28, max mem: 15.9 GB 
[10/23 21:54:20 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6095 s / batch. (data: 7.43e-04). ETA=18:25:09, max mem: 15.9 GB 
[10/23 21:55:23 visual_prompt]: 	Training 800/1106. train loss: 30.2459,	0.6480 s / batch. (data: 8.03e-04). ETA=19:33:54, max mem: 15.9 GB 
[10/23 21:56:26 visual_prompt]: 	Training 900/1106. train loss: 5.2480,	0.6561 s / batch. (data: 8.09e-03). ETA=19:47:28, max mem: 15.9 GB 
[10/23 21:57:30 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6092 s / batch. (data: 3.71e-04). ETA=18:21:31, max mem: 15.9 GB 
[10/23 21:58:33 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6190 s / batch. (data: 1.47e-04). ETA=18:38:14, max mem: 15.9 GB 
[10/23 21:58:37 visual_prompt]: Epoch 2 / 100: avg data time: 4.56e-03, avg batch time: 0.6331, average train loss: 23.4313
[10/23 21:59:27 visual_prompt]: 	Test 100/123. loss: 4.812, 0.2421 s / batch. (data: 3.10e-05)max mem: 15.88805 GB 
[10/23 21:59:38 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2312, average loss: 4.3982
[10/23 21:59:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.32	
[10/23 21:59:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/23 22:00:44 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6219 s / batch. (data: 8.78e-04). ETA=18:42:24, max mem: 15.9 GB 
[10/23 22:01:47 visual_prompt]: 	Training 200/1106. train loss: 54.1226,	0.6189 s / batch. (data: 3.19e-04). ETA=18:35:56, max mem: 15.9 GB 
[10/23 22:02:51 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6598 s / batch. (data: 3.27e-02). ETA=19:48:37, max mem: 15.9 GB 
[10/23 22:03:54 visual_prompt]: 	Training 400/1106. train loss: 42.3653,	0.6200 s / batch. (data: 2.83e-04). ETA=18:35:53, max mem: 15.9 GB 
[10/23 22:04:57 visual_prompt]: 	Training 500/1106. train loss: 135.7412,	0.6187 s / batch. (data: 3.87e-04). ETA=18:32:33, max mem: 15.9 GB 
[10/23 22:05:59 visual_prompt]: 	Training 600/1106. train loss: 23.2834,	0.6339 s / batch. (data: 1.24e-03). ETA=18:58:47, max mem: 15.9 GB 
[10/23 22:07:05 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6386 s / batch. (data: 8.49e-04). ETA=19:06:04, max mem: 15.9 GB 
[10/23 22:08:09 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6482 s / batch. (data: 5.91e-03). ETA=19:22:19, max mem: 15.9 GB 
[10/23 22:09:12 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6344 s / batch. (data: 3.19e-04). ETA=18:56:26, max mem: 15.9 GB 
[10/23 22:10:15 visual_prompt]: 	Training 1000/1106. train loss: 57.6222,	0.6454 s / batch. (data: 5.99e-03). ETA=19:15:11, max mem: 15.9 GB 
[10/23 22:11:18 visual_prompt]: 	Training 1100/1106. train loss: 78.9610,	0.6129 s / batch. (data: 2.70e-04). ETA=18:15:52, max mem: 15.9 GB 
[10/23 22:11:22 visual_prompt]: Epoch 3 / 100: avg data time: 8.06e-03, avg batch time: 0.6364, average train loss: 44.2857
[10/23 22:12:12 visual_prompt]: 	Test 100/123. loss: 40.254, 0.2326 s / batch. (data: 3.74e-05)max mem: 15.88805 GB 
[10/23 22:12:23 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2311, average loss: 44.3250
[10/23 22:12:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.30	
[10/23 22:12:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/23 22:13:29 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6115 s / batch. (data: 3.01e-04). ETA=18:12:23, max mem: 15.9 GB 
[10/23 22:14:32 visual_prompt]: 	Training 200/1106. train loss: 219.8725,	0.6479 s / batch. (data: 1.07e-02). ETA=19:16:21, max mem: 15.9 GB 
[10/23 22:15:35 visual_prompt]: 	Training 300/1106. train loss: 15.0034,	0.6329 s / batch. (data: 7.65e-04). ETA=18:48:24, max mem: 15.9 GB 
[10/23 22:16:38 visual_prompt]: 	Training 400/1106. train loss: 97.0201,	0.6320 s / batch. (data: 3.63e-04). ETA=18:45:50, max mem: 15.9 GB 
[10/23 22:17:41 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6421 s / batch. (data: 1.41e-02). ETA=19:02:43, max mem: 15.9 GB 
[10/23 22:18:44 visual_prompt]: 	Training 600/1106. train loss: 1.5750,	0.6356 s / batch. (data: 7.50e-04). ETA=18:50:07, max mem: 15.9 GB 
[10/23 22:19:47 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6097 s / batch. (data: 7.84e-04). ETA=18:03:06, max mem: 15.9 GB 
[10/23 22:20:49 visual_prompt]: 	Training 800/1106. train loss: 12.9150,	0.6185 s / batch. (data: 3.84e-04). ETA=18:17:39, max mem: 15.9 GB 
[10/23 22:21:52 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6373 s / batch. (data: 7.32e-04). ETA=18:49:56, max mem: 15.9 GB 
[10/23 22:22:55 visual_prompt]: 	Training 1000/1106. train loss: 12.1979,	0.6445 s / batch. (data: 1.06e-02). ETA=19:01:35, max mem: 15.9 GB 
[10/23 22:23:59 visual_prompt]: 	Training 1100/1106. train loss: 45.2297,	0.6123 s / batch. (data: 2.04e-04). ETA=18:03:30, max mem: 15.9 GB 
[10/23 22:24:02 visual_prompt]: Epoch 4 / 100: avg data time: 4.77e-03, avg batch time: 0.6318, average train loss: 65.8904
[10/23 22:24:53 visual_prompt]: 	Test 100/123. loss: 135.665, 0.2356 s / batch. (data: 3.08e-05)max mem: 15.88805 GB 
[10/23 22:25:04 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2325, average loss: 123.0385
[10/23 22:25:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.00	
[10/23 22:25:04 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/23 22:26:09 visual_prompt]: 	Training 100/1106. train loss: 97.2543,	0.6267 s / batch. (data: 8.53e-04). ETA=18:27:59, max mem: 15.9 GB 
[10/23 22:27:12 visual_prompt]: 	Training 200/1106. train loss: 53.9001,	0.6469 s / batch. (data: 1.49e-02). ETA=19:02:40, max mem: 15.9 GB 
[10/23 22:28:15 visual_prompt]: 	Training 300/1106. train loss: 25.6364,	0.6451 s / batch. (data: 7.63e-04). ETA=18:58:20, max mem: 15.9 GB 
[10/23 22:29:18 visual_prompt]: 	Training 400/1106. train loss: 44.2170,	0.6173 s / batch. (data: 3.18e-04). ETA=18:08:17, max mem: 15.9 GB 
[10/23 22:30:21 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6210 s / batch. (data: 7.73e-04). ETA=18:13:45, max mem: 15.9 GB 
[10/23 22:31:24 visual_prompt]: 	Training 600/1106. train loss: 47.7470,	0.6338 s / batch. (data: 8.20e-04). ETA=18:35:14, max mem: 15.9 GB 
[10/23 22:32:27 visual_prompt]: 	Training 700/1106. train loss: 140.3451,	0.6348 s / batch. (data: 1.20e-03). ETA=18:35:53, max mem: 15.9 GB 
[10/23 22:33:30 visual_prompt]: 	Training 800/1106. train loss: 10.2788,	0.6389 s / batch. (data: 7.53e-04). ETA=18:42:01, max mem: 15.9 GB 
[10/23 22:34:33 visual_prompt]: 	Training 900/1106. train loss: 340.6313,	0.6297 s / batch. (data: 3.65e-04). ETA=18:24:54, max mem: 15.9 GB 
[10/23 22:35:36 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6161 s / batch. (data: 3.24e-04). ETA=18:00:02, max mem: 15.9 GB 
[10/23 22:36:39 visual_prompt]: 	Training 1100/1106. train loss: 142.8685,	0.6143 s / batch. (data: 2.43e-04). ETA=17:55:49, max mem: 15.9 GB 
[10/23 22:36:42 visual_prompt]: Epoch 5 / 100: avg data time: 4.80e-03, avg batch time: 0.6316, average train loss: 91.4435
[10/23 22:37:33 visual_prompt]: 	Test 100/123. loss: 245.158, 0.2419 s / batch. (data: 3.60e-05)max mem: 15.88805 GB 
[10/23 22:37:44 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2326, average loss: 220.4447
[10/23 22:37:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.34	
[10/23 22:37:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/23 22:38:49 visual_prompt]: 	Training 100/1106. train loss: 417.2016,	0.6237 s / batch. (data: 3.04e-04). ETA=18:11:10, max mem: 15.9 GB 
[10/23 22:39:52 visual_prompt]: 	Training 200/1106. train loss: 421.2350,	0.6475 s / batch. (data: 7.50e-04). ETA=18:51:46, max mem: 15.9 GB 
[10/23 22:40:55 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6231 s / batch. (data: 3.21e-04). ETA=18:08:03, max mem: 15.9 GB 
[10/23 22:41:58 visual_prompt]: 	Training 400/1106. train loss: 224.2868,	0.6280 s / batch. (data: 3.30e-04). ETA=18:15:29, max mem: 15.9 GB 
[10/23 22:43:01 visual_prompt]: 	Training 500/1106. train loss: 139.6360,	0.6240 s / batch. (data: 3.02e-04). ETA=18:07:32, max mem: 15.9 GB 
[10/23 22:44:04 visual_prompt]: 	Training 600/1106. train loss: 72.8576,	0.6451 s / batch. (data: 7.81e-04). ETA=18:43:13, max mem: 15.9 GB 
[10/23 22:45:07 visual_prompt]: 	Training 700/1106. train loss: 64.9925,	0.6240 s / batch. (data: 3.17e-04). ETA=18:05:25, max mem: 15.9 GB 
[10/23 22:46:10 visual_prompt]: 	Training 800/1106. train loss: 210.1074,	0.6131 s / batch. (data: 3.20e-04). ETA=17:45:25, max mem: 15.9 GB 
[10/23 22:47:13 visual_prompt]: 	Training 900/1106. train loss: 128.3392,	0.6476 s / batch. (data: 5.87e-03). ETA=18:44:25, max mem: 15.9 GB 
[10/23 22:48:16 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6301 s / batch. (data: 7.96e-04). ETA=18:12:56, max mem: 15.9 GB 
[10/23 22:49:18 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6077 s / batch. (data: 1.80e-04). ETA=17:33:01, max mem: 15.9 GB 
[10/23 22:49:22 visual_prompt]: Epoch 6 / 100: avg data time: 4.22e-03, avg batch time: 0.6312, average train loss: 112.4610
[10/23 22:50:13 visual_prompt]: 	Test 100/123. loss: 287.542, 0.2253 s / batch. (data: 4.32e-05)max mem: 15.88805 GB 
[10/23 22:50:24 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2320, average loss: 259.2788
[10/23 22:50:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.31	
[10/23 22:50:24 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/23 22:51:28 visual_prompt]: 	Training 100/1106. train loss: 98.5208,	0.6193 s / batch. (data: 3.38e-04). ETA=17:52:01, max mem: 15.9 GB 
[10/23 22:52:31 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6139 s / batch. (data: 4.09e-04). ETA=17:41:40, max mem: 15.9 GB 
[10/23 22:53:34 visual_prompt]: 	Training 300/1106. train loss: 270.3148,	0.6197 s / batch. (data: 8.15e-04). ETA=17:50:40, max mem: 15.9 GB 
[10/23 22:54:37 visual_prompt]: 	Training 400/1106. train loss: 100.6968,	0.6176 s / batch. (data: 4.96e-04). ETA=17:46:01, max mem: 15.9 GB 
[10/23 22:55:40 visual_prompt]: 	Training 500/1106. train loss: 27.3817,	0.6234 s / batch. (data: 3.44e-03). ETA=17:55:00, max mem: 15.9 GB 
[10/23 22:56:43 visual_prompt]: 	Training 600/1106. train loss: 106.8540,	0.6249 s / batch. (data: 3.29e-04). ETA=17:56:34, max mem: 15.9 GB 
[10/23 22:57:46 visual_prompt]: 	Training 700/1106. train loss: 451.0879,	0.6400 s / batch. (data: 4.21e-04). ETA=18:21:24, max mem: 15.9 GB 
[10/23 22:58:49 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6184 s / batch. (data: 2.86e-04). ETA=17:43:12, max mem: 15.9 GB 
[10/23 22:59:52 visual_prompt]: 	Training 900/1106. train loss: 183.2745,	0.6135 s / batch. (data: 7.51e-04). ETA=17:33:51, max mem: 15.9 GB 
[10/23 23:00:55 visual_prompt]: 	Training 1000/1106. train loss: 117.1072,	0.6323 s / batch. (data: 3.15e-04). ETA=18:05:05, max mem: 15.9 GB 
[10/23 23:01:57 visual_prompt]: 	Training 1100/1106. train loss: 0.0001,	0.6186 s / batch. (data: 2.19e-04). ETA=17:40:28, max mem: 15.9 GB 
[10/23 23:02:01 visual_prompt]: Epoch 7 / 100: avg data time: 4.77e-03, avg batch time: 0.6306, average train loss: 139.2894
[10/23 23:02:51 visual_prompt]: 	Test 100/123. loss: 254.217, 0.2362 s / batch. (data: 3.08e-05)max mem: 15.88805 GB 
[10/23 23:03:03 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2331, average loss: 250.7735
[10/23 23:03:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.60	
[10/23 23:03:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/23 23:04:08 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6074 s / batch. (data: 3.39e-04). ETA=17:20:16, max mem: 15.9 GB 
[10/23 23:05:10 visual_prompt]: 	Training 200/1106. train loss: 125.4227,	0.6280 s / batch. (data: 2.91e-04). ETA=17:54:30, max mem: 15.9 GB 
[10/23 23:06:13 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6081 s / batch. (data: 2.77e-04). ETA=17:19:20, max mem: 15.9 GB 
[10/23 23:07:16 visual_prompt]: 	Training 400/1106. train loss: 322.0450,	0.6192 s / batch. (data: 4.03e-04). ETA=17:37:18, max mem: 15.9 GB 
[10/23 23:08:19 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6094 s / batch. (data: 2.99e-04). ETA=17:19:38, max mem: 15.9 GB 
[10/23 23:09:22 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6370 s / batch. (data: 2.10e-02). ETA=18:05:34, max mem: 15.9 GB 
[10/23 23:10:25 visual_prompt]: 	Training 700/1106. train loss: 303.9340,	0.6136 s / batch. (data: 8.09e-04). ETA=17:24:47, max mem: 15.9 GB 
[10/23 23:11:28 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6285 s / batch. (data: 2.83e-04). ETA=17:48:59, max mem: 15.9 GB 
[10/23 23:12:30 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6087 s / batch. (data: 3.63e-04). ETA=17:14:25, max mem: 15.9 GB 
[10/23 23:13:33 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6083 s / batch. (data: 3.30e-04). ETA=17:12:35, max mem: 15.9 GB 
[10/23 23:14:36 visual_prompt]: 	Training 1100/1106. train loss: 225.6636,	0.6176 s / batch. (data: 1.52e-04). ETA=17:27:24, max mem: 15.9 GB 
[10/23 23:14:40 visual_prompt]: Epoch 8 / 100: avg data time: 4.27e-03, avg batch time: 0.6303, average train loss: 148.1846
[10/23 23:15:30 visual_prompt]: 	Test 100/123. loss: 471.579, 0.2262 s / batch. (data: 3.08e-05)max mem: 15.88805 GB 
[10/23 23:15:41 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2334, average loss: 422.7750
[10/23 23:15:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.19	
[10/23 23:15:41 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/23 23:16:46 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6265 s / batch. (data: 3.30e-04). ETA=17:41:21, max mem: 15.9 GB 
[10/23 23:17:49 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6741 s / batch. (data: 8.35e-04). ETA=19:00:57, max mem: 15.9 GB 
[10/23 23:18:52 visual_prompt]: 	Training 300/1106. train loss: 401.5913,	0.6254 s / batch. (data: 7.73e-04). ETA=17:37:28, max mem: 15.9 GB 
[10/23 23:19:54 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6094 s / batch. (data: 3.57e-04). ETA=17:09:25, max mem: 15.9 GB 
[10/23 23:20:57 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6462 s / batch. (data: 1.42e-02). ETA=18:10:27, max mem: 15.9 GB 
[10/23 23:22:00 visual_prompt]: 	Training 600/1106. train loss: 91.3479,	0.6228 s / batch. (data: 2.74e-04). ETA=17:30:02, max mem: 15.9 GB 
[10/23 23:23:02 visual_prompt]: 	Training 700/1106. train loss: 240.4525,	0.6141 s / batch. (data: 2.57e-04). ETA=17:14:16, max mem: 15.9 GB 
[10/23 23:24:05 visual_prompt]: 	Training 800/1106. train loss: 482.6885,	0.6167 s / batch. (data: 4.95e-04). ETA=17:17:32, max mem: 15.9 GB 
[10/23 23:25:08 visual_prompt]: 	Training 900/1106. train loss: 240.2178,	0.6400 s / batch. (data: 3.63e-04). ETA=17:55:48, max mem: 15.9 GB 
[10/23 23:26:10 visual_prompt]: 	Training 1000/1106. train loss: 173.3219,	0.6334 s / batch. (data: 3.15e-04). ETA=17:43:38, max mem: 15.9 GB 
[10/23 23:27:13 visual_prompt]: 	Training 1100/1106. train loss: 51.7314,	0.6145 s / batch. (data: 1.87e-04). ETA=17:10:54, max mem: 15.9 GB 
[10/23 23:27:17 visual_prompt]: Epoch 9 / 100: avg data time: 4.60e-03, avg batch time: 0.6290, average train loss: 181.2720
[10/23 23:28:07 visual_prompt]: 	Test 100/123. loss: 17.589, 0.2262 s / batch. (data: 3.03e-05)max mem: 15.88805 GB 
[10/23 23:28:18 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.2329, average loss: 24.9190
[10/23 23:28:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 50.13	
[10/23 23:28:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/23 23:29:23 visual_prompt]: 	Training 100/1106. train loss: 27.7607,	0.6225 s / batch. (data: 3.12e-04). ETA=17:23:10, max mem: 15.9 GB 
[10/23 23:30:26 visual_prompt]: 	Training 200/1106. train loss: 318.3625,	0.6294 s / batch. (data: 1.60e-02). ETA=17:33:39, max mem: 15.9 GB 
[10/23 23:31:29 visual_prompt]: 	Training 300/1106. train loss: 163.0788,	0.6458 s / batch. (data: 1.19e-02). ETA=18:00:00, max mem: 15.9 GB 
[10/23 23:32:32 visual_prompt]: 	Training 400/1106. train loss: 15.1817,	0.6320 s / batch. (data: 7.98e-04). ETA=17:35:50, max mem: 15.9 GB 
[10/23 23:33:34 visual_prompt]: 	Training 500/1106. train loss: 102.1859,	0.6196 s / batch. (data: 3.15e-04). ETA=17:14:12, max mem: 15.9 GB 
[10/23 23:34:37 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6240 s / batch. (data: 8.62e-04). ETA=17:20:28, max mem: 15.9 GB 
[10/23 23:35:40 visual_prompt]: 	Training 700/1106. train loss: 158.6999,	0.6349 s / batch. (data: 8.38e-04). ETA=17:37:38, max mem: 15.9 GB 
[10/23 23:36:43 visual_prompt]: 	Training 800/1106. train loss: 44.7102,	0.6520 s / batch. (data: 7.94e-04). ETA=18:05:01, max mem: 15.9 GB 
[10/23 23:37:46 visual_prompt]: 	Training 900/1106. train loss: 214.9229,	0.6328 s / batch. (data: 8.74e-04). ETA=17:31:54, max mem: 15.9 GB 
[10/23 23:38:48 visual_prompt]: 	Training 1000/1106. train loss: 593.2907,	0.6353 s / batch. (data: 8.11e-04). ETA=17:35:03, max mem: 15.9 GB 
[10/23 23:39:51 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6080 s / batch. (data: 1.69e-04). ETA=16:48:42, max mem: 15.9 GB 
[10/23 23:39:55 visual_prompt]: Epoch 10 / 100: avg data time: 4.69e-03, avg batch time: 0.6298, average train loss: 175.3739
[10/23 23:40:45 visual_prompt]: 	Test 100/123. loss: 131.597, 0.2406 s / batch. (data: 4.94e-05)max mem: 15.88805 GB 
[10/23 23:40:56 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2332, average loss: 121.9892
[10/23 23:40:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.64	
[10/23 23:40:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/23 23:42:01 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6334 s / batch. (data: 7.69e-04). ETA=17:29:43, max mem: 15.9 GB 
[10/23 23:43:04 visual_prompt]: 	Training 200/1106. train loss: 285.0993,	0.6354 s / batch. (data: 6.02e-03). ETA=17:32:05, max mem: 15.9 GB 
[10/23 23:44:07 visual_prompt]: 	Training 300/1106. train loss: 341.5803,	0.6121 s / batch. (data: 3.14e-04). ETA=16:52:25, max mem: 15.9 GB 
[10/23 23:45:10 visual_prompt]: 	Training 400/1106. train loss: 968.9158,	0.6186 s / batch. (data: 2.82e-04). ETA=17:02:08, max mem: 15.9 GB 
[10/23 23:46:12 visual_prompt]: 	Training 500/1106. train loss: 240.5148,	0.6134 s / batch. (data: 3.09e-04). ETA=16:52:35, max mem: 15.9 GB 
[10/23 23:47:15 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6186 s / batch. (data: 8.35e-04). ETA=17:00:05, max mem: 15.9 GB 
[10/23 23:48:18 visual_prompt]: 	Training 700/1106. train loss: 768.4421,	0.6218 s / batch. (data: 3.07e-04). ETA=17:04:18, max mem: 15.9 GB 
[10/23 23:49:21 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6200 s / batch. (data: 7.63e-04). ETA=17:00:22, max mem: 15.9 GB 
[10/23 23:50:24 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6168 s / batch. (data: 3.24e-04). ETA=16:53:56, max mem: 15.9 GB 
[10/23 23:51:27 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	1.7141 s / batch. (data: 1.10e+00). ETA=1 day, 22:55:09, max mem: 15.9 GB 
[10/23 23:52:32 visual_prompt]: 	Training 1100/1106. train loss: 8.4036,	0.6190 s / batch. (data: 1.99e-04). ETA=16:55:34, max mem: 15.9 GB 
[10/23 23:52:35 visual_prompt]: Epoch 11 / 100: avg data time: 6.78e-03, avg batch time: 0.6322, average train loss: 221.0753
[10/23 23:53:26 visual_prompt]: 	Test 100/123. loss: 115.737, 0.2317 s / batch. (data: 2.86e-05)max mem: 15.88805 GB 
[10/23 23:53:37 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2333, average loss: 127.0357
[10/23 23:53:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.18	
[10/23 23:53:37 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/23 23:54:43 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6203 s / batch. (data: 3.21e-04). ETA=16:56:34, max mem: 15.9 GB 
[10/23 23:55:45 visual_prompt]: 	Training 200/1106. train loss: 394.2355,	0.6269 s / batch. (data: 8.22e-04). ETA=17:06:21, max mem: 15.9 GB 
[10/23 23:56:48 visual_prompt]: 	Training 300/1106. train loss: 387.9666,	0.6182 s / batch. (data: 3.17e-04). ETA=16:51:08, max mem: 15.9 GB 
[10/23 23:57:51 visual_prompt]: 	Training 400/1106. train loss: 696.7471,	0.6301 s / batch. (data: 1.05e-02). ETA=17:09:28, max mem: 15.9 GB 
[10/23 23:58:54 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6420 s / batch. (data: 1.39e-02). ETA=17:27:52, max mem: 15.9 GB 
[10/23 23:59:56 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6173 s / batch. (data: 8.33e-04). ETA=16:46:28, max mem: 15.9 GB 
[10/24 00:00:59 visual_prompt]: 	Training 700/1106. train loss: 180.2896,	0.6201 s / batch. (data: 3.04e-04). ETA=16:50:03, max mem: 15.9 GB 
[10/24 00:02:08 visual_prompt]: 	Training 800/1106. train loss: 139.4487,	0.6390 s / batch. (data: 5.48e-03). ETA=17:19:50, max mem: 15.9 GB 
[10/24 00:03:11 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6123 s / batch. (data: 7.56e-04). ETA=16:35:19, max mem: 15.9 GB 
[10/24 00:04:16 visual_prompt]: 	Training 1000/1106. train loss: 9.5004,	0.6308 s / batch. (data: 8.23e-04). ETA=17:04:21, max mem: 15.9 GB 
[10/24 00:05:20 visual_prompt]: 	Training 1100/1106. train loss: 15.8632,	0.6166 s / batch. (data: 1.45e-04). ETA=16:40:16, max mem: 15.9 GB 
[10/24 00:05:24 visual_prompt]: Epoch 12 / 100: avg data time: 1.48e-02, avg batch time: 0.6397, average train loss: 221.7713
[10/24 00:06:14 visual_prompt]: 	Test 100/123. loss: 84.217, 0.2481 s / batch. (data: 2.29e-05)max mem: 15.88805 GB 
[10/24 00:06:27 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2317, average loss: 97.5583
[10/24 00:06:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.65	
[10/24 00:06:27 visual_prompt]: Best epoch 12: best metric: -97.558
[10/24 00:06:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/24 00:07:32 visual_prompt]: 	Training 100/1106. train loss: 485.4920,	0.6165 s / batch. (data: 3.24e-04). ETA=16:39:01, max mem: 15.9 GB 
[10/24 00:08:35 visual_prompt]: 	Training 200/1106. train loss: 656.6922,	0.6179 s / batch. (data: 4.76e-04). ETA=16:40:10, max mem: 15.9 GB 
[10/24 00:09:38 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6215 s / batch. (data: 7.91e-04). ETA=16:44:59, max mem: 15.9 GB 
[10/24 00:10:41 visual_prompt]: 	Training 400/1106. train loss: 612.1642,	0.6281 s / batch. (data: 8.10e-04). ETA=16:54:43, max mem: 15.9 GB 
[10/24 00:11:43 visual_prompt]: 	Training 500/1106. train loss: 278.6561,	0.6404 s / batch. (data: 8.25e-04). ETA=17:13:23, max mem: 15.9 GB 
[10/24 00:12:46 visual_prompt]: 	Training 600/1106. train loss: 219.8076,	0.6253 s / batch. (data: 3.09e-04). ETA=16:48:07, max mem: 15.9 GB 
[10/24 00:13:49 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6320 s / batch. (data: 3.47e-04). ETA=16:57:48, max mem: 15.9 GB 
[10/24 00:14:52 visual_prompt]: 	Training 800/1106. train loss: 140.4039,	0.6336 s / batch. (data: 2.97e-04). ETA=16:59:23, max mem: 15.9 GB 
[10/24 00:15:55 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6087 s / batch. (data: 3.12e-04). ETA=16:18:20, max mem: 15.9 GB 
[10/24 00:16:57 visual_prompt]: 	Training 1000/1106. train loss: 15.1508,	0.6451 s / batch. (data: 7.49e-04). ETA=17:15:37, max mem: 15.9 GB 
[10/24 00:18:00 visual_prompt]: 	Training 1100/1106. train loss: 301.2523,	0.6123 s / batch. (data: 1.79e-04). ETA=16:22:00, max mem: 15.9 GB 
[10/24 00:18:04 visual_prompt]: Epoch 13 / 100: avg data time: 5.17e-03, avg batch time: 0.6304, average train loss: 217.0686
[10/24 00:18:55 visual_prompt]: 	Test 100/123. loss: 216.868, 0.2617 s / batch. (data: 3.50e-05)max mem: 15.88805 GB 
[10/24 00:19:05 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2314, average loss: 200.6579
[10/24 00:19:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.42	
[10/24 00:19:05 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/24 00:20:10 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6286 s / batch. (data: 8.09e-04). ETA=16:47:00, max mem: 15.9 GB 
[10/24 00:21:13 visual_prompt]: 	Training 200/1106. train loss: 642.1770,	0.6139 s / batch. (data: 3.85e-04). ETA=16:22:26, max mem: 15.9 GB 
[10/24 00:22:16 visual_prompt]: 	Training 300/1106. train loss: 88.4017,	0.6385 s / batch. (data: 1.05e-02). ETA=17:00:48, max mem: 15.9 GB 
[10/24 00:23:19 visual_prompt]: 	Training 400/1106. train loss: 457.3574,	0.6291 s / batch. (data: 8.28e-04). ETA=16:44:41, max mem: 15.9 GB 
[10/24 00:24:21 visual_prompt]: 	Training 500/1106. train loss: 176.0507,	0.6534 s / batch. (data: 4.22e-02). ETA=17:22:23, max mem: 15.9 GB 
[10/24 00:25:24 visual_prompt]: 	Training 600/1106. train loss: 395.3578,	0.6296 s / batch. (data: 8.01e-04). ETA=16:43:19, max mem: 15.9 GB 
[10/24 00:26:27 visual_prompt]: 	Training 700/1106. train loss: 892.3925,	0.6377 s / batch. (data: 5.42e-03). ETA=16:55:10, max mem: 15.9 GB 
[10/24 00:27:30 visual_prompt]: 	Training 800/1106. train loss: 0.0708,	0.6244 s / batch. (data: 1.20e-02). ETA=16:32:59, max mem: 15.9 GB 
[10/24 00:28:33 visual_prompt]: 	Training 900/1106. train loss: 8.5061,	0.6154 s / batch. (data: 3.20e-04). ETA=16:17:37, max mem: 15.9 GB 
[10/24 00:29:36 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6298 s / batch. (data: 3.15e-04). ETA=16:39:33, max mem: 15.9 GB 
[10/24 00:30:39 visual_prompt]: 	Training 1100/1106. train loss: 219.2570,	0.6123 s / batch. (data: 1.24e-04). ETA=16:10:45, max mem: 15.9 GB 
[10/24 00:30:42 visual_prompt]: Epoch 14 / 100: avg data time: 4.78e-03, avg batch time: 0.6305, average train loss: 195.2581
[10/24 00:31:34 visual_prompt]: 	Test 100/123. loss: 90.185, 0.2357 s / batch. (data: 3.19e-05)max mem: 15.88805 GB 
[10/24 00:31:44 visual_prompt]: Inference (val):avg data time: 1.12e-04, avg batch time: 0.2316, average loss: 98.7433
[10/24 00:31:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.76	
[10/24 00:31:44 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/24 00:32:49 visual_prompt]: 	Training 100/1106. train loss: 280.0917,	0.6261 s / batch. (data: 8.17e-04). ETA=16:31:26, max mem: 15.9 GB 
[10/24 00:33:51 visual_prompt]: 	Training 200/1106. train loss: 401.9496,	0.6183 s / batch. (data: 8.70e-04). ETA=16:18:03, max mem: 15.9 GB 
[10/24 00:34:54 visual_prompt]: 	Training 300/1106. train loss: 326.9548,	0.6360 s / batch. (data: 9.22e-04). ETA=16:45:03, max mem: 15.9 GB 
[10/24 00:36:02 visual_prompt]: 	Training 400/1106. train loss: 289.3702,	0.6194 s / batch. (data: 7.84e-04). ETA=16:17:50, max mem: 15.9 GB 
[10/24 00:37:06 visual_prompt]: 	Training 500/1106. train loss: 0.3078,	0.6240 s / batch. (data: 7.45e-04). ETA=16:24:04, max mem: 15.9 GB 
[10/24 00:38:09 visual_prompt]: 	Training 600/1106. train loss: 24.3263,	0.6309 s / batch. (data: 8.19e-04). ETA=16:33:47, max mem: 15.9 GB 
[10/24 00:39:18 visual_prompt]: 	Training 700/1106. train loss: 830.0529,	0.6320 s / batch. (data: 3.05e-04). ETA=16:34:30, max mem: 15.9 GB 
[10/24 00:40:22 visual_prompt]: 	Training 800/1106. train loss: 44.8379,	0.6276 s / batch. (data: 6.84e-04). ETA=16:26:32, max mem: 15.9 GB 
[10/24 00:41:25 visual_prompt]: 	Training 900/1106. train loss: 867.4948,	0.6171 s / batch. (data: 3.19e-04). ETA=16:09:02, max mem: 15.9 GB 
[10/24 00:42:28 visual_prompt]: 	Training 1000/1106. train loss: 423.0663,	0.6174 s / batch. (data: 2.63e-04). ETA=16:08:24, max mem: 15.9 GB 
[10/24 00:43:31 visual_prompt]: 	Training 1100/1106. train loss: 282.6359,	0.6132 s / batch. (data: 1.55e-04). ETA=16:00:49, max mem: 15.9 GB 
[10/24 00:43:35 visual_prompt]: Epoch 15 / 100: avg data time: 1.77e-02, avg batch time: 0.6422, average train loss: 225.6892
[10/24 00:44:27 visual_prompt]: 	Test 100/123. loss: 375.358, 0.2277 s / batch. (data: 3.00e-05)max mem: 15.88805 GB 
[10/24 00:44:37 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.2314, average loss: 334.0556
[10/24 00:44:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.73	
[10/24 00:44:37 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/24 00:45:42 visual_prompt]: 	Training 100/1106. train loss: 336.7470,	0.6181 s / batch. (data: 3.16e-04). ETA=16:07:25, max mem: 15.9 GB 
[10/24 00:46:45 visual_prompt]: 	Training 200/1106. train loss: 419.9223,	0.6263 s / batch. (data: 7.55e-04). ETA=16:19:12, max mem: 15.9 GB 
[10/24 00:47:48 visual_prompt]: 	Training 300/1106. train loss: 979.1212,	0.6330 s / batch. (data: 3.23e-04). ETA=16:28:39, max mem: 15.9 GB 
[10/24 00:48:50 visual_prompt]: 	Training 400/1106. train loss: 250.3063,	0.6192 s / batch. (data: 3.66e-04). ETA=16:06:05, max mem: 15.9 GB 
[10/24 00:49:53 visual_prompt]: 	Training 500/1106. train loss: 99.3463,	0.6527 s / batch. (data: 8.85e-04). ETA=16:57:18, max mem: 15.9 GB 
[10/24 00:50:56 visual_prompt]: 	Training 600/1106. train loss: 4.0155,	0.6193 s / batch. (data: 3.09e-04). ETA=16:04:11, max mem: 15.9 GB 
[10/24 00:51:59 visual_prompt]: 	Training 700/1106. train loss: 364.4892,	0.6221 s / batch. (data: 3.36e-04). ETA=16:07:25, max mem: 15.9 GB 
[10/24 00:53:02 visual_prompt]: 	Training 800/1106. train loss: 60.2867,	0.6387 s / batch. (data: 9.05e-04). ETA=16:32:08, max mem: 15.9 GB 
[10/24 00:54:05 visual_prompt]: 	Training 900/1106. train loss: 209.5277,	0.6378 s / batch. (data: 8.73e-04). ETA=16:29:46, max mem: 15.9 GB 
[10/24 00:55:07 visual_prompt]: 	Training 1000/1106. train loss: 870.8479,	0.6465 s / batch. (data: 3.45e-04). ETA=16:42:08, max mem: 15.9 GB 
[10/24 00:56:10 visual_prompt]: 	Training 1100/1106. train loss: 195.4234,	0.6182 s / batch. (data: 1.54e-04). ETA=15:57:13, max mem: 15.9 GB 
[10/24 00:56:14 visual_prompt]: Epoch 16 / 100: avg data time: 4.12e-03, avg batch time: 0.6299, average train loss: 230.8488
[10/24 00:57:06 visual_prompt]: 	Test 100/123. loss: 120.159, 0.2640 s / batch. (data: 2.57e-05)max mem: 15.88805 GB 
[10/24 00:57:16 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2334, average loss: 131.3931
[10/24 00:57:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.20	
[10/24 00:57:16 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/24 00:58:21 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6188 s / batch. (data: 3.59e-04). ETA=15:57:03, max mem: 15.9 GB 
[10/24 00:59:24 visual_prompt]: 	Training 200/1106. train loss: 743.2273,	0.6266 s / batch. (data: 4.02e-04). ETA=16:08:04, max mem: 15.9 GB 
[10/24 01:00:26 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6268 s / batch. (data: 5.58e-03). ETA=16:07:20, max mem: 15.9 GB 
[10/24 01:01:29 visual_prompt]: 	Training 400/1106. train loss: 229.7598,	0.6448 s / batch. (data: 2.48e-02). ETA=16:34:06, max mem: 15.9 GB 
[10/24 01:02:32 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6079 s / batch. (data: 3.19e-04). ETA=15:36:12, max mem: 15.9 GB 
[10/24 01:03:35 visual_prompt]: 	Training 600/1106. train loss: 81.6837,	0.6187 s / batch. (data: 2.70e-04). ETA=15:51:50, max mem: 15.9 GB 
[10/24 01:04:38 visual_prompt]: 	Training 700/1106. train loss: 136.9234,	0.6194 s / batch. (data: 2.54e-04). ETA=15:51:54, max mem: 15.9 GB 
[10/24 01:05:46 visual_prompt]: 	Training 800/1106. train loss: 286.7973,	0.6385 s / batch. (data: 7.58e-04). ETA=16:20:12, max mem: 15.9 GB 
[10/24 01:06:49 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6349 s / batch. (data: 7.68e-04). ETA=16:13:30, max mem: 15.9 GB 
[10/24 01:07:51 visual_prompt]: 	Training 1000/1106. train loss: 266.9612,	0.6337 s / batch. (data: 8.39e-04). ETA=16:10:36, max mem: 15.9 GB 
[10/24 01:08:54 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6071 s / batch. (data: 1.27e-04). ETA=15:28:53, max mem: 15.9 GB 
[10/24 01:08:58 visual_prompt]: Epoch 17 / 100: avg data time: 9.54e-03, avg batch time: 0.6349, average train loss: 226.6672
[10/24 01:09:48 visual_prompt]: 	Test 100/123. loss: 680.274, 0.2428 s / batch. (data: 4.74e-05)max mem: 15.88805 GB 
[10/24 01:09:58 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2345, average loss: 588.7328
[10/24 01:09:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.88	
[10/24 01:09:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/24 01:11:03 visual_prompt]: 	Training 100/1106. train loss: 369.6573,	0.6616 s / batch. (data: 1.18e-03). ETA=16:51:05, max mem: 15.9 GB 
[10/24 01:12:06 visual_prompt]: 	Training 200/1106. train loss: 457.4185,	0.6191 s / batch. (data: 3.03e-04). ETA=15:45:07, max mem: 15.9 GB 
[10/24 01:13:09 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6118 s / batch. (data: 3.65e-04). ETA=15:33:01, max mem: 15.9 GB 
[10/24 01:14:12 visual_prompt]: 	Training 400/1106. train loss: 93.2881,	0.6447 s / batch. (data: 8.89e-04). ETA=16:22:03, max mem: 15.9 GB 
[10/24 01:15:15 visual_prompt]: 	Training 500/1106. train loss: 257.7671,	0.6303 s / batch. (data: 7.88e-04). ETA=15:59:08, max mem: 15.9 GB 
[10/24 01:16:17 visual_prompt]: 	Training 600/1106. train loss: 114.0009,	0.6262 s / batch. (data: 3.90e-04). ETA=15:51:44, max mem: 15.9 GB 
[10/24 01:17:20 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6430 s / batch. (data: 7.83e-04). ETA=16:16:14, max mem: 15.9 GB 
[10/24 01:18:23 visual_prompt]: 	Training 800/1106. train loss: 15.4966,	0.6334 s / batch. (data: 3.36e-04). ETA=16:00:40, max mem: 15.9 GB 
[10/24 01:19:26 visual_prompt]: 	Training 900/1106. train loss: 304.3315,	0.6286 s / batch. (data: 7.63e-04). ETA=15:52:20, max mem: 15.9 GB 
[10/24 01:20:29 visual_prompt]: 	Training 1000/1106. train loss: 28.4514,	0.6443 s / batch. (data: 8.46e-04). ETA=16:15:01, max mem: 15.9 GB 
[10/24 01:21:32 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6079 s / batch. (data: 1.49e-04). ETA=15:18:53, max mem: 15.9 GB 
[10/24 01:21:35 visual_prompt]: Epoch 18 / 100: avg data time: 4.49e-03, avg batch time: 0.6301, average train loss: 216.7278
[10/24 01:22:26 visual_prompt]: 	Test 100/123. loss: 565.687, 0.2498 s / batch. (data: 3.70e-05)max mem: 15.88805 GB 
[10/24 01:22:36 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2327, average loss: 619.5292
[10/24 01:22:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.84	
[10/24 01:22:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[10/24 01:23:41 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6485 s / batch. (data: 2.85e-02). ETA=16:19:07, max mem: 15.9 GB 
[10/24 01:24:44 visual_prompt]: 	Training 200/1106. train loss: 207.1321,	0.6136 s / batch. (data: 3.11e-04). ETA=15:25:27, max mem: 15.9 GB 
[10/24 01:25:47 visual_prompt]: 	Training 300/1106. train loss: 42.5812,	0.6275 s / batch. (data: 3.35e-04). ETA=15:45:17, max mem: 15.9 GB 
[10/24 01:26:50 visual_prompt]: 	Training 400/1106. train loss: 542.2055,	0.6123 s / batch. (data: 3.18e-04). ETA=15:21:25, max mem: 15.9 GB 
[10/24 01:27:53 visual_prompt]: 	Training 500/1106. train loss: 385.2707,	0.6438 s / batch. (data: 8.27e-04). ETA=16:07:49, max mem: 15.9 GB 
[10/24 01:28:56 visual_prompt]: 	Training 600/1106. train loss: 459.8913,	0.6298 s / batch. (data: 7.77e-04). ETA=15:45:40, max mem: 15.9 GB 
[10/24 01:29:58 visual_prompt]: 	Training 700/1106. train loss: 33.5591,	0.6200 s / batch. (data: 3.29e-04). ETA=15:29:58, max mem: 15.9 GB 
[10/24 01:31:01 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6172 s / batch. (data: 7.55e-04). ETA=15:24:41, max mem: 15.9 GB 
[10/24 01:32:04 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6083 s / batch. (data: 3.97e-04). ETA=15:10:19, max mem: 15.9 GB 
[10/24 01:33:07 visual_prompt]: 	Training 1000/1106. train loss: 6.6420,	0.6244 s / batch. (data: 2.95e-04). ETA=15:33:26, max mem: 15.9 GB 
[10/24 01:34:09 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6194 s / batch. (data: 1.83e-04). ETA=15:24:52, max mem: 15.9 GB 
[10/24 01:34:13 visual_prompt]: Epoch 19 / 100: avg data time: 4.61e-03, avg batch time: 0.6299, average train loss: 205.1725
[10/24 01:35:04 visual_prompt]: 	Test 100/123. loss: 401.421, 0.2248 s / batch. (data: 4.29e-05)max mem: 15.88805 GB 
[10/24 01:35:14 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2323, average loss: 361.9676
[10/24 01:35:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.16	
[10/24 01:35:14 visual_prompt]: Stopping early.
[10/24 01:35:14 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 01:35:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 01:35:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 01:35:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 01:35:14 visual_prompt]: Training with config:
[10/24 01:35:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr50.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 01:35:14 visual_prompt]: Loading training data...
[10/24 01:35:14 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 01:35:14 visual_prompt]: Loading validation data...
[10/24 01:35:14 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 01:35:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/24 01:35:17 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/24 01:35:17 visual_prompt]: tuned percent:0.522
[10/24 01:35:17 visual_prompt]: Device used for model: 0
[10/24 01:35:17 visual_prompt]: Setting up Evaluator...
[10/24 01:35:17 visual_prompt]: Setting up Trainer...
[10/24 01:35:17 visual_prompt]: 	Setting up the optimizer...
[10/24 01:35:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 01:36:22 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6242 s / batch. (data: 3.27e-04). ETA=19:09:38, max mem: 15.9 GB 
[10/24 01:37:26 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6180 s / batch. (data: 2.80e-04). ETA=18:57:10, max mem: 15.9 GB 
[10/24 01:38:29 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6189 s / batch. (data: 3.17e-04). ETA=18:57:49, max mem: 15.9 GB 
[10/24 01:39:32 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6418 s / batch. (data: 7.93e-04). ETA=19:38:44, max mem: 15.9 GB 
[10/24 01:40:36 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6317 s / batch. (data: 3.25e-04). ETA=19:19:06, max mem: 15.9 GB 
[10/24 01:41:39 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6451 s / batch. (data: 8.23e-04). ETA=19:42:41, max mem: 15.9 GB 
[10/24 01:42:42 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6205 s / batch. (data: 3.19e-04). ETA=18:56:28, max mem: 15.9 GB 
[10/24 01:43:46 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6241 s / batch. (data: 2.66e-04). ETA=19:02:00, max mem: 15.9 GB 
[10/24 01:44:49 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6520 s / batch. (data: 8.50e-04). ETA=19:52:02, max mem: 15.9 GB 
[10/24 01:45:53 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6271 s / batch. (data: 3.32e-04). ETA=19:05:24, max mem: 15.9 GB 
[10/24 01:46:56 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6182 s / batch. (data: 1.37e-04). ETA=18:48:18, max mem: 15.9 GB 
[10/24 01:47:00 visual_prompt]: Epoch 1 / 100: avg data time: 5.64e-03, avg batch time: 0.6361, average train loss: 1.4028
[10/24 01:47:50 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2346 s / batch. (data: 3.62e-05)max mem: 15.89433 GB 
[10/24 01:48:01 visual_prompt]: Inference (val):avg data time: 2.32e-04, avg batch time: 0.2314, average loss: 1.3505
[10/24 01:48:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/24 01:48:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/24 01:49:06 visual_prompt]: 	Training 100/1106. train loss: 21.4100,	0.6295 s / batch. (data: 3.11e-04). ETA=19:07:46, max mem: 15.9 GB 
[10/24 01:50:09 visual_prompt]: 	Training 200/1106. train loss: 19.2485,	0.6387 s / batch. (data: 1.53e-02). ETA=19:23:31, max mem: 15.9 GB 
[10/24 01:51:12 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6360 s / batch. (data: 1.55e-02). ETA=19:17:25, max mem: 15.9 GB 
[10/24 01:52:15 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6257 s / batch. (data: 4.52e-04). ETA=18:57:43, max mem: 15.9 GB 
[10/24 01:53:18 visual_prompt]: 	Training 500/1106. train loss: 1.0976,	0.6311 s / batch. (data: 1.20e-02). ETA=19:06:21, max mem: 15.9 GB 
[10/24 01:54:22 visual_prompt]: 	Training 600/1106. train loss: 20.4981,	0.6241 s / batch. (data: 5.43e-03). ETA=18:52:39, max mem: 15.9 GB 
[10/24 01:55:25 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6364 s / batch. (data: 5.86e-03). ETA=19:13:58, max mem: 15.9 GB 
[10/24 01:56:28 visual_prompt]: 	Training 800/1106. train loss: 5.2499,	0.6204 s / batch. (data: 3.44e-04). ETA=18:43:52, max mem: 15.9 GB 
[10/24 01:57:31 visual_prompt]: 	Training 900/1106. train loss: 5.1987,	0.6365 s / batch. (data: 7.72e-04). ETA=19:12:03, max mem: 15.9 GB 
[10/24 01:58:35 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6303 s / batch. (data: 3.22e-04). ETA=18:59:47, max mem: 15.9 GB 
[10/24 01:59:38 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6077 s / batch. (data: 1.39e-04). ETA=18:17:55, max mem: 15.9 GB 
[10/24 01:59:42 visual_prompt]: Epoch 2 / 100: avg data time: 4.07e-03, avg batch time: 0.6332, average train loss: 25.7782
[10/24 02:00:32 visual_prompt]: 	Test 100/123. loss: 70.543, 0.2252 s / batch. (data: 3.10e-05)max mem: 15.89433 GB 
[10/24 02:00:42 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.2313, average loss: 63.2533
[10/24 02:00:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.30	
[10/24 02:00:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/24 02:01:48 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6302 s / batch. (data: 3.21e-04). ETA=18:57:18, max mem: 15.9 GB 
[10/24 02:02:51 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6279 s / batch. (data: 5.96e-03). ETA=18:52:09, max mem: 15.9 GB 
[10/24 02:03:54 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6086 s / batch. (data: 3.50e-04). ETA=18:16:20, max mem: 15.9 GB 
[10/24 02:04:57 visual_prompt]: 	Training 400/1106. train loss: 16.5279,	0.6174 s / batch. (data: 3.26e-04). ETA=18:31:10, max mem: 15.9 GB 
[10/24 02:06:00 visual_prompt]: 	Training 500/1106. train loss: 94.8930,	0.6178 s / batch. (data: 3.20e-04). ETA=18:30:48, max mem: 15.9 GB 
[10/24 02:07:03 visual_prompt]: 	Training 600/1106. train loss: 9.9854,	0.6343 s / batch. (data: 8.19e-04). ETA=18:59:32, max mem: 15.9 GB 
[10/24 02:08:06 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6076 s / batch. (data: 3.38e-04). ETA=18:10:27, max mem: 15.9 GB 
[10/24 02:09:09 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6240 s / batch. (data: 7.94e-04). ETA=18:38:54, max mem: 15.9 GB 
[10/24 02:10:12 visual_prompt]: 	Training 900/1106. train loss: 165.9785,	0.6400 s / batch. (data: 3.31e-04). ETA=19:06:28, max mem: 15.9 GB 
[10/24 02:11:15 visual_prompt]: 	Training 1000/1106. train loss: 214.1263,	0.6347 s / batch. (data: 7.50e-04). ETA=18:55:58, max mem: 15.9 GB 
[10/24 02:12:18 visual_prompt]: 	Training 1100/1106. train loss: 195.8976,	0.6118 s / batch. (data: 1.59e-04). ETA=18:14:00, max mem: 15.9 GB 
[10/24 02:12:22 visual_prompt]: Epoch 3 / 100: avg data time: 5.01e-03, avg batch time: 0.6322, average train loss: 49.6027
[10/24 02:13:12 visual_prompt]: 	Test 100/123. loss: 226.522, 0.2427 s / batch. (data: 3.53e-05)max mem: 15.89433 GB 
[10/24 02:13:23 visual_prompt]: Inference (val):avg data time: 1.22e-04, avg batch time: 0.2323, average loss: 163.9358
[10/24 02:13:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.63	
[10/24 02:13:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/24 02:14:27 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6159 s / batch. (data: 3.42e-04). ETA=18:20:17, max mem: 15.9 GB 
[10/24 02:15:30 visual_prompt]: 	Training 200/1106. train loss: 506.0186,	0.6176 s / batch. (data: 2.84e-04). ETA=18:22:17, max mem: 15.9 GB 
[10/24 02:16:33 visual_prompt]: 	Training 300/1106. train loss: 70.1136,	0.6132 s / batch. (data: 7.87e-04). ETA=18:13:16, max mem: 15.9 GB 
[10/24 02:17:36 visual_prompt]: 	Training 400/1106. train loss: 30.6747,	0.6453 s / batch. (data: 2.87e-02). ETA=19:09:30, max mem: 15.9 GB 
[10/24 02:18:39 visual_prompt]: 	Training 500/1106. train loss: 44.2135,	0.6190 s / batch. (data: 7.71e-04). ETA=18:21:38, max mem: 15.9 GB 
[10/24 02:19:42 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6264 s / batch. (data: 7.84e-04). ETA=18:33:50, max mem: 15.9 GB 
[10/24 02:20:44 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6121 s / batch. (data: 3.21e-04). ETA=18:07:17, max mem: 15.9 GB 
[10/24 02:21:47 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6067 s / batch. (data: 3.42e-04). ETA=17:56:42, max mem: 15.9 GB 
[10/24 02:22:50 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6399 s / batch. (data: 7.99e-04). ETA=18:54:37, max mem: 15.9 GB 
[10/24 02:23:53 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6335 s / batch. (data: 5.45e-03). ETA=18:42:10, max mem: 15.9 GB 
[10/24 02:24:56 visual_prompt]: 	Training 1100/1106. train loss: 156.7293,	0.6122 s / batch. (data: 1.44e-04). ETA=18:03:28, max mem: 15.9 GB 
[10/24 02:25:00 visual_prompt]: Epoch 4 / 100: avg data time: 4.76e-03, avg batch time: 0.6303, average train loss: 88.3366
[10/24 02:25:50 visual_prompt]: 	Test 100/123. loss: 3.331, 0.2248 s / batch. (data: 3.10e-05)max mem: 15.89433 GB 
[10/24 02:26:01 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.2331, average loss: 4.4587
[10/24 02:26:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.39	
[10/24 02:26:01 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/24 02:27:05 visual_prompt]: 	Training 100/1106. train loss: 43.7509,	0.6136 s / batch. (data: 2.76e-04). ETA=18:04:43, max mem: 15.9 GB 
[10/24 02:28:08 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6062 s / batch. (data: 3.31e-04). ETA=17:50:41, max mem: 15.9 GB 
[10/24 02:29:11 visual_prompt]: 	Training 300/1106. train loss: 103.9735,	0.6256 s / batch. (data: 8.01e-04). ETA=18:23:52, max mem: 15.9 GB 
[10/24 02:30:14 visual_prompt]: 	Training 400/1106. train loss: 73.2603,	0.6251 s / batch. (data: 7.57e-04). ETA=18:21:56, max mem: 15.9 GB 
[10/24 02:31:17 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6071 s / batch. (data: 3.35e-04). ETA=17:49:21, max mem: 15.9 GB 
[10/24 02:32:19 visual_prompt]: 	Training 600/1106. train loss: 3.1066,	0.6288 s / batch. (data: 7.68e-04). ETA=18:26:30, max mem: 15.9 GB 
[10/24 02:33:22 visual_prompt]: 	Training 700/1106. train loss: 44.9548,	0.6287 s / batch. (data: 3.19e-04). ETA=18:25:15, max mem: 15.9 GB 
[10/24 02:34:25 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6406 s / batch. (data: 7.94e-04). ETA=18:44:58, max mem: 15.9 GB 
[10/24 02:35:28 visual_prompt]: 	Training 900/1106. train loss: 140.6161,	0.6193 s / batch. (data: 2.90e-04). ETA=18:06:34, max mem: 15.9 GB 
[10/24 02:36:31 visual_prompt]: 	Training 1000/1106. train loss: 1.4080,	0.6321 s / batch. (data: 3.28e-04). ETA=18:27:57, max mem: 15.9 GB 
[10/24 02:37:34 visual_prompt]: 	Training 1100/1106. train loss: 15.2882,	0.6162 s / batch. (data: 1.89e-04). ETA=17:59:10, max mem: 15.9 GB 
[10/24 02:37:37 visual_prompt]: Epoch 5 / 100: avg data time: 4.36e-03, avg batch time: 0.6299, average train loss: 89.8157
[10/24 02:38:27 visual_prompt]: 	Test 100/123. loss: 393.917, 0.2254 s / batch. (data: 3.10e-05)max mem: 15.89433 GB 
[10/24 02:38:38 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.2326, average loss: 348.6667
[10/24 02:38:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.41	
[10/24 02:38:38 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/24 02:39:43 visual_prompt]: 	Training 100/1106. train loss: 141.0311,	0.6337 s / batch. (data: 8.27e-04). ETA=18:28:40, max mem: 15.9 GB 
[10/24 02:40:46 visual_prompt]: 	Training 200/1106. train loss: 274.1501,	0.6297 s / batch. (data: 7.90e-04). ETA=18:20:41, max mem: 15.9 GB 
[10/24 02:41:48 visual_prompt]: 	Training 300/1106. train loss: 75.2308,	0.6444 s / batch. (data: 7.89e-04). ETA=18:45:15, max mem: 15.9 GB 
[10/24 02:42:51 visual_prompt]: 	Training 400/1106. train loss: 80.7599,	0.6461 s / batch. (data: 7.85e-04). ETA=18:47:06, max mem: 15.9 GB 
[10/24 02:43:54 visual_prompt]: 	Training 500/1106. train loss: 122.8329,	0.6489 s / batch. (data: 1.29e-02). ETA=18:50:58, max mem: 15.9 GB 
[10/24 02:44:57 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6220 s / batch. (data: 3.59e-04). ETA=18:02:58, max mem: 15.9 GB 
[10/24 02:46:00 visual_prompt]: 	Training 700/1106. train loss: 205.6401,	0.6520 s / batch. (data: 7.54e-04). ETA=18:54:09, max mem: 15.9 GB 
[10/24 02:47:03 visual_prompt]: 	Training 800/1106. train loss: 403.4277,	0.6125 s / batch. (data: 2.83e-04). ETA=17:44:28, max mem: 15.9 GB 
[10/24 02:48:06 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6269 s / batch. (data: 3.00e-04). ETA=18:08:23, max mem: 15.9 GB 
[10/24 02:49:09 visual_prompt]: 	Training 1000/1106. train loss: 348.0742,	0.6320 s / batch. (data: 7.98e-04). ETA=18:16:11, max mem: 15.9 GB 
[10/24 02:50:11 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6066 s / batch. (data: 1.51e-04). ETA=17:31:05, max mem: 15.9 GB 
[10/24 02:50:15 visual_prompt]: Epoch 6 / 100: avg data time: 4.08e-03, avg batch time: 0.6301, average train loss: 128.6151
[10/24 02:51:05 visual_prompt]: 	Test 100/123. loss: 100.877, 0.2326 s / batch. (data: 4.34e-05)max mem: 15.89433 GB 
[10/24 02:51:16 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2340, average loss: 89.9512
[10/24 02:51:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.55	
[10/24 02:51:16 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/24 02:52:20 visual_prompt]: 	Training 100/1106. train loss: 0.0030,	0.6428 s / batch. (data: 8.08e-04). ETA=18:32:43, max mem: 15.9 GB 
[10/24 02:53:23 visual_prompt]: 	Training 200/1106. train loss: 171.0105,	0.6330 s / batch. (data: 8.52e-04). ETA=18:14:43, max mem: 15.9 GB 
[10/24 02:54:26 visual_prompt]: 	Training 300/1106. train loss: 187.7685,	0.6366 s / batch. (data: 1.26e-02). ETA=18:19:54, max mem: 15.9 GB 
[10/24 02:55:29 visual_prompt]: 	Training 400/1106. train loss: 10.8192,	0.6308 s / batch. (data: 7.96e-04). ETA=18:08:46, max mem: 15.9 GB 
[10/24 02:56:31 visual_prompt]: 	Training 500/1106. train loss: 105.3738,	0.6400 s / batch. (data: 1.20e-02). ETA=18:23:36, max mem: 15.9 GB 
[10/24 02:57:34 visual_prompt]: 	Training 600/1106. train loss: 62.3744,	0.6347 s / batch. (data: 3.32e-04). ETA=18:13:27, max mem: 15.9 GB 
[10/24 02:58:37 visual_prompt]: 	Training 700/1106. train loss: 367.7293,	0.6489 s / batch. (data: 7.46e-04). ETA=18:36:46, max mem: 15.9 GB 
[10/24 02:59:40 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6134 s / batch. (data: 3.27e-04). ETA=17:34:36, max mem: 15.9 GB 
[10/24 03:00:43 visual_prompt]: 	Training 900/1106. train loss: 119.4303,	0.6313 s / batch. (data: 8.59e-04). ETA=18:04:19, max mem: 15.9 GB 
[10/24 03:01:46 visual_prompt]: 	Training 1000/1106. train loss: 348.4556,	0.6243 s / batch. (data: 3.20e-04). ETA=17:51:24, max mem: 15.9 GB 
[10/24 03:02:49 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6063 s / batch. (data: 1.49e-04). ETA=17:19:24, max mem: 15.9 GB 
[10/24 03:02:52 visual_prompt]: Epoch 7 / 100: avg data time: 4.31e-03, avg batch time: 0.6296, average train loss: 160.9866
[10/24 03:03:43 visual_prompt]: 	Test 100/123. loss: 228.167, 0.2252 s / batch. (data: 2.86e-05)max mem: 15.89433 GB 
[10/24 03:03:53 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.2309, average loss: 207.1572
[10/24 03:03:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.32	
[10/24 03:03:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/24 03:04:58 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6065 s / batch. (data: 3.14e-04). ETA=17:18:40, max mem: 15.9 GB 
[10/24 03:06:01 visual_prompt]: 	Training 200/1106. train loss: 72.8415,	0.6278 s / batch. (data: 7.59e-04). ETA=17:54:06, max mem: 15.9 GB 
[10/24 03:07:04 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6476 s / batch. (data: 2.08e-02). ETA=18:26:52, max mem: 15.9 GB 
[10/24 03:08:06 visual_prompt]: 	Training 400/1106. train loss: 8.3221,	0.6193 s / batch. (data: 3.67e-04). ETA=17:37:29, max mem: 15.9 GB 
[10/24 03:09:09 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6271 s / batch. (data: 4.79e-04). ETA=17:49:53, max mem: 15.9 GB 
[10/24 03:10:12 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6217 s / batch. (data: 7.92e-04). ETA=17:39:34, max mem: 15.9 GB 
[10/24 03:11:15 visual_prompt]: 	Training 700/1106. train loss: 217.7063,	0.6369 s / batch. (data: 1.20e-02). ETA=18:04:25, max mem: 15.9 GB 
[10/24 03:12:18 visual_prompt]: 	Training 800/1106. train loss: 469.2848,	0.6346 s / batch. (data: 8.30e-04). ETA=17:59:24, max mem: 15.9 GB 
[10/24 03:13:21 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6315 s / batch. (data: 3.47e-04). ETA=17:53:04, max mem: 15.9 GB 
[10/24 03:14:24 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6172 s / batch. (data: 3.21e-04). ETA=17:27:50, max mem: 15.9 GB 
[10/24 03:15:26 visual_prompt]: 	Training 1100/1106. train loss: 141.3478,	0.6190 s / batch. (data: 1.48e-04). ETA=17:29:44, max mem: 15.9 GB 
[10/24 03:15:30 visual_prompt]: Epoch 8 / 100: avg data time: 4.03e-03, avg batch time: 0.6297, average train loss: 179.7730
[10/24 03:16:20 visual_prompt]: 	Test 100/123. loss: 281.050, 0.2402 s / batch. (data: 3.84e-05)max mem: 15.89433 GB 
[10/24 03:16:31 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2319, average loss: 251.6645
[10/24 03:16:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.82	
[10/24 03:16:31 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/24 03:17:36 visual_prompt]: 	Training 100/1106. train loss: 232.5383,	0.6466 s / batch. (data: 8.79e-04). ETA=18:15:29, max mem: 15.9 GB 
[10/24 03:18:38 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6190 s / batch. (data: 1.06e-02). ETA=17:27:36, max mem: 15.9 GB 
[10/24 03:19:41 visual_prompt]: 	Training 300/1106. train loss: 281.4479,	0.6277 s / batch. (data: 7.78e-04). ETA=17:41:18, max mem: 15.9 GB 
[10/24 03:20:44 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6480 s / batch. (data: 8.24e-04). ETA=18:14:36, max mem: 15.9 GB 
[10/24 03:21:47 visual_prompt]: 	Training 500/1106. train loss: 256.2754,	0.6311 s / batch. (data: 3.26e-04). ETA=17:44:56, max mem: 15.9 GB 
[10/24 03:22:50 visual_prompt]: 	Training 600/1106. train loss: 136.9646,	0.6178 s / batch. (data: 2.43e-04). ETA=17:21:27, max mem: 15.9 GB 
[10/24 03:23:53 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6221 s / batch. (data: 8.60e-04). ETA=17:27:49, max mem: 15.9 GB 
[10/24 03:24:56 visual_prompt]: 	Training 800/1106. train loss: 382.5134,	0.6292 s / batch. (data: 7.42e-04). ETA=17:38:37, max mem: 15.9 GB 
[10/24 03:25:59 visual_prompt]: 	Training 900/1106. train loss: 331.6386,	0.6344 s / batch. (data: 1.44e-02). ETA=17:46:20, max mem: 15.9 GB 
[10/24 03:27:02 visual_prompt]: 	Training 1000/1106. train loss: 1.7048,	0.6260 s / batch. (data: 3.12e-04). ETA=17:31:12, max mem: 15.9 GB 
[10/24 03:28:04 visual_prompt]: 	Training 1100/1106. train loss: 189.6265,	0.6142 s / batch. (data: 1.94e-04). ETA=17:10:20, max mem: 15.9 GB 
[10/24 03:28:08 visual_prompt]: Epoch 9 / 100: avg data time: 4.58e-03, avg batch time: 0.6305, average train loss: 184.9692
[10/24 03:28:59 visual_prompt]: 	Test 100/123. loss: 323.774, 0.2326 s / batch. (data: 4.27e-05)max mem: 15.89433 GB 
[10/24 03:29:09 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2322, average loss: 298.4542
[10/24 03:29:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.70	
[10/24 03:29:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/24 03:30:14 visual_prompt]: 	Training 100/1106. train loss: 141.7837,	0.6134 s / batch. (data: 2.79e-04). ETA=17:07:51, max mem: 15.9 GB 
[10/24 03:31:17 visual_prompt]: 	Training 200/1106. train loss: 191.6353,	0.6498 s / batch. (data: 2.75e-02). ETA=18:07:46, max mem: 15.9 GB 
[10/24 03:32:20 visual_prompt]: 	Training 300/1106. train loss: 582.9045,	0.6425 s / batch. (data: 7.67e-04). ETA=17:54:36, max mem: 15.9 GB 
[10/24 03:33:23 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6423 s / batch. (data: 7.47e-04). ETA=17:53:10, max mem: 15.9 GB 
[10/24 03:34:26 visual_prompt]: 	Training 500/1106. train loss: 408.8129,	0.6375 s / batch. (data: 7.91e-04). ETA=17:44:00, max mem: 15.9 GB 
[10/24 03:35:29 visual_prompt]: 	Training 600/1106. train loss: 127.7904,	0.6190 s / batch. (data: 3.21e-04). ETA=17:12:05, max mem: 15.9 GB 
[10/24 03:36:31 visual_prompt]: 	Training 700/1106. train loss: 467.5942,	0.6342 s / batch. (data: 1.15e-03). ETA=17:36:21, max mem: 15.9 GB 
[10/24 03:37:34 visual_prompt]: 	Training 800/1106. train loss: 166.6931,	0.6919 s / batch. (data: 5.86e-03). ETA=19:11:20, max mem: 15.9 GB 
[10/24 03:38:37 visual_prompt]: 	Training 900/1106. train loss: 85.9765,	0.6294 s / batch. (data: 7.49e-04). ETA=17:26:17, max mem: 15.9 GB 
[10/24 03:39:40 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6169 s / batch. (data: 1.05e-02). ETA=17:04:34, max mem: 15.9 GB 
[10/24 03:40:43 visual_prompt]: 	Training 1100/1106. train loss: 161.8220,	0.6165 s / batch. (data: 1.43e-04). ETA=17:02:53, max mem: 15.9 GB 
[10/24 03:40:47 visual_prompt]: Epoch 10 / 100: avg data time: 4.07e-03, avg batch time: 0.6303, average train loss: 226.8237
[10/24 03:41:37 visual_prompt]: 	Test 100/123. loss: 90.353, 0.2316 s / batch. (data: 2.96e-05)max mem: 15.89433 GB 
[10/24 03:41:48 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2327, average loss: 133.6490
[10/24 03:41:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.01	
[10/24 03:41:48 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/24 03:42:53 visual_prompt]: 	Training 100/1106. train loss: 44.0242,	0.6275 s / batch. (data: 3.18e-04). ETA=17:19:59, max mem: 15.9 GB 
[10/24 03:43:55 visual_prompt]: 	Training 200/1106. train loss: 40.9051,	0.6278 s / batch. (data: 1.38e-02). ETA=17:19:21, max mem: 15.9 GB 
[10/24 03:44:58 visual_prompt]: 	Training 300/1106. train loss: 45.6312,	0.6440 s / batch. (data: 1.81e-02). ETA=17:45:12, max mem: 15.9 GB 
[10/24 03:46:01 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6117 s / batch. (data: 2.92e-04). ETA=16:50:42, max mem: 15.9 GB 
[10/24 03:47:04 visual_prompt]: 	Training 500/1106. train loss: 4.1267,	0.6187 s / batch. (data: 3.23e-04). ETA=17:01:17, max mem: 15.9 GB 
[10/24 03:48:07 visual_prompt]: 	Training 600/1106. train loss: 547.5677,	0.6343 s / batch. (data: 7.26e-04). ETA=17:26:00, max mem: 15.9 GB 
[10/24 03:49:10 visual_prompt]: 	Training 700/1106. train loss: 2.8419,	0.6304 s / batch. (data: 8.03e-04). ETA=17:18:24, max mem: 15.9 GB 
[10/24 03:50:13 visual_prompt]: 	Training 800/1106. train loss: 25.3454,	0.6186 s / batch. (data: 3.29e-04). ETA=16:57:59, max mem: 15.9 GB 
[10/24 03:51:16 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6435 s / batch. (data: 7.38e-04). ETA=17:37:53, max mem: 15.9 GB 
[10/24 03:52:19 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6182 s / batch. (data: 2.75e-04). ETA=16:55:20, max mem: 15.9 GB 
[10/24 03:53:22 visual_prompt]: 	Training 1100/1106. train loss: 8.1785,	0.6183 s / batch. (data: 1.39e-04). ETA=16:54:24, max mem: 15.9 GB 
[10/24 03:53:25 visual_prompt]: Epoch 11 / 100: avg data time: 4.85e-03, avg batch time: 0.6307, average train loss: 213.0456
[10/24 03:54:16 visual_prompt]: 	Test 100/123. loss: 89.319, 0.2438 s / batch. (data: 4.36e-05)max mem: 15.89433 GB 
[10/24 03:54:26 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2329, average loss: 80.3773
[10/24 03:54:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.64	
[10/24 03:54:26 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/24 03:55:32 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6061 s / batch. (data: 3.56e-04). ETA=16:33:19, max mem: 15.9 GB 
[10/24 03:56:35 visual_prompt]: 	Training 200/1106. train loss: 342.2860,	0.6120 s / batch. (data: 3.29e-04). ETA=16:42:00, max mem: 15.9 GB 
[10/24 03:57:38 visual_prompt]: 	Training 300/1106. train loss: 740.9562,	0.6479 s / batch. (data: 2.89e-02). ETA=17:39:40, max mem: 15.9 GB 
[10/24 03:58:41 visual_prompt]: 	Training 400/1106. train loss: 455.9385,	0.6179 s / batch. (data: 2.85e-04). ETA=16:49:39, max mem: 15.9 GB 
[10/24 03:59:44 visual_prompt]: 	Training 500/1106. train loss: 292.2511,	0.6741 s / batch. (data: 2.20e-02). ETA=18:20:15, max mem: 15.9 GB 
[10/24 04:00:46 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6126 s / batch. (data: 3.21e-04). ETA=16:38:52, max mem: 15.9 GB 
[10/24 04:01:49 visual_prompt]: 	Training 700/1106. train loss: 280.3217,	0.6122 s / batch. (data: 3.13e-04). ETA=16:37:10, max mem: 15.9 GB 
[10/24 04:02:52 visual_prompt]: 	Training 800/1106. train loss: 80.1837,	0.6275 s / batch. (data: 7.14e-04). ETA=17:01:08, max mem: 15.9 GB 
[10/24 04:03:54 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6086 s / batch. (data: 3.10e-04). ETA=16:29:15, max mem: 15.9 GB 
[10/24 04:04:57 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6302 s / batch. (data: 8.01e-04). ETA=17:03:27, max mem: 15.9 GB 
[10/24 04:06:00 visual_prompt]: 	Training 1100/1106. train loss: 18.1768,	0.6169 s / batch. (data: 1.53e-04). ETA=16:40:44, max mem: 15.9 GB 
[10/24 04:06:04 visual_prompt]: Epoch 12 / 100: avg data time: 5.77e-03, avg batch time: 0.6307, average train loss: 242.0039
[10/24 04:06:54 visual_prompt]: 	Test 100/123. loss: 339.375, 0.2467 s / batch. (data: 2.79e-05)max mem: 15.89433 GB 
[10/24 04:07:05 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2323, average loss: 374.8248
[10/24 04:07:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.16	
[10/24 04:07:05 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/24 04:08:10 visual_prompt]: 	Training 100/1106. train loss: 827.8780,	0.6249 s / batch. (data: 7.17e-03). ETA=16:52:41, max mem: 15.9 GB 
[10/24 04:09:13 visual_prompt]: 	Training 200/1106. train loss: 188.3460,	0.6126 s / batch. (data: 3.69e-04). ETA=16:31:44, max mem: 15.9 GB 
[10/24 04:10:16 visual_prompt]: 	Training 300/1106. train loss: 0.0171,	0.6199 s / batch. (data: 3.20e-04). ETA=16:42:29, max mem: 15.9 GB 
[10/24 04:11:18 visual_prompt]: 	Training 400/1106. train loss: 453.6322,	0.6332 s / batch. (data: 7.36e-04). ETA=17:02:58, max mem: 15.9 GB 
[10/24 04:12:21 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6201 s / batch. (data: 2.52e-04). ETA=16:40:43, max mem: 15.9 GB 
[10/24 04:13:24 visual_prompt]: 	Training 600/1106. train loss: 207.2098,	0.6138 s / batch. (data: 4.61e-04). ETA=16:29:28, max mem: 15.9 GB 
[10/24 04:14:27 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6184 s / batch. (data: 8.32e-04). ETA=16:35:53, max mem: 15.9 GB 
[10/24 04:15:30 visual_prompt]: 	Training 800/1106. train loss: 59.4189,	0.6258 s / batch. (data: 3.35e-04). ETA=16:46:51, max mem: 15.9 GB 
[10/24 04:16:33 visual_prompt]: 	Training 900/1106. train loss: 236.1501,	0.6277 s / batch. (data: 8.05e-04). ETA=16:48:48, max mem: 15.9 GB 
[10/24 04:17:36 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6214 s / batch. (data: 3.25e-04). ETA=16:37:40, max mem: 15.9 GB 
[10/24 04:18:39 visual_prompt]: 	Training 1100/1106. train loss: 266.6450,	0.6122 s / batch. (data: 1.70e-04). ETA=16:21:55, max mem: 15.9 GB 
[10/24 04:18:43 visual_prompt]: Epoch 13 / 100: avg data time: 4.95e-03, avg batch time: 0.6309, average train loss: 229.2723
[10/24 04:19:33 visual_prompt]: 	Test 100/123. loss: 117.278, 0.2403 s / batch. (data: 3.05e-05)max mem: 15.89433 GB 
[10/24 04:19:43 visual_prompt]: Inference (val):avg data time: 2.01e-04, avg batch time: 0.2340, average loss: 151.2793
[10/24 04:19:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.21	
[10/24 04:19:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/24 04:20:49 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6154 s / batch. (data: 8.36e-04). ETA=16:25:55, max mem: 15.9 GB 
[10/24 04:21:51 visual_prompt]: 	Training 200/1106. train loss: 264.2234,	0.6337 s / batch. (data: 8.31e-04). ETA=16:54:12, max mem: 15.9 GB 
[10/24 04:22:54 visual_prompt]: 	Training 300/1106. train loss: 192.4313,	0.6400 s / batch. (data: 7.38e-04). ETA=17:03:13, max mem: 15.9 GB 
[10/24 04:23:57 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6225 s / batch. (data: 7.73e-04). ETA=16:34:05, max mem: 15.9 GB 
[10/24 04:25:00 visual_prompt]: 	Training 500/1106. train loss: 90.7869,	0.6229 s / batch. (data: 7.75e-04). ETA=16:33:43, max mem: 15.9 GB 
[10/24 04:26:03 visual_prompt]: 	Training 600/1106. train loss: 256.6204,	0.6126 s / batch. (data: 3.58e-04). ETA=16:16:13, max mem: 15.9 GB 
[10/24 04:27:05 visual_prompt]: 	Training 700/1106. train loss: 779.3112,	0.6313 s / batch. (data: 8.11e-04). ETA=16:45:03, max mem: 15.9 GB 
[10/24 04:28:08 visual_prompt]: 	Training 800/1106. train loss: 256.8331,	0.6356 s / batch. (data: 7.51e-04). ETA=16:50:48, max mem: 15.9 GB 
[10/24 04:29:11 visual_prompt]: 	Training 900/1106. train loss: 117.0954,	0.6218 s / batch. (data: 2.97e-04). ETA=16:27:53, max mem: 15.9 GB 
[10/24 04:30:14 visual_prompt]: 	Training 1000/1106. train loss: 109.9609,	0.6400 s / batch. (data: 3.15e-04). ETA=16:55:42, max mem: 15.9 GB 
[10/24 04:31:16 visual_prompt]: 	Training 1100/1106. train loss: 232.8784,	0.6134 s / batch. (data: 1.60e-04). ETA=16:12:27, max mem: 15.9 GB 
[10/24 04:31:20 visual_prompt]: Epoch 14 / 100: avg data time: 4.86e-03, avg batch time: 0.6297, average train loss: 240.0770
[10/24 04:32:10 visual_prompt]: 	Test 100/123. loss: 284.293, 0.2278 s / batch. (data: 2.67e-05)max mem: 15.89433 GB 
[10/24 04:32:21 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.2314, average loss: 292.7028
[10/24 04:32:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.70	
[10/24 04:32:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/24 04:33:26 visual_prompt]: 	Training 100/1106. train loss: 224.3937,	0.6117 s / batch. (data: 3.77e-04). ETA=16:08:38, max mem: 15.9 GB 
[10/24 04:34:28 visual_prompt]: 	Training 200/1106. train loss: 508.4631,	0.6136 s / batch. (data: 3.19e-04). ETA=16:10:40, max mem: 15.9 GB 
[10/24 04:35:31 visual_prompt]: 	Training 300/1106. train loss: 112.5125,	0.6142 s / batch. (data: 7.59e-04). ETA=16:10:40, max mem: 15.9 GB 
[10/24 04:36:34 visual_prompt]: 	Training 400/1106. train loss: 327.0547,	0.6448 s / batch. (data: 8.08e-04). ETA=16:57:53, max mem: 15.9 GB 
[10/24 04:37:36 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6360 s / batch. (data: 3.09e-04). ETA=16:42:53, max mem: 15.9 GB 
[10/24 04:38:39 visual_prompt]: 	Training 600/1106. train loss: 247.1132,	0.6120 s / batch. (data: 3.34e-04). ETA=16:04:08, max mem: 15.9 GB 
[10/24 04:39:42 visual_prompt]: 	Training 700/1106. train loss: 596.3273,	0.6440 s / batch. (data: 6.92e-04). ETA=16:53:25, max mem: 15.9 GB 
[10/24 04:40:45 visual_prompt]: 	Training 800/1106. train loss: 140.5951,	0.6322 s / batch. (data: 3.12e-04). ETA=16:33:50, max mem: 15.9 GB 
[10/24 04:41:48 visual_prompt]: 	Training 900/1106. train loss: 113.2127,	0.6186 s / batch. (data: 3.35e-04). ETA=16:11:18, max mem: 15.9 GB 
[10/24 04:42:51 visual_prompt]: 	Training 1000/1106. train loss: 457.6155,	0.6286 s / batch. (data: 7.10e-04). ETA=16:26:04, max mem: 15.9 GB 
[10/24 04:43:53 visual_prompt]: 	Training 1100/1106. train loss: 6.1861,	0.6162 s / batch. (data: 1.85e-04). ETA=16:05:34, max mem: 15.9 GB 
[10/24 04:43:57 visual_prompt]: Epoch 15 / 100: avg data time: 4.46e-03, avg batch time: 0.6296, average train loss: 249.5059
[10/24 04:44:48 visual_prompt]: 	Test 100/123. loss: 362.114, 0.2252 s / batch. (data: 3.70e-05)max mem: 15.89433 GB 
[10/24 04:44:58 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.2322, average loss: 326.8916
[10/24 04:44:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.68	
[10/24 04:44:58 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/24 04:46:03 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6180 s / batch. (data: 8.10e-04). ETA=16:07:19, max mem: 15.9 GB 
[10/24 04:47:06 visual_prompt]: 	Training 200/1106. train loss: 201.0496,	0.6402 s / batch. (data: 8.44e-04). ETA=16:40:57, max mem: 15.9 GB 
[10/24 04:48:08 visual_prompt]: 	Training 300/1106. train loss: 608.6624,	0.6184 s / batch. (data: 3.55e-04). ETA=16:05:49, max mem: 15.9 GB 
[10/24 04:49:11 visual_prompt]: 	Training 400/1106. train loss: 150.7858,	0.6324 s / batch. (data: 7.50e-04). ETA=16:26:39, max mem: 15.9 GB 
[10/24 04:50:14 visual_prompt]: 	Training 500/1106. train loss: 374.1824,	0.6134 s / batch. (data: 3.09e-04). ETA=15:56:00, max mem: 15.9 GB 
[10/24 04:51:17 visual_prompt]: 	Training 600/1106. train loss: 321.6922,	0.6423 s / batch. (data: 7.35e-04). ETA=16:39:53, max mem: 15.9 GB 
[10/24 04:52:20 visual_prompt]: 	Training 700/1106. train loss: 155.8160,	0.6139 s / batch. (data: 2.81e-04). ETA=15:54:46, max mem: 15.9 GB 
[10/24 04:53:22 visual_prompt]: 	Training 800/1106. train loss: 152.7410,	0.6386 s / batch. (data: 7.91e-04). ETA=16:31:59, max mem: 15.9 GB 
[10/24 04:54:25 visual_prompt]: 	Training 900/1106. train loss: 34.5210,	0.6148 s / batch. (data: 3.23e-04). ETA=15:54:00, max mem: 15.9 GB 
[10/24 04:55:28 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6202 s / batch. (data: 1.20e-02). ETA=16:01:20, max mem: 15.9 GB 
[10/24 04:56:31 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6066 s / batch. (data: 1.49e-04). ETA=15:39:21, max mem: 15.9 GB 
[10/24 04:56:34 visual_prompt]: Epoch 16 / 100: avg data time: 4.10e-03, avg batch time: 0.6294, average train loss: 243.5529
[10/24 04:57:25 visual_prompt]: 	Test 100/123. loss: 615.193, 0.2246 s / batch. (data: 2.98e-05)max mem: 15.89433 GB 
[10/24 04:57:35 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2322, average loss: 541.6396
[10/24 04:57:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.55	
[10/24 04:57:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/24 04:58:40 visual_prompt]: 	Training 100/1106. train loss: 134.4569,	0.6470 s / batch. (data: 3.56e-02). ETA=16:40:41, max mem: 15.9 GB 
[10/24 04:59:43 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6388 s / batch. (data: 2.67e-02). ETA=16:26:56, max mem: 15.9 GB 
[10/24 05:00:46 visual_prompt]: 	Training 300/1106. train loss: 657.4402,	0.6371 s / batch. (data: 7.98e-04). ETA=16:23:21, max mem: 15.9 GB 
[10/24 05:01:49 visual_prompt]: 	Training 400/1106. train loss: 1301.2094,	0.6240 s / batch. (data: 2.83e-04). ETA=16:02:05, max mem: 15.9 GB 
[10/24 05:02:52 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6085 s / batch. (data: 3.25e-04). ETA=15:37:09, max mem: 15.9 GB 
[10/24 05:03:54 visual_prompt]: 	Training 600/1106. train loss: 150.0594,	0.6343 s / batch. (data: 1.20e-02). ETA=16:15:47, max mem: 15.9 GB 
[10/24 05:04:57 visual_prompt]: 	Training 700/1106. train loss: 342.6539,	0.6349 s / batch. (data: 1.26e-02). ETA=16:15:43, max mem: 15.9 GB 
[10/24 05:06:00 visual_prompt]: 	Training 800/1106. train loss: 501.5843,	0.6203 s / batch. (data: 3.20e-04). ETA=15:52:13, max mem: 15.9 GB 
[10/24 05:07:03 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6392 s / batch. (data: 3.09e-02). ETA=16:20:09, max mem: 15.9 GB 
[10/24 05:08:05 visual_prompt]: 	Training 1000/1106. train loss: 722.2958,	0.6434 s / batch. (data: 8.24e-04). ETA=16:25:27, max mem: 15.9 GB 
[10/24 05:09:08 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6065 s / batch. (data: 1.65e-04). ETA=15:27:58, max mem: 15.9 GB 
[10/24 05:09:12 visual_prompt]: Epoch 17 / 100: avg data time: 5.06e-03, avg batch time: 0.6301, average train loss: 215.6933
[10/24 05:10:02 visual_prompt]: 	Test 100/123. loss: 239.018, 0.2378 s / batch. (data: 2.79e-05)max mem: 15.89433 GB 
[10/24 05:10:13 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.2330, average loss: 252.5473
[10/24 05:10:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.66	
[10/24 05:10:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/24 05:11:17 visual_prompt]: 	Training 100/1106. train loss: 553.8745,	0.6354 s / batch. (data: 7.50e-04). ETA=16:11:03, max mem: 15.9 GB 
[10/24 05:12:20 visual_prompt]: 	Training 200/1106. train loss: 3669.0400,	0.6239 s / batch. (data: 3.12e-04). ETA=15:52:29, max mem: 15.9 GB 
[10/24 05:13:23 visual_prompt]: 	Training 300/1106. train loss: 911.3390,	0.6176 s / batch. (data: 3.19e-04). ETA=15:41:45, max mem: 15.9 GB 
[10/24 05:14:26 visual_prompt]: 	Training 400/1106. train loss: 200.0653,	0.6358 s / batch. (data: 1.05e-02). ETA=16:08:34, max mem: 15.9 GB 
[10/24 05:15:30 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6336 s / batch. (data: 6.93e-04). ETA=16:04:08, max mem: 15.9 GB 
[10/24 05:16:33 visual_prompt]: 	Training 600/1106. train loss: 184.9065,	0.6401 s / batch. (data: 3.28e-04). ETA=16:12:57, max mem: 15.9 GB 
[10/24 05:17:35 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6080 s / batch. (data: 2.76e-04). ETA=15:23:07, max mem: 15.9 GB 
[10/24 05:18:38 visual_prompt]: 	Training 800/1106. train loss: 326.1772,	0.6243 s / batch. (data: 7.54e-04). ETA=15:46:49, max mem: 15.9 GB 
[10/24 05:19:41 visual_prompt]: 	Training 900/1106. train loss: 206.6882,	0.6284 s / batch. (data: 3.17e-04). ETA=15:51:59, max mem: 15.9 GB 
[10/24 05:20:44 visual_prompt]: 	Training 1000/1106. train loss: 12.1258,	0.6252 s / batch. (data: 3.08e-04). ETA=15:46:06, max mem: 15.9 GB 
[10/24 05:21:47 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6075 s / batch. (data: 1.44e-04). ETA=15:18:19, max mem: 15.9 GB 
[10/24 05:21:50 visual_prompt]: Epoch 18 / 100: avg data time: 4.61e-03, avg batch time: 0.6309, average train loss: 224.2659
[10/24 05:22:40 visual_prompt]: 	Test 100/123. loss: 241.517, 0.2396 s / batch. (data: 3.55e-05)max mem: 15.89433 GB 
[10/24 05:22:51 visual_prompt]: Inference (val):avg data time: 1.31e-04, avg batch time: 0.2330, average loss: 216.7141
[10/24 05:22:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.22	
[10/24 05:22:51 visual_prompt]: Stopping early.
[10/24 05:22:51 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 05:22:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 05:22:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 05:22:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 05:22:51 visual_prompt]: Training with config:
[10/24 05:22:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr50.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 05:22:51 visual_prompt]: Loading training data...
[10/24 05:22:51 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 05:22:51 visual_prompt]: Loading validation data...
[10/24 05:22:51 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 05:22:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/24 05:22:54 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/24 05:22:54 visual_prompt]: tuned percent:0.522
[10/24 05:22:54 visual_prompt]: Device used for model: 0
[10/24 05:22:54 visual_prompt]: Setting up Evaluator...
[10/24 05:22:54 visual_prompt]: Setting up Trainer...
[10/24 05:22:54 visual_prompt]: 	Setting up the optimizer...
[10/24 05:22:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 05:24:00 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6188 s / batch. (data: 3.10e-04). ETA=18:59:38, max mem: 15.9 GB 
[10/24 05:25:03 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6339 s / batch. (data: 7.43e-04). ETA=19:26:17, max mem: 15.9 GB 
[10/24 05:26:06 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6343 s / batch. (data: 7.66e-04). ETA=19:26:05, max mem: 15.9 GB 
[10/24 05:27:09 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6184 s / batch. (data: 3.02e-04). ETA=18:55:51, max mem: 15.9 GB 
[10/24 05:28:13 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6365 s / batch. (data: 5.43e-03). ETA=19:28:02, max mem: 15.9 GB 
[10/24 05:29:16 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6308 s / batch. (data: 8.03e-04). ETA=19:16:27, max mem: 15.9 GB 
[10/24 05:30:19 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6190 s / batch. (data: 2.88e-04). ETA=18:53:51, max mem: 15.9 GB 
[10/24 05:31:23 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6572 s / batch. (data: 7.20e-04). ETA=20:02:35, max mem: 15.9 GB 
[10/24 05:32:26 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6531 s / batch. (data: 3.06e-04). ETA=19:54:06, max mem: 15.9 GB 
[10/24 05:33:29 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6193 s / batch. (data: 4.40e-04). ETA=18:51:12, max mem: 15.9 GB 
[10/24 05:34:32 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6186 s / batch. (data: 1.63e-04). ETA=18:48:54, max mem: 15.9 GB 
[10/24 05:34:36 visual_prompt]: Epoch 1 / 100: avg data time: 4.79e-03, avg batch time: 0.6351, average train loss: 1.4028
[10/24 05:35:26 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2366 s / batch. (data: 4.03e-05)max mem: 15.89950 GB 
[10/24 05:35:37 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.2322, average loss: 1.3505
[10/24 05:35:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/24 05:35:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/24 05:36:42 visual_prompt]: 	Training 100/1106. train loss: 87.4860,	0.6360 s / batch. (data: 8.42e-04). ETA=19:19:33, max mem: 15.9 GB 
[10/24 05:37:45 visual_prompt]: 	Training 200/1106. train loss: 5.8361,	0.6184 s / batch. (data: 3.33e-04). ETA=18:46:32, max mem: 15.9 GB 
[10/24 05:38:48 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6479 s / batch. (data: 1.10e-03). ETA=19:39:08, max mem: 15.9 GB 
[10/24 05:39:51 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6232 s / batch. (data: 7.51e-04). ETA=18:53:07, max mem: 15.9 GB 
[10/24 05:40:54 visual_prompt]: 	Training 500/1106. train loss: 79.2520,	0.6227 s / batch. (data: 2.90e-04). ETA=18:51:09, max mem: 15.9 GB 
[10/24 05:41:57 visual_prompt]: 	Training 600/1106. train loss: 4.1420,	0.6440 s / batch. (data: 7.55e-04). ETA=19:28:51, max mem: 15.9 GB 
[10/24 05:43:00 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6190 s / batch. (data: 2.93e-04). ETA=18:42:28, max mem: 15.9 GB 
[10/24 05:44:04 visual_prompt]: 	Training 800/1106. train loss: 4.2994,	0.6723 s / batch. (data: 7.88e-04). ETA=20:17:52, max mem: 15.9 GB 
[10/24 05:45:07 visual_prompt]: 	Training 900/1106. train loss: 17.2950,	0.6263 s / batch. (data: 7.95e-03). ETA=18:53:31, max mem: 15.9 GB 
[10/24 05:46:10 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6361 s / batch. (data: 2.88e-04). ETA=19:10:08, max mem: 15.9 GB 
[10/24 05:47:13 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6169 s / batch. (data: 1.42e-04). ETA=18:34:25, max mem: 15.9 GB 
[10/24 05:47:17 visual_prompt]: Epoch 2 / 100: avg data time: 3.92e-03, avg batch time: 0.6325, average train loss: 27.5699
[10/24 05:48:07 visual_prompt]: 	Test 100/123. loss: 19.910, 0.2250 s / batch. (data: 3.89e-05)max mem: 15.89950 GB 
[10/24 05:48:18 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2334, average loss: 21.6190
[10/24 05:48:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.30	
[10/24 05:48:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/24 05:49:24 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6296 s / batch. (data: 7.93e-04). ETA=18:56:19, max mem: 15.9 GB 
[10/24 05:50:27 visual_prompt]: 	Training 200/1106. train loss: 0.6654,	0.6407 s / batch. (data: 7.55e-04). ETA=19:15:15, max mem: 15.9 GB 
[10/24 05:51:30 visual_prompt]: 	Training 300/1106. train loss: 4.3558,	0.6451 s / batch. (data: 3.57e-04). ETA=19:22:07, max mem: 15.9 GB 
[10/24 05:52:33 visual_prompt]: 	Training 400/1106. train loss: 11.6095,	0.6189 s / batch. (data: 2.92e-04). ETA=18:33:48, max mem: 15.9 GB 
[10/24 05:53:36 visual_prompt]: 	Training 500/1106. train loss: 281.0172,	0.6178 s / batch. (data: 3.38e-04). ETA=18:30:48, max mem: 15.9 GB 
[10/24 05:54:39 visual_prompt]: 	Training 600/1106. train loss: 987.3781,	0.6308 s / batch. (data: 3.26e-04). ETA=18:53:09, max mem: 15.9 GB 
[10/24 05:55:42 visual_prompt]: 	Training 700/1106. train loss: 61.8318,	0.6278 s / batch. (data: 3.45e-04). ETA=18:46:44, max mem: 15.9 GB 
[10/24 05:56:45 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6534 s / batch. (data: 1.14e-02). ETA=19:31:32, max mem: 15.9 GB 
[10/24 05:57:48 visual_prompt]: 	Training 900/1106. train loss: 288.9124,	0.6180 s / batch. (data: 3.13e-04). ETA=18:27:08, max mem: 15.9 GB 
[10/24 05:58:51 visual_prompt]: 	Training 1000/1106. train loss: 148.2046,	0.6178 s / batch. (data: 3.53e-04). ETA=18:25:41, max mem: 15.9 GB 
[10/24 05:59:54 visual_prompt]: 	Training 1100/1106. train loss: 1.9766,	0.6181 s / batch. (data: 1.50e-04). ETA=18:25:14, max mem: 15.9 GB 
[10/24 05:59:58 visual_prompt]: Epoch 3 / 100: avg data time: 5.46e-03, avg batch time: 0.6333, average train loss: 54.2831
[10/24 06:00:48 visual_prompt]: 	Test 100/123. loss: 20.820, 0.2277 s / batch. (data: 5.98e-05)max mem: 15.89950 GB 
[10/24 06:00:59 visual_prompt]: Inference (val):avg data time: 2.10e-04, avg batch time: 0.2320, average loss: 22.9650
[10/24 06:00:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.72	
[10/24 06:00:59 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/24 06:02:05 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6364 s / batch. (data: 3.04e-04). ETA=18:56:48, max mem: 15.9 GB 
[10/24 06:03:08 visual_prompt]: 	Training 200/1106. train loss: 182.3583,	0.6248 s / batch. (data: 3.07e-04). ETA=18:35:09, max mem: 15.9 GB 
[10/24 06:04:11 visual_prompt]: 	Training 300/1106. train loss: 45.9684,	0.6122 s / batch. (data: 3.35e-04). ETA=18:11:32, max mem: 15.9 GB 
[10/24 06:05:14 visual_prompt]: 	Training 400/1106. train loss: 68.9707,	0.6340 s / batch. (data: 8.16e-04). ETA=18:49:27, max mem: 15.9 GB 
[10/24 06:06:17 visual_prompt]: 	Training 500/1106. train loss: 473.9297,	0.6368 s / batch. (data: 1.76e-02). ETA=18:53:17, max mem: 15.9 GB 
[10/24 06:07:20 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6175 s / batch. (data: 3.12e-04). ETA=18:17:56, max mem: 15.9 GB 
[10/24 06:08:22 visual_prompt]: 	Training 700/1106. train loss: 366.6393,	0.6205 s / batch. (data: 9.02e-04). ETA=18:22:12, max mem: 15.9 GB 
[10/24 06:09:25 visual_prompt]: 	Training 800/1106. train loss: 10.2994,	0.6360 s / batch. (data: 3.30e-04). ETA=18:48:44, max mem: 15.9 GB 
[10/24 06:10:28 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6508 s / batch. (data: 3.48e-02). ETA=19:13:50, max mem: 15.9 GB 
[10/24 06:11:31 visual_prompt]: 	Training 1000/1106. train loss: 0.0001,	0.6520 s / batch. (data: 3.24e-04). ETA=19:14:57, max mem: 15.9 GB 
[10/24 06:12:34 visual_prompt]: 	Training 1100/1106. train loss: 349.1651,	0.6131 s / batch. (data: 1.64e-04). ETA=18:04:58, max mem: 15.9 GB 
[10/24 06:12:38 visual_prompt]: Epoch 4 / 100: avg data time: 5.33e-03, avg batch time: 0.6318, average train loss: 73.6439
[10/24 06:13:28 visual_prompt]: 	Test 100/123. loss: 34.670, 0.2245 s / batch. (data: 2.96e-05)max mem: 15.89950 GB 
[10/24 06:13:38 visual_prompt]: Inference (val):avg data time: 1.17e-04, avg batch time: 0.2325, average loss: 29.0544
[10/24 06:13:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.68	
[10/24 06:13:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/24 06:14:43 visual_prompt]: 	Training 100/1106. train loss: 37.9749,	0.6309 s / batch. (data: 3.32e-04). ETA=18:35:21, max mem: 15.9 GB 
[10/24 06:15:46 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6124 s / batch. (data: 2.99e-04). ETA=18:01:35, max mem: 15.9 GB 
[10/24 06:16:49 visual_prompt]: 	Training 300/1106. train loss: 41.0166,	0.6136 s / batch. (data: 3.13e-04). ETA=18:02:50, max mem: 15.9 GB 
[10/24 06:17:52 visual_prompt]: 	Training 400/1106. train loss: 281.8934,	0.6206 s / batch. (data: 3.22e-04). ETA=18:14:01, max mem: 15.9 GB 
[10/24 06:18:55 visual_prompt]: 	Training 500/1106. train loss: 220.6667,	0.6308 s / batch. (data: 3.30e-04). ETA=18:30:56, max mem: 15.9 GB 
[10/24 06:19:58 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6290 s / batch. (data: 2.91e-04). ETA=18:26:43, max mem: 15.9 GB 
[10/24 06:21:01 visual_prompt]: 	Training 700/1106. train loss: 128.3079,	0.6287 s / batch. (data: 8.17e-04). ETA=18:25:13, max mem: 15.9 GB 
[10/24 06:22:04 visual_prompt]: 	Training 800/1106. train loss: 15.2635,	0.6184 s / batch. (data: 3.02e-04). ETA=18:06:03, max mem: 15.9 GB 
[10/24 06:23:08 visual_prompt]: 	Training 900/1106. train loss: 102.3499,	0.6296 s / batch. (data: 6.59e-04). ETA=18:24:43, max mem: 15.9 GB 
[10/24 06:24:13 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6086 s / batch. (data: 3.51e-04). ETA=17:46:49, max mem: 15.9 GB 
[10/24 06:25:15 visual_prompt]: 	Training 1100/1106. train loss: 171.8636,	0.6121 s / batch. (data: 1.52e-04). ETA=17:52:01, max mem: 15.9 GB 
[10/24 06:25:19 visual_prompt]: Epoch 5 / 100: avg data time: 7.05e-03, avg batch time: 0.6335, average train loss: 97.7407
[10/24 06:26:10 visual_prompt]: 	Test 100/123. loss: 200.418, 0.2249 s / batch. (data: 4.65e-05)max mem: 15.89950 GB 
[10/24 06:26:20 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2321, average loss: 180.7326
[10/24 06:26:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.18	
[10/24 06:26:20 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/24 06:27:25 visual_prompt]: 	Training 100/1106. train loss: 308.3185,	0.6270 s / batch. (data: 8.66e-04). ETA=18:16:54, max mem: 15.9 GB 
[10/24 06:28:27 visual_prompt]: 	Training 200/1106. train loss: 91.4614,	0.6296 s / batch. (data: 1.06e-03). ETA=18:20:27, max mem: 15.9 GB 
[10/24 06:29:30 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6080 s / batch. (data: 3.31e-04). ETA=17:41:35, max mem: 15.9 GB 
[10/24 06:30:33 visual_prompt]: 	Training 400/1106. train loss: 92.6118,	0.6463 s / batch. (data: 8.52e-04). ETA=18:47:31, max mem: 15.9 GB 
[10/24 06:31:36 visual_prompt]: 	Training 500/1106. train loss: 231.2759,	0.6176 s / batch. (data: 5.49e-03). ETA=17:56:22, max mem: 15.9 GB 
[10/24 06:32:38 visual_prompt]: 	Training 600/1106. train loss: 192.5372,	0.6344 s / batch. (data: 1.16e-02). ETA=18:24:38, max mem: 15.9 GB 
[10/24 06:33:41 visual_prompt]: 	Training 700/1106. train loss: 40.6387,	0.6400 s / batch. (data: 7.56e-04). ETA=18:33:18, max mem: 15.9 GB 
[10/24 06:34:44 visual_prompt]: 	Training 800/1106. train loss: 391.3548,	0.6136 s / batch. (data: 4.01e-04). ETA=17:46:19, max mem: 15.9 GB 
[10/24 06:35:47 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6082 s / batch. (data: 7.56e-04). ETA=17:36:00, max mem: 15.9 GB 
[10/24 06:36:50 visual_prompt]: 	Training 1000/1106. train loss: 223.6675,	0.6355 s / batch. (data: 7.45e-04). ETA=18:22:20, max mem: 15.9 GB 
[10/24 06:37:53 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6080 s / batch. (data: 1.52e-04). ETA=17:33:29, max mem: 15.9 GB 
[10/24 06:37:57 visual_prompt]: Epoch 6 / 100: avg data time: 4.16e-03, avg batch time: 0.6300, average train loss: 137.8698
[10/24 06:38:47 visual_prompt]: 	Test 100/123. loss: 32.131, 0.2277 s / batch. (data: 2.81e-05)max mem: 15.89950 GB 
[10/24 06:38:57 visual_prompt]: Inference (val):avg data time: 3.16e-04, avg batch time: 0.2337, average loss: 35.6167
[10/24 06:38:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.36	
[10/24 06:38:57 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/24 06:40:04 visual_prompt]: 	Training 100/1106. train loss: 198.5371,	0.6110 s / batch. (data: 3.11e-04). ETA=17:37:41, max mem: 15.9 GB 
[10/24 06:41:06 visual_prompt]: 	Training 200/1106. train loss: 174.1944,	0.6353 s / batch. (data: 1.33e-02). ETA=18:18:41, max mem: 15.9 GB 
[10/24 06:42:10 visual_prompt]: 	Training 300/1106. train loss: 362.9369,	0.6296 s / batch. (data: 3.20e-04). ETA=18:07:41, max mem: 15.9 GB 
[10/24 06:43:12 visual_prompt]: 	Training 400/1106. train loss: 0.0960,	0.6329 s / batch. (data: 7.73e-04). ETA=18:12:23, max mem: 15.9 GB 
[10/24 06:44:15 visual_prompt]: 	Training 500/1106. train loss: 389.4062,	0.6358 s / batch. (data: 7.14e-03). ETA=18:16:26, max mem: 15.9 GB 
[10/24 06:45:18 visual_prompt]: 	Training 600/1106. train loss: 4.2427,	0.6311 s / batch. (data: 3.44e-04). ETA=18:07:13, max mem: 15.9 GB 
[10/24 06:46:21 visual_prompt]: 	Training 700/1106. train loss: 759.4497,	0.6354 s / batch. (data: 8.26e-04). ETA=18:13:32, max mem: 15.9 GB 
[10/24 06:47:24 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6385 s / batch. (data: 7.86e-04). ETA=18:17:48, max mem: 15.9 GB 
[10/24 06:48:27 visual_prompt]: 	Training 900/1106. train loss: 28.5014,	0.6174 s / batch. (data: 2.60e-04). ETA=17:40:36, max mem: 15.9 GB 
[10/24 06:49:29 visual_prompt]: 	Training 1000/1106. train loss: 179.6913,	0.6267 s / batch. (data: 8.06e-04). ETA=17:55:24, max mem: 15.9 GB 
[10/24 06:50:32 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6080 s / batch. (data: 1.38e-04). ETA=17:22:23, max mem: 15.9 GB 
[10/24 06:50:36 visual_prompt]: Epoch 7 / 100: avg data time: 5.68e-03, avg batch time: 0.6318, average train loss: 158.9135
[10/24 06:51:26 visual_prompt]: 	Test 100/123. loss: 198.920, 0.2379 s / batch. (data: 3.93e-05)max mem: 15.89950 GB 
[10/24 06:51:37 visual_prompt]: Inference (val):avg data time: 9.76e-05, avg batch time: 0.2327, average loss: 179.7784
[10/24 06:51:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.91	
[10/24 06:51:37 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/24 06:52:42 visual_prompt]: 	Training 100/1106. train loss: 541.1027,	0.6438 s / batch. (data: 7.97e-04). ETA=18:22:31, max mem: 15.9 GB 
[10/24 06:53:45 visual_prompt]: 	Training 200/1106. train loss: 129.4648,	0.6246 s / batch. (data: 1.02e-02). ETA=17:48:39, max mem: 15.9 GB 
[10/24 06:54:48 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6145 s / batch. (data: 3.04e-04). ETA=17:30:18, max mem: 15.9 GB 
[10/24 06:55:51 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6191 s / batch. (data: 1.21e-03). ETA=17:37:12, max mem: 15.9 GB 
[10/24 06:56:54 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6076 s / batch. (data: 3.27e-04). ETA=17:16:30, max mem: 15.9 GB 
[10/24 06:57:56 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6416 s / batch. (data: 5.93e-03). ETA=18:13:24, max mem: 15.9 GB 
[10/24 06:58:59 visual_prompt]: 	Training 700/1106. train loss: 93.5635,	0.6360 s / batch. (data: 1.48e-02). ETA=18:02:54, max mem: 15.9 GB 
[10/24 07:00:02 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6347 s / batch. (data: 3.22e-04). ETA=17:59:38, max mem: 15.9 GB 
[10/24 07:01:05 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6164 s / batch. (data: 4.44e-04). ETA=17:27:29, max mem: 15.9 GB 
[10/24 07:02:08 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6076 s / batch. (data: 3.40e-04). ETA=17:11:33, max mem: 15.9 GB 
[10/24 07:03:10 visual_prompt]: 	Training 1100/1106. train loss: 430.7654,	0.6171 s / batch. (data: 1.64e-04). ETA=17:26:32, max mem: 15.9 GB 
[10/24 07:03:14 visual_prompt]: Epoch 8 / 100: avg data time: 4.34e-03, avg batch time: 0.6301, average train loss: 159.2618
[10/24 07:04:04 visual_prompt]: 	Test 100/123. loss: 60.803, 0.2444 s / batch. (data: 8.30e-05)max mem: 15.89950 GB 
[10/24 07:04:15 visual_prompt]: Inference (val):avg data time: 1.62e-04, avg batch time: 0.2332, average loss: 66.4710
[10/24 07:04:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.38	
[10/24 07:04:15 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/24 07:05:21 visual_prompt]: 	Training 100/1106. train loss: 337.6813,	0.6330 s / batch. (data: 3.13e-04). ETA=17:52:24, max mem: 15.9 GB 
[10/24 07:06:23 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6440 s / batch. (data: 7.88e-04). ETA=18:09:58, max mem: 15.9 GB 
[10/24 07:07:26 visual_prompt]: 	Training 300/1106. train loss: 88.8203,	0.6144 s / batch. (data: 3.49e-04). ETA=17:18:56, max mem: 15.9 GB 
[10/24 07:08:29 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6210 s / batch. (data: 2.99e-04). ETA=17:28:59, max mem: 15.9 GB 
[10/24 07:09:32 visual_prompt]: 	Training 500/1106. train loss: 1431.4276,	0.6619 s / batch. (data: 1.61e-02). ETA=18:37:01, max mem: 15.9 GB 
[10/24 07:10:35 visual_prompt]: 	Training 600/1106. train loss: 270.0241,	0.6470 s / batch. (data: 1.90e-02). ETA=18:10:45, max mem: 15.9 GB 
[10/24 07:11:38 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6454 s / batch. (data: 7.77e-04). ETA=18:06:55, max mem: 15.9 GB 
[10/24 07:12:41 visual_prompt]: 	Training 800/1106. train loss: 410.7723,	0.6327 s / batch. (data: 7.63e-04). ETA=17:44:28, max mem: 15.9 GB 
[10/24 07:13:45 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6391 s / batch. (data: 7.27e-04). ETA=17:54:14, max mem: 15.9 GB 
[10/24 07:14:48 visual_prompt]: 	Training 1000/1106. train loss: 335.5426,	0.6327 s / batch. (data: 7.93e-04). ETA=17:42:23, max mem: 15.9 GB 
[10/24 07:15:50 visual_prompt]: 	Training 1100/1106. train loss: 197.3612,	0.6138 s / batch. (data: 1.59e-04). ETA=17:09:38, max mem: 15.9 GB 
[10/24 07:15:54 visual_prompt]: Epoch 9 / 100: avg data time: 4.90e-03, avg batch time: 0.6317, average train loss: 185.3024
[10/24 07:16:44 visual_prompt]: 	Test 100/123. loss: 30.750, 0.2446 s / batch. (data: 3.89e-05)max mem: 15.89950 GB 
[10/24 07:16:55 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2335, average loss: 26.1278
[10/24 07:16:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 39.39	
[10/24 07:16:55 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/24 07:18:00 visual_prompt]: 	Training 100/1106. train loss: 234.3524,	0.6236 s / batch. (data: 3.06e-04). ETA=17:24:56, max mem: 15.9 GB 
[10/24 07:19:03 visual_prompt]: 	Training 200/1106. train loss: 210.2939,	0.6465 s / batch. (data: 8.10e-04). ETA=18:02:20, max mem: 15.9 GB 
[10/24 07:20:06 visual_prompt]: 	Training 300/1106. train loss: 36.5345,	0.6278 s / batch. (data: 7.98e-04). ETA=17:29:57, max mem: 15.9 GB 
[10/24 07:21:09 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6367 s / batch. (data: 7.85e-04). ETA=17:43:43, max mem: 15.9 GB 
[10/24 07:22:11 visual_prompt]: 	Training 500/1106. train loss: 63.3951,	0.6169 s / batch. (data: 3.97e-04). ETA=17:09:44, max mem: 15.9 GB 
[10/24 07:23:14 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6128 s / batch. (data: 3.53e-04). ETA=17:01:46, max mem: 15.9 GB 
[10/24 07:24:18 visual_prompt]: 	Training 700/1106. train loss: 302.1675,	0.6193 s / batch. (data: 1.00e-03). ETA=17:11:37, max mem: 15.9 GB 
[10/24 07:25:20 visual_prompt]: 	Training 800/1106. train loss: 104.6954,	0.6240 s / batch. (data: 3.36e-04). ETA=17:18:23, max mem: 15.9 GB 
[10/24 07:26:23 visual_prompt]: 	Training 900/1106. train loss: 432.4864,	0.6140 s / batch. (data: 3.20e-04). ETA=17:00:43, max mem: 15.9 GB 
[10/24 07:27:26 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6185 s / batch. (data: 1.05e-02). ETA=17:07:13, max mem: 15.9 GB 
[10/24 07:28:29 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6077 s / batch. (data: 1.65e-04). ETA=16:48:15, max mem: 15.9 GB 
[10/24 07:28:33 visual_prompt]: Epoch 10 / 100: avg data time: 4.44e-03, avg batch time: 0.6311, average train loss: 232.5589
[10/24 07:29:23 visual_prompt]: 	Test 100/123. loss: 321.736, 0.2251 s / batch. (data: 2.88e-05)max mem: 15.89950 GB 
[10/24 07:29:34 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.2318, average loss: 353.7723
[10/24 07:29:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.79	
[10/24 07:29:34 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/24 07:30:39 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6200 s / batch. (data: 7.39e-04). ETA=17:07:36, max mem: 15.9 GB 
[10/24 07:31:42 visual_prompt]: 	Training 200/1106. train loss: 591.3458,	0.6308 s / batch. (data: 8.29e-04). ETA=17:24:23, max mem: 15.9 GB 
[10/24 07:32:45 visual_prompt]: 	Training 300/1106. train loss: 92.8588,	0.6473 s / batch. (data: 2.78e-02). ETA=17:50:33, max mem: 15.9 GB 
[10/24 07:33:48 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6205 s / batch. (data: 7.78e-04). ETA=17:05:17, max mem: 15.9 GB 
[10/24 07:34:51 visual_prompt]: 	Training 500/1106. train loss: 199.4991,	0.6203 s / batch. (data: 3.09e-04). ETA=17:03:52, max mem: 15.9 GB 
[10/24 07:35:54 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6238 s / batch. (data: 1.57e-02). ETA=17:08:39, max mem: 15.9 GB 
[10/24 07:36:56 visual_prompt]: 	Training 700/1106. train loss: 408.6069,	0.6416 s / batch. (data: 1.14e-02). ETA=17:36:53, max mem: 15.9 GB 
[10/24 07:37:59 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6273 s / batch. (data: 2.37e-04). ETA=17:12:18, max mem: 15.9 GB 
[10/24 07:39:02 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6256 s / batch. (data: 3.24e-04). ETA=17:08:27, max mem: 15.9 GB 
[10/24 07:40:07 visual_prompt]: 	Training 1000/1106. train loss: 203.7258,	0.6335 s / batch. (data: 7.87e-04). ETA=17:20:29, max mem: 15.9 GB 
[10/24 07:41:10 visual_prompt]: 	Training 1100/1106. train loss: 139.5569,	0.6146 s / batch. (data: 1.58e-04). ETA=16:48:21, max mem: 15.9 GB 
[10/24 07:41:14 visual_prompt]: Epoch 11 / 100: avg data time: 6.54e-03, avg batch time: 0.6328, average train loss: 270.1598
[10/24 07:42:04 visual_prompt]: 	Test 100/123. loss: 265.534, 0.2257 s / batch. (data: 4.20e-05)max mem: 15.89950 GB 
[10/24 07:42:15 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2320, average loss: 239.3103
[10/24 07:42:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.29	
[10/24 07:42:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/24 07:43:20 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6125 s / batch. (data: 7.90e-04). ETA=16:43:48, max mem: 15.9 GB 
[10/24 07:44:23 visual_prompt]: 	Training 200/1106. train loss: 242.5322,	0.6142 s / batch. (data: 3.06e-04). ETA=16:45:32, max mem: 15.9 GB 
[10/24 07:45:26 visual_prompt]: 	Training 300/1106. train loss: 180.9280,	0.6307 s / batch. (data: 3.55e-04). ETA=17:11:36, max mem: 15.9 GB 
[10/24 07:46:29 visual_prompt]: 	Training 400/1106. train loss: 399.7023,	0.6298 s / batch. (data: 3.40e-04). ETA=17:09:01, max mem: 15.9 GB 
[10/24 07:47:32 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6263 s / batch. (data: 8.29e-04). ETA=17:02:15, max mem: 15.9 GB 
[10/24 07:48:34 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6377 s / batch. (data: 8.62e-04). ETA=17:19:51, max mem: 15.9 GB 
[10/24 07:49:37 visual_prompt]: 	Training 700/1106. train loss: 84.8691,	0.6265 s / batch. (data: 7.18e-04). ETA=17:00:30, max mem: 15.9 GB 
[10/24 07:50:40 visual_prompt]: 	Training 800/1106. train loss: 308.5981,	0.6369 s / batch. (data: 8.17e-04). ETA=17:16:24, max mem: 15.9 GB 
[10/24 07:51:43 visual_prompt]: 	Training 900/1106. train loss: 311.0412,	0.6329 s / batch. (data: 8.16e-04). ETA=17:08:52, max mem: 15.9 GB 
[10/24 07:52:46 visual_prompt]: 	Training 1000/1106. train loss: 204.3012,	0.6127 s / batch. (data: 3.09e-04). ETA=16:34:57, max mem: 15.9 GB 
[10/24 07:53:48 visual_prompt]: 	Training 1100/1106. train loss: 291.7573,	0.6120 s / batch. (data: 1.48e-04). ETA=16:32:49, max mem: 15.9 GB 
[10/24 07:53:52 visual_prompt]: Epoch 12 / 100: avg data time: 5.57e-03, avg batch time: 0.6307, average train loss: 239.9962
[10/24 07:54:42 visual_prompt]: 	Test 100/123. loss: 158.867, 0.2253 s / batch. (data: 2.81e-05)max mem: 15.89950 GB 
[10/24 07:54:53 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2316, average loss: 179.4740
[10/24 07:54:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.07	
[10/24 07:54:53 visual_prompt]: Best epoch 12: best metric: -179.474
[10/24 07:54:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/24 07:55:58 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6090 s / batch. (data: 3.04e-04). ETA=16:26:51, max mem: 15.9 GB 
[10/24 07:57:01 visual_prompt]: 	Training 200/1106. train loss: 502.9717,	0.6427 s / batch. (data: 7.61e-04). ETA=17:20:26, max mem: 15.9 GB 
[10/24 07:58:04 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6079 s / batch. (data: 4.80e-04). ETA=16:23:00, max mem: 15.9 GB 
[10/24 07:59:07 visual_prompt]: 	Training 400/1106. train loss: 149.5543,	0.6176 s / batch. (data: 3.33e-04). ETA=16:37:44, max mem: 15.9 GB 
[10/24 08:00:09 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6083 s / batch. (data: 2.89e-04). ETA=16:21:43, max mem: 15.9 GB 
[10/24 08:01:12 visual_prompt]: 	Training 600/1106. train loss: 93.5226,	0.6269 s / batch. (data: 2.88e-04). ETA=16:50:43, max mem: 15.9 GB 
[10/24 08:02:15 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6292 s / batch. (data: 7.64e-04). ETA=16:53:22, max mem: 15.9 GB 
[10/24 08:03:18 visual_prompt]: 	Training 800/1106. train loss: 55.5870,	0.6486 s / batch. (data: 8.07e-04). ETA=17:23:30, max mem: 15.9 GB 
[10/24 08:04:21 visual_prompt]: 	Training 900/1106. train loss: 518.6501,	0.6178 s / batch. (data: 2.82e-04). ETA=16:32:54, max mem: 15.9 GB 
[10/24 08:05:24 visual_prompt]: 	Training 1000/1106. train loss: 992.4501,	0.6316 s / batch. (data: 7.96e-04). ETA=16:54:03, max mem: 15.9 GB 
[10/24 08:06:27 visual_prompt]: 	Training 1100/1106. train loss: 31.6204,	0.6152 s / batch. (data: 1.76e-04). ETA=16:26:36, max mem: 15.9 GB 
[10/24 08:06:31 visual_prompt]: Epoch 13 / 100: avg data time: 4.65e-03, avg batch time: 0.6307, average train loss: 285.1454
[10/24 08:07:21 visual_prompt]: 	Test 100/123. loss: 148.534, 0.2437 s / batch. (data: 3.17e-05)max mem: 15.89950 GB 
[10/24 08:07:31 visual_prompt]: Inference (val):avg data time: 3.14e-04, avg batch time: 0.2324, average loss: 162.4774
[10/24 08:07:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.07	
[10/24 08:07:31 visual_prompt]: Best epoch 13: best metric: -162.477
[10/24 08:07:31 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/24 08:08:36 visual_prompt]: 	Training 100/1106. train loss: 319.8002,	0.6240 s / batch. (data: 4.82e-04). ETA=16:39:37, max mem: 15.9 GB 
[10/24 08:09:39 visual_prompt]: 	Training 200/1106. train loss: 558.6016,	0.6135 s / batch. (data: 3.20e-04). ETA=16:21:50, max mem: 15.9 GB 
[10/24 08:10:42 visual_prompt]: 	Training 300/1106. train loss: 267.0535,	0.6127 s / batch. (data: 3.19e-04). ETA=16:19:26, max mem: 15.9 GB 
[10/24 08:11:45 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6216 s / batch. (data: 5.45e-03). ETA=16:32:45, max mem: 15.9 GB 
[10/24 08:12:48 visual_prompt]: 	Training 500/1106. train loss: 115.0708,	0.6401 s / batch. (data: 7.94e-04). ETA=17:01:15, max mem: 15.9 GB 
[10/24 08:13:51 visual_prompt]: 	Training 600/1106. train loss: 365.4273,	0.6471 s / batch. (data: 8.09e-04). ETA=17:11:19, max mem: 15.9 GB 
[10/24 08:14:53 visual_prompt]: 	Training 700/1106. train loss: 2.8027,	0.6519 s / batch. (data: 5.91e-03). ETA=17:17:52, max mem: 15.9 GB 
[10/24 08:15:56 visual_prompt]: 	Training 800/1106. train loss: 406.0083,	0.6374 s / batch. (data: 6.01e-03). ETA=16:53:46, max mem: 15.9 GB 
[10/24 08:16:59 visual_prompt]: 	Training 900/1106. train loss: 423.3364,	0.6409 s / batch. (data: 2.20e-02). ETA=16:58:07, max mem: 15.9 GB 
[10/24 08:18:02 visual_prompt]: 	Training 1000/1106. train loss: 403.2524,	0.6308 s / batch. (data: 1.42e-02). ETA=16:41:07, max mem: 15.9 GB 
[10/24 08:19:05 visual_prompt]: 	Training 1100/1106. train loss: 151.2441,	0.6124 s / batch. (data: 1.55e-04). ETA=16:10:52, max mem: 15.9 GB 
[10/24 08:19:08 visual_prompt]: Epoch 14 / 100: avg data time: 4.84e-03, avg batch time: 0.6302, average train loss: 249.8106
[10/24 08:19:58 visual_prompt]: 	Test 100/123. loss: 184.600, 0.2317 s / batch. (data: 2.96e-05)max mem: 15.89950 GB 
[10/24 08:20:09 visual_prompt]: Inference (val):avg data time: 9.41e-05, avg batch time: 0.2328, average loss: 169.2135
[10/24 08:20:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.65	
[10/24 08:20:09 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/24 08:21:14 visual_prompt]: 	Training 100/1106. train loss: 771.1382,	0.6122 s / batch. (data: 2.74e-04). ETA=16:09:24, max mem: 15.9 GB 
[10/24 08:22:17 visual_prompt]: 	Training 200/1106. train loss: 46.0594,	0.6118 s / batch. (data: 2.73e-04). ETA=16:07:45, max mem: 15.9 GB 
[10/24 08:23:19 visual_prompt]: 	Training 300/1106. train loss: 240.8434,	0.6320 s / batch. (data: 8.64e-04). ETA=16:38:46, max mem: 15.9 GB 
[10/24 08:24:22 visual_prompt]: 	Training 400/1106. train loss: 409.6314,	0.6313 s / batch. (data: 3.21e-04). ETA=16:36:36, max mem: 15.9 GB 
[10/24 08:25:25 visual_prompt]: 	Training 500/1106. train loss: 164.7146,	0.6187 s / batch. (data: 3.18e-04). ETA=16:15:36, max mem: 15.9 GB 
[10/24 08:26:28 visual_prompt]: 	Training 600/1106. train loss: 210.3940,	0.6343 s / batch. (data: 1.23e-02). ETA=16:39:10, max mem: 15.9 GB 
[10/24 08:27:31 visual_prompt]: 	Training 700/1106. train loss: 208.6012,	0.6383 s / batch. (data: 7.10e-04). ETA=16:44:30, max mem: 15.9 GB 
[10/24 08:28:33 visual_prompt]: 	Training 800/1106. train loss: 37.6986,	0.6136 s / batch. (data: 2.99e-04). ETA=16:04:28, max mem: 15.9 GB 
[10/24 08:29:36 visual_prompt]: 	Training 900/1106. train loss: 1199.4781,	0.6347 s / batch. (data: 1.29e-02). ETA=16:36:40, max mem: 15.9 GB 
[10/24 08:30:39 visual_prompt]: 	Training 1000/1106. train loss: 263.3890,	0.6413 s / batch. (data: 5.89e-03). ETA=16:45:57, max mem: 15.9 GB 
[10/24 08:31:42 visual_prompt]: 	Training 1100/1106. train loss: 297.7982,	0.6122 s / batch. (data: 1.62e-04). ETA=15:59:14, max mem: 15.9 GB 
[10/24 08:31:46 visual_prompt]: Epoch 15 / 100: avg data time: 4.17e-03, avg batch time: 0.6295, average train loss: 284.5635
[10/24 08:32:36 visual_prompt]: 	Test 100/123. loss: 110.195, 0.2255 s / batch. (data: 3.17e-05)max mem: 15.89950 GB 
[10/24 08:32:46 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2326, average loss: 85.7260
[10/24 08:32:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.94	
[10/24 08:32:46 visual_prompt]: Best epoch 15: best metric: -85.726
[10/24 08:32:46 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/24 08:33:51 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6065 s / batch. (data: 2.85e-04). ETA=15:49:19, max mem: 15.9 GB 
[10/24 08:34:54 visual_prompt]: 	Training 200/1106. train loss: 247.8087,	0.6186 s / batch. (data: 2.81e-04). ETA=16:07:09, max mem: 15.9 GB 
[10/24 08:35:57 visual_prompt]: 	Training 300/1106. train loss: 338.7211,	0.6498 s / batch. (data: 7.43e-04). ETA=16:54:55, max mem: 15.9 GB 
[10/24 08:37:00 visual_prompt]: 	Training 400/1106. train loss: 167.7831,	0.6188 s / batch. (data: 3.12e-04). ETA=16:05:25, max mem: 15.9 GB 
[10/24 08:38:02 visual_prompt]: 	Training 500/1106. train loss: 245.3673,	0.6320 s / batch. (data: 2.98e-04). ETA=16:25:01, max mem: 15.9 GB 
[10/24 08:39:05 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6072 s / batch. (data: 3.22e-04). ETA=15:45:19, max mem: 15.9 GB 
[10/24 08:40:08 visual_prompt]: 	Training 700/1106. train loss: 897.5934,	0.6474 s / batch. (data: 7.45e-04). ETA=16:46:49, max mem: 15.9 GB 
[10/24 08:41:11 visual_prompt]: 	Training 800/1106. train loss: 87.9337,	0.6294 s / batch. (data: 1.06e-02). ETA=16:17:44, max mem: 15.9 GB 
[10/24 08:42:14 visual_prompt]: 	Training 900/1106. train loss: 282.2012,	0.6288 s / batch. (data: 7.79e-04). ETA=16:15:50, max mem: 15.9 GB 
[10/24 08:43:16 visual_prompt]: 	Training 1000/1106. train loss: 411.7514,	0.6176 s / batch. (data: 2.56e-04). ETA=15:57:27, max mem: 15.9 GB 
[10/24 08:44:19 visual_prompt]: 	Training 1100/1106. train loss: 107.5401,	0.6183 s / batch. (data: 1.54e-04). ETA=15:57:29, max mem: 15.9 GB 
[10/24 08:44:23 visual_prompt]: Epoch 16 / 100: avg data time: 4.16e-03, avg batch time: 0.6300, average train loss: 239.2274
[10/24 08:45:13 visual_prompt]: 	Test 100/123. loss: 288.137, 0.2350 s / batch. (data: 3.79e-05)max mem: 15.89950 GB 
[10/24 08:45:24 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2325, average loss: 315.9947
[10/24 08:45:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.99	
[10/24 08:45:24 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/24 08:46:29 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6240 s / batch. (data: 3.38e-04). ETA=16:05:07, max mem: 15.9 GB 
[10/24 08:47:32 visual_prompt]: 	Training 200/1106. train loss: 0.7046,	0.6193 s / batch. (data: 3.04e-04). ETA=15:56:51, max mem: 15.9 GB 
[10/24 08:48:35 visual_prompt]: 	Training 300/1106. train loss: 732.5887,	0.6290 s / batch. (data: 7.99e-04). ETA=16:10:49, max mem: 15.9 GB 
[10/24 08:49:38 visual_prompt]: 	Training 400/1106. train loss: 77.7133,	0.6619 s / batch. (data: 1.09e-02). ETA=17:00:26, max mem: 15.9 GB 
[10/24 08:50:40 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6231 s / batch. (data: 7.91e-04). ETA=15:59:33, max mem: 15.9 GB 
[10/24 08:51:43 visual_prompt]: 	Training 600/1106. train loss: 395.0588,	0.6360 s / batch. (data: 7.97e-04). ETA=16:18:22, max mem: 15.9 GB 
[10/24 08:52:46 visual_prompt]: 	Training 700/1106. train loss: 670.9631,	0.6417 s / batch. (data: 8.12e-04). ETA=16:26:05, max mem: 15.9 GB 
[10/24 08:53:49 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6195 s / batch. (data: 8.08e-04). ETA=15:51:01, max mem: 15.9 GB 
[10/24 08:54:52 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6228 s / batch. (data: 7.43e-04). ETA=15:54:57, max mem: 15.9 GB 
[10/24 08:55:54 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6345 s / batch. (data: 7.63e-04). ETA=16:11:56, max mem: 15.9 GB 
[10/24 08:56:57 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6073 s / batch. (data: 1.44e-04). ETA=15:29:13, max mem: 15.9 GB 
[10/24 08:57:01 visual_prompt]: Epoch 17 / 100: avg data time: 4.73e-03, avg batch time: 0.6302, average train loss: 222.6916
[10/24 08:57:51 visual_prompt]: 	Test 100/123. loss: 141.572, 0.2344 s / batch. (data: 3.08e-05)max mem: 15.89950 GB 
[10/24 08:58:02 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.2321, average loss: 154.7203
[10/24 08:58:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 39.62	
[10/24 08:58:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/24 08:59:07 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6355 s / batch. (data: 3.05e-04). ETA=16:11:10, max mem: 15.9 GB 
[10/24 09:00:10 visual_prompt]: 	Training 200/1106. train loss: 133.0425,	0.6275 s / batch. (data: 5.77e-04). ETA=15:57:55, max mem: 15.9 GB 
[10/24 09:01:13 visual_prompt]: 	Training 300/1106. train loss: 1852.5836,	0.6378 s / batch. (data: 5.50e-03). ETA=16:12:33, max mem: 15.9 GB 
[10/24 09:02:15 visual_prompt]: 	Training 400/1106. train loss: 444.5279,	0.6321 s / batch. (data: 6.05e-03). ETA=16:02:54, max mem: 15.9 GB 
[10/24 09:03:18 visual_prompt]: 	Training 500/1106. train loss: 283.4081,	0.6188 s / batch. (data: 3.35e-04). ETA=15:41:38, max mem: 15.9 GB 
[10/24 09:04:21 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6418 s / batch. (data: 1.19e-02). ETA=16:15:32, max mem: 15.9 GB 
[10/24 09:05:24 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6598 s / batch. (data: 3.17e-02). ETA=16:41:44, max mem: 15.9 GB 
[10/24 09:06:27 visual_prompt]: 	Training 800/1106. train loss: 58.1194,	0.6259 s / batch. (data: 7.94e-04). ETA=15:49:19, max mem: 15.9 GB 
[10/24 09:07:30 visual_prompt]: 	Training 900/1106. train loss: 29.2506,	0.6250 s / batch. (data: 8.08e-04). ETA=15:46:51, max mem: 15.9 GB 
[10/24 09:08:33 visual_prompt]: 	Training 1000/1106. train loss: 43.0599,	0.6203 s / batch. (data: 3.24e-04). ETA=15:38:39, max mem: 15.9 GB 
[10/24 09:09:35 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6075 s / batch. (data: 1.49e-04). ETA=15:18:15, max mem: 15.9 GB 
[10/24 09:09:41 visual_prompt]: Epoch 18 / 100: avg data time: 4.72e-03, avg batch time: 0.6305, average train loss: 266.5562
[10/24 09:10:31 visual_prompt]: 	Test 100/123. loss: 91.238, 0.2248 s / batch. (data: 2.65e-05)max mem: 15.89950 GB 
[10/24 09:10:41 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.2321, average loss: 83.9631
[10/24 09:10:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.83	
[10/24 09:10:41 visual_prompt]: Best epoch 18: best metric: -83.963
[10/24 09:10:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[10/24 09:11:46 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6425 s / batch. (data: 2.39e-02). ETA=16:10:08, max mem: 15.9 GB 
[10/24 09:12:49 visual_prompt]: 	Training 200/1106. train loss: 521.6035,	0.6268 s / batch. (data: 8.35e-04). ETA=15:45:19, max mem: 15.9 GB 
[10/24 09:13:52 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6380 s / batch. (data: 1.10e-02). ETA=16:01:07, max mem: 15.9 GB 
[10/24 09:14:55 visual_prompt]: 	Training 400/1106. train loss: 496.1814,	0.6133 s / batch. (data: 3.20e-04). ETA=15:22:52, max mem: 15.9 GB 
[10/24 09:15:58 visual_prompt]: 	Training 500/1106. train loss: 497.9193,	0.6239 s / batch. (data: 2.96e-04). ETA=15:37:46, max mem: 15.9 GB 
[10/24 09:17:00 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6231 s / batch. (data: 3.40e-04). ETA=15:35:34, max mem: 15.9 GB 
[10/24 09:18:03 visual_prompt]: 	Training 700/1106. train loss: 123.8461,	0.6227 s / batch. (data: 3.53e-04). ETA=15:33:56, max mem: 15.9 GB 
[10/24 09:19:06 visual_prompt]: 	Training 800/1106. train loss: 208.5657,	0.6291 s / batch. (data: 8.00e-04). ETA=15:42:26, max mem: 15.9 GB 
[10/24 09:20:09 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6216 s / batch. (data: 7.98e-04). ETA=15:30:18, max mem: 15.9 GB 
[10/24 09:21:12 visual_prompt]: 	Training 1000/1106. train loss: 337.6283,	0.6459 s / batch. (data: 7.65e-04). ETA=16:05:35, max mem: 15.9 GB 
[10/24 09:22:15 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6075 s / batch. (data: 2.00e-04). ETA=15:07:09, max mem: 15.9 GB 
[10/24 09:22:18 visual_prompt]: Epoch 19 / 100: avg data time: 4.45e-03, avg batch time: 0.6301, average train loss: 233.9435
[10/24 09:23:09 visual_prompt]: 	Test 100/123. loss: 247.485, 0.2399 s / batch. (data: 2.38e-05)max mem: 15.89950 GB 
[10/24 09:23:19 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2334, average loss: 223.4750
[10/24 09:23:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.69	
[10/24 09:23:19 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[10/24 09:24:25 visual_prompt]: 	Training 100/1106. train loss: 359.4921,	0.6148 s / batch. (data: 3.08e-04). ETA=15:16:55, max mem: 15.9 GB 
[10/24 09:25:28 visual_prompt]: 	Training 200/1106. train loss: 96.6648,	0.6299 s / batch. (data: 7.79e-04). ETA=15:38:24, max mem: 15.9 GB 
[10/24 09:26:31 visual_prompt]: 	Training 300/1106. train loss: 385.7150,	0.6129 s / batch. (data: 3.08e-04). ETA=15:12:02, max mem: 15.9 GB 
[10/24 09:27:34 visual_prompt]: 	Training 400/1106. train loss: 160.8277,	0.6146 s / batch. (data: 4.80e-04). ETA=15:13:32, max mem: 15.9 GB 
[10/24 09:28:37 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6439 s / batch. (data: 2.78e-02). ETA=15:55:58, max mem: 15.9 GB 
[10/24 09:29:40 visual_prompt]: 	Training 600/1106. train loss: 107.3903,	0.6196 s / batch. (data: 3.29e-04). ETA=15:18:57, max mem: 15.9 GB 
[10/24 09:30:43 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6196 s / batch. (data: 7.49e-04). ETA=15:17:53, max mem: 15.9 GB 
[10/24 09:31:46 visual_prompt]: 	Training 800/1106. train loss: 33.9230,	0.6615 s / batch. (data: 3.01e-02). ETA=16:18:54, max mem: 15.9 GB 
[10/24 09:32:49 visual_prompt]: 	Training 900/1106. train loss: 120.5076,	0.6484 s / batch. (data: 7.82e-04). ETA=15:58:22, max mem: 15.9 GB 
[10/24 09:33:52 visual_prompt]: 	Training 1000/1106. train loss: 221.6574,	0.6198 s / batch. (data: 3.38e-04). ETA=15:15:05, max mem: 15.9 GB 
[10/24 09:34:55 visual_prompt]: 	Training 1100/1106. train loss: 102.8551,	0.6181 s / batch. (data: 2.87e-04). ETA=15:11:30, max mem: 15.9 GB 
[10/24 09:34:59 visual_prompt]: Epoch 20 / 100: avg data time: 5.55e-03, avg batch time: 0.6325, average train loss: 263.0784
[10/24 09:35:49 visual_prompt]: 	Test 100/123. loss: 533.411, 0.2255 s / batch. (data: 4.60e-05)max mem: 15.89950 GB 
[10/24 09:36:00 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.2324, average loss: 481.6289
[10/24 09:36:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.54	
[10/24 09:36:00 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[10/24 09:37:05 visual_prompt]: 	Training 100/1106. train loss: 20.0292,	0.6411 s / batch. (data: 9.07e-03). ETA=15:44:16, max mem: 15.9 GB 
[10/24 09:38:08 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6762 s / batch. (data: 1.69e-02). ETA=16:34:52, max mem: 15.9 GB 
[10/24 09:39:11 visual_prompt]: 	Training 300/1106. train loss: 520.5174,	0.6360 s / batch. (data: 3.24e-04). ETA=15:34:44, max mem: 15.9 GB 
[10/24 09:40:14 visual_prompt]: 	Training 400/1106. train loss: 281.2409,	0.6245 s / batch. (data: 7.37e-04). ETA=15:16:47, max mem: 15.9 GB 
[10/24 09:41:17 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6086 s / batch. (data: 3.03e-04). ETA=14:52:28, max mem: 15.9 GB 
[10/24 09:42:19 visual_prompt]: 	Training 600/1106. train loss: 741.4407,	0.6203 s / batch. (data: 3.26e-04). ETA=15:08:34, max mem: 15.9 GB 
[10/24 09:43:22 visual_prompt]: 	Training 700/1106. train loss: 518.8234,	0.6148 s / batch. (data: 3.14e-04). ETA=14:59:30, max mem: 15.9 GB 
[10/24 09:44:25 visual_prompt]: 	Training 800/1106. train loss: 6.0093,	0.6464 s / batch. (data: 8.01e-04). ETA=15:44:38, max mem: 15.9 GB 
[10/24 09:45:28 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6144 s / batch. (data: 2.63e-04). ETA=14:56:45, max mem: 15.9 GB 
[10/24 09:46:31 visual_prompt]: 	Training 1000/1106. train loss: 225.4378,	0.6249 s / batch. (data: 3.26e-04). ETA=15:11:08, max mem: 15.9 GB 
[10/24 09:47:34 visual_prompt]: 	Training 1100/1106. train loss: 1312.6666,	0.6195 s / batch. (data: 1.20e-04). ETA=15:02:12, max mem: 15.9 GB 
[10/24 09:47:37 visual_prompt]: Epoch 21 / 100: avg data time: 4.58e-03, avg batch time: 0.6307, average train loss: 229.2953
[10/24 09:48:27 visual_prompt]: 	Test 100/123. loss: 234.028, 0.2405 s / batch. (data: 4.05e-05)max mem: 15.89950 GB 
[10/24 09:48:38 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2323, average loss: 210.1609
[10/24 09:48:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.65	
[10/24 09:48:38 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[10/24 09:49:43 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6196 s / batch. (data: 1.19e-02). ETA=15:01:17, max mem: 15.9 GB 
[10/24 09:50:45 visual_prompt]: 	Training 200/1106. train loss: 2600.1360,	0.6477 s / batch. (data: 7.78e-04). ETA=15:41:06, max mem: 15.9 GB 
[10/24 09:51:48 visual_prompt]: 	Training 300/1106. train loss: 278.9817,	0.6341 s / batch. (data: 3.28e-04). ETA=15:20:13, max mem: 15.9 GB 
[10/24 09:52:51 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6191 s / batch. (data: 3.09e-04). ETA=14:57:23, max mem: 15.9 GB 
[10/24 09:53:54 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6224 s / batch. (data: 3.35e-04). ETA=15:01:13, max mem: 15.9 GB 
[10/24 09:54:57 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6202 s / batch. (data: 7.98e-04). ETA=14:56:58, max mem: 15.9 GB 
[10/24 09:56:00 visual_prompt]: 	Training 700/1106. train loss: 104.8786,	0.6179 s / batch. (data: 2.26e-04). ETA=14:52:35, max mem: 15.9 GB 
[10/24 09:57:03 visual_prompt]: 	Training 800/1106. train loss: 924.6542,	0.6342 s / batch. (data: 7.97e-04). ETA=15:15:01, max mem: 15.9 GB 
[10/24 09:58:06 visual_prompt]: 	Training 900/1106. train loss: 107.3223,	0.6215 s / batch. (data: 3.34e-04). ETA=14:55:43, max mem: 15.9 GB 
[10/24 09:59:09 visual_prompt]: 	Training 1000/1106. train loss: 580.6085,	0.6352 s / batch. (data: 7.46e-04). ETA=15:14:22, max mem: 15.9 GB 
[10/24 10:00:12 visual_prompt]: 	Training 1100/1106. train loss: 512.7764,	0.6173 s / batch. (data: 1.79e-04). ETA=14:47:40, max mem: 15.9 GB 
[10/24 10:00:16 visual_prompt]: Epoch 22 / 100: avg data time: 5.17e-03, avg batch time: 0.6309, average train loss: 253.2964
[10/24 10:01:06 visual_prompt]: 	Test 100/123. loss: 423.003, 0.2325 s / batch. (data: 3.96e-05)max mem: 15.89950 GB 
[10/24 10:01:16 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2325, average loss: 381.8878
[10/24 10:01:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.91	
[10/24 10:01:16 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[10/24 10:02:22 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6062 s / batch. (data: 2.68e-04). ETA=14:30:38, max mem: 15.9 GB 
[10/24 10:03:25 visual_prompt]: 	Training 200/1106. train loss: 57.0138,	0.6200 s / batch. (data: 7.54e-04). ETA=14:49:22, max mem: 15.9 GB 
[10/24 10:04:28 visual_prompt]: 	Training 300/1106. train loss: 81.2514,	0.6729 s / batch. (data: 3.82e-02). ETA=16:04:11, max mem: 15.9 GB 
[10/24 10:05:31 visual_prompt]: 	Training 400/1106. train loss: 21.4865,	0.6170 s / batch. (data: 3.28e-04). ETA=14:42:58, max mem: 15.9 GB 
[10/24 10:06:34 visual_prompt]: 	Training 500/1106. train loss: 16.9596,	0.6331 s / batch. (data: 3.11e-04). ETA=15:04:59, max mem: 15.9 GB 
[10/24 10:07:37 visual_prompt]: 	Training 600/1106. train loss: 252.1056,	0.6188 s / batch. (data: 3.17e-04). ETA=14:43:31, max mem: 15.9 GB 
[10/24 10:08:39 visual_prompt]: 	Training 700/1106. train loss: 359.0958,	0.6294 s / batch. (data: 7.98e-04). ETA=14:57:34, max mem: 15.9 GB 
[10/24 10:09:42 visual_prompt]: 	Training 800/1106. train loss: 36.2811,	0.6257 s / batch. (data: 7.93e-04). ETA=14:51:15, max mem: 15.9 GB 
[10/24 10:10:45 visual_prompt]: 	Training 900/1106. train loss: 52.8259,	0.6272 s / batch. (data: 7.66e-04). ETA=14:52:23, max mem: 15.9 GB 
[10/24 10:11:48 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6194 s / batch. (data: 7.69e-04). ETA=14:40:19, max mem: 15.9 GB 
[10/24 10:12:51 visual_prompt]: 	Training 1100/1106. train loss: 83.1766,	0.6134 s / batch. (data: 1.39e-04). ETA=14:30:45, max mem: 15.9 GB 
[10/24 10:12:54 visual_prompt]: Epoch 23 / 100: avg data time: 5.22e-03, avg batch time: 0.6310, average train loss: 237.4696
[10/24 10:13:50 visual_prompt]: 	Test 100/123. loss: 125.280, 0.2286 s / batch. (data: 4.15e-05)max mem: 15.89950 GB 
[10/24 10:14:00 visual_prompt]: Inference (val):avg data time: 9.00e-05, avg batch time: 0.2319, average loss: 113.9821
[10/24 10:14:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.59	
[10/24 10:14:00 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[10/24 10:15:06 visual_prompt]: 	Training 100/1106. train loss: 13.3821,	0.6163 s / batch. (data: 3.17e-04). ETA=14:33:47, max mem: 15.9 GB 
[10/24 10:16:08 visual_prompt]: 	Training 200/1106. train loss: 216.1696,	0.6236 s / batch. (data: 3.09e-04). ETA=14:42:59, max mem: 15.9 GB 
[10/24 10:17:11 visual_prompt]: 	Training 300/1106. train loss: 200.2573,	0.6343 s / batch. (data: 7.76e-04). ETA=14:57:09, max mem: 15.9 GB 
[10/24 10:18:14 visual_prompt]: 	Training 400/1106. train loss: 33.7572,	0.6190 s / batch. (data: 3.58e-04). ETA=14:34:26, max mem: 15.9 GB 
[10/24 10:19:17 visual_prompt]: 	Training 500/1106. train loss: 207.4552,	0.6187 s / batch. (data: 7.59e-04). ETA=14:32:56, max mem: 15.9 GB 
[10/24 10:20:20 visual_prompt]: 	Training 600/1106. train loss: 516.8557,	0.6300 s / batch. (data: 1.20e-02). ETA=14:47:50, max mem: 15.9 GB 
[10/24 10:21:22 visual_prompt]: 	Training 700/1106. train loss: 658.9598,	0.6236 s / batch. (data: 3.90e-04). ETA=14:37:53, max mem: 15.9 GB 
[10/24 10:22:26 visual_prompt]: 	Training 800/1106. train loss: 125.7556,	0.6281 s / batch. (data: 8.17e-04). ETA=14:43:06, max mem: 15.9 GB 
[10/24 10:23:29 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6335 s / batch. (data: 3.44e-04). ETA=14:49:42, max mem: 15.9 GB 
[10/24 10:24:31 visual_prompt]: 	Training 1000/1106. train loss: 46.7845,	0.6137 s / batch. (data: 7.59e-04). ETA=14:20:50, max mem: 15.9 GB 
[10/24 10:25:34 visual_prompt]: 	Training 1100/1106. train loss: 476.2261,	0.6118 s / batch. (data: 1.41e-04). ETA=14:17:10, max mem: 15.9 GB 
[10/24 10:25:38 visual_prompt]: Epoch 24 / 100: avg data time: 4.85e-03, avg batch time: 0.6306, average train loss: 244.4285
[10/24 10:26:28 visual_prompt]: 	Test 100/123. loss: 134.349, 0.2262 s / batch. (data: 3.74e-05)max mem: 15.89950 GB 
[10/24 10:26:39 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2324, average loss: 122.1818
[10/24 10:26:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.77	
[10/24 10:26:39 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[10/24 10:27:44 visual_prompt]: 	Training 100/1106. train loss: 92.4569,	0.6278 s / batch. (data: 1.06e-02). ETA=14:38:29, max mem: 15.9 GB 
[10/24 10:28:47 visual_prompt]: 	Training 200/1106. train loss: 25.0393,	0.6179 s / batch. (data: 3.03e-04). ETA=14:23:37, max mem: 15.9 GB 
[10/24 10:29:50 visual_prompt]: 	Training 300/1106. train loss: 200.7584,	0.6320 s / batch. (data: 8.53e-04). ETA=14:42:13, max mem: 15.9 GB 
[10/24 10:30:52 visual_prompt]: 	Training 400/1106. train loss: 776.8582,	0.6327 s / batch. (data: 8.50e-04). ETA=14:42:07, max mem: 15.9 GB 
[10/24 10:31:55 visual_prompt]: 	Training 500/1106. train loss: 334.8212,	0.6169 s / batch. (data: 4.97e-04). ETA=14:19:05, max mem: 15.9 GB 
[10/24 10:32:58 visual_prompt]: 	Training 600/1106. train loss: 392.6117,	0.6169 s / batch. (data: 2.57e-04). ETA=14:18:04, max mem: 15.9 GB 
[10/24 10:34:00 visual_prompt]: 	Training 700/1106. train loss: 215.4574,	0.6381 s / batch. (data: 1.59e-02). ETA=14:46:32, max mem: 15.9 GB 
[10/24 10:35:03 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6210 s / batch. (data: 8.18e-04). ETA=14:21:44, max mem: 15.9 GB 
[10/24 10:36:06 visual_prompt]: 	Training 900/1106. train loss: 383.7339,	0.6131 s / batch. (data: 3.13e-04). ETA=14:09:44, max mem: 15.9 GB 
[10/24 10:37:09 visual_prompt]: 	Training 1000/1106. train loss: 313.9962,	0.6123 s / batch. (data: 3.14e-04). ETA=14:07:34, max mem: 15.9 GB 
[10/24 10:38:12 visual_prompt]: 	Training 1100/1106. train loss: 622.9626,	0.6180 s / batch. (data: 1.55e-04). ETA=14:14:23, max mem: 15.9 GB 
[10/24 10:38:15 visual_prompt]: Epoch 25 / 100: avg data time: 4.95e-03, avg batch time: 0.6297, average train loss: 237.3509
[10/24 10:39:05 visual_prompt]: 	Test 100/123. loss: 117.571, 0.2261 s / batch. (data: 3.86e-05)max mem: 15.89950 GB 
[10/24 10:39:16 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2326, average loss: 129.0176
[10/24 10:39:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.05	
[10/24 10:39:16 visual_prompt]: Stopping early.
[10/24 10:39:17 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 10:39:17 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 10:39:17 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 10:39:17 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 10:39:17 visual_prompt]: Training with config:
[10/24 10:39:17 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr50.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 10:39:17 visual_prompt]: Loading training data...
[10/24 10:39:17 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 10:39:17 visual_prompt]: Loading validation data...
[10/24 10:39:17 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 10:39:17 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/24 10:39:19 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/24 10:39:19 visual_prompt]: tuned percent:0.522
[10/24 10:39:19 visual_prompt]: Device used for model: 0
[10/24 10:39:19 visual_prompt]: Setting up Evaluator...
[10/24 10:39:19 visual_prompt]: Setting up Trainer...
[10/24 10:39:19 visual_prompt]: 	Setting up the optimizer...
[10/24 10:39:19 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 10:40:25 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6327 s / batch. (data: 1.55e-02). ETA=19:25:16, max mem: 15.9 GB 
[10/24 10:41:28 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6192 s / batch. (data: 3.02e-04). ETA=18:59:16, max mem: 15.9 GB 
[10/24 10:42:31 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6561 s / batch. (data: 8.21e-04). ETA=20:06:12, max mem: 15.9 GB 
[10/24 10:43:35 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6444 s / batch. (data: 7.98e-04). ETA=19:43:38, max mem: 15.9 GB 
[10/24 10:44:38 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6396 s / batch. (data: 1.05e-02). ETA=19:33:42, max mem: 15.9 GB 
[10/24 10:45:41 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6516 s / batch. (data: 7.88e-04). ETA=19:54:40, max mem: 15.9 GB 
[10/24 10:46:45 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6338 s / batch. (data: 8.15e-04). ETA=19:20:57, max mem: 15.9 GB 
[10/24 10:47:48 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6196 s / batch. (data: 3.14e-04). ETA=18:53:50, max mem: 15.9 GB 
[10/24 10:48:51 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6245 s / batch. (data: 3.21e-04). ETA=19:01:47, max mem: 15.9 GB 
[10/24 10:49:54 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6325 s / batch. (data: 8.03e-04). ETA=19:15:25, max mem: 15.9 GB 
[10/24 10:50:57 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6191 s / batch. (data: 1.57e-04). ETA=18:49:49, max mem: 15.9 GB 
[10/24 10:51:01 visual_prompt]: Epoch 1 / 100: avg data time: 4.68e-03, avg batch time: 0.6345, average train loss: 1.4028
[10/24 10:51:51 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2342 s / batch. (data: 2.86e-05)max mem: 15.90529 GB 
[10/24 10:52:02 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2333, average loss: 1.3505
[10/24 10:52:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/24 10:52:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/24 10:53:06 visual_prompt]: 	Training 100/1106. train loss: 158.6233,	0.6287 s / batch. (data: 7.82e-04). ETA=19:06:14, max mem: 15.9 GB 
[10/24 10:54:09 visual_prompt]: 	Training 200/1106. train loss: 44.8116,	0.6416 s / batch. (data: 7.94e-04). ETA=19:28:40, max mem: 15.9 GB 
[10/24 10:55:13 visual_prompt]: 	Training 300/1106. train loss: 85.6117,	0.6370 s / batch. (data: 8.08e-04). ETA=19:19:13, max mem: 15.9 GB 
[10/24 10:56:15 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6074 s / batch. (data: 3.14e-04). ETA=18:24:27, max mem: 15.9 GB 
[10/24 10:57:18 visual_prompt]: 	Training 500/1106. train loss: 14.8512,	0.6479 s / batch. (data: 7.41e-04). ETA=19:36:59, max mem: 15.9 GB 
[10/24 10:58:22 visual_prompt]: 	Training 600/1106. train loss: 16.2388,	0.6232 s / batch. (data: 7.40e-04). ETA=18:50:57, max mem: 15.9 GB 
[10/24 10:59:25 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6157 s / batch. (data: 7.55e-04). ETA=18:36:29, max mem: 15.9 GB 
[10/24 11:00:28 visual_prompt]: 	Training 800/1106. train loss: 40.7202,	0.6303 s / batch. (data: 3.39e-04). ETA=19:01:54, max mem: 15.9 GB 
[10/24 11:01:31 visual_prompt]: 	Training 900/1106. train loss: 27.2108,	0.6474 s / batch. (data: 7.89e-04). ETA=19:31:41, max mem: 15.9 GB 
[10/24 11:02:34 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6170 s / batch. (data: 2.62e-04). ETA=18:35:37, max mem: 15.9 GB 
[10/24 11:03:37 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6191 s / batch. (data: 1.46e-04). ETA=18:38:22, max mem: 15.9 GB 
[10/24 11:03:41 visual_prompt]: Epoch 2 / 100: avg data time: 3.85e-03, avg batch time: 0.6322, average train loss: 34.1237
[10/24 11:04:31 visual_prompt]: 	Test 100/123. loss: 135.334, 0.2316 s / batch. (data: 3.77e-05)max mem: 15.90529 GB 
[10/24 11:04:42 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.2311, average loss: 121.9094
[10/24 11:04:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[10/24 11:04:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/24 11:05:48 visual_prompt]: 	Training 100/1106. train loss: 83.5547,	0.6283 s / batch. (data: 8.40e-04). ETA=18:54:02, max mem: 15.9 GB 
[10/24 11:06:51 visual_prompt]: 	Training 200/1106. train loss: 28.6865,	0.6586 s / batch. (data: 7.88e-04). ETA=19:47:37, max mem: 15.9 GB 
[10/24 11:07:54 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6193 s / batch. (data: 4.47e-04). ETA=18:35:41, max mem: 15.9 GB 
[10/24 11:08:56 visual_prompt]: 	Training 400/1106. train loss: 0.0542,	0.6486 s / batch. (data: 5.89e-03). ETA=19:27:17, max mem: 15.9 GB 
[10/24 11:10:04 visual_prompt]: 	Training 500/1106. train loss: 189.6586,	0.6181 s / batch. (data: 3.25e-04). ETA=18:31:28, max mem: 15.9 GB 
[10/24 11:11:07 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6197 s / batch. (data: 3.71e-04). ETA=18:33:15, max mem: 15.9 GB 
[10/24 11:12:10 visual_prompt]: 	Training 700/1106. train loss: 16.3731,	0.6456 s / batch. (data: 8.79e-04). ETA=19:18:43, max mem: 15.9 GB 
[10/24 11:13:13 visual_prompt]: 	Training 800/1106. train loss: 39.9350,	0.6443 s / batch. (data: 3.59e-04). ETA=19:15:22, max mem: 15.9 GB 
[10/24 11:14:16 visual_prompt]: 	Training 900/1106. train loss: 132.9415,	0.6284 s / batch. (data: 3.05e-04). ETA=18:45:50, max mem: 15.9 GB 
[10/24 11:15:19 visual_prompt]: 	Training 1000/1106. train loss: 89.7862,	0.6178 s / batch. (data: 3.26e-04). ETA=18:25:47, max mem: 15.9 GB 
[10/24 11:16:22 visual_prompt]: 	Training 1100/1106. train loss: 94.7405,	0.6132 s / batch. (data: 1.37e-04). ETA=18:16:33, max mem: 15.9 GB 
[10/24 11:16:25 visual_prompt]: Epoch 3 / 100: avg data time: 8.92e-03, avg batch time: 0.6360, average train loss: 53.9144
[10/24 11:17:15 visual_prompt]: 	Test 100/123. loss: 104.770, 0.2255 s / batch. (data: 3.86e-05)max mem: 15.90529 GB 
[10/24 11:17:26 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2322, average loss: 116.6702
[10/24 11:17:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.32	
[10/24 11:17:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/24 11:18:31 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6171 s / batch. (data: 2.67e-04). ETA=18:22:27, max mem: 15.9 GB 
[10/24 11:19:34 visual_prompt]: 	Training 200/1106. train loss: 321.9269,	0.6288 s / batch. (data: 8.07e-04). ETA=18:42:08, max mem: 15.9 GB 
[10/24 11:20:37 visual_prompt]: 	Training 300/1106. train loss: 90.8264,	0.6131 s / batch. (data: 8.07e-04). ETA=18:13:13, max mem: 15.9 GB 
[10/24 11:21:40 visual_prompt]: 	Training 400/1106. train loss: 76.6674,	0.6374 s / batch. (data: 1.21e-03). ETA=18:55:25, max mem: 15.9 GB 
[10/24 11:22:43 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6453 s / batch. (data: 3.21e-04). ETA=19:08:31, max mem: 15.9 GB 
[10/24 11:23:46 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6092 s / batch. (data: 3.18e-04). ETA=18:03:15, max mem: 15.9 GB 
[10/24 11:24:49 visual_prompt]: 	Training 700/1106. train loss: 295.6277,	0.6400 s / batch. (data: 8.05e-04). ETA=18:56:48, max mem: 15.9 GB 
[10/24 11:25:52 visual_prompt]: 	Training 800/1106. train loss: 119.9365,	0.6320 s / batch. (data: 3.18e-04). ETA=18:41:35, max mem: 15.9 GB 
[10/24 11:26:55 visual_prompt]: 	Training 900/1106. train loss: 366.4704,	0.6329 s / batch. (data: 8.04e-04). ETA=18:42:13, max mem: 15.9 GB 
[10/24 11:27:58 visual_prompt]: 	Training 1000/1106. train loss: 153.5802,	0.6374 s / batch. (data: 1.07e-02). ETA=18:49:04, max mem: 15.9 GB 
[10/24 11:29:01 visual_prompt]: 	Training 1100/1106. train loss: 219.8590,	0.6124 s / batch. (data: 1.78e-04). ETA=18:03:49, max mem: 15.9 GB 
[10/24 11:29:05 visual_prompt]: Epoch 4 / 100: avg data time: 4.37e-03, avg batch time: 0.6315, average train loss: 64.5602
[10/24 11:29:55 visual_prompt]: 	Test 100/123. loss: 86.875, 0.2247 s / batch. (data: 4.46e-05)max mem: 15.90529 GB 
[10/24 11:30:05 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2330, average loss: 96.1257
[10/24 11:30:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.41	
[10/24 11:30:05 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/24 11:31:10 visual_prompt]: 	Training 100/1106. train loss: 184.3729,	0.6348 s / batch. (data: 9.05e-04). ETA=18:42:22, max mem: 15.9 GB 
[10/24 11:32:13 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6134 s / batch. (data: 5.46e-03). ETA=18:03:30, max mem: 15.9 GB 
[10/24 11:33:16 visual_prompt]: 	Training 300/1106. train loss: 146.6105,	0.6237 s / batch. (data: 3.47e-04). ETA=18:20:38, max mem: 15.9 GB 
[10/24 11:34:19 visual_prompt]: 	Training 400/1106. train loss: 71.9173,	0.6271 s / batch. (data: 1.54e-02). ETA=18:25:33, max mem: 15.9 GB 
[10/24 11:35:22 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6082 s / batch. (data: 3.04e-04). ETA=17:51:07, max mem: 15.9 GB 
[10/24 11:36:25 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6195 s / batch. (data: 3.33e-04). ETA=18:10:06, max mem: 15.9 GB 
[10/24 11:37:27 visual_prompt]: 	Training 700/1106. train loss: 35.9653,	0.6250 s / batch. (data: 3.13e-04). ETA=18:18:40, max mem: 15.9 GB 
[10/24 11:38:30 visual_prompt]: 	Training 800/1106. train loss: 139.8474,	0.6175 s / batch. (data: 3.04e-04). ETA=18:04:29, max mem: 15.9 GB 
[10/24 11:39:33 visual_prompt]: 	Training 900/1106. train loss: 45.0045,	0.6171 s / batch. (data: 3.06e-04). ETA=18:02:43, max mem: 15.9 GB 
[10/24 11:40:36 visual_prompt]: 	Training 1000/1106. train loss: 15.5035,	0.6328 s / batch. (data: 3.36e-04). ETA=18:29:17, max mem: 15.9 GB 
[10/24 11:41:39 visual_prompt]: 	Training 1100/1106. train loss: 44.9550,	0.6114 s / batch. (data: 1.41e-04). ETA=17:50:42, max mem: 15.9 GB 
[10/24 11:41:43 visual_prompt]: Epoch 5 / 100: avg data time: 4.37e-03, avg batch time: 0.6305, average train loss: 90.1691
[10/24 11:42:33 visual_prompt]: 	Test 100/123. loss: 94.938, 0.2357 s / batch. (data: 3.03e-05)max mem: 15.90529 GB 
[10/24 11:42:44 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2334, average loss: 84.3313
[10/24 11:42:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.25	
[10/24 11:42:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/24 11:43:48 visual_prompt]: 	Training 100/1106. train loss: 376.6224,	0.6452 s / batch. (data: 5.97e-03). ETA=18:48:43, max mem: 15.9 GB 
[10/24 11:44:51 visual_prompt]: 	Training 200/1106. train loss: 185.7617,	0.6334 s / batch. (data: 3.09e-04). ETA=18:27:02, max mem: 15.9 GB 
[10/24 11:45:54 visual_prompt]: 	Training 300/1106. train loss: 4.9385,	0.6301 s / batch. (data: 7.81e-04). ETA=18:20:18, max mem: 15.9 GB 
[10/24 11:46:57 visual_prompt]: 	Training 400/1106. train loss: 19.5695,	0.6186 s / batch. (data: 2.72e-04). ETA=17:59:13, max mem: 15.9 GB 
[10/24 11:47:59 visual_prompt]: 	Training 500/1106. train loss: 42.4241,	0.6216 s / batch. (data: 3.71e-04). ETA=18:03:16, max mem: 15.9 GB 
[10/24 11:49:03 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6180 s / batch. (data: 3.20e-04). ETA=17:56:04, max mem: 15.9 GB 
[10/24 11:50:05 visual_prompt]: 	Training 700/1106. train loss: 204.6333,	0.6455 s / batch. (data: 5.86e-03). ETA=18:42:52, max mem: 15.9 GB 
[10/24 11:51:08 visual_prompt]: 	Training 800/1106. train loss: 334.7636,	0.6119 s / batch. (data: 3.14e-04). ETA=17:43:21, max mem: 15.9 GB 
[10/24 11:52:11 visual_prompt]: 	Training 900/1106. train loss: 522.8630,	0.6388 s / batch. (data: 1.60e-02). ETA=18:29:06, max mem: 15.9 GB 
[10/24 11:53:14 visual_prompt]: 	Training 1000/1106. train loss: 66.2373,	0.6462 s / batch. (data: 1.82e-02). ETA=18:40:51, max mem: 15.9 GB 
[10/24 11:54:17 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6077 s / batch. (data: 1.57e-04). ETA=17:33:07, max mem: 15.9 GB 
[10/24 11:54:21 visual_prompt]: Epoch 6 / 100: avg data time: 4.36e-03, avg batch time: 0.6302, average train loss: 111.4558
[10/24 11:55:10 visual_prompt]: 	Test 100/123. loss: 154.139, 0.2247 s / batch. (data: 3.10e-05)max mem: 15.90529 GB 
[10/24 11:55:21 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.2322, average loss: 174.2375
[10/24 11:55:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.27	
[10/24 11:55:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/24 11:56:26 visual_prompt]: 	Training 100/1106. train loss: 7.2124,	0.6305 s / batch. (data: 2.99e-04). ETA=18:11:23, max mem: 15.9 GB 
[10/24 11:57:29 visual_prompt]: 	Training 200/1106. train loss: 249.1403,	0.6191 s / batch. (data: 3.65e-04). ETA=17:50:45, max mem: 15.9 GB 
[10/24 11:58:32 visual_prompt]: 	Training 300/1106. train loss: 5.6131,	0.6281 s / batch. (data: 7.51e-04). ETA=18:05:11, max mem: 15.9 GB 
[10/24 11:59:34 visual_prompt]: 	Training 400/1106. train loss: 29.8362,	0.6281 s / batch. (data: 1.27e-03). ETA=18:04:05, max mem: 15.9 GB 
[10/24 12:00:37 visual_prompt]: 	Training 500/1106. train loss: 482.9820,	0.6304 s / batch. (data: 5.40e-03). ETA=18:07:02, max mem: 15.9 GB 
[10/24 12:01:40 visual_prompt]: 	Training 600/1106. train loss: 371.9716,	0.6266 s / batch. (data: 7.36e-04). ETA=17:59:27, max mem: 15.9 GB 
[10/24 12:02:43 visual_prompt]: 	Training 700/1106. train loss: 239.6512,	0.6399 s / batch. (data: 8.62e-04). ETA=18:21:23, max mem: 15.9 GB 
[10/24 12:03:46 visual_prompt]: 	Training 800/1106. train loss: 26.3824,	0.6176 s / batch. (data: 3.18e-04). ETA=17:41:51, max mem: 15.9 GB 
[10/24 12:04:49 visual_prompt]: 	Training 900/1106. train loss: 66.2758,	0.6403 s / batch. (data: 7.41e-04). ETA=18:19:55, max mem: 15.9 GB 
[10/24 12:05:52 visual_prompt]: 	Training 1000/1106. train loss: 85.1982,	0.6440 s / batch. (data: 8.64e-04). ETA=18:25:08, max mem: 15.9 GB 
[10/24 12:06:54 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6062 s / batch. (data: 1.47e-04). ETA=17:19:15, max mem: 15.9 GB 
[10/24 12:06:58 visual_prompt]: Epoch 7 / 100: avg data time: 4.00e-03, avg batch time: 0.6297, average train loss: 124.1007
[10/24 12:07:48 visual_prompt]: 	Test 100/123. loss: 123.880, 0.2286 s / batch. (data: 4.05e-05)max mem: 15.90529 GB 
[10/24 12:07:59 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.2315, average loss: 109.4972
[10/24 12:07:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.28	
[10/24 12:07:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/24 12:09:04 visual_prompt]: 	Training 100/1106. train loss: 182.6709,	0.6222 s / batch. (data: 7.69e-04). ETA=17:45:32, max mem: 15.9 GB 
[10/24 12:10:06 visual_prompt]: 	Training 200/1106. train loss: 96.0887,	0.6249 s / batch. (data: 7.92e-04). ETA=17:49:13, max mem: 15.9 GB 
[10/24 12:11:09 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6386 s / batch. (data: 7.48e-04). ETA=18:11:37, max mem: 15.9 GB 
[10/24 12:12:12 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6185 s / batch. (data: 2.29e-04). ETA=17:36:12, max mem: 15.9 GB 
[10/24 12:13:15 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6160 s / batch. (data: 3.84e-04). ETA=17:30:49, max mem: 15.9 GB 
[10/24 12:14:17 visual_prompt]: 	Training 600/1106. train loss: 37.4180,	0.6449 s / batch. (data: 1.78e-02). ETA=18:19:09, max mem: 15.9 GB 
[10/24 12:15:20 visual_prompt]: 	Training 700/1106. train loss: 26.8237,	0.6181 s / batch. (data: 2.80e-04). ETA=17:32:23, max mem: 15.9 GB 
[10/24 12:16:23 visual_prompt]: 	Training 800/1106. train loss: 3.6422,	0.6380 s / batch. (data: 7.50e-04). ETA=18:05:14, max mem: 15.9 GB 
[10/24 12:17:26 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6353 s / batch. (data: 1.01e-03). ETA=17:59:37, max mem: 15.9 GB 
[10/24 12:18:29 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6309 s / batch. (data: 7.87e-04). ETA=17:50:57, max mem: 15.9 GB 
[10/24 12:19:32 visual_prompt]: 	Training 1100/1106. train loss: 112.6568,	0.6164 s / batch. (data: 1.54e-04). ETA=17:25:27, max mem: 15.9 GB 
[10/24 12:19:35 visual_prompt]: Epoch 8 / 100: avg data time: 4.49e-03, avg batch time: 0.6296, average train loss: 149.5069
[10/24 12:20:26 visual_prompt]: 	Test 100/123. loss: 9.368, 0.2435 s / batch. (data: 4.12e-05)max mem: 15.90529 GB 
[10/24 12:20:36 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2327, average loss: 7.9082
[10/24 12:20:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 54.30	
[10/24 12:20:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/24 12:21:41 visual_prompt]: 	Training 100/1106. train loss: 534.1885,	0.6538 s / batch. (data: 9.06e-04). ETA=18:27:44, max mem: 15.9 GB 
[10/24 12:22:44 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6134 s / batch. (data: 3.13e-04). ETA=17:18:15, max mem: 15.9 GB 
[10/24 12:23:47 visual_prompt]: 	Training 300/1106. train loss: 275.9615,	0.6314 s / batch. (data: 3.91e-04). ETA=17:47:33, max mem: 15.9 GB 
[10/24 12:24:50 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6078 s / batch. (data: 3.30e-04). ETA=17:06:40, max mem: 15.9 GB 
[10/24 12:25:53 visual_prompt]: 	Training 500/1106. train loss: 594.6730,	0.6404 s / batch. (data: 3.13e-04). ETA=18:00:46, max mem: 15.9 GB 
[10/24 12:26:56 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6214 s / batch. (data: 3.55e-04). ETA=17:27:40, max mem: 15.9 GB 
[10/24 12:27:58 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6357 s / batch. (data: 8.10e-04). ETA=17:50:37, max mem: 15.9 GB 
[10/24 12:29:01 visual_prompt]: 	Training 800/1106. train loss: 103.9929,	0.6323 s / batch. (data: 7.68e-04). ETA=17:43:53, max mem: 15.9 GB 
[10/24 12:30:04 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6313 s / batch. (data: 1.14e-03). ETA=17:41:11, max mem: 15.9 GB 
[10/24 12:31:07 visual_prompt]: 	Training 1000/1106. train loss: 421.2297,	0.6181 s / batch. (data: 3.13e-04). ETA=17:17:58, max mem: 15.9 GB 
[10/24 12:32:10 visual_prompt]: 	Training 1100/1106. train loss: 52.8612,	0.6128 s / batch. (data: 1.61e-04). ETA=17:07:55, max mem: 15.9 GB 
[10/24 12:32:13 visual_prompt]: Epoch 9 / 100: avg data time: 4.92e-03, avg batch time: 0.6303, average train loss: 138.0690
[10/24 12:33:03 visual_prompt]: 	Test 100/123. loss: 35.817, 0.2384 s / batch. (data: 5.39e-05)max mem: 15.90529 GB 
[10/24 12:33:14 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.2320, average loss: 45.1639
[10/24 12:33:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.12	
[10/24 12:33:14 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/24 12:34:19 visual_prompt]: 	Training 100/1106. train loss: 118.3580,	0.6400 s / batch. (data: 2.69e-02). ETA=17:52:26, max mem: 15.9 GB 
[10/24 12:35:22 visual_prompt]: 	Training 200/1106. train loss: 21.6111,	0.6184 s / batch. (data: 3.23e-04). ETA=17:15:17, max mem: 15.9 GB 
[10/24 12:36:25 visual_prompt]: 	Training 300/1106. train loss: 156.2450,	0.6180 s / batch. (data: 3.28e-04). ETA=17:13:28, max mem: 15.9 GB 
[10/24 12:37:27 visual_prompt]: 	Training 400/1106. train loss: 5.7912,	0.6329 s / batch. (data: 8.14e-04). ETA=17:37:27, max mem: 15.9 GB 
[10/24 12:38:30 visual_prompt]: 	Training 500/1106. train loss: 262.5918,	0.6259 s / batch. (data: 8.08e-04). ETA=17:24:37, max mem: 15.9 GB 
[10/24 12:39:33 visual_prompt]: 	Training 600/1106. train loss: 734.5026,	0.6355 s / batch. (data: 1.20e-02). ETA=17:39:40, max mem: 15.9 GB 
[10/24 12:40:36 visual_prompt]: 	Training 700/1106. train loss: 413.6193,	0.6308 s / batch. (data: 3.40e-04). ETA=17:30:47, max mem: 15.9 GB 
[10/24 12:41:38 visual_prompt]: 	Training 800/1106. train loss: 150.2146,	0.6246 s / batch. (data: 3.31e-04). ETA=17:19:26, max mem: 15.9 GB 
[10/24 12:42:41 visual_prompt]: 	Training 900/1106. train loss: 149.4984,	0.6453 s / batch. (data: 1.66e-02). ETA=17:52:43, max mem: 15.9 GB 
[10/24 12:43:44 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6325 s / batch. (data: 1.19e-03). ETA=17:30:21, max mem: 15.9 GB 
[10/24 12:44:47 visual_prompt]: 	Training 1100/1106. train loss: 242.1559,	0.6186 s / batch. (data: 1.55e-04). ETA=17:06:15, max mem: 15.9 GB 
[10/24 12:44:51 visual_prompt]: Epoch 10 / 100: avg data time: 4.08e-03, avg batch time: 0.6295, average train loss: 207.3335
[10/24 12:45:41 visual_prompt]: 	Test 100/123. loss: 207.565, 0.2526 s / batch. (data: 4.36e-05)max mem: 15.90529 GB 
[10/24 12:45:52 visual_prompt]: Inference (val):avg data time: 3.35e-04, avg batch time: 0.2320, average loss: 185.6399
[10/24 12:45:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.36	
[10/24 12:45:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/24 12:46:57 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6422 s / batch. (data: 1.82e-02). ETA=17:44:23, max mem: 15.9 GB 
[10/24 12:48:00 visual_prompt]: 	Training 200/1106. train loss: 95.1102,	0.6368 s / batch. (data: 8.61e-04). ETA=17:34:15, max mem: 15.9 GB 
[10/24 12:49:03 visual_prompt]: 	Training 300/1106. train loss: 238.3183,	0.6266 s / batch. (data: 7.72e-04). ETA=17:16:24, max mem: 15.9 GB 
[10/24 12:50:06 visual_prompt]: 	Training 400/1106. train loss: 380.7223,	0.6293 s / batch. (data: 7.90e-04). ETA=17:19:45, max mem: 15.9 GB 
[10/24 12:51:08 visual_prompt]: 	Training 500/1106. train loss: 7.4665,	0.6250 s / batch. (data: 1.98e-04). ETA=17:11:41, max mem: 15.9 GB 
[10/24 12:52:11 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6210 s / batch. (data: 8.24e-04). ETA=17:04:01, max mem: 15.9 GB 
[10/24 12:53:14 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6107 s / batch. (data: 2.70e-04). ETA=16:46:05, max mem: 15.9 GB 
[10/24 12:54:17 visual_prompt]: 	Training 800/1106. train loss: 288.0156,	0.6184 s / batch. (data: 2.93e-04). ETA=16:57:39, max mem: 15.9 GB 
[10/24 12:55:20 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6289 s / batch. (data: 3.28e-04). ETA=17:13:54, max mem: 15.9 GB 
[10/24 12:56:23 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6354 s / batch. (data: 7.99e-04). ETA=17:23:33, max mem: 15.9 GB 
[10/24 12:57:25 visual_prompt]: 	Training 1100/1106. train loss: 87.1272,	0.6176 s / batch. (data: 2.42e-04). ETA=16:53:17, max mem: 15.9 GB 
[10/24 12:57:29 visual_prompt]: Epoch 11 / 100: avg data time: 4.62e-03, avg batch time: 0.6302, average train loss: 186.0429
[10/24 12:58:20 visual_prompt]: 	Test 100/123. loss: 25.772, 0.2517 s / batch. (data: 4.74e-05)max mem: 15.90529 GB 
[10/24 12:58:31 visual_prompt]: Inference (val):avg data time: 1.31e-04, avg batch time: 0.2310, average loss: 32.3011
[10/24 12:58:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.63	
[10/24 12:58:31 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/24 12:59:37 visual_prompt]: 	Training 100/1106. train loss: 373.7473,	0.6170 s / batch. (data: 3.30e-04). ETA=16:51:09, max mem: 15.9 GB 
[10/24 13:00:40 visual_prompt]: 	Training 200/1106. train loss: 46.2624,	0.6132 s / batch. (data: 3.22e-04). ETA=16:43:53, max mem: 15.9 GB 
[10/24 13:01:42 visual_prompt]: 	Training 300/1106. train loss: 140.1855,	0.6300 s / batch. (data: 3.18e-04). ETA=17:10:20, max mem: 15.9 GB 
[10/24 13:02:45 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6269 s / batch. (data: 4.25e-04). ETA=17:04:15, max mem: 15.9 GB 
[10/24 13:03:48 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6624 s / batch. (data: 4.05e-02). ETA=18:01:13, max mem: 15.9 GB 
[10/24 13:04:51 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6346 s / batch. (data: 7.15e-04). ETA=17:14:50, max mem: 15.9 GB 
[10/24 13:05:54 visual_prompt]: 	Training 700/1106. train loss: 418.5621,	0.6282 s / batch. (data: 3.58e-04). ETA=17:03:16, max mem: 15.9 GB 
[10/24 13:06:56 visual_prompt]: 	Training 800/1106. train loss: 405.6432,	0.6373 s / batch. (data: 7.31e-04). ETA=17:17:04, max mem: 15.9 GB 
[10/24 13:07:59 visual_prompt]: 	Training 900/1106. train loss: 206.5704,	0.6323 s / batch. (data: 8.11e-04). ETA=17:07:51, max mem: 15.9 GB 
[10/24 13:09:02 visual_prompt]: 	Training 1000/1106. train loss: 396.8335,	0.6259 s / batch. (data: 7.84e-04). ETA=16:56:27, max mem: 15.9 GB 
[10/24 13:10:05 visual_prompt]: 	Training 1100/1106. train loss: 508.1508,	0.6121 s / batch. (data: 1.49e-04). ETA=16:32:57, max mem: 15.9 GB 
[10/24 13:10:09 visual_prompt]: Epoch 12 / 100: avg data time: 5.16e-03, avg batch time: 0.6307, average train loss: 189.6536
[10/24 13:10:59 visual_prompt]: 	Test 100/123. loss: 136.455, 0.2355 s / batch. (data: 3.10e-05)max mem: 15.90529 GB 
[10/24 13:11:10 visual_prompt]: Inference (val):avg data time: 3.83e-04, avg batch time: 0.2327, average loss: 154.7266
[10/24 13:11:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.27	
[10/24 13:11:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/24 13:12:15 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6062 s / batch. (data: 3.44e-04). ETA=16:22:19, max mem: 15.9 GB 
[10/24 13:13:18 visual_prompt]: 	Training 200/1106. train loss: 93.7713,	0.6200 s / batch. (data: 2.95e-04). ETA=16:43:38, max mem: 15.9 GB 
[10/24 13:14:21 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6068 s / batch. (data: 3.25e-04). ETA=16:21:19, max mem: 15.9 GB 
[10/24 13:15:24 visual_prompt]: 	Training 400/1106. train loss: 419.5318,	0.6167 s / batch. (data: 3.42e-04). ETA=16:36:14, max mem: 15.9 GB 
[10/24 13:16:26 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6071 s / batch. (data: 2.80e-04). ETA=16:19:40, max mem: 15.9 GB 
[10/24 13:17:29 visual_prompt]: 	Training 600/1106. train loss: 723.1934,	0.6294 s / batch. (data: 1.19e-02). ETA=16:54:39, max mem: 15.9 GB 
[10/24 13:18:32 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6337 s / batch. (data: 7.98e-04). ETA=17:00:34, max mem: 15.9 GB 
[10/24 13:19:35 visual_prompt]: 	Training 800/1106. train loss: 559.3708,	0.6276 s / batch. (data: 7.91e-04). ETA=16:49:41, max mem: 15.9 GB 
[10/24 13:20:37 visual_prompt]: 	Training 900/1106. train loss: 284.0591,	0.6234 s / batch. (data: 3.05e-04). ETA=16:41:49, max mem: 15.9 GB 
[10/24 13:21:41 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6220 s / batch. (data: 3.10e-04). ETA=16:38:38, max mem: 15.9 GB 
[10/24 13:22:43 visual_prompt]: 	Training 1100/1106. train loss: 45.0481,	0.6123 s / batch. (data: 1.75e-04). ETA=16:22:00, max mem: 15.9 GB 
[10/24 13:22:47 visual_prompt]: Epoch 13 / 100: avg data time: 4.84e-03, avg batch time: 0.6303, average train loss: 216.8842
[10/24 13:23:38 visual_prompt]: 	Test 100/123. loss: 134.730, 0.2249 s / batch. (data: 3.12e-05)max mem: 15.90529 GB 
[10/24 13:23:48 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2330, average loss: 119.6923
[10/24 13:23:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.50	
[10/24 13:23:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/24 13:24:54 visual_prompt]: 	Training 100/1106. train loss: 267.8510,	0.6181 s / batch. (data: 3.49e-04). ETA=16:30:15, max mem: 15.9 GB 
[10/24 13:25:56 visual_prompt]: 	Training 200/1106. train loss: 349.1639,	0.6126 s / batch. (data: 3.50e-04). ETA=16:20:23, max mem: 15.9 GB 
[10/24 13:26:59 visual_prompt]: 	Training 300/1106. train loss: 274.6419,	0.6126 s / batch. (data: 3.15e-04). ETA=16:19:21, max mem: 15.9 GB 
[10/24 13:28:02 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6268 s / batch. (data: 3.31e-04). ETA=16:40:57, max mem: 15.9 GB 
[10/24 13:29:05 visual_prompt]: 	Training 500/1106. train loss: 142.8098,	0.6275 s / batch. (data: 2.78e-04). ETA=16:41:01, max mem: 15.9 GB 
[10/24 13:30:08 visual_prompt]: 	Training 600/1106. train loss: 41.3904,	0.6267 s / batch. (data: 8.05e-04). ETA=16:38:47, max mem: 15.9 GB 
[10/24 13:31:10 visual_prompt]: 	Training 700/1106. train loss: 460.6404,	0.6178 s / batch. (data: 7.63e-04). ETA=16:23:36, max mem: 15.9 GB 
[10/24 13:32:13 visual_prompt]: 	Training 800/1106. train loss: 159.2545,	0.6260 s / batch. (data: 7.61e-04). ETA=16:35:36, max mem: 15.9 GB 
[10/24 13:33:16 visual_prompt]: 	Training 900/1106. train loss: 131.0771,	0.6130 s / batch. (data: 3.15e-04). ETA=16:13:51, max mem: 15.9 GB 
[10/24 13:34:18 visual_prompt]: 	Training 1000/1106. train loss: 126.8285,	0.6545 s / batch. (data: 1.05e-02). ETA=17:18:44, max mem: 15.9 GB 
[10/24 13:35:21 visual_prompt]: 	Training 1100/1106. train loss: 108.3563,	0.6123 s / batch. (data: 2.11e-04). ETA=16:10:44, max mem: 15.9 GB 
[10/24 13:35:25 visual_prompt]: Epoch 14 / 100: avg data time: 4.60e-03, avg batch time: 0.6297, average train loss: 203.6600
[10/24 13:36:15 visual_prompt]: 	Test 100/123. loss: 178.731, 0.2246 s / batch. (data: 4.91e-05)max mem: 15.90529 GB 
[10/24 13:36:26 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2315, average loss: 156.0849
[10/24 13:36:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.27	
[10/24 13:36:26 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/24 13:37:30 visual_prompt]: 	Training 100/1106. train loss: 370.8721,	0.6374 s / batch. (data: 7.77e-04). ETA=16:49:24, max mem: 15.9 GB 
[10/24 13:38:33 visual_prompt]: 	Training 200/1106. train loss: 133.4857,	0.6381 s / batch. (data: 7.68e-04). ETA=16:49:25, max mem: 15.9 GB 
[10/24 13:39:35 visual_prompt]: 	Training 300/1106. train loss: 142.7001,	0.6220 s / batch. (data: 5.93e-03). ETA=16:22:59, max mem: 15.9 GB 
[10/24 13:40:38 visual_prompt]: 	Training 400/1106. train loss: 1423.8342,	0.6391 s / batch. (data: 8.03e-04). ETA=16:48:49, max mem: 15.9 GB 
[10/24 13:41:41 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6255 s / batch. (data: 8.56e-04). ETA=16:26:17, max mem: 15.9 GB 
[10/24 13:42:44 visual_prompt]: 	Training 600/1106. train loss: 338.2378,	0.6522 s / batch. (data: 3.74e-02). ETA=17:07:27, max mem: 15.9 GB 
[10/24 13:43:47 visual_prompt]: 	Training 700/1106. train loss: 171.3831,	0.6450 s / batch. (data: 8.06e-04). ETA=16:54:59, max mem: 15.9 GB 
[10/24 13:44:49 visual_prompt]: 	Training 800/1106. train loss: 12.1590,	0.6176 s / batch. (data: 3.13e-04). ETA=16:10:48, max mem: 15.9 GB 
[10/24 13:45:52 visual_prompt]: 	Training 900/1106. train loss: 461.5442,	0.6176 s / batch. (data: 7.69e-04). ETA=16:09:48, max mem: 15.9 GB 
[10/24 13:46:55 visual_prompt]: 	Training 1000/1106. train loss: 168.6863,	0.6262 s / batch. (data: 2.85e-04). ETA=16:22:13, max mem: 15.9 GB 
[10/24 13:47:58 visual_prompt]: 	Training 1100/1106. train loss: 113.0247,	0.6120 s / batch. (data: 1.54e-04). ETA=15:58:57, max mem: 15.9 GB 
[10/24 13:48:02 visual_prompt]: Epoch 15 / 100: avg data time: 4.10e-03, avg batch time: 0.6296, average train loss: 214.5515
[10/24 13:48:52 visual_prompt]: 	Test 100/123. loss: 6.901, 0.2247 s / batch. (data: 4.67e-05)max mem: 15.90529 GB 
[10/24 13:49:03 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2333, average loss: 10.4807
[10/24 13:49:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 56.27	
[10/24 13:49:03 visual_prompt]: Best epoch 15: best metric: -10.481
[10/24 13:49:03 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/24 13:50:08 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6293 s / batch. (data: 7.40e-04). ETA=16:24:55, max mem: 15.9 GB 
[10/24 13:51:11 visual_prompt]: 	Training 200/1106. train loss: 335.9509,	0.6278 s / batch. (data: 3.21e-04). ETA=16:21:36, max mem: 15.9 GB 
[10/24 13:52:13 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6081 s / batch. (data: 3.27e-04). ETA=15:49:48, max mem: 15.9 GB 
[10/24 13:53:16 visual_prompt]: 	Training 400/1106. train loss: 409.9231,	0.6320 s / batch. (data: 3.30e-04). ETA=16:26:04, max mem: 15.9 GB 
[10/24 13:54:19 visual_prompt]: 	Training 500/1106. train loss: 169.9318,	0.6203 s / batch. (data: 3.52e-04). ETA=16:06:44, max mem: 15.9 GB 
[10/24 13:55:22 visual_prompt]: 	Training 600/1106. train loss: 339.9310,	0.6312 s / batch. (data: 7.84e-04). ETA=16:22:39, max mem: 15.9 GB 
[10/24 13:56:25 visual_prompt]: 	Training 700/1106. train loss: 246.7124,	0.6151 s / batch. (data: 3.01e-04). ETA=15:56:34, max mem: 15.9 GB 
[10/24 13:57:28 visual_prompt]: 	Training 800/1106. train loss: 329.5035,	0.6528 s / batch. (data: 5.92e-03). ETA=16:54:07, max mem: 15.9 GB 
[10/24 13:58:30 visual_prompt]: 	Training 900/1106. train loss: 65.4670,	0.6122 s / batch. (data: 7.60e-04). ETA=15:50:06, max mem: 15.9 GB 
[10/24 13:59:33 visual_prompt]: 	Training 1000/1106. train loss: 214.1169,	0.6256 s / batch. (data: 2.93e-04). ETA=16:09:48, max mem: 15.9 GB 
[10/24 14:00:36 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6067 s / batch. (data: 1.54e-04). ETA=15:39:30, max mem: 15.9 GB 
[10/24 14:00:40 visual_prompt]: Epoch 16 / 100: avg data time: 4.49e-03, avg batch time: 0.6302, average train loss: 149.2674
[10/24 14:01:30 visual_prompt]: 	Test 100/123. loss: 97.855, 0.2301 s / batch. (data: 2.91e-05)max mem: 15.90529 GB 
[10/24 14:01:41 visual_prompt]: Inference (val):avg data time: 9.70e-05, avg batch time: 0.2315, average loss: 86.3813
[10/24 14:01:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.36	
[10/24 14:01:41 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/24 14:02:46 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6194 s / batch. (data: 7.73e-04). ETA=15:58:06, max mem: 15.9 GB 
[10/24 14:03:49 visual_prompt]: 	Training 200/1106. train loss: 98.6884,	0.6123 s / batch. (data: 3.34e-04). ETA=15:46:00, max mem: 15.9 GB 
[10/24 14:04:52 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6363 s / batch. (data: 7.76e-04). ETA=16:22:06, max mem: 15.9 GB 
[10/24 14:05:55 visual_prompt]: 	Training 400/1106. train loss: 1251.2004,	0.6393 s / batch. (data: 7.40e-04). ETA=16:25:38, max mem: 15.9 GB 
[10/24 14:06:58 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6220 s / batch. (data: 7.59e-04). ETA=15:57:58, max mem: 15.9 GB 
[10/24 14:08:01 visual_prompt]: 	Training 600/1106. train loss: 196.2970,	0.6372 s / batch. (data: 5.99e-03). ETA=16:20:20, max mem: 15.9 GB 
[10/24 14:09:03 visual_prompt]: 	Training 700/1106. train loss: 82.7629,	0.6238 s / batch. (data: 2.60e-04). ETA=15:58:38, max mem: 15.9 GB 
[10/24 14:10:06 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6470 s / batch. (data: 7.96e-04). ETA=16:33:13, max mem: 15.9 GB 
[10/24 14:11:09 visual_prompt]: 	Training 900/1106. train loss: 492.7748,	0.6320 s / batch. (data: 2.76e-04). ETA=16:09:09, max mem: 15.9 GB 
[10/24 14:12:12 visual_prompt]: 	Training 1000/1106. train loss: 244.7960,	0.6462 s / batch. (data: 1.46e-02). ETA=16:29:48, max mem: 15.9 GB 
[10/24 14:13:15 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6073 s / batch. (data: 1.51e-04). ETA=15:29:09, max mem: 15.9 GB 
[10/24 14:13:19 visual_prompt]: Epoch 17 / 100: avg data time: 4.72e-03, avg batch time: 0.6311, average train loss: 152.1178
[10/24 14:14:09 visual_prompt]: 	Test 100/123. loss: 42.446, 0.2473 s / batch. (data: 4.36e-05)max mem: 15.90529 GB 
[10/24 14:14:20 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2328, average loss: 40.1196
[10/24 14:14:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.38	
[10/24 14:14:20 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/24 14:15:24 visual_prompt]: 	Training 100/1106. train loss: 215.1428,	0.6405 s / batch. (data: 8.41e-04). ETA=16:18:48, max mem: 15.9 GB 
[10/24 14:16:27 visual_prompt]: 	Training 200/1106. train loss: 451.4674,	0.6185 s / batch. (data: 3.25e-04). ETA=15:44:13, max mem: 15.9 GB 
[10/24 14:17:30 visual_prompt]: 	Training 300/1106. train loss: 215.8709,	0.6320 s / batch. (data: 2.78e-04). ETA=16:03:48, max mem: 15.9 GB 
[10/24 14:18:32 visual_prompt]: 	Training 400/1106. train loss: 254.3704,	0.6242 s / batch. (data: 5.54e-03). ETA=15:50:52, max mem: 15.9 GB 
[10/24 14:19:35 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6081 s / batch. (data: 7.18e-04). ETA=15:25:15, max mem: 15.9 GB 
[10/24 14:20:38 visual_prompt]: 	Training 600/1106. train loss: 210.3066,	0.6362 s / batch. (data: 9.28e-04). ETA=16:06:58, max mem: 15.9 GB 
[10/24 14:21:41 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6205 s / batch. (data: 3.20e-04). ETA=15:42:02, max mem: 15.9 GB 
[10/24 14:22:44 visual_prompt]: 	Training 800/1106. train loss: 42.1388,	0.6293 s / batch. (data: 8.82e-04). ETA=15:54:28, max mem: 15.9 GB 
[10/24 14:23:47 visual_prompt]: 	Training 900/1106. train loss: 345.2985,	0.6148 s / batch. (data: 3.37e-04). ETA=15:31:20, max mem: 15.9 GB 
[10/24 14:24:49 visual_prompt]: 	Training 1000/1106. train loss: 27.0605,	0.6174 s / batch. (data: 2.92e-04). ETA=15:34:14, max mem: 15.9 GB 
[10/24 14:25:52 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6057 s / batch. (data: 1.60e-04). ETA=15:15:34, max mem: 15.9 GB 
[10/24 14:25:56 visual_prompt]: Epoch 18 / 100: avg data time: 4.38e-03, avg batch time: 0.6295, average train loss: 150.4878
[10/24 14:26:46 visual_prompt]: 	Test 100/123. loss: 149.712, 0.2249 s / batch. (data: 3.10e-05)max mem: 15.90529 GB 
[10/24 14:26:57 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2320, average loss: 127.0792
[10/24 14:26:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.77	
[10/24 14:26:57 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[10/24 14:28:02 visual_prompt]: 	Training 100/1106. train loss: 21.6577,	0.6172 s / batch. (data: 7.14e-04). ETA=15:31:54, max mem: 15.9 GB 
[10/24 14:29:05 visual_prompt]: 	Training 200/1106. train loss: 198.8299,	0.6248 s / batch. (data: 4.51e-04). ETA=15:42:23, max mem: 15.9 GB 
[10/24 14:30:08 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6154 s / batch. (data: 2.99e-04). ETA=15:27:06, max mem: 15.9 GB 
[10/24 14:31:10 visual_prompt]: 	Training 400/1106. train loss: 9.6503,	0.6358 s / batch. (data: 5.93e-03). ETA=15:56:52, max mem: 15.9 GB 
[10/24 14:32:13 visual_prompt]: 	Training 500/1106. train loss: 15.4476,	0.6219 s / batch. (data: 1.05e-02). ETA=15:34:46, max mem: 15.9 GB 
[10/24 14:33:16 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6241 s / batch. (data: 1.10e-03). ETA=15:37:02, max mem: 15.9 GB 
[10/24 14:34:19 visual_prompt]: 	Training 700/1106. train loss: 148.6887,	0.6267 s / batch. (data: 4.46e-04). ETA=15:39:59, max mem: 15.9 GB 
[10/24 14:35:22 visual_prompt]: 	Training 800/1106. train loss: 146.2982,	0.6301 s / batch. (data: 4.18e-04). ETA=15:43:58, max mem: 15.9 GB 
[10/24 14:36:25 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6136 s / batch. (data: 3.40e-04). ETA=15:18:16, max mem: 15.9 GB 
[10/24 14:37:27 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6186 s / batch. (data: 8.40e-04). ETA=15:24:44, max mem: 15.9 GB 
[10/24 14:38:30 visual_prompt]: 	Training 1100/1106. train loss: 503.0664,	0.6338 s / batch. (data: 1.72e-04). ETA=15:46:25, max mem: 15.9 GB 
[10/24 14:38:34 visual_prompt]: Epoch 19 / 100: avg data time: 4.50e-03, avg batch time: 0.6303, average train loss: 134.0912
[10/24 14:39:24 visual_prompt]: 	Test 100/123. loss: 711.233, 0.2318 s / batch. (data: 3.08e-05)max mem: 15.90529 GB 
[10/24 14:39:34 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2321, average loss: 636.2055
[10/24 14:39:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.58	
[10/24 14:39:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[10/24 14:40:40 visual_prompt]: 	Training 100/1106. train loss: 33.5879,	0.6276 s / batch. (data: 3.13e-04). ETA=15:35:57, max mem: 15.9 GB 
[10/24 14:41:43 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6129 s / batch. (data: 9.11e-04). ETA=15:13:06, max mem: 15.9 GB 
[10/24 14:42:45 visual_prompt]: 	Training 300/1106. train loss: 67.9599,	0.6413 s / batch. (data: 7.45e-04). ETA=15:54:19, max mem: 15.9 GB 
[10/24 14:43:48 visual_prompt]: 	Training 400/1106. train loss: 222.0287,	0.6232 s / batch. (data: 2.72e-04). ETA=15:26:19, max mem: 15.9 GB 
[10/24 14:44:51 visual_prompt]: 	Training 500/1106. train loss: 279.9847,	0.6331 s / batch. (data: 8.15e-04). ETA=15:40:01, max mem: 15.9 GB 
[10/24 14:45:54 visual_prompt]: 	Training 600/1106. train loss: 308.9604,	0.6406 s / batch. (data: 1.08e-03). ETA=15:50:01, max mem: 15.9 GB 
[10/24 14:46:57 visual_prompt]: 	Training 700/1106. train loss: 160.1700,	0.6304 s / batch. (data: 7.94e-04). ETA=15:33:51, max mem: 15.9 GB 
[10/24 14:48:00 visual_prompt]: 	Training 800/1106. train loss: 26.0823,	0.6312 s / batch. (data: 5.45e-04). ETA=15:34:00, max mem: 15.9 GB 
[10/24 14:49:03 visual_prompt]: 	Training 900/1106. train loss: 527.2388,	0.6272 s / batch. (data: 2.90e-04). ETA=15:27:00, max mem: 15.9 GB 
[10/24 14:50:05 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6218 s / batch. (data: 3.13e-04). ETA=15:18:00, max mem: 15.9 GB 
[10/24 14:51:08 visual_prompt]: 	Training 1100/1106. train loss: 68.6914,	0.6183 s / batch. (data: 2.15e-04). ETA=15:11:47, max mem: 15.9 GB 
[10/24 14:51:12 visual_prompt]: Epoch 20 / 100: avg data time: 4.76e-03, avg batch time: 0.6308, average train loss: 118.8811
[10/24 14:52:02 visual_prompt]: 	Test 100/123. loss: 73.575, 0.2406 s / batch. (data: 4.27e-05)max mem: 15.90529 GB 
[10/24 14:52:13 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.2333, average loss: 58.8075
[10/24 14:52:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.62	
[10/24 14:52:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[10/24 14:53:18 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6388 s / batch. (data: 1.05e-02). ETA=15:40:54, max mem: 15.9 GB 
[10/24 14:54:21 visual_prompt]: 	Training 200/1106. train loss: 273.2705,	0.6395 s / batch. (data: 3.06e-04). ETA=15:40:52, max mem: 15.9 GB 
[10/24 14:55:24 visual_prompt]: 	Training 300/1106. train loss: 82.9776,	0.6203 s / batch. (data: 3.14e-04). ETA=15:11:34, max mem: 15.9 GB 
[10/24 14:56:27 visual_prompt]: 	Training 400/1106. train loss: 192.9847,	0.6141 s / batch. (data: 2.95e-04). ETA=15:01:26, max mem: 15.9 GB 
[10/24 14:57:30 visual_prompt]: 	Training 500/1106. train loss: 768.8600,	0.6322 s / batch. (data: 3.12e-04). ETA=15:26:57, max mem: 15.9 GB 
[10/24 14:58:33 visual_prompt]: 	Training 600/1106. train loss: 200.4777,	0.6311 s / batch. (data: 8.08e-04). ETA=15:24:25, max mem: 15.9 GB 
[10/24 14:59:36 visual_prompt]: 	Training 700/1106. train loss: 525.6021,	0.6141 s / batch. (data: 3.35e-04). ETA=14:58:29, max mem: 15.9 GB 
[10/24 15:00:39 visual_prompt]: 	Training 800/1106. train loss: 26.0290,	0.6267 s / batch. (data: 5.40e-03). ETA=15:15:52, max mem: 15.9 GB 
[10/24 15:01:42 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6363 s / batch. (data: 7.45e-04). ETA=15:28:44, max mem: 15.9 GB 
[10/24 15:02:45 visual_prompt]: 	Training 1000/1106. train loss: 98.0629,	0.6217 s / batch. (data: 3.20e-04). ETA=15:06:24, max mem: 15.9 GB 
[10/24 15:03:48 visual_prompt]: 	Training 1100/1106. train loss: 395.8929,	0.6186 s / batch. (data: 1.70e-04). ETA=15:00:54, max mem: 15.9 GB 
[10/24 15:03:51 visual_prompt]: Epoch 21 / 100: avg data time: 4.88e-03, avg batch time: 0.6312, average train loss: 138.9669
[10/24 15:04:41 visual_prompt]: 	Test 100/123. loss: 52.758, 0.2252 s / batch. (data: 3.08e-05)max mem: 15.90529 GB 
[10/24 15:04:52 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2326, average loss: 37.5159
[10/24 15:04:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 54.53	
[10/24 15:04:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[10/24 15:05:57 visual_prompt]: 	Training 100/1106. train loss: 201.9927,	0.6303 s / batch. (data: 3.22e-04). ETA=15:16:45, max mem: 15.9 GB 
[10/24 15:07:00 visual_prompt]: 	Training 200/1106. train loss: 215.0100,	0.6330 s / batch. (data: 1.57e-02). ETA=15:19:45, max mem: 15.9 GB 
[10/24 15:08:02 visual_prompt]: 	Training 300/1106. train loss: 263.0182,	0.6316 s / batch. (data: 8.00e-04). ETA=15:16:33, max mem: 15.9 GB 
[10/24 15:09:05 visual_prompt]: 	Training 400/1106. train loss: 42.6809,	0.6330 s / batch. (data: 7.53e-04). ETA=15:17:34, max mem: 15.9 GB 
[10/24 15:10:08 visual_prompt]: 	Training 500/1106. train loss: 51.1411,	0.6180 s / batch. (data: 2.81e-04). ETA=14:54:48, max mem: 15.9 GB 
[10/24 15:11:11 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6281 s / batch. (data: 7.69e-04). ETA=15:08:24, max mem: 15.9 GB 
[10/24 15:12:13 visual_prompt]: 	Training 700/1106. train loss: 530.6969,	0.6312 s / batch. (data: 3.09e-04). ETA=15:11:49, max mem: 15.9 GB 
[10/24 15:13:16 visual_prompt]: 	Training 800/1106. train loss: 634.6000,	0.6244 s / batch. (data: 3.28e-04). ETA=15:01:00, max mem: 15.9 GB 
[10/24 15:14:19 visual_prompt]: 	Training 900/1106. train loss: 87.9417,	0.6166 s / batch. (data: 2.70e-04). ETA=14:48:41, max mem: 15.9 GB 
[10/24 15:15:22 visual_prompt]: 	Training 1000/1106. train loss: 8.1747,	0.6617 s / batch. (data: 3.20e-02). ETA=15:52:36, max mem: 15.9 GB 
[10/24 15:16:25 visual_prompt]: 	Training 1100/1106. train loss: 132.4765,	0.6170 s / batch. (data: 1.39e-04). ETA=14:47:10, max mem: 15.9 GB 
[10/24 15:16:29 visual_prompt]: Epoch 22 / 100: avg data time: 4.40e-03, avg batch time: 0.6298, average train loss: 137.9557
[10/24 15:17:18 visual_prompt]: 	Test 100/123. loss: 114.293, 0.2398 s / batch. (data: 3.03e-05)max mem: 15.90529 GB 
[10/24 15:17:29 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2343, average loss: 103.6007
[10/24 15:17:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.14	
[10/24 15:17:29 visual_prompt]: Stopping early.
[10/24 15:17:30 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 15:17:30 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 15:17:30 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 15:17:30 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 15:17:30 visual_prompt]: Training with config:
[10/24 15:17:30 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr25.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 15:17:30 visual_prompt]: Loading training data...
[10/24 15:17:30 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 15:17:30 visual_prompt]: Loading validation data...
[10/24 15:17:30 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 15:17:30 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/24 15:17:32 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/24 15:17:32 visual_prompt]: tuned percent:0.522
[10/24 15:17:32 visual_prompt]: Device used for model: 0
[10/24 15:17:32 visual_prompt]: Setting up Evaluator...
[10/24 15:17:32 visual_prompt]: Setting up Trainer...
[10/24 15:17:32 visual_prompt]: 	Setting up the optimizer...
[10/24 15:17:33 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 15:18:38 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6309 s / batch. (data: 7.77e-04). ETA=19:21:51, max mem: 15.9 GB 
[10/24 15:19:42 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6296 s / batch. (data: 3.17e-04). ETA=19:18:32, max mem: 15.9 GB 
[10/24 15:20:45 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6367 s / batch. (data: 7.71e-04). ETA=19:30:32, max mem: 15.9 GB 
[10/24 15:21:48 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6263 s / batch. (data: 3.22e-04). ETA=19:10:15, max mem: 15.9 GB 
[10/24 15:22:51 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6275 s / batch. (data: 4.62e-04). ETA=19:11:32, max mem: 15.9 GB 
[10/24 15:23:54 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6200 s / batch. (data: 2.97e-04). ETA=18:56:34, max mem: 15.9 GB 
[10/24 15:24:58 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6197 s / batch. (data: 2.84e-04). ETA=18:55:01, max mem: 15.9 GB 
[10/24 15:26:01 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6278 s / batch. (data: 4.77e-03). ETA=19:08:53, max mem: 15.9 GB 
[10/24 15:27:04 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6419 s / batch. (data: 3.09e-04). ETA=19:33:32, max mem: 15.9 GB 
[10/24 15:28:08 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6320 s / batch. (data: 8.22e-04). ETA=19:14:24, max mem: 15.9 GB 
[10/24 15:29:11 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6191 s / batch. (data: 1.66e-04). ETA=18:49:51, max mem: 15.9 GB 
[10/24 15:29:15 visual_prompt]: Epoch 1 / 100: avg data time: 4.70e-03, avg batch time: 0.6349, average train loss: 1.4028
[10/24 15:30:05 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2252 s / batch. (data: 3.50e-05)max mem: 15.91075 GB 
[10/24 15:30:16 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2326, average loss: 1.3505
[10/24 15:30:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/24 15:30:16 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/24 15:31:21 visual_prompt]: 	Training 100/1106. train loss: 1.6239,	0.6378 s / batch. (data: 1.56e-02). ETA=19:22:56, max mem: 15.9 GB 
[10/24 15:32:24 visual_prompt]: 	Training 200/1106. train loss: 2.7725,	0.6374 s / batch. (data: 8.12e-04). ETA=19:21:00, max mem: 15.9 GB 
[10/24 15:33:27 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6380 s / batch. (data: 3.17e-04). ETA=19:21:10, max mem: 15.9 GB 
[10/24 15:34:30 visual_prompt]: 	Training 400/1106. train loss: 0.0001,	0.6184 s / batch. (data: 3.34e-04). ETA=18:44:20, max mem: 15.9 GB 
[10/24 15:35:34 visual_prompt]: 	Training 500/1106. train loss: 0.7111,	0.6392 s / batch. (data: 1.17e-02). ETA=19:21:12, max mem: 15.9 GB 
[10/24 15:36:37 visual_prompt]: 	Training 600/1106. train loss: 8.2298,	0.6312 s / batch. (data: 3.14e-04). ETA=19:05:35, max mem: 15.9 GB 
[10/24 15:37:40 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6226 s / batch. (data: 2.98e-04). ETA=18:48:56, max mem: 15.9 GB 
[10/24 15:38:44 visual_prompt]: 	Training 800/1106. train loss: 13.7652,	0.6649 s / batch. (data: 7.76e-04). ETA=20:04:34, max mem: 15.9 GB 
[10/24 15:39:47 visual_prompt]: 	Training 900/1106. train loss: 3.3456,	0.6351 s / batch. (data: 7.64e-04). ETA=19:09:26, max mem: 15.9 GB 
[10/24 15:40:50 visual_prompt]: 	Training 1000/1106. train loss: 0.0001,	0.6182 s / batch. (data: 3.14e-04). ETA=18:37:50, max mem: 15.9 GB 
[10/24 15:41:53 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6172 s / batch. (data: 1.29e-04). ETA=18:34:57, max mem: 15.9 GB 
[10/24 15:41:57 visual_prompt]: Epoch 2 / 100: avg data time: 3.85e-03, avg batch time: 0.6342, average train loss: 11.4112
[10/24 15:42:47 visual_prompt]: 	Test 100/123. loss: 11.276, 0.2362 s / batch. (data: 3.91e-05)max mem: 15.91075 GB 
[10/24 15:42:58 visual_prompt]: Inference (val):avg data time: 9.27e-05, avg batch time: 0.2317, average loss: 8.3575
[10/24 15:42:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.74	
[10/24 15:42:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/24 15:44:04 visual_prompt]: 	Training 100/1106. train loss: 19.5193,	0.6422 s / batch. (data: 8.25e-04). ETA=19:19:07, max mem: 15.9 GB 
[10/24 15:45:07 visual_prompt]: 	Training 200/1106. train loss: 30.9693,	0.6296 s / batch. (data: 3.24e-04). ETA=18:55:11, max mem: 15.9 GB 
[10/24 15:46:11 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6554 s / batch. (data: 3.46e-03). ETA=19:40:39, max mem: 15.9 GB 
[10/24 15:47:14 visual_prompt]: 	Training 400/1106. train loss: 26.9235,	0.6453 s / batch. (data: 7.59e-04). ETA=19:21:21, max mem: 15.9 GB 
[10/24 15:48:17 visual_prompt]: 	Training 500/1106. train loss: 111.5969,	0.6179 s / batch. (data: 2.99e-04). ETA=18:31:02, max mem: 15.9 GB 
[10/24 15:49:20 visual_prompt]: 	Training 600/1106. train loss: 48.4811,	0.6185 s / batch. (data: 2.80e-04). ETA=18:31:06, max mem: 15.9 GB 
[10/24 15:50:23 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6189 s / batch. (data: 3.41e-04). ETA=18:30:44, max mem: 15.9 GB 
[10/24 15:51:27 visual_prompt]: 	Training 800/1106. train loss: 6.1280,	0.6440 s / batch. (data: 2.77e-04). ETA=19:14:46, max mem: 15.9 GB 
[10/24 15:52:30 visual_prompt]: 	Training 900/1106. train loss: 5.1029,	0.6231 s / batch. (data: 3.37e-04). ETA=18:36:19, max mem: 15.9 GB 
[10/24 15:53:33 visual_prompt]: 	Training 1000/1106. train loss: 46.8465,	0.6285 s / batch. (data: 8.18e-04). ETA=18:44:54, max mem: 15.9 GB 
[10/24 15:54:36 visual_prompt]: 	Training 1100/1106. train loss: 2.0821,	0.6188 s / batch. (data: 1.34e-04). ETA=18:26:34, max mem: 15.9 GB 
[10/24 15:54:40 visual_prompt]: Epoch 3 / 100: avg data time: 5.21e-03, avg batch time: 0.6342, average train loss: 22.8601
[10/24 15:55:30 visual_prompt]: 	Test 100/123. loss: 37.973, 0.2388 s / batch. (data: 3.29e-05)max mem: 15.91075 GB 
[10/24 15:55:40 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.2327, average loss: 41.8588
[10/24 15:55:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.19	
[10/24 15:55:40 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/24 15:56:45 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6315 s / batch. (data: 7.35e-04). ETA=18:48:09, max mem: 15.9 GB 
[10/24 15:57:49 visual_prompt]: 	Training 200/1106. train loss: 58.0342,	0.6480 s / batch. (data: 1.19e-02). ETA=19:16:29, max mem: 15.9 GB 
[10/24 15:58:52 visual_prompt]: 	Training 300/1106. train loss: 5.4254,	0.6192 s / batch. (data: 4.64e-04). ETA=18:24:06, max mem: 15.9 GB 
[10/24 15:59:55 visual_prompt]: 	Training 400/1106. train loss: 89.4394,	0.6411 s / batch. (data: 3.44e-04). ETA=19:02:00, max mem: 15.9 GB 
[10/24 16:00:58 visual_prompt]: 	Training 500/1106. train loss: 12.6764,	0.6320 s / batch. (data: 1.37e-02). ETA=18:44:44, max mem: 15.9 GB 
[10/24 16:02:01 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6160 s / batch. (data: 2.53e-04). ETA=18:15:14, max mem: 15.9 GB 
[10/24 16:03:04 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6340 s / batch. (data: 8.21e-04). ETA=18:46:14, max mem: 15.9 GB 
[10/24 16:04:07 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6196 s / batch. (data: 3.73e-04). ETA=18:19:32, max mem: 15.9 GB 
[10/24 16:05:10 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6477 s / batch. (data: 8.07e-04). ETA=19:08:27, max mem: 15.9 GB 
[10/24 16:06:13 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6335 s / batch. (data: 7.59e-04). ETA=18:42:06, max mem: 15.9 GB 
[10/24 16:07:16 visual_prompt]: 	Training 1100/1106. train loss: 0.8076,	0.6169 s / batch. (data: 1.29e-04). ETA=18:11:39, max mem: 15.9 GB 
[10/24 16:07:20 visual_prompt]: Epoch 4 / 100: avg data time: 4.64e-03, avg batch time: 0.6325, average train loss: 40.0193
[10/24 16:08:10 visual_prompt]: 	Test 100/123. loss: 13.659, 0.2256 s / batch. (data: 3.15e-05)max mem: 15.91075 GB 
[10/24 16:08:21 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2333, average loss: 13.5439
[10/24 16:08:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.25	
[10/24 16:08:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/24 16:09:26 visual_prompt]: 	Training 100/1106. train loss: 30.6430,	0.6253 s / batch. (data: 3.19e-04). ETA=18:25:27, max mem: 15.9 GB 
[10/24 16:10:29 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6364 s / batch. (data: 7.98e-04). ETA=18:44:00, max mem: 15.9 GB 
[10/24 16:11:31 visual_prompt]: 	Training 300/1106. train loss: 40.5308,	0.6193 s / batch. (data: 4.81e-04). ETA=18:12:44, max mem: 15.9 GB 
[10/24 16:12:34 visual_prompt]: 	Training 400/1106. train loss: 32.2056,	0.6320 s / batch. (data: 3.24e-04). ETA=18:34:10, max mem: 15.9 GB 
[10/24 16:13:38 visual_prompt]: 	Training 500/1106. train loss: 135.2270,	0.6314 s / batch. (data: 7.91e-04). ETA=18:32:05, max mem: 15.9 GB 
[10/24 16:14:41 visual_prompt]: 	Training 600/1106. train loss: 93.6038,	0.6465 s / batch. (data: 7.87e-04). ETA=18:57:32, max mem: 15.9 GB 
[10/24 16:15:43 visual_prompt]: 	Training 700/1106. train loss: 56.4659,	0.6256 s / batch. (data: 7.19e-04). ETA=18:19:49, max mem: 15.9 GB 
[10/24 16:16:46 visual_prompt]: 	Training 800/1106. train loss: 22.0298,	0.6175 s / batch. (data: 3.13e-04). ETA=18:04:26, max mem: 15.9 GB 
[10/24 16:17:49 visual_prompt]: 	Training 900/1106. train loss: 25.4546,	0.6211 s / batch. (data: 7.43e-04). ETA=18:09:45, max mem: 15.9 GB 
[10/24 16:18:52 visual_prompt]: 	Training 1000/1106. train loss: 0.7015,	0.6182 s / batch. (data: 3.14e-04). ETA=18:03:36, max mem: 15.9 GB 
[10/24 16:19:55 visual_prompt]: 	Training 1100/1106. train loss: 69.7327,	0.6132 s / batch. (data: 1.66e-04). ETA=17:53:50, max mem: 15.9 GB 
[10/24 16:19:59 visual_prompt]: Epoch 5 / 100: avg data time: 4.13e-03, avg batch time: 0.6314, average train loss: 43.4832
[10/24 16:20:49 visual_prompt]: 	Test 100/123. loss: 67.057, 0.2398 s / batch. (data: 3.12e-05)max mem: 15.91075 GB 
[10/24 16:21:00 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2319, average loss: 61.0011
[10/24 16:21:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.83	
[10/24 16:21:00 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/24 16:22:05 visual_prompt]: 	Training 100/1106. train loss: 37.8674,	0.6480 s / batch. (data: 8.03e-04). ETA=18:53:42, max mem: 15.9 GB 
[10/24 16:23:08 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6306 s / batch. (data: 9.06e-04). ETA=18:22:08, max mem: 15.9 GB 
[10/24 16:24:11 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6458 s / batch. (data: 8.29e-04). ETA=18:47:36, max mem: 15.9 GB 
[10/24 16:25:14 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6367 s / batch. (data: 5.84e-03). ETA=18:30:41, max mem: 15.9 GB 
[10/24 16:26:17 visual_prompt]: 	Training 500/1106. train loss: 73.6186,	0.6204 s / batch. (data: 3.15e-04). ETA=18:01:14, max mem: 15.9 GB 
[10/24 16:27:20 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6230 s / batch. (data: 8.10e-04). ETA=18:04:47, max mem: 15.9 GB 
[10/24 16:28:23 visual_prompt]: 	Training 700/1106. train loss: 20.6173,	0.6411 s / batch. (data: 8.17e-04). ETA=18:35:12, max mem: 15.9 GB 
[10/24 16:29:25 visual_prompt]: 	Training 800/1106. train loss: 48.5209,	0.6478 s / batch. (data: 8.49e-04). ETA=18:45:41, max mem: 15.9 GB 
[10/24 16:30:28 visual_prompt]: 	Training 900/1106. train loss: 163.4230,	0.6233 s / batch. (data: 1.59e-03). ETA=18:02:10, max mem: 15.9 GB 
[10/24 16:31:31 visual_prompt]: 	Training 1000/1106. train loss: 20.4092,	0.6313 s / batch. (data: 3.03e-04). ETA=18:15:02, max mem: 15.9 GB 
[10/24 16:32:34 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6073 s / batch. (data: 2.44e-04). ETA=17:32:25, max mem: 15.9 GB 
[10/24 16:32:38 visual_prompt]: Epoch 6 / 100: avg data time: 4.44e-03, avg batch time: 0.6312, average train loss: 63.2590
[10/24 16:33:28 visual_prompt]: 	Test 100/123. loss: 63.624, 0.2337 s / batch. (data: 3.67e-05)max mem: 15.91075 GB 
[10/24 16:33:39 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.2343, average loss: 68.5486
[10/24 16:33:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.60	
[10/24 16:33:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/24 16:34:44 visual_prompt]: 	Training 100/1106. train loss: 19.4690,	0.6556 s / batch. (data: 7.99e-04). ETA=18:54:56, max mem: 15.9 GB 
[10/24 16:35:46 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6226 s / batch. (data: 1.57e-02). ETA=17:56:40, max mem: 15.9 GB 
[10/24 16:36:49 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6471 s / batch. (data: 1.04e-03). ETA=18:38:02, max mem: 15.9 GB 
[10/24 16:37:52 visual_prompt]: 	Training 400/1106. train loss: 8.7838,	0.6297 s / batch. (data: 2.99e-04). ETA=18:06:52, max mem: 15.9 GB 
[10/24 16:38:55 visual_prompt]: 	Training 500/1106. train loss: 32.2636,	0.6287 s / batch. (data: 2.66e-04). ETA=18:04:09, max mem: 15.9 GB 
[10/24 16:39:58 visual_prompt]: 	Training 600/1106. train loss: 10.0598,	0.6179 s / batch. (data: 8.02e-04). ETA=17:44:27, max mem: 15.9 GB 
[10/24 16:41:01 visual_prompt]: 	Training 700/1106. train loss: 81.8056,	0.6470 s / batch. (data: 7.55e-04). ETA=18:33:33, max mem: 15.9 GB 
[10/24 16:42:04 visual_prompt]: 	Training 800/1106. train loss: 0.0020,	0.6305 s / batch. (data: 7.82e-04). ETA=18:04:05, max mem: 15.9 GB 
[10/24 16:43:08 visual_prompt]: 	Training 900/1106. train loss: 95.3753,	0.6214 s / batch. (data: 7.09e-04). ETA=17:47:26, max mem: 15.9 GB 
[10/24 16:44:10 visual_prompt]: 	Training 1000/1106. train loss: 111.1483,	0.6403 s / batch. (data: 5.46e-03). ETA=18:18:47, max mem: 15.9 GB 
[10/24 16:45:13 visual_prompt]: 	Training 1100/1106. train loss: 0.0003,	0.6188 s / batch. (data: 2.18e-04). ETA=17:40:55, max mem: 15.9 GB 
[10/24 16:45:17 visual_prompt]: Epoch 7 / 100: avg data time: 4.42e-03, avg batch time: 0.6314, average train loss: 78.4308
[10/24 16:46:08 visual_prompt]: 	Test 100/123. loss: 4.512, 0.2520 s / batch. (data: 3.08e-05)max mem: 15.91075 GB 
[10/24 16:46:18 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.2332, average loss: 3.7551
[10/24 16:46:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.07	
[10/24 16:46:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/24 16:47:23 visual_prompt]: 	Training 100/1106. train loss: 59.4937,	0.6344 s / batch. (data: 8.40e-04). ETA=18:06:33, max mem: 15.9 GB 
[10/24 16:48:26 visual_prompt]: 	Training 200/1106. train loss: 107.8860,	0.6209 s / batch. (data: 3.38e-04). ETA=17:42:15, max mem: 15.9 GB 
[10/24 16:49:29 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6428 s / batch. (data: 8.19e-04). ETA=18:18:47, max mem: 15.9 GB 
[10/24 16:50:32 visual_prompt]: 	Training 400/1106. train loss: 43.9048,	0.6187 s / batch. (data: 2.54e-04). ETA=17:36:27, max mem: 15.9 GB 
[10/24 16:51:35 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6077 s / batch. (data: 3.06e-04). ETA=17:16:37, max mem: 15.9 GB 
[10/24 16:52:38 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6503 s / batch. (data: 3.32e-04). ETA=18:28:19, max mem: 15.9 GB 
[10/24 16:53:41 visual_prompt]: 	Training 700/1106. train loss: 101.7574,	0.6210 s / batch. (data: 3.21e-04). ETA=17:37:18, max mem: 15.9 GB 
[10/24 16:54:44 visual_prompt]: 	Training 800/1106. train loss: 97.7011,	0.6187 s / batch. (data: 4.30e-04). ETA=17:32:27, max mem: 15.9 GB 
[10/24 16:55:47 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6235 s / batch. (data: 1.10e-03). ETA=17:39:32, max mem: 15.9 GB 
[10/24 16:56:50 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6225 s / batch. (data: 9.82e-04). ETA=17:36:49, max mem: 15.9 GB 
[10/24 16:57:53 visual_prompt]: 	Training 1100/1106. train loss: 28.6345,	0.6168 s / batch. (data: 1.71e-04). ETA=17:26:04, max mem: 15.9 GB 
[10/24 16:57:57 visual_prompt]: Epoch 8 / 100: avg data time: 4.88e-03, avg batch time: 0.6313, average train loss: 79.5466
[10/24 16:58:47 visual_prompt]: 	Test 100/123. loss: 52.700, 0.2513 s / batch. (data: 3.67e-05)max mem: 15.91075 GB 
[10/24 16:58:58 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.2330, average loss: 48.1838
[10/24 16:58:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.48	
[10/24 16:58:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/24 17:00:03 visual_prompt]: 	Training 100/1106. train loss: 108.9615,	0.6181 s / batch. (data: 2.88e-04). ETA=17:27:06, max mem: 15.9 GB 
[10/24 17:01:06 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6240 s / batch. (data: 8.14e-04). ETA=17:36:12, max mem: 15.9 GB 
[10/24 17:02:09 visual_prompt]: 	Training 300/1106. train loss: 40.5832,	0.6156 s / batch. (data: 3.32e-04). ETA=17:20:58, max mem: 15.9 GB 
[10/24 17:03:12 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6126 s / batch. (data: 5.43e-03). ETA=17:14:47, max mem: 15.9 GB 
[10/24 17:04:15 visual_prompt]: 	Training 500/1106. train loss: 194.4849,	0.6317 s / batch. (data: 5.51e-03). ETA=17:46:02, max mem: 15.9 GB 
[10/24 17:05:18 visual_prompt]: 	Training 600/1106. train loss: 168.7465,	0.6279 s / batch. (data: 7.47e-04). ETA=17:38:29, max mem: 15.9 GB 
[10/24 17:06:21 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6188 s / batch. (data: 3.35e-04). ETA=17:22:07, max mem: 15.9 GB 
[10/24 17:07:24 visual_prompt]: 	Training 800/1106. train loss: 53.4378,	0.6191 s / batch. (data: 3.16e-04). ETA=17:21:40, max mem: 15.9 GB 
[10/24 17:08:27 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6211 s / batch. (data: 7.68e-04). ETA=17:23:59, max mem: 15.9 GB 
[10/24 17:09:30 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6375 s / batch. (data: 2.07e-02). ETA=17:50:24, max mem: 15.9 GB 
[10/24 17:10:33 visual_prompt]: 	Training 1100/1106. train loss: 183.9284,	0.6125 s / batch. (data: 1.71e-04). ETA=17:07:31, max mem: 15.9 GB 
[10/24 17:10:36 visual_prompt]: Epoch 9 / 100: avg data time: 4.79e-03, avg batch time: 0.6314, average train loss: 89.2225
[10/24 17:11:26 visual_prompt]: 	Test 100/123. loss: 16.368, 0.2252 s / batch. (data: 2.67e-05)max mem: 15.91075 GB 
[10/24 17:11:37 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2322, average loss: 14.7485
[10/24 17:11:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.09	
[10/24 17:11:37 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/24 17:12:42 visual_prompt]: 	Training 100/1106. train loss: 126.6700,	0.6242 s / batch. (data: 7.12e-04). ETA=17:25:57, max mem: 15.9 GB 
[10/24 17:13:45 visual_prompt]: 	Training 200/1106. train loss: 22.1571,	0.6184 s / batch. (data: 2.80e-04). ETA=17:15:11, max mem: 15.9 GB 
[10/24 17:14:48 visual_prompt]: 	Training 300/1106. train loss: 14.5207,	0.6162 s / batch. (data: 3.13e-04). ETA=17:10:37, max mem: 15.9 GB 
[10/24 17:15:51 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6073 s / batch. (data: 2.93e-04). ETA=16:54:43, max mem: 15.9 GB 
[10/24 17:16:54 visual_prompt]: 	Training 500/1106. train loss: 76.1916,	0.6396 s / batch. (data: 5.46e-03). ETA=17:47:33, max mem: 15.9 GB 
[10/24 17:17:57 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6506 s / batch. (data: 3.75e-02). ETA=18:04:46, max mem: 15.9 GB 
[10/24 17:18:59 visual_prompt]: 	Training 700/1106. train loss: 313.6913,	0.6403 s / batch. (data: 7.63e-04). ETA=17:46:39, max mem: 15.9 GB 
[10/24 17:20:02 visual_prompt]: 	Training 800/1106. train loss: 314.6733,	0.6136 s / batch. (data: 3.24e-04). ETA=17:01:08, max mem: 15.9 GB 
[10/24 17:21:05 visual_prompt]: 	Training 900/1106. train loss: 191.1544,	0.6397 s / batch. (data: 7.71e-04). ETA=17:43:24, max mem: 15.9 GB 
[10/24 17:22:08 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6321 s / batch. (data: 7.98e-04). ETA=17:29:47, max mem: 15.9 GB 
[10/24 17:23:11 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6080 s / batch. (data: 1.60e-04). ETA=16:48:40, max mem: 15.9 GB 
[10/24 17:23:14 visual_prompt]: Epoch 10 / 100: avg data time: 4.34e-03, avg batch time: 0.6304, average train loss: 102.4997
[10/24 17:24:05 visual_prompt]: 	Test 100/123. loss: 130.905, 0.2446 s / batch. (data: 4.12e-05)max mem: 15.91075 GB 
[10/24 17:24:15 visual_prompt]: Inference (val):avg data time: 2.40e-04, avg batch time: 0.2325, average loss: 118.2025
[10/24 17:24:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.78	
[10/24 17:24:15 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/24 17:25:20 visual_prompt]: 	Training 100/1106. train loss: 37.7845,	0.6316 s / batch. (data: 7.96e-04). ETA=17:26:48, max mem: 15.9 GB 
[10/24 17:26:23 visual_prompt]: 	Training 200/1106. train loss: 120.0059,	0.6439 s / batch. (data: 1.59e-02). ETA=17:46:00, max mem: 15.9 GB 
[10/24 17:27:26 visual_prompt]: 	Training 300/1106. train loss: 480.5607,	0.6467 s / batch. (data: 7.48e-04). ETA=17:49:35, max mem: 15.9 GB 
[10/24 17:28:29 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6074 s / batch. (data: 3.27e-04). ETA=16:43:37, max mem: 15.9 GB 
[10/24 17:29:32 visual_prompt]: 	Training 500/1106. train loss: 249.4829,	0.6146 s / batch. (data: 2.88e-04). ETA=16:54:25, max mem: 15.9 GB 
[10/24 17:30:35 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6189 s / batch. (data: 8.51e-04). ETA=17:00:38, max mem: 15.9 GB 
[10/24 17:31:37 visual_prompt]: 	Training 700/1106. train loss: 124.8323,	0.6343 s / batch. (data: 7.39e-04). ETA=17:24:53, max mem: 15.9 GB 
[10/24 17:32:40 visual_prompt]: 	Training 800/1106. train loss: 61.6685,	0.6334 s / batch. (data: 8.25e-04). ETA=17:22:17, max mem: 15.9 GB 
[10/24 17:33:43 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6333 s / batch. (data: 8.11e-04). ETA=17:21:04, max mem: 15.9 GB 
[10/24 17:34:48 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6310 s / batch. (data: 1.32e-02). ETA=17:16:18, max mem: 15.9 GB 
[10/24 17:35:55 visual_prompt]: 	Training 1100/1106. train loss: 101.3351,	0.6125 s / batch. (data: 1.51e-04). ETA=16:44:56, max mem: 15.9 GB 
[10/24 17:35:59 visual_prompt]: Epoch 11 / 100: avg data time: 1.06e-02, avg batch time: 0.6358, average train loss: 113.6127
[10/24 17:36:48 visual_prompt]: 	Test 100/123. loss: 40.954, 0.2366 s / batch. (data: 3.27e-05)max mem: 15.91075 GB 
[10/24 17:37:00 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2335, average loss: 36.9202
[10/24 17:37:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.35	
[10/24 17:37:00 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/24 17:38:05 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6440 s / batch. (data: 3.17e-04). ETA=17:35:25, max mem: 15.9 GB 
[10/24 17:39:08 visual_prompt]: 	Training 200/1106. train loss: 91.5025,	0.6212 s / batch. (data: 3.37e-04). ETA=16:57:01, max mem: 15.9 GB 
[10/24 17:40:11 visual_prompt]: 	Training 300/1106. train loss: 205.8038,	0.6193 s / batch. (data: 3.31e-04). ETA=16:52:50, max mem: 15.9 GB 
[10/24 17:41:14 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6464 s / batch. (data: 7.37e-04). ETA=17:36:07, max mem: 15.9 GB 
[10/24 17:42:17 visual_prompt]: 	Training 500/1106. train loss: 50.1242,	0.6367 s / batch. (data: 1.05e-02). ETA=17:19:15, max mem: 15.9 GB 
[10/24 17:43:20 visual_prompt]: 	Training 600/1106. train loss: 209.3689,	0.6184 s / batch. (data: 3.52e-04). ETA=16:48:24, max mem: 15.9 GB 
[10/24 17:44:23 visual_prompt]: 	Training 700/1106. train loss: 149.5734,	0.6136 s / batch. (data: 2.84e-04). ETA=16:39:33, max mem: 15.9 GB 
[10/24 17:45:26 visual_prompt]: 	Training 800/1106. train loss: 17.8477,	0.6191 s / batch. (data: 2.65e-04). ETA=16:47:29, max mem: 15.9 GB 
[10/24 17:46:29 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6208 s / batch. (data: 3.88e-04). ETA=16:49:11, max mem: 15.9 GB 
[10/24 17:47:31 visual_prompt]: 	Training 1000/1106. train loss: 184.1198,	0.6294 s / batch. (data: 7.89e-04). ETA=17:02:05, max mem: 15.9 GB 
[10/24 17:48:34 visual_prompt]: 	Training 1100/1106. train loss: 85.0141,	0.6136 s / batch. (data: 1.55e-04). ETA=16:35:28, max mem: 15.9 GB 
[10/24 17:48:38 visual_prompt]: Epoch 12 / 100: avg data time: 5.10e-03, avg batch time: 0.6317, average train loss: 116.6718
[10/24 17:49:32 visual_prompt]: 	Test 100/123. loss: 71.299, 0.2255 s / batch. (data: 3.22e-05)max mem: 15.91075 GB 
[10/24 17:49:48 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.2305, average loss: 77.9660
[10/24 17:49:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.63	
[10/24 17:49:48 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/24 17:50:53 visual_prompt]: 	Training 100/1106. train loss: 316.0322,	0.6171 s / batch. (data: 2.93e-04). ETA=16:40:03, max mem: 15.9 GB 
[10/24 17:51:56 visual_prompt]: 	Training 200/1106. train loss: 189.3247,	0.6322 s / batch. (data: 2.49e-04). ETA=17:03:20, max mem: 15.9 GB 
[10/24 17:52:59 visual_prompt]: 	Training 300/1106. train loss: 363.6349,	0.6326 s / batch. (data: 3.32e-04). ETA=17:02:55, max mem: 15.9 GB 
[10/24 17:54:01 visual_prompt]: 	Training 400/1106. train loss: 223.4685,	0.6189 s / batch. (data: 3.45e-04). ETA=16:39:51, max mem: 15.9 GB 
[10/24 17:55:04 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6084 s / batch. (data: 4.23e-04). ETA=16:21:46, max mem: 15.9 GB 
[10/24 17:56:07 visual_prompt]: 	Training 600/1106. train loss: 85.0376,	0.6261 s / batch. (data: 7.67e-04). ETA=16:49:20, max mem: 15.9 GB 
[10/24 17:57:17 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6280 s / batch. (data: 7.47e-04). ETA=16:51:24, max mem: 15.9 GB 
[10/24 17:58:21 visual_prompt]: 	Training 800/1106. train loss: 104.7190,	0.6350 s / batch. (data: 8.08e-03). ETA=17:01:37, max mem: 15.9 GB 
[10/24 17:59:24 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6475 s / batch. (data: 7.13e-04). ETA=17:20:39, max mem: 15.9 GB 
[10/24 18:00:31 visual_prompt]: 	Training 1000/1106. train loss: 231.2770,	0.6320 s / batch. (data: 2.89e-04). ETA=16:54:38, max mem: 15.9 GB 
[10/24 18:01:34 visual_prompt]: 	Training 1100/1106. train loss: 58.3267,	0.6144 s / batch. (data: 2.12e-04). ETA=16:25:23, max mem: 15.9 GB 
[10/24 18:01:37 visual_prompt]: Epoch 13 / 100: avg data time: 1.74e-02, avg batch time: 0.6410, average train loss: 114.7335
[10/24 18:02:27 visual_prompt]: 	Test 100/123. loss: 46.862, 0.2265 s / batch. (data: 2.93e-05)max mem: 15.91075 GB 
[10/24 18:02:38 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.2321, average loss: 36.9477
[10/24 18:02:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.42	
[10/24 18:02:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/24 18:03:44 visual_prompt]: 	Training 100/1106. train loss: 160.9790,	0.6242 s / batch. (data: 8.27e-04). ETA=16:40:03, max mem: 15.9 GB 
[10/24 18:04:46 visual_prompt]: 	Training 200/1106. train loss: 115.1668,	0.6132 s / batch. (data: 3.12e-04). ETA=16:21:19, max mem: 15.9 GB 
[10/24 18:05:49 visual_prompt]: 	Training 300/1106. train loss: 239.7207,	0.6259 s / batch. (data: 7.66e-04). ETA=16:40:41, max mem: 15.9 GB 
[10/24 18:06:52 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6086 s / batch. (data: 3.38e-04). ETA=16:11:58, max mem: 15.9 GB 
[10/24 18:07:54 visual_prompt]: 	Training 500/1106. train loss: 178.5203,	0.6141 s / batch. (data: 3.28e-04). ETA=16:19:43, max mem: 15.9 GB 
[10/24 18:08:57 visual_prompt]: 	Training 600/1106. train loss: 173.7152,	0.6279 s / batch. (data: 3.31e-04). ETA=16:40:42, max mem: 15.9 GB 
[10/24 18:10:00 visual_prompt]: 	Training 700/1106. train loss: 374.9276,	0.6240 s / batch. (data: 7.34e-04). ETA=16:33:26, max mem: 15.9 GB 
[10/24 18:11:03 visual_prompt]: 	Training 800/1106. train loss: 348.8754,	0.6253 s / batch. (data: 1.19e-02). ETA=16:34:30, max mem: 15.9 GB 
[10/24 18:12:05 visual_prompt]: 	Training 900/1106. train loss: 40.7823,	0.6255 s / batch. (data: 7.15e-04). ETA=16:33:47, max mem: 15.9 GB 
[10/24 18:13:08 visual_prompt]: 	Training 1000/1106. train loss: 173.7703,	0.6356 s / batch. (data: 1.66e-02). ETA=16:48:39, max mem: 15.9 GB 
[10/24 18:14:11 visual_prompt]: 	Training 1100/1106. train loss: 108.1612,	0.6126 s / batch. (data: 1.57e-04). ETA=16:11:11, max mem: 15.9 GB 
[10/24 18:14:14 visual_prompt]: Epoch 14 / 100: avg data time: 4.65e-03, avg batch time: 0.6292, average train loss: 110.0129
[10/24 18:15:05 visual_prompt]: 	Test 100/123. loss: 30.584, 0.2260 s / batch. (data: 4.17e-05)max mem: 15.91075 GB 
[10/24 18:15:16 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2313, average loss: 33.4354
[10/24 18:15:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.09	
[10/24 18:15:16 visual_prompt]: Best epoch 14: best metric: -33.435
[10/24 18:15:16 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/24 18:16:20 visual_prompt]: 	Training 100/1106. train loss: 175.4070,	0.6168 s / batch. (data: 3.88e-03). ETA=16:16:47, max mem: 15.9 GB 
[10/24 18:17:23 visual_prompt]: 	Training 200/1106. train loss: 114.9922,	0.6136 s / batch. (data: 8.02e-04). ETA=16:10:40, max mem: 15.9 GB 
[10/24 18:18:26 visual_prompt]: 	Training 300/1106. train loss: 71.9408,	0.6321 s / batch. (data: 3.05e-04). ETA=16:38:48, max mem: 15.9 GB 
[10/24 18:19:29 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6233 s / batch. (data: 7.69e-04). ETA=16:23:59, max mem: 15.9 GB 
[10/24 18:20:32 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6286 s / batch. (data: 5.86e-03). ETA=16:31:13, max mem: 15.9 GB 
[10/24 18:21:34 visual_prompt]: 	Training 600/1106. train loss: 73.7822,	0.6666 s / batch. (data: 7.11e-03). ETA=17:30:00, max mem: 15.9 GB 
[10/24 18:22:37 visual_prompt]: 	Training 700/1106. train loss: 75.2433,	0.6476 s / batch. (data: 8.59e-04). ETA=16:59:01, max mem: 15.9 GB 
[10/24 18:23:40 visual_prompt]: 	Training 800/1106. train loss: 73.0382,	0.6488 s / batch. (data: 1.27e-02). ETA=16:59:51, max mem: 15.9 GB 
[10/24 18:24:43 visual_prompt]: 	Training 900/1106. train loss: 345.0231,	0.6212 s / batch. (data: 7.41e-04). ETA=16:15:30, max mem: 15.9 GB 
[10/24 18:25:45 visual_prompt]: 	Training 1000/1106. train loss: 51.6958,	0.6637 s / batch. (data: 1.11e-02). ETA=17:21:03, max mem: 15.9 GB 
[10/24 18:26:48 visual_prompt]: 	Training 1100/1106. train loss: 91.3883,	0.6134 s / batch. (data: 1.46e-04). ETA=16:01:09, max mem: 15.9 GB 
[10/24 18:26:52 visual_prompt]: Epoch 15 / 100: avg data time: 4.34e-03, avg batch time: 0.6296, average train loss: 105.3219
[10/24 18:27:42 visual_prompt]: 	Test 100/123. loss: 109.590, 0.2248 s / batch. (data: 2.84e-05)max mem: 15.91075 GB 
[10/24 18:27:53 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2330, average loss: 99.0605
[10/24 18:27:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.96	
[10/24 18:27:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/24 18:28:58 visual_prompt]: 	Training 100/1106. train loss: 136.9447,	0.6391 s / batch. (data: 8.50e-04). ETA=16:40:17, max mem: 15.9 GB 
[10/24 18:30:00 visual_prompt]: 	Training 200/1106. train loss: 53.6926,	0.6435 s / batch. (data: 8.53e-04). ETA=16:46:11, max mem: 15.9 GB 
[10/24 18:31:03 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6231 s / batch. (data: 8.01e-04). ETA=16:13:08, max mem: 15.9 GB 
[10/24 18:32:06 visual_prompt]: 	Training 400/1106. train loss: 135.8823,	0.6178 s / batch. (data: 3.40e-04). ETA=16:03:54, max mem: 15.9 GB 
[10/24 18:33:09 visual_prompt]: 	Training 500/1106. train loss: 73.3165,	0.6181 s / batch. (data: 3.02e-04). ETA=16:03:15, max mem: 15.9 GB 
[10/24 18:34:12 visual_prompt]: 	Training 600/1106. train loss: 556.4421,	0.6266 s / batch. (data: 3.24e-04). ETA=16:15:34, max mem: 15.9 GB 
[10/24 18:35:14 visual_prompt]: 	Training 700/1106. train loss: 19.2053,	0.6308 s / batch. (data: 7.54e-04). ETA=16:21:00, max mem: 15.9 GB 
[10/24 18:36:23 visual_prompt]: 	Training 800/1106. train loss: 57.0340,	0.6560 s / batch. (data: 2.94e-04). ETA=16:59:04, max mem: 15.9 GB 
[10/24 18:37:26 visual_prompt]: 	Training 900/1106. train loss: 334.2365,	0.6151 s / batch. (data: 3.36e-04). ETA=15:54:29, max mem: 15.9 GB 
[10/24 18:38:29 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6275 s / batch. (data: 7.35e-04). ETA=16:12:44, max mem: 15.9 GB 
[10/24 18:39:32 visual_prompt]: 	Training 1100/1106. train loss: 275.5450,	0.6185 s / batch. (data: 1.38e-04). ETA=15:57:48, max mem: 15.9 GB 
[10/24 18:39:36 visual_prompt]: Epoch 16 / 100: avg data time: 9.42e-03, avg batch time: 0.6351, average train loss: 97.8061
[10/24 18:40:26 visual_prompt]: 	Test 100/123. loss: 109.468, 0.2333 s / batch. (data: 5.72e-05)max mem: 15.91075 GB 
[10/24 18:40:37 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2340, average loss: 100.4887
[10/24 18:40:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.89	
[10/24 18:40:37 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/24 18:41:42 visual_prompt]: 	Training 100/1106. train loss: 122.7931,	0.6177 s / batch. (data: 3.52e-04). ETA=15:55:23, max mem: 15.9 GB 
[10/24 18:42:45 visual_prompt]: 	Training 200/1106. train loss: 14.3997,	0.6351 s / batch. (data: 1.27e-02). ETA=16:21:12, max mem: 15.9 GB 
[10/24 18:43:47 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6589 s / batch. (data: 3.85e-02). ETA=16:56:56, max mem: 15.9 GB 
[10/24 18:44:50 visual_prompt]: 	Training 400/1106. train loss: 192.8803,	0.6436 s / batch. (data: 6.03e-03). ETA=16:32:11, max mem: 15.9 GB 
[10/24 18:45:53 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6174 s / batch. (data: 2.74e-04). ETA=15:50:46, max mem: 15.9 GB 
[10/24 18:46:56 visual_prompt]: 	Training 600/1106. train loss: 82.4032,	0.6252 s / batch. (data: 3.23e-04). ETA=16:01:51, max mem: 15.9 GB 
[10/24 18:47:59 visual_prompt]: 	Training 700/1106. train loss: 11.8036,	0.6238 s / batch. (data: 7.94e-04). ETA=15:58:34, max mem: 15.9 GB 
[10/24 18:49:02 visual_prompt]: 	Training 800/1106. train loss: 8.8124,	0.6187 s / batch. (data: 3.24e-04). ETA=15:49:41, max mem: 15.9 GB 
[10/24 18:50:05 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6381 s / batch. (data: 5.91e-03). ETA=16:18:30, max mem: 15.9 GB 
[10/24 18:51:07 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6076 s / batch. (data: 3.32e-04). ETA=15:30:41, max mem: 15.9 GB 
[10/24 18:52:10 visual_prompt]: 	Training 1100/1106. train loss: 32.8415,	0.6168 s / batch. (data: 1.29e-04). ETA=15:43:42, max mem: 15.9 GB 
[10/24 18:52:14 visual_prompt]: Epoch 17 / 100: avg data time: 4.70e-03, avg batch time: 0.6302, average train loss: 105.4368
[10/24 18:53:04 visual_prompt]: 	Test 100/123. loss: 88.686, 0.2249 s / batch. (data: 3.17e-05)max mem: 15.91075 GB 
[10/24 18:53:15 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2331, average loss: 80.2282
[10/24 18:53:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.08	
[10/24 18:53:15 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/24 18:54:20 visual_prompt]: 	Training 100/1106. train loss: 626.7708,	0.6184 s / batch. (data: 3.14e-04). ETA=15:45:08, max mem: 15.9 GB 
[10/24 18:55:22 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6224 s / batch. (data: 3.51e-04). ETA=15:50:09, max mem: 15.9 GB 
[10/24 18:56:25 visual_prompt]: 	Training 300/1106. train loss: 20.4745,	0.6495 s / batch. (data: 9.05e-04). ETA=16:30:26, max mem: 15.9 GB 
[10/24 18:57:28 visual_prompt]: 	Training 400/1106. train loss: 98.8701,	0.6294 s / batch. (data: 1.33e-02). ETA=15:58:42, max mem: 15.9 GB 
[10/24 18:58:31 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6300 s / batch. (data: 1.20e-02). ETA=15:58:41, max mem: 15.9 GB 
[10/24 18:59:34 visual_prompt]: 	Training 600/1106. train loss: 493.5202,	0.6342 s / batch. (data: 1.27e-03). ETA=16:03:57, max mem: 15.9 GB 
[10/24 19:00:37 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6220 s / batch. (data: 1.31e-02). ETA=15:44:19, max mem: 15.9 GB 
[10/24 19:01:40 visual_prompt]: 	Training 800/1106. train loss: 68.9318,	0.6138 s / batch. (data: 3.15e-04). ETA=15:30:53, max mem: 15.9 GB 
[10/24 19:02:42 visual_prompt]: 	Training 900/1106. train loss: 95.8815,	0.6327 s / batch. (data: 9.73e-04). ETA=15:58:30, max mem: 15.9 GB 
[10/24 19:03:45 visual_prompt]: 	Training 1000/1106. train loss: 59.2108,	0.6156 s / batch. (data: 7.74e-04). ETA=15:31:36, max mem: 15.9 GB 
[10/24 19:04:48 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6070 s / batch. (data: 1.46e-04). ETA=15:17:38, max mem: 15.9 GB 
[10/24 19:04:52 visual_prompt]: Epoch 18 / 100: avg data time: 4.36e-03, avg batch time: 0.6303, average train loss: 105.9837
[10/24 19:05:42 visual_prompt]: 	Test 100/123. loss: 134.202, 0.2249 s / batch. (data: 2.88e-05)max mem: 15.91075 GB 
[10/24 19:05:52 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.2319, average loss: 120.6211
[10/24 19:05:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.84	
[10/24 19:05:52 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[10/24 19:06:57 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6081 s / batch. (data: 7.87e-04). ETA=15:18:04, max mem: 15.9 GB 
[10/24 19:08:00 visual_prompt]: 	Training 200/1106. train loss: 178.9804,	0.6122 s / batch. (data: 2.49e-04). ETA=15:23:14, max mem: 15.9 GB 
[10/24 19:09:03 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6341 s / batch. (data: 7.88e-04). ETA=15:55:19, max mem: 15.9 GB 
[10/24 19:10:06 visual_prompt]: 	Training 400/1106. train loss: 203.0878,	0.6119 s / batch. (data: 4.57e-04). ETA=15:20:46, max mem: 15.9 GB 
[10/24 19:11:09 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6460 s / batch. (data: 3.23e-02). ETA=16:11:06, max mem: 15.9 GB 
[10/24 19:12:11 visual_prompt]: 	Training 600/1106. train loss: 234.1320,	0.6176 s / batch. (data: 2.81e-04). ETA=15:27:18, max mem: 15.9 GB 
[10/24 19:13:14 visual_prompt]: 	Training 700/1106. train loss: 6.2111,	0.6393 s / batch. (data: 8.12e-04). ETA=15:58:53, max mem: 15.9 GB 
[10/24 19:14:17 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6075 s / batch. (data: 3.10e-04). ETA=15:10:09, max mem: 15.9 GB 
[10/24 19:15:20 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6073 s / batch. (data: 3.49e-04). ETA=15:08:46, max mem: 15.9 GB 
[10/24 19:16:23 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6172 s / batch. (data: 7.39e-04). ETA=15:22:41, max mem: 15.9 GB 
[10/24 19:17:25 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6122 s / batch. (data: 1.87e-04). ETA=15:14:08, max mem: 15.9 GB 
[10/24 19:17:29 visual_prompt]: Epoch 19 / 100: avg data time: 4.05e-03, avg batch time: 0.6299, average train loss: 118.8256
[10/24 19:18:20 visual_prompt]: 	Test 100/123. loss: 232.980, 0.2254 s / batch. (data: 4.51e-05)max mem: 15.91075 GB 
[10/24 19:18:30 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2320, average loss: 210.2457
[10/24 19:18:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.68	
[10/24 19:18:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[10/24 19:19:35 visual_prompt]: 	Training 100/1106. train loss: 13.9015,	0.6478 s / batch. (data: 5.91e-03). ETA=16:06:06, max mem: 15.9 GB 
[10/24 19:20:38 visual_prompt]: 	Training 200/1106. train loss: 53.1599,	0.6339 s / batch. (data: 7.77e-04). ETA=15:44:23, max mem: 15.9 GB 
[10/24 19:21:40 visual_prompt]: 	Training 300/1106. train loss: 89.9237,	0.6303 s / batch. (data: 7.62e-04). ETA=15:37:55, max mem: 15.9 GB 
[10/24 19:22:43 visual_prompt]: 	Training 400/1106. train loss: 7.5615,	0.6229 s / batch. (data: 3.13e-04). ETA=15:25:50, max mem: 15.9 GB 
[10/24 19:23:46 visual_prompt]: 	Training 500/1106. train loss: 121.9243,	0.6191 s / batch. (data: 7.97e-04). ETA=15:19:08, max mem: 15.9 GB 
[10/24 19:24:52 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6360 s / batch. (data: 3.54e-04). ETA=15:43:12, max mem: 15.9 GB 
[10/24 19:25:55 visual_prompt]: 	Training 700/1106. train loss: 42.0336,	0.6240 s / batch. (data: 3.23e-04). ETA=15:24:24, max mem: 15.9 GB 
[10/24 19:26:57 visual_prompt]: 	Training 800/1106. train loss: 7.3715,	0.6311 s / batch. (data: 7.97e-04). ETA=15:33:51, max mem: 15.9 GB 
[10/24 19:28:00 visual_prompt]: 	Training 900/1106. train loss: 7.3547,	0.6194 s / batch. (data: 2.54e-04). ETA=15:15:27, max mem: 15.9 GB 
[10/24 19:29:03 visual_prompt]: 	Training 1000/1106. train loss: 42.5196,	0.6269 s / batch. (data: 7.20e-04). ETA=15:25:31, max mem: 15.9 GB 
[10/24 19:30:06 visual_prompt]: 	Training 1100/1106. train loss: 194.8586,	0.6173 s / batch. (data: 1.44e-04). ETA=15:10:22, max mem: 15.9 GB 
[10/24 19:30:10 visual_prompt]: Epoch 20 / 100: avg data time: 6.93e-03, avg batch time: 0.6334, average train loss: 114.1494
[10/24 19:31:00 visual_prompt]: 	Test 100/123. loss: 153.191, 0.2256 s / batch. (data: 3.10e-05)max mem: 15.91075 GB 
[10/24 19:31:11 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2323, average loss: 168.3730
[10/24 19:31:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.28	
[10/24 19:31:11 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[10/24 19:32:16 visual_prompt]: 	Training 100/1106. train loss: 916.4036,	0.6186 s / batch. (data: 3.35e-04). ETA=15:11:08, max mem: 15.9 GB 
[10/24 19:33:19 visual_prompt]: 	Training 200/1106. train loss: 258.9501,	0.6361 s / batch. (data: 5.86e-03). ETA=15:35:54, max mem: 15.9 GB 
[10/24 19:34:22 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6596 s / batch. (data: 8.10e-04). ETA=16:09:21, max mem: 15.9 GB 
[10/24 19:35:25 visual_prompt]: 	Training 400/1106. train loss: 111.2521,	0.6359 s / batch. (data: 2.15e-02). ETA=15:33:28, max mem: 15.9 GB 
[10/24 19:36:28 visual_prompt]: 	Training 500/1106. train loss: 190.2236,	0.6347 s / batch. (data: 8.64e-04). ETA=15:30:40, max mem: 15.9 GB 
[10/24 19:37:31 visual_prompt]: 	Training 600/1106. train loss: 192.3185,	0.6189 s / batch. (data: 3.05e-04). ETA=15:06:30, max mem: 15.9 GB 
[10/24 19:38:33 visual_prompt]: 	Training 700/1106. train loss: 132.3729,	0.6276 s / batch. (data: 3.24e-04). ETA=15:18:06, max mem: 15.9 GB 
[10/24 19:39:36 visual_prompt]: 	Training 800/1106. train loss: 181.0904,	0.6355 s / batch. (data: 7.47e-04). ETA=15:28:37, max mem: 15.9 GB 
[10/24 19:40:39 visual_prompt]: 	Training 900/1106. train loss: 295.7754,	0.6360 s / batch. (data: 3.05e-04). ETA=15:28:18, max mem: 15.9 GB 
[10/24 19:41:42 visual_prompt]: 	Training 1000/1106. train loss: 0.0655,	0.6453 s / batch. (data: 8.36e-04). ETA=15:40:48, max mem: 15.9 GB 
[10/24 19:42:45 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6077 s / batch. (data: 1.40e-04). ETA=14:45:01, max mem: 15.9 GB 
[10/24 19:42:49 visual_prompt]: Epoch 21 / 100: avg data time: 4.74e-03, avg batch time: 0.6310, average train loss: 117.8210
[10/24 19:43:39 visual_prompt]: 	Test 100/123. loss: 96.517, 0.2385 s / batch. (data: 2.98e-05)max mem: 15.91075 GB 
[10/24 19:43:50 visual_prompt]: Inference (val):avg data time: 2.82e-04, avg batch time: 0.2327, average loss: 87.8122
[10/24 19:43:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.01	
[10/24 19:43:50 visual_prompt]: Stopping early.
[10/24 19:43:50 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 19:43:50 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 19:43:50 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 19:43:50 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 19:43:50 visual_prompt]: Training with config:
[10/24 19:43:50 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr25.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 19:43:50 visual_prompt]: Loading training data...
[10/24 19:43:50 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 19:43:50 visual_prompt]: Loading validation data...
[10/24 19:43:50 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 19:43:50 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/24 19:43:52 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/24 19:43:52 visual_prompt]: tuned percent:0.522
[10/24 19:43:53 visual_prompt]: Device used for model: 0
[10/24 19:43:53 visual_prompt]: Setting up Evaluator...
[10/24 19:43:53 visual_prompt]: Setting up Trainer...
[10/24 19:43:53 visual_prompt]: 	Setting up the optimizer...
[10/24 19:43:53 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 19:44:59 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6453 s / batch. (data: 9.25e-04). ETA=19:48:20, max mem: 15.9 GB 
[10/24 19:46:02 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6303 s / batch. (data: 7.24e-04). ETA=19:19:39, max mem: 15.9 GB 
[10/24 19:47:05 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6187 s / batch. (data: 3.50e-04). ETA=18:57:18, max mem: 15.9 GB 
[10/24 19:48:08 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6328 s / batch. (data: 3.15e-04). ETA=19:22:17, max mem: 15.9 GB 
[10/24 19:49:12 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6602 s / batch. (data: 1.05e-02). ETA=20:11:31, max mem: 15.9 GB 
[10/24 19:50:15 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6305 s / batch. (data: 7.40e-04). ETA=19:15:49, max mem: 15.9 GB 
[10/24 19:51:18 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6197 s / batch. (data: 3.91e-04). ETA=18:55:00, max mem: 15.9 GB 
[10/24 19:52:21 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6374 s / batch. (data: 7.72e-04). ETA=19:26:28, max mem: 15.9 GB 
[10/24 19:53:25 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6200 s / batch. (data: 3.03e-04). ETA=18:53:33, max mem: 15.9 GB 
[10/24 19:54:28 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6360 s / batch. (data: 7.97e-04). ETA=19:21:47, max mem: 15.9 GB 
[10/24 19:55:32 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6176 s / batch. (data: 1.63e-04). ETA=18:47:04, max mem: 15.9 GB 
[10/24 19:55:35 visual_prompt]: Epoch 1 / 100: avg data time: 5.03e-03, avg batch time: 0.6352, average train loss: 1.4028
[10/24 19:56:25 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2258 s / batch. (data: 3.27e-05)max mem: 15.91075 GB 
[10/24 19:56:36 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2320, average loss: 1.3505
[10/24 19:56:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/24 19:56:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/24 19:57:41 visual_prompt]: 	Training 100/1106. train loss: 24.5193,	0.6177 s / batch. (data: 2.46e-04). ETA=18:46:17, max mem: 15.9 GB 
[10/24 19:58:44 visual_prompt]: 	Training 200/1106. train loss: 17.6197,	0.6335 s / batch. (data: 8.79e-04). ETA=19:14:01, max mem: 15.9 GB 
[10/24 19:59:47 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6457 s / batch. (data: 7.85e-04). ETA=19:35:06, max mem: 15.9 GB 
[10/24 20:00:50 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6258 s / batch. (data: 3.38e-04). ETA=18:57:49, max mem: 15.9 GB 
[10/24 20:01:54 visual_prompt]: 	Training 500/1106. train loss: 8.9271,	0.6840 s / batch. (data: 3.99e-02). ETA=20:42:27, max mem: 15.9 GB 
[10/24 20:02:57 visual_prompt]: 	Training 600/1106. train loss: 3.8181,	0.6422 s / batch. (data: 8.01e-04). ETA=19:25:35, max mem: 15.9 GB 
[10/24 20:04:00 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6347 s / batch. (data: 7.72e-04). ETA=19:10:47, max mem: 15.9 GB 
[10/24 20:05:03 visual_prompt]: 	Training 800/1106. train loss: 12.8612,	0.6200 s / batch. (data: 3.13e-04). ETA=18:43:11, max mem: 15.9 GB 
[10/24 20:06:07 visual_prompt]: 	Training 900/1106. train loss: 12.3197,	0.6320 s / batch. (data: 1.20e-02). ETA=19:03:54, max mem: 15.9 GB 
[10/24 20:07:10 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6396 s / batch. (data: 1.56e-02). ETA=19:16:34, max mem: 15.9 GB 
[10/24 20:08:13 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6184 s / batch. (data: 1.62e-04). ETA=18:37:14, max mem: 15.9 GB 
[10/24 20:08:17 visual_prompt]: Epoch 2 / 100: avg data time: 4.07e-03, avg batch time: 0.6337, average train loss: 12.3614
[10/24 20:09:07 visual_prompt]: 	Test 100/123. loss: 7.658, 0.2466 s / batch. (data: 3.98e-05)max mem: 15.91075 GB 
[10/24 20:09:18 visual_prompt]: Inference (val):avg data time: 1.32e-04, avg batch time: 0.2331, average loss: 6.9327
[10/24 20:09:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.26	
[10/24 20:09:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/24 20:10:24 visual_prompt]: 	Training 100/1106. train loss: 0.3535,	0.6409 s / batch. (data: 8.43e-04). ETA=19:16:41, max mem: 15.9 GB 
[10/24 20:11:27 visual_prompt]: 	Training 200/1106. train loss: 29.7258,	0.6520 s / batch. (data: 8.61e-04). ETA=19:35:38, max mem: 15.9 GB 
[10/24 20:12:30 visual_prompt]: 	Training 300/1106. train loss: 6.8673,	0.6253 s / batch. (data: 3.15e-04). ETA=18:46:23, max mem: 15.9 GB 
[10/24 20:13:33 visual_prompt]: 	Training 400/1106. train loss: 25.6901,	0.6321 s / batch. (data: 1.23e-02). ETA=18:57:42, max mem: 15.9 GB 
[10/24 20:14:36 visual_prompt]: 	Training 500/1106. train loss: 73.9383,	0.6173 s / batch. (data: 3.06e-04). ETA=18:29:54, max mem: 15.9 GB 
[10/24 20:15:39 visual_prompt]: 	Training 600/1106. train loss: 301.2530,	0.6187 s / batch. (data: 3.29e-04). ETA=18:31:28, max mem: 15.9 GB 
[10/24 20:16:43 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6245 s / batch. (data: 3.19e-04). ETA=18:40:49, max mem: 15.9 GB 
[10/24 20:17:46 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6184 s / batch. (data: 3.04e-04). ETA=18:28:50, max mem: 15.9 GB 
[10/24 20:18:49 visual_prompt]: 	Training 900/1106. train loss: 12.1604,	0.6440 s / batch. (data: 3.10e-04). ETA=19:13:41, max mem: 15.9 GB 
[10/24 20:19:52 visual_prompt]: 	Training 1000/1106. train loss: 54.6757,	0.6187 s / batch. (data: 3.35e-04). ETA=18:27:23, max mem: 15.9 GB 
[10/24 20:20:55 visual_prompt]: 	Training 1100/1106. train loss: 63.4358,	0.6117 s / batch. (data: 1.68e-04). ETA=18:13:43, max mem: 15.9 GB 
[10/24 20:20:59 visual_prompt]: Epoch 3 / 100: avg data time: 5.37e-03, avg batch time: 0.6340, average train loss: 26.6599
[10/24 20:21:49 visual_prompt]: 	Test 100/123. loss: 34.133, 0.2255 s / batch. (data: 4.27e-05)max mem: 15.91075 GB 
[10/24 20:22:00 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2317, average loss: 37.3333
[10/24 20:22:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.06	
[10/24 20:22:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/24 20:23:05 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6326 s / batch. (data: 3.23e-04). ETA=18:50:05, max mem: 15.9 GB 
[10/24 20:24:08 visual_prompt]: 	Training 200/1106. train loss: 164.6472,	0.6434 s / batch. (data: 8.25e-04). ETA=19:08:13, max mem: 15.9 GB 
[10/24 20:25:11 visual_prompt]: 	Training 300/1106. train loss: 1.9370,	0.6179 s / batch. (data: 2.99e-04). ETA=18:21:43, max mem: 15.9 GB 
[10/24 20:26:14 visual_prompt]: 	Training 400/1106. train loss: 5.1361,	0.6353 s / batch. (data: 8.03e-04). ETA=18:51:41, max mem: 15.9 GB 
[10/24 20:27:17 visual_prompt]: 	Training 500/1106. train loss: 4.2917,	0.6191 s / batch. (data: 4.79e-04). ETA=18:21:44, max mem: 15.9 GB 
[10/24 20:28:21 visual_prompt]: 	Training 600/1106. train loss: 23.2744,	0.6316 s / batch. (data: 3.15e-04). ETA=18:43:00, max mem: 15.9 GB 
[10/24 20:29:24 visual_prompt]: 	Training 700/1106. train loss: 43.3586,	0.6454 s / batch. (data: 3.15e-04). ETA=19:06:28, max mem: 15.9 GB 
[10/24 20:30:27 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6174 s / batch. (data: 2.95e-04). ETA=18:15:36, max mem: 15.9 GB 
[10/24 20:31:29 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6328 s / batch. (data: 1.14e-03). ETA=18:41:59, max mem: 15.9 GB 
[10/24 20:32:32 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6120 s / batch. (data: 3.01e-04). ETA=18:04:04, max mem: 15.9 GB 
[10/24 20:33:36 visual_prompt]: 	Training 1100/1106. train loss: 14.4156,	0.6187 s / batch. (data: 1.39e-04). ETA=18:14:59, max mem: 15.9 GB 
[10/24 20:33:39 visual_prompt]: Epoch 4 / 100: avg data time: 4.40e-03, avg batch time: 0.6326, average train loss: 36.8057
[10/24 20:34:29 visual_prompt]: 	Test 100/123. loss: 4.116, 0.2397 s / batch. (data: 3.08e-05)max mem: 15.91075 GB 
[10/24 20:34:40 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.2322, average loss: 2.8251
[10/24 20:34:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.98	
[10/24 20:34:40 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/24 20:35:45 visual_prompt]: 	Training 100/1106. train loss: 50.6507,	0.6395 s / batch. (data: 8.34e-04). ETA=18:50:36, max mem: 15.9 GB 
[10/24 20:36:48 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6221 s / batch. (data: 3.04e-04). ETA=18:18:44, max mem: 15.9 GB 
[10/24 20:37:51 visual_prompt]: 	Training 300/1106. train loss: 76.0419,	0.6319 s / batch. (data: 8.11e-04). ETA=18:34:57, max mem: 15.9 GB 
[10/24 20:38:54 visual_prompt]: 	Training 400/1106. train loss: 69.1398,	0.6399 s / batch. (data: 7.48e-04). ETA=18:48:07, max mem: 15.9 GB 
[10/24 20:39:58 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6195 s / batch. (data: 3.19e-04). ETA=18:11:01, max mem: 15.9 GB 
[10/24 20:41:01 visual_prompt]: 	Training 600/1106. train loss: 46.3404,	0.6403 s / batch. (data: 7.63e-04). ETA=18:46:35, max mem: 15.9 GB 
[10/24 20:42:04 visual_prompt]: 	Training 700/1106. train loss: 98.4349,	0.6151 s / batch. (data: 2.98e-04). ETA=18:01:22, max mem: 15.9 GB 
[10/24 20:43:07 visual_prompt]: 	Training 800/1106. train loss: 87.4513,	0.6442 s / batch. (data: 8.07e-04). ETA=18:51:18, max mem: 15.9 GB 
[10/24 20:44:10 visual_prompt]: 	Training 900/1106. train loss: 27.1970,	0.6318 s / batch. (data: 1.35e-02). ETA=18:28:32, max mem: 15.9 GB 
[10/24 20:45:13 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6571 s / batch. (data: 2.98e-02). ETA=19:11:53, max mem: 15.9 GB 
[10/24 20:46:17 visual_prompt]: 	Training 1100/1106. train loss: 96.7316,	0.6140 s / batch. (data: 1.76e-04). ETA=17:55:13, max mem: 15.9 GB 
[10/24 20:46:20 visual_prompt]: Epoch 5 / 100: avg data time: 4.67e-03, avg batch time: 0.6330, average train loss: 45.6176
[10/24 20:47:11 visual_prompt]: 	Test 100/123. loss: 76.197, 0.2455 s / batch. (data: 2.96e-05)max mem: 15.91075 GB 
[10/24 20:47:21 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2327, average loss: 69.0951
[10/24 20:47:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.59	
[10/24 20:47:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/24 20:48:26 visual_prompt]: 	Training 100/1106. train loss: 186.7173,	0.6206 s / batch. (data: 3.39e-04). ETA=18:05:48, max mem: 15.9 GB 
[10/24 20:49:30 visual_prompt]: 	Training 200/1106. train loss: 211.7769,	0.6188 s / batch. (data: 3.16e-04). ETA=18:01:31, max mem: 15.9 GB 
[10/24 20:50:33 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6098 s / batch. (data: 8.67e-04). ETA=17:44:45, max mem: 15.9 GB 
[10/24 20:51:35 visual_prompt]: 	Training 400/1106. train loss: 83.2867,	0.6471 s / batch. (data: 7.62e-04). ETA=18:48:50, max mem: 15.9 GB 
[10/24 20:52:38 visual_prompt]: 	Training 500/1106. train loss: 110.1973,	0.6239 s / batch. (data: 8.20e-04). ETA=18:07:26, max mem: 15.9 GB 
[10/24 20:53:41 visual_prompt]: 	Training 600/1106. train loss: 22.2043,	0.6205 s / batch. (data: 4.62e-04). ETA=18:00:27, max mem: 15.9 GB 
[10/24 20:54:44 visual_prompt]: 	Training 700/1106. train loss: 141.3908,	0.6136 s / batch. (data: 3.10e-04). ETA=17:47:25, max mem: 15.9 GB 
[10/24 20:55:48 visual_prompt]: 	Training 800/1106. train loss: 182.8361,	0.6257 s / batch. (data: 1.26e-02). ETA=18:07:21, max mem: 15.9 GB 
[10/24 20:56:51 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6200 s / batch. (data: 2.64e-04). ETA=17:56:30, max mem: 15.9 GB 
[10/24 20:57:54 visual_prompt]: 	Training 1000/1106. train loss: 7.8574,	0.6381 s / batch. (data: 7.97e-04). ETA=18:26:45, max mem: 15.9 GB 
[10/24 20:58:59 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6193 s / batch. (data: 1.98e-04). ETA=17:53:07, max mem: 15.9 GB 
[10/24 20:59:03 visual_prompt]: Epoch 6 / 100: avg data time: 6.15e-03, avg batch time: 0.6340, average train loss: 70.1587
[10/24 20:59:53 visual_prompt]: 	Test 100/123. loss: 66.694, 0.2478 s / batch. (data: 2.74e-05)max mem: 15.91075 GB 
[10/24 21:00:03 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2321, average loss: 61.4782
[10/24 21:00:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.37	
[10/24 21:00:03 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/24 21:01:09 visual_prompt]: 	Training 100/1106. train loss: 4.4517,	0.6227 s / batch. (data: 2.81e-04). ETA=17:57:56, max mem: 15.9 GB 
[10/24 21:02:12 visual_prompt]: 	Training 200/1106. train loss: 247.0433,	0.6346 s / batch. (data: 1.66e-02). ETA=18:17:30, max mem: 15.9 GB 
[10/24 21:03:15 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6447 s / batch. (data: 2.76e-02). ETA=18:33:49, max mem: 15.9 GB 
[10/24 21:04:18 visual_prompt]: 	Training 400/1106. train loss: 203.2740,	0.6152 s / batch. (data: 3.11e-04). ETA=17:41:57, max mem: 15.9 GB 
[10/24 21:05:21 visual_prompt]: 	Training 500/1106. train loss: 6.3257,	0.6176 s / batch. (data: 3.13e-04). ETA=17:45:04, max mem: 15.9 GB 
[10/24 21:06:24 visual_prompt]: 	Training 600/1106. train loss: 236.7452,	0.6205 s / batch. (data: 3.03e-04). ETA=17:49:00, max mem: 15.9 GB 
[10/24 21:07:27 visual_prompt]: 	Training 700/1106. train loss: 327.9857,	0.6457 s / batch. (data: 1.20e-02). ETA=18:31:13, max mem: 15.9 GB 
[10/24 21:08:31 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	2.1485 s / batch. (data: 1.54e+00). ETA=2 days, 13:34:09, max mem: 15.9 GB 
[10/24 21:09:40 visual_prompt]: 	Training 900/1106. train loss: 85.0776,	0.6271 s / batch. (data: 2.78e-04). ETA=17:57:11, max mem: 15.9 GB 
[10/24 21:10:43 visual_prompt]: 	Training 1000/1106. train loss: 0.4185,	0.6316 s / batch. (data: 1.40e-02). ETA=18:03:50, max mem: 15.9 GB 
[10/24 21:11:46 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6076 s / batch. (data: 1.45e-04). ETA=17:21:43, max mem: 15.9 GB 
[10/24 21:11:49 visual_prompt]: Epoch 7 / 100: avg data time: 1.15e-02, avg batch time: 0.6381, average train loss: 74.8636
[10/24 21:12:41 visual_prompt]: 	Test 100/123. loss: 67.111, 0.2309 s / batch. (data: 3.22e-05)max mem: 15.91075 GB 
[10/24 21:12:52 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2329, average loss: 60.5444
[10/24 21:12:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.39	
[10/24 21:12:52 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/24 21:13:57 visual_prompt]: 	Training 100/1106. train loss: 411.3835,	0.6180 s / batch. (data: 8.02e-04). ETA=17:38:28, max mem: 15.9 GB 
[10/24 21:15:00 visual_prompt]: 	Training 200/1106. train loss: 57.7175,	0.6134 s / batch. (data: 2.74e-04). ETA=17:29:25, max mem: 15.9 GB 
[10/24 21:16:03 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6400 s / batch. (data: 8.31e-04). ETA=18:13:57, max mem: 15.9 GB 
[10/24 21:17:06 visual_prompt]: 	Training 400/1106. train loss: 52.0988,	0.6193 s / batch. (data: 2.53e-04). ETA=17:37:31, max mem: 15.9 GB 
[10/24 21:18:09 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6274 s / batch. (data: 1.16e-03). ETA=17:50:18, max mem: 15.9 GB 
[10/24 21:19:12 visual_prompt]: 	Training 600/1106. train loss: 265.1094,	0.6383 s / batch. (data: 3.33e-04). ETA=18:07:50, max mem: 15.9 GB 
[10/24 21:20:15 visual_prompt]: 	Training 700/1106. train loss: 65.2296,	0.6267 s / batch. (data: 8.13e-04). ETA=17:47:06, max mem: 15.9 GB 
[10/24 21:21:18 visual_prompt]: 	Training 800/1106. train loss: 155.6464,	0.6190 s / batch. (data: 3.14e-04). ETA=17:32:51, max mem: 15.9 GB 
[10/24 21:22:21 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6153 s / batch. (data: 1.25e-03). ETA=17:25:30, max mem: 15.9 GB 
[10/24 21:23:24 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6329 s / batch. (data: 3.21e-04). ETA=17:54:28, max mem: 15.9 GB 
[10/24 21:24:27 visual_prompt]: 	Training 1100/1106. train loss: 75.3786,	0.6181 s / batch. (data: 1.67e-04). ETA=17:28:16, max mem: 15.9 GB 
[10/24 21:24:30 visual_prompt]: Epoch 8 / 100: avg data time: 5.00e-03, avg batch time: 0.6314, average train loss: 85.4984
[10/24 21:25:20 visual_prompt]: 	Test 100/123. loss: 183.931, 0.2249 s / batch. (data: 2.65e-05)max mem: 15.91075 GB 
[10/24 21:25:31 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.2323, average loss: 165.3182
[10/24 21:25:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.84	
[10/24 21:25:31 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/24 21:26:36 visual_prompt]: 	Training 100/1106. train loss: 34.7268,	0.6184 s / batch. (data: 3.13e-04). ETA=17:27:46, max mem: 15.9 GB 
[10/24 21:27:39 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6280 s / batch. (data: 7.99e-03). ETA=17:42:50, max mem: 15.9 GB 
[10/24 21:28:42 visual_prompt]: 	Training 300/1106. train loss: 322.7923,	0.6138 s / batch. (data: 3.06e-04). ETA=17:17:46, max mem: 15.9 GB 
[10/24 21:29:45 visual_prompt]: 	Training 400/1106. train loss: 317.5252,	0.6440 s / batch. (data: 7.59e-04). ETA=18:07:54, max mem: 15.9 GB 
[10/24 21:30:48 visual_prompt]: 	Training 500/1106. train loss: 106.2589,	0.6480 s / batch. (data: 8.61e-04). ETA=18:13:30, max mem: 15.9 GB 
[10/24 21:31:51 visual_prompt]: 	Training 600/1106. train loss: 118.7958,	0.6320 s / batch. (data: 3.80e-04). ETA=17:45:23, max mem: 15.9 GB 
[10/24 21:32:54 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6088 s / batch. (data: 3.06e-04). ETA=17:05:15, max mem: 15.9 GB 
[10/24 21:33:57 visual_prompt]: 	Training 800/1106. train loss: 78.0836,	0.6141 s / batch. (data: 3.23e-04). ETA=17:13:17, max mem: 15.9 GB 
[10/24 21:35:00 visual_prompt]: 	Training 900/1106. train loss: 139.4953,	0.6280 s / batch. (data: 7.28e-04). ETA=17:35:32, max mem: 15.9 GB 
[10/24 21:36:03 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6397 s / batch. (data: 7.56e-04). ETA=17:54:14, max mem: 15.9 GB 
[10/24 21:37:06 visual_prompt]: 	Training 1100/1106. train loss: 53.7882,	0.6187 s / batch. (data: 1.66e-04). ETA=17:17:53, max mem: 15.9 GB 
[10/24 21:37:09 visual_prompt]: Epoch 9 / 100: avg data time: 4.48e-03, avg batch time: 0.6312, average train loss: 93.1130
[10/24 21:37:59 visual_prompt]: 	Test 100/123. loss: 59.600, 0.2276 s / batch. (data: 3.39e-05)max mem: 15.91075 GB 
[10/24 21:38:10 visual_prompt]: Inference (val):avg data time: 1.12e-04, avg batch time: 0.2317, average loss: 53.7956
[10/24 21:38:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.19	
[10/24 21:38:10 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/24 21:39:15 visual_prompt]: 	Training 100/1106. train loss: 150.4392,	0.6123 s / batch. (data: 3.18e-04). ETA=17:06:04, max mem: 15.9 GB 
[10/24 21:40:18 visual_prompt]: 	Training 200/1106. train loss: 98.0129,	0.6271 s / batch. (data: 7.94e-04). ETA=17:29:47, max mem: 15.9 GB 
[10/24 21:41:20 visual_prompt]: 	Training 300/1106. train loss: 93.3190,	0.6228 s / batch. (data: 2.67e-04). ETA=17:21:32, max mem: 15.9 GB 
[10/24 21:42:23 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6199 s / batch. (data: 7.47e-04). ETA=17:15:46, max mem: 15.9 GB 
[10/24 21:43:26 visual_prompt]: 	Training 500/1106. train loss: 19.0002,	0.6360 s / batch. (data: 8.54e-04). ETA=17:41:36, max mem: 15.9 GB 
[10/24 21:44:29 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6197 s / batch. (data: 7.73e-04). ETA=17:13:15, max mem: 15.9 GB 
[10/24 21:45:32 visual_prompt]: 	Training 700/1106. train loss: 575.5989,	0.6188 s / batch. (data: 3.04e-04). ETA=17:10:42, max mem: 15.9 GB 
[10/24 21:46:35 visual_prompt]: 	Training 800/1106. train loss: 46.1705,	0.6360 s / batch. (data: 3.41e-04). ETA=17:38:22, max mem: 15.9 GB 
[10/24 21:47:38 visual_prompt]: 	Training 900/1106. train loss: 97.0456,	0.6400 s / batch. (data: 7.84e-04). ETA=17:43:57, max mem: 15.9 GB 
[10/24 21:48:40 visual_prompt]: 	Training 1000/1106. train loss: 131.3607,	0.6384 s / batch. (data: 3.14e-04). ETA=17:40:09, max mem: 15.9 GB 
[10/24 21:49:43 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6115 s / batch. (data: 1.36e-04). ETA=16:54:31, max mem: 15.9 GB 
[10/24 21:49:47 visual_prompt]: Epoch 10 / 100: avg data time: 4.29e-03, avg batch time: 0.6301, average train loss: 114.5166
[10/24 21:50:37 visual_prompt]: 	Test 100/123. loss: 20.624, 0.2406 s / batch. (data: 4.01e-05)max mem: 15.91075 GB 
[10/24 21:50:48 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2320, average loss: 18.2629
[10/24 21:50:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.43	
[10/24 21:50:48 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/24 21:51:53 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6382 s / batch. (data: 7.82e-04). ETA=17:37:37, max mem: 15.9 GB 
[10/24 21:52:56 visual_prompt]: 	Training 200/1106. train loss: 413.7233,	0.6138 s / batch. (data: 4.07e-04). ETA=16:56:13, max mem: 15.9 GB 
[10/24 21:53:59 visual_prompt]: 	Training 300/1106. train loss: 207.5752,	0.6290 s / batch. (data: 8.18e-04). ETA=17:20:25, max mem: 15.9 GB 
[10/24 21:55:01 visual_prompt]: 	Training 400/1106. train loss: 326.8474,	0.6193 s / batch. (data: 2.93e-04). ETA=17:03:14, max mem: 15.9 GB 
[10/24 21:56:04 visual_prompt]: 	Training 500/1106. train loss: 371.3863,	0.6125 s / batch. (data: 3.68e-04). ETA=16:51:04, max mem: 15.9 GB 
[10/24 21:57:07 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6228 s / batch. (data: 3.29e-04). ETA=17:06:57, max mem: 15.9 GB 
[10/24 21:58:11 visual_prompt]: 	Training 700/1106. train loss: 169.6854,	0.6336 s / batch. (data: 7.69e-04). ETA=17:23:41, max mem: 15.9 GB 
[10/24 21:59:13 visual_prompt]: 	Training 800/1106. train loss: 187.1485,	0.6379 s / batch. (data: 5.84e-03). ETA=17:29:41, max mem: 15.9 GB 
[10/24 22:00:16 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6084 s / batch. (data: 7.67e-04). ETA=16:40:11, max mem: 15.9 GB 
[10/24 22:01:19 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6095 s / batch. (data: 2.79e-04). ETA=16:40:58, max mem: 15.9 GB 
[10/24 22:02:22 visual_prompt]: 	Training 1100/1106. train loss: 48.2975,	0.6140 s / batch. (data: 1.58e-04). ETA=16:47:22, max mem: 15.9 GB 
[10/24 22:02:26 visual_prompt]: Epoch 11 / 100: avg data time: 4.42e-03, avg batch time: 0.6310, average train loss: 128.7926
[10/24 22:03:16 visual_prompt]: 	Test 100/123. loss: 29.050, 0.2257 s / batch. (data: 3.74e-05)max mem: 15.91075 GB 
[10/24 22:03:26 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2314, average loss: 32.2410
[10/24 22:03:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.67	
[10/24 22:03:26 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/24 22:04:32 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6289 s / batch. (data: 1.11e-03). ETA=17:10:43, max mem: 15.9 GB 
[10/24 22:05:35 visual_prompt]: 	Training 200/1106. train loss: 101.0730,	0.6148 s / batch. (data: 3.29e-04). ETA=16:46:31, max mem: 15.9 GB 
[10/24 22:06:38 visual_prompt]: 	Training 300/1106. train loss: 189.7538,	0.6307 s / batch. (data: 1.29e-02). ETA=17:11:30, max mem: 15.9 GB 
[10/24 22:07:41 visual_prompt]: 	Training 400/1106. train loss: 117.6784,	0.6488 s / batch. (data: 3.87e-03). ETA=17:40:05, max mem: 15.9 GB 
[10/24 22:08:43 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6233 s / batch. (data: 8.16e-04). ETA=16:57:24, max mem: 15.9 GB 
[10/24 22:09:46 visual_prompt]: 	Training 600/1106. train loss: 427.4761,	0.6389 s / batch. (data: 3.19e-04). ETA=17:21:48, max mem: 15.9 GB 
[10/24 22:10:49 visual_prompt]: 	Training 700/1106. train loss: 84.8015,	0.6286 s / batch. (data: 2.61e-04). ETA=17:03:53, max mem: 15.9 GB 
[10/24 22:11:52 visual_prompt]: 	Training 800/1106. train loss: 48.3012,	0.6192 s / batch. (data: 3.40e-04). ETA=16:47:39, max mem: 15.9 GB 
[10/24 22:12:55 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6133 s / batch. (data: 5.46e-03). ETA=16:37:00, max mem: 15.9 GB 
[10/24 22:13:57 visual_prompt]: 	Training 1000/1106. train loss: 133.9087,	0.6149 s / batch. (data: 3.38e-04). ETA=16:38:32, max mem: 15.9 GB 
[10/24 22:15:00 visual_prompt]: 	Training 1100/1106. train loss: 32.3823,	0.6179 s / batch. (data: 1.49e-04). ETA=16:42:21, max mem: 15.9 GB 
[10/24 22:15:04 visual_prompt]: Epoch 12 / 100: avg data time: 5.09e-03, avg batch time: 0.6307, average train loss: 130.7792
[10/24 22:15:54 visual_prompt]: 	Test 100/123. loss: 55.282, 0.2477 s / batch. (data: 3.05e-05)max mem: 15.91075 GB 
[10/24 22:16:05 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.2337, average loss: 49.2748
[10/24 22:16:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.84	
[10/24 22:16:05 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/24 22:17:10 visual_prompt]: 	Training 100/1106. train loss: 3.2313,	0.6425 s / batch. (data: 7.97e-04). ETA=17:21:09, max mem: 15.9 GB 
[10/24 22:18:13 visual_prompt]: 	Training 200/1106. train loss: 261.9313,	0.6136 s / batch. (data: 2.53e-04). ETA=16:33:22, max mem: 15.9 GB 
[10/24 22:19:16 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6215 s / batch. (data: 8.18e-04). ETA=16:44:58, max mem: 15.9 GB 
[10/24 22:20:18 visual_prompt]: 	Training 400/1106. train loss: 346.8749,	0.6305 s / batch. (data: 7.75e-04). ETA=16:58:34, max mem: 15.9 GB 
[10/24 22:21:21 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6133 s / batch. (data: 3.23e-04). ETA=16:29:44, max mem: 15.9 GB 
[10/24 22:22:24 visual_prompt]: 	Training 600/1106. train loss: 79.1147,	0.6120 s / batch. (data: 2.96e-04). ETA=16:26:33, max mem: 15.9 GB 
[10/24 22:23:27 visual_prompt]: 	Training 700/1106. train loss: 37.7221,	0.6494 s / batch. (data: 1.34e-02). ETA=17:25:46, max mem: 15.9 GB 
[10/24 22:24:30 visual_prompt]: 	Training 800/1106. train loss: 17.5144,	0.6136 s / batch. (data: 2.90e-04). ETA=16:27:08, max mem: 15.9 GB 
[10/24 22:25:33 visual_prompt]: 	Training 900/1106. train loss: 325.9316,	0.6447 s / batch. (data: 1.13e-03). ETA=17:16:09, max mem: 15.9 GB 
[10/24 22:26:36 visual_prompt]: 	Training 1000/1106. train loss: 197.3320,	0.6324 s / batch. (data: 3.50e-04). ETA=16:55:20, max mem: 15.9 GB 
[10/24 22:27:39 visual_prompt]: 	Training 1100/1106. train loss: 76.2719,	0.6128 s / batch. (data: 1.74e-04). ETA=16:22:46, max mem: 15.9 GB 
[10/24 22:27:43 visual_prompt]: Epoch 13 / 100: avg data time: 4.65e-03, avg batch time: 0.6309, average train loss: 118.1208
[10/24 22:28:33 visual_prompt]: 	Test 100/123. loss: 34.913, 0.2253 s / batch. (data: 3.00e-05)max mem: 15.91075 GB 
[10/24 22:28:43 visual_prompt]: Inference (val):avg data time: 6.37e-05, avg batch time: 0.2323, average loss: 31.6482
[10/24 22:28:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.93	
[10/24 22:28:43 visual_prompt]: Best epoch 13: best metric: -31.648
[10/24 22:28:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/24 22:29:49 visual_prompt]: 	Training 100/1106. train loss: 219.7962,	0.6168 s / batch. (data: 3.28e-04). ETA=16:28:08, max mem: 15.9 GB 
[10/24 22:30:52 visual_prompt]: 	Training 200/1106. train loss: 472.0587,	0.6140 s / batch. (data: 3.25e-04). ETA=16:22:35, max mem: 15.9 GB 
[10/24 22:31:55 visual_prompt]: 	Training 300/1106. train loss: 61.9508,	0.6193 s / batch. (data: 3.39e-04). ETA=16:30:03, max mem: 15.9 GB 
[10/24 22:32:58 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6066 s / batch. (data: 2.85e-04). ETA=16:08:50, max mem: 15.9 GB 
[10/24 22:34:00 visual_prompt]: 	Training 500/1106. train loss: 35.1263,	0.6247 s / batch. (data: 7.94e-04). ETA=16:36:37, max mem: 15.9 GB 
[10/24 22:35:03 visual_prompt]: 	Training 600/1106. train loss: 28.5796,	0.6487 s / batch. (data: 8.44e-04). ETA=17:13:53, max mem: 15.9 GB 
[10/24 22:36:06 visual_prompt]: 	Training 700/1106. train loss: 112.0890,	0.6200 s / batch. (data: 2.96e-04). ETA=16:27:04, max mem: 15.9 GB 
[10/24 22:37:09 visual_prompt]: 	Training 800/1106. train loss: 666.7400,	0.6181 s / batch. (data: 7.39e-04). ETA=16:23:02, max mem: 15.9 GB 
[10/24 22:38:12 visual_prompt]: 	Training 900/1106. train loss: 75.2722,	0.6259 s / batch. (data: 7.49e-04). ETA=16:34:18, max mem: 15.9 GB 
[10/24 22:39:15 visual_prompt]: 	Training 1000/1106. train loss: 519.6763,	0.6320 s / batch. (data: 2.96e-04). ETA=16:43:00, max mem: 15.9 GB 
[10/24 22:40:18 visual_prompt]: 	Training 1100/1106. train loss: 103.5692,	0.6125 s / batch. (data: 1.67e-04). ETA=16:11:00, max mem: 15.9 GB 
[10/24 22:40:21 visual_prompt]: Epoch 14 / 100: avg data time: 4.57e-03, avg batch time: 0.6311, average train loss: 118.1210
[10/24 22:41:11 visual_prompt]: 	Test 100/123. loss: 42.451, 0.2356 s / batch. (data: 3.10e-05)max mem: 15.91075 GB 
[10/24 22:41:22 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2333, average loss: 45.2138
[10/24 22:41:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.10	
[10/24 22:41:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/24 22:42:27 visual_prompt]: 	Training 100/1106. train loss: 748.6628,	0.6404 s / batch. (data: 7.91e-04). ETA=16:54:07, max mem: 15.9 GB 
[10/24 22:43:29 visual_prompt]: 	Training 200/1106. train loss: 49.5659,	0.6302 s / batch. (data: 1.19e-03). ETA=16:36:57, max mem: 15.9 GB 
[10/24 22:44:33 visual_prompt]: 	Training 300/1106. train loss: 268.7130,	0.6413 s / batch. (data: 1.29e-02). ETA=16:53:24, max mem: 15.9 GB 
[10/24 22:45:35 visual_prompt]: 	Training 400/1106. train loss: 533.2347,	0.6324 s / batch. (data: 7.79e-04). ETA=16:38:15, max mem: 15.9 GB 
[10/24 22:46:38 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6122 s / batch. (data: 3.28e-04). ETA=16:05:25, max mem: 15.9 GB 
[10/24 22:47:41 visual_prompt]: 	Training 600/1106. train loss: 14.2885,	0.6402 s / batch. (data: 8.13e-04). ETA=16:48:28, max mem: 15.9 GB 
[10/24 22:48:44 visual_prompt]: 	Training 700/1106. train loss: 71.2274,	0.6358 s / batch. (data: 7.65e-04). ETA=16:40:32, max mem: 15.9 GB 
[10/24 22:49:46 visual_prompt]: 	Training 800/1106. train loss: 113.6589,	0.6414 s / batch. (data: 7.61e-04). ETA=16:48:11, max mem: 15.9 GB 
[10/24 22:50:49 visual_prompt]: 	Training 900/1106. train loss: 198.1887,	0.6217 s / batch. (data: 3.24e-04). ETA=16:16:11, max mem: 15.9 GB 
[10/24 22:51:52 visual_prompt]: 	Training 1000/1106. train loss: 230.6272,	0.6346 s / batch. (data: 7.45e-04). ETA=16:35:27, max mem: 15.9 GB 
[10/24 22:52:55 visual_prompt]: 	Training 1100/1106. train loss: 275.7973,	0.6121 s / batch. (data: 1.75e-04). ETA=15:59:08, max mem: 15.9 GB 
[10/24 22:52:59 visual_prompt]: Epoch 15 / 100: avg data time: 4.11e-03, avg batch time: 0.6300, average train loss: 126.5842
[10/24 22:53:49 visual_prompt]: 	Test 100/123. loss: 81.379, 0.2357 s / batch. (data: 2.79e-05)max mem: 15.91075 GB 
[10/24 22:53:59 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2333, average loss: 88.6188
[10/24 22:53:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.70	
[10/24 22:53:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/24 22:55:04 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6277 s / batch. (data: 8.68e-04). ETA=16:22:24, max mem: 15.9 GB 
[10/24 22:56:07 visual_prompt]: 	Training 200/1106. train loss: 75.5150,	0.6268 s / batch. (data: 8.77e-04). ETA=16:20:01, max mem: 15.9 GB 
[10/24 22:57:10 visual_prompt]: 	Training 300/1106. train loss: 150.9041,	0.6195 s / batch. (data: 3.11e-04). ETA=16:07:37, max mem: 15.9 GB 
[10/24 22:58:13 visual_prompt]: 	Training 400/1106. train loss: 129.7024,	0.6375 s / batch. (data: 8.04e-04). ETA=16:34:38, max mem: 15.9 GB 
[10/24 22:59:16 visual_prompt]: 	Training 500/1106. train loss: 401.7729,	0.6248 s / batch. (data: 3.38e-04). ETA=16:13:45, max mem: 15.9 GB 
[10/24 23:00:19 visual_prompt]: 	Training 600/1106. train loss: 97.2826,	0.6327 s / batch. (data: 7.95e-04). ETA=16:25:03, max mem: 15.9 GB 
[10/24 23:01:22 visual_prompt]: 	Training 700/1106. train loss: 150.0333,	0.6264 s / batch. (data: 7.36e-04). ETA=16:14:08, max mem: 15.9 GB 
[10/24 23:02:25 visual_prompt]: 	Training 800/1106. train loss: 143.8675,	0.6127 s / batch. (data: 2.97e-04). ETA=15:51:54, max mem: 15.9 GB 
[10/24 23:03:28 visual_prompt]: 	Training 900/1106. train loss: 47.7978,	0.6301 s / batch. (data: 8.18e-04). ETA=16:17:45, max mem: 15.9 GB 
[10/24 23:04:31 visual_prompt]: 	Training 1000/1106. train loss: 83.0511,	0.6555 s / batch. (data: 9.11e-04). ETA=16:56:04, max mem: 15.9 GB 
[10/24 23:05:34 visual_prompt]: 	Training 1100/1106. train loss: 185.8646,	0.6174 s / batch. (data: 1.68e-04). ETA=15:56:06, max mem: 15.9 GB 
[10/24 23:05:37 visual_prompt]: Epoch 16 / 100: avg data time: 4.40e-03, avg batch time: 0.6310, average train loss: 124.3782
[10/24 23:06:28 visual_prompt]: 	Test 100/123. loss: 116.885, 0.2255 s / batch. (data: 5.41e-05)max mem: 15.91075 GB 
[10/24 23:06:38 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2321, average loss: 124.4265
[10/24 23:06:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.98	
[10/24 23:06:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/24 23:07:44 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6166 s / batch. (data: 3.01e-04). ETA=15:53:44, max mem: 15.9 GB 
[10/24 23:08:47 visual_prompt]: 	Training 200/1106. train loss: 185.1731,	0.6279 s / batch. (data: 7.95e-04). ETA=16:10:05, max mem: 15.9 GB 
[10/24 23:09:49 visual_prompt]: 	Training 300/1106. train loss: 546.8869,	0.6370 s / batch. (data: 8.22e-04). ETA=16:23:07, max mem: 15.9 GB 
[10/24 23:10:52 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6641 s / batch. (data: 2.80e-02). ETA=17:03:47, max mem: 15.9 GB 
[10/24 23:11:55 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6327 s / batch. (data: 7.94e-04). ETA=16:14:22, max mem: 15.9 GB 
[10/24 23:12:58 visual_prompt]: 	Training 600/1106. train loss: 332.0647,	0.6257 s / batch. (data: 3.44e-04). ETA=16:02:38, max mem: 15.9 GB 
[10/24 23:14:01 visual_prompt]: 	Training 700/1106. train loss: 171.4584,	0.6387 s / batch. (data: 7.90e-04). ETA=16:21:31, max mem: 15.9 GB 
[10/24 23:15:04 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6227 s / batch. (data: 7.85e-04). ETA=15:55:55, max mem: 15.9 GB 
[10/24 23:16:07 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6120 s / batch. (data: 2.72e-04). ETA=15:38:22, max mem: 15.9 GB 
[10/24 23:17:10 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6198 s / batch. (data: 3.21e-04). ETA=15:49:22, max mem: 15.9 GB 
[10/24 23:18:13 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6076 s / batch. (data: 1.50e-04). ETA=15:29:43, max mem: 15.9 GB 
[10/24 23:18:17 visual_prompt]: Epoch 17 / 100: avg data time: 4.49e-03, avg batch time: 0.6314, average train loss: 123.7055
[10/24 23:19:07 visual_prompt]: 	Test 100/123. loss: 27.243, 0.2438 s / batch. (data: 3.03e-05)max mem: 15.91075 GB 
[10/24 23:19:20 visual_prompt]: Inference (val):avg data time: 2.28e-04, avg batch time: 0.2335, average loss: 25.3818
[10/24 23:19:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.45	
[10/24 23:19:20 visual_prompt]: Best epoch 17: best metric: -25.382
[10/24 23:19:20 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/24 23:20:26 visual_prompt]: 	Training 100/1106. train loss: 501.4211,	0.6421 s / batch. (data: 7.51e-04). ETA=16:21:15, max mem: 15.9 GB 
[10/24 23:21:29 visual_prompt]: 	Training 200/1106. train loss: 195.5617,	0.6284 s / batch. (data: 2.89e-04). ETA=15:59:23, max mem: 15.9 GB 
[10/24 23:22:32 visual_prompt]: 	Training 300/1106. train loss: 643.8955,	0.6255 s / batch. (data: 3.28e-04). ETA=15:53:54, max mem: 15.9 GB 
[10/24 23:23:34 visual_prompt]: 	Training 400/1106. train loss: 165.2491,	0.6295 s / batch. (data: 7.51e-04). ETA=15:58:52, max mem: 15.9 GB 
[10/24 23:24:38 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6253 s / batch. (data: 9.13e-04). ETA=15:51:31, max mem: 15.9 GB 
[10/24 23:25:40 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6278 s / batch. (data: 7.96e-04). ETA=15:54:12, max mem: 15.9 GB 
[10/24 23:26:43 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6255 s / batch. (data: 7.82e-04). ETA=15:49:38, max mem: 15.9 GB 
[10/24 23:27:46 visual_prompt]: 	Training 800/1106. train loss: 155.6706,	0.6258 s / batch. (data: 8.08e-04). ETA=15:49:08, max mem: 15.9 GB 
[10/24 23:28:49 visual_prompt]: 	Training 900/1106. train loss: 358.0078,	0.6259 s / batch. (data: 8.09e-04). ETA=15:48:09, max mem: 15.9 GB 
[10/24 23:29:52 visual_prompt]: 	Training 1000/1106. train loss: 98.3634,	0.6140 s / batch. (data: 3.08e-04). ETA=15:29:13, max mem: 15.9 GB 
[10/24 23:30:54 visual_prompt]: 	Training 1100/1106. train loss: 91.5761,	0.6178 s / batch. (data: 1.48e-04). ETA=15:33:55, max mem: 15.9 GB 
[10/24 23:30:58 visual_prompt]: Epoch 18 / 100: avg data time: 5.27e-03, avg batch time: 0.6310, average train loss: 117.5346
[10/24 23:31:48 visual_prompt]: 	Test 100/123. loss: 43.310, 0.2353 s / batch. (data: 2.77e-05)max mem: 15.91075 GB 
[10/24 23:31:58 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.2332, average loss: 48.6654
[10/24 23:31:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.86	
[10/24 23:31:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[10/24 23:33:03 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6333 s / batch. (data: 8.16e-04). ETA=15:56:11, max mem: 15.9 GB 
[10/24 23:34:06 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6282 s / batch. (data: 1.60e-02). ETA=15:47:28, max mem: 15.9 GB 
[10/24 23:35:09 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6440 s / batch. (data: 7.74e-04). ETA=16:10:10, max mem: 15.9 GB 
[10/24 23:36:12 visual_prompt]: 	Training 400/1106. train loss: 88.1939,	0.6254 s / batch. (data: 3.24e-04). ETA=15:41:07, max mem: 15.9 GB 
[10/24 23:37:15 visual_prompt]: 	Training 500/1106. train loss: 51.7091,	0.6322 s / batch. (data: 7.90e-04). ETA=15:50:16, max mem: 15.9 GB 
[10/24 23:38:18 visual_prompt]: 	Training 600/1106. train loss: 765.3746,	0.6192 s / batch. (data: 4.28e-04). ETA=15:29:48, max mem: 15.9 GB 
[10/24 23:39:33 visual_prompt]: 	Training 700/1106. train loss: 4.8585,	0.6182 s / batch. (data: 3.05e-04). ETA=15:27:14, max mem: 15.9 GB 
[10/24 23:40:36 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6075 s / batch. (data: 3.25e-04). ETA=15:10:08, max mem: 15.9 GB 
[10/24 23:41:39 visual_prompt]: 	Training 900/1106. train loss: 74.2979,	0.6179 s / batch. (data: 3.15e-04). ETA=15:24:43, max mem: 15.9 GB 
[10/24 23:42:42 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6318 s / batch. (data: 8.09e-04). ETA=15:44:29, max mem: 15.9 GB 
[10/24 23:43:45 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6178 s / batch. (data: 1.96e-04). ETA=15:22:28, max mem: 15.9 GB 
[10/24 23:43:49 visual_prompt]: Epoch 19 / 100: avg data time: 1.64e-02, avg batch time: 0.6420, average train loss: 122.0435
[10/24 23:44:41 visual_prompt]: 	Test 100/123. loss: 70.870, 0.2250 s / batch. (data: 3.58e-05)max mem: 15.91075 GB 
[10/24 23:44:52 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2310, average loss: 65.1144
[10/24 23:44:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.71	
[10/24 23:44:53 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[10/24 23:45:58 visual_prompt]: 	Training 100/1106. train loss: 32.3897,	0.6560 s / batch. (data: 7.92e-04). ETA=16:18:18, max mem: 15.9 GB 
[10/24 23:47:01 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6203 s / batch. (data: 3.01e-03). ETA=15:24:03, max mem: 15.9 GB 
[10/24 23:48:04 visual_prompt]: 	Training 300/1106. train loss: 20.2761,	0.6276 s / batch. (data: 7.97e-03). ETA=15:33:56, max mem: 15.9 GB 
[10/24 23:49:07 visual_prompt]: 	Training 400/1106. train loss: 158.0844,	0.6596 s / batch. (data: 4.06e-02). ETA=16:20:27, max mem: 15.9 GB 
[10/24 23:50:11 visual_prompt]: 	Training 500/1106. train loss: 73.9700,	0.6452 s / batch. (data: 1.71e-02). ETA=15:57:53, max mem: 15.9 GB 
[10/24 23:51:14 visual_prompt]: 	Training 600/1106. train loss: 569.4030,	0.6352 s / batch. (data: 1.57e-02). ETA=15:42:00, max mem: 15.9 GB 
[10/24 23:52:17 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6584 s / batch. (data: 3.84e-02). ETA=16:15:19, max mem: 15.9 GB 
[10/24 23:53:20 visual_prompt]: 	Training 800/1106. train loss: 75.9686,	0.6323 s / batch. (data: 1.20e-03). ETA=15:35:38, max mem: 15.9 GB 
[10/24 23:54:22 visual_prompt]: 	Training 900/1106. train loss: 324.3801,	0.6613 s / batch. (data: 7.99e-04). ETA=16:17:31, max mem: 15.9 GB 
[10/24 23:55:25 visual_prompt]: 	Training 1000/1106. train loss: 682.7183,	0.6439 s / batch. (data: 1.68e-02). ETA=15:50:37, max mem: 15.9 GB 
[10/24 23:56:28 visual_prompt]: 	Training 1100/1106. train loss: 0.6427,	0.6132 s / batch. (data: 1.63e-04). ETA=15:04:17, max mem: 15.9 GB 
[10/24 23:56:32 visual_prompt]: Epoch 20 / 100: avg data time: 5.42e-03, avg batch time: 0.6324, average train loss: 126.1492
[10/24 23:57:22 visual_prompt]: 	Test 100/123. loss: 60.593, 0.2392 s / batch. (data: 2.98e-05)max mem: 15.91075 GB 
[10/24 23:57:32 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.2321, average loss: 67.1120
[10/24 23:57:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.66	
[10/24 23:57:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[10/24 23:58:38 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6465 s / batch. (data: 5.90e-03). ETA=15:52:18, max mem: 15.9 GB 
[10/24 23:59:41 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6345 s / batch. (data: 1.06e-02). ETA=15:33:34, max mem: 15.9 GB 
[10/25 00:00:44 visual_prompt]: 	Training 300/1106. train loss: 489.8066,	0.6370 s / batch. (data: 9.07e-03). ETA=15:36:13, max mem: 15.9 GB 
[10/25 00:01:47 visual_prompt]: 	Training 400/1106. train loss: 34.3874,	0.6612 s / batch. (data: 8.50e-04). ETA=16:10:35, max mem: 15.9 GB 
[10/25 00:02:50 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6139 s / batch. (data: 4.39e-04). ETA=15:00:07, max mem: 15.9 GB 
[10/25 00:03:53 visual_prompt]: 	Training 600/1106. train loss: 1.4701,	0.6178 s / batch. (data: 3.34e-04). ETA=15:04:49, max mem: 15.9 GB 
[10/25 00:04:56 visual_prompt]: 	Training 700/1106. train loss: 62.1117,	0.6271 s / batch. (data: 7.08e-04). ETA=15:17:27, max mem: 15.9 GB 
[10/25 00:05:59 visual_prompt]: 	Training 800/1106. train loss: 476.4156,	0.6400 s / batch. (data: 7.91e-04). ETA=15:35:11, max mem: 15.9 GB 
[10/25 00:07:02 visual_prompt]: 	Training 900/1106. train loss: 71.5161,	0.6480 s / batch. (data: 7.42e-04). ETA=15:45:48, max mem: 15.9 GB 
[10/25 00:08:05 visual_prompt]: 	Training 1000/1106. train loss: 83.7901,	0.6193 s / batch. (data: 3.20e-04). ETA=15:02:56, max mem: 15.9 GB 
[10/25 00:09:09 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6072 s / batch. (data: 1.70e-04). ETA=14:44:19, max mem: 15.9 GB 
[10/25 00:09:12 visual_prompt]: Epoch 21 / 100: avg data time: 4.89e-03, avg batch time: 0.6327, average train loss: 112.3945
[10/25 00:10:05 visual_prompt]: 	Test 100/123. loss: 81.663, 0.2525 s / batch. (data: 7.08e-05)max mem: 15.91075 GB 
[10/25 00:10:21 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2313, average loss: 73.8199
[10/25 00:10:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.70	
[10/25 00:10:21 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[10/25 00:11:26 visual_prompt]: 	Training 100/1106. train loss: 23.1520,	0.6179 s / batch. (data: 7.67e-04). ETA=14:58:43, max mem: 15.9 GB 
[10/25 00:12:29 visual_prompt]: 	Training 200/1106. train loss: 267.4109,	0.6389 s / batch. (data: 8.17e-04). ETA=15:28:17, max mem: 15.9 GB 
[10/25 00:13:32 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6166 s / batch. (data: 2.40e-03). ETA=14:54:47, max mem: 15.9 GB 
[10/25 00:14:35 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6192 s / batch. (data: 3.13e-04). ETA=14:57:35, max mem: 15.9 GB 
[10/25 00:15:38 visual_prompt]: 	Training 500/1106. train loss: 86.5861,	0.6311 s / batch. (data: 3.19e-04). ETA=15:13:47, max mem: 15.9 GB 
[10/25 00:16:41 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6082 s / batch. (data: 4.26e-04). ETA=14:39:38, max mem: 15.9 GB 
[10/25 00:17:44 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6135 s / batch. (data: 3.09e-04). ETA=14:46:18, max mem: 15.9 GB 
[10/25 00:18:47 visual_prompt]: 	Training 800/1106. train loss: 13.3801,	0.6198 s / batch. (data: 3.07e-04). ETA=14:54:14, max mem: 15.9 GB 
[10/25 00:19:50 visual_prompt]: 	Training 900/1106. train loss: 124.8602,	0.6134 s / batch. (data: 2.75e-04). ETA=14:44:04, max mem: 15.9 GB 
[10/25 00:20:54 visual_prompt]: 	Training 1000/1106. train loss: 12.2789,	0.6480 s / batch. (data: 8.02e-04). ETA=15:32:46, max mem: 15.9 GB 
[10/25 00:21:57 visual_prompt]: 	Training 1100/1106. train loss: 134.0816,	0.6188 s / batch. (data: 1.80e-04). ETA=14:49:45, max mem: 15.9 GB 
[10/25 00:22:01 visual_prompt]: Epoch 22 / 100: avg data time: 5.01e-03, avg batch time: 0.6326, average train loss: 97.9287
[10/25 00:22:53 visual_prompt]: 	Test 100/123. loss: 152.176, 0.2515 s / batch. (data: 3.45e-04)max mem: 15.91075 GB 
[10/25 00:23:04 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.2319, average loss: 131.8432
[10/25 00:23:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.33	
[10/25 00:23:04 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[10/25 00:24:11 visual_prompt]: 	Training 100/1106. train loss: 215.5821,	0.6422 s / batch. (data: 1.82e-02). ETA=15:22:15, max mem: 15.9 GB 
[10/25 00:25:14 visual_prompt]: 	Training 200/1106. train loss: 103.7693,	0.6426 s / batch. (data: 8.21e-04). ETA=15:21:49, max mem: 15.9 GB 
[10/25 00:26:17 visual_prompt]: 	Training 300/1106. train loss: 139.8119,	0.6268 s / batch. (data: 7.99e-04). ETA=14:58:02, max mem: 15.9 GB 
[10/25 00:27:20 visual_prompt]: 	Training 400/1106. train loss: 14.2595,	0.6331 s / batch. (data: 8.79e-04). ETA=15:06:05, max mem: 15.9 GB 
[10/25 00:28:23 visual_prompt]: 	Training 500/1106. train loss: 15.0234,	0.6219 s / batch. (data: 3.29e-04). ETA=14:48:55, max mem: 15.9 GB 
[10/25 00:29:26 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6086 s / batch. (data: 3.27e-04). ETA=14:28:58, max mem: 15.9 GB 
[10/25 00:30:29 visual_prompt]: 	Training 700/1106. train loss: 151.3967,	0.6262 s / batch. (data: 8.05e-04). ETA=14:53:01, max mem: 15.9 GB 
[10/25 00:31:32 visual_prompt]: 	Training 800/1106. train loss: 45.9363,	0.6243 s / batch. (data: 3.02e-04). ETA=14:49:18, max mem: 15.9 GB 
[10/25 00:32:35 visual_prompt]: 	Training 900/1106. train loss: 258.1361,	0.6256 s / batch. (data: 3.44e-04). ETA=14:50:07, max mem: 15.9 GB 
[10/25 00:33:38 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6347 s / batch. (data: 8.04e-04). ETA=15:02:03, max mem: 15.9 GB 
[10/25 00:34:41 visual_prompt]: 	Training 1100/1106. train loss: 58.9415,	0.6139 s / batch. (data: 1.51e-04). ETA=14:31:28, max mem: 15.9 GB 
[10/25 00:34:45 visual_prompt]: Epoch 23 / 100: avg data time: 6.11e-03, avg batch time: 0.6335, average train loss: 127.1168
[10/25 00:35:36 visual_prompt]: 	Test 100/123. loss: 38.962, 0.2249 s / batch. (data: 4.36e-05)max mem: 15.91075 GB 
[10/25 00:35:48 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2316, average loss: 43.3219
[10/25 00:35:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.02	
[10/25 00:35:48 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[10/25 00:36:53 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6288 s / batch. (data: 8.26e-04). ETA=14:51:26, max mem: 15.9 GB 
[10/25 00:37:56 visual_prompt]: 	Training 200/1106. train loss: 152.1737,	0.6147 s / batch. (data: 2.94e-04). ETA=14:30:23, max mem: 15.9 GB 
[10/25 00:38:59 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6354 s / batch. (data: 8.83e-04). ETA=14:58:37, max mem: 15.9 GB 
[10/25 00:40:02 visual_prompt]: 	Training 400/1106. train loss: 168.3738,	0.6134 s / batch. (data: 3.39e-04). ETA=14:26:32, max mem: 15.9 GB 
[10/25 00:41:05 visual_prompt]: 	Training 500/1106. train loss: 384.8529,	0.6427 s / batch. (data: 5.89e-03). ETA=15:06:54, max mem: 15.9 GB 
[10/25 00:42:08 visual_prompt]: 	Training 600/1106. train loss: 102.4772,	0.6125 s / batch. (data: 3.11e-04). ETA=14:23:13, max mem: 15.9 GB 
[10/25 00:43:12 visual_prompt]: 	Training 700/1106. train loss: 119.9294,	0.6341 s / batch. (data: 3.16e-04). ETA=14:52:39, max mem: 15.9 GB 
[10/25 00:44:15 visual_prompt]: 	Training 800/1106. train loss: 413.8970,	0.6138 s / batch. (data: 3.02e-04). ETA=14:23:03, max mem: 15.9 GB 
[10/25 00:45:18 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6181 s / batch. (data: 3.20e-04). ETA=14:27:59, max mem: 15.9 GB 
[10/25 00:46:21 visual_prompt]: 	Training 1000/1106. train loss: 72.7657,	0.6280 s / batch. (data: 7.98e-04). ETA=14:40:52, max mem: 15.9 GB 
[10/25 00:47:24 visual_prompt]: 	Training 1100/1106. train loss: 102.5525,	0.6134 s / batch. (data: 1.46e-04). ETA=14:19:19, max mem: 15.9 GB 
[10/25 00:47:28 visual_prompt]: Epoch 24 / 100: avg data time: 5.97e-03, avg batch time: 0.6331, average train loss: 116.0251
[10/25 00:48:19 visual_prompt]: 	Test 100/123. loss: 16.981, 0.2255 s / batch. (data: 4.77e-05)max mem: 15.91075 GB 
[10/25 00:48:30 visual_prompt]: Inference (val):avg data time: 2.60e-04, avg batch time: 0.2319, average loss: 17.9934
[10/25 00:48:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.54	
[10/25 00:48:30 visual_prompt]: Best epoch 24: best metric: -17.993
[10/25 00:48:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[10/25 00:49:36 visual_prompt]: 	Training 100/1106. train loss: 23.8716,	0.6610 s / batch. (data: 3.03e-04). ETA=15:24:54, max mem: 15.9 GB 
[10/25 00:50:39 visual_prompt]: 	Training 200/1106. train loss: 31.8906,	0.6264 s / batch. (data: 3.41e-04). ETA=14:35:25, max mem: 15.9 GB 
[10/25 00:51:42 visual_prompt]: 	Training 300/1106. train loss: 92.5490,	0.6255 s / batch. (data: 3.56e-04). ETA=14:33:07, max mem: 15.9 GB 
[10/25 00:52:45 visual_prompt]: 	Training 400/1106. train loss: 166.5957,	0.6440 s / batch. (data: 2.15e-04). ETA=14:57:51, max mem: 15.9 GB 
[10/25 00:53:48 visual_prompt]: 	Training 500/1106. train loss: 148.8714,	0.6332 s / batch. (data: 8.97e-04). ETA=14:41:46, max mem: 15.9 GB 
[10/25 00:54:51 visual_prompt]: 	Training 600/1106. train loss: 332.8841,	0.6469 s / batch. (data: 8.53e-04). ETA=14:59:51, max mem: 15.9 GB 
[10/25 00:55:54 visual_prompt]: 	Training 700/1106. train loss: 86.6632,	0.6320 s / batch. (data: 2.99e-04). ETA=14:38:00, max mem: 15.9 GB 
[10/25 00:56:57 visual_prompt]: 	Training 800/1106. train loss: 142.9478,	0.6328 s / batch. (data: 3.08e-04). ETA=14:38:01, max mem: 15.9 GB 
[10/25 00:58:00 visual_prompt]: 	Training 900/1106. train loss: 21.2022,	0.6183 s / batch. (data: 2.91e-04). ETA=14:16:57, max mem: 15.9 GB 
[10/25 00:59:03 visual_prompt]: 	Training 1000/1106. train loss: 13.0145,	0.6325 s / batch. (data: 7.89e-04). ETA=14:35:30, max mem: 15.9 GB 
[10/25 01:00:06 visual_prompt]: 	Training 1100/1106. train loss: 451.7282,	0.6179 s / batch. (data: 1.74e-04). ETA=14:14:17, max mem: 15.9 GB 
[10/25 01:00:10 visual_prompt]: Epoch 25 / 100: avg data time: 5.19e-03, avg batch time: 0.6325, average train loss: 108.4790
[10/25 01:01:02 visual_prompt]: 	Test 100/123. loss: 154.785, 0.2317 s / batch. (data: 3.17e-05)max mem: 15.91075 GB 
[10/25 01:01:12 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.2331, average loss: 139.3006
[10/25 01:01:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.59	
[10/25 01:01:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[10/25 01:02:18 visual_prompt]: 	Training 100/1106. train loss: 1.7535,	0.6373 s / batch. (data: 2.86e-04). ETA=14:40:01, max mem: 15.9 GB 
[10/25 01:03:21 visual_prompt]: 	Training 200/1106. train loss: 266.1630,	0.6220 s / batch. (data: 8.51e-04). ETA=14:17:49, max mem: 15.9 GB 
[10/25 01:04:24 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6295 s / batch. (data: 1.30e-02). ETA=14:27:07, max mem: 15.9 GB 
[10/25 01:05:27 visual_prompt]: 	Training 400/1106. train loss: 182.1003,	0.6121 s / batch. (data: 3.11e-04). ETA=14:02:09, max mem: 15.9 GB 
[10/25 01:06:30 visual_prompt]: 	Training 500/1106. train loss: 108.9824,	0.6234 s / batch. (data: 1.05e-02). ETA=14:16:40, max mem: 15.9 GB 
[10/25 01:07:33 visual_prompt]: 	Training 600/1106. train loss: 54.4287,	0.6360 s / batch. (data: 3.37e-04). ETA=14:32:55, max mem: 15.9 GB 
[10/25 01:08:36 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6336 s / batch. (data: 8.44e-04). ETA=14:28:30, max mem: 15.9 GB 
[10/25 01:09:39 visual_prompt]: 	Training 800/1106. train loss: 271.2348,	0.6180 s / batch. (data: 3.09e-04). ETA=14:06:11, max mem: 15.9 GB 
[10/25 01:10:42 visual_prompt]: 	Training 900/1106. train loss: 103.4888,	0.6389 s / batch. (data: 1.03e-03). ETA=14:33:44, max mem: 15.9 GB 
[10/25 01:11:45 visual_prompt]: 	Training 1000/1106. train loss: 28.7736,	0.6362 s / batch. (data: 5.90e-03). ETA=14:28:59, max mem: 15.9 GB 
[10/25 01:12:48 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6165 s / batch. (data: 1.47e-04). ETA=14:01:01, max mem: 15.9 GB 
[10/25 01:12:52 visual_prompt]: Epoch 26 / 100: avg data time: 4.98e-03, avg batch time: 0.6324, average train loss: 115.5925
[10/25 01:13:44 visual_prompt]: 	Test 100/123. loss: 60.473, 0.2398 s / batch. (data: 2.69e-05)max mem: 15.91075 GB 
[10/25 01:13:54 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.2324, average loss: 54.5459
[10/25 01:13:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.52	
[10/25 01:13:54 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[10/25 01:15:00 visual_prompt]: 	Training 100/1106. train loss: 857.1221,	0.6169 s / batch. (data: 3.23e-04). ETA=14:00:27, max mem: 15.9 GB 
[10/25 01:16:03 visual_prompt]: 	Training 200/1106. train loss: 187.2483,	0.6181 s / batch. (data: 7.93e-04). ETA=14:01:07, max mem: 15.9 GB 
[10/25 01:17:06 visual_prompt]: 	Training 300/1106. train loss: 318.2576,	0.6350 s / batch. (data: 8.01e-04). ETA=14:23:01, max mem: 15.9 GB 
[10/25 01:18:09 visual_prompt]: 	Training 400/1106. train loss: 321.5532,	0.6364 s / batch. (data: 3.19e-04). ETA=14:23:50, max mem: 15.9 GB 
[10/25 01:19:12 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6125 s / batch. (data: 5.47e-03). ETA=13:50:21, max mem: 15.9 GB 
[10/25 01:20:15 visual_prompt]: 	Training 600/1106. train loss: 156.3926,	0.6239 s / batch. (data: 3.95e-04). ETA=14:04:51, max mem: 15.9 GB 
[10/25 01:21:18 visual_prompt]: 	Training 700/1106. train loss: 226.0295,	0.6324 s / batch. (data: 8.21e-04). ETA=14:15:12, max mem: 15.9 GB 
[10/25 01:22:21 visual_prompt]: 	Training 800/1106. train loss: 29.4855,	0.6226 s / batch. (data: 3.14e-04). ETA=14:00:54, max mem: 15.9 GB 
[10/25 01:23:23 visual_prompt]: 	Training 900/1106. train loss: 83.2462,	0.6195 s / batch. (data: 8.86e-04). ETA=13:55:43, max mem: 15.9 GB 
[10/25 01:24:26 visual_prompt]: 	Training 1000/1106. train loss: 235.1633,	0.6237 s / batch. (data: 3.42e-04). ETA=14:00:22, max mem: 15.9 GB 
[10/25 01:25:29 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6174 s / batch. (data: 1.96e-04). ETA=13:50:54, max mem: 15.9 GB 
[10/25 01:25:33 visual_prompt]: Epoch 27 / 100: avg data time: 5.31e-03, avg batch time: 0.6317, average train loss: 122.8583
[10/25 01:26:25 visual_prompt]: 	Test 100/123. loss: 207.807, 0.2333 s / batch. (data: 3.98e-05)max mem: 15.91075 GB 
[10/25 01:26:36 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2335, average loss: 230.6631
[10/25 01:26:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.41	
[10/25 01:26:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[10/25 01:27:41 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6068 s / batch. (data: 3.50e-04). ETA=13:35:32, max mem: 15.9 GB 
[10/25 01:28:44 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6224 s / batch. (data: 8.58e-04). ETA=13:55:25, max mem: 15.9 GB 
[10/25 01:29:47 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6202 s / batch. (data: 3.28e-04). ETA=13:51:25, max mem: 15.9 GB 
[10/25 01:30:50 visual_prompt]: 	Training 400/1106. train loss: 138.3862,	0.6480 s / batch. (data: 8.16e-04). ETA=14:27:36, max mem: 15.9 GB 
[10/25 01:31:53 visual_prompt]: 	Training 500/1106. train loss: 11.5823,	0.6203 s / batch. (data: 3.17e-04). ETA=13:49:31, max mem: 15.9 GB 
[10/25 01:32:56 visual_prompt]: 	Training 600/1106. train loss: 167.6088,	0.6592 s / batch. (data: 7.62e-04). ETA=14:40:23, max mem: 15.9 GB 
[10/25 01:33:59 visual_prompt]: 	Training 700/1106. train loss: 97.7232,	0.6324 s / batch. (data: 3.55e-04). ETA=14:03:34, max mem: 15.9 GB 
[10/25 01:35:10 visual_prompt]: 	Training 800/1106. train loss: 81.7778,	0.6315 s / batch. (data: 3.13e-04). ETA=14:01:21, max mem: 15.9 GB 
[10/25 01:36:13 visual_prompt]: 	Training 900/1106. train loss: 33.5665,	0.6193 s / batch. (data: 3.48e-04). ETA=13:44:01, max mem: 15.9 GB 
[10/25 01:37:16 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6455 s / batch. (data: 3.50e-04). ETA=14:17:51, max mem: 15.9 GB 
[10/25 01:38:19 visual_prompt]: 	Training 1100/1106. train loss: 235.6552,	0.6136 s / batch. (data: 1.54e-04). ETA=13:34:22, max mem: 15.9 GB 
[10/25 01:38:23 visual_prompt]: Epoch 28 / 100: avg data time: 1.21e-02, avg batch time: 0.6394, average train loss: 117.4811
[10/25 01:39:14 visual_prompt]: 	Test 100/123. loss: 8.354, 0.2317 s / batch. (data: 2.72e-05)max mem: 15.91075 GB 
[10/25 01:39:25 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.2322, average loss: 7.1980
[10/25 01:39:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.15	
[10/25 01:39:25 visual_prompt]: Best epoch 28: best metric: -7.198
[10/25 01:39:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[10/25 01:40:31 visual_prompt]: 	Training 100/1106. train loss: 53.0760,	0.6400 s / batch. (data: 2.85e-04). ETA=14:08:19, max mem: 15.9 GB 
[10/25 01:41:34 visual_prompt]: 	Training 200/1106. train loss: 51.4524,	0.6640 s / batch. (data: 3.70e-04). ETA=14:39:05, max mem: 15.9 GB 
[10/25 01:42:38 visual_prompt]: 	Training 300/1106. train loss: 29.1055,	0.6182 s / batch. (data: 3.17e-04). ETA=13:37:21, max mem: 15.9 GB 
[10/25 01:43:41 visual_prompt]: 	Training 400/1106. train loss: 90.0214,	0.6239 s / batch. (data: 3.50e-04). ETA=13:43:52, max mem: 15.9 GB 
[10/25 01:44:44 visual_prompt]: 	Training 500/1106. train loss: 21.9607,	0.6417 s / batch. (data: 1.33e-02). ETA=14:06:21, max mem: 15.9 GB 
[10/25 01:45:47 visual_prompt]: 	Training 600/1106. train loss: 108.1535,	0.6200 s / batch. (data: 3.74e-04). ETA=13:36:38, max mem: 15.9 GB 
[10/25 01:46:51 visual_prompt]: 	Training 700/1106. train loss: 98.5215,	0.6360 s / batch. (data: 2.66e-04). ETA=13:56:41, max mem: 15.9 GB 
[10/25 01:47:54 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6323 s / batch. (data: 8.27e-04). ETA=13:50:46, max mem: 15.9 GB 
[10/25 01:48:57 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6269 s / batch. (data: 4.70e-04). ETA=13:42:34, max mem: 15.9 GB 
[10/25 01:50:01 visual_prompt]: 	Training 1000/1106. train loss: 108.1691,	0.6413 s / batch. (data: 5.43e-03). ETA=14:00:23, max mem: 15.9 GB 
[10/25 01:51:04 visual_prompt]: 	Training 1100/1106. train loss: 45.3825,	0.6146 s / batch. (data: 1.68e-04). ETA=13:24:29, max mem: 15.9 GB 
[10/25 01:51:08 visual_prompt]: Epoch 29 / 100: avg data time: 5.51e-03, avg batch time: 0.6351, average train loss: 113.7869
[10/25 01:52:01 visual_prompt]: 	Test 100/123. loss: 128.132, 0.2267 s / batch. (data: 3.53e-05)max mem: 15.91075 GB 
[10/25 01:52:12 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2321, average loss: 115.0742
[10/25 01:52:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.60	
[10/25 01:52:12 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[10/25 01:53:17 visual_prompt]: 	Training 100/1106. train loss: 156.5910,	0.6207 s / batch. (data: 3.17e-04). ETA=13:31:17, max mem: 15.9 GB 
[10/25 01:54:20 visual_prompt]: 	Training 200/1106. train loss: 231.7879,	0.6345 s / batch. (data: 8.16e-04). ETA=13:48:19, max mem: 15.9 GB 
[10/25 01:55:23 visual_prompt]: 	Training 300/1106. train loss: 40.3250,	0.6334 s / batch. (data: 1.34e-02). ETA=13:45:51, max mem: 15.9 GB 
[10/25 01:56:27 visual_prompt]: 	Training 400/1106. train loss: 79.3280,	0.6293 s / batch. (data: 6.95e-04). ETA=13:39:26, max mem: 15.9 GB 
[10/25 01:57:30 visual_prompt]: 	Training 500/1106. train loss: 89.1430,	0.6287 s / batch. (data: 3.27e-04). ETA=13:37:38, max mem: 15.9 GB 
[10/25 01:58:33 visual_prompt]: 	Training 600/1106. train loss: 118.0113,	0.6560 s / batch. (data: 8.04e-03). ETA=14:11:58, max mem: 15.9 GB 
[10/25 01:59:36 visual_prompt]: 	Training 700/1106. train loss: 180.4695,	0.6307 s / batch. (data: 7.52e-04). ETA=13:38:02, max mem: 15.9 GB 
[10/25 02:00:39 visual_prompt]: 	Training 800/1106. train loss: 124.9436,	0.6151 s / batch. (data: 8.16e-04). ETA=13:16:50, max mem: 15.9 GB 
[10/25 02:01:42 visual_prompt]: 	Training 900/1106. train loss: 16.3471,	0.6276 s / batch. (data: 3.46e-04). ETA=13:32:00, max mem: 15.9 GB 
[10/25 02:02:45 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6090 s / batch. (data: 8.21e-04). ETA=13:06:56, max mem: 15.9 GB 
[10/25 02:03:48 visual_prompt]: 	Training 1100/1106. train loss: 329.2981,	0.6186 s / batch. (data: 1.89e-04). ETA=13:18:14, max mem: 15.9 GB 
[10/25 02:03:52 visual_prompt]: Epoch 30 / 100: avg data time: 4.98e-03, avg batch time: 0.6326, average train loss: 109.7843
[10/25 02:04:43 visual_prompt]: 	Test 100/123. loss: 13.077, 0.2263 s / batch. (data: 3.10e-05)max mem: 15.91075 GB 
[10/25 02:04:54 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.2317, average loss: 12.7927
[10/25 02:04:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 46.23	
[10/25 02:04:54 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[10/25 02:06:00 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6278 s / batch. (data: 3.35e-04). ETA=13:29:00, max mem: 15.9 GB 
[10/25 02:07:03 visual_prompt]: 	Training 200/1106. train loss: 343.1135,	0.6419 s / batch. (data: 8.21e-04). ETA=13:46:08, max mem: 15.9 GB 
[10/25 02:08:06 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6339 s / batch. (data: 1.06e-02). ETA=13:34:50, max mem: 15.9 GB 
[10/25 02:09:09 visual_prompt]: 	Training 400/1106. train loss: 300.0972,	0.6324 s / batch. (data: 3.35e-04). ETA=13:31:47, max mem: 15.9 GB 
[10/25 02:10:12 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6260 s / batch. (data: 8.52e-04). ETA=13:22:33, max mem: 15.9 GB 
[10/25 02:11:15 visual_prompt]: 	Training 600/1106. train loss: 94.7254,	0.6320 s / batch. (data: 3.29e-04). ETA=13:29:09, max mem: 15.9 GB 
[10/25 02:12:18 visual_prompt]: 	Training 700/1106. train loss: 1154.0796,	0.6332 s / batch. (data: 8.35e-04). ETA=13:29:37, max mem: 15.9 GB 
[10/25 02:13:21 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6094 s / batch. (data: 3.76e-04). ETA=12:58:15, max mem: 15.9 GB 
[10/25 02:14:24 visual_prompt]: 	Training 900/1106. train loss: 150.9526,	0.6446 s / batch. (data: 8.19e-04). ETA=13:42:02, max mem: 15.9 GB 
[10/25 02:15:27 visual_prompt]: 	Training 1000/1106. train loss: 375.0662,	0.6251 s / batch. (data: 3.08e-04). ETA=13:16:11, max mem: 15.9 GB 
[10/25 02:16:30 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6091 s / batch. (data: 1.49e-04). ETA=12:54:45, max mem: 15.9 GB 
[10/25 02:16:34 visual_prompt]: Epoch 31 / 100: avg data time: 5.17e-03, avg batch time: 0.6328, average train loss: 102.3317
[10/25 02:17:25 visual_prompt]: 	Test 100/123. loss: 49.083, 0.2256 s / batch. (data: 4.01e-05)max mem: 15.91075 GB 
[10/25 02:17:36 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.2330, average loss: 53.3371
[10/25 02:17:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.56	
[10/25 02:17:36 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[10/25 02:18:41 visual_prompt]: 	Training 100/1106. train loss: 126.6069,	0.6201 s / batch. (data: 2.90e-04). ETA=13:07:39, max mem: 15.9 GB 
[10/25 02:19:44 visual_prompt]: 	Training 200/1106. train loss: 170.8268,	0.6333 s / batch. (data: 2.95e-04). ETA=13:23:23, max mem: 15.9 GB 
[10/25 02:20:47 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6121 s / batch. (data: 3.33e-04). ETA=12:55:25, max mem: 15.9 GB 
[10/25 02:21:51 visual_prompt]: 	Training 400/1106. train loss: 80.4923,	0.6369 s / batch. (data: 1.68e-03). ETA=13:25:52, max mem: 15.9 GB 
[10/25 02:22:54 visual_prompt]: 	Training 500/1106. train loss: 24.7565,	0.6431 s / batch. (data: 7.77e-04). ETA=13:32:36, max mem: 15.9 GB 
[10/25 02:23:57 visual_prompt]: 	Training 600/1106. train loss: 15.9651,	0.6341 s / batch. (data: 2.99e-04). ETA=13:20:10, max mem: 15.9 GB 
[10/25 02:25:01 visual_prompt]: 	Training 700/1106. train loss: 345.8220,	0.6219 s / batch. (data: 3.44e-04). ETA=13:03:44, max mem: 15.9 GB 
[10/25 02:26:04 visual_prompt]: 	Training 800/1106. train loss: 86.3265,	0.6555 s / batch. (data: 4.51e-04). ETA=13:45:02, max mem: 15.9 GB 
[10/25 02:27:07 visual_prompt]: 	Training 900/1106. train loss: 0.0003,	0.6319 s / batch. (data: 7.89e-04). ETA=13:14:11, max mem: 15.9 GB 
[10/25 02:28:10 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6240 s / batch. (data: 2.96e-04). ETA=13:03:15, max mem: 15.9 GB 
[10/25 02:29:13 visual_prompt]: 	Training 1100/1106. train loss: 404.3026,	0.6192 s / batch. (data: 1.71e-04). ETA=12:56:15, max mem: 15.9 GB 
[10/25 02:29:16 visual_prompt]: Epoch 32 / 100: avg data time: 5.31e-03, avg batch time: 0.6331, average train loss: 110.8642
[10/25 02:30:08 visual_prompt]: 	Test 100/123. loss: 113.100, 0.2292 s / batch. (data: 5.00e-04)max mem: 15.91075 GB 
[10/25 02:30:19 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.2320, average loss: 124.6308
[10/25 02:30:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.53	
[10/25 02:30:19 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[10/25 02:31:23 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6227 s / batch. (data: 3.32e-04). ETA=12:59:30, max mem: 15.9 GB 
[10/25 02:32:26 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6181 s / batch. (data: 8.08e-04). ETA=12:52:46, max mem: 15.9 GB 
[10/25 02:33:29 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6290 s / batch. (data: 2.66e-04). ETA=13:05:14, max mem: 15.9 GB 
[10/25 02:34:32 visual_prompt]: 	Training 400/1106. train loss: 193.3268,	0.6317 s / batch. (data: 8.01e-04). ETA=13:07:37, max mem: 15.9 GB 
[10/25 02:35:35 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6247 s / batch. (data: 3.03e-04). ETA=12:57:47, max mem: 15.9 GB 
[10/25 02:36:38 visual_prompt]: 	Training 600/1106. train loss: 80.8401,	0.6416 s / batch. (data: 7.95e-04). ETA=13:17:48, max mem: 15.9 GB 
[10/25 02:37:41 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6458 s / batch. (data: 2.91e-04). ETA=13:21:54, max mem: 15.9 GB 
[10/25 02:38:44 visual_prompt]: 	Training 800/1106. train loss: 115.5135,	0.6256 s / batch. (data: 2.99e-04). ETA=12:55:52, max mem: 15.9 GB 
[10/25 02:39:47 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6222 s / batch. (data: 7.93e-04). ETA=12:50:35, max mem: 15.9 GB 
[10/25 02:40:50 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6513 s / batch. (data: 3.46e-02). ETA=13:25:33, max mem: 15.9 GB 
[10/25 02:41:53 visual_prompt]: 	Training 1100/1106. train loss: 2.7556,	0.6191 s / batch. (data: 1.53e-04). ETA=12:44:40, max mem: 15.9 GB 
[10/25 02:41:56 visual_prompt]: Epoch 33 / 100: avg data time: 4.07e-03, avg batch time: 0.6305, average train loss: 100.0059
[10/25 02:42:46 visual_prompt]: 	Test 100/123. loss: 21.729, 0.2257 s / batch. (data: 3.10e-05)max mem: 15.91075 GB 
[10/25 02:42:57 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2323, average loss: 24.7393
[10/25 02:42:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.04	
[10/25 02:42:57 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[10/25 02:44:01 visual_prompt]: 	Training 100/1106. train loss: 106.7115,	0.6380 s / batch. (data: 7.77e-04). ETA=13:06:56, max mem: 15.9 GB 
[10/25 02:45:04 visual_prompt]: 	Training 200/1106. train loss: 230.0456,	0.6346 s / batch. (data: 3.41e-04). ETA=13:01:40, max mem: 15.9 GB 
[10/25 02:46:07 visual_prompt]: 	Training 300/1106. train loss: 179.2769,	0.6414 s / batch. (data: 7.89e-04). ETA=13:08:58, max mem: 15.9 GB 
[10/25 02:47:10 visual_prompt]: 	Training 400/1106. train loss: 72.2988,	0.6440 s / batch. (data: 1.21e-02). ETA=13:11:06, max mem: 15.9 GB 
[10/25 02:48:13 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6091 s / batch. (data: 3.25e-04). ETA=12:27:09, max mem: 15.9 GB 
[10/25 02:49:16 visual_prompt]: 	Training 600/1106. train loss: 66.7859,	0.6599 s / batch. (data: 1.13e-03). ETA=13:28:26, max mem: 15.9 GB 
[10/25 02:50:19 visual_prompt]: 	Training 700/1106. train loss: 222.8192,	0.6137 s / batch. (data: 3.35e-04). ETA=12:30:49, max mem: 15.9 GB 
[10/25 02:51:21 visual_prompt]: 	Training 800/1106. train loss: 338.2643,	0.6197 s / batch. (data: 3.29e-04). ETA=12:37:06, max mem: 15.9 GB 
[10/25 02:52:25 visual_prompt]: 	Training 900/1106. train loss: 7.3981,	0.6202 s / batch. (data: 3.28e-04). ETA=12:36:39, max mem: 15.9 GB 
[10/25 02:53:27 visual_prompt]: 	Training 1000/1106. train loss: 64.3225,	0.6166 s / batch. (data: 2.73e-04). ETA=12:31:17, max mem: 15.9 GB 
[10/25 02:54:30 visual_prompt]: 	Training 1100/1106. train loss: 109.7883,	0.6140 s / batch. (data: 1.59e-04). ETA=12:27:04, max mem: 15.9 GB 
[10/25 02:54:34 visual_prompt]: Epoch 34 / 100: avg data time: 4.23e-03, avg batch time: 0.6306, average train loss: 105.4052
[10/25 02:55:24 visual_prompt]: 	Test 100/123. loss: 73.578, 0.2286 s / batch. (data: 4.34e-05)max mem: 15.91075 GB 
[10/25 02:55:34 visual_prompt]: Inference (val):avg data time: 2.51e-04, avg batch time: 0.2327, average loss: 81.6002
[10/25 02:55:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.97	
[10/25 02:55:34 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[10/25 02:56:40 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6200 s / batch. (data: 5.03e-04). ETA=12:33:17, max mem: 15.9 GB 
[10/25 02:57:42 visual_prompt]: 	Training 200/1106. train loss: 134.1078,	0.6516 s / batch. (data: 8.47e-04). ETA=13:10:31, max mem: 15.9 GB 
[10/25 02:58:46 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6137 s / batch. (data: 3.02e-04). ETA=12:23:35, max mem: 15.9 GB 
[10/25 02:59:49 visual_prompt]: 	Training 400/1106. train loss: 70.0733,	0.6228 s / batch. (data: 2.99e-04). ETA=12:33:36, max mem: 15.9 GB 
[10/25 03:00:51 visual_prompt]: 	Training 500/1106. train loss: 65.4526,	0.6427 s / batch. (data: 5.43e-03). ETA=12:56:30, max mem: 15.9 GB 
[10/25 03:01:54 visual_prompt]: 	Training 600/1106. train loss: 67.1779,	0.6544 s / batch. (data: 5.94e-03). ETA=13:09:33, max mem: 15.9 GB 
[10/25 03:02:58 visual_prompt]: 	Training 700/1106. train loss: 0.0053,	0.6410 s / batch. (data: 7.72e-04). ETA=12:52:21, max mem: 15.9 GB 
[10/25 03:04:01 visual_prompt]: 	Training 800/1106. train loss: 357.8712,	0.6144 s / batch. (data: 3.00e-04). ETA=12:19:19, max mem: 15.9 GB 
[10/25 03:05:04 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6085 s / batch. (data: 3.13e-04). ETA=12:11:12, max mem: 15.9 GB 
[10/25 03:06:06 visual_prompt]: 	Training 1000/1106. train loss: 14.8167,	0.6357 s / batch. (data: 2.87e-04). ETA=12:42:46, max mem: 15.9 GB 
[10/25 03:07:09 visual_prompt]: 	Training 1100/1106. train loss: 159.2403,	0.6185 s / batch. (data: 1.60e-04). ETA=12:21:04, max mem: 15.9 GB 
[10/25 03:07:13 visual_prompt]: Epoch 35 / 100: avg data time: 4.74e-03, avg batch time: 0.6318, average train loss: 100.3226
[10/25 03:08:03 visual_prompt]: 	Test 100/123. loss: 123.932, 0.2317 s / batch. (data: 2.79e-05)max mem: 15.91075 GB 
[10/25 03:08:14 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.2321, average loss: 111.6956
[10/25 03:08:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.19	
[10/25 03:08:14 visual_prompt]: Stopping early.
[10/25 03:08:14 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 03:08:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 03:08:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 03:08:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 03:08:14 visual_prompt]: Training with config:
[10/25 03:08:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr25.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 03:08:14 visual_prompt]: Loading training data...
[10/25 03:08:14 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 03:08:14 visual_prompt]: Loading validation data...
[10/25 03:08:14 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 03:08:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/25 03:08:22 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/25 03:08:22 visual_prompt]: tuned percent:0.522
[10/25 03:08:22 visual_prompt]: Device used for model: 0
[10/25 03:08:22 visual_prompt]: Setting up Evaluator...
[10/25 03:08:22 visual_prompt]: Setting up Trainer...
[10/25 03:08:22 visual_prompt]: 	Setting up the optimizer...
[10/25 03:08:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 03:09:28 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6179 s / batch. (data: 2.76e-04). ETA=18:57:54, max mem: 15.9 GB 
[10/25 03:10:31 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6319 s / batch. (data: 9.16e-04). ETA=19:22:44, max mem: 15.9 GB 
[10/25 03:11:34 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6218 s / batch. (data: 3.29e-04). ETA=19:03:07, max mem: 15.9 GB 
[10/25 03:12:38 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6427 s / batch. (data: 3.09e-04). ETA=19:40:22, max mem: 15.9 GB 
[10/25 03:13:41 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6193 s / batch. (data: 3.22e-04). ETA=18:56:26, max mem: 15.9 GB 
[10/25 03:14:45 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6240 s / batch. (data: 3.24e-04). ETA=19:03:56, max mem: 15.9 GB 
[10/25 03:15:48 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6310 s / batch. (data: 8.77e-03). ETA=19:15:46, max mem: 15.9 GB 
[10/25 03:16:51 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6235 s / batch. (data: 3.68e-04). ETA=19:01:05, max mem: 15.9 GB 
[10/25 03:17:55 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6531 s / batch. (data: 1.10e-02). ETA=19:54:05, max mem: 15.9 GB 
[10/25 03:18:58 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6310 s / batch. (data: 3.28e-04). ETA=19:12:39, max mem: 15.9 GB 
[10/25 03:20:01 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6172 s / batch. (data: 1.49e-04). ETA=18:46:21, max mem: 15.9 GB 
[10/25 03:20:05 visual_prompt]: Epoch 1 / 100: avg data time: 4.72e-03, avg batch time: 0.6354, average train loss: 1.4028
[10/25 03:20:55 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2254 s / batch. (data: 3.79e-05)max mem: 15.91075 GB 
[10/25 03:21:06 visual_prompt]: Inference (val):avg data time: 1.32e-04, avg batch time: 0.2334, average loss: 1.3505
[10/25 03:21:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/25 03:21:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/25 03:22:10 visual_prompt]: 	Training 100/1106. train loss: 60.5930,	0.6288 s / batch. (data: 2.93e-04). ETA=19:06:28, max mem: 15.9 GB 
[10/25 03:23:14 visual_prompt]: 	Training 200/1106. train loss: 16.6184,	0.6321 s / batch. (data: 1.30e-03). ETA=19:11:29, max mem: 15.9 GB 
[10/25 03:24:17 visual_prompt]: 	Training 300/1106. train loss: 0.0002,	0.6344 s / batch. (data: 5.87e-03). ETA=19:14:33, max mem: 15.9 GB 
[10/25 03:25:20 visual_prompt]: 	Training 400/1106. train loss: 0.0116,	0.6311 s / batch. (data: 8.11e-04). ETA=19:07:27, max mem: 15.9 GB 
[10/25 03:26:23 visual_prompt]: 	Training 500/1106. train loss: 59.6030,	0.6221 s / batch. (data: 3.04e-04). ETA=18:50:02, max mem: 15.9 GB 
[10/25 03:27:27 visual_prompt]: 	Training 600/1106. train loss: 2.3879,	0.6233 s / batch. (data: 3.42e-04). ETA=18:51:14, max mem: 15.9 GB 
[10/25 03:28:30 visual_prompt]: 	Training 700/1106. train loss: 0.0009,	0.6353 s / batch. (data: 2.95e-04). ETA=19:11:55, max mem: 15.9 GB 
[10/25 03:29:33 visual_prompt]: 	Training 800/1106. train loss: 8.0687,	0.6734 s / batch. (data: 5.88e-03). ETA=20:19:54, max mem: 15.9 GB 
[10/25 03:30:36 visual_prompt]: 	Training 900/1106. train loss: 1.7011,	0.6327 s / batch. (data: 3.26e-04). ETA=19:05:11, max mem: 15.9 GB 
[10/25 03:31:40 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6226 s / batch. (data: 3.24e-04). ETA=18:45:49, max mem: 15.9 GB 
[10/25 03:32:43 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6180 s / batch. (data: 1.34e-04). ETA=18:36:23, max mem: 15.9 GB 
[10/25 03:32:47 visual_prompt]: Epoch 2 / 100: avg data time: 4.16e-03, avg batch time: 0.6335, average train loss: 14.4283
[10/25 03:33:37 visual_prompt]: 	Test 100/123. loss: 47.039, 0.2319 s / batch. (data: 2.65e-05)max mem: 15.91075 GB 
[10/25 03:33:47 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.2332, average loss: 42.3837
[10/25 03:33:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.32	
[10/25 03:33:47 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/25 03:34:54 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6236 s / batch. (data: 6.25e-04). ETA=18:45:33, max mem: 15.9 GB 
[10/25 03:35:57 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6307 s / batch. (data: 3.20e-04). ETA=18:57:18, max mem: 15.9 GB 
[10/25 03:37:00 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6559 s / batch. (data: 9.73e-03). ETA=19:41:31, max mem: 15.9 GB 
[10/25 03:38:03 visual_prompt]: 	Training 400/1106. train loss: 13.8526,	0.6300 s / batch. (data: 1.30e-02). ETA=18:53:49, max mem: 15.9 GB 
[10/25 03:39:06 visual_prompt]: 	Training 500/1106. train loss: 72.8209,	0.6319 s / batch. (data: 8.33e-04). ETA=18:56:16, max mem: 15.9 GB 
[10/25 03:40:09 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6243 s / batch. (data: 3.18e-04). ETA=18:41:35, max mem: 15.9 GB 
[10/25 03:41:12 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6411 s / batch. (data: 8.27e-04). ETA=19:10:38, max mem: 15.9 GB 
[10/25 03:42:15 visual_prompt]: 	Training 800/1106. train loss: 21.0994,	0.6283 s / batch. (data: 8.16e-04). ETA=18:46:38, max mem: 15.9 GB 
[10/25 03:43:19 visual_prompt]: 	Training 900/1106. train loss: 44.0743,	0.6288 s / batch. (data: 1.05e-02). ETA=18:46:32, max mem: 15.9 GB 
[10/25 03:44:22 visual_prompt]: 	Training 1000/1106. train loss: 20.9386,	0.6194 s / batch. (data: 2.97e-04). ETA=18:28:36, max mem: 15.9 GB 
[10/25 03:45:25 visual_prompt]: 	Training 1100/1106. train loss: 25.6863,	0.6171 s / batch. (data: 1.54e-04). ETA=18:23:27, max mem: 15.9 GB 
[10/25 03:45:29 visual_prompt]: Epoch 3 / 100: avg data time: 5.29e-03, avg batch time: 0.6342, average train loss: 23.2931
[10/25 03:46:19 visual_prompt]: 	Test 100/123. loss: 27.532, 0.2260 s / batch. (data: 4.15e-05)max mem: 15.91075 GB 
[10/25 03:46:29 visual_prompt]: Inference (val):avg data time: 9.37e-05, avg batch time: 0.2331, average loss: 30.3531
[10/25 03:46:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.14	
[10/25 03:46:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/25 03:47:35 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6304 s / batch. (data: 2.83e-04). ETA=18:46:04, max mem: 15.9 GB 
[10/25 03:48:38 visual_prompt]: 	Training 200/1106. train loss: 112.0422,	0.6329 s / batch. (data: 8.46e-04). ETA=18:49:27, max mem: 15.9 GB 
[10/25 03:49:41 visual_prompt]: 	Training 300/1106. train loss: 4.4839,	0.6325 s / batch. (data: 8.37e-04). ETA=18:47:51, max mem: 15.9 GB 
[10/25 03:50:44 visual_prompt]: 	Training 400/1106. train loss: 39.3375,	0.6201 s / batch. (data: 2.03e-03). ETA=18:24:40, max mem: 15.9 GB 
[10/25 03:51:47 visual_prompt]: 	Training 500/1106. train loss: 44.8946,	0.6436 s / batch. (data: 7.94e-04). ETA=19:05:25, max mem: 15.9 GB 
[10/25 03:52:50 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6377 s / batch. (data: 7.86e-04). ETA=18:53:48, max mem: 15.9 GB 
[10/25 03:53:53 visual_prompt]: 	Training 700/1106. train loss: 184.8901,	0.6195 s / batch. (data: 3.26e-04). ETA=18:20:31, max mem: 15.9 GB 
[10/25 03:54:56 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6186 s / batch. (data: 3.18e-04). ETA=18:17:53, max mem: 15.9 GB 
[10/25 03:55:59 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6080 s / batch. (data: 3.17e-04). ETA=17:58:01, max mem: 15.9 GB 
[10/25 03:57:02 visual_prompt]: 	Training 1000/1106. train loss: 0.0001,	0.6396 s / batch. (data: 1.56e-02). ETA=18:52:57, max mem: 15.9 GB 
[10/25 03:58:05 visual_prompt]: 	Training 1100/1106. train loss: 140.5187,	0.6131 s / batch. (data: 1.70e-04). ETA=18:04:57, max mem: 15.9 GB 
[10/25 03:58:09 visual_prompt]: Epoch 4 / 100: avg data time: 4.48e-03, avg batch time: 0.6326, average train loss: 32.5587
[10/25 03:58:59 visual_prompt]: 	Test 100/123. loss: 23.819, 0.2397 s / batch. (data: 3.84e-05)max mem: 15.91075 GB 
[10/25 03:59:10 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.2319, average loss: 26.2890
[10/25 03:59:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.53	
[10/25 03:59:10 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/25 04:00:15 visual_prompt]: 	Training 100/1106. train loss: 20.2781,	0.6425 s / batch. (data: 7.98e-04). ETA=18:55:53, max mem: 15.9 GB 
[10/25 04:01:18 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6301 s / batch. (data: 8.06e-04). ETA=18:32:51, max mem: 15.9 GB 
[10/25 04:02:20 visual_prompt]: 	Training 300/1106. train loss: 84.8374,	0.6250 s / batch. (data: 2.97e-04). ETA=18:22:54, max mem: 15.9 GB 
[10/25 04:03:23 visual_prompt]: 	Training 400/1106. train loss: 61.6705,	0.6278 s / batch. (data: 5.42e-03). ETA=18:26:43, max mem: 15.9 GB 
[10/25 04:04:27 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6291 s / batch. (data: 7.91e-04). ETA=18:28:04, max mem: 15.9 GB 
[10/25 04:05:30 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6226 s / batch. (data: 3.40e-04). ETA=18:15:35, max mem: 15.9 GB 
[10/25 04:06:33 visual_prompt]: 	Training 700/1106. train loss: 14.7386,	0.6330 s / batch. (data: 8.29e-04). ETA=18:32:44, max mem: 15.9 GB 
[10/25 04:07:36 visual_prompt]: 	Training 800/1106. train loss: 22.6587,	0.6190 s / batch. (data: 3.04e-04). ETA=18:07:12, max mem: 15.9 GB 
[10/25 04:08:39 visual_prompt]: 	Training 900/1106. train loss: 60.0040,	0.6412 s / batch. (data: 7.74e-04). ETA=18:45:08, max mem: 15.9 GB 
[10/25 04:09:41 visual_prompt]: 	Training 1000/1106. train loss: 28.5309,	0.6372 s / batch. (data: 5.94e-03). ETA=18:37:00, max mem: 15.9 GB 
[10/25 04:10:44 visual_prompt]: 	Training 1100/1106. train loss: 100.9567,	0.6118 s / batch. (data: 1.88e-04). ETA=17:51:21, max mem: 15.9 GB 
[10/25 04:10:48 visual_prompt]: Epoch 5 / 100: avg data time: 4.04e-03, avg batch time: 0.6312, average train loss: 45.9440
[10/25 04:11:37 visual_prompt]: 	Test 100/123. loss: 198.512, 0.2254 s / batch. (data: 3.43e-05)max mem: 15.91075 GB 
[10/25 04:11:48 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.2328, average loss: 217.5785
[10/25 04:11:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.71	
[10/25 04:11:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/25 04:12:53 visual_prompt]: 	Training 100/1106. train loss: 23.7469,	0.6369 s / batch. (data: 8.73e-04). ETA=18:34:14, max mem: 15.9 GB 
[10/25 04:13:56 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6150 s / batch. (data: 3.32e-04). ETA=17:54:56, max mem: 15.9 GB 
[10/25 04:14:59 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6327 s / batch. (data: 4.42e-04). ETA=18:24:53, max mem: 15.9 GB 
[10/25 04:16:02 visual_prompt]: 	Training 400/1106. train loss: 79.0362,	0.6282 s / batch. (data: 3.12e-04). ETA=18:15:58, max mem: 15.9 GB 
[10/25 04:17:04 visual_prompt]: 	Training 500/1106. train loss: 74.5540,	0.6152 s / batch. (data: 3.14e-04). ETA=17:52:13, max mem: 15.9 GB 
[10/25 04:18:07 visual_prompt]: 	Training 600/1106. train loss: 118.0798,	0.6189 s / batch. (data: 3.09e-04). ETA=17:57:39, max mem: 15.9 GB 
[10/25 04:19:11 visual_prompt]: 	Training 700/1106. train loss: 22.4985,	0.6507 s / batch. (data: 3.13e-04). ETA=18:51:55, max mem: 15.9 GB 
[10/25 04:20:14 visual_prompt]: 	Training 800/1106. train loss: 117.8486,	0.6276 s / batch. (data: 2.56e-04). ETA=18:10:39, max mem: 15.9 GB 
[10/25 04:21:17 visual_prompt]: 	Training 900/1106. train loss: 19.0900,	0.6347 s / batch. (data: 7.87e-04). ETA=18:21:53, max mem: 15.9 GB 
[10/25 04:22:20 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6486 s / batch. (data: 7.90e-04). ETA=18:45:03, max mem: 15.9 GB 
[10/25 04:23:22 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6063 s / batch. (data: 1.52e-04). ETA=17:30:36, max mem: 15.9 GB 
[10/25 04:23:26 visual_prompt]: Epoch 6 / 100: avg data time: 4.10e-03, avg batch time: 0.6311, average train loss: 61.4173
[10/25 04:24:16 visual_prompt]: 	Test 100/123. loss: 167.080, 0.2248 s / batch. (data: 3.36e-05)max mem: 15.91075 GB 
[10/25 04:24:26 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.2330, average loss: 183.2552
[10/25 04:24:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.98	
[10/25 04:24:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/25 04:25:31 visual_prompt]: 	Training 100/1106. train loss: 223.9428,	0.6228 s / batch. (data: 3.22e-04). ETA=17:58:11, max mem: 15.9 GB 
[10/25 04:26:34 visual_prompt]: 	Training 200/1106. train loss: 88.6273,	0.6388 s / batch. (data: 8.07e-04). ETA=18:24:47, max mem: 15.9 GB 
[10/25 04:27:37 visual_prompt]: 	Training 300/1106. train loss: 0.1021,	0.6437 s / batch. (data: 9.53e-04). ETA=18:32:07, max mem: 15.9 GB 
[10/25 04:28:40 visual_prompt]: 	Training 400/1106. train loss: 111.0155,	0.6177 s / batch. (data: 3.22e-04). ETA=17:46:12, max mem: 15.9 GB 
[10/25 04:29:43 visual_prompt]: 	Training 500/1106. train loss: 333.5473,	0.6404 s / batch. (data: 1.05e-02). ETA=18:24:13, max mem: 15.9 GB 
[10/25 04:30:46 visual_prompt]: 	Training 600/1106. train loss: 193.3076,	0.6127 s / batch. (data: 3.02e-04). ETA=17:35:30, max mem: 15.9 GB 
[10/25 04:31:49 visual_prompt]: 	Training 700/1106. train loss: 52.7754,	0.6173 s / batch. (data: 3.19e-04). ETA=17:42:26, max mem: 15.9 GB 
[10/25 04:32:52 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6151 s / batch. (data: 3.41e-04). ETA=17:37:38, max mem: 15.9 GB 
[10/25 04:33:54 visual_prompt]: 	Training 900/1106. train loss: 25.3437,	0.6441 s / batch. (data: 8.03e-04). ETA=18:26:21, max mem: 15.9 GB 
[10/25 04:34:57 visual_prompt]: 	Training 1000/1106. train loss: 41.8918,	0.6404 s / batch. (data: 2.91e-04). ETA=18:18:57, max mem: 15.9 GB 
[10/25 04:36:00 visual_prompt]: 	Training 1100/1106. train loss: 302.6747,	0.6176 s / batch. (data: 1.39e-04). ETA=17:38:53, max mem: 15.9 GB 
[10/25 04:36:04 visual_prompt]: Epoch 7 / 100: avg data time: 4.39e-03, avg batch time: 0.6303, average train loss: 74.9160
[10/25 04:36:53 visual_prompt]: 	Test 100/123. loss: 148.734, 0.2247 s / batch. (data: 3.96e-05)max mem: 15.91075 GB 
[10/25 04:37:04 visual_prompt]: Inference (val):avg data time: 4.47e-05, avg batch time: 0.2315, average loss: 134.4320
[10/25 04:37:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.93	
[10/25 04:37:04 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/25 04:38:09 visual_prompt]: 	Training 100/1106. train loss: 175.4333,	0.6362 s / batch. (data: 9.00e-04). ETA=18:09:31, max mem: 15.9 GB 
[10/25 04:39:12 visual_prompt]: 	Training 200/1106. train loss: 21.3959,	0.6244 s / batch. (data: 3.26e-04). ETA=17:48:17, max mem: 15.9 GB 
[10/25 04:40:15 visual_prompt]: 	Training 300/1106. train loss: 65.3047,	0.6597 s / batch. (data: 7.71e-04). ETA=18:47:38, max mem: 15.9 GB 
[10/25 04:41:18 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6079 s / batch. (data: 2.58e-04). ETA=17:18:01, max mem: 15.9 GB 
[10/25 04:42:20 visual_prompt]: 	Training 500/1106. train loss: 6.0213,	0.6186 s / batch. (data: 3.34e-04). ETA=17:35:19, max mem: 15.9 GB 
[10/25 04:43:23 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6566 s / batch. (data: 7.08e-04). ETA=18:38:59, max mem: 15.9 GB 
[10/25 04:44:26 visual_prompt]: 	Training 700/1106. train loss: 30.4676,	0.6224 s / batch. (data: 5.45e-03). ETA=17:39:40, max mem: 15.9 GB 
[10/25 04:45:29 visual_prompt]: 	Training 800/1106. train loss: 20.3265,	0.6316 s / batch. (data: 8.32e-04). ETA=17:54:24, max mem: 15.9 GB 
[10/25 04:46:31 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6082 s / batch. (data: 3.24e-04). ETA=17:13:28, max mem: 15.9 GB 
[10/25 04:47:34 visual_prompt]: 	Training 1000/1106. train loss: 175.4325,	0.6185 s / batch. (data: 3.30e-04). ETA=17:30:00, max mem: 15.9 GB 
[10/25 04:48:37 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6169 s / batch. (data: 1.62e-04). ETA=17:26:13, max mem: 15.9 GB 
[10/25 04:48:41 visual_prompt]: Epoch 8 / 100: avg data time: 3.84e-03, avg batch time: 0.6302, average train loss: 103.7949
[10/25 04:49:30 visual_prompt]: 	Test 100/123. loss: 0.720, 0.2379 s / batch. (data: 3.46e-05)max mem: 15.91075 GB 
[10/25 04:49:41 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2329, average loss: 0.7051
[10/25 04:49:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.66	
[10/25 04:49:41 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/25 04:50:46 visual_prompt]: 	Training 100/1106. train loss: 309.2645,	0.6325 s / batch. (data: 7.45e-04). ETA=17:51:33, max mem: 15.9 GB 
[10/25 04:51:49 visual_prompt]: 	Training 200/1106. train loss: 10.3709,	0.6535 s / batch. (data: 1.11e-02). ETA=18:26:08, max mem: 15.9 GB 
[10/25 04:52:52 visual_prompt]: 	Training 300/1106. train loss: 120.0977,	0.6269 s / batch. (data: 9.73e-04). ETA=17:40:02, max mem: 15.9 GB 
[10/25 04:53:55 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6424 s / batch. (data: 8.10e-04). ETA=18:05:08, max mem: 15.9 GB 
[10/25 04:54:57 visual_prompt]: 	Training 500/1106. train loss: 420.0358,	0.6205 s / batch. (data: 3.14e-04). ETA=17:27:06, max mem: 15.9 GB 
[10/25 04:56:01 visual_prompt]: 	Training 600/1106. train loss: 69.4152,	0.6275 s / batch. (data: 8.43e-04). ETA=17:37:49, max mem: 15.9 GB 
[10/25 04:57:03 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6203 s / batch. (data: 8.23e-04). ETA=17:24:41, max mem: 15.9 GB 
[10/25 04:58:06 visual_prompt]: 	Training 800/1106. train loss: 15.0666,	0.6343 s / batch. (data: 7.83e-04). ETA=17:47:13, max mem: 15.9 GB 
[10/25 04:59:09 visual_prompt]: 	Training 900/1106. train loss: 49.4939,	0.6728 s / batch. (data: 8.14e-04). ETA=18:50:48, max mem: 15.9 GB 
[10/25 05:00:12 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6205 s / batch. (data: 3.32e-04). ETA=17:21:58, max mem: 15.9 GB 
[10/25 05:01:15 visual_prompt]: 	Training 1100/1106. train loss: 80.0398,	0.6125 s / batch. (data: 1.43e-04). ETA=17:07:27, max mem: 15.9 GB 
[10/25 05:01:19 visual_prompt]: Epoch 9 / 100: avg data time: 4.56e-03, avg batch time: 0.6306, average train loss: 91.8988
[10/25 05:02:08 visual_prompt]: 	Test 100/123. loss: 77.922, 0.2329 s / batch. (data: 3.98e-05)max mem: 15.91075 GB 
[10/25 05:02:19 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2330, average loss: 141.4103
[10/25 05:02:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.53	
[10/25 05:02:19 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/25 05:03:24 visual_prompt]: 	Training 100/1106. train loss: 99.2399,	0.6296 s / batch. (data: 8.34e-04). ETA=17:35:05, max mem: 15.9 GB 
[10/25 05:04:26 visual_prompt]: 	Training 200/1106. train loss: 690.8031,	0.6263 s / batch. (data: 8.13e-04). ETA=17:28:25, max mem: 15.9 GB 
[10/25 05:05:29 visual_prompt]: 	Training 300/1106. train loss: 86.6419,	0.6171 s / batch. (data: 8.08e-04). ETA=17:12:02, max mem: 15.9 GB 
[10/25 05:06:31 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6068 s / batch. (data: 2.92e-04). ETA=16:53:52, max mem: 15.9 GB 
[10/25 05:07:34 visual_prompt]: 	Training 500/1106. train loss: 9.0267,	0.6280 s / batch. (data: 3.08e-04). ETA=17:28:09, max mem: 15.9 GB 
[10/25 05:08:37 visual_prompt]: 	Training 600/1106. train loss: 16.1839,	0.6331 s / batch. (data: 7.83e-04). ETA=17:35:40, max mem: 15.9 GB 
[10/25 05:09:40 visual_prompt]: 	Training 700/1106. train loss: 239.7010,	0.6188 s / batch. (data: 3.26e-04). ETA=17:10:46, max mem: 15.9 GB 
[10/25 05:10:43 visual_prompt]: 	Training 800/1106. train loss: 33.6794,	0.6198 s / batch. (data: 3.20e-04). ETA=17:11:28, max mem: 15.9 GB 
[10/25 05:11:46 visual_prompt]: 	Training 900/1106. train loss: 92.5709,	0.6366 s / batch. (data: 1.66e-02). ETA=17:38:18, max mem: 15.9 GB 
[10/25 05:12:48 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6402 s / batch. (data: 8.38e-04). ETA=17:43:18, max mem: 15.9 GB 
[10/25 05:13:51 visual_prompt]: 	Training 1100/1106. train loss: 258.5326,	0.6176 s / batch. (data: 1.53e-04). ETA=17:04:41, max mem: 15.9 GB 
[10/25 05:13:55 visual_prompt]: Epoch 10 / 100: avg data time: 4.03e-03, avg batch time: 0.6291, average train loss: 120.9362
[10/25 05:14:45 visual_prompt]: 	Test 100/123. loss: 15.414, 0.2276 s / batch. (data: 3.00e-05)max mem: 15.91075 GB 
[10/25 05:14:55 visual_prompt]: Inference (val):avg data time: 5.54e-05, avg batch time: 0.2319, average loss: 16.6054
[10/25 05:14:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.30	
[10/25 05:14:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/25 05:16:00 visual_prompt]: 	Training 100/1106. train loss: 344.0775,	0.6200 s / batch. (data: 7.90e-04). ETA=17:07:30, max mem: 15.9 GB 
[10/25 05:17:03 visual_prompt]: 	Training 200/1106. train loss: 121.8311,	0.6403 s / batch. (data: 8.42e-04). ETA=17:40:02, max mem: 15.9 GB 
[10/25 05:18:06 visual_prompt]: 	Training 300/1106. train loss: 59.4446,	0.6126 s / batch. (data: 3.59e-04). ETA=16:53:09, max mem: 15.9 GB 
[10/25 05:19:09 visual_prompt]: 	Training 400/1106. train loss: 210.7343,	0.6317 s / batch. (data: 8.30e-04). ETA=17:23:45, max mem: 15.9 GB 
[10/25 05:20:12 visual_prompt]: 	Training 500/1106. train loss: 87.5238,	0.6369 s / batch. (data: 8.99e-04). ETA=17:31:14, max mem: 15.9 GB 
[10/25 05:21:15 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6189 s / batch. (data: 7.79e-04). ETA=17:00:30, max mem: 15.9 GB 
[10/25 05:22:18 visual_prompt]: 	Training 700/1106. train loss: 504.9359,	0.6323 s / batch. (data: 7.30e-04). ETA=17:21:39, max mem: 15.9 GB 
[10/25 05:23:21 visual_prompt]: 	Training 800/1106. train loss: 237.0805,	0.6340 s / batch. (data: 4.51e-03). ETA=17:23:23, max mem: 15.9 GB 
[10/25 05:24:23 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6399 s / batch. (data: 8.37e-04). ETA=17:32:03, max mem: 15.9 GB 
[10/25 05:25:26 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6176 s / batch. (data: 3.18e-04). ETA=16:54:13, max mem: 15.9 GB 
[10/25 05:26:29 visual_prompt]: 	Training 1100/1106. train loss: 91.3099,	0.6134 s / batch. (data: 1.97e-04). ETA=16:46:24, max mem: 15.9 GB 
[10/25 05:26:33 visual_prompt]: Epoch 11 / 100: avg data time: 4.78e-03, avg batch time: 0.6307, average train loss: 125.8866
[10/25 05:27:23 visual_prompt]: 	Test 100/123. loss: 74.225, 0.2290 s / batch. (data: 2.79e-05)max mem: 15.91075 GB 
[10/25 05:27:33 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.2315, average loss: 67.2638
[10/25 05:27:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.40	
[10/25 05:27:33 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/25 05:28:39 visual_prompt]: 	Training 100/1106. train loss: 286.1913,	0.6110 s / batch. (data: 2.94e-04). ETA=16:41:24, max mem: 15.9 GB 
[10/25 05:29:42 visual_prompt]: 	Training 200/1106. train loss: 45.6293,	0.6345 s / batch. (data: 7.95e-04). ETA=17:18:50, max mem: 15.9 GB 
[10/25 05:30:45 visual_prompt]: 	Training 300/1106. train loss: 165.1596,	0.6318 s / batch. (data: 3.05e-04). ETA=17:13:20, max mem: 15.9 GB 
[10/25 05:31:47 visual_prompt]: 	Training 400/1106. train loss: 376.8125,	0.6444 s / batch. (data: 2.91e-04). ETA=17:32:56, max mem: 15.9 GB 
[10/25 05:32:50 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6280 s / batch. (data: 8.24e-04). ETA=17:05:03, max mem: 15.9 GB 
[10/25 05:33:53 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6107 s / batch. (data: 4.21e-04). ETA=16:35:45, max mem: 15.9 GB 
[10/25 05:34:56 visual_prompt]: 	Training 700/1106. train loss: 176.7245,	0.6243 s / batch. (data: 9.01e-04). ETA=16:56:59, max mem: 15.9 GB 
[10/25 05:35:59 visual_prompt]: 	Training 800/1106. train loss: 147.5689,	0.6132 s / batch. (data: 3.35e-04). ETA=16:37:48, max mem: 15.9 GB 
[10/25 05:37:01 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6314 s / batch. (data: 8.30e-04). ETA=17:06:27, max mem: 15.9 GB 
[10/25 05:38:04 visual_prompt]: 	Training 1000/1106. train loss: 82.8496,	0.6368 s / batch. (data: 7.93e-04). ETA=17:14:04, max mem: 15.9 GB 
[10/25 05:39:07 visual_prompt]: 	Training 1100/1106. train loss: 509.4506,	0.6125 s / batch. (data: 1.62e-04). ETA=16:33:41, max mem: 15.9 GB 
[10/25 05:39:11 visual_prompt]: Epoch 12 / 100: avg data time: 5.12e-03, avg batch time: 0.6303, average train loss: 129.0668
[10/25 05:40:01 visual_prompt]: 	Test 100/123. loss: 8.788, 0.2245 s / batch. (data: 3.12e-05)max mem: 15.91075 GB 
[10/25 05:40:11 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2308, average loss: 9.8244
[10/25 05:40:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.87	
[10/25 05:40:11 visual_prompt]: Best epoch 12: best metric: -9.824
[10/25 05:40:11 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/25 05:41:16 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6119 s / batch. (data: 3.62e-04). ETA=16:31:30, max mem: 15.9 GB 
[10/25 05:42:19 visual_prompt]: 	Training 200/1106. train loss: 45.0675,	0.6362 s / batch. (data: 7.91e-04). ETA=17:09:51, max mem: 15.9 GB 
[10/25 05:43:22 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6222 s / batch. (data: 7.95e-04). ETA=16:46:07, max mem: 15.9 GB 
[10/25 05:44:25 visual_prompt]: 	Training 400/1106. train loss: 149.4167,	0.6318 s / batch. (data: 8.51e-04). ETA=17:00:37, max mem: 15.9 GB 
[10/25 05:45:27 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6131 s / batch. (data: 2.88e-04). ETA=16:29:27, max mem: 15.9 GB 
[10/25 05:46:30 visual_prompt]: 	Training 600/1106. train loss: 119.6193,	0.6160 s / batch. (data: 3.74e-04). ETA=16:33:01, max mem: 15.9 GB 
[10/25 05:47:33 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6200 s / batch. (data: 2.96e-04). ETA=16:38:28, max mem: 15.9 GB 
[10/25 05:48:36 visual_prompt]: 	Training 800/1106. train loss: 4.7134,	0.6291 s / batch. (data: 2.78e-04). ETA=16:52:08, max mem: 15.9 GB 
[10/25 05:49:39 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6295 s / batch. (data: 7.58e-04). ETA=16:51:42, max mem: 15.9 GB 
[10/25 05:50:42 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6307 s / batch. (data: 8.06e-04). ETA=16:52:35, max mem: 15.9 GB 
[10/25 05:51:45 visual_prompt]: 	Training 1100/1106. train loss: 132.4709,	0.6126 s / batch. (data: 2.07e-04). ETA=16:22:33, max mem: 15.9 GB 
[10/25 05:51:49 visual_prompt]: Epoch 13 / 100: avg data time: 4.69e-03, avg batch time: 0.6304, average train loss: 133.6009
[10/25 05:52:38 visual_prompt]: 	Test 100/123. loss: 4.657, 0.2252 s / batch. (data: 3.05e-05)max mem: 15.91075 GB 
[10/25 05:52:49 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2315, average loss: 5.6895
[10/25 05:52:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.32	
[10/25 05:52:49 visual_prompt]: Best epoch 13: best metric: -5.690
[10/25 05:52:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/25 05:53:54 visual_prompt]: 	Training 100/1106. train loss: 26.5557,	0.6172 s / batch. (data: 3.53e-04). ETA=16:28:51, max mem: 15.9 GB 
[10/25 05:54:57 visual_prompt]: 	Training 200/1106. train loss: 75.8408,	0.6132 s / batch. (data: 3.24e-04). ETA=16:21:16, max mem: 15.9 GB 
[10/25 05:56:00 visual_prompt]: 	Training 300/1106. train loss: 93.5661,	0.6263 s / batch. (data: 3.29e-04). ETA=16:41:16, max mem: 15.9 GB 
[10/25 05:57:03 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6233 s / batch. (data: 9.05e-04). ETA=16:35:26, max mem: 15.9 GB 
[10/25 05:58:06 visual_prompt]: 	Training 500/1106. train loss: 51.6373,	0.6304 s / batch. (data: 8.20e-04). ETA=16:45:40, max mem: 15.9 GB 
[10/25 05:59:09 visual_prompt]: 	Training 600/1106. train loss: 23.8472,	0.6305 s / batch. (data: 3.26e-04). ETA=16:44:46, max mem: 15.9 GB 
[10/25 06:00:12 visual_prompt]: 	Training 700/1106. train loss: 210.4142,	0.6230 s / batch. (data: 3.19e-04). ETA=16:31:46, max mem: 15.9 GB 
[10/25 06:01:14 visual_prompt]: 	Training 800/1106. train loss: 124.1098,	0.6149 s / batch. (data: 2.74e-04). ETA=16:17:50, max mem: 15.9 GB 
[10/25 06:02:17 visual_prompt]: 	Training 900/1106. train loss: 188.7702,	0.6203 s / batch. (data: 3.18e-04). ETA=16:25:28, max mem: 15.9 GB 
[10/25 06:03:20 visual_prompt]: 	Training 1000/1106. train loss: 208.9030,	0.6288 s / batch. (data: 5.43e-03). ETA=16:37:59, max mem: 15.9 GB 
[10/25 06:04:23 visual_prompt]: 	Training 1100/1106. train loss: 17.5410,	0.6189 s / batch. (data: 1.47e-04). ETA=16:21:13, max mem: 15.9 GB 
[10/25 06:04:27 visual_prompt]: Epoch 14 / 100: avg data time: 4.54e-03, avg batch time: 0.6310, average train loss: 114.6824
[10/25 06:05:16 visual_prompt]: 	Test 100/123. loss: 83.529, 0.2254 s / batch. (data: 3.91e-05)max mem: 15.91075 GB 
[10/25 06:05:27 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2325, average loss: 75.0515
[10/25 06:05:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.04	
[10/25 06:05:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/25 06:06:32 visual_prompt]: 	Training 100/1106. train loss: 48.1864,	0.6507 s / batch. (data: 8.67e-04). ETA=17:10:31, max mem: 15.9 GB 
[10/25 06:07:34 visual_prompt]: 	Training 200/1106. train loss: 67.7711,	0.6244 s / batch. (data: 3.46e-04). ETA=16:27:45, max mem: 15.9 GB 
[10/25 06:08:37 visual_prompt]: 	Training 300/1106. train loss: 79.1674,	0.6264 s / batch. (data: 3.18e-04). ETA=16:29:48, max mem: 15.9 GB 
[10/25 06:09:40 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6329 s / batch. (data: 8.21e-04). ETA=16:39:03, max mem: 15.9 GB 
[10/25 06:10:42 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6296 s / batch. (data: 7.91e-04). ETA=16:32:51, max mem: 15.9 GB 
[10/25 06:11:45 visual_prompt]: 	Training 600/1106. train loss: 94.6962,	0.6264 s / batch. (data: 3.23e-04). ETA=16:26:49, max mem: 15.9 GB 
[10/25 06:12:48 visual_prompt]: 	Training 700/1106. train loss: 102.3650,	0.6269 s / batch. (data: 7.43e-04). ETA=16:26:26, max mem: 15.9 GB 
[10/25 06:13:51 visual_prompt]: 	Training 800/1106. train loss: 26.2719,	0.6187 s / batch. (data: 3.39e-04). ETA=16:12:30, max mem: 15.9 GB 
[10/25 06:14:54 visual_prompt]: 	Training 900/1106. train loss: 144.4017,	0.6431 s / batch. (data: 5.47e-03). ETA=16:49:54, max mem: 15.9 GB 
[10/25 06:15:57 visual_prompt]: 	Training 1000/1106. train loss: 55.4786,	0.6482 s / batch. (data: 5.89e-03). ETA=16:56:44, max mem: 15.9 GB 
[10/25 06:17:00 visual_prompt]: 	Training 1100/1106. train loss: 99.6858,	0.6115 s / batch. (data: 1.89e-04). ETA=15:58:09, max mem: 15.9 GB 
[10/25 06:17:03 visual_prompt]: Epoch 15 / 100: avg data time: 4.32e-03, avg batch time: 0.6297, average train loss: 130.5085
[10/25 06:17:53 visual_prompt]: 	Test 100/123. loss: 51.817, 0.2642 s / batch. (data: 4.10e-05)max mem: 15.91075 GB 
[10/25 06:18:04 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.2319, average loss: 46.5546
[10/25 06:18:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.07	
[10/25 06:18:04 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/25 06:19:08 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6328 s / batch. (data: 8.33e-04). ETA=16:30:28, max mem: 15.9 GB 
[10/25 06:20:11 visual_prompt]: 	Training 200/1106. train loss: 200.5121,	0.6259 s / batch. (data: 3.03e-04). ETA=16:18:38, max mem: 15.9 GB 
[10/25 06:21:14 visual_prompt]: 	Training 300/1106. train loss: 255.1569,	0.6174 s / batch. (data: 3.41e-04). ETA=16:04:16, max mem: 15.9 GB 
[10/25 06:22:17 visual_prompt]: 	Training 400/1106. train loss: 159.6145,	0.6372 s / batch. (data: 8.31e-04). ETA=16:34:06, max mem: 15.9 GB 
[10/25 06:23:20 visual_prompt]: 	Training 500/1106. train loss: 110.8159,	0.6254 s / batch. (data: 5.45e-03). ETA=16:14:38, max mem: 15.9 GB 
[10/25 06:24:23 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6231 s / batch. (data: 7.93e-04). ETA=16:10:03, max mem: 15.9 GB 
[10/25 06:25:26 visual_prompt]: 	Training 700/1106. train loss: 80.0893,	0.6355 s / batch. (data: 3.24e-04). ETA=16:28:20, max mem: 15.9 GB 
[10/25 06:26:29 visual_prompt]: 	Training 800/1106. train loss: 133.6008,	0.6320 s / batch. (data: 7.95e-03). ETA=16:21:51, max mem: 15.9 GB 
[10/25 06:27:31 visual_prompt]: 	Training 900/1106. train loss: 57.2924,	0.6315 s / batch. (data: 8.35e-04). ETA=16:19:57, max mem: 15.9 GB 
[10/25 06:28:34 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6073 s / batch. (data: 3.16e-04). ETA=15:41:24, max mem: 15.9 GB 
[10/25 06:29:37 visual_prompt]: 	Training 1100/1106. train loss: 435.4764,	0.6185 s / batch. (data: 1.64e-04). ETA=15:57:42, max mem: 15.9 GB 
[10/25 06:29:41 visual_prompt]: Epoch 16 / 100: avg data time: 4.18e-03, avg batch time: 0.6303, average train loss: 137.7743
[10/25 06:30:30 visual_prompt]: 	Test 100/123. loss: 89.621, 0.2382 s / batch. (data: 2.88e-05)max mem: 15.91075 GB 
[10/25 06:30:41 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2330, average loss: 96.0274
[10/25 06:30:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.38	
[10/25 06:30:41 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/25 06:31:47 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6349 s / batch. (data: 8.04e-04). ETA=16:21:58, max mem: 15.9 GB 
[10/25 06:32:50 visual_prompt]: 	Training 200/1106. train loss: 32.1779,	0.6280 s / batch. (data: 9.97e-04). ETA=16:10:14, max mem: 15.9 GB 
[10/25 06:33:52 visual_prompt]: 	Training 300/1106. train loss: 362.5815,	0.6308 s / batch. (data: 1.20e-02). ETA=16:13:35, max mem: 15.9 GB 
[10/25 06:34:55 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6198 s / batch. (data: 7.16e-04). ETA=15:55:29, max mem: 15.9 GB 
[10/25 06:35:58 visual_prompt]: 	Training 500/1106. train loss: 91.7561,	0.6300 s / batch. (data: 7.66e-04). ETA=16:10:10, max mem: 15.9 GB 
[10/25 06:37:01 visual_prompt]: 	Training 600/1106. train loss: 66.1710,	0.6186 s / batch. (data: 5.51e-03). ETA=15:51:38, max mem: 15.9 GB 
[10/25 06:38:04 visual_prompt]: 	Training 700/1106. train loss: 123.6458,	0.6263 s / batch. (data: 8.30e-04). ETA=16:02:26, max mem: 15.9 GB 
[10/25 06:39:07 visual_prompt]: 	Training 800/1106. train loss: 105.6989,	0.6355 s / batch. (data: 1.64e-02). ETA=16:15:35, max mem: 15.9 GB 
[10/25 06:40:09 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6197 s / batch. (data: 8.06e-04). ETA=15:50:19, max mem: 15.9 GB 
[10/25 06:41:12 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6089 s / batch. (data: 3.48e-04). ETA=15:32:37, max mem: 15.9 GB 
[10/25 06:42:15 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6081 s / batch. (data: 1.40e-04). ETA=15:30:23, max mem: 15.9 GB 
[10/25 06:42:19 visual_prompt]: Epoch 17 / 100: avg data time: 4.81e-03, avg batch time: 0.6308, average train loss: 108.9413
[10/25 06:43:09 visual_prompt]: 	Test 100/123. loss: 15.520, 0.2278 s / batch. (data: 3.15e-05)max mem: 15.91075 GB 
[10/25 06:43:19 visual_prompt]: Inference (val):avg data time: 4.45e-05, avg batch time: 0.2319, average loss: 14.5941
[10/25 06:43:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.49	
[10/25 06:43:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/25 06:44:24 visual_prompt]: 	Training 100/1106. train loss: 365.1511,	0.6157 s / batch. (data: 3.30e-04). ETA=15:40:59, max mem: 15.9 GB 
[10/25 06:45:27 visual_prompt]: 	Training 200/1106. train loss: 237.2891,	0.6430 s / batch. (data: 8.26e-04). ETA=16:21:36, max mem: 15.9 GB 
[10/25 06:46:30 visual_prompt]: 	Training 300/1106. train loss: 882.7492,	0.6509 s / batch. (data: 1.10e-02). ETA=16:32:35, max mem: 15.9 GB 
[10/25 06:47:32 visual_prompt]: 	Training 400/1106. train loss: 247.4934,	0.6139 s / batch. (data: 3.23e-04). ETA=15:35:07, max mem: 15.9 GB 
[10/25 06:48:35 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6259 s / batch. (data: 4.05e-04). ETA=15:52:27, max mem: 15.9 GB 
[10/25 06:49:38 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6207 s / batch. (data: 3.15e-04). ETA=15:43:27, max mem: 15.9 GB 
[10/25 06:50:41 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6178 s / batch. (data: 3.14e-04). ETA=15:37:56, max mem: 15.9 GB 
[10/25 06:51:44 visual_prompt]: 	Training 800/1106. train loss: 57.3580,	0.6304 s / batch. (data: 7.33e-04). ETA=15:56:09, max mem: 15.9 GB 
[10/25 06:52:46 visual_prompt]: 	Training 900/1106. train loss: 24.4496,	0.6530 s / batch. (data: 2.75e-02). ETA=16:29:13, max mem: 15.9 GB 
[10/25 06:53:50 visual_prompt]: 	Training 1000/1106. train loss: 93.4380,	0.6136 s / batch. (data: 3.48e-04). ETA=15:28:34, max mem: 15.9 GB 
[10/25 06:54:52 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6067 s / batch. (data: 1.45e-04). ETA=15:17:05, max mem: 15.9 GB 
[10/25 06:54:56 visual_prompt]: Epoch 18 / 100: avg data time: 4.27e-03, avg batch time: 0.6300, average train loss: 107.1602
[10/25 06:55:46 visual_prompt]: 	Test 100/123. loss: 99.174, 0.2248 s / batch. (data: 3.17e-05)max mem: 15.91075 GB 
[10/25 06:55:57 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.2319, average loss: 88.6203
[10/25 06:55:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.88	
[10/25 06:55:57 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[10/25 06:57:02 visual_prompt]: 	Training 100/1106. train loss: 123.7441,	0.6291 s / batch. (data: 8.16e-04). ETA=15:49:51, max mem: 15.9 GB 
[10/25 06:58:04 visual_prompt]: 	Training 200/1106. train loss: 113.8206,	0.6113 s / batch. (data: 3.75e-04). ETA=15:22:01, max mem: 15.9 GB 
[10/25 06:59:07 visual_prompt]: 	Training 300/1106. train loss: 76.2709,	0.6596 s / batch. (data: 4.20e-02). ETA=16:33:46, max mem: 15.9 GB 
[10/25 07:00:10 visual_prompt]: 	Training 400/1106. train loss: 73.4840,	0.6159 s / batch. (data: 2.93e-04). ETA=15:26:54, max mem: 15.9 GB 
[10/25 07:01:13 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6242 s / batch. (data: 8.12e-04). ETA=15:38:17, max mem: 15.9 GB 
[10/25 07:02:16 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6203 s / batch. (data: 7.83e-04). ETA=15:31:27, max mem: 15.9 GB 
[10/25 07:03:19 visual_prompt]: 	Training 700/1106. train loss: 29.4370,	0.6327 s / batch. (data: 9.58e-04). ETA=15:48:54, max mem: 15.9 GB 
[10/25 07:04:22 visual_prompt]: 	Training 800/1106. train loss: 126.0950,	0.6298 s / batch. (data: 7.74e-04). ETA=15:43:38, max mem: 15.9 GB 
[10/25 07:05:24 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6140 s / batch. (data: 3.10e-04). ETA=15:18:52, max mem: 15.9 GB 
[10/25 07:06:27 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6445 s / batch. (data: 5.91e-03). ETA=16:03:24, max mem: 15.9 GB 
[10/25 07:07:30 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6064 s / batch. (data: 1.86e-04). ETA=15:05:30, max mem: 15.9 GB 
[10/25 07:07:33 visual_prompt]: Epoch 19 / 100: avg data time: 4.62e-03, avg batch time: 0.6301, average train loss: 125.9688
[10/25 07:08:23 visual_prompt]: 	Test 100/123. loss: 70.280, 0.2301 s / batch. (data: 4.08e-05)max mem: 15.91075 GB 
[10/25 07:08:34 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.2322, average loss: 64.5641
[10/25 07:08:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.40	
[10/25 07:08:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[10/25 07:09:39 visual_prompt]: 	Training 100/1106. train loss: 65.5329,	0.6124 s / batch. (data: 3.60e-04). ETA=15:13:23, max mem: 15.9 GB 
[10/25 07:10:42 visual_prompt]: 	Training 200/1106. train loss: 113.4851,	0.6298 s / batch. (data: 8.18e-04). ETA=15:38:15, max mem: 15.9 GB 
[10/25 07:11:45 visual_prompt]: 	Training 300/1106. train loss: 5.8671,	0.6436 s / batch. (data: 9.03e-04). ETA=15:57:45, max mem: 15.9 GB 
[10/25 07:12:48 visual_prompt]: 	Training 400/1106. train loss: 227.3198,	0.6464 s / batch. (data: 5.92e-03). ETA=16:00:49, max mem: 15.9 GB 
[10/25 07:13:51 visual_prompt]: 	Training 500/1106. train loss: 29.0313,	0.6428 s / batch. (data: 8.16e-04). ETA=15:54:27, max mem: 15.9 GB 
[10/25 07:14:54 visual_prompt]: 	Training 600/1106. train loss: 495.0229,	0.6292 s / batch. (data: 1.20e-02). ETA=15:33:10, max mem: 15.9 GB 
[10/25 07:15:57 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6081 s / batch. (data: 3.36e-04). ETA=15:00:54, max mem: 15.9 GB 
[10/25 07:17:00 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6233 s / batch. (data: 8.28e-04). ETA=15:22:19, max mem: 15.9 GB 
[10/25 07:18:02 visual_prompt]: 	Training 900/1106. train loss: 277.5616,	0.6285 s / batch. (data: 7.69e-04). ETA=15:28:58, max mem: 15.9 GB 
[10/25 07:19:05 visual_prompt]: 	Training 1000/1106. train loss: 67.5194,	0.6170 s / batch. (data: 3.06e-04). ETA=15:10:54, max mem: 15.9 GB 
[10/25 07:20:08 visual_prompt]: 	Training 1100/1106. train loss: 199.0411,	0.6184 s / batch. (data: 1.37e-04). ETA=15:12:03, max mem: 15.9 GB 
[10/25 07:20:12 visual_prompt]: Epoch 20 / 100: avg data time: 4.49e-03, avg batch time: 0.6313, average train loss: 105.9228
[10/25 07:21:02 visual_prompt]: 	Test 100/123. loss: 27.017, 0.2255 s / batch. (data: 4.67e-05)max mem: 15.91075 GB 
[10/25 07:21:12 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.2321, average loss: 24.2843
[10/25 07:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.32	
[10/25 07:21:12 visual_prompt]: Stopping early.
[10/25 07:21:12 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 07:21:12 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 07:21:12 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 07:21:12 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 07:21:12 visual_prompt]: Training with config:
[10/25 07:21:12 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr25.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 07:21:12 visual_prompt]: Loading training data...
[10/25 07:21:12 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 07:21:12 visual_prompt]: Loading validation data...
[10/25 07:21:12 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 07:21:12 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/25 07:21:15 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/25 07:21:15 visual_prompt]: tuned percent:0.522
[10/25 07:21:15 visual_prompt]: Device used for model: 0
[10/25 07:21:15 visual_prompt]: Setting up Evaluator...
[10/25 07:21:15 visual_prompt]: Setting up Trainer...
[10/25 07:21:15 visual_prompt]: 	Setting up the optimizer...
[10/25 07:21:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 07:22:20 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6327 s / batch. (data: 1.60e-02). ETA=19:25:15, max mem: 15.9 GB 
[10/25 07:23:24 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6182 s / batch. (data: 2.80e-04). ETA=18:57:30, max mem: 15.9 GB 
[10/25 07:24:27 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6404 s / batch. (data: 8.35e-04). ETA=19:37:15, max mem: 15.9 GB 
[10/25 07:25:30 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6465 s / batch. (data: 8.01e-04). ETA=19:47:21, max mem: 15.9 GB 
[10/25 07:26:33 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6280 s / batch. (data: 3.10e-04). ETA=19:12:26, max mem: 15.9 GB 
[10/25 07:27:36 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6186 s / batch. (data: 3.37e-04). ETA=18:54:07, max mem: 15.9 GB 
[10/25 07:28:39 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6276 s / batch. (data: 3.40e-04). ETA=19:09:35, max mem: 15.9 GB 
[10/25 07:29:43 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6233 s / batch. (data: 3.21e-04). ETA=19:00:38, max mem: 15.9 GB 
[10/25 07:30:46 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6231 s / batch. (data: 3.32e-04). ETA=18:59:12, max mem: 15.9 GB 
[10/25 07:31:49 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6349 s / batch. (data: 3.40e-04). ETA=19:19:46, max mem: 15.9 GB 
[10/25 07:32:52 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6187 s / batch. (data: 1.71e-04). ETA=18:49:04, max mem: 15.9 GB 
[10/25 07:32:56 visual_prompt]: Epoch 1 / 100: avg data time: 4.33e-03, avg batch time: 0.6340, average train loss: 1.4028
[10/25 07:33:46 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2352 s / batch. (data: 3.03e-05)max mem: 15.91075 GB 
[10/25 07:33:56 visual_prompt]: Inference (val):avg data time: 2.46e-04, avg batch time: 0.2338, average loss: 1.3505
[10/25 07:33:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/25 07:33:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/25 07:35:01 visual_prompt]: 	Training 100/1106. train loss: 43.2928,	0.6321 s / batch. (data: 1.05e-02). ETA=19:12:25, max mem: 15.9 GB 
[10/25 07:36:05 visual_prompt]: 	Training 200/1106. train loss: 13.7925,	0.6407 s / batch. (data: 8.05e-04). ETA=19:27:02, max mem: 15.9 GB 
[10/25 07:37:08 visual_prompt]: 	Training 300/1106. train loss: 0.0565,	0.6577 s / batch. (data: 2.80e-02). ETA=19:56:52, max mem: 15.9 GB 
[10/25 07:38:11 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6403 s / batch. (data: 6.97e-04). ETA=19:24:15, max mem: 15.9 GB 
[10/25 07:39:14 visual_prompt]: 	Training 500/1106. train loss: 27.4238,	0.6340 s / batch. (data: 1.60e-02). ETA=19:11:45, max mem: 15.9 GB 
[10/25 07:40:18 visual_prompt]: 	Training 600/1106. train loss: 9.9406,	0.6340 s / batch. (data: 3.30e-04). ETA=19:10:40, max mem: 15.9 GB 
[10/25 07:41:21 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6234 s / batch. (data: 2.76e-04). ETA=18:50:24, max mem: 15.9 GB 
[10/25 07:42:24 visual_prompt]: 	Training 800/1106. train loss: 1.6986,	0.6486 s / batch. (data: 8.13e-04). ETA=19:34:56, max mem: 15.9 GB 
[10/25 07:43:27 visual_prompt]: 	Training 900/1106. train loss: 3.1213,	0.6399 s / batch. (data: 5.92e-03). ETA=19:18:09, max mem: 15.9 GB 
[10/25 07:44:31 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6283 s / batch. (data: 7.69e-04). ETA=18:56:05, max mem: 15.9 GB 
[10/25 07:45:34 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6192 s / batch. (data: 1.63e-04). ETA=18:38:37, max mem: 15.9 GB 
[10/25 07:45:38 visual_prompt]: Epoch 2 / 100: avg data time: 3.81e-03, avg batch time: 0.6338, average train loss: 16.8761
[10/25 07:46:27 visual_prompt]: 	Test 100/123. loss: 1.237, 0.2356 s / batch. (data: 2.91e-05)max mem: 15.91075 GB 
[10/25 07:46:38 visual_prompt]: Inference (val):avg data time: 9.89e-05, avg batch time: 0.2318, average loss: 1.1272
[10/25 07:46:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.94	
[10/25 07:46:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/25 07:47:44 visual_prompt]: 	Training 100/1106. train loss: 84.1160,	0.6357 s / batch. (data: 3.25e-04). ETA=19:07:15, max mem: 15.9 GB 
[10/25 07:48:47 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6182 s / batch. (data: 3.20e-04). ETA=18:34:43, max mem: 15.9 GB 
[10/25 07:49:50 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6473 s / batch. (data: 8.16e-04). ETA=19:26:08, max mem: 15.9 GB 
[10/25 07:50:53 visual_prompt]: 	Training 400/1106. train loss: 4.0453,	0.6295 s / batch. (data: 7.82e-04). ETA=18:52:53, max mem: 15.9 GB 
[10/25 07:51:56 visual_prompt]: 	Training 500/1106. train loss: 100.1586,	0.6321 s / batch. (data: 8.10e-04). ETA=18:56:32, max mem: 15.9 GB 
[10/25 07:53:00 visual_prompt]: 	Training 600/1106. train loss: 5.2604,	0.6174 s / batch. (data: 3.32e-04). ETA=18:29:12, max mem: 15.9 GB 
[10/25 07:54:03 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6182 s / batch. (data: 3.30e-04). ETA=18:29:37, max mem: 15.9 GB 
[10/25 07:55:06 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6390 s / batch. (data: 7.83e-04). ETA=19:05:44, max mem: 15.9 GB 
[10/25 07:56:09 visual_prompt]: 	Training 900/1106. train loss: 11.1299,	0.6318 s / batch. (data: 3.25e-04). ETA=18:51:46, max mem: 15.9 GB 
[10/25 07:57:12 visual_prompt]: 	Training 1000/1106. train loss: 26.0419,	0.6431 s / batch. (data: 7.44e-04). ETA=19:10:58, max mem: 15.9 GB 
[10/25 07:58:15 visual_prompt]: 	Training 1100/1106. train loss: 7.6603,	0.6173 s / batch. (data: 1.50e-04). ETA=18:23:45, max mem: 15.9 GB 
[10/25 07:58:19 visual_prompt]: Epoch 3 / 100: avg data time: 4.88e-03, avg batch time: 0.6339, average train loss: 21.5948
[10/25 07:59:08 visual_prompt]: 	Test 100/123. loss: 29.223, 0.2358 s / batch. (data: 2.77e-05)max mem: 15.91075 GB 
[10/25 07:59:19 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2329, average loss: 32.3328
[10/25 07:59:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.67	
[10/25 07:59:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/25 08:00:24 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6184 s / batch. (data: 3.04e-04). ETA=18:24:36, max mem: 15.9 GB 
[10/25 08:01:27 visual_prompt]: 	Training 200/1106. train loss: 177.1742,	0.6337 s / batch. (data: 3.48e-04). ETA=18:50:59, max mem: 15.9 GB 
[10/25 08:02:31 visual_prompt]: 	Training 300/1106. train loss: 14.6053,	0.6234 s / batch. (data: 1.12e-03). ETA=18:31:31, max mem: 15.9 GB 
[10/25 08:03:34 visual_prompt]: 	Training 400/1106. train loss: 3.6913,	0.6276 s / batch. (data: 7.94e-04). ETA=18:38:01, max mem: 15.9 GB 
[10/25 08:04:37 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6279 s / batch. (data: 3.27e-04). ETA=18:37:24, max mem: 15.9 GB 
[10/25 08:05:41 visual_prompt]: 	Training 600/1106. train loss: 0.3717,	0.6728 s / batch. (data: 3.77e-02). ETA=19:56:16, max mem: 15.9 GB 
[10/25 08:06:43 visual_prompt]: 	Training 700/1106. train loss: 123.9401,	0.6305 s / batch. (data: 3.54e-04). ETA=18:40:01, max mem: 15.9 GB 
[10/25 08:07:46 visual_prompt]: 	Training 800/1106. train loss: 27.5820,	0.6310 s / batch. (data: 8.27e-04). ETA=18:39:45, max mem: 15.9 GB 
[10/25 08:08:49 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6262 s / batch. (data: 3.12e-04). ETA=18:30:12, max mem: 15.9 GB 
[10/25 08:09:52 visual_prompt]: 	Training 1000/1106. train loss: 16.8953,	0.6472 s / batch. (data: 8.14e-04). ETA=19:06:23, max mem: 15.9 GB 
[10/25 08:10:56 visual_prompt]: 	Training 1100/1106. train loss: 111.5216,	0.6139 s / batch. (data: 1.80e-04). ETA=18:06:22, max mem: 15.9 GB 
[10/25 08:11:00 visual_prompt]: Epoch 4 / 100: avg data time: 4.43e-03, avg batch time: 0.6335, average train loss: 30.5408
[10/25 08:11:50 visual_prompt]: 	Test 100/123. loss: 16.378, 0.2259 s / batch. (data: 3.08e-05)max mem: 15.91075 GB 
[10/25 08:12:00 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2324, average loss: 19.1793
[10/25 08:12:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.58	
[10/25 08:12:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/25 08:13:05 visual_prompt]: 	Training 100/1106. train loss: 26.9837,	0.6168 s / batch. (data: 3.27e-04). ETA=18:10:28, max mem: 15.9 GB 
[10/25 08:14:07 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6172 s / batch. (data: 3.32e-04). ETA=18:10:10, max mem: 15.9 GB 
[10/25 08:15:10 visual_prompt]: 	Training 300/1106. train loss: 98.9988,	0.6253 s / batch. (data: 7.81e-04). ETA=18:23:19, max mem: 15.9 GB 
[10/25 08:16:13 visual_prompt]: 	Training 400/1106. train loss: 132.5071,	0.6246 s / batch. (data: 7.27e-04). ETA=18:21:11, max mem: 15.9 GB 
[10/25 08:17:16 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6242 s / batch. (data: 8.32e-04). ETA=18:19:20, max mem: 15.9 GB 
[10/25 08:18:19 visual_prompt]: 	Training 600/1106. train loss: 3.7928,	0.6335 s / batch. (data: 7.88e-04). ETA=18:34:45, max mem: 15.9 GB 
[10/25 08:19:22 visual_prompt]: 	Training 700/1106. train loss: 41.2185,	0.6174 s / batch. (data: 3.17e-04). ETA=18:05:17, max mem: 15.9 GB 
[10/25 08:20:25 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6324 s / batch. (data: 7.66e-04). ETA=18:30:40, max mem: 15.9 GB 
[10/25 08:21:28 visual_prompt]: 	Training 900/1106. train loss: 92.9566,	0.6131 s / batch. (data: 3.18e-04). ETA=17:55:47, max mem: 15.9 GB 
[10/25 08:22:31 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6384 s / batch. (data: 8.15e-04). ETA=18:39:03, max mem: 15.9 GB 
[10/25 08:23:34 visual_prompt]: 	Training 1100/1106. train loss: 18.2732,	0.6173 s / batch. (data: 1.59e-04). ETA=18:01:01, max mem: 15.9 GB 
[10/25 08:23:38 visual_prompt]: Epoch 5 / 100: avg data time: 3.84e-03, avg batch time: 0.6309, average train loss: 46.3265
[10/25 08:24:27 visual_prompt]: 	Test 100/123. loss: 10.669, 0.2258 s / batch. (data: 4.15e-05)max mem: 15.91075 GB 
[10/25 08:24:38 visual_prompt]: Inference (val):avg data time: 4.64e-05, avg batch time: 0.2322, average loss: 9.3550
[10/25 08:24:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.83	
[10/25 08:24:38 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/25 08:25:43 visual_prompt]: 	Training 100/1106. train loss: 0.0515,	0.6440 s / batch. (data: 1.20e-02). ETA=18:46:42, max mem: 15.9 GB 
[10/25 08:26:46 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6445 s / batch. (data: 8.09e-04). ETA=18:46:31, max mem: 15.9 GB 
[10/25 08:27:50 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6129 s / batch. (data: 3.53e-04). ETA=17:50:09, max mem: 15.9 GB 
[10/25 08:28:52 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6222 s / batch. (data: 8.85e-04). ETA=18:05:25, max mem: 15.9 GB 
[10/25 08:29:55 visual_prompt]: 	Training 500/1106. train loss: 10.3171,	0.6315 s / batch. (data: 4.40e-04). ETA=18:20:32, max mem: 15.9 GB 
[10/25 08:30:58 visual_prompt]: 	Training 600/1106. train loss: 81.0797,	0.6276 s / batch. (data: 1.06e-02). ETA=18:12:41, max mem: 15.9 GB 
[10/25 08:32:02 visual_prompt]: 	Training 700/1106. train loss: 24.1648,	0.6290 s / batch. (data: 1.20e-02). ETA=18:14:09, max mem: 15.9 GB 
[10/25 08:33:04 visual_prompt]: 	Training 800/1106. train loss: 102.6743,	0.6245 s / batch. (data: 3.09e-04). ETA=18:05:19, max mem: 15.9 GB 
[10/25 08:34:07 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6360 s / batch. (data: 8.00e-04). ETA=18:24:11, max mem: 15.9 GB 
[10/25 08:35:10 visual_prompt]: 	Training 1000/1106. train loss: 266.4210,	0.6427 s / batch. (data: 3.25e-04). ETA=18:34:50, max mem: 15.9 GB 
[10/25 08:36:13 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6068 s / batch. (data: 1.69e-04). ETA=17:31:28, max mem: 15.9 GB 
[10/25 08:36:17 visual_prompt]: Epoch 6 / 100: avg data time: 4.00e-03, avg batch time: 0.6314, average train loss: 55.7331
[10/25 08:37:07 visual_prompt]: 	Test 100/123. loss: 30.689, 0.2685 s / batch. (data: 4.01e-05)max mem: 15.91075 GB 
[10/25 08:37:17 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2327, average loss: 34.0670
[10/25 08:37:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.97	
[10/25 08:37:17 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/25 08:38:22 visual_prompt]: 	Training 100/1106. train loss: 49.4446,	0.6357 s / batch. (data: 8.25e-04). ETA=18:20:31, max mem: 15.9 GB 
[10/25 08:39:25 visual_prompt]: 	Training 200/1106. train loss: 226.8054,	0.6354 s / batch. (data: 8.59e-04). ETA=18:18:47, max mem: 15.9 GB 
[10/25 08:40:28 visual_prompt]: 	Training 300/1106. train loss: 39.3147,	0.6313 s / batch. (data: 8.07e-04). ETA=18:10:47, max mem: 15.9 GB 
[10/25 08:41:31 visual_prompt]: 	Training 400/1106. train loss: 39.9985,	0.6150 s / batch. (data: 3.39e-04). ETA=17:41:27, max mem: 15.9 GB 
[10/25 08:42:34 visual_prompt]: 	Training 500/1106. train loss: 83.8561,	0.6285 s / batch. (data: 1.31e-02). ETA=18:03:45, max mem: 15.9 GB 
[10/25 08:43:37 visual_prompt]: 	Training 600/1106. train loss: 142.2539,	0.6381 s / batch. (data: 7.77e-04). ETA=18:19:14, max mem: 15.9 GB 
[10/25 08:44:40 visual_prompt]: 	Training 700/1106. train loss: 265.9731,	0.6397 s / batch. (data: 7.72e-04). ETA=18:21:01, max mem: 15.9 GB 
[10/25 08:45:43 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6222 s / batch. (data: 7.80e-04). ETA=17:49:48, max mem: 15.9 GB 
[10/25 08:46:46 visual_prompt]: 	Training 900/1106. train loss: 85.4020,	0.6258 s / batch. (data: 8.31e-04). ETA=17:54:55, max mem: 15.9 GB 
[10/25 08:47:49 visual_prompt]: 	Training 1000/1106. train loss: 59.7632,	0.6173 s / batch. (data: 3.11e-04). ETA=17:39:19, max mem: 15.9 GB 
[10/25 08:48:52 visual_prompt]: 	Training 1100/1106. train loss: 124.7041,	0.6179 s / batch. (data: 1.55e-04). ETA=17:39:24, max mem: 15.9 GB 
[10/25 08:48:55 visual_prompt]: Epoch 7 / 100: avg data time: 4.15e-03, avg batch time: 0.6313, average train loss: 58.9722
[10/25 08:49:45 visual_prompt]: 	Test 100/123. loss: 93.061, 0.2251 s / batch. (data: 3.12e-05)max mem: 15.91075 GB 
[10/25 08:49:56 visual_prompt]: Inference (val):avg data time: 1.35e-04, avg batch time: 0.2321, average loss: 83.9150
[10/25 08:49:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.17	
[10/25 08:49:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/25 08:51:01 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6295 s / batch. (data: 8.25e-04). ETA=17:58:07, max mem: 15.9 GB 
[10/25 08:52:04 visual_prompt]: 	Training 200/1106. train loss: 31.3398,	0.6202 s / batch. (data: 3.28e-04). ETA=17:41:07, max mem: 15.9 GB 
[10/25 08:53:07 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6342 s / batch. (data: 8.39e-04). ETA=18:04:03, max mem: 15.9 GB 
[10/25 08:54:10 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6154 s / batch. (data: 5.40e-03). ETA=17:30:50, max mem: 15.9 GB 
[10/25 08:55:13 visual_prompt]: 	Training 500/1106. train loss: 2.1990,	0.6300 s / batch. (data: 3.75e-04). ETA=17:54:48, max mem: 15.9 GB 
[10/25 08:56:16 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6302 s / batch. (data: 3.30e-04). ETA=17:54:05, max mem: 15.9 GB 
[10/25 08:57:18 visual_prompt]: 	Training 700/1106. train loss: 69.6689,	0.6130 s / batch. (data: 3.25e-04). ETA=17:23:44, max mem: 15.9 GB 
[10/25 08:58:21 visual_prompt]: 	Training 800/1106. train loss: 33.5944,	0.6351 s / batch. (data: 8.16e-04). ETA=18:00:20, max mem: 15.9 GB 
[10/25 08:59:25 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6212 s / batch. (data: 7.62e-04). ETA=17:35:35, max mem: 15.9 GB 
[10/25 09:00:27 visual_prompt]: 	Training 1000/1106. train loss: 294.3003,	0.6479 s / batch. (data: 8.49e-04). ETA=18:19:49, max mem: 15.9 GB 
[10/25 09:01:30 visual_prompt]: 	Training 1100/1106. train loss: 17.9451,	0.6188 s / batch. (data: 1.65e-04). ETA=17:29:31, max mem: 15.9 GB 
[10/25 09:01:34 visual_prompt]: Epoch 8 / 100: avg data time: 4.72e-03, avg batch time: 0.6312, average train loss: 71.3541
[10/25 09:02:26 visual_prompt]: 	Test 100/123. loss: 8.967, 0.2276 s / batch. (data: 3.19e-05)max mem: 15.91075 GB 
[10/25 09:02:37 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.2329, average loss: 11.2308
[10/25 09:02:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.37	
[10/25 09:02:37 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/25 09:03:42 visual_prompt]: 	Training 100/1106. train loss: 262.4111,	0.6269 s / batch. (data: 8.06e-04). ETA=17:42:00, max mem: 15.9 GB 
[10/25 09:04:45 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6432 s / batch. (data: 3.29e-04). ETA=18:08:38, max mem: 15.9 GB 
[10/25 09:05:48 visual_prompt]: 	Training 300/1106. train loss: 81.5391,	0.6132 s / batch. (data: 3.24e-04). ETA=17:16:49, max mem: 15.9 GB 
[10/25 09:06:51 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6278 s / batch. (data: 7.31e-04). ETA=17:40:24, max mem: 15.9 GB 
[10/25 09:07:54 visual_prompt]: 	Training 500/1106. train loss: 182.3483,	0.6178 s / batch. (data: 3.25e-04). ETA=17:22:30, max mem: 15.9 GB 
[10/25 09:08:57 visual_prompt]: 	Training 600/1106. train loss: 19.5862,	0.6568 s / batch. (data: 8.28e-04). ETA=18:27:17, max mem: 15.9 GB 
[10/25 09:10:00 visual_prompt]: 	Training 700/1106. train loss: 11.0554,	0.6339 s / batch. (data: 8.63e-04). ETA=17:47:41, max mem: 15.9 GB 
[10/25 09:11:03 visual_prompt]: 	Training 800/1106. train loss: 37.9345,	0.6311 s / batch. (data: 2.42e-04). ETA=17:41:52, max mem: 15.9 GB 
[10/25 09:12:07 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6198 s / batch. (data: 9.16e-04). ETA=17:21:43, max mem: 15.9 GB 
[10/25 09:13:09 visual_prompt]: 	Training 1000/1106. train loss: 6.0300,	0.6191 s / batch. (data: 3.35e-04). ETA=17:19:34, max mem: 15.9 GB 
[10/25 09:14:12 visual_prompt]: 	Training 1100/1106. train loss: 35.9857,	0.6170 s / batch. (data: 1.67e-04). ETA=17:15:04, max mem: 15.9 GB 
[10/25 09:14:16 visual_prompt]: Epoch 9 / 100: avg data time: 4.80e-03, avg batch time: 0.6322, average train loss: 62.4343
[10/25 09:15:09 visual_prompt]: 	Test 100/123. loss: 14.619, 0.2278 s / batch. (data: 3.70e-05)max mem: 15.91075 GB 
[10/25 09:15:20 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.2330, average loss: 13.0668
[10/25 09:15:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.29	
[10/25 09:15:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/25 09:16:25 visual_prompt]: 	Training 100/1106. train loss: 81.2421,	0.6378 s / batch. (data: 2.57e-02). ETA=17:48:52, max mem: 15.9 GB 
[10/25 09:17:28 visual_prompt]: 	Training 200/1106. train loss: 10.0528,	0.6316 s / batch. (data: 8.65e-04). ETA=17:37:23, max mem: 15.9 GB 
[10/25 09:18:35 visual_prompt]: 	Training 300/1106. train loss: 15.6759,	0.6307 s / batch. (data: 3.31e-04). ETA=17:34:44, max mem: 15.9 GB 
[10/25 09:19:38 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6452 s / batch. (data: 8.90e-04). ETA=17:57:59, max mem: 15.9 GB 
[10/25 09:20:41 visual_prompt]: 	Training 500/1106. train loss: 60.1689,	0.6264 s / batch. (data: 7.62e-04). ETA=17:25:35, max mem: 15.9 GB 
[10/25 09:21:44 visual_prompt]: 	Training 600/1106. train loss: 10.1917,	0.6443 s / batch. (data: 3.38e-04). ETA=17:54:18, max mem: 15.9 GB 
[10/25 09:22:47 visual_prompt]: 	Training 700/1106. train loss: 253.7137,	0.6178 s / batch. (data: 3.33e-04). ETA=17:09:02, max mem: 15.9 GB 
[10/25 09:23:50 visual_prompt]: 	Training 800/1106. train loss: 39.2837,	0.6444 s / batch. (data: 3.46e-04). ETA=17:52:22, max mem: 15.9 GB 
[10/25 09:24:54 visual_prompt]: 	Training 900/1106. train loss: 23.5776,	0.6488 s / batch. (data: 8.34e-04). ETA=17:58:34, max mem: 15.9 GB 
[10/25 09:25:57 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6388 s / batch. (data: 2.99e-04). ETA=17:40:49, max mem: 15.9 GB 
[10/25 09:27:00 visual_prompt]: 	Training 1100/1106. train loss: 54.7700,	0.6188 s / batch. (data: 1.54e-04). ETA=17:06:36, max mem: 15.9 GB 
[10/25 09:27:04 visual_prompt]: Epoch 10 / 100: avg data time: 8.59e-03, avg batch time: 0.6365, average train loss: 95.6052
[10/25 09:27:55 visual_prompt]: 	Test 100/123. loss: 30.829, 0.2258 s / batch. (data: 4.27e-05)max mem: 15.91075 GB 
[10/25 09:28:07 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2320, average loss: 27.6856
[10/25 09:28:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.08	
[10/25 09:28:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/25 09:29:12 visual_prompt]: 	Training 100/1106. train loss: 347.1469,	0.6799 s / batch. (data: 2.70e-02). ETA=18:46:45, max mem: 15.9 GB 
[10/25 09:30:15 visual_prompt]: 	Training 200/1106. train loss: 74.4355,	0.6253 s / batch. (data: 3.24e-04). ETA=17:15:18, max mem: 15.9 GB 
[10/25 09:31:18 visual_prompt]: 	Training 300/1106. train loss: 51.5237,	0.6240 s / batch. (data: 2.84e-04). ETA=17:12:07, max mem: 15.9 GB 
[10/25 09:32:21 visual_prompt]: 	Training 400/1106. train loss: 252.3958,	0.6178 s / batch. (data: 3.03e-04). ETA=17:00:45, max mem: 15.9 GB 
[10/25 09:33:24 visual_prompt]: 	Training 500/1106. train loss: 10.1704,	0.6319 s / batch. (data: 3.08e-04). ETA=17:23:01, max mem: 15.9 GB 
[10/25 09:34:27 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6240 s / batch. (data: 7.58e-04). ETA=17:09:02, max mem: 15.9 GB 
[10/25 09:35:30 visual_prompt]: 	Training 700/1106. train loss: 52.7979,	0.6333 s / batch. (data: 1.56e-02). ETA=17:23:19, max mem: 15.9 GB 
[10/25 09:36:33 visual_prompt]: 	Training 800/1106. train loss: 12.2006,	0.6336 s / batch. (data: 3.03e-04). ETA=17:22:44, max mem: 15.9 GB 
[10/25 09:37:36 visual_prompt]: 	Training 900/1106. train loss: 182.3058,	0.6191 s / batch. (data: 2.89e-04). ETA=16:57:47, max mem: 15.9 GB 
[10/25 09:38:40 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6218 s / batch. (data: 7.93e-04). ETA=17:01:07, max mem: 15.9 GB 
[10/25 09:39:42 visual_prompt]: 	Training 1100/1106. train loss: 290.5355,	0.6135 s / batch. (data: 1.53e-04). ETA=16:46:30, max mem: 15.9 GB 
[10/25 09:39:46 visual_prompt]: Epoch 11 / 100: avg data time: 4.63e-03, avg batch time: 0.6324, average train loss: 83.3820
[10/25 09:40:38 visual_prompt]: 	Test 100/123. loss: 212.802, 0.2406 s / batch. (data: 5.01e-05)max mem: 15.91075 GB 
[10/25 09:40:47 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.2329, average loss: 192.1664
[10/25 09:40:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.23	
[10/25 09:40:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/25 09:41:53 visual_prompt]: 	Training 100/1106. train loss: 110.5872,	0.6161 s / batch. (data: 4.84e-04). ETA=16:49:43, max mem: 15.9 GB 
[10/25 09:42:56 visual_prompt]: 	Training 200/1106. train loss: 48.5052,	0.6138 s / batch. (data: 3.21e-04). ETA=16:44:56, max mem: 15.9 GB 
[10/25 09:43:59 visual_prompt]: 	Training 300/1106. train loss: 131.4540,	0.6332 s / batch. (data: 2.59e-04). ETA=17:15:35, max mem: 15.9 GB 
[10/25 09:45:02 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6201 s / batch. (data: 3.09e-04). ETA=16:53:07, max mem: 15.9 GB 
[10/25 09:46:05 visual_prompt]: 	Training 500/1106. train loss: 135.1235,	0.6334 s / batch. (data: 2.99e-04). ETA=17:13:54, max mem: 15.9 GB 
[10/25 09:47:08 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6188 s / batch. (data: 7.76e-04). ETA=16:49:01, max mem: 15.9 GB 
[10/25 09:48:10 visual_prompt]: 	Training 700/1106. train loss: 12.7412,	0.6314 s / batch. (data: 7.61e-04). ETA=17:08:32, max mem: 15.9 GB 
[10/25 09:49:13 visual_prompt]: 	Training 800/1106. train loss: 60.6021,	0.6149 s / batch. (data: 2.78e-04). ETA=16:40:33, max mem: 15.9 GB 
[10/25 09:50:16 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6470 s / batch. (data: 3.09e-04). ETA=17:31:45, max mem: 15.9 GB 
[10/25 09:51:19 visual_prompt]: 	Training 1000/1106. train loss: 116.8290,	0.6386 s / batch. (data: 8.20e-04). ETA=17:16:58, max mem: 15.9 GB 
[10/25 09:52:21 visual_prompt]: 	Training 1100/1106. train loss: 230.4016,	0.6119 s / batch. (data: 1.56e-04). ETA=16:32:39, max mem: 15.9 GB 
[10/25 09:52:25 visual_prompt]: Epoch 12 / 100: avg data time: 5.39e-03, avg batch time: 0.6308, average train loss: 90.3960
[10/25 09:53:15 visual_prompt]: 	Test 100/123. loss: 27.396, 0.2414 s / batch. (data: 3.46e-05)max mem: 15.91075 GB 
[10/25 09:53:25 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.2317, average loss: 24.5422
[10/25 09:53:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.75	
[10/25 09:53:25 visual_prompt]: Best epoch 12: best metric: -24.542
[10/25 09:53:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/25 09:54:30 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6216 s / batch. (data: 8.42e-04). ETA=16:47:20, max mem: 15.9 GB 
[10/25 09:55:33 visual_prompt]: 	Training 200/1106. train loss: 67.6504,	0.6119 s / batch. (data: 2.67e-04). ETA=16:30:35, max mem: 15.9 GB 
[10/25 09:56:36 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6066 s / batch. (data: 3.03e-04). ETA=16:20:56, max mem: 15.9 GB 
[10/25 09:57:39 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6400 s / batch. (data: 8.26e-04). ETA=17:13:53, max mem: 15.9 GB 
[10/25 09:58:42 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6197 s / batch. (data: 7.99e-04). ETA=16:40:07, max mem: 15.9 GB 
[10/25 09:59:45 visual_prompt]: 	Training 600/1106. train loss: 289.8235,	0.6319 s / batch. (data: 1.20e-02). ETA=16:58:42, max mem: 15.9 GB 
[10/25 10:00:47 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6288 s / batch. (data: 7.44e-04). ETA=16:52:38, max mem: 15.9 GB 
[10/25 10:01:50 visual_prompt]: 	Training 800/1106. train loss: 127.5260,	0.6358 s / batch. (data: 1.06e-02). ETA=17:02:56, max mem: 15.9 GB 
[10/25 10:02:53 visual_prompt]: 	Training 900/1106. train loss: 478.8485,	0.6185 s / batch. (data: 3.11e-04). ETA=16:34:02, max mem: 15.9 GB 
[10/25 10:03:56 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6086 s / batch. (data: 7.77e-04). ETA=16:17:06, max mem: 15.9 GB 
[10/25 10:04:59 visual_prompt]: 	Training 1100/1106. train loss: 339.5282,	0.6128 s / batch. (data: 1.82e-04). ETA=16:22:45, max mem: 15.9 GB 
[10/25 10:05:02 visual_prompt]: Epoch 13 / 100: avg data time: 4.96e-03, avg batch time: 0.6301, average train loss: 93.0719
[10/25 10:05:54 visual_prompt]: 	Test 100/123. loss: 114.287, 0.2391 s / batch. (data: 4.08e-05)max mem: 15.91075 GB 
[10/25 10:06:05 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.2325, average loss: 128.3579
[10/25 10:06:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.04	
[10/25 10:06:05 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/25 10:07:10 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6257 s / batch. (data: 7.74e-04). ETA=16:42:25, max mem: 15.9 GB 
[10/25 10:08:13 visual_prompt]: 	Training 200/1106. train loss: 66.2773,	0.6420 s / batch. (data: 8.00e-04). ETA=17:07:25, max mem: 15.9 GB 
[10/25 10:09:16 visual_prompt]: 	Training 300/1106. train loss: 106.2595,	0.6320 s / batch. (data: 1.17e-03). ETA=16:50:23, max mem: 15.9 GB 
[10/25 10:10:19 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6340 s / batch. (data: 8.02e-04). ETA=16:52:35, max mem: 15.9 GB 
[10/25 10:11:22 visual_prompt]: 	Training 500/1106. train loss: 186.7568,	0.6272 s / batch. (data: 7.88e-04). ETA=16:40:37, max mem: 15.9 GB 
[10/25 10:12:24 visual_prompt]: 	Training 600/1106. train loss: 131.0853,	0.6238 s / batch. (data: 3.11e-04). ETA=16:34:08, max mem: 15.9 GB 
[10/25 10:13:27 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6490 s / batch. (data: 6.08e-03). ETA=17:13:11, max mem: 15.9 GB 
[10/25 10:14:30 visual_prompt]: 	Training 800/1106. train loss: 0.0012,	0.6423 s / batch. (data: 9.61e-04). ETA=17:01:30, max mem: 15.9 GB 
[10/25 10:15:38 visual_prompt]: 	Training 900/1106. train loss: 92.4234,	0.6296 s / batch. (data: 7.98e-04). ETA=16:40:17, max mem: 15.9 GB 
[10/25 10:16:41 visual_prompt]: 	Training 1000/1106. train loss: 112.9902,	0.6240 s / batch. (data: 3.56e-04). ETA=16:30:17, max mem: 15.9 GB 
[10/25 10:17:43 visual_prompt]: 	Training 1100/1106. train loss: 15.1395,	0.6182 s / batch. (data: 1.63e-04). ETA=16:20:07, max mem: 15.9 GB 
[10/25 10:17:47 visual_prompt]: Epoch 14 / 100: avg data time: 9.60e-03, avg batch time: 0.6350, average train loss: 91.9161
[10/25 10:18:38 visual_prompt]: 	Test 100/123. loss: 36.921, 0.2438 s / batch. (data: 2.81e-05)max mem: 15.91075 GB 
[10/25 10:18:49 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.2332, average loss: 31.2681
[10/25 10:18:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.21	
[10/25 10:18:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/25 10:19:53 visual_prompt]: 	Training 100/1106. train loss: 178.5365,	0.6253 s / batch. (data: 8.34e-04). ETA=16:30:17, max mem: 15.9 GB 
[10/25 10:20:56 visual_prompt]: 	Training 200/1106. train loss: 175.0430,	0.6125 s / batch. (data: 3.03e-04). ETA=16:08:53, max mem: 15.9 GB 
[10/25 10:21:59 visual_prompt]: 	Training 300/1106. train loss: 14.1094,	0.6179 s / batch. (data: 1.08e-03). ETA=16:16:29, max mem: 15.9 GB 
[10/25 10:23:02 visual_prompt]: 	Training 400/1106. train loss: 73.8156,	0.6341 s / batch. (data: 8.56e-04). ETA=16:40:59, max mem: 15.9 GB 
[10/25 10:24:04 visual_prompt]: 	Training 500/1106. train loss: 9.5594,	0.6520 s / batch. (data: 8.18e-04). ETA=17:08:07, max mem: 15.9 GB 
[10/25 10:25:07 visual_prompt]: 	Training 600/1106. train loss: 9.6314,	0.6307 s / batch. (data: 7.72e-04). ETA=16:33:32, max mem: 15.9 GB 
[10/25 10:26:10 visual_prompt]: 	Training 700/1106. train loss: 133.5498,	0.6130 s / batch. (data: 7.46e-04). ETA=16:04:41, max mem: 15.9 GB 
[10/25 10:27:13 visual_prompt]: 	Training 800/1106. train loss: 62.0535,	0.6310 s / batch. (data: 5.87e-03). ETA=16:31:50, max mem: 15.9 GB 
[10/25 10:28:16 visual_prompt]: 	Training 900/1106. train loss: 280.3013,	0.6318 s / batch. (data: 7.94e-04). ETA=16:32:09, max mem: 15.9 GB 
[10/25 10:29:18 visual_prompt]: 	Training 1000/1106. train loss: 104.7286,	0.6350 s / batch. (data: 1.59e-02). ETA=16:36:07, max mem: 15.9 GB 
[10/25 10:30:21 visual_prompt]: 	Training 1100/1106. train loss: 30.7453,	0.6183 s / batch. (data: 1.35e-04). ETA=16:08:54, max mem: 15.9 GB 
[10/25 10:30:25 visual_prompt]: Epoch 15 / 100: avg data time: 4.57e-03, avg batch time: 0.6296, average train loss: 96.8095
[10/25 10:31:16 visual_prompt]: 	Test 100/123. loss: 3.702, 0.2251 s / batch. (data: 4.03e-05)max mem: 15.91075 GB 
[10/25 10:31:27 visual_prompt]: Inference (val):avg data time: 1.02e-04, avg batch time: 0.2323, average loss: 7.0724
[10/25 10:31:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 57.34	
[10/25 10:31:27 visual_prompt]: Best epoch 15: best metric: -7.072
[10/25 10:31:27 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/25 10:32:31 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6318 s / batch. (data: 7.78e-04). ETA=16:28:55, max mem: 15.9 GB 
[10/25 10:33:34 visual_prompt]: 	Training 200/1106. train loss: 35.6506,	0.6199 s / batch. (data: 7.70e-04). ETA=16:09:11, max mem: 15.9 GB 
[10/25 10:34:37 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6075 s / batch. (data: 3.15e-04). ETA=15:48:49, max mem: 15.9 GB 
[10/25 10:35:40 visual_prompt]: 	Training 400/1106. train loss: 14.6324,	0.6388 s / batch. (data: 8.17e-04). ETA=16:36:38, max mem: 15.9 GB 
[10/25 10:36:43 visual_prompt]: 	Training 500/1106. train loss: 41.3721,	0.6467 s / batch. (data: 6.12e-03). ETA=16:47:56, max mem: 15.9 GB 
[10/25 10:37:45 visual_prompt]: 	Training 600/1106. train loss: 5.9516,	0.6230 s / batch. (data: 3.02e-04). ETA=16:09:53, max mem: 15.9 GB 
[10/25 10:38:49 visual_prompt]: 	Training 700/1106. train loss: 176.9873,	0.6129 s / batch. (data: 2.79e-04). ETA=15:53:10, max mem: 15.9 GB 
[10/25 10:39:55 visual_prompt]: 	Training 800/1106. train loss: 200.3369,	0.6273 s / batch. (data: 3.02e-04). ETA=16:14:26, max mem: 15.9 GB 
[10/25 10:41:00 visual_prompt]: 	Training 900/1106. train loss: 72.4176,	0.6127 s / batch. (data: 3.32e-04). ETA=15:50:52, max mem: 15.9 GB 
[10/25 10:42:03 visual_prompt]: 	Training 1000/1106. train loss: 91.3107,	0.6183 s / batch. (data: 3.06e-04). ETA=15:58:29, max mem: 15.9 GB 
[10/25 10:43:06 visual_prompt]: 	Training 1100/1106. train loss: 68.5221,	0.6318 s / batch. (data: 1.87e-04). ETA=16:18:16, max mem: 15.9 GB 
[10/25 10:43:10 visual_prompt]: Epoch 16 / 100: avg data time: 1.07e-02, avg batch time: 0.6358, average train loss: 83.7084
[10/25 10:44:16 visual_prompt]: 	Test 100/123. loss: 69.936, 0.2274 s / batch. (data: 3.98e-05)max mem: 15.91075 GB 
[10/25 10:44:31 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2278, average loss: 61.9688
[10/25 10:44:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.98	
[10/25 10:44:31 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/25 10:45:36 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6617 s / batch. (data: 3.95e-02). ETA=17:03:24, max mem: 15.9 GB 
[10/25 10:46:39 visual_prompt]: 	Training 200/1106. train loss: 24.6632,	0.6160 s / batch. (data: 2.94e-04). ETA=15:51:46, max mem: 15.9 GB 
[10/25 10:47:42 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6288 s / batch. (data: 7.68e-04). ETA=16:10:27, max mem: 15.9 GB 
[10/25 10:48:45 visual_prompt]: 	Training 400/1106. train loss: 637.7482,	0.6270 s / batch. (data: 3.35e-04). ETA=16:06:37, max mem: 15.9 GB 
[10/25 10:49:56 visual_prompt]: 	Training 500/1106. train loss: 101.7324,	0.6189 s / batch. (data: 3.11e-04). ETA=15:53:10, max mem: 15.9 GB 
[10/25 10:50:58 visual_prompt]: 	Training 600/1106. train loss: 56.3045,	0.6276 s / batch. (data: 2.79e-04). ETA=16:05:30, max mem: 15.9 GB 
[10/25 10:52:01 visual_prompt]: 	Training 700/1106. train loss: 136.6826,	0.6148 s / batch. (data: 7.85e-04). ETA=15:44:49, max mem: 15.9 GB 
[10/25 10:53:04 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6583 s / batch. (data: 2.92e-02). ETA=16:50:36, max mem: 15.9 GB 
[10/25 10:54:07 visual_prompt]: 	Training 900/1106. train loss: 33.6312,	0.6222 s / batch. (data: 2.94e-03). ETA=15:54:00, max mem: 15.9 GB 
[10/25 10:55:09 visual_prompt]: 	Training 1000/1106. train loss: 207.5184,	0.6282 s / batch. (data: 1.05e-02). ETA=16:02:10, max mem: 15.9 GB 
[10/25 10:56:20 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6073 s / batch. (data: 1.51e-04). ETA=15:29:13, max mem: 15.9 GB 
[10/25 10:56:24 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e-02, avg batch time: 0.6452, average train loss: 93.2523
[10/25 10:57:15 visual_prompt]: 	Test 100/123. loss: 14.383, 0.2398 s / batch. (data: 3.39e-05)max mem: 15.91075 GB 
[10/25 10:57:26 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.2309, average loss: 18.4141
[10/25 10:57:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 55.97	
[10/25 10:57:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/25 10:58:31 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6078 s / batch. (data: 3.22e-04). ETA=15:28:53, max mem: 15.9 GB 
[10/25 10:59:33 visual_prompt]: 	Training 200/1106. train loss: 392.6171,	0.6180 s / batch. (data: 2.58e-04). ETA=15:43:23, max mem: 15.9 GB 
[10/25 11:00:36 visual_prompt]: 	Training 300/1106. train loss: 591.9543,	0.6240 s / batch. (data: 7.82e-04). ETA=15:51:36, max mem: 15.9 GB 
[10/25 11:01:39 visual_prompt]: 	Training 400/1106. train loss: 15.1673,	0.6376 s / batch. (data: 8.70e-04). ETA=16:11:12, max mem: 15.9 GB 
[10/25 11:02:50 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6185 s / batch. (data: 1.14e-03). ETA=15:41:04, max mem: 15.9 GB 
[10/25 11:03:53 visual_prompt]: 	Training 600/1106. train loss: 0.0001,	0.6176 s / batch. (data: 3.37e-04). ETA=15:38:46, max mem: 15.9 GB 
[10/25 11:04:56 visual_prompt]: 	Training 700/1106. train loss: 66.1300,	0.6184 s / batch. (data: 2.99e-04). ETA=15:38:53, max mem: 15.9 GB 
[10/25 11:05:59 visual_prompt]: 	Training 800/1106. train loss: 59.0994,	0.6239 s / batch. (data: 3.35e-04). ETA=15:46:12, max mem: 15.9 GB 
[10/25 11:07:01 visual_prompt]: 	Training 900/1106. train loss: 336.3386,	0.6152 s / batch. (data: 4.69e-04). ETA=15:31:59, max mem: 15.9 GB 
[10/25 11:08:04 visual_prompt]: 	Training 1000/1106. train loss: 82.7130,	0.6144 s / batch. (data: 3.36e-04). ETA=15:29:45, max mem: 15.9 GB 
[10/25 11:09:07 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6065 s / batch. (data: 1.44e-04). ETA=15:16:49, max mem: 15.9 GB 
[10/25 11:09:12 visual_prompt]: Epoch 18 / 100: avg data time: 1.23e-02, avg batch time: 0.6375, average train loss: 90.0696
[10/25 11:10:03 visual_prompt]: 	Test 100/123. loss: 76.523, 0.2366 s / batch. (data: 5.44e-05)max mem: 15.91075 GB 
[10/25 11:10:14 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2314, average loss: 96.5826
[10/25 11:10:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.18	
[10/25 11:10:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[10/25 11:11:21 visual_prompt]: 	Training 100/1106. train loss: 89.4768,	0.6302 s / batch. (data: 3.16e-04). ETA=15:51:27, max mem: 15.9 GB 
[10/25 11:12:28 visual_prompt]: 	Training 200/1106. train loss: 246.5482,	0.6444 s / batch. (data: 1.97e-02). ETA=16:11:48, max mem: 15.9 GB 
[10/25 11:13:31 visual_prompt]: 	Training 300/1106. train loss: 44.3283,	0.6201 s / batch. (data: 3.18e-04). ETA=15:34:10, max mem: 15.9 GB 
[10/25 11:14:34 visual_prompt]: 	Training 400/1106. train loss: 28.3338,	0.6195 s / batch. (data: 3.89e-04). ETA=15:32:20, max mem: 15.9 GB 
[10/25 11:15:37 visual_prompt]: 	Training 500/1106. train loss: 31.7939,	0.6185 s / batch. (data: 1.71e-04). ETA=15:29:42, max mem: 15.9 GB 
[10/25 11:16:42 visual_prompt]: 	Training 600/1106. train loss: 56.5049,	0.6187 s / batch. (data: 4.42e-04). ETA=15:29:01, max mem: 15.9 GB 
[10/25 11:17:45 visual_prompt]: 	Training 700/1106. train loss: 129.4884,	0.6126 s / batch. (data: 3.72e-04). ETA=15:18:50, max mem: 15.9 GB 
[10/25 11:18:48 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6376 s / batch. (data: 7.67e-04). ETA=15:55:11, max mem: 15.9 GB 
[10/25 11:19:51 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6206 s / batch. (data: 4.36e-04). ETA=15:28:41, max mem: 15.9 GB 
[10/25 11:20:54 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6345 s / batch. (data: 3.30e-04). ETA=15:48:29, max mem: 15.9 GB 
[10/25 11:21:57 visual_prompt]: 	Training 1100/1106. train loss: 558.1444,	0.6201 s / batch. (data: 2.07e-04). ETA=15:25:58, max mem: 15.9 GB 
[10/25 11:22:00 visual_prompt]: Epoch 19 / 100: avg data time: 1.17e-02, avg batch time: 0.6386, average train loss: 76.7553
[10/25 11:22:51 visual_prompt]: 	Test 100/123. loss: 552.256, 0.2326 s / batch. (data: 4.24e-05)max mem: 15.91075 GB 
[10/25 11:23:02 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2328, average loss: 498.1460
[10/25 11:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.79	
[10/25 11:23:02 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[10/25 11:24:11 visual_prompt]: 	Training 100/1106. train loss: 46.7098,	0.6519 s / batch. (data: 2.71e-02). ETA=16:12:17, max mem: 15.9 GB 
[10/25 11:25:14 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6099 s / batch. (data: 4.87e-04). ETA=15:08:34, max mem: 15.9 GB 
[10/25 11:26:21 visual_prompt]: 	Training 300/1106. train loss: 147.6977,	0.6633 s / batch. (data: 1.60e-02). ETA=16:27:06, max mem: 15.9 GB 
[10/25 11:27:24 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6242 s / batch. (data: 2.69e-04). ETA=15:27:50, max mem: 15.9 GB 
[10/25 11:28:27 visual_prompt]: 	Training 500/1106. train loss: 9.6477,	0.6369 s / batch. (data: 5.89e-03). ETA=15:45:41, max mem: 15.9 GB 
[10/25 11:29:30 visual_prompt]: 	Training 600/1106. train loss: 209.1864,	0.6322 s / batch. (data: 1.25e-03). ETA=15:37:41, max mem: 15.9 GB 
[10/25 11:30:33 visual_prompt]: 	Training 700/1106. train loss: 141.5207,	0.6551 s / batch. (data: 8.51e-04). ETA=16:10:24, max mem: 15.9 GB 
[10/25 11:31:36 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6300 s / batch. (data: 7.22e-04). ETA=15:32:19, max mem: 15.9 GB 
[10/25 11:32:38 visual_prompt]: 	Training 900/1106. train loss: 289.8657,	0.6331 s / batch. (data: 3.37e-04). ETA=15:35:45, max mem: 15.9 GB 
[10/25 11:33:41 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6400 s / batch. (data: 3.17e-04). ETA=15:44:54, max mem: 15.9 GB 
[10/25 11:34:44 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6076 s / batch. (data: 1.65e-04). ETA=14:56:00, max mem: 15.9 GB 
[10/25 11:34:48 visual_prompt]: Epoch 20 / 100: avg data time: 1.33e-02, avg batch time: 0.6383, average train loss: 89.9092
[10/25 11:35:39 visual_prompt]: 	Test 100/123. loss: 102.898, 0.2279 s / batch. (data: 2.57e-05)max mem: 15.91075 GB 
[10/25 11:35:50 visual_prompt]: Inference (val):avg data time: 9.72e-05, avg batch time: 0.2317, average loss: 86.0293
[10/25 11:35:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.74	
[10/25 11:35:50 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[10/25 11:36:55 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6178 s / batch. (data: 1.05e-02). ETA=15:10:02, max mem: 15.9 GB 
[10/25 11:37:58 visual_prompt]: 	Training 200/1106. train loss: 220.1638,	0.6512 s / batch. (data: 7.86e-04). ETA=15:58:04, max mem: 15.9 GB 
[10/25 11:39:01 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6406 s / batch. (data: 1.05e-02). ETA=15:41:25, max mem: 15.9 GB 
[10/25 11:40:06 visual_prompt]: 	Training 400/1106. train loss: 97.2217,	0.6131 s / batch. (data: 2.55e-04). ETA=15:00:02, max mem: 15.9 GB 
[10/25 11:41:09 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6262 s / batch. (data: 3.50e-04). ETA=15:18:16, max mem: 15.9 GB 
[10/25 11:42:12 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6235 s / batch. (data: 7.49e-04). ETA=15:13:11, max mem: 15.9 GB 
[10/25 11:43:15 visual_prompt]: 	Training 700/1106. train loss: 227.4444,	0.6318 s / batch. (data: 8.21e-04). ETA=15:24:19, max mem: 15.9 GB 
[10/25 11:44:18 visual_prompt]: 	Training 800/1106. train loss: 9.9607,	0.6520 s / batch. (data: 3.18e-04). ETA=15:52:45, max mem: 15.9 GB 
[10/25 11:45:20 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6443 s / batch. (data: 8.18e-04). ETA=15:40:27, max mem: 15.9 GB 
[10/25 11:46:23 visual_prompt]: 	Training 1000/1106. train loss: 86.7202,	0.6155 s / batch. (data: 3.17e-04). ETA=14:57:23, max mem: 15.9 GB 
[10/25 11:47:30 visual_prompt]: 	Training 1100/1106. train loss: 110.8477,	0.6190 s / batch. (data: 2.19e-04). ETA=15:01:30, max mem: 15.9 GB 
[10/25 11:47:34 visual_prompt]: Epoch 21 / 100: avg data time: 1.19e-02, avg batch time: 0.6368, average train loss: 93.0315
[10/25 11:48:25 visual_prompt]: 	Test 100/123. loss: 8.632, 0.2359 s / batch. (data: 3.41e-05)max mem: 15.91075 GB 
[10/25 11:48:36 visual_prompt]: Inference (val):avg data time: 1.19e-04, avg batch time: 0.2318, average loss: 10.0003
[10/25 11:48:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.67	
[10/25 11:48:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[10/25 11:49:41 visual_prompt]: 	Training 100/1106. train loss: 34.3204,	0.6461 s / batch. (data: 1.41e-02). ETA=15:39:51, max mem: 15.9 GB 
[10/25 11:50:43 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6084 s / batch. (data: 3.13e-04). ETA=14:43:55, max mem: 15.9 GB 
[10/25 11:51:46 visual_prompt]: 	Training 300/1106. train loss: 115.6436,	0.6188 s / batch. (data: 3.40e-04). ETA=14:57:58, max mem: 15.9 GB 
[10/25 11:52:49 visual_prompt]: 	Training 400/1106. train loss: 32.1497,	0.6322 s / batch. (data: 3.04e-04). ETA=15:16:29, max mem: 15.9 GB 
[10/25 11:53:52 visual_prompt]: 	Training 500/1106. train loss: 122.6059,	0.6314 s / batch. (data: 3.14e-04). ETA=15:14:08, max mem: 15.9 GB 
[10/25 11:54:55 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6200 s / batch. (data: 3.11e-04). ETA=14:56:37, max mem: 15.9 GB 
[10/25 11:55:58 visual_prompt]: 	Training 700/1106. train loss: 349.7961,	0.6192 s / batch. (data: 3.19e-04). ETA=14:54:29, max mem: 15.9 GB 
[10/25 11:57:01 visual_prompt]: 	Training 800/1106. train loss: 500.9382,	0.6370 s / batch. (data: 1.25e-03). ETA=15:19:11, max mem: 15.9 GB 
[10/25 11:58:04 visual_prompt]: 	Training 900/1106. train loss: 86.8637,	0.6137 s / batch. (data: 4.90e-04). ETA=14:44:31, max mem: 15.9 GB 
[10/25 11:59:17 visual_prompt]: 	Training 1000/1106. train loss: 8.2818,	0.6295 s / batch. (data: 7.72e-04). ETA=15:06:10, max mem: 15.9 GB 
[10/25 12:00:20 visual_prompt]: 	Training 1100/1106. train loss: 160.2779,	0.6168 s / batch. (data: 1.71e-04). ETA=14:46:55, max mem: 15.9 GB 
[10/25 12:00:24 visual_prompt]: Epoch 22 / 100: avg data time: 1.47e-02, avg batch time: 0.6403, average train loss: 86.0008
[10/25 12:01:15 visual_prompt]: 	Test 100/123. loss: 153.510, 0.2257 s / batch. (data: 2.81e-05)max mem: 15.91075 GB 
[10/25 12:01:26 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2321, average loss: 137.3276
[10/25 12:01:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.68	
[10/25 12:01:26 visual_prompt]: Stopping early.
[10/25 12:01:26 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 12:01:26 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 12:01:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 12:01:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 12:01:26 visual_prompt]: Training with config:
[10/25 12:01:26 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr10.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 12:01:26 visual_prompt]: Loading training data...
[10/25 12:01:26 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 12:01:26 visual_prompt]: Loading validation data...
[10/25 12:01:26 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 12:01:26 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/25 12:01:36 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/25 12:01:36 visual_prompt]: tuned percent:0.522
[10/25 12:01:36 visual_prompt]: Device used for model: 0
[10/25 12:01:36 visual_prompt]: Setting up Evaluator...
[10/25 12:01:36 visual_prompt]: Setting up Trainer...
[10/25 12:01:36 visual_prompt]: 	Setting up the optimizer...
[10/25 12:01:36 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 12:02:41 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6173 s / batch. (data: 3.05e-04). ETA=18:56:56, max mem: 15.9 GB 
[10/25 12:03:45 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6208 s / batch. (data: 1.97e-03). ETA=19:02:20, max mem: 15.9 GB 
[10/25 12:04:48 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6483 s / batch. (data: 9.34e-04). ETA=19:51:50, max mem: 15.9 GB 
[10/25 12:05:51 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6355 s / batch. (data: 3.03e-04). ETA=19:27:08, max mem: 15.9 GB 
[10/25 12:06:54 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6245 s / batch. (data: 1.43e-03). ETA=19:05:57, max mem: 15.9 GB 
[10/25 12:07:57 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6235 s / batch. (data: 7.98e-04). ETA=19:03:05, max mem: 15.9 GB 
[10/25 12:09:01 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6195 s / batch. (data: 3.26e-04). ETA=18:54:47, max mem: 15.9 GB 
[10/25 12:10:04 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6650 s / batch. (data: 1.10e-02). ETA=20:16:57, max mem: 15.9 GB 
[10/25 12:11:08 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6502 s / batch. (data: 1.10e-02). ETA=19:48:44, max mem: 15.9 GB 
[10/25 12:12:23 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6166 s / batch. (data: 3.29e-04). ETA=18:46:21, max mem: 15.9 GB 
[10/25 12:13:27 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6176 s / batch. (data: 1.65e-04). ETA=18:47:01, max mem: 15.9 GB 
[10/25 12:13:30 visual_prompt]: Epoch 1 / 100: avg data time: 1.67e-02, avg batch time: 0.6460, average train loss: 1.4028
[10/25 12:14:21 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2445 s / batch. (data: 3.00e-05)max mem: 15.91075 GB 
[10/25 12:14:32 visual_prompt]: Inference (val):avg data time: 9.80e-05, avg batch time: 0.2320, average loss: 1.3505
[10/25 12:14:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/25 12:14:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/25 12:15:37 visual_prompt]: 	Training 100/1106. train loss: 6.1467,	0.6280 s / batch. (data: 2.76e-04). ETA=19:05:04, max mem: 15.9 GB 
[10/25 12:16:40 visual_prompt]: 	Training 200/1106. train loss: 2.9514,	0.6183 s / batch. (data: 3.11e-04). ETA=18:46:16, max mem: 15.9 GB 
[10/25 12:17:44 visual_prompt]: 	Training 300/1106. train loss: 0.0197,	0.6594 s / batch. (data: 7.72e-04). ETA=20:00:06, max mem: 15.9 GB 
[10/25 12:18:47 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6315 s / batch. (data: 8.71e-04). ETA=19:08:11, max mem: 15.9 GB 
[10/25 12:19:50 visual_prompt]: 	Training 500/1106. train loss: 0.7522,	0.6212 s / batch. (data: 9.48e-04). ETA=18:48:26, max mem: 15.9 GB 
[10/25 12:20:53 visual_prompt]: 	Training 600/1106. train loss: 14.0426,	0.6480 s / batch. (data: 7.93e-04). ETA=19:36:02, max mem: 15.9 GB 
[10/25 12:21:56 visual_prompt]: 	Training 700/1106. train loss: 0.0034,	0.6177 s / batch. (data: 3.55e-04). ETA=18:39:57, max mem: 15.9 GB 
[10/25 12:22:59 visual_prompt]: 	Training 800/1106. train loss: 2.1126,	0.6328 s / batch. (data: 5.44e-03). ETA=19:06:21, max mem: 15.9 GB 
[10/25 12:24:03 visual_prompt]: 	Training 900/1106. train loss: 1.9993,	0.6392 s / batch. (data: 2.14e-02). ETA=19:16:57, max mem: 15.9 GB 
[10/25 12:25:06 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6445 s / batch. (data: 3.04e-04). ETA=19:25:26, max mem: 15.9 GB 
[10/25 12:26:09 visual_prompt]: 	Training 1100/1106. train loss: 0.0833,	0.6181 s / batch. (data: 1.51e-04). ETA=18:36:34, max mem: 15.9 GB 
[10/25 12:26:13 visual_prompt]: Epoch 2 / 100: avg data time: 4.33e-03, avg batch time: 0.6338, average train loss: 5.1417
[10/25 12:27:04 visual_prompt]: 	Test 100/123. loss: 4.062, 0.2249 s / batch. (data: 4.03e-05)max mem: 15.91075 GB 
[10/25 12:27:14 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.2332, average loss: 3.7202
[10/25 12:27:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.46	
[10/25 12:27:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/25 12:28:20 visual_prompt]: 	Training 100/1106. train loss: 0.0227,	0.6175 s / batch. (data: 3.17e-04). ETA=18:34:24, max mem: 15.9 GB 
[10/25 12:29:24 visual_prompt]: 	Training 200/1106. train loss: 14.3250,	0.6292 s / batch. (data: 1.20e-02). ETA=18:54:32, max mem: 15.9 GB 
[10/25 12:30:28 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6327 s / batch. (data: 3.24e-04). ETA=18:59:45, max mem: 15.9 GB 
[10/25 12:31:31 visual_prompt]: 	Training 400/1106. train loss: 1.4884,	0.6328 s / batch. (data: 9.34e-04). ETA=18:58:54, max mem: 15.9 GB 
[10/25 12:32:34 visual_prompt]: 	Training 500/1106. train loss: 55.7648,	0.6324 s / batch. (data: 8.30e-04). ETA=18:57:07, max mem: 15.9 GB 
[10/25 12:33:38 visual_prompt]: 	Training 600/1106. train loss: 0.7335,	0.6196 s / batch. (data: 4.67e-04). ETA=18:33:08, max mem: 15.9 GB 
[10/25 12:34:41 visual_prompt]: 	Training 700/1106. train loss: 0.2030,	0.6205 s / batch. (data: 7.93e-04). ETA=18:33:43, max mem: 15.9 GB 
[10/25 12:35:44 visual_prompt]: 	Training 800/1106. train loss: 0.0018,	0.6490 s / batch. (data: 7.37e-04). ETA=19:23:45, max mem: 15.9 GB 
[10/25 12:36:48 visual_prompt]: 	Training 900/1106. train loss: 3.9141,	0.6239 s / batch. (data: 5.44e-03). ETA=18:37:37, max mem: 15.9 GB 
[10/25 12:37:51 visual_prompt]: 	Training 1000/1106. train loss: 23.4155,	0.6367 s / batch. (data: 8.64e-03). ETA=18:59:30, max mem: 15.9 GB 
[10/25 12:38:54 visual_prompt]: 	Training 1100/1106. train loss: 3.5410,	0.6240 s / batch. (data: 1.99e-04). ETA=18:35:51, max mem: 15.9 GB 
[10/25 12:38:58 visual_prompt]: Epoch 3 / 100: avg data time: 6.10e-03, avg batch time: 0.6366, average train loss: 9.1255
[10/25 12:39:50 visual_prompt]: 	Test 100/123. loss: 11.258, 0.2286 s / batch. (data: 4.29e-05)max mem: 15.91075 GB 
[10/25 12:40:01 visual_prompt]: Inference (val):avg data time: 2.76e-04, avg batch time: 0.2329, average loss: 12.3586
[10/25 12:40:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.03	
[10/25 12:40:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/25 12:41:07 visual_prompt]: 	Training 100/1106. train loss: 35.8640,	0.6243 s / batch. (data: 3.45e-04). ETA=18:35:11, max mem: 15.9 GB 
[10/25 12:42:10 visual_prompt]: 	Training 200/1106. train loss: 27.9718,	0.6309 s / batch. (data: 2.66e-04). ETA=18:46:01, max mem: 15.9 GB 
[10/25 12:43:13 visual_prompt]: 	Training 300/1106. train loss: 1.3977,	0.6337 s / batch. (data: 8.22e-04). ETA=18:49:51, max mem: 15.9 GB 
[10/25 12:44:16 visual_prompt]: 	Training 400/1106. train loss: 24.5680,	0.6182 s / batch. (data: 3.38e-04). ETA=18:21:17, max mem: 15.9 GB 
[10/25 12:45:20 visual_prompt]: 	Training 500/1106. train loss: 1.2042,	0.6317 s / batch. (data: 7.78e-04). ETA=18:44:18, max mem: 15.9 GB 
[10/25 12:46:23 visual_prompt]: 	Training 600/1106. train loss: 0.0136,	0.6317 s / batch. (data: 3.04e-04). ETA=18:43:12, max mem: 15.9 GB 
[10/25 12:47:26 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6303 s / batch. (data: 3.14e-04). ETA=18:39:39, max mem: 15.9 GB 
[10/25 12:48:29 visual_prompt]: 	Training 800/1106. train loss: 0.0012,	0.6218 s / batch. (data: 3.30e-04). ETA=18:23:29, max mem: 15.9 GB 
[10/25 12:49:32 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6345 s / batch. (data: 7.23e-04). ETA=18:44:58, max mem: 15.9 GB 
[10/25 12:50:36 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6505 s / batch. (data: 3.84e-03). ETA=19:12:18, max mem: 15.9 GB 
[10/25 12:51:39 visual_prompt]: 	Training 1100/1106. train loss: 11.5305,	0.6177 s / batch. (data: 1.52e-04). ETA=18:13:06, max mem: 15.9 GB 
[10/25 12:51:43 visual_prompt]: Epoch 4 / 100: avg data time: 4.73e-03, avg batch time: 0.6342, average train loss: 14.5035
[10/25 12:52:33 visual_prompt]: 	Test 100/123. loss: 15.265, 0.2256 s / batch. (data: 5.48e-05)max mem: 15.91075 GB 
[10/25 12:52:45 visual_prompt]: Inference (val):avg data time: 3.42e-04, avg batch time: 0.2320, average loss: 13.7194
[10/25 12:52:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.41	
[10/25 12:52:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/25 12:53:50 visual_prompt]: 	Training 100/1106. train loss: 17.3709,	0.6293 s / batch. (data: 3.29e-04). ETA=18:32:32, max mem: 15.9 GB 
[10/25 12:54:53 visual_prompt]: 	Training 200/1106. train loss: 27.3419,	0.6332 s / batch. (data: 1.09e-03). ETA=18:38:25, max mem: 15.9 GB 
[10/25 12:55:56 visual_prompt]: 	Training 300/1106. train loss: 14.1754,	0.6317 s / batch. (data: 3.55e-04). ETA=18:34:45, max mem: 15.9 GB 
[10/25 12:57:00 visual_prompt]: 	Training 400/1106. train loss: 54.3334,	0.6222 s / batch. (data: 7.52e-04). ETA=18:16:49, max mem: 15.9 GB 
[10/25 12:58:03 visual_prompt]: 	Training 500/1106. train loss: 0.0374,	0.6310 s / batch. (data: 7.85e-04). ETA=18:31:23, max mem: 15.9 GB 
[10/25 12:59:07 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6307 s / batch. (data: 7.86e-04). ETA=18:29:43, max mem: 15.9 GB 
[10/25 13:00:10 visual_prompt]: 	Training 700/1106. train loss: 22.2473,	0.6343 s / batch. (data: 3.22e-04). ETA=18:34:58, max mem: 15.9 GB 
[10/25 13:01:13 visual_prompt]: 	Training 800/1106. train loss: 19.8221,	0.6192 s / batch. (data: 7.77e-04). ETA=18:07:28, max mem: 15.9 GB 
[10/25 13:02:16 visual_prompt]: 	Training 900/1106. train loss: 19.6736,	0.6199 s / batch. (data: 2.91e-04). ETA=18:07:41, max mem: 15.9 GB 
[10/25 13:03:19 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6408 s / batch. (data: 3.11e-04). ETA=18:43:16, max mem: 15.9 GB 
[10/25 13:04:23 visual_prompt]: 	Training 1100/1106. train loss: 8.0360,	0.6188 s / batch. (data: 2.18e-04). ETA=18:03:46, max mem: 15.9 GB 
[10/25 13:04:27 visual_prompt]: Epoch 5 / 100: avg data time: 4.65e-03, avg batch time: 0.6346, average train loss: 18.9752
[10/25 13:05:17 visual_prompt]: 	Test 100/123. loss: 46.539, 0.2305 s / batch. (data: 3.17e-05)max mem: 15.91075 GB 
[10/25 13:05:28 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2333, average loss: 42.1742
[10/25 13:05:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.63	
[10/25 13:05:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/25 13:06:33 visual_prompt]: 	Training 100/1106. train loss: 21.3363,	0.6293 s / batch. (data: 3.13e-04). ETA=18:20:54, max mem: 15.9 GB 
[10/25 13:07:37 visual_prompt]: 	Training 200/1106. train loss: 42.4491,	0.6260 s / batch. (data: 7.60e-04). ETA=18:14:07, max mem: 15.9 GB 
[10/25 13:08:40 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6178 s / batch. (data: 8.24e-04). ETA=17:58:46, max mem: 15.9 GB 
[10/25 13:09:43 visual_prompt]: 	Training 400/1106. train loss: 10.6429,	0.6472 s / batch. (data: 5.98e-03). ETA=18:49:04, max mem: 15.9 GB 
[10/25 13:10:46 visual_prompt]: 	Training 500/1106. train loss: 59.7557,	0.6187 s / batch. (data: 3.15e-04). ETA=17:58:22, max mem: 15.9 GB 
[10/25 13:11:49 visual_prompt]: 	Training 600/1106. train loss: 14.9934,	0.6195 s / batch. (data: 3.10e-04). ETA=17:58:40, max mem: 15.9 GB 
[10/25 13:12:53 visual_prompt]: 	Training 700/1106. train loss: 19.8739,	0.6278 s / batch. (data: 1.05e-02). ETA=18:12:00, max mem: 15.9 GB 
[10/25 13:13:56 visual_prompt]: 	Training 800/1106. train loss: 54.8784,	0.6169 s / batch. (data: 2.52e-04). ETA=17:52:00, max mem: 15.9 GB 
[10/25 13:15:02 visual_prompt]: 	Training 900/1106. train loss: 0.0001,	0.6309 s / batch. (data: 3.04e-04). ETA=18:15:23, max mem: 15.9 GB 
[10/25 13:16:05 visual_prompt]: 	Training 1000/1106. train loss: 88.3568,	0.6419 s / batch. (data: 2.78e-04). ETA=18:33:22, max mem: 15.9 GB 
[10/25 13:17:08 visual_prompt]: 	Training 1100/1106. train loss: 22.2601,	0.6184 s / batch. (data: 1.43e-04). ETA=17:51:35, max mem: 15.9 GB 
[10/25 13:17:12 visual_prompt]: Epoch 6 / 100: avg data time: 7.15e-03, avg batch time: 0.6362, average train loss: 24.0322
[10/25 13:18:04 visual_prompt]: 	Test 100/123. loss: 29.590, 0.2303 s / batch. (data: 2.91e-05)max mem: 15.91075 GB 
[10/25 13:18:15 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2320, average loss: 39.3231
[10/25 13:18:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.77	
[10/25 13:18:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/25 13:19:20 visual_prompt]: 	Training 100/1106. train loss: 46.8966,	0.6121 s / batch. (data: 2.66e-04). ETA=17:39:33, max mem: 15.9 GB 
[10/25 13:20:24 visual_prompt]: 	Training 200/1106. train loss: 94.5042,	0.6314 s / batch. (data: 1.37e-02). ETA=18:11:52, max mem: 15.9 GB 
[10/25 13:21:27 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6345 s / batch. (data: 5.48e-03). ETA=18:16:12, max mem: 15.9 GB 
[10/25 13:22:30 visual_prompt]: 	Training 400/1106. train loss: 50.8223,	0.6297 s / batch. (data: 3.52e-04). ETA=18:06:56, max mem: 15.9 GB 
[10/25 13:23:33 visual_prompt]: 	Training 500/1106. train loss: 3.6050,	0.6507 s / batch. (data: 7.27e-04). ETA=18:42:07, max mem: 15.9 GB 
[10/25 13:24:36 visual_prompt]: 	Training 600/1106. train loss: 3.7764,	0.6247 s / batch. (data: 2.83e-04). ETA=17:56:07, max mem: 15.9 GB 
[10/25 13:25:39 visual_prompt]: 	Training 700/1106. train loss: 99.4375,	0.6373 s / batch. (data: 3.29e-04). ETA=18:16:46, max mem: 15.9 GB 
[10/25 13:26:42 visual_prompt]: 	Training 800/1106. train loss: 23.3320,	0.6206 s / batch. (data: 8.20e-04). ETA=17:47:08, max mem: 15.9 GB 
[10/25 13:27:45 visual_prompt]: 	Training 900/1106. train loss: 48.9012,	0.6151 s / batch. (data: 3.30e-04). ETA=17:36:38, max mem: 15.9 GB 
[10/25 13:28:48 visual_prompt]: 	Training 1000/1106. train loss: 12.6098,	0.6288 s / batch. (data: 5.42e-03). ETA=17:59:03, max mem: 15.9 GB 
[10/25 13:29:57 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6108 s / batch. (data: 1.45e-04). ETA=17:27:08, max mem: 15.9 GB 
[10/25 13:30:01 visual_prompt]: Epoch 7 / 100: avg data time: 9.68e-03, avg batch time: 0.6374, average train loss: 28.9812
[10/25 13:31:04 visual_prompt]: 	Test 100/123. loss: 13.383, 0.2355 s / batch. (data: 3.03e-05)max mem: 15.91075 GB 
[10/25 13:31:16 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2306, average loss: 14.6633
[10/25 13:31:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.53	
[10/25 13:31:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/25 13:32:23 visual_prompt]: 	Training 100/1106. train loss: 86.6096,	0.6180 s / batch. (data: 3.50e-04). ETA=17:38:28, max mem: 15.9 GB 
[10/25 13:33:31 visual_prompt]: 	Training 200/1106. train loss: 10.3255,	0.6383 s / batch. (data: 1.30e-02). ETA=18:12:06, max mem: 15.9 GB 
[10/25 13:34:34 visual_prompt]: 	Training 300/1106. train loss: 98.9392,	0.6302 s / batch. (data: 1.20e-02). ETA=17:57:10, max mem: 15.9 GB 
[10/25 13:35:39 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6179 s / batch. (data: 3.07e-04). ETA=17:35:04, max mem: 15.9 GB 
[10/25 13:36:42 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6447 s / batch. (data: 8.10e-04). ETA=18:19:52, max mem: 15.9 GB 
[10/25 13:37:45 visual_prompt]: 	Training 600/1106. train loss: 47.3049,	0.6194 s / batch. (data: 9.51e-04). ETA=17:35:35, max mem: 15.9 GB 
[10/25 13:38:49 visual_prompt]: 	Training 700/1106. train loss: 49.3068,	0.6143 s / batch. (data: 3.15e-04). ETA=17:25:52, max mem: 15.9 GB 
[10/25 13:39:52 visual_prompt]: 	Training 800/1106. train loss: 16.5429,	0.6341 s / batch. (data: 3.03e-04). ETA=17:58:36, max mem: 15.9 GB 
[10/25 13:40:55 visual_prompt]: 	Training 900/1106. train loss: 0.0063,	0.6415 s / batch. (data: 1.11e-02). ETA=18:10:03, max mem: 15.9 GB 
[10/25 13:41:58 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6230 s / batch. (data: 8.15e-04). ETA=17:37:39, max mem: 15.9 GB 
[10/25 13:43:01 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6181 s / batch. (data: 1.59e-04). ETA=17:28:21, max mem: 15.9 GB 
[10/25 13:43:05 visual_prompt]: Epoch 8 / 100: avg data time: 1.24e-02, avg batch time: 0.6406, average train loss: 32.1638
[10/25 13:43:56 visual_prompt]: 	Test 100/123. loss: 87.844, 0.2256 s / batch. (data: 5.36e-05)max mem: 15.91075 GB 
[10/25 13:44:07 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.2323, average loss: 61.6175
[10/25 13:44:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.48	
[10/25 13:44:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/25 13:45:13 visual_prompt]: 	Training 100/1106. train loss: 57.7966,	0.6411 s / batch. (data: 6.01e-03). ETA=18:06:11, max mem: 15.9 GB 
[10/25 13:46:16 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6514 s / batch. (data: 8.23e-04). ETA=18:22:31, max mem: 15.9 GB 
[10/25 13:47:19 visual_prompt]: 	Training 300/1106. train loss: 91.3931,	0.6132 s / batch. (data: 3.51e-04). ETA=17:16:54, max mem: 15.9 GB 
[10/25 13:48:22 visual_prompt]: 	Training 400/1106. train loss: 31.3632,	0.6186 s / batch. (data: 2.86e-04). ETA=17:24:56, max mem: 15.9 GB 
[10/25 13:49:26 visual_prompt]: 	Training 500/1106. train loss: 146.6165,	0.6460 s / batch. (data: 5.48e-03). ETA=18:10:08, max mem: 15.9 GB 
[10/25 13:50:29 visual_prompt]: 	Training 600/1106. train loss: 36.9612,	0.6157 s / batch. (data: 2.76e-04). ETA=17:18:02, max mem: 15.9 GB 
[10/25 13:51:32 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6325 s / batch. (data: 8.68e-04). ETA=17:45:18, max mem: 15.9 GB 
[10/25 13:52:35 visual_prompt]: 	Training 800/1106. train loss: 14.1977,	0.6210 s / batch. (data: 3.61e-04). ETA=17:24:51, max mem: 15.9 GB 
[10/25 13:53:39 visual_prompt]: 	Training 900/1106. train loss: 77.1083,	0.6318 s / batch. (data: 8.18e-04). ETA=17:41:58, max mem: 15.9 GB 
[10/25 13:54:41 visual_prompt]: 	Training 1000/1106. train loss: 48.7408,	0.6192 s / batch. (data: 3.18e-04). ETA=17:19:45, max mem: 15.9 GB 
[10/25 13:55:44 visual_prompt]: 	Training 1100/1106. train loss: 14.7324,	0.6185 s / batch. (data: 1.54e-04). ETA=17:17:29, max mem: 15.9 GB 
[10/25 13:55:48 visual_prompt]: Epoch 9 / 100: avg data time: 4.65e-03, avg batch time: 0.6337, average train loss: 33.5272
[10/25 13:56:38 visual_prompt]: 	Test 100/123. loss: 31.956, 0.2356 s / batch. (data: 3.53e-05)max mem: 15.91075 GB 
[10/25 13:56:49 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.2326, average loss: 28.5068
[10/25 13:56:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.05	
[10/25 13:56:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/25 13:57:54 visual_prompt]: 	Training 100/1106. train loss: 104.0396,	0.6321 s / batch. (data: 7.77e-04). ETA=17:39:17, max mem: 15.9 GB 
[10/25 13:58:57 visual_prompt]: 	Training 200/1106. train loss: 81.3235,	0.6436 s / batch. (data: 2.62e-02). ETA=17:57:31, max mem: 15.9 GB 
[10/25 13:59:59 visual_prompt]: 	Training 300/1106. train loss: 132.9566,	0.6279 s / batch. (data: 3.12e-04). ETA=17:30:11, max mem: 15.9 GB 
[10/25 14:01:02 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6247 s / batch. (data: 8.20e-04). ETA=17:23:48, max mem: 15.9 GB 
[10/25 14:02:05 visual_prompt]: 	Training 500/1106. train loss: 24.1575,	0.6462 s / batch. (data: 8.05e-04). ETA=17:58:35, max mem: 15.9 GB 
[10/25 14:03:08 visual_prompt]: 	Training 600/1106. train loss: 110.0760,	0.6200 s / batch. (data: 8.09e-04). ETA=17:13:47, max mem: 15.9 GB 
[10/25 14:04:12 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6237 s / batch. (data: 7.98e-04). ETA=17:18:52, max mem: 15.9 GB 
[10/25 14:05:15 visual_prompt]: 	Training 800/1106. train loss: 0.3427,	0.6602 s / batch. (data: 1.64e-02). ETA=18:18:41, max mem: 15.9 GB 
[10/25 14:06:17 visual_prompt]: 	Training 900/1106. train loss: 109.7212,	0.6285 s / batch. (data: 8.02e-04). ETA=17:24:46, max mem: 15.9 GB 
[10/25 14:07:20 visual_prompt]: 	Training 1000/1106. train loss: 25.0601,	0.6322 s / batch. (data: 7.52e-04). ETA=17:29:58, max mem: 15.9 GB 
[10/25 14:08:23 visual_prompt]: 	Training 1100/1106. train loss: 42.0017,	0.6196 s / batch. (data: 1.32e-04). ETA=17:08:00, max mem: 15.9 GB 
[10/25 14:08:27 visual_prompt]: Epoch 10 / 100: avg data time: 4.56e-03, avg batch time: 0.6314, average train loss: 43.6057
[10/25 14:09:17 visual_prompt]: 	Test 100/123. loss: 1.949, 0.2566 s / batch. (data: 4.36e-05)max mem: 15.91075 GB 
[10/25 14:09:28 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.2317, average loss: 1.7684
[10/25 14:09:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.03	
[10/25 14:09:28 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/25 14:10:33 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6209 s / batch. (data: 3.35e-04). ETA=17:09:05, max mem: 15.9 GB 
[10/25 14:11:36 visual_prompt]: 	Training 200/1106. train loss: 9.8018,	0.6189 s / batch. (data: 2.68e-04). ETA=17:04:37, max mem: 15.9 GB 
[10/25 14:12:39 visual_prompt]: 	Training 300/1106. train loss: 32.2828,	0.6189 s / batch. (data: 7.77e-04). ETA=17:03:40, max mem: 15.9 GB 
[10/25 14:13:42 visual_prompt]: 	Training 400/1106. train loss: 89.0267,	0.6428 s / batch. (data: 7.98e-04). ETA=17:42:06, max mem: 15.9 GB 
[10/25 14:14:45 visual_prompt]: 	Training 500/1106. train loss: 133.0962,	0.6455 s / batch. (data: 8.18e-04). ETA=17:45:28, max mem: 15.9 GB 
[10/25 14:15:48 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6081 s / batch. (data: 3.23e-04). ETA=16:42:48, max mem: 15.9 GB 
[10/25 14:16:51 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6200 s / batch. (data: 7.27e-04). ETA=17:01:19, max mem: 15.9 GB 
[10/25 14:17:54 visual_prompt]: 	Training 800/1106. train loss: 144.3148,	0.6560 s / batch. (data: 8.21e-04). ETA=17:59:29, max mem: 15.9 GB 
[10/25 14:18:57 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6085 s / batch. (data: 3.07e-04). ETA=16:40:21, max mem: 15.9 GB 
[10/25 14:20:00 visual_prompt]: 	Training 1000/1106. train loss: 1.8484,	0.6397 s / batch. (data: 7.25e-04). ETA=17:30:31, max mem: 15.9 GB 
[10/25 14:21:03 visual_prompt]: 	Training 1100/1106. train loss: 15.0018,	0.6172 s / batch. (data: 1.71e-04). ETA=16:52:32, max mem: 15.9 GB 
[10/25 14:21:07 visual_prompt]: Epoch 11 / 100: avg data time: 4.49e-03, avg batch time: 0.6320, average train loss: 42.7677
[10/25 14:21:59 visual_prompt]: 	Test 100/123. loss: 14.474, 0.2634 s / batch. (data: 4.43e-05)max mem: 15.91075 GB 
[10/25 14:22:09 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.2327, average loss: 12.9429
[10/25 14:22:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.11	
[10/25 14:22:09 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/25 14:23:16 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6413 s / batch. (data: 7.49e-04). ETA=17:30:57, max mem: 15.9 GB 
[10/25 14:24:19 visual_prompt]: 	Training 200/1106. train loss: 82.3152,	0.6141 s / batch. (data: 3.12e-04). ETA=16:45:25, max mem: 15.9 GB 
[10/25 14:25:22 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6233 s / batch. (data: 7.74e-04). ETA=16:59:28, max mem: 15.9 GB 
[10/25 14:26:25 visual_prompt]: 	Training 400/1106. train loss: 256.4879,	0.6307 s / batch. (data: 3.31e-04). ETA=17:10:29, max mem: 15.9 GB 
[10/25 14:27:28 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6252 s / batch. (data: 6.98e-04). ETA=17:00:28, max mem: 15.9 GB 
[10/25 14:28:31 visual_prompt]: 	Training 600/1106. train loss: 7.1752,	0.6240 s / batch. (data: 3.54e-04). ETA=16:57:27, max mem: 15.9 GB 
[10/25 14:29:34 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6381 s / batch. (data: 7.98e-03). ETA=17:19:22, max mem: 15.9 GB 
[10/25 14:30:36 visual_prompt]: 	Training 800/1106. train loss: 10.4285,	0.6360 s / batch. (data: 2.95e-04). ETA=17:14:55, max mem: 15.9 GB 
[10/25 14:31:39 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6201 s / batch. (data: 1.02e-02). ETA=16:47:57, max mem: 15.9 GB 
[10/25 14:32:42 visual_prompt]: 	Training 1000/1106. train loss: 6.6917,	0.6320 s / batch. (data: 7.93e-04). ETA=17:06:15, max mem: 15.9 GB 
[10/25 14:33:45 visual_prompt]: 	Training 1100/1106. train loss: 8.3383,	0.6185 s / batch. (data: 1.44e-04). ETA=16:43:23, max mem: 15.9 GB 
[10/25 14:33:49 visual_prompt]: Epoch 12 / 100: avg data time: 5.45e-03, avg batch time: 0.6329, average train loss: 46.8702
[10/25 14:34:39 visual_prompt]: 	Test 100/123. loss: 3.555, 0.2253 s / batch. (data: 3.93e-05)max mem: 15.91075 GB 
[10/25 14:34:50 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.2329, average loss: 3.7848
[10/25 14:34:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.85	
[10/25 14:34:50 visual_prompt]: Best epoch 12: best metric: -3.785
[10/25 14:34:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/25 14:35:55 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6237 s / batch. (data: 7.87e-04). ETA=16:50:44, max mem: 15.9 GB 
[10/25 14:36:58 visual_prompt]: 	Training 200/1106. train loss: 41.6113,	0.6303 s / batch. (data: 3.45e-04). ETA=17:00:20, max mem: 15.9 GB 
[10/25 14:38:01 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6245 s / batch. (data: 8.29e-04). ETA=16:49:53, max mem: 15.9 GB 
[10/25 14:39:19 visual_prompt]: 	Training 400/1106. train loss: 156.9954,	0.6167 s / batch. (data: 1.87e-04). ETA=16:36:17, max mem: 15.9 GB 
[10/25 14:40:23 visual_prompt]: 	Training 500/1106. train loss: 27.0866,	0.6275 s / batch. (data: 3.96e-04). ETA=16:52:40, max mem: 15.9 GB 
[10/25 14:41:28 visual_prompt]: 	Training 600/1106. train loss: 11.1545,	0.6475 s / batch. (data: 1.24e-02). ETA=17:23:51, max mem: 15.9 GB 
[10/25 14:42:31 visual_prompt]: 	Training 700/1106. train loss: 40.9583,	0.6586 s / batch. (data: 1.64e-02). ETA=17:40:36, max mem: 15.9 GB 
[10/25 14:43:34 visual_prompt]: 	Training 800/1106. train loss: 56.8443,	0.6291 s / batch. (data: 8.13e-04). ETA=16:52:09, max mem: 15.9 GB 
[10/25 14:44:37 visual_prompt]: 	Training 900/1106. train loss: 124.2522,	0.6516 s / batch. (data: 1.96e-02). ETA=17:27:10, max mem: 15.9 GB 
[10/25 14:45:40 visual_prompt]: 	Training 1000/1106. train loss: 115.7907,	0.6476 s / batch. (data: 3.08e-04). ETA=17:19:37, max mem: 15.9 GB 
[10/25 14:46:43 visual_prompt]: 	Training 1100/1106. train loss: 92.9492,	0.6135 s / batch. (data: 1.65e-04). ETA=16:23:55, max mem: 15.9 GB 
[10/25 14:46:47 visual_prompt]: Epoch 13 / 100: avg data time: 2.07e-02, avg batch time: 0.6488, average train loss: 45.4552
[10/25 14:47:38 visual_prompt]: 	Test 100/123. loss: 21.633, 0.2541 s / batch. (data: 4.27e-05)max mem: 15.91075 GB 
[10/25 14:47:48 visual_prompt]: Inference (val):avg data time: 1.27e-04, avg batch time: 0.2321, average loss: 23.3908
[10/25 14:47:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.18	
[10/25 14:47:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/25 14:48:53 visual_prompt]: 	Training 100/1106. train loss: 11.3115,	0.6401 s / batch. (data: 8.08e-04). ETA=17:05:32, max mem: 15.9 GB 
[10/25 14:49:56 visual_prompt]: 	Training 200/1106. train loss: 137.0627,	0.6212 s / batch. (data: 3.41e-04). ETA=16:34:09, max mem: 15.9 GB 
[10/25 14:50:59 visual_prompt]: 	Training 300/1106. train loss: 116.5321,	0.6577 s / batch. (data: 3.36e-02). ETA=17:31:26, max mem: 15.9 GB 
[10/25 14:52:02 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6074 s / batch. (data: 3.07e-04). ETA=16:10:00, max mem: 15.9 GB 
[10/25 14:53:05 visual_prompt]: 	Training 500/1106. train loss: 11.8846,	0.6400 s / batch. (data: 7.98e-04). ETA=17:00:59, max mem: 15.9 GB 
[10/25 14:54:08 visual_prompt]: 	Training 600/1106. train loss: 10.4343,	0.6185 s / batch. (data: 3.19e-04). ETA=16:25:46, max mem: 15.9 GB 
[10/25 14:55:11 visual_prompt]: 	Training 700/1106. train loss: 6.7907,	0.6191 s / batch. (data: 3.01e-04). ETA=16:25:33, max mem: 15.9 GB 
[10/25 14:56:14 visual_prompt]: 	Training 800/1106. train loss: 2.9617,	0.6299 s / batch. (data: 7.84e-04). ETA=16:41:43, max mem: 15.9 GB 
[10/25 14:57:17 visual_prompt]: 	Training 900/1106. train loss: 13.7419,	0.6179 s / batch. (data: 3.23e-04). ETA=16:21:43, max mem: 15.9 GB 
[10/25 14:58:20 visual_prompt]: 	Training 1000/1106. train loss: 177.7343,	0.6430 s / batch. (data: 7.73e-04). ETA=17:00:28, max mem: 15.9 GB 
[10/25 14:59:23 visual_prompt]: 	Training 1100/1106. train loss: 52.9075,	0.6123 s / batch. (data: 1.61e-04). ETA=16:10:46, max mem: 15.9 GB 
[10/25 14:59:27 visual_prompt]: Epoch 14 / 100: avg data time: 4.33e-03, avg batch time: 0.6316, average train loss: 45.6685
[10/25 15:00:16 visual_prompt]: 	Test 100/123. loss: 13.666, 0.2333 s / batch. (data: 2.77e-05)max mem: 15.91075 GB 
[10/25 15:00:27 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2317, average loss: 15.1594
[10/25 15:00:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.93	
[10/25 15:00:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/25 15:01:32 visual_prompt]: 	Training 100/1106. train loss: 66.7784,	0.6281 s / batch. (data: 2.55e-04). ETA=16:34:35, max mem: 15.9 GB 
[10/25 15:02:35 visual_prompt]: 	Training 200/1106. train loss: 34.3307,	0.6398 s / batch. (data: 2.98e-04). ETA=16:52:04, max mem: 15.9 GB 
[10/25 15:03:37 visual_prompt]: 	Training 300/1106. train loss: 81.3374,	0.6327 s / batch. (data: 7.47e-04). ETA=16:39:47, max mem: 15.9 GB 
[10/25 15:04:40 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6323 s / batch. (data: 3.40e-04). ETA=16:38:09, max mem: 15.9 GB 
[10/25 15:05:43 visual_prompt]: 	Training 500/1106. train loss: 28.8154,	0.6374 s / batch. (data: 3.32e-04). ETA=16:45:10, max mem: 15.9 GB 
[10/25 15:06:46 visual_prompt]: 	Training 600/1106. train loss: 14.8117,	0.6205 s / batch. (data: 2.84e-04). ETA=16:17:27, max mem: 15.9 GB 
[10/25 15:07:49 visual_prompt]: 	Training 700/1106. train loss: 159.0582,	0.6333 s / batch. (data: 7.75e-04). ETA=16:36:37, max mem: 15.9 GB 
[10/25 15:08:52 visual_prompt]: 	Training 800/1106. train loss: 16.1152,	0.6530 s / batch. (data: 7.98e-04). ETA=17:06:25, max mem: 15.9 GB 
[10/25 15:09:55 visual_prompt]: 	Training 900/1106. train loss: 206.3544,	0.6294 s / batch. (data: 3.44e-04). ETA=16:28:17, max mem: 15.9 GB 
[10/25 15:10:58 visual_prompt]: 	Training 1000/1106. train loss: 52.6322,	0.6139 s / batch. (data: 2.80e-04). ETA=16:02:59, max mem: 15.9 GB 
[10/25 15:12:01 visual_prompt]: 	Training 1100/1106. train loss: 10.7279,	0.6189 s / batch. (data: 2.12e-04). ETA=16:09:50, max mem: 15.9 GB 
[10/25 15:12:05 visual_prompt]: Epoch 15 / 100: avg data time: 3.73e-03, avg batch time: 0.6310, average train loss: 47.3116
[10/25 15:12:55 visual_prompt]: 	Test 100/123. loss: 133.860, 0.2257 s / batch. (data: 3.89e-05)max mem: 15.91075 GB 
[10/25 15:13:05 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.2318, average loss: 118.5119
[10/25 15:13:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.85	
[10/25 15:13:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/25 15:14:10 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6086 s / batch. (data: 3.01e-04). ETA=15:52:28, max mem: 15.9 GB 
[10/25 15:15:13 visual_prompt]: 	Training 200/1106. train loss: 0.1998,	0.6192 s / batch. (data: 8.00e-04). ETA=16:08:03, max mem: 15.9 GB 
[10/25 15:16:16 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6355 s / batch. (data: 3.00e-04). ETA=16:32:37, max mem: 15.9 GB 
[10/25 15:17:19 visual_prompt]: 	Training 400/1106. train loss: 46.8186,	0.6374 s / batch. (data: 7.19e-04). ETA=16:34:25, max mem: 15.9 GB 
[10/25 15:18:22 visual_prompt]: 	Training 500/1106. train loss: 72.9939,	0.6361 s / batch. (data: 8.45e-04). ETA=16:31:18, max mem: 15.9 GB 
[10/25 15:19:25 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6133 s / batch. (data: 7.86e-04). ETA=15:54:45, max mem: 15.9 GB 
[10/25 15:20:28 visual_prompt]: 	Training 700/1106. train loss: 80.7113,	0.6312 s / batch. (data: 3.01e-04). ETA=16:21:36, max mem: 15.9 GB 
[10/25 15:21:31 visual_prompt]: 	Training 800/1106. train loss: 4.4138,	0.6378 s / batch. (data: 3.34e-04). ETA=16:30:53, max mem: 15.9 GB 
[10/25 15:22:34 visual_prompt]: 	Training 900/1106. train loss: 22.7875,	0.6211 s / batch. (data: 7.76e-04). ETA=16:03:46, max mem: 15.9 GB 
[10/25 15:23:37 visual_prompt]: 	Training 1000/1106. train loss: 30.7473,	0.6460 s / batch. (data: 7.87e-04). ETA=16:41:28, max mem: 15.9 GB 
[10/25 15:24:40 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6176 s / batch. (data: 1.44e-04). ETA=15:56:19, max mem: 15.9 GB 
[10/25 15:24:44 visual_prompt]: Epoch 16 / 100: avg data time: 4.42e-03, avg batch time: 0.6320, average train loss: 47.4599
[10/25 15:25:35 visual_prompt]: 	Test 100/123. loss: 110.836, 0.2366 s / batch. (data: 4.27e-05)max mem: 15.91075 GB 
[10/25 15:25:46 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2315, average loss: 122.4307
[10/25 15:25:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.68	
[10/25 15:25:46 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/25 15:26:51 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6434 s / batch. (data: 7.96e-04). ETA=16:35:06, max mem: 15.9 GB 
[10/25 15:27:54 visual_prompt]: 	Training 200/1106. train loss: 82.9534,	0.6295 s / batch. (data: 8.97e-04). ETA=16:12:39, max mem: 15.9 GB 
[10/25 15:28:57 visual_prompt]: 	Training 300/1106. train loss: 109.4528,	0.6413 s / batch. (data: 8.76e-04). ETA=16:29:46, max mem: 15.9 GB 
[10/25 15:30:00 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6268 s / batch. (data: 2.86e-04). ETA=16:06:21, max mem: 15.9 GB 
[10/25 15:31:03 visual_prompt]: 	Training 500/1106. train loss: 74.1691,	0.6183 s / batch. (data: 2.94e-04). ETA=15:52:09, max mem: 15.9 GB 
[10/25 15:32:06 visual_prompt]: 	Training 600/1106. train loss: 28.9356,	0.6374 s / batch. (data: 1.06e-02). ETA=16:20:32, max mem: 15.9 GB 
[10/25 15:33:09 visual_prompt]: 	Training 700/1106. train loss: 21.5381,	0.6306 s / batch. (data: 7.41e-04). ETA=16:09:06, max mem: 15.9 GB 
[10/25 15:34:12 visual_prompt]: 	Training 800/1106. train loss: 1.1501,	0.6305 s / batch. (data: 2.78e-04). ETA=16:07:51, max mem: 15.9 GB 
[10/25 15:35:15 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6320 s / batch. (data: 7.67e-04). ETA=16:09:03, max mem: 15.9 GB 
[10/25 15:36:18 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6184 s / batch. (data: 3.23e-04). ETA=15:47:17, max mem: 15.9 GB 
[10/25 15:37:21 visual_prompt]: 	Training 1100/1106. train loss: 51.2182,	0.6182 s / batch. (data: 1.53e-04). ETA=15:45:55, max mem: 15.9 GB 
[10/25 15:37:25 visual_prompt]: Epoch 17 / 100: avg data time: 4.67e-03, avg batch time: 0.6317, average train loss: 46.9747
[10/25 15:38:18 visual_prompt]: 	Test 100/123. loss: 34.067, 0.2357 s / batch. (data: 3.34e-05)max mem: 15.91075 GB 
[10/25 15:38:31 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2301, average loss: 36.9797
[10/25 15:38:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.04	
[10/25 15:38:31 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/25 15:39:37 visual_prompt]: 	Training 100/1106. train loss: 123.1337,	0.6249 s / batch. (data: 3.36e-04). ETA=15:55:00, max mem: 15.9 GB 
[10/25 15:40:45 visual_prompt]: 	Training 200/1106. train loss: 96.5348,	0.6209 s / batch. (data: 8.22e-04). ETA=15:47:57, max mem: 15.9 GB 
[10/25 15:41:50 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6240 s / batch. (data: 8.20e-04). ETA=15:51:33, max mem: 15.9 GB 
[10/25 15:42:53 visual_prompt]: 	Training 400/1106. train loss: 75.4723,	0.6365 s / batch. (data: 7.76e-04). ETA=16:09:38, max mem: 15.9 GB 
[10/25 15:43:56 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6196 s / batch. (data: 3.15e-04). ETA=15:42:52, max mem: 15.9 GB 
[10/25 15:44:59 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6401 s / batch. (data: 1.52e-02). ETA=16:12:55, max mem: 15.9 GB 
[10/25 15:46:02 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6243 s / batch. (data: 7.97e-04). ETA=15:47:48, max mem: 15.9 GB 
[10/25 15:47:05 visual_prompt]: 	Training 800/1106. train loss: 86.6115,	0.6258 s / batch. (data: 7.63e-04). ETA=15:49:04, max mem: 15.9 GB 
[10/25 15:48:09 visual_prompt]: 	Training 900/1106. train loss: 5.0458,	0.6547 s / batch. (data: 2.74e-02). ETA=16:31:52, max mem: 15.9 GB 
[10/25 15:49:12 visual_prompt]: 	Training 1000/1106. train loss: 43.4874,	0.6153 s / batch. (data: 3.13e-04). ETA=15:31:11, max mem: 15.9 GB 
[10/25 15:50:15 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6089 s / batch. (data: 2.01e-04). ETA=15:20:23, max mem: 15.9 GB 
[10/25 15:50:19 visual_prompt]: Epoch 18 / 100: avg data time: 1.26e-02, avg batch time: 0.6398, average train loss: 41.2234
[10/25 15:51:09 visual_prompt]: 	Test 100/123. loss: 76.274, 0.2317 s / batch. (data: 3.27e-05)max mem: 15.91075 GB 
[10/25 15:51:20 visual_prompt]: Inference (val):avg data time: 1.06e-04, avg batch time: 0.2336, average loss: 69.1275
[10/25 15:51:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.89	
[10/25 15:51:20 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/25 15:52:26 visual_prompt]: 	Training 100/1106. train loss: 13.6935,	0.6326 s / batch. (data: 8.33e-04). ETA=15:55:09, max mem: 15.9 GB 
[10/25 15:53:29 visual_prompt]: 	Training 200/1106. train loss: 64.1446,	0.6145 s / batch. (data: 3.85e-04). ETA=15:26:47, max mem: 15.9 GB 
[10/25 15:54:32 visual_prompt]: 	Training 300/1106. train loss: 40.8884,	0.6372 s / batch. (data: 5.93e-03). ETA=15:59:59, max mem: 15.9 GB 
[10/25 15:55:35 visual_prompt]: 	Training 400/1106. train loss: 30.6979,	0.6176 s / batch. (data: 3.64e-04). ETA=15:29:28, max mem: 15.9 GB 
[10/25 15:56:38 visual_prompt]: 	Training 500/1106. train loss: 45.8783,	0.6187 s / batch. (data: 3.76e-04). ETA=15:30:04, max mem: 15.9 GB 
[10/25 15:57:41 visual_prompt]: 	Training 600/1106. train loss: 47.8579,	0.6194 s / batch. (data: 3.36e-04). ETA=15:29:59, max mem: 15.9 GB 
[10/25 15:58:44 visual_prompt]: 	Training 700/1106. train loss: 15.0759,	0.6328 s / batch. (data: 9.10e-04). ETA=15:49:05, max mem: 15.9 GB 
[10/25 15:59:47 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6474 s / batch. (data: 7.85e-04). ETA=16:09:51, max mem: 15.9 GB 
[10/25 16:00:51 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6176 s / batch. (data: 3.24e-04). ETA=15:24:12, max mem: 15.9 GB 
[10/25 16:01:54 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6323 s / batch. (data: 8.54e-04). ETA=15:45:10, max mem: 15.9 GB 
[10/25 16:02:57 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6301 s / batch. (data: 1.72e-04). ETA=15:40:53, max mem: 15.9 GB 
[10/25 16:03:00 visual_prompt]: Epoch 19 / 100: avg data time: 4.88e-03, avg batch time: 0.6330, average train loss: 46.3218
[10/25 16:03:51 visual_prompt]: 	Test 100/123. loss: 10.295, 0.2251 s / batch. (data: 4.32e-05)max mem: 15.91075 GB 
[10/25 16:04:01 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2329, average loss: 9.3101
[10/25 16:04:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.93	
[10/25 16:04:01 visual_prompt]: Stopping early.
[10/25 16:04:01 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 16:04:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 16:04:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 16:04:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 16:04:01 visual_prompt]: Training with config:
[10/25 16:04:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr10.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 16:04:01 visual_prompt]: Loading training data...
[10/25 16:04:01 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 16:04:02 visual_prompt]: Loading validation data...
[10/25 16:04:02 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 16:04:02 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/25 16:04:10 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/25 16:04:10 visual_prompt]: tuned percent:0.522
[10/25 16:04:10 visual_prompt]: Device used for model: 0
[10/25 16:04:10 visual_prompt]: Setting up Evaluator...
[10/25 16:04:10 visual_prompt]: Setting up Trainer...
[10/25 16:04:10 visual_prompt]: 	Setting up the optimizer...
[10/25 16:04:10 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 16:05:16 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6296 s / batch. (data: 3.49e-04). ETA=19:19:30, max mem: 15.9 GB 
[10/25 16:06:19 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6416 s / batch. (data: 2.99e-04). ETA=19:40:31, max mem: 15.9 GB 
[10/25 16:07:22 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6268 s / batch. (data: 3.23e-04). ETA=19:12:21, max mem: 15.9 GB 
[10/25 16:08:26 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6293 s / batch. (data: 3.71e-04). ETA=19:15:49, max mem: 15.9 GB 
[10/25 16:09:29 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6189 s / batch. (data: 3.20e-04). ETA=18:55:45, max mem: 15.9 GB 
[10/25 16:10:32 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6187 s / batch. (data: 2.84e-04). ETA=18:54:21, max mem: 15.9 GB 
[10/25 16:11:35 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6321 s / batch. (data: 7.31e-03). ETA=19:17:44, max mem: 15.9 GB 
[10/25 16:12:39 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6320 s / batch. (data: 3.36e-04). ETA=19:16:30, max mem: 15.9 GB 
[10/25 16:13:42 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6510 s / batch. (data: 8.03e-04). ETA=19:50:13, max mem: 15.9 GB 
[10/25 16:14:46 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6194 s / batch. (data: 3.36e-04). ETA=18:51:26, max mem: 15.9 GB 
[10/25 16:15:49 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6189 s / batch. (data: 1.48e-04). ETA=18:49:31, max mem: 15.9 GB 
[10/25 16:15:53 visual_prompt]: Epoch 1 / 100: avg data time: 5.14e-03, avg batch time: 0.6356, average train loss: 1.4028
[10/25 16:16:44 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2316 s / batch. (data: 3.22e-05)max mem: 15.91075 GB 
[10/25 16:16:55 visual_prompt]: Inference (val):avg data time: 2.56e-04, avg batch time: 0.2335, average loss: 1.3505
[10/25 16:16:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/25 16:16:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/25 16:18:00 visual_prompt]: 	Training 100/1106. train loss: 1.7100,	0.6306 s / batch. (data: 3.08e-04). ETA=19:09:45, max mem: 15.9 GB 
[10/25 16:19:03 visual_prompt]: 	Training 200/1106. train loss: 3.2151,	0.6312 s / batch. (data: 8.15e-04). ETA=19:09:41, max mem: 15.9 GB 
[10/25 16:20:07 visual_prompt]: 	Training 300/1106. train loss: 0.1563,	0.6560 s / batch. (data: 3.64e-04). ETA=19:53:47, max mem: 15.9 GB 
[10/25 16:21:10 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6311 s / batch. (data: 7.85e-04). ETA=19:07:28, max mem: 15.9 GB 
[10/25 16:22:13 visual_prompt]: 	Training 500/1106. train loss: 2.1564,	0.6498 s / batch. (data: 8.20e-04). ETA=19:40:26, max mem: 15.9 GB 
[10/25 16:23:17 visual_prompt]: 	Training 600/1106. train loss: 1.2461,	0.6240 s / batch. (data: 3.50e-04). ETA=18:52:30, max mem: 15.9 GB 
[10/25 16:24:20 visual_prompt]: 	Training 700/1106. train loss: 0.0002,	0.6198 s / batch. (data: 8.04e-04). ETA=18:43:53, max mem: 15.9 GB 
[10/25 16:25:23 visual_prompt]: 	Training 800/1106. train loss: 6.1891,	0.6320 s / batch. (data: 3.45e-04). ETA=19:04:53, max mem: 15.9 GB 
[10/25 16:26:27 visual_prompt]: 	Training 900/1106. train loss: 0.9419,	0.6387 s / batch. (data: 2.83e-04). ETA=19:15:59, max mem: 15.9 GB 
[10/25 16:27:30 visual_prompt]: 	Training 1000/1106. train loss: 0.0689,	0.6463 s / batch. (data: 1.20e-02). ETA=19:28:38, max mem: 15.9 GB 
[10/25 16:28:34 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6184 s / batch. (data: 2.76e-04). ETA=18:37:13, max mem: 15.9 GB 
[10/25 16:28:37 visual_prompt]: Epoch 2 / 100: avg data time: 4.16e-03, avg batch time: 0.6349, average train loss: 3.5626
[10/25 16:29:29 visual_prompt]: 	Test 100/123. loss: 15.245, 0.2250 s / batch. (data: 4.55e-05)max mem: 15.91075 GB 
[10/25 16:29:40 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.2323, average loss: 13.7605
[10/25 16:29:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.56	
[10/25 16:29:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/25 16:30:47 visual_prompt]: 	Training 100/1106. train loss: 28.9298,	0.6331 s / batch. (data: 3.59e-04). ETA=19:02:39, max mem: 15.9 GB 
[10/25 16:31:51 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6566 s / batch. (data: 8.65e-04). ETA=19:43:57, max mem: 15.9 GB 
[10/25 16:32:54 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6640 s / batch. (data: 8.69e-04). ETA=19:56:15, max mem: 15.9 GB 
[10/25 16:33:58 visual_prompt]: 	Training 400/1106. train loss: 5.1477,	0.6200 s / batch. (data: 3.81e-04). ETA=18:35:54, max mem: 15.9 GB 
[10/25 16:35:01 visual_prompt]: 	Training 500/1106. train loss: 25.5062,	0.6198 s / batch. (data: 3.26e-04). ETA=18:34:25, max mem: 15.9 GB 
[10/25 16:36:05 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6315 s / batch. (data: 8.30e-04). ETA=18:54:22, max mem: 15.9 GB 
[10/25 16:37:08 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6509 s / batch. (data: 1.29e-03). ETA=19:28:18, max mem: 15.9 GB 
[10/25 16:38:12 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6440 s / batch. (data: 8.37e-04). ETA=19:14:43, max mem: 15.9 GB 
[10/25 16:39:15 visual_prompt]: 	Training 900/1106. train loss: 12.4604,	0.6434 s / batch. (data: 1.25e-02). ETA=19:12:38, max mem: 15.9 GB 
[10/25 16:40:19 visual_prompt]: 	Training 1000/1106. train loss: 24.8876,	0.6199 s / batch. (data: 3.21e-04). ETA=18:29:29, max mem: 15.9 GB 
[10/25 16:41:22 visual_prompt]: 	Training 1100/1106. train loss: 3.6136,	0.6195 s / batch. (data: 1.48e-04). ETA=18:27:43, max mem: 15.9 GB 
[10/25 16:41:26 visual_prompt]: Epoch 3 / 100: avg data time: 5.93e-03, avg batch time: 0.6380, average train loss: 8.7661
[10/25 16:42:17 visual_prompt]: 	Test 100/123. loss: 5.449, 0.2366 s / batch. (data: 4.67e-05)max mem: 15.91075 GB 
[10/25 16:42:28 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.2342, average loss: 5.9644
[10/25 16:42:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.91	
[10/25 16:42:28 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/25 16:43:33 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6168 s / batch. (data: 3.03e-04). ETA=18:21:45, max mem: 15.9 GB 
[10/25 16:44:44 visual_prompt]: 	Training 200/1106. train loss: 174.3007,	0.6520 s / batch. (data: 7.97e-04). ETA=19:23:34, max mem: 15.9 GB 
[10/25 16:45:47 visual_prompt]: 	Training 300/1106. train loss: 9.3755,	0.6318 s / batch. (data: 8.37e-04). ETA=18:46:28, max mem: 15.9 GB 
[10/25 16:46:51 visual_prompt]: 	Training 400/1106. train loss: 14.3281,	0.6357 s / batch. (data: 1.19e-02). ETA=18:52:20, max mem: 15.9 GB 
[10/25 16:47:54 visual_prompt]: 	Training 500/1106. train loss: 14.5855,	0.6281 s / batch. (data: 3.52e-04). ETA=18:37:54, max mem: 15.9 GB 
[10/25 16:48:58 visual_prompt]: 	Training 600/1106. train loss: 19.3834,	0.6289 s / batch. (data: 3.34e-04). ETA=18:38:12, max mem: 15.9 GB 
[10/25 16:50:01 visual_prompt]: 	Training 700/1106. train loss: 25.7760,	0.6552 s / batch. (data: 1.06e-02). ETA=19:23:56, max mem: 15.9 GB 
[10/25 16:51:04 visual_prompt]: 	Training 800/1106. train loss: 12.2513,	0.6178 s / batch. (data: 3.45e-04). ETA=18:16:26, max mem: 15.9 GB 
[10/25 16:52:08 visual_prompt]: 	Training 900/1106. train loss: 35.6281,	0.6644 s / batch. (data: 4.48e-02). ETA=19:38:01, max mem: 15.9 GB 
[10/25 16:53:11 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6240 s / batch. (data: 3.28e-04). ETA=18:25:22, max mem: 15.9 GB 
[10/25 16:54:21 visual_prompt]: 	Training 1100/1106. train loss: 18.0929,	0.6182 s / batch. (data: 1.71e-04). ETA=18:14:06, max mem: 15.9 GB 
[10/25 16:54:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.78e-02, avg batch time: 0.6484, average train loss: 15.4437
[10/25 16:55:17 visual_prompt]: 	Test 100/123. loss: 4.350, 0.2285 s / batch. (data: 4.51e-05)max mem: 15.91075 GB 
[10/25 16:55:28 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.2311, average loss: 3.9260
[10/25 16:55:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.00	
[10/25 16:55:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/25 16:56:33 visual_prompt]: 	Training 100/1106. train loss: 14.4634,	0.6435 s / batch. (data: 8.49e-04). ETA=18:57:42, max mem: 15.9 GB 
[10/25 16:57:38 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6358 s / batch. (data: 1.28e-02). ETA=18:43:04, max mem: 15.9 GB 
[10/25 16:58:41 visual_prompt]: 	Training 300/1106. train loss: 25.4288,	0.6321 s / batch. (data: 8.08e-04). ETA=18:35:21, max mem: 15.9 GB 
[10/25 16:59:44 visual_prompt]: 	Training 400/1106. train loss: 46.4899,	0.6291 s / batch. (data: 1.56e-02). ETA=18:28:59, max mem: 15.9 GB 
[10/25 17:00:48 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6083 s / batch. (data: 3.21e-04). ETA=17:51:24, max mem: 15.9 GB 
[10/25 17:01:51 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6197 s / batch. (data: 8.35e-04). ETA=18:10:30, max mem: 15.9 GB 
[10/25 17:02:55 visual_prompt]: 	Training 700/1106. train loss: 27.5350,	0.6243 s / batch. (data: 3.58e-04). ETA=18:17:26, max mem: 15.9 GB 
[10/25 17:03:58 visual_prompt]: 	Training 800/1106. train loss: 57.0465,	0.6195 s / batch. (data: 2.91e-04). ETA=18:07:55, max mem: 15.9 GB 
[10/25 17:05:01 visual_prompt]: 	Training 900/1106. train loss: 24.3653,	0.6356 s / batch. (data: 1.23e-02). ETA=18:35:17, max mem: 15.9 GB 
[10/25 17:06:05 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6234 s / batch. (data: 1.07e-03). ETA=18:12:50, max mem: 15.9 GB 
[10/25 17:07:08 visual_prompt]: 	Training 1100/1106. train loss: 9.1945,	0.6200 s / batch. (data: 1.65e-04). ETA=18:05:48, max mem: 15.9 GB 
[10/25 17:07:12 visual_prompt]: Epoch 5 / 100: avg data time: 6.16e-03, avg batch time: 0.6364, average train loss: 20.0251
[10/25 17:08:02 visual_prompt]: 	Test 100/123. loss: 34.570, 0.2330 s / batch. (data: 6.25e-05)max mem: 15.91075 GB 
[10/25 17:08:13 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.2327, average loss: 31.1052
[10/25 17:08:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.75	
[10/25 17:08:13 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/25 17:09:18 visual_prompt]: 	Training 100/1106. train loss: 4.0634,	0.6322 s / batch. (data: 8.30e-04). ETA=18:26:05, max mem: 15.9 GB 
[10/25 17:10:21 visual_prompt]: 	Training 200/1106. train loss: 50.4204,	0.6265 s / batch. (data: 3.13e-04). ETA=18:15:04, max mem: 15.9 GB 
[10/25 17:11:25 visual_prompt]: 	Training 300/1106. train loss: 0.0118,	0.6311 s / batch. (data: 8.42e-04). ETA=18:22:00, max mem: 15.9 GB 
[10/25 17:12:28 visual_prompt]: 	Training 400/1106. train loss: 0.3101,	0.6329 s / batch. (data: 3.10e-04). ETA=18:24:09, max mem: 15.9 GB 
[10/25 17:13:31 visual_prompt]: 	Training 500/1106. train loss: 4.3926,	0.6334 s / batch. (data: 8.41e-04). ETA=18:23:54, max mem: 15.9 GB 
[10/25 17:14:34 visual_prompt]: 	Training 600/1106. train loss: 50.3340,	0.6304 s / batch. (data: 2.89e-04). ETA=18:17:35, max mem: 15.9 GB 
[10/25 17:15:38 visual_prompt]: 	Training 700/1106. train loss: 0.7773,	0.6686 s / batch. (data: 1.61e-02). ETA=19:23:00, max mem: 15.9 GB 
[10/25 17:16:41 visual_prompt]: 	Training 800/1106. train loss: 37.7082,	0.6480 s / batch. (data: 7.94e-04). ETA=18:46:06, max mem: 15.9 GB 
[10/25 17:17:44 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6085 s / batch. (data: 2.96e-04). ETA=17:36:27, max mem: 15.9 GB 
[10/25 17:18:48 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6437 s / batch. (data: 8.11e-04). ETA=18:36:33, max mem: 15.9 GB 
[10/25 17:19:51 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6084 s / batch. (data: 2.33e-04). ETA=17:34:20, max mem: 15.9 GB 
[10/25 17:19:55 visual_prompt]: Epoch 6 / 100: avg data time: 4.54e-03, avg batch time: 0.6346, average train loss: 25.0174
[10/25 17:20:46 visual_prompt]: 	Test 100/123. loss: 4.340, 0.2370 s / batch. (data: 4.70e-05)max mem: 15.91075 GB 
[10/25 17:20:57 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.2328, average loss: 4.2162
[10/25 17:20:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.63	
[10/25 17:20:57 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/25 17:22:02 visual_prompt]: 	Training 100/1106. train loss: 41.3164,	0.6306 s / batch. (data: 9.47e-04). ETA=18:11:33, max mem: 15.9 GB 
[10/25 17:23:06 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6124 s / batch. (data: 3.00e-04). ETA=17:39:03, max mem: 15.9 GB 
[10/25 17:24:09 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6293 s / batch. (data: 1.01e-03). ETA=18:07:11, max mem: 15.9 GB 
[10/25 17:25:12 visual_prompt]: 	Training 400/1106. train loss: 12.0239,	0.6288 s / batch. (data: 3.28e-04). ETA=18:05:23, max mem: 15.9 GB 
[10/25 17:26:15 visual_prompt]: 	Training 500/1106. train loss: 11.8349,	0.6360 s / batch. (data: 3.19e-04). ETA=18:16:42, max mem: 15.9 GB 
[10/25 17:27:19 visual_prompt]: 	Training 600/1106. train loss: 14.5099,	0.6404 s / batch. (data: 6.61e-03). ETA=18:23:17, max mem: 15.9 GB 
[10/25 17:28:22 visual_prompt]: 	Training 700/1106. train loss: 70.1576,	0.6256 s / batch. (data: 2.88e-04). ETA=17:56:43, max mem: 15.9 GB 
[10/25 17:29:25 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6299 s / batch. (data: 8.49e-04). ETA=18:03:06, max mem: 15.9 GB 
[10/25 17:30:29 visual_prompt]: 	Training 900/1106. train loss: 11.6541,	0.6352 s / batch. (data: 2.77e-04). ETA=18:11:10, max mem: 15.9 GB 
[10/25 17:31:31 visual_prompt]: 	Training 1000/1106. train loss: 12.3490,	0.6364 s / batch. (data: 1.20e-03). ETA=18:12:07, max mem: 15.9 GB 
[10/25 17:32:35 visual_prompt]: 	Training 1100/1106. train loss: 26.7056,	0.6176 s / batch. (data: 1.58e-04). ETA=17:38:52, max mem: 15.9 GB 
[10/25 17:32:38 visual_prompt]: Epoch 7 / 100: avg data time: 4.30e-03, avg batch time: 0.6342, average train loss: 31.1541
[10/25 17:33:29 visual_prompt]: 	Test 100/123. loss: 38.482, 0.2252 s / batch. (data: 3.84e-05)max mem: 15.91075 GB 
[10/25 17:33:40 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.2323, average loss: 35.0175
[10/25 17:33:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.88	
[10/25 17:33:40 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/25 17:34:45 visual_prompt]: 	Training 100/1106. train loss: 86.7569,	0.6184 s / batch. (data: 3.11e-04). ETA=17:39:04, max mem: 15.9 GB 
[10/25 17:35:48 visual_prompt]: 	Training 200/1106. train loss: 13.3235,	0.6263 s / batch. (data: 3.06e-04). ETA=17:51:30, max mem: 15.9 GB 
[10/25 17:36:51 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6315 s / batch. (data: 8.31e-04). ETA=17:59:22, max mem: 15.9 GB 
[10/25 17:37:54 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6179 s / batch. (data: 2.42e-04). ETA=17:35:09, max mem: 15.9 GB 
[10/25 17:38:58 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6227 s / batch. (data: 5.44e-03). ETA=17:42:19, max mem: 15.9 GB 
[10/25 17:40:01 visual_prompt]: 	Training 600/1106. train loss: 18.3559,	0.6449 s / batch. (data: 7.88e-04). ETA=18:19:10, max mem: 15.9 GB 
[10/25 17:41:04 visual_prompt]: 	Training 700/1106. train loss: 76.2945,	0.6182 s / batch. (data: 5.91e-03). ETA=17:32:35, max mem: 15.9 GB 
[10/25 17:42:07 visual_prompt]: 	Training 800/1106. train loss: 97.1253,	0.6433 s / batch. (data: 8.08e-04). ETA=18:14:15, max mem: 15.9 GB 
[10/25 17:43:10 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6127 s / batch. (data: 2.65e-04). ETA=17:21:12, max mem: 15.9 GB 
[10/25 17:44:14 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6178 s / batch. (data: 4.15e-04). ETA=17:28:45, max mem: 15.9 GB 
[10/25 17:45:17 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6193 s / batch. (data: 1.90e-04). ETA=17:30:22, max mem: 15.9 GB 
[10/25 17:45:21 visual_prompt]: Epoch 8 / 100: avg data time: 5.34e-03, avg batch time: 0.6342, average train loss: 30.5052
[10/25 17:46:13 visual_prompt]: 	Test 100/123. loss: 20.941, 0.2255 s / batch. (data: 4.48e-05)max mem: 15.91075 GB 
[10/25 17:46:25 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.2317, average loss: 18.6071
[10/25 17:46:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.90	
[10/25 17:46:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/25 17:47:30 visual_prompt]: 	Training 100/1106. train loss: 105.7321,	0.6278 s / batch. (data: 3.35e-04). ETA=17:43:39, max mem: 15.9 GB 
[10/25 17:48:33 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6203 s / batch. (data: 4.94e-04). ETA=17:29:57, max mem: 15.9 GB 
[10/25 17:49:36 visual_prompt]: 	Training 300/1106. train loss: 40.4179,	0.6156 s / batch. (data: 3.52e-04). ETA=17:20:54, max mem: 15.9 GB 
[10/25 17:50:39 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6237 s / batch. (data: 5.44e-03). ETA=17:33:29, max mem: 15.9 GB 
[10/25 17:51:43 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6280 s / batch. (data: 3.09e-04). ETA=17:39:46, max mem: 15.9 GB 
[10/25 17:52:46 visual_prompt]: 	Training 600/1106. train loss: 26.8081,	0.6293 s / batch. (data: 2.97e-04). ETA=17:40:58, max mem: 15.9 GB 
[10/25 17:53:49 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6300 s / batch. (data: 3.25e-04). ETA=17:41:03, max mem: 15.9 GB 
[10/25 17:54:53 visual_prompt]: 	Training 800/1106. train loss: 18.3484,	0.6344 s / batch. (data: 8.52e-04). ETA=17:47:21, max mem: 15.9 GB 
[10/25 17:55:56 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6356 s / batch. (data: 8.04e-04). ETA=17:48:21, max mem: 15.9 GB 
[10/25 17:56:59 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6423 s / batch. (data: 8.59e-04). ETA=17:58:37, max mem: 15.9 GB 
[10/25 17:58:03 visual_prompt]: 	Training 1100/1106. train loss: 19.6571,	0.6185 s / batch. (data: 1.61e-04). ETA=17:17:36, max mem: 15.9 GB 
[10/25 17:58:07 visual_prompt]: Epoch 9 / 100: avg data time: 6.02e-03, avg batch time: 0.6351, average train loss: 35.5773
[10/25 17:58:58 visual_prompt]: 	Test 100/123. loss: 45.204, 0.2454 s / batch. (data: 3.08e-05)max mem: 15.91075 GB 
[10/25 17:59:08 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.2320, average loss: 40.6098
[10/25 17:59:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.59	
[10/25 17:59:08 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/25 18:00:13 visual_prompt]: 	Training 100/1106. train loss: 152.1136,	0.6122 s / batch. (data: 2.75e-04). ETA=17:05:58, max mem: 15.9 GB 
[10/25 18:01:16 visual_prompt]: 	Training 200/1106. train loss: 40.3461,	0.6422 s / batch. (data: 3.26e-04). ETA=17:55:04, max mem: 15.9 GB 
[10/25 18:02:19 visual_prompt]: 	Training 300/1106. train loss: 53.2975,	0.6129 s / batch. (data: 2.25e-04). ETA=17:05:05, max mem: 15.9 GB 
[10/25 18:03:23 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6450 s / batch. (data: 1.24e-03). ETA=17:57:36, max mem: 15.9 GB 
[10/25 18:04:26 visual_prompt]: 	Training 500/1106. train loss: 17.3414,	0.6471 s / batch. (data: 2.31e-02). ETA=18:00:05, max mem: 15.9 GB 
[10/25 18:05:29 visual_prompt]: 	Training 600/1106. train loss: 106.2027,	0.6359 s / batch. (data: 1.20e-02). ETA=17:40:15, max mem: 15.9 GB 
[10/25 18:06:32 visual_prompt]: 	Training 700/1106. train loss: 129.1223,	0.6331 s / batch. (data: 1.15e-03). ETA=17:34:35, max mem: 15.9 GB 
[10/25 18:07:35 visual_prompt]: 	Training 800/1106. train loss: 63.5188,	0.6471 s / batch. (data: 1.10e-02). ETA=17:56:47, max mem: 15.9 GB 
[10/25 18:08:38 visual_prompt]: 	Training 900/1106. train loss: 73.2846,	0.6523 s / batch. (data: 8.35e-04). ETA=18:04:26, max mem: 15.9 GB 
[10/25 18:09:41 visual_prompt]: 	Training 1000/1106. train loss: 15.1541,	0.6648 s / batch. (data: 1.56e-02). ETA=18:24:06, max mem: 15.9 GB 
[10/25 18:10:44 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6185 s / batch. (data: 1.36e-04). ETA=17:06:09, max mem: 15.9 GB 
[10/25 18:10:48 visual_prompt]: Epoch 10 / 100: avg data time: 4.28e-03, avg batch time: 0.6325, average train loss: 49.0911
[10/25 18:11:38 visual_prompt]: 	Test 100/123. loss: 1.237, 0.2398 s / batch. (data: 2.41e-05)max mem: 15.91075 GB 
[10/25 18:11:48 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.2329, average loss: 1.1074
[10/25 18:11:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.52	
[10/25 18:11:48 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/25 18:12:53 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6416 s / batch. (data: 2.13e-02). ETA=17:43:15, max mem: 15.9 GB 
[10/25 18:13:57 visual_prompt]: 	Training 200/1106. train loss: 23.8125,	0.6411 s / batch. (data: 6.09e-03). ETA=17:41:28, max mem: 15.9 GB 
[10/25 18:15:00 visual_prompt]: 	Training 300/1106. train loss: 23.7760,	0.6320 s / batch. (data: 7.80e-04). ETA=17:25:18, max mem: 15.9 GB 
[10/25 18:16:03 visual_prompt]: 	Training 400/1106. train loss: 0.0057,	0.6441 s / batch. (data: 9.77e-04). ETA=17:44:16, max mem: 15.9 GB 
[10/25 18:17:07 visual_prompt]: 	Training 500/1106. train loss: 28.1228,	0.6411 s / batch. (data: 8.71e-04). ETA=17:38:14, max mem: 15.9 GB 
[10/25 18:18:10 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6182 s / batch. (data: 3.33e-04). ETA=16:59:24, max mem: 15.9 GB 
[10/25 18:19:13 visual_prompt]: 	Training 700/1106. train loss: 111.5070,	0.6236 s / batch. (data: 5.46e-03). ETA=17:07:16, max mem: 15.9 GB 
[10/25 18:20:16 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6518 s / batch. (data: 1.83e-02). ETA=17:52:34, max mem: 15.9 GB 
[10/25 18:21:19 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6231 s / batch. (data: 3.29e-04). ETA=17:04:22, max mem: 15.9 GB 
[10/25 18:22:23 visual_prompt]: 	Training 1000/1106. train loss: 4.9809,	0.6242 s / batch. (data: 8.01e-04). ETA=17:05:09, max mem: 15.9 GB 
[10/25 18:23:26 visual_prompt]: 	Training 1100/1106. train loss: 4.3107,	0.6184 s / batch. (data: 1.41e-04). ETA=16:54:32, max mem: 15.9 GB 
[10/25 18:23:29 visual_prompt]: Epoch 11 / 100: avg data time: 4.70e-03, avg batch time: 0.6340, average train loss: 48.6266
[10/25 18:24:20 visual_prompt]: 	Test 100/123. loss: 61.358, 0.2397 s / batch. (data: 3.50e-05)max mem: 15.91075 GB 
[10/25 18:24:31 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.2333, average loss: 55.6473
[10/25 18:24:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.89	
[10/25 18:24:31 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/25 18:25:37 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6138 s / batch. (data: 3.50e-04). ETA=16:45:59, max mem: 15.9 GB 
[10/25 18:26:40 visual_prompt]: 	Training 200/1106. train loss: 17.8369,	0.6337 s / batch. (data: 8.91e-04). ETA=17:17:29, max mem: 15.9 GB 
[10/25 18:27:43 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6215 s / batch. (data: 2.98e-04). ETA=16:56:34, max mem: 15.9 GB 
[10/25 18:28:46 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6173 s / batch. (data: 3.27e-04). ETA=16:48:34, max mem: 15.9 GB 
[10/25 18:29:49 visual_prompt]: 	Training 500/1106. train loss: 142.2167,	0.6302 s / batch. (data: 1.08e-02). ETA=17:08:41, max mem: 15.9 GB 
[10/25 18:30:52 visual_prompt]: 	Training 600/1106. train loss: 64.2382,	0.6325 s / batch. (data: 3.04e-04). ETA=17:11:15, max mem: 15.9 GB 
[10/25 18:31:55 visual_prompt]: 	Training 700/1106. train loss: 19.7694,	0.6178 s / batch. (data: 2.80e-04). ETA=16:46:16, max mem: 15.9 GB 
[10/25 18:32:58 visual_prompt]: 	Training 800/1106. train loss: 13.7606,	0.6183 s / batch. (data: 3.11e-04). ETA=16:46:05, max mem: 15.9 GB 
[10/25 18:34:02 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6434 s / batch. (data: 9.33e-04). ETA=17:25:56, max mem: 15.9 GB 
[10/25 18:35:04 visual_prompt]: 	Training 1000/1106. train loss: 2.0494,	0.6191 s / batch. (data: 2.97e-04). ETA=16:45:26, max mem: 15.9 GB 
[10/25 18:36:07 visual_prompt]: 	Training 1100/1106. train loss: 4.3937,	0.6193 s / batch. (data: 1.54e-04). ETA=16:44:41, max mem: 15.9 GB 
[10/25 18:36:11 visual_prompt]: Epoch 12 / 100: avg data time: 5.35e-03, avg batch time: 0.6332, average train loss: 48.9991
[10/25 18:37:01 visual_prompt]: 	Test 100/123. loss: 114.614, 0.2255 s / batch. (data: 3.03e-05)max mem: 15.91075 GB 
[10/25 18:37:11 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.2319, average loss: 104.1373
[10/25 18:37:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[10/25 18:37:11 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/25 18:38:17 visual_prompt]: 	Training 100/1106. train loss: 4.9540,	0.6330 s / batch. (data: 1.13e-03). ETA=17:05:41, max mem: 15.9 GB 
[10/25 18:39:20 visual_prompt]: 	Training 200/1106. train loss: 46.9648,	0.6273 s / batch. (data: 4.33e-04). ETA=16:55:32, max mem: 15.9 GB 
[10/25 18:40:23 visual_prompt]: 	Training 300/1106. train loss: 186.6184,	0.6200 s / batch. (data: 3.13e-04). ETA=16:42:41, max mem: 15.9 GB 
[10/25 18:41:26 visual_prompt]: 	Training 400/1106. train loss: 107.3346,	0.6329 s / batch. (data: 8.09e-04). ETA=17:02:21, max mem: 15.9 GB 
[10/25 18:42:29 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6395 s / batch. (data: 3.54e-04). ETA=17:11:57, max mem: 15.9 GB 
[10/25 18:43:32 visual_prompt]: 	Training 600/1106. train loss: 13.2931,	0.6336 s / batch. (data: 7.80e-04). ETA=17:01:27, max mem: 15.9 GB 
[10/25 18:44:35 visual_prompt]: 	Training 700/1106. train loss: 30.9828,	0.6326 s / batch. (data: 8.44e-04). ETA=16:58:44, max mem: 15.9 GB 
[10/25 18:45:38 visual_prompt]: 	Training 800/1106. train loss: 31.1850,	0.6195 s / batch. (data: 3.21e-04). ETA=16:36:42, max mem: 15.9 GB 
[10/25 18:46:41 visual_prompt]: 	Training 900/1106. train loss: 71.4547,	0.6308 s / batch. (data: 3.93e-04). ETA=16:53:47, max mem: 15.9 GB 
[10/25 18:47:44 visual_prompt]: 	Training 1000/1106. train loss: 37.2085,	0.6265 s / batch. (data: 3.19e-04). ETA=16:45:47, max mem: 15.9 GB 
[10/25 18:48:48 visual_prompt]: 	Training 1100/1106. train loss: 91.4504,	0.6137 s / batch. (data: 1.89e-04). ETA=16:24:13, max mem: 15.9 GB 
[10/25 18:48:51 visual_prompt]: Epoch 13 / 100: avg data time: 4.28e-03, avg batch time: 0.6328, average train loss: 46.4686
[10/25 18:49:41 visual_prompt]: 	Test 100/123. loss: 13.789, 0.2251 s / batch. (data: 3.22e-05)max mem: 15.91075 GB 
[10/25 18:49:52 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2326, average loss: 15.2070
[10/25 18:49:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.14	
[10/25 18:49:52 visual_prompt]: Best epoch 13: best metric: -15.207
[10/25 18:49:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/25 18:50:57 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6119 s / batch. (data: 3.21e-04). ETA=16:20:20, max mem: 15.9 GB 
[10/25 18:52:00 visual_prompt]: 	Training 200/1106. train loss: 74.4512,	0.6264 s / batch. (data: 8.32e-04). ETA=16:42:29, max mem: 15.9 GB 
[10/25 18:53:04 visual_prompt]: 	Training 300/1106. train loss: 26.1458,	0.6250 s / batch. (data: 8.41e-04). ETA=16:39:09, max mem: 15.9 GB 
[10/25 18:54:07 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6361 s / batch. (data: 8.28e-04). ETA=16:55:52, max mem: 15.9 GB 
[10/25 18:55:10 visual_prompt]: 	Training 500/1106. train loss: 24.7912,	0.6331 s / batch. (data: 7.71e-04). ETA=16:50:05, max mem: 15.9 GB 
[10/25 18:56:13 visual_prompt]: 	Training 600/1106. train loss: 53.1805,	0.6144 s / batch. (data: 3.21e-04). ETA=16:19:13, max mem: 15.9 GB 
[10/25 18:57:16 visual_prompt]: 	Training 700/1106. train loss: 0.2586,	0.6396 s / batch. (data: 3.33e-04). ETA=16:58:19, max mem: 15.9 GB 
[10/25 18:58:19 visual_prompt]: 	Training 800/1106. train loss: 13.8014,	0.6482 s / batch. (data: 8.14e-04). ETA=17:10:49, max mem: 15.9 GB 
[10/25 18:59:22 visual_prompt]: 	Training 900/1106. train loss: 16.9894,	0.6341 s / batch. (data: 1.59e-02). ETA=16:47:27, max mem: 15.9 GB 
[10/25 19:00:25 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6235 s / batch. (data: 5.44e-03). ETA=16:29:27, max mem: 15.9 GB 
[10/25 19:01:28 visual_prompt]: 	Training 1100/1106. train loss: 61.3309,	0.6143 s / batch. (data: 1.59e-04). ETA=16:13:57, max mem: 15.9 GB 
[10/25 19:01:32 visual_prompt]: Epoch 14 / 100: avg data time: 4.63e-03, avg batch time: 0.6331, average train loss: 46.1833
[10/25 19:02:22 visual_prompt]: 	Test 100/123. loss: 46.219, 0.2392 s / batch. (data: 3.96e-05)max mem: 15.91075 GB 
[10/25 19:02:33 visual_prompt]: Inference (val):avg data time: 9.86e-05, avg batch time: 0.2318, average loss: 50.6632
[10/25 19:02:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.05	
[10/25 19:02:33 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/25 19:03:38 visual_prompt]: 	Training 100/1106. train loss: 4.6196,	0.6189 s / batch. (data: 2.90e-04). ETA=16:20:01, max mem: 15.9 GB 
[10/25 19:04:41 visual_prompt]: 	Training 200/1106. train loss: 82.7448,	0.6261 s / batch. (data: 8.38e-04). ETA=16:30:31, max mem: 15.9 GB 
[10/25 19:05:44 visual_prompt]: 	Training 300/1106. train loss: 37.8253,	0.6207 s / batch. (data: 3.29e-04). ETA=16:20:50, max mem: 15.9 GB 
[10/25 19:06:47 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6299 s / batch. (data: 8.46e-04). ETA=16:34:18, max mem: 15.9 GB 
[10/25 19:07:50 visual_prompt]: 	Training 500/1106. train loss: 0.0001,	0.6338 s / batch. (data: 7.84e-04). ETA=16:39:32, max mem: 15.9 GB 
[10/25 19:08:53 visual_prompt]: 	Training 600/1106. train loss: 46.3502,	0.6141 s / batch. (data: 3.19e-04). ETA=16:07:24, max mem: 15.9 GB 
[10/25 19:09:56 visual_prompt]: 	Training 700/1106. train loss: 40.8452,	0.6293 s / batch. (data: 7.33e-04). ETA=16:30:19, max mem: 15.9 GB 
[10/25 19:10:59 visual_prompt]: 	Training 800/1106. train loss: 31.2973,	0.6308 s / batch. (data: 3.24e-04). ETA=16:31:39, max mem: 15.9 GB 
[10/25 19:12:02 visual_prompt]: 	Training 900/1106. train loss: 118.1073,	0.6345 s / batch. (data: 8.15e-04). ETA=16:36:24, max mem: 15.9 GB 
[10/25 19:13:05 visual_prompt]: 	Training 1000/1106. train loss: 30.2378,	0.6196 s / batch. (data: 3.12e-04). ETA=16:11:52, max mem: 15.9 GB 
[10/25 19:14:08 visual_prompt]: 	Training 1100/1106. train loss: 220.0521,	0.6120 s / batch. (data: 1.60e-04). ETA=15:58:55, max mem: 15.9 GB 
[10/25 19:14:12 visual_prompt]: Epoch 15 / 100: avg data time: 4.03e-03, avg batch time: 0.6322, average train loss: 57.6171
[10/25 19:15:02 visual_prompt]: 	Test 100/123. loss: 153.864, 0.2350 s / batch. (data: 3.22e-05)max mem: 15.91075 GB 
[10/25 19:15:13 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.2320, average loss: 138.7282
[10/25 19:15:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.83	
[10/25 19:15:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/25 19:16:18 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6329 s / batch. (data: 8.27e-04). ETA=16:30:37, max mem: 15.9 GB 
[10/25 19:17:21 visual_prompt]: 	Training 200/1106. train loss: 24.1162,	0.6297 s / batch. (data: 7.71e-04). ETA=16:24:34, max mem: 15.9 GB 
[10/25 19:18:24 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6229 s / batch. (data: 3.26e-04). ETA=16:12:49, max mem: 15.9 GB 
[10/25 19:19:27 visual_prompt]: 	Training 400/1106. train loss: 55.1100,	0.6408 s / batch. (data: 8.21e-04). ETA=16:39:47, max mem: 15.9 GB 
[10/25 19:20:30 visual_prompt]: 	Training 500/1106. train loss: 59.8602,	0.6480 s / batch. (data: 7.96e-04). ETA=16:49:57, max mem: 15.9 GB 
[10/25 19:21:33 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6486 s / batch. (data: 7.94e-04). ETA=16:49:46, max mem: 15.9 GB 
[10/25 19:22:37 visual_prompt]: 	Training 700/1106. train loss: 26.7243,	0.6323 s / batch. (data: 7.30e-04). ETA=16:23:17, max mem: 15.9 GB 
[10/25 19:23:40 visual_prompt]: 	Training 800/1106. train loss: 7.9696,	0.6314 s / batch. (data: 3.53e-04). ETA=16:20:52, max mem: 15.9 GB 
[10/25 19:24:43 visual_prompt]: 	Training 900/1106. train loss: 14.9741,	0.6316 s / batch. (data: 3.20e-04). ETA=16:20:10, max mem: 15.9 GB 
[10/25 19:25:46 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6412 s / batch. (data: 7.58e-04). ETA=16:33:55, max mem: 15.9 GB 
[10/25 19:26:50 visual_prompt]: 	Training 1100/1106. train loss: 217.1639,	0.6190 s / batch. (data: 1.36e-04). ETA=15:58:28, max mem: 15.9 GB 
[10/25 19:26:53 visual_prompt]: Epoch 16 / 100: avg data time: 4.55e-03, avg batch time: 0.6334, average train loss: 45.1143
[10/25 19:27:45 visual_prompt]: 	Test 100/123. loss: 35.340, 0.2277 s / batch. (data: 2.88e-05)max mem: 15.91075 GB 
[10/25 19:27:56 visual_prompt]: Inference (val):avg data time: 1.32e-04, avg batch time: 0.2327, average loss: 38.6181
[10/25 19:27:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.88	
[10/25 19:27:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/25 19:29:02 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6383 s / batch. (data: 1.43e-02). ETA=16:27:18, max mem: 15.9 GB 
[10/25 19:30:05 visual_prompt]: 	Training 200/1106. train loss: 30.3129,	0.6584 s / batch. (data: 9.16e-04). ETA=16:57:14, max mem: 15.9 GB 
[10/25 19:31:09 visual_prompt]: 	Training 300/1106. train loss: 72.2367,	0.6177 s / batch. (data: 3.21e-04). ETA=15:53:25, max mem: 15.9 GB 
[10/25 19:32:12 visual_prompt]: 	Training 400/1106. train loss: 136.4375,	0.6585 s / batch. (data: 1.10e-02). ETA=16:55:17, max mem: 15.9 GB 
[10/25 19:33:15 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6076 s / batch. (data: 2.93e-04). ETA=15:35:47, max mem: 15.9 GB 
[10/25 19:34:18 visual_prompt]: 	Training 600/1106. train loss: 155.6936,	0.6423 s / batch. (data: 5.98e-03). ETA=16:28:08, max mem: 15.9 GB 
[10/25 19:35:21 visual_prompt]: 	Training 700/1106. train loss: 19.1267,	0.6179 s / batch. (data: 3.11e-04). ETA=15:49:34, max mem: 15.9 GB 
[10/25 19:36:24 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6183 s / batch. (data: 3.03e-04). ETA=15:49:04, max mem: 15.9 GB 
[10/25 19:37:27 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6459 s / batch. (data: 1.56e-02). ETA=16:30:23, max mem: 15.9 GB 
[10/25 19:38:30 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6140 s / batch. (data: 3.33e-04). ETA=15:40:25, max mem: 15.9 GB 
[10/25 19:39:33 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6070 s / batch. (data: 1.61e-04). ETA=15:28:47, max mem: 15.9 GB 
[10/25 19:39:37 visual_prompt]: Epoch 17 / 100: avg data time: 5.06e-03, avg batch time: 0.6335, average train loss: 52.6906
[10/25 19:40:28 visual_prompt]: 	Test 100/123. loss: 18.732, 0.2486 s / batch. (data: 3.93e-05)max mem: 15.91075 GB 
[10/25 19:40:38 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.2330, average loss: 16.7948
[10/25 19:40:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.82	
[10/25 19:40:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/25 19:41:43 visual_prompt]: 	Training 100/1106. train loss: 106.7852,	0.6310 s / batch. (data: 3.42e-04). ETA=16:04:20, max mem: 15.9 GB 
[10/25 19:42:46 visual_prompt]: 	Training 200/1106. train loss: 52.2741,	0.6193 s / batch. (data: 3.12e-04). ETA=15:45:22, max mem: 15.9 GB 
[10/25 19:43:49 visual_prompt]: 	Training 300/1106. train loss: 495.8181,	0.6192 s / batch. (data: 2.86e-04). ETA=15:44:11, max mem: 15.9 GB 
[10/25 19:44:52 visual_prompt]: 	Training 400/1106. train loss: 109.4459,	0.6318 s / batch. (data: 9.78e-04). ETA=16:02:28, max mem: 15.9 GB 
[10/25 19:45:56 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6131 s / batch. (data: 2.36e-04). ETA=15:32:57, max mem: 15.9 GB 
[10/25 19:46:58 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6315 s / batch. (data: 7.98e-04). ETA=15:59:52, max mem: 15.9 GB 
[10/25 19:48:01 visual_prompt]: 	Training 700/1106. train loss: 0.0738,	0.6175 s / batch. (data: 2.91e-04). ETA=15:37:29, max mem: 15.9 GB 
[10/25 19:49:04 visual_prompt]: 	Training 800/1106. train loss: 31.7582,	0.6311 s / batch. (data: 8.03e-04). ETA=15:57:12, max mem: 15.9 GB 
[10/25 19:50:07 visual_prompt]: 	Training 900/1106. train loss: 39.2423,	0.6239 s / batch. (data: 3.04e-04). ETA=15:45:12, max mem: 15.9 GB 
[10/25 19:51:11 visual_prompt]: 	Training 1000/1106. train loss: 30.9112,	0.6336 s / batch. (data: 8.83e-04). ETA=15:58:52, max mem: 15.9 GB 
[10/25 19:52:13 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6079 s / batch. (data: 1.41e-04). ETA=15:18:51, max mem: 15.9 GB 
[10/25 19:52:17 visual_prompt]: Epoch 18 / 100: avg data time: 4.18e-03, avg batch time: 0.6321, average train loss: 47.3443
[10/25 19:53:07 visual_prompt]: 	Test 100/123. loss: 109.033, 0.2262 s / batch. (data: 3.15e-05)max mem: 15.91075 GB 
[10/25 19:53:17 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.2323, average loss: 98.3225
[10/25 19:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.06	
[10/25 19:53:17 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/25 19:54:22 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6197 s / batch. (data: 8.23e-04). ETA=15:35:39, max mem: 15.9 GB 
[10/25 19:55:25 visual_prompt]: 	Training 200/1106. train loss: 81.5315,	0.6517 s / batch. (data: 1.05e-02). ETA=16:22:49, max mem: 15.9 GB 
[10/25 19:56:28 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6224 s / batch. (data: 5.47e-03). ETA=15:37:41, max mem: 15.9 GB 
[10/25 19:57:31 visual_prompt]: 	Training 400/1106. train loss: 32.9376,	0.6188 s / batch. (data: 2.77e-04). ETA=15:31:10, max mem: 15.9 GB 
[10/25 19:58:35 visual_prompt]: 	Training 500/1106. train loss: 298.0979,	0.6330 s / batch. (data: 7.52e-04). ETA=15:51:28, max mem: 15.9 GB 
[10/25 19:59:38 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6174 s / batch. (data: 5.52e-04). ETA=15:27:03, max mem: 15.9 GB 
[10/25 20:00:41 visual_prompt]: 	Training 700/1106. train loss: 21.4647,	0.6320 s / batch. (data: 7.90e-04). ETA=15:47:50, max mem: 15.9 GB 
[10/25 20:01:44 visual_prompt]: 	Training 800/1106. train loss: 39.0186,	0.6251 s / batch. (data: 3.86e-04). ETA=15:36:31, max mem: 15.9 GB 
[10/25 20:02:47 visual_prompt]: 	Training 900/1106. train loss: 72.6179,	0.6402 s / batch. (data: 8.13e-04). ETA=15:58:00, max mem: 15.9 GB 
[10/25 20:03:50 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6390 s / batch. (data: 8.47e-04). ETA=15:55:17, max mem: 15.9 GB 
[10/25 20:04:53 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6071 s / batch. (data: 2.05e-04). ETA=15:06:33, max mem: 15.9 GB 
[10/25 20:04:57 visual_prompt]: Epoch 19 / 100: avg data time: 4.30e-03, avg batch time: 0.6323, average train loss: 51.9440
[10/25 20:05:47 visual_prompt]: 	Test 100/123. loss: 164.586, 0.2260 s / batch. (data: 4.43e-05)max mem: 15.91075 GB 
[10/25 20:05:57 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.2317, average loss: 149.2204
[10/25 20:05:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.16	
[10/25 20:05:57 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[10/25 20:07:02 visual_prompt]: 	Training 100/1106. train loss: 77.2589,	0.6497 s / batch. (data: 8.04e-04). ETA=16:09:02, max mem: 15.9 GB 
[10/25 20:08:05 visual_prompt]: 	Training 200/1106. train loss: 18.3881,	0.6341 s / batch. (data: 3.32e-04). ETA=15:44:39, max mem: 15.9 GB 
[10/25 20:09:08 visual_prompt]: 	Training 300/1106. train loss: 44.5936,	0.6338 s / batch. (data: 8.54e-04). ETA=15:43:06, max mem: 15.9 GB 
[10/25 20:10:12 visual_prompt]: 	Training 400/1106. train loss: 36.2711,	0.6349 s / batch. (data: 1.14e-03). ETA=15:43:47, max mem: 15.9 GB 
[10/25 20:11:15 visual_prompt]: 	Training 500/1106. train loss: 43.8184,	0.6441 s / batch. (data: 7.84e-04). ETA=15:56:24, max mem: 15.9 GB 
[10/25 20:12:18 visual_prompt]: 	Training 600/1106. train loss: 7.9852,	0.6186 s / batch. (data: 2.82e-04). ETA=15:17:29, max mem: 15.9 GB 
[10/25 20:13:21 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6240 s / batch. (data: 3.19e-04). ETA=15:24:21, max mem: 15.9 GB 
[10/25 20:14:25 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6339 s / batch. (data: 7.85e-04). ETA=15:38:03, max mem: 15.9 GB 
[10/25 20:15:27 visual_prompt]: 	Training 900/1106. train loss: 64.4762,	0.6220 s / batch. (data: 3.18e-04). ETA=15:19:19, max mem: 15.9 GB 
[10/25 20:16:30 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6317 s / batch. (data: 8.08e-04). ETA=15:32:41, max mem: 15.9 GB 
[10/25 20:17:34 visual_prompt]: 	Training 1100/1106. train loss: 69.2932,	0.6184 s / batch. (data: 1.60e-04). ETA=15:11:58, max mem: 15.9 GB 
[10/25 20:17:38 visual_prompt]: Epoch 20 / 100: avg data time: 4.76e-03, avg batch time: 0.6334, average train loss: 52.6835
[10/25 20:18:27 visual_prompt]: 	Test 100/123. loss: 6.069, 0.2433 s / batch. (data: 3.60e-05)max mem: 15.91075 GB 
[10/25 20:18:38 visual_prompt]: Inference (val):avg data time: 1.18e-04, avg batch time: 0.2321, average loss: 5.1492
[10/25 20:18:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.00	
[10/25 20:18:38 visual_prompt]: Best epoch 20: best metric: -5.149
[10/25 20:18:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[10/25 20:19:43 visual_prompt]: 	Training 100/1106. train loss: 51.0885,	0.6411 s / batch. (data: 8.23e-04). ETA=15:44:17, max mem: 15.9 GB 
[10/25 20:20:46 visual_prompt]: 	Training 200/1106. train loss: 36.2326,	0.6640 s / batch. (data: 8.08e-04). ETA=16:17:01, max mem: 15.9 GB 
[10/25 20:21:50 visual_prompt]: 	Training 300/1106. train loss: 53.7393,	0.6347 s / batch. (data: 8.37e-04). ETA=15:32:51, max mem: 15.9 GB 
[10/25 20:22:52 visual_prompt]: 	Training 400/1106. train loss: 96.3842,	0.6295 s / batch. (data: 1.58e-02). ETA=15:24:03, max mem: 15.9 GB 
[10/25 20:23:56 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6140 s / batch. (data: 3.18e-04). ETA=15:00:21, max mem: 15.9 GB 
[10/25 20:24:59 visual_prompt]: 	Training 600/1106. train loss: 37.3478,	0.6411 s / batch. (data: 8.26e-04). ETA=15:38:58, max mem: 15.9 GB 
[10/25 20:26:02 visual_prompt]: 	Training 700/1106. train loss: 15.3318,	0.6352 s / batch. (data: 8.44e-04). ETA=15:29:18, max mem: 15.9 GB 
[10/25 20:27:05 visual_prompt]: 	Training 800/1106. train loss: 29.8003,	0.6305 s / batch. (data: 1.20e-02). ETA=15:21:20, max mem: 15.9 GB 
[10/25 20:28:08 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6440 s / batch. (data: 8.27e-04). ETA=15:39:59, max mem: 15.9 GB 
[10/25 20:29:11 visual_prompt]: 	Training 1000/1106. train loss: 30.1870,	0.6429 s / batch. (data: 8.11e-04). ETA=15:37:18, max mem: 15.9 GB 
[10/25 20:30:14 visual_prompt]: 	Training 1100/1106. train loss: 16.5275,	0.6204 s / batch. (data: 1.43e-04). ETA=15:03:30, max mem: 15.9 GB 
[10/25 20:30:18 visual_prompt]: Epoch 21 / 100: avg data time: 4.51e-03, avg batch time: 0.6329, average train loss: 54.4599
[10/25 20:31:08 visual_prompt]: 	Test 100/123. loss: 26.791, 0.2367 s / batch. (data: 3.00e-05)max mem: 15.91075 GB 
[10/25 20:31:19 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2343, average loss: 24.1878
[10/25 20:31:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.57	
[10/25 20:31:19 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[10/25 20:32:24 visual_prompt]: 	Training 100/1106. train loss: 0.7373,	0.6495 s / batch. (data: 2.05e-02). ETA=15:44:43, max mem: 15.9 GB 
[10/25 20:33:27 visual_prompt]: 	Training 200/1106. train loss: 48.1502,	0.6344 s / batch. (data: 8.24e-04). ETA=15:21:41, max mem: 15.9 GB 
[10/25 20:34:30 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6323 s / batch. (data: 8.47e-04). ETA=15:17:35, max mem: 15.9 GB 
[10/25 20:35:33 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6235 s / batch. (data: 3.14e-04). ETA=15:03:45, max mem: 15.9 GB 
[10/25 20:36:36 visual_prompt]: 	Training 500/1106. train loss: 68.6118,	0.6430 s / batch. (data: 8.13e-04). ETA=15:30:56, max mem: 15.9 GB 
[10/25 20:37:39 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6223 s / batch. (data: 7.85e-04). ETA=14:59:59, max mem: 15.9 GB 
[10/25 20:38:42 visual_prompt]: 	Training 700/1106. train loss: 18.1375,	0.6511 s / batch. (data: 7.93e-04). ETA=15:40:33, max mem: 15.9 GB 
[10/25 20:39:45 visual_prompt]: 	Training 800/1106. train loss: 99.4596,	0.6317 s / batch. (data: 8.35e-04). ETA=15:11:31, max mem: 15.9 GB 
[10/25 20:40:49 visual_prompt]: 	Training 900/1106. train loss: 45.2873,	0.6406 s / batch. (data: 8.10e-04). ETA=15:23:18, max mem: 15.9 GB 
[10/25 20:41:51 visual_prompt]: 	Training 1000/1106. train loss: 118.3765,	0.6146 s / batch. (data: 2.83e-04). ETA=14:44:48, max mem: 15.9 GB 
[10/25 20:42:55 visual_prompt]: 	Training 1100/1106. train loss: 50.7535,	0.6176 s / batch. (data: 1.63e-04). ETA=14:47:59, max mem: 15.9 GB 
[10/25 20:42:58 visual_prompt]: Epoch 22 / 100: avg data time: 4.23e-03, avg batch time: 0.6324, average train loss: 45.2313
[10/25 20:43:48 visual_prompt]: 	Test 100/123. loss: 12.312, 0.2399 s / batch. (data: 3.15e-05)max mem: 15.91075 GB 
[10/25 20:43:59 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2331, average loss: 11.1646
[10/25 20:43:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.21	
[10/25 20:43:59 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[10/25 20:45:05 visual_prompt]: 	Training 100/1106. train loss: 105.6768,	0.6240 s / batch. (data: 7.82e-04). ETA=14:56:11, max mem: 15.9 GB 
[10/25 20:46:08 visual_prompt]: 	Training 200/1106. train loss: 4.7865,	0.6222 s / batch. (data: 3.00e-04). ETA=14:52:32, max mem: 15.9 GB 
[10/25 20:47:11 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6336 s / batch. (data: 8.18e-04). ETA=15:07:51, max mem: 15.9 GB 
[10/25 20:48:15 visual_prompt]: 	Training 400/1106. train loss: 26.2799,	0.6198 s / batch. (data: 3.47e-04). ETA=14:47:03, max mem: 15.9 GB 
[10/25 20:49:18 visual_prompt]: 	Training 500/1106. train loss: 60.8382,	0.6342 s / batch. (data: 8.59e-04). ETA=15:06:31, max mem: 15.9 GB 
[10/25 20:50:21 visual_prompt]: 	Training 600/1106. train loss: 22.4782,	0.6449 s / batch. (data: 8.16e-04). ETA=15:20:45, max mem: 15.9 GB 
[10/25 20:51:24 visual_prompt]: 	Training 700/1106. train loss: 15.3164,	0.6479 s / batch. (data: 8.05e-04). ETA=15:24:00, max mem: 15.9 GB 
[10/25 20:52:28 visual_prompt]: 	Training 800/1106. train loss: 12.7255,	0.6308 s / batch. (data: 1.20e-02). ETA=14:58:31, max mem: 15.9 GB 
[10/25 20:53:31 visual_prompt]: 	Training 900/1106. train loss: 5.8215,	0.6724 s / batch. (data: 3.75e-02). ETA=15:56:41, max mem: 15.9 GB 
[10/25 20:54:34 visual_prompt]: 	Training 1000/1106. train loss: 42.9049,	0.6641 s / batch. (data: 7.88e-04). ETA=15:43:45, max mem: 15.9 GB 
[10/25 20:55:37 visual_prompt]: 	Training 1100/1106. train loss: 1.1224,	0.6188 s / batch. (data: 1.41e-04). ETA=14:38:26, max mem: 15.9 GB 
[10/25 20:55:41 visual_prompt]: Epoch 23 / 100: avg data time: 5.08e-03, avg batch time: 0.6344, average train loss: 46.0399
[10/25 20:56:31 visual_prompt]: 	Test 100/123. loss: 110.311, 0.2254 s / batch. (data: 5.27e-05)max mem: 15.91075 GB 
[10/25 20:56:41 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.2321, average loss: 99.5005
[10/25 20:56:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.24	
[10/25 20:56:41 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[10/25 20:57:47 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6177 s / batch. (data: 3.35e-04). ETA=14:35:44, max mem: 15.9 GB 
[10/25 20:58:50 visual_prompt]: 	Training 200/1106. train loss: 141.3227,	0.6261 s / batch. (data: 8.24e-04). ETA=14:46:31, max mem: 15.9 GB 
[10/25 20:59:52 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6318 s / batch. (data: 3.05e-04). ETA=14:53:39, max mem: 15.9 GB 
[10/25 21:00:55 visual_prompt]: 	Training 400/1106. train loss: 16.9724,	0.6329 s / batch. (data: 8.49e-04). ETA=14:54:05, max mem: 15.9 GB 
[10/25 21:01:59 visual_prompt]: 	Training 500/1106. train loss: 36.8894,	0.6308 s / batch. (data: 7.64e-04). ETA=14:50:07, max mem: 15.9 GB 
[10/25 21:03:02 visual_prompt]: 	Training 600/1106. train loss: 128.9840,	0.6272 s / batch. (data: 7.91e-04). ETA=14:44:00, max mem: 15.9 GB 
[10/25 21:04:05 visual_prompt]: 	Training 700/1106. train loss: 45.6055,	0.6484 s / batch. (data: 4.99e-03). ETA=15:12:41, max mem: 15.9 GB 
[10/25 21:05:08 visual_prompt]: 	Training 800/1106. train loss: 28.5968,	0.6186 s / batch. (data: 2.42e-04). ETA=14:29:49, max mem: 15.9 GB 
[10/25 21:06:11 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6071 s / batch. (data: 3.16e-04). ETA=14:12:32, max mem: 15.9 GB 
[10/25 21:07:14 visual_prompt]: 	Training 1000/1106. train loss: 8.6248,	0.6324 s / batch. (data: 7.67e-04). ETA=14:47:06, max mem: 15.9 GB 
[10/25 21:08:17 visual_prompt]: 	Training 1100/1106. train loss: 34.1224,	0.6171 s / batch. (data: 1.65e-04). ETA=14:24:32, max mem: 15.9 GB 
[10/25 21:08:21 visual_prompt]: Epoch 24 / 100: avg data time: 4.23e-03, avg batch time: 0.6324, average train loss: 48.1681
[10/25 21:09:12 visual_prompt]: 	Test 100/123. loss: 38.941, 0.2615 s / batch. (data: 3.27e-05)max mem: 15.91075 GB 
[10/25 21:09:22 visual_prompt]: Inference (val):avg data time: 1.30e-04, avg batch time: 0.2330, average loss: 43.5828
[10/25 21:09:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.29	
[10/25 21:09:22 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[10/25 21:10:27 visual_prompt]: 	Training 100/1106. train loss: 0.0053,	0.6226 s / batch. (data: 5.46e-03). ETA=14:31:08, max mem: 15.9 GB 
[10/25 21:11:30 visual_prompt]: 	Training 200/1106. train loss: 37.5040,	0.6328 s / batch. (data: 3.10e-04). ETA=14:44:21, max mem: 15.9 GB 
[10/25 21:12:33 visual_prompt]: 	Training 300/1106. train loss: 4.2686,	0.6205 s / batch. (data: 2.94e-04). ETA=14:26:08, max mem: 15.9 GB 
[10/25 21:13:36 visual_prompt]: 	Training 400/1106. train loss: 114.6911,	0.6281 s / batch. (data: 3.03e-04). ETA=14:35:46, max mem: 15.9 GB 
[10/25 21:14:39 visual_prompt]: 	Training 500/1106. train loss: 0.0001,	0.6325 s / batch. (data: 7.93e-04). ETA=14:40:49, max mem: 15.9 GB 
[10/25 21:15:42 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6335 s / batch. (data: 7.81e-04). ETA=14:41:06, max mem: 15.9 GB 
[10/25 21:16:45 visual_prompt]: 	Training 700/1106. train loss: 30.1134,	0.6338 s / batch. (data: 3.31e-04). ETA=14:40:33, max mem: 15.9 GB 
[10/25 21:17:49 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6301 s / batch. (data: 2.92e-04). ETA=14:34:22, max mem: 15.9 GB 
[10/25 21:19:01 visual_prompt]: 	Training 900/1106. train loss: 38.2535,	0.6180 s / batch. (data: 7.34e-04). ETA=14:16:32, max mem: 15.9 GB 
[10/25 21:20:04 visual_prompt]: 	Training 1000/1106. train loss: 9.2797,	0.6376 s / batch. (data: 3.19e-04). ETA=14:42:33, max mem: 15.9 GB 
[10/25 21:21:07 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6072 s / batch. (data: 1.70e-04). ETA=13:59:26, max mem: 15.9 GB 
[10/25 21:21:11 visual_prompt]: Epoch 25 / 100: avg data time: 1.30e-02, avg batch time: 0.6412, average train loss: 45.5754
[10/25 21:22:04 visual_prompt]: 	Test 100/123. loss: 87.259, 0.2250 s / batch. (data: 3.62e-05)max mem: 15.91075 GB 
[10/25 21:22:15 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.2320, average loss: 78.9454
[10/25 21:22:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.41	
[10/25 21:22:15 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[10/25 21:23:21 visual_prompt]: 	Training 100/1106. train loss: 30.8878,	0.6400 s / batch. (data: 2.95e-04). ETA=14:43:42, max mem: 15.9 GB 
[10/25 21:24:24 visual_prompt]: 	Training 200/1106. train loss: 2.8292,	0.6193 s / batch. (data: 3.15e-04). ETA=14:14:03, max mem: 15.9 GB 
[10/25 21:25:28 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6299 s / batch. (data: 3.25e-04). ETA=14:27:38, max mem: 15.9 GB 
[10/25 21:26:31 visual_prompt]: 	Training 400/1106. train loss: 3.0345,	0.6464 s / batch. (data: 7.89e-04). ETA=14:49:21, max mem: 15.9 GB 
[10/25 21:27:35 visual_prompt]: 	Training 500/1106. train loss: 11.7452,	0.6351 s / batch. (data: 8.43e-04). ETA=14:32:45, max mem: 15.9 GB 
[10/25 21:28:38 visual_prompt]: 	Training 600/1106. train loss: 25.3872,	0.6640 s / batch. (data: 2.06e-02). ETA=15:11:20, max mem: 15.9 GB 
[10/25 21:29:41 visual_prompt]: 	Training 700/1106. train loss: 40.5201,	0.6186 s / batch. (data: 2.89e-04). ETA=14:08:01, max mem: 15.9 GB 
[10/25 21:30:44 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6084 s / batch. (data: 2.13e-04). ETA=13:52:56, max mem: 15.9 GB 
[10/25 21:31:47 visual_prompt]: 	Training 900/1106. train loss: 29.6630,	0.6296 s / batch. (data: 3.10e-04). ETA=14:20:59, max mem: 15.9 GB 
[10/25 21:32:50 visual_prompt]: 	Training 1000/1106. train loss: 10.7137,	0.6192 s / batch. (data: 3.06e-04). ETA=14:05:40, max mem: 15.9 GB 
[10/25 21:33:54 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6146 s / batch. (data: 1.51e-04). ETA=13:58:25, max mem: 15.9 GB 
[10/25 21:33:58 visual_prompt]: Epoch 26 / 100: avg data time: 5.40e-03, avg batch time: 0.6347, average train loss: 42.4385
[10/25 21:34:50 visual_prompt]: 	Test 100/123. loss: 55.300, 0.2286 s / batch. (data: 5.77e-05)max mem: 15.91075 GB 
[10/25 21:35:01 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.2313, average loss: 49.8682
[10/25 21:35:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/25 21:35:01 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[10/25 21:36:07 visual_prompt]: 	Training 100/1106. train loss: 198.1734,	0.6325 s / batch. (data: 1.19e-03). ETA=14:21:44, max mem: 15.9 GB 
[10/25 21:37:10 visual_prompt]: 	Training 200/1106. train loss: 15.0009,	0.6407 s / batch. (data: 6.01e-03). ETA=14:31:46, max mem: 15.9 GB 
[10/25 21:38:13 visual_prompt]: 	Training 300/1106. train loss: 1.4219,	0.6192 s / batch. (data: 3.01e-04). ETA=14:01:31, max mem: 15.9 GB 
[10/25 21:39:17 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6192 s / batch. (data: 2.88e-04). ETA=14:00:34, max mem: 15.9 GB 
[10/25 21:40:20 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6257 s / batch. (data: 8.23e-04). ETA=14:08:13, max mem: 15.9 GB 
[10/25 21:41:23 visual_prompt]: 	Training 600/1106. train loss: 279.5085,	0.6387 s / batch. (data: 7.93e-04). ETA=14:24:50, max mem: 15.9 GB 
[10/25 21:42:27 visual_prompt]: 	Training 700/1106. train loss: 22.3024,	0.6381 s / batch. (data: 8.16e-04). ETA=14:22:57, max mem: 15.9 GB 
[10/25 21:43:30 visual_prompt]: 	Training 800/1106. train loss: 47.6794,	0.6138 s / batch. (data: 8.29e-04). ETA=13:49:06, max mem: 15.9 GB 
[10/25 21:44:33 visual_prompt]: 	Training 900/1106. train loss: 14.0037,	0.6180 s / batch. (data: 2.94e-04). ETA=13:53:44, max mem: 15.9 GB 
[10/25 21:45:36 visual_prompt]: 	Training 1000/1106. train loss: 79.8213,	0.6494 s / batch. (data: 1.10e-02). ETA=14:34:59, max mem: 15.9 GB 
[10/25 21:46:39 visual_prompt]: 	Training 1100/1106. train loss: 0.0837,	0.6188 s / batch. (data: 1.95e-04). ETA=13:52:47, max mem: 15.9 GB 
[10/25 21:46:43 visual_prompt]: Epoch 27 / 100: avg data time: 5.51e-03, avg batch time: 0.6348, average train loss: 47.1301
[10/25 21:47:35 visual_prompt]: 	Test 100/123. loss: 78.510, 0.2418 s / batch. (data: 4.32e-05)max mem: 15.91075 GB 
[10/25 21:47:46 visual_prompt]: Inference (val):avg data time: 9.00e-04, avg batch time: 0.2329, average loss: 86.3927
[10/25 21:47:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.92	
[10/25 21:47:46 visual_prompt]: Stopping early.
[10/25 21:47:46 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 21:47:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 21:47:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 21:47:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 21:47:46 visual_prompt]: Training with config:
[10/25 21:47:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr10.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 21:47:46 visual_prompt]: Loading training data...
[10/25 21:47:46 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 21:47:46 visual_prompt]: Loading validation data...
[10/25 21:47:46 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 21:47:46 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/25 21:47:49 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/25 21:47:49 visual_prompt]: tuned percent:0.522
[10/25 21:47:50 visual_prompt]: Device used for model: 0
[10/25 21:47:50 visual_prompt]: Setting up Evaluator...
[10/25 21:47:50 visual_prompt]: Setting up Trainer...
[10/25 21:47:50 visual_prompt]: 	Setting up the optimizer...
[10/25 21:47:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 21:48:56 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6177 s / batch. (data: 7.62e-04). ETA=18:57:41, max mem: 15.9 GB 
[10/25 21:49:59 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6258 s / batch. (data: 5.42e-03). ETA=19:11:33, max mem: 15.9 GB 
[10/25 21:51:03 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6180 s / batch. (data: 3.18e-04). ETA=18:56:01, max mem: 15.9 GB 
[10/25 21:52:06 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6202 s / batch. (data: 3.46e-04). ETA=18:59:11, max mem: 15.9 GB 
[10/25 21:53:10 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6474 s / batch. (data: 5.48e-03). ETA=19:47:59, max mem: 15.9 GB 
[10/25 21:54:14 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6189 s / batch. (data: 2.96e-04). ETA=18:54:43, max mem: 15.9 GB 
[10/25 21:55:17 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6191 s / batch. (data: 7.99e-04). ETA=18:54:00, max mem: 15.9 GB 
[10/25 21:56:21 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6353 s / batch. (data: 3.07e-04). ETA=19:22:32, max mem: 15.9 GB 
[10/25 21:57:25 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6237 s / batch. (data: 2.97e-04). ETA=19:00:15, max mem: 15.9 GB 
[10/25 21:58:28 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6310 s / batch. (data: 3.22e-04). ETA=19:12:38, max mem: 15.9 GB 
[10/25 21:59:32 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6183 s / batch. (data: 2.29e-04). ETA=18:48:21, max mem: 15.9 GB 
[10/25 21:59:36 visual_prompt]: Epoch 1 / 100: avg data time: 5.92e-03, avg batch time: 0.6383, average train loss: 1.4028
[10/25 22:00:27 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2439 s / batch. (data: 2.57e-05)max mem: 15.91637 GB 
[10/25 22:00:39 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2334, average loss: 1.3505
[10/25 22:00:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/25 22:00:39 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/25 22:01:44 visual_prompt]: 	Training 100/1106. train loss: 23.4051,	0.6324 s / batch. (data: 5.45e-03). ETA=19:13:05, max mem: 15.9 GB 
[10/25 22:02:47 visual_prompt]: 	Training 200/1106. train loss: 10.2662,	0.6191 s / batch. (data: 3.02e-04). ETA=18:47:43, max mem: 15.9 GB 
[10/25 22:03:51 visual_prompt]: 	Training 300/1106. train loss: 0.0126,	0.6240 s / batch. (data: 3.30e-04). ETA=18:55:39, max mem: 15.9 GB 
[10/25 22:04:54 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6180 s / batch. (data: 2.89e-04). ETA=18:43:38, max mem: 15.9 GB 
[10/25 22:05:58 visual_prompt]: 	Training 500/1106. train loss: 11.3309,	0.6352 s / batch. (data: 8.02e-03). ETA=19:13:52, max mem: 15.9 GB 
[10/25 22:07:01 visual_prompt]: 	Training 600/1106. train loss: 4.1070,	0.6269 s / batch. (data: 3.52e-04). ETA=18:57:47, max mem: 15.9 GB 
[10/25 22:08:05 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6185 s / batch. (data: 3.21e-04). ETA=18:41:24, max mem: 15.9 GB 
[10/25 22:09:08 visual_prompt]: 	Training 800/1106. train loss: 3.1605,	0.6478 s / batch. (data: 1.57e-02). ETA=19:33:28, max mem: 15.9 GB 
[10/25 22:10:11 visual_prompt]: 	Training 900/1106. train loss: 4.6072,	0.6424 s / batch. (data: 8.05e-04). ETA=19:22:37, max mem: 15.9 GB 
[10/25 22:11:15 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6185 s / batch. (data: 3.13e-04). ETA=18:38:25, max mem: 15.9 GB 
[10/25 22:12:18 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6168 s / batch. (data: 1.64e-04). ETA=18:34:15, max mem: 15.9 GB 
[10/25 22:12:22 visual_prompt]: Epoch 2 / 100: avg data time: 5.16e-03, avg batch time: 0.6359, average train loss: 6.6899
[10/25 22:13:14 visual_prompt]: 	Test 100/123. loss: 12.414, 0.2277 s / batch. (data: 3.19e-05)max mem: 15.91637 GB 
[10/25 22:13:25 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.2329, average loss: 11.1894
[10/25 22:13:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.37	
[10/25 22:13:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/25 22:14:32 visual_prompt]: 	Training 100/1106. train loss: 0.0001,	0.6325 s / batch. (data: 7.77e-04). ETA=19:01:27, max mem: 15.9 GB 
[10/25 22:15:35 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6339 s / batch. (data: 3.29e-04). ETA=19:03:01, max mem: 15.9 GB 
[10/25 22:16:39 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6596 s / batch. (data: 3.80e-02). ETA=19:48:11, max mem: 15.9 GB 
[10/25 22:17:42 visual_prompt]: 	Training 400/1106. train loss: 0.4691,	0.6206 s / batch. (data: 4.77e-04). ETA=18:36:56, max mem: 15.9 GB 
[10/25 22:18:46 visual_prompt]: 	Training 500/1106. train loss: 11.6193,	0.6322 s / batch. (data: 1.30e-02). ETA=18:56:42, max mem: 15.9 GB 
[10/25 22:19:49 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6505 s / batch. (data: 3.26e-02). ETA=19:28:34, max mem: 15.9 GB 
[10/25 22:20:53 visual_prompt]: 	Training 700/1106. train loss: 0.3884,	0.6342 s / batch. (data: 8.60e-04). ETA=18:58:14, max mem: 15.9 GB 
[10/25 22:21:56 visual_prompt]: 	Training 800/1106. train loss: 0.0009,	0.6405 s / batch. (data: 1.25e-02). ETA=19:08:31, max mem: 15.9 GB 
[10/25 22:23:00 visual_prompt]: 	Training 900/1106. train loss: 9.5186,	0.6489 s / batch. (data: 8.72e-04). ETA=19:22:32, max mem: 15.9 GB 
[10/25 22:24:03 visual_prompt]: 	Training 1000/1106. train loss: 8.5267,	0.6189 s / batch. (data: 3.21e-04). ETA=18:27:37, max mem: 15.9 GB 
[10/25 22:25:07 visual_prompt]: 	Training 1100/1106. train loss: 4.7125,	0.6193 s / batch. (data: 1.75e-04). ETA=18:27:20, max mem: 15.9 GB 
[10/25 22:25:11 visual_prompt]: Epoch 3 / 100: avg data time: 6.12e-03, avg batch time: 0.6377, average train loss: 7.9460
[10/25 22:26:03 visual_prompt]: 	Test 100/123. loss: 5.927, 0.2247 s / batch. (data: 4.32e-05)max mem: 15.91637 GB 
[10/25 22:26:14 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.2327, average loss: 6.5348
[10/25 22:26:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.76	
[10/25 22:26:14 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/25 22:27:19 visual_prompt]: 	Training 100/1106. train loss: 0.0252,	0.6393 s / batch. (data: 3.19e-04). ETA=19:02:02, max mem: 15.9 GB 
[10/25 22:28:23 visual_prompt]: 	Training 200/1106. train loss: 55.2922,	0.6185 s / batch. (data: 3.12e-04). ETA=18:23:52, max mem: 15.9 GB 
[10/25 22:29:26 visual_prompt]: 	Training 300/1106. train loss: 2.0916,	0.6415 s / batch. (data: 1.10e-02). ETA=19:03:45, max mem: 15.9 GB 
[10/25 22:30:30 visual_prompt]: 	Training 400/1106. train loss: 2.6251,	0.6213 s / batch. (data: 3.03e-04). ETA=18:26:42, max mem: 15.9 GB 
[10/25 22:31:33 visual_prompt]: 	Training 500/1106. train loss: 18.6333,	0.6432 s / batch. (data: 2.50e-02). ETA=19:04:43, max mem: 15.9 GB 
[10/25 22:32:36 visual_prompt]: 	Training 600/1106. train loss: 8.7875,	0.6198 s / batch. (data: 3.21e-04). ETA=18:22:06, max mem: 15.9 GB 
[10/25 22:33:40 visual_prompt]: 	Training 700/1106. train loss: 38.2799,	0.6514 s / batch. (data: 8.71e-04). ETA=19:17:10, max mem: 15.9 GB 
[10/25 22:34:43 visual_prompt]: 	Training 800/1106. train loss: 0.0151,	0.6454 s / batch. (data: 7.95e-04). ETA=19:05:24, max mem: 15.9 GB 
[10/25 22:35:46 visual_prompt]: 	Training 900/1106. train loss: 30.3358,	0.6299 s / batch. (data: 2.89e-04). ETA=18:36:45, max mem: 15.9 GB 
[10/25 22:36:50 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6447 s / batch. (data: 3.57e-04). ETA=19:01:59, max mem: 15.9 GB 
[10/25 22:37:53 visual_prompt]: 	Training 1100/1106. train loss: 16.4115,	0.6184 s / batch. (data: 1.66e-04). ETA=18:14:19, max mem: 15.9 GB 
[10/25 22:37:57 visual_prompt]: Epoch 4 / 100: avg data time: 5.25e-03, avg batch time: 0.6360, average train loss: 12.8852
[10/25 22:38:49 visual_prompt]: 	Test 100/123. loss: 7.301, 0.2364 s / batch. (data: 3.34e-05)max mem: 15.91637 GB 
[10/25 22:39:00 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.2327, average loss: 6.5741
[10/25 22:39:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.95	
[10/25 22:39:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/25 22:40:06 visual_prompt]: 	Training 100/1106. train loss: 17.7233,	0.6397 s / batch. (data: 8.25e-04). ETA=18:50:54, max mem: 15.9 GB 
[10/25 22:41:10 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6290 s / batch. (data: 3.28e-04). ETA=18:31:02, max mem: 15.9 GB 
[10/25 22:42:13 visual_prompt]: 	Training 300/1106. train loss: 31.3137,	0.6173 s / batch. (data: 3.32e-04). ETA=18:09:14, max mem: 15.9 GB 
[10/25 22:43:19 visual_prompt]: 	Training 400/1106. train loss: 13.1205,	0.6475 s / batch. (data: 5.93e-03). ETA=19:01:27, max mem: 15.9 GB 
[10/25 22:44:23 visual_prompt]: 	Training 500/1106. train loss: 0.6576,	0.6452 s / batch. (data: 8.16e-04). ETA=18:56:17, max mem: 15.9 GB 
[10/25 22:45:26 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6294 s / batch. (data: 8.32e-04). ETA=18:27:25, max mem: 15.9 GB 
[10/25 22:46:33 visual_prompt]: 	Training 700/1106. train loss: 16.0873,	0.6330 s / batch. (data: 2.57e-04). ETA=18:32:42, max mem: 15.9 GB 
[10/25 22:47:40 visual_prompt]: 	Training 800/1106. train loss: 6.3810,	0.6245 s / batch. (data: 3.16e-04). ETA=18:16:46, max mem: 15.9 GB 
[10/25 22:48:44 visual_prompt]: 	Training 900/1106. train loss: 17.9051,	0.6399 s / batch. (data: 8.20e-04). ETA=18:42:49, max mem: 15.9 GB 
[10/25 22:49:47 visual_prompt]: 	Training 1000/1106. train loss: 23.5516,	0.6240 s / batch. (data: 7.66e-04). ETA=18:13:48, max mem: 15.9 GB 
[10/25 22:50:50 visual_prompt]: 	Training 1100/1106. train loss: 23.4452,	0.6182 s / batch. (data: 1.74e-04). ETA=18:02:41, max mem: 15.9 GB 
[10/25 22:50:54 visual_prompt]: Epoch 5 / 100: avg data time: 1.51e-02, avg batch time: 0.6449, average train loss: 20.8661
[10/25 22:51:44 visual_prompt]: 	Test 100/123. loss: 32.045, 0.2357 s / batch. (data: 3.10e-05)max mem: 15.91637 GB 
[10/25 22:51:55 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2316, average loss: 28.7970
[10/25 22:51:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.38	
[10/25 22:51:55 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/25 22:53:00 visual_prompt]: 	Training 100/1106. train loss: 48.4565,	0.6342 s / batch. (data: 8.47e-04). ETA=18:29:28, max mem: 15.9 GB 
[10/25 22:54:03 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6176 s / batch. (data: 3.30e-04). ETA=17:59:23, max mem: 15.9 GB 
[10/25 22:55:06 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6387 s / batch. (data: 7.98e-04). ETA=18:35:16, max mem: 15.9 GB 
[10/25 22:56:10 visual_prompt]: 	Training 400/1106. train loss: 27.7933,	0.6197 s / batch. (data: 2.70e-04). ETA=18:01:02, max mem: 15.9 GB 
[10/25 22:57:13 visual_prompt]: 	Training 500/1106. train loss: 9.5212,	0.6315 s / batch. (data: 3.24e-04). ETA=18:20:38, max mem: 15.9 GB 
[10/25 22:58:16 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6491 s / batch. (data: 8.15e-04). ETA=18:50:09, max mem: 15.9 GB 
[10/25 22:59:19 visual_prompt]: 	Training 700/1106. train loss: 77.8168,	0.6541 s / batch. (data: 1.61e-02). ETA=18:57:51, max mem: 15.9 GB 
[10/25 23:00:22 visual_prompt]: 	Training 800/1106. train loss: 67.5725,	0.6249 s / batch. (data: 3.16e-04). ETA=18:05:59, max mem: 15.9 GB 
[10/25 23:01:26 visual_prompt]: 	Training 900/1106. train loss: 98.2886,	0.6473 s / batch. (data: 1.60e-02). ETA=18:43:53, max mem: 15.9 GB 
[10/25 23:02:29 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6369 s / batch. (data: 8.18e-04). ETA=18:24:39, max mem: 15.9 GB 
[10/25 23:03:32 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6067 s / batch. (data: 1.39e-04). ETA=17:31:19, max mem: 15.9 GB 
[10/25 23:03:36 visual_prompt]: Epoch 6 / 100: avg data time: 4.16e-03, avg batch time: 0.6339, average train loss: 23.8353
[10/25 23:04:26 visual_prompt]: 	Test 100/123. loss: 5.175, 0.2258 s / batch. (data: 2.86e-05)max mem: 15.91637 GB 
[10/25 23:04:36 visual_prompt]: Inference (val):avg data time: 1.13e-04, avg batch time: 0.2325, average loss: 4.6970
[10/25 23:04:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.86	
[10/25 23:04:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/25 23:05:42 visual_prompt]: 	Training 100/1106. train loss: 20.0877,	0.6170 s / batch. (data: 3.39e-04). ETA=17:48:01, max mem: 15.9 GB 
[10/25 23:06:45 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6169 s / batch. (data: 2.75e-04). ETA=17:46:50, max mem: 15.9 GB 
[10/25 23:07:48 visual_prompt]: 	Training 300/1106. train loss: 18.2126,	0.6360 s / batch. (data: 3.39e-04). ETA=18:18:50, max mem: 15.9 GB 
[10/25 23:08:51 visual_prompt]: 	Training 400/1106. train loss: 40.3543,	0.6156 s / batch. (data: 3.53e-04). ETA=17:42:35, max mem: 15.9 GB 
[10/25 23:09:54 visual_prompt]: 	Training 500/1106. train loss: 62.3014,	0.6401 s / batch. (data: 3.14e-04). ETA=18:23:45, max mem: 15.9 GB 
[10/25 23:10:58 visual_prompt]: 	Training 600/1106. train loss: 92.1608,	0.6211 s / batch. (data: 3.36e-04). ETA=17:49:57, max mem: 15.9 GB 
[10/25 23:12:01 visual_prompt]: 	Training 700/1106. train loss: 77.1923,	0.6406 s / batch. (data: 9.00e-04). ETA=18:22:31, max mem: 15.9 GB 
[10/25 23:13:04 visual_prompt]: 	Training 800/1106. train loss: 32.9716,	0.6183 s / batch. (data: 3.82e-04). ETA=17:43:03, max mem: 15.9 GB 
[10/25 23:14:07 visual_prompt]: 	Training 900/1106. train loss: 14.3400,	0.6334 s / batch. (data: 2.62e-04). ETA=18:07:58, max mem: 15.9 GB 
[10/25 23:15:10 visual_prompt]: 	Training 1000/1106. train loss: 12.0723,	0.6174 s / batch. (data: 2.65e-04). ETA=17:39:30, max mem: 15.9 GB 
[10/25 23:16:13 visual_prompt]: 	Training 1100/1106. train loss: 16.7959,	0.6177 s / batch. (data: 1.50e-04). ETA=17:38:56, max mem: 15.9 GB 
[10/25 23:16:17 visual_prompt]: Epoch 7 / 100: avg data time: 4.16e-03, avg batch time: 0.6329, average train loss: 30.3053
[10/25 23:17:07 visual_prompt]: 	Test 100/123. loss: 0.747, 0.2309 s / batch. (data: 3.00e-05)max mem: 15.91637 GB 
[10/25 23:17:17 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2322, average loss: 0.8892
[10/25 23:17:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.69	
[10/25 23:17:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/25 23:18:22 visual_prompt]: 	Training 100/1106. train loss: 55.1054,	0.6227 s / batch. (data: 2.93e-04). ETA=17:46:22, max mem: 15.9 GB 
[10/25 23:19:25 visual_prompt]: 	Training 200/1106. train loss: 24.4395,	0.6214 s / batch. (data: 3.09e-04). ETA=17:43:14, max mem: 15.9 GB 
[10/25 23:20:28 visual_prompt]: 	Training 300/1106. train loss: 51.9736,	0.6474 s / batch. (data: 8.34e-04). ETA=18:26:37, max mem: 15.9 GB 
[10/25 23:21:32 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6081 s / batch. (data: 2.39e-04). ETA=17:18:21, max mem: 15.9 GB 
[10/25 23:22:35 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6341 s / batch. (data: 7.98e-04). ETA=18:01:45, max mem: 15.9 GB 
[10/25 23:23:38 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6440 s / batch. (data: 7.44e-04). ETA=18:17:35, max mem: 15.9 GB 
[10/25 23:24:41 visual_prompt]: 	Training 700/1106. train loss: 48.4959,	0.6125 s / batch. (data: 3.42e-04). ETA=17:22:55, max mem: 15.9 GB 
[10/25 23:25:44 visual_prompt]: 	Training 800/1106. train loss: 6.7818,	0.6560 s / batch. (data: 8.26e-04). ETA=18:35:46, max mem: 15.9 GB 
[10/25 23:26:47 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6075 s / batch. (data: 4.49e-04). ETA=17:12:19, max mem: 15.9 GB 
[10/25 23:27:50 visual_prompt]: 	Training 1000/1106. train loss: 269.4610,	0.6319 s / batch. (data: 1.22e-03). ETA=17:52:42, max mem: 15.9 GB 
[10/25 23:28:53 visual_prompt]: 	Training 1100/1106. train loss: 39.7767,	0.6188 s / batch. (data: 1.68e-04). ETA=17:29:31, max mem: 15.9 GB 
[10/25 23:28:57 visual_prompt]: Epoch 8 / 100: avg data time: 4.51e-03, avg batch time: 0.6327, average train loss: 34.4624
[10/25 23:29:47 visual_prompt]: 	Test 100/123. loss: 0.842, 0.2346 s / batch. (data: 2.74e-05)max mem: 15.91637 GB 
[10/25 23:29:57 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.2324, average loss: 0.7895
[10/25 23:29:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.89	
[10/25 23:29:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/25 23:31:02 visual_prompt]: 	Training 100/1106. train loss: 282.3400,	0.6170 s / batch. (data: 3.27e-04). ETA=17:25:22, max mem: 15.9 GB 
[10/25 23:32:05 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6158 s / batch. (data: 2.72e-04). ETA=17:22:17, max mem: 15.9 GB 
[10/25 23:33:08 visual_prompt]: 	Training 300/1106. train loss: 42.3178,	0.6156 s / batch. (data: 3.40e-04). ETA=17:20:56, max mem: 15.9 GB 
[10/25 23:34:12 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6390 s / batch. (data: 2.21e-02). ETA=17:59:28, max mem: 15.9 GB 
[10/25 23:35:15 visual_prompt]: 	Training 500/1106. train loss: 33.8147,	0.6679 s / batch. (data: 8.37e-04). ETA=18:47:10, max mem: 15.9 GB 
[10/25 23:36:18 visual_prompt]: 	Training 600/1106. train loss: 26.9132,	0.6294 s / batch. (data: 7.81e-04). ETA=17:41:01, max mem: 15.9 GB 
[10/25 23:37:21 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6194 s / batch. (data: 3.24e-04). ETA=17:23:07, max mem: 15.9 GB 
[10/25 23:38:24 visual_prompt]: 	Training 800/1106. train loss: 31.3857,	0.6416 s / batch. (data: 3.78e-04). ETA=17:59:32, max mem: 15.9 GB 
[10/25 23:39:28 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6400 s / batch. (data: 3.23e-04). ETA=17:55:46, max mem: 15.9 GB 
[10/25 23:40:31 visual_prompt]: 	Training 1000/1106. train loss: 2.7270,	0.6224 s / batch. (data: 3.18e-04). ETA=17:25:12, max mem: 15.9 GB 
[10/25 23:41:34 visual_prompt]: 	Training 1100/1106. train loss: 43.5190,	0.6141 s / batch. (data: 1.50e-04). ETA=17:10:09, max mem: 15.9 GB 
[10/25 23:41:37 visual_prompt]: Epoch 9 / 100: avg data time: 4.55e-03, avg batch time: 0.6330, average train loss: 34.2270
[10/25 23:42:27 visual_prompt]: 	Test 100/123. loss: 55.662, 0.2358 s / batch. (data: 2.88e-05)max mem: 15.91637 GB 
[10/25 23:42:38 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2325, average loss: 61.0178
[10/25 23:42:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.92	
[10/25 23:42:38 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/25 23:43:44 visual_prompt]: 	Training 100/1106. train loss: 62.9277,	0.6284 s / batch. (data: 1.50e-02). ETA=17:33:05, max mem: 15.9 GB 
[10/25 23:44:46 visual_prompt]: 	Training 200/1106. train loss: 261.8511,	0.6385 s / batch. (data: 5.97e-03). ETA=17:48:53, max mem: 15.9 GB 
[10/25 23:45:49 visual_prompt]: 	Training 300/1106. train loss: 22.9044,	0.6182 s / batch. (data: 2.68e-04). ETA=17:13:51, max mem: 15.9 GB 
[10/25 23:46:53 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6250 s / batch. (data: 1.21e-03). ETA=17:24:10, max mem: 15.9 GB 
[10/25 23:47:56 visual_prompt]: 	Training 500/1106. train loss: 13.3923,	0.6320 s / batch. (data: 2.99e-04). ETA=17:34:55, max mem: 15.9 GB 
[10/25 23:48:59 visual_prompt]: 	Training 600/1106. train loss: 207.5486,	0.6393 s / batch. (data: 3.38e-04). ETA=17:46:01, max mem: 15.9 GB 
[10/25 23:50:02 visual_prompt]: 	Training 700/1106. train loss: 69.2942,	0.6334 s / batch. (data: 3.36e-04). ETA=17:35:08, max mem: 15.9 GB 
[10/25 23:51:06 visual_prompt]: 	Training 800/1106. train loss: 11.8580,	0.6326 s / batch. (data: 8.19e-04). ETA=17:32:46, max mem: 15.9 GB 
[10/25 23:52:09 visual_prompt]: 	Training 900/1106. train loss: 47.4646,	0.6213 s / batch. (data: 3.22e-04). ETA=17:12:51, max mem: 15.9 GB 
[10/25 23:53:12 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6421 s / batch. (data: 8.37e-04). ETA=17:46:24, max mem: 15.9 GB 
[10/25 23:54:15 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6188 s / batch. (data: 1.67e-04). ETA=17:06:35, max mem: 15.9 GB 
[10/25 23:54:19 visual_prompt]: Epoch 10 / 100: avg data time: 4.15e-03, avg batch time: 0.6335, average train loss: 41.1596
[10/25 23:55:09 visual_prompt]: 	Test 100/123. loss: 1.375, 0.2357 s / batch. (data: 3.43e-05)max mem: 15.91637 GB 
[10/25 23:55:19 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2326, average loss: 1.4694
[10/25 23:55:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.33	
[10/25 23:55:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/25 23:56:25 visual_prompt]: 	Training 100/1106. train loss: 225.8642,	0.6600 s / batch. (data: 8.24e-04). ETA=18:13:48, max mem: 15.9 GB 
[10/25 23:57:28 visual_prompt]: 	Training 200/1106. train loss: 12.5687,	0.6372 s / batch. (data: 6.04e-03). ETA=17:35:03, max mem: 15.9 GB 
[10/25 23:58:31 visual_prompt]: 	Training 300/1106. train loss: 92.7959,	0.6141 s / batch. (data: 3.68e-04). ETA=16:55:39, max mem: 15.9 GB 
[10/25 23:59:34 visual_prompt]: 	Training 400/1106. train loss: 56.8049,	0.6334 s / batch. (data: 7.89e-04). ETA=17:26:32, max mem: 15.9 GB 
[10/26 00:00:37 visual_prompt]: 	Training 500/1106. train loss: 48.4839,	0.6391 s / batch. (data: 7.93e-04). ETA=17:34:53, max mem: 15.9 GB 
[10/26 00:01:40 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6150 s / batch. (data: 7.71e-04). ETA=16:54:07, max mem: 15.9 GB 
[10/26 00:02:43 visual_prompt]: 	Training 700/1106. train loss: 142.5671,	0.6367 s / batch. (data: 7.91e-04). ETA=17:28:46, max mem: 15.9 GB 
[10/26 00:03:46 visual_prompt]: 	Training 800/1106. train loss: 97.5416,	0.6316 s / batch. (data: 1.20e-02). ETA=17:19:19, max mem: 15.9 GB 
[10/26 00:04:49 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6360 s / batch. (data: 8.66e-04). ETA=17:25:31, max mem: 15.9 GB 
[10/26 00:05:52 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6328 s / batch. (data: 9.78e-04). ETA=17:19:13, max mem: 15.9 GB 
[10/26 00:06:55 visual_prompt]: 	Training 1100/1106. train loss: 2.1073,	0.6177 s / batch. (data: 1.47e-04). ETA=16:53:26, max mem: 15.9 GB 
[10/26 00:06:59 visual_prompt]: Epoch 11 / 100: avg data time: 4.40e-03, avg batch time: 0.6322, average train loss: 38.1451
[10/26 00:07:49 visual_prompt]: 	Test 100/123. loss: 17.983, 0.2251 s / batch. (data: 2.69e-05)max mem: 15.91637 GB 
[10/26 00:07:59 visual_prompt]: Inference (val):avg data time: 2.05e-04, avg batch time: 0.2328, average loss: 16.1093
[10/26 00:07:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.12	
[10/26 00:07:59 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/26 00:09:05 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6132 s / batch. (data: 3.17e-04). ETA=16:44:56, max mem: 15.9 GB 
[10/26 00:10:08 visual_prompt]: 	Training 200/1106. train loss: 6.7429,	0.6176 s / batch. (data: 3.42e-04). ETA=16:51:04, max mem: 15.9 GB 
[10/26 00:11:11 visual_prompt]: 	Training 300/1106. train loss: 121.7036,	0.6190 s / batch. (data: 3.36e-04). ETA=16:52:28, max mem: 15.9 GB 
[10/26 00:12:14 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6340 s / batch. (data: 8.27e-04). ETA=17:15:54, max mem: 15.9 GB 
[10/26 00:13:17 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6297 s / batch. (data: 1.37e-02). ETA=17:07:48, max mem: 15.9 GB 
[10/26 00:14:20 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6390 s / batch. (data: 7.61e-04). ETA=17:21:51, max mem: 15.9 GB 
[10/26 00:15:23 visual_prompt]: 	Training 700/1106. train loss: 39.0259,	0.6324 s / batch. (data: 3.64e-04). ETA=17:10:08, max mem: 15.9 GB 
[10/26 00:16:27 visual_prompt]: 	Training 800/1106. train loss: 59.7423,	0.6382 s / batch. (data: 7.58e-04). ETA=17:18:27, max mem: 15.9 GB 
[10/26 00:17:30 visual_prompt]: 	Training 900/1106. train loss: 129.4753,	0.6368 s / batch. (data: 7.54e-04). ETA=17:15:07, max mem: 15.9 GB 
[10/26 00:18:32 visual_prompt]: 	Training 1000/1106. train loss: 27.3597,	0.6316 s / batch. (data: 7.78e-04). ETA=17:05:35, max mem: 15.9 GB 
[10/26 00:19:35 visual_prompt]: 	Training 1100/1106. train loss: 117.3780,	0.6137 s / batch. (data: 1.66e-04). ETA=16:35:36, max mem: 15.9 GB 
[10/26 00:19:39 visual_prompt]: Epoch 12 / 100: avg data time: 5.14e-03, avg batch time: 0.6326, average train loss: 58.5446
[10/26 00:20:29 visual_prompt]: 	Test 100/123. loss: 51.875, 0.2305 s / batch. (data: 2.79e-05)max mem: 15.91637 GB 
[10/26 00:20:39 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2325, average loss: 46.7708
[10/26 00:20:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.38	
[10/26 00:20:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/26 00:21:45 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6322 s / batch. (data: 8.00e-04). ETA=17:04:30, max mem: 15.9 GB 
[10/26 00:22:48 visual_prompt]: 	Training 200/1106. train loss: 97.3913,	0.6131 s / batch. (data: 3.35e-04). ETA=16:32:32, max mem: 15.9 GB 
[10/26 00:23:51 visual_prompt]: 	Training 300/1106. train loss: 0.0002,	0.6312 s / batch. (data: 3.22e-04). ETA=17:00:45, max mem: 15.9 GB 
[10/26 00:24:54 visual_prompt]: 	Training 400/1106. train loss: 0.0006,	0.6305 s / batch. (data: 8.29e-04). ETA=16:58:34, max mem: 15.9 GB 
[10/26 00:25:57 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6183 s / batch. (data: 3.10e-04). ETA=16:37:49, max mem: 15.9 GB 
[10/26 00:27:00 visual_prompt]: 	Training 600/1106. train loss: 22.8785,	0.6182 s / batch. (data: 3.10e-04). ETA=16:36:36, max mem: 15.9 GB 
[10/26 00:28:03 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6266 s / batch. (data: 3.19e-04). ETA=16:49:11, max mem: 15.9 GB 
[10/26 00:29:06 visual_prompt]: 	Training 800/1106. train loss: 13.7373,	0.6351 s / batch. (data: 8.34e-04). ETA=17:01:47, max mem: 15.9 GB 
[10/26 00:30:09 visual_prompt]: 	Training 900/1106. train loss: 45.8830,	0.6188 s / batch. (data: 3.11e-04). ETA=16:34:30, max mem: 15.9 GB 
[10/26 00:31:12 visual_prompt]: 	Training 1000/1106. train loss: 29.0761,	0.6179 s / batch. (data: 3.33e-04). ETA=16:32:03, max mem: 15.9 GB 
[10/26 00:32:15 visual_prompt]: 	Training 1100/1106. train loss: 160.4080,	0.6131 s / batch. (data: 2.51e-04). ETA=16:23:12, max mem: 15.9 GB 
[10/26 00:32:19 visual_prompt]: Epoch 13 / 100: avg data time: 4.39e-03, avg batch time: 0.6324, average train loss: 44.0732
[10/26 00:33:09 visual_prompt]: 	Test 100/123. loss: 33.977, 0.2318 s / batch. (data: 2.84e-05)max mem: 15.91637 GB 
[10/26 00:33:19 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2324, average loss: 37.4344
[10/26 00:33:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.67	
[10/26 00:33:19 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/26 00:34:25 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6276 s / batch. (data: 8.31e-04). ETA=16:45:26, max mem: 15.9 GB 
[10/26 00:35:28 visual_prompt]: 	Training 200/1106. train loss: 27.7745,	0.6343 s / batch. (data: 7.92e-04). ETA=16:55:06, max mem: 15.9 GB 
[10/26 00:36:31 visual_prompt]: 	Training 300/1106. train loss: 31.9543,	0.6400 s / batch. (data: 3.54e-04). ETA=17:03:08, max mem: 15.9 GB 
[10/26 00:37:34 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6363 s / batch. (data: 1.10e-02). ETA=16:56:06, max mem: 15.9 GB 
[10/26 00:38:37 visual_prompt]: 	Training 500/1106. train loss: 13.9143,	0.6202 s / batch. (data: 3.22e-04). ETA=16:29:23, max mem: 15.9 GB 
[10/26 00:39:40 visual_prompt]: 	Training 600/1106. train loss: 74.2771,	0.6398 s / batch. (data: 7.97e-04). ETA=16:59:38, max mem: 15.9 GB 
[10/26 00:40:43 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6106 s / batch. (data: 3.16e-04). ETA=16:12:07, max mem: 15.9 GB 
[10/26 00:41:46 visual_prompt]: 	Training 800/1106. train loss: 82.3541,	0.6134 s / batch. (data: 3.40e-04). ETA=16:15:29, max mem: 15.9 GB 
[10/26 00:42:49 visual_prompt]: 	Training 900/1106. train loss: 33.1998,	0.6350 s / batch. (data: 7.54e-04). ETA=16:48:44, max mem: 15.9 GB 
[10/26 00:43:53 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6383 s / batch. (data: 8.09e-04). ETA=16:53:00, max mem: 15.9 GB 
[10/26 00:44:56 visual_prompt]: 	Training 1100/1106. train loss: 16.3858,	0.6194 s / batch. (data: 1.63e-04). ETA=16:21:54, max mem: 15.9 GB 
[10/26 00:44:59 visual_prompt]: Epoch 14 / 100: avg data time: 4.57e-03, avg batch time: 0.6330, average train loss: 44.7011
[10/26 00:45:50 visual_prompt]: 	Test 100/123. loss: 6.402, 0.2252 s / batch. (data: 4.51e-05)max mem: 15.91637 GB 
[10/26 00:46:00 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2329, average loss: 5.8523
[10/26 00:46:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.74	
[10/26 00:46:00 visual_prompt]: Best epoch 14: best metric: -5.852
[10/26 00:46:00 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/26 00:47:05 visual_prompt]: 	Training 100/1106. train loss: 173.2571,	0.6224 s / batch. (data: 2.94e-04). ETA=16:25:38, max mem: 15.9 GB 
[10/26 00:48:08 visual_prompt]: 	Training 200/1106. train loss: 32.4031,	0.6321 s / batch. (data: 8.70e-04). ETA=16:39:54, max mem: 15.9 GB 
[10/26 00:49:11 visual_prompt]: 	Training 300/1106. train loss: 110.6741,	0.6238 s / batch. (data: 3.06e-04). ETA=16:25:47, max mem: 15.9 GB 
[10/26 00:50:14 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6206 s / batch. (data: 3.45e-04). ETA=16:19:42, max mem: 15.9 GB 
[10/26 00:51:17 visual_prompt]: 	Training 500/1106. train loss: 19.8609,	0.6474 s / batch. (data: 7.35e-04). ETA=17:00:53, max mem: 15.9 GB 
[10/26 00:52:20 visual_prompt]: 	Training 600/1106. train loss: 79.4410,	0.6135 s / batch. (data: 2.98e-04). ETA=16:06:21, max mem: 15.9 GB 
[10/26 00:53:23 visual_prompt]: 	Training 700/1106. train loss: 28.4753,	0.6174 s / batch. (data: 2.95e-04). ETA=16:11:32, max mem: 15.9 GB 
[10/26 00:54:26 visual_prompt]: 	Training 800/1106. train loss: 41.4774,	0.6400 s / batch. (data: 9.06e-04). ETA=16:45:58, max mem: 15.9 GB 
[10/26 00:55:29 visual_prompt]: 	Training 900/1106. train loss: 66.3081,	0.6417 s / batch. (data: 2.50e-02). ETA=16:47:42, max mem: 15.9 GB 
[10/26 00:56:32 visual_prompt]: 	Training 1000/1106. train loss: 30.3896,	0.6184 s / batch. (data: 2.58e-04). ETA=16:09:59, max mem: 15.9 GB 
[10/26 00:57:35 visual_prompt]: 	Training 1100/1106. train loss: 256.6254,	0.6131 s / batch. (data: 1.54e-04). ETA=16:00:40, max mem: 15.9 GB 
[10/26 00:57:39 visual_prompt]: Epoch 15 / 100: avg data time: 3.82e-03, avg batch time: 0.6319, average train loss: 51.0963
[10/26 00:58:29 visual_prompt]: 	Test 100/123. loss: 37.744, 0.2286 s / batch. (data: 3.89e-05)max mem: 15.91637 GB 
[10/26 00:58:39 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2324, average loss: 41.6221
[10/26 00:58:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.18	
[10/26 00:58:39 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/26 00:59:44 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6385 s / batch. (data: 2.73e-04). ETA=16:39:23, max mem: 15.9 GB 
[10/26 01:00:47 visual_prompt]: 	Training 200/1106. train loss: 20.2677,	0.6434 s / batch. (data: 8.43e-04). ETA=16:45:52, max mem: 15.9 GB 
[10/26 01:01:50 visual_prompt]: 	Training 300/1106. train loss: 60.8769,	0.6190 s / batch. (data: 3.26e-04). ETA=16:06:41, max mem: 15.9 GB 
[10/26 01:02:54 visual_prompt]: 	Training 400/1106. train loss: 49.7493,	0.6245 s / batch. (data: 7.65e-04). ETA=16:14:19, max mem: 15.9 GB 
[10/26 01:03:57 visual_prompt]: 	Training 500/1106. train loss: 53.7476,	0.6360 s / batch. (data: 3.11e-04). ETA=16:31:13, max mem: 15.9 GB 
[10/26 01:04:59 visual_prompt]: 	Training 600/1106. train loss: 86.7203,	0.6240 s / batch. (data: 2.73e-04). ETA=16:11:29, max mem: 15.9 GB 
[10/26 01:06:02 visual_prompt]: 	Training 700/1106. train loss: 39.2551,	0.6274 s / batch. (data: 7.79e-04). ETA=16:15:39, max mem: 15.9 GB 
[10/26 01:07:05 visual_prompt]: 	Training 800/1106. train loss: 21.8632,	0.6298 s / batch. (data: 3.18e-04). ETA=16:18:20, max mem: 15.9 GB 
[10/26 01:08:08 visual_prompt]: 	Training 900/1106. train loss: 33.0453,	0.6337 s / batch. (data: 8.35e-04). ETA=16:23:27, max mem: 15.9 GB 
[10/26 01:09:11 visual_prompt]: 	Training 1000/1106. train loss: 52.4604,	0.6179 s / batch. (data: 3.44e-04). ETA=15:57:55, max mem: 15.9 GB 
[10/26 01:10:14 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6107 s / batch. (data: 1.45e-04). ETA=15:45:41, max mem: 15.9 GB 
[10/26 01:10:18 visual_prompt]: Epoch 16 / 100: avg data time: 3.81e-03, avg batch time: 0.6316, average train loss: 43.1309
[10/26 01:11:08 visual_prompt]: 	Test 100/123. loss: 25.296, 0.2248 s / batch. (data: 2.98e-05)max mem: 15.91637 GB 
[10/26 01:11:19 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2327, average loss: 17.9063
[10/26 01:11:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.10	
[10/26 01:11:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/26 01:12:24 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6097 s / batch. (data: 3.29e-04). ETA=15:42:58, max mem: 15.9 GB 
[10/26 01:13:27 visual_prompt]: 	Training 200/1106. train loss: 0.3293,	0.6189 s / batch. (data: 3.02e-04). ETA=15:56:15, max mem: 15.9 GB 
[10/26 01:14:30 visual_prompt]: 	Training 300/1106. train loss: 28.9519,	0.6330 s / batch. (data: 1.29e-02). ETA=16:17:02, max mem: 15.9 GB 
[10/26 01:15:33 visual_prompt]: 	Training 400/1106. train loss: 869.5384,	0.6388 s / batch. (data: 7.59e-04). ETA=16:24:51, max mem: 15.9 GB 
[10/26 01:16:36 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6161 s / batch. (data: 3.53e-04). ETA=15:48:50, max mem: 15.9 GB 
[10/26 01:17:39 visual_prompt]: 	Training 600/1106. train loss: 26.1838,	0.6176 s / batch. (data: 2.77e-04). ETA=15:50:08, max mem: 15.9 GB 
[10/26 01:18:42 visual_prompt]: 	Training 700/1106. train loss: 30.9343,	0.6180 s / batch. (data: 3.63e-04). ETA=15:49:41, max mem: 15.9 GB 
[10/26 01:19:45 visual_prompt]: 	Training 800/1106. train loss: 19.5921,	0.6321 s / batch. (data: 2.93e-04). ETA=16:10:22, max mem: 15.9 GB 
[10/26 01:20:48 visual_prompt]: 	Training 900/1106. train loss: 255.6687,	0.6436 s / batch. (data: 7.90e-04). ETA=16:26:53, max mem: 15.9 GB 
[10/26 01:21:51 visual_prompt]: 	Training 1000/1106. train loss: 6.6706,	0.6301 s / batch. (data: 7.94e-04). ETA=16:05:05, max mem: 15.9 GB 
[10/26 01:22:54 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6064 s / batch. (data: 1.54e-04). ETA=15:27:47, max mem: 15.9 GB 
[10/26 01:22:58 visual_prompt]: Epoch 17 / 100: avg data time: 4.56e-03, avg batch time: 0.6324, average train loss: 46.4667
[10/26 01:23:48 visual_prompt]: 	Test 100/123. loss: 63.084, 0.2326 s / batch. (data: 4.36e-05)max mem: 15.91637 GB 
[10/26 01:23:59 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.2322, average loss: 68.1407
[10/26 01:23:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.52	
[10/26 01:23:59 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/26 01:25:03 visual_prompt]: 	Training 100/1106. train loss: 222.9611,	0.6311 s / batch. (data: 8.29e-04). ETA=16:04:32, max mem: 15.9 GB 
[10/26 01:26:06 visual_prompt]: 	Training 200/1106. train loss: 229.3746,	0.6338 s / batch. (data: 7.93e-04). ETA=16:07:30, max mem: 15.9 GB 
[10/26 01:27:10 visual_prompt]: 	Training 300/1106. train loss: 296.4660,	0.6476 s / batch. (data: 3.22e-04). ETA=16:27:30, max mem: 15.9 GB 
[10/26 01:28:12 visual_prompt]: 	Training 400/1106. train loss: 142.4961,	0.6284 s / batch. (data: 1.27e-03). ETA=15:57:15, max mem: 15.9 GB 
[10/26 01:29:16 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6090 s / batch. (data: 3.74e-04). ETA=15:26:39, max mem: 15.9 GB 
[10/26 01:30:19 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6306 s / batch. (data: 7.97e-04). ETA=15:58:31, max mem: 15.9 GB 
[10/26 01:31:22 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6400 s / batch. (data: 7.85e-04). ETA=16:11:42, max mem: 15.9 GB 
[10/26 01:32:25 visual_prompt]: 	Training 800/1106. train loss: 33.1187,	0.6311 s / batch. (data: 3.20e-04). ETA=15:57:05, max mem: 15.9 GB 
[10/26 01:33:27 visual_prompt]: 	Training 900/1106. train loss: 268.1121,	0.6336 s / batch. (data: 1.20e-02). ETA=15:59:53, max mem: 15.9 GB 
[10/26 01:34:30 visual_prompt]: 	Training 1000/1106. train loss: 45.8741,	0.6272 s / batch. (data: 8.10e-04). ETA=15:49:07, max mem: 15.9 GB 
[10/26 01:35:33 visual_prompt]: 	Training 1100/1106. train loss: 22.4004,	0.6187 s / batch. (data: 1.66e-04). ETA=15:35:13, max mem: 15.9 GB 
[10/26 01:35:37 visual_prompt]: Epoch 18 / 100: avg data time: 4.40e-03, avg batch time: 0.6316, average train loss: 54.0637
[10/26 01:36:27 visual_prompt]: 	Test 100/123. loss: 53.617, 0.2317 s / batch. (data: 2.88e-05)max mem: 15.91637 GB 
[10/26 01:36:38 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.2310, average loss: 48.4080
[10/26 01:36:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.55	
[10/26 01:36:38 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/26 01:37:43 visual_prompt]: 	Training 100/1106. train loss: 74.6403,	0.6240 s / batch. (data: 3.09e-04). ETA=15:42:12, max mem: 15.9 GB 
[10/26 01:38:46 visual_prompt]: 	Training 200/1106. train loss: 73.1862,	0.6253 s / batch. (data: 1.30e-02). ETA=15:43:08, max mem: 15.9 GB 
[10/26 01:39:49 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6468 s / batch. (data: 8.36e-04). ETA=16:14:24, max mem: 15.9 GB 
[10/26 01:40:52 visual_prompt]: 	Training 400/1106. train loss: 17.1900,	0.6189 s / batch. (data: 3.03e-04). ETA=15:31:20, max mem: 15.9 GB 
[10/26 01:41:55 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6343 s / batch. (data: 7.92e-04). ETA=15:53:31, max mem: 15.9 GB 
[10/26 01:42:58 visual_prompt]: 	Training 600/1106. train loss: 44.9756,	0.6259 s / batch. (data: 3.20e-04). ETA=15:39:45, max mem: 15.9 GB 
[10/26 01:44:02 visual_prompt]: 	Training 700/1106. train loss: 4.7063,	0.6202 s / batch. (data: 3.17e-04). ETA=15:30:13, max mem: 15.9 GB 
[10/26 01:45:05 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6083 s / batch. (data: 3.26e-04). ETA=15:11:22, max mem: 15.9 GB 
[10/26 01:46:08 visual_prompt]: 	Training 900/1106. train loss: 17.7381,	0.6174 s / batch. (data: 3.12e-04). ETA=15:23:55, max mem: 15.9 GB 
[10/26 01:47:11 visual_prompt]: 	Training 1000/1106. train loss: 8.0667,	0.6243 s / batch. (data: 2.84e-04). ETA=15:33:18, max mem: 15.9 GB 
[10/26 01:48:14 visual_prompt]: 	Training 1100/1106. train loss: 225.4837,	0.6169 s / batch. (data: 1.69e-04). ETA=15:21:07, max mem: 15.9 GB 
[10/26 01:48:18 visual_prompt]: Epoch 19 / 100: avg data time: 4.53e-03, avg batch time: 0.6326, average train loss: 41.8261
[10/26 01:49:08 visual_prompt]: 	Test 100/123. loss: 294.792, 0.2257 s / batch. (data: 6.15e-05)max mem: 15.91637 GB 
[10/26 01:49:18 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.2332, average loss: 265.2812
[10/26 01:49:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.16	
[10/26 01:49:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[10/26 01:50:24 visual_prompt]: 	Training 100/1106. train loss: 50.8663,	0.6126 s / batch. (data: 2.81e-04). ETA=15:13:43, max mem: 15.9 GB 
[10/26 01:51:27 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6436 s / batch. (data: 8.31e-04). ETA=15:58:51, max mem: 15.9 GB 
[10/26 01:52:30 visual_prompt]: 	Training 300/1106. train loss: 52.8105,	0.6330 s / batch. (data: 7.73e-04). ETA=15:41:57, max mem: 15.9 GB 
[10/26 01:53:33 visual_prompt]: 	Training 400/1106. train loss: 11.0893,	0.6364 s / batch. (data: 3.07e-04). ETA=15:45:53, max mem: 15.9 GB 
[10/26 01:54:36 visual_prompt]: 	Training 500/1106. train loss: 78.1878,	0.6418 s / batch. (data: 7.64e-04). ETA=15:52:58, max mem: 15.9 GB 
[10/26 01:55:39 visual_prompt]: 	Training 600/1106. train loss: 33.7978,	0.6332 s / batch. (data: 3.13e-04). ETA=15:39:04, max mem: 15.9 GB 
[10/26 01:56:42 visual_prompt]: 	Training 700/1106. train loss: 33.2084,	0.6293 s / batch. (data: 7.52e-04). ETA=15:32:19, max mem: 15.9 GB 
[10/26 01:57:44 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6080 s / batch. (data: 3.32e-04). ETA=14:59:38, max mem: 15.9 GB 
[10/26 01:58:47 visual_prompt]: 	Training 900/1106. train loss: 224.7066,	0.6319 s / batch. (data: 3.07e-04). ETA=15:34:02, max mem: 15.9 GB 
[10/26 01:59:50 visual_prompt]: 	Training 1000/1106. train loss: 26.7826,	0.6456 s / batch. (data: 1.36e-02). ETA=15:53:10, max mem: 15.9 GB 
[10/26 02:00:53 visual_prompt]: 	Training 1100/1106. train loss: 90.5437,	0.6184 s / batch. (data: 4.79e-04). ETA=15:11:56, max mem: 15.9 GB 
[10/26 02:00:57 visual_prompt]: Epoch 20 / 100: avg data time: 4.76e-03, avg batch time: 0.6320, average train loss: 46.6303
[10/26 02:01:47 visual_prompt]: 	Test 100/123. loss: 41.960, 0.2251 s / batch. (data: 2.96e-05)max mem: 15.91637 GB 
[10/26 02:01:58 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2321, average loss: 46.6717
[10/26 02:01:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.12	
[10/26 02:01:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[10/26 02:03:03 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6400 s / batch. (data: 3.18e-04). ETA=15:42:42, max mem: 15.9 GB 
[10/26 02:04:06 visual_prompt]: 	Training 200/1106. train loss: 86.4574,	0.6480 s / batch. (data: 3.10e-04). ETA=15:53:23, max mem: 15.9 GB 
[10/26 02:05:10 visual_prompt]: 	Training 300/1106. train loss: 40.7823,	0.6488 s / batch. (data: 5.90e-03). ETA=15:53:28, max mem: 15.9 GB 
[10/26 02:06:12 visual_prompt]: 	Training 400/1106. train loss: 109.1672,	0.6378 s / batch. (data: 6.00e-03). ETA=15:36:17, max mem: 15.9 GB 
[10/26 02:07:16 visual_prompt]: 	Training 500/1106. train loss: 203.6524,	0.6185 s / batch. (data: 3.34e-04). ETA=15:06:58, max mem: 15.9 GB 
[10/26 02:08:18 visual_prompt]: 	Training 600/1106. train loss: 348.6924,	0.6306 s / batch. (data: 7.97e-04). ETA=15:23:33, max mem: 15.9 GB 
[10/26 02:09:21 visual_prompt]: 	Training 700/1106. train loss: 84.7570,	0.6142 s / batch. (data: 3.11e-04). ETA=14:58:34, max mem: 15.9 GB 
[10/26 02:10:24 visual_prompt]: 	Training 800/1106. train loss: 48.5479,	0.6207 s / batch. (data: 2.69e-04). ETA=15:07:01, max mem: 15.9 GB 
[10/26 02:11:27 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6205 s / batch. (data: 1.29e-02). ETA=15:05:43, max mem: 15.9 GB 
[10/26 02:12:30 visual_prompt]: 	Training 1000/1106. train loss: 28.6605,	0.6315 s / batch. (data: 8.16e-04). ETA=15:20:40, max mem: 15.9 GB 
[10/26 02:13:33 visual_prompt]: 	Training 1100/1106. train loss: 14.1982,	0.6185 s / batch. (data: 1.54e-04). ETA=15:00:45, max mem: 15.9 GB 
[10/26 02:13:37 visual_prompt]: Epoch 21 / 100: avg data time: 4.51e-03, avg batch time: 0.6324, average train loss: 52.1312
[10/26 02:14:27 visual_prompt]: 	Test 100/123. loss: 33.550, 0.2254 s / batch. (data: 3.60e-05)max mem: 15.91637 GB 
[10/26 02:14:37 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.2339, average loss: 30.4788
[10/26 02:14:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.36	
[10/26 02:14:37 visual_prompt]: Stopping early.
[10/26 02:14:37 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 02:14:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 02:14:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 02:14:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 02:14:38 visual_prompt]: Training with config:
[10/26 02:14:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr10.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 02:14:38 visual_prompt]: Loading training data...
[10/26 02:14:38 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 02:14:38 visual_prompt]: Loading validation data...
[10/26 02:14:38 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 02:14:38 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/26 02:14:40 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/26 02:14:40 visual_prompt]: tuned percent:0.522
[10/26 02:14:40 visual_prompt]: Device used for model: 0
[10/26 02:14:40 visual_prompt]: Setting up Evaluator...
[10/26 02:14:40 visual_prompt]: Setting up Trainer...
[10/26 02:14:40 visual_prompt]: 	Setting up the optimizer...
[10/26 02:14:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 02:15:46 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6237 s / batch. (data: 3.45e-04). ETA=19:08:40, max mem: 15.9 GB 
[10/26 02:16:49 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6408 s / batch. (data: 7.80e-04). ETA=19:39:05, max mem: 15.9 GB 
[10/26 02:17:53 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6467 s / batch. (data: 8.11e-04). ETA=19:48:51, max mem: 15.9 GB 
[10/26 02:18:56 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6303 s / batch. (data: 2.93e-04). ETA=19:17:37, max mem: 15.9 GB 
[10/26 02:19:59 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6562 s / batch. (data: 2.44e-02). ETA=20:04:08, max mem: 15.9 GB 
[10/26 02:21:03 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6320 s / batch. (data: 7.79e-04). ETA=19:18:45, max mem: 15.9 GB 
[10/26 02:22:06 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6468 s / batch. (data: 9.43e-04). ETA=19:44:44, max mem: 15.9 GB 
[10/26 02:23:10 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6440 s / batch. (data: 7.24e-04). ETA=19:38:33, max mem: 15.9 GB 
[10/26 02:24:13 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6508 s / batch. (data: 3.14e-04). ETA=19:49:49, max mem: 15.9 GB 
[10/26 02:25:16 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6189 s / batch. (data: 3.47e-04). ETA=18:50:33, max mem: 15.9 GB 
[10/26 02:26:20 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6181 s / batch. (data: 2.10e-04). ETA=18:47:58, max mem: 15.9 GB 
[10/26 02:26:23 visual_prompt]: Epoch 1 / 100: avg data time: 4.82e-03, avg batch time: 0.6357, average train loss: 1.4028
[10/26 02:27:13 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2386 s / batch. (data: 3.91e-05)max mem: 15.92341 GB 
[10/26 02:27:24 visual_prompt]: Inference (val):avg data time: 1.31e-04, avg batch time: 0.2328, average loss: 1.3505
[10/26 02:27:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/26 02:27:24 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/26 02:28:28 visual_prompt]: 	Training 100/1106. train loss: 8.2329,	0.6213 s / batch. (data: 2.96e-04). ETA=18:52:45, max mem: 15.9 GB 
[10/26 02:29:32 visual_prompt]: 	Training 200/1106. train loss: 0.9971,	0.6326 s / batch. (data: 7.79e-04). ETA=19:12:23, max mem: 15.9 GB 
[10/26 02:30:35 visual_prompt]: 	Training 300/1106. train loss: 3.1441,	0.6240 s / batch. (data: 3.22e-04). ETA=18:55:39, max mem: 15.9 GB 
[10/26 02:31:38 visual_prompt]: 	Training 400/1106. train loss: 0.0048,	0.6191 s / batch. (data: 3.97e-04). ETA=18:45:40, max mem: 15.9 GB 
[10/26 02:32:41 visual_prompt]: 	Training 500/1106. train loss: 15.0241,	0.6381 s / batch. (data: 5.87e-03). ETA=19:19:13, max mem: 15.9 GB 
[10/26 02:33:44 visual_prompt]: 	Training 600/1106. train loss: 3.3630,	0.6192 s / batch. (data: 4.11e-04). ETA=18:43:45, max mem: 15.9 GB 
[10/26 02:34:48 visual_prompt]: 	Training 700/1106. train loss: 0.0121,	0.6350 s / batch. (data: 1.20e-02). ETA=19:11:29, max mem: 15.9 GB 
[10/26 02:35:51 visual_prompt]: 	Training 800/1106. train loss: 5.2975,	0.6440 s / batch. (data: 8.03e-04). ETA=19:26:42, max mem: 15.9 GB 
[10/26 02:36:54 visual_prompt]: 	Training 900/1106. train loss: 0.4576,	0.6301 s / batch. (data: 3.09e-04). ETA=19:00:26, max mem: 15.9 GB 
[10/26 02:37:57 visual_prompt]: 	Training 1000/1106. train loss: 0.0002,	0.6313 s / batch. (data: 3.91e-04). ETA=19:01:36, max mem: 15.9 GB 
[10/26 02:39:01 visual_prompt]: 	Training 1100/1106. train loss: 0.0124,	0.6178 s / batch. (data: 1.33e-04). ETA=18:36:00, max mem: 15.9 GB 
[10/26 02:39:04 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-03, avg batch time: 0.6334, average train loss: 6.9544
[10/26 02:39:54 visual_prompt]: 	Test 100/123. loss: 17.935, 0.2247 s / batch. (data: 3.41e-05)max mem: 15.92341 GB 
[10/26 02:40:05 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2323, average loss: 16.1472
[10/26 02:40:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.01	
[10/26 02:40:05 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/26 02:41:11 visual_prompt]: 	Training 100/1106. train loss: 21.2589,	0.6324 s / batch. (data: 7.91e-04). ETA=19:01:17, max mem: 15.9 GB 
[10/26 02:42:15 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6192 s / batch. (data: 3.15e-04). ETA=18:36:25, max mem: 15.9 GB 
[10/26 02:43:18 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6568 s / batch. (data: 1.68e-02). ETA=19:43:08, max mem: 15.9 GB 
[10/26 02:44:21 visual_prompt]: 	Training 400/1106. train loss: 5.1936,	0.6320 s / batch. (data: 7.56e-04). ETA=18:57:28, max mem: 15.9 GB 
[10/26 02:45:24 visual_prompt]: 	Training 500/1106. train loss: 28.6354,	0.6177 s / batch. (data: 3.49e-04). ETA=18:30:43, max mem: 15.9 GB 
[10/26 02:46:28 visual_prompt]: 	Training 600/1106. train loss: 0.2283,	0.6184 s / batch. (data: 2.75e-04). ETA=18:31:00, max mem: 15.9 GB 
[10/26 02:47:31 visual_prompt]: 	Training 700/1106. train loss: 0.1187,	0.6356 s / batch. (data: 8.23e-04). ETA=19:00:46, max mem: 15.9 GB 
[10/26 02:48:34 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6442 s / batch. (data: 1.11e-03). ETA=19:15:06, max mem: 15.9 GB 
[10/26 02:49:38 visual_prompt]: 	Training 900/1106. train loss: 51.4848,	0.6235 s / batch. (data: 3.14e-04). ETA=18:36:54, max mem: 15.9 GB 
[10/26 02:50:41 visual_prompt]: 	Training 1000/1106. train loss: 16.5636,	0.6429 s / batch. (data: 5.92e-03). ETA=19:10:36, max mem: 15.9 GB 
[10/26 02:51:44 visual_prompt]: 	Training 1100/1106. train loss: 7.7735,	0.6171 s / batch. (data: 1.50e-04). ETA=18:23:32, max mem: 15.9 GB 
[10/26 02:51:48 visual_prompt]: Epoch 3 / 100: avg data time: 5.39e-03, avg batch time: 0.6357, average train loss: 9.6698
[10/26 02:52:38 visual_prompt]: 	Test 100/123. loss: 22.757, 0.2250 s / batch. (data: 4.22e-05)max mem: 15.92341 GB 
[10/26 02:52:49 visual_prompt]: Inference (val):avg data time: 9.71e-05, avg batch time: 0.2316, average loss: 25.1383
[10/26 02:52:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.55	
[10/26 02:52:49 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/26 02:53:54 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6345 s / batch. (data: 8.71e-04). ETA=18:53:30, max mem: 15.9 GB 
[10/26 02:54:57 visual_prompt]: 	Training 200/1106. train loss: 78.0139,	0.6317 s / batch. (data: 8.01e-04). ETA=18:47:26, max mem: 15.9 GB 
[10/26 02:56:00 visual_prompt]: 	Training 300/1106. train loss: 0.2403,	0.6183 s / batch. (data: 4.51e-04). ETA=18:22:24, max mem: 15.9 GB 
[10/26 02:57:04 visual_prompt]: 	Training 400/1106. train loss: 0.4286,	0.6417 s / batch. (data: 1.05e-02). ETA=19:03:04, max mem: 15.9 GB 
[10/26 02:58:07 visual_prompt]: 	Training 500/1106. train loss: 0.0386,	0.6176 s / batch. (data: 3.18e-04). ETA=18:19:07, max mem: 15.9 GB 
[10/26 02:59:10 visual_prompt]: 	Training 600/1106. train loss: 0.0427,	0.6312 s / batch. (data: 8.12e-04). ETA=18:42:19, max mem: 15.9 GB 
[10/26 03:00:13 visual_prompt]: 	Training 700/1106. train loss: 55.8299,	0.6190 s / batch. (data: 3.24e-04). ETA=18:19:37, max mem: 15.9 GB 
[10/26 03:01:16 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6322 s / batch. (data: 8.03e-04). ETA=18:42:02, max mem: 15.9 GB 
[10/26 03:02:19 visual_prompt]: 	Training 900/1106. train loss: 29.6817,	0.6746 s / batch. (data: 2.66e-02). ETA=19:56:03, max mem: 15.9 GB 
[10/26 03:03:22 visual_prompt]: 	Training 1000/1106. train loss: 2.6143,	0.6327 s / batch. (data: 7.90e-04). ETA=18:40:44, max mem: 15.9 GB 
[10/26 03:04:26 visual_prompt]: 	Training 1100/1106. train loss: 43.9399,	0.6132 s / batch. (data: 1.59e-04). ETA=18:05:06, max mem: 15.9 GB 
[10/26 03:04:30 visual_prompt]: Epoch 4 / 100: avg data time: 4.70e-03, avg batch time: 0.6338, average train loss: 11.3971
[10/26 03:05:19 visual_prompt]: 	Test 100/123. loss: 8.135, 0.2483 s / batch. (data: 4.34e-05)max mem: 15.92341 GB 
[10/26 03:05:30 visual_prompt]: Inference (val):avg data time: 4.53e-05, avg batch time: 0.2343, average loss: 9.2517
[10/26 03:05:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.82	
[10/26 03:05:30 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/26 03:06:35 visual_prompt]: 	Training 100/1106. train loss: 10.4839,	0.6241 s / batch. (data: 4.09e-04). ETA=18:23:18, max mem: 15.9 GB 
[10/26 03:07:38 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6181 s / batch. (data: 4.31e-04). ETA=18:11:43, max mem: 15.9 GB 
[10/26 03:08:42 visual_prompt]: 	Training 300/1106. train loss: 7.2271,	0.6185 s / batch. (data: 3.11e-04). ETA=18:11:25, max mem: 15.9 GB 
[10/26 03:09:45 visual_prompt]: 	Training 400/1106. train loss: 17.1649,	0.6284 s / batch. (data: 5.42e-03). ETA=18:27:52, max mem: 15.9 GB 
[10/26 03:10:48 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6177 s / batch. (data: 2.98e-04). ETA=18:07:56, max mem: 15.9 GB 
[10/26 03:11:51 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6232 s / batch. (data: 3.22e-04). ETA=18:16:38, max mem: 15.9 GB 
[10/26 03:12:54 visual_prompt]: 	Training 700/1106. train loss: 32.5870,	0.6335 s / batch. (data: 8.37e-04). ETA=18:33:38, max mem: 15.9 GB 
[10/26 03:13:57 visual_prompt]: 	Training 800/1106. train loss: 17.3638,	0.6180 s / batch. (data: 3.08e-04). ETA=18:05:19, max mem: 15.9 GB 
[10/26 03:15:01 visual_prompt]: 	Training 900/1106. train loss: 11.3464,	0.6321 s / batch. (data: 7.33e-04). ETA=18:29:01, max mem: 15.9 GB 
[10/26 03:16:04 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6296 s / batch. (data: 1.22e-02). ETA=18:23:41, max mem: 15.9 GB 
[10/26 03:17:07 visual_prompt]: 	Training 1100/1106. train loss: 13.8585,	0.6166 s / batch. (data: 1.53e-04). ETA=17:59:46, max mem: 15.9 GB 
[10/26 03:17:11 visual_prompt]: Epoch 5 / 100: avg data time: 4.26e-03, avg batch time: 0.6334, average train loss: 14.2390
[10/26 03:18:00 visual_prompt]: 	Test 100/123. loss: 13.652, 0.2253 s / batch. (data: 3.62e-05)max mem: 15.92341 GB 
[10/26 03:18:11 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.2318, average loss: 12.2119
[10/26 03:18:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.59	
[10/26 03:18:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/26 03:19:16 visual_prompt]: 	Training 100/1106. train loss: 5.0853,	0.6294 s / batch. (data: 1.30e-02). ETA=18:21:06, max mem: 15.9 GB 
[10/26 03:20:19 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6190 s / batch. (data: 2.96e-04). ETA=18:01:54, max mem: 15.9 GB 
[10/26 03:21:22 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6196 s / batch. (data: 8.19e-04). ETA=18:01:59, max mem: 15.9 GB 
[10/26 03:22:26 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6186 s / batch. (data: 3.35e-04). ETA=17:59:09, max mem: 15.9 GB 
[10/26 03:23:29 visual_prompt]: 	Training 500/1106. train loss: 2.5571,	0.6188 s / batch. (data: 2.73e-04). ETA=17:58:23, max mem: 15.9 GB 
[10/26 03:24:32 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6180 s / batch. (data: 3.27e-04). ETA=17:56:00, max mem: 15.9 GB 
[10/26 03:25:35 visual_prompt]: 	Training 700/1106. train loss: 1.9150,	0.6170 s / batch. (data: 3.24e-04). ETA=17:53:14, max mem: 15.9 GB 
[10/26 03:26:39 visual_prompt]: 	Training 800/1106. train loss: 44.6903,	0.6116 s / batch. (data: 3.14e-04). ETA=17:42:50, max mem: 15.9 GB 
[10/26 03:27:42 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6444 s / batch. (data: 7.46e-04). ETA=18:38:48, max mem: 15.9 GB 
[10/26 03:28:45 visual_prompt]: 	Training 1000/1106. train loss: 1.2034,	0.6463 s / batch. (data: 2.68e-02). ETA=18:40:57, max mem: 15.9 GB 
[10/26 03:29:48 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6185 s / batch. (data: 1.51e-04). ETA=17:51:43, max mem: 15.9 GB 
[10/26 03:29:51 visual_prompt]: Epoch 6 / 100: avg data time: 3.71e-03, avg batch time: 0.6330, average train loss: 18.4254
[10/26 03:30:41 visual_prompt]: 	Test 100/123. loss: 6.434, 0.2354 s / batch. (data: 3.10e-05)max mem: 15.92341 GB 
[10/26 03:30:52 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.2318, average loss: 5.8284
[10/26 03:30:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.24	
[10/26 03:30:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/26 03:31:57 visual_prompt]: 	Training 100/1106. train loss: 0.8344,	0.6180 s / batch. (data: 3.82e-04). ETA=17:49:50, max mem: 15.9 GB 
[10/26 03:33:00 visual_prompt]: 	Training 200/1106. train loss: 79.8605,	0.6190 s / batch. (data: 3.59e-04). ETA=17:50:31, max mem: 15.9 GB 
[10/26 03:34:03 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6208 s / batch. (data: 3.19e-04). ETA=17:52:33, max mem: 15.9 GB 
[10/26 03:35:06 visual_prompt]: 	Training 400/1106. train loss: 0.1517,	0.6314 s / batch. (data: 8.03e-04). ETA=18:09:53, max mem: 15.9 GB 
[10/26 03:36:10 visual_prompt]: 	Training 500/1106. train loss: 80.9803,	0.6473 s / batch. (data: 2.72e-04). ETA=18:36:12, max mem: 15.9 GB 
[10/26 03:37:13 visual_prompt]: 	Training 600/1106. train loss: 48.7243,	0.6139 s / batch. (data: 3.03e-04). ETA=17:37:39, max mem: 15.9 GB 
[10/26 03:38:16 visual_prompt]: 	Training 700/1106. train loss: 56.5040,	0.6555 s / batch. (data: 8.73e-04). ETA=18:48:11, max mem: 15.9 GB 
[10/26 03:39:19 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6309 s / batch. (data: 8.42e-04). ETA=18:04:49, max mem: 15.9 GB 
[10/26 03:40:23 visual_prompt]: 	Training 900/1106. train loss: 31.3744,	0.6318 s / batch. (data: 2.96e-04). ETA=18:05:15, max mem: 15.9 GB 
[10/26 03:41:26 visual_prompt]: 	Training 1000/1106. train loss: 10.6432,	0.6244 s / batch. (data: 6.40e-03). ETA=17:51:34, max mem: 15.9 GB 
[10/26 03:42:29 visual_prompt]: 	Training 1100/1106. train loss: 28.2053,	0.6187 s / batch. (data: 1.54e-04). ETA=17:40:37, max mem: 15.9 GB 
[10/26 03:42:33 visual_prompt]: Epoch 7 / 100: avg data time: 4.02e-03, avg batch time: 0.6337, average train loss: 22.6321
[10/26 03:43:23 visual_prompt]: 	Test 100/123. loss: 14.142, 0.2437 s / batch. (data: 3.12e-05)max mem: 15.92341 GB 
[10/26 03:43:33 visual_prompt]: Inference (val):avg data time: 9.81e-05, avg batch time: 0.2310, average loss: 12.7301
[10/26 03:43:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.85	
[10/26 03:43:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/26 03:44:38 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6413 s / batch. (data: 7.99e-04). ETA=18:18:19, max mem: 15.9 GB 
[10/26 03:45:41 visual_prompt]: 	Training 200/1106. train loss: 16.7423,	0.6453 s / batch. (data: 8.05e-04). ETA=18:24:01, max mem: 15.9 GB 
[10/26 03:46:45 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6436 s / batch. (data: 7.92e-04). ETA=18:20:05, max mem: 15.9 GB 
[10/26 03:47:48 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6133 s / batch. (data: 2.28e-04). ETA=17:27:17, max mem: 15.9 GB 
[10/26 03:48:51 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6295 s / batch. (data: 5.43e-03). ETA=17:53:55, max mem: 15.9 GB 
[10/26 03:49:54 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6267 s / batch. (data: 3.10e-04). ETA=17:48:03, max mem: 15.9 GB 
[10/26 03:50:57 visual_prompt]: 	Training 700/1106. train loss: 19.7348,	0.6193 s / batch. (data: 3.15e-04). ETA=17:34:21, max mem: 15.9 GB 
[10/26 03:52:00 visual_prompt]: 	Training 800/1106. train loss: 35.4319,	0.6486 s / batch. (data: 2.21e-02). ETA=18:23:16, max mem: 15.9 GB 
[10/26 03:53:03 visual_prompt]: 	Training 900/1106. train loss: 0.7153,	0.6174 s / batch. (data: 3.47e-04). ETA=17:29:09, max mem: 15.9 GB 
[10/26 03:54:06 visual_prompt]: 	Training 1000/1106. train loss: 43.4001,	0.6188 s / batch. (data: 4.67e-04). ETA=17:30:30, max mem: 15.9 GB 
[10/26 03:55:09 visual_prompt]: 	Training 1100/1106. train loss: 8.3717,	0.6162 s / batch. (data: 1.56e-04). ETA=17:25:08, max mem: 15.9 GB 
[10/26 03:55:13 visual_prompt]: Epoch 8 / 100: avg data time: 4.46e-03, avg batch time: 0.6326, average train loss: 24.6478
[10/26 03:56:03 visual_prompt]: 	Test 100/123. loss: 9.247, 0.2292 s / batch. (data: 3.91e-05)max mem: 15.92341 GB 
[10/26 03:56:14 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2315, average loss: 8.2102
[10/26 03:56:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.94	
[10/26 03:56:14 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/26 03:57:19 visual_prompt]: 	Training 100/1106. train loss: 116.1917,	0.6418 s / batch. (data: 2.52e-02). ETA=18:07:16, max mem: 15.9 GB 
[10/26 03:58:22 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6121 s / batch. (data: 3.41e-04). ETA=17:16:04, max mem: 15.9 GB 
[10/26 03:59:25 visual_prompt]: 	Training 300/1106. train loss: 71.6447,	0.6276 s / batch. (data: 8.03e-04). ETA=17:41:09, max mem: 15.9 GB 
[10/26 04:00:28 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6151 s / batch. (data: 3.09e-04). ETA=17:19:03, max mem: 15.9 GB 
[10/26 04:01:31 visual_prompt]: 	Training 500/1106. train loss: 112.2598,	0.6709 s / batch. (data: 1.10e-02). ETA=18:52:14, max mem: 15.9 GB 
[10/26 04:02:34 visual_prompt]: 	Training 600/1106. train loss: 5.3968,	0.6428 s / batch. (data: 8.02e-04). ETA=18:03:38, max mem: 15.9 GB 
[10/26 04:03:37 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6181 s / batch. (data: 3.09e-04). ETA=17:20:58, max mem: 15.9 GB 
[10/26 04:04:41 visual_prompt]: 	Training 800/1106. train loss: 11.4060,	0.6379 s / batch. (data: 3.24e-04). ETA=17:53:20, max mem: 15.9 GB 
[10/26 04:05:44 visual_prompt]: 	Training 900/1106. train loss: 33.6840,	0.6453 s / batch. (data: 5.75e-04). ETA=18:04:36, max mem: 15.9 GB 
[10/26 04:06:47 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6308 s / batch. (data: 7.61e-04). ETA=17:39:18, max mem: 15.9 GB 
[10/26 04:07:50 visual_prompt]: 	Training 1100/1106. train loss: 32.2946,	0.6159 s / batch. (data: 1.37e-04). ETA=17:13:10, max mem: 15.9 GB 
[10/26 04:07:53 visual_prompt]: Epoch 9 / 100: avg data time: 4.89e-03, avg batch time: 0.6325, average train loss: 27.6711
[10/26 04:08:44 visual_prompt]: 	Test 100/123. loss: 17.493, 0.2380 s / batch. (data: 2.91e-05)max mem: 15.92341 GB 
[10/26 04:08:54 visual_prompt]: Inference (val):avg data time: 2.33e-04, avg batch time: 0.2317, average loss: 19.5376
[10/26 04:08:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.25	
[10/26 04:08:54 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/26 04:09:59 visual_prompt]: 	Training 100/1106. train loss: 46.6708,	0.6306 s / batch. (data: 7.82e-04). ETA=17:36:46, max mem: 15.9 GB 
[10/26 04:11:02 visual_prompt]: 	Training 200/1106. train loss: 4.8445,	0.6179 s / batch. (data: 3.27e-04). ETA=17:14:23, max mem: 15.9 GB 
[10/26 04:12:04 visual_prompt]: 	Training 300/1106. train loss: 76.1314,	0.6200 s / batch. (data: 3.40e-04). ETA=17:16:53, max mem: 15.9 GB 
[10/26 04:13:08 visual_prompt]: 	Training 400/1106. train loss: 25.9983,	0.6311 s / batch. (data: 1.11e-03). ETA=17:34:26, max mem: 15.9 GB 
[10/26 04:14:11 visual_prompt]: 	Training 500/1106. train loss: 19.8879,	0.6479 s / batch. (data: 1.12e-02). ETA=18:01:22, max mem: 15.9 GB 
[10/26 04:15:14 visual_prompt]: 	Training 600/1106. train loss: 129.6018,	0.6320 s / batch. (data: 3.32e-04). ETA=17:33:46, max mem: 15.9 GB 
[10/26 04:16:17 visual_prompt]: 	Training 700/1106. train loss: 8.0823,	0.6457 s / batch. (data: 9.86e-04). ETA=17:55:37, max mem: 15.9 GB 
[10/26 04:17:20 visual_prompt]: 	Training 800/1106. train loss: 12.3471,	0.6440 s / batch. (data: 8.35e-04). ETA=17:51:41, max mem: 15.9 GB 
[10/26 04:18:23 visual_prompt]: 	Training 900/1106. train loss: 79.5072,	0.6270 s / batch. (data: 3.24e-04). ETA=17:22:21, max mem: 15.9 GB 
[10/26 04:19:26 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6436 s / batch. (data: 7.98e-03). ETA=17:48:53, max mem: 15.9 GB 
[10/26 04:20:29 visual_prompt]: 	Training 1100/1106. train loss: 2.1020,	0.6163 s / batch. (data: 1.70e-04). ETA=17:02:32, max mem: 15.9 GB 
[10/26 04:20:33 visual_prompt]: Epoch 10 / 100: avg data time: 3.83e-03, avg batch time: 0.6319, average train loss: 38.3915
[10/26 04:21:22 visual_prompt]: 	Test 100/123. loss: 18.348, 0.2272 s / batch. (data: 2.86e-05)max mem: 15.92341 GB 
[10/26 04:21:33 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2320, average loss: 16.5184
[10/26 04:21:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.70	
[10/26 04:21:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/26 04:22:38 visual_prompt]: 	Training 100/1106. train loss: 124.5439,	0.6204 s / batch. (data: 7.64e-04). ETA=17:08:15, max mem: 15.9 GB 
[10/26 04:23:41 visual_prompt]: 	Training 200/1106. train loss: 31.6969,	0.6402 s / batch. (data: 8.07e-04). ETA=17:39:55, max mem: 15.9 GB 
[10/26 04:24:44 visual_prompt]: 	Training 300/1106. train loss: 64.7448,	0.6314 s / batch. (data: 7.26e-04). ETA=17:24:18, max mem: 15.9 GB 
[10/26 04:25:48 visual_prompt]: 	Training 400/1106. train loss: 54.9290,	0.6368 s / batch. (data: 7.79e-04). ETA=17:32:15, max mem: 15.9 GB 
[10/26 04:26:51 visual_prompt]: 	Training 500/1106. train loss: 0.4568,	0.6328 s / batch. (data: 7.37e-04). ETA=17:24:33, max mem: 15.9 GB 
[10/26 04:27:54 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6165 s / batch. (data: 7.89e-04). ETA=16:56:37, max mem: 15.9 GB 
[10/26 04:28:56 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6211 s / batch. (data: 7.74e-04). ETA=17:03:06, max mem: 15.9 GB 
[10/26 04:30:00 visual_prompt]: 	Training 800/1106. train loss: 11.3578,	0.6447 s / batch. (data: 1.34e-02). ETA=17:41:02, max mem: 15.9 GB 
[10/26 04:31:03 visual_prompt]: 	Training 900/1106. train loss: 53.0086,	0.6177 s / batch. (data: 3.39e-04). ETA=16:55:31, max mem: 15.9 GB 
[10/26 04:32:06 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6098 s / batch. (data: 3.09e-04). ETA=16:41:33, max mem: 15.9 GB 
[10/26 04:33:08 visual_prompt]: 	Training 1100/1106. train loss: 32.1043,	0.6174 s / batch. (data: 1.49e-04). ETA=16:52:52, max mem: 15.9 GB 
[10/26 04:33:12 visual_prompt]: Epoch 11 / 100: avg data time: 4.38e-03, avg batch time: 0.6319, average train loss: 34.6075
[10/26 04:34:02 visual_prompt]: 	Test 100/123. loss: 11.041, 0.2353 s / batch. (data: 4.03e-05)max mem: 15.92341 GB 
[10/26 04:34:13 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2325, average loss: 12.5504
[10/26 04:34:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.65	
[10/26 04:34:13 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/26 04:35:19 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6178 s / batch. (data: 3.20e-04). ETA=16:52:27, max mem: 15.9 GB 
[10/26 04:36:22 visual_prompt]: 	Training 200/1106. train loss: 0.9730,	0.6429 s / batch. (data: 8.23e-04). ETA=17:32:34, max mem: 15.9 GB 
[10/26 04:37:25 visual_prompt]: 	Training 300/1106. train loss: 93.0208,	0.6478 s / batch. (data: 2.81e-04). ETA=17:39:33, max mem: 15.9 GB 
[10/26 04:38:28 visual_prompt]: 	Training 400/1106. train loss: 1.2356,	0.6351 s / batch. (data: 7.80e-04). ETA=17:17:45, max mem: 15.9 GB 
[10/26 04:39:31 visual_prompt]: 	Training 500/1106. train loss: 86.2861,	0.6230 s / batch. (data: 2.71e-04). ETA=16:56:48, max mem: 15.9 GB 
[10/26 04:40:34 visual_prompt]: 	Training 600/1106. train loss: 15.2151,	0.6232 s / batch. (data: 7.50e-04). ETA=16:56:12, max mem: 15.9 GB 
[10/26 04:41:37 visual_prompt]: 	Training 700/1106. train loss: 20.1023,	0.6336 s / batch. (data: 7.63e-04). ETA=17:12:05, max mem: 15.9 GB 
[10/26 04:42:40 visual_prompt]: 	Training 800/1106. train loss: 26.0333,	0.6192 s / batch. (data: 3.01e-04). ETA=16:47:38, max mem: 15.9 GB 
[10/26 04:43:43 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6433 s / batch. (data: 8.00e-04). ETA=17:25:44, max mem: 15.9 GB 
[10/26 04:44:46 visual_prompt]: 	Training 1000/1106. train loss: 70.2770,	0.6390 s / batch. (data: 7.63e-04). ETA=17:17:41, max mem: 15.9 GB 
[10/26 04:45:50 visual_prompt]: 	Training 1100/1106. train loss: 97.4484,	0.6129 s / batch. (data: 1.56e-04). ETA=16:34:20, max mem: 15.9 GB 
[10/26 04:45:53 visual_prompt]: Epoch 12 / 100: avg data time: 4.99e-03, avg batch time: 0.6333, average train loss: 33.2365
[10/26 04:46:43 visual_prompt]: 	Test 100/123. loss: 2.145, 0.2446 s / batch. (data: 4.17e-05)max mem: 15.92341 GB 
[10/26 04:46:54 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2332, average loss: 4.0822
[10/26 04:46:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.12	
[10/26 04:46:54 visual_prompt]: Best epoch 12: best metric: -4.082
[10/26 04:46:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/26 04:47:59 visual_prompt]: 	Training 100/1106. train loss: 231.7540,	0.6313 s / batch. (data: 7.63e-04). ETA=17:02:55, max mem: 15.9 GB 
[10/26 04:49:02 visual_prompt]: 	Training 200/1106. train loss: 61.7183,	0.6221 s / batch. (data: 7.89e-04). ETA=16:46:58, max mem: 15.9 GB 
[10/26 04:50:05 visual_prompt]: 	Training 300/1106. train loss: 0.5049,	0.6176 s / batch. (data: 3.37e-04). ETA=16:38:45, max mem: 15.9 GB 
[10/26 04:51:08 visual_prompt]: 	Training 400/1106. train loss: 71.7070,	0.6187 s / batch. (data: 3.58e-04). ETA=16:39:29, max mem: 15.9 GB 
[10/26 04:52:11 visual_prompt]: 	Training 500/1106. train loss: 4.4094,	0.6376 s / batch. (data: 7.62e-04). ETA=17:08:57, max mem: 15.9 GB 
[10/26 04:53:14 visual_prompt]: 	Training 600/1106. train loss: 5.2783,	0.6330 s / batch. (data: 1.53e-02). ETA=17:00:26, max mem: 15.9 GB 
[10/26 04:54:17 visual_prompt]: 	Training 700/1106. train loss: 35.1590,	0.6333 s / batch. (data: 3.17e-04). ETA=16:59:57, max mem: 15.9 GB 
[10/26 04:55:20 visual_prompt]: 	Training 800/1106. train loss: 32.6473,	0.6186 s / batch. (data: 2.95e-04). ETA=16:35:13, max mem: 15.9 GB 
[10/26 04:56:24 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6573 s / batch. (data: 4.08e-02). ETA=17:36:25, max mem: 15.9 GB 
[10/26 04:57:27 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6217 s / batch. (data: 8.47e-04). ETA=16:38:04, max mem: 15.9 GB 
[10/26 04:58:30 visual_prompt]: 	Training 1100/1106. train loss: 23.6651,	0.6175 s / batch. (data: 1.71e-04). ETA=16:30:24, max mem: 15.9 GB 
[10/26 04:58:33 visual_prompt]: Epoch 13 / 100: avg data time: 4.60e-03, avg batch time: 0.6323, average train loss: 37.4785
[10/26 04:59:24 visual_prompt]: 	Test 100/123. loss: 22.402, 0.2377 s / batch. (data: 3.89e-05)max mem: 15.92341 GB 
[10/26 04:59:34 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.2324, average loss: 20.2268
[10/26 04:59:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.32	
[10/26 04:59:34 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/26 05:00:40 visual_prompt]: 	Training 100/1106. train loss: 29.9771,	0.6453 s / batch. (data: 7.71e-04). ETA=17:13:48, max mem: 15.9 GB 
[10/26 05:01:42 visual_prompt]: 	Training 200/1106. train loss: 44.6806,	0.6253 s / batch. (data: 8.15e-04). ETA=16:40:37, max mem: 15.9 GB 
[10/26 05:02:46 visual_prompt]: 	Training 300/1106. train loss: 45.9035,	0.6240 s / batch. (data: 3.15e-04). ETA=16:37:33, max mem: 15.9 GB 
[10/26 05:03:49 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6126 s / batch. (data: 8.32e-04). ETA=16:18:23, max mem: 15.9 GB 
[10/26 05:04:52 visual_prompt]: 	Training 500/1106. train loss: 40.4977,	0.6181 s / batch. (data: 3.26e-04). ETA=16:26:05, max mem: 15.9 GB 
[10/26 05:05:55 visual_prompt]: 	Training 600/1106. train loss: 30.7770,	0.6287 s / batch. (data: 3.45e-04). ETA=16:41:57, max mem: 15.9 GB 
[10/26 05:06:58 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6390 s / batch. (data: 7.96e-04). ETA=16:57:20, max mem: 15.9 GB 
[10/26 05:08:01 visual_prompt]: 	Training 800/1106. train loss: 21.7468,	0.6547 s / batch. (data: 3.36e-02). ETA=17:21:10, max mem: 15.9 GB 
[10/26 05:09:04 visual_prompt]: 	Training 900/1106. train loss: 14.7986,	0.6412 s / batch. (data: 7.88e-04). ETA=16:58:36, max mem: 15.9 GB 
[10/26 05:10:07 visual_prompt]: 	Training 1000/1106. train loss: 11.8635,	0.6331 s / batch. (data: 6.98e-04). ETA=16:44:43, max mem: 15.9 GB 
[10/26 05:11:10 visual_prompt]: 	Training 1100/1106. train loss: 23.6536,	0.6170 s / batch. (data: 1.61e-04). ETA=16:18:12, max mem: 15.9 GB 
[10/26 05:11:14 visual_prompt]: Epoch 14 / 100: avg data time: 4.75e-03, avg batch time: 0.6327, average train loss: 31.8007
[10/26 05:12:03 visual_prompt]: 	Test 100/123. loss: 1.501, 0.2411 s / batch. (data: 4.08e-05)max mem: 15.92341 GB 
[10/26 05:12:14 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2322, average loss: 1.5321
[10/26 05:12:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 42.68	rocauc: 51.64	
[10/26 05:12:14 visual_prompt]: Best epoch 14: best metric: -1.532
[10/26 05:12:14 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/26 05:13:19 visual_prompt]: 	Training 100/1106. train loss: 34.5650,	0.6174 s / batch. (data: 3.29e-04). ETA=16:17:44, max mem: 15.9 GB 
[10/26 05:14:22 visual_prompt]: 	Training 200/1106. train loss: 24.7211,	0.6324 s / batch. (data: 7.60e-04). ETA=16:40:25, max mem: 15.9 GB 
[10/26 05:15:25 visual_prompt]: 	Training 300/1106. train loss: 48.2212,	0.6380 s / batch. (data: 7.95e-04). ETA=16:48:12, max mem: 15.9 GB 
[10/26 05:16:29 visual_prompt]: 	Training 400/1106. train loss: 305.2171,	0.6317 s / batch. (data: 8.08e-04). ETA=16:37:13, max mem: 15.9 GB 
[10/26 05:17:31 visual_prompt]: 	Training 500/1106. train loss: 0.0993,	0.6169 s / batch. (data: 3.22e-04). ETA=16:12:49, max mem: 15.9 GB 
[10/26 05:18:34 visual_prompt]: 	Training 600/1106. train loss: 109.2182,	0.6341 s / batch. (data: 7.94e-04). ETA=16:38:49, max mem: 15.9 GB 
[10/26 05:19:37 visual_prompt]: 	Training 700/1106. train loss: 33.5162,	0.6412 s / batch. (data: 2.63e-02). ETA=16:48:59, max mem: 15.9 GB 
[10/26 05:20:40 visual_prompt]: 	Training 800/1106. train loss: 33.9572,	0.6219 s / batch. (data: 3.28e-04). ETA=16:17:32, max mem: 15.9 GB 
[10/26 05:21:44 visual_prompt]: 	Training 900/1106. train loss: 74.6902,	0.6457 s / batch. (data: 3.36e-04). ETA=16:53:52, max mem: 15.9 GB 
[10/26 05:22:47 visual_prompt]: 	Training 1000/1106. train loss: 7.1354,	0.6224 s / batch. (data: 2.89e-04). ETA=16:16:21, max mem: 15.9 GB 
[10/26 05:23:50 visual_prompt]: 	Training 1100/1106. train loss: 100.2845,	0.6119 s / batch. (data: 1.53e-04). ETA=15:58:46, max mem: 15.9 GB 
[10/26 05:23:54 visual_prompt]: Epoch 15 / 100: avg data time: 4.06e-03, avg batch time: 0.6324, average train loss: 43.2634
[10/26 05:24:44 visual_prompt]: 	Test 100/123. loss: 72.649, 0.2326 s / batch. (data: 4.10e-05)max mem: 15.92341 GB 
[10/26 05:24:54 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2318, average loss: 79.9774
[10/26 05:24:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.48	
[10/26 05:24:54 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/26 05:25:59 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6164 s / batch. (data: 3.20e-04). ETA=16:04:43, max mem: 15.9 GB 
[10/26 05:27:02 visual_prompt]: 	Training 200/1106. train loss: 11.8115,	0.6176 s / batch. (data: 3.00e-04). ETA=16:05:33, max mem: 15.9 GB 
[10/26 05:28:05 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6393 s / batch. (data: 7.70e-04). ETA=16:38:30, max mem: 15.9 GB 
[10/26 05:29:08 visual_prompt]: 	Training 400/1106. train loss: 110.9579,	0.6292 s / batch. (data: 3.19e-04). ETA=16:21:34, max mem: 15.9 GB 
[10/26 05:30:11 visual_prompt]: 	Training 500/1106. train loss: 30.2063,	0.6480 s / batch. (data: 8.16e-04). ETA=16:49:55, max mem: 15.9 GB 
[10/26 05:31:14 visual_prompt]: 	Training 600/1106. train loss: 22.9468,	0.6188 s / batch. (data: 3.38e-04). ETA=16:03:25, max mem: 15.9 GB 
[10/26 05:32:18 visual_prompt]: 	Training 700/1106. train loss: 8.7799,	0.6439 s / batch. (data: 7.68e-04). ETA=16:41:21, max mem: 15.9 GB 
[10/26 05:33:21 visual_prompt]: 	Training 800/1106. train loss: 96.0372,	0.6434 s / batch. (data: 5.93e-03). ETA=16:39:31, max mem: 15.9 GB 
[10/26 05:34:24 visual_prompt]: 	Training 900/1106. train loss: 7.3734,	0.6447 s / batch. (data: 8.53e-04). ETA=16:40:29, max mem: 15.9 GB 
[10/26 05:35:27 visual_prompt]: 	Training 1000/1106. train loss: 69.6492,	0.6187 s / batch. (data: 3.06e-04). ETA=15:59:01, max mem: 15.9 GB 
[10/26 05:36:30 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6065 s / batch. (data: 1.39e-04). ETA=15:39:13, max mem: 15.9 GB 
[10/26 05:36:34 visual_prompt]: Epoch 16 / 100: avg data time: 4.12e-03, avg batch time: 0.6322, average train loss: 33.8372
[10/26 05:37:24 visual_prompt]: 	Test 100/123. loss: 14.168, 0.2252 s / batch. (data: 3.96e-05)max mem: 15.92341 GB 
[10/26 05:37:34 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.2332, average loss: 12.8319
[10/26 05:37:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.53	
[10/26 05:37:34 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/26 05:38:40 visual_prompt]: 	Training 100/1106. train loss: 1.4244,	0.6165 s / batch. (data: 4.15e-04). ETA=15:53:32, max mem: 15.9 GB 
[10/26 05:39:43 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6234 s / batch. (data: 5.44e-03). ETA=16:03:08, max mem: 15.9 GB 
[10/26 05:40:46 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6470 s / batch. (data: 1.05e-02). ETA=16:38:33, max mem: 15.9 GB 
[10/26 05:41:49 visual_prompt]: 	Training 400/1106. train loss: 239.9983,	0.6435 s / batch. (data: 1.10e-02). ETA=16:32:04, max mem: 15.9 GB 
[10/26 05:42:52 visual_prompt]: 	Training 500/1106. train loss: 0.8629,	0.6183 s / batch. (data: 2.70e-04). ETA=15:52:16, max mem: 15.9 GB 
[10/26 05:43:56 visual_prompt]: 	Training 600/1106. train loss: 7.1346,	0.6294 s / batch. (data: 1.20e-02). ETA=16:08:16, max mem: 15.9 GB 
[10/26 05:44:59 visual_prompt]: 	Training 700/1106. train loss: 0.0373,	0.6319 s / batch. (data: 7.81e-04). ETA=16:11:02, max mem: 15.9 GB 
[10/26 05:46:02 visual_prompt]: 	Training 800/1106. train loss: 10.8997,	0.6178 s / batch. (data: 3.30e-04). ETA=15:48:20, max mem: 15.9 GB 
[10/26 05:47:05 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6314 s / batch. (data: 2.62e-04). ETA=16:08:10, max mem: 15.9 GB 
[10/26 05:48:08 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6086 s / batch. (data: 3.22e-04). ETA=15:32:15, max mem: 15.9 GB 
[10/26 05:49:11 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6175 s / batch. (data: 1.63e-04). ETA=15:44:53, max mem: 15.9 GB 
[10/26 05:49:14 visual_prompt]: Epoch 17 / 100: avg data time: 4.45e-03, avg batch time: 0.6328, average train loss: 36.4691
[10/26 05:50:05 visual_prompt]: 	Test 100/123. loss: 6.424, 0.2244 s / batch. (data: 4.08e-05)max mem: 15.92341 GB 
[10/26 05:50:15 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2331, average loss: 6.6810
[10/26 05:50:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.07	
[10/26 05:50:15 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/26 05:51:20 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6281 s / batch. (data: 9.31e-04). ETA=16:00:00, max mem: 15.9 GB 
[10/26 05:52:23 visual_prompt]: 	Training 200/1106. train loss: 148.7183,	0.6433 s / batch. (data: 8.57e-04). ETA=16:22:01, max mem: 15.9 GB 
[10/26 05:53:26 visual_prompt]: 	Training 300/1106. train loss: 189.0644,	0.6183 s / batch. (data: 3.18e-04). ETA=15:42:55, max mem: 15.9 GB 
[10/26 05:54:29 visual_prompt]: 	Training 400/1106. train loss: 33.9241,	0.6156 s / batch. (data: 3.57e-04). ETA=15:37:48, max mem: 15.9 GB 
[10/26 05:55:32 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6372 s / batch. (data: 3.04e-02). ETA=16:09:39, max mem: 15.9 GB 
[10/26 05:56:35 visual_prompt]: 	Training 600/1106. train loss: 30.3311,	0.6267 s / batch. (data: 3.16e-04). ETA=15:52:36, max mem: 15.9 GB 
[10/26 05:57:38 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6309 s / batch. (data: 8.00e-04). ETA=15:57:50, max mem: 15.9 GB 
[10/26 05:58:41 visual_prompt]: 	Training 800/1106. train loss: 28.4428,	0.6402 s / batch. (data: 8.07e-04). ETA=16:10:52, max mem: 15.9 GB 
[10/26 05:59:44 visual_prompt]: 	Training 900/1106. train loss: 175.3996,	0.6525 s / batch. (data: 1.20e-03). ETA=16:28:34, max mem: 15.9 GB 
[10/26 06:00:47 visual_prompt]: 	Training 1000/1106. train loss: 31.2823,	0.6181 s / batch. (data: 3.09e-04). ETA=15:35:24, max mem: 15.9 GB 
[10/26 06:01:50 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6069 s / batch. (data: 1.45e-04). ETA=15:17:20, max mem: 15.9 GB 
[10/26 06:01:54 visual_prompt]: Epoch 18 / 100: avg data time: 4.30e-03, avg batch time: 0.6318, average train loss: 35.5760
[10/26 06:02:44 visual_prompt]: 	Test 100/123. loss: 13.952, 0.2454 s / batch. (data: 3.84e-05)max mem: 15.92341 GB 
[10/26 06:02:54 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2322, average loss: 13.4274
[10/26 06:02:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.62	
[10/26 06:02:54 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/26 06:04:00 visual_prompt]: 	Training 100/1106. train loss: 0.0003,	0.6384 s / batch. (data: 7.72e-04). ETA=16:03:56, max mem: 15.9 GB 
[10/26 06:05:03 visual_prompt]: 	Training 200/1106. train loss: 50.9047,	0.6150 s / batch. (data: 4.42e-04). ETA=15:27:30, max mem: 15.9 GB 
[10/26 06:06:06 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6229 s / batch. (data: 7.94e-04). ETA=15:38:27, max mem: 15.9 GB 
[10/26 06:07:09 visual_prompt]: 	Training 400/1106. train loss: 29.9601,	0.6350 s / batch. (data: 8.00e-04). ETA=15:55:36, max mem: 15.9 GB 
[10/26 06:08:12 visual_prompt]: 	Training 500/1106. train loss: 10.6438,	0.6302 s / batch. (data: 8.07e-04). ETA=15:47:23, max mem: 15.9 GB 
[10/26 06:09:15 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6077 s / batch. (data: 3.21e-04). ETA=15:12:26, max mem: 15.9 GB 
[10/26 06:10:18 visual_prompt]: 	Training 700/1106. train loss: 10.5189,	0.6336 s / batch. (data: 9.43e-04). ETA=15:50:17, max mem: 15.9 GB 
[10/26 06:11:22 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6196 s / batch. (data: 3.20e-04). ETA=15:28:19, max mem: 15.9 GB 
[10/26 06:12:25 visual_prompt]: 	Training 900/1106. train loss: 0.0336,	0.6298 s / batch. (data: 3.10e-04). ETA=15:42:34, max mem: 15.9 GB 
[10/26 06:13:27 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6232 s / batch. (data: 8.76e-04). ETA=15:31:37, max mem: 15.9 GB 
[10/26 06:14:31 visual_prompt]: 	Training 1100/1106. train loss: 67.3537,	0.6175 s / batch. (data: 1.74e-04). ETA=15:22:01, max mem: 15.9 GB 
[10/26 06:14:34 visual_prompt]: Epoch 19 / 100: avg data time: 4.07e-03, avg batch time: 0.6328, average train loss: 31.8030
[10/26 06:15:24 visual_prompt]: 	Test 100/123. loss: 153.502, 0.2488 s / batch. (data: 4.10e-05)max mem: 15.92341 GB 
[10/26 06:15:35 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2318, average loss: 139.0392
[10/26 06:15:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.07	
[10/26 06:15:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[10/26 06:16:41 visual_prompt]: 	Training 100/1106. train loss: 6.4272,	0.6323 s / batch. (data: 1.19e-03). ETA=15:42:58, max mem: 15.9 GB 
[10/26 06:17:44 visual_prompt]: 	Training 200/1106. train loss: 21.5890,	0.6533 s / batch. (data: 9.02e-04). ETA=16:13:13, max mem: 15.9 GB 
[10/26 06:18:47 visual_prompt]: 	Training 300/1106. train loss: 54.7533,	0.6680 s / batch. (data: 8.40e-04). ETA=16:34:04, max mem: 15.9 GB 
[10/26 06:19:51 visual_prompt]: 	Training 400/1106. train loss: 9.4948,	0.6406 s / batch. (data: 7.92e-04). ETA=15:52:16, max mem: 15.9 GB 
[10/26 06:20:54 visual_prompt]: 	Training 500/1106. train loss: 72.3370,	0.6447 s / batch. (data: 8.59e-04). ETA=15:57:11, max mem: 15.9 GB 
[10/26 06:21:57 visual_prompt]: 	Training 600/1106. train loss: 21.3013,	0.6195 s / batch. (data: 2.78e-04). ETA=15:18:46, max mem: 15.9 GB 
[10/26 06:23:00 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6247 s / batch. (data: 8.05e-04). ETA=15:25:22, max mem: 15.9 GB 
[10/26 06:24:03 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6086 s / batch. (data: 2.90e-04). ETA=15:00:34, max mem: 15.9 GB 
[10/26 06:25:06 visual_prompt]: 	Training 900/1106. train loss: 172.1871,	0.6184 s / batch. (data: 3.27e-04). ETA=15:14:00, max mem: 15.9 GB 
[10/26 06:26:09 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6513 s / batch. (data: 7.76e-04). ETA=16:01:31, max mem: 15.9 GB 
[10/26 06:27:13 visual_prompt]: 	Training 1100/1106. train loss: 29.1769,	0.6172 s / batch. (data: 2.09e-04). ETA=15:10:15, max mem: 15.9 GB 
[10/26 06:27:16 visual_prompt]: Epoch 20 / 100: avg data time: 4.82e-03, avg batch time: 0.6339, average train loss: 29.3827
[10/26 06:28:07 visual_prompt]: 	Test 100/123. loss: 0.972, 0.2362 s / batch. (data: 3.89e-05)max mem: 15.92341 GB 
[10/26 06:28:17 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2338, average loss: 4.2496
[10/26 06:28:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 54.57	
[10/26 06:28:17 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[10/26 06:29:23 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6438 s / batch. (data: 8.27e-04). ETA=15:48:19, max mem: 15.9 GB 
[10/26 06:30:26 visual_prompt]: 	Training 200/1106. train loss: 41.0662,	0.6855 s / batch. (data: 1.10e-02). ETA=16:48:33, max mem: 15.9 GB 
[10/26 06:31:29 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6311 s / batch. (data: 8.54e-04). ETA=15:27:26, max mem: 15.9 GB 
[10/26 06:32:32 visual_prompt]: 	Training 400/1106. train loss: 93.6149,	0.6241 s / batch. (data: 1.20e-02). ETA=15:16:12, max mem: 15.9 GB 
[10/26 06:33:35 visual_prompt]: 	Training 500/1106. train loss: 198.2433,	0.6310 s / batch. (data: 7.96e-04). ETA=15:25:17, max mem: 15.9 GB 
[10/26 06:34:38 visual_prompt]: 	Training 600/1106. train loss: 231.8794,	0.6331 s / batch. (data: 2.98e-04). ETA=15:27:19, max mem: 15.9 GB 
[10/26 06:35:41 visual_prompt]: 	Training 700/1106. train loss: 16.3251,	0.6175 s / batch. (data: 2.95e-04). ETA=15:03:26, max mem: 15.9 GB 
[10/26 06:36:44 visual_prompt]: 	Training 800/1106. train loss: 36.5582,	0.6274 s / batch. (data: 8.13e-04). ETA=15:16:51, max mem: 15.9 GB 
[10/26 06:37:47 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6360 s / batch. (data: 7.91e-04). ETA=15:28:21, max mem: 15.9 GB 
[10/26 06:38:50 visual_prompt]: 	Training 1000/1106. train loss: 16.1711,	0.6414 s / batch. (data: 7.88e-04). ETA=15:35:10, max mem: 15.9 GB 
[10/26 06:39:53 visual_prompt]: 	Training 1100/1106. train loss: 23.6636,	0.6173 s / batch. (data: 1.32e-04). ETA=14:58:59, max mem: 15.9 GB 
[10/26 06:39:57 visual_prompt]: Epoch 21 / 100: avg data time: 4.58e-03, avg batch time: 0.6326, average train loss: 35.8901
[10/26 06:40:47 visual_prompt]: 	Test 100/123. loss: 29.302, 0.2249 s / batch. (data: 2.91e-05)max mem: 15.92341 GB 
[10/26 06:40:57 visual_prompt]: Inference (val):avg data time: 9.82e-05, avg batch time: 0.2320, average loss: 25.3265
[10/26 06:40:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.96	
[10/26 06:40:57 visual_prompt]: Stopping early.
[10/26 06:40:57 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 06:40:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 06:40:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 06:40:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 06:40:57 visual_prompt]: Training with config:
[10/26 06:40:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr5.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 06:40:57 visual_prompt]: Loading training data...
[10/26 06:40:57 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 06:40:57 visual_prompt]: Loading validation data...
[10/26 06:40:57 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 06:40:57 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/26 06:41:00 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/26 06:41:00 visual_prompt]: tuned percent:0.522
[10/26 06:41:00 visual_prompt]: Device used for model: 0
[10/26 06:41:00 visual_prompt]: Setting up Evaluator...
[10/26 06:41:00 visual_prompt]: Setting up Trainer...
[10/26 06:41:00 visual_prompt]: 	Setting up the optimizer...
[10/26 06:41:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 06:42:06 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6476 s / batch. (data: 8.55e-04). ETA=19:52:37, max mem: 15.9 GB 
[10/26 06:43:09 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6176 s / batch. (data: 3.03e-04). ETA=18:56:26, max mem: 15.9 GB 
[10/26 06:44:13 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6310 s / batch. (data: 8.23e-04). ETA=19:19:59, max mem: 15.9 GB 
[10/26 06:45:16 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6392 s / batch. (data: 8.12e-04). ETA=19:34:02, max mem: 15.9 GB 
[10/26 06:46:19 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6240 s / batch. (data: 2.82e-04). ETA=19:05:00, max mem: 15.9 GB 
[10/26 06:47:23 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6322 s / batch. (data: 3.25e-04). ETA=19:18:59, max mem: 15.9 GB 
[10/26 06:48:26 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6286 s / batch. (data: 3.45e-04). ETA=19:11:28, max mem: 15.9 GB 
[10/26 06:49:29 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6230 s / batch. (data: 5.43e-03). ETA=19:00:04, max mem: 15.9 GB 
[10/26 06:50:33 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6470 s / batch. (data: 8.02e-04). ETA=19:42:52, max mem: 15.9 GB 
[10/26 06:51:36 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6338 s / batch. (data: 3.48e-04). ETA=19:17:39, max mem: 15.9 GB 
[10/26 06:52:39 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6189 s / batch. (data: 1.60e-04). ETA=18:49:32, max mem: 15.9 GB 
[10/26 06:52:43 visual_prompt]: Epoch 1 / 100: avg data time: 4.90e-03, avg batch time: 0.6357, average train loss: 1.4028
[10/26 06:53:33 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2252 s / batch. (data: 5.32e-05)max mem: 15.92958 GB 
[10/26 06:53:44 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2321, average loss: 1.3505
[10/26 06:53:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/26 06:53:44 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/26 06:54:49 visual_prompt]: 	Training 100/1106. train loss: 2.2406,	0.6320 s / batch. (data: 2.73e-04). ETA=19:12:19, max mem: 15.9 GB 
[10/26 06:55:52 visual_prompt]: 	Training 200/1106. train loss: 0.6176,	0.6170 s / batch. (data: 3.28e-04). ETA=18:43:50, max mem: 15.9 GB 
[10/26 06:56:55 visual_prompt]: 	Training 300/1106. train loss: 0.0947,	0.6219 s / batch. (data: 3.25e-04). ETA=18:51:50, max mem: 15.9 GB 
[10/26 06:57:58 visual_prompt]: 	Training 400/1106. train loss: 0.0011,	0.6295 s / batch. (data: 8.19e-04). ETA=19:04:29, max mem: 15.9 GB 
[10/26 06:59:02 visual_prompt]: 	Training 500/1106. train loss: 4.1370,	0.6280 s / batch. (data: 3.26e-04). ETA=19:00:43, max mem: 15.9 GB 
[10/26 07:00:05 visual_prompt]: 	Training 600/1106. train loss: 1.0633,	0.6317 s / batch. (data: 1.20e-02). ETA=19:06:30, max mem: 15.9 GB 
[10/26 07:01:08 visual_prompt]: 	Training 700/1106. train loss: 0.0101,	0.6193 s / batch. (data: 3.02e-04). ETA=18:43:00, max mem: 15.9 GB 
[10/26 07:02:11 visual_prompt]: 	Training 800/1106. train loss: 0.7556,	0.6431 s / batch. (data: 8.02e-04). ETA=19:25:03, max mem: 15.9 GB 
[10/26 07:03:14 visual_prompt]: 	Training 900/1106. train loss: 0.7456,	0.6346 s / batch. (data: 7.76e-04). ETA=19:08:37, max mem: 15.9 GB 
[10/26 07:04:18 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6344 s / batch. (data: 2.87e-04). ETA=19:07:04, max mem: 15.9 GB 
[10/26 07:05:21 visual_prompt]: 	Training 1100/1106. train loss: 0.0006,	0.6183 s / batch. (data: 2.60e-04). ETA=18:36:56, max mem: 15.9 GB 
[10/26 07:05:25 visual_prompt]: Epoch 2 / 100: avg data time: 3.84e-03, avg batch time: 0.6338, average train loss: 2.2289
[10/26 07:06:15 visual_prompt]: 	Test 100/123. loss: 1.311, 0.2399 s / batch. (data: 2.93e-05)max mem: 15.92958 GB 
[10/26 07:06:25 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.2328, average loss: 1.4139
[10/26 07:06:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.97	
[10/26 07:06:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/26 07:07:31 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6285 s / batch. (data: 8.24e-04). ETA=18:54:19, max mem: 15.9 GB 
[10/26 07:08:35 visual_prompt]: 	Training 200/1106. train loss: 10.7556,	0.6288 s / batch. (data: 7.94e-04). ETA=18:53:53, max mem: 15.9 GB 
[10/26 07:09:38 visual_prompt]: 	Training 300/1106. train loss: 0.1069,	0.6468 s / batch. (data: 8.27e-04). ETA=19:25:06, max mem: 15.9 GB 
[10/26 07:10:41 visual_prompt]: 	Training 400/1106. train loss: 1.8768,	0.6402 s / batch. (data: 1.29e-02). ETA=19:12:12, max mem: 15.9 GB 
[10/26 07:11:45 visual_prompt]: 	Training 500/1106. train loss: 15.1369,	0.6174 s / batch. (data: 3.21e-04). ETA=18:30:05, max mem: 15.9 GB 
[10/26 07:12:48 visual_prompt]: 	Training 600/1106. train loss: 0.0001,	0.6431 s / batch. (data: 1.23e-03). ETA=19:15:15, max mem: 15.9 GB 
[10/26 07:13:51 visual_prompt]: 	Training 700/1106. train loss: 0.1363,	0.6182 s / batch. (data: 3.08e-04). ETA=18:29:38, max mem: 15.9 GB 
[10/26 07:14:55 visual_prompt]: 	Training 800/1106. train loss: 0.0001,	0.6298 s / batch. (data: 1.20e-02). ETA=18:49:16, max mem: 15.9 GB 
[10/26 07:15:58 visual_prompt]: 	Training 900/1106. train loss: 0.0115,	0.6394 s / batch. (data: 7.99e-03). ETA=19:05:24, max mem: 15.9 GB 
[10/26 07:17:01 visual_prompt]: 	Training 1000/1106. train loss: 6.7192,	0.6332 s / batch. (data: 1.39e-02). ETA=18:53:15, max mem: 15.9 GB 
[10/26 07:18:05 visual_prompt]: 	Training 1100/1106. train loss: 0.6812,	0.6184 s / batch. (data: 1.41e-04). ETA=18:25:50, max mem: 15.9 GB 
[10/26 07:18:08 visual_prompt]: Epoch 3 / 100: avg data time: 5.12e-03, avg batch time: 0.6354, average train loss: 4.5529
[10/26 07:18:57 visual_prompt]: 	Test 100/123. loss: 2.595, 0.2400 s / batch. (data: 4.03e-05)max mem: 15.92958 GB 
[10/26 07:19:09 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2326, average loss: 2.8363
[10/26 07:19:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.96	
[10/26 07:19:09 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/26 07:20:14 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6373 s / batch. (data: 1.33e-02). ETA=18:58:29, max mem: 15.9 GB 
[10/26 07:21:18 visual_prompt]: 	Training 200/1106. train loss: 3.4988,	0.6190 s / batch. (data: 3.02e-04). ETA=18:24:45, max mem: 15.9 GB 
[10/26 07:22:21 visual_prompt]: 	Training 300/1106. train loss: 6.6139,	0.6329 s / batch. (data: 8.39e-04). ETA=18:48:26, max mem: 15.9 GB 
[10/26 07:23:24 visual_prompt]: 	Training 400/1106. train loss: 0.7837,	0.6218 s / batch. (data: 3.02e-04). ETA=18:27:43, max mem: 15.9 GB 
[10/26 07:24:27 visual_prompt]: 	Training 500/1106. train loss: 0.0102,	0.6303 s / batch. (data: 3.00e-04). ETA=18:41:42, max mem: 15.9 GB 
[10/26 07:25:31 visual_prompt]: 	Training 600/1106. train loss: 4.5675,	0.6191 s / batch. (data: 3.21e-04). ETA=18:20:43, max mem: 15.9 GB 
[10/26 07:26:34 visual_prompt]: 	Training 700/1106. train loss: 13.2336,	0.6278 s / batch. (data: 3.03e-04). ETA=18:35:11, max mem: 15.9 GB 
[10/26 07:27:37 visual_prompt]: 	Training 800/1106. train loss: 10.8948,	0.6311 s / batch. (data: 8.43e-04). ETA=18:40:01, max mem: 15.9 GB 
[10/26 07:28:40 visual_prompt]: 	Training 900/1106. train loss: 25.0584,	0.6443 s / batch. (data: 5.92e-03). ETA=19:02:22, max mem: 15.9 GB 
[10/26 07:29:43 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6185 s / batch. (data: 3.19e-04). ETA=18:15:40, max mem: 15.9 GB 
[10/26 07:30:47 visual_prompt]: 	Training 1100/1106. train loss: 3.7494,	0.6191 s / batch. (data: 1.60e-04). ETA=18:15:41, max mem: 15.9 GB 
[10/26 07:30:51 visual_prompt]: Epoch 4 / 100: avg data time: 4.40e-03, avg batch time: 0.6349, average train loss: 7.3553
[10/26 07:31:41 visual_prompt]: 	Test 100/123. loss: 7.933, 0.2316 s / batch. (data: 3.19e-05)max mem: 15.92958 GB 
[10/26 07:31:51 visual_prompt]: Inference (val):avg data time: 1.30e-04, avg batch time: 0.2322, average loss: 7.1564
[10/26 07:31:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.56	
[10/26 07:31:51 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/26 07:32:57 visual_prompt]: 	Training 100/1106. train loss: 2.8702,	0.6463 s / batch. (data: 7.86e-04). ETA=19:02:41, max mem: 15.9 GB 
[10/26 07:34:00 visual_prompt]: 	Training 200/1106. train loss: 0.0030,	0.6438 s / batch. (data: 1.30e-02). ETA=18:57:04, max mem: 15.9 GB 
[10/26 07:35:03 visual_prompt]: 	Training 300/1106. train loss: 13.9467,	0.6349 s / batch. (data: 8.18e-04). ETA=18:40:17, max mem: 15.9 GB 
[10/26 07:36:06 visual_prompt]: 	Training 400/1106. train loss: 10.4277,	0.6174 s / batch. (data: 2.72e-04). ETA=18:08:26, max mem: 15.9 GB 
[10/26 07:37:09 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6192 s / batch. (data: 3.37e-04). ETA=18:10:37, max mem: 15.9 GB 
[10/26 07:38:13 visual_prompt]: 	Training 600/1106. train loss: 21.4637,	0.6407 s / batch. (data: 3.10e-04). ETA=18:47:27, max mem: 15.9 GB 
[10/26 07:39:16 visual_prompt]: 	Training 700/1106. train loss: 2.0277,	0.6189 s / batch. (data: 3.28e-04). ETA=18:08:02, max mem: 15.9 GB 
[10/26 07:40:19 visual_prompt]: 	Training 800/1106. train loss: 12.2891,	0.6325 s / batch. (data: 3.04e-04). ETA=18:30:46, max mem: 15.9 GB 
[10/26 07:41:23 visual_prompt]: 	Training 900/1106. train loss: 8.9119,	0.6461 s / batch. (data: 7.59e-04). ETA=18:53:39, max mem: 15.9 GB 
[10/26 07:42:26 visual_prompt]: 	Training 1000/1106. train loss: 0.0003,	0.6316 s / batch. (data: 1.20e-02). ETA=18:27:07, max mem: 15.9 GB 
[10/26 07:43:29 visual_prompt]: 	Training 1100/1106. train loss: 5.4157,	0.6187 s / batch. (data: 1.86e-04). ETA=18:03:30, max mem: 15.9 GB 
[10/26 07:43:33 visual_prompt]: Epoch 5 / 100: avg data time: 3.90e-03, avg batch time: 0.6342, average train loss: 9.4194
[10/26 07:44:23 visual_prompt]: 	Test 100/123. loss: 24.913, 0.2406 s / batch. (data: 4.72e-05)max mem: 15.92958 GB 
[10/26 07:44:33 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2321, average loss: 20.7072
[10/26 07:44:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.14	
[10/26 07:44:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/26 07:45:38 visual_prompt]: 	Training 100/1106. train loss: 1.4448,	0.6300 s / batch. (data: 3.38e-04). ETA=18:22:07, max mem: 15.9 GB 
[10/26 07:46:42 visual_prompt]: 	Training 200/1106. train loss: 0.1432,	0.6308 s / batch. (data: 7.86e-04). ETA=18:22:31, max mem: 15.9 GB 
[10/26 07:47:45 visual_prompt]: 	Training 300/1106. train loss: 0.0338,	0.6281 s / batch. (data: 2.94e-04). ETA=18:16:47, max mem: 15.9 GB 
[10/26 07:48:49 visual_prompt]: 	Training 400/1106. train loss: 20.8656,	0.6169 s / batch. (data: 2.88e-04). ETA=17:56:08, max mem: 15.9 GB 
[10/26 07:49:52 visual_prompt]: 	Training 500/1106. train loss: 5.6694,	0.6417 s / batch. (data: 7.92e-04). ETA=18:38:24, max mem: 15.9 GB 
[10/26 07:50:55 visual_prompt]: 	Training 600/1106. train loss: 1.6492,	0.6281 s / batch. (data: 8.01e-04). ETA=18:13:42, max mem: 15.9 GB 
[10/26 07:51:58 visual_prompt]: 	Training 700/1106. train loss: 31.9857,	0.6320 s / batch. (data: 3.12e-04). ETA=18:19:22, max mem: 15.9 GB 
[10/26 07:53:02 visual_prompt]: 	Training 800/1106. train loss: 42.8978,	0.6330 s / batch. (data: 7.84e-04). ETA=18:19:58, max mem: 15.9 GB 
[10/26 07:54:05 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6371 s / batch. (data: 8.26e-04). ETA=18:26:06, max mem: 15.9 GB 
[10/26 07:55:08 visual_prompt]: 	Training 1000/1106. train loss: 23.0910,	0.6368 s / batch. (data: 8.14e-04). ETA=18:24:30, max mem: 15.9 GB 
[10/26 07:56:12 visual_prompt]: 	Training 1100/1106. train loss: 13.9964,	0.6188 s / batch. (data: 1.60e-04). ETA=17:52:17, max mem: 15.9 GB 
[10/26 07:56:15 visual_prompt]: Epoch 6 / 100: avg data time: 4.19e-03, avg batch time: 0.6348, average train loss: 13.2756
[10/26 07:57:06 visual_prompt]: 	Test 100/123. loss: 6.297, 0.2433 s / batch. (data: 4.08e-05)max mem: 15.92958 GB 
[10/26 07:57:16 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2320, average loss: 5.6727
[10/26 07:57:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.18	
[10/26 07:57:16 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/26 07:58:21 visual_prompt]: 	Training 100/1106. train loss: 14.0470,	0.6319 s / batch. (data: 8.06e-04). ETA=18:13:56, max mem: 15.9 GB 
[10/26 07:59:24 visual_prompt]: 	Training 200/1106. train loss: 6.0405,	0.6499 s / batch. (data: 7.51e-04). ETA=18:43:58, max mem: 15.9 GB 
[10/26 08:00:28 visual_prompt]: 	Training 300/1106. train loss: 43.2260,	0.6486 s / batch. (data: 4.51e-03). ETA=18:40:39, max mem: 15.9 GB 
[10/26 08:01:31 visual_prompt]: 	Training 400/1106. train loss: 6.5111,	0.6372 s / batch. (data: 8.21e-04). ETA=18:19:48, max mem: 15.9 GB 
[10/26 08:02:34 visual_prompt]: 	Training 500/1106. train loss: 10.0573,	0.6298 s / batch. (data: 3.09e-04). ETA=18:06:00, max mem: 15.9 GB 
[10/26 08:03:38 visual_prompt]: 	Training 600/1106. train loss: 58.1189,	0.6187 s / batch. (data: 2.85e-04). ETA=17:45:49, max mem: 15.9 GB 
[10/26 08:04:41 visual_prompt]: 	Training 700/1106. train loss: 0.0020,	0.6560 s / batch. (data: 8.01e-04). ETA=18:49:05, max mem: 15.9 GB 
[10/26 08:05:44 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6313 s / batch. (data: 8.08e-04). ETA=18:05:23, max mem: 15.9 GB 
[10/26 08:06:47 visual_prompt]: 	Training 900/1106. train loss: 8.2975,	0.6256 s / batch. (data: 3.18e-04). ETA=17:54:38, max mem: 15.9 GB 
[10/26 08:07:50 visual_prompt]: 	Training 1000/1106. train loss: 8.1334,	0.6494 s / batch. (data: 1.56e-02). ETA=18:34:28, max mem: 15.9 GB 
[10/26 08:08:53 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6180 s / batch. (data: 1.46e-04). ETA=17:39:29, max mem: 15.9 GB 
[10/26 08:08:57 visual_prompt]: Epoch 7 / 100: avg data time: 4.29e-03, avg batch time: 0.6338, average train loss: 14.4410
[10/26 08:09:47 visual_prompt]: 	Test 100/123. loss: 18.104, 0.2388 s / batch. (data: 3.77e-05)max mem: 15.92958 GB 
[10/26 08:09:57 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2322, average loss: 16.3547
[10/26 08:09:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.97	
[10/26 08:09:57 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/26 08:11:03 visual_prompt]: 	Training 100/1106. train loss: 76.0945,	0.6309 s / batch. (data: 8.27e-04). ETA=18:00:29, max mem: 15.9 GB 
[10/26 08:12:06 visual_prompt]: 	Training 200/1106. train loss: 11.1630,	0.6323 s / batch. (data: 3.04e-04). ETA=18:01:47, max mem: 15.9 GB 
[10/26 08:13:09 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6203 s / batch. (data: 3.21e-04). ETA=17:40:18, max mem: 15.9 GB 
[10/26 08:14:12 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6190 s / batch. (data: 2.34e-04). ETA=17:37:04, max mem: 15.9 GB 
[10/26 08:15:15 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6472 s / batch. (data: 8.91e-04). ETA=18:24:05, max mem: 15.9 GB 
[10/26 08:16:19 visual_prompt]: 	Training 600/1106. train loss: 0.0095,	0.6488 s / batch. (data: 8.08e-04). ETA=18:25:47, max mem: 15.9 GB 
[10/26 08:17:22 visual_prompt]: 	Training 700/1106. train loss: 1.3715,	0.6183 s / batch. (data: 3.86e-04). ETA=17:32:42, max mem: 15.9 GB 
[10/26 08:18:25 visual_prompt]: 	Training 800/1106. train loss: 15.5092,	0.6357 s / batch. (data: 1.59e-02). ETA=18:01:23, max mem: 15.9 GB 
[10/26 08:19:29 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6386 s / batch. (data: 9.70e-04). ETA=18:05:11, max mem: 15.9 GB 
[10/26 08:20:32 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6570 s / batch. (data: 8.37e-04). ETA=18:35:22, max mem: 15.9 GB 
[10/26 08:21:36 visual_prompt]: 	Training 1100/1106. train loss: 37.1057,	0.6179 s / batch. (data: 1.58e-04). ETA=17:27:52, max mem: 15.9 GB 
[10/26 08:21:39 visual_prompt]: Epoch 8 / 100: avg data time: 4.20e-03, avg batch time: 0.6345, average train loss: 16.1246
[10/26 08:22:29 visual_prompt]: 	Test 100/123. loss: 34.051, 0.2317 s / batch. (data: 3.24e-05)max mem: 15.92958 GB 
[10/26 08:22:40 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2324, average loss: 31.1657
[10/26 08:22:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.08	
[10/26 08:22:40 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/26 08:23:45 visual_prompt]: 	Training 100/1106. train loss: 5.5842,	0.6179 s / batch. (data: 2.87e-04). ETA=17:26:50, max mem: 15.9 GB 
[10/26 08:24:48 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6338 s / batch. (data: 3.19e-04). ETA=17:52:43, max mem: 15.9 GB 
[10/26 08:25:52 visual_prompt]: 	Training 300/1106. train loss: 3.1105,	0.6400 s / batch. (data: 8.69e-04). ETA=18:02:07, max mem: 15.9 GB 
[10/26 08:26:55 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6464 s / batch. (data: 7.70e-04). ETA=18:11:54, max mem: 15.9 GB 
[10/26 08:27:58 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6470 s / batch. (data: 8.39e-04). ETA=18:11:45, max mem: 15.9 GB 
[10/26 08:29:01 visual_prompt]: 	Training 600/1106. train loss: 18.3902,	0.6233 s / batch. (data: 3.01e-04). ETA=17:30:44, max mem: 15.9 GB 
[10/26 08:30:04 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6201 s / batch. (data: 3.14e-04). ETA=17:24:26, max mem: 15.9 GB 
[10/26 08:31:08 visual_prompt]: 	Training 800/1106. train loss: 29.8246,	0.6362 s / batch. (data: 7.22e-04). ETA=17:50:30, max mem: 15.9 GB 
[10/26 08:32:11 visual_prompt]: 	Training 900/1106. train loss: 4.3795,	0.6333 s / batch. (data: 7.95e-04). ETA=17:44:24, max mem: 15.9 GB 
[10/26 08:33:15 visual_prompt]: 	Training 1000/1106. train loss: 16.9385,	0.6234 s / batch. (data: 3.20e-04). ETA=17:26:47, max mem: 15.9 GB 
[10/26 08:34:18 visual_prompt]: 	Training 1100/1106. train loss: 7.0503,	0.6192 s / batch. (data: 1.50e-04). ETA=17:18:42, max mem: 15.9 GB 
[10/26 08:34:21 visual_prompt]: Epoch 9 / 100: avg data time: 4.50e-03, avg batch time: 0.6341, average train loss: 18.8005
[10/26 08:35:11 visual_prompt]: 	Test 100/123. loss: 4.613, 0.2315 s / batch. (data: 3.17e-05)max mem: 15.92958 GB 
[10/26 08:35:22 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2336, average loss: 4.8314
[10/26 08:35:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 57.11	
[10/26 08:35:22 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/26 08:36:27 visual_prompt]: 	Training 100/1106. train loss: 9.3858,	0.6169 s / batch. (data: 2.92e-04). ETA=17:13:48, max mem: 15.9 GB 
[10/26 08:37:30 visual_prompt]: 	Training 200/1106. train loss: 9.4387,	0.6329 s / batch. (data: 7.96e-04). ETA=17:39:27, max mem: 15.9 GB 
[10/26 08:38:33 visual_prompt]: 	Training 300/1106. train loss: 37.9919,	0.6416 s / batch. (data: 9.01e-04). ETA=17:52:59, max mem: 15.9 GB 
[10/26 08:39:37 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6191 s / batch. (data: 2.91e-04). ETA=17:14:26, max mem: 15.9 GB 
[10/26 08:40:40 visual_prompt]: 	Training 500/1106. train loss: 27.6280,	0.6211 s / batch. (data: 3.48e-04). ETA=17:16:41, max mem: 15.9 GB 
[10/26 08:41:43 visual_prompt]: 	Training 600/1106. train loss: 14.9935,	0.6321 s / batch. (data: 8.10e-04). ETA=17:33:54, max mem: 15.9 GB 
[10/26 08:42:46 visual_prompt]: 	Training 700/1106. train loss: 68.9370,	0.6349 s / batch. (data: 9.54e-04). ETA=17:37:35, max mem: 15.9 GB 
[10/26 08:43:50 visual_prompt]: 	Training 800/1106. train loss: 4.8872,	0.6412 s / batch. (data: 8.31e-04). ETA=17:46:56, max mem: 15.9 GB 
[10/26 08:44:53 visual_prompt]: 	Training 900/1106. train loss: 29.8595,	0.6371 s / batch. (data: 8.22e-04). ETA=17:39:06, max mem: 15.9 GB 
[10/26 08:45:56 visual_prompt]: 	Training 1000/1106. train loss: 0.1396,	0.6496 s / batch. (data: 5.90e-03). ETA=17:58:47, max mem: 15.9 GB 
[10/26 08:46:59 visual_prompt]: 	Training 1100/1106. train loss: 0.0001,	0.6189 s / batch. (data: 1.56e-04). ETA=17:06:51, max mem: 15.9 GB 
[10/26 08:47:03 visual_prompt]: Epoch 10 / 100: avg data time: 4.26e-03, avg batch time: 0.6338, average train loss: 22.8349
[10/26 08:47:53 visual_prompt]: 	Test 100/123. loss: 11.104, 0.2250 s / batch. (data: 4.63e-05)max mem: 15.92958 GB 
[10/26 08:48:03 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.2321, average loss: 12.1263
[10/26 08:48:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.38	
[10/26 08:48:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/26 08:49:09 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6320 s / batch. (data: 1.42e-03). ETA=17:27:26, max mem: 15.9 GB 
[10/26 08:50:12 visual_prompt]: 	Training 200/1106. train loss: 24.0985,	0.6352 s / batch. (data: 3.04e-04). ETA=17:31:45, max mem: 15.9 GB 
[10/26 08:51:15 visual_prompt]: 	Training 300/1106. train loss: 46.2775,	0.6273 s / batch. (data: 8.24e-04). ETA=17:17:35, max mem: 15.9 GB 
[10/26 08:52:18 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6319 s / batch. (data: 3.17e-04). ETA=17:24:08, max mem: 15.9 GB 
[10/26 08:53:21 visual_prompt]: 	Training 500/1106. train loss: 47.1233,	0.6273 s / batch. (data: 8.10e-04). ETA=17:15:23, max mem: 15.9 GB 
[10/26 08:54:24 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6190 s / batch. (data: 7.72e-04). ETA=17:00:39, max mem: 15.9 GB 
[10/26 08:55:27 visual_prompt]: 	Training 700/1106. train loss: 15.8533,	0.6191 s / batch. (data: 3.15e-04). ETA=16:59:50, max mem: 15.9 GB 
[10/26 08:56:31 visual_prompt]: 	Training 800/1106. train loss: 15.9077,	0.6336 s / batch. (data: 7.55e-04). ETA=17:22:38, max mem: 15.9 GB 
[10/26 08:57:34 visual_prompt]: 	Training 900/1106. train loss: 19.5621,	0.6340 s / batch. (data: 8.12e-04). ETA=17:22:19, max mem: 15.9 GB 
[10/26 08:58:37 visual_prompt]: 	Training 1000/1106. train loss: 0.0006,	0.6181 s / batch. (data: 4.14e-04). ETA=16:55:06, max mem: 15.9 GB 
[10/26 08:59:40 visual_prompt]: 	Training 1100/1106. train loss: 2.3865,	0.6186 s / batch. (data: 3.67e-04). ETA=16:54:51, max mem: 15.9 GB 
[10/26 08:59:44 visual_prompt]: Epoch 11 / 100: avg data time: 4.29e-03, avg batch time: 0.6337, average train loss: 25.2499
[10/26 09:00:34 visual_prompt]: 	Test 100/123. loss: 42.351, 0.2366 s / batch. (data: 4.67e-05)max mem: 15.92958 GB 
[10/26 09:00:45 visual_prompt]: Inference (val):avg data time: 7.38e-05, avg batch time: 0.2336, average loss: 37.4735
[10/26 09:00:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.87	
[10/26 09:00:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/26 09:01:51 visual_prompt]: 	Training 100/1106. train loss: 14.2227,	0.6183 s / batch. (data: 2.79e-04). ETA=16:53:22, max mem: 15.9 GB 
[10/26 09:02:55 visual_prompt]: 	Training 200/1106. train loss: 22.9261,	0.6189 s / batch. (data: 2.75e-04). ETA=16:53:12, max mem: 15.9 GB 
[10/26 09:03:58 visual_prompt]: 	Training 300/1106. train loss: 15.4363,	0.6190 s / batch. (data: 2.94e-04). ETA=16:52:24, max mem: 15.9 GB 
[10/26 09:05:01 visual_prompt]: 	Training 400/1106. train loss: 64.0146,	0.6464 s / batch. (data: 8.31e-04). ETA=17:36:08, max mem: 15.9 GB 
[10/26 09:06:04 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6367 s / batch. (data: 8.30e-04). ETA=17:19:14, max mem: 15.9 GB 
[10/26 09:07:07 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6507 s / batch. (data: 3.17e-02). ETA=17:40:59, max mem: 15.9 GB 
[10/26 09:08:11 visual_prompt]: 	Training 700/1106. train loss: 7.9172,	0.6380 s / batch. (data: 7.08e-04). ETA=17:19:14, max mem: 15.9 GB 
[10/26 09:09:14 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6320 s / batch. (data: 8.16e-04). ETA=17:08:24, max mem: 15.9 GB 
[10/26 09:10:17 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6187 s / batch. (data: 2.83e-04). ETA=16:45:40, max mem: 15.9 GB 
[10/26 09:11:20 visual_prompt]: 	Training 1000/1106. train loss: 16.8266,	0.6425 s / batch. (data: 7.87e-04). ETA=17:23:20, max mem: 15.9 GB 
[10/26 09:12:23 visual_prompt]: 	Training 1100/1106. train loss: 14.6532,	0.6194 s / batch. (data: 1.47e-04). ETA=16:44:53, max mem: 15.9 GB 
[10/26 09:12:27 visual_prompt]: Epoch 12 / 100: avg data time: 5.01e-03, avg batch time: 0.6346, average train loss: 23.8922
[10/26 09:13:17 visual_prompt]: 	Test 100/123. loss: 15.766, 0.2287 s / batch. (data: 4.32e-05)max mem: 15.92958 GB 
[10/26 09:13:27 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2318, average loss: 14.2569
[10/26 09:13:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.96	
[10/26 09:13:27 visual_prompt]: Best epoch 12: best metric: -14.257
[10/26 09:13:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/26 09:14:33 visual_prompt]: 	Training 100/1106. train loss: 25.8221,	0.6178 s / batch. (data: 3.19e-04). ETA=16:41:11, max mem: 15.9 GB 
[10/26 09:15:36 visual_prompt]: 	Training 200/1106. train loss: 71.3405,	0.6380 s / batch. (data: 7.53e-04). ETA=17:12:43, max mem: 15.9 GB 
[10/26 09:16:39 visual_prompt]: 	Training 300/1106. train loss: 36.6485,	0.6181 s / batch. (data: 3.26e-04). ETA=16:39:33, max mem: 15.9 GB 
[10/26 09:17:42 visual_prompt]: 	Training 400/1106. train loss: 52.9177,	0.6187 s / batch. (data: 3.20e-04). ETA=16:39:30, max mem: 15.9 GB 
[10/26 09:18:45 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6532 s / batch. (data: 7.71e-04). ETA=17:34:03, max mem: 15.9 GB 
[10/26 09:19:49 visual_prompt]: 	Training 600/1106. train loss: 17.1948,	0.6291 s / batch. (data: 7.79e-04). ETA=16:54:15, max mem: 15.9 GB 
[10/26 09:20:52 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6524 s / batch. (data: 8.23e-04). ETA=17:30:44, max mem: 15.9 GB 
[10/26 09:21:55 visual_prompt]: 	Training 800/1106. train loss: 2.9832,	0.6372 s / batch. (data: 3.30e-04). ETA=17:05:03, max mem: 15.9 GB 
[10/26 09:22:58 visual_prompt]: 	Training 900/1106. train loss: 4.9549,	0.6437 s / batch. (data: 8.12e-04). ETA=17:14:29, max mem: 15.9 GB 
[10/26 09:24:01 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6141 s / batch. (data: 3.00e-04). ETA=16:25:53, max mem: 15.9 GB 
[10/26 09:25:05 visual_prompt]: 	Training 1100/1106. train loss: 33.8615,	0.6143 s / batch. (data: 1.83e-04). ETA=16:25:08, max mem: 15.9 GB 
[10/26 09:25:09 visual_prompt]: Epoch 13 / 100: avg data time: 4.48e-03, avg batch time: 0.6339, average train loss: 24.5644
[10/26 09:25:58 visual_prompt]: 	Test 100/123. loss: 16.690, 0.2303 s / batch. (data: 2.46e-05)max mem: 15.92958 GB 
[10/26 09:26:09 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.2321, average loss: 15.0015
[10/26 09:26:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.86	
[10/26 09:26:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/26 09:27:14 visual_prompt]: 	Training 100/1106. train loss: 31.1614,	0.6331 s / batch. (data: 3.13e-04). ETA=16:54:11, max mem: 15.9 GB 
[10/26 09:28:17 visual_prompt]: 	Training 200/1106. train loss: 90.7292,	0.6289 s / batch. (data: 8.35e-04). ETA=16:46:25, max mem: 15.9 GB 
[10/26 09:29:20 visual_prompt]: 	Training 300/1106. train loss: 77.6375,	0.6280 s / batch. (data: 3.29e-04). ETA=16:43:58, max mem: 15.9 GB 
[10/26 09:30:23 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6216 s / batch. (data: 2.74e-03). ETA=16:32:38, max mem: 15.9 GB 
[10/26 09:31:26 visual_prompt]: 	Training 500/1106. train loss: 15.9432,	0.6267 s / batch. (data: 3.31e-04). ETA=16:39:44, max mem: 15.9 GB 
[10/26 09:32:30 visual_prompt]: 	Training 600/1106. train loss: 0.0001,	0.6315 s / batch. (data: 8.15e-04). ETA=16:46:28, max mem: 15.9 GB 
[10/26 09:33:33 visual_prompt]: 	Training 700/1106. train loss: 21.2760,	0.6459 s / batch. (data: 1.20e-02). ETA=17:08:18, max mem: 15.9 GB 
[10/26 09:34:36 visual_prompt]: 	Training 800/1106. train loss: 16.6131,	0.6361 s / batch. (data: 5.89e-03). ETA=16:51:41, max mem: 15.9 GB 
[10/26 09:35:39 visual_prompt]: 	Training 900/1106. train loss: 26.8644,	0.6186 s / batch. (data: 2.82e-04). ETA=16:22:43, max mem: 15.9 GB 
[10/26 09:36:43 visual_prompt]: 	Training 1000/1106. train loss: 13.0943,	0.6454 s / batch. (data: 8.22e-04). ETA=17:04:18, max mem: 15.9 GB 
[10/26 09:37:46 visual_prompt]: 	Training 1100/1106. train loss: 18.9799,	0.6187 s / batch. (data: 1.54e-04). ETA=16:20:53, max mem: 15.9 GB 
[10/26 09:37:50 visual_prompt]: Epoch 14 / 100: avg data time: 4.19e-03, avg batch time: 0.6334, average train loss: 22.4876
[10/26 09:38:39 visual_prompt]: 	Test 100/123. loss: 7.172, 0.2255 s / batch. (data: 3.55e-05)max mem: 15.92958 GB 
[10/26 09:38:50 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2326, average loss: 6.5076
[10/26 09:38:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.78	
[10/26 09:38:50 visual_prompt]: Best epoch 14: best metric: -6.508
[10/26 09:38:50 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/26 09:39:55 visual_prompt]: 	Training 100/1106. train loss: 61.6858,	0.6266 s / batch. (data: 7.04e-04). ETA=16:32:17, max mem: 15.9 GB 
[10/26 09:40:58 visual_prompt]: 	Training 200/1106. train loss: 44.3951,	0.6264 s / batch. (data: 7.80e-04). ETA=16:30:56, max mem: 15.9 GB 
[10/26 09:42:01 visual_prompt]: 	Training 300/1106. train loss: 11.2201,	0.6233 s / batch. (data: 5.41e-03). ETA=16:24:59, max mem: 15.9 GB 
[10/26 09:43:04 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6184 s / batch. (data: 2.96e-04). ETA=16:16:14, max mem: 15.9 GB 
[10/26 09:44:07 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6207 s / batch. (data: 3.16e-04). ETA=16:18:50, max mem: 15.9 GB 
[10/26 09:45:10 visual_prompt]: 	Training 600/1106. train loss: 16.0911,	0.6306 s / batch. (data: 7.57e-04). ETA=16:33:19, max mem: 15.9 GB 
[10/26 09:46:14 visual_prompt]: 	Training 700/1106. train loss: 25.7904,	0.6189 s / batch. (data: 3.03e-04). ETA=16:13:51, max mem: 15.9 GB 
[10/26 09:47:17 visual_prompt]: 	Training 800/1106. train loss: 58.5798,	0.6338 s / batch. (data: 1.05e-02). ETA=16:36:20, max mem: 15.9 GB 
[10/26 09:48:20 visual_prompt]: 	Training 900/1106. train loss: 41.5765,	0.6288 s / batch. (data: 3.04e-04). ETA=16:27:19, max mem: 15.9 GB 
[10/26 09:49:23 visual_prompt]: 	Training 1000/1106. train loss: 37.2364,	0.6253 s / batch. (data: 7.16e-04). ETA=16:20:46, max mem: 15.9 GB 
[10/26 09:50:26 visual_prompt]: 	Training 1100/1106. train loss: 62.9422,	0.6123 s / batch. (data: 1.60e-04). ETA=15:59:27, max mem: 15.9 GB 
[10/26 09:50:30 visual_prompt]: Epoch 15 / 100: avg data time: 4.23e-03, avg batch time: 0.6329, average train loss: 26.4998
[10/26 09:51:20 visual_prompt]: 	Test 100/123. loss: 7.353, 0.2253 s / batch. (data: 5.08e-05)max mem: 15.92958 GB 
[10/26 09:51:31 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.2311, average loss: 5.6921
[10/26 09:51:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[10/26 09:51:31 visual_prompt]: Best epoch 15: best metric: -5.692
[10/26 09:51:31 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/26 09:52:35 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6279 s / batch. (data: 8.61e-04). ETA=16:22:47, max mem: 15.9 GB 
[10/26 09:53:38 visual_prompt]: 	Training 200/1106. train loss: 15.3368,	0.6436 s / batch. (data: 7.37e-04). ETA=16:46:15, max mem: 15.9 GB 
[10/26 09:54:42 visual_prompt]: 	Training 300/1106. train loss: 87.0681,	0.6168 s / batch. (data: 3.04e-04). ETA=16:03:20, max mem: 15.9 GB 
[10/26 09:55:45 visual_prompt]: 	Training 400/1106. train loss: 9.9831,	0.6324 s / batch. (data: 7.78e-04). ETA=16:26:39, max mem: 15.9 GB 
[10/26 09:56:48 visual_prompt]: 	Training 500/1106. train loss: 44.3811,	0.6368 s / batch. (data: 5.49e-03). ETA=16:32:26, max mem: 15.9 GB 
[10/26 09:57:51 visual_prompt]: 	Training 600/1106. train loss: 11.6146,	0.6188 s / batch. (data: 2.51e-04). ETA=16:03:19, max mem: 15.9 GB 
[10/26 09:58:54 visual_prompt]: 	Training 700/1106. train loss: 17.4192,	0.6357 s / batch. (data: 7.95e-04). ETA=16:28:32, max mem: 15.9 GB 
[10/26 09:59:58 visual_prompt]: 	Training 800/1106. train loss: 13.4359,	0.6191 s / batch. (data: 4.38e-04). ETA=16:01:43, max mem: 15.9 GB 
[10/26 10:01:01 visual_prompt]: 	Training 900/1106. train loss: 18.4484,	0.6306 s / batch. (data: 7.80e-04). ETA=16:18:36, max mem: 15.9 GB 
[10/26 10:02:04 visual_prompt]: 	Training 1000/1106. train loss: 124.1317,	0.6175 s / batch. (data: 3.62e-04). ETA=15:57:17, max mem: 15.9 GB 
[10/26 10:03:07 visual_prompt]: 	Training 1100/1106. train loss: 34.5811,	0.6184 s / batch. (data: 1.41e-04). ETA=15:57:35, max mem: 15.9 GB 
[10/26 10:03:11 visual_prompt]: Epoch 16 / 100: avg data time: 4.13e-03, avg batch time: 0.6332, average train loss: 22.1471
[10/26 10:04:01 visual_prompt]: 	Test 100/123. loss: 28.191, 0.2396 s / batch. (data: 4.29e-05)max mem: 15.92958 GB 
[10/26 10:04:11 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2331, average loss: 31.3873
[10/26 10:04:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.55	
[10/26 10:04:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/26 10:05:17 visual_prompt]: 	Training 100/1106. train loss: 1.4292,	0.6171 s / batch. (data: 3.24e-04). ETA=15:54:29, max mem: 15.9 GB 
[10/26 10:06:20 visual_prompt]: 	Training 200/1106. train loss: 28.0556,	0.6405 s / batch. (data: 8.44e-04). ETA=16:29:33, max mem: 15.9 GB 
[10/26 10:07:23 visual_prompt]: 	Training 300/1106. train loss: 0.0388,	0.6190 s / batch. (data: 2.94e-04). ETA=15:55:17, max mem: 15.9 GB 
[10/26 10:08:26 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6341 s / batch. (data: 2.97e-04). ETA=16:17:33, max mem: 15.9 GB 
[10/26 10:09:29 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6194 s / batch. (data: 2.78e-04). ETA=15:53:58, max mem: 15.9 GB 
[10/26 10:10:33 visual_prompt]: 	Training 600/1106. train loss: 0.7488,	0.6684 s / batch. (data: 7.98e-04). ETA=17:08:11, max mem: 15.9 GB 
[10/26 10:11:36 visual_prompt]: 	Training 700/1106. train loss: 0.0001,	0.6458 s / batch. (data: 2.75e-04). ETA=16:32:26, max mem: 15.9 GB 
[10/26 10:12:39 visual_prompt]: 	Training 800/1106. train loss: 14.8107,	0.6196 s / batch. (data: 2.88e-04). ETA=15:51:08, max mem: 15.9 GB 
[10/26 10:13:42 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6349 s / batch. (data: 2.99e-04). ETA=16:13:37, max mem: 15.9 GB 
[10/26 10:14:45 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6187 s / batch. (data: 3.24e-04). ETA=15:47:45, max mem: 15.9 GB 
[10/26 10:15:49 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6149 s / batch. (data: 1.57e-04). ETA=15:40:49, max mem: 15.9 GB 
[10/26 10:15:52 visual_prompt]: Epoch 17 / 100: avg data time: 4.66e-03, avg batch time: 0.6338, average train loss: 22.9359
[10/26 10:16:42 visual_prompt]: 	Test 100/123. loss: 13.702, 0.2478 s / batch. (data: 2.88e-05)max mem: 15.92958 GB 
[10/26 10:16:53 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2333, average loss: 15.0274
[10/26 10:16:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.24	
[10/26 10:16:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/26 10:17:58 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6426 s / batch. (data: 8.16e-04). ETA=16:22:09, max mem: 15.9 GB 
[10/26 10:19:01 visual_prompt]: 	Training 200/1106. train loss: 34.7333,	0.6419 s / batch. (data: 7.77e-04). ETA=16:19:57, max mem: 15.9 GB 
[10/26 10:20:04 visual_prompt]: 	Training 300/1106. train loss: 69.5090,	0.6440 s / batch. (data: 3.18e-04). ETA=16:22:07, max mem: 15.9 GB 
[10/26 10:21:07 visual_prompt]: 	Training 400/1106. train loss: 49.1877,	0.6280 s / batch. (data: 2.82e-04). ETA=15:56:39, max mem: 15.9 GB 
[10/26 10:22:11 visual_prompt]: 	Training 500/1106. train loss: 52.6302,	0.6195 s / batch. (data: 3.00e-04). ETA=15:42:40, max mem: 15.9 GB 
[10/26 10:23:14 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6184 s / batch. (data: 3.19e-04). ETA=15:39:54, max mem: 15.9 GB 
[10/26 10:24:17 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6215 s / batch. (data: 7.99e-04). ETA=15:43:40, max mem: 15.9 GB 
[10/26 10:25:20 visual_prompt]: 	Training 800/1106. train loss: 2.0266,	0.6171 s / batch. (data: 3.10e-04). ETA=15:35:54, max mem: 15.9 GB 
[10/26 10:26:24 visual_prompt]: 	Training 900/1106. train loss: 36.5181,	0.6475 s / batch. (data: 7.52e-04). ETA=16:21:00, max mem: 15.9 GB 
[10/26 10:27:27 visual_prompt]: 	Training 1000/1106. train loss: 6.0849,	0.6313 s / batch. (data: 2.98e-04). ETA=15:55:21, max mem: 15.9 GB 
[10/26 10:28:30 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6165 s / batch. (data: 1.51e-04). ETA=15:31:56, max mem: 15.9 GB 
[10/26 10:28:34 visual_prompt]: Epoch 18 / 100: avg data time: 4.58e-03, avg batch time: 0.6336, average train loss: 24.7325
[10/26 10:29:24 visual_prompt]: 	Test 100/123. loss: 18.610, 0.2441 s / batch. (data: 2.96e-05)max mem: 15.92958 GB 
[10/26 10:29:34 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2324, average loss: 16.9870
[10/26 10:29:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.41	
[10/26 10:29:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/26 10:30:40 visual_prompt]: 	Training 100/1106. train loss: 0.0006,	0.6304 s / batch. (data: 2.64e-04). ETA=15:51:49, max mem: 15.9 GB 
[10/26 10:31:43 visual_prompt]: 	Training 200/1106. train loss: 4.2506,	0.6420 s / batch. (data: 1.45e-02). ETA=16:08:14, max mem: 15.9 GB 
[10/26 10:32:46 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6722 s / batch. (data: 5.86e-03). ETA=16:52:43, max mem: 15.9 GB 
[10/26 10:33:49 visual_prompt]: 	Training 400/1106. train loss: 41.5267,	0.6202 s / batch. (data: 7.72e-04). ETA=15:33:22, max mem: 15.9 GB 
[10/26 10:34:52 visual_prompt]: 	Training 500/1106. train loss: 15.5257,	0.6184 s / batch. (data: 3.02e-04). ETA=15:29:33, max mem: 15.9 GB 
[10/26 10:35:56 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6316 s / batch. (data: 7.92e-04). ETA=15:48:21, max mem: 15.9 GB 
[10/26 10:36:59 visual_prompt]: 	Training 700/1106. train loss: 2.4806,	0.6311 s / batch. (data: 3.12e-04). ETA=15:46:35, max mem: 15.9 GB 
[10/26 10:38:02 visual_prompt]: 	Training 800/1106. train loss: 0.3312,	0.6422 s / batch. (data: 8.42e-04). ETA=16:02:09, max mem: 15.9 GB 
[10/26 10:39:05 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6327 s / batch. (data: 8.35e-04). ETA=15:46:53, max mem: 15.9 GB 
[10/26 10:40:08 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6407 s / batch. (data: 2.30e-02). ETA=15:57:42, max mem: 15.9 GB 
[10/26 10:41:12 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6120 s / batch. (data: 1.62e-04). ETA=15:13:54, max mem: 15.9 GB 
[10/26 10:41:16 visual_prompt]: Epoch 19 / 100: avg data time: 4.12e-03, avg batch time: 0.6340, average train loss: 23.8041
[10/26 10:42:06 visual_prompt]: 	Test 100/123. loss: 72.448, 0.2520 s / batch. (data: 2.19e-05)max mem: 15.92958 GB 
[10/26 10:42:17 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2315, average loss: 65.4076
[10/26 10:42:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.43	
[10/26 10:42:17 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[10/26 10:43:22 visual_prompt]: 	Training 100/1106. train loss: 27.1226,	0.6337 s / batch. (data: 2.68e-04). ETA=15:45:05, max mem: 15.9 GB 
[10/26 10:44:25 visual_prompt]: 	Training 200/1106. train loss: 124.0142,	0.6314 s / batch. (data: 8.06e-04). ETA=15:40:39, max mem: 15.9 GB 
[10/26 10:45:28 visual_prompt]: 	Training 300/1106. train loss: 13.0434,	0.6600 s / batch. (data: 4.08e-02). ETA=16:22:05, max mem: 15.9 GB 
[10/26 10:46:32 visual_prompt]: 	Training 400/1106. train loss: 1.9355,	0.6334 s / batch. (data: 7.36e-04). ETA=15:41:33, max mem: 15.9 GB 
[10/26 10:47:35 visual_prompt]: 	Training 500/1106. train loss: 40.1564,	0.6287 s / batch. (data: 3.46e-04). ETA=15:33:28, max mem: 15.9 GB 
[10/26 10:48:38 visual_prompt]: 	Training 600/1106. train loss: 4.9608,	0.6605 s / batch. (data: 1.48e-02). ETA=16:19:38, max mem: 15.9 GB 
[10/26 10:49:41 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6735 s / batch. (data: 4.64e-02). ETA=16:37:40, max mem: 15.9 GB 
[10/26 10:50:45 visual_prompt]: 	Training 800/1106. train loss: 8.0862,	0.6263 s / batch. (data: 2.71e-04). ETA=15:26:49, max mem: 15.9 GB 
[10/26 10:51:48 visual_prompt]: 	Training 900/1106. train loss: 82.6544,	0.6312 s / batch. (data: 3.39e-04). ETA=15:32:57, max mem: 15.9 GB 
[10/26 10:52:51 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6517 s / batch. (data: 3.44e-02). ETA=16:02:14, max mem: 15.9 GB 
[10/26 10:53:54 visual_prompt]: 	Training 1100/1106. train loss: 26.9470,	0.6181 s / batch. (data: 1.54e-04). ETA=15:11:35, max mem: 15.9 GB 
[10/26 10:53:58 visual_prompt]: Epoch 20 / 100: avg data time: 5.10e-03, avg batch time: 0.6343, average train loss: 23.1170
[10/26 10:54:48 visual_prompt]: 	Test 100/123. loss: 20.464, 0.2259 s / batch. (data: 3.72e-05)max mem: 15.92958 GB 
[10/26 10:54:59 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.2315, average loss: 23.6988
[10/26 10:54:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.48	
[10/26 10:54:59 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[10/26 10:56:04 visual_prompt]: 	Training 100/1106. train loss: 74.8250,	0.6491 s / batch. (data: 8.29e-04). ETA=15:56:05, max mem: 15.9 GB 
[10/26 10:57:08 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6505 s / batch. (data: 1.10e-02). ETA=15:57:05, max mem: 15.9 GB 
[10/26 10:58:11 visual_prompt]: 	Training 300/1106. train loss: 88.0989,	0.6537 s / batch. (data: 5.29e-03). ETA=16:00:45, max mem: 15.9 GB 
[10/26 10:59:14 visual_prompt]: 	Training 400/1106. train loss: 8.2914,	0.6211 s / batch. (data: 3.53e-04). ETA=15:11:43, max mem: 15.9 GB 
[10/26 11:00:17 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6206 s / batch. (data: 3.21e-04). ETA=15:10:01, max mem: 15.9 GB 
[10/26 11:01:20 visual_prompt]: 	Training 600/1106. train loss: 41.9779,	0.6179 s / batch. (data: 3.26e-04). ETA=15:05:04, max mem: 15.9 GB 
[10/26 11:02:23 visual_prompt]: 	Training 700/1106. train loss: 9.5028,	0.6197 s / batch. (data: 3.10e-04). ETA=15:06:41, max mem: 15.9 GB 
[10/26 11:03:26 visual_prompt]: 	Training 800/1106. train loss: 69.0435,	0.6197 s / batch. (data: 5.44e-03). ETA=15:05:32, max mem: 15.9 GB 
[10/26 11:04:29 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6391 s / batch. (data: 7.39e-04). ETA=15:32:55, max mem: 15.9 GB 
[10/26 11:05:32 visual_prompt]: 	Training 1000/1106. train loss: 3.6043,	0.6308 s / batch. (data: 7.85e-04). ETA=15:19:43, max mem: 15.9 GB 
[10/26 11:06:36 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6176 s / batch. (data: 1.53e-04). ETA=14:59:24, max mem: 15.9 GB 
[10/26 11:06:39 visual_prompt]: Epoch 21 / 100: avg data time: 4.52e-03, avg batch time: 0.6335, average train loss: 24.0992
[10/26 11:07:29 visual_prompt]: 	Test 100/123. loss: 14.427, 0.2437 s / batch. (data: 3.29e-05)max mem: 15.92958 GB 
[10/26 11:07:40 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.2324, average loss: 13.0136
[10/26 11:07:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.24	
[10/26 11:07:40 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[10/26 11:08:45 visual_prompt]: 	Training 100/1106. train loss: 64.5476,	0.6392 s / batch. (data: 8.34e-04). ETA=15:29:47, max mem: 15.9 GB 
[10/26 11:09:48 visual_prompt]: 	Training 200/1106. train loss: 46.9345,	0.6245 s / batch. (data: 5.44e-03). ETA=15:07:19, max mem: 15.9 GB 
[10/26 11:10:51 visual_prompt]: 	Training 300/1106. train loss: 11.7546,	0.6176 s / batch. (data: 2.83e-04). ETA=14:56:20, max mem: 15.9 GB 
[10/26 11:11:54 visual_prompt]: 	Training 400/1106. train loss: 13.1395,	0.6195 s / batch. (data: 7.57e-04). ETA=14:57:59, max mem: 15.9 GB 
[10/26 11:12:58 visual_prompt]: 	Training 500/1106. train loss: 18.6276,	0.6324 s / batch. (data: 7.96e-04). ETA=15:15:39, max mem: 15.9 GB 
[10/26 11:14:01 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6469 s / batch. (data: 8.01e-04). ETA=15:35:30, max mem: 15.9 GB 
[10/26 11:15:04 visual_prompt]: 	Training 700/1106. train loss: 0.7016,	0.6199 s / batch. (data: 3.18e-04). ETA=14:55:30, max mem: 15.9 GB 
[10/26 11:16:07 visual_prompt]: 	Training 800/1106. train loss: 17.9643,	0.6204 s / batch. (data: 7.47e-04). ETA=14:55:12, max mem: 15.9 GB 
[10/26 11:17:10 visual_prompt]: 	Training 900/1106. train loss: 22.7545,	0.6485 s / batch. (data: 3.61e-04). ETA=15:34:39, max mem: 15.9 GB 
[10/26 11:18:14 visual_prompt]: 	Training 1000/1106. train loss: 13.3095,	0.6370 s / batch. (data: 3.12e-04). ETA=15:16:56, max mem: 15.9 GB 
[10/26 11:19:17 visual_prompt]: 	Training 1100/1106. train loss: 104.7452,	0.6189 s / batch. (data: 1.37e-04). ETA=14:49:50, max mem: 15.9 GB 
[10/26 11:19:21 visual_prompt]: Epoch 22 / 100: avg data time: 4.49e-03, avg batch time: 0.6337, average train loss: 21.1787
[10/26 11:20:11 visual_prompt]: 	Test 100/123. loss: 24.200, 0.2299 s / batch. (data: 2.96e-05)max mem: 15.92958 GB 
[10/26 11:20:21 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2319, average loss: 21.9451
[10/26 11:20:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.42	
[10/26 11:20:21 visual_prompt]: Stopping early.
[10/26 11:20:22 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 11:20:22 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 11:20:22 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 11:20:22 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 11:20:22 visual_prompt]: Training with config:
[10/26 11:20:22 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr5.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 11:20:22 visual_prompt]: Loading training data...
[10/26 11:20:22 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 11:20:22 visual_prompt]: Loading validation data...
[10/26 11:20:22 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 11:20:22 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/26 11:20:24 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/26 11:20:24 visual_prompt]: tuned percent:0.522
[10/26 11:20:24 visual_prompt]: Device used for model: 0
[10/26 11:20:24 visual_prompt]: Setting up Evaluator...
[10/26 11:20:24 visual_prompt]: Setting up Trainer...
[10/26 11:20:24 visual_prompt]: 	Setting up the optimizer...
[10/26 11:20:24 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 11:21:30 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6186 s / batch. (data: 2.75e-04). ETA=18:59:12, max mem: 15.9 GB 
[10/26 11:22:34 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6395 s / batch. (data: 7.34e-04). ETA=19:36:37, max mem: 15.9 GB 
[10/26 11:23:37 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6380 s / batch. (data: 8.46e-04). ETA=19:32:51, max mem: 15.9 GB 
[10/26 11:24:40 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6545 s / batch. (data: 8.22e-04). ETA=20:02:08, max mem: 15.9 GB 
[10/26 11:25:44 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6384 s / batch. (data: 3.08e-04). ETA=19:31:29, max mem: 15.9 GB 
[10/26 11:26:47 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6305 s / batch. (data: 2.97e-04). ETA=19:15:53, max mem: 15.9 GB 
[10/26 11:27:50 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6276 s / batch. (data: 3.29e-04). ETA=19:09:35, max mem: 15.9 GB 
[10/26 11:28:53 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6456 s / batch. (data: 1.05e-02). ETA=19:41:27, max mem: 15.9 GB 
[10/26 11:29:57 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6634 s / batch. (data: 7.85e-04). ETA=20:12:52, max mem: 15.9 GB 
[10/26 11:31:00 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6191 s / batch. (data: 3.15e-04). ETA=18:50:54, max mem: 15.9 GB 
[10/26 11:32:03 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6186 s / batch. (data: 1.64e-04). ETA=18:48:58, max mem: 15.9 GB 
[10/26 11:32:07 visual_prompt]: Epoch 1 / 100: avg data time: 4.83e-03, avg batch time: 0.6354, average train loss: 1.4028
[10/26 11:32:57 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2558 s / batch. (data: 5.32e-05)max mem: 15.93560 GB 
[10/26 11:33:08 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2337, average loss: 1.3505
[10/26 11:33:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/26 11:33:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/26 11:34:13 visual_prompt]: 	Training 100/1106. train loss: 3.9168,	0.6174 s / batch. (data: 3.19e-04). ETA=18:45:44, max mem: 15.9 GB 
[10/26 11:35:16 visual_prompt]: 	Training 200/1106. train loss: 1.6623,	0.6184 s / batch. (data: 4.39e-04). ETA=18:46:25, max mem: 15.9 GB 
[10/26 11:36:19 visual_prompt]: 	Training 300/1106. train loss: 1.0638,	0.6400 s / batch. (data: 3.17e-04). ETA=19:24:47, max mem: 15.9 GB 
[10/26 11:37:23 visual_prompt]: 	Training 400/1106. train loss: 0.0001,	0.6219 s / batch. (data: 3.38e-04). ETA=18:50:43, max mem: 15.9 GB 
[10/26 11:38:26 visual_prompt]: 	Training 500/1106. train loss: 1.6616,	0.6400 s / batch. (data: 8.03e-03). ETA=19:22:35, max mem: 15.9 GB 
[10/26 11:39:29 visual_prompt]: 	Training 600/1106. train loss: 0.6981,	0.6577 s / batch. (data: 1.37e-02). ETA=19:53:39, max mem: 15.9 GB 
[10/26 11:40:32 visual_prompt]: 	Training 700/1106. train loss: 0.0003,	0.6466 s / batch. (data: 7.70e-04). ETA=19:32:21, max mem: 15.9 GB 
[10/26 11:41:36 visual_prompt]: 	Training 800/1106. train loss: 0.6950,	0.6473 s / batch. (data: 3.46e-04). ETA=19:32:41, max mem: 15.9 GB 
[10/26 11:42:39 visual_prompt]: 	Training 900/1106. train loss: 0.7161,	0.6307 s / batch. (data: 3.19e-04). ETA=19:01:26, max mem: 15.9 GB 
[10/26 11:43:42 visual_prompt]: 	Training 1000/1106. train loss: 0.0264,	0.6492 s / batch. (data: 7.04e-04). ETA=19:33:49, max mem: 15.9 GB 
[10/26 11:44:45 visual_prompt]: 	Training 1100/1106. train loss: 0.0007,	0.6180 s / batch. (data: 1.39e-04). ETA=18:36:31, max mem: 15.9 GB 
[10/26 11:44:49 visual_prompt]: Epoch 2 / 100: avg data time: 3.85e-03, avg batch time: 0.6340, average train loss: 2.1638
[10/26 11:45:39 visual_prompt]: 	Test 100/123. loss: 5.420, 0.2250 s / batch. (data: 4.22e-05)max mem: 15.93560 GB 
[10/26 11:45:50 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.2322, average loss: 4.9240
[10/26 11:45:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.38	
[10/26 11:45:50 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/26 11:46:56 visual_prompt]: 	Training 100/1106. train loss: 0.1617,	0.6179 s / batch. (data: 2.64e-04). ETA=18:35:09, max mem: 15.9 GB 
[10/26 11:47:59 visual_prompt]: 	Training 200/1106. train loss: 3.5340,	0.6535 s / batch. (data: 1.34e-02). ETA=19:38:17, max mem: 15.9 GB 
[10/26 11:49:02 visual_prompt]: 	Training 300/1106. train loss: 2.4773,	0.6257 s / batch. (data: 5.44e-03). ETA=18:47:07, max mem: 15.9 GB 
[10/26 11:50:05 visual_prompt]: 	Training 400/1106. train loss: 2.5922,	0.6190 s / batch. (data: 3.52e-04). ETA=18:33:59, max mem: 15.9 GB 
[10/26 11:51:08 visual_prompt]: 	Training 500/1106. train loss: 6.0093,	0.6299 s / batch. (data: 8.03e-04). ETA=18:52:42, max mem: 15.9 GB 
[10/26 11:52:12 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6574 s / batch. (data: 8.81e-04). ETA=19:40:58, max mem: 15.9 GB 
[10/26 11:53:15 visual_prompt]: 	Training 700/1106. train loss: 0.1388,	0.6184 s / batch. (data: 3.49e-04). ETA=18:29:56, max mem: 15.9 GB 
[10/26 11:54:18 visual_prompt]: 	Training 800/1106. train loss: 0.0006,	0.6319 s / batch. (data: 1.15e-03). ETA=18:53:01, max mem: 15.9 GB 
[10/26 11:55:21 visual_prompt]: 	Training 900/1106. train loss: 1.6514,	0.6400 s / batch. (data: 7.99e-03). ETA=19:06:32, max mem: 15.9 GB 
[10/26 11:56:24 visual_prompt]: 	Training 1000/1106. train loss: 6.6689,	0.6182 s / batch. (data: 3.02e-04). ETA=18:26:25, max mem: 15.9 GB 
[10/26 11:57:27 visual_prompt]: 	Training 1100/1106. train loss: 5.1926,	0.6171 s / batch. (data: 1.55e-04). ETA=18:23:27, max mem: 15.9 GB 
[10/26 11:57:31 visual_prompt]: Epoch 3 / 100: avg data time: 5.07e-03, avg batch time: 0.6343, average train loss: 4.2002
[10/26 11:58:21 visual_prompt]: 	Test 100/123. loss: 2.623, 0.2409 s / batch. (data: 4.65e-05)max mem: 15.93560 GB 
[10/26 11:58:32 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.2320, average loss: 2.9983
[10/26 11:58:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.70	
[10/26 11:58:32 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/26 11:59:38 visual_prompt]: 	Training 100/1106. train loss: 0.0017,	0.6501 s / batch. (data: 2.30e-02). ETA=19:21:19, max mem: 15.9 GB 
[10/26 12:00:41 visual_prompt]: 	Training 200/1106. train loss: 47.8055,	0.6378 s / batch. (data: 5.87e-03). ETA=18:58:18, max mem: 15.9 GB 
[10/26 12:01:44 visual_prompt]: 	Training 300/1106. train loss: 0.8420,	0.6197 s / batch. (data: 2.94e-04). ETA=18:24:54, max mem: 15.9 GB 
[10/26 12:02:47 visual_prompt]: 	Training 400/1106. train loss: 1.8001,	0.6320 s / batch. (data: 2.95e-04). ETA=18:45:49, max mem: 15.9 GB 
[10/26 12:03:52 visual_prompt]: 	Training 500/1106. train loss: 2.4362,	0.6185 s / batch. (data: 3.16e-04). ETA=18:20:40, max mem: 15.9 GB 
[10/26 12:04:55 visual_prompt]: 	Training 600/1106. train loss: 7.0920,	0.6320 s / batch. (data: 8.12e-04). ETA=18:43:45, max mem: 15.9 GB 
[10/26 12:05:58 visual_prompt]: 	Training 700/1106. train loss: 0.0229,	0.6475 s / batch. (data: 1.62e-02). ETA=19:10:10, max mem: 15.9 GB 
[10/26 12:07:01 visual_prompt]: 	Training 800/1106. train loss: 10.5739,	0.6254 s / batch. (data: 3.26e-04). ETA=18:29:57, max mem: 15.9 GB 
[10/26 12:08:05 visual_prompt]: 	Training 900/1106. train loss: 6.7186,	0.6185 s / batch. (data: 7.51e-04). ETA=18:16:35, max mem: 15.9 GB 
[10/26 12:09:08 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6635 s / batch. (data: 2.89e-02). ETA=19:35:19, max mem: 15.9 GB 
[10/26 12:10:12 visual_prompt]: 	Training 1100/1106. train loss: 12.0155,	0.6177 s / batch. (data: 1.49e-04). ETA=18:13:12, max mem: 15.9 GB 
[10/26 12:10:15 visual_prompt]: Epoch 4 / 100: avg data time: 5.83e-03, avg batch time: 0.6357, average train loss: 6.6983
[10/26 12:11:05 visual_prompt]: 	Test 100/123. loss: 5.152, 0.2399 s / batch. (data: 2.53e-05)max mem: 15.93560 GB 
[10/26 12:11:16 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.2308, average loss: 4.6649
[10/26 12:11:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.25	
[10/26 12:11:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/26 12:12:21 visual_prompt]: 	Training 100/1106. train loss: 7.4091,	0.6171 s / batch. (data: 3.17e-04). ETA=18:11:02, max mem: 15.9 GB 
[10/26 12:13:24 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6465 s / batch. (data: 8.29e-04). ETA=19:01:49, max mem: 15.9 GB 
[10/26 12:14:27 visual_prompt]: 	Training 300/1106. train loss: 1.1874,	0.6326 s / batch. (data: 7.99e-04). ETA=18:36:12, max mem: 15.9 GB 
[10/26 12:15:31 visual_prompt]: 	Training 400/1106. train loss: 12.7625,	0.6365 s / batch. (data: 7.29e-04). ETA=18:42:04, max mem: 15.9 GB 
[10/26 12:16:34 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6296 s / batch. (data: 8.09e-04). ETA=18:28:56, max mem: 15.9 GB 
[10/26 12:17:37 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6195 s / batch. (data: 7.73e-04). ETA=18:10:05, max mem: 15.9 GB 
[10/26 12:18:41 visual_prompt]: 	Training 700/1106. train loss: 0.6899,	0.6227 s / batch. (data: 3.24e-04). ETA=18:14:35, max mem: 15.9 GB 
[10/26 12:19:44 visual_prompt]: 	Training 800/1106. train loss: 0.0001,	0.6314 s / batch. (data: 8.09e-04). ETA=18:28:52, max mem: 15.9 GB 
[10/26 12:20:47 visual_prompt]: 	Training 900/1106. train loss: 17.5544,	0.6331 s / batch. (data: 3.05e-04). ETA=18:30:45, max mem: 15.9 GB 
[10/26 12:21:51 visual_prompt]: 	Training 1000/1106. train loss: 4.1519,	0.6391 s / batch. (data: 8.09e-04). ETA=18:40:15, max mem: 15.9 GB 
[10/26 12:22:54 visual_prompt]: 	Training 1100/1106. train loss: 11.9332,	0.6164 s / batch. (data: 2.07e-04). ETA=17:59:31, max mem: 15.9 GB 
[10/26 12:22:58 visual_prompt]: Epoch 5 / 100: avg data time: 4.17e-03, avg batch time: 0.6344, average train loss: 10.2001
[10/26 12:23:48 visual_prompt]: 	Test 100/123. loss: 18.178, 0.2251 s / batch. (data: 3.08e-05)max mem: 15.93560 GB 
[10/26 12:23:58 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2340, average loss: 19.9460
[10/26 12:23:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.74	
[10/26 12:23:58 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/26 12:25:04 visual_prompt]: 	Training 100/1106. train loss: 3.1629,	0.6174 s / batch. (data: 3.16e-04). ETA=18:00:10, max mem: 15.9 GB 
[10/26 12:26:07 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6480 s / batch. (data: 8.21e-04). ETA=18:52:34, max mem: 15.9 GB 
[10/26 12:27:10 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6318 s / batch. (data: 7.93e-04). ETA=18:23:15, max mem: 15.9 GB 
[10/26 12:28:14 visual_prompt]: 	Training 400/1106. train loss: 0.7364,	0.6287 s / batch. (data: 3.07e-04). ETA=18:16:45, max mem: 15.9 GB 
[10/26 12:29:17 visual_prompt]: 	Training 500/1106. train loss: 3.3718,	0.6295 s / batch. (data: 3.09e-04). ETA=18:17:11, max mem: 15.9 GB 
[10/26 12:30:20 visual_prompt]: 	Training 600/1106. train loss: 5.6327,	0.6313 s / batch. (data: 5.42e-03). ETA=18:19:15, max mem: 15.9 GB 
[10/26 12:31:24 visual_prompt]: 	Training 700/1106. train loss: 2.4516,	0.6433 s / batch. (data: 8.34e-04). ETA=18:38:59, max mem: 15.9 GB 
[10/26 12:32:27 visual_prompt]: 	Training 800/1106. train loss: 38.4101,	0.6430 s / batch. (data: 7.53e-04). ETA=18:37:23, max mem: 15.9 GB 
[10/26 12:33:30 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6198 s / batch. (data: 3.07e-04). ETA=17:56:03, max mem: 15.9 GB 
[10/26 12:34:34 visual_prompt]: 	Training 1000/1106. train loss: 0.0042,	0.6409 s / batch. (data: 1.69e-02). ETA=18:31:39, max mem: 15.9 GB 
[10/26 12:35:37 visual_prompt]: 	Training 1100/1106. train loss: 121.1309,	0.6183 s / batch. (data: 1.39e-04). ETA=17:51:23, max mem: 15.9 GB 
[10/26 12:35:41 visual_prompt]: Epoch 6 / 100: avg data time: 4.58e-03, avg batch time: 0.6349, average train loss: 12.0937
[10/26 12:36:30 visual_prompt]: 	Test 100/123. loss: 10.711, 0.2254 s / batch. (data: 3.79e-05)max mem: 15.93560 GB 
[10/26 12:36:41 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2315, average loss: 9.5590
[10/26 12:36:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.53	
[10/26 12:36:41 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/26 12:37:46 visual_prompt]: 	Training 100/1106. train loss: 51.9618,	0.6136 s / batch. (data: 3.31e-04). ETA=17:42:09, max mem: 15.9 GB 
[10/26 12:38:49 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6388 s / batch. (data: 3.63e-04). ETA=18:24:46, max mem: 15.9 GB 
[10/26 12:39:52 visual_prompt]: 	Training 300/1106. train loss: 25.3729,	0.6309 s / batch. (data: 2.87e-04). ETA=18:10:05, max mem: 15.9 GB 
[10/26 12:40:56 visual_prompt]: 	Training 400/1106. train loss: 24.9575,	0.6492 s / batch. (data: 7.90e-04). ETA=18:40:30, max mem: 15.9 GB 
[10/26 12:41:59 visual_prompt]: 	Training 500/1106. train loss: 18.2016,	0.6419 s / batch. (data: 1.05e-02). ETA=18:26:50, max mem: 15.9 GB 
[10/26 12:43:02 visual_prompt]: 	Training 600/1106. train loss: 11.0930,	0.6190 s / batch. (data: 3.23e-04). ETA=17:46:22, max mem: 15.9 GB 
[10/26 12:44:05 visual_prompt]: 	Training 700/1106. train loss: 63.7017,	0.6313 s / batch. (data: 7.96e-04). ETA=18:06:29, max mem: 15.9 GB 
[10/26 12:45:09 visual_prompt]: 	Training 800/1106. train loss: 0.0016,	0.6186 s / batch. (data: 3.61e-04). ETA=17:43:36, max mem: 15.9 GB 
[10/26 12:46:12 visual_prompt]: 	Training 900/1106. train loss: 3.0479,	0.6325 s / batch. (data: 8.15e-04). ETA=18:06:31, max mem: 15.9 GB 
[10/26 12:47:15 visual_prompt]: 	Training 1000/1106. train loss: 23.8426,	0.6602 s / batch. (data: 7.87e-04). ETA=18:52:56, max mem: 15.9 GB 
[10/26 12:48:18 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6077 s / batch. (data: 1.37e-04). ETA=17:21:46, max mem: 15.9 GB 
[10/26 12:48:22 visual_prompt]: Epoch 7 / 100: avg data time: 4.00e-03, avg batch time: 0.6336, average train loss: 16.2359
[10/26 12:49:12 visual_prompt]: 	Test 100/123. loss: 43.823, 0.2254 s / batch. (data: 3.72e-05)max mem: 15.93560 GB 
[10/26 12:49:23 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2337, average loss: 39.5494
[10/26 12:49:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.26	
[10/26 12:49:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/26 12:50:28 visual_prompt]: 	Training 100/1106. train loss: 8.3407,	0.6226 s / batch. (data: 2.91e-04). ETA=17:46:18, max mem: 15.9 GB 
[10/26 12:51:31 visual_prompt]: 	Training 200/1106. train loss: 6.3114,	0.6360 s / batch. (data: 8.12e-04). ETA=18:08:06, max mem: 15.9 GB 
[10/26 12:52:34 visual_prompt]: 	Training 300/1106. train loss: 16.1551,	0.6480 s / batch. (data: 7.99e-04). ETA=18:27:36, max mem: 15.9 GB 
[10/26 12:53:37 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6186 s / batch. (data: 2.32e-04). ETA=17:36:25, max mem: 15.9 GB 
[10/26 12:54:41 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6179 s / batch. (data: 3.33e-04). ETA=17:34:07, max mem: 15.9 GB 
[10/26 12:55:44 visual_prompt]: 	Training 600/1106. train loss: 5.4096,	0.6480 s / batch. (data: 8.01e-04). ETA=18:24:21, max mem: 15.9 GB 
[10/26 12:56:47 visual_prompt]: 	Training 700/1106. train loss: 17.7402,	0.6503 s / batch. (data: 7.45e-04). ETA=18:27:14, max mem: 15.9 GB 
[10/26 12:57:50 visual_prompt]: 	Training 800/1106. train loss: 32.9300,	0.6498 s / batch. (data: 5.98e-03). ETA=18:25:13, max mem: 15.9 GB 
[10/26 12:58:54 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6537 s / batch. (data: 7.69e-04). ETA=18:30:52, max mem: 15.9 GB 
[10/26 12:59:57 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6533 s / batch. (data: 8.14e-04). ETA=18:29:08, max mem: 15.9 GB 
[10/26 13:01:00 visual_prompt]: 	Training 1100/1106. train loss: 9.6791,	0.6173 s / batch. (data: 1.62e-04). ETA=17:26:50, max mem: 15.9 GB 
[10/26 13:01:04 visual_prompt]: Epoch 8 / 100: avg data time: 4.18e-03, avg batch time: 0.6340, average train loss: 17.3159
[10/26 13:01:54 visual_prompt]: 	Test 100/123. loss: 19.278, 0.2322 s / batch. (data: 3.53e-05)max mem: 15.93560 GB 
[10/26 13:02:04 visual_prompt]: Inference (val):avg data time: 1.30e-04, avg batch time: 0.2329, average loss: 17.3806
[10/26 13:02:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.96	
[10/26 13:02:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/26 13:03:10 visual_prompt]: 	Training 100/1106. train loss: 70.7482,	0.6174 s / batch. (data: 3.31e-04). ETA=17:26:03, max mem: 15.9 GB 
[10/26 13:04:13 visual_prompt]: 	Training 200/1106. train loss: 1.1691,	0.6432 s / batch. (data: 7.85e-04). ETA=18:08:36, max mem: 15.9 GB 
[10/26 13:05:16 visual_prompt]: 	Training 300/1106. train loss: 67.9171,	0.6145 s / batch. (data: 3.13e-04). ETA=17:18:58, max mem: 15.9 GB 
[10/26 13:06:19 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6182 s / batch. (data: 3.00e-04). ETA=17:24:20, max mem: 15.9 GB 
[10/26 13:07:23 visual_prompt]: 	Training 500/1106. train loss: 47.9295,	0.6400 s / batch. (data: 7.88e-04). ETA=18:00:02, max mem: 15.9 GB 
[10/26 13:08:26 visual_prompt]: 	Training 600/1106. train loss: 24.6827,	0.6280 s / batch. (data: 7.53e-04). ETA=17:38:43, max mem: 15.9 GB 
[10/26 13:09:29 visual_prompt]: 	Training 700/1106. train loss: 25.7786,	0.6304 s / batch. (data: 7.58e-04). ETA=17:41:41, max mem: 15.9 GB 
[10/26 13:10:33 visual_prompt]: 	Training 800/1106. train loss: 36.7459,	0.6204 s / batch. (data: 3.02e-04). ETA=17:23:52, max mem: 15.9 GB 
[10/26 13:11:36 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6339 s / batch. (data: 7.61e-04). ETA=17:45:25, max mem: 15.9 GB 
[10/26 13:12:39 visual_prompt]: 	Training 1000/1106. train loss: 12.6345,	0.6494 s / batch. (data: 1.19e-03). ETA=18:10:25, max mem: 15.9 GB 
[10/26 13:13:42 visual_prompt]: 	Training 1100/1106. train loss: 1.8894,	0.6188 s / batch. (data: 1.89e-04). ETA=17:18:04, max mem: 15.9 GB 
[10/26 13:13:46 visual_prompt]: Epoch 9 / 100: avg data time: 4.72e-03, avg batch time: 0.6345, average train loss: 17.4179
[10/26 13:14:37 visual_prompt]: 	Test 100/123. loss: 6.725, 0.2257 s / batch. (data: 2.84e-05)max mem: 15.93560 GB 
[10/26 13:14:48 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2324, average loss: 7.3893
[10/26 13:14:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.56	
[10/26 13:14:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/26 13:15:53 visual_prompt]: 	Training 100/1106. train loss: 7.5357,	0.6317 s / batch. (data: 7.88e-04). ETA=17:38:33, max mem: 15.9 GB 
[10/26 13:16:56 visual_prompt]: 	Training 200/1106. train loss: 11.3470,	0.6191 s / batch. (data: 3.29e-04). ETA=17:16:23, max mem: 15.9 GB 
[10/26 13:17:59 visual_prompt]: 	Training 300/1106. train loss: 121.7224,	0.6440 s / batch. (data: 8.02e-04). ETA=17:56:58, max mem: 15.9 GB 
[10/26 13:19:02 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6401 s / batch. (data: 7.93e-04). ETA=17:49:30, max mem: 15.9 GB 
[10/26 13:20:05 visual_prompt]: 	Training 500/1106. train loss: 65.0389,	0.6320 s / batch. (data: 3.39e-04). ETA=17:34:51, max mem: 15.9 GB 
[10/26 13:21:08 visual_prompt]: 	Training 600/1106. train loss: 69.1492,	0.6476 s / batch. (data: 1.10e-02). ETA=17:59:49, max mem: 15.9 GB 
[10/26 13:22:12 visual_prompt]: 	Training 700/1106. train loss: 41.3523,	0.6196 s / batch. (data: 3.04e-04). ETA=17:12:08, max mem: 15.9 GB 
[10/26 13:23:15 visual_prompt]: 	Training 800/1106. train loss: 21.7799,	0.6480 s / batch. (data: 7.51e-04). ETA=17:58:21, max mem: 15.9 GB 
[10/26 13:24:18 visual_prompt]: 	Training 900/1106. train loss: 9.3945,	0.6229 s / batch. (data: 2.94e-04). ETA=17:15:30, max mem: 15.9 GB 
[10/26 13:25:22 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6745 s / batch. (data: 3.85e-02). ETA=18:40:14, max mem: 15.9 GB 
[10/26 13:26:25 visual_prompt]: 	Training 1100/1106. train loss: 246.3587,	0.6184 s / batch. (data: 1.45e-04). ETA=17:05:57, max mem: 15.9 GB 
[10/26 13:26:28 visual_prompt]: Epoch 10 / 100: avg data time: 3.84e-03, avg batch time: 0.6330, average train loss: 23.2656
[10/26 13:27:18 visual_prompt]: 	Test 100/123. loss: 9.162, 0.2477 s / batch. (data: 3.12e-05)max mem: 15.93560 GB 
[10/26 13:27:29 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2333, average loss: 8.2134
[10/26 13:27:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.63	
[10/26 13:27:29 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/26 13:28:34 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6441 s / batch. (data: 1.61e-02). ETA=17:47:27, max mem: 15.9 GB 
[10/26 13:29:38 visual_prompt]: 	Training 200/1106. train loss: 36.5012,	0.6430 s / batch. (data: 8.49e-04). ETA=17:44:33, max mem: 15.9 GB 
[10/26 13:30:41 visual_prompt]: 	Training 300/1106. train loss: 9.2422,	0.6300 s / batch. (data: 2.83e-04). ETA=17:21:59, max mem: 15.9 GB 
[10/26 13:31:44 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6183 s / batch. (data: 3.25e-04). ETA=17:01:38, max mem: 15.9 GB 
[10/26 13:32:47 visual_prompt]: 	Training 500/1106. train loss: 6.7174,	0.6204 s / batch. (data: 5.05e-04). ETA=17:04:02, max mem: 15.9 GB 
[10/26 13:33:50 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6319 s / batch. (data: 8.32e-04). ETA=17:22:01, max mem: 15.9 GB 
[10/26 13:34:53 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6251 s / batch. (data: 3.02e-04). ETA=17:09:40, max mem: 15.9 GB 
[10/26 13:35:57 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6350 s / batch. (data: 1.55e-02). ETA=17:25:01, max mem: 15.9 GB 
[10/26 13:37:00 visual_prompt]: 	Training 900/1106. train loss: 21.1746,	0.6217 s / batch. (data: 3.27e-04). ETA=17:02:00, max mem: 15.9 GB 
[10/26 13:38:03 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6129 s / batch. (data: 3.31e-04). ETA=16:46:35, max mem: 15.9 GB 
[10/26 13:39:06 visual_prompt]: 	Training 1100/1106. train loss: 3.1848,	0.6184 s / batch. (data: 1.66e-04). ETA=16:54:33, max mem: 15.9 GB 
[10/26 13:39:10 visual_prompt]: Epoch 11 / 100: avg data time: 4.44e-03, avg batch time: 0.6335, average train loss: 22.9611
[10/26 13:39:59 visual_prompt]: 	Test 100/123. loss: 10.219, 0.2318 s / batch. (data: 2.77e-05)max mem: 15.93560 GB 
[10/26 13:40:10 visual_prompt]: Inference (val):avg data time: 1.91e-04, avg batch time: 0.2318, average loss: 11.2257
[10/26 13:40:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.78	
[10/26 13:40:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/26 13:41:16 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6214 s / batch. (data: 3.09e-04). ETA=16:58:21, max mem: 15.9 GB 
[10/26 13:42:20 visual_prompt]: 	Training 200/1106. train loss: 0.8615,	0.6266 s / batch. (data: 3.14e-04). ETA=17:05:49, max mem: 15.9 GB 
[10/26 13:43:23 visual_prompt]: 	Training 300/1106. train loss: 66.6340,	0.6395 s / batch. (data: 8.13e-04). ETA=17:25:52, max mem: 15.9 GB 
[10/26 13:44:26 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6477 s / batch. (data: 7.84e-04). ETA=17:38:17, max mem: 15.9 GB 
[10/26 13:45:29 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6170 s / batch. (data: 7.37e-04). ETA=16:47:06, max mem: 15.9 GB 
[10/26 13:46:33 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6781 s / batch. (data: 3.62e-02). ETA=18:25:43, max mem: 15.9 GB 
[10/26 13:47:36 visual_prompt]: 	Training 700/1106. train loss: 13.7624,	0.6431 s / batch. (data: 7.41e-04). ETA=17:27:33, max mem: 15.9 GB 
[10/26 13:48:39 visual_prompt]: 	Training 800/1106. train loss: 6.2323,	0.6370 s / batch. (data: 8.29e-04). ETA=17:16:33, max mem: 15.9 GB 
[10/26 13:49:42 visual_prompt]: 	Training 900/1106. train loss: 17.7759,	0.6190 s / batch. (data: 2.74e-04). ETA=16:46:16, max mem: 15.9 GB 
[10/26 13:50:46 visual_prompt]: 	Training 1000/1106. train loss: 10.8216,	0.6205 s / batch. (data: 6.26e-04). ETA=16:47:40, max mem: 15.9 GB 
[10/26 13:51:49 visual_prompt]: 	Training 1100/1106. train loss: 8.2689,	0.6196 s / batch. (data: 1.50e-04). ETA=16:45:10, max mem: 15.9 GB 
[10/26 13:51:53 visual_prompt]: Epoch 12 / 100: avg data time: 5.05e-03, avg batch time: 0.6351, average train loss: 22.2169
[10/26 13:52:42 visual_prompt]: 	Test 100/123. loss: 12.539, 0.2255 s / batch. (data: 3.29e-05)max mem: 15.93560 GB 
[10/26 13:52:53 visual_prompt]: Inference (val):avg data time: 9.85e-05, avg batch time: 0.2326, average loss: 11.1938
[10/26 13:52:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.28	
[10/26 13:52:53 visual_prompt]: Best epoch 12: best metric: -11.194
[10/26 13:52:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/26 13:53:59 visual_prompt]: 	Training 100/1106. train loss: 36.0571,	0.6318 s / batch. (data: 8.36e-04). ETA=17:03:47, max mem: 15.9 GB 
[10/26 13:55:02 visual_prompt]: 	Training 200/1106. train loss: 13.8944,	0.6321 s / batch. (data: 3.18e-04). ETA=17:03:14, max mem: 15.9 GB 
[10/26 13:56:05 visual_prompt]: 	Training 300/1106. train loss: 25.4006,	0.6318 s / batch. (data: 7.92e-04). ETA=17:01:42, max mem: 15.9 GB 
[10/26 13:57:08 visual_prompt]: 	Training 400/1106. train loss: 40.6893,	0.6309 s / batch. (data: 8.01e-04). ETA=16:59:13, max mem: 15.9 GB 
[10/26 13:58:11 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6253 s / batch. (data: 7.98e-04). ETA=16:49:09, max mem: 15.9 GB 
[10/26 13:59:15 visual_prompt]: 	Training 600/1106. train loss: 77.2296,	0.6138 s / batch. (data: 3.15e-04). ETA=16:29:31, max mem: 15.9 GB 
[10/26 14:00:18 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6384 s / batch. (data: 1.19e-02). ETA=17:08:03, max mem: 15.9 GB 
[10/26 14:01:22 visual_prompt]: 	Training 800/1106. train loss: 20.3600,	0.6495 s / batch. (data: 8.35e-04). ETA=17:24:56, max mem: 15.9 GB 
[10/26 14:02:25 visual_prompt]: 	Training 900/1106. train loss: 47.4319,	0.6282 s / batch. (data: 4.31e-04). ETA=16:49:36, max mem: 15.9 GB 
[10/26 14:03:28 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6199 s / batch. (data: 2.93e-04). ETA=16:35:14, max mem: 15.9 GB 
[10/26 14:04:31 visual_prompt]: 	Training 1100/1106. train loss: 7.8322,	0.6197 s / batch. (data: 1.63e-04). ETA=16:33:56, max mem: 15.9 GB 
[10/26 14:04:35 visual_prompt]: Epoch 13 / 100: avg data time: 4.74e-03, avg batch time: 0.6345, average train loss: 25.9805
[10/26 14:05:24 visual_prompt]: 	Test 100/123. loss: 6.606, 0.2250 s / batch. (data: 4.46e-05)max mem: 15.93560 GB 
[10/26 14:05:36 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.2335, average loss: 5.9700
[10/26 14:05:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.95	
[10/26 14:05:36 visual_prompt]: Best epoch 13: best metric: -5.970
[10/26 14:05:36 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/26 14:06:41 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6182 s / batch. (data: 3.27e-04). ETA=16:30:21, max mem: 15.9 GB 
[10/26 14:07:44 visual_prompt]: 	Training 200/1106. train loss: 24.9156,	0.6182 s / batch. (data: 3.19e-04). ETA=16:29:25, max mem: 15.9 GB 
[10/26 14:08:47 visual_prompt]: 	Training 300/1106. train loss: 17.2016,	0.6246 s / batch. (data: 5.46e-03). ETA=16:38:35, max mem: 15.9 GB 
[10/26 14:09:50 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6161 s / batch. (data: 3.02e-04). ETA=16:23:51, max mem: 15.9 GB 
[10/26 14:10:53 visual_prompt]: 	Training 500/1106. train loss: 39.1626,	0.6140 s / batch. (data: 3.32e-04). ETA=16:19:35, max mem: 15.9 GB 
[10/26 14:11:57 visual_prompt]: 	Training 600/1106. train loss: 13.5030,	0.6354 s / batch. (data: 8.31e-04). ETA=16:52:42, max mem: 15.9 GB 
[10/26 14:13:00 visual_prompt]: 	Training 700/1106. train loss: 14.2677,	0.6191 s / batch. (data: 3.17e-04). ETA=16:25:33, max mem: 15.9 GB 
[10/26 14:14:03 visual_prompt]: 	Training 800/1106. train loss: 47.5138,	0.6226 s / batch. (data: 3.14e-04). ETA=16:30:13, max mem: 15.9 GB 
[10/26 14:15:06 visual_prompt]: 	Training 900/1106. train loss: 26.0209,	0.6191 s / batch. (data: 3.31e-04). ETA=16:23:32, max mem: 15.9 GB 
[10/26 14:16:09 visual_prompt]: 	Training 1000/1106. train loss: 19.5860,	0.6184 s / batch. (data: 3.04e-04). ETA=16:21:22, max mem: 15.9 GB 
[10/26 14:17:12 visual_prompt]: 	Training 1100/1106. train loss: 24.9693,	0.6186 s / batch. (data: 1.56e-04). ETA=16:20:42, max mem: 15.9 GB 
[10/26 14:17:16 visual_prompt]: Epoch 14 / 100: avg data time: 4.43e-03, avg batch time: 0.6331, average train loss: 20.7884
[10/26 14:18:06 visual_prompt]: 	Test 100/123. loss: 27.395, 0.2364 s / batch. (data: 3.22e-05)max mem: 15.93560 GB 
[10/26 14:18:17 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.2330, average loss: 24.6916
[10/26 14:18:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.51	
[10/26 14:18:17 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/26 14:19:21 visual_prompt]: 	Training 100/1106. train loss: 6.9949,	0.6186 s / batch. (data: 3.11e-04). ETA=16:19:39, max mem: 15.9 GB 
[10/26 14:20:25 visual_prompt]: 	Training 200/1106. train loss: 49.7562,	0.6253 s / batch. (data: 3.20e-04). ETA=16:29:15, max mem: 15.9 GB 
[10/26 14:21:28 visual_prompt]: 	Training 300/1106. train loss: 45.7602,	0.6456 s / batch. (data: 8.51e-04). ETA=17:00:17, max mem: 15.9 GB 
[10/26 14:22:31 visual_prompt]: 	Training 400/1106. train loss: 193.7466,	0.6301 s / batch. (data: 3.13e-04). ETA=16:34:42, max mem: 15.9 GB 
[10/26 14:23:34 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6314 s / batch. (data: 7.81e-04). ETA=16:35:43, max mem: 15.9 GB 
[10/26 14:24:37 visual_prompt]: 	Training 600/1106. train loss: 28.8957,	0.6304 s / batch. (data: 2.13e-03). ETA=16:33:06, max mem: 15.9 GB 
[10/26 14:25:40 visual_prompt]: 	Training 700/1106. train loss: 120.3292,	0.6441 s / batch. (data: 8.03e-03). ETA=16:53:29, max mem: 15.9 GB 
[10/26 14:26:43 visual_prompt]: 	Training 800/1106. train loss: 20.3962,	0.6521 s / batch. (data: 1.60e-02). ETA=17:05:02, max mem: 15.9 GB 
[10/26 14:27:46 visual_prompt]: 	Training 900/1106. train loss: 117.7548,	0.6348 s / batch. (data: 3.60e-04). ETA=16:36:52, max mem: 15.9 GB 
[10/26 14:28:50 visual_prompt]: 	Training 1000/1106. train loss: 21.8131,	0.6464 s / batch. (data: 7.79e-04). ETA=16:53:57, max mem: 15.9 GB 
[10/26 14:29:53 visual_prompt]: 	Training 1100/1106. train loss: 20.7840,	0.6192 s / batch. (data: 1.82e-04). ETA=16:10:15, max mem: 15.9 GB 
[10/26 14:29:57 visual_prompt]: Epoch 15 / 100: avg data time: 3.52e-03, avg batch time: 0.6329, average train loss: 29.9405
[10/26 14:30:47 visual_prompt]: 	Test 100/123. loss: 0.725, 0.2274 s / batch. (data: 2.91e-05)max mem: 15.93560 GB 
[10/26 14:30:57 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.2341, average loss: 0.7339
[10/26 14:30:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.30	
[10/26 14:30:57 visual_prompt]: Best epoch 15: best metric: -0.734
[10/26 14:30:57 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/26 14:32:02 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6298 s / batch. (data: 7.53e-04). ETA=16:25:46, max mem: 15.9 GB 
[10/26 14:33:05 visual_prompt]: 	Training 200/1106. train loss: 25.5300,	0.6368 s / batch. (data: 7.74e-04). ETA=16:35:35, max mem: 15.9 GB 
[10/26 14:34:08 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6277 s / batch. (data: 3.20e-04). ETA=16:20:19, max mem: 15.9 GB 
[10/26 14:35:11 visual_prompt]: 	Training 400/1106. train loss: 56.3367,	0.6232 s / batch. (data: 3.33e-04). ETA=16:12:16, max mem: 15.9 GB 
[10/26 14:36:15 visual_prompt]: 	Training 500/1106. train loss: 22.1722,	0.6485 s / batch. (data: 8.10e-04). ETA=16:50:41, max mem: 15.9 GB 
[10/26 14:37:18 visual_prompt]: 	Training 600/1106. train loss: 0.0569,	0.6181 s / batch. (data: 3.04e-04). ETA=16:02:12, max mem: 15.9 GB 
[10/26 14:38:21 visual_prompt]: 	Training 700/1106. train loss: 40.3453,	0.6151 s / batch. (data: 3.13e-04). ETA=15:56:33, max mem: 15.9 GB 
[10/26 14:39:24 visual_prompt]: 	Training 800/1106. train loss: 9.9508,	0.6188 s / batch. (data: 4.20e-04). ETA=16:01:19, max mem: 15.9 GB 
[10/26 14:40:27 visual_prompt]: 	Training 900/1106. train loss: 0.6411,	0.6356 s / batch. (data: 7.87e-04). ETA=16:26:23, max mem: 15.9 GB 
[10/26 14:41:31 visual_prompt]: 	Training 1000/1106. train loss: 3.2715,	0.6324 s / batch. (data: 1.44e-02). ETA=16:20:19, max mem: 15.9 GB 
[10/26 14:42:34 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6182 s / batch. (data: 1.39e-04). ETA=15:57:20, max mem: 15.9 GB 
[10/26 14:42:38 visual_prompt]: Epoch 16 / 100: avg data time: 3.89e-03, avg batch time: 0.6336, average train loss: 22.5916
[10/26 14:43:28 visual_prompt]: 	Test 100/123. loss: 3.012, 0.2259 s / batch. (data: 2.88e-05)max mem: 15.93560 GB 
[10/26 14:43:38 visual_prompt]: Inference (val):avg data time: 9.98e-05, avg batch time: 0.2325, average loss: 2.8244
[10/26 14:43:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[10/26 14:43:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/26 14:44:44 visual_prompt]: 	Training 100/1106. train loss: 6.3466,	0.6179 s / batch. (data: 3.07e-04). ETA=15:55:42, max mem: 15.9 GB 
[10/26 14:45:47 visual_prompt]: 	Training 200/1106. train loss: 24.1793,	0.6240 s / batch. (data: 2.87e-04). ETA=16:04:05, max mem: 15.9 GB 
[10/26 14:46:50 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6326 s / batch. (data: 7.98e-04). ETA=16:16:24, max mem: 15.9 GB 
[10/26 14:47:53 visual_prompt]: 	Training 400/1106. train loss: 87.5229,	0.6358 s / batch. (data: 7.73e-04). ETA=16:20:17, max mem: 15.9 GB 
[10/26 14:48:57 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6170 s / batch. (data: 3.39e-04). ETA=15:50:11, max mem: 15.9 GB 
[10/26 14:50:00 visual_prompt]: 	Training 600/1106. train loss: 7.8849,	0.6483 s / batch. (data: 8.34e-04). ETA=16:37:16, max mem: 15.9 GB 
[10/26 14:51:03 visual_prompt]: 	Training 700/1106. train loss: 11.6690,	0.6487 s / batch. (data: 7.70e-04). ETA=16:36:53, max mem: 15.9 GB 
[10/26 14:52:07 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6471 s / batch. (data: 9.10e-04). ETA=16:33:19, max mem: 15.9 GB 
[10/26 14:53:10 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6393 s / batch. (data: 9.36e-04). ETA=16:20:16, max mem: 15.9 GB 
[10/26 14:54:13 visual_prompt]: 	Training 1000/1106. train loss: 76.1817,	0.6200 s / batch. (data: 3.50e-04). ETA=15:49:40, max mem: 15.9 GB 
[10/26 14:55:16 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6072 s / batch. (data: 1.62e-04). ETA=15:29:06, max mem: 15.9 GB 
[10/26 14:55:20 visual_prompt]: Epoch 17 / 100: avg data time: 4.77e-03, avg batch time: 0.6343, average train loss: 22.2519
[10/26 14:56:09 visual_prompt]: 	Test 100/123. loss: 7.594, 0.2251 s / batch. (data: 2.79e-05)max mem: 15.93560 GB 
[10/26 14:56:21 visual_prompt]: Inference (val):avg data time: 1.04e-04, avg batch time: 0.2327, average loss: 8.3066
[10/26 14:56:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.02	
[10/26 14:56:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/26 14:57:26 visual_prompt]: 	Training 100/1106. train loss: 207.8388,	0.6229 s / batch. (data: 5.47e-03). ETA=15:52:01, max mem: 15.9 GB 
[10/26 14:58:29 visual_prompt]: 	Training 200/1106. train loss: 116.6167,	0.6194 s / batch. (data: 3.30e-04). ETA=15:45:38, max mem: 15.9 GB 
[10/26 14:59:32 visual_prompt]: 	Training 300/1106. train loss: 102.3358,	0.6580 s / batch. (data: 4.08e-02). ETA=16:43:22, max mem: 15.9 GB 
[10/26 15:00:35 visual_prompt]: 	Training 400/1106. train loss: 64.9884,	0.6489 s / batch. (data: 1.29e-02). ETA=16:28:30, max mem: 15.9 GB 
[10/26 15:01:38 visual_prompt]: 	Training 500/1106. train loss: 15.5464,	0.6285 s / batch. (data: 1.05e-02). ETA=15:56:18, max mem: 15.9 GB 
[10/26 15:02:41 visual_prompt]: 	Training 600/1106. train loss: 1.5716,	0.6219 s / batch. (data: 2.77e-03). ETA=15:45:20, max mem: 15.9 GB 
[10/26 15:03:44 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6400 s / batch. (data: 8.13e-04). ETA=16:11:42, max mem: 15.9 GB 
[10/26 15:04:47 visual_prompt]: 	Training 800/1106. train loss: 11.4726,	0.6196 s / batch. (data: 3.21e-04). ETA=15:39:39, max mem: 15.9 GB 
[10/26 15:05:50 visual_prompt]: 	Training 900/1106. train loss: 6.9264,	0.6394 s / batch. (data: 7.79e-04). ETA=16:08:40, max mem: 15.9 GB 
[10/26 15:06:54 visual_prompt]: 	Training 1000/1106. train loss: 35.3496,	0.6296 s / batch. (data: 3.37e-04). ETA=15:52:43, max mem: 15.9 GB 
[10/26 15:07:57 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6107 s / batch. (data: 1.59e-04). ETA=15:23:10, max mem: 15.9 GB 
[10/26 15:08:00 visual_prompt]: Epoch 18 / 100: avg data time: 4.24e-03, avg batch time: 0.6326, average train loss: 25.6318
[10/26 15:08:50 visual_prompt]: 	Test 100/123. loss: 22.072, 0.2366 s / batch. (data: 4.98e-05)max mem: 15.93560 GB 
[10/26 15:09:01 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.2338, average loss: 19.8483
[10/26 15:09:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.07	
[10/26 15:09:01 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/26 15:10:06 visual_prompt]: 	Training 100/1106. train loss: 17.7893,	0.6323 s / batch. (data: 8.24e-04). ETA=15:54:38, max mem: 15.9 GB 
[10/26 15:11:09 visual_prompt]: 	Training 200/1106. train loss: 36.9778,	0.6353 s / batch. (data: 7.64e-04). ETA=15:58:10, max mem: 15.9 GB 
[10/26 15:12:13 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6319 s / batch. (data: 4.70e-03). ETA=15:52:02, max mem: 15.9 GB 
[10/26 15:13:16 visual_prompt]: 	Training 400/1106. train loss: 3.9223,	0.6181 s / batch. (data: 3.47e-04). ETA=15:30:06, max mem: 15.9 GB 
[10/26 15:14:19 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6216 s / batch. (data: 3.10e-04). ETA=15:34:19, max mem: 15.9 GB 
[10/26 15:15:22 visual_prompt]: 	Training 600/1106. train loss: 8.3828,	0.6318 s / batch. (data: 8.00e-04). ETA=15:48:41, max mem: 15.9 GB 
[10/26 15:16:26 visual_prompt]: 	Training 700/1106. train loss: 16.4412,	0.6450 s / batch. (data: 8.18e-04). ETA=16:07:28, max mem: 15.9 GB 
[10/26 15:17:29 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6350 s / batch. (data: 8.26e-04). ETA=15:51:17, max mem: 15.9 GB 
[10/26 15:18:32 visual_prompt]: 	Training 900/1106. train loss: 3.4001,	0.6491 s / batch. (data: 7.72e-04). ETA=16:11:23, max mem: 15.9 GB 
[10/26 15:19:35 visual_prompt]: 	Training 1000/1106. train loss: 189.6471,	0.6315 s / batch. (data: 2.77e-04). ETA=15:43:59, max mem: 15.9 GB 
[10/26 15:20:39 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6112 s / batch. (data: 1.93e-04). ETA=15:12:41, max mem: 15.9 GB 
[10/26 15:20:42 visual_prompt]: Epoch 19 / 100: avg data time: 4.31e-03, avg batch time: 0.6342, average train loss: 25.8408
[10/26 15:21:33 visual_prompt]: 	Test 100/123. loss: 34.068, 0.2249 s / batch. (data: 4.60e-05)max mem: 15.93560 GB 
[10/26 15:21:43 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.2330, average loss: 30.2536
[10/26 15:21:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.06	
[10/26 15:21:43 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[10/26 15:22:49 visual_prompt]: 	Training 100/1106. train loss: 39.4861,	0.6242 s / batch. (data: 2.78e-04). ETA=15:30:56, max mem: 15.9 GB 
[10/26 15:23:52 visual_prompt]: 	Training 200/1106. train loss: 0.0029,	0.6179 s / batch. (data: 3.50e-04). ETA=15:20:31, max mem: 15.9 GB 
[10/26 15:24:55 visual_prompt]: 	Training 300/1106. train loss: 13.6135,	0.6425 s / batch. (data: 1.05e-03). ETA=15:56:03, max mem: 15.9 GB 
[10/26 15:25:58 visual_prompt]: 	Training 400/1106. train loss: 24.3130,	0.6313 s / batch. (data: 3.17e-04). ETA=15:38:20, max mem: 15.9 GB 
[10/26 15:27:02 visual_prompt]: 	Training 500/1106. train loss: 0.1808,	0.6230 s / batch. (data: 2.99e-04). ETA=15:24:56, max mem: 15.9 GB 
[10/26 15:28:05 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6191 s / batch. (data: 3.29e-04). ETA=15:18:06, max mem: 15.9 GB 
[10/26 15:29:08 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6301 s / batch. (data: 2.87e-04). ETA=15:33:26, max mem: 15.9 GB 
[10/26 15:30:11 visual_prompt]: 	Training 800/1106. train loss: 38.5163,	0.6304 s / batch. (data: 7.43e-04). ETA=15:32:50, max mem: 15.9 GB 
[10/26 15:31:14 visual_prompt]: 	Training 900/1106. train loss: 374.2623,	0.6188 s / batch. (data: 3.04e-04). ETA=15:14:38, max mem: 15.9 GB 
[10/26 15:32:17 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6518 s / batch. (data: 7.97e-04). ETA=16:02:24, max mem: 15.9 GB 
[10/26 15:33:21 visual_prompt]: 	Training 1100/1106. train loss: 37.5657,	0.6174 s / batch. (data: 1.54e-04). ETA=15:10:28, max mem: 15.9 GB 
[10/26 15:33:24 visual_prompt]: Epoch 20 / 100: avg data time: 4.85e-03, avg batch time: 0.6339, average train loss: 26.0501
[10/26 15:34:14 visual_prompt]: 	Test 100/123. loss: 11.696, 0.2397 s / batch. (data: 2.88e-05)max mem: 15.93560 GB 
[10/26 15:34:25 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2326, average loss: 12.6122
[10/26 15:34:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.55	
[10/26 15:34:25 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[10/26 15:35:30 visual_prompt]: 	Training 100/1106. train loss: 0.0001,	0.6511 s / batch. (data: 2.30e-02). ETA=15:59:02, max mem: 15.9 GB 
[10/26 15:36:34 visual_prompt]: 	Training 200/1106. train loss: 41.4945,	0.6320 s / batch. (data: 3.15e-04). ETA=15:29:52, max mem: 15.9 GB 
[10/26 15:37:37 visual_prompt]: 	Training 300/1106. train loss: 0.0050,	0.6240 s / batch. (data: 7.70e-04). ETA=15:17:06, max mem: 15.9 GB 
[10/26 15:38:40 visual_prompt]: 	Training 400/1106. train loss: 4.8112,	0.6334 s / batch. (data: 8.13e-04). ETA=15:29:53, max mem: 15.9 GB 
[10/26 15:39:43 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6445 s / batch. (data: 8.38e-04). ETA=15:44:59, max mem: 15.9 GB 
[10/26 15:40:47 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6172 s / batch. (data: 2.80e-04). ETA=15:04:03, max mem: 15.9 GB 
[10/26 15:41:50 visual_prompt]: 	Training 700/1106. train loss: 14.0004,	0.6195 s / batch. (data: 3.68e-04). ETA=15:06:23, max mem: 15.9 GB 
[10/26 15:42:53 visual_prompt]: 	Training 800/1106. train loss: 15.7925,	0.6444 s / batch. (data: 3.21e-04). ETA=15:41:45, max mem: 15.9 GB 
[10/26 15:43:56 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6443 s / batch. (data: 1.23e-02). ETA=15:40:29, max mem: 15.9 GB 
[10/26 15:44:59 visual_prompt]: 	Training 1000/1106. train loss: 23.0857,	0.6194 s / batch. (data: 3.17e-04). ETA=15:03:03, max mem: 15.9 GB 
[10/26 15:46:02 visual_prompt]: 	Training 1100/1106. train loss: 0.0047,	0.6188 s / batch. (data: 1.43e-04). ETA=15:01:10, max mem: 15.9 GB 
[10/26 15:46:06 visual_prompt]: Epoch 21 / 100: avg data time: 4.42e-03, avg batch time: 0.6333, average train loss: 25.7076
[10/26 15:46:55 visual_prompt]: 	Test 100/123. loss: 36.524, 0.2263 s / batch. (data: 2.84e-05)max mem: 15.93560 GB 
[10/26 15:47:06 visual_prompt]: Inference (val):avg data time: 9.68e-05, avg batch time: 0.2323, average loss: 32.9415
[10/26 15:47:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.42	
[10/26 15:47:06 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[10/26 15:48:11 visual_prompt]: 	Training 100/1106. train loss: 0.3525,	0.6400 s / batch. (data: 3.40e-04). ETA=15:30:55, max mem: 15.9 GB 
[10/26 15:49:14 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6311 s / batch. (data: 7.77e-04). ETA=15:16:58, max mem: 15.9 GB 
[10/26 15:50:17 visual_prompt]: 	Training 300/1106. train loss: 0.0007,	0.6328 s / batch. (data: 1.51e-02). ETA=15:18:21, max mem: 15.9 GB 
[10/26 15:51:21 visual_prompt]: 	Training 400/1106. train loss: 0.0001,	0.6390 s / batch. (data: 7.90e-04). ETA=15:26:20, max mem: 15.9 GB 
[10/26 15:52:24 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6330 s / batch. (data: 7.96e-04). ETA=15:16:32, max mem: 15.9 GB 
[10/26 15:53:27 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6107 s / batch. (data: 3.16e-04). ETA=14:43:10, max mem: 15.9 GB 
[10/26 15:54:31 visual_prompt]: 	Training 700/1106. train loss: 30.2656,	0.6313 s / batch. (data: 1.13e-03). ETA=15:11:53, max mem: 15.9 GB 
[10/26 15:55:34 visual_prompt]: 	Training 800/1106. train loss: 0.3573,	0.6385 s / batch. (data: 3.73e-04). ETA=15:21:18, max mem: 15.9 GB 
[10/26 15:56:37 visual_prompt]: 	Training 900/1106. train loss: 8.8344,	0.6449 s / batch. (data: 7.88e-04). ETA=15:29:30, max mem: 15.9 GB 
[10/26 15:57:40 visual_prompt]: 	Training 1000/1106. train loss: 61.9609,	0.6307 s / batch. (data: 3.23e-04). ETA=15:07:51, max mem: 15.9 GB 
[10/26 15:58:43 visual_prompt]: 	Training 1100/1106. train loss: 0.0184,	0.6193 s / batch. (data: 1.56e-04). ETA=14:50:27, max mem: 15.9 GB 
[10/26 15:58:47 visual_prompt]: Epoch 22 / 100: avg data time: 4.20e-03, avg batch time: 0.6340, average train loss: 20.5869
[10/26 15:59:37 visual_prompt]: 	Test 100/123. loss: 14.630, 0.2387 s / batch. (data: 3.81e-05)max mem: 15.93560 GB 
[10/26 15:59:48 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2325, average loss: 13.0225
[10/26 15:59:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.88	
[10/26 15:59:48 visual_prompt]: Stopping early.
[10/26 15:59:48 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 15:59:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 15:59:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 15:59:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 15:59:48 visual_prompt]: Training with config:
[10/26 15:59:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr5.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 15:59:48 visual_prompt]: Loading training data...
[10/26 15:59:48 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 15:59:48 visual_prompt]: Loading validation data...
[10/26 15:59:48 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 15:59:48 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/26 15:59:51 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/26 15:59:51 visual_prompt]: tuned percent:0.522
[10/26 15:59:51 visual_prompt]: Device used for model: 0
[10/26 15:59:51 visual_prompt]: Setting up Evaluator...
[10/26 15:59:51 visual_prompt]: Setting up Trainer...
[10/26 15:59:51 visual_prompt]: 	Setting up the optimizer...
[10/26 15:59:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 16:00:57 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6273 s / batch. (data: 2.96e-04). ETA=19:15:18, max mem: 15.9 GB 
[10/26 16:02:00 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6333 s / batch. (data: 7.95e-04). ETA=19:25:15, max mem: 15.9 GB 
[10/26 16:03:03 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6189 s / batch. (data: 2.84e-04). ETA=18:57:40, max mem: 15.9 GB 
[10/26 16:04:07 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6483 s / batch. (data: 7.95e-04). ETA=19:50:43, max mem: 15.9 GB 
[10/26 16:05:10 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6320 s / batch. (data: 3.09e-04). ETA=19:19:42, max mem: 15.9 GB 
[10/26 16:06:13 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6181 s / batch. (data: 3.13e-04). ETA=18:53:15, max mem: 15.9 GB 
[10/26 16:07:17 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6398 s / batch. (data: 5.48e-03). ETA=19:31:50, max mem: 15.9 GB 
[10/26 16:08:20 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6377 s / batch. (data: 7.45e-04). ETA=19:26:58, max mem: 15.9 GB 
[10/26 16:09:23 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6640 s / batch. (data: 7.51e-04). ETA=20:14:01, max mem: 15.9 GB 
[10/26 16:10:27 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6193 s / batch. (data: 3.21e-04). ETA=18:51:19, max mem: 15.9 GB 
[10/26 16:11:30 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6178 s / batch. (data: 1.56e-04). ETA=18:47:32, max mem: 15.9 GB 
[10/26 16:11:34 visual_prompt]: Epoch 1 / 100: avg data time: 4.51e-03, avg batch time: 0.6352, average train loss: 1.4028
[10/26 16:12:24 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2418 s / batch. (data: 4.05e-05)max mem: 15.94077 GB 
[10/26 16:12:34 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.2319, average loss: 1.3505
[10/26 16:12:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/26 16:12:34 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/26 16:13:39 visual_prompt]: 	Training 100/1106. train loss: 1.2870,	0.6315 s / batch. (data: 8.28e-04). ETA=19:11:17, max mem: 15.9 GB 
[10/26 16:14:43 visual_prompt]: 	Training 200/1106. train loss: 3.0209,	0.6302 s / batch. (data: 3.41e-04). ETA=19:07:53, max mem: 15.9 GB 
[10/26 16:15:46 visual_prompt]: 	Training 300/1106. train loss: 0.9145,	0.6342 s / batch. (data: 8.31e-04). ETA=19:14:05, max mem: 15.9 GB 
[10/26 16:16:49 visual_prompt]: 	Training 400/1106. train loss: 0.1681,	0.6328 s / batch. (data: 8.25e-04). ETA=19:10:32, max mem: 15.9 GB 
[10/26 16:17:53 visual_prompt]: 	Training 500/1106. train loss: 0.9863,	0.6680 s / batch. (data: 4.05e-02). ETA=20:13:26, max mem: 15.9 GB 
[10/26 16:18:56 visual_prompt]: 	Training 600/1106. train loss: 1.3496,	0.6403 s / batch. (data: 7.83e-04). ETA=19:21:59, max mem: 15.9 GB 
[10/26 16:19:59 visual_prompt]: 	Training 700/1106. train loss: 0.0007,	0.6294 s / batch. (data: 3.18e-04). ETA=19:01:11, max mem: 15.9 GB 
[10/26 16:21:03 visual_prompt]: 	Training 800/1106. train loss: 2.4151,	0.6599 s / batch. (data: 5.94e-03). ETA=19:55:29, max mem: 15.9 GB 
[10/26 16:22:06 visual_prompt]: 	Training 900/1106. train loss: 0.9531,	0.6337 s / batch. (data: 8.05e-04). ETA=19:06:55, max mem: 15.9 GB 
[10/26 16:23:10 visual_prompt]: 	Training 1000/1106. train loss: 0.1291,	0.6480 s / batch. (data: 3.14e-04). ETA=19:31:44, max mem: 15.9 GB 
[10/26 16:24:13 visual_prompt]: 	Training 1100/1106. train loss: 0.0757,	0.6189 s / batch. (data: 1.40e-04). ETA=18:38:09, max mem: 15.9 GB 
[10/26 16:24:17 visual_prompt]: Epoch 2 / 100: avg data time: 4.32e-03, avg batch time: 0.6349, average train loss: 2.4593
[10/26 16:25:07 visual_prompt]: 	Test 100/123. loss: 5.681, 0.2277 s / batch. (data: 2.93e-05)max mem: 15.94077 GB 
[10/26 16:25:17 visual_prompt]: Inference (val):avg data time: 1.21e-04, avg batch time: 0.2320, average loss: 5.1185
[10/26 16:25:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.84	
[10/26 16:25:17 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/26 16:26:23 visual_prompt]: 	Training 100/1106. train loss: 0.0013,	0.6184 s / batch. (data: 3.47e-04). ETA=18:36:06, max mem: 15.9 GB 
[10/26 16:27:26 visual_prompt]: 	Training 200/1106. train loss: 0.0001,	0.6184 s / batch. (data: 3.19e-04). ETA=18:35:08, max mem: 15.9 GB 
[10/26 16:28:30 visual_prompt]: 	Training 300/1106. train loss: 0.0609,	0.6416 s / batch. (data: 4.89e-04). ETA=19:15:50, max mem: 15.9 GB 
[10/26 16:29:33 visual_prompt]: 	Training 400/1106. train loss: 2.1378,	0.6453 s / batch. (data: 8.64e-04). ETA=19:21:19, max mem: 15.9 GB 
[10/26 16:30:36 visual_prompt]: 	Training 500/1106. train loss: 7.2371,	0.6189 s / batch. (data: 4.73e-04). ETA=18:32:56, max mem: 15.9 GB 
[10/26 16:31:39 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6188 s / batch. (data: 1.07e-03). ETA=18:31:35, max mem: 15.9 GB 
[10/26 16:32:42 visual_prompt]: 	Training 700/1106. train loss: 2.9126,	0.6374 s / batch. (data: 7.84e-04). ETA=19:03:57, max mem: 15.9 GB 
[10/26 16:33:46 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6403 s / batch. (data: 8.02e-04). ETA=19:08:06, max mem: 15.9 GB 
[10/26 16:34:49 visual_prompt]: 	Training 900/1106. train loss: 3.5824,	0.6250 s / batch. (data: 5.44e-03). ETA=18:39:42, max mem: 15.9 GB 
[10/26 16:35:52 visual_prompt]: 	Training 1000/1106. train loss: 11.0780,	0.6354 s / batch. (data: 7.82e-04). ETA=18:57:17, max mem: 15.9 GB 
[10/26 16:36:56 visual_prompt]: 	Training 1100/1106. train loss: 1.8095,	0.6192 s / batch. (data: 1.57e-04). ETA=18:27:09, max mem: 15.9 GB 
[10/26 16:37:00 visual_prompt]: Epoch 3 / 100: avg data time: 5.08e-03, avg batch time: 0.6349, average train loss: 4.6089
[10/26 16:37:50 visual_prompt]: 	Test 100/123. loss: 1.659, 0.2406 s / batch. (data: 6.34e-05)max mem: 15.94077 GB 
[10/26 16:38:00 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.2318, average loss: 1.8456
[10/26 16:38:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.00	
[10/26 16:38:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/26 16:39:06 visual_prompt]: 	Training 100/1106. train loss: 0.0172,	0.6173 s / batch. (data: 2.92e-04). ETA=18:22:39, max mem: 15.9 GB 
[10/26 16:40:09 visual_prompt]: 	Training 200/1106. train loss: 31.4533,	0.6741 s / batch. (data: 3.97e-02). ETA=20:02:59, max mem: 15.9 GB 
[10/26 16:41:12 visual_prompt]: 	Training 300/1106. train loss: 2.1499,	0.6296 s / batch. (data: 8.00e-04). ETA=18:42:34, max mem: 15.9 GB 
[10/26 16:42:16 visual_prompt]: 	Training 400/1106. train loss: 5.4371,	0.6307 s / batch. (data: 8.39e-04). ETA=18:43:33, max mem: 15.9 GB 
[10/26 16:43:19 visual_prompt]: 	Training 500/1106. train loss: 0.1902,	0.6183 s / batch. (data: 3.09e-04). ETA=18:20:28, max mem: 15.9 GB 
[10/26 16:44:22 visual_prompt]: 	Training 600/1106. train loss: 4.4411,	0.6177 s / batch. (data: 3.17e-04). ETA=18:18:18, max mem: 15.9 GB 
[10/26 16:45:25 visual_prompt]: 	Training 700/1106. train loss: 4.1768,	0.6187 s / batch. (data: 3.24e-04). ETA=18:19:06, max mem: 15.9 GB 
[10/26 16:46:28 visual_prompt]: 	Training 800/1106. train loss: 0.1827,	0.6177 s / batch. (data: 3.39e-04). ETA=18:16:16, max mem: 15.9 GB 
[10/26 16:47:31 visual_prompt]: 	Training 900/1106. train loss: 0.1070,	0.6332 s / batch. (data: 3.33e-04). ETA=18:42:43, max mem: 15.9 GB 
[10/26 16:48:35 visual_prompt]: 	Training 1000/1106. train loss: 0.0009,	0.6251 s / batch. (data: 3.11e-04). ETA=18:27:16, max mem: 15.9 GB 
[10/26 16:49:38 visual_prompt]: 	Training 1100/1106. train loss: 14.2074,	0.6189 s / batch. (data: 1.51e-04). ETA=18:15:14, max mem: 15.9 GB 
[10/26 16:49:42 visual_prompt]: Epoch 4 / 100: avg data time: 4.39e-03, avg batch time: 0.6342, average train loss: 5.8528
[10/26 16:50:31 visual_prompt]: 	Test 100/123. loss: 1.259, 0.2348 s / batch. (data: 3.84e-05)max mem: 15.94077 GB 
[10/26 16:50:42 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2321, average loss: 1.4305
[10/26 16:50:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.11	
[10/26 16:50:42 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/26 16:51:47 visual_prompt]: 	Training 100/1106. train loss: 14.7932,	0.6308 s / batch. (data: 8.34e-04). ETA=18:35:17, max mem: 15.9 GB 
[10/26 16:52:50 visual_prompt]: 	Training 200/1106. train loss: 26.0046,	0.6445 s / batch. (data: 1.65e-02). ETA=18:58:26, max mem: 15.9 GB 
[10/26 16:53:54 visual_prompt]: 	Training 300/1106. train loss: 1.1101,	0.6219 s / batch. (data: 3.35e-04). ETA=18:17:22, max mem: 15.9 GB 
[10/26 16:54:57 visual_prompt]: 	Training 400/1106. train loss: 17.5856,	0.6232 s / batch. (data: 7.33e-04). ETA=18:18:43, max mem: 15.9 GB 
[10/26 16:56:00 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6422 s / batch. (data: 7.94e-04). ETA=18:51:01, max mem: 15.9 GB 
[10/26 16:57:04 visual_prompt]: 	Training 600/1106. train loss: 0.0980,	0.6321 s / batch. (data: 8.11e-04). ETA=18:32:15, max mem: 15.9 GB 
[10/26 16:58:07 visual_prompt]: 	Training 700/1106. train loss: 8.7786,	0.6398 s / batch. (data: 8.17e-04). ETA=18:44:41, max mem: 15.9 GB 
[10/26 16:59:10 visual_prompt]: 	Training 800/1106. train loss: 0.4527,	0.6188 s / batch. (data: 3.11e-04). ETA=18:06:47, max mem: 15.9 GB 
[10/26 17:00:13 visual_prompt]: 	Training 900/1106. train loss: 1.5575,	0.6468 s / batch. (data: 7.53e-04). ETA=18:54:53, max mem: 15.9 GB 
[10/26 17:01:17 visual_prompt]: 	Training 1000/1106. train loss: 7.8904,	0.6351 s / batch. (data: 8.07e-04). ETA=18:33:19, max mem: 15.9 GB 
[10/26 17:02:20 visual_prompt]: 	Training 1100/1106. train loss: 4.0861,	0.6200 s / batch. (data: 1.80e-04). ETA=18:05:42, max mem: 15.9 GB 
[10/26 17:02:24 visual_prompt]: Epoch 5 / 100: avg data time: 3.74e-03, avg batch time: 0.6345, average train loss: 6.3521
[10/26 17:03:14 visual_prompt]: 	Test 100/123. loss: 8.852, 0.2406 s / batch. (data: 5.10e-05)max mem: 15.94077 GB 
[10/26 17:03:24 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2320, average loss: 9.7592
[10/26 17:03:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.11	
[10/26 17:03:24 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/26 17:04:30 visual_prompt]: 	Training 100/1106. train loss: 15.6455,	0.6518 s / batch. (data: 1.11e-02). ETA=19:00:23, max mem: 15.9 GB 
[10/26 17:05:33 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6314 s / batch. (data: 8.13e-04). ETA=18:23:31, max mem: 15.9 GB 
[10/26 17:06:36 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6306 s / batch. (data: 8.08e-04). ETA=18:21:07, max mem: 15.9 GB 
[10/26 17:07:39 visual_prompt]: 	Training 400/1106. train loss: 6.0684,	0.6327 s / batch. (data: 2.81e-04). ETA=18:23:39, max mem: 15.9 GB 
[10/26 17:08:42 visual_prompt]: 	Training 500/1106. train loss: 2.6663,	0.6190 s / batch. (data: 3.21e-04). ETA=17:58:48, max mem: 15.9 GB 
[10/26 17:09:46 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6377 s / batch. (data: 5.91e-03). ETA=18:30:15, max mem: 15.9 GB 
[10/26 17:10:49 visual_prompt]: 	Training 700/1106. train loss: 14.1034,	0.6430 s / batch. (data: 8.20e-04). ETA=18:38:29, max mem: 15.9 GB 
[10/26 17:11:52 visual_prompt]: 	Training 800/1106. train loss: 7.2658,	0.6322 s / batch. (data: 3.19e-04). ETA=18:18:34, max mem: 15.9 GB 
[10/26 17:12:56 visual_prompt]: 	Training 900/1106. train loss: 44.6235,	0.6408 s / batch. (data: 8.32e-04). ETA=18:32:26, max mem: 15.9 GB 
[10/26 17:13:59 visual_prompt]: 	Training 1000/1106. train loss: 0.0002,	0.6355 s / batch. (data: 7.98e-04). ETA=18:22:16, max mem: 15.9 GB 
[10/26 17:15:02 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6181 s / batch. (data: 1.37e-04). ETA=17:51:03, max mem: 15.9 GB 
[10/26 17:15:06 visual_prompt]: Epoch 6 / 100: avg data time: 4.27e-03, avg batch time: 0.6342, average train loss: 10.5652
[10/26 17:15:56 visual_prompt]: 	Test 100/123. loss: 13.217, 0.2356 s / batch. (data: 2.84e-05)max mem: 15.94077 GB 
[10/26 17:16:06 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2321, average loss: 14.5185
[10/26 17:16:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.29	
[10/26 17:16:06 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/26 17:17:11 visual_prompt]: 	Training 100/1106. train loss: 16.6056,	0.6442 s / batch. (data: 8.20e-04). ETA=18:35:05, max mem: 15.9 GB 
[10/26 17:18:14 visual_prompt]: 	Training 200/1106. train loss: 0.9252,	0.6275 s / batch. (data: 7.95e-04). ETA=18:05:09, max mem: 15.9 GB 
[10/26 17:19:18 visual_prompt]: 	Training 300/1106. train loss: 2.7328,	0.6344 s / batch. (data: 3.18e-04). ETA=18:16:06, max mem: 15.9 GB 
[10/26 17:20:21 visual_prompt]: 	Training 400/1106. train loss: 11.7479,	0.6466 s / batch. (data: 8.28e-04). ETA=18:36:04, max mem: 15.9 GB 
[10/26 17:21:24 visual_prompt]: 	Training 500/1106. train loss: 18.2345,	0.6428 s / batch. (data: 1.05e-02). ETA=18:28:28, max mem: 15.9 GB 
[10/26 17:22:27 visual_prompt]: 	Training 600/1106. train loss: 2.4132,	0.6192 s / batch. (data: 3.14e-04). ETA=17:46:38, max mem: 15.9 GB 
[10/26 17:23:30 visual_prompt]: 	Training 700/1106. train loss: 24.4859,	0.6375 s / batch. (data: 6.02e-03). ETA=18:17:10, max mem: 15.9 GB 
[10/26 17:24:34 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6323 s / batch. (data: 7.80e-04). ETA=18:07:10, max mem: 15.9 GB 
[10/26 17:25:37 visual_prompt]: 	Training 900/1106. train loss: 8.4377,	0.6354 s / batch. (data: 7.74e-04). ETA=18:11:25, max mem: 15.9 GB 
[10/26 17:26:40 visual_prompt]: 	Training 1000/1106. train loss: 6.1250,	0.6330 s / batch. (data: 8.37e-04). ETA=18:06:12, max mem: 15.9 GB 
[10/26 17:27:43 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6172 s / batch. (data: 1.47e-04). ETA=17:38:11, max mem: 15.9 GB 
[10/26 17:27:47 visual_prompt]: Epoch 7 / 100: avg data time: 4.16e-03, avg batch time: 0.6339, average train loss: 8.2163
[10/26 17:28:37 visual_prompt]: 	Test 100/123. loss: 3.716, 0.2350 s / batch. (data: 3.17e-05)max mem: 15.94077 GB 
[10/26 17:28:48 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2333, average loss: 3.3313
[10/26 17:28:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.16	
[10/26 17:28:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/26 17:29:54 visual_prompt]: 	Training 100/1106. train loss: 28.6781,	0.6306 s / batch. (data: 7.81e-04). ETA=17:59:54, max mem: 15.9 GB 
[10/26 17:30:57 visual_prompt]: 	Training 200/1106. train loss: 9.1528,	0.6343 s / batch. (data: 2.95e-04). ETA=18:05:14, max mem: 15.9 GB 
[10/26 17:32:00 visual_prompt]: 	Training 300/1106. train loss: 8.4199,	0.6584 s / batch. (data: 7.57e-04). ETA=18:45:26, max mem: 15.9 GB 
[10/26 17:33:04 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6196 s / batch. (data: 4.10e-04). ETA=17:38:00, max mem: 15.9 GB 
[10/26 17:34:07 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6389 s / batch. (data: 2.07e-02). ETA=18:09:59, max mem: 15.9 GB 
[10/26 17:35:10 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6335 s / batch. (data: 3.11e-04). ETA=17:59:42, max mem: 15.9 GB 
[10/26 17:36:13 visual_prompt]: 	Training 700/1106. train loss: 3.8484,	0.6320 s / batch. (data: 1.20e-02). ETA=17:56:08, max mem: 15.9 GB 
[10/26 17:37:17 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6381 s / batch. (data: 5.56e-03). ETA=18:05:24, max mem: 15.9 GB 
[10/26 17:38:20 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6351 s / batch. (data: 8.12e-04). ETA=17:59:11, max mem: 15.9 GB 
[10/26 17:39:24 visual_prompt]: 	Training 1000/1106. train loss: 0.4285,	0.6347 s / batch. (data: 7.64e-04). ETA=17:57:27, max mem: 15.9 GB 
[10/26 17:40:27 visual_prompt]: 	Training 1100/1106. train loss: 1.0962,	0.6189 s / batch. (data: 1.60e-04). ETA=17:29:41, max mem: 15.9 GB 
[10/26 17:40:31 visual_prompt]: Epoch 8 / 100: avg data time: 4.40e-03, avg batch time: 0.6351, average train loss: 14.3410
[10/26 17:41:20 visual_prompt]: 	Test 100/123. loss: 17.950, 0.2255 s / batch. (data: 2.84e-05)max mem: 15.94077 GB 
[10/26 17:41:31 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2323, average loss: 16.1433
[10/26 17:41:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.43	
[10/26 17:41:31 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/26 17:42:36 visual_prompt]: 	Training 100/1106. train loss: 13.5174,	0.6337 s / batch. (data: 8.06e-04). ETA=17:53:33, max mem: 15.9 GB 
[10/26 17:43:39 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6523 s / batch. (data: 5.91e-03). ETA=18:24:03, max mem: 15.9 GB 
[10/26 17:44:43 visual_prompt]: 	Training 300/1106. train loss: 33.7750,	0.6315 s / batch. (data: 7.71e-04). ETA=17:47:48, max mem: 15.9 GB 
[10/26 17:45:46 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6300 s / batch. (data: 3.22e-04). ETA=17:44:10, max mem: 15.9 GB 
[10/26 17:46:49 visual_prompt]: 	Training 500/1106. train loss: 67.1572,	0.6360 s / batch. (data: 3.21e-04). ETA=17:53:16, max mem: 15.9 GB 
[10/26 17:47:52 visual_prompt]: 	Training 600/1106. train loss: 7.7319,	0.6440 s / batch. (data: 8.26e-04). ETA=18:05:38, max mem: 15.9 GB 
[10/26 17:48:56 visual_prompt]: 	Training 700/1106. train loss: 14.7628,	0.6179 s / batch. (data: 3.22e-04). ETA=17:20:36, max mem: 15.9 GB 
[10/26 17:49:59 visual_prompt]: 	Training 800/1106. train loss: 2.5609,	0.6308 s / batch. (data: 8.02e-04). ETA=17:41:25, max mem: 15.9 GB 
[10/26 17:51:03 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6207 s / batch. (data: 3.15e-04). ETA=17:23:14, max mem: 15.9 GB 
[10/26 17:52:06 visual_prompt]: 	Training 1000/1106. train loss: 0.6302,	0.6202 s / batch. (data: 8.23e-04). ETA=17:21:28, max mem: 15.9 GB 
[10/26 17:53:09 visual_prompt]: 	Training 1100/1106. train loss: 2.8020,	0.6186 s / batch. (data: 1.47e-04). ETA=17:17:38, max mem: 15.9 GB 
[10/26 17:53:13 visual_prompt]: Epoch 9 / 100: avg data time: 4.29e-03, avg batch time: 0.6346, average train loss: 11.6522
[10/26 17:54:03 visual_prompt]: 	Test 100/123. loss: 6.972, 0.2486 s / batch. (data: 3.60e-05)max mem: 15.94077 GB 
[10/26 17:54:13 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.2338, average loss: 6.2753
[10/26 17:54:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.82	
[10/26 17:54:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/26 17:55:18 visual_prompt]: 	Training 100/1106. train loss: 1.5530,	0.6242 s / batch. (data: 2.74e-04). ETA=17:26:03, max mem: 15.9 GB 
[10/26 17:56:22 visual_prompt]: 	Training 200/1106. train loss: 17.6002,	0.6255 s / batch. (data: 3.50e-04). ETA=17:27:04, max mem: 15.9 GB 
[10/26 17:57:25 visual_prompt]: 	Training 300/1106. train loss: 26.1430,	0.6318 s / batch. (data: 5.53e-03). ETA=17:36:35, max mem: 15.9 GB 
[10/26 17:58:28 visual_prompt]: 	Training 400/1106. train loss: 0.0687,	0.6510 s / batch. (data: 7.80e-04). ETA=18:07:44, max mem: 15.9 GB 
[10/26 17:59:31 visual_prompt]: 	Training 500/1106. train loss: 13.9488,	0.6192 s / batch. (data: 3.29e-04). ETA=17:13:28, max mem: 15.9 GB 
[10/26 18:00:35 visual_prompt]: 	Training 600/1106. train loss: 33.4606,	0.6505 s / batch. (data: 5.94e-03). ETA=18:04:36, max mem: 15.9 GB 
[10/26 18:01:38 visual_prompt]: 	Training 700/1106. train loss: 15.0572,	0.6242 s / batch. (data: 3.29e-04). ETA=17:19:42, max mem: 15.9 GB 
[10/26 18:02:41 visual_prompt]: 	Training 800/1106. train loss: 5.3295,	0.6189 s / batch. (data: 3.02e-04). ETA=17:09:53, max mem: 15.9 GB 
[10/26 18:03:44 visual_prompt]: 	Training 900/1106. train loss: 30.2022,	0.6304 s / batch. (data: 7.81e-04). ETA=17:28:01, max mem: 15.9 GB 
[10/26 18:04:47 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6200 s / batch. (data: 3.03e-04). ETA=17:09:37, max mem: 15.9 GB 
[10/26 18:05:50 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6181 s / batch. (data: 2.09e-04). ETA=17:05:34, max mem: 15.9 GB 
[10/26 18:05:54 visual_prompt]: Epoch 10 / 100: avg data time: 4.12e-03, avg batch time: 0.6338, average train loss: 17.8250
[10/26 18:06:44 visual_prompt]: 	Test 100/123. loss: 7.782, 0.2257 s / batch. (data: 4.60e-05)max mem: 15.94077 GB 
[10/26 18:06:54 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2329, average loss: 7.0025
[10/26 18:06:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.69	
[10/26 18:06:54 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/26 18:08:00 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6230 s / batch. (data: 9.76e-04). ETA=17:12:32, max mem: 15.9 GB 
[10/26 18:09:03 visual_prompt]: 	Training 200/1106. train loss: 2.4083,	0.6345 s / batch. (data: 1.56e-02). ETA=17:30:34, max mem: 15.9 GB 
[10/26 18:10:06 visual_prompt]: 	Training 300/1106. train loss: 1.4977,	0.6200 s / batch. (data: 4.57e-04). ETA=17:05:31, max mem: 15.9 GB 
[10/26 18:11:10 visual_prompt]: 	Training 400/1106. train loss: 12.2567,	0.6371 s / batch. (data: 8.33e-04). ETA=17:32:39, max mem: 15.9 GB 
[10/26 18:12:13 visual_prompt]: 	Training 500/1106. train loss: 8.5386,	0.6356 s / batch. (data: 3.34e-04). ETA=17:29:12, max mem: 15.9 GB 
[10/26 18:13:16 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6360 s / batch. (data: 8.00e-04). ETA=17:28:44, max mem: 15.9 GB 
[10/26 18:14:20 visual_prompt]: 	Training 700/1106. train loss: 1.0482,	0.6190 s / batch. (data: 2.85e-04). ETA=16:59:44, max mem: 15.9 GB 
[10/26 18:15:23 visual_prompt]: 	Training 800/1106. train loss: 14.8672,	0.6189 s / batch. (data: 2.83e-04). ETA=16:58:30, max mem: 15.9 GB 
[10/26 18:16:26 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6083 s / batch. (data: 3.30e-04). ETA=16:39:57, max mem: 15.9 GB 
[10/26 18:17:29 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6300 s / batch. (data: 7.28e-04). ETA=17:14:44, max mem: 15.9 GB 
[10/26 18:18:32 visual_prompt]: 	Training 1100/1106. train loss: 7.1506,	0.6185 s / batch. (data: 1.61e-04). ETA=16:54:48, max mem: 15.9 GB 
[10/26 18:18:36 visual_prompt]: Epoch 11 / 100: avg data time: 4.35e-03, avg batch time: 0.6341, average train loss: 23.0870
[10/26 18:19:26 visual_prompt]: 	Test 100/123. loss: 5.179, 0.2252 s / batch. (data: 2.96e-05)max mem: 15.94077 GB 
[10/26 18:19:37 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2324, average loss: 4.6891
[10/26 18:19:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.33	
[10/26 18:19:37 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/26 18:20:43 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6211 s / batch. (data: 3.08e-04). ETA=16:57:51, max mem: 15.9 GB 
[10/26 18:21:46 visual_prompt]: 	Training 200/1106. train loss: 6.6938,	0.6262 s / batch. (data: 2.98e-04). ETA=17:05:12, max mem: 15.9 GB 
[10/26 18:22:49 visual_prompt]: 	Training 300/1106. train loss: 130.2747,	0.6184 s / batch. (data: 3.35e-04). ETA=16:51:30, max mem: 15.9 GB 
[10/26 18:23:52 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6281 s / batch. (data: 7.49e-04). ETA=17:06:15, max mem: 15.9 GB 
[10/26 18:24:55 visual_prompt]: 	Training 500/1106. train loss: 41.1279,	0.6336 s / batch. (data: 2.83e-04). ETA=17:14:11, max mem: 15.9 GB 
[10/26 18:25:58 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6312 s / batch. (data: 8.20e-04). ETA=17:09:12, max mem: 15.9 GB 
[10/26 18:27:02 visual_prompt]: 	Training 700/1106. train loss: 22.1866,	0.6283 s / batch. (data: 7.73e-04). ETA=17:03:29, max mem: 15.9 GB 
[10/26 18:28:05 visual_prompt]: 	Training 800/1106. train loss: 18.4463,	0.6339 s / batch. (data: 7.85e-04). ETA=17:11:33, max mem: 15.9 GB 
[10/26 18:29:08 visual_prompt]: 	Training 900/1106. train loss: 0.6060,	0.6446 s / batch. (data: 7.85e-04). ETA=17:27:54, max mem: 15.9 GB 
[10/26 18:30:11 visual_prompt]: 	Training 1000/1106. train loss: 18.9028,	0.6261 s / batch. (data: 3.38e-04). ETA=16:56:41, max mem: 15.9 GB 
[10/26 18:31:14 visual_prompt]: 	Training 1100/1106. train loss: 77.2254,	0.6128 s / batch. (data: 1.51e-04). ETA=16:34:02, max mem: 15.9 GB 
[10/26 18:31:18 visual_prompt]: Epoch 12 / 100: avg data time: 4.98e-03, avg batch time: 0.6342, average train loss: 23.9558
[10/26 18:32:08 visual_prompt]: 	Test 100/123. loss: 15.668, 0.2300 s / batch. (data: 3.89e-05)max mem: 15.94077 GB 
[10/26 18:32:19 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2330, average loss: 14.0428
[10/26 18:32:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.11	
[10/26 18:32:19 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/26 18:33:24 visual_prompt]: 	Training 100/1106. train loss: 35.1376,	0.6176 s / batch. (data: 2.94e-04). ETA=16:40:45, max mem: 15.9 GB 
[10/26 18:34:27 visual_prompt]: 	Training 200/1106. train loss: 29.5510,	0.6182 s / batch. (data: 3.36e-04). ETA=16:40:46, max mem: 15.9 GB 
[10/26 18:35:30 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6255 s / batch. (data: 7.76e-04). ETA=16:51:33, max mem: 15.9 GB 
[10/26 18:36:34 visual_prompt]: 	Training 400/1106. train loss: 0.0210,	0.6323 s / batch. (data: 8.20e-04). ETA=17:01:31, max mem: 15.9 GB 
[10/26 18:37:37 visual_prompt]: 	Training 500/1106. train loss: 1.0043,	0.6206 s / batch. (data: 2.73e-04). ETA=16:41:34, max mem: 15.9 GB 
[10/26 18:38:40 visual_prompt]: 	Training 600/1106. train loss: 10.3099,	0.6462 s / batch. (data: 6.73e-04). ETA=17:21:47, max mem: 15.9 GB 
[10/26 18:39:43 visual_prompt]: 	Training 700/1106. train loss: 30.5755,	0.6567 s / batch. (data: 7.84e-04). ETA=17:37:36, max mem: 15.9 GB 
[10/26 18:40:47 visual_prompt]: 	Training 800/1106. train loss: 20.3106,	0.6338 s / batch. (data: 7.59e-04). ETA=16:59:41, max mem: 15.9 GB 
[10/26 18:41:50 visual_prompt]: 	Training 900/1106. train loss: 139.1644,	0.6373 s / batch. (data: 3.66e-04). ETA=17:04:15, max mem: 15.9 GB 
[10/26 18:42:53 visual_prompt]: 	Training 1000/1106. train loss: 7.1577,	0.6381 s / batch. (data: 3.22e-04). ETA=17:04:28, max mem: 15.9 GB 
[10/26 18:43:57 visual_prompt]: 	Training 1100/1106. train loss: 1.2700,	0.6178 s / batch. (data: 2.44e-04). ETA=16:30:49, max mem: 15.9 GB 
[10/26 18:44:00 visual_prompt]: Epoch 13 / 100: avg data time: 4.60e-03, avg batch time: 0.6343, average train loss: 18.3484
[10/26 18:44:50 visual_prompt]: 	Test 100/123. loss: 14.174, 0.2318 s / batch. (data: 2.72e-05)max mem: 15.94077 GB 
[10/26 18:45:01 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.2318, average loss: 12.7826
[10/26 18:45:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.08	
[10/26 18:45:01 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/26 18:46:06 visual_prompt]: 	Training 100/1106. train loss: 26.8029,	0.6285 s / batch. (data: 7.62e-04). ETA=16:46:50, max mem: 15.9 GB 
[10/26 18:47:09 visual_prompt]: 	Training 200/1106. train loss: 24.0141,	0.6265 s / batch. (data: 3.30e-04). ETA=16:42:33, max mem: 15.9 GB 
[10/26 18:48:13 visual_prompt]: 	Training 300/1106. train loss: 22.0194,	0.6342 s / batch. (data: 3.25e-04). ETA=16:53:56, max mem: 15.9 GB 
[10/26 18:49:16 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6356 s / batch. (data: 8.71e-04). ETA=16:55:01, max mem: 15.9 GB 
[10/26 18:50:19 visual_prompt]: 	Training 500/1106. train loss: 57.3025,	0.6299 s / batch. (data: 1.05e-03). ETA=16:44:57, max mem: 15.9 GB 
[10/26 18:51:22 visual_prompt]: 	Training 600/1106. train loss: 6.1633,	0.6326 s / batch. (data: 7.93e-04). ETA=16:48:06, max mem: 15.9 GB 
[10/26 18:52:26 visual_prompt]: 	Training 700/1106. train loss: 2.9753,	0.6400 s / batch. (data: 7.52e-04). ETA=16:58:56, max mem: 15.9 GB 
[10/26 18:53:29 visual_prompt]: 	Training 800/1106. train loss: 11.6852,	0.6314 s / batch. (data: 3.17e-04). ETA=16:44:04, max mem: 15.9 GB 
[10/26 18:54:32 visual_prompt]: 	Training 900/1106. train loss: 4.0597,	0.6260 s / batch. (data: 3.23e-04). ETA=16:34:31, max mem: 15.9 GB 
[10/26 18:55:36 visual_prompt]: 	Training 1000/1106. train loss: 0.0004,	0.6581 s / batch. (data: 1.16e-03). ETA=17:24:21, max mem: 15.9 GB 
[10/26 18:56:39 visual_prompt]: 	Training 1100/1106. train loss: 3.0417,	0.6185 s / batch. (data: 1.56e-04). ETA=16:20:32, max mem: 15.9 GB 
[10/26 18:56:43 visual_prompt]: Epoch 14 / 100: avg data time: 4.26e-03, avg batch time: 0.6348, average train loss: 17.6799
[10/26 18:57:33 visual_prompt]: 	Test 100/123. loss: 38.271, 0.2383 s / batch. (data: 4.24e-05)max mem: 15.94077 GB 
[10/26 18:57:43 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2314, average loss: 34.3409
[10/26 18:57:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.19	
[10/26 18:57:43 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/26 18:58:48 visual_prompt]: 	Training 100/1106. train loss: 66.2855,	0.6299 s / batch. (data: 5.48e-03). ETA=16:37:29, max mem: 15.9 GB 
[10/26 18:59:51 visual_prompt]: 	Training 200/1106. train loss: 74.7369,	0.6211 s / batch. (data: 7.78e-03). ETA=16:22:33, max mem: 15.9 GB 
[10/26 19:00:54 visual_prompt]: 	Training 300/1106. train loss: 10.1340,	0.6169 s / batch. (data: 3.10e-04). ETA=16:14:49, max mem: 15.9 GB 
[10/26 19:01:58 visual_prompt]: 	Training 400/1106. train loss: 211.7044,	0.6305 s / batch. (data: 8.43e-04). ETA=16:35:17, max mem: 15.9 GB 
[10/26 19:03:01 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6464 s / batch. (data: 1.04e-02). ETA=16:59:23, max mem: 15.9 GB 
[10/26 19:04:04 visual_prompt]: 	Training 600/1106. train loss: 36.2278,	0.6175 s / batch. (data: 4.75e-04). ETA=16:12:46, max mem: 15.9 GB 
[10/26 19:05:07 visual_prompt]: 	Training 700/1106. train loss: 41.8502,	0.6173 s / batch. (data: 3.05e-04). ETA=16:11:22, max mem: 15.9 GB 
[10/26 19:06:10 visual_prompt]: 	Training 800/1106. train loss: 18.0554,	0.6279 s / batch. (data: 1.04e-02). ETA=16:27:05, max mem: 15.9 GB 
[10/26 19:07:14 visual_prompt]: 	Training 900/1106. train loss: 29.2635,	0.6185 s / batch. (data: 3.14e-04). ETA=16:11:12, max mem: 15.9 GB 
[10/26 19:08:17 visual_prompt]: 	Training 1000/1106. train loss: 51.2766,	0.6207 s / batch. (data: 3.11e-04). ETA=16:13:33, max mem: 15.9 GB 
[10/26 19:09:20 visual_prompt]: 	Training 1100/1106. train loss: 39.1359,	0.6132 s / batch. (data: 1.53e-04). ETA=16:00:47, max mem: 15.9 GB 
[10/26 19:09:24 visual_prompt]: Epoch 15 / 100: avg data time: 4.19e-03, avg batch time: 0.6333, average train loss: 23.2676
[10/26 19:10:14 visual_prompt]: 	Test 100/123. loss: 27.532, 0.2247 s / batch. (data: 2.96e-05)max mem: 15.94077 GB 
[10/26 19:10:25 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2315, average loss: 30.2037
[10/26 19:10:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.46	
[10/26 19:10:25 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/26 19:11:30 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6219 s / batch. (data: 3.39e-04). ETA=16:13:20, max mem: 15.9 GB 
[10/26 19:12:33 visual_prompt]: 	Training 200/1106. train loss: 19.5262,	0.6402 s / batch. (data: 8.19e-04). ETA=16:40:59, max mem: 15.9 GB 
[10/26 19:13:36 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6318 s / batch. (data: 8.13e-04). ETA=16:26:44, max mem: 15.9 GB 
[10/26 19:14:39 visual_prompt]: 	Training 400/1106. train loss: 5.6939,	0.6334 s / batch. (data: 3.20e-04). ETA=16:28:12, max mem: 15.9 GB 
[10/26 19:15:43 visual_prompt]: 	Training 500/1106. train loss: 21.9058,	0.6447 s / batch. (data: 7.62e-04). ETA=16:44:44, max mem: 15.9 GB 
[10/26 19:16:46 visual_prompt]: 	Training 600/1106. train loss: 74.0051,	0.6327 s / batch. (data: 7.94e-04). ETA=16:25:03, max mem: 15.9 GB 
[10/26 19:17:49 visual_prompt]: 	Training 700/1106. train loss: 9.4010,	0.6525 s / batch. (data: 8.21e-04). ETA=16:54:47, max mem: 15.9 GB 
[10/26 19:18:53 visual_prompt]: 	Training 800/1106. train loss: 10.5592,	0.6189 s / batch. (data: 2.88e-04). ETA=16:01:29, max mem: 15.9 GB 
[10/26 19:19:56 visual_prompt]: 	Training 900/1106. train loss: 11.7938,	0.6194 s / batch. (data: 3.20e-04). ETA=16:01:08, max mem: 15.9 GB 
[10/26 19:20:59 visual_prompt]: 	Training 1000/1106. train loss: 74.5481,	0.6186 s / batch. (data: 2.53e-04). ETA=15:58:59, max mem: 15.9 GB 
[10/26 19:22:02 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6093 s / batch. (data: 1.46e-04). ETA=15:43:25, max mem: 15.9 GB 
[10/26 19:22:06 visual_prompt]: Epoch 16 / 100: avg data time: 4.06e-03, avg batch time: 0.6338, average train loss: 23.2576
[10/26 19:22:56 visual_prompt]: 	Test 100/123. loss: 0.671, 0.2556 s / batch. (data: 4.84e-05)max mem: 15.94077 GB 
[10/26 19:23:06 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.2335, average loss: 0.7214
[10/26 19:23:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 44.90	
[10/26 19:23:06 visual_prompt]: Best epoch 16: best metric: -0.721
[10/26 19:23:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/26 19:24:12 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6185 s / batch. (data: 3.31e-04). ETA=15:56:36, max mem: 15.9 GB 
[10/26 19:25:15 visual_prompt]: 	Training 200/1106. train loss: 4.1083,	0.6330 s / batch. (data: 7.93e-04). ETA=16:18:01, max mem: 15.9 GB 
[10/26 19:26:18 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6231 s / batch. (data: 3.17e-04). ETA=16:01:38, max mem: 15.9 GB 
[10/26 19:27:21 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6400 s / batch. (data: 1.33e-02). ETA=16:26:42, max mem: 15.9 GB 
[10/26 19:28:25 visual_prompt]: 	Training 500/1106. train loss: 21.1339,	0.6187 s / batch. (data: 3.33e-04). ETA=15:52:53, max mem: 15.9 GB 
[10/26 19:29:28 visual_prompt]: 	Training 600/1106. train loss: 1.1548,	0.6195 s / batch. (data: 3.15e-04). ETA=15:52:59, max mem: 15.9 GB 
[10/26 19:30:31 visual_prompt]: 	Training 700/1106. train loss: 50.2557,	0.6146 s / batch. (data: 4.29e-04). ETA=15:44:30, max mem: 15.9 GB 
[10/26 19:31:34 visual_prompt]: 	Training 800/1106. train loss: 4.3347,	0.6184 s / batch. (data: 4.38e-04). ETA=15:49:14, max mem: 15.9 GB 
[10/26 19:32:37 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6281 s / batch. (data: 2.95e-04). ETA=16:03:05, max mem: 15.9 GB 
[10/26 19:33:40 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6310 s / batch. (data: 7.73e-04). ETA=16:06:34, max mem: 15.9 GB 
[10/26 19:34:44 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6079 s / batch. (data: 1.61e-04). ETA=15:30:06, max mem: 15.9 GB 
[10/26 19:34:48 visual_prompt]: Epoch 17 / 100: avg data time: 4.39e-03, avg batch time: 0.6340, average train loss: 16.3829
[10/26 19:35:37 visual_prompt]: 	Test 100/123. loss: 42.210, 0.2251 s / batch. (data: 2.88e-05)max mem: 15.94077 GB 
[10/26 19:35:48 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2325, average loss: 46.2984
[10/26 19:35:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 37.06	
[10/26 19:35:48 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/26 19:36:53 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6147 s / batch. (data: 3.25e-04). ETA=15:39:25, max mem: 15.9 GB 
[10/26 19:37:56 visual_prompt]: 	Training 200/1106. train loss: 0.0585,	0.6400 s / batch. (data: 7.91e-04). ETA=16:17:05, max mem: 15.9 GB 
[10/26 19:38:59 visual_prompt]: 	Training 300/1106. train loss: 233.5798,	0.6270 s / batch. (data: 3.13e-04). ETA=15:56:08, max mem: 15.9 GB 
[10/26 19:40:02 visual_prompt]: 	Training 400/1106. train loss: 46.0016,	0.6341 s / batch. (data: 7.70e-04). ETA=16:05:54, max mem: 15.9 GB 
[10/26 19:41:05 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6489 s / batch. (data: 8.31e-04). ETA=16:27:21, max mem: 15.9 GB 
[10/26 19:42:08 visual_prompt]: 	Training 600/1106. train loss: 60.1976,	0.6261 s / batch. (data: 1.13e-03). ETA=15:51:42, max mem: 15.9 GB 
[10/26 19:43:12 visual_prompt]: 	Training 700/1106. train loss: 62.0404,	0.6457 s / batch. (data: 1.55e-02). ETA=16:20:20, max mem: 15.9 GB 
[10/26 19:44:15 visual_prompt]: 	Training 800/1106. train loss: 5.0525,	0.6298 s / batch. (data: 8.38e-04). ETA=15:55:09, max mem: 15.9 GB 
[10/26 19:45:18 visual_prompt]: 	Training 900/1106. train loss: 28.0907,	0.6304 s / batch. (data: 3.39e-04). ETA=15:55:06, max mem: 15.9 GB 
[10/26 19:46:21 visual_prompt]: 	Training 1000/1106. train loss: 4.2097,	0.6267 s / batch. (data: 3.15e-04). ETA=15:48:19, max mem: 15.9 GB 
[10/26 19:47:24 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6170 s / batch. (data: 1.59e-04). ETA=15:32:38, max mem: 15.9 GB 
[10/26 19:47:28 visual_prompt]: Epoch 18 / 100: avg data time: 4.24e-03, avg batch time: 0.6328, average train loss: 25.1074
[10/26 19:48:17 visual_prompt]: 	Test 100/123. loss: 11.721, 0.2277 s / batch. (data: 3.12e-05)max mem: 15.94077 GB 
[10/26 19:48:28 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2310, average loss: 10.5509
[10/26 19:48:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.04	
[10/26 19:48:28 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/26 19:49:33 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6459 s / batch. (data: 8.19e-04). ETA=16:15:13, max mem: 15.9 GB 
[10/26 19:50:37 visual_prompt]: 	Training 200/1106. train loss: 14.8393,	0.6311 s / batch. (data: 8.20e-04). ETA=15:51:47, max mem: 15.9 GB 
[10/26 19:51:40 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6744 s / batch. (data: 1.10e-02). ETA=16:55:58, max mem: 15.9 GB 
[10/26 19:52:43 visual_prompt]: 	Training 400/1106. train loss: 4.8302,	0.6174 s / batch. (data: 3.50e-04). ETA=15:29:02, max mem: 15.9 GB 
[10/26 19:53:46 visual_prompt]: 	Training 500/1106. train loss: 37.4833,	0.6480 s / batch. (data: 8.02e-04). ETA=16:14:05, max mem: 15.9 GB 
[10/26 19:54:49 visual_prompt]: 	Training 600/1106. train loss: 9.1267,	0.6311 s / batch. (data: 7.60e-04). ETA=15:47:39, max mem: 15.9 GB 
[10/26 19:55:53 visual_prompt]: 	Training 700/1106. train loss: 15.7202,	0.6278 s / batch. (data: 3.31e-04). ETA=15:41:37, max mem: 15.9 GB 
[10/26 19:56:56 visual_prompt]: 	Training 800/1106. train loss: 11.6771,	0.6343 s / batch. (data: 3.35e-04). ETA=15:50:14, max mem: 15.9 GB 
[10/26 19:57:59 visual_prompt]: 	Training 900/1106. train loss: 0.0005,	0.6188 s / batch. (data: 3.18e-04). ETA=15:26:06, max mem: 15.9 GB 
[10/26 19:59:02 visual_prompt]: 	Training 1000/1106. train loss: 0.1940,	0.6305 s / batch. (data: 7.54e-04). ETA=15:42:29, max mem: 15.9 GB 
[10/26 20:00:05 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6181 s / batch. (data: 2.04e-04). ETA=15:22:53, max mem: 15.9 GB 
[10/26 20:00:09 visual_prompt]: Epoch 19 / 100: avg data time: 4.17e-03, avg batch time: 0.6334, average train loss: 19.3453
[10/26 20:00:59 visual_prompt]: 	Test 100/123. loss: 49.543, 0.2279 s / batch. (data: 2.93e-05)max mem: 15.94077 GB 
[10/26 20:01:09 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2334, average loss: 54.3294
[10/26 20:01:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.10	
[10/26 20:01:09 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[10/26 20:02:15 visual_prompt]: 	Training 100/1106. train loss: 20.3878,	0.6303 s / batch. (data: 2.54e-04). ETA=15:40:03, max mem: 15.9 GB 
[10/26 20:03:18 visual_prompt]: 	Training 200/1106. train loss: 28.3630,	0.6300 s / batch. (data: 8.37e-04). ETA=15:38:33, max mem: 15.9 GB 
[10/26 20:04:21 visual_prompt]: 	Training 300/1106. train loss: 17.5568,	0.6356 s / batch. (data: 3.19e-04). ETA=15:45:47, max mem: 15.9 GB 
[10/26 20:05:25 visual_prompt]: 	Training 400/1106. train loss: 5.7731,	0.6303 s / batch. (data: 3.39e-04). ETA=15:36:53, max mem: 15.9 GB 
[10/26 20:06:28 visual_prompt]: 	Training 500/1106. train loss: 4.5064,	0.6274 s / batch. (data: 3.24e-04). ETA=15:31:29, max mem: 15.9 GB 
[10/26 20:07:31 visual_prompt]: 	Training 600/1106. train loss: 38.0037,	0.6441 s / batch. (data: 1.09e-02). ETA=15:55:19, max mem: 15.9 GB 
[10/26 20:08:35 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6482 s / batch. (data: 8.23e-04). ETA=16:00:12, max mem: 15.9 GB 
[10/26 20:09:38 visual_prompt]: 	Training 800/1106. train loss: 7.2439,	0.6360 s / batch. (data: 5.47e-03). ETA=15:41:07, max mem: 15.9 GB 
[10/26 20:10:41 visual_prompt]: 	Training 900/1106. train loss: 61.0922,	0.6325 s / batch. (data: 3.14e-04). ETA=15:34:55, max mem: 15.9 GB 
[10/26 20:11:45 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6431 s / batch. (data: 1.34e-02). ETA=15:49:30, max mem: 15.9 GB 
[10/26 20:12:48 visual_prompt]: 	Training 1100/1106. train loss: 0.0019,	0.6190 s / batch. (data: 1.45e-04). ETA=15:12:51, max mem: 15.9 GB 
[10/26 20:12:52 visual_prompt]: Epoch 20 / 100: avg data time: 5.04e-03, avg batch time: 0.6352, average train loss: 15.6431
[10/26 20:13:42 visual_prompt]: 	Test 100/123. loss: 6.658, 0.2250 s / batch. (data: 7.34e-05)max mem: 15.94077 GB 
[10/26 20:13:52 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2334, average loss: 5.9605
[10/26 20:13:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.67	
[10/26 20:13:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[10/26 20:14:58 visual_prompt]: 	Training 100/1106. train loss: 55.5434,	0.6516 s / batch. (data: 3.44e-04). ETA=15:59:47, max mem: 15.9 GB 
[10/26 20:16:01 visual_prompt]: 	Training 200/1106. train loss: 74.2379,	0.6358 s / batch. (data: 8.50e-04). ETA=15:35:30, max mem: 15.9 GB 
[10/26 20:17:04 visual_prompt]: 	Training 300/1106. train loss: 0.0075,	0.6338 s / batch. (data: 7.90e-04). ETA=15:31:30, max mem: 15.9 GB 
[10/26 20:18:07 visual_prompt]: 	Training 400/1106. train loss: 39.5903,	0.6454 s / batch. (data: 7.95e-04). ETA=15:47:27, max mem: 15.9 GB 
[10/26 20:19:10 visual_prompt]: 	Training 500/1106. train loss: 6.2326,	0.6224 s / batch. (data: 3.18e-04). ETA=15:12:40, max mem: 15.9 GB 
[10/26 20:20:13 visual_prompt]: 	Training 600/1106. train loss: 38.0308,	0.6469 s / batch. (data: 9.47e-04). ETA=15:47:32, max mem: 15.9 GB 
[10/26 20:21:16 visual_prompt]: 	Training 700/1106. train loss: 12.1240,	0.6327 s / batch. (data: 8.24e-04). ETA=15:25:39, max mem: 15.9 GB 
[10/26 20:22:19 visual_prompt]: 	Training 800/1106. train loss: 32.6809,	0.6435 s / batch. (data: 7.97e-04). ETA=15:40:25, max mem: 15.9 GB 
[10/26 20:23:23 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6460 s / batch. (data: 1.09e-03). ETA=15:42:59, max mem: 15.9 GB 
[10/26 20:24:26 visual_prompt]: 	Training 1000/1106. train loss: 50.5344,	0.6416 s / batch. (data: 8.96e-04). ETA=15:35:28, max mem: 15.9 GB 
[10/26 20:25:28 visual_prompt]: 	Training 1100/1106. train loss: 46.3792,	0.6168 s / batch. (data: 1.46e-04). ETA=14:58:11, max mem: 15.9 GB 
[10/26 20:25:32 visual_prompt]: Epoch 21 / 100: avg data time: 4.61e-03, avg batch time: 0.6329, average train loss: 25.7321
[10/26 20:26:22 visual_prompt]: 	Test 100/123. loss: 24.052, 0.2592 s / batch. (data: 3.03e-05)max mem: 15.94077 GB 
[10/26 20:26:33 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2328, average loss: 21.8059
[10/26 20:26:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.74	
[10/26 20:26:33 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[10/26 20:27:38 visual_prompt]: 	Training 100/1106. train loss: 1.9387,	0.6531 s / batch. (data: 1.99e-02). ETA=15:49:59, max mem: 15.9 GB 
[10/26 20:28:41 visual_prompt]: 	Training 200/1106. train loss: 30.0985,	0.6201 s / batch. (data: 3.56e-04). ETA=15:00:58, max mem: 15.9 GB 
[10/26 20:29:44 visual_prompt]: 	Training 300/1106. train loss: 7.1960,	0.6299 s / batch. (data: 8.05e-04). ETA=15:14:07, max mem: 15.9 GB 
[10/26 20:30:47 visual_prompt]: 	Training 400/1106. train loss: 14.7647,	0.6323 s / batch. (data: 3.09e-04). ETA=15:16:31, max mem: 15.9 GB 
[10/26 20:31:50 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6320 s / batch. (data: 3.38e-04). ETA=15:15:01, max mem: 15.9 GB 
[10/26 20:32:53 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6155 s / batch. (data: 3.14e-04). ETA=14:50:07, max mem: 15.9 GB 
[10/26 20:33:56 visual_prompt]: 	Training 700/1106. train loss: 66.6030,	0.6401 s / batch. (data: 9.63e-03). ETA=15:24:43, max mem: 15.9 GB 
[10/26 20:35:00 visual_prompt]: 	Training 800/1106. train loss: 18.7107,	0.6359 s / batch. (data: 7.91e-04). ETA=15:17:34, max mem: 15.9 GB 
[10/26 20:36:03 visual_prompt]: 	Training 900/1106. train loss: 7.8314,	0.6426 s / batch. (data: 8.23e-04). ETA=15:26:04, max mem: 15.9 GB 
[10/26 20:37:06 visual_prompt]: 	Training 1000/1106. train loss: 74.5578,	0.6377 s / batch. (data: 1.16e-03). ETA=15:18:03, max mem: 15.9 GB 
[10/26 20:38:09 visual_prompt]: 	Training 1100/1106. train loss: 47.9371,	0.6177 s / batch. (data: 1.64e-04). ETA=14:48:12, max mem: 15.9 GB 
[10/26 20:38:13 visual_prompt]: Epoch 22 / 100: avg data time: 3.96e-03, avg batch time: 0.6332, average train loss: 23.9766
[10/26 20:39:03 visual_prompt]: 	Test 100/123. loss: 39.527, 0.2247 s / batch. (data: 3.05e-05)max mem: 15.94077 GB 
[10/26 20:39:14 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2325, average loss: 35.6528
[10/26 20:39:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.92	
[10/26 20:39:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[10/26 20:40:20 visual_prompt]: 	Training 100/1106. train loss: 26.8196,	0.6170 s / batch. (data: 3.30e-04). ETA=14:46:04, max mem: 15.9 GB 
[10/26 20:41:23 visual_prompt]: 	Training 200/1106. train loss: 13.8450,	0.6543 s / batch. (data: 8.26e-04). ETA=15:38:37, max mem: 15.9 GB 
[10/26 20:42:27 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6488 s / batch. (data: 1.75e-02). ETA=15:29:40, max mem: 15.9 GB 
[10/26 20:43:30 visual_prompt]: 	Training 400/1106. train loss: 6.4291,	0.6187 s / batch. (data: 3.04e-04). ETA=14:45:27, max mem: 15.9 GB 
[10/26 20:44:33 visual_prompt]: 	Training 500/1106. train loss: 5.0503,	0.6224 s / batch. (data: 2.81e-04). ETA=14:49:40, max mem: 15.9 GB 
[10/26 20:45:37 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6313 s / batch. (data: 8.36e-04). ETA=15:01:19, max mem: 15.9 GB 
[10/26 20:46:40 visual_prompt]: 	Training 700/1106. train loss: 9.9727,	0.6196 s / batch. (data: 3.26e-04). ETA=14:43:33, max mem: 15.9 GB 
[10/26 20:47:43 visual_prompt]: 	Training 800/1106. train loss: 3.1434,	0.6403 s / batch. (data: 8.16e-04). ETA=15:12:02, max mem: 15.9 GB 
[10/26 20:48:47 visual_prompt]: 	Training 900/1106. train loss: 0.5661,	0.6407 s / batch. (data: 3.19e-04). ETA=15:11:37, max mem: 15.9 GB 
[10/26 20:49:50 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6495 s / batch. (data: 8.14e-04). ETA=15:22:58, max mem: 15.9 GB 
[10/26 20:50:53 visual_prompt]: 	Training 1100/1106. train loss: 11.3108,	0.6164 s / batch. (data: 1.65e-04). ETA=14:34:55, max mem: 15.9 GB 
[10/26 20:50:57 visual_prompt]: Epoch 23 / 100: avg data time: 5.06e-03, avg batch time: 0.6353, average train loss: 19.4618
[10/26 20:51:47 visual_prompt]: 	Test 100/123. loss: 8.615, 0.2301 s / batch. (data: 2.98e-05)max mem: 15.94077 GB 
[10/26 20:51:57 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2315, average loss: 7.7649
[10/26 20:51:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.97	
[10/26 20:51:57 visual_prompt]: Stopping early.
[10/26 20:51:57 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 20:51:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 20:51:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 20:51:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 20:51:57 visual_prompt]: Training with config:
[10/26 20:51:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr5.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 20:51:57 visual_prompt]: Loading training data...
[10/26 20:51:57 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 20:51:57 visual_prompt]: Loading validation data...
[10/26 20:51:57 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 20:51:57 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/26 20:52:00 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/26 20:52:00 visual_prompt]: tuned percent:0.522
[10/26 20:52:00 visual_prompt]: Device used for model: 0
[10/26 20:52:00 visual_prompt]: Setting up Evaluator...
[10/26 20:52:00 visual_prompt]: Setting up Trainer...
[10/26 20:52:00 visual_prompt]: 	Setting up the optimizer...
[10/26 20:52:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 20:53:06 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6341 s / batch. (data: 8.32e-04). ETA=19:27:52, max mem: 15.9 GB 
[10/26 20:54:10 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6283 s / batch. (data: 3.61e-04). ETA=19:16:03, max mem: 15.9 GB 
[10/26 20:55:13 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6360 s / batch. (data: 3.36e-04). ETA=19:29:15, max mem: 15.9 GB 
[10/26 20:56:16 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6377 s / batch. (data: 3.29e-04). ETA=19:31:15, max mem: 15.9 GB 
[10/26 20:57:20 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6560 s / batch. (data: 3.06e-04). ETA=20:03:48, max mem: 15.9 GB 
[10/26 20:58:23 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6438 s / batch. (data: 8.05e-04). ETA=19:40:19, max mem: 15.9 GB 
[10/26 20:59:27 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6203 s / batch. (data: 3.43e-04). ETA=18:56:08, max mem: 15.9 GB 
[10/26 21:00:30 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6464 s / batch. (data: 8.40e-04). ETA=19:42:53, max mem: 15.9 GB 
[10/26 21:01:33 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6720 s / batch. (data: 8.18e-04). ETA=20:28:35, max mem: 15.9 GB 
[10/26 21:02:37 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6336 s / batch. (data: 8.13e-04). ETA=19:17:17, max mem: 15.9 GB 
[10/26 21:03:40 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6192 s / batch. (data: 1.65e-04). ETA=18:50:02, max mem: 15.9 GB 
[10/26 21:03:44 visual_prompt]: Epoch 1 / 100: avg data time: 5.07e-03, avg batch time: 0.6361, average train loss: 1.4028
[10/26 21:04:35 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2255 s / batch. (data: 3.39e-05)max mem: 15.94594 GB 
[10/26 21:04:45 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2316, average loss: 1.3505
[10/26 21:04:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/26 21:04:45 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/26 21:05:50 visual_prompt]: 	Training 100/1106. train loss: 9.1345,	0.6256 s / batch. (data: 3.11e-04). ETA=19:00:33, max mem: 15.9 GB 
[10/26 21:06:53 visual_prompt]: 	Training 200/1106. train loss: 1.6953,	0.6440 s / batch. (data: 8.17e-04). ETA=19:33:03, max mem: 15.9 GB 
[10/26 21:07:57 visual_prompt]: 	Training 300/1106. train loss: 1.1467,	0.6504 s / batch. (data: 7.63e-04). ETA=19:43:34, max mem: 15.9 GB 
[10/26 21:09:00 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6311 s / batch. (data: 8.27e-04). ETA=19:07:32, max mem: 15.9 GB 
[10/26 21:10:03 visual_prompt]: 	Training 500/1106. train loss: 1.1030,	0.6293 s / batch. (data: 3.07e-04). ETA=19:03:07, max mem: 15.9 GB 
[10/26 21:11:07 visual_prompt]: 	Training 600/1106. train loss: 1.2849,	0.6534 s / batch. (data: 1.61e-02). ETA=19:45:53, max mem: 15.9 GB 
[10/26 21:12:10 visual_prompt]: 	Training 700/1106. train loss: 0.1206,	0.6186 s / batch. (data: 3.08e-04). ETA=18:41:39, max mem: 15.9 GB 
[10/26 21:13:14 visual_prompt]: 	Training 800/1106. train loss: 0.7310,	0.6469 s / batch. (data: 1.56e-02). ETA=19:31:50, max mem: 15.9 GB 
[10/26 21:14:17 visual_prompt]: 	Training 900/1106. train loss: 2.8868,	0.6200 s / batch. (data: 3.36e-04). ETA=18:42:09, max mem: 15.9 GB 
[10/26 21:15:20 visual_prompt]: 	Training 1000/1106. train loss: 0.0711,	0.6446 s / batch. (data: 7.51e-04). ETA=19:25:33, max mem: 15.9 GB 
[10/26 21:16:24 visual_prompt]: 	Training 1100/1106. train loss: 0.0557,	0.6182 s / batch. (data: 1.49e-04). ETA=18:36:49, max mem: 15.9 GB 
[10/26 21:16:27 visual_prompt]: Epoch 2 / 100: avg data time: 4.60e-03, avg batch time: 0.6349, average train loss: 2.8239
[10/26 21:17:17 visual_prompt]: 	Test 100/123. loss: 5.824, 0.2258 s / batch. (data: 3.89e-05)max mem: 15.94594 GB 
[10/26 21:17:28 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.2326, average loss: 5.2503
[10/26 21:17:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.11	
[10/26 21:17:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/26 21:18:34 visual_prompt]: 	Training 100/1106. train loss: 0.4251,	0.6359 s / batch. (data: 8.34e-04). ETA=19:07:37, max mem: 15.9 GB 
[10/26 21:19:37 visual_prompt]: 	Training 200/1106. train loss: 0.0440,	0.6278 s / batch. (data: 1.03e-03). ETA=18:51:55, max mem: 15.9 GB 
[10/26 21:20:41 visual_prompt]: 	Training 300/1106. train loss: 0.0396,	0.6299 s / batch. (data: 3.12e-04). ETA=18:54:42, max mem: 15.9 GB 
[10/26 21:21:44 visual_prompt]: 	Training 400/1106. train loss: 2.3943,	0.6469 s / batch. (data: 8.48e-04). ETA=19:24:19, max mem: 15.9 GB 
[10/26 21:22:48 visual_prompt]: 	Training 500/1106. train loss: 11.9085,	0.6319 s / batch. (data: 7.98e-04). ETA=18:56:17, max mem: 15.9 GB 
[10/26 21:23:51 visual_prompt]: 	Training 600/1106. train loss: 0.0001,	0.6483 s / batch. (data: 3.27e-04). ETA=19:24:40, max mem: 15.9 GB 
[10/26 21:24:54 visual_prompt]: 	Training 700/1106. train loss: 0.0214,	0.6344 s / batch. (data: 3.36e-04). ETA=18:58:39, max mem: 15.9 GB 
[10/26 21:25:57 visual_prompt]: 	Training 800/1106. train loss: 0.6901,	0.6404 s / batch. (data: 1.24e-02). ETA=19:08:23, max mem: 15.9 GB 
[10/26 21:27:01 visual_prompt]: 	Training 900/1106. train loss: 3.9054,	0.6176 s / batch. (data: 2.98e-04). ETA=18:26:28, max mem: 15.9 GB 
[10/26 21:28:04 visual_prompt]: 	Training 1000/1106. train loss: 5.6675,	0.6362 s / batch. (data: 7.49e-04). ETA=18:58:40, max mem: 15.9 GB 
[10/26 21:29:08 visual_prompt]: 	Training 1100/1106. train loss: 1.8808,	0.6184 s / batch. (data: 1.39e-04). ETA=18:25:45, max mem: 15.9 GB 
[10/26 21:29:11 visual_prompt]: Epoch 3 / 100: avg data time: 5.39e-03, avg batch time: 0.6359, average train loss: 3.3538
[10/26 21:30:02 visual_prompt]: 	Test 100/123. loss: 1.620, 0.2446 s / batch. (data: 4.43e-05)max mem: 15.94594 GB 
[10/26 21:30:12 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2322, average loss: 1.8974
[10/26 21:30:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.92	
[10/26 21:30:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/26 21:31:17 visual_prompt]: 	Training 100/1106. train loss: 0.0001,	0.6330 s / batch. (data: 1.11e-03). ETA=18:50:41, max mem: 15.9 GB 
[10/26 21:32:21 visual_prompt]: 	Training 200/1106. train loss: 25.3912,	0.6418 s / batch. (data: 7.88e-04). ETA=19:05:29, max mem: 15.9 GB 
[10/26 21:33:24 visual_prompt]: 	Training 300/1106. train loss: 1.7979,	0.6312 s / batch. (data: 8.08e-04). ETA=18:45:27, max mem: 15.9 GB 
[10/26 21:34:27 visual_prompt]: 	Training 400/1106. train loss: 6.0108,	0.6410 s / batch. (data: 3.14e-04). ETA=19:01:53, max mem: 15.9 GB 
[10/26 21:35:30 visual_prompt]: 	Training 500/1106. train loss: 25.8107,	0.6197 s / batch. (data: 2.93e-04). ETA=18:22:49, max mem: 15.9 GB 
[10/26 21:36:34 visual_prompt]: 	Training 600/1106. train loss: 3.4585,	0.6392 s / batch. (data: 8.03e-04). ETA=18:56:25, max mem: 15.9 GB 
[10/26 21:37:37 visual_prompt]: 	Training 700/1106. train loss: 29.1024,	0.6426 s / batch. (data: 8.12e-04). ETA=19:01:29, max mem: 15.9 GB 
[10/26 21:38:40 visual_prompt]: 	Training 800/1106. train loss: 3.6277,	0.6461 s / batch. (data: 7.82e-04). ETA=19:06:34, max mem: 15.9 GB 
[10/26 21:39:43 visual_prompt]: 	Training 900/1106. train loss: 0.0001,	0.6344 s / batch. (data: 8.14e-04). ETA=18:44:45, max mem: 15.9 GB 
[10/26 21:40:47 visual_prompt]: 	Training 1000/1106. train loss: 12.6314,	0.6434 s / batch. (data: 2.47e-02). ETA=18:59:37, max mem: 15.9 GB 
[10/26 21:41:50 visual_prompt]: 	Training 1100/1106. train loss: 8.6948,	0.6189 s / batch. (data: 1.20e-04). ETA=18:15:17, max mem: 15.9 GB 
[10/26 21:41:54 visual_prompt]: Epoch 4 / 100: avg data time: 4.44e-03, avg batch time: 0.6348, average train loss: 4.6904
[10/26 21:42:44 visual_prompt]: 	Test 100/123. loss: 2.881, 0.2381 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[10/26 21:42:55 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2338, average loss: 2.5904
[10/26 21:42:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.21	
[10/26 21:42:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/26 21:44:00 visual_prompt]: 	Training 100/1106. train loss: 4.6053,	0.6168 s / batch. (data: 3.25e-04). ETA=18:10:25, max mem: 15.9 GB 
[10/26 21:45:03 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6401 s / batch. (data: 8.17e-04). ETA=18:50:37, max mem: 15.9 GB 
[10/26 21:46:06 visual_prompt]: 	Training 300/1106. train loss: 6.8642,	0.6444 s / batch. (data: 7.97e-04). ETA=18:57:11, max mem: 15.9 GB 
[10/26 21:47:09 visual_prompt]: 	Training 400/1106. train loss: 17.3123,	0.6311 s / batch. (data: 7.60e-04). ETA=18:32:34, max mem: 15.9 GB 
[10/26 21:48:13 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6178 s / batch. (data: 3.22e-04). ETA=18:08:11, max mem: 15.9 GB 
[10/26 21:49:16 visual_prompt]: 	Training 600/1106. train loss: 0.9383,	0.6316 s / batch. (data: 8.13e-04). ETA=18:31:24, max mem: 15.9 GB 
[10/26 21:50:19 visual_prompt]: 	Training 700/1106. train loss: 2.8877,	0.6330 s / batch. (data: 8.07e-04). ETA=18:32:46, max mem: 15.9 GB 
[10/26 21:51:22 visual_prompt]: 	Training 800/1106. train loss: 7.0363,	0.6340 s / batch. (data: 8.18e-04). ETA=18:33:27, max mem: 15.9 GB 
[10/26 21:52:26 visual_prompt]: 	Training 900/1106. train loss: 7.1462,	0.6185 s / batch. (data: 3.68e-04). ETA=18:05:13, max mem: 15.9 GB 
[10/26 21:53:29 visual_prompt]: 	Training 1000/1106. train loss: 12.3472,	0.6335 s / batch. (data: 1.02e-02). ETA=18:30:25, max mem: 15.9 GB 
[10/26 21:54:32 visual_prompt]: 	Training 1100/1106. train loss: 5.2504,	0.6178 s / batch. (data: 1.86e-04). ETA=18:01:59, max mem: 15.9 GB 
[10/26 21:54:36 visual_prompt]: Epoch 5 / 100: avg data time: 4.50e-03, avg batch time: 0.6342, average train loss: 5.3152
[10/26 21:55:26 visual_prompt]: 	Test 100/123. loss: 2.436, 0.2246 s / batch. (data: 3.79e-05)max mem: 15.94594 GB 
[10/26 21:55:36 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2334, average loss: 2.1978
[10/26 21:55:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.96	
[10/26 21:55:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/26 21:56:42 visual_prompt]: 	Training 100/1106. train loss: 0.9593,	0.6199 s / batch. (data: 6.16e-04). ETA=18:04:32, max mem: 15.9 GB 
[10/26 21:57:45 visual_prompt]: 	Training 200/1106. train loss: 5.7648,	0.6172 s / batch. (data: 3.32e-04). ETA=17:58:47, max mem: 15.9 GB 
[10/26 21:58:48 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6417 s / batch. (data: 2.92e-04). ETA=18:40:32, max mem: 15.9 GB 
[10/26 21:59:51 visual_prompt]: 	Training 400/1106. train loss: 0.0072,	0.6480 s / batch. (data: 2.72e-04). ETA=18:50:25, max mem: 15.9 GB 
[10/26 22:00:54 visual_prompt]: 	Training 500/1106. train loss: 7.9368,	0.6428 s / batch. (data: 7.78e-04). ETA=18:40:20, max mem: 15.9 GB 
[10/26 22:01:58 visual_prompt]: 	Training 600/1106. train loss: 4.5831,	0.6181 s / batch. (data: 2.86e-04). ETA=17:56:15, max mem: 15.9 GB 
[10/26 22:03:01 visual_prompt]: 	Training 700/1106. train loss: 5.1212,	0.6563 s / batch. (data: 2.44e-02). ETA=19:01:37, max mem: 15.9 GB 
[10/26 22:04:04 visual_prompt]: 	Training 800/1106. train loss: 10.6679,	0.6340 s / batch. (data: 1.15e-03). ETA=18:21:48, max mem: 15.9 GB 
[10/26 22:05:08 visual_prompt]: 	Training 900/1106. train loss: 0.0044,	0.6175 s / batch. (data: 2.79e-04). ETA=17:52:08, max mem: 15.9 GB 
[10/26 22:06:11 visual_prompt]: 	Training 1000/1106. train loss: 8.2713,	0.6352 s / batch. (data: 1.10e-03). ETA=18:21:41, max mem: 15.9 GB 
[10/26 22:07:14 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6171 s / batch. (data: 1.48e-04). ETA=17:49:14, max mem: 15.9 GB 
[10/26 22:07:18 visual_prompt]: Epoch 6 / 100: avg data time: 4.03e-03, avg batch time: 0.6345, average train loss: 6.8313
[10/26 22:08:08 visual_prompt]: 	Test 100/123. loss: 7.071, 0.2255 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/26 22:08:19 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.2331, average loss: 7.9079
[10/26 22:08:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.65	
[10/26 22:08:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/26 22:09:24 visual_prompt]: 	Training 100/1106. train loss: 2.7870,	0.6352 s / batch. (data: 8.06e-04). ETA=18:19:33, max mem: 15.9 GB 
[10/26 22:10:27 visual_prompt]: 	Training 200/1106. train loss: 19.6883,	0.6360 s / batch. (data: 8.34e-04). ETA=18:19:57, max mem: 15.9 GB 
[10/26 22:11:31 visual_prompt]: 	Training 300/1106. train loss: 10.1492,	0.6366 s / batch. (data: 4.56e-04). ETA=18:19:55, max mem: 15.9 GB 
[10/26 22:12:34 visual_prompt]: 	Training 400/1106. train loss: 15.1209,	0.6361 s / batch. (data: 8.42e-04). ETA=18:17:54, max mem: 15.9 GB 
[10/26 22:13:37 visual_prompt]: 	Training 500/1106. train loss: 5.3641,	0.6606 s / batch. (data: 2.46e-02). ETA=18:59:07, max mem: 15.9 GB 
[10/26 22:14:41 visual_prompt]: 	Training 600/1106. train loss: 23.7962,	0.6547 s / batch. (data: 7.83e-04). ETA=18:47:50, max mem: 15.9 GB 
[10/26 22:15:44 visual_prompt]: 	Training 700/1106. train loss: 34.3567,	0.6344 s / batch. (data: 2.85e-04). ETA=18:11:55, max mem: 15.9 GB 
[10/26 22:16:48 visual_prompt]: 	Training 800/1106. train loss: 6.2459,	0.6304 s / batch. (data: 7.94e-04). ETA=18:03:50, max mem: 15.9 GB 
[10/26 22:17:51 visual_prompt]: 	Training 900/1106. train loss: 0.9244,	0.6198 s / batch. (data: 3.15e-04). ETA=17:44:41, max mem: 15.9 GB 
[10/26 22:18:54 visual_prompt]: 	Training 1000/1106. train loss: 0.7322,	0.6201 s / batch. (data: 3.36e-04). ETA=17:44:10, max mem: 15.9 GB 
[10/26 22:19:57 visual_prompt]: 	Training 1100/1106. train loss: 8.6826,	0.6192 s / batch. (data: 1.54e-04). ETA=17:41:35, max mem: 15.9 GB 
[10/26 22:20:01 visual_prompt]: Epoch 7 / 100: avg data time: 4.38e-03, avg batch time: 0.6351, average train loss: 8.3525
[10/26 22:20:51 visual_prompt]: 	Test 100/123. loss: 6.561, 0.2248 s / batch. (data: 2.29e-05)max mem: 15.94594 GB 
[10/26 22:21:02 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.2320, average loss: 5.8857
[10/26 22:21:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.43	
[10/26 22:21:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/26 22:22:07 visual_prompt]: 	Training 100/1106. train loss: 23.4371,	0.6486 s / batch. (data: 8.06e-04). ETA=18:30:52, max mem: 15.9 GB 
[10/26 22:23:10 visual_prompt]: 	Training 200/1106. train loss: 5.3094,	0.6335 s / batch. (data: 7.36e-04). ETA=18:03:54, max mem: 15.9 GB 
[10/26 22:24:13 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6492 s / batch. (data: 2.92e-02). ETA=18:29:38, max mem: 15.9 GB 
[10/26 22:25:16 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6180 s / batch. (data: 2.69e-04). ETA=17:35:15, max mem: 15.9 GB 
[10/26 22:26:20 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6198 s / batch. (data: 7.57e-04). ETA=17:37:22, max mem: 15.9 GB 
[10/26 22:27:23 visual_prompt]: 	Training 600/1106. train loss: 7.4636,	0.6410 s / batch. (data: 7.52e-04). ETA=18:12:24, max mem: 15.9 GB 
[10/26 22:28:26 visual_prompt]: 	Training 700/1106. train loss: 4.0595,	0.6440 s / batch. (data: 8.02e-04). ETA=18:16:31, max mem: 15.9 GB 
[10/26 22:29:30 visual_prompt]: 	Training 800/1106. train loss: 18.1473,	0.6204 s / batch. (data: 3.43e-04). ETA=17:35:16, max mem: 15.9 GB 
[10/26 22:30:33 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6180 s / batch. (data: 2.79e-04). ETA=17:30:14, max mem: 15.9 GB 
[10/26 22:31:36 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6193 s / batch. (data: 3.57e-04). ETA=17:31:19, max mem: 15.9 GB 
[10/26 22:32:39 visual_prompt]: 	Training 1100/1106. train loss: 29.0698,	0.6177 s / batch. (data: 1.46e-04). ETA=17:27:38, max mem: 15.9 GB 
[10/26 22:32:43 visual_prompt]: Epoch 8 / 100: avg data time: 4.44e-03, avg batch time: 0.6341, average train loss: 10.0294
[10/26 22:33:33 visual_prompt]: 	Test 100/123. loss: 5.269, 0.2253 s / batch. (data: 4.24e-05)max mem: 15.94594 GB 
[10/26 22:33:44 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.2322, average loss: 4.6224
[10/26 22:33:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.22	
[10/26 22:33:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/26 22:34:49 visual_prompt]: 	Training 100/1106. train loss: 48.9241,	0.6439 s / batch. (data: 7.92e-04). ETA=18:10:57, max mem: 15.9 GB 
[10/26 22:35:53 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6568 s / batch. (data: 1.61e-02). ETA=18:31:40, max mem: 15.9 GB 
[10/26 22:36:56 visual_prompt]: 	Training 300/1106. train loss: 12.7981,	0.6196 s / batch. (data: 3.32e-04). ETA=17:27:37, max mem: 15.9 GB 
[10/26 22:37:59 visual_prompt]: 	Training 400/1106. train loss: 0.0001,	0.6217 s / batch. (data: 2.98e-04). ETA=17:30:05, max mem: 15.9 GB 
[10/26 22:39:03 visual_prompt]: 	Training 500/1106. train loss: 46.3640,	0.6570 s / batch. (data: 5.41e-03). ETA=18:28:46, max mem: 15.9 GB 
[10/26 22:40:06 visual_prompt]: 	Training 600/1106. train loss: 5.0926,	0.6198 s / batch. (data: 3.47e-04). ETA=17:24:57, max mem: 15.9 GB 
[10/26 22:41:09 visual_prompt]: 	Training 700/1106. train loss: 0.0112,	0.6255 s / batch. (data: 3.25e-04). ETA=17:33:30, max mem: 15.9 GB 
[10/26 22:42:13 visual_prompt]: 	Training 800/1106. train loss: 6.1492,	0.6483 s / batch. (data: 7.67e-04). ETA=18:10:48, max mem: 15.9 GB 
[10/26 22:43:16 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6199 s / batch. (data: 2.75e-04). ETA=17:21:53, max mem: 15.9 GB 
[10/26 22:44:19 visual_prompt]: 	Training 1000/1106. train loss: 8.2232,	0.6188 s / batch. (data: 3.42e-04). ETA=17:19:05, max mem: 15.9 GB 
[10/26 22:45:22 visual_prompt]: 	Training 1100/1106. train loss: 0.8238,	0.6184 s / batch. (data: 1.55e-04). ETA=17:17:19, max mem: 15.9 GB 
[10/26 22:45:26 visual_prompt]: Epoch 9 / 100: avg data time: 4.51e-03, avg batch time: 0.6352, average train loss: 8.3044
[10/26 22:46:16 visual_prompt]: 	Test 100/123. loss: 1.795, 0.2398 s / batch. (data: 2.65e-05)max mem: 15.94594 GB 
[10/26 22:46:27 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2324, average loss: 1.6476
[10/26 22:46:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.82	
[10/26 22:46:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/26 22:47:32 visual_prompt]: 	Training 100/1106. train loss: 9.7729,	0.6307 s / batch. (data: 9.13e-04). ETA=17:36:55, max mem: 15.9 GB 
[10/26 22:48:35 visual_prompt]: 	Training 200/1106. train loss: 3.0215,	0.6337 s / batch. (data: 8.03e-04). ETA=17:40:53, max mem: 15.9 GB 
[10/26 22:49:38 visual_prompt]: 	Training 300/1106. train loss: 13.9722,	0.6178 s / batch. (data: 3.66e-04). ETA=17:13:12, max mem: 15.9 GB 
[10/26 22:50:41 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6237 s / batch. (data: 3.23e-04). ETA=17:22:03, max mem: 15.9 GB 
[10/26 22:51:45 visual_prompt]: 	Training 500/1106. train loss: 3.8702,	0.6239 s / batch. (data: 7.88e-04). ETA=17:21:18, max mem: 15.9 GB 
[10/26 22:52:48 visual_prompt]: 	Training 600/1106. train loss: 20.9963,	0.6200 s / batch. (data: 7.87e-04). ETA=17:13:49, max mem: 15.9 GB 
[10/26 22:53:51 visual_prompt]: 	Training 700/1106. train loss: 19.0608,	0.6465 s / batch. (data: 1.19e-03). ETA=17:56:53, max mem: 15.9 GB 
[10/26 22:54:54 visual_prompt]: 	Training 800/1106. train loss: 6.4079,	0.6457 s / batch. (data: 8.14e-04). ETA=17:54:34, max mem: 15.9 GB 
[10/26 22:55:57 visual_prompt]: 	Training 900/1106. train loss: 29.9512,	0.6345 s / batch. (data: 3.31e-04). ETA=17:34:49, max mem: 15.9 GB 
[10/26 22:57:01 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6480 s / batch. (data: 3.53e-04). ETA=17:56:10, max mem: 15.9 GB 
[10/26 22:58:04 visual_prompt]: 	Training 1100/1106. train loss: 30.0188,	0.6198 s / batch. (data: 1.57e-04). ETA=17:08:13, max mem: 15.9 GB 
[10/26 22:58:08 visual_prompt]: Epoch 10 / 100: avg data time: 3.80e-03, avg batch time: 0.6340, average train loss: 11.8149
[10/26 22:58:57 visual_prompt]: 	Test 100/123. loss: 2.174, 0.2317 s / batch. (data: 3.29e-05)max mem: 15.94594 GB 
[10/26 22:59:08 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.2345, average loss: 2.3893
[10/26 22:59:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.82	
[10/26 22:59:08 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/26 23:00:13 visual_prompt]: 	Training 100/1106. train loss: 40.7207,	0.6240 s / batch. (data: 6.54e-03). ETA=17:14:10, max mem: 15.9 GB 
[10/26 23:01:17 visual_prompt]: 	Training 200/1106. train loss: 18.8185,	0.6331 s / batch. (data: 1.56e-02). ETA=17:28:10, max mem: 15.9 GB 
[10/26 23:02:20 visual_prompt]: 	Training 300/1106. train loss: 22.4502,	0.6367 s / batch. (data: 4.12e-04). ETA=17:33:08, max mem: 15.9 GB 
[10/26 23:03:23 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6193 s / batch. (data: 3.10e-04). ETA=17:03:21, max mem: 15.9 GB 
[10/26 23:04:27 visual_prompt]: 	Training 500/1106. train loss: 1.1518,	0.6348 s / batch. (data: 8.07e-04). ETA=17:27:49, max mem: 15.9 GB 
[10/26 23:05:30 visual_prompt]: 	Training 600/1106. train loss: 22.1839,	0.6324 s / batch. (data: 7.44e-04). ETA=17:22:45, max mem: 15.9 GB 
[10/26 23:06:33 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6282 s / batch. (data: 3.20e-04). ETA=17:14:54, max mem: 15.9 GB 
[10/26 23:07:36 visual_prompt]: 	Training 800/1106. train loss: 26.5476,	0.6325 s / batch. (data: 5.45e-03). ETA=17:20:52, max mem: 15.9 GB 
[10/26 23:08:40 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6314 s / batch. (data: 3.27e-04). ETA=17:18:05, max mem: 15.9 GB 
[10/26 23:09:43 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6482 s / batch. (data: 4.74e-04). ETA=17:44:37, max mem: 15.9 GB 
[10/26 23:10:47 visual_prompt]: 	Training 1100/1106. train loss: 16.7167,	0.6184 s / batch. (data: 1.43e-04). ETA=16:54:31, max mem: 15.9 GB 
[10/26 23:10:50 visual_prompt]: Epoch 11 / 100: avg data time: 4.55e-03, avg batch time: 0.6348, average train loss: 10.5820
[10/26 23:11:40 visual_prompt]: 	Test 100/123. loss: 6.856, 0.2606 s / batch. (data: 4.17e-05)max mem: 15.94594 GB 
[10/26 23:11:51 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.2332, average loss: 7.6152
[10/26 23:11:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.50	
[10/26 23:11:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/26 23:12:57 visual_prompt]: 	Training 100/1106. train loss: 0.1089,	0.6189 s / batch. (data: 3.07e-04). ETA=16:54:18, max mem: 15.9 GB 
[10/26 23:14:00 visual_prompt]: 	Training 200/1106. train loss: 1.5787,	0.6176 s / batch. (data: 3.16e-04). ETA=16:51:06, max mem: 15.9 GB 
[10/26 23:15:04 visual_prompt]: 	Training 300/1106. train loss: 7.7152,	0.6189 s / batch. (data: 3.21e-04). ETA=16:52:16, max mem: 15.9 GB 
[10/26 23:16:07 visual_prompt]: 	Training 400/1106. train loss: 13.2553,	0.6297 s / batch. (data: 3.37e-04). ETA=17:08:47, max mem: 15.9 GB 
[10/26 23:17:10 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6369 s / batch. (data: 3.18e-04). ETA=17:19:33, max mem: 15.9 GB 
[10/26 23:18:13 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6558 s / batch. (data: 3.78e-02). ETA=17:49:22, max mem: 15.9 GB 
[10/26 23:19:17 visual_prompt]: 	Training 700/1106. train loss: 15.2578,	0.6338 s / batch. (data: 7.41e-04). ETA=17:12:27, max mem: 15.9 GB 
[10/26 23:20:20 visual_prompt]: 	Training 800/1106. train loss: 8.4210,	0.6363 s / batch. (data: 8.08e-04). ETA=17:15:27, max mem: 15.9 GB 
[10/26 23:21:23 visual_prompt]: 	Training 900/1106. train loss: 0.0008,	0.6575 s / batch. (data: 3.74e-02). ETA=17:48:45, max mem: 15.9 GB 
[10/26 23:22:26 visual_prompt]: 	Training 1000/1106. train loss: 24.0467,	0.6335 s / batch. (data: 8.44e-04). ETA=17:08:43, max mem: 15.9 GB 
[10/26 23:23:29 visual_prompt]: 	Training 1100/1106. train loss: 36.1319,	0.6170 s / batch. (data: 1.64e-04). ETA=16:40:55, max mem: 15.9 GB 
[10/26 23:23:33 visual_prompt]: Epoch 12 / 100: avg data time: 5.15e-03, avg batch time: 0.6352, average train loss: 12.3450
[10/26 23:24:23 visual_prompt]: 	Test 100/123. loss: 10.566, 0.2597 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/26 23:24:33 visual_prompt]: Inference (val):avg data time: 1.82e-04, avg batch time: 0.2332, average loss: 12.2842
[10/26 23:24:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.69	
[10/26 23:24:33 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/26 23:25:39 visual_prompt]: 	Training 100/1106. train loss: 24.9922,	0.6308 s / batch. (data: 8.37e-04). ETA=17:02:09, max mem: 15.9 GB 
[10/26 23:26:42 visual_prompt]: 	Training 200/1106. train loss: 2.4116,	0.6287 s / batch. (data: 8.79e-04). ETA=16:57:41, max mem: 15.9 GB 
[10/26 23:27:46 visual_prompt]: 	Training 300/1106. train loss: 16.1945,	0.6320 s / batch. (data: 7.88e-04). ETA=17:02:02, max mem: 15.9 GB 
[10/26 23:28:49 visual_prompt]: 	Training 400/1106. train loss: 27.8538,	0.6191 s / batch. (data: 3.10e-04). ETA=16:40:11, max mem: 15.9 GB 
[10/26 23:29:52 visual_prompt]: 	Training 500/1106. train loss: 5.9866,	0.6445 s / batch. (data: 8.21e-04). ETA=17:20:09, max mem: 15.9 GB 
[10/26 23:30:55 visual_prompt]: 	Training 600/1106. train loss: 31.2488,	0.6240 s / batch. (data: 3.01e-04). ETA=16:45:55, max mem: 15.9 GB 
[10/26 23:31:58 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6293 s / batch. (data: 7.98e-04). ETA=16:53:31, max mem: 15.9 GB 
[10/26 23:33:02 visual_prompt]: 	Training 800/1106. train loss: 3.9760,	0.6327 s / batch. (data: 7.88e-04). ETA=16:57:51, max mem: 15.9 GB 
[10/26 23:34:05 visual_prompt]: 	Training 900/1106. train loss: 63.8122,	0.6224 s / batch. (data: 3.08e-04). ETA=16:40:12, max mem: 15.9 GB 
[10/26 23:35:09 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6326 s / batch. (data: 8.77e-04). ETA=16:55:35, max mem: 15.9 GB 
[10/26 23:36:12 visual_prompt]: 	Training 1100/1106. train loss: 5.4534,	0.6176 s / batch. (data: 1.72e-04). ETA=16:30:34, max mem: 15.9 GB 
[10/26 23:36:16 visual_prompt]: Epoch 13 / 100: avg data time: 4.46e-03, avg batch time: 0.6347, average train loss: 12.4943
[10/26 23:37:06 visual_prompt]: 	Test 100/123. loss: 5.178, 0.2251 s / batch. (data: 5.17e-05)max mem: 15.94594 GB 
[10/26 23:37:16 visual_prompt]: Inference (val):avg data time: 6.32e-05, avg batch time: 0.2327, average loss: 4.4655
[10/26 23:37:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.87	
[10/26 23:37:16 visual_prompt]: Best epoch 13: best metric: -4.465
[10/26 23:37:16 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/26 23:38:21 visual_prompt]: 	Training 100/1106. train loss: 20.7541,	0.6206 s / batch. (data: 3.18e-04). ETA=16:34:08, max mem: 15.9 GB 
[10/26 23:39:24 visual_prompt]: 	Training 200/1106. train loss: 16.5252,	0.6320 s / batch. (data: 2.99e-04). ETA=16:51:24, max mem: 15.9 GB 
[10/26 23:40:28 visual_prompt]: 	Training 300/1106. train loss: 18.3737,	0.6416 s / batch. (data: 8.25e-04). ETA=17:05:46, max mem: 15.9 GB 
[10/26 23:41:31 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6344 s / batch. (data: 8.14e-04). ETA=16:53:13, max mem: 15.9 GB 
[10/26 23:42:35 visual_prompt]: 	Training 500/1106. train loss: 39.8599,	0.6272 s / batch. (data: 8.03e-04). ETA=16:40:36, max mem: 15.9 GB 
[10/26 23:43:38 visual_prompt]: 	Training 600/1106. train loss: 11.1342,	0.6205 s / batch. (data: 3.09e-04). ETA=16:28:49, max mem: 15.9 GB 
[10/26 23:44:41 visual_prompt]: 	Training 700/1106. train loss: 9.1466,	0.6560 s / batch. (data: 1.20e-02). ETA=17:24:22, max mem: 15.9 GB 
[10/26 23:45:44 visual_prompt]: 	Training 800/1106. train loss: 11.3006,	0.6405 s / batch. (data: 8.19e-04). ETA=16:58:36, max mem: 15.9 GB 
[10/26 23:46:48 visual_prompt]: 	Training 900/1106. train loss: 4.5914,	0.6711 s / batch. (data: 4.01e-02). ETA=17:46:06, max mem: 15.9 GB 
[10/26 23:47:51 visual_prompt]: 	Training 1000/1106. train loss: 3.8522,	0.6371 s / batch. (data: 7.74e-04). ETA=16:51:06, max mem: 15.9 GB 
[10/26 23:48:54 visual_prompt]: 	Training 1100/1106. train loss: 5.0480,	0.6172 s / batch. (data: 1.54e-04). ETA=16:18:29, max mem: 15.9 GB 
[10/26 23:48:58 visual_prompt]: Epoch 14 / 100: avg data time: 4.62e-03, avg batch time: 0.6349, average train loss: 11.5581
[10/26 23:49:48 visual_prompt]: 	Test 100/123. loss: 7.023, 0.2398 s / batch. (data: 2.57e-05)max mem: 15.94594 GB 
[10/26 23:49:58 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2338, average loss: 5.7012
[10/26 23:49:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.38	
[10/26 23:49:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/26 23:51:03 visual_prompt]: 	Training 100/1106. train loss: 9.3110,	0.6289 s / batch. (data: 3.77e-04). ETA=16:35:54, max mem: 15.9 GB 
[10/26 23:52:07 visual_prompt]: 	Training 200/1106. train loss: 24.2537,	0.6444 s / batch. (data: 2.64e-04). ETA=16:59:20, max mem: 15.9 GB 
[10/26 23:53:10 visual_prompt]: 	Training 300/1106. train loss: 38.1092,	0.6395 s / batch. (data: 2.56e-02). ETA=16:50:33, max mem: 15.9 GB 
[10/26 23:54:13 visual_prompt]: 	Training 400/1106. train loss: 47.3247,	0.6443 s / batch. (data: 8.22e-04). ETA=16:57:01, max mem: 15.9 GB 
[10/26 23:55:16 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6184 s / batch. (data: 2.92e-04). ETA=16:15:06, max mem: 15.9 GB 
[10/26 23:56:19 visual_prompt]: 	Training 600/1106. train loss: 3.7887,	0.6179 s / batch. (data: 3.32e-04). ETA=16:13:19, max mem: 15.9 GB 
[10/26 23:57:23 visual_prompt]: 	Training 700/1106. train loss: 17.6780,	0.6306 s / batch. (data: 8.57e-04). ETA=16:32:21, max mem: 15.9 GB 
[10/26 23:58:26 visual_prompt]: 	Training 800/1106. train loss: 5.9260,	0.6326 s / batch. (data: 8.35e-04). ETA=16:34:20, max mem: 15.9 GB 
[10/26 23:59:29 visual_prompt]: 	Training 900/1106. train loss: 14.6693,	0.6420 s / batch. (data: 8.28e-04). ETA=16:48:04, max mem: 15.9 GB 
[10/27 00:00:32 visual_prompt]: 	Training 1000/1106. train loss: 0.2747,	0.6277 s / batch. (data: 1.05e-02). ETA=16:24:41, max mem: 15.9 GB 
[10/27 00:01:36 visual_prompt]: 	Training 1100/1106. train loss: 1.8503,	0.6188 s / batch. (data: 1.50e-04). ETA=16:09:37, max mem: 15.9 GB 
[10/27 00:01:39 visual_prompt]: Epoch 15 / 100: avg data time: 3.98e-03, avg batch time: 0.6338, average train loss: 12.4964
[10/27 00:02:29 visual_prompt]: 	Test 100/123. loss: 0.817, 0.2401 s / batch. (data: 2.79e-05)max mem: 15.94594 GB 
[10/27 00:02:40 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.2322, average loss: 1.4218
[10/27 00:02:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.71	
[10/27 00:02:40 visual_prompt]: Best epoch 15: best metric: -1.422
[10/27 00:02:40 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/27 00:03:45 visual_prompt]: 	Training 100/1106. train loss: 0.0015,	0.6203 s / batch. (data: 3.14e-04). ETA=16:10:51, max mem: 15.9 GB 
[10/27 00:04:48 visual_prompt]: 	Training 200/1106. train loss: 8.3479,	0.6335 s / batch. (data: 7.65e-04). ETA=16:30:32, max mem: 15.9 GB 
[10/27 00:05:51 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6185 s / batch. (data: 3.25e-04). ETA=16:05:55, max mem: 15.9 GB 
[10/27 00:06:55 visual_prompt]: 	Training 400/1106. train loss: 22.8033,	0.6335 s / batch. (data: 3.19e-04). ETA=16:28:22, max mem: 15.9 GB 
[10/27 00:07:58 visual_prompt]: 	Training 500/1106. train loss: 15.4100,	0.6421 s / batch. (data: 8.12e-04). ETA=16:40:43, max mem: 15.9 GB 
[10/27 00:09:01 visual_prompt]: 	Training 600/1106. train loss: 0.2851,	0.6340 s / batch. (data: 7.42e-04). ETA=16:27:03, max mem: 15.9 GB 
[10/27 00:10:05 visual_prompt]: 	Training 700/1106. train loss: 1.8581,	0.6188 s / batch. (data: 3.21e-04). ETA=16:02:23, max mem: 15.9 GB 
[10/27 00:11:08 visual_prompt]: 	Training 800/1106. train loss: 32.3208,	0.6213 s / batch. (data: 3.06e-04). ETA=16:05:14, max mem: 15.9 GB 
[10/27 00:12:11 visual_prompt]: 	Training 900/1106. train loss: 2.5852,	0.6197 s / batch. (data: 2.95e-04). ETA=16:01:41, max mem: 15.9 GB 
[10/27 00:13:14 visual_prompt]: 	Training 1000/1106. train loss: 27.6189,	0.6200 s / batch. (data: 3.58e-04). ETA=16:01:05, max mem: 15.9 GB 
[10/27 00:14:18 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6179 s / batch. (data: 1.48e-04). ETA=15:56:45, max mem: 15.9 GB 
[10/27 00:14:22 visual_prompt]: Epoch 16 / 100: avg data time: 4.08e-03, avg batch time: 0.6348, average train loss: 11.0848
[10/27 00:15:12 visual_prompt]: 	Test 100/123. loss: 2.837, 0.2397 s / batch. (data: 4.96e-05)max mem: 15.94594 GB 
[10/27 00:15:22 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.2336, average loss: 2.4090
[10/27 00:15:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.03	
[10/27 00:15:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/27 00:16:27 visual_prompt]: 	Training 100/1106. train loss: 8.5249,	0.6174 s / batch. (data: 3.37e-04). ETA=15:54:59, max mem: 15.9 GB 
[10/27 00:17:31 visual_prompt]: 	Training 200/1106. train loss: 2.0513,	0.6204 s / batch. (data: 3.09e-04). ETA=15:58:32, max mem: 15.9 GB 
[10/27 00:18:34 visual_prompt]: 	Training 300/1106. train loss: 53.4327,	0.6189 s / batch. (data: 2.94e-04). ETA=15:55:09, max mem: 15.9 GB 
[10/27 00:19:37 visual_prompt]: 	Training 400/1106. train loss: 53.5722,	0.6395 s / batch. (data: 2.18e-02). ETA=16:25:55, max mem: 15.9 GB 
[10/27 00:20:40 visual_prompt]: 	Training 500/1106. train loss: 10.3943,	0.6330 s / batch. (data: 8.17e-04). ETA=16:14:48, max mem: 15.9 GB 
[10/27 00:21:44 visual_prompt]: 	Training 600/1106. train loss: 4.2692,	0.6548 s / batch. (data: 7.28e-04). ETA=16:47:18, max mem: 15.9 GB 
[10/27 00:22:47 visual_prompt]: 	Training 700/1106. train loss: 8.1130,	0.6185 s / batch. (data: 3.27e-04). ETA=15:50:31, max mem: 15.9 GB 
[10/27 00:23:50 visual_prompt]: 	Training 800/1106. train loss: 27.6566,	0.6371 s / batch. (data: 7.66e-04). ETA=16:18:00, max mem: 15.9 GB 
[10/27 00:24:53 visual_prompt]: 	Training 900/1106. train loss: 65.4630,	0.6184 s / batch. (data: 2.99e-04). ETA=15:48:19, max mem: 15.9 GB 
[10/27 00:25:56 visual_prompt]: 	Training 1000/1106. train loss: 0.0005,	0.6192 s / batch. (data: 3.20e-04). ETA=15:48:27, max mem: 15.9 GB 
[10/27 00:27:00 visual_prompt]: 	Training 1100/1106. train loss: 0.4512,	0.6187 s / batch. (data: 1.35e-04). ETA=15:46:39, max mem: 15.9 GB 
[10/27 00:27:04 visual_prompt]: Epoch 17 / 100: avg data time: 4.27e-03, avg batch time: 0.6342, average train loss: 9.8608
[10/27 00:27:53 visual_prompt]: 	Test 100/123. loss: 3.049, 0.2255 s / batch. (data: 2.55e-05)max mem: 15.94594 GB 
[10/27 00:28:04 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2336, average loss: 2.7568
[10/27 00:28:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.76	
[10/27 00:28:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/27 00:29:09 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6433 s / batch. (data: 8.07e-04). ETA=16:23:07, max mem: 15.9 GB 
[10/27 00:30:12 visual_prompt]: 	Training 200/1106. train loss: 43.0543,	0.6188 s / batch. (data: 3.03e-04). ETA=15:44:39, max mem: 15.9 GB 
[10/27 00:31:16 visual_prompt]: 	Training 300/1106. train loss: 88.1431,	0.6650 s / batch. (data: 1.11e-02). ETA=16:54:03, max mem: 15.9 GB 
[10/27 00:32:19 visual_prompt]: 	Training 400/1106. train loss: 22.3566,	0.6365 s / batch. (data: 5.51e-03). ETA=16:09:30, max mem: 15.9 GB 
[10/27 00:33:22 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6419 s / batch. (data: 7.75e-04). ETA=16:16:45, max mem: 15.9 GB 
[10/27 00:34:25 visual_prompt]: 	Training 600/1106. train loss: 8.7219,	0.6193 s / batch. (data: 4.34e-04). ETA=15:41:15, max mem: 15.9 GB 
[10/27 00:35:29 visual_prompt]: 	Training 700/1106. train loss: 5.8418,	0.6329 s / batch. (data: 7.44e-04). ETA=16:00:55, max mem: 15.9 GB 
[10/27 00:36:32 visual_prompt]: 	Training 800/1106. train loss: 0.7779,	0.6243 s / batch. (data: 3.23e-04). ETA=15:46:47, max mem: 15.9 GB 
[10/27 00:37:35 visual_prompt]: 	Training 900/1106. train loss: 61.7379,	0.6242 s / batch. (data: 1.20e-02). ETA=15:45:36, max mem: 15.9 GB 
[10/27 00:38:39 visual_prompt]: 	Training 1000/1106. train loss: 10.4577,	0.6378 s / batch. (data: 3.29e-04). ETA=16:05:12, max mem: 15.9 GB 
[10/27 00:39:42 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6180 s / batch. (data: 1.64e-04). ETA=15:34:09, max mem: 15.9 GB 
[10/27 00:39:46 visual_prompt]: Epoch 18 / 100: avg data time: 4.15e-03, avg batch time: 0.6344, average train loss: 14.2513
[10/27 00:40:35 visual_prompt]: 	Test 100/123. loss: 2.720, 0.2269 s / batch. (data: 5.98e-05)max mem: 15.94594 GB 
[10/27 00:40:46 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2327, average loss: 5.7191
[10/27 00:40:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.01	
[10/27 00:40:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/27 00:41:52 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6323 s / batch. (data: 8.57e-04). ETA=15:54:39, max mem: 15.9 GB 
[10/27 00:42:55 visual_prompt]: 	Training 200/1106. train loss: 22.1447,	0.6202 s / batch. (data: 4.95e-04). ETA=15:35:22, max mem: 15.9 GB 
[10/27 00:43:58 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6520 s / batch. (data: 8.08e-04). ETA=16:22:19, max mem: 15.9 GB 
[10/27 00:45:01 visual_prompt]: 	Training 400/1106. train loss: 0.4974,	0.6267 s / batch. (data: 3.37e-04). ETA=15:43:01, max mem: 15.9 GB 
[10/27 00:46:05 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6383 s / batch. (data: 5.44e-03). ETA=15:59:30, max mem: 15.9 GB 
[10/27 00:47:08 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6354 s / batch. (data: 8.05e-04). ETA=15:54:03, max mem: 15.9 GB 
[10/27 00:48:12 visual_prompt]: 	Training 700/1106. train loss: 0.7711,	0.6208 s / batch. (data: 3.03e-04). ETA=15:31:08, max mem: 15.9 GB 
[10/27 00:49:15 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6297 s / batch. (data: 3.12e-04). ETA=15:43:23, max mem: 15.9 GB 
[10/27 00:50:19 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6199 s / batch. (data: 3.23e-04). ETA=15:27:39, max mem: 15.9 GB 
[10/27 00:51:22 visual_prompt]: 	Training 1000/1106. train loss: 0.0023,	0.6246 s / batch. (data: 3.15e-04). ETA=15:33:42, max mem: 15.9 GB 
[10/27 00:52:25 visual_prompt]: 	Training 1100/1106. train loss: 87.4418,	0.6182 s / batch. (data: 1.80e-04). ETA=15:23:04, max mem: 15.9 GB 
[10/27 00:52:29 visual_prompt]: Epoch 19 / 100: avg data time: 4.46e-03, avg batch time: 0.6352, average train loss: 10.1634
[10/27 00:53:18 visual_prompt]: 	Test 100/123. loss: 52.239, 0.2365 s / batch. (data: 4.27e-05)max mem: 15.94594 GB 
[10/27 00:53:29 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2326, average loss: 46.9895
[10/27 00:53:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.09	
[10/27 00:53:29 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[10/27 00:54:35 visual_prompt]: 	Training 100/1106. train loss: 11.6234,	0.6377 s / batch. (data: 7.99e-04). ETA=15:51:04, max mem: 15.9 GB 
[10/27 00:55:38 visual_prompt]: 	Training 200/1106. train loss: 0.5271,	0.6197 s / batch. (data: 3.07e-04). ETA=15:23:08, max mem: 15.9 GB 
[10/27 00:56:41 visual_prompt]: 	Training 300/1106. train loss: 14.1256,	0.6435 s / batch. (data: 1.10e-02). ETA=15:57:39, max mem: 15.9 GB 
[10/27 00:57:44 visual_prompt]: 	Training 400/1106. train loss: 9.9792,	0.6554 s / batch. (data: 7.93e-04). ETA=16:14:09, max mem: 15.9 GB 
[10/27 00:58:48 visual_prompt]: 	Training 500/1106. train loss: 1.1852,	0.6499 s / batch. (data: 2.58e-02). ETA=16:04:53, max mem: 15.9 GB 
[10/27 00:59:51 visual_prompt]: 	Training 600/1106. train loss: 18.2086,	0.6370 s / batch. (data: 7.69e-04). ETA=15:44:45, max mem: 15.9 GB 
[10/27 01:00:55 visual_prompt]: 	Training 700/1106. train loss: 0.0279,	0.6337 s / batch. (data: 8.08e-04). ETA=15:38:46, max mem: 15.9 GB 
[10/27 01:01:58 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6188 s / batch. (data: 2.95e-04). ETA=15:15:42, max mem: 15.9 GB 
[10/27 01:03:01 visual_prompt]: 	Training 900/1106. train loss: 19.4735,	0.6412 s / batch. (data: 7.56e-04). ETA=15:47:48, max mem: 15.9 GB 
[10/27 01:04:05 visual_prompt]: 	Training 1000/1106. train loss: 19.4077,	0.6192 s / batch. (data: 3.29e-04). ETA=15:14:10, max mem: 15.9 GB 
[10/27 01:05:08 visual_prompt]: 	Training 1100/1106. train loss: 8.4691,	0.6188 s / batch. (data: 1.40e-04). ETA=15:12:33, max mem: 15.9 GB 
[10/27 01:05:12 visual_prompt]: Epoch 20 / 100: avg data time: 4.27e-03, avg batch time: 0.6355, average train loss: 10.2896
[10/27 01:06:01 visual_prompt]: 	Test 100/123. loss: 14.971, 0.2343 s / batch. (data: 3.60e-05)max mem: 15.94594 GB 
[10/27 01:06:12 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2327, average loss: 18.6304
[10/27 01:06:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.41	
[10/27 01:06:12 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[10/27 01:07:18 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6520 s / batch. (data: 8.05e-04). ETA=16:00:22, max mem: 15.9 GB 
[10/27 01:08:21 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6331 s / batch. (data: 3.36e-04). ETA=15:31:26, max mem: 15.9 GB 
[10/27 01:09:25 visual_prompt]: 	Training 300/1106. train loss: 42.7570,	0.6343 s / batch. (data: 8.21e-04). ETA=15:32:10, max mem: 15.9 GB 
[10/27 01:10:28 visual_prompt]: 	Training 400/1106. train loss: 5.5799,	0.6258 s / batch. (data: 2.82e-04). ETA=15:18:44, max mem: 15.9 GB 
[10/27 01:11:31 visual_prompt]: 	Training 500/1106. train loss: 82.4984,	0.6383 s / batch. (data: 3.33e-04). ETA=15:35:57, max mem: 15.9 GB 
[10/27 01:12:34 visual_prompt]: 	Training 600/1106. train loss: 45.8656,	0.6458 s / batch. (data: 8.28e-04). ETA=15:45:48, max mem: 15.9 GB 
[10/27 01:13:38 visual_prompt]: 	Training 700/1106. train loss: 12.6973,	0.6330 s / batch. (data: 8.00e-04). ETA=15:26:01, max mem: 15.9 GB 
[10/27 01:14:41 visual_prompt]: 	Training 800/1106. train loss: 10.9417,	0.6335 s / batch. (data: 7.81e-04). ETA=15:25:45, max mem: 15.9 GB 
[10/27 01:15:44 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6099 s / batch. (data: 2.90e-04). ETA=14:50:10, max mem: 15.9 GB 
[10/27 01:16:48 visual_prompt]: 	Training 1000/1106. train loss: 4.8186,	0.6352 s / batch. (data: 8.00e-04). ETA=15:26:09, max mem: 15.9 GB 
[10/27 01:17:51 visual_prompt]: 	Training 1100/1106. train loss: 36.8933,	0.6193 s / batch. (data: 1.23e-04). ETA=15:01:50, max mem: 15.9 GB 
[10/27 01:17:54 visual_prompt]: Epoch 21 / 100: avg data time: 4.53e-03, avg batch time: 0.6348, average train loss: 11.4531
[10/27 01:18:44 visual_prompt]: 	Test 100/123. loss: 15.406, 0.2390 s / batch. (data: 3.55e-05)max mem: 15.94594 GB 
[10/27 01:18:55 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2343, average loss: 14.1350
[10/27 01:18:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.20	
[10/27 01:18:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[10/27 01:20:00 visual_prompt]: 	Training 100/1106. train loss: 9.6052,	0.6418 s / batch. (data: 8.13e-04). ETA=15:33:29, max mem: 15.9 GB 
[10/27 01:21:03 visual_prompt]: 	Training 200/1106. train loss: 19.1789,	0.6262 s / batch. (data: 2.93e-04). ETA=15:09:47, max mem: 15.9 GB 
[10/27 01:22:06 visual_prompt]: 	Training 300/1106. train loss: 8.5921,	0.6315 s / batch. (data: 3.36e-04). ETA=15:16:26, max mem: 15.9 GB 
[10/27 01:23:09 visual_prompt]: 	Training 400/1106. train loss: 19.5180,	0.6335 s / batch. (data: 7.67e-04). ETA=15:18:18, max mem: 15.9 GB 
[10/27 01:24:12 visual_prompt]: 	Training 500/1106. train loss: 1.0184,	0.6348 s / batch. (data: 7.52e-04). ETA=15:19:08, max mem: 15.9 GB 
[10/27 01:25:16 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6458 s / batch. (data: 8.00e-04). ETA=15:33:54, max mem: 15.9 GB 
[10/27 01:26:19 visual_prompt]: 	Training 700/1106. train loss: 2.2714,	0.6210 s / batch. (data: 3.08e-03). ETA=14:57:06, max mem: 15.9 GB 
[10/27 01:27:22 visual_prompt]: 	Training 800/1106. train loss: 15.5681,	0.6315 s / batch. (data: 7.85e-04). ETA=15:11:09, max mem: 15.9 GB 
[10/27 01:28:26 visual_prompt]: 	Training 900/1106. train loss: 2.7214,	0.6340 s / batch. (data: 7.73e-04). ETA=15:13:46, max mem: 15.9 GB 
[10/27 01:29:29 visual_prompt]: 	Training 1000/1106. train loss: 30.2000,	0.6688 s / batch. (data: 5.87e-03). ETA=16:02:44, max mem: 15.9 GB 
[10/27 01:30:32 visual_prompt]: 	Training 1100/1106. train loss: 11.6935,	0.6180 s / batch. (data: 1.49e-04). ETA=14:48:39, max mem: 15.9 GB 
[10/27 01:30:36 visual_prompt]: Epoch 22 / 100: avg data time: 4.21e-03, avg batch time: 0.6339, average train loss: 9.8192
[10/27 01:31:26 visual_prompt]: 	Test 100/123. loss: 3.380, 0.2252 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/27 01:31:36 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2334, average loss: 3.1955
[10/27 01:31:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.99	
[10/27 01:31:36 visual_prompt]: Stopping early.
[10/27 01:31:36 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 01:31:36 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 01:31:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 01:31:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 01:31:36 visual_prompt]: Training with config:
[10/27 01:31:36 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr2.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 01:31:36 visual_prompt]: Loading training data...
[10/27 01:31:36 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 01:31:36 visual_prompt]: Loading validation data...
[10/27 01:31:36 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 01:31:37 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/27 01:31:39 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/27 01:31:39 visual_prompt]: tuned percent:0.522
[10/27 01:31:39 visual_prompt]: Device used for model: 0
[10/27 01:31:39 visual_prompt]: Setting up Evaluator...
[10/27 01:31:39 visual_prompt]: Setting up Trainer...
[10/27 01:31:39 visual_prompt]: 	Setting up the optimizer...
[10/27 01:31:39 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 01:32:45 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6267 s / batch. (data: 8.06e-04). ETA=19:14:10, max mem: 15.9 GB 
[10/27 01:33:48 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6438 s / batch. (data: 7.40e-04). ETA=19:44:31, max mem: 15.9 GB 
[10/27 01:34:51 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6325 s / batch. (data: 4.59e-04). ETA=19:22:46, max mem: 15.9 GB 
[10/27 01:35:54 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6330 s / batch. (data: 8.28e-04). ETA=19:22:41, max mem: 15.9 GB 
[10/27 01:36:58 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6346 s / batch. (data: 1.60e-02). ETA=19:24:33, max mem: 15.9 GB 
[10/27 01:38:01 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6470 s / batch. (data: 8.10e-04). ETA=19:46:12, max mem: 15.9 GB 
[10/27 01:39:05 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6399 s / batch. (data: 7.19e-04). ETA=19:32:09, max mem: 15.9 GB 
[10/27 01:40:08 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6199 s / batch. (data: 7.99e-04). ETA=18:54:23, max mem: 15.9 GB 
[10/27 01:41:11 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6327 s / batch. (data: 8.23e-04). ETA=19:16:46, max mem: 15.9 GB 
[10/27 01:42:15 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6191 s / batch. (data: 3.23e-04). ETA=18:50:50, max mem: 15.9 GB 
[10/27 01:43:18 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6185 s / batch. (data: 1.60e-04). ETA=18:48:40, max mem: 15.9 GB 
[10/27 01:43:22 visual_prompt]: Epoch 1 / 100: avg data time: 4.63e-03, avg batch time: 0.6352, average train loss: 1.4028
[10/27 01:44:11 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2253 s / batch. (data: 2.34e-05)max mem: 15.94594 GB 
[10/27 01:44:22 visual_prompt]: Inference (val):avg data time: 9.92e-05, avg batch time: 0.2325, average loss: 1.3505
[10/27 01:44:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/27 01:44:22 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/27 01:45:27 visual_prompt]: 	Training 100/1106. train loss: 2.9742,	0.6336 s / batch. (data: 3.42e-04). ETA=19:15:16, max mem: 15.9 GB 
[10/27 01:46:30 visual_prompt]: 	Training 200/1106. train loss: 0.8049,	0.6327 s / batch. (data: 1.13e-03). ETA=19:12:28, max mem: 15.9 GB 
[10/27 01:47:33 visual_prompt]: 	Training 300/1106. train loss: 0.5079,	0.6193 s / batch. (data: 3.21e-04). ETA=18:47:04, max mem: 15.9 GB 
[10/27 01:48:36 visual_prompt]: 	Training 400/1106. train loss: 0.0153,	0.6327 s / batch. (data: 8.27e-04). ETA=19:10:27, max mem: 15.9 GB 
[10/27 01:49:40 visual_prompt]: 	Training 500/1106. train loss: 0.6611,	0.6190 s / batch. (data: 2.87e-04). ETA=18:44:26, max mem: 15.9 GB 
[10/27 01:50:43 visual_prompt]: 	Training 600/1106. train loss: 0.9000,	0.6219 s / batch. (data: 3.16e-04). ETA=18:48:36, max mem: 15.9 GB 
[10/27 01:51:46 visual_prompt]: 	Training 700/1106. train loss: 0.2135,	0.6420 s / batch. (data: 7.67e-04). ETA=19:24:08, max mem: 15.9 GB 
[10/27 01:52:49 visual_prompt]: 	Training 800/1106. train loss: 0.8143,	0.6540 s / batch. (data: 1.09e-02). ETA=19:44:41, max mem: 15.9 GB 
[10/27 01:53:52 visual_prompt]: 	Training 900/1106. train loss: 0.8883,	0.6457 s / batch. (data: 8.04e-04). ETA=19:28:40, max mem: 15.9 GB 
[10/27 01:54:56 visual_prompt]: 	Training 1000/1106. train loss: 0.1286,	0.6239 s / batch. (data: 6.74e-04). ETA=18:48:04, max mem: 15.9 GB 
[10/27 01:55:59 visual_prompt]: 	Training 1100/1106. train loss: 0.0207,	0.6192 s / batch. (data: 1.35e-04). ETA=18:38:41, max mem: 15.9 GB 
[10/27 01:56:03 visual_prompt]: Epoch 2 / 100: avg data time: 3.71e-03, avg batch time: 0.6334, average train loss: 1.4602
[10/27 01:56:52 visual_prompt]: 	Test 100/123. loss: 4.363, 0.2265 s / batch. (data: 3.77e-05)max mem: 15.94594 GB 
[10/27 01:57:03 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.2314, average loss: 3.9423
[10/27 01:57:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.40	
[10/27 01:57:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/27 01:58:09 visual_prompt]: 	Training 100/1106. train loss: 0.0045,	0.6384 s / batch. (data: 3.06e-04). ETA=19:12:10, max mem: 15.9 GB 
[10/27 01:59:12 visual_prompt]: 	Training 200/1106. train loss: 0.6154,	0.6335 s / batch. (data: 9.51e-04). ETA=19:02:21, max mem: 15.9 GB 
[10/27 02:00:16 visual_prompt]: 	Training 300/1106. train loss: 0.0001,	0.6480 s / batch. (data: 8.35e-04). ETA=19:27:19, max mem: 15.9 GB 
[10/27 02:01:19 visual_prompt]: 	Training 400/1106. train loss: 3.0260,	0.6272 s / batch. (data: 3.10e-04). ETA=18:48:45, max mem: 15.9 GB 
[10/27 02:02:22 visual_prompt]: 	Training 500/1106. train loss: 10.5997,	0.6187 s / batch. (data: 3.52e-04). ETA=18:32:26, max mem: 15.9 GB 
[10/27 02:03:25 visual_prompt]: 	Training 600/1106. train loss: 0.0005,	0.6184 s / batch. (data: 3.36e-04). ETA=18:30:52, max mem: 15.9 GB 
[10/27 02:04:28 visual_prompt]: 	Training 700/1106. train loss: 0.6747,	0.6202 s / batch. (data: 3.34e-04). ETA=18:33:02, max mem: 15.9 GB 
[10/27 02:05:32 visual_prompt]: 	Training 800/1106. train loss: 0.0007,	0.6314 s / batch. (data: 1.31e-02). ETA=18:52:11, max mem: 15.9 GB 
[10/27 02:06:35 visual_prompt]: 	Training 900/1106. train loss: 13.8787,	0.6321 s / batch. (data: 3.20e-04). ETA=18:52:20, max mem: 15.9 GB 
[10/27 02:07:38 visual_prompt]: 	Training 1000/1106. train loss: 4.7876,	0.6240 s / batch. (data: 2.90e-04). ETA=18:36:51, max mem: 15.9 GB 
[10/27 02:08:41 visual_prompt]: 	Training 1100/1106. train loss: 0.9167,	0.6174 s / batch. (data: 1.58e-04). ETA=18:23:58, max mem: 15.9 GB 
[10/27 02:08:45 visual_prompt]: Epoch 3 / 100: avg data time: 4.92e-03, avg batch time: 0.6347, average train loss: 2.5146
[10/27 02:09:35 visual_prompt]: 	Test 100/123. loss: 1.558, 0.2251 s / batch. (data: 3.81e-05)max mem: 15.94594 GB 
[10/27 02:09:46 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.2332, average loss: 1.7018
[10/27 02:09:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.87	
[10/27 02:09:46 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/27 02:10:51 visual_prompt]: 	Training 100/1106. train loss: 8.1872,	0.6364 s / batch. (data: 8.10e-04). ETA=18:56:52, max mem: 15.9 GB 
[10/27 02:11:55 visual_prompt]: 	Training 200/1106. train loss: 8.0367,	0.6400 s / batch. (data: 1.26e-02). ETA=19:02:14, max mem: 15.9 GB 
[10/27 02:12:58 visual_prompt]: 	Training 300/1106. train loss: 0.7537,	0.6202 s / batch. (data: 3.26e-04). ETA=18:25:49, max mem: 15.9 GB 
[10/27 02:14:01 visual_prompt]: 	Training 400/1106. train loss: 2.8654,	0.6506 s / batch. (data: 8.68e-04). ETA=19:19:00, max mem: 15.9 GB 
[10/27 02:15:05 visual_prompt]: 	Training 500/1106. train loss: 0.0142,	0.6217 s / batch. (data: 3.28e-04). ETA=18:26:23, max mem: 15.9 GB 
[10/27 02:16:08 visual_prompt]: 	Training 600/1106. train loss: 0.5451,	0.6189 s / batch. (data: 3.19e-04). ETA=18:20:25, max mem: 15.9 GB 
[10/27 02:17:11 visual_prompt]: 	Training 700/1106. train loss: 1.7296,	0.6615 s / batch. (data: 3.76e-02). ETA=19:35:04, max mem: 15.9 GB 
[10/27 02:18:14 visual_prompt]: 	Training 800/1106. train loss: 0.2465,	0.6363 s / batch. (data: 8.06e-04). ETA=18:49:18, max mem: 15.9 GB 
[10/27 02:19:18 visual_prompt]: 	Training 900/1106. train loss: 0.0003,	0.6394 s / batch. (data: 7.76e-04). ETA=18:53:37, max mem: 15.9 GB 
[10/27 02:20:21 visual_prompt]: 	Training 1000/1106. train loss: 0.0008,	0.6281 s / batch. (data: 3.14e-04). ETA=18:32:31, max mem: 15.9 GB 
[10/27 02:21:24 visual_prompt]: 	Training 1100/1106. train loss: 37.4068,	0.6184 s / batch. (data: 1.36e-04). ETA=18:14:20, max mem: 15.9 GB 
[10/27 02:21:28 visual_prompt]: Epoch 4 / 100: avg data time: 4.47e-03, avg batch time: 0.6350, average train loss: 3.4685
[10/27 02:22:18 visual_prompt]: 	Test 100/123. loss: 0.683, 0.2259 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/27 02:22:29 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2329, average loss: 0.6929
[10/27 02:22:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.33	
[10/27 02:22:29 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/27 02:23:34 visual_prompt]: 	Training 100/1106. train loss: 5.3210,	0.6224 s / batch. (data: 3.19e-04). ETA=18:20:25, max mem: 15.9 GB 
[10/27 02:24:37 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6446 s / batch. (data: 3.64e-04). ETA=18:58:27, max mem: 15.9 GB 
[10/27 02:25:40 visual_prompt]: 	Training 300/1106. train loss: 6.0390,	0.6325 s / batch. (data: 7.92e-04). ETA=18:36:11, max mem: 15.9 GB 
[10/27 02:26:44 visual_prompt]: 	Training 400/1106. train loss: 10.6478,	0.6274 s / batch. (data: 7.40e-04). ETA=18:26:09, max mem: 15.9 GB 
[10/27 02:27:47 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6182 s / batch. (data: 3.17e-04). ETA=18:08:51, max mem: 15.9 GB 
[10/27 02:28:50 visual_prompt]: 	Training 600/1106. train loss: 0.0044,	0.6445 s / batch. (data: 3.72e-04). ETA=18:54:05, max mem: 15.9 GB 
[10/27 02:29:54 visual_prompt]: 	Training 700/1106. train loss: 2.4008,	0.6413 s / batch. (data: 8.11e-04). ETA=18:47:26, max mem: 15.9 GB 
[10/27 02:30:57 visual_prompt]: 	Training 800/1106. train loss: 3.9534,	0.6324 s / batch. (data: 7.81e-04). ETA=18:30:42, max mem: 15.9 GB 
[10/27 02:32:00 visual_prompt]: 	Training 900/1106. train loss: 3.7375,	0.6276 s / batch. (data: 3.04e-04). ETA=18:21:13, max mem: 15.9 GB 
[10/27 02:33:04 visual_prompt]: 	Training 1000/1106. train loss: 0.2883,	0.6280 s / batch. (data: 3.37e-04). ETA=18:20:53, max mem: 15.9 GB 
[10/27 02:34:07 visual_prompt]: 	Training 1100/1106. train loss: 11.9201,	0.6175 s / batch. (data: 2.01e-04). ETA=18:01:21, max mem: 15.9 GB 
[10/27 02:34:11 visual_prompt]: Epoch 5 / 100: avg data time: 4.02e-03, avg batch time: 0.6349, average train loss: 4.4533
[10/27 02:35:01 visual_prompt]: 	Test 100/123. loss: 10.272, 0.2268 s / batch. (data: 3.62e-05)max mem: 15.94594 GB 
[10/27 02:35:11 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2325, average loss: 9.1337
[10/27 02:35:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.33	
[10/27 02:35:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/27 02:36:16 visual_prompt]: 	Training 100/1106. train loss: 5.1942,	0.6400 s / batch. (data: 7.56e-04). ETA=18:39:44, max mem: 15.9 GB 
[10/27 02:37:20 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6314 s / batch. (data: 8.19e-04). ETA=18:23:34, max mem: 15.9 GB 
[10/27 02:38:23 visual_prompt]: 	Training 300/1106. train loss: 0.1961,	0.6307 s / batch. (data: 3.36e-04). ETA=18:21:22, max mem: 15.9 GB 
[10/27 02:39:26 visual_prompt]: 	Training 400/1106. train loss: 1.0339,	0.6326 s / batch. (data: 3.30e-04). ETA=18:23:31, max mem: 15.9 GB 
[10/27 02:40:29 visual_prompt]: 	Training 500/1106. train loss: 11.3748,	0.6453 s / batch. (data: 1.07e-02). ETA=18:44:37, max mem: 15.9 GB 
[10/27 02:41:33 visual_prompt]: 	Training 600/1106. train loss: 9.2050,	0.6389 s / batch. (data: 8.00e-04). ETA=18:32:21, max mem: 15.9 GB 
[10/27 02:42:36 visual_prompt]: 	Training 700/1106. train loss: 11.5119,	0.6360 s / batch. (data: 3.18e-04). ETA=18:26:17, max mem: 15.9 GB 
[10/27 02:43:40 visual_prompt]: 	Training 800/1106. train loss: 22.1658,	0.6230 s / batch. (data: 3.32e-04). ETA=18:02:42, max mem: 15.9 GB 
[10/27 02:44:43 visual_prompt]: 	Training 900/1106. train loss: 74.7403,	0.6196 s / batch. (data: 2.72e-04). ETA=17:55:44, max mem: 15.9 GB 
[10/27 02:45:46 visual_prompt]: 	Training 1000/1106. train loss: 2.8018,	0.6327 s / batch. (data: 2.93e-04). ETA=18:17:28, max mem: 15.9 GB 
[10/27 02:46:49 visual_prompt]: 	Training 1100/1106. train loss: 19.9914,	0.6195 s / batch. (data: 1.48e-04). ETA=17:53:30, max mem: 15.9 GB 
[10/27 02:46:53 visual_prompt]: Epoch 6 / 100: avg data time: 4.31e-03, avg batch time: 0.6347, average train loss: 6.7977
[10/27 02:47:43 visual_prompt]: 	Test 100/123. loss: 13.583, 0.2255 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/27 02:47:54 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.2339, average loss: 12.5074
[10/27 02:47:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.00	
[10/27 02:47:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/27 02:48:59 visual_prompt]: 	Training 100/1106. train loss: 3.1026,	0.6428 s / batch. (data: 8.23e-04). ETA=18:32:42, max mem: 15.9 GB 
[10/27 02:50:02 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6194 s / batch. (data: 3.24e-04). ETA=17:51:06, max mem: 15.9 GB 
[10/27 02:51:06 visual_prompt]: 	Training 300/1106. train loss: 20.5672,	0.6357 s / batch. (data: 1.20e-02). ETA=18:18:22, max mem: 15.9 GB 
[10/27 02:52:09 visual_prompt]: 	Training 400/1106. train loss: 21.4848,	0.6460 s / batch. (data: 7.76e-04). ETA=18:35:00, max mem: 15.9 GB 
[10/27 02:53:12 visual_prompt]: 	Training 500/1106. train loss: 10.5967,	0.6246 s / batch. (data: 2.74e-04). ETA=17:57:05, max mem: 15.9 GB 
[10/27 02:54:16 visual_prompt]: 	Training 600/1106. train loss: 4.1439,	0.6326 s / batch. (data: 3.20e-04). ETA=18:09:44, max mem: 15.9 GB 
[10/27 02:55:19 visual_prompt]: 	Training 700/1106. train loss: 74.2262,	0.6350 s / batch. (data: 3.10e-04). ETA=18:12:50, max mem: 15.9 GB 
[10/27 02:56:22 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6307 s / batch. (data: 7.89e-04). ETA=18:04:30, max mem: 15.9 GB 
[10/27 02:57:26 visual_prompt]: 	Training 900/1106. train loss: 4.0774,	0.6491 s / batch. (data: 7.55e-04). ETA=18:34:58, max mem: 15.9 GB 
[10/27 02:58:29 visual_prompt]: 	Training 1000/1106. train loss: 6.4378,	0.6307 s / batch. (data: 1.25e-02). ETA=18:02:22, max mem: 15.9 GB 
[10/27 02:59:32 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6184 s / batch. (data: 1.59e-04). ETA=17:40:11, max mem: 15.9 GB 
[10/27 02:59:36 visual_prompt]: Epoch 7 / 100: avg data time: 4.22e-03, avg batch time: 0.6350, average train loss: 7.4361
[10/27 03:00:26 visual_prompt]: 	Test 100/123. loss: 4.804, 0.2255 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/27 03:00:37 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2337, average loss: 5.2427
[10/27 03:00:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.86	
[10/27 03:00:37 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/27 03:01:42 visual_prompt]: 	Training 100/1106. train loss: 13.4735,	0.6478 s / batch. (data: 8.51e-04). ETA=18:29:28, max mem: 15.9 GB 
[10/27 03:02:45 visual_prompt]: 	Training 200/1106. train loss: 18.7954,	0.6323 s / batch. (data: 2.70e-04). ETA=18:01:47, max mem: 15.9 GB 
[10/27 03:03:48 visual_prompt]: 	Training 300/1106. train loss: 0.2908,	0.6687 s / batch. (data: 7.53e-04). ETA=19:03:00, max mem: 15.9 GB 
[10/27 03:04:51 visual_prompt]: 	Training 400/1106. train loss: 5.1876,	0.6183 s / batch. (data: 2.50e-04). ETA=17:35:48, max mem: 15.9 GB 
[10/27 03:05:55 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6456 s / batch. (data: 8.45e-04). ETA=18:21:19, max mem: 15.9 GB 
[10/27 03:06:58 visual_prompt]: 	Training 600/1106. train loss: 0.0039,	0.6184 s / batch. (data: 3.30e-04). ETA=17:33:56, max mem: 15.9 GB 
[10/27 03:08:02 visual_prompt]: 	Training 700/1106. train loss: 7.1443,	0.6208 s / batch. (data: 2.85e-04). ETA=17:36:57, max mem: 15.9 GB 
[10/27 03:09:05 visual_prompt]: 	Training 800/1106. train loss: 5.7254,	0.6251 s / batch. (data: 2.98e-04). ETA=17:43:14, max mem: 15.9 GB 
[10/27 03:10:08 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6194 s / batch. (data: 3.59e-04). ETA=17:32:29, max mem: 15.9 GB 
[10/27 03:11:12 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6372 s / batch. (data: 8.25e-04). ETA=18:01:45, max mem: 15.9 GB 
[10/27 03:12:15 visual_prompt]: 	Training 1100/1106. train loss: 4.1378,	0.6177 s / batch. (data: 1.44e-04). ETA=17:27:31, max mem: 15.9 GB 
[10/27 03:12:19 visual_prompt]: Epoch 8 / 100: avg data time: 4.30e-03, avg batch time: 0.6349, average train loss: 8.9994
[10/27 03:13:08 visual_prompt]: 	Test 100/123. loss: 15.516, 0.2392 s / batch. (data: 3.96e-05)max mem: 15.94594 GB 
[10/27 03:13:19 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2322, average loss: 14.0120
[10/27 03:13:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.58	
[10/27 03:13:19 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/27 03:14:25 visual_prompt]: 	Training 100/1106. train loss: 2.3152,	0.6399 s / batch. (data: 7.56e-04). ETA=18:04:03, max mem: 15.9 GB 
[10/27 03:15:28 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6400 s / batch. (data: 3.92e-04). ETA=18:03:11, max mem: 15.9 GB 
[10/27 03:16:32 visual_prompt]: 	Training 300/1106. train loss: 20.3032,	0.6183 s / batch. (data: 3.26e-04). ETA=17:25:32, max mem: 15.9 GB 
[10/27 03:17:35 visual_prompt]: 	Training 400/1106. train loss: 0.0010,	0.6399 s / batch. (data: 7.44e-04). ETA=18:00:50, max mem: 15.9 GB 
[10/27 03:18:38 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6627 s / batch. (data: 1.10e-02). ETA=18:38:20, max mem: 15.9 GB 
[10/27 03:19:42 visual_prompt]: 	Training 600/1106. train loss: 23.9055,	0.6318 s / batch. (data: 7.25e-04). ETA=17:45:05, max mem: 15.9 GB 
[10/27 03:20:45 visual_prompt]: 	Training 700/1106. train loss: 24.9212,	0.6583 s / batch. (data: 7.54e-04). ETA=18:28:43, max mem: 15.9 GB 
[10/27 03:21:48 visual_prompt]: 	Training 800/1106. train loss: 7.4262,	0.6345 s / batch. (data: 3.29e-04). ETA=17:47:38, max mem: 15.9 GB 
[10/27 03:22:52 visual_prompt]: 	Training 900/1106. train loss: 16.5248,	0.6474 s / batch. (data: 7.59e-04). ETA=18:08:06, max mem: 15.9 GB 
[10/27 03:23:55 visual_prompt]: 	Training 1000/1106. train loss: 1.0547,	0.6244 s / batch. (data: 2.89e-04). ETA=17:28:28, max mem: 15.9 GB 
[10/27 03:24:58 visual_prompt]: 	Training 1100/1106. train loss: 3.7102,	0.6187 s / batch. (data: 1.48e-04). ETA=17:17:57, max mem: 15.9 GB 
[10/27 03:25:02 visual_prompt]: Epoch 9 / 100: avg data time: 4.60e-03, avg batch time: 0.6353, average train loss: 9.0963
[10/27 03:25:52 visual_prompt]: 	Test 100/123. loss: 0.992, 0.2328 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/27 03:26:02 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2325, average loss: 1.0594
[10/27 03:26:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.43	
[10/27 03:26:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/27 03:27:08 visual_prompt]: 	Training 100/1106. train loss: 8.2549,	0.6335 s / batch. (data: 3.10e-04). ETA=17:41:40, max mem: 15.9 GB 
[10/27 03:28:11 visual_prompt]: 	Training 200/1106. train loss: 5.2279,	0.6200 s / batch. (data: 3.58e-04). ETA=17:17:56, max mem: 15.9 GB 
[10/27 03:29:14 visual_prompt]: 	Training 300/1106. train loss: 6.4772,	0.6769 s / batch. (data: 1.60e-02). ETA=18:52:08, max mem: 15.9 GB 
[10/27 03:30:18 visual_prompt]: 	Training 400/1106. train loss: 0.0845,	0.6422 s / batch. (data: 3.35e-04). ETA=17:52:59, max mem: 15.9 GB 
[10/27 03:31:21 visual_prompt]: 	Training 500/1106. train loss: 12.6011,	0.6193 s / batch. (data: 3.31e-04). ETA=17:13:39, max mem: 15.9 GB 
[10/27 03:32:24 visual_prompt]: 	Training 600/1106. train loss: 44.3891,	0.6247 s / batch. (data: 3.16e-04). ETA=17:21:39, max mem: 15.9 GB 
[10/27 03:33:27 visual_prompt]: 	Training 700/1106. train loss: 46.3123,	0.6353 s / batch. (data: 8.17e-04). ETA=17:38:20, max mem: 15.9 GB 
[10/27 03:34:31 visual_prompt]: 	Training 800/1106. train loss: 1.1411,	0.6473 s / batch. (data: 7.82e-04). ETA=17:57:08, max mem: 15.9 GB 
[10/27 03:35:34 visual_prompt]: 	Training 900/1106. train loss: 6.2662,	0.6192 s / batch. (data: 3.01e-04). ETA=17:09:19, max mem: 15.9 GB 
[10/27 03:36:37 visual_prompt]: 	Training 1000/1106. train loss: 1.1316,	0.6520 s / batch. (data: 7.67e-04). ETA=18:02:51, max mem: 15.9 GB 
[10/27 03:37:40 visual_prompt]: 	Training 1100/1106. train loss: 8.9318,	0.6189 s / batch. (data: 1.46e-04). ETA=17:06:47, max mem: 15.9 GB 
[10/27 03:37:44 visual_prompt]: Epoch 10 / 100: avg data time: 4.07e-03, avg batch time: 0.6344, average train loss: 10.6282
[10/27 03:38:34 visual_prompt]: 	Test 100/123. loss: 13.070, 0.2440 s / batch. (data: 2.67e-05)max mem: 15.94594 GB 
[10/27 03:38:45 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2322, average loss: 14.3768
[10/27 03:38:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.46	
[10/27 03:38:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/27 03:39:50 visual_prompt]: 	Training 100/1106. train loss: 20.5499,	0.6293 s / batch. (data: 2.61e-04). ETA=17:22:56, max mem: 15.9 GB 
[10/27 03:40:53 visual_prompt]: 	Training 200/1106. train loss: 2.5411,	0.6453 s / batch. (data: 1.02e-03). ETA=17:48:20, max mem: 15.9 GB 
[10/27 03:41:56 visual_prompt]: 	Training 300/1106. train loss: 8.9446,	0.6177 s / batch. (data: 2.96e-04). ETA=17:01:44, max mem: 15.9 GB 
[10/27 03:43:00 visual_prompt]: 	Training 400/1106. train loss: 14.1444,	0.6187 s / batch. (data: 3.28e-04). ETA=17:02:19, max mem: 15.9 GB 
[10/27 03:44:03 visual_prompt]: 	Training 500/1106. train loss: 1.7636,	0.6237 s / batch. (data: 2.79e-04). ETA=17:09:35, max mem: 15.9 GB 
[10/27 03:45:06 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6403 s / batch. (data: 7.58e-04). ETA=17:35:52, max mem: 15.9 GB 
[10/27 03:46:09 visual_prompt]: 	Training 700/1106. train loss: 12.7443,	0.6319 s / batch. (data: 7.53e-04). ETA=17:20:56, max mem: 15.9 GB 
[10/27 03:47:13 visual_prompt]: 	Training 800/1106. train loss: 4.3300,	0.6428 s / batch. (data: 1.08e-02). ETA=17:37:46, max mem: 15.9 GB 
[10/27 03:48:16 visual_prompt]: 	Training 900/1106. train loss: 7.6764,	0.6187 s / batch. (data: 3.24e-04). ETA=16:57:11, max mem: 15.9 GB 
[10/27 03:49:19 visual_prompt]: 	Training 1000/1106. train loss: 21.0953,	0.6457 s / batch. (data: 5.94e-03). ETA=17:40:30, max mem: 15.9 GB 
[10/27 03:50:23 visual_prompt]: 	Training 1100/1106. train loss: 14.8702,	0.6184 s / batch. (data: 1.38e-04). ETA=16:54:39, max mem: 15.9 GB 
[10/27 03:50:26 visual_prompt]: Epoch 11 / 100: avg data time: 4.77e-03, avg batch time: 0.6345, average train loss: 12.3429
[10/27 03:51:16 visual_prompt]: 	Test 100/123. loss: 3.327, 0.2254 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/27 03:51:27 visual_prompt]: Inference (val):avg data time: 1.15e-04, avg batch time: 0.2330, average loss: 2.9916
[10/27 03:51:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.01	
[10/27 03:51:27 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/27 03:52:33 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6178 s / batch. (data: 3.56e-04). ETA=16:52:30, max mem: 15.9 GB 
[10/27 03:53:36 visual_prompt]: 	Training 200/1106. train loss: 8.0998,	0.6331 s / batch. (data: 8.23e-04). ETA=17:16:29, max mem: 15.9 GB 
[10/27 03:54:40 visual_prompt]: 	Training 300/1106. train loss: 14.1542,	0.6305 s / batch. (data: 1.05e-02). ETA=17:11:16, max mem: 15.9 GB 
[10/27 03:55:43 visual_prompt]: 	Training 400/1106. train loss: 8.0783,	0.6339 s / batch. (data: 8.13e-04). ETA=17:15:41, max mem: 15.9 GB 
[10/27 03:56:46 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6575 s / batch. (data: 3.59e-02). ETA=17:53:14, max mem: 15.9 GB 
[10/27 03:57:49 visual_prompt]: 	Training 600/1106. train loss: 26.4689,	0.6334 s / batch. (data: 7.43e-04). ETA=17:12:49, max mem: 15.9 GB 
[10/27 03:58:53 visual_prompt]: 	Training 700/1106. train loss: 0.6849,	0.6181 s / batch. (data: 2.59e-04). ETA=16:46:50, max mem: 15.9 GB 
[10/27 03:59:56 visual_prompt]: 	Training 800/1106. train loss: 4.4414,	0.6233 s / batch. (data: 2.96e-04). ETA=16:54:10, max mem: 15.9 GB 
[10/27 04:00:59 visual_prompt]: 	Training 900/1106. train loss: 8.7255,	0.6199 s / batch. (data: 2.73e-04). ETA=16:47:37, max mem: 15.9 GB 
[10/27 04:02:02 visual_prompt]: 	Training 1000/1106. train loss: 4.3542,	0.6459 s / batch. (data: 7.97e-04). ETA=17:28:50, max mem: 15.9 GB 
[10/27 04:03:06 visual_prompt]: 	Training 1100/1106. train loss: 15.8185,	0.6180 s / batch. (data: 1.69e-04). ETA=16:42:33, max mem: 15.9 GB 
[10/27 04:03:10 visual_prompt]: Epoch 12 / 100: avg data time: 5.19e-03, avg batch time: 0.6355, average train loss: 11.7110
[10/27 04:03:59 visual_prompt]: 	Test 100/123. loss: 0.812, 0.2357 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[10/27 04:04:10 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2319, average loss: 0.8447
[10/27 04:04:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.54	
[10/27 04:04:10 visual_prompt]: Best epoch 12: best metric: -0.845
[10/27 04:04:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/27 04:05:16 visual_prompt]: 	Training 100/1106. train loss: 0.0011,	0.6185 s / batch. (data: 2.93e-04). ETA=16:42:17, max mem: 15.9 GB 
[10/27 04:06:19 visual_prompt]: 	Training 200/1106. train loss: 10.5958,	0.6326 s / batch. (data: 7.18e-04). ETA=17:04:07, max mem: 15.9 GB 
[10/27 04:07:22 visual_prompt]: 	Training 300/1106. train loss: 0.0010,	0.6173 s / batch. (data: 3.14e-04). ETA=16:38:12, max mem: 15.9 GB 
[10/27 04:08:26 visual_prompt]: 	Training 400/1106. train loss: 39.9962,	0.6183 s / batch. (data: 3.36e-04). ETA=16:38:52, max mem: 15.9 GB 
[10/27 04:09:29 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6465 s / batch. (data: 7.82e-04). ETA=17:23:18, max mem: 15.9 GB 
[10/27 04:10:32 visual_prompt]: 	Training 600/1106. train loss: 16.2679,	0.6369 s / batch. (data: 3.54e-04). ETA=17:06:46, max mem: 15.9 GB 
[10/27 04:11:35 visual_prompt]: 	Training 700/1106. train loss: 0.1161,	0.6488 s / batch. (data: 3.03e-04). ETA=17:24:54, max mem: 15.9 GB 
[10/27 04:12:39 visual_prompt]: 	Training 800/1106. train loss: 4.6360,	0.6312 s / batch. (data: 8.12e-04). ETA=16:55:25, max mem: 15.9 GB 
[10/27 04:13:42 visual_prompt]: 	Training 900/1106. train loss: 20.3119,	0.6401 s / batch. (data: 1.31e-02). ETA=17:08:44, max mem: 15.9 GB 
[10/27 04:14:45 visual_prompt]: 	Training 1000/1106. train loss: 34.0342,	0.6340 s / batch. (data: 7.92e-04). ETA=16:57:48, max mem: 15.9 GB 
[10/27 04:15:49 visual_prompt]: 	Training 1100/1106. train loss: 1.1937,	0.6188 s / batch. (data: 1.83e-04). ETA=16:32:29, max mem: 15.9 GB 
[10/27 04:15:52 visual_prompt]: Epoch 13 / 100: avg data time: 4.59e-03, avg batch time: 0.6348, average train loss: 11.8067
[10/27 04:16:42 visual_prompt]: 	Test 100/123. loss: 5.014, 0.2328 s / batch. (data: 4.32e-05)max mem: 15.94594 GB 
[10/27 04:16:53 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2320, average loss: 4.4734
[10/27 04:16:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.83	
[10/27 04:16:53 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/27 04:17:58 visual_prompt]: 	Training 100/1106. train loss: 17.0890,	0.6331 s / batch. (data: 8.55e-04). ETA=16:54:15, max mem: 15.9 GB 
[10/27 04:19:02 visual_prompt]: 	Training 200/1106. train loss: 14.9710,	0.6317 s / batch. (data: 4.22e-04). ETA=16:50:57, max mem: 15.9 GB 
[10/27 04:20:05 visual_prompt]: 	Training 300/1106. train loss: 11.0876,	0.6326 s / batch. (data: 7.80e-04). ETA=16:51:16, max mem: 15.9 GB 
[10/27 04:21:08 visual_prompt]: 	Training 400/1106. train loss: 3.4856,	0.6290 s / batch. (data: 1.05e-02). ETA=16:44:28, max mem: 15.9 GB 
[10/27 04:22:11 visual_prompt]: 	Training 500/1106. train loss: 21.2571,	0.6303 s / batch. (data: 2.69e-04). ETA=16:45:29, max mem: 15.9 GB 
[10/27 04:23:15 visual_prompt]: 	Training 600/1106. train loss: 0.8340,	0.6184 s / batch. (data: 3.34e-04). ETA=16:25:34, max mem: 15.9 GB 
[10/27 04:24:18 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6408 s / batch. (data: 3.35e-04). ETA=17:00:09, max mem: 15.9 GB 
[10/27 04:25:21 visual_prompt]: 	Training 800/1106. train loss: 16.5158,	0.6336 s / batch. (data: 8.33e-04). ETA=16:47:43, max mem: 15.9 GB 
[10/27 04:26:25 visual_prompt]: 	Training 900/1106. train loss: 38.4431,	0.6382 s / batch. (data: 1.20e-02). ETA=16:53:57, max mem: 15.9 GB 
[10/27 04:27:28 visual_prompt]: 	Training 1000/1106. train loss: 29.7752,	0.6462 s / batch. (data: 7.48e-04). ETA=17:05:28, max mem: 15.9 GB 
[10/27 04:28:31 visual_prompt]: 	Training 1100/1106. train loss: 31.9229,	0.6177 s / batch. (data: 1.55e-04). ETA=16:19:21, max mem: 15.9 GB 
[10/27 04:28:35 visual_prompt]: Epoch 14 / 100: avg data time: 4.74e-03, avg batch time: 0.6348, average train loss: 12.8460
[10/27 04:29:25 visual_prompt]: 	Test 100/123. loss: 20.866, 0.2257 s / batch. (data: 2.72e-05)max mem: 15.94594 GB 
[10/27 04:29:35 visual_prompt]: Inference (val):avg data time: 9.82e-05, avg batch time: 0.2311, average loss: 18.8959
[10/27 04:29:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.89	
[10/27 04:29:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/27 04:30:40 visual_prompt]: 	Training 100/1106. train loss: 20.3286,	0.6238 s / batch. (data: 3.15e-04). ETA=16:27:53, max mem: 15.9 GB 
[10/27 04:31:44 visual_prompt]: 	Training 200/1106. train loss: 28.1964,	0.6457 s / batch. (data: 8.20e-04). ETA=17:01:22, max mem: 15.9 GB 
[10/27 04:32:47 visual_prompt]: 	Training 300/1106. train loss: 3.2146,	0.6483 s / batch. (data: 1.44e-02). ETA=17:04:31, max mem: 15.9 GB 
[10/27 04:33:50 visual_prompt]: 	Training 400/1106. train loss: 6.4058,	0.6450 s / batch. (data: 8.20e-04). ETA=16:58:15, max mem: 15.9 GB 
[10/27 04:34:53 visual_prompt]: 	Training 500/1106. train loss: 0.0002,	0.6480 s / batch. (data: 7.48e-04). ETA=17:01:52, max mem: 15.9 GB 
[10/27 04:35:57 visual_prompt]: 	Training 600/1106. train loss: 15.4949,	0.6291 s / batch. (data: 3.25e-04). ETA=16:31:03, max mem: 15.9 GB 
[10/27 04:37:00 visual_prompt]: 	Training 700/1106. train loss: 41.4693,	0.6344 s / batch. (data: 3.76e-04). ETA=16:38:21, max mem: 15.9 GB 
[10/27 04:38:03 visual_prompt]: 	Training 800/1106. train loss: 9.8037,	0.6488 s / batch. (data: 7.69e-04). ETA=16:59:54, max mem: 15.9 GB 
[10/27 04:39:06 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6327 s / batch. (data: 3.12e-04). ETA=16:33:29, max mem: 15.9 GB 
[10/27 04:40:09 visual_prompt]: 	Training 1000/1106. train loss: 9.9461,	0.6184 s / batch. (data: 2.70e-04). ETA=16:10:00, max mem: 15.9 GB 
[10/27 04:41:13 visual_prompt]: 	Training 1100/1106. train loss: 7.9463,	0.6193 s / batch. (data: 1.46e-04). ETA=16:10:22, max mem: 15.9 GB 
[10/27 04:41:16 visual_prompt]: Epoch 15 / 100: avg data time: 3.93e-03, avg batch time: 0.6338, average train loss: 11.9255
[10/27 04:42:06 visual_prompt]: 	Test 100/123. loss: 13.304, 0.2358 s / batch. (data: 2.57e-05)max mem: 15.94594 GB 
[10/27 04:42:17 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2328, average loss: 11.5427
[10/27 04:42:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.62	
[10/27 04:42:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/27 04:43:22 visual_prompt]: 	Training 100/1106. train loss: 0.0001,	0.6197 s / batch. (data: 2.71e-04). ETA=16:09:59, max mem: 15.9 GB 
[10/27 04:44:25 visual_prompt]: 	Training 200/1106. train loss: 3.5377,	0.6305 s / batch. (data: 7.99e-04). ETA=16:25:46, max mem: 15.9 GB 
[10/27 04:45:28 visual_prompt]: 	Training 300/1106. train loss: 30.8482,	0.6185 s / batch. (data: 3.18e-04). ETA=16:05:57, max mem: 15.9 GB 
[10/27 04:46:32 visual_prompt]: 	Training 400/1106. train loss: 5.2006,	0.6245 s / batch. (data: 3.41e-04). ETA=16:14:19, max mem: 15.9 GB 
[10/27 04:47:35 visual_prompt]: 	Training 500/1106. train loss: 12.7163,	0.6340 s / batch. (data: 7.44e-04). ETA=16:28:07, max mem: 15.9 GB 
[10/27 04:48:38 visual_prompt]: 	Training 600/1106. train loss: 0.0018,	0.6300 s / batch. (data: 7.74e-04). ETA=16:20:52, max mem: 15.9 GB 
[10/27 04:49:41 visual_prompt]: 	Training 700/1106. train loss: 5.6852,	0.6207 s / batch. (data: 3.12e-04). ETA=16:05:18, max mem: 15.9 GB 
[10/27 04:50:45 visual_prompt]: 	Training 800/1106. train loss: 6.7616,	0.6179 s / batch. (data: 2.50e-04). ETA=15:59:56, max mem: 15.9 GB 
[10/27 04:51:48 visual_prompt]: 	Training 900/1106. train loss: 9.8445,	0.6206 s / batch. (data: 3.16e-04). ETA=16:03:06, max mem: 15.9 GB 
[10/27 04:52:51 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6465 s / batch. (data: 7.59e-04). ETA=16:42:15, max mem: 15.9 GB 
[10/27 04:53:54 visual_prompt]: 	Training 1100/1106. train loss: 19.9441,	0.6175 s / batch. (data: 1.59e-04). ETA=15:56:14, max mem: 15.9 GB 
[10/27 04:53:58 visual_prompt]: Epoch 16 / 100: avg data time: 4.32e-03, avg batch time: 0.6340, average train loss: 10.9717
[10/27 04:54:48 visual_prompt]: 	Test 100/123. loss: 5.673, 0.2246 s / batch. (data: 3.98e-05)max mem: 15.94594 GB 
[10/27 04:54:59 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.2318, average loss: 6.1354
[10/27 04:54:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.38	
[10/27 04:54:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/27 04:56:04 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6379 s / batch. (data: 8.09e-04). ETA=16:26:38, max mem: 15.9 GB 
[10/27 04:57:07 visual_prompt]: 	Training 200/1106. train loss: 5.3876,	0.6231 s / batch. (data: 2.94e-04). ETA=16:02:43, max mem: 15.9 GB 
[10/27 04:58:11 visual_prompt]: 	Training 300/1106. train loss: 37.2599,	0.6178 s / batch. (data: 3.42e-04). ETA=15:53:31, max mem: 15.9 GB 
[10/27 04:59:14 visual_prompt]: 	Training 400/1106. train loss: 30.2444,	0.6535 s / batch. (data: 8.25e-04). ETA=16:47:34, max mem: 15.9 GB 
[10/27 05:00:17 visual_prompt]: 	Training 500/1106. train loss: 0.4830,	0.6255 s / batch. (data: 4.68e-04). ETA=16:03:17, max mem: 15.9 GB 
[10/27 05:01:20 visual_prompt]: 	Training 600/1106. train loss: 2.0557,	0.6514 s / batch. (data: 7.71e-04). ETA=16:42:06, max mem: 15.9 GB 
[10/27 05:02:24 visual_prompt]: 	Training 700/1106. train loss: 4.4739,	0.6333 s / batch. (data: 7.93e-04). ETA=16:13:15, max mem: 15.9 GB 
[10/27 05:03:27 visual_prompt]: 	Training 800/1106. train loss: 0.2882,	0.6312 s / batch. (data: 7.64e-04). ETA=16:08:56, max mem: 15.9 GB 
[10/27 05:04:30 visual_prompt]: 	Training 900/1106. train loss: 24.4493,	0.6197 s / batch. (data: 3.11e-04). ETA=15:50:16, max mem: 15.9 GB 
[10/27 05:05:33 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6494 s / batch. (data: 8.03e-04). ETA=16:34:42, max mem: 15.9 GB 
[10/27 05:06:36 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6194 s / batch. (data: 1.63e-04). ETA=15:47:40, max mem: 15.9 GB 
[10/27 05:06:40 visual_prompt]: Epoch 17 / 100: avg data time: 4.50e-03, avg batch time: 0.6342, average train loss: 12.5150
[10/27 05:07:30 visual_prompt]: 	Test 100/123. loss: 2.824, 0.2352 s / batch. (data: 4.91e-05)max mem: 15.94594 GB 
[10/27 05:07:41 visual_prompt]: Inference (val):avg data time: 9.23e-05, avg batch time: 0.2316, average loss: 3.0461
[10/27 05:07:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.51	
[10/27 05:07:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/27 05:08:46 visual_prompt]: 	Training 100/1106. train loss: 34.1850,	0.6374 s / batch. (data: 7.96e-04). ETA=16:14:12, max mem: 15.9 GB 
[10/27 05:09:50 visual_prompt]: 	Training 200/1106. train loss: 31.8918,	0.6392 s / batch. (data: 7.74e-04). ETA=16:15:45, max mem: 15.9 GB 
[10/27 05:10:53 visual_prompt]: 	Training 300/1106. train loss: 17.0431,	0.6460 s / batch. (data: 8.22e-04). ETA=16:25:10, max mem: 15.9 GB 
[10/27 05:11:56 visual_prompt]: 	Training 400/1106. train loss: 4.1790,	0.6289 s / batch. (data: 1.05e-02). ETA=15:58:00, max mem: 15.9 GB 
[10/27 05:12:59 visual_prompt]: 	Training 500/1106. train loss: 12.4944,	0.6271 s / batch. (data: 2.75e-04). ETA=15:54:11, max mem: 15.9 GB 
[10/27 05:14:03 visual_prompt]: 	Training 600/1106. train loss: 0.0120,	0.6411 s / batch. (data: 7.89e-04). ETA=16:14:23, max mem: 15.9 GB 
[10/27 05:15:06 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6324 s / batch. (data: 7.31e-04). ETA=16:00:12, max mem: 15.9 GB 
[10/27 05:16:09 visual_prompt]: 	Training 800/1106. train loss: 2.2782,	0.6243 s / batch. (data: 3.20e-04). ETA=15:46:53, max mem: 15.9 GB 
[10/27 05:17:12 visual_prompt]: 	Training 900/1106. train loss: 19.7241,	0.6240 s / batch. (data: 2.43e-04). ETA=15:45:22, max mem: 15.9 GB 
[10/27 05:18:16 visual_prompt]: 	Training 1000/1106. train loss: 13.5152,	0.6193 s / batch. (data: 3.50e-04). ETA=15:37:11, max mem: 15.9 GB 
[10/27 05:19:19 visual_prompt]: 	Training 1100/1106. train loss: 2.5090,	0.6175 s / batch. (data: 1.62e-04). ETA=15:33:27, max mem: 15.9 GB 
[10/27 05:19:23 visual_prompt]: Epoch 18 / 100: avg data time: 4.64e-03, avg batch time: 0.6346, average train loss: 12.2015
[10/27 05:20:13 visual_prompt]: 	Test 100/123. loss: 16.699, 0.2365 s / batch. (data: 4.36e-05)max mem: 15.94594 GB 
[10/27 05:20:23 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2315, average loss: 14.5503
[10/27 05:20:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.42	
[10/27 05:20:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/27 05:21:28 visual_prompt]: 	Training 100/1106. train loss: 1.5742,	0.6304 s / batch. (data: 2.81e-04). ETA=15:51:53, max mem: 15.9 GB 
[10/27 05:22:32 visual_prompt]: 	Training 200/1106. train loss: 1.1173,	0.6179 s / batch. (data: 3.25e-04). ETA=15:31:52, max mem: 15.9 GB 
[10/27 05:23:35 visual_prompt]: 	Training 300/1106. train loss: 0.0412,	0.6274 s / batch. (data: 3.11e-04). ETA=15:45:12, max mem: 15.9 GB 
[10/27 05:24:38 visual_prompt]: 	Training 400/1106. train loss: 9.1055,	0.6182 s / batch. (data: 2.68e-04). ETA=15:30:18, max mem: 15.9 GB 
[10/27 05:25:41 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6321 s / batch. (data: 3.07e-04). ETA=15:50:10, max mem: 15.9 GB 
[10/27 05:26:45 visual_prompt]: 	Training 600/1106. train loss: 0.0054,	0.6180 s / batch. (data: 3.19e-04). ETA=15:27:54, max mem: 15.9 GB 
[10/27 05:27:48 visual_prompt]: 	Training 700/1106. train loss: 2.4106,	0.6175 s / batch. (data: 3.26e-04). ETA=15:26:06, max mem: 15.9 GB 
[10/27 05:28:51 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6321 s / batch. (data: 3.12e-04). ETA=15:47:03, max mem: 15.9 GB 
[10/27 05:29:55 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6178 s / batch. (data: 3.17e-04). ETA=15:24:31, max mem: 15.9 GB 
[10/27 05:30:57 visual_prompt]: 	Training 1000/1106. train loss: 13.4356,	0.6301 s / batch. (data: 7.85e-04). ETA=15:41:53, max mem: 15.9 GB 
[10/27 05:32:01 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6188 s / batch. (data: 1.81e-04). ETA=15:23:56, max mem: 15.9 GB 
[10/27 05:32:05 visual_prompt]: Epoch 19 / 100: avg data time: 4.22e-03, avg batch time: 0.6343, average train loss: 11.2645
[10/27 05:32:54 visual_prompt]: 	Test 100/123. loss: 7.554, 0.2250 s / batch. (data: 2.67e-05)max mem: 15.94594 GB 
[10/27 05:33:05 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.2330, average loss: 6.8370
[10/27 05:33:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.53	
[10/27 05:33:05 visual_prompt]: Stopping early.
[10/27 05:33:05 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 05:33:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 05:33:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 05:33:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 05:33:05 visual_prompt]: Training with config:
[10/27 05:33:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr2.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 05:33:05 visual_prompt]: Loading training data...
[10/27 05:33:05 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 05:33:05 visual_prompt]: Loading validation data...
[10/27 05:33:05 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 05:33:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/27 05:33:08 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/27 05:33:08 visual_prompt]: tuned percent:0.522
[10/27 05:33:08 visual_prompt]: Device used for model: 0
[10/27 05:33:08 visual_prompt]: Setting up Evaluator...
[10/27 05:33:08 visual_prompt]: Setting up Trainer...
[10/27 05:33:08 visual_prompt]: 	Setting up the optimizer...
[10/27 05:33:08 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 05:34:14 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6326 s / batch. (data: 5.45e-03). ETA=19:25:03, max mem: 15.9 GB 
[10/27 05:35:17 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6433 s / batch. (data: 7.61e-04). ETA=19:43:40, max mem: 15.9 GB 
[10/27 05:36:20 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6434 s / batch. (data: 7.49e-04). ETA=19:42:49, max mem: 15.9 GB 
[10/27 05:37:23 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6335 s / batch. (data: 8.18e-04). ETA=19:23:36, max mem: 15.9 GB 
[10/27 05:38:27 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6235 s / batch. (data: 5.42e-03). ETA=19:04:12, max mem: 15.9 GB 
[10/27 05:39:30 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6189 s / batch. (data: 3.08e-04). ETA=18:54:43, max mem: 15.9 GB 
[10/27 05:40:33 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6197 s / batch. (data: 3.29e-04). ETA=18:55:03, max mem: 15.9 GB 
[10/27 05:41:37 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6199 s / batch. (data: 3.19e-04). ETA=18:54:20, max mem: 15.9 GB 
[10/27 05:42:40 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6642 s / batch. (data: 1.69e-02). ETA=20:14:24, max mem: 15.9 GB 
[10/27 05:43:44 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6338 s / batch. (data: 8.14e-04). ETA=19:17:45, max mem: 15.9 GB 
[10/27 05:44:47 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6177 s / batch. (data: 1.38e-04). ETA=18:47:18, max mem: 15.9 GB 
[10/27 05:44:51 visual_prompt]: Epoch 1 / 100: avg data time: 4.68e-03, avg batch time: 0.6352, average train loss: 1.4028
[10/27 05:45:40 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2263 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/27 05:45:51 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2319, average loss: 1.3505
[10/27 05:45:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/27 05:45:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/27 05:46:56 visual_prompt]: 	Training 100/1106. train loss: 5.2396,	0.6405 s / batch. (data: 7.71e-04). ETA=19:27:51, max mem: 15.9 GB 
[10/27 05:47:59 visual_prompt]: 	Training 200/1106. train loss: 0.7736,	0.6258 s / batch. (data: 3.26e-04). ETA=18:59:52, max mem: 15.9 GB 
[10/27 05:49:03 visual_prompt]: 	Training 300/1106. train loss: 0.4808,	0.6378 s / batch. (data: 7.75e-04). ETA=19:20:47, max mem: 15.9 GB 
[10/27 05:50:06 visual_prompt]: 	Training 400/1106. train loss: 0.0004,	0.6321 s / batch. (data: 3.97e-04). ETA=19:09:16, max mem: 15.9 GB 
[10/27 05:51:09 visual_prompt]: 	Training 500/1106. train loss: 4.9177,	0.6395 s / batch. (data: 1.58e-02). ETA=19:21:36, max mem: 15.9 GB 
[10/27 05:52:12 visual_prompt]: 	Training 600/1106. train loss: 0.9580,	0.6471 s / batch. (data: 7.86e-04). ETA=19:34:20, max mem: 15.9 GB 
[10/27 05:53:16 visual_prompt]: 	Training 700/1106. train loss: 0.0820,	0.6434 s / batch. (data: 7.72e-04). ETA=19:26:43, max mem: 15.9 GB 
[10/27 05:54:19 visual_prompt]: 	Training 800/1106. train loss: 1.0474,	0.6240 s / batch. (data: 3.23e-04). ETA=18:50:20, max mem: 15.9 GB 
[10/27 05:55:22 visual_prompt]: 	Training 900/1106. train loss: 1.5201,	0.6188 s / batch. (data: 3.05e-04). ETA=18:40:00, max mem: 15.9 GB 
[10/27 05:56:26 visual_prompt]: 	Training 1000/1106. train loss: 0.1747,	0.6201 s / batch. (data: 7.31e-04). ETA=18:41:17, max mem: 15.9 GB 
[10/27 05:57:29 visual_prompt]: 	Training 1100/1106. train loss: 0.0034,	0.6187 s / batch. (data: 1.48e-04). ETA=18:37:47, max mem: 15.9 GB 
[10/27 05:57:32 visual_prompt]: Epoch 2 / 100: avg data time: 3.71e-03, avg batch time: 0.6341, average train loss: 1.5923
[10/27 05:58:23 visual_prompt]: 	Test 100/123. loss: 2.989, 0.2399 s / batch. (data: 3.15e-05)max mem: 15.94594 GB 
[10/27 05:58:33 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2325, average loss: 2.6904
[10/27 05:58:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.78	
[10/27 05:58:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/27 05:59:40 visual_prompt]: 	Training 100/1106. train loss: 0.6002,	0.6174 s / batch. (data: 2.92e-04). ETA=18:34:11, max mem: 15.9 GB 
[10/27 06:00:43 visual_prompt]: 	Training 200/1106. train loss: 0.2850,	0.6389 s / batch. (data: 8.26e-04). ETA=19:11:56, max mem: 15.9 GB 
[10/27 06:01:46 visual_prompt]: 	Training 300/1106. train loss: 0.1288,	0.6388 s / batch. (data: 7.78e-04). ETA=19:10:45, max mem: 15.9 GB 
[10/27 06:02:49 visual_prompt]: 	Training 400/1106. train loss: 0.6985,	0.6195 s / batch. (data: 3.23e-04). ETA=18:34:54, max mem: 15.9 GB 
[10/27 06:03:52 visual_prompt]: 	Training 500/1106. train loss: 4.3420,	0.6196 s / batch. (data: 3.30e-04). ETA=18:34:04, max mem: 15.9 GB 
[10/27 06:04:56 visual_prompt]: 	Training 600/1106. train loss: 0.0475,	0.6332 s / batch. (data: 3.32e-04). ETA=18:57:34, max mem: 15.9 GB 
[10/27 06:05:59 visual_prompt]: 	Training 700/1106. train loss: 0.1210,	0.6197 s / batch. (data: 3.15e-04). ETA=18:32:11, max mem: 15.9 GB 
[10/27 06:07:02 visual_prompt]: 	Training 800/1106. train loss: 2.3861,	0.6364 s / batch. (data: 8.36e-04). ETA=19:01:10, max mem: 15.9 GB 
[10/27 06:08:06 visual_prompt]: 	Training 900/1106. train loss: 1.9469,	0.6414 s / batch. (data: 1.01e-02). ETA=19:08:57, max mem: 15.9 GB 
[10/27 06:09:09 visual_prompt]: 	Training 1000/1106. train loss: 4.3715,	0.6298 s / batch. (data: 3.96e-04). ETA=18:47:08, max mem: 15.9 GB 
[10/27 06:10:12 visual_prompt]: 	Training 1100/1106. train loss: 0.8755,	0.6171 s / batch. (data: 1.34e-04). ETA=18:23:31, max mem: 15.9 GB 
[10/27 06:10:16 visual_prompt]: Epoch 3 / 100: avg data time: 5.40e-03, avg batch time: 0.6356, average train loss: 1.8476
[10/27 06:11:06 visual_prompt]: 	Test 100/123. loss: 1.020, 0.2429 s / batch. (data: 3.89e-05)max mem: 15.94594 GB 
[10/27 06:11:17 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.2335, average loss: 1.1137
[10/27 06:11:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[10/27 06:11:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/27 06:12:22 visual_prompt]: 	Training 100/1106. train loss: 0.8307,	0.6176 s / batch. (data: 3.39e-04). ETA=18:23:14, max mem: 15.9 GB 
[10/27 06:13:25 visual_prompt]: 	Training 200/1106. train loss: 11.7521,	0.6291 s / batch. (data: 3.13e-04). ETA=18:42:47, max mem: 15.9 GB 
[10/27 06:14:29 visual_prompt]: 	Training 300/1106. train loss: 1.2787,	0.6330 s / batch. (data: 7.46e-04). ETA=18:48:38, max mem: 15.9 GB 
[10/27 06:15:32 visual_prompt]: 	Training 400/1106. train loss: 2.0792,	0.6322 s / batch. (data: 3.47e-04). ETA=18:46:06, max mem: 15.9 GB 
[10/27 06:16:35 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6464 s / batch. (data: 8.08e-04). ETA=19:10:23, max mem: 15.9 GB 
[10/27 06:17:39 visual_prompt]: 	Training 600/1106. train loss: 1.7204,	0.6320 s / batch. (data: 2.84e-04). ETA=18:43:40, max mem: 15.9 GB 
[10/27 06:18:42 visual_prompt]: 	Training 700/1106. train loss: 13.0674,	0.6365 s / batch. (data: 5.42e-03). ETA=18:50:38, max mem: 15.9 GB 
[10/27 06:19:45 visual_prompt]: 	Training 800/1106. train loss: 1.8205,	0.6272 s / batch. (data: 3.53e-04). ETA=18:33:04, max mem: 15.9 GB 
[10/27 06:20:48 visual_prompt]: 	Training 900/1106. train loss: 6.5498,	0.6407 s / batch. (data: 7.68e-04). ETA=18:55:57, max mem: 15.9 GB 
[10/27 06:21:51 visual_prompt]: 	Training 1000/1106. train loss: 4.0092,	0.6454 s / batch. (data: 5.93e-03). ETA=19:03:17, max mem: 15.9 GB 
[10/27 06:22:55 visual_prompt]: 	Training 1100/1106. train loss: 8.9345,	0.6185 s / batch. (data: 1.39e-04). ETA=18:14:34, max mem: 15.9 GB 
[10/27 06:22:58 visual_prompt]: Epoch 4 / 100: avg data time: 4.11e-03, avg batch time: 0.6343, average train loss: 3.2116
[10/27 06:23:48 visual_prompt]: 	Test 100/123. loss: 1.948, 0.2381 s / batch. (data: 4.17e-05)max mem: 15.94594 GB 
[10/27 06:23:59 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2325, average loss: 2.1410
[10/27 06:23:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.52	
[10/27 06:23:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/27 06:25:04 visual_prompt]: 	Training 100/1106. train loss: 2.6320,	0.6371 s / batch. (data: 8.03e-04). ETA=18:46:20, max mem: 15.9 GB 
[10/27 06:26:07 visual_prompt]: 	Training 200/1106. train loss: 0.0001,	0.6275 s / batch. (data: 2.98e-04). ETA=18:28:20, max mem: 15.9 GB 
[10/27 06:27:10 visual_prompt]: 	Training 300/1106. train loss: 1.4970,	0.6484 s / batch. (data: 8.62e-04). ETA=19:04:15, max mem: 15.9 GB 
[10/27 06:28:14 visual_prompt]: 	Training 400/1106. train loss: 24.3011,	0.6338 s / batch. (data: 7.72e-04). ETA=18:37:16, max mem: 15.9 GB 
[10/27 06:29:17 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6219 s / batch. (data: 3.29e-04). ETA=18:15:22, max mem: 15.9 GB 
[10/27 06:30:21 visual_prompt]: 	Training 600/1106. train loss: 0.1725,	0.6306 s / batch. (data: 8.19e-04). ETA=18:29:38, max mem: 15.9 GB 
[10/27 06:31:24 visual_prompt]: 	Training 700/1106. train loss: 5.5896,	0.6392 s / batch. (data: 8.25e-04). ETA=18:43:36, max mem: 15.9 GB 
[10/27 06:32:27 visual_prompt]: 	Training 800/1106. train loss: 7.5344,	0.6325 s / batch. (data: 7.86e-04). ETA=18:30:51, max mem: 15.9 GB 
[10/27 06:33:31 visual_prompt]: 	Training 900/1106. train loss: 4.7514,	0.6406 s / batch. (data: 5.83e-03). ETA=18:43:58, max mem: 15.9 GB 
[10/27 06:34:34 visual_prompt]: 	Training 1000/1106. train loss: 1.2456,	0.6335 s / batch. (data: 8.09e-04). ETA=18:30:27, max mem: 15.9 GB 
[10/27 06:35:37 visual_prompt]: 	Training 1100/1106. train loss: 8.4644,	0.6175 s / batch. (data: 1.49e-04). ETA=18:01:28, max mem: 15.9 GB 
[10/27 06:35:41 visual_prompt]: Epoch 5 / 100: avg data time: 4.24e-03, avg batch time: 0.6349, average train loss: 4.5412
[10/27 06:36:31 visual_prompt]: 	Test 100/123. loss: 13.698, 0.2451 s / batch. (data: 2.55e-05)max mem: 15.94594 GB 
[10/27 06:36:42 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2327, average loss: 15.0713
[10/27 06:36:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.77	
[10/27 06:36:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/27 06:37:47 visual_prompt]: 	Training 100/1106. train loss: 5.2147,	0.6164 s / batch. (data: 2.91e-04). ETA=17:58:20, max mem: 15.9 GB 
[10/27 06:38:50 visual_prompt]: 	Training 200/1106. train loss: 0.0010,	0.6190 s / batch. (data: 3.46e-04). ETA=18:01:59, max mem: 15.9 GB 
[10/27 06:39:53 visual_prompt]: 	Training 300/1106. train loss: 0.2269,	0.6323 s / batch. (data: 7.89e-04). ETA=18:24:03, max mem: 15.9 GB 
[10/27 06:40:57 visual_prompt]: 	Training 400/1106. train loss: 7.5901,	0.6334 s / batch. (data: 2.83e-04). ETA=18:25:00, max mem: 15.9 GB 
[10/27 06:42:00 visual_prompt]: 	Training 500/1106. train loss: 7.2185,	0.6409 s / batch. (data: 7.96e-04). ETA=18:37:01, max mem: 15.9 GB 
[10/27 06:43:03 visual_prompt]: 	Training 600/1106. train loss: 12.1529,	0.6440 s / batch. (data: 7.96e-04). ETA=18:41:18, max mem: 15.9 GB 
[10/27 06:44:06 visual_prompt]: 	Training 700/1106. train loss: 3.2702,	0.6378 s / batch. (data: 5.43e-03). ETA=18:29:22, max mem: 15.9 GB 
[10/27 06:45:10 visual_prompt]: 	Training 800/1106. train loss: 14.6436,	0.6188 s / batch. (data: 3.33e-04). ETA=17:55:23, max mem: 15.9 GB 
[10/27 06:46:13 visual_prompt]: 	Training 900/1106. train loss: 8.8585,	0.6438 s / batch. (data: 7.10e-04). ETA=18:37:48, max mem: 15.9 GB 
[10/27 06:47:16 visual_prompt]: 	Training 1000/1106. train loss: 0.3267,	0.6177 s / batch. (data: 2.85e-04). ETA=17:51:19, max mem: 15.9 GB 
[10/27 06:48:19 visual_prompt]: 	Training 1100/1106. train loss: 0.0410,	0.6182 s / batch. (data: 1.71e-04). ETA=17:51:16, max mem: 15.9 GB 
[10/27 06:48:23 visual_prompt]: Epoch 6 / 100: avg data time: 4.28e-03, avg batch time: 0.6341, average train loss: 5.3803
[10/27 06:49:13 visual_prompt]: 	Test 100/123. loss: 0.693, 0.2617 s / batch. (data: 3.15e-05)max mem: 15.94594 GB 
[10/27 06:49:24 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.2339, average loss: 0.6908
[10/27 06:49:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.92	
[10/27 06:49:24 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/27 06:50:29 visual_prompt]: 	Training 100/1106. train loss: 23.8195,	0.6250 s / batch. (data: 3.17e-04). ETA=18:01:51, max mem: 15.9 GB 
[10/27 06:51:32 visual_prompt]: 	Training 200/1106. train loss: 1.2297,	0.6592 s / batch. (data: 1.71e-02). ETA=19:00:00, max mem: 15.9 GB 
[10/27 06:52:35 visual_prompt]: 	Training 300/1106. train loss: 9.3941,	0.6428 s / batch. (data: 1.26e-03). ETA=18:30:30, max mem: 15.9 GB 
[10/27 06:53:39 visual_prompt]: 	Training 400/1106. train loss: 0.9005,	0.6412 s / batch. (data: 9.59e-04). ETA=18:26:50, max mem: 15.9 GB 
[10/27 06:54:42 visual_prompt]: 	Training 500/1106. train loss: 13.3733,	0.6364 s / batch. (data: 7.59e-04). ETA=18:17:24, max mem: 15.9 GB 
[10/27 06:55:45 visual_prompt]: 	Training 600/1106. train loss: 4.1323,	0.6403 s / batch. (data: 7.98e-04). ETA=18:23:00, max mem: 15.9 GB 
[10/27 06:56:49 visual_prompt]: 	Training 700/1106. train loss: 38.5075,	0.6198 s / batch. (data: 2.90e-04). ETA=17:46:40, max mem: 15.9 GB 
[10/27 06:57:52 visual_prompt]: 	Training 800/1106. train loss: 5.2256,	0.6363 s / batch. (data: 8.21e-04). ETA=18:14:00, max mem: 15.9 GB 
[10/27 06:58:55 visual_prompt]: 	Training 900/1106. train loss: 1.5875,	0.6468 s / batch. (data: 7.49e-04). ETA=18:30:57, max mem: 15.9 GB 
[10/27 06:59:59 visual_prompt]: 	Training 1000/1106. train loss: 15.4756,	0.6335 s / batch. (data: 2.19e-04). ETA=18:07:12, max mem: 15.9 GB 
[10/27 07:01:02 visual_prompt]: 	Training 1100/1106. train loss: 17.5795,	0.6187 s / batch. (data: 1.40e-04). ETA=17:40:38, max mem: 15.9 GB 
[10/27 07:01:06 visual_prompt]: Epoch 7 / 100: avg data time: 4.19e-03, avg batch time: 0.6346, average train loss: 6.6078
[10/27 07:01:56 visual_prompt]: 	Test 100/123. loss: 12.498, 0.2399 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/27 07:02:07 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2324, average loss: 11.2886
[10/27 07:02:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.43	
[10/27 07:02:07 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/27 07:03:12 visual_prompt]: 	Training 100/1106. train loss: 7.8947,	0.6310 s / batch. (data: 1.32e-02). ETA=18:00:40, max mem: 15.9 GB 
[10/27 07:04:15 visual_prompt]: 	Training 200/1106. train loss: 1.6395,	0.6303 s / batch. (data: 7.98e-04). ETA=17:58:30, max mem: 15.9 GB 
[10/27 07:05:18 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6349 s / batch. (data: 2.92e-04). ETA=18:05:17, max mem: 15.9 GB 
[10/27 07:06:22 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6183 s / batch. (data: 2.66e-04). ETA=17:35:51, max mem: 15.9 GB 
[10/27 07:07:26 visual_prompt]: 	Training 500/1106. train loss: 0.0179,	0.6470 s / batch. (data: 1.90e-02). ETA=18:23:42, max mem: 15.9 GB 
[10/27 07:08:29 visual_prompt]: 	Training 600/1106. train loss: 0.0001,	0.6323 s / batch. (data: 1.53e-02). ETA=17:57:37, max mem: 15.9 GB 
[10/27 07:09:32 visual_prompt]: 	Training 700/1106. train loss: 12.3409,	0.6405 s / batch. (data: 8.01e-04). ETA=18:10:33, max mem: 15.9 GB 
[10/27 07:10:35 visual_prompt]: 	Training 800/1106. train loss: 2.3891,	0.6301 s / batch. (data: 1.07e-02). ETA=17:51:49, max mem: 15.9 GB 
[10/27 07:11:38 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6179 s / batch. (data: 4.50e-04). ETA=17:30:00, max mem: 15.9 GB 
[10/27 07:12:42 visual_prompt]: 	Training 1000/1106. train loss: 1.6940,	0.6196 s / batch. (data: 3.16e-04). ETA=17:31:54, max mem: 15.9 GB 
[10/27 07:13:45 visual_prompt]: 	Training 1100/1106. train loss: 1.7011,	0.6187 s / batch. (data: 1.34e-04). ETA=17:29:21, max mem: 15.9 GB 
[10/27 07:13:49 visual_prompt]: Epoch 8 / 100: avg data time: 4.39e-03, avg batch time: 0.6347, average train loss: 8.8708
[10/27 07:14:39 visual_prompt]: 	Test 100/123. loss: 4.936, 0.2259 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[10/27 07:14:50 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2312, average loss: 4.4373
[10/27 07:14:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.34	
[10/27 07:14:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/27 07:15:55 visual_prompt]: 	Training 100/1106. train loss: 46.6247,	0.6256 s / batch. (data: 5.51e-03). ETA=17:39:53, max mem: 15.9 GB 
[10/27 07:16:58 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6413 s / batch. (data: 3.19e-04). ETA=18:05:21, max mem: 15.9 GB 
[10/27 07:18:01 visual_prompt]: 	Training 300/1106. train loss: 9.0121,	0.6175 s / batch. (data: 3.19e-04). ETA=17:24:01, max mem: 15.9 GB 
[10/27 07:19:05 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6263 s / batch. (data: 2.72e-04). ETA=17:37:55, max mem: 15.9 GB 
[10/27 07:20:08 visual_prompt]: 	Training 500/1106. train loss: 10.8746,	0.6295 s / batch. (data: 3.31e-04). ETA=17:42:19, max mem: 15.9 GB 
[10/27 07:21:11 visual_prompt]: 	Training 600/1106. train loss: 1.5742,	0.6328 s / batch. (data: 7.35e-04). ETA=17:46:46, max mem: 15.9 GB 
[10/27 07:22:14 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6217 s / batch. (data: 3.26e-04). ETA=17:27:00, max mem: 15.9 GB 
[10/27 07:23:18 visual_prompt]: 	Training 800/1106. train loss: 2.3731,	0.6309 s / batch. (data: 3.17e-04). ETA=17:41:34, max mem: 15.9 GB 
[10/27 07:24:21 visual_prompt]: 	Training 900/1106. train loss: 14.9465,	0.6312 s / batch. (data: 8.39e-04). ETA=17:40:58, max mem: 15.9 GB 
[10/27 07:25:24 visual_prompt]: 	Training 1000/1106. train loss: 1.9772,	0.6188 s / batch. (data: 3.55e-04). ETA=17:19:03, max mem: 15.9 GB 
[10/27 07:26:28 visual_prompt]: 	Training 1100/1106. train loss: 4.3048,	0.6176 s / batch. (data: 1.46e-04). ETA=17:15:57, max mem: 15.9 GB 
[10/27 07:26:31 visual_prompt]: Epoch 9 / 100: avg data time: 4.20e-03, avg batch time: 0.6343, average train loss: 8.7254
[10/27 07:27:21 visual_prompt]: 	Test 100/123. loss: 4.623, 0.2357 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/27 07:27:32 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.2321, average loss: 4.1531
[10/27 07:27:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.89	
[10/27 07:27:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/27 07:28:37 visual_prompt]: 	Training 100/1106. train loss: 7.2335,	0.6420 s / batch. (data: 1.10e-02). ETA=17:55:50, max mem: 15.9 GB 
[10/27 07:29:40 visual_prompt]: 	Training 200/1106. train loss: 19.5659,	0.6201 s / batch. (data: 5.46e-03). ETA=17:18:02, max mem: 15.9 GB 
[10/27 07:30:43 visual_prompt]: 	Training 300/1106. train loss: 0.6461,	0.6190 s / batch. (data: 7.08e-04). ETA=17:15:18, max mem: 15.9 GB 
[10/27 07:31:46 visual_prompt]: 	Training 400/1106. train loss: 0.1213,	0.6281 s / batch. (data: 2.85e-04). ETA=17:29:23, max mem: 15.9 GB 
[10/27 07:32:50 visual_prompt]: 	Training 500/1106. train loss: 1.1262,	0.6586 s / batch. (data: 1.62e-02). ETA=18:19:14, max mem: 15.9 GB 
[10/27 07:33:53 visual_prompt]: 	Training 600/1106. train loss: 30.6861,	0.6413 s / batch. (data: 3.36e-04). ETA=17:49:18, max mem: 15.9 GB 
[10/27 07:34:57 visual_prompt]: 	Training 700/1106. train loss: 27.0467,	0.6344 s / batch. (data: 3.06e-04). ETA=17:36:43, max mem: 15.9 GB 
[10/27 07:36:00 visual_prompt]: 	Training 800/1106. train loss: 2.1302,	0.6255 s / batch. (data: 2.64e-04). ETA=17:20:51, max mem: 15.9 GB 
[10/27 07:37:03 visual_prompt]: 	Training 900/1106. train loss: 7.3225,	0.6478 s / batch. (data: 7.84e-04). ETA=17:56:50, max mem: 15.9 GB 
[10/27 07:38:06 visual_prompt]: 	Training 1000/1106. train loss: 0.0024,	0.6429 s / batch. (data: 7.63e-04). ETA=17:47:38, max mem: 15.9 GB 
[10/27 07:39:10 visual_prompt]: 	Training 1100/1106. train loss: 0.0004,	0.6182 s / batch. (data: 1.55e-04). ETA=17:05:41, max mem: 15.9 GB 
[10/27 07:39:14 visual_prompt]: Epoch 10 / 100: avg data time: 3.71e-03, avg batch time: 0.6347, average train loss: 9.1604
[10/27 07:40:04 visual_prompt]: 	Test 100/123. loss: 3.479, 0.2414 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/27 07:40:14 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2334, average loss: 4.5740
[10/27 07:40:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.41	
[10/27 07:40:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/27 07:41:20 visual_prompt]: 	Training 100/1106. train loss: 10.5608,	0.6190 s / batch. (data: 7.53e-04). ETA=17:05:53, max mem: 15.9 GB 
[10/27 07:42:23 visual_prompt]: 	Training 200/1106. train loss: 17.7607,	0.6327 s / batch. (data: 8.52e-04). ETA=17:27:34, max mem: 15.9 GB 
[10/27 07:43:26 visual_prompt]: 	Training 300/1106. train loss: 20.5473,	0.6406 s / batch. (data: 7.46e-04). ETA=17:39:28, max mem: 15.9 GB 
[10/27 07:44:30 visual_prompt]: 	Training 400/1106. train loss: 0.3650,	0.6170 s / batch. (data: 3.09e-04). ETA=16:59:28, max mem: 15.9 GB 
[10/27 07:45:33 visual_prompt]: 	Training 500/1106. train loss: 14.8114,	0.6437 s / batch. (data: 8.00e-04). ETA=17:42:35, max mem: 15.9 GB 
[10/27 07:46:36 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6611 s / batch. (data: 8.02e-04). ETA=18:10:10, max mem: 15.9 GB 
[10/27 07:47:39 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6197 s / batch. (data: 3.23e-04). ETA=17:00:51, max mem: 15.9 GB 
[10/27 07:48:43 visual_prompt]: 	Training 800/1106. train loss: 8.5823,	0.6529 s / batch. (data: 7.55e-04). ETA=17:54:23, max mem: 15.9 GB 
[10/27 07:49:46 visual_prompt]: 	Training 900/1106. train loss: 1.5806,	0.6256 s / batch. (data: 3.30e-04). ETA=17:08:28, max mem: 15.9 GB 
[10/27 07:50:49 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6441 s / batch. (data: 1.61e-02). ETA=17:37:49, max mem: 15.9 GB 
[10/27 07:51:52 visual_prompt]: 	Training 1100/1106. train loss: 3.4057,	0.6198 s / batch. (data: 1.67e-04). ETA=16:56:48, max mem: 15.9 GB 
[10/27 07:51:56 visual_prompt]: Epoch 11 / 100: avg data time: 4.24e-03, avg batch time: 0.6345, average train loss: 11.0960
[10/27 07:52:46 visual_prompt]: 	Test 100/123. loss: 5.582, 0.2250 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[10/27 07:52:57 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.2340, average loss: 4.9674
[10/27 07:52:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.62	
[10/27 07:52:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/27 07:54:03 visual_prompt]: 	Training 100/1106. train loss: 0.0001,	0.6186 s / batch. (data: 2.98e-04). ETA=16:53:51, max mem: 15.9 GB 
[10/27 07:55:06 visual_prompt]: 	Training 200/1106. train loss: 8.6317,	0.6230 s / batch. (data: 3.09e-04). ETA=16:59:58, max mem: 15.9 GB 
[10/27 07:56:09 visual_prompt]: 	Training 300/1106. train loss: 6.1529,	0.6307 s / batch. (data: 3.18e-04). ETA=17:11:30, max mem: 15.9 GB 
[10/27 07:57:13 visual_prompt]: 	Training 400/1106. train loss: 0.0003,	0.6416 s / batch. (data: 7.94e-04). ETA=17:28:15, max mem: 15.9 GB 
[10/27 07:58:16 visual_prompt]: 	Training 500/1106. train loss: 45.3220,	0.6317 s / batch. (data: 7.56e-04). ETA=17:11:03, max mem: 15.9 GB 
[10/27 07:59:19 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6182 s / batch. (data: 3.36e-04). ETA=16:48:00, max mem: 15.9 GB 
[10/27 08:00:22 visual_prompt]: 	Training 700/1106. train loss: 3.2136,	0.6191 s / batch. (data: 2.58e-04). ETA=16:48:27, max mem: 15.9 GB 
[10/27 08:01:26 visual_prompt]: 	Training 800/1106. train loss: 11.3921,	0.6335 s / batch. (data: 7.80e-04). ETA=17:10:48, max mem: 15.9 GB 
[10/27 08:02:29 visual_prompt]: 	Training 900/1106. train loss: 7.2638,	0.6190 s / batch. (data: 2.94e-04). ETA=16:46:13, max mem: 15.9 GB 
[10/27 08:03:32 visual_prompt]: 	Training 1000/1106. train loss: 5.0389,	0.6254 s / batch. (data: 3.25e-04). ETA=16:55:36, max mem: 15.9 GB 
[10/27 08:04:35 visual_prompt]: 	Training 1100/1106. train loss: 1.0668,	0.6182 s / batch. (data: 1.46e-04). ETA=16:42:51, max mem: 15.9 GB 
[10/27 08:04:39 visual_prompt]: Epoch 12 / 100: avg data time: 5.37e-03, avg batch time: 0.6353, average train loss: 11.9346
[10/27 08:05:29 visual_prompt]: 	Test 100/123. loss: 4.240, 0.2256 s / batch. (data: 2.79e-05)max mem: 15.94594 GB 
[10/27 08:05:40 visual_prompt]: Inference (val):avg data time: 1.02e-04, avg batch time: 0.2323, average loss: 4.6327
[10/27 08:05:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.89	
[10/27 08:05:40 visual_prompt]: Best epoch 12: best metric: -4.633
[10/27 08:05:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/27 08:06:45 visual_prompt]: 	Training 100/1106. train loss: 14.8987,	0.6370 s / batch. (data: 7.75e-04). ETA=17:12:14, max mem: 15.9 GB 
[10/27 08:07:48 visual_prompt]: 	Training 200/1106. train loss: 3.2098,	0.6176 s / batch. (data: 3.24e-04). ETA=16:39:45, max mem: 15.9 GB 
[10/27 08:08:52 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6180 s / batch. (data: 3.16e-04). ETA=16:39:25, max mem: 15.9 GB 
[10/27 08:09:55 visual_prompt]: 	Training 400/1106. train loss: 12.8398,	0.6337 s / batch. (data: 7.78e-04). ETA=17:03:41, max mem: 15.9 GB 
[10/27 08:10:58 visual_prompt]: 	Training 500/1106. train loss: 5.3849,	0.6308 s / batch. (data: 3.22e-04). ETA=16:58:02, max mem: 15.9 GB 
[10/27 08:12:01 visual_prompt]: 	Training 600/1106. train loss: 1.8287,	0.6247 s / batch. (data: 3.17e-04). ETA=16:47:03, max mem: 15.9 GB 
[10/27 08:13:05 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6520 s / batch. (data: 8.15e-04). ETA=17:30:00, max mem: 15.9 GB 
[10/27 08:14:08 visual_prompt]: 	Training 800/1106. train loss: 18.0266,	0.6281 s / batch. (data: 3.16e-04). ETA=16:50:24, max mem: 15.9 GB 
[10/27 08:15:11 visual_prompt]: 	Training 900/1106. train loss: 0.0838,	0.6194 s / batch. (data: 7.62e-04). ETA=16:35:29, max mem: 15.9 GB 
[10/27 08:16:15 visual_prompt]: 	Training 1000/1106. train loss: 34.4354,	0.6186 s / batch. (data: 3.24e-04). ETA=16:33:13, max mem: 15.9 GB 
[10/27 08:17:18 visual_prompt]: 	Training 1100/1106. train loss: 13.2828,	0.6194 s / batch. (data: 1.74e-04). ETA=16:33:27, max mem: 15.9 GB 
[10/27 08:17:22 visual_prompt]: Epoch 13 / 100: avg data time: 4.44e-03, avg batch time: 0.6347, average train loss: 11.3820
[10/27 08:18:11 visual_prompt]: 	Test 100/123. loss: 1.979, 0.2266 s / batch. (data: 4.15e-05)max mem: 15.94594 GB 
[10/27 08:18:22 visual_prompt]: Inference (val):avg data time: 9.81e-05, avg batch time: 0.2327, average loss: 2.1754
[10/27 08:18:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.39	
[10/27 08:18:22 visual_prompt]: Best epoch 13: best metric: -2.175
[10/27 08:18:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/27 08:19:27 visual_prompt]: 	Training 100/1106. train loss: 22.6359,	0.6285 s / batch. (data: 3.35e-04). ETA=16:46:51, max mem: 15.9 GB 
[10/27 08:20:31 visual_prompt]: 	Training 200/1106. train loss: 13.7534,	0.6245 s / batch. (data: 3.49e-04). ETA=16:39:22, max mem: 15.9 GB 
[10/27 08:21:34 visual_prompt]: 	Training 300/1106. train loss: 3.7371,	0.6289 s / batch. (data: 7.69e-04). ETA=16:45:21, max mem: 15.9 GB 
[10/27 08:22:37 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6342 s / batch. (data: 1.64e-02). ETA=16:52:53, max mem: 15.9 GB 
[10/27 08:23:41 visual_prompt]: 	Training 500/1106. train loss: 5.3861,	0.6182 s / batch. (data: 3.10e-04). ETA=16:26:15, max mem: 15.9 GB 
[10/27 08:24:44 visual_prompt]: 	Training 600/1106. train loss: 6.5675,	0.6345 s / batch. (data: 9.24e-04). ETA=16:51:11, max mem: 15.9 GB 
[10/27 08:25:47 visual_prompt]: 	Training 700/1106. train loss: 1.3682,	0.6560 s / batch. (data: 8.11e-04). ETA=17:24:21, max mem: 15.9 GB 
[10/27 08:26:50 visual_prompt]: 	Training 800/1106. train loss: 8.5574,	0.6304 s / batch. (data: 7.66e-04). ETA=16:42:35, max mem: 15.9 GB 
[10/27 08:27:53 visual_prompt]: 	Training 900/1106. train loss: 10.7043,	0.6289 s / batch. (data: 1.05e-02). ETA=16:39:10, max mem: 15.9 GB 
[10/27 08:28:57 visual_prompt]: 	Training 1000/1106. train loss: 20.9667,	0.6869 s / batch. (data: 5.09e-02). ETA=18:10:03, max mem: 15.9 GB 
[10/27 08:30:00 visual_prompt]: 	Training 1100/1106. train loss: 7.4472,	0.6185 s / batch. (data: 1.47e-04). ETA=16:20:28, max mem: 15.9 GB 
[10/27 08:30:04 visual_prompt]: Epoch 14 / 100: avg data time: 4.50e-03, avg batch time: 0.6345, average train loss: 9.1081
[10/27 08:30:53 visual_prompt]: 	Test 100/123. loss: 21.656, 0.2246 s / batch. (data: 3.89e-05)max mem: 15.94594 GB 
[10/27 08:31:04 visual_prompt]: Inference (val):avg data time: 1.26e-04, avg batch time: 0.2327, average loss: 19.4705
[10/27 08:31:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.84	
[10/27 08:31:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/27 08:32:09 visual_prompt]: 	Training 100/1106. train loss: 37.7890,	0.6276 s / batch. (data: 7.16e-04). ETA=16:33:48, max mem: 15.9 GB 
[10/27 08:33:12 visual_prompt]: 	Training 200/1106. train loss: 12.2016,	0.6464 s / batch. (data: 2.91e-02). ETA=17:02:35, max mem: 15.9 GB 
[10/27 08:34:15 visual_prompt]: 	Training 300/1106. train loss: 5.8683,	0.6643 s / batch. (data: 1.60e-02). ETA=17:29:48, max mem: 15.9 GB 
[10/27 08:35:18 visual_prompt]: 	Training 400/1106. train loss: 7.5176,	0.6338 s / batch. (data: 3.23e-04). ETA=16:40:35, max mem: 15.9 GB 
[10/27 08:36:22 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6560 s / batch. (data: 8.18e-04). ETA=17:14:27, max mem: 15.9 GB 
[10/27 08:37:25 visual_prompt]: 	Training 600/1106. train loss: 0.4101,	0.6314 s / batch. (data: 3.00e-04). ETA=16:34:38, max mem: 15.9 GB 
[10/27 08:38:28 visual_prompt]: 	Training 700/1106. train loss: 17.4826,	0.6188 s / batch. (data: 2.71e-04). ETA=16:13:41, max mem: 15.9 GB 
[10/27 08:39:31 visual_prompt]: 	Training 800/1106. train loss: 5.6265,	0.6511 s / batch. (data: 3.14e-04). ETA=17:03:33, max mem: 15.9 GB 
[10/27 08:40:34 visual_prompt]: 	Training 900/1106. train loss: 10.6534,	0.6355 s / batch. (data: 7.66e-04). ETA=16:37:49, max mem: 15.9 GB 
[10/27 08:41:37 visual_prompt]: 	Training 1000/1106. train loss: 3.7164,	0.6273 s / batch. (data: 2.87e-04). ETA=16:24:02, max mem: 15.9 GB 
[10/27 08:42:41 visual_prompt]: 	Training 1100/1106. train loss: 53.6548,	0.6130 s / batch. (data: 1.49e-04). ETA=16:00:31, max mem: 15.9 GB 
[10/27 08:42:44 visual_prompt]: Epoch 15 / 100: avg data time: 4.08e-03, avg batch time: 0.6330, average train loss: 17.6230
[10/27 08:43:34 visual_prompt]: 	Test 100/123. loss: 2.660, 0.2317 s / batch. (data: 3.67e-05)max mem: 15.94594 GB 
[10/27 08:43:45 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2310, average loss: 2.5869
[10/27 08:43:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.26	
[10/27 08:43:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/27 08:44:50 visual_prompt]: 	Training 100/1106. train loss: 0.8172,	0.6483 s / batch. (data: 7.67e-04). ETA=16:54:38, max mem: 15.9 GB 
[10/27 08:45:53 visual_prompt]: 	Training 200/1106. train loss: 3.8961,	0.6309 s / batch. (data: 3.22e-04). ETA=16:26:21, max mem: 15.9 GB 
[10/27 08:46:56 visual_prompt]: 	Training 300/1106. train loss: 2.7855,	0.6183 s / batch. (data: 3.15e-04). ETA=16:05:44, max mem: 15.9 GB 
[10/27 08:48:00 visual_prompt]: 	Training 400/1106. train loss: 16.8383,	0.6347 s / batch. (data: 8.13e-04). ETA=16:30:12, max mem: 15.9 GB 
[10/27 08:49:03 visual_prompt]: 	Training 500/1106. train loss: 18.9686,	0.6400 s / batch. (data: 3.27e-04). ETA=16:37:26, max mem: 15.9 GB 
[10/27 08:50:06 visual_prompt]: 	Training 600/1106. train loss: 1.5301,	0.6179 s / batch. (data: 3.54e-04). ETA=16:01:57, max mem: 15.9 GB 
[10/27 08:51:09 visual_prompt]: 	Training 700/1106. train loss: 7.4583,	0.6398 s / batch. (data: 7.72e-04). ETA=16:34:57, max mem: 15.9 GB 
[10/27 08:52:12 visual_prompt]: 	Training 800/1106. train loss: 2.3177,	0.6353 s / batch. (data: 8.40e-04). ETA=16:26:59, max mem: 15.9 GB 
[10/27 08:53:16 visual_prompt]: 	Training 900/1106. train loss: 7.0778,	0.6324 s / batch. (data: 7.98e-04). ETA=16:21:22, max mem: 15.9 GB 
[10/27 08:54:19 visual_prompt]: 	Training 1000/1106. train loss: 32.0942,	0.6186 s / batch. (data: 3.10e-04). ETA=15:58:51, max mem: 15.9 GB 
[10/27 08:55:22 visual_prompt]: 	Training 1100/1106. train loss: 25.2658,	0.6176 s / batch. (data: 1.60e-04). ETA=15:56:25, max mem: 15.9 GB 
[10/27 08:55:26 visual_prompt]: Epoch 16 / 100: avg data time: 3.84e-03, avg batch time: 0.6339, average train loss: 10.2760
[10/27 08:56:16 visual_prompt]: 	Test 100/123. loss: 7.866, 0.2517 s / batch. (data: 2.53e-05)max mem: 15.94594 GB 
[10/27 08:56:26 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.2330, average loss: 8.7504
[10/27 08:56:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.64	
[10/27 08:56:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/27 08:57:32 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6369 s / batch. (data: 3.24e-04). ETA=16:25:05, max mem: 15.9 GB 
[10/27 08:58:35 visual_prompt]: 	Training 200/1106. train loss: 8.2027,	0.6289 s / batch. (data: 2.90e-04). ETA=16:11:40, max mem: 15.9 GB 
[10/27 08:59:38 visual_prompt]: 	Training 300/1106. train loss: 0.4190,	0.6214 s / batch. (data: 3.16e-04). ETA=15:59:04, max mem: 15.9 GB 
[10/27 09:00:42 visual_prompt]: 	Training 400/1106. train loss: 34.7001,	0.6309 s / batch. (data: 8.09e-04). ETA=16:12:40, max mem: 15.9 GB 
[10/27 09:01:45 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6171 s / batch. (data: 3.12e-04). ETA=15:50:21, max mem: 15.9 GB 
[10/27 09:02:48 visual_prompt]: 	Training 600/1106. train loss: 14.7045,	0.6187 s / batch. (data: 3.06e-04). ETA=15:51:50, max mem: 15.9 GB 
[10/27 09:03:51 visual_prompt]: 	Training 700/1106. train loss: 2.4380,	0.6601 s / batch. (data: 9.30e-04). ETA=16:54:28, max mem: 15.9 GB 
[10/27 09:04:54 visual_prompt]: 	Training 800/1106. train loss: 0.0057,	0.6370 s / batch. (data: 6.03e-03). ETA=16:17:46, max mem: 15.9 GB 
[10/27 09:05:58 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6565 s / batch. (data: 1.31e-02). ETA=16:46:42, max mem: 15.9 GB 
[10/27 09:07:01 visual_prompt]: 	Training 1000/1106. train loss: 37.5833,	0.6184 s / batch. (data: 3.13e-04). ETA=15:47:16, max mem: 15.9 GB 
[10/27 09:08:04 visual_prompt]: 	Training 1100/1106. train loss: 0.0001,	0.6172 s / batch. (data: 1.59e-04). ETA=15:44:17, max mem: 15.9 GB 
[10/27 09:08:08 visual_prompt]: Epoch 17 / 100: avg data time: 4.23e-03, avg batch time: 0.6341, average train loss: 11.8153
[10/27 09:08:58 visual_prompt]: 	Test 100/123. loss: 7.758, 0.2431 s / batch. (data: 2.55e-05)max mem: 15.94594 GB 
[10/27 09:09:08 visual_prompt]: Inference (val):avg data time: 1.19e-04, avg batch time: 0.2327, average loss: 8.5320
[10/27 09:09:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.90	
[10/27 09:09:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/27 09:10:14 visual_prompt]: 	Training 100/1106. train loss: 30.8414,	0.6313 s / batch. (data: 3.20e-04). ETA=16:04:48, max mem: 15.9 GB 
[10/27 09:11:17 visual_prompt]: 	Training 200/1106. train loss: 0.4250,	0.6321 s / batch. (data: 7.88e-04). ETA=16:04:58, max mem: 15.9 GB 
[10/27 09:12:20 visual_prompt]: 	Training 300/1106. train loss: 22.0808,	0.6346 s / batch. (data: 3.19e-04). ETA=16:07:44, max mem: 15.9 GB 
[10/27 09:13:23 visual_prompt]: 	Training 400/1106. train loss: 25.2903,	0.6362 s / batch. (data: 7.86e-04). ETA=16:09:03, max mem: 15.9 GB 
[10/27 09:14:27 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6268 s / batch. (data: 5.45e-03). ETA=15:53:41, max mem: 15.9 GB 
[10/27 09:15:30 visual_prompt]: 	Training 600/1106. train loss: 0.1615,	0.6186 s / batch. (data: 3.18e-04). ETA=15:40:18, max mem: 15.9 GB 
[10/27 09:16:33 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6444 s / batch. (data: 3.34e-04). ETA=16:18:22, max mem: 15.9 GB 
[10/27 09:17:37 visual_prompt]: 	Training 800/1106. train loss: 7.8983,	0.6330 s / batch. (data: 8.02e-04). ETA=15:59:57, max mem: 15.9 GB 
[10/27 09:18:40 visual_prompt]: 	Training 900/1106. train loss: 36.0343,	0.6341 s / batch. (data: 8.13e-04). ETA=16:00:37, max mem: 15.9 GB 
[10/27 09:19:43 visual_prompt]: 	Training 1000/1106. train loss: 15.5634,	0.6231 s / batch. (data: 3.73e-04). ETA=15:42:56, max mem: 15.9 GB 
[10/27 09:20:46 visual_prompt]: 	Training 1100/1106. train loss: 12.5051,	0.6185 s / batch. (data: 1.65e-04). ETA=15:34:54, max mem: 15.9 GB 
[10/27 09:20:50 visual_prompt]: Epoch 18 / 100: avg data time: 4.16e-03, avg batch time: 0.6343, average train loss: 12.4975
[10/27 09:21:40 visual_prompt]: 	Test 100/123. loss: 2.792, 0.2357 s / batch. (data: 5.48e-05)max mem: 15.94594 GB 
[10/27 09:21:50 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2321, average loss: 3.5648
[10/27 09:21:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.56	
[10/27 09:21:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/27 09:22:56 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6216 s / batch. (data: 3.53e-04). ETA=15:38:30, max mem: 15.9 GB 
[10/27 09:23:59 visual_prompt]: 	Training 200/1106. train loss: 15.7945,	0.6303 s / batch. (data: 7.87e-04). ETA=15:50:40, max mem: 15.9 GB 
[10/27 09:25:02 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6608 s / batch. (data: 2.25e-02). ETA=16:35:34, max mem: 15.9 GB 
[10/27 09:26:05 visual_prompt]: 	Training 400/1106. train loss: 7.5371,	0.6209 s / batch. (data: 2.93e-04). ETA=15:34:21, max mem: 15.9 GB 
[10/27 09:27:09 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6483 s / batch. (data: 8.23e-04). ETA=16:14:32, max mem: 15.9 GB 
[10/27 09:28:12 visual_prompt]: 	Training 600/1106. train loss: 8.6351,	0.6314 s / batch. (data: 7.86e-04). ETA=15:48:05, max mem: 15.9 GB 
[10/27 09:29:15 visual_prompt]: 	Training 700/1106. train loss: 1.0306,	0.6300 s / batch. (data: 3.20e-04). ETA=15:44:53, max mem: 15.9 GB 
[10/27 09:30:18 visual_prompt]: 	Training 800/1106. train loss: 13.2445,	0.6197 s / batch. (data: 3.10e-04). ETA=15:28:27, max mem: 15.9 GB 
[10/27 09:31:22 visual_prompt]: 	Training 900/1106. train loss: 4.1605,	0.6185 s / batch. (data: 3.08e-04). ETA=15:25:32, max mem: 15.9 GB 
[10/27 09:32:24 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6280 s / batch. (data: 2.53e-04). ETA=15:38:45, max mem: 15.9 GB 
[10/27 09:33:28 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6183 s / batch. (data: 1.78e-04). ETA=15:23:10, max mem: 15.9 GB 
[10/27 09:33:32 visual_prompt]: Epoch 19 / 100: avg data time: 3.98e-03, avg batch time: 0.6339, average train loss: 12.5257
[10/27 09:34:21 visual_prompt]: 	Test 100/123. loss: 25.005, 0.2316 s / batch. (data: 5.22e-05)max mem: 15.94594 GB 
[10/27 09:34:32 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2331, average loss: 22.7997
[10/27 09:34:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.16	
[10/27 09:34:32 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[10/27 09:35:38 visual_prompt]: 	Training 100/1106. train loss: 3.9726,	0.6291 s / batch. (data: 7.45e-04). ETA=15:38:17, max mem: 15.9 GB 
[10/27 09:36:41 visual_prompt]: 	Training 200/1106. train loss: 0.1401,	0.6296 s / batch. (data: 7.93e-04). ETA=15:37:56, max mem: 15.9 GB 
[10/27 09:37:44 visual_prompt]: 	Training 300/1106. train loss: 19.3823,	0.6182 s / batch. (data: 3.33e-04). ETA=15:19:58, max mem: 15.9 GB 
[10/27 09:38:47 visual_prompt]: 	Training 400/1106. train loss: 4.9991,	0.6388 s / batch. (data: 1.47e-02). ETA=15:49:28, max mem: 15.9 GB 
[10/27 09:39:51 visual_prompt]: 	Training 500/1106. train loss: 9.9389,	0.6170 s / batch. (data: 3.13e-04). ETA=15:16:07, max mem: 15.9 GB 
[10/27 09:40:54 visual_prompt]: 	Training 600/1106. train loss: 4.7551,	0.6298 s / batch. (data: 1.26e-02). ETA=15:34:00, max mem: 15.9 GB 
[10/27 09:41:57 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6333 s / batch. (data: 8.33e-04). ETA=15:38:11, max mem: 15.9 GB 
[10/27 09:43:00 visual_prompt]: 	Training 800/1106. train loss: 0.1411,	0.6341 s / batch. (data: 9.22e-04). ETA=15:38:15, max mem: 15.9 GB 
[10/27 09:44:04 visual_prompt]: 	Training 900/1106. train loss: 0.0023,	0.6401 s / batch. (data: 7.40e-04). ETA=15:46:08, max mem: 15.9 GB 
[10/27 09:45:07 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6375 s / batch. (data: 7.65e-04). ETA=15:41:17, max mem: 15.9 GB 
[10/27 09:46:11 visual_prompt]: 	Training 1100/1106. train loss: 1.9758,	0.6178 s / batch. (data: 1.33e-04). ETA=15:11:03, max mem: 15.9 GB 
[10/27 09:46:14 visual_prompt]: Epoch 20 / 100: avg data time: 4.88e-03, avg batch time: 0.6348, average train loss: 10.7062
[10/27 09:47:04 visual_prompt]: 	Test 100/123. loss: 0.791, 0.2316 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/27 09:47:15 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.2334, average loss: 0.7244
[10/27 09:47:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.82	
[10/27 09:47:15 visual_prompt]: Best epoch 20: best metric: -0.724
[10/27 09:47:15 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[10/27 09:48:20 visual_prompt]: 	Training 100/1106. train loss: 40.9465,	0.6586 s / batch. (data: 7.85e-04). ETA=16:10:03, max mem: 15.9 GB 
[10/27 09:49:24 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6509 s / batch. (data: 8.39e-04). ETA=15:57:45, max mem: 15.9 GB 
[10/27 09:50:27 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6167 s / batch. (data: 3.17e-04). ETA=15:06:23, max mem: 15.9 GB 
[10/27 09:51:30 visual_prompt]: 	Training 400/1106. train loss: 34.4800,	0.6486 s / batch. (data: 7.83e-04). ETA=15:52:11, max mem: 15.9 GB 
[10/27 09:52:33 visual_prompt]: 	Training 500/1106. train loss: 30.8077,	0.6264 s / batch. (data: 3.45e-04). ETA=15:18:29, max mem: 15.9 GB 
[10/27 09:53:36 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6437 s / batch. (data: 7.97e-04). ETA=15:42:50, max mem: 15.9 GB 
[10/27 09:54:39 visual_prompt]: 	Training 700/1106. train loss: 14.6120,	0.6195 s / batch. (data: 3.14e-04). ETA=15:06:20, max mem: 15.9 GB 
[10/27 09:55:42 visual_prompt]: 	Training 800/1106. train loss: 20.0545,	0.6284 s / batch. (data: 1.05e-02). ETA=15:18:16, max mem: 15.9 GB 
[10/27 09:56:46 visual_prompt]: 	Training 900/1106. train loss: 4.0947,	0.6520 s / batch. (data: 7.84e-04). ETA=15:51:44, max mem: 15.9 GB 
[10/27 09:57:49 visual_prompt]: 	Training 1000/1106. train loss: 5.4715,	0.6338 s / batch. (data: 3.21e-04). ETA=15:24:01, max mem: 15.9 GB 
[10/27 09:58:52 visual_prompt]: 	Training 1100/1106. train loss: 46.7418,	0.6177 s / batch. (data: 1.56e-04). ETA=14:59:31, max mem: 15.9 GB 
[10/27 09:58:56 visual_prompt]: Epoch 21 / 100: avg data time: 4.56e-03, avg batch time: 0.6337, average train loss: 12.0342
[10/27 09:59:45 visual_prompt]: 	Test 100/123. loss: 3.004, 0.2476 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/27 09:59:56 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2329, average loss: 2.7200
[10/27 09:59:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.22	
[10/27 09:59:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[10/27 10:01:01 visual_prompt]: 	Training 100/1106. train loss: 17.1877,	0.6544 s / batch. (data: 3.74e-02). ETA=15:51:49, max mem: 15.9 GB 
[10/27 10:02:04 visual_prompt]: 	Training 200/1106. train loss: 0.0002,	0.6468 s / batch. (data: 8.28e-04). ETA=15:39:44, max mem: 15.9 GB 
[10/27 10:03:08 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6185 s / batch. (data: 2.74e-04). ETA=14:57:31, max mem: 15.9 GB 
[10/27 10:04:11 visual_prompt]: 	Training 400/1106. train loss: 12.4078,	0.6490 s / batch. (data: 7.74e-04). ETA=15:40:42, max mem: 15.9 GB 
[10/27 10:05:14 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6417 s / batch. (data: 7.57e-04). ETA=15:29:06, max mem: 15.9 GB 
[10/27 10:06:17 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6304 s / batch. (data: 8.14e-04). ETA=15:11:45, max mem: 15.9 GB 
[10/27 10:07:21 visual_prompt]: 	Training 700/1106. train loss: 3.0310,	0.6280 s / batch. (data: 3.25e-04). ETA=15:07:12, max mem: 15.9 GB 
[10/27 10:08:24 visual_prompt]: 	Training 800/1106. train loss: 32.3918,	0.6390 s / batch. (data: 1.17e-03). ETA=15:21:59, max mem: 15.9 GB 
[10/27 10:09:27 visual_prompt]: 	Training 900/1106. train loss: 2.7118,	0.6246 s / batch. (data: 2.83e-04). ETA=15:00:10, max mem: 15.9 GB 
[10/27 10:10:30 visual_prompt]: 	Training 1000/1106. train loss: 24.6638,	0.6307 s / batch. (data: 1.34e-02). ETA=15:07:52, max mem: 15.9 GB 
[10/27 10:11:34 visual_prompt]: 	Training 1100/1106. train loss: 62.2981,	0.6173 s / batch. (data: 1.50e-04). ETA=14:47:38, max mem: 15.9 GB 
[10/27 10:11:38 visual_prompt]: Epoch 22 / 100: avg data time: 4.29e-03, avg batch time: 0.6344, average train loss: 11.7748
[10/27 10:12:27 visual_prompt]: 	Test 100/123. loss: 24.625, 0.2390 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[10/27 10:12:38 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2315, average loss: 26.8337
[10/27 10:12:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.64	
[10/27 10:12:38 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[10/27 10:13:44 visual_prompt]: 	Training 100/1106. train loss: 18.7032,	0.6314 s / batch. (data: 2.80e-04). ETA=15:06:44, max mem: 15.9 GB 
[10/27 10:14:47 visual_prompt]: 	Training 200/1106. train loss: 6.7978,	0.6190 s / batch. (data: 2.81e-04). ETA=14:47:53, max mem: 15.9 GB 
[10/27 10:15:51 visual_prompt]: 	Training 300/1106. train loss: 0.4105,	0.6354 s / batch. (data: 3.25e-04). ETA=15:10:24, max mem: 15.9 GB 
[10/27 10:16:54 visual_prompt]: 	Training 400/1106. train loss: 18.1182,	0.6337 s / batch. (data: 8.06e-04). ETA=15:06:57, max mem: 15.9 GB 
[10/27 10:17:57 visual_prompt]: 	Training 500/1106. train loss: 0.6954,	0.6445 s / batch. (data: 1.37e-02). ETA=15:21:20, max mem: 15.9 GB 
[10/27 10:19:01 visual_prompt]: 	Training 600/1106. train loss: 6.6042,	0.6199 s / batch. (data: 3.55e-04). ETA=14:45:03, max mem: 15.9 GB 
[10/27 10:20:04 visual_prompt]: 	Training 700/1106. train loss: 3.9138,	0.6316 s / batch. (data: 7.94e-04). ETA=15:00:46, max mem: 15.9 GB 
[10/27 10:21:07 visual_prompt]: 	Training 800/1106. train loss: 8.5453,	0.6190 s / batch. (data: 2.94e-04). ETA=14:41:41, max mem: 15.9 GB 
[10/27 10:22:10 visual_prompt]: 	Training 900/1106. train loss: 5.5290,	0.6335 s / batch. (data: 3.54e-04). ETA=15:01:16, max mem: 15.9 GB 
[10/27 10:23:13 visual_prompt]: 	Training 1000/1106. train loss: 0.0001,	0.6402 s / batch. (data: 3.26e-04). ETA=15:09:50, max mem: 15.9 GB 
[10/27 10:24:17 visual_prompt]: 	Training 1100/1106. train loss: 3.5558,	0.6170 s / batch. (data: 1.49e-04). ETA=14:35:47, max mem: 15.9 GB 
[10/27 10:24:20 visual_prompt]: Epoch 23 / 100: avg data time: 4.87e-03, avg batch time: 0.6350, average train loss: 11.9816
[10/27 10:25:10 visual_prompt]: 	Test 100/123. loss: 2.964, 0.2437 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/27 10:25:21 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.2319, average loss: 2.8277
[10/27 10:25:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.23	
[10/27 10:25:21 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[10/27 10:26:26 visual_prompt]: 	Training 100/1106. train loss: 18.5779,	0.6170 s / batch. (data: 3.10e-04). ETA=14:34:42, max mem: 15.9 GB 
[10/27 10:27:29 visual_prompt]: 	Training 200/1106. train loss: 26.6741,	0.6183 s / batch. (data: 3.24e-04). ETA=14:35:34, max mem: 15.9 GB 
[10/27 10:28:33 visual_prompt]: 	Training 300/1106. train loss: 0.0541,	0.6219 s / batch. (data: 3.23e-04). ETA=14:39:36, max mem: 15.9 GB 
[10/27 10:29:36 visual_prompt]: 	Training 400/1106. train loss: 5.1525,	0.6186 s / batch. (data: 3.14e-04). ETA=14:33:52, max mem: 15.9 GB 
[10/27 10:30:39 visual_prompt]: 	Training 500/1106. train loss: 22.9802,	0.6311 s / batch. (data: 8.17e-04). ETA=14:50:31, max mem: 15.9 GB 
[10/27 10:31:42 visual_prompt]: 	Training 600/1106. train loss: 1.6411,	0.6174 s / batch. (data: 2.60e-04). ETA=14:30:07, max mem: 15.9 GB 
[10/27 10:32:45 visual_prompt]: 	Training 700/1106. train loss: 25.4328,	0.6324 s / batch. (data: 1.24e-02). ETA=14:50:14, max mem: 15.9 GB 
[10/27 10:33:49 visual_prompt]: 	Training 800/1106. train loss: 3.4976,	0.6297 s / batch. (data: 2.97e-04). ETA=14:45:20, max mem: 15.9 GB 
[10/27 10:34:52 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6312 s / batch. (data: 8.13e-04). ETA=14:46:30, max mem: 15.9 GB 
[10/27 10:35:55 visual_prompt]: 	Training 1000/1106. train loss: 0.3364,	0.6365 s / batch. (data: 6.79e-04). ETA=14:52:45, max mem: 15.9 GB 
[10/27 10:36:58 visual_prompt]: 	Training 1100/1106. train loss: 11.9809,	0.6174 s / batch. (data: 1.89e-04). ETA=14:25:02, max mem: 15.9 GB 
[10/27 10:37:02 visual_prompt]: Epoch 24 / 100: avg data time: 4.44e-03, avg batch time: 0.6343, average train loss: 9.3838
[10/27 10:37:52 visual_prompt]: 	Test 100/123. loss: 5.590, 0.2322 s / batch. (data: 3.65e-05)max mem: 15.94594 GB 
[10/27 10:38:03 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2332, average loss: 6.1181
[10/27 10:38:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.75	
[10/27 10:38:03 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[10/27 10:39:08 visual_prompt]: 	Training 100/1106. train loss: 1.2878,	0.6504 s / batch. (data: 8.14e-04). ETA=15:10:01, max mem: 15.9 GB 
[10/27 10:40:12 visual_prompt]: 	Training 200/1106. train loss: 5.9752,	0.6716 s / batch. (data: 5.92e-03). ETA=15:38:40, max mem: 15.9 GB 
[10/27 10:41:15 visual_prompt]: 	Training 300/1106. train loss: 10.3076,	0.6609 s / batch. (data: 6.95e-04). ETA=15:22:34, max mem: 15.9 GB 
[10/27 10:42:18 visual_prompt]: 	Training 400/1106. train loss: 33.8055,	0.6185 s / batch. (data: 2.97e-04). ETA=14:22:18, max mem: 15.9 GB 
[10/27 10:43:22 visual_prompt]: 	Training 500/1106. train loss: 47.5119,	0.6183 s / batch. (data: 3.61e-04). ETA=14:20:59, max mem: 15.9 GB 
[10/27 10:44:25 visual_prompt]: 	Training 600/1106. train loss: 25.6712,	0.6328 s / batch. (data: 7.44e-04). ETA=14:40:09, max mem: 15.9 GB 
[10/27 10:45:28 visual_prompt]: 	Training 700/1106. train loss: 8.6942,	0.6527 s / batch. (data: 5.91e-03). ETA=15:06:45, max mem: 15.9 GB 
[10/27 10:46:32 visual_prompt]: 	Training 800/1106. train loss: 0.0006,	0.6268 s / batch. (data: 3.23e-04). ETA=14:29:43, max mem: 15.9 GB 
[10/27 10:47:35 visual_prompt]: 	Training 900/1106. train loss: 14.0279,	0.6196 s / batch. (data: 3.08e-04). ETA=14:18:42, max mem: 15.9 GB 
[10/27 10:48:38 visual_prompt]: 	Training 1000/1106. train loss: 7.7251,	0.6243 s / batch. (data: 3.19e-04). ETA=14:24:10, max mem: 15.9 GB 
[10/27 10:49:42 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6184 s / batch. (data: 1.50e-04). ETA=14:14:59, max mem: 15.9 GB 
[10/27 10:49:45 visual_prompt]: Epoch 25 / 100: avg data time: 4.34e-03, avg batch time: 0.6353, average train loss: 9.9751
[10/27 10:50:35 visual_prompt]: 	Test 100/123. loss: 20.182, 0.2293 s / batch. (data: 4.13e-03)max mem: 15.94594 GB 
[10/27 10:50:46 visual_prompt]: Inference (val):avg data time: 7.27e-05, avg batch time: 0.2317, average loss: 18.1849
[10/27 10:50:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.32	
[10/27 10:50:46 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[10/27 10:51:51 visual_prompt]: 	Training 100/1106. train loss: 15.4122,	0.6357 s / batch. (data: 1.95e-02). ETA=14:37:45, max mem: 15.9 GB 
[10/27 10:52:54 visual_prompt]: 	Training 200/1106. train loss: 44.3367,	0.6265 s / batch. (data: 8.16e-04). ETA=14:24:02, max mem: 15.9 GB 
[10/27 10:53:57 visual_prompt]: 	Training 300/1106. train loss: 18.0720,	0.6445 s / batch. (data: 2.55e-02). ETA=14:47:45, max mem: 15.9 GB 
[10/27 10:55:00 visual_prompt]: 	Training 400/1106. train loss: 3.4957,	0.6307 s / batch. (data: 7.82e-04). ETA=14:27:46, max mem: 15.9 GB 
[10/27 10:56:04 visual_prompt]: 	Training 500/1106. train loss: 22.0977,	0.6389 s / batch. (data: 2.95e-04). ETA=14:38:00, max mem: 15.9 GB 
[10/27 10:57:07 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6465 s / batch. (data: 1.04e-02). ETA=14:47:16, max mem: 15.9 GB 
[10/27 10:58:10 visual_prompt]: 	Training 700/1106. train loss: 7.3367,	0.6301 s / batch. (data: 8.00e-04). ETA=14:23:43, max mem: 15.9 GB 
[10/27 10:59:13 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6186 s / batch. (data: 3.44e-04). ETA=14:06:59, max mem: 15.9 GB 
[10/27 11:00:16 visual_prompt]: 	Training 900/1106. train loss: 0.0022,	0.6199 s / batch. (data: 3.21e-04). ETA=14:07:44, max mem: 15.9 GB 
[10/27 11:01:19 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6262 s / batch. (data: 7.09e-04). ETA=14:15:20, max mem: 15.9 GB 
[10/27 11:02:23 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6174 s / batch. (data: 1.69e-04). ETA=14:02:15, max mem: 15.9 GB 
[10/27 11:02:26 visual_prompt]: Epoch 26 / 100: avg data time: 4.05e-03, avg batch time: 0.6335, average train loss: 10.8087
[10/27 11:03:17 visual_prompt]: 	Test 100/123. loss: 2.483, 0.2260 s / batch. (data: 4.36e-05)max mem: 15.94594 GB 
[10/27 11:03:27 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.2319, average loss: 2.2421
[10/27 11:03:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.46	
[10/27 11:03:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[10/27 11:04:33 visual_prompt]: 	Training 100/1106. train loss: 22.2482,	0.6234 s / batch. (data: 3.15e-04). ETA=14:09:21, max mem: 15.9 GB 
[10/27 11:05:36 visual_prompt]: 	Training 200/1106. train loss: 0.0003,	0.6773 s / batch. (data: 3.72e-02). ETA=15:21:34, max mem: 15.9 GB 
[10/27 11:06:39 visual_prompt]: 	Training 300/1106. train loss: 9.0920,	0.6189 s / batch. (data: 2.99e-04). ETA=14:01:08, max mem: 15.9 GB 
[10/27 11:07:43 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6175 s / batch. (data: 3.30e-04). ETA=13:58:15, max mem: 15.9 GB 
[10/27 11:08:46 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6480 s / batch. (data: 7.37e-04). ETA=14:38:31, max mem: 15.9 GB 
[10/27 11:09:49 visual_prompt]: 	Training 600/1106. train loss: 48.2524,	0.6178 s / batch. (data: 2.60e-04). ETA=13:56:30, max mem: 15.9 GB 
[10/27 11:10:52 visual_prompt]: 	Training 700/1106. train loss: 20.6314,	0.6308 s / batch. (data: 7.79e-04). ETA=14:13:07, max mem: 15.9 GB 
[10/27 11:11:56 visual_prompt]: 	Training 800/1106. train loss: 3.5124,	0.6478 s / batch. (data: 7.94e-04). ETA=14:35:00, max mem: 15.9 GB 
[10/27 11:12:59 visual_prompt]: 	Training 900/1106. train loss: 15.0546,	0.6307 s / batch. (data: 1.17e-03). ETA=14:10:47, max mem: 15.9 GB 
[10/27 11:14:02 visual_prompt]: 	Training 1000/1106. train loss: 3.6670,	0.6310 s / batch. (data: 7.87e-04). ETA=14:10:11, max mem: 15.9 GB 
[10/27 11:15:05 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6171 s / batch. (data: 1.42e-04). ETA=13:50:26, max mem: 15.9 GB 
[10/27 11:15:09 visual_prompt]: Epoch 27 / 100: avg data time: 4.57e-03, avg batch time: 0.6346, average train loss: 9.6952
[10/27 11:15:59 visual_prompt]: 	Test 100/123. loss: 20.661, 0.2252 s / batch. (data: 4.46e-05)max mem: 15.94594 GB 
[10/27 11:16:09 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.2336, average loss: 22.7357
[10/27 11:16:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.41	
[10/27 11:16:09 visual_prompt]: Stopping early.
[10/27 11:16:09 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 11:16:09 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 11:16:09 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 11:16:09 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 11:16:09 visual_prompt]: Training with config:
[10/27 11:16:09 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr2.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 11:16:09 visual_prompt]: Loading training data...
[10/27 11:16:09 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 11:16:09 visual_prompt]: Loading validation data...
[10/27 11:16:09 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 11:16:09 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/27 11:16:12 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/27 11:16:12 visual_prompt]: tuned percent:0.522
[10/27 11:16:12 visual_prompt]: Device used for model: 0
[10/27 11:16:12 visual_prompt]: Setting up Evaluator...
[10/27 11:16:12 visual_prompt]: Setting up Trainer...
[10/27 11:16:12 visual_prompt]: 	Setting up the optimizer...
[10/27 11:16:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 11:17:18 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6175 s / batch. (data: 3.05e-04). ETA=18:57:13, max mem: 15.9 GB 
[10/27 11:18:21 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6187 s / batch. (data: 3.11e-04). ETA=18:58:19, max mem: 15.9 GB 
[10/27 11:19:24 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6191 s / batch. (data: 3.41e-04). ETA=18:58:04, max mem: 15.9 GB 
[10/27 11:20:28 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6188 s / batch. (data: 2.91e-04). ETA=18:56:31, max mem: 15.9 GB 
[10/27 11:21:31 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6540 s / batch. (data: 1.05e-02). ETA=20:00:08, max mem: 15.9 GB 
[10/27 11:22:35 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6197 s / batch. (data: 3.03e-04). ETA=18:56:06, max mem: 15.9 GB 
[10/27 11:23:38 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6328 s / batch. (data: 7.20e-04). ETA=19:19:05, max mem: 15.9 GB 
[10/27 11:24:41 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6438 s / batch. (data: 5.45e-03). ETA=19:38:11, max mem: 15.9 GB 
[10/27 11:25:44 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6536 s / batch. (data: 7.75e-04). ETA=19:55:04, max mem: 15.9 GB 
[10/27 11:26:48 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6385 s / batch. (data: 8.29e-04). ETA=19:26:16, max mem: 15.9 GB 
[10/27 11:27:51 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6182 s / batch. (data: 1.56e-04). ETA=18:48:15, max mem: 15.9 GB 
[10/27 11:27:55 visual_prompt]: Epoch 1 / 100: avg data time: 4.72e-03, avg batch time: 0.6350, average train loss: 1.4028
[10/27 11:28:45 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2353 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[10/27 11:28:55 visual_prompt]: Inference (val):avg data time: 1.31e-04, avg batch time: 0.2336, average loss: 1.3505
[10/27 11:28:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/27 11:28:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/27 11:30:00 visual_prompt]: 	Training 100/1106. train loss: 0.8096,	0.6612 s / batch. (data: 1.62e-02). ETA=20:05:27, max mem: 15.9 GB 
[10/27 11:31:03 visual_prompt]: 	Training 200/1106. train loss: 0.5400,	0.6199 s / batch. (data: 3.18e-04). ETA=18:49:07, max mem: 15.9 GB 
[10/27 11:32:07 visual_prompt]: 	Training 300/1106. train loss: 0.4986,	0.6353 s / batch. (data: 5.56e-03). ETA=19:16:16, max mem: 15.9 GB 
[10/27 11:33:10 visual_prompt]: 	Training 400/1106. train loss: 0.0003,	0.6418 s / batch. (data: 8.03e-04). ETA=19:27:00, max mem: 15.9 GB 
[10/27 11:34:13 visual_prompt]: 	Training 500/1106. train loss: 4.2644,	0.6254 s / batch. (data: 2.23e-03). ETA=18:56:01, max mem: 15.9 GB 
[10/27 11:35:17 visual_prompt]: 	Training 600/1106. train loss: 0.7832,	0.6360 s / batch. (data: 7.85e-04). ETA=19:14:14, max mem: 15.9 GB 
[10/27 11:36:20 visual_prompt]: 	Training 700/1106. train loss: 0.0327,	0.6320 s / batch. (data: 3.09e-04). ETA=19:06:01, max mem: 15.9 GB 
[10/27 11:37:24 visual_prompt]: 	Training 800/1106. train loss: 0.8295,	0.6502 s / batch. (data: 8.06e-04). ETA=19:37:53, max mem: 15.9 GB 
[10/27 11:38:27 visual_prompt]: 	Training 900/1106. train loss: 1.6618,	0.6520 s / batch. (data: 5.94e-03). ETA=19:40:03, max mem: 15.9 GB 
[10/27 11:39:30 visual_prompt]: 	Training 1000/1106. train loss: 0.1348,	0.6320 s / batch. (data: 3.10e-04). ETA=19:02:50, max mem: 15.9 GB 
[10/27 11:40:33 visual_prompt]: 	Training 1100/1106. train loss: 0.3956,	0.6190 s / batch. (data: 1.63e-04). ETA=18:38:20, max mem: 15.9 GB 
[10/27 11:40:37 visual_prompt]: Epoch 2 / 100: avg data time: 4.04e-03, avg batch time: 0.6347, average train loss: 1.8260
[10/27 11:41:28 visual_prompt]: 	Test 100/123. loss: 2.650, 0.2473 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/27 11:41:38 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.2325, average loss: 2.4008
[10/27 11:41:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.13	
[10/27 11:41:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/27 11:42:44 visual_prompt]: 	Training 100/1106. train loss: 0.4023,	0.6311 s / batch. (data: 8.38e-04). ETA=18:59:03, max mem: 15.9 GB 
[10/27 11:43:47 visual_prompt]: 	Training 200/1106. train loss: 0.0207,	0.6325 s / batch. (data: 3.39e-04). ETA=19:00:29, max mem: 15.9 GB 
[10/27 11:44:51 visual_prompt]: 	Training 300/1106. train loss: 0.2166,	0.6359 s / batch. (data: 7.95e-04). ETA=19:05:35, max mem: 15.9 GB 
[10/27 11:45:54 visual_prompt]: 	Training 400/1106. train loss: 0.7979,	0.6481 s / batch. (data: 2.62e-04). ETA=19:26:29, max mem: 15.9 GB 
[10/27 11:46:58 visual_prompt]: 	Training 500/1106. train loss: 5.6996,	0.6435 s / batch. (data: 1.06e-03). ETA=19:17:06, max mem: 15.9 GB 
[10/27 11:48:01 visual_prompt]: 	Training 600/1106. train loss: 0.0006,	0.6462 s / batch. (data: 7.57e-04). ETA=19:20:52, max mem: 15.9 GB 
[10/27 11:49:04 visual_prompt]: 	Training 700/1106. train loss: 0.2439,	0.6584 s / batch. (data: 3.17e-04). ETA=19:41:46, max mem: 15.9 GB 
[10/27 11:50:08 visual_prompt]: 	Training 800/1106. train loss: 1.3541,	0.6395 s / batch. (data: 7.68e-04). ETA=19:06:37, max mem: 15.9 GB 
[10/27 11:51:11 visual_prompt]: 	Training 900/1106. train loss: 2.1178,	0.6447 s / batch. (data: 1.56e-02). ETA=19:14:58, max mem: 15.9 GB 
[10/27 11:52:14 visual_prompt]: 	Training 1000/1106. train loss: 3.3220,	0.6475 s / batch. (data: 8.26e-04). ETA=19:18:53, max mem: 15.9 GB 
[10/27 11:53:18 visual_prompt]: 	Training 1100/1106. train loss: 1.3500,	0.6186 s / batch. (data: 1.62e-04). ETA=18:26:05, max mem: 15.9 GB 
[10/27 11:53:22 visual_prompt]: Epoch 3 / 100: avg data time: 5.02e-03, avg batch time: 0.6363, average train loss: 1.8442
[10/27 11:54:12 visual_prompt]: 	Test 100/123. loss: 1.152, 0.2406 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[10/27 11:54:23 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2338, average loss: 1.2662
[10/27 11:54:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.84	
[10/27 11:54:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/27 11:55:28 visual_prompt]: 	Training 100/1106. train loss: 0.2081,	0.6178 s / batch. (data: 2.98e-04). ETA=18:23:40, max mem: 15.9 GB 
[10/27 11:56:31 visual_prompt]: 	Training 200/1106. train loss: 14.8007,	0.6567 s / batch. (data: 7.18e-04). ETA=19:32:05, max mem: 15.9 GB 
[10/27 11:57:35 visual_prompt]: 	Training 300/1106. train loss: 1.4660,	0.6234 s / batch. (data: 3.02e-04). ETA=18:31:30, max mem: 15.9 GB 
[10/27 11:58:38 visual_prompt]: 	Training 400/1106. train loss: 1.2933,	0.6342 s / batch. (data: 2.85e-04). ETA=18:49:43, max mem: 15.9 GB 
[10/27 11:59:41 visual_prompt]: 	Training 500/1106. train loss: 0.6087,	0.6488 s / batch. (data: 2.89e-02). ETA=19:14:41, max mem: 15.9 GB 
[10/27 12:00:45 visual_prompt]: 	Training 600/1106. train loss: 3.5136,	0.6432 s / batch. (data: 1.10e-02). ETA=19:03:40, max mem: 15.9 GB 
[10/27 12:01:48 visual_prompt]: 	Training 700/1106. train loss: 8.0056,	0.6349 s / batch. (data: 8.02e-04). ETA=18:47:45, max mem: 15.9 GB 
[10/27 12:02:51 visual_prompt]: 	Training 800/1106. train loss: 0.1955,	0.6339 s / batch. (data: 8.03e-04). ETA=18:44:56, max mem: 15.9 GB 
[10/27 12:03:54 visual_prompt]: 	Training 900/1106. train loss: 9.5946,	0.6190 s / batch. (data: 3.16e-04). ETA=18:17:33, max mem: 15.9 GB 
[10/27 12:04:58 visual_prompt]: 	Training 1000/1106. train loss: 7.2746,	0.6406 s / batch. (data: 3.23e-04). ETA=18:54:42, max mem: 15.9 GB 
[10/27 12:06:01 visual_prompt]: 	Training 1100/1106. train loss: 6.5462,	0.6174 s / batch. (data: 1.63e-04). ETA=18:12:40, max mem: 15.9 GB 
[10/27 12:06:05 visual_prompt]: Epoch 4 / 100: avg data time: 4.56e-03, avg batch time: 0.6349, average train loss: 2.6245
[10/27 12:06:55 visual_prompt]: 	Test 100/123. loss: 1.191, 0.2312 s / batch. (data: 2.79e-05)max mem: 15.94594 GB 
[10/27 12:07:06 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2321, average loss: 1.3051
[10/27 12:07:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.94	
[10/27 12:07:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/27 12:08:11 visual_prompt]: 	Training 100/1106. train loss: 2.2324,	0.6178 s / batch. (data: 3.17e-04). ETA=18:12:17, max mem: 15.9 GB 
[10/27 12:09:14 visual_prompt]: 	Training 200/1106. train loss: 0.0001,	0.6326 s / batch. (data: 8.29e-04). ETA=18:37:18, max mem: 15.9 GB 
[10/27 12:10:17 visual_prompt]: 	Training 300/1106. train loss: 1.2809,	0.6298 s / batch. (data: 3.15e-04). ETA=18:31:17, max mem: 15.9 GB 
[10/27 12:11:20 visual_prompt]: 	Training 400/1106. train loss: 3.7319,	0.6314 s / batch. (data: 7.91e-04). ETA=18:33:01, max mem: 15.9 GB 
[10/27 12:12:24 visual_prompt]: 	Training 500/1106. train loss: 0.0072,	0.6340 s / batch. (data: 8.20e-04). ETA=18:36:43, max mem: 15.9 GB 
[10/27 12:13:27 visual_prompt]: 	Training 600/1106. train loss: 4.6855,	0.6470 s / batch. (data: 8.08e-04). ETA=18:58:24, max mem: 15.9 GB 
[10/27 12:14:30 visual_prompt]: 	Training 700/1106. train loss: 1.1699,	0.6189 s / batch. (data: 3.31e-04). ETA=18:07:55, max mem: 15.9 GB 
[10/27 12:15:34 visual_prompt]: 	Training 800/1106. train loss: 0.8942,	0.6390 s / batch. (data: 8.45e-04). ETA=18:42:11, max mem: 15.9 GB 
[10/27 12:16:37 visual_prompt]: 	Training 900/1106. train loss: 1.9447,	0.6447 s / batch. (data: 8.35e-04). ETA=18:51:12, max mem: 15.9 GB 
[10/27 12:17:40 visual_prompt]: 	Training 1000/1106. train loss: 7.5236,	0.6208 s / batch. (data: 3.20e-04). ETA=18:08:09, max mem: 15.9 GB 
[10/27 12:18:43 visual_prompt]: 	Training 1100/1106. train loss: 1.5502,	0.6185 s / batch. (data: 2.67e-04). ETA=18:03:10, max mem: 15.9 GB 
[10/27 12:18:47 visual_prompt]: Epoch 5 / 100: avg data time: 4.13e-03, avg batch time: 0.6342, average train loss: 2.9656
[10/27 12:19:37 visual_prompt]: 	Test 100/123. loss: 7.832, 0.2252 s / batch. (data: 2.57e-05)max mem: 15.94594 GB 
[10/27 12:19:48 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.2325, average loss: 6.9483
[10/27 12:19:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.75	
[10/27 12:19:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/27 12:20:53 visual_prompt]: 	Training 100/1106. train loss: 1.8604,	0.6475 s / batch. (data: 1.55e-02). ETA=18:52:52, max mem: 15.9 GB 
[10/27 12:21:57 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6464 s / batch. (data: 8.17e-04). ETA=18:49:52, max mem: 15.9 GB 
[10/27 12:23:00 visual_prompt]: 	Training 300/1106. train loss: 0.0004,	0.6439 s / batch. (data: 8.71e-04). ETA=18:44:21, max mem: 15.9 GB 
[10/27 12:24:03 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6273 s / batch. (data: 5.39e-04). ETA=18:14:17, max mem: 15.9 GB 
[10/27 12:25:06 visual_prompt]: 	Training 500/1106. train loss: 2.3659,	0.6195 s / batch. (data: 2.89e-04). ETA=17:59:39, max mem: 15.9 GB 
[10/27 12:26:10 visual_prompt]: 	Training 600/1106. train loss: 0.2624,	0.6180 s / batch. (data: 3.12e-04). ETA=17:56:07, max mem: 15.9 GB 
[10/27 12:27:13 visual_prompt]: 	Training 700/1106. train loss: 11.9738,	0.6516 s / batch. (data: 1.10e-02). ETA=18:53:24, max mem: 15.9 GB 
[10/27 12:28:17 visual_prompt]: 	Training 800/1106. train loss: 6.4415,	0.6339 s / batch. (data: 7.31e-04). ETA=18:21:37, max mem: 15.9 GB 
[10/27 12:29:21 visual_prompt]: 	Training 900/1106. train loss: 7.8553,	0.6662 s / batch. (data: 3.73e-04). ETA=19:16:37, max mem: 15.9 GB 
[10/27 12:30:24 visual_prompt]: 	Training 1000/1106. train loss: 1.8303,	0.6468 s / batch. (data: 7.23e-04). ETA=18:41:56, max mem: 15.9 GB 
[10/27 12:31:28 visual_prompt]: 	Training 1100/1106. train loss: 1.2710,	0.6201 s / batch. (data: 1.60e-04). ETA=17:54:33, max mem: 15.9 GB 
[10/27 12:31:31 visual_prompt]: Epoch 6 / 100: avg data time: 4.21e-03, avg batch time: 0.6360, average train loss: 4.7052
[10/27 12:32:21 visual_prompt]: 	Test 100/123. loss: 1.533, 0.2255 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/27 12:32:32 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2321, average loss: 1.6831
[10/27 12:32:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.98	
[10/27 12:32:32 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/27 12:33:37 visual_prompt]: 	Training 100/1106. train loss: 5.4888,	0.6395 s / batch. (data: 8.52e-04). ETA=18:26:57, max mem: 15.9 GB 
[10/27 12:34:40 visual_prompt]: 	Training 200/1106. train loss: 4.3973,	0.6348 s / batch. (data: 9.21e-04). ETA=18:17:48, max mem: 15.9 GB 
[10/27 12:35:44 visual_prompt]: 	Training 300/1106. train loss: 2.4519,	0.6219 s / batch. (data: 1.09e-03). ETA=17:54:30, max mem: 15.9 GB 
[10/27 12:36:47 visual_prompt]: 	Training 400/1106. train loss: 4.9824,	0.6382 s / batch. (data: 9.38e-04). ETA=18:21:39, max mem: 15.9 GB 
[10/27 12:37:50 visual_prompt]: 	Training 500/1106. train loss: 3.9606,	0.6185 s / batch. (data: 3.10e-04). ETA=17:46:27, max mem: 15.9 GB 
[10/27 12:38:54 visual_prompt]: 	Training 600/1106. train loss: 9.4210,	0.6491 s / batch. (data: 8.01e-04). ETA=18:38:14, max mem: 15.9 GB 
[10/27 12:39:57 visual_prompt]: 	Training 700/1106. train loss: 7.9455,	0.6612 s / batch. (data: 5.99e-03). ETA=18:57:59, max mem: 15.9 GB 
[10/27 12:41:01 visual_prompt]: 	Training 800/1106. train loss: 0.0146,	0.6305 s / batch. (data: 9.16e-04). ETA=18:04:07, max mem: 15.9 GB 
[10/27 12:42:04 visual_prompt]: 	Training 900/1106. train loss: 1.9817,	0.6312 s / batch. (data: 7.51e-04). ETA=18:04:16, max mem: 15.9 GB 
[10/27 12:43:07 visual_prompt]: 	Training 1000/1106. train loss: 2.2688,	0.6497 s / batch. (data: 3.67e-04). ETA=18:35:00, max mem: 15.9 GB 
[10/27 12:44:10 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6185 s / batch. (data: 1.37e-04). ETA=17:40:16, max mem: 15.9 GB 
[10/27 12:44:14 visual_prompt]: Epoch 7 / 100: avg data time: 4.07e-03, avg batch time: 0.6349, average train loss: 4.3349
[10/27 12:45:04 visual_prompt]: 	Test 100/123. loss: 5.854, 0.2249 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/27 12:45:15 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2321, average loss: 5.2783
[10/27 12:45:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.50	
[10/27 12:45:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/27 12:46:20 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6306 s / batch. (data: 1.20e-02). ETA=17:59:57, max mem: 15.9 GB 
[10/27 12:47:23 visual_prompt]: 	Training 200/1106. train loss: 0.9205,	0.6187 s / batch. (data: 3.29e-04). ETA=17:38:32, max mem: 15.9 GB 
[10/27 12:48:26 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6542 s / batch. (data: 2.69e-02). ETA=18:38:11, max mem: 15.9 GB 
[10/27 12:49:30 visual_prompt]: 	Training 400/1106. train loss: 0.0004,	0.6187 s / batch. (data: 2.47e-04). ETA=17:36:29, max mem: 15.9 GB 
[10/27 12:50:33 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6334 s / batch. (data: 5.45e-03). ETA=18:00:37, max mem: 15.9 GB 
[10/27 12:51:36 visual_prompt]: 	Training 600/1106. train loss: 0.0470,	0.6228 s / batch. (data: 2.86e-04). ETA=17:41:23, max mem: 15.9 GB 
[10/27 12:52:40 visual_prompt]: 	Training 700/1106. train loss: 5.5836,	0.6174 s / batch. (data: 3.38e-04). ETA=17:31:15, max mem: 15.9 GB 
[10/27 12:53:43 visual_prompt]: 	Training 800/1106. train loss: 7.1320,	0.6334 s / batch. (data: 1.57e-02). ETA=17:57:28, max mem: 15.9 GB 
[10/27 12:54:46 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6429 s / batch. (data: 7.97e-04). ETA=18:12:28, max mem: 15.9 GB 
[10/27 12:55:50 visual_prompt]: 	Training 1000/1106. train loss: 9.0869,	0.6191 s / batch. (data: 3.19e-04). ETA=17:30:57, max mem: 15.9 GB 
[10/27 12:56:53 visual_prompt]: 	Training 1100/1106. train loss: 5.2579,	0.6179 s / batch. (data: 1.57e-04). ETA=17:27:51, max mem: 15.9 GB 
[10/27 12:56:57 visual_prompt]: Epoch 8 / 100: avg data time: 4.43e-03, avg batch time: 0.6345, average train loss: 4.3177
[10/27 12:57:47 visual_prompt]: 	Test 100/123. loss: 0.862, 0.2251 s / batch. (data: 3.15e-05)max mem: 15.94594 GB 
[10/27 12:57:57 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2332, average loss: 0.8009
[10/27 12:57:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.88	
[10/27 12:57:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/27 12:59:03 visual_prompt]: 	Training 100/1106. train loss: 21.9961,	0.6471 s / batch. (data: 8.24e-04). ETA=18:16:21, max mem: 15.9 GB 
[10/27 13:00:06 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6286 s / batch. (data: 4.59e-04). ETA=17:43:57, max mem: 15.9 GB 
[10/27 13:01:10 visual_prompt]: 	Training 300/1106. train loss: 9.5903,	0.6202 s / batch. (data: 3.19e-04). ETA=17:28:40, max mem: 15.9 GB 
[10/27 13:02:13 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6451 s / batch. (data: 1.40e-02). ETA=18:09:42, max mem: 15.9 GB 
[10/27 13:03:17 visual_prompt]: 	Training 500/1106. train loss: 13.7709,	0.6520 s / batch. (data: 8.12e-04). ETA=18:20:14, max mem: 15.9 GB 
[10/27 13:04:20 visual_prompt]: 	Training 600/1106. train loss: 2.2841,	0.6185 s / batch. (data: 2.94e-04). ETA=17:22:47, max mem: 15.9 GB 
[10/27 13:05:23 visual_prompt]: 	Training 700/1106. train loss: 0.0892,	0.6350 s / batch. (data: 8.57e-04). ETA=17:49:25, max mem: 15.9 GB 
[10/27 13:06:27 visual_prompt]: 	Training 800/1106. train loss: 1.5667,	0.6478 s / batch. (data: 7.34e-04). ETA=18:09:53, max mem: 15.9 GB 
[10/27 13:07:31 visual_prompt]: 	Training 900/1106. train loss: 0.0588,	0.6448 s / batch. (data: 7.30e-04). ETA=18:03:51, max mem: 15.9 GB 
[10/27 13:08:34 visual_prompt]: 	Training 1000/1106. train loss: 3.7881,	0.6369 s / batch. (data: 7.29e-04). ETA=17:49:29, max mem: 15.9 GB 
[10/27 13:09:37 visual_prompt]: 	Training 1100/1106. train loss: 4.4913,	0.6184 s / batch. (data: 1.57e-04). ETA=17:17:24, max mem: 15.9 GB 
[10/27 13:09:41 visual_prompt]: Epoch 9 / 100: avg data time: 4.60e-03, avg batch time: 0.6359, average train loss: 5.5590
[10/27 13:10:31 visual_prompt]: 	Test 100/123. loss: 2.988, 0.2262 s / batch. (data: 1.07e-04)max mem: 15.94594 GB 
[10/27 13:10:41 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2318, average loss: 2.6970
[10/27 13:10:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.58	
[10/27 13:10:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/27 13:11:47 visual_prompt]: 	Training 100/1106. train loss: 7.8152,	0.6386 s / batch. (data: 7.47e-04). ETA=17:50:13, max mem: 15.9 GB 
[10/27 13:12:50 visual_prompt]: 	Training 200/1106. train loss: 68.0133,	0.6137 s / batch. (data: 2.97e-04). ETA=17:07:25, max mem: 15.9 GB 
[10/27 13:13:53 visual_prompt]: 	Training 300/1106. train loss: 10.6563,	0.6440 s / batch. (data: 2.95e-04). ETA=17:57:03, max mem: 15.9 GB 
[10/27 13:14:57 visual_prompt]: 	Training 400/1106. train loss: 2.2172,	0.6302 s / batch. (data: 2.81e-04). ETA=17:32:58, max mem: 15.9 GB 
[10/27 13:16:00 visual_prompt]: 	Training 500/1106. train loss: 3.1691,	0.6412 s / batch. (data: 7.79e-04). ETA=17:50:16, max mem: 15.9 GB 
[10/27 13:17:03 visual_prompt]: 	Training 600/1106. train loss: 30.2453,	0.6580 s / batch. (data: 7.88e-04). ETA=18:17:11, max mem: 15.9 GB 
[10/27 13:18:07 visual_prompt]: 	Training 700/1106. train loss: 16.4710,	0.6192 s / batch. (data: 3.14e-04). ETA=17:11:27, max mem: 15.9 GB 
[10/27 13:19:10 visual_prompt]: 	Training 800/1106. train loss: 7.3450,	0.6581 s / batch. (data: 2.72e-02). ETA=18:15:13, max mem: 15.9 GB 
[10/27 13:20:13 visual_prompt]: 	Training 900/1106. train loss: 26.7074,	0.6335 s / batch. (data: 1.35e-02). ETA=17:33:12, max mem: 15.9 GB 
[10/27 13:21:16 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6568 s / batch. (data: 2.07e-02). ETA=18:10:50, max mem: 15.9 GB 
[10/27 13:22:19 visual_prompt]: 	Training 1100/1106. train loss: 4.5503,	0.6189 s / batch. (data: 1.62e-04). ETA=17:06:45, max mem: 15.9 GB 
[10/27 13:22:23 visual_prompt]: Epoch 10 / 100: avg data time: 4.36e-03, avg batch time: 0.6346, average train loss: 12.3611
[10/27 13:23:13 visual_prompt]: 	Test 100/123. loss: 3.482, 0.2252 s / batch. (data: 4.17e-05)max mem: 15.94594 GB 
[10/27 13:23:24 visual_prompt]: Inference (val):avg data time: 9.02e-05, avg batch time: 0.2328, average loss: 3.1380
[10/27 13:23:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.05	
[10/27 13:23:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/27 13:24:30 visual_prompt]: 	Training 100/1106. train loss: 7.2635,	0.6418 s / batch. (data: 1.10e-02). ETA=17:43:40, max mem: 15.9 GB 
[10/27 13:25:33 visual_prompt]: 	Training 200/1106. train loss: 5.5993,	0.6328 s / batch. (data: 7.86e-04). ETA=17:27:41, max mem: 15.9 GB 
[10/27 13:26:36 visual_prompt]: 	Training 300/1106. train loss: 0.7341,	0.6300 s / batch. (data: 2.74e-04). ETA=17:22:05, max mem: 15.9 GB 
[10/27 13:27:39 visual_prompt]: 	Training 400/1106. train loss: 17.8431,	0.6329 s / batch. (data: 8.14e-04). ETA=17:25:43, max mem: 15.9 GB 
[10/27 13:28:43 visual_prompt]: 	Training 500/1106. train loss: 15.9257,	0.6306 s / batch. (data: 8.07e-04). ETA=17:20:57, max mem: 15.9 GB 
[10/27 13:29:46 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6200 s / batch. (data: 3.42e-04). ETA=17:02:18, max mem: 15.9 GB 
[10/27 13:30:49 visual_prompt]: 	Training 700/1106. train loss: 10.2815,	0.6319 s / batch. (data: 7.45e-04). ETA=17:20:56, max mem: 15.9 GB 
[10/27 13:31:53 visual_prompt]: 	Training 800/1106. train loss: 12.3453,	0.6190 s / batch. (data: 2.96e-04). ETA=16:58:40, max mem: 15.9 GB 
[10/27 13:32:56 visual_prompt]: 	Training 900/1106. train loss: 0.0001,	0.6242 s / batch. (data: 3.29e-04). ETA=17:06:07, max mem: 15.9 GB 
[10/27 13:33:59 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6324 s / batch. (data: 7.85e-04). ETA=17:18:36, max mem: 15.9 GB 
[10/27 13:35:02 visual_prompt]: 	Training 1100/1106. train loss: 8.6490,	0.6194 s / batch. (data: 1.73e-04). ETA=16:56:12, max mem: 15.9 GB 
[10/27 13:35:06 visual_prompt]: Epoch 11 / 100: avg data time: 4.15e-03, avg batch time: 0.6347, average train loss: 7.9297
[10/27 13:35:56 visual_prompt]: 	Test 100/123. loss: 7.660, 0.2597 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/27 13:36:07 visual_prompt]: Inference (val):avg data time: 9.65e-05, avg batch time: 0.2324, average loss: 6.9229
[10/27 13:36:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.15	
[10/27 13:36:07 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/27 13:37:13 visual_prompt]: 	Training 100/1106. train loss: 0.4149,	0.6299 s / batch. (data: 2.87e-04). ETA=17:12:22, max mem: 15.9 GB 
[10/27 13:38:16 visual_prompt]: 	Training 200/1106. train loss: 3.8762,	0.6348 s / batch. (data: 9.06e-04). ETA=17:19:16, max mem: 15.9 GB 
[10/27 13:39:20 visual_prompt]: 	Training 300/1106. train loss: 4.4974,	0.6720 s / batch. (data: 4.03e-02). ETA=18:19:04, max mem: 15.9 GB 
[10/27 13:40:23 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6339 s / batch. (data: 7.98e-04). ETA=17:15:46, max mem: 15.9 GB 
[10/27 13:41:26 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6486 s / batch. (data: 8.71e-04). ETA=17:38:41, max mem: 15.9 GB 
[10/27 13:42:30 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6360 s / batch. (data: 8.09e-04). ETA=17:17:02, max mem: 15.9 GB 
[10/27 13:43:33 visual_prompt]: 	Training 700/1106. train loss: 1.5798,	0.6318 s / batch. (data: 3.04e-04). ETA=17:09:09, max mem: 15.9 GB 
[10/27 13:44:37 visual_prompt]: 	Training 800/1106. train loss: 7.0154,	0.6219 s / batch. (data: 3.08e-04). ETA=16:52:03, max mem: 15.9 GB 
[10/27 13:45:40 visual_prompt]: 	Training 900/1106. train loss: 0.8049,	0.6206 s / batch. (data: 2.95e-04). ETA=16:48:48, max mem: 15.9 GB 
[10/27 13:46:43 visual_prompt]: 	Training 1000/1106. train loss: 6.6023,	0.6459 s / batch. (data: 7.48e-04). ETA=17:28:50, max mem: 15.9 GB 
[10/27 13:47:46 visual_prompt]: 	Training 1100/1106. train loss: 1.1623,	0.6179 s / batch. (data: 1.54e-04). ETA=16:42:19, max mem: 15.9 GB 
[10/27 13:47:50 visual_prompt]: Epoch 12 / 100: avg data time: 5.15e-03, avg batch time: 0.6358, average train loss: 8.7840
[10/27 13:48:40 visual_prompt]: 	Test 100/123. loss: 2.609, 0.2380 s / batch. (data: 4.20e-05)max mem: 15.94594 GB 
[10/27 13:48:50 visual_prompt]: Inference (val):avg data time: 9.91e-05, avg batch time: 0.2333, average loss: 2.8878
[10/27 13:48:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.83	
[10/27 13:48:50 visual_prompt]: Best epoch 12: best metric: -2.888
[10/27 13:48:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/27 13:49:56 visual_prompt]: 	Training 100/1106. train loss: 14.7511,	0.6262 s / batch. (data: 3.27e-04). ETA=16:54:42, max mem: 15.9 GB 
[10/27 13:50:59 visual_prompt]: 	Training 200/1106. train loss: 13.2747,	0.6311 s / batch. (data: 5.47e-03). ETA=17:01:41, max mem: 15.9 GB 
[10/27 13:52:03 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6199 s / batch. (data: 3.20e-04). ETA=16:42:23, max mem: 15.9 GB 
[10/27 13:53:06 visual_prompt]: 	Training 400/1106. train loss: 3.2056,	0.6199 s / batch. (data: 7.91e-04). ETA=16:41:20, max mem: 15.9 GB 
[10/27 13:54:09 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6445 s / batch. (data: 8.21e-04). ETA=17:20:08, max mem: 15.9 GB 
[10/27 13:55:12 visual_prompt]: 	Training 600/1106. train loss: 2.6361,	0.6188 s / batch. (data: 3.17e-04). ETA=16:37:37, max mem: 15.9 GB 
[10/27 13:56:15 visual_prompt]: 	Training 700/1106. train loss: 0.0495,	0.6323 s / batch. (data: 1.05e-02). ETA=16:58:18, max mem: 15.9 GB 
[10/27 13:57:19 visual_prompt]: 	Training 800/1106. train loss: 6.6433,	0.6340 s / batch. (data: 7.97e-04). ETA=17:00:01, max mem: 15.9 GB 
[10/27 13:58:22 visual_prompt]: 	Training 900/1106. train loss: 0.0053,	0.6376 s / batch. (data: 5.44e-03). ETA=17:04:42, max mem: 15.9 GB 
[10/27 13:59:25 visual_prompt]: 	Training 1000/1106. train loss: 10.1535,	0.6331 s / batch. (data: 7.87e-04). ETA=16:56:23, max mem: 15.9 GB 
[10/27 14:00:29 visual_prompt]: 	Training 1100/1106. train loss: 14.1128,	0.6179 s / batch. (data: 1.87e-04). ETA=16:30:57, max mem: 15.9 GB 
[10/27 14:00:33 visual_prompt]: Epoch 13 / 100: avg data time: 4.49e-03, avg batch time: 0.6348, average train loss: 7.5467
[10/27 14:01:23 visual_prompt]: 	Test 100/123. loss: 2.653, 0.2356 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[10/27 14:01:33 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2325, average loss: 2.8966
[10/27 14:01:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.14	
[10/27 14:01:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/27 14:02:38 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6314 s / batch. (data: 8.45e-04). ETA=16:51:34, max mem: 15.9 GB 
[10/27 14:03:41 visual_prompt]: 	Training 200/1106. train loss: 17.6532,	0.6463 s / batch. (data: 7.94e-04). ETA=17:14:14, max mem: 15.9 GB 
[10/27 14:04:45 visual_prompt]: 	Training 300/1106. train loss: 14.0876,	0.6205 s / batch. (data: 2.88e-04). ETA=16:32:00, max mem: 15.9 GB 
[10/27 14:05:48 visual_prompt]: 	Training 400/1106. train loss: 0.2966,	0.6327 s / batch. (data: 3.10e-04). ETA=16:50:29, max mem: 15.9 GB 
[10/27 14:06:51 visual_prompt]: 	Training 500/1106. train loss: 1.1219,	0.6231 s / batch. (data: 3.08e-04). ETA=16:34:03, max mem: 15.9 GB 
[10/27 14:07:55 visual_prompt]: 	Training 600/1106. train loss: 0.8932,	0.6191 s / batch. (data: 3.45e-04). ETA=16:26:43, max mem: 15.9 GB 
[10/27 14:08:58 visual_prompt]: 	Training 700/1106. train loss: 0.0084,	0.6277 s / batch. (data: 3.15e-04). ETA=16:39:22, max mem: 15.9 GB 
[10/27 14:10:01 visual_prompt]: 	Training 800/1106. train loss: 4.9605,	0.6434 s / batch. (data: 8.15e-04). ETA=17:03:10, max mem: 15.9 GB 
[10/27 14:11:05 visual_prompt]: 	Training 900/1106. train loss: 0.7418,	0.6302 s / batch. (data: 7.43e-04). ETA=16:41:12, max mem: 15.9 GB 
[10/27 14:12:08 visual_prompt]: 	Training 1000/1106. train loss: 0.0067,	0.6360 s / batch. (data: 2.69e-04). ETA=16:49:20, max mem: 15.9 GB 
[10/27 14:13:11 visual_prompt]: 	Training 1100/1106. train loss: 2.2819,	0.6183 s / batch. (data: 1.32e-04). ETA=16:20:13, max mem: 15.9 GB 
[10/27 14:13:15 visual_prompt]: Epoch 14 / 100: avg data time: 4.16e-03, avg batch time: 0.6350, average train loss: 5.5761
[10/27 14:14:05 visual_prompt]: 	Test 100/123. loss: 1.975, 0.2358 s / batch. (data: 2.81e-05)max mem: 15.94594 GB 
[10/27 14:14:16 visual_prompt]: Inference (val):avg data time: 1.21e-04, avg batch time: 0.2318, average loss: 2.1105
[10/27 14:14:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.47	
[10/27 14:14:16 visual_prompt]: Best epoch 14: best metric: -2.111
[10/27 14:14:16 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/27 14:15:21 visual_prompt]: 	Training 100/1106. train loss: 18.9798,	0.6168 s / batch. (data: 2.76e-04). ETA=16:16:48, max mem: 15.9 GB 
[10/27 14:16:24 visual_prompt]: 	Training 200/1106. train loss: 1.2048,	0.6598 s / batch. (data: 2.79e-02). ETA=17:23:41, max mem: 15.9 GB 
[10/27 14:17:27 visual_prompt]: 	Training 300/1106. train loss: 22.0707,	0.6188 s / batch. (data: 2.99e-04). ETA=16:17:48, max mem: 15.9 GB 
[10/27 14:18:31 visual_prompt]: 	Training 400/1106. train loss: 31.9721,	0.6316 s / batch. (data: 3.21e-04). ETA=16:36:59, max mem: 15.9 GB 
[10/27 14:19:34 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6400 s / batch. (data: 7.29e-04). ETA=16:49:14, max mem: 15.9 GB 
[10/27 14:20:37 visual_prompt]: 	Training 600/1106. train loss: 21.1245,	0.6382 s / batch. (data: 8.05e-04). ETA=16:45:15, max mem: 15.9 GB 
[10/27 14:21:41 visual_prompt]: 	Training 700/1106. train loss: 10.7129,	0.6333 s / batch. (data: 3.19e-04). ETA=16:36:29, max mem: 15.9 GB 
[10/27 14:22:44 visual_prompt]: 	Training 800/1106. train loss: 4.0023,	0.6527 s / batch. (data: 7.40e-04). ETA=17:06:02, max mem: 15.9 GB 
[10/27 14:23:47 visual_prompt]: 	Training 900/1106. train loss: 9.9682,	0.6400 s / batch. (data: 5.88e-03). ETA=16:44:54, max mem: 15.9 GB 
[10/27 14:24:50 visual_prompt]: 	Training 1000/1106. train loss: 1.0281,	0.6311 s / batch. (data: 2.79e-04). ETA=16:29:54, max mem: 15.9 GB 
[10/27 14:25:54 visual_prompt]: 	Training 1100/1106. train loss: 3.6553,	0.6186 s / batch. (data: 1.54e-04). ETA=16:09:19, max mem: 15.9 GB 
[10/27 14:25:57 visual_prompt]: Epoch 15 / 100: avg data time: 3.87e-03, avg batch time: 0.6341, average train loss: 8.2214
[10/27 14:26:47 visual_prompt]: 	Test 100/123. loss: 2.355, 0.2371 s / batch. (data: 3.48e-05)max mem: 15.94594 GB 
[10/27 14:26:58 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2339, average loss: 2.1448
[10/27 14:26:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.60	
[10/27 14:26:58 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/27 14:28:03 visual_prompt]: 	Training 100/1106. train loss: 0.0091,	0.6174 s / batch. (data: 2.68e-04). ETA=16:06:18, max mem: 15.9 GB 
[10/27 14:29:06 visual_prompt]: 	Training 200/1106. train loss: 0.4700,	0.6345 s / batch. (data: 7.70e-04). ETA=16:32:02, max mem: 15.9 GB 
[10/27 14:30:10 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6319 s / batch. (data: 3.19e-04). ETA=16:26:59, max mem: 15.9 GB 
[10/27 14:31:13 visual_prompt]: 	Training 400/1106. train loss: 0.8833,	0.6188 s / batch. (data: 3.33e-04). ETA=16:05:24, max mem: 15.9 GB 
[10/27 14:32:17 visual_prompt]: 	Training 500/1106. train loss: 5.5811,	0.6199 s / batch. (data: 3.01e-04). ETA=16:06:07, max mem: 15.9 GB 
[10/27 14:33:20 visual_prompt]: 	Training 600/1106. train loss: 0.1901,	0.6345 s / batch. (data: 8.66e-04). ETA=16:27:52, max mem: 15.9 GB 
[10/27 14:34:23 visual_prompt]: 	Training 700/1106. train loss: 0.9514,	0.6367 s / batch. (data: 7.60e-04). ETA=16:30:12, max mem: 15.9 GB 
[10/27 14:35:27 visual_prompt]: 	Training 800/1106. train loss: 3.3406,	0.6186 s / batch. (data: 3.13e-04). ETA=16:00:58, max mem: 15.9 GB 
[10/27 14:36:30 visual_prompt]: 	Training 900/1106. train loss: 6.3228,	0.6205 s / batch. (data: 3.44e-04). ETA=16:02:58, max mem: 15.9 GB 
[10/27 14:37:34 visual_prompt]: 	Training 1000/1106. train loss: 8.6270,	0.6343 s / batch. (data: 7.75e-04). ETA=16:23:19, max mem: 15.9 GB 
[10/27 14:38:37 visual_prompt]: 	Training 1100/1106. train loss: 0.0001,	0.6190 s / batch. (data: 1.59e-04). ETA=15:58:29, max mem: 15.9 GB 
[10/27 14:38:41 visual_prompt]: Epoch 16 / 100: avg data time: 3.83e-03, avg batch time: 0.6353, average train loss: 7.3569
[10/27 14:39:32 visual_prompt]: 	Test 100/123. loss: 11.992, 0.2248 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/27 14:39:42 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.2337, average loss: 10.7670
[10/27 14:39:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.52	
[10/27 14:39:42 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/27 14:40:47 visual_prompt]: 	Training 100/1106. train loss: 0.0882,	0.6490 s / batch. (data: 7.57e-04). ETA=16:43:48, max mem: 15.9 GB 
[10/27 14:41:51 visual_prompt]: 	Training 200/1106. train loss: 0.9123,	0.6380 s / batch. (data: 3.15e-04). ETA=16:25:43, max mem: 15.9 GB 
[10/27 14:42:54 visual_prompt]: 	Training 300/1106. train loss: 2.6134,	0.6394 s / batch. (data: 1.60e-02). ETA=16:26:48, max mem: 15.9 GB 
[10/27 14:43:58 visual_prompt]: 	Training 400/1106. train loss: 56.8641,	0.6264 s / batch. (data: 7.41e-04). ETA=16:05:48, max mem: 15.9 GB 
[10/27 14:45:01 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6344 s / batch. (data: 7.49e-04). ETA=16:17:02, max mem: 15.9 GB 
[10/27 14:46:04 visual_prompt]: 	Training 600/1106. train loss: 29.3216,	0.6392 s / batch. (data: 5.46e-03). ETA=16:23:19, max mem: 15.9 GB 
[10/27 14:47:07 visual_prompt]: 	Training 700/1106. train loss: 0.7512,	0.6280 s / batch. (data: 3.29e-04). ETA=16:05:05, max mem: 15.9 GB 
[10/27 14:48:11 visual_prompt]: 	Training 800/1106. train loss: 25.2768,	0.6340 s / batch. (data: 8.49e-04). ETA=16:13:14, max mem: 15.9 GB 
[10/27 14:49:14 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6440 s / batch. (data: 8.00e-04). ETA=16:27:33, max mem: 15.9 GB 
[10/27 14:50:17 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6242 s / batch. (data: 3.05e-04). ETA=15:56:05, max mem: 15.9 GB 
[10/27 14:51:21 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6187 s / batch. (data: 1.63e-04). ETA=15:46:41, max mem: 15.9 GB 
[10/27 14:51:25 visual_prompt]: Epoch 17 / 100: avg data time: 4.80e-03, avg batch time: 0.6355, average train loss: 9.7448
[10/27 14:52:15 visual_prompt]: 	Test 100/123. loss: 12.298, 0.2279 s / batch. (data: 4.94e-05)max mem: 15.94594 GB 
[10/27 14:52:25 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2336, average loss: 13.3777
[10/27 14:52:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.21	
[10/27 14:52:25 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/27 14:53:31 visual_prompt]: 	Training 100/1106. train loss: 68.7991,	0.6208 s / batch. (data: 3.50e-04). ETA=15:48:41, max mem: 15.9 GB 
[10/27 14:54:34 visual_prompt]: 	Training 200/1106. train loss: 29.8651,	0.6751 s / batch. (data: 7.84e-04). ETA=17:10:37, max mem: 15.9 GB 
[10/27 14:55:37 visual_prompt]: 	Training 300/1106. train loss: 53.3660,	0.6440 s / batch. (data: 8.04e-04). ETA=16:22:06, max mem: 15.9 GB 
[10/27 14:56:40 visual_prompt]: 	Training 400/1106. train loss: 25.4260,	0.6400 s / batch. (data: 8.28e-04). ETA=16:14:54, max mem: 15.9 GB 
[10/27 14:57:44 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6334 s / batch. (data: 7.94e-04). ETA=16:03:44, max mem: 15.9 GB 
[10/27 14:58:47 visual_prompt]: 	Training 600/1106. train loss: 12.3028,	0.6230 s / batch. (data: 3.24e-04). ETA=15:46:57, max mem: 15.9 GB 
[10/27 14:59:50 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6240 s / batch. (data: 3.34e-04). ETA=15:47:23, max mem: 15.9 GB 
[10/27 15:00:53 visual_prompt]: 	Training 800/1106. train loss: 10.9799,	0.6336 s / batch. (data: 7.83e-04). ETA=16:01:00, max mem: 15.9 GB 
[10/27 15:01:56 visual_prompt]: 	Training 900/1106. train loss: 25.7168,	0.6301 s / batch. (data: 3.76e-04). ETA=15:54:34, max mem: 15.9 GB 
[10/27 15:03:00 visual_prompt]: 	Training 1000/1106. train loss: 4.0733,	0.6183 s / batch. (data: 2.94e-04). ETA=15:35:44, max mem: 15.9 GB 
[10/27 15:04:03 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6177 s / batch. (data: 1.62e-04). ETA=15:33:41, max mem: 15.9 GB 
[10/27 15:04:07 visual_prompt]: Epoch 18 / 100: avg data time: 4.36e-03, avg batch time: 0.6342, average train loss: 11.3022
[10/27 15:04:57 visual_prompt]: 	Test 100/123. loss: 7.852, 0.2477 s / batch. (data: 4.96e-05)max mem: 15.94594 GB 
[10/27 15:05:07 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.2334, average loss: 8.6300
[10/27 15:05:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.91	
[10/27 15:05:07 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/27 15:06:13 visual_prompt]: 	Training 100/1106. train loss: 0.0005,	0.6324 s / batch. (data: 7.39e-04). ETA=15:54:46, max mem: 15.9 GB 
[10/27 15:07:16 visual_prompt]: 	Training 200/1106. train loss: 11.4414,	0.6265 s / batch. (data: 7.85e-04). ETA=15:44:51, max mem: 15.9 GB 
[10/27 15:08:19 visual_prompt]: 	Training 300/1106. train loss: 0.0002,	0.6230 s / batch. (data: 2.68e-04). ETA=15:38:31, max mem: 15.9 GB 
[10/27 15:09:22 visual_prompt]: 	Training 400/1106. train loss: 0.8329,	0.6615 s / batch. (data: 8.14e-04). ETA=16:35:30, max mem: 15.9 GB 
[10/27 15:10:26 visual_prompt]: 	Training 500/1106. train loss: 2.8135,	0.6345 s / batch. (data: 1.60e-02). ETA=15:53:46, max mem: 15.9 GB 
[10/27 15:11:29 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6306 s / batch. (data: 8.19e-04). ETA=15:46:52, max mem: 15.9 GB 
[10/27 15:12:33 visual_prompt]: 	Training 700/1106. train loss: 1.3544,	0.6198 s / batch. (data: 3.20e-04). ETA=15:29:41, max mem: 15.9 GB 
[10/27 15:13:36 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6339 s / batch. (data: 7.75e-04). ETA=15:49:38, max mem: 15.9 GB 
[10/27 15:14:39 visual_prompt]: 	Training 900/1106. train loss: 2.5315,	0.6341 s / batch. (data: 8.42e-04). ETA=15:48:56, max mem: 15.9 GB 
[10/27 15:15:42 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6420 s / batch. (data: 7.69e-04). ETA=15:59:38, max mem: 15.9 GB 
[10/27 15:16:46 visual_prompt]: 	Training 1100/1106. train loss: 82.4807,	0.6186 s / batch. (data: 1.89e-04). ETA=15:23:37, max mem: 15.9 GB 
[10/27 15:16:49 visual_prompt]: Epoch 19 / 100: avg data time: 4.48e-03, avg batch time: 0.6349, average train loss: 9.3763
[10/27 15:17:39 visual_prompt]: 	Test 100/123. loss: 56.383, 0.2253 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/27 15:17:50 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2330, average loss: 50.9253
[10/27 15:17:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.69	
[10/27 15:17:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[10/27 15:18:55 visual_prompt]: 	Training 100/1106. train loss: 0.4418,	0.6182 s / batch. (data: 3.35e-04). ETA=15:21:56, max mem: 15.9 GB 
[10/27 15:19:58 visual_prompt]: 	Training 200/1106. train loss: 4.9040,	0.6320 s / batch. (data: 2.74e-04). ETA=15:41:31, max mem: 15.9 GB 
[10/27 15:21:02 visual_prompt]: 	Training 300/1106. train loss: 8.0751,	0.6680 s / batch. (data: 9.83e-04). ETA=16:34:03, max mem: 15.9 GB 
[10/27 15:22:05 visual_prompt]: 	Training 400/1106. train loss: 0.6204,	0.6285 s / batch. (data: 3.26e-04). ETA=15:34:13, max mem: 15.9 GB 
[10/27 15:23:08 visual_prompt]: 	Training 500/1106. train loss: 4.0899,	0.6281 s / batch. (data: 1.04e-02). ETA=15:32:32, max mem: 15.9 GB 
[10/27 15:24:11 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6441 s / batch. (data: 8.11e-04). ETA=15:55:15, max mem: 15.9 GB 
[10/27 15:25:15 visual_prompt]: 	Training 700/1106. train loss: 0.0054,	0.6440 s / batch. (data: 7.42e-04). ETA=15:54:05, max mem: 15.9 GB 
[10/27 15:26:18 visual_prompt]: 	Training 800/1106. train loss: 0.0727,	0.6314 s / batch. (data: 7.04e-04). ETA=15:34:17, max mem: 15.9 GB 
[10/27 15:27:21 visual_prompt]: 	Training 900/1106. train loss: 17.7785,	0.6626 s / batch. (data: 8.12e-04). ETA=16:19:18, max mem: 15.9 GB 
[10/27 15:28:24 visual_prompt]: 	Training 1000/1106. train loss: 0.1702,	0.6318 s / batch. (data: 1.11e-03). ETA=15:32:51, max mem: 15.9 GB 
[10/27 15:29:28 visual_prompt]: 	Training 1100/1106. train loss: 0.0727,	0.6183 s / batch. (data: 1.62e-04). ETA=15:11:53, max mem: 15.9 GB 
[10/27 15:29:32 visual_prompt]: Epoch 20 / 100: avg data time: 4.74e-03, avg batch time: 0.6346, average train loss: 6.9857
[10/27 15:30:21 visual_prompt]: 	Test 100/123. loss: 1.034, 0.2264 s / batch. (data: 2.81e-05)max mem: 15.94594 GB 
[10/27 15:30:32 visual_prompt]: Inference (val):avg data time: 1.00e-04, avg batch time: 0.2333, average loss: 0.9169
[10/27 15:30:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.76	
[10/27 15:30:32 visual_prompt]: Best epoch 20: best metric: -0.917
[10/27 15:30:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[10/27 15:31:38 visual_prompt]: 	Training 100/1106. train loss: 1.9134,	0.6440 s / batch. (data: 3.20e-04). ETA=15:48:36, max mem: 15.9 GB 
[10/27 15:32:41 visual_prompt]: 	Training 200/1106. train loss: 12.8070,	0.6525 s / batch. (data: 5.89e-03). ETA=15:59:59, max mem: 15.9 GB 
[10/27 15:33:44 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6336 s / batch. (data: 3.25e-04). ETA=15:31:10, max mem: 15.9 GB 
[10/27 15:34:48 visual_prompt]: 	Training 400/1106. train loss: 14.7999,	0.6188 s / batch. (data: 7.32e-04). ETA=15:08:25, max mem: 15.9 GB 
[10/27 15:35:51 visual_prompt]: 	Training 500/1106. train loss: 6.3772,	0.6186 s / batch. (data: 3.04e-04). ETA=15:07:02, max mem: 15.9 GB 
[10/27 15:36:54 visual_prompt]: 	Training 600/1106. train loss: 0.5279,	0.6481 s / batch. (data: 1.22e-03). ETA=15:49:15, max mem: 15.9 GB 
[10/27 15:37:57 visual_prompt]: 	Training 700/1106. train loss: 31.3091,	0.6302 s / batch. (data: 9.40e-04). ETA=15:21:56, max mem: 15.9 GB 
[10/27 15:39:01 visual_prompt]: 	Training 800/1106. train loss: 12.0341,	0.6188 s / batch. (data: 3.28e-04). ETA=15:04:15, max mem: 15.9 GB 
[10/27 15:40:04 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6185 s / batch. (data: 3.18e-04). ETA=15:02:44, max mem: 15.9 GB 
[10/27 15:41:07 visual_prompt]: 	Training 1000/1106. train loss: 20.7046,	0.6346 s / batch. (data: 3.47e-04). ETA=15:25:14, max mem: 15.9 GB 
[10/27 15:42:10 visual_prompt]: 	Training 1100/1106. train loss: 6.2113,	0.6183 s / batch. (data: 1.45e-04). ETA=15:00:28, max mem: 15.9 GB 
[10/27 15:42:14 visual_prompt]: Epoch 21 / 100: avg data time: 4.39e-03, avg batch time: 0.6347, average train loss: 12.1985
[10/27 15:43:04 visual_prompt]: 	Test 100/123. loss: 3.343, 0.2306 s / batch. (data: 3.89e-05)max mem: 15.94594 GB 
[10/27 15:43:14 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.2320, average loss: 3.6656
[10/27 15:43:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.03	
[10/27 15:43:14 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[10/27 15:44:20 visual_prompt]: 	Training 100/1106. train loss: 16.0304,	0.6273 s / batch. (data: 3.01e-04). ETA=15:12:30, max mem: 15.9 GB 
[10/27 15:45:23 visual_prompt]: 	Training 200/1106. train loss: 0.0006,	0.6273 s / batch. (data: 3.90e-04). ETA=15:11:21, max mem: 15.9 GB 
[10/27 15:46:26 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6310 s / batch. (data: 8.19e-04). ETA=15:15:47, max mem: 15.9 GB 
[10/27 15:47:29 visual_prompt]: 	Training 400/1106. train loss: 11.0765,	0.6326 s / batch. (data: 3.53e-04). ETA=15:17:01, max mem: 15.9 GB 
[10/27 15:48:33 visual_prompt]: 	Training 500/1106. train loss: 9.6733,	0.6372 s / batch. (data: 7.32e-04). ETA=15:22:37, max mem: 15.9 GB 
[10/27 15:49:36 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6316 s / batch. (data: 8.02e-04). ETA=15:13:25, max mem: 15.9 GB 
[10/27 15:50:39 visual_prompt]: 	Training 700/1106. train loss: 9.0825,	0.6351 s / batch. (data: 2.96e-04). ETA=15:17:26, max mem: 15.9 GB 
[10/27 15:51:42 visual_prompt]: 	Training 800/1106. train loss: 34.9183,	0.6385 s / batch. (data: 8.10e-04). ETA=15:21:13, max mem: 15.9 GB 
[10/27 15:52:46 visual_prompt]: 	Training 900/1106. train loss: 5.7842,	0.6200 s / batch. (data: 3.27e-04). ETA=14:53:29, max mem: 15.9 GB 
[10/27 15:53:49 visual_prompt]: 	Training 1000/1106. train loss: 1.3900,	0.6235 s / batch. (data: 7.43e-04). ETA=14:57:35, max mem: 15.9 GB 
[10/27 15:54:52 visual_prompt]: 	Training 1100/1106. train loss: 15.1281,	0.6185 s / batch. (data: 1.32e-04). ETA=14:49:18, max mem: 15.9 GB 
[10/27 15:54:56 visual_prompt]: Epoch 22 / 100: avg data time: 4.13e-03, avg batch time: 0.6344, average train loss: 8.9150
[10/27 15:55:46 visual_prompt]: 	Test 100/123. loss: 0.694, 0.2476 s / batch. (data: 3.77e-05)max mem: 15.94594 GB 
[10/27 15:55:57 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.2345, average loss: 0.6931
[10/27 15:55:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.86	
[10/27 15:55:57 visual_prompt]: Best epoch 22: best metric: -0.693
[10/27 15:55:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[10/27 15:57:02 visual_prompt]: 	Training 100/1106. train loss: 0.0431,	0.6663 s / batch. (data: 7.38e-04). ETA=15:56:53, max mem: 15.9 GB 
[10/27 15:58:06 visual_prompt]: 	Training 200/1106. train loss: 7.1968,	0.6202 s / batch. (data: 7.44e-04). ETA=14:49:37, max mem: 15.9 GB 
[10/27 15:59:09 visual_prompt]: 	Training 300/1106. train loss: 0.3790,	0.6439 s / batch. (data: 7.81e-04). ETA=15:22:32, max mem: 15.9 GB 
[10/27 16:00:13 visual_prompt]: 	Training 400/1106. train loss: 1.2740,	0.6297 s / batch. (data: 3.15e-04). ETA=15:01:07, max mem: 15.9 GB 
[10/27 16:01:16 visual_prompt]: 	Training 500/1106. train loss: 5.2417,	0.6506 s / batch. (data: 1.10e-02). ETA=15:30:02, max mem: 15.9 GB 
[10/27 16:02:19 visual_prompt]: 	Training 600/1106. train loss: 1.6870,	0.6455 s / batch. (data: 7.98e-04). ETA=15:21:37, max mem: 15.9 GB 
[10/27 16:03:22 visual_prompt]: 	Training 700/1106. train loss: 2.5083,	0.6187 s / batch. (data: 3.14e-04). ETA=14:42:17, max mem: 15.9 GB 
[10/27 16:04:25 visual_prompt]: 	Training 800/1106. train loss: 16.0736,	0.6169 s / batch. (data: 3.66e-04). ETA=14:38:48, max mem: 15.9 GB 
[10/27 16:05:28 visual_prompt]: 	Training 900/1106. train loss: 4.2884,	0.6393 s / batch. (data: 7.75e-04). ETA=15:09:32, max mem: 15.9 GB 
[10/27 16:06:32 visual_prompt]: 	Training 1000/1106. train loss: 1.0131,	0.6409 s / batch. (data: 1.28e-02). ETA=15:10:47, max mem: 15.9 GB 
[10/27 16:07:35 visual_prompt]: 	Training 1100/1106. train loss: 2.2470,	0.6173 s / batch. (data: 1.67e-04). ETA=14:36:13, max mem: 15.9 GB 
[10/27 16:07:38 visual_prompt]: Epoch 23 / 100: avg data time: 4.72e-03, avg batch time: 0.6345, average train loss: 7.9065
[10/27 16:08:28 visual_prompt]: 	Test 100/123. loss: 3.930, 0.2242 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/27 16:08:39 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2319, average loss: 3.5429
[10/27 16:08:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.02	
[10/27 16:08:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[10/27 16:09:44 visual_prompt]: 	Training 100/1106. train loss: 2.1075,	0.6452 s / batch. (data: 8.54e-04). ETA=15:14:39, max mem: 15.9 GB 
[10/27 16:10:47 visual_prompt]: 	Training 200/1106. train loss: 19.8245,	0.6332 s / batch. (data: 8.17e-04). ETA=14:56:39, max mem: 15.9 GB 
[10/27 16:11:51 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6182 s / batch. (data: 3.22e-04). ETA=14:34:23, max mem: 15.9 GB 
[10/27 16:12:54 visual_prompt]: 	Training 400/1106. train loss: 2.1358,	0.6278 s / batch. (data: 8.94e-04). ETA=14:46:50, max mem: 15.9 GB 
[10/27 16:13:57 visual_prompt]: 	Training 500/1106. train loss: 24.1704,	0.6188 s / batch. (data: 5.32e-04). ETA=14:33:11, max mem: 15.9 GB 
[10/27 16:15:01 visual_prompt]: 	Training 600/1106. train loss: 10.2804,	0.6317 s / batch. (data: 8.72e-04). ETA=14:50:20, max mem: 15.9 GB 
[10/27 16:16:04 visual_prompt]: 	Training 700/1106. train loss: 16.1754,	0.6440 s / batch. (data: 7.88e-04). ETA=15:06:35, max mem: 15.9 GB 
[10/27 16:17:08 visual_prompt]: 	Training 800/1106. train loss: 3.9460,	0.6568 s / batch. (data: 7.98e-04). ETA=15:23:30, max mem: 15.9 GB 
[10/27 16:18:11 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6473 s / batch. (data: 7.80e-04). ETA=15:09:06, max mem: 15.9 GB 
[10/27 16:19:14 visual_prompt]: 	Training 1000/1106. train loss: 4.2946,	0.6200 s / batch. (data: 3.21e-04). ETA=14:29:37, max mem: 15.9 GB 
[10/27 16:20:18 visual_prompt]: 	Training 1100/1106. train loss: 0.4278,	0.6190 s / batch. (data: 1.65e-04). ETA=14:27:14, max mem: 15.9 GB 
[10/27 16:20:21 visual_prompt]: Epoch 24 / 100: avg data time: 4.58e-03, avg batch time: 0.6354, average train loss: 6.8368
[10/27 16:21:11 visual_prompt]: 	Test 100/123. loss: 4.510, 0.2252 s / batch. (data: 4.20e-05)max mem: 15.94594 GB 
[10/27 16:21:22 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.2331, average loss: 4.0938
[10/27 16:21:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.32	
[10/27 16:21:22 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[10/27 16:22:27 visual_prompt]: 	Training 100/1106. train loss: 0.8545,	0.6740 s / batch. (data: 1.09e-02). ETA=15:43:07, max mem: 15.9 GB 
[10/27 16:23:31 visual_prompt]: 	Training 200/1106. train loss: 4.2258,	0.6378 s / batch. (data: 5.41e-03). ETA=14:51:22, max mem: 15.9 GB 
[10/27 16:24:34 visual_prompt]: 	Training 300/1106. train loss: 2.3047,	0.6328 s / batch. (data: 7.88e-04). ETA=14:43:24, max mem: 15.9 GB 
[10/27 16:25:37 visual_prompt]: 	Training 400/1106. train loss: 14.9731,	0.6469 s / batch. (data: 7.59e-04). ETA=15:01:58, max mem: 15.9 GB 
[10/27 16:26:40 visual_prompt]: 	Training 500/1106. train loss: 8.5192,	0.6193 s / batch. (data: 3.32e-04). ETA=14:22:24, max mem: 15.9 GB 
[10/27 16:27:43 visual_prompt]: 	Training 600/1106. train loss: 15.4444,	0.6387 s / batch. (data: 7.79e-04). ETA=14:48:21, max mem: 15.9 GB 
[10/27 16:28:47 visual_prompt]: 	Training 700/1106. train loss: 27.7479,	0.6451 s / batch. (data: 8.12e-04). ETA=14:56:14, max mem: 15.9 GB 
[10/27 16:29:50 visual_prompt]: 	Training 800/1106. train loss: 0.5389,	0.6361 s / batch. (data: 1.22e-03). ETA=14:42:39, max mem: 15.9 GB 
[10/27 16:30:53 visual_prompt]: 	Training 900/1106. train loss: 10.6007,	0.6184 s / batch. (data: 3.14e-04). ETA=14:17:01, max mem: 15.9 GB 
[10/27 16:31:56 visual_prompt]: 	Training 1000/1106. train loss: 2.1197,	0.6192 s / batch. (data: 2.83e-04). ETA=14:17:08, max mem: 15.9 GB 
[10/27 16:33:00 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6198 s / batch. (data: 1.59e-04). ETA=14:16:57, max mem: 15.9 GB 
[10/27 16:33:04 visual_prompt]: Epoch 25 / 100: avg data time: 4.47e-03, avg batch time: 0.6345, average train loss: 10.8051
[10/27 16:33:53 visual_prompt]: 	Test 100/123. loss: 2.400, 0.2285 s / batch. (data: 2.57e-05)max mem: 15.94594 GB 
[10/27 16:34:04 visual_prompt]: Inference (val):avg data time: 1.91e-04, avg batch time: 0.2320, average loss: 2.1631
[10/27 16:34:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.54	
[10/27 16:34:04 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[10/27 16:35:09 visual_prompt]: 	Training 100/1106. train loss: 1.9134,	0.6334 s / batch. (data: 3.20e-04). ETA=14:34:39, max mem: 15.9 GB 
[10/27 16:36:12 visual_prompt]: 	Training 200/1106. train loss: 2.2088,	0.6205 s / batch. (data: 3.18e-04). ETA=14:15:45, max mem: 15.9 GB 
[10/27 16:37:16 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6331 s / batch. (data: 3.14e-04). ETA=14:32:02, max mem: 15.9 GB 
[10/27 16:38:19 visual_prompt]: 	Training 400/1106. train loss: 24.9665,	0.6370 s / batch. (data: 8.05e-04). ETA=14:36:21, max mem: 15.9 GB 
[10/27 16:39:23 visual_prompt]: 	Training 500/1106. train loss: 14.0132,	0.6453 s / batch. (data: 7.97e-04). ETA=14:46:43, max mem: 15.9 GB 
[10/27 16:40:26 visual_prompt]: 	Training 600/1106. train loss: 0.0017,	0.6442 s / batch. (data: 2.79e-04). ETA=14:44:06, max mem: 15.9 GB 
[10/27 16:41:29 visual_prompt]: 	Training 700/1106. train loss: 39.7500,	0.6283 s / batch. (data: 3.12e-04). ETA=14:21:17, max mem: 15.9 GB 
[10/27 16:42:32 visual_prompt]: 	Training 800/1106. train loss: 11.8113,	0.6258 s / batch. (data: 3.19e-04). ETA=14:16:50, max mem: 15.9 GB 
[10/27 16:43:36 visual_prompt]: 	Training 900/1106. train loss: 9.7245,	0.6201 s / batch. (data: 3.29e-04). ETA=14:07:58, max mem: 15.9 GB 
[10/27 16:44:39 visual_prompt]: 	Training 1000/1106. train loss: 7.0502,	0.6352 s / batch. (data: 7.74e-04). ETA=14:27:31, max mem: 15.9 GB 
[10/27 16:45:42 visual_prompt]: 	Training 1100/1106. train loss: 0.0586,	0.6201 s / batch. (data: 1.55e-04). ETA=14:05:56, max mem: 15.9 GB 
[10/27 16:45:46 visual_prompt]: Epoch 26 / 100: avg data time: 4.27e-03, avg batch time: 0.6351, average train loss: 10.2784
[10/27 16:46:36 visual_prompt]: 	Test 100/123. loss: 3.158, 0.2384 s / batch. (data: 3.36e-05)max mem: 15.94594 GB 
[10/27 16:46:47 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2334, average loss: 2.7926
[10/27 16:46:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.24	
[10/27 16:46:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[10/27 16:47:52 visual_prompt]: 	Training 100/1106. train loss: 48.3341,	0.6454 s / batch. (data: 7.78e-04). ETA=14:39:13, max mem: 15.9 GB 
[10/27 16:48:56 visual_prompt]: 	Training 200/1106. train loss: 5.1119,	0.6342 s / batch. (data: 7.90e-04). ETA=14:23:02, max mem: 15.9 GB 
[10/27 16:49:59 visual_prompt]: 	Training 300/1106. train loss: 2.8399,	0.6188 s / batch. (data: 3.29e-04). ETA=14:01:01, max mem: 15.9 GB 
[10/27 16:51:02 visual_prompt]: 	Training 400/1106. train loss: 21.9769,	0.6317 s / batch. (data: 1.20e-02). ETA=14:17:30, max mem: 15.9 GB 
[10/27 16:52:06 visual_prompt]: 	Training 500/1106. train loss: 16.5757,	0.6504 s / batch. (data: 1.58e-02). ETA=14:41:50, max mem: 15.9 GB 
[10/27 16:53:09 visual_prompt]: 	Training 600/1106. train loss: 28.5605,	0.6300 s / batch. (data: 1.24e-02). ETA=14:13:07, max mem: 15.9 GB 
[10/27 16:54:13 visual_prompt]: 	Training 700/1106. train loss: 0.0298,	0.6316 s / batch. (data: 7.96e-04). ETA=14:14:13, max mem: 15.9 GB 
[10/27 16:55:16 visual_prompt]: 	Training 800/1106. train loss: 1.1037,	0.6339 s / batch. (data: 2.77e-04). ETA=14:16:13, max mem: 15.9 GB 
[10/27 16:56:19 visual_prompt]: 	Training 900/1106. train loss: 4.0901,	0.6186 s / batch. (data: 3.50e-04). ETA=13:54:33, max mem: 15.9 GB 
[10/27 16:57:23 visual_prompt]: 	Training 1000/1106. train loss: 0.0438,	0.6297 s / batch. (data: 7.90e-04). ETA=14:08:30, max mem: 15.9 GB 
[10/27 16:58:26 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6181 s / batch. (data: 1.49e-04). ETA=13:51:48, max mem: 15.9 GB 
[10/27 16:58:30 visual_prompt]: Epoch 27 / 100: avg data time: 4.57e-03, avg batch time: 0.6356, average train loss: 7.5114
[10/27 16:59:19 visual_prompt]: 	Test 100/123. loss: 24.178, 0.2277 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/27 16:59:30 visual_prompt]: Inference (val):avg data time: 4.47e-05, avg batch time: 0.2325, average loss: 26.5256
[10/27 16:59:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.47	
[10/27 16:59:30 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[10/27 17:00:35 visual_prompt]: 	Training 100/1106. train loss: 0.0001,	0.6304 s / batch. (data: 7.68e-04). ETA=14:07:17, max mem: 15.9 GB 
[10/27 17:01:39 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6322 s / batch. (data: 3.22e-04). ETA=14:08:34, max mem: 15.9 GB 
[10/27 17:02:42 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6350 s / batch. (data: 7.98e-04). ETA=14:11:15, max mem: 15.9 GB 
[10/27 17:03:45 visual_prompt]: 	Training 400/1106. train loss: 0.0303,	0.6325 s / batch. (data: 1.22e-03). ETA=14:06:56, max mem: 15.9 GB 
[10/27 17:04:49 visual_prompt]: 	Training 500/1106. train loss: 0.9749,	0.6688 s / batch. (data: 4.10e-02). ETA=14:54:26, max mem: 15.9 GB 
[10/27 17:05:52 visual_prompt]: 	Training 600/1106. train loss: 20.2877,	0.6448 s / batch. (data: 1.70e-02). ETA=14:21:14, max mem: 15.9 GB 
[10/27 17:06:55 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6401 s / batch. (data: 2.99e-04). ETA=14:13:49, max mem: 15.9 GB 
[10/27 17:07:59 visual_prompt]: 	Training 800/1106. train loss: 41.8715,	0.6323 s / batch. (data: 8.07e-04). ETA=14:02:22, max mem: 15.9 GB 
[10/27 17:09:02 visual_prompt]: 	Training 900/1106. train loss: 1.2115,	0.6195 s / batch. (data: 3.06e-04). ETA=13:44:21, max mem: 15.9 GB 
[10/27 17:10:05 visual_prompt]: 	Training 1000/1106. train loss: 0.2862,	0.6524 s / batch. (data: 3.62e-04). ETA=14:27:03, max mem: 15.9 GB 
[10/27 17:11:09 visual_prompt]: 	Training 1100/1106. train loss: 22.0361,	0.6189 s / batch. (data: 1.51e-04). ETA=13:41:25, max mem: 15.9 GB 
[10/27 17:11:12 visual_prompt]: Epoch 28 / 100: avg data time: 4.26e-03, avg batch time: 0.6350, average train loss: 8.5373
[10/27 17:12:02 visual_prompt]: 	Test 100/123. loss: 15.826, 0.2358 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[10/27 17:12:13 visual_prompt]: Inference (val):avg data time: 9.80e-05, avg batch time: 0.2321, average loss: 17.4510
[10/27 17:12:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.68	
[10/27 17:12:13 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[10/27 17:13:18 visual_prompt]: 	Training 100/1106. train loss: 22.8414,	0.6209 s / batch. (data: 3.00e-04). ETA=13:42:58, max mem: 15.9 GB 
[10/27 17:14:22 visual_prompt]: 	Training 200/1106. train loss: 17.3576,	0.6304 s / batch. (data: 1.16e-02). ETA=13:54:35, max mem: 15.9 GB 
[10/27 17:15:25 visual_prompt]: 	Training 300/1106. train loss: 9.1510,	0.6315 s / batch. (data: 8.37e-04). ETA=13:54:54, max mem: 15.9 GB 
[10/27 17:16:28 visual_prompt]: 	Training 400/1106. train loss: 7.3559,	0.6319 s / batch. (data: 7.72e-04). ETA=13:54:23, max mem: 15.9 GB 
[10/27 17:17:31 visual_prompt]: 	Training 500/1106. train loss: 3.2033,	0.6396 s / batch. (data: 8.25e-04). ETA=14:03:29, max mem: 15.9 GB 
[10/27 17:18:34 visual_prompt]: 	Training 600/1106. train loss: 8.5749,	0.6217 s / batch. (data: 3.30e-04). ETA=13:38:52, max mem: 15.9 GB 
[10/27 17:19:38 visual_prompt]: 	Training 700/1106. train loss: 8.0046,	0.6399 s / batch. (data: 2.70e-04). ETA=14:01:45, max mem: 15.9 GB 
[10/27 17:20:41 visual_prompt]: 	Training 800/1106. train loss: 0.0019,	0.6331 s / batch. (data: 7.62e-04). ETA=13:51:51, max mem: 15.9 GB 
[10/27 17:21:45 visual_prompt]: 	Training 900/1106. train loss: 0.0310,	0.6283 s / batch. (data: 3.22e-04). ETA=13:44:27, max mem: 15.9 GB 
[10/27 17:22:48 visual_prompt]: 	Training 1000/1106. train loss: 46.2806,	0.6185 s / batch. (data: 3.32e-04). ETA=13:30:35, max mem: 15.9 GB 
[10/27 17:23:51 visual_prompt]: 	Training 1100/1106. train loss: 2.5861,	0.6182 s / batch. (data: 1.89e-04). ETA=13:29:11, max mem: 15.9 GB 
[10/27 17:23:55 visual_prompt]: Epoch 29 / 100: avg data time: 3.85e-03, avg batch time: 0.6347, average train loss: 9.1896
[10/27 17:24:45 visual_prompt]: 	Test 100/123. loss: 5.203, 0.2400 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/27 17:24:55 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2325, average loss: 4.8068
[10/27 17:24:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.34	
[10/27 17:24:55 visual_prompt]: Stopping early.
[10/27 17:24:55 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 17:24:55 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 17:24:55 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 17:24:55 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 17:24:55 visual_prompt]: Training with config:
[10/27 17:24:55 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr2.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 17:24:55 visual_prompt]: Loading training data...
[10/27 17:24:55 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 17:24:55 visual_prompt]: Loading validation data...
[10/27 17:24:55 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 17:24:55 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/27 17:24:58 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/27 17:24:58 visual_prompt]: tuned percent:0.522
[10/27 17:24:58 visual_prompt]: Device used for model: 0
[10/27 17:24:58 visual_prompt]: Setting up Evaluator...
[10/27 17:24:58 visual_prompt]: Setting up Trainer...
[10/27 17:24:58 visual_prompt]: 	Setting up the optimizer...
[10/27 17:24:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 17:26:03 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6264 s / batch. (data: 3.25e-04). ETA=19:13:38, max mem: 15.9 GB 
[10/27 17:27:06 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6474 s / batch. (data: 8.22e-04). ETA=19:51:15, max mem: 15.9 GB 
[10/27 17:28:10 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6330 s / batch. (data: 1.37e-02). ETA=19:23:44, max mem: 15.9 GB 
[10/27 17:29:13 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6331 s / batch. (data: 3.18e-04). ETA=19:22:47, max mem: 15.9 GB 
[10/27 17:30:16 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6562 s / batch. (data: 2.42e-02). ETA=20:04:05, max mem: 15.9 GB 
[10/27 17:31:20 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6193 s / batch. (data: 3.16e-04). ETA=18:55:18, max mem: 15.9 GB 
[10/27 17:32:23 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6468 s / batch. (data: 2.78e-02). ETA=19:44:41, max mem: 15.9 GB 
[10/27 17:33:26 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6421 s / batch. (data: 7.76e-04). ETA=19:34:58, max mem: 15.9 GB 
[10/27 17:34:30 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6440 s / batch. (data: 2.92e-04). ETA=19:37:30, max mem: 15.9 GB 
[10/27 17:35:33 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6402 s / batch. (data: 7.74e-04). ETA=19:29:21, max mem: 15.9 GB 
[10/27 17:36:36 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6177 s / batch. (data: 1.35e-04). ETA=18:47:18, max mem: 15.9 GB 
[10/27 17:36:40 visual_prompt]: Epoch 1 / 100: avg data time: 4.68e-03, avg batch time: 0.6351, average train loss: 1.4028
[10/27 17:37:30 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2478 s / batch. (data: 1.13e-02)max mem: 15.94594 GB 
[10/27 17:37:41 visual_prompt]: Inference (val):avg data time: 1.30e-04, avg batch time: 0.2335, average loss: 1.3505
[10/27 17:37:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/27 17:37:41 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/27 17:38:45 visual_prompt]: 	Training 100/1106. train loss: 0.9431,	0.6450 s / batch. (data: 5.43e-03). ETA=19:35:56, max mem: 15.9 GB 
[10/27 17:39:49 visual_prompt]: 	Training 200/1106. train loss: 0.8005,	0.6214 s / batch. (data: 3.24e-04). ETA=18:51:52, max mem: 15.9 GB 
[10/27 17:40:52 visual_prompt]: 	Training 300/1106. train loss: 0.5077,	0.6475 s / batch. (data: 7.06e-04). ETA=19:38:21, max mem: 15.9 GB 
[10/27 17:41:55 visual_prompt]: 	Training 400/1106. train loss: 0.0008,	0.6213 s / batch. (data: 1.06e-03). ETA=18:49:44, max mem: 15.9 GB 
[10/27 17:42:59 visual_prompt]: 	Training 500/1106. train loss: 6.1416,	0.6185 s / batch. (data: 2.73e-04). ETA=18:43:28, max mem: 15.9 GB 
[10/27 17:44:02 visual_prompt]: 	Training 600/1106. train loss: 0.7661,	0.6258 s / batch. (data: 3.30e-04). ETA=18:55:42, max mem: 15.9 GB 
[10/27 17:45:05 visual_prompt]: 	Training 700/1106. train loss: 0.0200,	0.6195 s / batch. (data: 7.51e-04). ETA=18:43:20, max mem: 15.9 GB 
[10/27 17:46:08 visual_prompt]: 	Training 800/1106. train loss: 0.7383,	0.6240 s / batch. (data: 5.44e-03). ETA=18:50:22, max mem: 15.9 GB 
[10/27 17:47:12 visual_prompt]: 	Training 900/1106. train loss: 1.4065,	0.6340 s / batch. (data: 7.56e-04). ETA=19:07:23, max mem: 15.9 GB 
[10/27 17:48:15 visual_prompt]: 	Training 1000/1106. train loss: 0.1558,	0.6256 s / batch. (data: 4.19e-04). ETA=18:51:15, max mem: 15.9 GB 
[10/27 17:49:18 visual_prompt]: 	Training 1100/1106. train loss: 0.3888,	0.6179 s / batch. (data: 1.51e-04). ETA=18:36:19, max mem: 15.9 GB 
[10/27 17:49:22 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-03, avg batch time: 0.6339, average train loss: 1.8660
[10/27 17:50:12 visual_prompt]: 	Test 100/123. loss: 2.867, 0.2285 s / batch. (data: 4.84e-05)max mem: 15.94594 GB 
[10/27 17:50:22 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2335, average loss: 2.6052
[10/27 17:50:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.87	
[10/27 17:50:22 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/27 17:51:29 visual_prompt]: 	Training 100/1106. train loss: 0.6027,	0.6358 s / batch. (data: 8.19e-04). ETA=19:07:27, max mem: 15.9 GB 
[10/27 17:52:32 visual_prompt]: 	Training 200/1106. train loss: 0.0006,	0.6353 s / batch. (data: 8.18e-04). ETA=19:05:31, max mem: 15.9 GB 
[10/27 17:53:35 visual_prompt]: 	Training 300/1106. train loss: 0.2510,	0.6493 s / batch. (data: 1.10e-02). ETA=19:29:43, max mem: 15.9 GB 
[10/27 17:54:39 visual_prompt]: 	Training 400/1106. train loss: 0.8740,	0.6471 s / batch. (data: 7.78e-04). ETA=19:24:36, max mem: 15.9 GB 
[10/27 17:55:42 visual_prompt]: 	Training 500/1106. train loss: 6.9019,	0.6182 s / batch. (data: 4.89e-04). ETA=18:31:32, max mem: 15.9 GB 
[10/27 17:56:45 visual_prompt]: 	Training 600/1106. train loss: 0.0003,	0.6198 s / batch. (data: 4.68e-04). ETA=18:33:26, max mem: 15.9 GB 
[10/27 17:57:49 visual_prompt]: 	Training 700/1106. train loss: 0.9003,	0.6199 s / batch. (data: 3.21e-04). ETA=18:32:33, max mem: 15.9 GB 
[10/27 17:58:52 visual_prompt]: 	Training 800/1106. train loss: 1.4127,	0.6275 s / batch. (data: 2.78e-04). ETA=18:45:15, max mem: 15.9 GB 
[10/27 17:59:55 visual_prompt]: 	Training 900/1106. train loss: 1.2792,	0.6335 s / batch. (data: 7.72e-04). ETA=18:54:53, max mem: 15.9 GB 
[10/27 18:00:59 visual_prompt]: 	Training 1000/1106. train loss: 3.9079,	0.6476 s / batch. (data: 8.17e-04). ETA=19:19:00, max mem: 15.9 GB 
[10/27 18:02:02 visual_prompt]: 	Training 1100/1106. train loss: 2.5543,	0.6180 s / batch. (data: 1.41e-04). ETA=18:25:08, max mem: 15.9 GB 
[10/27 18:02:06 visual_prompt]: Epoch 3 / 100: avg data time: 5.08e-03, avg batch time: 0.6360, average train loss: 2.2228
[10/27 18:02:55 visual_prompt]: 	Test 100/123. loss: 4.781, 0.2266 s / batch. (data: 4.05e-05)max mem: 15.94594 GB 
[10/27 18:03:06 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2319, average loss: 5.3204
[10/27 18:03:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.81	
[10/27 18:03:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/27 18:04:12 visual_prompt]: 	Training 100/1106. train loss: 0.1546,	0.6301 s / batch. (data: 8.16e-04). ETA=18:45:30, max mem: 15.9 GB 
[10/27 18:05:15 visual_prompt]: 	Training 200/1106. train loss: 16.7116,	0.6547 s / batch. (data: 7.66e-04). ETA=19:28:23, max mem: 15.9 GB 
[10/27 18:06:18 visual_prompt]: 	Training 300/1106. train loss: 2.6292,	0.6322 s / batch. (data: 8.27e-04). ETA=18:47:17, max mem: 15.9 GB 
[10/27 18:07:22 visual_prompt]: 	Training 400/1106. train loss: 0.9492,	0.6595 s / batch. (data: 2.54e-02). ETA=19:34:47, max mem: 15.9 GB 
[10/27 18:08:25 visual_prompt]: 	Training 500/1106. train loss: 0.5173,	0.6189 s / batch. (data: 2.91e-04). ETA=18:21:22, max mem: 15.9 GB 
[10/27 18:09:28 visual_prompt]: 	Training 600/1106. train loss: 0.7799,	0.6567 s / batch. (data: 5.44e-03). ETA=19:27:38, max mem: 15.9 GB 
[10/27 18:10:31 visual_prompt]: 	Training 700/1106. train loss: 4.6623,	0.6343 s / batch. (data: 8.02e-04). ETA=18:46:49, max mem: 15.9 GB 
[10/27 18:11:34 visual_prompt]: 	Training 800/1106. train loss: 0.1965,	0.6180 s / batch. (data: 2.95e-04). ETA=18:16:49, max mem: 15.9 GB 
[10/27 18:12:38 visual_prompt]: 	Training 900/1106. train loss: 8.0929,	0.6200 s / batch. (data: 3.49e-04). ETA=18:19:16, max mem: 15.9 GB 
[10/27 18:13:41 visual_prompt]: 	Training 1000/1106. train loss: 8.8092,	0.6533 s / batch. (data: 1.10e-02). ETA=19:17:13, max mem: 15.9 GB 
[10/27 18:14:44 visual_prompt]: 	Training 1100/1106. train loss: 9.9363,	0.6198 s / batch. (data: 1.69e-04). ETA=18:16:55, max mem: 15.9 GB 
[10/27 18:14:48 visual_prompt]: Epoch 4 / 100: avg data time: 4.44e-03, avg batch time: 0.6346, average train loss: 3.6101
[10/27 18:15:38 visual_prompt]: 	Test 100/123. loss: 1.315, 0.2254 s / batch. (data: 5.15e-05)max mem: 15.94594 GB 
[10/27 18:15:49 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2329, average loss: 1.4751
[10/27 18:15:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.07	
[10/27 18:15:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/27 18:16:54 visual_prompt]: 	Training 100/1106. train loss: 2.7278,	0.6322 s / batch. (data: 8.36e-04). ETA=18:37:36, max mem: 15.9 GB 
[10/27 18:17:57 visual_prompt]: 	Training 200/1106. train loss: 0.0163,	0.6398 s / batch. (data: 8.24e-04). ETA=18:49:58, max mem: 15.9 GB 
[10/27 18:19:00 visual_prompt]: 	Training 300/1106. train loss: 2.1084,	0.6198 s / batch. (data: 3.15e-04). ETA=18:13:40, max mem: 15.9 GB 
[10/27 18:20:04 visual_prompt]: 	Training 400/1106. train loss: 7.9877,	0.6379 s / batch. (data: 3.34e-04). ETA=18:44:34, max mem: 15.9 GB 
[10/27 18:21:07 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6345 s / batch. (data: 3.19e-04). ETA=18:37:32, max mem: 15.9 GB 
[10/27 18:22:11 visual_prompt]: 	Training 600/1106. train loss: 0.6020,	0.6327 s / batch. (data: 3.22e-04). ETA=18:33:17, max mem: 15.9 GB 
[10/27 18:23:14 visual_prompt]: 	Training 700/1106. train loss: 1.8263,	0.6353 s / batch. (data: 8.70e-04). ETA=18:36:45, max mem: 15.9 GB 
[10/27 18:24:17 visual_prompt]: 	Training 800/1106. train loss: 6.7499,	0.6346 s / batch. (data: 7.76e-04). ETA=18:34:30, max mem: 15.9 GB 
[10/27 18:25:21 visual_prompt]: 	Training 900/1106. train loss: 0.7744,	0.6203 s / batch. (data: 2.73e-04). ETA=18:08:18, max mem: 15.9 GB 
[10/27 18:26:24 visual_prompt]: 	Training 1000/1106. train loss: 0.0005,	0.6360 s / batch. (data: 3.08e-04). ETA=18:34:50, max mem: 15.9 GB 
[10/27 18:27:27 visual_prompt]: 	Training 1100/1106. train loss: 4.1445,	0.6191 s / batch. (data: 2.03e-04). ETA=18:04:15, max mem: 15.9 GB 
[10/27 18:27:31 visual_prompt]: Epoch 5 / 100: avg data time: 4.05e-03, avg batch time: 0.6351, average train loss: 3.0388
[10/27 18:28:21 visual_prompt]: 	Test 100/123. loss: 4.316, 0.2473 s / batch. (data: 7.10e-05)max mem: 15.94594 GB 
[10/27 18:28:32 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2325, average loss: 3.9400
[10/27 18:28:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[10/27 18:28:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/27 18:29:37 visual_prompt]: 	Training 100/1106. train loss: 7.1306,	0.6576 s / batch. (data: 7.72e-04). ETA=19:10:23, max mem: 15.9 GB 
[10/27 18:30:40 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6437 s / batch. (data: 7.94e-04). ETA=18:45:05, max mem: 15.9 GB 
[10/27 18:31:44 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6482 s / batch. (data: 8.49e-04). ETA=18:51:54, max mem: 15.9 GB 
[10/27 18:32:47 visual_prompt]: 	Training 400/1106. train loss: 5.6221,	0.6407 s / batch. (data: 4.29e-04). ETA=18:37:43, max mem: 15.9 GB 
[10/27 18:33:50 visual_prompt]: 	Training 500/1106. train loss: 5.1446,	0.6338 s / batch. (data: 7.42e-04). ETA=18:24:35, max mem: 15.9 GB 
[10/27 18:34:53 visual_prompt]: 	Training 600/1106. train loss: 0.0188,	0.6428 s / batch. (data: 7.90e-04). ETA=18:39:11, max mem: 15.9 GB 
[10/27 18:35:57 visual_prompt]: 	Training 700/1106. train loss: 6.9387,	0.6327 s / batch. (data: 5.44e-03). ETA=18:20:31, max mem: 15.9 GB 
[10/27 18:37:00 visual_prompt]: 	Training 800/1106. train loss: 6.7229,	0.6185 s / batch. (data: 3.42e-04). ETA=17:54:55, max mem: 15.9 GB 
[10/27 18:38:03 visual_prompt]: 	Training 900/1106. train loss: 16.2011,	0.6192 s / batch. (data: 3.71e-04). ETA=17:54:58, max mem: 15.9 GB 
[10/27 18:39:06 visual_prompt]: 	Training 1000/1106. train loss: 2.4746,	0.6308 s / batch. (data: 7.92e-04). ETA=18:14:11, max mem: 15.9 GB 
[10/27 18:40:10 visual_prompt]: 	Training 1100/1106. train loss: 1.8356,	0.6181 s / batch. (data: 1.54e-04). ETA=17:51:01, max mem: 15.9 GB 
[10/27 18:40:13 visual_prompt]: Epoch 6 / 100: avg data time: 3.92e-03, avg batch time: 0.6344, average train loss: 5.0829
[10/27 18:41:03 visual_prompt]: 	Test 100/123. loss: 1.511, 0.2296 s / batch. (data: 3.96e-05)max mem: 15.94594 GB 
[10/27 18:41:14 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2325, average loss: 1.7534
[10/27 18:41:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.38	
[10/27 18:41:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/27 18:42:19 visual_prompt]: 	Training 100/1106. train loss: 4.7444,	0.6179 s / batch. (data: 3.00e-04). ETA=17:49:41, max mem: 15.9 GB 
[10/27 18:43:22 visual_prompt]: 	Training 200/1106. train loss: 9.4290,	0.6327 s / batch. (data: 8.03e-04). ETA=18:14:14, max mem: 15.9 GB 
[10/27 18:44:25 visual_prompt]: 	Training 300/1106. train loss: 8.1105,	0.6398 s / batch. (data: 7.87e-04). ETA=18:25:28, max mem: 15.9 GB 
[10/27 18:45:29 visual_prompt]: 	Training 400/1106. train loss: 0.5524,	0.6185 s / batch. (data: 3.60e-04). ETA=17:47:33, max mem: 15.9 GB 
[10/27 18:46:32 visual_prompt]: 	Training 500/1106. train loss: 6.5355,	0.6254 s / batch. (data: 2.94e-04). ETA=17:58:28, max mem: 15.9 GB 
[10/27 18:47:36 visual_prompt]: 	Training 600/1106. train loss: 1.4722,	0.6303 s / batch. (data: 7.40e-04). ETA=18:05:48, max mem: 15.9 GB 
[10/27 18:48:39 visual_prompt]: 	Training 700/1106. train loss: 20.9775,	0.6295 s / batch. (data: 2.92e-04). ETA=18:03:23, max mem: 15.9 GB 
[10/27 18:49:42 visual_prompt]: 	Training 800/1106. train loss: 0.8371,	0.6319 s / batch. (data: 8.16e-04). ETA=18:06:30, max mem: 15.9 GB 
[10/27 18:50:46 visual_prompt]: 	Training 900/1106. train loss: 1.5451,	0.6235 s / batch. (data: 2.82e-04). ETA=17:51:03, max mem: 15.9 GB 
[10/27 18:51:49 visual_prompt]: 	Training 1000/1106. train loss: 5.0679,	0.6237 s / batch. (data: 7.62e-04). ETA=17:50:16, max mem: 15.9 GB 
[10/27 18:52:52 visual_prompt]: 	Training 1100/1106. train loss: 4.8269,	0.6180 s / batch. (data: 1.39e-04). ETA=17:39:27, max mem: 15.9 GB 
[10/27 18:52:56 visual_prompt]: Epoch 7 / 100: avg data time: 4.22e-03, avg batch time: 0.6349, average train loss: 3.9760
[10/27 18:53:46 visual_prompt]: 	Test 100/123. loss: 6.086, 0.2260 s / batch. (data: 2.86e-05)max mem: 15.94594 GB 
[10/27 18:53:56 visual_prompt]: Inference (val):avg data time: 1.30e-04, avg batch time: 0.2327, average loss: 5.3186
[10/27 18:53:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.47	
[10/27 18:53:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/27 18:55:02 visual_prompt]: 	Training 100/1106. train loss: 8.2895,	0.6421 s / batch. (data: 8.51e-04). ETA=18:19:39, max mem: 15.9 GB 
[10/27 18:56:05 visual_prompt]: 	Training 200/1106. train loss: 1.7371,	0.6196 s / batch. (data: 2.93e-04). ETA=17:40:08, max mem: 15.9 GB 
[10/27 18:57:08 visual_prompt]: 	Training 300/1106. train loss: 0.0359,	0.6343 s / batch. (data: 2.77e-04). ETA=18:04:09, max mem: 15.9 GB 
[10/27 18:58:11 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6190 s / batch. (data: 2.38e-04). ETA=17:37:01, max mem: 15.9 GB 
[10/27 18:59:15 visual_prompt]: 	Training 500/1106. train loss: 0.5880,	0.6473 s / batch. (data: 7.72e-04). ETA=18:24:12, max mem: 15.9 GB 
[10/27 19:00:18 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6431 s / batch. (data: 7.94e-04). ETA=18:16:00, max mem: 15.9 GB 
[10/27 19:01:21 visual_prompt]: 	Training 700/1106. train loss: 4.1983,	0.6354 s / batch. (data: 2.68e-04). ETA=18:01:53, max mem: 15.9 GB 
[10/27 19:02:25 visual_prompt]: 	Training 800/1106. train loss: 9.5809,	0.6313 s / batch. (data: 3.20e-04). ETA=17:53:44, max mem: 15.9 GB 
[10/27 19:03:28 visual_prompt]: 	Training 900/1106. train loss: 0.2066,	0.6193 s / batch. (data: 2.88e-04). ETA=17:32:18, max mem: 15.9 GB 
[10/27 19:04:31 visual_prompt]: 	Training 1000/1106. train loss: 1.7049,	0.6182 s / batch. (data: 3.16e-04). ETA=17:29:25, max mem: 15.9 GB 
[10/27 19:05:35 visual_prompt]: 	Training 1100/1106. train loss: 0.0687,	0.6179 s / batch. (data: 1.97e-04). ETA=17:27:57, max mem: 15.9 GB 
[10/27 19:05:39 visual_prompt]: Epoch 8 / 100: avg data time: 4.18e-03, avg batch time: 0.6349, average train loss: 4.1112
[10/27 19:06:29 visual_prompt]: 	Test 100/123. loss: 2.671, 0.2332 s / batch. (data: 3.79e-05)max mem: 15.94594 GB 
[10/27 19:06:39 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2322, average loss: 3.0202
[10/27 19:06:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.80	
[10/27 19:06:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/27 19:07:45 visual_prompt]: 	Training 100/1106. train loss: 23.5279,	0.6388 s / batch. (data: 7.64e-04). ETA=18:02:13, max mem: 15.9 GB 
[10/27 19:08:48 visual_prompt]: 	Training 200/1106. train loss: 0.0348,	0.6391 s / batch. (data: 8.11e-04). ETA=18:01:37, max mem: 15.9 GB 
[10/27 19:09:52 visual_prompt]: 	Training 300/1106. train loss: 0.3845,	0.6327 s / batch. (data: 3.18e-04). ETA=17:49:45, max mem: 15.9 GB 
[10/27 19:10:55 visual_prompt]: 	Training 400/1106. train loss: 2.2670,	0.6336 s / batch. (data: 3.05e-04). ETA=17:50:16, max mem: 15.9 GB 
[10/27 19:11:59 visual_prompt]: 	Training 500/1106. train loss: 26.6017,	0.6277 s / batch. (data: 4.18e-04). ETA=17:39:13, max mem: 15.9 GB 
[10/27 19:13:02 visual_prompt]: 	Training 600/1106. train loss: 6.5066,	0.6337 s / batch. (data: 1.59e-02). ETA=17:48:22, max mem: 15.9 GB 
[10/27 19:14:05 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6219 s / batch. (data: 3.10e-04). ETA=17:27:27, max mem: 15.9 GB 
[10/27 19:15:09 visual_prompt]: 	Training 800/1106. train loss: 0.5843,	0.6584 s / batch. (data: 8.10e-04). ETA=18:27:42, max mem: 15.9 GB 
[10/27 19:16:12 visual_prompt]: 	Training 900/1106. train loss: 2.5835,	0.6317 s / batch. (data: 8.27e-04). ETA=17:41:50, max mem: 15.9 GB 
[10/27 19:17:15 visual_prompt]: 	Training 1000/1106. train loss: 1.1828,	0.6368 s / batch. (data: 8.16e-04). ETA=17:49:20, max mem: 15.9 GB 
[10/27 19:18:19 visual_prompt]: 	Training 1100/1106. train loss: 1.7068,	0.6193 s / batch. (data: 1.55e-04). ETA=17:18:58, max mem: 15.9 GB 
[10/27 19:18:22 visual_prompt]: Epoch 9 / 100: avg data time: 4.58e-03, avg batch time: 0.6355, average train loss: 4.3290
[10/27 19:19:12 visual_prompt]: 	Test 100/123. loss: 1.045, 0.2251 s / batch. (data: 4.41e-05)max mem: 15.94594 GB 
[10/27 19:19:23 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.2331, average loss: 1.2418
[10/27 19:19:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.28	
[10/27 19:19:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/27 19:20:29 visual_prompt]: 	Training 100/1106. train loss: 4.0762,	0.6329 s / batch. (data: 8.80e-04). ETA=17:40:31, max mem: 15.9 GB 
[10/27 19:21:32 visual_prompt]: 	Training 200/1106. train loss: 24.7768,	0.6391 s / batch. (data: 7.86e-04). ETA=17:49:53, max mem: 15.9 GB 
[10/27 19:22:35 visual_prompt]: 	Training 300/1106. train loss: 10.1891,	0.6400 s / batch. (data: 4.03e-04). ETA=17:50:19, max mem: 15.9 GB 
[10/27 19:23:38 visual_prompt]: 	Training 400/1106. train loss: 1.1406,	0.6312 s / batch. (data: 3.55e-04). ETA=17:34:33, max mem: 15.9 GB 
[10/27 19:24:42 visual_prompt]: 	Training 500/1106. train loss: 4.3393,	0.6236 s / batch. (data: 5.42e-03). ETA=17:20:53, max mem: 15.9 GB 
[10/27 19:25:45 visual_prompt]: 	Training 600/1106. train loss: 11.4275,	0.6240 s / batch. (data: 3.52e-04). ETA=17:20:25, max mem: 15.9 GB 
[10/27 19:26:48 visual_prompt]: 	Training 700/1106. train loss: 16.0604,	0.6471 s / batch. (data: 8.06e-04). ETA=17:57:59, max mem: 15.9 GB 
[10/27 19:27:52 visual_prompt]: 	Training 800/1106. train loss: 1.1841,	0.6403 s / batch. (data: 5.87e-03). ETA=17:45:30, max mem: 15.9 GB 
[10/27 19:28:55 visual_prompt]: 	Training 900/1106. train loss: 1.8116,	0.6397 s / batch. (data: 7.51e-04). ETA=17:43:31, max mem: 15.9 GB 
[10/27 19:29:58 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6326 s / batch. (data: 7.75e-04). ETA=17:30:40, max mem: 15.9 GB 
[10/27 19:31:01 visual_prompt]: 	Training 1100/1106. train loss: 0.0169,	0.6174 s / batch. (data: 1.30e-04). ETA=17:04:24, max mem: 15.9 GB 
[10/27 19:31:05 visual_prompt]: Epoch 10 / 100: avg data time: 3.94e-03, avg batch time: 0.6349, average train loss: 7.0148
[10/27 19:31:55 visual_prompt]: 	Test 100/123. loss: 6.866, 0.2327 s / batch. (data: 4.39e-05)max mem: 15.94594 GB 
[10/27 19:32:06 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2334, average loss: 6.1866
[10/27 19:32:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.01	
[10/27 19:32:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/27 19:33:12 visual_prompt]: 	Training 100/1106. train loss: 5.7587,	0.6435 s / batch. (data: 3.14e-04). ETA=17:46:26, max mem: 15.9 GB 
[10/27 19:34:15 visual_prompt]: 	Training 200/1106. train loss: 6.1009,	0.6234 s / batch. (data: 3.35e-04). ETA=17:12:11, max mem: 15.9 GB 
[10/27 19:35:18 visual_prompt]: 	Training 300/1106. train loss: 6.3463,	0.6377 s / batch. (data: 7.85e-04). ETA=17:34:45, max mem: 15.9 GB 
[10/27 19:36:21 visual_prompt]: 	Training 400/1106. train loss: 0.0004,	0.6197 s / batch. (data: 3.41e-04). ETA=17:03:56, max mem: 15.9 GB 
[10/27 19:37:25 visual_prompt]: 	Training 500/1106. train loss: 3.0214,	0.6334 s / batch. (data: 8.12e-04). ETA=17:25:33, max mem: 15.9 GB 
[10/27 19:38:28 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6263 s / batch. (data: 3.00e-04). ETA=17:12:44, max mem: 15.9 GB 
[10/27 19:39:31 visual_prompt]: 	Training 700/1106. train loss: 18.4840,	0.6305 s / batch. (data: 7.01e-04). ETA=17:18:40, max mem: 15.9 GB 
[10/27 19:40:35 visual_prompt]: 	Training 800/1106. train loss: 19.4739,	0.6185 s / batch. (data: 2.77e-04). ETA=16:57:47, max mem: 15.9 GB 
[10/27 19:41:38 visual_prompt]: 	Training 900/1106. train loss: 0.0021,	0.6309 s / batch. (data: 2.88e-04). ETA=17:17:09, max mem: 15.9 GB 
[10/27 19:42:41 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6273 s / batch. (data: 2.98e-04). ETA=17:10:17, max mem: 15.9 GB 
[10/27 19:43:44 visual_prompt]: 	Training 1100/1106. train loss: 1.3973,	0.6183 s / batch. (data: 1.55e-04). ETA=16:54:30, max mem: 15.9 GB 
[10/27 19:43:48 visual_prompt]: Epoch 11 / 100: avg data time: 4.54e-03, avg batch time: 0.6348, average train loss: 5.4778
[10/27 19:44:38 visual_prompt]: 	Test 100/123. loss: 2.597, 0.2605 s / batch. (data: 6.29e-05)max mem: 15.94594 GB 
[10/27 19:44:49 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2332, average loss: 1.9367
[10/27 19:44:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.77	
[10/27 19:44:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/27 19:45:55 visual_prompt]: 	Training 100/1106. train loss: 0.1472,	0.6339 s / batch. (data: 7.40e-04). ETA=17:18:51, max mem: 15.9 GB 
[10/27 19:46:59 visual_prompt]: 	Training 200/1106. train loss: 2.6225,	0.6387 s / batch. (data: 8.36e-04). ETA=17:25:38, max mem: 15.9 GB 
[10/27 19:48:02 visual_prompt]: 	Training 300/1106. train loss: 26.9833,	0.6436 s / batch. (data: 2.48e-02). ETA=17:32:43, max mem: 15.9 GB 
[10/27 19:49:06 visual_prompt]: 	Training 400/1106. train loss: 14.6486,	0.6332 s / batch. (data: 8.43e-04). ETA=17:14:38, max mem: 15.9 GB 
[10/27 19:50:09 visual_prompt]: 	Training 500/1106. train loss: 10.1178,	0.6439 s / batch. (data: 7.97e-04). ETA=17:30:56, max mem: 15.9 GB 
[10/27 19:51:12 visual_prompt]: 	Training 600/1106. train loss: 0.0566,	0.6195 s / batch. (data: 3.19e-04). ETA=16:50:06, max mem: 15.9 GB 
[10/27 19:52:16 visual_prompt]: 	Training 700/1106. train loss: 0.6311,	0.6426 s / batch. (data: 7.18e-04). ETA=17:26:44, max mem: 15.9 GB 
[10/27 19:53:19 visual_prompt]: 	Training 800/1106. train loss: 6.9023,	0.6313 s / batch. (data: 8.06e-04). ETA=17:07:13, max mem: 15.9 GB 
[10/27 19:54:23 visual_prompt]: 	Training 900/1106. train loss: 8.7502,	0.6200 s / batch. (data: 3.70e-04). ETA=16:47:46, max mem: 15.9 GB 
[10/27 19:55:26 visual_prompt]: 	Training 1000/1106. train loss: 1.9671,	0.6418 s / batch. (data: 7.97e-04). ETA=17:22:12, max mem: 15.9 GB 
[10/27 19:56:29 visual_prompt]: 	Training 1100/1106. train loss: 26.0016,	0.6189 s / batch. (data: 1.38e-04). ETA=16:43:58, max mem: 15.9 GB 
[10/27 19:56:33 visual_prompt]: Epoch 12 / 100: avg data time: 5.18e-03, avg batch time: 0.6366, average train loss: 6.9513
[10/27 19:57:23 visual_prompt]: 	Test 100/123. loss: 26.627, 0.2444 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[10/27 19:57:33 visual_prompt]: Inference (val):avg data time: 4.60e-05, avg batch time: 0.2314, average loss: 29.6363
[10/27 19:57:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[10/27 19:57:33 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/27 19:58:39 visual_prompt]: 	Training 100/1106. train loss: 0.0543,	0.6318 s / batch. (data: 8.29e-04). ETA=17:03:51, max mem: 15.9 GB 
[10/27 19:59:42 visual_prompt]: 	Training 200/1106. train loss: 5.1555,	0.6343 s / batch. (data: 6.84e-04). ETA=17:06:44, max mem: 15.9 GB 
[10/27 20:00:46 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6311 s / batch. (data: 7.97e-04). ETA=17:00:35, max mem: 15.9 GB 
[10/27 20:01:49 visual_prompt]: 	Training 400/1106. train loss: 2.6025,	0.6327 s / batch. (data: 8.01e-04). ETA=17:02:05, max mem: 15.9 GB 
[10/27 20:02:52 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6201 s / batch. (data: 3.20e-04). ETA=16:40:45, max mem: 15.9 GB 
[10/27 20:03:55 visual_prompt]: 	Training 600/1106. train loss: 1.2924,	0.6182 s / batch. (data: 3.22e-04). ETA=16:36:40, max mem: 15.9 GB 
[10/27 20:04:59 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6533 s / batch. (data: 7.43e-04). ETA=17:32:06, max mem: 15.9 GB 
[10/27 20:06:02 visual_prompt]: 	Training 800/1106. train loss: 1.5637,	0.6314 s / batch. (data: 3.22e-04). ETA=16:55:47, max mem: 15.9 GB 
[10/27 20:07:06 visual_prompt]: 	Training 900/1106. train loss: 10.6744,	0.6445 s / batch. (data: 7.47e-04). ETA=17:15:50, max mem: 15.9 GB 
[10/27 20:08:09 visual_prompt]: 	Training 1000/1106. train loss: 1.3905,	0.6189 s / batch. (data: 2.89e-04). ETA=16:33:41, max mem: 15.9 GB 
[10/27 20:09:13 visual_prompt]: 	Training 1100/1106. train loss: 9.6632,	0.6192 s / batch. (data: 2.51e-04). ETA=16:33:00, max mem: 15.9 GB 
[10/27 20:09:16 visual_prompt]: Epoch 13 / 100: avg data time: 4.42e-03, avg batch time: 0.6355, average train loss: 5.1854
[10/27 20:10:06 visual_prompt]: 	Test 100/123. loss: 0.559, 0.2406 s / batch. (data: 4.94e-05)max mem: 15.94594 GB 
[10/27 20:10:17 visual_prompt]: Inference (val):avg data time: 2.10e-04, avg batch time: 0.2337, average loss: 1.0431
[10/27 20:10:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 52.58	
[10/27 20:10:17 visual_prompt]: Best epoch 13: best metric: -1.043
[10/27 20:10:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/27 20:11:22 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6340 s / batch. (data: 2.83e-04). ETA=16:55:43, max mem: 15.9 GB 
[10/27 20:12:26 visual_prompt]: 	Training 200/1106. train loss: 4.6871,	0.6484 s / batch. (data: 8.11e-04). ETA=17:17:39, max mem: 15.9 GB 
[10/27 20:13:29 visual_prompt]: 	Training 300/1106. train loss: 7.3398,	0.6243 s / batch. (data: 3.13e-04). ETA=16:38:04, max mem: 15.9 GB 
[10/27 20:14:33 visual_prompt]: 	Training 400/1106. train loss: 0.0254,	0.6341 s / batch. (data: 7.80e-04). ETA=16:52:36, max mem: 15.9 GB 
[10/27 20:15:36 visual_prompt]: 	Training 500/1106. train loss: 3.4840,	0.6189 s / batch. (data: 3.99e-04). ETA=16:27:26, max mem: 15.9 GB 
[10/27 20:16:40 visual_prompt]: 	Training 600/1106. train loss: 8.2439,	0.6618 s / batch. (data: 8.05e-04). ETA=17:34:39, max mem: 15.9 GB 
[10/27 20:17:43 visual_prompt]: 	Training 700/1106. train loss: 5.8660,	0.6267 s / batch. (data: 3.28e-04). ETA=16:37:39, max mem: 15.9 GB 
[10/27 20:18:46 visual_prompt]: 	Training 800/1106. train loss: 0.9146,	0.6324 s / batch. (data: 3.84e-04). ETA=16:45:46, max mem: 15.9 GB 
[10/27 20:19:50 visual_prompt]: 	Training 900/1106. train loss: 12.1214,	0.6187 s / batch. (data: 3.24e-04). ETA=16:22:56, max mem: 15.9 GB 
[10/27 20:20:53 visual_prompt]: 	Training 1000/1106. train loss: 0.0001,	0.6440 s / batch. (data: 7.50e-04). ETA=17:02:04, max mem: 15.9 GB 
[10/27 20:21:56 visual_prompt]: 	Training 1100/1106. train loss: 1.5559,	0.6199 s / batch. (data: 1.43e-04). ETA=16:22:45, max mem: 15.9 GB 
[10/27 20:22:00 visual_prompt]: Epoch 14 / 100: avg data time: 4.45e-03, avg batch time: 0.6358, average train loss: 6.3116
[10/27 20:22:50 visual_prompt]: 	Test 100/123. loss: 7.447, 0.2246 s / batch. (data: 3.89e-05)max mem: 15.94594 GB 
[10/27 20:23:01 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2320, average loss: 6.6660
[10/27 20:23:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.23	
[10/27 20:23:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/27 20:24:05 visual_prompt]: 	Training 100/1106. train loss: 16.1300,	0.6181 s / batch. (data: 3.06e-04). ETA=16:18:51, max mem: 15.9 GB 
[10/27 20:25:09 visual_prompt]: 	Training 200/1106. train loss: 12.8154,	0.6327 s / batch. (data: 3.14e-04). ETA=16:40:50, max mem: 15.9 GB 
[10/27 20:26:12 visual_prompt]: 	Training 300/1106. train loss: 0.9773,	0.6485 s / batch. (data: 2.85e-02). ETA=17:04:47, max mem: 15.9 GB 
[10/27 20:27:15 visual_prompt]: 	Training 400/1106. train loss: 0.0001,	0.6300 s / batch. (data: 3.21e-04). ETA=16:34:26, max mem: 15.9 GB 
[10/27 20:28:18 visual_prompt]: 	Training 500/1106. train loss: 10.8792,	0.6292 s / batch. (data: 7.74e-04). ETA=16:32:09, max mem: 15.9 GB 
[10/27 20:29:22 visual_prompt]: 	Training 600/1106. train loss: 5.2011,	0.6424 s / batch. (data: 8.31e-04). ETA=16:51:52, max mem: 15.9 GB 
[10/27 20:30:25 visual_prompt]: 	Training 700/1106. train loss: 6.0916,	0.6357 s / batch. (data: 7.64e-04). ETA=16:40:17, max mem: 15.9 GB 
[10/27 20:31:28 visual_prompt]: 	Training 800/1106. train loss: 0.7638,	0.6352 s / batch. (data: 7.89e-04). ETA=16:38:31, max mem: 15.9 GB 
[10/27 20:32:31 visual_prompt]: 	Training 900/1106. train loss: 12.5969,	0.6183 s / batch. (data: 3.13e-04). ETA=16:10:52, max mem: 15.9 GB 
[10/27 20:33:35 visual_prompt]: 	Training 1000/1106. train loss: 6.1227,	0.6181 s / batch. (data: 2.70e-04). ETA=16:09:35, max mem: 15.9 GB 
[10/27 20:34:38 visual_prompt]: 	Training 1100/1106. train loss: 6.2166,	0.6182 s / batch. (data: 1.60e-04). ETA=16:08:43, max mem: 15.9 GB 
[10/27 20:34:42 visual_prompt]: Epoch 15 / 100: avg data time: 3.93e-03, avg batch time: 0.6341, average train loss: 6.1985
[10/27 20:35:32 visual_prompt]: 	Test 100/123. loss: 1.025, 0.2266 s / batch. (data: 3.91e-05)max mem: 15.94594 GB 
[10/27 20:35:42 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.2320, average loss: 1.6602
[10/27 20:35:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.44	
[10/27 20:35:42 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/27 20:36:48 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6185 s / batch. (data: 3.20e-04). ETA=16:07:59, max mem: 15.9 GB 
[10/27 20:37:51 visual_prompt]: 	Training 200/1106. train loss: 18.7589,	0.6381 s / batch. (data: 2.99e-04). ETA=16:37:40, max mem: 15.9 GB 
[10/27 20:38:54 visual_prompt]: 	Training 300/1106. train loss: 0.0382,	0.6360 s / batch. (data: 8.43e-04). ETA=16:33:20, max mem: 15.9 GB 
[10/27 20:39:58 visual_prompt]: 	Training 400/1106. train loss: 18.0767,	0.6361 s / batch. (data: 3.38e-04). ETA=16:32:26, max mem: 15.9 GB 
[10/27 20:41:01 visual_prompt]: 	Training 500/1106. train loss: 5.7275,	0.6463 s / batch. (data: 7.18e-04). ETA=16:47:13, max mem: 15.9 GB 
[10/27 20:42:05 visual_prompt]: 	Training 600/1106. train loss: 0.0562,	0.6355 s / batch. (data: 8.85e-04). ETA=16:29:17, max mem: 15.9 GB 
[10/27 20:43:08 visual_prompt]: 	Training 700/1106. train loss: 7.3360,	0.6252 s / batch. (data: 2.67e-04). ETA=16:12:15, max mem: 15.9 GB 
[10/27 20:44:11 visual_prompt]: 	Training 800/1106. train loss: 6.4922,	0.6319 s / batch. (data: 1.24e-02). ETA=16:21:43, max mem: 15.9 GB 
[10/27 20:45:15 visual_prompt]: 	Training 900/1106. train loss: 0.5732,	0.6197 s / batch. (data: 3.13e-04). ETA=16:01:43, max mem: 15.9 GB 
[10/27 20:46:18 visual_prompt]: 	Training 1000/1106. train loss: 17.3416,	0.6355 s / batch. (data: 7.53e-04). ETA=16:25:10, max mem: 15.9 GB 
[10/27 20:47:22 visual_prompt]: 	Training 1100/1106. train loss: 13.9194,	0.6179 s / batch. (data: 1.57e-04). ETA=15:56:45, max mem: 15.9 GB 
[10/27 20:47:25 visual_prompt]: Epoch 16 / 100: avg data time: 4.45e-03, avg batch time: 0.6357, average train loss: 5.9446
[10/27 20:48:15 visual_prompt]: 	Test 100/123. loss: 3.776, 0.2279 s / batch. (data: 3.74e-05)max mem: 15.94594 GB 
[10/27 20:48:26 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2311, average loss: 3.4582
[10/27 20:48:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.17	
[10/27 20:48:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/27 20:49:32 visual_prompt]: 	Training 100/1106. train loss: 2.0535,	0.6369 s / batch. (data: 6.00e-03). ETA=16:25:10, max mem: 15.9 GB 
[10/27 20:50:35 visual_prompt]: 	Training 200/1106. train loss: 2.5698,	0.6503 s / batch. (data: 7.94e-04). ETA=16:44:49, max mem: 15.9 GB 
[10/27 20:51:38 visual_prompt]: 	Training 300/1106. train loss: 1.1221,	0.6284 s / batch. (data: 1.08e-02). ETA=16:09:52, max mem: 15.9 GB 
[10/27 20:52:41 visual_prompt]: 	Training 400/1106. train loss: 50.6947,	0.6181 s / batch. (data: 2.92e-04). ETA=15:53:00, max mem: 15.9 GB 
[10/27 20:53:45 visual_prompt]: 	Training 500/1106. train loss: 1.2769,	0.6221 s / batch. (data: 5.02e-04). ETA=15:58:02, max mem: 15.9 GB 
[10/27 20:54:48 visual_prompt]: 	Training 600/1106. train loss: 14.8562,	0.6433 s / batch. (data: 7.78e-04). ETA=16:29:40, max mem: 15.9 GB 
[10/27 20:55:52 visual_prompt]: 	Training 700/1106. train loss: 5.6425,	0.6400 s / batch. (data: 1.20e-02). ETA=16:23:31, max mem: 15.9 GB 
[10/27 20:56:55 visual_prompt]: 	Training 800/1106. train loss: 10.0335,	0.6183 s / batch. (data: 2.98e-04). ETA=15:49:03, max mem: 15.9 GB 
[10/27 20:57:58 visual_prompt]: 	Training 900/1106. train loss: 4.8106,	0.6300 s / batch. (data: 3.00e-04). ETA=16:05:58, max mem: 15.9 GB 
[10/27 20:59:01 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6453 s / batch. (data: 8.16e-04). ETA=16:28:24, max mem: 15.9 GB 
[10/27 21:00:05 visual_prompt]: 	Training 1100/1106. train loss: 0.0001,	0.6192 s / batch. (data: 1.57e-04). ETA=15:47:27, max mem: 15.9 GB 
[10/27 21:00:08 visual_prompt]: Epoch 17 / 100: avg data time: 4.40e-03, avg batch time: 0.6349, average train loss: 6.5663
[10/27 21:00:59 visual_prompt]: 	Test 100/123. loss: 1.225, 0.2247 s / batch. (data: 4.34e-05)max mem: 15.94594 GB 
[10/27 21:01:09 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2321, average loss: 1.3150
[10/27 21:01:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 59.88	
[10/27 21:01:09 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/27 21:02:15 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6319 s / batch. (data: 8.44e-04). ETA=16:05:43, max mem: 15.9 GB 
[10/27 21:03:18 visual_prompt]: 	Training 200/1106. train loss: 14.8174,	0.6315 s / batch. (data: 3.37e-04). ETA=16:04:07, max mem: 15.9 GB 
[10/27 21:04:21 visual_prompt]: 	Training 300/1106. train loss: 56.5789,	0.6195 s / batch. (data: 3.18e-04). ETA=15:44:46, max mem: 15.9 GB 
[10/27 21:05:25 visual_prompt]: 	Training 400/1106. train loss: 7.3028,	0.6651 s / batch. (data: 3.42e-02). ETA=16:53:07, max mem: 15.9 GB 
[10/27 21:06:28 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6280 s / batch. (data: 2.98e-04). ETA=15:55:38, max mem: 15.9 GB 
[10/27 21:07:31 visual_prompt]: 	Training 600/1106. train loss: 0.3857,	0.6428 s / batch. (data: 3.33e-04). ETA=16:16:57, max mem: 15.9 GB 
[10/27 21:08:35 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6482 s / batch. (data: 7.86e-04). ETA=16:24:05, max mem: 15.9 GB 
[10/27 21:09:38 visual_prompt]: 	Training 800/1106. train loss: 3.5417,	0.6199 s / batch. (data: 2.96e-04). ETA=15:40:07, max mem: 15.9 GB 
[10/27 21:10:41 visual_prompt]: 	Training 900/1106. train loss: 7.0957,	0.6200 s / batch. (data: 3.17e-04). ETA=15:39:18, max mem: 15.9 GB 
[10/27 21:11:45 visual_prompt]: 	Training 1000/1106. train loss: 2.1769,	0.6261 s / batch. (data: 3.23e-04). ETA=15:47:26, max mem: 15.9 GB 
[10/27 21:12:48 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6183 s / batch. (data: 2.10e-04). ETA=15:34:42, max mem: 15.9 GB 
[10/27 21:12:52 visual_prompt]: Epoch 18 / 100: avg data time: 4.42e-03, avg batch time: 0.6354, average train loss: 6.7775
[10/27 21:13:42 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2317 s / batch. (data: 4.12e-05)max mem: 15.94594 GB 
[10/27 21:13:53 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2325, average loss: 1.5377
[10/27 21:13:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 55.02	
[10/27 21:13:53 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/27 21:14:58 visual_prompt]: 	Training 100/1106. train loss: 0.0001,	0.6298 s / batch. (data: 7.95e-04). ETA=15:50:58, max mem: 15.9 GB 
[10/27 21:16:01 visual_prompt]: 	Training 200/1106. train loss: 12.5378,	0.6526 s / batch. (data: 5.98e-03). ETA=16:24:17, max mem: 15.9 GB 
[10/27 21:17:05 visual_prompt]: 	Training 300/1106. train loss: 0.0002,	0.6517 s / batch. (data: 9.01e-04). ETA=16:21:52, max mem: 15.9 GB 
[10/27 21:18:08 visual_prompt]: 	Training 400/1106. train loss: 2.0900,	0.6701 s / batch. (data: 3.86e-02). ETA=16:48:27, max mem: 15.9 GB 
[10/27 21:19:11 visual_prompt]: 	Training 500/1106. train loss: 0.0845,	0.6352 s / batch. (data: 1.58e-02). ETA=15:54:52, max mem: 15.9 GB 
[10/27 21:20:14 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6192 s / batch. (data: 3.24e-04). ETA=15:29:44, max mem: 15.9 GB 
[10/27 21:21:18 visual_prompt]: 	Training 700/1106. train loss: 1.9563,	0.6346 s / batch. (data: 3.23e-04). ETA=15:51:51, max mem: 15.9 GB 
[10/27 21:22:21 visual_prompt]: 	Training 800/1106. train loss: 0.5862,	0.6324 s / batch. (data: 8.08e-04). ETA=15:47:29, max mem: 15.9 GB 
[10/27 21:23:25 visual_prompt]: 	Training 900/1106. train loss: 5.7114,	0.6505 s / batch. (data: 8.57e-04). ETA=16:13:28, max mem: 15.9 GB 
[10/27 21:24:28 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6442 s / batch. (data: 2.34e-03). ETA=16:02:57, max mem: 15.9 GB 
[10/27 21:25:31 visual_prompt]: 	Training 1100/1106. train loss: 37.4708,	0.6333 s / batch. (data: 1.81e-04). ETA=15:45:40, max mem: 15.9 GB 
[10/27 21:25:35 visual_prompt]: Epoch 19 / 100: avg data time: 4.03e-03, avg batch time: 0.6351, average train loss: 6.6454
[10/27 21:26:25 visual_prompt]: 	Test 100/123. loss: 40.811, 0.2317 s / batch. (data: 2.79e-05)max mem: 15.94594 GB 
[10/27 21:26:35 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2339, average loss: 36.7091
[10/27 21:26:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.85	
[10/27 21:26:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[10/27 21:27:41 visual_prompt]: 	Training 100/1106. train loss: 7.0545,	0.6328 s / batch. (data: 3.08e-04). ETA=15:43:44, max mem: 15.9 GB 
[10/27 21:28:44 visual_prompt]: 	Training 200/1106. train loss: 0.0406,	0.6347 s / batch. (data: 1.57e-02). ETA=15:45:33, max mem: 15.9 GB 
[10/27 21:29:48 visual_prompt]: 	Training 300/1106. train loss: 1.5055,	0.6560 s / batch. (data: 1.20e-02). ETA=16:16:08, max mem: 15.9 GB 
[10/27 21:30:51 visual_prompt]: 	Training 400/1106. train loss: 1.1745,	0.6276 s / batch. (data: 2.80e-04). ETA=15:32:54, max mem: 15.9 GB 
[10/27 21:31:55 visual_prompt]: 	Training 500/1106. train loss: 1.7882,	0.6350 s / batch. (data: 1.62e-02). ETA=15:42:52, max mem: 15.9 GB 
[10/27 21:32:58 visual_prompt]: 	Training 600/1106. train loss: 7.1786,	0.6320 s / batch. (data: 3.40e-04). ETA=15:37:17, max mem: 15.9 GB 
[10/27 21:34:01 visual_prompt]: 	Training 700/1106. train loss: 0.0003,	0.6353 s / batch. (data: 2.99e-04). ETA=15:41:09, max mem: 15.9 GB 
[10/27 21:35:05 visual_prompt]: 	Training 800/1106. train loss: 0.0002,	0.6473 s / batch. (data: 7.89e-04). ETA=15:57:51, max mem: 15.9 GB 
[10/27 21:36:08 visual_prompt]: 	Training 900/1106. train loss: 8.6210,	0.6243 s / batch. (data: 2.89e-04). ETA=15:22:44, max mem: 15.9 GB 
[10/27 21:37:11 visual_prompt]: 	Training 1000/1106. train loss: 0.0082,	0.6315 s / batch. (data: 7.78e-04). ETA=15:32:18, max mem: 15.9 GB 
[10/27 21:38:15 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6189 s / batch. (data: 1.43e-04). ETA=15:12:46, max mem: 15.9 GB 
[10/27 21:38:18 visual_prompt]: Epoch 20 / 100: avg data time: 4.95e-03, avg batch time: 0.6355, average train loss: 5.7311
[10/27 21:39:08 visual_prompt]: 	Test 100/123. loss: 1.814, 0.2256 s / batch. (data: 4.53e-05)max mem: 15.94594 GB 
[10/27 21:39:19 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.2321, average loss: 1.9050
[10/27 21:39:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 55.59	
[10/27 21:39:19 visual_prompt]: Stopping early.
[10/27 21:39:19 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 21:39:19 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 21:39:19 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 21:39:19 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 21:39:19 visual_prompt]: Training with config:
[10/27 21:39:19 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr1.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 21:39:19 visual_prompt]: Loading training data...
[10/27 21:39:19 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 21:39:19 visual_prompt]: Loading validation data...
[10/27 21:39:19 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 21:39:19 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/27 21:39:21 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/27 21:39:21 visual_prompt]: tuned percent:0.522
[10/27 21:39:21 visual_prompt]: Device used for model: 0
[10/27 21:39:21 visual_prompt]: Setting up Evaluator...
[10/27 21:39:21 visual_prompt]: Setting up Trainer...
[10/27 21:39:21 visual_prompt]: 	Setting up the optimizer...
[10/27 21:39:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 21:40:27 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6177 s / batch. (data: 2.61e-04). ETA=18:57:35, max mem: 15.9 GB 
[10/27 21:41:30 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6323 s / batch. (data: 8.29e-04). ETA=19:23:29, max mem: 15.9 GB 
[10/27 21:42:34 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6197 s / batch. (data: 3.24e-04). ETA=18:59:12, max mem: 15.9 GB 
[10/27 21:43:37 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6354 s / batch. (data: 3.19e-04). ETA=19:26:59, max mem: 15.9 GB 
[10/27 21:44:40 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6400 s / batch. (data: 7.98e-03). ETA=19:34:27, max mem: 15.9 GB 
[10/27 21:45:44 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6321 s / batch. (data: 7.76e-04). ETA=19:18:54, max mem: 15.9 GB 
[10/27 21:46:47 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6206 s / batch. (data: 3.14e-04). ETA=18:56:44, max mem: 15.9 GB 
[10/27 21:47:50 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6279 s / batch. (data: 7.34e-04). ETA=19:08:59, max mem: 15.9 GB 
[10/27 21:48:54 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6360 s / batch. (data: 5.45e-03). ETA=19:22:49, max mem: 15.9 GB 
[10/27 21:49:57 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6408 s / batch. (data: 8.19e-04). ETA=19:30:28, max mem: 15.9 GB 
[10/27 21:51:00 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6194 s / batch. (data: 1.52e-04). ETA=18:50:23, max mem: 15.9 GB 
[10/27 21:51:04 visual_prompt]: Epoch 1 / 100: avg data time: 4.62e-03, avg batch time: 0.6354, average train loss: 1.4028
[10/27 21:51:55 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2253 s / batch. (data: 2.67e-05)max mem: 15.94594 GB 
[10/27 21:52:05 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.2329, average loss: 1.3505
[10/27 21:52:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/27 21:52:05 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/27 21:53:09 visual_prompt]: 	Training 100/1106. train loss: 2.0501,	0.6470 s / batch. (data: 1.77e-02). ETA=19:39:41, max mem: 15.9 GB 
[10/27 21:54:13 visual_prompt]: 	Training 200/1106. train loss: 0.7751,	0.6308 s / batch. (data: 3.27e-04). ETA=19:09:07, max mem: 15.9 GB 
[10/27 21:55:16 visual_prompt]: 	Training 300/1106. train loss: 0.5789,	0.6509 s / batch. (data: 7.81e-04). ETA=19:44:33, max mem: 15.9 GB 
[10/27 21:56:19 visual_prompt]: 	Training 400/1106. train loss: 0.0216,	0.6277 s / batch. (data: 3.21e-04). ETA=19:01:16, max mem: 15.9 GB 
[10/27 21:57:23 visual_prompt]: 	Training 500/1106. train loss: 1.0224,	0.6626 s / batch. (data: 2.79e-02). ETA=20:03:37, max mem: 15.9 GB 
[10/27 21:58:26 visual_prompt]: 	Training 600/1106. train loss: 0.7230,	0.6358 s / batch. (data: 3.33e-04). ETA=19:13:54, max mem: 15.9 GB 
[10/27 21:59:29 visual_prompt]: 	Training 700/1106. train loss: 0.3042,	0.6203 s / batch. (data: 3.33e-04). ETA=18:44:49, max mem: 15.9 GB 
[10/27 22:00:32 visual_prompt]: 	Training 800/1106. train loss: 1.0955,	0.6348 s / batch. (data: 5.86e-04). ETA=19:09:58, max mem: 15.9 GB 
[10/27 22:01:36 visual_prompt]: 	Training 900/1106. train loss: 1.2027,	0.6297 s / batch. (data: 7.50e-04). ETA=18:59:36, max mem: 15.9 GB 
[10/27 22:02:39 visual_prompt]: 	Training 1000/1106. train loss: 0.0889,	0.6527 s / batch. (data: 7.76e-04). ETA=19:40:16, max mem: 15.9 GB 
[10/27 22:03:43 visual_prompt]: 	Training 1100/1106. train loss: 0.2486,	0.6188 s / batch. (data: 1.59e-04). ETA=18:37:49, max mem: 15.9 GB 
[10/27 22:03:46 visual_prompt]: Epoch 2 / 100: avg data time: 3.79e-03, avg batch time: 0.6344, average train loss: 1.0003
[10/27 22:04:36 visual_prompt]: 	Test 100/123. loss: 1.167, 0.2525 s / batch. (data: 6.65e-05)max mem: 15.94594 GB 
[10/27 22:04:47 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2327, average loss: 1.0637
[10/27 22:04:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.01	
[10/27 22:04:47 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/27 22:05:53 visual_prompt]: 	Training 100/1106. train loss: 0.8823,	0.6305 s / batch. (data: 8.07e-04). ETA=18:57:51, max mem: 15.9 GB 
[10/27 22:06:56 visual_prompt]: 	Training 200/1106. train loss: 0.1620,	0.6179 s / batch. (data: 3.15e-04). ETA=18:34:04, max mem: 15.9 GB 
[10/27 22:07:59 visual_prompt]: 	Training 300/1106. train loss: 0.1575,	0.6425 s / batch. (data: 8.15e-04). ETA=19:17:24, max mem: 15.9 GB 
[10/27 22:09:02 visual_prompt]: 	Training 400/1106. train loss: 0.6999,	0.6515 s / batch. (data: 7.88e-04). ETA=19:32:35, max mem: 15.9 GB 
[10/27 22:10:06 visual_prompt]: 	Training 500/1106. train loss: 3.4256,	0.6191 s / batch. (data: 3.26e-04). ETA=18:33:13, max mem: 15.9 GB 
[10/27 22:11:09 visual_prompt]: 	Training 600/1106. train loss: 0.0187,	0.6306 s / batch. (data: 3.30e-04). ETA=18:52:55, max mem: 15.9 GB 
[10/27 22:12:12 visual_prompt]: 	Training 700/1106. train loss: 0.6489,	0.6311 s / batch. (data: 7.83e-04). ETA=18:52:36, max mem: 15.9 GB 
[10/27 22:13:15 visual_prompt]: 	Training 800/1106. train loss: 0.0004,	0.6240 s / batch. (data: 7.12e-04). ETA=18:39:00, max mem: 15.9 GB 
[10/27 22:14:19 visual_prompt]: 	Training 900/1106. train loss: 2.4013,	0.6251 s / batch. (data: 5.45e-03). ETA=18:39:47, max mem: 15.9 GB 
[10/27 22:15:22 visual_prompt]: 	Training 1000/1106. train loss: 0.8214,	0.6512 s / batch. (data: 7.95e-04). ETA=19:25:28, max mem: 15.9 GB 
[10/27 22:16:25 visual_prompt]: 	Training 1100/1106. train loss: 2.3614,	0.6199 s / batch. (data: 1.47e-04). ETA=18:28:22, max mem: 15.9 GB 
[10/27 22:16:29 visual_prompt]: Epoch 3 / 100: avg data time: 5.08e-03, avg batch time: 0.6348, average train loss: 1.2057
[10/27 22:17:19 visual_prompt]: 	Test 100/123. loss: 0.754, 0.2257 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/27 22:17:29 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2329, average loss: 0.7984
[10/27 22:17:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.23	
[10/27 22:17:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/27 22:18:35 visual_prompt]: 	Training 100/1106. train loss: 0.3467,	0.6652 s / batch. (data: 1.10e-02). ETA=19:48:16, max mem: 15.9 GB 
[10/27 22:19:38 visual_prompt]: 	Training 200/1106. train loss: 5.7314,	0.6575 s / batch. (data: 2.15e-02). ETA=19:33:27, max mem: 15.9 GB 
[10/27 22:20:42 visual_prompt]: 	Training 300/1106. train loss: 0.8066,	0.6336 s / batch. (data: 7.50e-04). ETA=18:49:45, max mem: 15.9 GB 
[10/27 22:21:45 visual_prompt]: 	Training 400/1106. train loss: 0.9578,	0.6359 s / batch. (data: 2.77e-04). ETA=18:52:51, max mem: 15.9 GB 
[10/27 22:22:48 visual_prompt]: 	Training 500/1106. train loss: 0.8151,	0.6443 s / batch. (data: 7.87e-04). ETA=19:06:44, max mem: 15.9 GB 
[10/27 22:23:52 visual_prompt]: 	Training 600/1106. train loss: 1.1350,	0.6200 s / batch. (data: 3.03e-04). ETA=18:22:21, max mem: 15.9 GB 
[10/27 22:24:55 visual_prompt]: 	Training 700/1106. train loss: 0.4251,	0.6280 s / batch. (data: 3.19e-04). ETA=18:35:36, max mem: 15.9 GB 
[10/27 22:25:58 visual_prompt]: 	Training 800/1106. train loss: 0.6670,	0.6331 s / batch. (data: 8.21e-04). ETA=18:43:36, max mem: 15.9 GB 
[10/27 22:27:02 visual_prompt]: 	Training 900/1106. train loss: 2.2328,	0.6392 s / batch. (data: 5.47e-03). ETA=18:53:15, max mem: 15.9 GB 
[10/27 22:28:05 visual_prompt]: 	Training 1000/1106. train loss: 0.2025,	0.6201 s / batch. (data: 7.58e-04). ETA=18:18:24, max mem: 15.9 GB 
[10/27 22:29:09 visual_prompt]: 	Training 1100/1106. train loss: 4.1245,	0.6189 s / batch. (data: 1.50e-04). ETA=18:15:17, max mem: 15.9 GB 
[10/27 22:29:12 visual_prompt]: Epoch 4 / 100: avg data time: 4.25e-03, avg batch time: 0.6356, average train loss: 1.5331
[10/27 22:30:02 visual_prompt]: 	Test 100/123. loss: 0.942, 0.2477 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/27 22:30:13 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.2314, average loss: 0.8680
[10/27 22:30:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.63	
[10/27 22:30:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/27 22:31:18 visual_prompt]: 	Training 100/1106. train loss: 1.1981,	0.6464 s / batch. (data: 7.91e-04). ETA=19:02:44, max mem: 15.9 GB 
[10/27 22:32:21 visual_prompt]: 	Training 200/1106. train loss: 0.1074,	0.6400 s / batch. (data: 8.15e-04). ETA=18:50:23, max mem: 15.9 GB 
[10/27 22:33:25 visual_prompt]: 	Training 300/1106. train loss: 0.9838,	0.6422 s / batch. (data: 8.27e-04). ETA=18:53:18, max mem: 15.9 GB 
[10/27 22:34:28 visual_prompt]: 	Training 400/1106. train loss: 3.8572,	0.6340 s / batch. (data: 7.83e-04). ETA=18:37:40, max mem: 15.9 GB 
[10/27 22:35:31 visual_prompt]: 	Training 500/1106. train loss: 0.1386,	0.6192 s / batch. (data: 3.13e-04). ETA=18:10:32, max mem: 15.9 GB 
[10/27 22:36:35 visual_prompt]: 	Training 600/1106. train loss: 0.2880,	0.6338 s / batch. (data: 3.18e-04). ETA=18:35:19, max mem: 15.9 GB 
[10/27 22:37:38 visual_prompt]: 	Training 700/1106. train loss: 1.5685,	0.6304 s / batch. (data: 3.18e-04). ETA=18:28:07, max mem: 15.9 GB 
[10/27 22:38:41 visual_prompt]: 	Training 800/1106. train loss: 2.9746,	0.6185 s / batch. (data: 2.99e-04). ETA=18:06:10, max mem: 15.9 GB 
[10/27 22:39:45 visual_prompt]: 	Training 900/1106. train loss: 1.2962,	0.6185 s / batch. (data: 4.42e-04). ETA=18:05:09, max mem: 15.9 GB 
[10/27 22:40:48 visual_prompt]: 	Training 1000/1106. train loss: 0.4058,	0.6328 s / batch. (data: 7.93e-04). ETA=18:29:16, max mem: 15.9 GB 
[10/27 22:41:51 visual_prompt]: 	Training 1100/1106. train loss: 3.1583,	0.6187 s / batch. (data: 1.90e-04). ETA=18:03:32, max mem: 15.9 GB 
[10/27 22:41:55 visual_prompt]: Epoch 5 / 100: avg data time: 3.84e-03, avg batch time: 0.6347, average train loss: 1.8720
[10/27 22:42:45 visual_prompt]: 	Test 100/123. loss: 5.397, 0.2250 s / batch. (data: 3.15e-05)max mem: 15.94594 GB 
[10/27 22:42:55 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2334, average loss: 4.8687
[10/27 22:42:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.56	
[10/27 22:42:55 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/27 22:44:00 visual_prompt]: 	Training 100/1106. train loss: 0.7120,	0.6298 s / batch. (data: 8.01e-04). ETA=18:21:51, max mem: 15.9 GB 
[10/27 22:45:04 visual_prompt]: 	Training 200/1106. train loss: 5.5262,	0.6385 s / batch. (data: 7.89e-04). ETA=18:35:59, max mem: 15.9 GB 
[10/27 22:46:07 visual_prompt]: 	Training 300/1106. train loss: 0.1433,	0.6395 s / batch. (data: 8.18e-04). ETA=18:36:39, max mem: 15.9 GB 
[10/27 22:47:11 visual_prompt]: 	Training 400/1106. train loss: 5.1879,	0.6301 s / batch. (data: 2.98e-04). ETA=18:19:14, max mem: 15.9 GB 
[10/27 22:48:14 visual_prompt]: 	Training 500/1106. train loss: 2.2145,	0.6193 s / batch. (data: 3.24e-04). ETA=17:59:17, max mem: 15.9 GB 
[10/27 22:49:17 visual_prompt]: 	Training 600/1106. train loss: 2.8919,	0.6646 s / batch. (data: 1.70e-02). ETA=19:17:10, max mem: 15.9 GB 
[10/27 22:50:21 visual_prompt]: 	Training 700/1106. train loss: 4.3488,	0.6467 s / batch. (data: 6.23e-04). ETA=18:44:59, max mem: 15.9 GB 
[10/27 22:51:24 visual_prompt]: 	Training 800/1106. train loss: 5.9759,	0.6190 s / batch. (data: 3.56e-04). ETA=17:55:40, max mem: 15.9 GB 
[10/27 22:52:27 visual_prompt]: 	Training 900/1106. train loss: 0.6908,	0.6313 s / batch. (data: 2.75e-04). ETA=18:16:00, max mem: 15.9 GB 
[10/27 22:53:31 visual_prompt]: 	Training 1000/1106. train loss: 0.3400,	0.6540 s / batch. (data: 1.40e-02). ETA=18:54:25, max mem: 15.9 GB 
[10/27 22:54:34 visual_prompt]: 	Training 1100/1106. train loss: 0.7759,	0.6187 s / batch. (data: 1.51e-04). ETA=17:52:03, max mem: 15.9 GB 
[10/27 22:54:38 visual_prompt]: Epoch 6 / 100: avg data time: 4.19e-03, avg batch time: 0.6351, average train loss: 2.4413
[10/27 22:55:28 visual_prompt]: 	Test 100/123. loss: 0.758, 0.2256 s / batch. (data: 4.08e-05)max mem: 15.94594 GB 
[10/27 22:55:38 visual_prompt]: Inference (val):avg data time: 1.19e-04, avg batch time: 0.2329, average loss: 0.7200
[10/27 22:55:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.23	
[10/27 22:55:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/27 22:56:43 visual_prompt]: 	Training 100/1106. train loss: 4.2410,	0.6279 s / batch. (data: 3.06e-04). ETA=18:06:53, max mem: 15.9 GB 
[10/27 22:57:47 visual_prompt]: 	Training 200/1106. train loss: 0.0276,	0.6330 s / batch. (data: 3.40e-04). ETA=18:14:47, max mem: 15.9 GB 
[10/27 22:58:50 visual_prompt]: 	Training 300/1106. train loss: 6.5361,	0.6186 s / batch. (data: 2.93e-04). ETA=17:48:48, max mem: 15.9 GB 
[10/27 22:59:53 visual_prompt]: 	Training 400/1106. train loss: 2.1674,	0.6341 s / batch. (data: 1.01e-03). ETA=18:14:29, max mem: 15.9 GB 
[10/27 23:00:57 visual_prompt]: 	Training 500/1106. train loss: 8.9593,	0.6292 s / batch. (data: 9.19e-03). ETA=18:04:59, max mem: 15.9 GB 
[10/27 23:02:00 visual_prompt]: 	Training 600/1106. train loss: 2.0646,	0.6334 s / batch. (data: 3.27e-04). ETA=18:11:08, max mem: 15.9 GB 
[10/27 23:03:04 visual_prompt]: 	Training 700/1106. train loss: 10.2507,	0.6513 s / batch. (data: 1.55e-02). ETA=18:40:55, max mem: 15.9 GB 
[10/27 23:04:07 visual_prompt]: 	Training 800/1106. train loss: 0.0258,	0.6320 s / batch. (data: 8.29e-04). ETA=18:06:41, max mem: 15.9 GB 
[10/27 23:05:10 visual_prompt]: 	Training 900/1106. train loss: 1.1892,	0.6388 s / batch. (data: 7.79e-04). ETA=18:17:19, max mem: 15.9 GB 
[10/27 23:06:14 visual_prompt]: 	Training 1000/1106. train loss: 1.7969,	0.6470 s / batch. (data: 1.56e-02). ETA=18:30:16, max mem: 15.9 GB 
[10/27 23:07:17 visual_prompt]: 	Training 1100/1106. train loss: 0.0001,	0.6181 s / batch. (data: 1.59e-04). ETA=17:39:40, max mem: 15.9 GB 
[10/27 23:07:21 visual_prompt]: Epoch 7 / 100: avg data time: 4.34e-03, avg batch time: 0.6354, average train loss: 2.8613
[10/27 23:08:11 visual_prompt]: 	Test 100/123. loss: 1.086, 0.2391 s / batch. (data: 5.58e-05)max mem: 15.94594 GB 
[10/27 23:08:22 visual_prompt]: Inference (val):avg data time: 9.58e-05, avg batch time: 0.2337, average loss: 1.1693
[10/27 23:08:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.99	
[10/27 23:08:22 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/27 23:09:27 visual_prompt]: 	Training 100/1106. train loss: 10.7177,	0.6413 s / batch. (data: 7.97e-04). ETA=18:18:18, max mem: 15.9 GB 
[10/27 23:10:30 visual_prompt]: 	Training 200/1106. train loss: 0.9533,	0.6400 s / batch. (data: 1.01e-03). ETA=18:15:00, max mem: 15.9 GB 
[10/27 23:11:34 visual_prompt]: 	Training 300/1106. train loss: 2.5949,	0.6360 s / batch. (data: 8.24e-04). ETA=18:07:07, max mem: 15.9 GB 
[10/27 23:12:37 visual_prompt]: 	Training 400/1106. train loss: 1.6525,	0.6200 s / batch. (data: 2.44e-04). ETA=17:38:45, max mem: 15.9 GB 
[10/27 23:13:41 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6431 s / batch. (data: 1.62e-02). ETA=18:17:09, max mem: 15.9 GB 
[10/27 23:14:44 visual_prompt]: 	Training 600/1106. train loss: 2.3341,	0.6413 s / batch. (data: 7.95e-04). ETA=18:12:56, max mem: 15.9 GB 
[10/27 23:15:47 visual_prompt]: 	Training 700/1106. train loss: 2.6329,	0.6509 s / batch. (data: 7.36e-03). ETA=18:28:16, max mem: 15.9 GB 
[10/27 23:16:51 visual_prompt]: 	Training 800/1106. train loss: 4.1156,	0.6203 s / batch. (data: 3.20e-04). ETA=17:35:04, max mem: 15.9 GB 
[10/27 23:17:54 visual_prompt]: 	Training 900/1106. train loss: 0.0001,	0.6191 s / batch. (data: 3.19e-04). ETA=17:32:02, max mem: 15.9 GB 
[10/27 23:18:57 visual_prompt]: 	Training 1000/1106. train loss: 0.0277,	0.6402 s / batch. (data: 8.07e-04). ETA=18:06:47, max mem: 15.9 GB 
[10/27 23:20:01 visual_prompt]: 	Training 1100/1106. train loss: 0.2640,	0.6172 s / batch. (data: 1.31e-04). ETA=17:26:43, max mem: 15.9 GB 
[10/27 23:20:05 visual_prompt]: Epoch 8 / 100: avg data time: 4.65e-03, avg batch time: 0.6354, average train loss: 2.9321
[10/27 23:20:54 visual_prompt]: 	Test 100/123. loss: 2.784, 0.2317 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/27 23:21:05 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2320, average loss: 2.5103
[10/27 23:21:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.84	
[10/27 23:21:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/27 23:22:10 visual_prompt]: 	Training 100/1106. train loss: 5.2608,	0.6318 s / batch. (data: 8.85e-04). ETA=17:50:25, max mem: 15.9 GB 
[10/27 23:23:14 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6328 s / batch. (data: 8.20e-04). ETA=17:51:02, max mem: 15.9 GB 
[10/27 23:24:17 visual_prompt]: 	Training 300/1106. train loss: 5.8252,	0.6337 s / batch. (data: 8.41e-04). ETA=17:51:28, max mem: 15.9 GB 
[10/27 23:25:20 visual_prompt]: 	Training 400/1106. train loss: 0.2895,	0.6197 s / batch. (data: 2.45e-04). ETA=17:26:52, max mem: 15.9 GB 
[10/27 23:26:24 visual_prompt]: 	Training 500/1106. train loss: 1.4590,	0.6236 s / batch. (data: 3.17e-04). ETA=17:32:22, max mem: 15.9 GB 
[10/27 23:27:27 visual_prompt]: 	Training 600/1106. train loss: 3.3056,	0.6464 s / batch. (data: 8.38e-04). ETA=18:09:45, max mem: 15.9 GB 
[10/27 23:28:30 visual_prompt]: 	Training 700/1106. train loss: 0.0072,	0.6191 s / batch. (data: 3.34e-04). ETA=17:22:44, max mem: 15.9 GB 
[10/27 23:29:34 visual_prompt]: 	Training 800/1106. train loss: 2.8968,	0.6322 s / batch. (data: 7.52e-04). ETA=17:43:44, max mem: 15.9 GB 
[10/27 23:30:37 visual_prompt]: 	Training 900/1106. train loss: 1.2276,	0.6491 s / batch. (data: 1.31e-02). ETA=18:11:01, max mem: 15.9 GB 
[10/27 23:31:40 visual_prompt]: 	Training 1000/1106. train loss: 3.8549,	0.6201 s / batch. (data: 3.13e-04). ETA=17:21:19, max mem: 15.9 GB 
[10/27 23:32:43 visual_prompt]: 	Training 1100/1106. train loss: 0.6132,	0.6171 s / batch. (data: 1.44e-04). ETA=17:15:15, max mem: 15.9 GB 
[10/27 23:32:47 visual_prompt]: Epoch 9 / 100: avg data time: 4.49e-03, avg batch time: 0.6347, average train loss: 3.2588
[10/27 23:33:37 visual_prompt]: 	Test 100/123. loss: 1.916, 0.2358 s / batch. (data: 5.03e-05)max mem: 15.94594 GB 
[10/27 23:33:48 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.2325, average loss: 1.7223
[10/27 23:33:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.79	
[10/27 23:33:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/27 23:34:53 visual_prompt]: 	Training 100/1106. train loss: 3.8863,	0.6485 s / batch. (data: 5.93e-03). ETA=18:06:42, max mem: 15.9 GB 
[10/27 23:35:57 visual_prompt]: 	Training 200/1106. train loss: 1.4220,	0.6328 s / batch. (data: 8.37e-04). ETA=17:39:24, max mem: 15.9 GB 
[10/27 23:37:00 visual_prompt]: 	Training 300/1106. train loss: 1.3379,	0.6440 s / batch. (data: 7.98e-04). ETA=17:57:02, max mem: 15.9 GB 
[10/27 23:38:03 visual_prompt]: 	Training 400/1106. train loss: 1.6900,	0.6485 s / batch. (data: 7.79e-04). ETA=18:03:25, max mem: 15.9 GB 
[10/27 23:39:06 visual_prompt]: 	Training 500/1106. train loss: 3.8191,	0.6422 s / batch. (data: 5.97e-03). ETA=17:51:50, max mem: 15.9 GB 
[10/27 23:40:10 visual_prompt]: 	Training 600/1106. train loss: 11.1001,	0.6573 s / batch. (data: 3.33e-02). ETA=18:16:02, max mem: 15.9 GB 
[10/27 23:41:13 visual_prompt]: 	Training 700/1106. train loss: 19.2503,	0.6295 s / batch. (data: 8.76e-04). ETA=17:28:37, max mem: 15.9 GB 
[10/27 23:42:16 visual_prompt]: 	Training 800/1106. train loss: 1.3423,	0.6600 s / batch. (data: 8.44e-04). ETA=18:18:16, max mem: 15.9 GB 
[10/27 23:43:19 visual_prompt]: 	Training 900/1106. train loss: 1.0123,	0.6361 s / batch. (data: 8.06e-04). ETA=17:37:30, max mem: 15.9 GB 
[10/27 23:44:23 visual_prompt]: 	Training 1000/1106. train loss: 0.0265,	0.6491 s / batch. (data: 2.91e-02). ETA=17:58:03, max mem: 15.9 GB 
[10/27 23:45:26 visual_prompt]: 	Training 1100/1106. train loss: 0.0186,	0.6191 s / batch. (data: 1.52e-04). ETA=17:07:13, max mem: 15.9 GB 
[10/27 23:45:30 visual_prompt]: Epoch 10 / 100: avg data time: 4.22e-03, avg batch time: 0.6348, average train loss: 3.8089
[10/27 23:46:19 visual_prompt]: 	Test 100/123. loss: 1.282, 0.2399 s / batch. (data: 2.43e-05)max mem: 15.94594 GB 
[10/27 23:46:30 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.2327, average loss: 1.3991
[10/27 23:46:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.91	
[10/27 23:46:30 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/27 23:47:35 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6183 s / batch. (data: 3.18e-04). ETA=17:04:39, max mem: 15.9 GB 
[10/27 23:48:39 visual_prompt]: 	Training 200/1106. train loss: 4.3173,	0.6770 s / batch. (data: 4.10e-02). ETA=18:40:51, max mem: 15.9 GB 
[10/27 23:49:42 visual_prompt]: 	Training 300/1106. train loss: 0.6788,	0.6338 s / batch. (data: 7.88e-04). ETA=17:28:22, max mem: 15.9 GB 
[10/27 23:50:46 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6314 s / batch. (data: 7.75e-04). ETA=17:23:12, max mem: 15.9 GB 
[10/27 23:51:49 visual_prompt]: 	Training 500/1106. train loss: 1.3404,	0.6307 s / batch. (data: 8.16e-04). ETA=17:21:01, max mem: 15.9 GB 
[10/27 23:52:53 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6325 s / batch. (data: 7.94e-04). ETA=17:23:01, max mem: 15.9 GB 
[10/27 23:53:56 visual_prompt]: 	Training 700/1106. train loss: 13.4421,	0.6331 s / batch. (data: 7.62e-04). ETA=17:22:53, max mem: 15.9 GB 
[10/27 23:54:59 visual_prompt]: 	Training 800/1106. train loss: 0.8994,	0.6405 s / batch. (data: 5.63e-03). ETA=17:34:01, max mem: 15.9 GB 
[10/27 23:56:02 visual_prompt]: 	Training 900/1106. train loss: 0.0206,	0.6253 s / batch. (data: 2.83e-04). ETA=17:07:56, max mem: 15.9 GB 
[10/27 23:57:06 visual_prompt]: 	Training 1000/1106. train loss: 0.0018,	0.6195 s / batch. (data: 2.94e-04). ETA=16:57:27, max mem: 15.9 GB 
[10/27 23:58:09 visual_prompt]: 	Training 1100/1106. train loss: 3.8857,	0.6197 s / batch. (data: 1.36e-04). ETA=16:56:38, max mem: 15.9 GB 
[10/27 23:58:13 visual_prompt]: Epoch 11 / 100: avg data time: 4.23e-03, avg batch time: 0.6352, average train loss: 4.2410
[10/27 23:59:03 visual_prompt]: 	Test 100/123. loss: 3.910, 0.2311 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/27 23:59:13 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2321, average loss: 3.5282
[10/27 23:59:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.95	
[10/27 23:59:13 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/28 00:00:19 visual_prompt]: 	Training 100/1106. train loss: 0.8493,	0.6256 s / batch. (data: 2.95e-04). ETA=17:05:14, max mem: 15.9 GB 
[10/28 00:01:23 visual_prompt]: 	Training 200/1106. train loss: 1.4350,	0.6303 s / batch. (data: 8.54e-04). ETA=17:11:58, max mem: 15.9 GB 
[10/28 00:02:26 visual_prompt]: 	Training 300/1106. train loss: 17.1787,	0.6552 s / batch. (data: 3.69e-02). ETA=17:51:38, max mem: 15.9 GB 
[10/28 00:03:29 visual_prompt]: 	Training 400/1106. train loss: 0.3061,	0.6244 s / batch. (data: 5.42e-03). ETA=17:00:14, max mem: 15.9 GB 
[10/28 00:04:33 visual_prompt]: 	Training 500/1106. train loss: 0.0010,	0.6438 s / batch. (data: 7.83e-04). ETA=17:30:54, max mem: 15.9 GB 
[10/28 00:05:36 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6555 s / batch. (data: 1.18e-03). ETA=17:48:53, max mem: 15.9 GB 
[10/28 00:06:39 visual_prompt]: 	Training 700/1106. train loss: 2.2254,	0.6412 s / batch. (data: 7.52e-04). ETA=17:24:31, max mem: 15.9 GB 
[10/28 00:07:43 visual_prompt]: 	Training 800/1106. train loss: 3.6103,	0.6305 s / batch. (data: 7.80e-04). ETA=17:05:56, max mem: 15.9 GB 
[10/28 00:08:46 visual_prompt]: 	Training 900/1106. train loss: 0.0324,	0.6321 s / batch. (data: 7.57e-04). ETA=17:07:29, max mem: 15.9 GB 
[10/28 00:09:49 visual_prompt]: 	Training 1000/1106. train loss: 10.5762,	0.6319 s / batch. (data: 7.78e-04). ETA=17:06:08, max mem: 15.9 GB 
[10/28 00:10:52 visual_prompt]: 	Training 1100/1106. train loss: 1.0603,	0.6172 s / batch. (data: 1.65e-04). ETA=16:41:13, max mem: 15.9 GB 
[10/28 00:10:56 visual_prompt]: Epoch 12 / 100: avg data time: 4.65e-03, avg batch time: 0.6356, average train loss: 4.4378
[10/28 00:11:46 visual_prompt]: 	Test 100/123. loss: 10.091, 0.2251 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/28 00:11:57 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2320, average loss: 9.2121
[10/28 00:11:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.63	
[10/28 00:11:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/28 00:13:02 visual_prompt]: 	Training 100/1106. train loss: 6.0889,	0.6190 s / batch. (data: 3.12e-04). ETA=16:43:05, max mem: 15.9 GB 
[10/28 00:14:05 visual_prompt]: 	Training 200/1106. train loss: 11.2657,	0.6405 s / batch. (data: 7.57e-04). ETA=17:16:48, max mem: 15.9 GB 
[10/28 00:15:09 visual_prompt]: 	Training 300/1106. train loss: 2.8574,	0.6216 s / batch. (data: 3.06e-04). ETA=16:45:07, max mem: 15.9 GB 
[10/28 00:16:12 visual_prompt]: 	Training 400/1106. train loss: 9.4165,	0.6312 s / batch. (data: 8.24e-04). ETA=16:59:36, max mem: 15.9 GB 
[10/28 00:17:15 visual_prompt]: 	Training 500/1106. train loss: 0.2393,	0.6239 s / batch. (data: 4.68e-04). ETA=16:46:53, max mem: 15.9 GB 
[10/28 00:18:18 visual_prompt]: 	Training 600/1106. train loss: 1.3075,	0.6185 s / batch. (data: 3.21e-04). ETA=16:37:02, max mem: 15.9 GB 
[10/28 00:19:22 visual_prompt]: 	Training 700/1106. train loss: 0.0359,	0.6474 s / batch. (data: 8.18e-04). ETA=17:22:32, max mem: 15.9 GB 
[10/28 00:20:25 visual_prompt]: 	Training 800/1106. train loss: 0.7719,	0.6383 s / batch. (data: 7.30e-04). ETA=17:06:54, max mem: 15.9 GB 
[10/28 00:21:29 visual_prompt]: 	Training 900/1106. train loss: 0.3627,	0.6314 s / batch. (data: 1.36e-02). ETA=16:54:47, max mem: 15.9 GB 
[10/28 00:22:32 visual_prompt]: 	Training 1000/1106. train loss: 0.0088,	0.6322 s / batch. (data: 8.23e-04). ETA=16:54:56, max mem: 15.9 GB 
[10/28 00:23:35 visual_prompt]: 	Training 1100/1106. train loss: 3.6586,	0.6197 s / batch. (data: 2.00e-04). ETA=16:33:54, max mem: 15.9 GB 
[10/28 00:23:39 visual_prompt]: Epoch 13 / 100: avg data time: 4.61e-03, avg batch time: 0.6350, average train loss: 4.0218
[10/28 00:24:29 visual_prompt]: 	Test 100/123. loss: 0.816, 0.2342 s / batch. (data: 5.17e-05)max mem: 15.94594 GB 
[10/28 00:24:39 visual_prompt]: Inference (val):avg data time: 3.14e-04, avg batch time: 0.2331, average loss: 0.8714
[10/28 00:24:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.85	
[10/28 00:24:39 visual_prompt]: Best epoch 13: best metric: -0.871
[10/28 00:24:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/28 00:25:45 visual_prompt]: 	Training 100/1106. train loss: 5.0730,	0.6303 s / batch. (data: 8.12e-04). ETA=16:49:47, max mem: 15.9 GB 
[10/28 00:26:48 visual_prompt]: 	Training 200/1106. train loss: 6.2029,	0.6333 s / batch. (data: 8.58e-04). ETA=16:53:29, max mem: 15.9 GB 
[10/28 00:27:52 visual_prompt]: 	Training 300/1106. train loss: 0.9022,	0.6299 s / batch. (data: 8.02e-04). ETA=16:46:58, max mem: 15.9 GB 
[10/28 00:28:55 visual_prompt]: 	Training 400/1106. train loss: 0.1477,	0.6331 s / batch. (data: 1.03e-03). ETA=16:51:03, max mem: 15.9 GB 
[10/28 00:29:58 visual_prompt]: 	Training 500/1106. train loss: 2.6662,	0.6191 s / batch. (data: 3.43e-04). ETA=16:27:45, max mem: 15.9 GB 
[10/28 00:31:02 visual_prompt]: 	Training 600/1106. train loss: 0.9664,	0.6344 s / batch. (data: 3.12e-04). ETA=16:51:06, max mem: 15.9 GB 
[10/28 00:32:05 visual_prompt]: 	Training 700/1106. train loss: 2.7330,	0.6202 s / batch. (data: 7.48e-04). ETA=16:27:18, max mem: 15.9 GB 
[10/28 00:33:08 visual_prompt]: 	Training 800/1106. train loss: 10.4784,	0.6247 s / batch. (data: 3.39e-04). ETA=16:33:29, max mem: 15.9 GB 
[10/28 00:34:12 visual_prompt]: 	Training 900/1106. train loss: 2.0830,	0.6601 s / batch. (data: 2.82e-02). ETA=17:28:37, max mem: 15.9 GB 
[10/28 00:35:15 visual_prompt]: 	Training 1000/1106. train loss: 1.0092,	0.6439 s / batch. (data: 1.22e-03). ETA=17:01:55, max mem: 15.9 GB 
[10/28 00:36:18 visual_prompt]: 	Training 1100/1106. train loss: 5.1140,	0.6174 s / batch. (data: 1.45e-04). ETA=16:18:50, max mem: 15.9 GB 
[10/28 00:36:22 visual_prompt]: Epoch 14 / 100: avg data time: 4.51e-03, avg batch time: 0.6350, average train loss: 4.4460
[10/28 00:37:11 visual_prompt]: 	Test 100/123. loss: 1.475, 0.2254 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[10/28 00:37:22 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2325, average loss: 1.5043
[10/28 00:37:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.95	
[10/28 00:37:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/28 00:38:27 visual_prompt]: 	Training 100/1106. train loss: 11.6980,	0.6353 s / batch. (data: 5.92e-03). ETA=16:46:00, max mem: 15.9 GB 
[10/28 00:39:31 visual_prompt]: 	Training 200/1106. train loss: 2.6402,	0.6205 s / batch. (data: 3.00e-04). ETA=16:21:39, max mem: 15.9 GB 
[10/28 00:40:34 visual_prompt]: 	Training 300/1106. train loss: 0.7251,	0.6341 s / batch. (data: 8.06e-04). ETA=16:41:58, max mem: 15.9 GB 
[10/28 00:41:37 visual_prompt]: 	Training 400/1106. train loss: 1.6049,	0.6332 s / batch. (data: 8.35e-04). ETA=16:39:38, max mem: 15.9 GB 
[10/28 00:42:41 visual_prompt]: 	Training 500/1106. train loss: 0.0015,	0.6280 s / batch. (data: 2.99e-04). ETA=16:30:19, max mem: 15.9 GB 
[10/28 00:43:44 visual_prompt]: 	Training 600/1106. train loss: 0.4793,	0.6439 s / batch. (data: 8.95e-04). ETA=16:54:18, max mem: 15.9 GB 
[10/28 00:44:47 visual_prompt]: 	Training 700/1106. train loss: 5.3029,	0.6187 s / batch. (data: 3.02e-04). ETA=16:13:38, max mem: 15.9 GB 
[10/28 00:45:50 visual_prompt]: 	Training 800/1106. train loss: 2.9064,	0.6351 s / batch. (data: 8.36e-04). ETA=16:38:18, max mem: 15.9 GB 
[10/28 00:46:54 visual_prompt]: 	Training 900/1106. train loss: 23.2489,	0.6518 s / batch. (data: 7.88e-04). ETA=17:03:30, max mem: 15.9 GB 
[10/28 00:47:57 visual_prompt]: 	Training 1000/1106. train loss: 3.7743,	0.6185 s / batch. (data: 3.12e-04). ETA=16:10:08, max mem: 15.9 GB 
[10/28 00:49:00 visual_prompt]: 	Training 1100/1106. train loss: 1.2777,	0.6194 s / batch. (data: 1.39e-04). ETA=16:10:34, max mem: 15.9 GB 
[10/28 00:49:04 visual_prompt]: Epoch 15 / 100: avg data time: 3.94e-03, avg batch time: 0.6344, average train loss: 4.4980
[10/28 00:49:53 visual_prompt]: 	Test 100/123. loss: 2.910, 0.2251 s / batch. (data: 3.65e-05)max mem: 15.94594 GB 
[10/28 00:50:04 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2322, average loss: 2.4872
[10/28 00:50:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.23	
[10/28 00:50:04 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/28 00:51:10 visual_prompt]: 	Training 100/1106. train loss: 0.0002,	0.6491 s / batch. (data: 8.23e-04). ETA=16:55:56, max mem: 15.9 GB 
[10/28 00:52:13 visual_prompt]: 	Training 200/1106. train loss: 3.0096,	0.6333 s / batch. (data: 2.99e-04). ETA=16:30:06, max mem: 15.9 GB 
[10/28 00:53:16 visual_prompt]: 	Training 300/1106. train loss: 3.5808,	0.6315 s / batch. (data: 7.98e-04). ETA=16:26:13, max mem: 15.9 GB 
[10/28 00:54:20 visual_prompt]: 	Training 400/1106. train loss: 2.8103,	0.6460 s / batch. (data: 7.99e-04). ETA=16:47:53, max mem: 15.9 GB 
[10/28 00:55:23 visual_prompt]: 	Training 500/1106. train loss: 5.8137,	0.6487 s / batch. (data: 8.05e-04). ETA=16:51:00, max mem: 15.9 GB 
[10/28 00:56:26 visual_prompt]: 	Training 600/1106. train loss: 0.0001,	0.6340 s / batch. (data: 7.39e-04). ETA=16:27:00, max mem: 15.9 GB 
[10/28 00:57:30 visual_prompt]: 	Training 700/1106. train loss: 2.6587,	0.6191 s / batch. (data: 2.90e-04). ETA=16:02:44, max mem: 15.9 GB 
[10/28 00:58:33 visual_prompt]: 	Training 800/1106. train loss: 1.1061,	0.6321 s / batch. (data: 3.14e-04). ETA=16:21:55, max mem: 15.9 GB 
[10/28 00:59:37 visual_prompt]: 	Training 900/1106. train loss: 3.0874,	0.6330 s / batch. (data: 3.29e-04). ETA=16:22:21, max mem: 15.9 GB 
[10/28 01:00:40 visual_prompt]: 	Training 1000/1106. train loss: 0.0253,	0.6289 s / batch. (data: 3.14e-04). ETA=16:14:58, max mem: 15.9 GB 
[10/28 01:01:43 visual_prompt]: 	Training 1100/1106. train loss: 28.4423,	0.6190 s / batch. (data: 1.52e-04). ETA=15:58:35, max mem: 15.9 GB 
[10/28 01:01:47 visual_prompt]: Epoch 16 / 100: avg data time: 4.21e-03, avg batch time: 0.6355, average train loss: 4.3051
[10/28 01:02:37 visual_prompt]: 	Test 100/123. loss: 1.522, 0.2253 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/28 01:02:48 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.2317, average loss: 1.7827
[10/28 01:02:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.38	
[10/28 01:02:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/28 01:03:53 visual_prompt]: 	Training 100/1106. train loss: 0.0001,	0.6319 s / batch. (data: 2.75e-04). ETA=16:17:24, max mem: 15.9 GB 
[10/28 01:04:57 visual_prompt]: 	Training 200/1106. train loss: 1.7640,	0.6483 s / batch. (data: 1.01e-02). ETA=16:41:40, max mem: 15.9 GB 
[10/28 01:06:00 visual_prompt]: 	Training 300/1106. train loss: 13.7373,	0.6197 s / batch. (data: 3.43e-04). ETA=15:56:28, max mem: 15.9 GB 
[10/28 01:07:03 visual_prompt]: 	Training 400/1106. train loss: 4.2120,	0.6344 s / batch. (data: 1.56e-02). ETA=16:18:01, max mem: 15.9 GB 
[10/28 01:08:06 visual_prompt]: 	Training 500/1106. train loss: 0.0033,	0.6310 s / batch. (data: 8.38e-04). ETA=16:11:42, max mem: 15.9 GB 
[10/28 01:09:10 visual_prompt]: 	Training 600/1106. train loss: 9.3951,	0.6402 s / batch. (data: 8.04e-04). ETA=16:24:53, max mem: 15.9 GB 
[10/28 01:10:13 visual_prompt]: 	Training 700/1106. train loss: 6.0161,	0.6308 s / batch. (data: 2.97e-04). ETA=16:09:26, max mem: 15.9 GB 
[10/28 01:11:16 visual_prompt]: 	Training 800/1106. train loss: 5.8439,	0.6173 s / batch. (data: 2.99e-04). ETA=15:47:37, max mem: 15.9 GB 
[10/28 01:12:19 visual_prompt]: 	Training 900/1106. train loss: 0.0370,	0.6303 s / batch. (data: 1.28e-02). ETA=16:06:28, max mem: 15.9 GB 
[10/28 01:13:23 visual_prompt]: 	Training 1000/1106. train loss: 4.2284,	0.6194 s / batch. (data: 3.28e-04). ETA=15:48:44, max mem: 15.9 GB 
[10/28 01:14:26 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6181 s / batch. (data: 1.72e-04). ETA=15:45:45, max mem: 15.9 GB 
[10/28 01:14:30 visual_prompt]: Epoch 17 / 100: avg data time: 4.41e-03, avg batch time: 0.6348, average train loss: 3.9816
[10/28 01:15:20 visual_prompt]: 	Test 100/123. loss: 1.000, 0.2253 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/28 01:15:30 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2318, average loss: 0.9212
[10/28 01:15:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.41	
[10/28 01:15:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/28 01:16:35 visual_prompt]: 	Training 100/1106. train loss: 16.5426,	0.6234 s / batch. (data: 3.09e-04). ETA=15:52:47, max mem: 15.9 GB 
[10/28 01:17:39 visual_prompt]: 	Training 200/1106. train loss: 5.2323,	0.6322 s / batch. (data: 8.08e-04). ETA=16:05:03, max mem: 15.9 GB 
[10/28 01:18:42 visual_prompt]: 	Training 300/1106. train loss: 1.2361,	0.6297 s / batch. (data: 3.06e-04). ETA=16:00:13, max mem: 15.9 GB 
[10/28 01:19:45 visual_prompt]: 	Training 400/1106. train loss: 13.6622,	0.6529 s / batch. (data: 7.55e-04). ETA=16:34:36, max mem: 15.9 GB 
[10/28 01:20:49 visual_prompt]: 	Training 500/1106. train loss: 0.0017,	0.6284 s / batch. (data: 8.08e-04). ETA=15:56:15, max mem: 15.9 GB 
[10/28 01:21:52 visual_prompt]: 	Training 600/1106. train loss: 3.7555,	0.6564 s / batch. (data: 1.04e-03). ETA=16:37:43, max mem: 15.9 GB 
[10/28 01:22:55 visual_prompt]: 	Training 700/1106. train loss: 18.2765,	0.6362 s / batch. (data: 7.70e-04). ETA=16:05:59, max mem: 15.9 GB 
[10/28 01:23:58 visual_prompt]: 	Training 800/1106. train loss: 1.9436,	0.6352 s / batch. (data: 7.76e-04). ETA=16:03:20, max mem: 15.9 GB 
[10/28 01:25:02 visual_prompt]: 	Training 900/1106. train loss: 8.1760,	0.6283 s / batch. (data: 8.63e-03). ETA=15:51:48, max mem: 15.9 GB 
[10/28 01:26:05 visual_prompt]: 	Training 1000/1106. train loss: 1.3792,	0.6203 s / batch. (data: 3.29e-04). ETA=15:38:40, max mem: 15.9 GB 
[10/28 01:27:09 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6178 s / batch. (data: 1.55e-04). ETA=15:33:57, max mem: 15.9 GB 
[10/28 01:27:12 visual_prompt]: Epoch 18 / 100: avg data time: 4.39e-03, avg batch time: 0.6350, average train loss: 4.5787
[10/28 01:28:02 visual_prompt]: 	Test 100/123. loss: 21.079, 0.2397 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/28 01:28:13 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.2334, average loss: 18.9439
[10/28 01:28:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.15	
[10/28 01:28:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/28 01:29:18 visual_prompt]: 	Training 100/1106. train loss: 4.6119,	0.6425 s / batch. (data: 5.95e-03). ETA=16:10:07, max mem: 15.9 GB 
[10/28 01:30:21 visual_prompt]: 	Training 200/1106. train loss: 0.5969,	0.6338 s / batch. (data: 8.23e-04). ETA=15:55:51, max mem: 15.9 GB 
[10/28 01:31:25 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6400 s / batch. (data: 5.44e-03). ETA=16:04:09, max mem: 15.9 GB 
[10/28 01:32:28 visual_prompt]: 	Training 400/1106. train loss: 5.3631,	0.6198 s / batch. (data: 7.90e-04). ETA=15:32:42, max mem: 15.9 GB 
[10/28 01:33:31 visual_prompt]: 	Training 500/1106. train loss: 0.0064,	0.6495 s / batch. (data: 8.36e-04). ETA=16:16:16, max mem: 15.9 GB 
[10/28 01:34:35 visual_prompt]: 	Training 600/1106. train loss: 0.0303,	0.6302 s / batch. (data: 8.27e-04). ETA=15:46:18, max mem: 15.9 GB 
[10/28 01:35:38 visual_prompt]: 	Training 700/1106. train loss: 0.7849,	0.6313 s / batch. (data: 7.99e-04). ETA=15:46:49, max mem: 15.9 GB 
[10/28 01:36:41 visual_prompt]: 	Training 800/1106. train loss: 0.0033,	0.6424 s / batch. (data: 7.78e-04). ETA=16:02:25, max mem: 15.9 GB 
[10/28 01:37:45 visual_prompt]: 	Training 900/1106. train loss: 0.0011,	0.6345 s / batch. (data: 9.32e-04). ETA=15:49:30, max mem: 15.9 GB 
[10/28 01:38:48 visual_prompt]: 	Training 1000/1106. train loss: 0.0002,	0.6307 s / batch. (data: 7.20e-04). ETA=15:42:47, max mem: 15.9 GB 
[10/28 01:39:51 visual_prompt]: 	Training 1100/1106. train loss: 0.0007,	0.6194 s / batch. (data: 1.70e-04). ETA=15:24:51, max mem: 15.9 GB 
[10/28 01:39:55 visual_prompt]: Epoch 19 / 100: avg data time: 4.35e-03, avg batch time: 0.6348, average train loss: 4.4674
[10/28 01:40:44 visual_prompt]: 	Test 100/123. loss: 8.082, 0.2277 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/28 01:40:55 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2335, average loss: 7.5960
[10/28 01:40:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.33	
[10/28 01:40:55 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/28 01:42:00 visual_prompt]: 	Training 100/1106. train loss: 2.9708,	0.6279 s / batch. (data: 2.99e-04). ETA=15:36:24, max mem: 15.9 GB 
[10/28 01:43:04 visual_prompt]: 	Training 200/1106. train loss: 9.8535,	0.6312 s / batch. (data: 7.82e-04). ETA=15:40:17, max mem: 15.9 GB 
[10/28 01:44:07 visual_prompt]: 	Training 300/1106. train loss: 3.9611,	0.6345 s / batch. (data: 3.38e-04). ETA=15:44:11, max mem: 15.9 GB 
[10/28 01:45:10 visual_prompt]: 	Training 400/1106. train loss: 0.7327,	0.6353 s / batch. (data: 8.31e-04). ETA=15:44:19, max mem: 15.9 GB 
[10/28 01:46:14 visual_prompt]: 	Training 500/1106. train loss: 2.7706,	0.6204 s / batch. (data: 2.92e-04). ETA=15:21:05, max mem: 15.9 GB 
[10/28 01:47:17 visual_prompt]: 	Training 600/1106. train loss: 0.0011,	0.6526 s / batch. (data: 8.21e-04). ETA=16:07:54, max mem: 15.9 GB 
[10/28 01:48:20 visual_prompt]: 	Training 700/1106. train loss: 0.0003,	0.6399 s / batch. (data: 7.88e-04). ETA=15:47:58, max mem: 15.9 GB 
[10/28 01:49:24 visual_prompt]: 	Training 800/1106. train loss: 7.2215,	0.6504 s / batch. (data: 5.54e-03). ETA=16:02:30, max mem: 15.9 GB 
[10/28 01:50:27 visual_prompt]: 	Training 900/1106. train loss: 0.1628,	0.6198 s / batch. (data: 3.15e-04). ETA=15:16:06, max mem: 15.9 GB 
[10/28 01:51:30 visual_prompt]: 	Training 1000/1106. train loss: 0.0053,	0.6337 s / batch. (data: 7.82e-04). ETA=15:35:40, max mem: 15.9 GB 
[10/28 01:52:34 visual_prompt]: 	Training 1100/1106. train loss: 4.4414,	0.6190 s / batch. (data: 1.53e-04). ETA=15:12:54, max mem: 15.9 GB 
[10/28 01:52:37 visual_prompt]: Epoch 20 / 100: avg data time: 4.40e-03, avg batch time: 0.6348, average train loss: 4.9034
[10/28 01:53:27 visual_prompt]: 	Test 100/123. loss: 0.662, 0.2371 s / batch. (data: 2.86e-05)max mem: 15.94594 GB 
[10/28 01:53:38 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.2330, average loss: 0.7199
[10/28 01:53:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 47.03	
[10/28 01:53:38 visual_prompt]: Best epoch 20: best metric: -0.720
[10/28 01:53:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[10/28 01:54:43 visual_prompt]: 	Training 100/1106. train loss: 21.3864,	0.6280 s / batch. (data: 3.06e-04). ETA=15:25:00, max mem: 15.9 GB 
[10/28 01:55:47 visual_prompt]: 	Training 200/1106. train loss: 7.3798,	0.6558 s / batch. (data: 1.05e-02). ETA=16:04:53, max mem: 15.9 GB 
[10/28 01:56:50 visual_prompt]: 	Training 300/1106. train loss: 0.0397,	0.6197 s / batch. (data: 3.29e-04). ETA=15:10:45, max mem: 15.9 GB 
[10/28 01:57:53 visual_prompt]: 	Training 400/1106. train loss: 3.2184,	0.6309 s / batch. (data: 8.71e-04). ETA=15:26:07, max mem: 15.9 GB 
[10/28 01:58:57 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6209 s / batch. (data: 3.19e-04). ETA=15:10:30, max mem: 15.9 GB 
[10/28 02:00:00 visual_prompt]: 	Training 600/1106. train loss: 0.0566,	0.6415 s / batch. (data: 8.06e-04). ETA=15:39:35, max mem: 15.9 GB 
[10/28 02:01:03 visual_prompt]: 	Training 700/1106. train loss: 1.7640,	0.6259 s / batch. (data: 2.96e-04). ETA=15:15:42, max mem: 15.9 GB 
[10/28 02:02:07 visual_prompt]: 	Training 800/1106. train loss: 5.4376,	0.6315 s / batch. (data: 3.29e-04). ETA=15:22:47, max mem: 15.9 GB 
[10/28 02:03:10 visual_prompt]: 	Training 900/1106. train loss: 8.2729,	0.6340 s / batch. (data: 8.07e-04). ETA=15:25:23, max mem: 15.9 GB 
[10/28 02:04:13 visual_prompt]: 	Training 1000/1106. train loss: 0.7674,	0.6199 s / batch. (data: 3.32e-04). ETA=15:03:49, max mem: 15.9 GB 
[10/28 02:05:16 visual_prompt]: 	Training 1100/1106. train loss: 0.1732,	0.6190 s / batch. (data: 1.47e-04). ETA=15:01:27, max mem: 15.9 GB 
[10/28 02:05:20 visual_prompt]: Epoch 21 / 100: avg data time: 4.44e-03, avg batch time: 0.6350, average train loss: 4.2463
[10/28 02:06:10 visual_prompt]: 	Test 100/123. loss: 4.006, 0.2276 s / batch. (data: 2.72e-05)max mem: 15.94594 GB 
[10/28 02:06:21 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2339, average loss: 3.6073
[10/28 02:06:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.69	
[10/28 02:06:21 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[10/28 02:07:26 visual_prompt]: 	Training 100/1106. train loss: 5.1024,	0.6276 s / batch. (data: 7.82e-04). ETA=15:12:49, max mem: 15.9 GB 
[10/28 02:08:29 visual_prompt]: 	Training 200/1106. train loss: 6.1683,	0.6314 s / batch. (data: 8.42e-04). ETA=15:17:22, max mem: 15.9 GB 
[10/28 02:09:33 visual_prompt]: 	Training 300/1106. train loss: 0.0015,	0.6302 s / batch. (data: 8.30e-04). ETA=15:14:30, max mem: 15.9 GB 
[10/28 02:10:36 visual_prompt]: 	Training 400/1106. train loss: 0.0001,	0.6331 s / batch. (data: 8.22e-04). ETA=15:17:42, max mem: 15.9 GB 
[10/28 02:11:39 visual_prompt]: 	Training 500/1106. train loss: 0.6112,	0.6303 s / batch. (data: 8.14e-04). ETA=15:12:40, max mem: 15.9 GB 
[10/28 02:12:42 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6187 s / batch. (data: 3.71e-04). ETA=14:54:45, max mem: 15.9 GB 
[10/28 02:13:46 visual_prompt]: 	Training 700/1106. train loss: 5.6235,	0.6307 s / batch. (data: 7.88e-04). ETA=15:11:07, max mem: 15.9 GB 
[10/28 02:14:49 visual_prompt]: 	Training 800/1106. train loss: 14.1437,	0.6297 s / batch. (data: 6.15e-04). ETA=15:08:37, max mem: 15.9 GB 
[10/28 02:15:52 visual_prompt]: 	Training 900/1106. train loss: 23.9545,	0.6353 s / batch. (data: 8.28e-04). ETA=15:15:33, max mem: 15.9 GB 
[10/28 02:16:56 visual_prompt]: 	Training 1000/1106. train loss: 5.4283,	0.6228 s / batch. (data: 3.29e-04). ETA=14:56:30, max mem: 15.9 GB 
[10/28 02:17:59 visual_prompt]: 	Training 1100/1106. train loss: 2.7944,	0.6195 s / batch. (data: 2.15e-04). ETA=14:50:50, max mem: 15.9 GB 
[10/28 02:18:03 visual_prompt]: Epoch 22 / 100: avg data time: 4.25e-03, avg batch time: 0.6350, average train loss: 4.1777
[10/28 02:18:53 visual_prompt]: 	Test 100/123. loss: 3.449, 0.2255 s / batch. (data: 5.72e-05)max mem: 15.94594 GB 
[10/28 02:19:03 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2321, average loss: 3.1388
[10/28 02:19:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.03	
[10/28 02:19:03 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[10/28 02:20:09 visual_prompt]: 	Training 100/1106. train loss: 0.5532,	0.6469 s / batch. (data: 8.02e-04). ETA=15:29:05, max mem: 15.9 GB 
[10/28 02:21:13 visual_prompt]: 	Training 200/1106. train loss: 5.1663,	0.6360 s / batch. (data: 1.20e-02). ETA=15:12:15, max mem: 15.9 GB 
[10/28 02:22:16 visual_prompt]: 	Training 300/1106. train loss: 0.0182,	0.6204 s / batch. (data: 2.82e-04). ETA=14:48:52, max mem: 15.9 GB 
[10/28 02:23:20 visual_prompt]: 	Training 400/1106. train loss: 5.3037,	0.6322 s / batch. (data: 8.00e-04). ETA=15:04:42, max mem: 15.9 GB 
[10/28 02:24:23 visual_prompt]: 	Training 500/1106. train loss: 1.5333,	0.6377 s / batch. (data: 7.17e-03). ETA=15:11:34, max mem: 15.9 GB 
[10/28 02:25:26 visual_prompt]: 	Training 600/1106. train loss: 6.6944,	0.6194 s / batch. (data: 3.11e-04). ETA=14:44:20, max mem: 15.9 GB 
[10/28 02:26:29 visual_prompt]: 	Training 700/1106. train loss: 9.5012,	0.6337 s / batch. (data: 7.97e-04). ETA=15:03:40, max mem: 15.9 GB 
[10/28 02:27:32 visual_prompt]: 	Training 800/1106. train loss: 4.7337,	0.6397 s / batch. (data: 7.62e-04). ETA=15:11:15, max mem: 15.9 GB 
[10/28 02:28:36 visual_prompt]: 	Training 900/1106. train loss: 0.9837,	0.6496 s / batch. (data: 8.70e-04). ETA=15:24:11, max mem: 15.9 GB 
[10/28 02:29:39 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6728 s / batch. (data: 1.60e-02). ETA=15:56:04, max mem: 15.9 GB 
[10/28 02:30:42 visual_prompt]: 	Training 1100/1106. train loss: 2.6226,	0.6189 s / batch. (data: 1.61e-04). ETA=14:38:34, max mem: 15.9 GB 
[10/28 02:30:46 visual_prompt]: Epoch 23 / 100: avg data time: 5.12e-03, avg batch time: 0.6352, average train loss: 4.2300
[10/28 02:31:36 visual_prompt]: 	Test 100/123. loss: 11.627, 0.2388 s / batch. (data: 3.86e-05)max mem: 15.94594 GB 
[10/28 02:31:46 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2325, average loss: 8.2505
[10/28 02:31:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 48.83	
[10/28 02:31:46 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[10/28 02:32:52 visual_prompt]: 	Training 100/1106. train loss: 0.0002,	0.6386 s / batch. (data: 8.39e-04). ETA=15:05:22, max mem: 15.9 GB 
[10/28 02:33:55 visual_prompt]: 	Training 200/1106. train loss: 0.7034,	0.6324 s / batch. (data: 7.67e-04). ETA=14:55:33, max mem: 15.9 GB 
[10/28 02:34:58 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6327 s / batch. (data: 8.44e-04). ETA=14:54:49, max mem: 15.9 GB 
[10/28 02:36:01 visual_prompt]: 	Training 400/1106. train loss: 2.8656,	0.6362 s / batch. (data: 8.04e-04). ETA=14:58:41, max mem: 15.9 GB 
[10/28 02:37:05 visual_prompt]: 	Training 500/1106. train loss: 3.2585,	0.6560 s / batch. (data: 1.53e-02). ETA=15:25:35, max mem: 15.9 GB 
[10/28 02:38:08 visual_prompt]: 	Training 600/1106. train loss: 1.6123,	0.6400 s / batch. (data: 2.44e-04). ETA=15:02:01, max mem: 15.9 GB 
[10/28 02:39:11 visual_prompt]: 	Training 700/1106. train loss: 0.0693,	0.6412 s / batch. (data: 2.34e-02). ETA=15:02:40, max mem: 15.9 GB 
[10/28 02:40:15 visual_prompt]: 	Training 800/1106. train loss: 4.4360,	0.6350 s / batch. (data: 3.39e-04). ETA=14:52:50, max mem: 15.9 GB 
[10/28 02:41:18 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6183 s / batch. (data: 3.22e-04). ETA=14:28:18, max mem: 15.9 GB 
[10/28 02:42:22 visual_prompt]: 	Training 1000/1106. train loss: 3.6576,	0.6352 s / batch. (data: 7.26e-04). ETA=14:50:58, max mem: 15.9 GB 
[10/28 02:43:25 visual_prompt]: 	Training 1100/1106. train loss: 5.4776,	0.6173 s / batch. (data: 1.46e-04). ETA=14:24:53, max mem: 15.9 GB 
[10/28 02:43:28 visual_prompt]: Epoch 24 / 100: avg data time: 4.77e-03, avg batch time: 0.6347, average train loss: 3.8872
[10/28 02:44:18 visual_prompt]: 	Test 100/123. loss: 4.159, 0.2446 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[10/28 02:44:29 visual_prompt]: Inference (val):avg data time: 9.89e-05, avg batch time: 0.2311, average loss: 4.9817
[10/28 02:44:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.99	
[10/28 02:44:29 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[10/28 02:45:34 visual_prompt]: 	Training 100/1106. train loss: 2.2017,	0.6431 s / batch. (data: 8.33e-04). ETA=14:59:49, max mem: 15.9 GB 
[10/28 02:46:38 visual_prompt]: 	Training 200/1106. train loss: 1.9198,	0.6442 s / batch. (data: 8.12e-04). ETA=15:00:21, max mem: 15.9 GB 
[10/28 02:47:41 visual_prompt]: 	Training 300/1106. train loss: 6.4958,	0.6401 s / batch. (data: 1.60e-02). ETA=14:53:28, max mem: 15.9 GB 
[10/28 02:48:44 visual_prompt]: 	Training 400/1106. train loss: 10.0163,	0.6327 s / batch. (data: 3.03e-04). ETA=14:42:06, max mem: 15.9 GB 
[10/28 02:49:48 visual_prompt]: 	Training 500/1106. train loss: 0.2050,	0.6174 s / batch. (data: 2.38e-04). ETA=14:19:45, max mem: 15.9 GB 
[10/28 02:50:51 visual_prompt]: 	Training 600/1106. train loss: 3.8596,	0.6181 s / batch. (data: 3.87e-04). ETA=14:19:46, max mem: 15.9 GB 
[10/28 02:51:55 visual_prompt]: 	Training 700/1106. train loss: 6.0390,	0.6414 s / batch. (data: 1.10e-02). ETA=14:51:08, max mem: 15.9 GB 
[10/28 02:52:58 visual_prompt]: 	Training 800/1106. train loss: 0.0047,	0.6339 s / batch. (data: 8.14e-04). ETA=14:39:34, max mem: 15.9 GB 
[10/28 02:54:01 visual_prompt]: 	Training 900/1106. train loss: 7.3138,	0.6341 s / batch. (data: 8.16e-04). ETA=14:38:45, max mem: 15.9 GB 
[10/28 02:55:04 visual_prompt]: 	Training 1000/1106. train loss: 0.7648,	0.6369 s / batch. (data: 8.20e-04). ETA=14:41:38, max mem: 15.9 GB 
[10/28 02:56:08 visual_prompt]: 	Training 1100/1106. train loss: 7.5019,	0.6181 s / batch. (data: 1.83e-04). ETA=14:14:35, max mem: 15.9 GB 
[10/28 02:56:11 visual_prompt]: Epoch 25 / 100: avg data time: 4.79e-03, avg batch time: 0.6352, average train loss: 3.5157
[10/28 02:57:01 visual_prompt]: 	Test 100/123. loss: 10.493, 0.2355 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/28 02:57:12 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.2324, average loss: 9.4153
[10/28 02:57:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.21	
[10/28 02:57:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[10/28 02:58:18 visual_prompt]: 	Training 100/1106. train loss: 2.8428,	0.6417 s / batch. (data: 8.67e-04). ETA=14:46:03, max mem: 15.9 GB 
[10/28 02:59:21 visual_prompt]: 	Training 200/1106. train loss: 0.9380,	0.6369 s / batch. (data: 8.12e-04). ETA=14:38:26, max mem: 15.9 GB 
[10/28 03:00:24 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6308 s / batch. (data: 8.08e-04). ETA=14:28:52, max mem: 15.9 GB 
[10/28 03:01:27 visual_prompt]: 	Training 400/1106. train loss: 2.5971,	0.6277 s / batch. (data: 3.32e-04). ETA=14:23:35, max mem: 15.9 GB 
[10/28 03:02:31 visual_prompt]: 	Training 500/1106. train loss: 1.5445,	0.6304 s / batch. (data: 7.93e-04). ETA=14:26:15, max mem: 15.9 GB 
[10/28 03:03:34 visual_prompt]: 	Training 600/1106. train loss: 2.4204,	0.6368 s / batch. (data: 1.05e-02). ETA=14:34:01, max mem: 15.9 GB 
[10/28 03:04:37 visual_prompt]: 	Training 700/1106. train loss: 0.0127,	0.6259 s / batch. (data: 3.12e-04). ETA=14:18:00, max mem: 15.9 GB 
[10/28 03:05:41 visual_prompt]: 	Training 800/1106. train loss: 6.6348,	0.6320 s / batch. (data: 1.20e-02). ETA=14:25:15, max mem: 15.9 GB 
[10/28 03:06:44 visual_prompt]: 	Training 900/1106. train loss: 6.2149,	0.6201 s / batch. (data: 3.30e-04). ETA=14:08:02, max mem: 15.9 GB 
[10/28 03:07:47 visual_prompt]: 	Training 1000/1106. train loss: 0.2804,	0.6335 s / batch. (data: 2.74e-04). ETA=14:25:13, max mem: 15.9 GB 
[10/28 03:08:51 visual_prompt]: 	Training 1100/1106. train loss: 0.0285,	0.6185 s / batch. (data: 1.48e-04). ETA=14:03:45, max mem: 15.9 GB 
[10/28 03:08:55 visual_prompt]: Epoch 26 / 100: avg data time: 4.64e-03, avg batch time: 0.6353, average train loss: 3.5556
[10/28 03:09:44 visual_prompt]: 	Test 100/123. loss: 3.563, 0.2252 s / batch. (data: 3.84e-05)max mem: 15.94594 GB 
[10/28 03:09:55 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2341, average loss: 3.2049
[10/28 03:09:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.94	
[10/28 03:09:55 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[10/28 03:11:00 visual_prompt]: 	Training 100/1106. train loss: 24.3836,	0.6278 s / batch. (data: 3.04e-04). ETA=14:15:15, max mem: 15.9 GB 
[10/28 03:12:04 visual_prompt]: 	Training 200/1106. train loss: 7.3077,	0.6776 s / batch. (data: 8.95e-04). ETA=15:22:01, max mem: 15.9 GB 
[10/28 03:13:07 visual_prompt]: 	Training 300/1106. train loss: 1.9507,	0.6190 s / batch. (data: 3.16e-04). ETA=14:01:13, max mem: 15.9 GB 
[10/28 03:14:11 visual_prompt]: 	Training 400/1106. train loss: 1.3933,	0.6286 s / batch. (data: 3.01e-04). ETA=14:13:19, max mem: 15.9 GB 
[10/28 03:15:14 visual_prompt]: 	Training 500/1106. train loss: 4.4304,	0.6746 s / batch. (data: 5.86e-03). ETA=15:14:35, max mem: 15.9 GB 
[10/28 03:16:17 visual_prompt]: 	Training 600/1106. train loss: 7.6581,	0.6185 s / batch. (data: 3.31e-04). ETA=13:57:26, max mem: 15.9 GB 
[10/28 03:17:21 visual_prompt]: 	Training 700/1106. train loss: 3.6499,	0.6331 s / batch. (data: 8.27e-04). ETA=14:16:12, max mem: 15.9 GB 
[10/28 03:18:24 visual_prompt]: 	Training 800/1106. train loss: 1.0807,	0.6473 s / batch. (data: 7.97e-04). ETA=14:34:18, max mem: 15.9 GB 
[10/28 03:19:27 visual_prompt]: 	Training 900/1106. train loss: 0.6848,	0.6192 s / batch. (data: 3.12e-04). ETA=13:55:18, max mem: 15.9 GB 
[10/28 03:20:31 visual_prompt]: 	Training 1000/1106. train loss: 19.2602,	0.6309 s / batch. (data: 7.96e-04). ETA=14:10:01, max mem: 15.9 GB 
[10/28 03:21:34 visual_prompt]: 	Training 1100/1106. train loss: 2.9365,	0.6176 s / batch. (data: 1.48e-04). ETA=13:51:05, max mem: 15.9 GB 
[10/28 03:21:37 visual_prompt]: Epoch 27 / 100: avg data time: 4.27e-03, avg batch time: 0.6351, average train loss: 4.4163
[10/28 03:22:27 visual_prompt]: 	Test 100/123. loss: 7.785, 0.2559 s / batch. (data: 2.38e-05)max mem: 15.94594 GB 
[10/28 03:22:38 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2329, average loss: 7.0185
[10/28 03:22:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.15	
[10/28 03:22:38 visual_prompt]: Stopping early.
[10/28 03:22:38 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 03:22:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 03:22:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 03:22:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 03:22:38 visual_prompt]: Training with config:
[10/28 03:22:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr1.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 03:22:38 visual_prompt]: Loading training data...
[10/28 03:22:38 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 03:22:38 visual_prompt]: Loading validation data...
[10/28 03:22:38 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 03:22:38 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/28 03:22:40 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/28 03:22:40 visual_prompt]: tuned percent:0.522
[10/28 03:22:40 visual_prompt]: Device used for model: 0
[10/28 03:22:40 visual_prompt]: Setting up Evaluator...
[10/28 03:22:40 visual_prompt]: Setting up Trainer...
[10/28 03:22:40 visual_prompt]: 	Setting up the optimizer...
[10/28 03:22:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 03:23:46 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6655 s / batch. (data: 3.35e-02). ETA=20:25:41, max mem: 15.9 GB 
[10/28 03:24:50 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6344 s / batch. (data: 1.61e-02). ETA=19:27:12, max mem: 15.9 GB 
[10/28 03:25:53 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6360 s / batch. (data: 7.73e-04). ETA=19:29:13, max mem: 15.9 GB 
[10/28 03:26:56 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6316 s / batch. (data: 8.08e-04). ETA=19:19:59, max mem: 15.9 GB 
[10/28 03:28:00 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6473 s / batch. (data: 3.07e-04). ETA=19:47:46, max mem: 15.9 GB 
[10/28 03:29:03 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6398 s / batch. (data: 8.32e-04). ETA=19:32:57, max mem: 15.9 GB 
[10/28 03:30:07 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6313 s / batch. (data: 1.20e-02). ETA=19:16:19, max mem: 15.9 GB 
[10/28 03:31:10 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6254 s / batch. (data: 2.92e-04). ETA=19:04:31, max mem: 15.9 GB 
[10/28 03:32:13 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6552 s / batch. (data: 5.44e-03). ETA=19:57:56, max mem: 15.9 GB 
[10/28 03:33:17 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6308 s / batch. (data: 8.17e-04). ETA=19:12:13, max mem: 15.9 GB 
[10/28 03:34:20 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6181 s / batch. (data: 1.44e-04). ETA=18:48:04, max mem: 15.9 GB 
[10/28 03:34:24 visual_prompt]: Epoch 1 / 100: avg data time: 4.93e-03, avg batch time: 0.6360, average train loss: 1.4028
[10/28 03:35:14 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2261 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/28 03:35:24 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2335, average loss: 1.3505
[10/28 03:35:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/28 03:35:24 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/28 03:36:29 visual_prompt]: 	Training 100/1106. train loss: 2.7343,	0.6505 s / batch. (data: 2.88e-02). ETA=19:46:00, max mem: 15.9 GB 
[10/28 03:37:32 visual_prompt]: 	Training 200/1106. train loss: 1.0004,	0.6312 s / batch. (data: 3.00e-04). ETA=19:09:45, max mem: 15.9 GB 
[10/28 03:38:36 visual_prompt]: 	Training 300/1106. train loss: 1.4783,	0.6180 s / batch. (data: 3.04e-04). ETA=18:44:41, max mem: 15.9 GB 
[10/28 03:39:39 visual_prompt]: 	Training 400/1106. train loss: 0.0094,	0.6195 s / batch. (data: 3.06e-04). ETA=18:46:23, max mem: 15.9 GB 
[10/28 03:40:42 visual_prompt]: 	Training 500/1106. train loss: 2.3072,	0.6715 s / batch. (data: 3.35e-02). ETA=20:19:50, max mem: 15.9 GB 
[10/28 03:41:46 visual_prompt]: 	Training 600/1106. train loss: 0.7223,	0.6355 s / batch. (data: 1.62e-02). ETA=19:13:25, max mem: 15.9 GB 
[10/28 03:42:49 visual_prompt]: 	Training 700/1106. train loss: 0.3812,	0.6443 s / batch. (data: 8.05e-04). ETA=19:28:20, max mem: 15.9 GB 
[10/28 03:43:52 visual_prompt]: 	Training 800/1106. train loss: 1.0114,	0.6520 s / batch. (data: 3.02e-04). ETA=19:41:12, max mem: 15.9 GB 
[10/28 03:44:55 visual_prompt]: 	Training 900/1106. train loss: 1.3307,	0.6198 s / batch. (data: 7.61e-04). ETA=18:41:48, max mem: 15.9 GB 
[10/28 03:45:59 visual_prompt]: 	Training 1000/1106. train loss: 0.0412,	0.6192 s / batch. (data: 3.15e-04). ETA=18:39:41, max mem: 15.9 GB 
[10/28 03:47:02 visual_prompt]: 	Training 1100/1106. train loss: 0.3101,	0.6195 s / batch. (data: 1.54e-04). ETA=18:39:06, max mem: 15.9 GB 
[10/28 03:47:06 visual_prompt]: Epoch 2 / 100: avg data time: 3.92e-03, avg batch time: 0.6343, average train loss: 1.0869
[10/28 03:47:56 visual_prompt]: 	Test 100/123. loss: 1.250, 0.2257 s / batch. (data: 3.50e-05)max mem: 15.94594 GB 
[10/28 03:48:06 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2336, average loss: 1.1363
[10/28 03:48:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.74	
[10/28 03:48:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/28 03:49:12 visual_prompt]: 	Training 100/1106. train loss: 1.7902,	0.6236 s / batch. (data: 3.27e-04). ETA=18:45:29, max mem: 15.9 GB 
[10/28 03:50:15 visual_prompt]: 	Training 200/1106. train loss: 0.3119,	0.6418 s / batch. (data: 8.40e-04). ETA=19:17:18, max mem: 15.9 GB 
[10/28 03:51:19 visual_prompt]: 	Training 300/1106. train loss: 0.2617,	0.6288 s / batch. (data: 3.35e-04). ETA=18:52:50, max mem: 15.9 GB 
[10/28 03:52:22 visual_prompt]: 	Training 400/1106. train loss: 0.6894,	0.6348 s / batch. (data: 8.19e-04). ETA=19:02:27, max mem: 15.9 GB 
[10/28 03:53:25 visual_prompt]: 	Training 500/1106. train loss: 1.3923,	0.6313 s / batch. (data: 8.45e-04). ETA=18:55:04, max mem: 15.9 GB 
[10/28 03:54:28 visual_prompt]: 	Training 600/1106. train loss: 0.0444,	0.6402 s / batch. (data: 8.28e-04). ETA=19:10:05, max mem: 15.9 GB 
[10/28 03:55:32 visual_prompt]: 	Training 700/1106. train loss: 0.9962,	0.6191 s / batch. (data: 3.23e-04). ETA=18:31:06, max mem: 15.9 GB 
[10/28 03:56:35 visual_prompt]: 	Training 800/1106. train loss: 0.1921,	0.6248 s / batch. (data: 2.97e-04). ETA=18:40:18, max mem: 15.9 GB 
[10/28 03:57:39 visual_prompt]: 	Training 900/1106. train loss: 4.1804,	0.6178 s / batch. (data: 3.30e-04). ETA=18:26:45, max mem: 15.9 GB 
[10/28 03:58:42 visual_prompt]: 	Training 1000/1106. train loss: 1.1964,	0.6335 s / batch. (data: 1.60e-02). ETA=18:53:52, max mem: 15.9 GB 
[10/28 03:59:45 visual_prompt]: 	Training 1100/1106. train loss: 0.9827,	0.6189 s / batch. (data: 1.46e-04). ETA=18:26:39, max mem: 15.9 GB 
[10/28 03:59:49 visual_prompt]: Epoch 3 / 100: avg data time: 4.99e-03, avg batch time: 0.6353, average train loss: 1.0513
[10/28 04:00:39 visual_prompt]: 	Test 100/123. loss: 1.455, 0.2248 s / batch. (data: 4.08e-05)max mem: 15.94594 GB 
[10/28 04:00:49 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2328, average loss: 1.5953
[10/28 04:00:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.47	
[10/28 04:00:49 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/28 04:01:55 visual_prompt]: 	Training 100/1106. train loss: 0.3010,	0.6263 s / batch. (data: 3.04e-04). ETA=18:38:50, max mem: 15.9 GB 
[10/28 04:02:59 visual_prompt]: 	Training 200/1106. train loss: 7.2034,	0.6626 s / batch. (data: 1.86e-02). ETA=19:42:37, max mem: 15.9 GB 
[10/28 04:04:02 visual_prompt]: 	Training 300/1106. train loss: 0.8601,	0.6181 s / batch. (data: 2.94e-04). ETA=18:22:00, max mem: 15.9 GB 
[10/28 04:05:05 visual_prompt]: 	Training 400/1106. train loss: 1.4353,	0.6202 s / batch. (data: 1.05e-03). ETA=18:24:52, max mem: 15.9 GB 
[10/28 04:06:08 visual_prompt]: 	Training 500/1106. train loss: 0.7876,	0.6241 s / batch. (data: 3.23e-04). ETA=18:30:37, max mem: 15.9 GB 
[10/28 04:07:12 visual_prompt]: 	Training 600/1106. train loss: 0.8075,	0.6522 s / batch. (data: 1.62e-02). ETA=19:19:42, max mem: 15.9 GB 
[10/28 04:08:15 visual_prompt]: 	Training 700/1106. train loss: 2.6052,	0.6210 s / batch. (data: 3.31e-04). ETA=18:23:05, max mem: 15.9 GB 
[10/28 04:09:18 visual_prompt]: 	Training 800/1106. train loss: 0.4238,	0.6319 s / batch. (data: 7.84e-04). ETA=18:41:28, max mem: 15.9 GB 
[10/28 04:10:21 visual_prompt]: 	Training 900/1106. train loss: 3.6156,	0.6334 s / batch. (data: 8.14e-04). ETA=18:43:06, max mem: 15.9 GB 
[10/28 04:11:25 visual_prompt]: 	Training 1000/1106. train loss: 4.7116,	0.6413 s / batch. (data: 6.09e-03). ETA=18:56:03, max mem: 15.9 GB 
[10/28 04:12:28 visual_prompt]: 	Training 1100/1106. train loss: 3.0738,	0.6181 s / batch. (data: 1.52e-04). ETA=18:13:53, max mem: 15.9 GB 
[10/28 04:12:32 visual_prompt]: Epoch 4 / 100: avg data time: 4.58e-03, avg batch time: 0.6352, average train loss: 1.4794
[10/28 04:13:22 visual_prompt]: 	Test 100/123. loss: 0.145, 0.2324 s / batch. (data: 3.89e-05)max mem: 15.94594 GB 
[10/28 04:13:33 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.2326, average loss: 1.2460
[10/28 04:13:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 52.08	
[10/28 04:13:33 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/28 04:14:38 visual_prompt]: 	Training 100/1106. train loss: 0.7088,	0.6412 s / batch. (data: 8.07e-04). ETA=18:53:39, max mem: 15.9 GB 
[10/28 04:15:41 visual_prompt]: 	Training 200/1106. train loss: 0.0066,	0.6560 s / batch. (data: 2.51e-02). ETA=19:18:44, max mem: 15.9 GB 
[10/28 04:16:44 visual_prompt]: 	Training 300/1106. train loss: 0.9309,	0.6331 s / batch. (data: 8.02e-04). ETA=18:37:06, max mem: 15.9 GB 
[10/28 04:17:48 visual_prompt]: 	Training 400/1106. train loss: 2.8470,	0.6459 s / batch. (data: 1.61e-02). ETA=18:58:35, max mem: 15.9 GB 
[10/28 04:18:51 visual_prompt]: 	Training 500/1106. train loss: 0.1694,	0.6337 s / batch. (data: 7.50e-04). ETA=18:36:05, max mem: 15.9 GB 
[10/28 04:19:54 visual_prompt]: 	Training 600/1106. train loss: 1.2586,	0.6238 s / batch. (data: 5.30e-04). ETA=18:17:39, max mem: 15.9 GB 
[10/28 04:20:58 visual_prompt]: 	Training 700/1106. train loss: 1.0900,	0.6299 s / batch. (data: 7.54e-04). ETA=18:27:19, max mem: 15.9 GB 
[10/28 04:22:01 visual_prompt]: 	Training 800/1106. train loss: 1.6634,	0.6190 s / batch. (data: 3.09e-04). ETA=18:07:07, max mem: 15.9 GB 
[10/28 04:23:04 visual_prompt]: 	Training 900/1106. train loss: 2.2126,	0.6334 s / batch. (data: 7.79e-04). ETA=18:31:19, max mem: 15.9 GB 
[10/28 04:24:07 visual_prompt]: 	Training 1000/1106. train loss: 0.0228,	0.6244 s / batch. (data: 3.31e-04). ETA=18:14:28, max mem: 15.9 GB 
[10/28 04:25:11 visual_prompt]: 	Training 1100/1106. train loss: 2.6575,	0.6191 s / batch. (data: 2.11e-04). ETA=18:04:12, max mem: 15.9 GB 
[10/28 04:25:14 visual_prompt]: Epoch 5 / 100: avg data time: 4.42e-03, avg batch time: 0.6345, average train loss: 1.6277
[10/28 04:26:04 visual_prompt]: 	Test 100/123. loss: 1.068, 0.2259 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[10/28 04:26:15 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2322, average loss: 1.1615
[10/28 04:26:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.76	
[10/28 04:26:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/28 04:27:20 visual_prompt]: 	Training 100/1106. train loss: 1.7758,	0.6282 s / batch. (data: 1.11e-02). ETA=18:19:05, max mem: 15.9 GB 
[10/28 04:28:24 visual_prompt]: 	Training 200/1106. train loss: 0.1554,	0.6185 s / batch. (data: 3.20e-04). ETA=18:01:05, max mem: 15.9 GB 
[10/28 04:29:27 visual_prompt]: 	Training 300/1106. train loss: 0.0031,	0.6348 s / batch. (data: 8.32e-04). ETA=18:28:29, max mem: 15.9 GB 
[10/28 04:30:30 visual_prompt]: 	Training 400/1106. train loss: 0.1900,	0.6365 s / batch. (data: 5.91e-03). ETA=18:30:22, max mem: 15.9 GB 
[10/28 04:31:33 visual_prompt]: 	Training 500/1106. train loss: 3.0043,	0.6194 s / batch. (data: 2.88e-04). ETA=17:59:28, max mem: 15.9 GB 
[10/28 04:32:37 visual_prompt]: 	Training 600/1106. train loss: 0.6927,	0.6309 s / batch. (data: 8.33e-04). ETA=18:18:28, max mem: 15.9 GB 
[10/28 04:33:40 visual_prompt]: 	Training 700/1106. train loss: 7.0911,	0.6197 s / batch. (data: 3.20e-04). ETA=17:57:58, max mem: 15.9 GB 
[10/28 04:34:43 visual_prompt]: 	Training 800/1106. train loss: 5.7353,	0.6590 s / batch. (data: 4.07e-02). ETA=19:05:11, max mem: 15.9 GB 
[10/28 04:35:46 visual_prompt]: 	Training 900/1106. train loss: 11.2382,	0.6340 s / batch. (data: 3.17e-04). ETA=18:20:45, max mem: 15.9 GB 
[10/28 04:36:50 visual_prompt]: 	Training 1000/1106. train loss: 0.0449,	0.6362 s / batch. (data: 5.43e-03). ETA=18:23:25, max mem: 15.9 GB 
[10/28 04:37:53 visual_prompt]: 	Training 1100/1106. train loss: 0.3050,	0.6175 s / batch. (data: 2.08e-04). ETA=17:49:57, max mem: 15.9 GB 
[10/28 04:37:57 visual_prompt]: Epoch 6 / 100: avg data time: 4.05e-03, avg batch time: 0.6344, average train loss: 2.5603
[10/28 04:38:47 visual_prompt]: 	Test 100/123. loss: 0.693, 0.2356 s / batch. (data: 3.50e-05)max mem: 15.94594 GB 
[10/28 04:38:57 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2317, average loss: 0.6981
[10/28 04:38:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.09	
[10/28 04:38:57 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/28 04:40:02 visual_prompt]: 	Training 100/1106. train loss: 5.4376,	0.6301 s / batch. (data: 7.89e-04). ETA=18:10:46, max mem: 15.9 GB 
[10/28 04:41:05 visual_prompt]: 	Training 200/1106. train loss: 4.3713,	0.6394 s / batch. (data: 8.05e-04). ETA=18:25:51, max mem: 15.9 GB 
[10/28 04:42:08 visual_prompt]: 	Training 300/1106. train loss: 4.1050,	0.6215 s / batch. (data: 3.00e-04). ETA=17:53:50, max mem: 15.9 GB 
[10/28 04:43:11 visual_prompt]: 	Training 400/1106. train loss: 3.5914,	0.6187 s / batch. (data: 3.13e-04). ETA=17:47:54, max mem: 15.9 GB 
[10/28 04:44:15 visual_prompt]: 	Training 500/1106. train loss: 1.4965,	0.6587 s / batch. (data: 1.05e-02). ETA=18:55:49, max mem: 15.9 GB 
[10/28 04:45:18 visual_prompt]: 	Training 600/1106. train loss: 14.9732,	0.6214 s / batch. (data: 4.59e-04). ETA=17:50:35, max mem: 15.9 GB 
[10/28 04:46:22 visual_prompt]: 	Training 700/1106. train loss: 9.3819,	0.6295 s / batch. (data: 3.16e-04). ETA=18:03:25, max mem: 15.9 GB 
[10/28 04:47:25 visual_prompt]: 	Training 800/1106. train loss: 0.6614,	0.6329 s / batch. (data: 7.98e-04). ETA=18:08:17, max mem: 15.9 GB 
[10/28 04:48:28 visual_prompt]: 	Training 900/1106. train loss: 0.8676,	0.6190 s / batch. (data: 2.99e-04). ETA=17:43:21, max mem: 15.9 GB 
[10/28 04:49:31 visual_prompt]: 	Training 1000/1106. train loss: 3.0852,	0.6481 s / batch. (data: 8.05e-04). ETA=18:32:10, max mem: 15.9 GB 
[10/28 04:50:34 visual_prompt]: 	Training 1100/1106. train loss: 0.0004,	0.6175 s / batch. (data: 1.41e-04). ETA=17:38:34, max mem: 15.9 GB 
[10/28 04:50:38 visual_prompt]: Epoch 7 / 100: avg data time: 3.99e-03, avg batch time: 0.6340, average train loss: 3.1500
[10/28 04:51:28 visual_prompt]: 	Test 100/123. loss: 2.940, 0.2397 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/28 04:51:39 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2320, average loss: 2.6618
[10/28 04:51:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.28	
[10/28 04:51:39 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/28 04:52:44 visual_prompt]: 	Training 100/1106. train loss: 10.1206,	0.6400 s / batch. (data: 8.69e-04). ETA=18:16:03, max mem: 15.9 GB 
[10/28 04:53:47 visual_prompt]: 	Training 200/1106. train loss: 3.7037,	0.6339 s / batch. (data: 7.51e-04). ETA=18:04:32, max mem: 15.9 GB 
[10/28 04:54:50 visual_prompt]: 	Training 300/1106. train loss: 1.9340,	0.6286 s / batch. (data: 9.53e-04). ETA=17:54:30, max mem: 15.9 GB 
[10/28 04:55:54 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6189 s / batch. (data: 2.42e-04). ETA=17:36:50, max mem: 15.9 GB 
[10/28 04:56:57 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6226 s / batch. (data: 3.19e-04). ETA=17:42:04, max mem: 15.9 GB 
[10/28 04:58:00 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6320 s / batch. (data: 7.93e-04). ETA=17:57:07, max mem: 15.9 GB 
[10/28 04:59:03 visual_prompt]: 	Training 700/1106. train loss: 3.3215,	0.6313 s / batch. (data: 8.12e-04). ETA=17:54:49, max mem: 15.9 GB 
[10/28 05:00:07 visual_prompt]: 	Training 800/1106. train loss: 4.8016,	0.6198 s / batch. (data: 3.23e-04). ETA=17:34:18, max mem: 15.9 GB 
[10/28 05:01:10 visual_prompt]: 	Training 900/1106. train loss: 0.0006,	0.6305 s / batch. (data: 2.61e-04). ETA=17:51:20, max mem: 15.9 GB 
[10/28 05:02:13 visual_prompt]: 	Training 1000/1106. train loss: 0.5670,	0.6192 s / batch. (data: 3.54e-04). ETA=17:31:13, max mem: 15.9 GB 
[10/28 05:03:16 visual_prompt]: 	Training 1100/1106. train loss: 1.9039,	0.6190 s / batch. (data: 1.55e-04). ETA=17:29:44, max mem: 15.9 GB 
[10/28 05:03:20 visual_prompt]: Epoch 8 / 100: avg data time: 4.09e-03, avg batch time: 0.6343, average train loss: 2.7940
[10/28 05:04:10 visual_prompt]: 	Test 100/123. loss: 1.463, 0.2317 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[10/28 05:04:21 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2328, average loss: 1.2969
[10/28 05:04:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.10	
[10/28 05:04:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/28 05:05:26 visual_prompt]: 	Training 100/1106. train loss: 16.0928,	0.6342 s / batch. (data: 8.11e-04). ETA=17:54:25, max mem: 15.9 GB 
[10/28 05:06:29 visual_prompt]: 	Training 200/1106. train loss: 0.0013,	0.6403 s / batch. (data: 7.96e-04). ETA=18:03:39, max mem: 15.9 GB 
[10/28 05:07:33 visual_prompt]: 	Training 300/1106. train loss: 6.4155,	0.6190 s / batch. (data: 3.34e-04). ETA=17:26:39, max mem: 15.9 GB 
[10/28 05:08:36 visual_prompt]: 	Training 400/1106. train loss: 0.1404,	0.6373 s / batch. (data: 7.82e-04). ETA=17:56:33, max mem: 15.9 GB 
[10/28 05:09:39 visual_prompt]: 	Training 500/1106. train loss: 17.5576,	0.6360 s / batch. (data: 3.11e-04). ETA=17:53:15, max mem: 15.9 GB 
[10/28 05:10:42 visual_prompt]: 	Training 600/1106. train loss: 1.1880,	0.6181 s / batch. (data: 3.29e-04). ETA=17:22:03, max mem: 15.9 GB 
[10/28 05:11:45 visual_prompt]: 	Training 700/1106. train loss: 0.2236,	0.6188 s / batch. (data: 3.23e-04). ETA=17:22:15, max mem: 15.9 GB 
[10/28 05:12:49 visual_prompt]: 	Training 800/1106. train loss: 2.2863,	0.6309 s / batch. (data: 8.50e-04). ETA=17:41:29, max mem: 15.9 GB 
[10/28 05:13:52 visual_prompt]: 	Training 900/1106. train loss: 0.0050,	0.6456 s / batch. (data: 7.61e-04). ETA=18:05:08, max mem: 15.9 GB 
[10/28 05:14:56 visual_prompt]: 	Training 1000/1106. train loss: 0.0420,	0.6356 s / batch. (data: 8.28e-04). ETA=17:47:19, max mem: 15.9 GB 
[10/28 05:15:59 visual_prompt]: 	Training 1100/1106. train loss: 1.3406,	0.6179 s / batch. (data: 1.36e-04). ETA=17:16:35, max mem: 15.9 GB 
[10/28 05:16:03 visual_prompt]: Epoch 9 / 100: avg data time: 4.39e-03, avg batch time: 0.6345, average train loss: 2.5142
[10/28 05:16:52 visual_prompt]: 	Test 100/123. loss: 1.386, 0.2262 s / batch. (data: 5.17e-05)max mem: 15.94594 GB 
[10/28 05:17:03 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.2336, average loss: 1.2648
[10/28 05:17:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.25	
[10/28 05:17:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/28 05:18:08 visual_prompt]: 	Training 100/1106. train loss: 4.6406,	0.6264 s / batch. (data: 3.47e-04). ETA=17:29:46, max mem: 15.9 GB 
[10/28 05:19:11 visual_prompt]: 	Training 200/1106. train loss: 2.8542,	0.6308 s / batch. (data: 3.19e-04). ETA=17:36:03, max mem: 15.9 GB 
[10/28 05:20:15 visual_prompt]: 	Training 300/1106. train loss: 7.5339,	0.6295 s / batch. (data: 1.05e-02). ETA=17:32:45, max mem: 15.9 GB 
[10/28 05:21:18 visual_prompt]: 	Training 400/1106. train loss: 0.0012,	0.6309 s / batch. (data: 8.00e-04). ETA=17:34:05, max mem: 15.9 GB 
[10/28 05:22:21 visual_prompt]: 	Training 500/1106. train loss: 6.7514,	0.6494 s / batch. (data: 1.34e-02). ETA=18:03:50, max mem: 15.9 GB 
[10/28 05:23:24 visual_prompt]: 	Training 600/1106. train loss: 8.8211,	0.6491 s / batch. (data: 7.82e-04). ETA=18:02:24, max mem: 15.9 GB 
[10/28 05:24:28 visual_prompt]: 	Training 700/1106. train loss: 7.2976,	0.6392 s / batch. (data: 8.45e-04). ETA=17:44:48, max mem: 15.9 GB 
[10/28 05:25:31 visual_prompt]: 	Training 800/1106. train loss: 0.8595,	0.6461 s / batch. (data: 2.61e-02). ETA=17:55:13, max mem: 15.9 GB 
[10/28 05:26:34 visual_prompt]: 	Training 900/1106. train loss: 0.8247,	0.6560 s / batch. (data: 7.96e-04). ETA=18:10:34, max mem: 15.9 GB 
[10/28 05:27:37 visual_prompt]: 	Training 1000/1106. train loss: 1.4819,	0.6491 s / batch. (data: 5.46e-03). ETA=17:58:00, max mem: 15.9 GB 
[10/28 05:28:41 visual_prompt]: 	Training 1100/1106. train loss: 0.0136,	0.6181 s / batch. (data: 1.63e-04). ETA=17:05:30, max mem: 15.9 GB 
[10/28 05:28:44 visual_prompt]: Epoch 10 / 100: avg data time: 4.23e-03, avg batch time: 0.6342, average train loss: 3.7720
[10/28 05:29:34 visual_prompt]: 	Test 100/123. loss: 2.366, 0.2404 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[10/28 05:29:45 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.2331, average loss: 2.5366
[10/28 05:29:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.73	
[10/28 05:29:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/28 05:30:50 visual_prompt]: 	Training 100/1106. train loss: 24.4842,	0.6405 s / batch. (data: 2.06e-02). ETA=17:41:34, max mem: 15.9 GB 
[10/28 05:31:54 visual_prompt]: 	Training 200/1106. train loss: 6.7954,	0.6218 s / batch. (data: 2.73e-04). ETA=17:09:27, max mem: 15.9 GB 
[10/28 05:32:57 visual_prompt]: 	Training 300/1106. train loss: 4.1916,	0.6309 s / batch. (data: 8.50e-04). ETA=17:23:32, max mem: 15.9 GB 
[10/28 05:34:00 visual_prompt]: 	Training 400/1106. train loss: 0.0408,	0.6309 s / batch. (data: 7.89e-04). ETA=17:22:26, max mem: 15.9 GB 
[10/28 05:35:03 visual_prompt]: 	Training 500/1106. train loss: 1.4345,	0.6335 s / batch. (data: 7.52e-04). ETA=17:25:38, max mem: 15.9 GB 
[10/28 05:36:07 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6222 s / batch. (data: 3.41e-04). ETA=17:05:55, max mem: 15.9 GB 
[10/28 05:37:10 visual_prompt]: 	Training 700/1106. train loss: 8.8820,	0.6511 s / batch. (data: 7.88e-04). ETA=17:52:37, max mem: 15.9 GB 
[10/28 05:38:13 visual_prompt]: 	Training 800/1106. train loss: 5.8681,	0.6196 s / batch. (data: 2.64e-04). ETA=16:59:43, max mem: 15.9 GB 
[10/28 05:39:16 visual_prompt]: 	Training 900/1106. train loss: 2.4664,	0.6430 s / batch. (data: 8.18e-04). ETA=17:37:01, max mem: 15.9 GB 
[10/28 05:40:19 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6356 s / batch. (data: 8.46e-04). ETA=17:23:55, max mem: 15.9 GB 
[10/28 05:41:23 visual_prompt]: 	Training 1100/1106. train loss: 0.6891,	0.6180 s / batch. (data: 2.04e-04). ETA=16:53:54, max mem: 15.9 GB 
[10/28 05:41:26 visual_prompt]: Epoch 11 / 100: avg data time: 4.31e-03, avg batch time: 0.6343, average train loss: 3.9806
[10/28 05:42:16 visual_prompt]: 	Test 100/123. loss: 2.557, 0.2269 s / batch. (data: 3.34e-05)max mem: 15.94594 GB 
[10/28 05:42:27 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2321, average loss: 2.3036
[10/28 05:42:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.42	
[10/28 05:42:27 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/28 05:43:33 visual_prompt]: 	Training 100/1106. train loss: 1.0320,	0.6274 s / batch. (data: 3.41e-04). ETA=17:08:11, max mem: 15.9 GB 
[10/28 05:44:36 visual_prompt]: 	Training 200/1106. train loss: 0.8362,	0.6312 s / batch. (data: 8.14e-04). ETA=17:13:25, max mem: 15.9 GB 
[10/28 05:45:39 visual_prompt]: 	Training 300/1106. train loss: 13.0560,	0.6440 s / batch. (data: 7.67e-04). ETA=17:33:15, max mem: 15.9 GB 
[10/28 05:46:43 visual_prompt]: 	Training 400/1106. train loss: 0.1986,	0.6276 s / batch. (data: 2.83e-04). ETA=17:05:24, max mem: 15.9 GB 
[10/28 05:47:46 visual_prompt]: 	Training 500/1106. train loss: 0.0090,	0.6321 s / batch. (data: 1.26e-02). ETA=17:11:44, max mem: 15.9 GB 
[10/28 05:48:49 visual_prompt]: 	Training 600/1106. train loss: 0.0006,	0.6442 s / batch. (data: 2.62e-02). ETA=17:30:28, max mem: 15.9 GB 
[10/28 05:49:53 visual_prompt]: 	Training 700/1106. train loss: 1.1042,	0.6184 s / batch. (data: 2.93e-04). ETA=16:47:17, max mem: 15.9 GB 
[10/28 05:50:56 visual_prompt]: 	Training 800/1106. train loss: 5.6227,	0.6183 s / batch. (data: 2.75e-04). ETA=16:46:11, max mem: 15.9 GB 
[10/28 05:52:00 visual_prompt]: 	Training 900/1106. train loss: 0.3677,	0.6528 s / batch. (data: 1.36e-02). ETA=17:41:06, max mem: 15.9 GB 
[10/28 05:53:03 visual_prompt]: 	Training 1000/1106. train loss: 4.3290,	0.6336 s / batch. (data: 7.80e-04). ETA=17:08:50, max mem: 15.9 GB 
[10/28 05:54:06 visual_prompt]: 	Training 1100/1106. train loss: 0.7677,	0.6188 s / batch. (data: 1.41e-04). ETA=16:43:50, max mem: 15.9 GB 
[10/28 05:54:10 visual_prompt]: Epoch 12 / 100: avg data time: 4.87e-03, avg batch time: 0.6352, average train loss: 4.0159
[10/28 05:55:00 visual_prompt]: 	Test 100/123. loss: 0.696, 0.2315 s / batch. (data: 5.48e-05)max mem: 15.94594 GB 
[10/28 05:55:10 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.2331, average loss: 0.7129
[10/28 05:55:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.44	
[10/28 05:55:10 visual_prompt]: Best epoch 12: best metric: -0.713
[10/28 05:55:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/28 05:56:15 visual_prompt]: 	Training 100/1106. train loss: 0.1345,	0.6178 s / batch. (data: 3.23e-04). ETA=16:41:09, max mem: 15.9 GB 
[10/28 05:57:19 visual_prompt]: 	Training 200/1106. train loss: 4.5400,	0.6437 s / batch. (data: 8.67e-04). ETA=17:22:02, max mem: 15.9 GB 
[10/28 05:58:22 visual_prompt]: 	Training 300/1106. train loss: 0.0011,	0.6171 s / batch. (data: 3.13e-04). ETA=16:37:51, max mem: 15.9 GB 
[10/28 05:59:25 visual_prompt]: 	Training 400/1106. train loss: 0.2608,	0.6197 s / batch. (data: 3.14e-04). ETA=16:41:04, max mem: 15.9 GB 
[10/28 06:00:29 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6199 s / batch. (data: 3.32e-04). ETA=16:40:25, max mem: 15.9 GB 
[10/28 06:01:32 visual_prompt]: 	Training 600/1106. train loss: 0.9907,	0.6204 s / batch. (data: 4.99e-04). ETA=16:40:13, max mem: 15.9 GB 
[10/28 06:02:35 visual_prompt]: 	Training 700/1106. train loss: 0.0016,	0.6321 s / batch. (data: 1.30e-02). ETA=16:58:01, max mem: 15.9 GB 
[10/28 06:03:39 visual_prompt]: 	Training 800/1106. train loss: 0.7380,	0.6330 s / batch. (data: 3.05e-04). ETA=16:58:26, max mem: 15.9 GB 
[10/28 06:04:42 visual_prompt]: 	Training 900/1106. train loss: 0.4366,	0.6343 s / batch. (data: 1.18e-03). ETA=16:59:24, max mem: 15.9 GB 
[10/28 06:05:45 visual_prompt]: 	Training 1000/1106. train loss: 0.3302,	0.6201 s / batch. (data: 3.26e-04). ETA=16:35:32, max mem: 15.9 GB 
[10/28 06:06:49 visual_prompt]: 	Training 1100/1106. train loss: 7.3445,	0.6187 s / batch. (data: 1.91e-04). ETA=16:32:13, max mem: 15.9 GB 
[10/28 06:06:53 visual_prompt]: Epoch 13 / 100: avg data time: 4.46e-03, avg batch time: 0.6354, average train loss: 3.7180
[10/28 06:07:43 visual_prompt]: 	Test 100/123. loss: 0.755, 0.2408 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/28 06:07:54 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2334, average loss: 0.7169
[10/28 06:07:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.12	
[10/28 06:07:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/28 06:09:00 visual_prompt]: 	Training 100/1106. train loss: 3.3547,	0.6494 s / batch. (data: 8.00e-04). ETA=17:20:21, max mem: 15.9 GB 
[10/28 06:10:03 visual_prompt]: 	Training 200/1106. train loss: 1.6390,	0.6323 s / batch. (data: 2.95e-04). ETA=16:51:56, max mem: 15.9 GB 
[10/28 06:11:06 visual_prompt]: 	Training 300/1106. train loss: 4.4103,	0.6394 s / batch. (data: 8.23e-04). ETA=17:02:10, max mem: 15.9 GB 
[10/28 06:12:10 visual_prompt]: 	Training 400/1106. train loss: 0.0610,	0.6278 s / batch. (data: 3.20e-04). ETA=16:42:39, max mem: 15.9 GB 
[10/28 06:13:13 visual_prompt]: 	Training 500/1106. train loss: 6.6511,	0.6341 s / batch. (data: 8.79e-04). ETA=16:51:36, max mem: 15.9 GB 
[10/28 06:14:16 visual_prompt]: 	Training 600/1106. train loss: 2.8520,	0.6296 s / batch. (data: 3.29e-04). ETA=16:43:20, max mem: 15.9 GB 
[10/28 06:15:20 visual_prompt]: 	Training 700/1106. train loss: 0.0005,	0.6446 s / batch. (data: 8.15e-04). ETA=17:06:16, max mem: 15.9 GB 
[10/28 06:16:23 visual_prompt]: 	Training 800/1106. train loss: 2.1774,	0.6381 s / batch. (data: 7.67e-04). ETA=16:54:46, max mem: 15.9 GB 
[10/28 06:17:27 visual_prompt]: 	Training 900/1106. train loss: 1.6568,	0.6568 s / batch. (data: 7.88e-04). ETA=17:23:30, max mem: 15.9 GB 
[10/28 06:18:30 visual_prompt]: 	Training 1000/1106. train loss: 14.1465,	0.6191 s / batch. (data: 2.96e-04). ETA=16:22:33, max mem: 15.9 GB 
[10/28 06:19:33 visual_prompt]: 	Training 1100/1106. train loss: 3.5770,	0.6186 s / batch. (data: 1.27e-04). ETA=16:20:46, max mem: 15.9 GB 
[10/28 06:19:37 visual_prompt]: Epoch 14 / 100: avg data time: 4.78e-03, avg batch time: 0.6359, average train loss: 4.1086
[10/28 06:20:27 visual_prompt]: 	Test 100/123. loss: 1.375, 0.2357 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/28 06:20:38 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.2327, average loss: 1.5029
[10/28 06:20:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.94	
[10/28 06:20:38 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/28 06:21:42 visual_prompt]: 	Training 100/1106. train loss: 12.5022,	0.6174 s / batch. (data: 3.39e-04). ETA=16:17:45, max mem: 15.9 GB 
[10/28 06:22:46 visual_prompt]: 	Training 200/1106. train loss: 1.6046,	0.6509 s / batch. (data: 7.96e-04). ETA=17:09:41, max mem: 15.9 GB 
[10/28 06:23:49 visual_prompt]: 	Training 300/1106. train loss: 1.7417,	0.6326 s / batch. (data: 1.05e-03). ETA=16:39:45, max mem: 15.9 GB 
[10/28 06:24:52 visual_prompt]: 	Training 400/1106. train loss: 2.8059,	0.6458 s / batch. (data: 8.19e-04). ETA=16:59:23, max mem: 15.9 GB 
[10/28 06:25:55 visual_prompt]: 	Training 500/1106. train loss: 0.0056,	0.6231 s / batch. (data: 7.84e-04). ETA=16:22:35, max mem: 15.9 GB 
[10/28 06:26:59 visual_prompt]: 	Training 600/1106. train loss: 6.5174,	0.6403 s / batch. (data: 8.84e-04). ETA=16:48:41, max mem: 15.9 GB 
[10/28 06:28:02 visual_prompt]: 	Training 700/1106. train loss: 21.9897,	0.6335 s / batch. (data: 7.69e-04). ETA=16:36:48, max mem: 15.9 GB 
[10/28 06:29:05 visual_prompt]: 	Training 800/1106. train loss: 1.9085,	0.6214 s / batch. (data: 2.96e-04). ETA=16:16:51, max mem: 15.9 GB 
[10/28 06:30:09 visual_prompt]: 	Training 900/1106. train loss: 11.8419,	0.6385 s / batch. (data: 7.96e-04). ETA=16:42:41, max mem: 15.9 GB 
[10/28 06:31:12 visual_prompt]: 	Training 1000/1106. train loss: 3.0945,	0.6201 s / batch. (data: 4.43e-04). ETA=16:12:44, max mem: 15.9 GB 
[10/28 06:32:15 visual_prompt]: 	Training 1100/1106. train loss: 0.7447,	0.6195 s / batch. (data: 1.57e-04). ETA=16:10:45, max mem: 15.9 GB 
[10/28 06:32:19 visual_prompt]: Epoch 15 / 100: avg data time: 3.92e-03, avg batch time: 0.6341, average train loss: 3.8083
[10/28 06:33:09 visual_prompt]: 	Test 100/123. loss: 0.726, 0.2257 s / batch. (data: 3.34e-05)max mem: 15.94594 GB 
[10/28 06:33:20 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2327, average loss: 0.7576
[10/28 06:33:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.46	
[10/28 06:33:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/28 06:34:24 visual_prompt]: 	Training 100/1106. train loss: 0.0001,	0.6302 s / batch. (data: 3.37e-04). ETA=16:26:25, max mem: 15.9 GB 
[10/28 06:35:28 visual_prompt]: 	Training 200/1106. train loss: 5.8321,	0.6403 s / batch. (data: 8.14e-04). ETA=16:41:10, max mem: 15.9 GB 
[10/28 06:36:31 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6197 s / batch. (data: 3.30e-04). ETA=16:07:54, max mem: 15.9 GB 
[10/28 06:37:34 visual_prompt]: 	Training 400/1106. train loss: 10.2375,	0.6324 s / batch. (data: 3.19e-04). ETA=16:26:39, max mem: 15.9 GB 
[10/28 06:38:38 visual_prompt]: 	Training 500/1106. train loss: 4.4975,	0.6449 s / batch. (data: 3.22e-04). ETA=16:45:03, max mem: 15.9 GB 
[10/28 06:39:41 visual_prompt]: 	Training 600/1106. train loss: 0.2214,	0.6394 s / batch. (data: 8.09e-04). ETA=16:35:28, max mem: 15.9 GB 
[10/28 06:40:44 visual_prompt]: 	Training 700/1106. train loss: 5.5295,	0.6207 s / batch. (data: 5.37e-04). ETA=16:05:15, max mem: 15.9 GB 
[10/28 06:41:48 visual_prompt]: 	Training 800/1106. train loss: 8.3002,	0.6468 s / batch. (data: 7.88e-04). ETA=16:44:50, max mem: 15.9 GB 
[10/28 06:42:51 visual_prompt]: 	Training 900/1106. train loss: 1.8243,	0.6191 s / batch. (data: 4.36e-04). ETA=16:00:47, max mem: 15.9 GB 
[10/28 06:43:54 visual_prompt]: 	Training 1000/1106. train loss: 6.7569,	0.6510 s / batch. (data: 7.89e-04). ETA=16:49:06, max mem: 15.9 GB 
[10/28 06:44:58 visual_prompt]: 	Training 1100/1106. train loss: 8.2046,	0.6194 s / batch. (data: 1.67e-04). ETA=15:59:07, max mem: 15.9 GB 
[10/28 06:45:01 visual_prompt]: Epoch 16 / 100: avg data time: 3.50e-03, avg batch time: 0.6345, average train loss: 4.4473
[10/28 06:45:51 visual_prompt]: 	Test 100/123. loss: 0.862, 0.2260 s / batch. (data: 3.86e-05)max mem: 15.94594 GB 
[10/28 06:46:02 visual_prompt]: Inference (val):avg data time: 9.68e-05, avg batch time: 0.2327, average loss: 1.3236
[10/28 06:46:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.85	
[10/28 06:46:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/28 06:47:07 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6286 s / batch. (data: 1.10e-02). ETA=16:12:21, max mem: 15.9 GB 
[10/28 06:48:11 visual_prompt]: 	Training 200/1106. train loss: 2.1298,	0.6199 s / batch. (data: 3.35e-04). ETA=15:57:46, max mem: 15.9 GB 
[10/28 06:49:14 visual_prompt]: 	Training 300/1106. train loss: 8.5154,	0.6564 s / batch. (data: 7.98e-04). ETA=16:53:08, max mem: 15.9 GB 
[10/28 06:50:17 visual_prompt]: 	Training 400/1106. train loss: 0.0001,	0.6562 s / batch. (data: 8.29e-04). ETA=16:51:43, max mem: 15.9 GB 
[10/28 06:51:21 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6335 s / batch. (data: 8.18e-04). ETA=16:15:41, max mem: 15.9 GB 
[10/28 06:52:24 visual_prompt]: 	Training 600/1106. train loss: 2.8843,	0.6393 s / batch. (data: 5.90e-03). ETA=16:23:27, max mem: 15.9 GB 
[10/28 06:53:27 visual_prompt]: 	Training 700/1106. train loss: 2.2931,	0.6485 s / batch. (data: 7.72e-04). ETA=16:36:37, max mem: 15.9 GB 
[10/28 06:54:31 visual_prompt]: 	Training 800/1106. train loss: 6.9137,	0.6176 s / batch. (data: 3.19e-04). ETA=15:48:04, max mem: 15.9 GB 
[10/28 06:55:34 visual_prompt]: 	Training 900/1106. train loss: 0.0384,	0.6288 s / batch. (data: 7.75e-04). ETA=16:04:08, max mem: 15.9 GB 
[10/28 06:56:37 visual_prompt]: 	Training 1000/1106. train loss: 2.5741,	0.6300 s / batch. (data: 7.91e-04). ETA=16:04:56, max mem: 15.9 GB 
[10/28 06:57:41 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6185 s / batch. (data: 1.46e-04). ETA=15:46:23, max mem: 15.9 GB 
[10/28 06:57:45 visual_prompt]: Epoch 17 / 100: avg data time: 4.40e-03, avg batch time: 0.6353, average train loss: 5.0568
[10/28 06:58:34 visual_prompt]: 	Test 100/123. loss: 5.772, 0.2406 s / batch. (data: 4.34e-05)max mem: 15.94594 GB 
[10/28 06:58:45 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.2331, average loss: 6.1916
[10/28 06:58:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.76	
[10/28 06:58:45 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/28 06:59:50 visual_prompt]: 	Training 100/1106. train loss: 10.5779,	0.6174 s / batch. (data: 3.91e-04). ETA=15:43:34, max mem: 15.9 GB 
[10/28 07:00:54 visual_prompt]: 	Training 200/1106. train loss: 0.0000,	0.6341 s / batch. (data: 8.15e-04). ETA=16:08:02, max mem: 15.9 GB 
[10/28 07:01:57 visual_prompt]: 	Training 300/1106. train loss: 27.9178,	0.6350 s / batch. (data: 2.95e-04). ETA=16:08:17, max mem: 15.9 GB 
[10/28 07:03:00 visual_prompt]: 	Training 400/1106. train loss: 12.7155,	0.6458 s / batch. (data: 7.72e-04). ETA=16:23:46, max mem: 15.9 GB 
[10/28 07:04:04 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6335 s / batch. (data: 7.81e-04). ETA=16:04:01, max mem: 15.9 GB 
[10/28 07:05:07 visual_prompt]: 	Training 600/1106. train loss: 2.5419,	0.6249 s / batch. (data: 3.43e-04). ETA=15:49:47, max mem: 15.9 GB 
[10/28 07:06:10 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6337 s / batch. (data: 7.70e-04). ETA=16:02:11, max mem: 15.9 GB 
[10/28 07:07:13 visual_prompt]: 	Training 800/1106. train loss: 0.6960,	0.6368 s / batch. (data: 7.77e-04). ETA=16:05:44, max mem: 15.9 GB 
[10/28 07:08:17 visual_prompt]: 	Training 900/1106. train loss: 8.8197,	0.6300 s / batch. (data: 3.33e-04). ETA=15:54:27, max mem: 15.9 GB 
[10/28 07:09:20 visual_prompt]: 	Training 1000/1106. train loss: 4.4815,	0.6200 s / batch. (data: 3.35e-04). ETA=15:38:14, max mem: 15.9 GB 
[10/28 07:10:23 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6179 s / batch. (data: 1.54e-04). ETA=15:33:58, max mem: 15.9 GB 
[10/28 07:10:27 visual_prompt]: Epoch 18 / 100: avg data time: 4.19e-03, avg batch time: 0.6347, average train loss: 4.5701
[10/28 07:11:17 visual_prompt]: 	Test 100/123. loss: 0.797, 0.2401 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[10/28 07:11:28 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2327, average loss: 0.7449
[10/28 07:11:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.77	
[10/28 07:11:28 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/28 07:12:33 visual_prompt]: 	Training 100/1106. train loss: 0.0377,	0.6306 s / batch. (data: 7.60e-04). ETA=15:52:05, max mem: 15.9 GB 
[10/28 07:13:36 visual_prompt]: 	Training 200/1106. train loss: 2.9359,	0.6298 s / batch. (data: 3.14e-04). ETA=15:49:49, max mem: 15.9 GB 
[10/28 07:14:40 visual_prompt]: 	Training 300/1106. train loss: 0.4812,	0.6293 s / batch. (data: 3.53e-04). ETA=15:48:04, max mem: 15.9 GB 
[10/28 07:15:43 visual_prompt]: 	Training 400/1106. train loss: 0.9322,	0.6197 s / batch. (data: 3.13e-04). ETA=15:32:36, max mem: 15.9 GB 
[10/28 07:16:46 visual_prompt]: 	Training 500/1106. train loss: 4.5951,	0.6280 s / batch. (data: 7.88e-04). ETA=15:43:57, max mem: 15.9 GB 
[10/28 07:17:49 visual_prompt]: 	Training 600/1106. train loss: 0.1048,	0.6182 s / batch. (data: 3.04e-04). ETA=15:28:13, max mem: 15.9 GB 
[10/28 07:18:52 visual_prompt]: 	Training 700/1106. train loss: 0.7488,	0.6250 s / batch. (data: 3.24e-04). ETA=15:37:25, max mem: 15.9 GB 
[10/28 07:19:56 visual_prompt]: 	Training 800/1106. train loss: 3.3965,	0.6306 s / batch. (data: 7.95e-04). ETA=15:44:45, max mem: 15.9 GB 
[10/28 07:20:59 visual_prompt]: 	Training 900/1106. train loss: 0.0015,	0.6184 s / batch. (data: 3.50e-04). ETA=15:25:23, max mem: 15.9 GB 
[10/28 07:22:02 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6396 s / batch. (data: 7.68e-04). ETA=15:56:04, max mem: 15.9 GB 
[10/28 07:23:05 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6188 s / batch. (data: 2.11e-04). ETA=15:23:59, max mem: 15.9 GB 
[10/28 07:23:09 visual_prompt]: Epoch 19 / 100: avg data time: 4.29e-03, avg batch time: 0.6344, average train loss: 3.9692
[10/28 07:23:59 visual_prompt]: 	Test 100/123. loss: 19.473, 0.2317 s / batch. (data: 5.17e-05)max mem: 15.94594 GB 
[10/28 07:24:10 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2319, average loss: 17.5809
[10/28 07:24:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.45	
[10/28 07:24:10 visual_prompt]: Stopping early.
[10/28 07:24:10 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 07:24:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 07:24:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 07:24:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 07:24:10 visual_prompt]: Training with config:
[10/28 07:24:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr1.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 07:24:10 visual_prompt]: Loading training data...
[10/28 07:24:10 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 07:24:10 visual_prompt]: Loading validation data...
[10/28 07:24:10 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 07:24:10 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/28 07:24:12 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/28 07:24:12 visual_prompt]: tuned percent:0.522
[10/28 07:24:12 visual_prompt]: Device used for model: 0
[10/28 07:24:12 visual_prompt]: Setting up Evaluator...
[10/28 07:24:12 visual_prompt]: Setting up Trainer...
[10/28 07:24:12 visual_prompt]: 	Setting up the optimizer...
[10/28 07:24:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 07:25:18 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6291 s / batch. (data: 3.27e-04). ETA=19:18:37, max mem: 15.9 GB 
[10/28 07:26:22 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6337 s / batch. (data: 1.37e-02). ETA=19:25:57, max mem: 15.9 GB 
[10/28 07:27:25 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6335 s / batch. (data: 3.23e-04). ETA=19:24:35, max mem: 15.9 GB 
[10/28 07:28:28 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6314 s / batch. (data: 8.22e-04). ETA=19:19:44, max mem: 15.9 GB 
[10/28 07:29:31 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6440 s / batch. (data: 3.34e-04). ETA=19:41:45, max mem: 15.9 GB 
[10/28 07:30:34 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6326 s / batch. (data: 8.05e-04). ETA=19:19:46, max mem: 15.9 GB 
[10/28 07:31:38 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6426 s / batch. (data: 7.61e-04). ETA=19:36:57, max mem: 15.9 GB 
[10/28 07:32:41 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6193 s / batch. (data: 3.24e-04). ETA=18:53:17, max mem: 15.9 GB 
[10/28 07:33:45 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6313 s / batch. (data: 5.45e-03). ETA=19:14:08, max mem: 15.9 GB 
[10/28 07:34:48 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6332 s / batch. (data: 8.01e-04). ETA=19:16:35, max mem: 15.9 GB 
[10/28 07:35:51 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6185 s / batch. (data: 1.38e-04). ETA=18:48:44, max mem: 15.9 GB 
[10/28 07:35:55 visual_prompt]: Epoch 1 / 100: avg data time: 4.36e-03, avg batch time: 0.6351, average train loss: 1.4028
[10/28 07:36:45 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2526 s / batch. (data: 4.24e-05)max mem: 15.94594 GB 
[10/28 07:36:55 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2335, average loss: 1.3505
[10/28 07:36:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/28 07:36:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/28 07:38:00 visual_prompt]: 	Training 100/1106. train loss: 2.8305,	0.6358 s / batch. (data: 7.42e-04). ETA=19:19:12, max mem: 15.9 GB 
[10/28 07:39:04 visual_prompt]: 	Training 200/1106. train loss: 0.9954,	0.6395 s / batch. (data: 8.43e-04). ETA=19:24:54, max mem: 15.9 GB 
[10/28 07:40:07 visual_prompt]: 	Training 300/1106. train loss: 1.4596,	0.6305 s / batch. (data: 8.11e-04). ETA=19:07:31, max mem: 15.9 GB 
[10/28 07:41:10 visual_prompt]: 	Training 400/1106. train loss: 0.0083,	0.6179 s / batch. (data: 3.14e-04). ETA=18:43:27, max mem: 15.9 GB 
[10/28 07:42:14 visual_prompt]: 	Training 500/1106. train loss: 2.5632,	0.6487 s / batch. (data: 1.10e-02). ETA=19:38:27, max mem: 15.9 GB 
[10/28 07:43:17 visual_prompt]: 	Training 600/1106. train loss: 0.6301,	0.6351 s / batch. (data: 7.55e-04). ETA=19:12:36, max mem: 15.9 GB 
[10/28 07:44:20 visual_prompt]: 	Training 700/1106. train loss: 0.2619,	0.6486 s / batch. (data: 5.20e-04). ETA=19:36:04, max mem: 15.9 GB 
[10/28 07:45:23 visual_prompt]: 	Training 800/1106. train loss: 1.0640,	0.6316 s / batch. (data: 5.54e-04). ETA=19:04:06, max mem: 15.9 GB 
[10/28 07:46:26 visual_prompt]: 	Training 900/1106. train loss: 1.3448,	0.6305 s / batch. (data: 7.64e-04). ETA=19:01:12, max mem: 15.9 GB 
[10/28 07:47:30 visual_prompt]: 	Training 1000/1106. train loss: 0.0474,	0.6289 s / batch. (data: 7.98e-04). ETA=18:57:17, max mem: 15.9 GB 
[10/28 07:48:33 visual_prompt]: 	Training 1100/1106. train loss: 0.4512,	0.6188 s / batch. (data: 1.71e-04). ETA=18:37:56, max mem: 15.9 GB 
[10/28 07:48:37 visual_prompt]: Epoch 2 / 100: avg data time: 3.93e-03, avg batch time: 0.6343, average train loss: 1.1216
[10/28 07:49:27 visual_prompt]: 	Test 100/123. loss: 1.275, 0.2477 s / batch. (data: 2.81e-05)max mem: 15.94594 GB 
[10/28 07:49:37 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2341, average loss: 1.1778
[10/28 07:49:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.55	
[10/28 07:49:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/28 07:50:44 visual_prompt]: 	Training 100/1106. train loss: 0.5760,	0.6449 s / batch. (data: 8.16e-04). ETA=19:23:54, max mem: 15.9 GB 
[10/28 07:51:47 visual_prompt]: 	Training 200/1106. train loss: 0.1900,	0.6345 s / batch. (data: 7.83e-04). ETA=19:04:00, max mem: 15.9 GB 
[10/28 07:52:50 visual_prompt]: 	Training 300/1106. train loss: 0.1081,	0.6474 s / batch. (data: 1.61e-02). ETA=19:26:17, max mem: 15.9 GB 
[10/28 07:53:53 visual_prompt]: 	Training 400/1106. train loss: 0.7090,	0.6360 s / batch. (data: 1.20e-02). ETA=19:04:39, max mem: 15.9 GB 
[10/28 07:54:56 visual_prompt]: 	Training 500/1106. train loss: 1.8649,	0.6174 s / batch. (data: 4.93e-04). ETA=18:30:13, max mem: 15.9 GB 
[10/28 07:56:00 visual_prompt]: 	Training 600/1106. train loss: 0.0420,	0.6354 s / batch. (data: 8.22e-04). ETA=19:01:30, max mem: 15.9 GB 
[10/28 07:57:03 visual_prompt]: 	Training 700/1106. train loss: 0.4989,	0.6317 s / batch. (data: 8.04e-04). ETA=18:53:47, max mem: 15.9 GB 
[10/28 07:58:06 visual_prompt]: 	Training 800/1106. train loss: 1.2780,	0.6383 s / batch. (data: 3.64e-04). ETA=19:04:32, max mem: 15.9 GB 
[10/28 07:59:10 visual_prompt]: 	Training 900/1106. train loss: 1.8417,	0.6195 s / batch. (data: 3.02e-04). ETA=18:29:51, max mem: 15.9 GB 
[10/28 08:00:13 visual_prompt]: 	Training 1000/1106. train loss: 1.5127,	0.6341 s / batch. (data: 7.15e-04). ETA=18:54:50, max mem: 15.9 GB 
[10/28 08:01:16 visual_prompt]: 	Training 1100/1106. train loss: 0.9138,	0.6182 s / batch. (data: 1.48e-04). ETA=18:25:30, max mem: 15.9 GB 
[10/28 08:01:20 visual_prompt]: Epoch 3 / 100: avg data time: 4.87e-03, avg batch time: 0.6349, average train loss: 1.3005
[10/28 08:02:10 visual_prompt]: 	Test 100/123. loss: 2.090, 0.2397 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/28 08:02:20 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2315, average loss: 2.3349
[10/28 08:02:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.60	
[10/28 08:02:20 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/28 08:03:25 visual_prompt]: 	Training 100/1106. train loss: 0.4009,	0.6179 s / batch. (data: 3.33e-04). ETA=18:23:43, max mem: 15.9 GB 
[10/28 08:04:29 visual_prompt]: 	Training 200/1106. train loss: 8.2855,	0.6353 s / batch. (data: 8.54e-04). ETA=18:53:45, max mem: 15.9 GB 
[10/28 08:05:32 visual_prompt]: 	Training 300/1106. train loss: 0.9528,	0.6285 s / batch. (data: 3.85e-04). ETA=18:40:34, max mem: 15.9 GB 
[10/28 08:06:36 visual_prompt]: 	Training 400/1106. train loss: 0.8192,	0.6357 s / batch. (data: 7.81e-04). ETA=18:52:24, max mem: 15.9 GB 
[10/28 08:07:39 visual_prompt]: 	Training 500/1106. train loss: 0.6718,	0.6433 s / batch. (data: 7.78e-04). ETA=19:04:48, max mem: 15.9 GB 
[10/28 08:08:42 visual_prompt]: 	Training 600/1106. train loss: 0.0764,	0.6324 s / batch. (data: 7.38e-04). ETA=18:44:25, max mem: 15.9 GB 
[10/28 08:09:46 visual_prompt]: 	Training 700/1106. train loss: 4.1545,	0.6307 s / batch. (data: 8.40e-04). ETA=18:40:17, max mem: 15.9 GB 
[10/28 08:10:49 visual_prompt]: 	Training 800/1106. train loss: 0.2707,	0.6453 s / batch. (data: 7.87e-04). ETA=19:05:17, max mem: 15.9 GB 
[10/28 08:11:52 visual_prompt]: 	Training 900/1106. train loss: 4.6834,	0.6286 s / batch. (data: 7.33e-04). ETA=18:34:35, max mem: 15.9 GB 
[10/28 08:12:55 visual_prompt]: 	Training 1000/1106. train loss: 3.2852,	0.6282 s / batch. (data: 3.20e-04). ETA=18:32:49, max mem: 15.9 GB 
[10/28 08:13:59 visual_prompt]: 	Training 1100/1106. train loss: 3.0818,	0.6177 s / batch. (data: 1.59e-04). ETA=18:13:10, max mem: 15.9 GB 
[10/28 08:14:02 visual_prompt]: Epoch 4 / 100: avg data time: 4.54e-03, avg batch time: 0.6348, average train loss: 1.5176
[10/28 08:14:52 visual_prompt]: 	Test 100/123. loss: 0.843, 0.2247 s / batch. (data: 2.77e-05)max mem: 15.94594 GB 
[10/28 08:15:03 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2323, average loss: 0.9083
[10/28 08:15:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.53	
[10/28 08:15:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/28 08:16:08 visual_prompt]: 	Training 100/1106. train loss: 0.7290,	0.6453 s / batch. (data: 9.82e-04). ETA=19:00:46, max mem: 15.9 GB 
[10/28 08:17:11 visual_prompt]: 	Training 200/1106. train loss: 0.0043,	0.6450 s / batch. (data: 3.71e-04). ETA=18:59:09, max mem: 15.9 GB 
[10/28 08:18:14 visual_prompt]: 	Training 300/1106. train loss: 0.8534,	0.6315 s / batch. (data: 1.15e-03). ETA=18:34:24, max mem: 15.9 GB 
[10/28 08:19:18 visual_prompt]: 	Training 400/1106. train loss: 2.6166,	0.6188 s / batch. (data: 3.08e-04). ETA=18:10:58, max mem: 15.9 GB 
[10/28 08:20:21 visual_prompt]: 	Training 500/1106. train loss: 0.2329,	0.6482 s / batch. (data: 8.37e-04). ETA=19:01:35, max mem: 15.9 GB 
[10/28 08:21:24 visual_prompt]: 	Training 600/1106. train loss: 4.2940,	0.6196 s / batch. (data: 3.24e-04). ETA=18:10:15, max mem: 15.9 GB 
[10/28 08:22:28 visual_prompt]: 	Training 700/1106. train loss: 0.7809,	0.6333 s / batch. (data: 8.24e-04). ETA=18:33:19, max mem: 15.9 GB 
[10/28 08:23:31 visual_prompt]: 	Training 800/1106. train loss: 0.9370,	0.6200 s / batch. (data: 2.99e-04). ETA=18:08:50, max mem: 15.9 GB 
[10/28 08:24:35 visual_prompt]: 	Training 900/1106. train loss: 1.5659,	0.6260 s / batch. (data: 3.72e-04). ETA=18:18:21, max mem: 15.9 GB 
[10/28 08:25:38 visual_prompt]: 	Training 1000/1106. train loss: 0.5694,	0.6308 s / batch. (data: 3.14e-04). ETA=18:25:44, max mem: 15.9 GB 
[10/28 08:26:41 visual_prompt]: 	Training 1100/1106. train loss: 1.8361,	0.6192 s / batch. (data: 2.01e-04). ETA=18:04:24, max mem: 15.9 GB 
[10/28 08:26:45 visual_prompt]: Epoch 5 / 100: avg data time: 4.05e-03, avg batch time: 0.6351, average train loss: 1.5427
[10/28 08:27:34 visual_prompt]: 	Test 100/123. loss: 1.132, 0.2271 s / batch. (data: 3.89e-05)max mem: 15.94594 GB 
[10/28 08:27:46 visual_prompt]: Inference (val):avg data time: 2.87e-04, avg batch time: 0.2334, average loss: 1.0247
[10/28 08:27:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.71	
[10/28 08:27:46 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/28 08:28:51 visual_prompt]: 	Training 100/1106. train loss: 0.9963,	0.6509 s / batch. (data: 1.20e-03). ETA=18:58:41, max mem: 15.9 GB 
[10/28 08:29:54 visual_prompt]: 	Training 200/1106. train loss: 0.0001,	0.6294 s / batch. (data: 8.22e-04). ETA=18:20:03, max mem: 15.9 GB 
[10/28 08:30:58 visual_prompt]: 	Training 300/1106. train loss: 0.0057,	0.6192 s / batch. (data: 3.56e-04). ETA=18:01:16, max mem: 15.9 GB 
[10/28 08:32:01 visual_prompt]: 	Training 400/1106. train loss: 0.1049,	0.6197 s / batch. (data: 2.97e-04). ETA=18:01:06, max mem: 15.9 GB 
[10/28 08:33:04 visual_prompt]: 	Training 500/1106. train loss: 0.7099,	0.6375 s / batch. (data: 3.44e-04). ETA=18:31:02, max mem: 15.9 GB 
[10/28 08:34:07 visual_prompt]: 	Training 600/1106. train loss: 0.3262,	0.6440 s / batch. (data: 7.93e-04). ETA=18:41:20, max mem: 15.9 GB 
[10/28 08:35:11 visual_prompt]: 	Training 700/1106. train loss: 1.0155,	0.6306 s / batch. (data: 8.15e-04). ETA=18:16:58, max mem: 15.9 GB 
[10/28 08:36:14 visual_prompt]: 	Training 800/1106. train loss: 2.9111,	0.6344 s / batch. (data: 8.03e-04). ETA=18:22:33, max mem: 15.9 GB 
[10/28 08:37:18 visual_prompt]: 	Training 900/1106. train loss: 4.3843,	0.6347 s / batch. (data: 2.93e-04). ETA=18:21:51, max mem: 15.9 GB 
[10/28 08:38:21 visual_prompt]: 	Training 1000/1106. train loss: 0.0363,	0.6300 s / batch. (data: 1.32e-02). ETA=18:12:40, max mem: 15.9 GB 
[10/28 08:39:24 visual_prompt]: 	Training 1100/1106. train loss: 0.0008,	0.6177 s / batch. (data: 1.66e-04). ETA=17:50:27, max mem: 15.9 GB 
[10/28 08:39:28 visual_prompt]: Epoch 6 / 100: avg data time: 4.47e-03, avg batch time: 0.6348, average train loss: 1.6379
[10/28 08:40:18 visual_prompt]: 	Test 100/123. loss: 2.150, 0.2262 s / batch. (data: 3.84e-05)max mem: 15.94594 GB 
[10/28 08:40:29 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2333, average loss: 2.3799
[10/28 08:40:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.33	
[10/28 08:40:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/28 08:41:34 visual_prompt]: 	Training 100/1106. train loss: 2.8588,	0.6318 s / batch. (data: 7.78e-04). ETA=18:13:41, max mem: 15.9 GB 
[10/28 08:42:37 visual_prompt]: 	Training 200/1106. train loss: 4.8312,	0.6339 s / batch. (data: 8.64e-04). ETA=18:16:17, max mem: 15.9 GB 
[10/28 08:43:41 visual_prompt]: 	Training 300/1106. train loss: 0.1968,	0.6470 s / batch. (data: 8.04e-04). ETA=18:37:46, max mem: 15.9 GB 
[10/28 08:44:44 visual_prompt]: 	Training 400/1106. train loss: 2.2745,	0.6518 s / batch. (data: 8.28e-04). ETA=18:45:05, max mem: 15.9 GB 
[10/28 08:45:48 visual_prompt]: 	Training 500/1106. train loss: 2.3212,	0.6395 s / batch. (data: 5.40e-03). ETA=18:22:47, max mem: 15.9 GB 
[10/28 08:46:51 visual_prompt]: 	Training 600/1106. train loss: 4.6734,	0.6336 s / batch. (data: 7.60e-04). ETA=18:11:29, max mem: 15.9 GB 
[10/28 08:47:54 visual_prompt]: 	Training 700/1106. train loss: 5.7645,	0.6447 s / batch. (data: 1.37e-02). ETA=18:29:39, max mem: 15.9 GB 
[10/28 08:48:58 visual_prompt]: 	Training 800/1106. train loss: 1.3042,	0.6334 s / batch. (data: 7.71e-04). ETA=18:09:02, max mem: 15.9 GB 
[10/28 08:50:01 visual_prompt]: 	Training 900/1106. train loss: 0.7580,	0.6188 s / batch. (data: 2.83e-04). ETA=17:42:51, max mem: 15.9 GB 
[10/28 08:51:04 visual_prompt]: 	Training 1000/1106. train loss: 2.6939,	0.6190 s / batch. (data: 3.71e-04). ETA=17:42:12, max mem: 15.9 GB 
[10/28 08:52:08 visual_prompt]: 	Training 1100/1106. train loss: 2.3934,	0.6186 s / batch. (data: 1.37e-04). ETA=17:40:32, max mem: 15.9 GB 
[10/28 08:52:12 visual_prompt]: Epoch 7 / 100: avg data time: 4.34e-03, avg batch time: 0.6356, average train loss: 1.8493
[10/28 08:53:01 visual_prompt]: 	Test 100/123. loss: 1.339, 0.2365 s / batch. (data: 3.98e-05)max mem: 15.94594 GB 
[10/28 08:53:12 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2336, average loss: 1.2114
[10/28 08:53:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.69	
[10/28 08:53:12 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/28 08:54:17 visual_prompt]: 	Training 100/1106. train loss: 0.3220,	0.6480 s / batch. (data: 8.27e-04). ETA=18:29:45, max mem: 15.9 GB 
[10/28 08:55:21 visual_prompt]: 	Training 200/1106. train loss: 1.4012,	0.6186 s / batch. (data: 3.19e-04). ETA=17:38:26, max mem: 15.9 GB 
[10/28 08:56:24 visual_prompt]: 	Training 300/1106. train loss: 1.2906,	0.6355 s / batch. (data: 7.85e-04). ETA=18:06:12, max mem: 15.9 GB 
[10/28 08:57:27 visual_prompt]: 	Training 400/1106. train loss: 0.0081,	0.6197 s / batch. (data: 2.51e-04). ETA=17:38:12, max mem: 15.9 GB 
[10/28 08:58:31 visual_prompt]: 	Training 500/1106. train loss: 0.0401,	0.6199 s / batch. (data: 3.17e-04). ETA=17:37:27, max mem: 15.9 GB 
[10/28 08:59:34 visual_prompt]: 	Training 600/1106. train loss: 0.0005,	0.6538 s / batch. (data: 1.10e-02). ETA=18:34:13, max mem: 15.9 GB 
[10/28 09:00:38 visual_prompt]: 	Training 700/1106. train loss: 1.6496,	0.6439 s / batch. (data: 1.25e-02). ETA=18:16:21, max mem: 15.9 GB 
[10/28 09:01:41 visual_prompt]: 	Training 800/1106. train loss: 4.7047,	0.6437 s / batch. (data: 2.02e-02). ETA=18:14:56, max mem: 15.9 GB 
[10/28 09:02:44 visual_prompt]: 	Training 900/1106. train loss: 0.1202,	0.6194 s / batch. (data: 3.17e-04). ETA=17:32:27, max mem: 15.9 GB 
[10/28 09:03:48 visual_prompt]: 	Training 1000/1106. train loss: 1.0791,	0.6192 s / batch. (data: 3.31e-04). ETA=17:31:09, max mem: 15.9 GB 
[10/28 09:04:51 visual_prompt]: 	Training 1100/1106. train loss: 0.0246,	0.6185 s / batch. (data: 1.38e-04). ETA=17:28:57, max mem: 15.9 GB 
[10/28 09:04:55 visual_prompt]: Epoch 8 / 100: avg data time: 4.17e-03, avg batch time: 0.6353, average train loss: 2.1799
[10/28 09:05:45 visual_prompt]: 	Test 100/123. loss: 0.808, 0.2272 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/28 09:05:55 visual_prompt]: Inference (val):avg data time: 3.11e-04, avg batch time: 0.2320, average loss: 0.7494
[10/28 09:05:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.56	
[10/28 09:05:55 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/28 09:07:01 visual_prompt]: 	Training 100/1106. train loss: 9.0689,	0.6336 s / batch. (data: 4.25e-04). ETA=17:53:21, max mem: 15.9 GB 
[10/28 09:08:04 visual_prompt]: 	Training 200/1106. train loss: 0.0057,	0.6479 s / batch. (data: 1.47e-03). ETA=18:16:34, max mem: 15.9 GB 
[10/28 09:09:07 visual_prompt]: 	Training 300/1106. train loss: 0.8401,	0.6308 s / batch. (data: 8.31e-04). ETA=17:46:38, max mem: 15.9 GB 
[10/28 09:10:10 visual_prompt]: 	Training 400/1106. train loss: 0.1554,	0.6462 s / batch. (data: 1.60e-02). ETA=18:11:28, max mem: 15.9 GB 
[10/28 09:11:14 visual_prompt]: 	Training 500/1106. train loss: 5.4621,	0.6442 s / batch. (data: 8.33e-04). ETA=18:07:05, max mem: 15.9 GB 
[10/28 09:12:17 visual_prompt]: 	Training 600/1106. train loss: 0.8453,	0.6406 s / batch. (data: 3.01e-04). ETA=17:59:55, max mem: 15.9 GB 
[10/28 09:13:21 visual_prompt]: 	Training 700/1106. train loss: 0.0287,	0.6518 s / batch. (data: 7.95e-04). ETA=18:17:46, max mem: 15.9 GB 
[10/28 09:14:24 visual_prompt]: 	Training 800/1106. train loss: 0.9643,	0.6541 s / batch. (data: 7.48e-04). ETA=18:20:35, max mem: 15.9 GB 
[10/28 09:15:27 visual_prompt]: 	Training 900/1106. train loss: 0.0053,	0.6323 s / batch. (data: 7.53e-04). ETA=17:42:49, max mem: 15.9 GB 
[10/28 09:16:31 visual_prompt]: 	Training 1000/1106. train loss: 0.3543,	0.6447 s / batch. (data: 8.07e-04). ETA=18:02:35, max mem: 15.9 GB 
[10/28 09:17:34 visual_prompt]: 	Training 1100/1106. train loss: 1.0440,	0.6181 s / batch. (data: 1.50e-04). ETA=17:16:56, max mem: 15.9 GB 
[10/28 09:17:38 visual_prompt]: Epoch 9 / 100: avg data time: 4.26e-03, avg batch time: 0.6348, average train loss: 2.0526
[10/28 09:18:28 visual_prompt]: 	Test 100/123. loss: 0.833, 0.2478 s / batch. (data: 2.60e-05)max mem: 15.94594 GB 
[10/28 09:18:38 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2322, average loss: 0.7784
[10/28 09:18:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.00	
[10/28 09:18:38 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/28 09:19:43 visual_prompt]: 	Training 100/1106. train loss: 3.7856,	0.6301 s / batch. (data: 3.25e-04). ETA=17:35:50, max mem: 15.9 GB 
[10/28 09:20:46 visual_prompt]: 	Training 200/1106. train loss: 12.7950,	0.6187 s / batch. (data: 2.96e-04). ETA=17:15:43, max mem: 15.9 GB 
[10/28 09:21:49 visual_prompt]: 	Training 300/1106. train loss: 0.5465,	0.6227 s / batch. (data: 5.44e-03). ETA=17:21:26, max mem: 15.9 GB 
[10/28 09:22:53 visual_prompt]: 	Training 400/1106. train loss: 18.3364,	0.6319 s / batch. (data: 7.99e-04). ETA=17:35:40, max mem: 15.9 GB 
[10/28 09:23:56 visual_prompt]: 	Training 500/1106. train loss: 0.9733,	0.6289 s / batch. (data: 7.77e-04). ETA=17:29:39, max mem: 15.9 GB 
[10/28 09:24:59 visual_prompt]: 	Training 600/1106. train loss: 13.8278,	0.6524 s / batch. (data: 7.67e-04). ETA=18:07:47, max mem: 15.9 GB 
[10/28 09:26:03 visual_prompt]: 	Training 700/1106. train loss: 6.9883,	0.6427 s / batch. (data: 8.54e-04). ETA=17:50:39, max mem: 15.9 GB 
[10/28 09:27:06 visual_prompt]: 	Training 800/1106. train loss: 2.9265,	0.6294 s / batch. (data: 1.05e-02). ETA=17:27:18, max mem: 15.9 GB 
[10/28 09:28:09 visual_prompt]: 	Training 900/1106. train loss: 0.7705,	0.6343 s / batch. (data: 3.19e-04). ETA=17:34:24, max mem: 15.9 GB 
[10/28 09:29:12 visual_prompt]: 	Training 1000/1106. train loss: 1.4676,	0.6355 s / batch. (data: 7.71e-04). ETA=17:35:27, max mem: 15.9 GB 
[10/28 09:30:15 visual_prompt]: 	Training 1100/1106. train loss: 1.4411,	0.6184 s / batch. (data: 1.36e-04). ETA=17:06:03, max mem: 15.9 GB 
[10/28 09:30:19 visual_prompt]: Epoch 10 / 100: avg data time: 3.86e-03, avg batch time: 0.6339, average train loss: 3.3286
[10/28 09:31:09 visual_prompt]: 	Test 100/123. loss: 1.717, 0.2255 s / batch. (data: 2.48e-05)max mem: 15.94594 GB 
[10/28 09:31:20 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2324, average loss: 1.5509
[10/28 09:31:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.23	
[10/28 09:31:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/28 09:32:25 visual_prompt]: 	Training 100/1106. train loss: 0.0012,	0.6320 s / batch. (data: 1.37e-02). ETA=17:27:28, max mem: 15.9 GB 
[10/28 09:33:28 visual_prompt]: 	Training 200/1106. train loss: 3.6018,	0.6211 s / batch. (data: 2.93e-04). ETA=17:08:18, max mem: 15.9 GB 
[10/28 09:34:32 visual_prompt]: 	Training 300/1106. train loss: 4.3170,	0.6310 s / batch. (data: 8.32e-04). ETA=17:23:44, max mem: 15.9 GB 
[10/28 09:35:35 visual_prompt]: 	Training 400/1106. train loss: 1.7993,	0.6400 s / batch. (data: 8.02e-04). ETA=17:37:28, max mem: 15.9 GB 
[10/28 09:36:38 visual_prompt]: 	Training 500/1106. train loss: 0.4846,	0.6245 s / batch. (data: 3.29e-04). ETA=17:10:54, max mem: 15.9 GB 
[10/28 09:37:41 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6215 s / batch. (data: 3.06e-04). ETA=17:04:46, max mem: 15.9 GB 
[10/28 09:38:45 visual_prompt]: 	Training 700/1106. train loss: 0.0010,	0.6434 s / batch. (data: 1.07e-03). ETA=17:39:58, max mem: 15.9 GB 
[10/28 09:39:48 visual_prompt]: 	Training 800/1106. train loss: 1.2749,	0.6617 s / batch. (data: 8.19e-04). ETA=18:08:53, max mem: 15.9 GB 
[10/28 09:40:51 visual_prompt]: 	Training 900/1106. train loss: 0.0048,	0.6187 s / batch. (data: 3.33e-04). ETA=16:57:08, max mem: 15.9 GB 
[10/28 09:41:54 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6447 s / batch. (data: 7.45e-04). ETA=17:38:47, max mem: 15.9 GB 
[10/28 09:42:57 visual_prompt]: 	Training 1100/1106. train loss: 0.6640,	0.6179 s / batch. (data: 1.88e-04). ETA=16:53:41, max mem: 15.9 GB 
[10/28 09:43:01 visual_prompt]: Epoch 11 / 100: avg data time: 4.50e-03, avg batch time: 0.6343, average train loss: 3.3775
[10/28 09:43:51 visual_prompt]: 	Test 100/123. loss: 1.567, 0.2257 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/28 09:44:02 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2324, average loss: 1.3670
[10/28 09:44:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.61	
[10/28 09:44:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/28 09:45:08 visual_prompt]: 	Training 100/1106. train loss: 3.1458,	0.6182 s / batch. (data: 2.89e-04). ETA=16:53:08, max mem: 15.9 GB 
[10/28 09:46:11 visual_prompt]: 	Training 200/1106. train loss: 0.9450,	0.6296 s / batch. (data: 2.67e-04). ETA=17:10:50, max mem: 15.9 GB 
[10/28 09:47:15 visual_prompt]: 	Training 300/1106. train loss: 4.2393,	0.6373 s / batch. (data: 3.09e-04). ETA=17:22:20, max mem: 15.9 GB 
[10/28 09:48:18 visual_prompt]: 	Training 400/1106. train loss: 0.7869,	0.6468 s / batch. (data: 1.56e-02). ETA=17:36:45, max mem: 15.9 GB 
[10/28 09:49:21 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6181 s / batch. (data: 3.14e-04). ETA=16:48:53, max mem: 15.9 GB 
[10/28 09:50:24 visual_prompt]: 	Training 600/1106. train loss: 0.0005,	0.6332 s / batch. (data: 8.44e-04). ETA=17:12:28, max mem: 15.9 GB 
[10/28 09:51:28 visual_prompt]: 	Training 700/1106. train loss: 5.4227,	0.6195 s / batch. (data: 2.76e-04). ETA=16:49:05, max mem: 15.9 GB 
[10/28 09:52:31 visual_prompt]: 	Training 800/1106. train loss: 1.0217,	0.6298 s / batch. (data: 3.08e-04). ETA=17:04:49, max mem: 15.9 GB 
[10/28 09:53:34 visual_prompt]: 	Training 900/1106. train loss: 1.0072,	0.6425 s / batch. (data: 7.63e-04). ETA=17:24:24, max mem: 15.9 GB 
[10/28 09:54:37 visual_prompt]: 	Training 1000/1106. train loss: 3.6115,	0.6340 s / batch. (data: 8.00e-04). ETA=17:09:30, max mem: 15.9 GB 
[10/28 09:55:41 visual_prompt]: 	Training 1100/1106. train loss: 5.9749,	0.6181 s / batch. (data: 1.57e-04). ETA=16:42:43, max mem: 15.9 GB 
[10/28 09:55:44 visual_prompt]: Epoch 12 / 100: avg data time: 5.03e-03, avg batch time: 0.6353, average train loss: 3.8652
[10/28 09:56:34 visual_prompt]: 	Test 100/123. loss: 1.686, 0.2254 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[10/28 09:56:45 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2319, average loss: 1.5201
[10/28 09:56:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.89	
[10/28 09:56:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/28 09:57:50 visual_prompt]: 	Training 100/1106. train loss: 0.9637,	0.6331 s / batch. (data: 8.48e-04). ETA=17:05:50, max mem: 15.9 GB 
[10/28 09:58:54 visual_prompt]: 	Training 200/1106. train loss: 0.8985,	0.6321 s / batch. (data: 8.34e-04). ETA=17:03:10, max mem: 15.9 GB 
[10/28 09:59:57 visual_prompt]: 	Training 300/1106. train loss: 3.1960,	0.6180 s / batch. (data: 3.24e-04). ETA=16:39:25, max mem: 15.9 GB 
[10/28 10:01:00 visual_prompt]: 	Training 400/1106. train loss: 1.2856,	0.6360 s / batch. (data: 3.05e-04). ETA=17:07:21, max mem: 15.9 GB 
[10/28 10:02:04 visual_prompt]: 	Training 500/1106. train loss: 0.0164,	0.6191 s / batch. (data: 2.84e-04). ETA=16:39:05, max mem: 15.9 GB 
[10/28 10:03:07 visual_prompt]: 	Training 600/1106. train loss: 2.8494,	0.6185 s / batch. (data: 3.09e-04). ETA=16:37:08, max mem: 15.9 GB 
[10/28 10:04:10 visual_prompt]: 	Training 700/1106. train loss: 0.0891,	0.6400 s / batch. (data: 8.61e-04). ETA=17:10:38, max mem: 15.9 GB 
[10/28 10:05:13 visual_prompt]: 	Training 800/1106. train loss: 2.0179,	0.6382 s / batch. (data: 8.14e-04). ETA=17:06:41, max mem: 15.9 GB 
[10/28 10:06:17 visual_prompt]: 	Training 900/1106. train loss: 10.4417,	0.6468 s / batch. (data: 7.91e-04). ETA=17:19:27, max mem: 15.9 GB 
[10/28 10:07:20 visual_prompt]: 	Training 1000/1106. train loss: 0.0036,	0.6303 s / batch. (data: 7.91e-04). ETA=16:51:51, max mem: 15.9 GB 
[10/28 10:08:23 visual_prompt]: 	Training 1100/1106. train loss: 3.8462,	0.6185 s / batch. (data: 1.93e-04). ETA=16:31:58, max mem: 15.9 GB 
[10/28 10:08:27 visual_prompt]: Epoch 13 / 100: avg data time: 4.30e-03, avg batch time: 0.6350, average train loss: 2.0980
[10/28 10:09:17 visual_prompt]: 	Test 100/123. loss: 0.707, 0.2397 s / batch. (data: 5.22e-05)max mem: 15.94594 GB 
[10/28 10:09:28 visual_prompt]: Inference (val):avg data time: 4.62e-05, avg batch time: 0.2320, average loss: 0.7210
[10/28 10:09:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.28	
[10/28 10:09:28 visual_prompt]: Best epoch 13: best metric: -0.721
[10/28 10:09:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/28 10:10:33 visual_prompt]: 	Training 100/1106. train loss: 0.0006,	0.6408 s / batch. (data: 3.54e-04). ETA=17:06:32, max mem: 15.9 GB 
[10/28 10:11:37 visual_prompt]: 	Training 200/1106. train loss: 1.2257,	0.6367 s / batch. (data: 8.43e-04). ETA=16:58:58, max mem: 15.9 GB 
[10/28 10:12:40 visual_prompt]: 	Training 300/1106. train loss: 2.6579,	0.6336 s / batch. (data: 8.21e-04). ETA=16:52:52, max mem: 15.9 GB 
[10/28 10:13:43 visual_prompt]: 	Training 400/1106. train loss: 0.0001,	0.6172 s / batch. (data: 2.94e-04). ETA=16:25:38, max mem: 15.9 GB 
[10/28 10:14:46 visual_prompt]: 	Training 500/1106. train loss: 2.8811,	0.6337 s / batch. (data: 8.07e-04). ETA=16:51:01, max mem: 15.9 GB 
[10/28 10:15:50 visual_prompt]: 	Training 600/1106. train loss: 1.9570,	0.6450 s / batch. (data: 7.90e-04). ETA=17:07:55, max mem: 15.9 GB 
[10/28 10:16:53 visual_prompt]: 	Training 700/1106. train loss: 0.2605,	0.6176 s / batch. (data: 3.30e-04). ETA=16:23:17, max mem: 15.9 GB 
[10/28 10:17:56 visual_prompt]: 	Training 800/1106. train loss: 3.3093,	0.6441 s / batch. (data: 8.17e-04). ETA=17:04:19, max mem: 15.9 GB 
[10/28 10:18:59 visual_prompt]: 	Training 900/1106. train loss: 1.5807,	0.6176 s / batch. (data: 3.37e-04). ETA=16:21:09, max mem: 15.9 GB 
[10/28 10:20:03 visual_prompt]: 	Training 1000/1106. train loss: 1.4198,	0.6573 s / batch. (data: 1.25e-03). ETA=17:23:11, max mem: 15.9 GB 
[10/28 10:21:06 visual_prompt]: 	Training 1100/1106. train loss: 0.9412,	0.6185 s / batch. (data: 1.73e-04). ETA=16:20:31, max mem: 15.9 GB 
[10/28 10:21:09 visual_prompt]: Epoch 14 / 100: avg data time: 4.37e-03, avg batch time: 0.6343, average train loss: 2.1464
[10/28 10:21:59 visual_prompt]: 	Test 100/123. loss: 1.804, 0.2407 s / batch. (data: 4.29e-05)max mem: 15.94594 GB 
[10/28 10:22:10 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2329, average loss: 1.6262
[10/28 10:22:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.56	
[10/28 10:22:10 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/28 10:23:15 visual_prompt]: 	Training 100/1106. train loss: 7.3704,	0.6308 s / batch. (data: 7.78e-04). ETA=16:38:53, max mem: 15.9 GB 
[10/28 10:24:18 visual_prompt]: 	Training 200/1106. train loss: 2.0944,	0.6208 s / batch. (data: 2.51e-04). ETA=16:21:59, max mem: 15.9 GB 
[10/28 10:25:21 visual_prompt]: 	Training 300/1106. train loss: 7.4743,	0.6340 s / batch. (data: 3.03e-04). ETA=16:41:54, max mem: 15.9 GB 
[10/28 10:26:25 visual_prompt]: 	Training 400/1106. train loss: 35.6207,	0.6407 s / batch. (data: 8.18e-04). ETA=16:51:21, max mem: 15.9 GB 
[10/28 10:27:28 visual_prompt]: 	Training 500/1106. train loss: 3.6190,	0.6358 s / batch. (data: 1.57e-02). ETA=16:42:34, max mem: 15.9 GB 
[10/28 10:28:32 visual_prompt]: 	Training 600/1106. train loss: 1.1806,	0.6195 s / batch. (data: 2.94e-04). ETA=16:15:54, max mem: 15.9 GB 
[10/28 10:29:35 visual_prompt]: 	Training 700/1106. train loss: 0.5251,	0.6239 s / batch. (data: 3.55e-04). ETA=16:21:42, max mem: 15.9 GB 
[10/28 10:30:38 visual_prompt]: 	Training 800/1106. train loss: 0.6370,	0.6279 s / batch. (data: 7.62e-04). ETA=16:27:04, max mem: 15.9 GB 
[10/28 10:31:42 visual_prompt]: 	Training 900/1106. train loss: 6.2918,	0.6340 s / batch. (data: 3.33e-04). ETA=16:35:30, max mem: 15.9 GB 
[10/28 10:32:45 visual_prompt]: 	Training 1000/1106. train loss: 0.6978,	0.6204 s / batch. (data: 2.57e-04). ETA=16:13:05, max mem: 15.9 GB 
[10/28 10:33:48 visual_prompt]: 	Training 1100/1106. train loss: 5.1399,	0.6199 s / batch. (data: 1.62e-04). ETA=16:11:22, max mem: 15.9 GB 
[10/28 10:33:52 visual_prompt]: Epoch 15 / 100: avg data time: 3.93e-03, avg batch time: 0.6344, average train loss: 3.9037
[10/28 10:34:43 visual_prompt]: 	Test 100/123. loss: 1.422, 0.2254 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/28 10:34:53 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2342, average loss: 1.5535
[10/28 10:34:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.47	
[10/28 10:34:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/28 10:35:58 visual_prompt]: 	Training 100/1106. train loss: 0.0048,	0.6396 s / batch. (data: 8.17e-04). ETA=16:41:04, max mem: 15.9 GB 
[10/28 10:37:01 visual_prompt]: 	Training 200/1106. train loss: 1.5213,	0.6282 s / batch. (data: 2.98e-04). ETA=16:22:07, max mem: 15.9 GB 
[10/28 10:38:05 visual_prompt]: 	Training 300/1106. train loss: 0.0000,	0.6233 s / batch. (data: 3.36e-04). ETA=16:13:26, max mem: 15.9 GB 
[10/28 10:39:08 visual_prompt]: 	Training 400/1106. train loss: 9.9843,	0.6196 s / batch. (data: 3.01e-04). ETA=16:06:39, max mem: 15.9 GB 
[10/28 10:40:11 visual_prompt]: 	Training 500/1106. train loss: 0.8414,	0.6371 s / batch. (data: 5.44e-03). ETA=16:32:51, max mem: 15.9 GB 
[10/28 10:41:15 visual_prompt]: 	Training 600/1106. train loss: 1.5578,	0.6339 s / batch. (data: 7.93e-04). ETA=16:26:49, max mem: 15.9 GB 
[10/28 10:42:18 visual_prompt]: 	Training 700/1106. train loss: 0.7947,	0.6208 s / batch. (data: 3.15e-04). ETA=16:05:30, max mem: 15.9 GB 
[10/28 10:43:21 visual_prompt]: 	Training 800/1106. train loss: 0.9138,	0.6289 s / batch. (data: 1.05e-02). ETA=16:16:58, max mem: 15.9 GB 
[10/28 10:44:24 visual_prompt]: 	Training 900/1106. train loss: 1.2440,	0.6359 s / batch. (data: 8.07e-04). ETA=16:26:50, max mem: 15.9 GB 
[10/28 10:45:28 visual_prompt]: 	Training 1000/1106. train loss: 5.1858,	0.6533 s / batch. (data: 3.62e-02). ETA=16:52:41, max mem: 15.9 GB 
[10/28 10:46:31 visual_prompt]: 	Training 1100/1106. train loss: 0.0011,	0.6190 s / batch. (data: 1.61e-04). ETA=15:58:30, max mem: 15.9 GB 
[10/28 10:46:35 visual_prompt]: Epoch 16 / 100: avg data time: 4.14e-03, avg batch time: 0.6347, average train loss: 2.5118
[10/28 10:47:25 visual_prompt]: 	Test 100/123. loss: 1.764, 0.2557 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[10/28 10:47:36 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2329, average loss: 1.5775
[10/28 10:47:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.32	
[10/28 10:47:36 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/28 10:48:41 visual_prompt]: 	Training 100/1106. train loss: 0.0083,	0.6330 s / batch. (data: 3.10e-04). ETA=16:19:01, max mem: 15.9 GB 
[10/28 10:49:45 visual_prompt]: 	Training 200/1106. train loss: 0.6837,	0.6315 s / batch. (data: 8.44e-04). ETA=16:15:41, max mem: 15.9 GB 
[10/28 10:50:48 visual_prompt]: 	Training 300/1106. train loss: 0.0982,	0.6337 s / batch. (data: 1.59e-02). ETA=16:18:05, max mem: 15.9 GB 
[10/28 10:51:51 visual_prompt]: 	Training 400/1106. train loss: 0.1678,	0.6308 s / batch. (data: 2.87e-04). ETA=16:12:28, max mem: 15.9 GB 
[10/28 10:52:55 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6305 s / batch. (data: 7.66e-04). ETA=16:10:57, max mem: 15.9 GB 
[10/28 10:53:58 visual_prompt]: 	Training 600/1106. train loss: 2.8877,	0.6290 s / batch. (data: 5.46e-03). ETA=16:07:40, max mem: 15.9 GB 
[10/28 10:55:01 visual_prompt]: 	Training 700/1106. train loss: 0.7154,	0.6332 s / batch. (data: 3.21e-04). ETA=16:13:07, max mem: 15.9 GB 
[10/28 10:56:04 visual_prompt]: 	Training 800/1106. train loss: 6.5408,	0.6328 s / batch. (data: 7.59e-04). ETA=16:11:23, max mem: 15.9 GB 
[10/28 10:57:08 visual_prompt]: 	Training 900/1106. train loss: 2.2278,	0.6295 s / batch. (data: 3.03e-04). ETA=16:05:14, max mem: 15.9 GB 
[10/28 10:58:11 visual_prompt]: 	Training 1000/1106. train loss: 0.0532,	0.6282 s / batch. (data: 2.93e-04). ETA=16:02:13, max mem: 15.9 GB 
[10/28 10:59:14 visual_prompt]: 	Training 1100/1106. train loss: 0.0001,	0.6187 s / batch. (data: 1.64e-04). ETA=15:46:37, max mem: 15.9 GB 
[10/28 10:59:18 visual_prompt]: Epoch 17 / 100: avg data time: 4.83e-03, avg batch time: 0.6349, average train loss: 2.2796
[10/28 11:00:09 visual_prompt]: 	Test 100/123. loss: 2.807, 0.2255 s / batch. (data: 2.57e-05)max mem: 15.94594 GB 
[10/28 11:00:19 visual_prompt]: Inference (val):avg data time: 9.81e-05, avg batch time: 0.2340, average loss: 3.0893
[10/28 11:00:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.98	
[10/28 11:00:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/28 11:01:24 visual_prompt]: 	Training 100/1106. train loss: 0.0002,	0.6236 s / batch. (data: 5.44e-03). ETA=15:53:04, max mem: 15.9 GB 
[10/28 11:02:28 visual_prompt]: 	Training 200/1106. train loss: 22.6146,	0.6324 s / batch. (data: 3.06e-04). ETA=16:05:27, max mem: 15.9 GB 
[10/28 11:03:31 visual_prompt]: 	Training 300/1106. train loss: 44.1444,	0.6187 s / batch. (data: 3.03e-04). ETA=15:43:33, max mem: 15.9 GB 
[10/28 11:04:34 visual_prompt]: 	Training 400/1106. train loss: 7.9365,	0.6388 s / batch. (data: 8.23e-04). ETA=16:13:05, max mem: 15.9 GB 
[10/28 11:05:38 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6418 s / batch. (data: 8.11e-04). ETA=16:16:33, max mem: 15.9 GB 
[10/28 11:06:41 visual_prompt]: 	Training 600/1106. train loss: 2.4091,	0.6303 s / batch. (data: 8.29e-04). ETA=15:58:00, max mem: 15.9 GB 
[10/28 11:07:44 visual_prompt]: 	Training 700/1106. train loss: 0.0015,	0.6609 s / batch. (data: 1.09e-02). ETA=16:43:29, max mem: 15.9 GB 
[10/28 11:08:47 visual_prompt]: 	Training 800/1106. train loss: 1.9720,	0.6312 s / batch. (data: 8.25e-04). ETA=15:57:16, max mem: 15.9 GB 
[10/28 11:09:50 visual_prompt]: 	Training 900/1106. train loss: 5.9611,	0.6389 s / batch. (data: 1.52e-02). ETA=16:07:56, max mem: 15.9 GB 
[10/28 11:10:54 visual_prompt]: 	Training 1000/1106. train loss: 1.2798,	0.6339 s / batch. (data: 3.34e-04). ETA=15:59:18, max mem: 15.9 GB 
[10/28 11:11:57 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6180 s / batch. (data: 1.51e-04). ETA=15:34:13, max mem: 15.9 GB 
[10/28 11:12:01 visual_prompt]: Epoch 18 / 100: avg data time: 4.49e-03, avg batch time: 0.6344, average train loss: 3.4365
[10/28 11:12:50 visual_prompt]: 	Test 100/123. loss: 1.825, 0.2270 s / batch. (data: 3.98e-05)max mem: 15.94594 GB 
[10/28 11:13:01 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2318, average loss: 2.0194
[10/28 11:13:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.59	
[10/28 11:13:01 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/28 11:14:07 visual_prompt]: 	Training 100/1106. train loss: 0.0197,	0.6426 s / batch. (data: 8.07e-04). ETA=16:10:17, max mem: 15.9 GB 
[10/28 11:15:10 visual_prompt]: 	Training 200/1106. train loss: 4.1848,	0.6341 s / batch. (data: 7.94e-04). ETA=15:56:22, max mem: 15.9 GB 
[10/28 11:16:13 visual_prompt]: 	Training 300/1106. train loss: 0.1073,	0.6200 s / batch. (data: 4.39e-04). ETA=15:34:05, max mem: 15.9 GB 
[10/28 11:17:17 visual_prompt]: 	Training 400/1106. train loss: 1.1595,	0.6547 s / batch. (data: 1.60e-02). ETA=16:25:12, max mem: 15.9 GB 
[10/28 11:18:20 visual_prompt]: 	Training 500/1106. train loss: 0.5910,	0.6360 s / batch. (data: 9.26e-04). ETA=15:55:59, max mem: 15.9 GB 
[10/28 11:19:23 visual_prompt]: 	Training 600/1106. train loss: 0.0030,	0.6189 s / batch. (data: 3.49e-04). ETA=15:29:17, max mem: 15.9 GB 
[10/28 11:20:27 visual_prompt]: 	Training 700/1106. train loss: 1.8821,	0.6328 s / batch. (data: 3.04e-04). ETA=15:49:09, max mem: 15.9 GB 
[10/28 11:21:30 visual_prompt]: 	Training 800/1106. train loss: 0.0001,	0.6315 s / batch. (data: 3.22e-04). ETA=15:46:09, max mem: 15.9 GB 
[10/28 11:22:33 visual_prompt]: 	Training 900/1106. train loss: 0.0651,	0.6371 s / batch. (data: 7.57e-04). ETA=15:53:26, max mem: 15.9 GB 
[10/28 11:23:37 visual_prompt]: 	Training 1000/1106. train loss: 0.0012,	0.6345 s / batch. (data: 2.92e-04). ETA=15:48:27, max mem: 15.9 GB 
[10/28 11:24:40 visual_prompt]: 	Training 1100/1106. train loss: 0.0311,	0.6374 s / batch. (data: 1.70e-04). ETA=15:51:49, max mem: 15.9 GB 
[10/28 11:24:44 visual_prompt]: Epoch 19 / 100: avg data time: 4.26e-03, avg batch time: 0.6351, average train loss: 2.2180
[10/28 11:25:34 visual_prompt]: 	Test 100/123. loss: 16.279, 0.2257 s / batch. (data: 4.10e-05)max mem: 15.94594 GB 
[10/28 11:25:45 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.2318, average loss: 14.7424
[10/28 11:25:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.25	
[10/28 11:25:45 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/28 11:26:50 visual_prompt]: 	Training 100/1106. train loss: 3.2942,	0.6320 s / batch. (data: 3.40e-04). ETA=15:42:39, max mem: 15.9 GB 
[10/28 11:27:54 visual_prompt]: 	Training 200/1106. train loss: 3.1261,	0.6235 s / batch. (data: 5.45e-03). ETA=15:28:51, max mem: 15.9 GB 
[10/28 11:28:57 visual_prompt]: 	Training 300/1106. train loss: 5.9508,	0.6560 s / batch. (data: 8.74e-04). ETA=16:16:15, max mem: 15.9 GB 
[10/28 11:30:00 visual_prompt]: 	Training 400/1106. train loss: 0.5381,	0.6435 s / batch. (data: 9.38e-04). ETA=15:56:35, max mem: 15.9 GB 
[10/28 11:31:03 visual_prompt]: 	Training 500/1106. train loss: 2.3984,	0.6357 s / batch. (data: 8.12e-04). ETA=15:43:47, max mem: 15.9 GB 
[10/28 11:32:07 visual_prompt]: 	Training 600/1106. train loss: 1.3896,	0.6184 s / batch. (data: 3.15e-04). ETA=15:17:11, max mem: 15.9 GB 
[10/28 11:33:10 visual_prompt]: 	Training 700/1106. train loss: 39.1018,	0.6335 s / batch. (data: 3.13e-04). ETA=15:38:30, max mem: 15.9 GB 
[10/28 11:34:13 visual_prompt]: 	Training 800/1106. train loss: 0.8131,	0.6425 s / batch. (data: 7.95e-04). ETA=15:50:48, max mem: 15.9 GB 
[10/28 11:35:17 visual_prompt]: 	Training 900/1106. train loss: 14.7358,	0.6347 s / batch. (data: 8.04e-04). ETA=15:38:06, max mem: 15.9 GB 
[10/28 11:36:20 visual_prompt]: 	Training 1000/1106. train loss: 0.0015,	0.6311 s / batch. (data: 1.21e-02). ETA=15:31:44, max mem: 15.9 GB 
[10/28 11:37:23 visual_prompt]: 	Training 1100/1106. train loss: 1.0862,	0.6191 s / batch. (data: 3.32e-04). ETA=15:13:02, max mem: 15.9 GB 
[10/28 11:37:27 visual_prompt]: Epoch 20 / 100: avg data time: 4.65e-03, avg batch time: 0.6351, average train loss: 3.9850
[10/28 11:38:17 visual_prompt]: 	Test 100/123. loss: 0.545, 0.2469 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/28 11:38:28 visual_prompt]: Inference (val):avg data time: 1.06e-04, avg batch time: 0.2328, average loss: 9.5022
[10/28 11:38:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 47.49	
[10/28 11:38:28 visual_prompt]: Stopping early.
[10/28 11:38:28 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 11:38:28 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 11:38:28 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 11:38:28 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 11:38:28 visual_prompt]: Training with config:
[10/28 11:38:28 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 11:38:28 visual_prompt]: Loading training data...
[10/28 11:38:28 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 11:38:28 visual_prompt]: Loading validation data...
[10/28 11:38:28 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 11:38:28 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/28 11:38:31 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/28 11:38:31 visual_prompt]: tuned percent:0.522
[10/28 11:38:31 visual_prompt]: Device used for model: 0
[10/28 11:38:31 visual_prompt]: Setting up Evaluator...
[10/28 11:38:31 visual_prompt]: Setting up Trainer...
[10/28 11:38:31 visual_prompt]: 	Setting up the optimizer...
[10/28 11:38:31 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 11:39:36 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6184 s / batch. (data: 3.52e-04). ETA=18:58:50, max mem: 15.9 GB 
[10/28 11:40:40 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6332 s / batch. (data: 8.20e-04). ETA=19:25:00, max mem: 15.9 GB 
[10/28 11:41:43 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6329 s / batch. (data: 8.85e-04). ETA=19:23:28, max mem: 15.9 GB 
[10/28 11:42:46 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6331 s / batch. (data: 7.84e-04). ETA=19:22:45, max mem: 15.9 GB 
[10/28 11:43:49 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6360 s / batch. (data: 2.96e-04). ETA=19:27:02, max mem: 15.9 GB 
[10/28 11:44:53 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6341 s / batch. (data: 7.93e-04). ETA=19:22:25, max mem: 15.9 GB 
[10/28 11:45:56 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6193 s / batch. (data: 3.21e-04). ETA=18:54:21, max mem: 15.9 GB 
[10/28 11:46:59 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6241 s / batch. (data: 2.93e-04). ETA=19:02:06, max mem: 15.9 GB 
[10/28 11:48:03 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6320 s / batch. (data: 3.15e-04). ETA=19:15:32, max mem: 15.9 GB 
[10/28 11:49:06 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6188 s / batch. (data: 3.03e-04). ETA=18:50:15, max mem: 15.9 GB 
[10/28 11:50:10 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6189 s / batch. (data: 1.53e-04). ETA=18:49:34, max mem: 15.9 GB 
[10/28 11:50:14 visual_prompt]: Epoch 1 / 100: avg data time: 4.66e-03, avg batch time: 0.6353, average train loss: 1.4028
[10/28 11:51:04 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2510 s / batch. (data: 3.48e-05)max mem: 15.94594 GB 
[10/28 11:51:14 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.2342, average loss: 1.3505
[10/28 11:51:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/28 11:51:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/28 11:52:19 visual_prompt]: 	Training 100/1106. train loss: 2.8555,	0.6398 s / batch. (data: 8.15e-04). ETA=19:26:28, max mem: 15.9 GB 
[10/28 11:53:23 visual_prompt]: 	Training 200/1106. train loss: 0.9935,	0.6309 s / batch. (data: 2.30e-04). ETA=19:09:14, max mem: 15.9 GB 
[10/28 11:54:26 visual_prompt]: 	Training 300/1106. train loss: 1.4886,	0.6177 s / batch. (data: 3.29e-04). ETA=18:44:10, max mem: 15.9 GB 
[10/28 11:55:30 visual_prompt]: 	Training 400/1106. train loss: 0.0083,	0.6195 s / batch. (data: 2.89e-04). ETA=18:46:26, max mem: 15.9 GB 
[10/28 11:56:33 visual_prompt]: 	Training 500/1106. train loss: 2.6084,	0.6722 s / batch. (data: 8.38e-04). ETA=20:21:10, max mem: 15.9 GB 
[10/28 11:57:37 visual_prompt]: 	Training 600/1106. train loss: 0.6147,	0.6474 s / batch. (data: 8.07e-04). ETA=19:34:56, max mem: 15.9 GB 
[10/28 11:58:40 visual_prompt]: 	Training 700/1106. train loss: 0.2501,	0.6444 s / batch. (data: 2.50e-02). ETA=19:28:27, max mem: 15.9 GB 
[10/28 11:59:43 visual_prompt]: 	Training 800/1106. train loss: 1.0758,	0.6212 s / batch. (data: 3.31e-04). ETA=18:45:19, max mem: 15.9 GB 
[10/28 12:00:46 visual_prompt]: 	Training 900/1106. train loss: 1.3447,	0.6389 s / batch. (data: 8.10e-04). ETA=19:16:20, max mem: 15.9 GB 
[10/28 12:01:50 visual_prompt]: 	Training 1000/1106. train loss: 0.0490,	0.6178 s / batch. (data: 3.10e-04). ETA=18:37:06, max mem: 15.9 GB 
[10/28 12:02:53 visual_prompt]: 	Training 1100/1106. train loss: 0.4710,	0.6184 s / batch. (data: 1.37e-04). ETA=18:37:11, max mem: 15.9 GB 
[10/28 12:02:57 visual_prompt]: Epoch 2 / 100: avg data time: 4.28e-03, avg batch time: 0.6351, average train loss: 1.1250
[10/28 12:03:47 visual_prompt]: 	Test 100/123. loss: 1.282, 0.2257 s / batch. (data: 2.62e-05)max mem: 15.94594 GB 
[10/28 12:03:58 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2319, average loss: 1.1876
[10/28 12:03:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.44	
[10/28 12:03:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/28 12:05:04 visual_prompt]: 	Training 100/1106. train loss: 0.5683,	0.6237 s / batch. (data: 3.10e-04). ETA=18:45:43, max mem: 15.9 GB 
[10/28 12:06:07 visual_prompt]: 	Training 200/1106. train loss: 0.2059,	0.6323 s / batch. (data: 3.68e-04). ETA=19:00:06, max mem: 15.9 GB 
[10/28 12:07:11 visual_prompt]: 	Training 300/1106. train loss: 0.1051,	0.6370 s / batch. (data: 7.78e-04). ETA=19:07:34, max mem: 15.9 GB 
[10/28 12:08:14 visual_prompt]: 	Training 400/1106. train loss: 0.7171,	0.6360 s / batch. (data: 2.90e-04). ETA=19:04:42, max mem: 15.9 GB 
[10/28 12:09:17 visual_prompt]: 	Training 500/1106. train loss: 2.0823,	0.6194 s / batch. (data: 2.99e-04). ETA=18:33:47, max mem: 15.9 GB 
[10/28 12:10:21 visual_prompt]: 	Training 600/1106. train loss: 0.0300,	0.6417 s / batch. (data: 1.19e-03). ETA=19:12:48, max mem: 15.9 GB 
[10/28 12:11:24 visual_prompt]: 	Training 700/1106. train loss: 0.5718,	0.6349 s / batch. (data: 8.02e-04). ETA=18:59:35, max mem: 15.9 GB 
[10/28 12:12:27 visual_prompt]: 	Training 800/1106. train loss: 1.1389,	0.6286 s / batch. (data: 2.84e-04). ETA=18:47:12, max mem: 15.9 GB 
[10/28 12:13:30 visual_prompt]: 	Training 900/1106. train loss: 1.8545,	0.6387 s / batch. (data: 1.20e-02). ETA=19:04:12, max mem: 15.9 GB 
[10/28 12:14:34 visual_prompt]: 	Training 1000/1106. train loss: 1.5614,	0.6367 s / batch. (data: 1.27e-02). ETA=18:59:39, max mem: 15.9 GB 
[10/28 12:15:37 visual_prompt]: 	Training 1100/1106. train loss: 0.8106,	0.6179 s / batch. (data: 1.42e-04). ETA=18:24:57, max mem: 15.9 GB 
[10/28 12:15:41 visual_prompt]: Epoch 3 / 100: avg data time: 5.25e-03, avg batch time: 0.6354, average train loss: 1.3257
[10/28 12:16:31 visual_prompt]: 	Test 100/123. loss: 1.794, 0.2255 s / batch. (data: 5.41e-05)max mem: 15.94594 GB 
[10/28 12:16:42 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2318, average loss: 2.0356
[10/28 12:16:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.60	
[10/28 12:16:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/28 12:17:47 visual_prompt]: 	Training 100/1106. train loss: 0.5147,	0.6325 s / batch. (data: 8.64e-04). ETA=18:49:49, max mem: 15.9 GB 
[10/28 12:18:51 visual_prompt]: 	Training 200/1106. train loss: 8.8754,	0.6186 s / batch. (data: 3.21e-04). ETA=18:24:02, max mem: 15.9 GB 
[10/28 12:19:54 visual_prompt]: 	Training 300/1106. train loss: 0.9271,	0.6371 s / batch. (data: 7.56e-04). ETA=18:56:01, max mem: 15.9 GB 
[10/28 12:20:58 visual_prompt]: 	Training 400/1106. train loss: 0.7498,	0.6385 s / batch. (data: 5.87e-03). ETA=18:57:19, max mem: 15.9 GB 
[10/28 12:22:00 visual_prompt]: 	Training 500/1106. train loss: 0.5399,	0.6327 s / batch. (data: 3.16e-04). ETA=18:46:01, max mem: 15.9 GB 
[10/28 12:23:04 visual_prompt]: 	Training 600/1106. train loss: 0.0514,	0.6486 s / batch. (data: 3.21e-04). ETA=19:13:15, max mem: 15.9 GB 
[10/28 12:24:07 visual_prompt]: 	Training 700/1106. train loss: 4.5027,	0.6333 s / batch. (data: 7.90e-04). ETA=18:45:01, max mem: 15.9 GB 
[10/28 12:25:10 visual_prompt]: 	Training 800/1106. train loss: 0.4261,	0.6195 s / batch. (data: 3.06e-04). ETA=18:19:23, max mem: 15.9 GB 
[10/28 12:26:14 visual_prompt]: 	Training 900/1106. train loss: 4.6314,	0.6250 s / batch. (data: 3.39e-04). ETA=18:28:13, max mem: 15.9 GB 
[10/28 12:27:17 visual_prompt]: 	Training 1000/1106. train loss: 3.6436,	0.6228 s / batch. (data: 3.10e-04). ETA=18:23:12, max mem: 15.9 GB 
[10/28 12:28:20 visual_prompt]: 	Training 1100/1106. train loss: 4.3203,	0.6171 s / batch. (data: 1.50e-04). ETA=18:12:09, max mem: 15.9 GB 
[10/28 12:28:24 visual_prompt]: Epoch 4 / 100: avg data time: 4.50e-03, avg batch time: 0.6347, average train loss: 1.6308
[10/28 12:29:14 visual_prompt]: 	Test 100/123. loss: 1.128, 0.2495 s / batch. (data: 4.32e-05)max mem: 15.94594 GB 
[10/28 12:29:24 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2326, average loss: 1.2713
[10/28 12:29:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.39	
[10/28 12:29:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/28 12:30:30 visual_prompt]: 	Training 100/1106. train loss: 0.8494,	0.6342 s / batch. (data: 8.37e-04). ETA=18:41:18, max mem: 15.9 GB 
[10/28 12:31:33 visual_prompt]: 	Training 200/1106. train loss: 0.0043,	0.6250 s / batch. (data: 2.83e-04). ETA=18:23:53, max mem: 15.9 GB 
[10/28 12:32:36 visual_prompt]: 	Training 300/1106. train loss: 1.1889,	0.6315 s / batch. (data: 8.24e-04). ETA=18:34:25, max mem: 15.9 GB 
[10/28 12:33:40 visual_prompt]: 	Training 400/1106. train loss: 3.3191,	0.6450 s / batch. (data: 7.85e-04). ETA=18:57:08, max mem: 15.9 GB 
[10/28 12:34:43 visual_prompt]: 	Training 500/1106. train loss: 0.0184,	0.6563 s / batch. (data: 7.84e-04). ETA=19:15:56, max mem: 15.9 GB 
[10/28 12:35:46 visual_prompt]: 	Training 600/1106. train loss: 3.0872,	0.6290 s / batch. (data: 3.18e-04). ETA=18:26:44, max mem: 15.9 GB 
[10/28 12:36:49 visual_prompt]: 	Training 700/1106. train loss: 1.0145,	0.6332 s / batch. (data: 8.49e-04). ETA=18:33:11, max mem: 15.9 GB 
[10/28 12:37:53 visual_prompt]: 	Training 800/1106. train loss: 0.8286,	0.6324 s / batch. (data: 7.71e-04). ETA=18:30:36, max mem: 15.9 GB 
[10/28 12:38:56 visual_prompt]: 	Training 900/1106. train loss: 1.5097,	0.6400 s / batch. (data: 3.05e-04). ETA=18:42:56, max mem: 15.9 GB 
[10/28 12:39:59 visual_prompt]: 	Training 1000/1106. train loss: 0.1083,	0.6329 s / batch. (data: 7.81e-04). ETA=18:29:29, max mem: 15.9 GB 
[10/28 12:41:03 visual_prompt]: 	Training 1100/1106. train loss: 1.7778,	0.6186 s / batch. (data: 1.68e-04). ETA=18:03:22, max mem: 15.9 GB 
[10/28 12:41:07 visual_prompt]: Epoch 5 / 100: avg data time: 4.16e-03, avg batch time: 0.6348, average train loss: 1.8584
[10/28 12:41:57 visual_prompt]: 	Test 100/123. loss: 1.250, 0.2407 s / batch. (data: 4.96e-05)max mem: 15.94594 GB 
[10/28 12:42:08 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2328, average loss: 1.0929
[10/28 12:42:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.29	
[10/28 12:42:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/28 12:43:13 visual_prompt]: 	Training 100/1106. train loss: 0.5588,	0.6248 s / batch. (data: 3.41e-04). ETA=18:13:10, max mem: 15.9 GB 
[10/28 12:44:16 visual_prompt]: 	Training 200/1106. train loss: 0.7721,	0.6325 s / batch. (data: 8.37e-04). ETA=18:25:30, max mem: 15.9 GB 
[10/28 12:45:20 visual_prompt]: 	Training 300/1106. train loss: 0.0002,	0.6418 s / batch. (data: 3.13e-04). ETA=18:40:37, max mem: 15.9 GB 
[10/28 12:46:23 visual_prompt]: 	Training 400/1106. train loss: 0.2135,	0.6204 s / batch. (data: 7.76e-04). ETA=18:02:12, max mem: 15.9 GB 
[10/28 12:47:26 visual_prompt]: 	Training 500/1106. train loss: 1.0049,	0.6236 s / batch. (data: 3.26e-04). ETA=18:06:49, max mem: 15.9 GB 
[10/28 12:48:29 visual_prompt]: 	Training 600/1106. train loss: 0.2881,	0.6184 s / batch. (data: 3.34e-04). ETA=17:56:48, max mem: 15.9 GB 
[10/28 12:49:33 visual_prompt]: 	Training 700/1106. train loss: 1.7662,	0.6336 s / batch. (data: 3.00e-04). ETA=18:22:09, max mem: 15.9 GB 
[10/28 12:50:36 visual_prompt]: 	Training 800/1106. train loss: 3.7140,	0.6316 s / batch. (data: 8.19e-04). ETA=18:17:38, max mem: 15.9 GB 
[10/28 12:51:39 visual_prompt]: 	Training 900/1106. train loss: 4.7178,	0.6498 s / batch. (data: 7.50e-04). ETA=18:48:11, max mem: 15.9 GB 
[10/28 12:52:43 visual_prompt]: 	Training 1000/1106. train loss: 0.0067,	0.6611 s / batch. (data: 3.82e-02). ETA=19:06:39, max mem: 15.9 GB 
[10/28 12:53:46 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6172 s / batch. (data: 1.40e-04). ETA=17:49:34, max mem: 15.9 GB 
[10/28 12:53:49 visual_prompt]: Epoch 6 / 100: avg data time: 4.10e-03, avg batch time: 0.6344, average train loss: 1.9578
[10/28 12:54:39 visual_prompt]: 	Test 100/123. loss: 2.164, 0.2397 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/28 12:54:50 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2318, average loss: 2.5795
[10/28 12:54:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.22	
[10/28 12:54:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/28 12:55:55 visual_prompt]: 	Training 100/1106. train loss: 3.3911,	0.6323 s / batch. (data: 8.32e-04). ETA=18:14:30, max mem: 15.9 GB 
[10/28 12:56:58 visual_prompt]: 	Training 200/1106. train loss: 2.2372,	0.6337 s / batch. (data: 7.70e-04). ETA=18:15:55, max mem: 15.9 GB 
[10/28 12:58:01 visual_prompt]: 	Training 300/1106. train loss: 2.3118,	0.6307 s / batch. (data: 8.99e-04). ETA=18:09:38, max mem: 15.9 GB 
[10/28 12:59:05 visual_prompt]: 	Training 400/1106. train loss: 3.2985,	0.6327 s / batch. (data: 7.64e-04). ETA=18:12:01, max mem: 15.9 GB 
[10/28 13:00:08 visual_prompt]: 	Training 500/1106. train loss: 1.9193,	0.6485 s / batch. (data: 5.44e-03). ETA=18:38:15, max mem: 15.9 GB 
[10/28 13:01:11 visual_prompt]: 	Training 600/1106. train loss: 6.7440,	0.6301 s / batch. (data: 3.45e-04). ETA=18:05:28, max mem: 15.9 GB 
[10/28 13:02:15 visual_prompt]: 	Training 700/1106. train loss: 8.5520,	0.6432 s / batch. (data: 8.03e-04). ETA=18:27:00, max mem: 15.9 GB 
[10/28 13:03:18 visual_prompt]: 	Training 800/1106. train loss: 1.1431,	0.6198 s / batch. (data: 3.24e-04). ETA=17:45:37, max mem: 15.9 GB 
[10/28 13:04:21 visual_prompt]: 	Training 900/1106. train loss: 1.3770,	0.6216 s / batch. (data: 3.22e-04). ETA=17:47:45, max mem: 15.9 GB 
[10/28 13:05:24 visual_prompt]: 	Training 1000/1106. train loss: 2.6135,	0.6566 s / batch. (data: 8.17e-04). ETA=18:46:49, max mem: 15.9 GB 
[10/28 13:06:28 visual_prompt]: 	Training 1100/1106. train loss: 3.9200,	0.6186 s / batch. (data: 1.54e-04). ETA=17:40:26, max mem: 15.9 GB 
[10/28 13:06:31 visual_prompt]: Epoch 7 / 100: avg data time: 4.23e-03, avg batch time: 0.6342, average train loss: 2.0777
[10/28 13:07:21 visual_prompt]: 	Test 100/123. loss: 1.626, 0.2367 s / batch. (data: 4.60e-05)max mem: 15.94594 GB 
[10/28 13:07:32 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2323, average loss: 1.4779
[10/28 13:07:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.23	
[10/28 13:07:32 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/28 13:08:37 visual_prompt]: 	Training 100/1106. train loss: 1.6892,	0.6352 s / batch. (data: 8.14e-04). ETA=18:07:49, max mem: 15.9 GB 
[10/28 13:09:40 visual_prompt]: 	Training 200/1106. train loss: 1.0630,	0.6290 s / batch. (data: 1.09e-02). ETA=17:56:11, max mem: 15.9 GB 
[10/28 13:10:43 visual_prompt]: 	Training 300/1106. train loss: 0.2150,	0.6527 s / batch. (data: 7.86e-04). ETA=18:35:35, max mem: 15.9 GB 
[10/28 13:11:47 visual_prompt]: 	Training 400/1106. train loss: 0.0086,	0.6193 s / batch. (data: 2.39e-04). ETA=17:37:31, max mem: 15.9 GB 
[10/28 13:12:50 visual_prompt]: 	Training 500/1106. train loss: 0.1310,	0.6193 s / batch. (data: 3.59e-04). ETA=17:36:29, max mem: 15.9 GB 
[10/28 13:13:53 visual_prompt]: 	Training 600/1106. train loss: 0.0117,	0.6182 s / batch. (data: 3.19e-04). ETA=17:33:38, max mem: 15.9 GB 
[10/28 13:14:57 visual_prompt]: 	Training 700/1106. train loss: 2.3446,	0.6285 s / batch. (data: 7.70e-04). ETA=17:50:07, max mem: 15.9 GB 
[10/28 13:16:00 visual_prompt]: 	Training 800/1106. train loss: 1.0601,	0.6177 s / batch. (data: 3.02e-04). ETA=17:30:43, max mem: 15.9 GB 
[10/28 13:17:03 visual_prompt]: 	Training 900/1106. train loss: 0.0053,	0.6560 s / batch. (data: 9.32e-04). ETA=18:34:39, max mem: 15.9 GB 
[10/28 13:18:06 visual_prompt]: 	Training 1000/1106. train loss: 0.7635,	0.6216 s / batch. (data: 3.24e-04). ETA=17:35:16, max mem: 15.9 GB 
[10/28 13:19:10 visual_prompt]: 	Training 1100/1106. train loss: 0.2086,	0.6178 s / batch. (data: 1.58e-04). ETA=17:27:46, max mem: 15.9 GB 
[10/28 13:19:13 visual_prompt]: Epoch 8 / 100: avg data time: 3.98e-03, avg batch time: 0.6343, average train loss: 2.3483
[10/28 13:20:04 visual_prompt]: 	Test 100/123. loss: 1.509, 0.2285 s / batch. (data: 5.15e-05)max mem: 15.94594 GB 
[10/28 13:20:14 visual_prompt]: Inference (val):avg data time: 1.08e-04, avg batch time: 0.2328, average loss: 1.3921
[10/28 13:20:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.58	
[10/28 13:20:14 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/28 13:21:19 visual_prompt]: 	Training 100/1106. train loss: 8.1079,	0.6296 s / batch. (data: 3.04e-04). ETA=17:46:41, max mem: 15.9 GB 
[10/28 13:22:23 visual_prompt]: 	Training 200/1106. train loss: 0.1552,	0.6193 s / batch. (data: 3.35e-04). ETA=17:28:13, max mem: 15.9 GB 
[10/28 13:23:26 visual_prompt]: 	Training 300/1106. train loss: 3.0855,	0.6319 s / batch. (data: 8.11e-04). ETA=17:48:23, max mem: 15.9 GB 
[10/28 13:24:29 visual_prompt]: 	Training 400/1106. train loss: 0.2212,	0.6331 s / batch. (data: 7.99e-04). ETA=17:49:22, max mem: 15.9 GB 
[10/28 13:25:32 visual_prompt]: 	Training 500/1106. train loss: 7.8563,	0.6276 s / batch. (data: 3.39e-04). ETA=17:39:07, max mem: 15.9 GB 
[10/28 13:26:36 visual_prompt]: 	Training 600/1106. train loss: 1.2861,	0.6352 s / batch. (data: 7.84e-04). ETA=17:50:53, max mem: 15.9 GB 
[10/28 13:27:39 visual_prompt]: 	Training 700/1106. train loss: 0.0256,	0.6536 s / batch. (data: 7.52e-04). ETA=18:20:49, max mem: 15.9 GB 
[10/28 13:28:42 visual_prompt]: 	Training 800/1106. train loss: 0.9630,	0.6250 s / batch. (data: 3.29e-04). ETA=17:31:33, max mem: 15.9 GB 
[10/28 13:29:46 visual_prompt]: 	Training 900/1106. train loss: 0.1009,	0.6392 s / batch. (data: 7.82e-04). ETA=17:54:20, max mem: 15.9 GB 
[10/28 13:30:49 visual_prompt]: 	Training 1000/1106. train loss: 1.3145,	0.6358 s / batch. (data: 8.04e-04). ETA=17:47:39, max mem: 15.9 GB 
[10/28 13:31:52 visual_prompt]: 	Training 1100/1106. train loss: 0.8482,	0.6178 s / batch. (data: 1.40e-04). ETA=17:16:27, max mem: 15.9 GB 
[10/28 13:31:56 visual_prompt]: Epoch 9 / 100: avg data time: 4.11e-03, avg batch time: 0.6348, average train loss: 1.9363
[10/28 13:32:46 visual_prompt]: 	Test 100/123. loss: 0.717, 0.2275 s / batch. (data: 3.96e-05)max mem: 15.94594 GB 
[10/28 13:32:57 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2320, average loss: 0.7134
[10/28 13:32:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 57.57	
[10/28 13:32:57 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/28 13:34:02 visual_prompt]: 	Training 100/1106. train loss: 1.6047,	0.6182 s / batch. (data: 2.85e-04). ETA=17:16:02, max mem: 15.9 GB 
[10/28 13:35:05 visual_prompt]: 	Training 200/1106. train loss: 10.2207,	0.6247 s / batch. (data: 3.47e-04). ETA=17:25:49, max mem: 15.9 GB 
[10/28 13:36:08 visual_prompt]: 	Training 300/1106. train loss: 5.9247,	0.6594 s / batch. (data: 1.10e-02). ETA=18:22:46, max mem: 15.9 GB 
[10/28 13:37:12 visual_prompt]: 	Training 400/1106. train loss: 0.2654,	0.6321 s / batch. (data: 7.92e-04). ETA=17:36:05, max mem: 15.9 GB 
[10/28 13:38:15 visual_prompt]: 	Training 500/1106. train loss: 0.7783,	0.6520 s / batch. (data: 1.20e-02). ETA=18:08:17, max mem: 15.9 GB 
[10/28 13:39:18 visual_prompt]: 	Training 600/1106. train loss: 4.9251,	0.6228 s / batch. (data: 3.28e-04). ETA=17:18:25, max mem: 15.9 GB 
[10/28 13:40:21 visual_prompt]: 	Training 700/1106. train loss: 6.3264,	0.6327 s / batch. (data: 8.01e-04). ETA=17:33:52, max mem: 15.9 GB 
[10/28 13:41:25 visual_prompt]: 	Training 800/1106. train loss: 3.1524,	0.6400 s / batch. (data: 3.07e-04). ETA=17:45:03, max mem: 15.9 GB 
[10/28 13:42:28 visual_prompt]: 	Training 900/1106. train loss: 5.7853,	0.6608 s / batch. (data: 2.88e-02). ETA=18:18:32, max mem: 15.9 GB 
[10/28 13:43:31 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6182 s / batch. (data: 3.10e-04). ETA=17:06:37, max mem: 15.9 GB 
[10/28 13:44:34 visual_prompt]: 	Training 1100/1106. train loss: 0.2865,	0.6182 s / batch. (data: 1.69e-04). ETA=17:05:42, max mem: 15.9 GB 
[10/28 13:44:38 visual_prompt]: Epoch 10 / 100: avg data time: 4.06e-03, avg batch time: 0.6342, average train loss: 3.0496
[10/28 13:45:28 visual_prompt]: 	Test 100/123. loss: 0.882, 0.2400 s / batch. (data: 3.72e-05)max mem: 15.94594 GB 
[10/28 13:45:38 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2319, average loss: 0.8263
[10/28 13:45:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.43	
[10/28 13:45:38 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/28 13:46:44 visual_prompt]: 	Training 100/1106. train loss: 13.5276,	0.6181 s / batch. (data: 3.19e-04). ETA=17:04:25, max mem: 15.9 GB 
[10/28 13:47:47 visual_prompt]: 	Training 200/1106. train loss: 1.0750,	0.6397 s / batch. (data: 7.71e-04). ETA=17:39:04, max mem: 15.9 GB 
[10/28 13:48:50 visual_prompt]: 	Training 300/1106. train loss: 6.8304,	0.6180 s / batch. (data: 2.83e-04). ETA=17:02:08, max mem: 15.9 GB 
[10/28 13:49:53 visual_prompt]: 	Training 400/1106. train loss: 0.8800,	0.6190 s / batch. (data: 3.34e-04). ETA=17:02:51, max mem: 15.9 GB 
[10/28 13:50:56 visual_prompt]: 	Training 500/1106. train loss: 0.9848,	0.6190 s / batch. (data: 3.32e-04). ETA=17:01:47, max mem: 15.9 GB 
[10/28 13:52:00 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6365 s / batch. (data: 8.29e-04). ETA=17:29:35, max mem: 15.9 GB 
[10/28 13:53:03 visual_prompt]: 	Training 700/1106. train loss: 0.1307,	0.6328 s / batch. (data: 8.28e-04). ETA=17:22:26, max mem: 15.9 GB 
[10/28 13:54:06 visual_prompt]: 	Training 800/1106. train loss: 5.9261,	0.6360 s / batch. (data: 3.23e-04). ETA=17:26:36, max mem: 15.9 GB 
[10/28 13:55:09 visual_prompt]: 	Training 900/1106. train loss: 0.0209,	0.6290 s / batch. (data: 8.93e-04). ETA=17:13:59, max mem: 15.9 GB 
[10/28 13:56:13 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6352 s / batch. (data: 1.20e-02). ETA=17:23:08, max mem: 15.9 GB 
[10/28 13:57:16 visual_prompt]: 	Training 1100/1106. train loss: 1.5478,	0.6193 s / batch. (data: 2.68e-04). ETA=16:56:03, max mem: 15.9 GB 
[10/28 13:57:20 visual_prompt]: Epoch 11 / 100: avg data time: 4.13e-03, avg batch time: 0.6341, average train loss: 2.6096
[10/28 13:58:10 visual_prompt]: 	Test 100/123. loss: 1.645, 0.2357 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[10/28 13:58:20 visual_prompt]: Inference (val):avg data time: 1.03e-04, avg batch time: 0.2319, average loss: 1.9402
[10/28 13:58:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.35	
[10/28 13:58:20 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/28 13:59:26 visual_prompt]: 	Training 100/1106. train loss: 0.0322,	0.6329 s / batch. (data: 3.33e-04). ETA=17:17:15, max mem: 15.9 GB 
[10/28 14:00:30 visual_prompt]: 	Training 200/1106. train loss: 0.8805,	0.6345 s / batch. (data: 8.16e-04). ETA=17:18:51, max mem: 15.9 GB 
[10/28 14:01:33 visual_prompt]: 	Training 300/1106. train loss: 2.4062,	0.6326 s / batch. (data: 8.80e-04). ETA=17:14:41, max mem: 15.9 GB 
[10/28 14:02:36 visual_prompt]: 	Training 400/1106. train loss: 0.6744,	0.6338 s / batch. (data: 7.66e-04). ETA=17:15:34, max mem: 15.9 GB 
[10/28 14:03:39 visual_prompt]: 	Training 500/1106. train loss: 1.4419,	0.6328 s / batch. (data: 8.10e-04). ETA=17:12:55, max mem: 15.9 GB 
[10/28 14:04:43 visual_prompt]: 	Training 600/1106. train loss: 0.7402,	0.6195 s / batch. (data: 3.03e-04). ETA=16:50:08, max mem: 15.9 GB 
[10/28 14:05:46 visual_prompt]: 	Training 700/1106. train loss: 2.4860,	0.6481 s / batch. (data: 7.48e-04). ETA=17:35:42, max mem: 15.9 GB 
[10/28 14:06:49 visual_prompt]: 	Training 800/1106. train loss: 1.8392,	0.6470 s / batch. (data: 8.07e-04). ETA=17:32:45, max mem: 15.9 GB 
[10/28 14:07:53 visual_prompt]: 	Training 900/1106. train loss: 4.2463,	0.6314 s / batch. (data: 3.18e-04). ETA=17:06:22, max mem: 15.9 GB 
[10/28 14:08:56 visual_prompt]: 	Training 1000/1106. train loss: 4.8977,	0.6330 s / batch. (data: 8.20e-04). ETA=17:07:59, max mem: 15.9 GB 
[10/28 14:09:59 visual_prompt]: 	Training 1100/1106. train loss: 5.8570,	0.6192 s / batch. (data: 1.48e-04). ETA=16:44:26, max mem: 15.9 GB 
[10/28 14:10:03 visual_prompt]: Epoch 12 / 100: avg data time: 5.18e-03, avg batch time: 0.6354, average train loss: 2.6720
[10/28 14:10:53 visual_prompt]: 	Test 100/123. loss: 1.230, 0.2315 s / batch. (data: 3.72e-05)max mem: 15.94594 GB 
[10/28 14:11:04 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.2315, average loss: 0.9789
[10/28 14:11:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 55.04	
[10/28 14:11:04 visual_prompt]: Best epoch 12: best metric: -0.979
[10/28 14:11:04 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/28 14:12:09 visual_prompt]: 	Training 100/1106. train loss: 45.0064,	0.6327 s / batch. (data: 1.11e-03). ETA=17:05:13, max mem: 15.9 GB 
[10/28 14:13:12 visual_prompt]: 	Training 200/1106. train loss: 1.6621,	0.6183 s / batch. (data: 2.42e-04). ETA=16:40:51, max mem: 15.9 GB 
[10/28 14:14:15 visual_prompt]: 	Training 300/1106. train loss: 0.0043,	0.6197 s / batch. (data: 3.26e-04). ETA=16:42:09, max mem: 15.9 GB 
[10/28 14:15:19 visual_prompt]: 	Training 400/1106. train loss: 6.5002,	0.6198 s / batch. (data: 3.18e-04). ETA=16:41:11, max mem: 15.9 GB 
[10/28 14:16:22 visual_prompt]: 	Training 500/1106. train loss: 0.0018,	0.6344 s / batch. (data: 8.64e-04). ETA=17:03:48, max mem: 15.9 GB 
[10/28 14:17:26 visual_prompt]: 	Training 600/1106. train loss: 2.1363,	0.6244 s / batch. (data: 2.64e-04). ETA=16:46:41, max mem: 15.9 GB 
[10/28 14:18:29 visual_prompt]: 	Training 700/1106. train loss: 0.3936,	0.6319 s / batch. (data: 7.18e-04). ETA=16:57:43, max mem: 15.9 GB 
[10/28 14:19:32 visual_prompt]: 	Training 800/1106. train loss: 0.6646,	0.6187 s / batch. (data: 3.39e-04). ETA=16:35:22, max mem: 15.9 GB 
[10/28 14:20:36 visual_prompt]: 	Training 900/1106. train loss: 10.8482,	0.6357 s / batch. (data: 7.68e-04). ETA=17:01:36, max mem: 15.9 GB 
[10/28 14:21:39 visual_prompt]: 	Training 1000/1106. train loss: 3.7711,	0.6348 s / batch. (data: 7.93e-04). ETA=16:59:05, max mem: 15.9 GB 
[10/28 14:22:42 visual_prompt]: 	Training 1100/1106. train loss: 1.3605,	0.6198 s / batch. (data: 1.85e-04). ETA=16:34:00, max mem: 15.9 GB 
[10/28 14:22:46 visual_prompt]: Epoch 13 / 100: avg data time: 4.41e-03, avg batch time: 0.6352, average train loss: 2.4572
[10/28 14:23:36 visual_prompt]: 	Test 100/123. loss: 1.480, 0.2318 s / batch. (data: 4.89e-05)max mem: 15.94594 GB 
[10/28 14:23:47 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2330, average loss: 1.3537
[10/28 14:23:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.61	
[10/28 14:23:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/28 14:24:52 visual_prompt]: 	Training 100/1106. train loss: 0.0002,	0.6327 s / batch. (data: 8.75e-04). ETA=16:53:40, max mem: 15.9 GB 
[10/28 14:25:56 visual_prompt]: 	Training 200/1106. train loss: 2.8779,	0.6339 s / batch. (data: 8.09e-04). ETA=16:54:32, max mem: 15.9 GB 
[10/28 14:26:59 visual_prompt]: 	Training 300/1106. train loss: 3.4343,	0.6312 s / batch. (data: 3.11e-04). ETA=16:49:05, max mem: 15.9 GB 
[10/28 14:28:02 visual_prompt]: 	Training 400/1106. train loss: 0.0002,	0.6198 s / batch. (data: 8.19e-04). ETA=16:29:46, max mem: 15.9 GB 
[10/28 14:29:06 visual_prompt]: 	Training 500/1106. train loss: 1.1317,	0.6192 s / batch. (data: 3.20e-04). ETA=16:27:51, max mem: 15.9 GB 
[10/28 14:30:09 visual_prompt]: 	Training 600/1106. train loss: 0.9566,	0.6209 s / batch. (data: 3.42e-04). ETA=16:29:32, max mem: 15.9 GB 
[10/28 14:31:12 visual_prompt]: 	Training 700/1106. train loss: 0.0429,	0.6361 s / batch. (data: 1.21e-03). ETA=16:52:41, max mem: 15.9 GB 
[10/28 14:32:16 visual_prompt]: 	Training 800/1106. train loss: 2.4908,	0.6191 s / batch. (data: 3.16e-04). ETA=16:24:39, max mem: 15.9 GB 
[10/28 14:33:19 visual_prompt]: 	Training 900/1106. train loss: 0.9144,	0.6339 s / batch. (data: 7.42e-04). ETA=16:47:01, max mem: 15.9 GB 
[10/28 14:34:23 visual_prompt]: 	Training 1000/1106. train loss: 3.7190,	0.6607 s / batch. (data: 7.63e-04). ETA=17:28:37, max mem: 15.9 GB 
[10/28 14:35:26 visual_prompt]: 	Training 1100/1106. train loss: 1.6038,	0.6190 s / batch. (data: 1.46e-04). ETA=16:21:16, max mem: 15.9 GB 
[10/28 14:35:30 visual_prompt]: Epoch 14 / 100: avg data time: 4.16e-03, avg batch time: 0.6354, average train loss: 2.6680
[10/28 14:36:20 visual_prompt]: 	Test 100/123. loss: 3.052, 0.2353 s / batch. (data: 3.91e-05)max mem: 15.94594 GB 
[10/28 14:36:30 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.2313, average loss: 2.6761
[10/28 14:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.90	
[10/28 14:36:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/28 14:37:36 visual_prompt]: 	Training 100/1106. train loss: 9.6108,	0.6401 s / batch. (data: 8.22e-04). ETA=16:53:43, max mem: 15.9 GB 
[10/28 14:38:39 visual_prompt]: 	Training 200/1106. train loss: 1.6843,	0.6328 s / batch. (data: 7.50e-04). ETA=16:41:04, max mem: 15.9 GB 
[10/28 14:39:42 visual_prompt]: 	Training 300/1106. train loss: 6.9378,	0.6359 s / batch. (data: 8.28e-04). ETA=16:44:55, max mem: 15.9 GB 
[10/28 14:40:46 visual_prompt]: 	Training 400/1106. train loss: 19.6584,	0.6375 s / batch. (data: 8.41e-04). ETA=16:46:24, max mem: 15.9 GB 
[10/28 14:41:49 visual_prompt]: 	Training 500/1106. train loss: 0.3195,	0.6337 s / batch. (data: 1.64e-02). ETA=16:39:20, max mem: 15.9 GB 
[10/28 14:42:53 visual_prompt]: 	Training 600/1106. train loss: 4.3348,	0.6182 s / batch. (data: 3.17e-04). ETA=16:13:53, max mem: 15.9 GB 
[10/28 14:43:56 visual_prompt]: 	Training 700/1106. train loss: 4.9189,	0.6356 s / batch. (data: 7.75e-04). ETA=16:40:07, max mem: 15.9 GB 
[10/28 14:44:59 visual_prompt]: 	Training 800/1106. train loss: 1.0423,	0.6570 s / batch. (data: 1.11e-02). ETA=17:12:45, max mem: 15.9 GB 
[10/28 14:46:02 visual_prompt]: 	Training 900/1106. train loss: 4.5039,	0.6300 s / batch. (data: 7.50e-04). ETA=16:29:19, max mem: 15.9 GB 
[10/28 14:47:06 visual_prompt]: 	Training 1000/1106. train loss: 2.7969,	0.6353 s / batch. (data: 7.63e-04). ETA=16:36:31, max mem: 15.9 GB 
[10/28 14:48:09 visual_prompt]: 	Training 1100/1106. train loss: 3.3686,	0.6194 s / batch. (data: 1.43e-04). ETA=16:10:34, max mem: 15.9 GB 
[10/28 14:48:13 visual_prompt]: Epoch 15 / 100: avg data time: 4.16e-03, avg batch time: 0.6352, average train loss: 2.9289
[10/28 14:49:03 visual_prompt]: 	Test 100/123. loss: 1.881, 0.2257 s / batch. (data: 4.41e-05)max mem: 15.94594 GB 
[10/28 14:49:13 visual_prompt]: Inference (val):avg data time: 4.74e-05, avg batch time: 0.2328, average loss: 2.2953
[10/28 14:49:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.65	
[10/28 14:49:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/28 14:50:18 visual_prompt]: 	Training 100/1106. train loss: 0.0140,	0.6178 s / batch. (data: 3.06e-04). ETA=16:07:00, max mem: 15.9 GB 
[10/28 14:51:22 visual_prompt]: 	Training 200/1106. train loss: 3.8167,	0.6387 s / batch. (data: 8.28e-04). ETA=16:38:34, max mem: 15.9 GB 
[10/28 14:52:25 visual_prompt]: 	Training 300/1106. train loss: 0.0002,	0.6202 s / batch. (data: 3.25e-04). ETA=16:08:34, max mem: 15.9 GB 
[10/28 14:53:28 visual_prompt]: 	Training 400/1106. train loss: 3.5748,	0.6387 s / batch. (data: 8.12e-04). ETA=16:36:24, max mem: 15.9 GB 
[10/28 14:54:32 visual_prompt]: 	Training 500/1106. train loss: 3.8876,	0.6419 s / batch. (data: 3.09e-04). ETA=16:40:22, max mem: 15.9 GB 
[10/28 14:55:35 visual_prompt]: 	Training 600/1106. train loss: 0.3326,	0.6357 s / batch. (data: 1.05e-03). ETA=16:29:44, max mem: 15.9 GB 
[10/28 14:56:38 visual_prompt]: 	Training 700/1106. train loss: 2.8690,	0.6209 s / batch. (data: 2.85e-04). ETA=16:05:31, max mem: 15.9 GB 
[10/28 14:57:42 visual_prompt]: 	Training 800/1106. train loss: 3.4024,	0.6485 s / batch. (data: 5.44e-03). ETA=16:47:22, max mem: 15.9 GB 
[10/28 14:58:45 visual_prompt]: 	Training 900/1106. train loss: 0.5044,	0.6194 s / batch. (data: 3.19e-04). ETA=16:01:11, max mem: 15.9 GB 
[10/28 14:59:48 visual_prompt]: 	Training 1000/1106. train loss: 2.7046,	0.6334 s / batch. (data: 8.13e-04). ETA=16:21:51, max mem: 15.9 GB 
[10/28 15:00:51 visual_prompt]: 	Training 1100/1106. train loss: 0.0003,	0.6191 s / batch. (data: 1.59e-04). ETA=15:58:38, max mem: 15.9 GB 
[10/28 15:00:55 visual_prompt]: Epoch 16 / 100: avg data time: 4.04e-03, avg batch time: 0.6347, average train loss: 2.3075
[10/28 15:01:45 visual_prompt]: 	Test 100/123. loss: 1.313, 0.2260 s / batch. (data: 2.60e-05)max mem: 15.94594 GB 
[10/28 15:01:56 visual_prompt]: Inference (val):avg data time: 9.68e-05, avg batch time: 0.2321, average loss: 1.1876
[10/28 15:01:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.24	
[10/28 15:01:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/28 15:03:02 visual_prompt]: 	Training 100/1106. train loss: 0.4244,	0.6228 s / batch. (data: 5.47e-03). ETA=16:03:16, max mem: 15.9 GB 
[10/28 15:04:05 visual_prompt]: 	Training 200/1106. train loss: 0.2644,	0.6484 s / batch. (data: 8.54e-04). ETA=16:41:45, max mem: 15.9 GB 
[10/28 15:05:08 visual_prompt]: 	Training 300/1106. train loss: 0.8923,	0.6320 s / batch. (data: 7.76e-04). ETA=16:15:21, max mem: 15.9 GB 
[10/28 15:06:11 visual_prompt]: 	Training 400/1106. train loss: 20.1638,	0.6320 s / batch. (data: 3.10e-04). ETA=16:14:23, max mem: 15.9 GB 
[10/28 15:07:15 visual_prompt]: 	Training 500/1106. train loss: 1.7116,	0.6323 s / batch. (data: 3.42e-04). ETA=16:13:44, max mem: 15.9 GB 
[10/28 15:08:18 visual_prompt]: 	Training 600/1106. train loss: 2.2103,	0.6396 s / batch. (data: 1.05e-02). ETA=16:23:57, max mem: 15.9 GB 
[10/28 15:09:21 visual_prompt]: 	Training 700/1106. train loss: 1.6504,	0.6277 s / batch. (data: 3.92e-04). ETA=16:04:39, max mem: 15.9 GB 
[10/28 15:10:25 visual_prompt]: 	Training 800/1106. train loss: 7.9277,	0.6251 s / batch. (data: 7.93e-04). ETA=15:59:38, max mem: 15.9 GB 
[10/28 15:11:28 visual_prompt]: 	Training 900/1106. train loss: 6.6455,	0.6197 s / batch. (data: 2.91e-04). ETA=15:50:14, max mem: 15.9 GB 
[10/28 15:12:31 visual_prompt]: 	Training 1000/1106. train loss: 0.0026,	0.6396 s / batch. (data: 7.84e-04). ETA=16:19:39, max mem: 15.9 GB 
[10/28 15:13:35 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6191 s / batch. (data: 1.53e-04). ETA=15:47:15, max mem: 15.9 GB 
[10/28 15:13:39 visual_prompt]: Epoch 17 / 100: avg data time: 4.64e-03, avg batch time: 0.6352, average train loss: 2.4951
[10/28 15:14:28 visual_prompt]: 	Test 100/123. loss: 3.174, 0.2266 s / batch. (data: 2.81e-05)max mem: 15.94594 GB 
[10/28 15:14:39 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2328, average loss: 3.7900
[10/28 15:14:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.51	
[10/28 15:14:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/28 15:15:45 visual_prompt]: 	Training 100/1106. train loss: 0.0005,	0.6374 s / batch. (data: 5.98e-03). ETA=16:14:04, max mem: 15.9 GB 
[10/28 15:16:48 visual_prompt]: 	Training 200/1106. train loss: 12.5186,	0.6348 s / batch. (data: 3.15e-04). ETA=16:09:06, max mem: 15.9 GB 
[10/28 15:17:51 visual_prompt]: 	Training 300/1106. train loss: 16.6036,	0.6198 s / batch. (data: 2.90e-04). ETA=15:45:07, max mem: 15.9 GB 
[10/28 15:18:54 visual_prompt]: 	Training 400/1106. train loss: 2.9102,	0.6585 s / batch. (data: 2.81e-02). ETA=16:43:03, max mem: 15.9 GB 
[10/28 15:19:58 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6423 s / batch. (data: 8.05e-04). ETA=16:17:24, max mem: 15.9 GB 
[10/28 15:21:01 visual_prompt]: 	Training 600/1106. train loss: 0.0029,	0.6553 s / batch. (data: 1.03e-03). ETA=16:35:59, max mem: 15.9 GB 
[10/28 15:22:05 visual_prompt]: 	Training 700/1106. train loss: 0.0888,	0.6304 s / batch. (data: 3.18e-04). ETA=15:57:12, max mem: 15.9 GB 
[10/28 15:23:08 visual_prompt]: 	Training 800/1106. train loss: 0.3063,	0.6199 s / batch. (data: 2.86e-04). ETA=15:40:08, max mem: 15.9 GB 
[10/28 15:24:11 visual_prompt]: 	Training 900/1106. train loss: 11.2906,	0.6400 s / batch. (data: 7.88e-04). ETA=16:09:33, max mem: 15.9 GB 
[10/28 15:25:14 visual_prompt]: 	Training 1000/1106. train loss: 1.4641,	0.6273 s / batch. (data: 3.29e-04). ETA=15:49:15, max mem: 15.9 GB 
[10/28 15:26:18 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6187 s / batch. (data: 1.51e-04). ETA=15:35:14, max mem: 15.9 GB 
[10/28 15:26:21 visual_prompt]: Epoch 18 / 100: avg data time: 4.39e-03, avg batch time: 0.6349, average train loss: 2.7069
[10/28 15:27:11 visual_prompt]: 	Test 100/123. loss: 1.035, 0.2278 s / batch. (data: 4.79e-05)max mem: 15.94594 GB 
[10/28 15:27:22 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2326, average loss: 2.0006
[10/28 15:27:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.50	
[10/28 15:27:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/28 15:28:27 visual_prompt]: 	Training 100/1106. train loss: 0.0079,	0.6414 s / batch. (data: 1.10e-02). ETA=16:08:25, max mem: 15.9 GB 
[10/28 15:29:30 visual_prompt]: 	Training 200/1106. train loss: 4.5564,	0.6459 s / batch. (data: 3.75e-04). ETA=16:14:10, max mem: 15.9 GB 
[10/28 15:30:34 visual_prompt]: 	Training 300/1106. train loss: 0.6582,	0.6520 s / batch. (data: 7.94e-04). ETA=16:22:18, max mem: 15.9 GB 
[10/28 15:31:37 visual_prompt]: 	Training 400/1106. train loss: 1.0718,	0.6422 s / batch. (data: 8.21e-04). ETA=16:06:29, max mem: 15.9 GB 
[10/28 15:32:40 visual_prompt]: 	Training 500/1106. train loss: 2.6208,	0.6583 s / batch. (data: 2.77e-02). ETA=16:29:33, max mem: 15.9 GB 
[10/28 15:33:44 visual_prompt]: 	Training 600/1106. train loss: 0.0015,	0.6332 s / batch. (data: 8.21e-04). ETA=15:50:47, max mem: 15.9 GB 
[10/28 15:34:47 visual_prompt]: 	Training 700/1106. train loss: 1.4700,	0.6348 s / batch. (data: 3.24e-04). ETA=15:52:11, max mem: 15.9 GB 
[10/28 15:35:50 visual_prompt]: 	Training 800/1106. train loss: 0.3697,	0.6193 s / batch. (data: 4.41e-04). ETA=15:27:47, max mem: 15.9 GB 
[10/28 15:36:54 visual_prompt]: 	Training 900/1106. train loss: 0.0125,	0.6435 s / batch. (data: 1.09e-03). ETA=16:03:05, max mem: 15.9 GB 
[10/28 15:37:57 visual_prompt]: 	Training 1000/1106. train loss: 0.0041,	0.6441 s / batch. (data: 8.18e-04). ETA=16:02:54, max mem: 15.9 GB 
[10/28 15:39:00 visual_prompt]: 	Training 1100/1106. train loss: 5.8417,	0.6302 s / batch. (data: 1.73e-04). ETA=15:40:57, max mem: 15.9 GB 
[10/28 15:39:04 visual_prompt]: Epoch 19 / 100: avg data time: 3.94e-03, avg batch time: 0.6345, average train loss: 2.5669
[10/28 15:39:53 visual_prompt]: 	Test 100/123. loss: 12.464, 0.2269 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/28 15:40:04 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2326, average loss: 11.1285
[10/28 15:40:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.91	
[10/28 15:40:04 visual_prompt]: Stopping early.
[10/28 15:40:04 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 15:40:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 15:40:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 15:40:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 15:40:04 visual_prompt]: Training with config:
[10/28 15:40:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 15:40:04 visual_prompt]: Loading training data...
[10/28 15:40:04 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 15:40:04 visual_prompt]: Loading validation data...
[10/28 15:40:04 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 15:40:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/28 15:40:07 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/28 15:40:07 visual_prompt]: tuned percent:0.522
[10/28 15:40:07 visual_prompt]: Device used for model: 0
[10/28 15:40:07 visual_prompt]: Setting up Evaluator...
[10/28 15:40:07 visual_prompt]: Setting up Trainer...
[10/28 15:40:07 visual_prompt]: 	Setting up the optimizer...
[10/28 15:40:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 15:41:13 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6474 s / batch. (data: 5.93e-03). ETA=19:52:18, max mem: 15.9 GB 
[10/28 15:42:16 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6373 s / batch. (data: 7.55e-04). ETA=19:32:36, max mem: 15.9 GB 
[10/28 15:43:19 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6291 s / batch. (data: 3.39e-04). ETA=19:16:31, max mem: 15.9 GB 
[10/28 15:44:22 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6391 s / batch. (data: 8.36e-04). ETA=19:33:49, max mem: 15.9 GB 
[10/28 15:45:26 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6240 s / batch. (data: 3.15e-04). ETA=19:05:01, max mem: 15.9 GB 
[10/28 15:46:29 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6228 s / batch. (data: 3.13e-04). ETA=19:01:53, max mem: 15.9 GB 
[10/28 15:47:33 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6195 s / batch. (data: 3.29e-04). ETA=18:54:39, max mem: 15.9 GB 
[10/28 15:48:36 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6285 s / batch. (data: 8.57e-04). ETA=19:10:06, max mem: 15.9 GB 
[10/28 15:49:39 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6640 s / batch. (data: 7.64e-04). ETA=20:13:59, max mem: 15.9 GB 
[10/28 15:50:43 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6189 s / batch. (data: 3.13e-04). ETA=18:50:27, max mem: 15.9 GB 
[10/28 15:51:46 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6176 s / batch. (data: 2.18e-04). ETA=18:47:04, max mem: 15.9 GB 
[10/28 15:51:50 visual_prompt]: Epoch 1 / 100: avg data time: 5.15e-03, avg batch time: 0.6355, average train loss: 1.4028
[10/28 15:52:40 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2257 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/28 15:52:50 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2322, average loss: 1.3505
[10/28 15:52:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/28 15:52:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/28 15:53:56 visual_prompt]: 	Training 100/1106. train loss: 1.1258,	0.6319 s / batch. (data: 2.98e-04). ETA=19:12:07, max mem: 15.9 GB 
[10/28 15:54:59 visual_prompt]: 	Training 200/1106. train loss: 0.9552,	0.6459 s / batch. (data: 1.06e-03). ETA=19:36:34, max mem: 15.9 GB 
[10/28 15:56:02 visual_prompt]: 	Training 300/1106. train loss: 0.6296,	0.6190 s / batch. (data: 3.28e-04). ETA=18:46:27, max mem: 15.9 GB 
[10/28 15:57:05 visual_prompt]: 	Training 400/1106. train loss: 0.2130,	0.6197 s / batch. (data: 3.20e-04). ETA=18:46:41, max mem: 15.9 GB 
[10/28 15:58:09 visual_prompt]: 	Training 500/1106. train loss: 0.8665,	0.6311 s / batch. (data: 3.12e-04). ETA=19:06:30, max mem: 15.9 GB 
[10/28 15:59:12 visual_prompt]: 	Training 600/1106. train loss: 0.7207,	0.6384 s / batch. (data: 5.45e-03). ETA=19:18:40, max mem: 15.9 GB 
[10/28 16:00:16 visual_prompt]: 	Training 700/1106. train loss: 0.9928,	0.6489 s / batch. (data: 8.31e-04). ETA=19:36:36, max mem: 15.9 GB 
[10/28 16:01:19 visual_prompt]: 	Training 800/1106. train loss: 0.8697,	0.6305 s / batch. (data: 3.29e-04). ETA=19:02:15, max mem: 15.9 GB 
[10/28 16:02:22 visual_prompt]: 	Training 900/1106. train loss: 1.0276,	0.6442 s / batch. (data: 1.02e-03). ETA=19:25:58, max mem: 15.9 GB 
[10/28 16:03:26 visual_prompt]: 	Training 1000/1106. train loss: 0.2497,	0.6320 s / batch. (data: 3.09e-04). ETA=19:02:47, max mem: 15.9 GB 
[10/28 16:04:29 visual_prompt]: 	Training 1100/1106. train loss: 0.3264,	0.6193 s / batch. (data: 1.28e-04). ETA=18:38:48, max mem: 15.9 GB 
[10/28 16:04:33 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-03, avg batch time: 0.6347, average train loss: 0.8545
[10/28 16:05:22 visual_prompt]: 	Test 100/123. loss: 1.520, 0.2254 s / batch. (data: 3.53e-05)max mem: 15.94594 GB 
[10/28 16:05:33 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.2325, average loss: 1.3765
[10/28 16:05:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.24	
[10/28 16:05:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/28 16:06:40 visual_prompt]: 	Training 100/1106. train loss: 1.4676,	0.6177 s / batch. (data: 3.20e-04). ETA=18:34:47, max mem: 15.9 GB 
[10/28 16:07:43 visual_prompt]: 	Training 200/1106. train loss: 0.4454,	0.6663 s / batch. (data: 1.93e-02). ETA=20:01:25, max mem: 15.9 GB 
[10/28 16:08:46 visual_prompt]: 	Training 300/1106. train loss: 0.2804,	0.6360 s / batch. (data: 3.29e-04). ETA=19:05:41, max mem: 15.9 GB 
[10/28 16:09:49 visual_prompt]: 	Training 400/1106. train loss: 0.7013,	0.6417 s / batch. (data: 2.97e-04). ETA=19:14:51, max mem: 15.9 GB 
[10/28 16:10:53 visual_prompt]: 	Training 500/1106. train loss: 1.1362,	0.6354 s / batch. (data: 1.24e-03). ETA=19:02:36, max mem: 15.9 GB 
[10/28 16:11:56 visual_prompt]: 	Training 600/1106. train loss: 0.1297,	0.6196 s / batch. (data: 2.73e-04). ETA=18:33:01, max mem: 15.9 GB 
[10/28 16:13:00 visual_prompt]: 	Training 700/1106. train loss: 2.1399,	0.6456 s / batch. (data: 7.93e-04). ETA=19:18:38, max mem: 15.9 GB 
[10/28 16:14:03 visual_prompt]: 	Training 800/1106. train loss: 0.6783,	0.6589 s / batch. (data: 9.19e-04). ETA=19:41:26, max mem: 15.9 GB 
[10/28 16:15:07 visual_prompt]: 	Training 900/1106. train loss: 3.5694,	0.6309 s / batch. (data: 1.05e-02). ETA=18:50:17, max mem: 15.9 GB 
[10/28 16:16:10 visual_prompt]: 	Training 1000/1106. train loss: 0.7114,	0.6430 s / batch. (data: 7.62e-04). ETA=19:10:45, max mem: 15.9 GB 
[10/28 16:17:13 visual_prompt]: 	Training 1100/1106. train loss: 0.7054,	0.6198 s / batch. (data: 2.45e-04). ETA=18:28:12, max mem: 15.9 GB 
[10/28 16:17:17 visual_prompt]: Epoch 3 / 100: avg data time: 5.08e-03, avg batch time: 0.6363, average train loss: 0.9124
[10/28 16:18:07 visual_prompt]: 	Test 100/123. loss: 0.804, 0.2397 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/28 16:18:18 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2337, average loss: 0.8507
[10/28 16:18:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.68	
[10/28 16:18:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/28 16:19:23 visual_prompt]: 	Training 100/1106. train loss: 0.2975,	0.6422 s / batch. (data: 8.47e-04). ETA=19:07:11, max mem: 15.9 GB 
[10/28 16:20:27 visual_prompt]: 	Training 200/1106. train loss: 3.0613,	0.6241 s / batch. (data: 3.20e-04). ETA=18:33:46, max mem: 15.9 GB 
[10/28 16:21:30 visual_prompt]: 	Training 300/1106. train loss: 0.8869,	0.6440 s / batch. (data: 7.80e-04). ETA=19:08:21, max mem: 15.9 GB 
[10/28 16:22:33 visual_prompt]: 	Training 400/1106. train loss: 0.6706,	0.6504 s / batch. (data: 7.88e-04). ETA=19:18:36, max mem: 15.9 GB 
[10/28 16:23:37 visual_prompt]: 	Training 500/1106. train loss: 1.0229,	0.6338 s / batch. (data: 3.15e-04). ETA=18:47:55, max mem: 15.9 GB 
[10/28 16:24:40 visual_prompt]: 	Training 600/1106. train loss: 0.5466,	0.6421 s / batch. (data: 8.17e-04). ETA=19:01:42, max mem: 15.9 GB 
[10/28 16:25:43 visual_prompt]: 	Training 700/1106. train loss: 2.3169,	0.6198 s / batch. (data: 3.20e-04). ETA=18:20:57, max mem: 15.9 GB 
[10/28 16:26:46 visual_prompt]: 	Training 800/1106. train loss: 0.8381,	0.6297 s / batch. (data: 2.93e-04). ETA=18:37:32, max mem: 15.9 GB 
[10/28 16:27:50 visual_prompt]: 	Training 900/1106. train loss: 1.2068,	0.6440 s / batch. (data: 8.32e-04). ETA=19:01:52, max mem: 15.9 GB 
[10/28 16:28:53 visual_prompt]: 	Training 1000/1106. train loss: 0.8047,	0.6231 s / batch. (data: 3.14e-04). ETA=18:23:41, max mem: 15.9 GB 
[10/28 16:29:56 visual_prompt]: 	Training 1100/1106. train loss: 1.6338,	0.6185 s / batch. (data: 1.58e-04). ETA=18:14:37, max mem: 15.9 GB 
[10/28 16:30:00 visual_prompt]: Epoch 4 / 100: avg data time: 4.84e-03, avg batch time: 0.6347, average train loss: 1.0939
[10/28 16:30:50 visual_prompt]: 	Test 100/123. loss: 0.696, 0.2524 s / batch. (data: 3.98e-05)max mem: 15.94594 GB 
[10/28 16:31:00 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2325, average loss: 0.7012
[10/28 16:31:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.77	
[10/28 16:31:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/28 16:32:05 visual_prompt]: 	Training 100/1106. train loss: 0.7351,	0.6256 s / batch. (data: 3.47e-04). ETA=18:26:04, max mem: 15.9 GB 
[10/28 16:33:09 visual_prompt]: 	Training 200/1106. train loss: 0.0185,	0.6222 s / batch. (data: 2.90e-04). ETA=18:18:53, max mem: 15.9 GB 
[10/28 16:34:12 visual_prompt]: 	Training 300/1106. train loss: 0.6763,	0.6302 s / batch. (data: 1.13e-03). ETA=18:31:59, max mem: 15.9 GB 
[10/28 16:35:15 visual_prompt]: 	Training 400/1106. train loss: 2.2560,	0.6383 s / batch. (data: 6.36e-03). ETA=18:45:12, max mem: 15.9 GB 
[10/28 16:36:19 visual_prompt]: 	Training 500/1106. train loss: 0.5391,	0.6322 s / batch. (data: 8.44e-04). ETA=18:33:25, max mem: 15.9 GB 
[10/28 16:37:22 visual_prompt]: 	Training 600/1106. train loss: 0.8154,	0.6344 s / batch. (data: 7.81e-04). ETA=18:36:14, max mem: 15.9 GB 
[10/28 16:38:25 visual_prompt]: 	Training 700/1106. train loss: 1.3231,	0.6303 s / batch. (data: 7.94e-04). ETA=18:28:02, max mem: 15.9 GB 
[10/28 16:39:28 visual_prompt]: 	Training 800/1106. train loss: 0.5342,	0.6457 s / batch. (data: 3.59e-04). ETA=18:53:57, max mem: 15.9 GB 
[10/28 16:40:31 visual_prompt]: 	Training 900/1106. train loss: 1.4256,	0.6240 s / batch. (data: 2.78e-04). ETA=18:14:52, max mem: 15.9 GB 
[10/28 16:41:35 visual_prompt]: 	Training 1000/1106. train loss: 0.3276,	0.6220 s / batch. (data: 3.01e-04). ETA=18:10:23, max mem: 15.9 GB 
[10/28 16:42:38 visual_prompt]: 	Training 1100/1106. train loss: 1.1007,	0.6188 s / batch. (data: 1.90e-04). ETA=18:03:36, max mem: 15.9 GB 
[10/28 16:42:42 visual_prompt]: Epoch 5 / 100: avg data time: 4.72e-03, avg batch time: 0.6345, average train loss: 1.1875
[10/28 16:43:32 visual_prompt]: 	Test 100/123. loss: 0.702, 0.2260 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/28 16:43:42 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2321, average loss: 0.6889
[10/28 16:43:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.16	
[10/28 16:43:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/28 16:44:48 visual_prompt]: 	Training 100/1106. train loss: 0.8145,	0.6480 s / batch. (data: 5.93e-03). ETA=18:53:44, max mem: 15.9 GB 
[10/28 16:45:51 visual_prompt]: 	Training 200/1106. train loss: 0.0309,	0.6302 s / batch. (data: 7.65e-04). ETA=18:21:29, max mem: 15.9 GB 
[10/28 16:46:54 visual_prompt]: 	Training 300/1106. train loss: 0.6505,	0.6327 s / batch. (data: 3.20e-04). ETA=18:24:45, max mem: 15.9 GB 
[10/28 16:47:57 visual_prompt]: 	Training 400/1106. train loss: 2.4580,	0.6239 s / batch. (data: 6.15e-03). ETA=18:08:22, max mem: 15.9 GB 
[10/28 16:49:01 visual_prompt]: 	Training 500/1106. train loss: 1.6704,	0.6437 s / batch. (data: 2.62e-02). ETA=18:41:46, max mem: 15.9 GB 
[10/28 16:50:04 visual_prompt]: 	Training 600/1106. train loss: 0.3995,	0.6191 s / batch. (data: 2.98e-04). ETA=17:57:58, max mem: 15.9 GB 
[10/28 16:51:07 visual_prompt]: 	Training 700/1106. train loss: 0.7811,	0.6680 s / batch. (data: 8.00e-04). ETA=19:21:58, max mem: 15.9 GB 
[10/28 16:52:11 visual_prompt]: 	Training 800/1106. train loss: 2.8411,	0.6340 s / batch. (data: 7.96e-04). ETA=18:21:47, max mem: 15.9 GB 
[10/28 16:53:14 visual_prompt]: 	Training 900/1106. train loss: 4.0704,	0.6574 s / batch. (data: 3.73e-02). ETA=19:01:19, max mem: 15.9 GB 
[10/28 16:54:17 visual_prompt]: 	Training 1000/1106. train loss: 0.1702,	0.6470 s / batch. (data: 7.90e-04). ETA=18:42:15, max mem: 15.9 GB 
[10/28 16:55:21 visual_prompt]: 	Training 1100/1106. train loss: 0.0636,	0.6183 s / batch. (data: 1.58e-04). ETA=17:51:26, max mem: 15.9 GB 
[10/28 16:55:24 visual_prompt]: Epoch 6 / 100: avg data time: 4.54e-03, avg batch time: 0.6346, average train loss: 1.4830
[10/28 16:56:14 visual_prompt]: 	Test 100/123. loss: 0.721, 0.2258 s / batch. (data: 5.15e-05)max mem: 15.94594 GB 
[10/28 16:56:25 visual_prompt]: Inference (val):avg data time: 1.03e-04, avg batch time: 0.2338, average loss: 0.7539
[10/28 16:56:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.83	
[10/28 16:56:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/28 16:57:30 visual_prompt]: 	Training 100/1106. train loss: 2.6810,	0.6372 s / batch. (data: 7.77e-04). ETA=18:23:05, max mem: 15.9 GB 
[10/28 16:58:33 visual_prompt]: 	Training 200/1106. train loss: 1.1943,	0.6473 s / batch. (data: 8.30e-04). ETA=18:39:23, max mem: 15.9 GB 
[10/28 16:59:36 visual_prompt]: 	Training 300/1106. train loss: 2.5127,	0.6381 s / batch. (data: 7.36e-04). ETA=18:22:23, max mem: 15.9 GB 
[10/28 17:00:39 visual_prompt]: 	Training 400/1106. train loss: 1.2847,	0.6188 s / batch. (data: 3.44e-04). ETA=17:48:04, max mem: 15.9 GB 
[10/28 17:01:43 visual_prompt]: 	Training 500/1106. train loss: 3.5515,	0.6560 s / batch. (data: 1.60e-02). ETA=18:51:09, max mem: 15.9 GB 
[10/28 17:02:46 visual_prompt]: 	Training 600/1106. train loss: 4.5916,	0.6359 s / batch. (data: 7.60e-04). ETA=18:15:34, max mem: 15.9 GB 
[10/28 17:03:49 visual_prompt]: 	Training 700/1106. train loss: 6.6270,	0.6450 s / batch. (data: 8.11e-04). ETA=18:30:05, max mem: 15.9 GB 
[10/28 17:04:53 visual_prompt]: 	Training 800/1106. train loss: 0.5722,	0.6322 s / batch. (data: 8.13e-04). ETA=18:07:04, max mem: 15.9 GB 
[10/28 17:05:56 visual_prompt]: 	Training 900/1106. train loss: 0.7755,	0.6262 s / batch. (data: 3.14e-04). ETA=17:55:36, max mem: 15.9 GB 
[10/28 17:06:59 visual_prompt]: 	Training 1000/1106. train loss: 1.1488,	0.6204 s / batch. (data: 3.27e-04). ETA=17:44:36, max mem: 15.9 GB 
[10/28 17:08:02 visual_prompt]: 	Training 1100/1106. train loss: 0.7157,	0.6178 s / batch. (data: 1.54e-04). ETA=17:39:04, max mem: 15.9 GB 
[10/28 17:08:06 visual_prompt]: Epoch 7 / 100: avg data time: 3.94e-03, avg batch time: 0.6341, average train loss: 1.6796
[10/28 17:08:57 visual_prompt]: 	Test 100/123. loss: 2.716, 0.2325 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/28 17:09:06 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.2325, average loss: 2.4507
[10/28 17:09:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.53	
[10/28 17:09:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/28 17:10:12 visual_prompt]: 	Training 100/1106. train loss: 4.5905,	0.6334 s / batch. (data: 2.85e-04). ETA=18:04:49, max mem: 15.9 GB 
[10/28 17:11:15 visual_prompt]: 	Training 200/1106. train loss: 0.8086,	0.6357 s / batch. (data: 8.01e-04). ETA=18:07:42, max mem: 15.9 GB 
[10/28 17:12:18 visual_prompt]: 	Training 300/1106. train loss: 0.0119,	0.6562 s / batch. (data: 7.82e-04). ETA=18:41:42, max mem: 15.9 GB 
[10/28 17:13:22 visual_prompt]: 	Training 400/1106. train loss: 2.7094,	0.6186 s / batch. (data: 2.55e-04). ETA=17:36:18, max mem: 15.9 GB 
[10/28 17:14:25 visual_prompt]: 	Training 500/1106. train loss: 0.0002,	0.6315 s / batch. (data: 7.74e-04). ETA=17:57:18, max mem: 15.9 GB 
[10/28 17:15:28 visual_prompt]: 	Training 600/1106. train loss: 0.0001,	0.6189 s / batch. (data: 3.08e-04). ETA=17:34:50, max mem: 15.9 GB 
[10/28 17:16:31 visual_prompt]: 	Training 700/1106. train loss: 2.3495,	0.6356 s / batch. (data: 8.23e-04). ETA=18:02:08, max mem: 15.9 GB 
[10/28 17:17:35 visual_prompt]: 	Training 800/1106. train loss: 2.7851,	0.6320 s / batch. (data: 7.94e-04). ETA=17:54:59, max mem: 15.9 GB 
[10/28 17:18:38 visual_prompt]: 	Training 900/1106. train loss: 0.2012,	0.6251 s / batch. (data: 4.30e-04). ETA=17:42:15, max mem: 15.9 GB 
[10/28 17:19:41 visual_prompt]: 	Training 1000/1106. train loss: 3.1863,	0.6308 s / batch. (data: 8.22e-04). ETA=17:50:51, max mem: 15.9 GB 
[10/28 17:20:45 visual_prompt]: 	Training 1100/1106. train loss: 0.4637,	0.6190 s / batch. (data: 1.57e-04). ETA=17:29:44, max mem: 15.9 GB 
[10/28 17:20:49 visual_prompt]: Epoch 8 / 100: avg data time: 4.49e-03, avg batch time: 0.6348, average train loss: 1.8545
[10/28 17:21:38 visual_prompt]: 	Test 100/123. loss: 1.522, 0.2276 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/28 17:21:49 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2328, average loss: 1.3780
[10/28 17:21:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.76	
[10/28 17:21:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/28 17:22:55 visual_prompt]: 	Training 100/1106. train loss: 2.4336,	0.6334 s / batch. (data: 8.05e-04). ETA=17:53:08, max mem: 15.9 GB 
[10/28 17:23:58 visual_prompt]: 	Training 200/1106. train loss: 0.0074,	0.6442 s / batch. (data: 7.74e-04). ETA=18:10:17, max mem: 15.9 GB 
[10/28 17:25:01 visual_prompt]: 	Training 300/1106. train loss: 3.8244,	0.6188 s / batch. (data: 3.03e-04). ETA=17:26:20, max mem: 15.9 GB 
[10/28 17:26:04 visual_prompt]: 	Training 400/1106. train loss: 0.0569,	0.6392 s / batch. (data: 7.88e-04). ETA=17:59:46, max mem: 15.9 GB 
[10/28 17:27:08 visual_prompt]: 	Training 500/1106. train loss: 6.4647,	0.6188 s / batch. (data: 3.23e-04). ETA=17:24:17, max mem: 15.9 GB 
[10/28 17:28:11 visual_prompt]: 	Training 600/1106. train loss: 2.6235,	0.6203 s / batch. (data: 3.41e-04). ETA=17:25:48, max mem: 15.9 GB 
[10/28 17:29:14 visual_prompt]: 	Training 700/1106. train loss: 0.7506,	0.6437 s / batch. (data: 3.06e-04). ETA=18:04:02, max mem: 15.9 GB 
[10/28 17:30:18 visual_prompt]: 	Training 800/1106. train loss: 2.1315,	0.6331 s / batch. (data: 7.92e-04). ETA=17:45:16, max mem: 15.9 GB 
[10/28 17:31:21 visual_prompt]: 	Training 900/1106. train loss: 0.1778,	0.6359 s / batch. (data: 7.77e-04). ETA=17:48:50, max mem: 15.9 GB 
[10/28 17:32:24 visual_prompt]: 	Training 1000/1106. train loss: 0.5686,	0.6376 s / batch. (data: 3.01e-04). ETA=17:50:39, max mem: 15.9 GB 
[10/28 17:33:27 visual_prompt]: 	Training 1100/1106. train loss: 1.4456,	0.6182 s / batch. (data: 1.56e-04). ETA=17:17:00, max mem: 15.9 GB 
[10/28 17:33:31 visual_prompt]: Epoch 9 / 100: avg data time: 4.68e-03, avg batch time: 0.6344, average train loss: 1.7606
[10/28 17:34:21 visual_prompt]: 	Test 100/123. loss: 3.116, 0.2517 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/28 17:34:31 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.2327, average loss: 3.4068
[10/28 17:34:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[10/28 17:34:31 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/28 17:35:36 visual_prompt]: 	Training 100/1106. train loss: 0.9987,	0.6453 s / batch. (data: 7.37e-04). ETA=18:01:19, max mem: 15.9 GB 
[10/28 17:36:39 visual_prompt]: 	Training 200/1106. train loss: 0.7578,	0.6389 s / batch. (data: 1.24e-03). ETA=17:49:37, max mem: 15.9 GB 
[10/28 17:37:42 visual_prompt]: 	Training 300/1106. train loss: 2.0428,	0.6437 s / batch. (data: 3.04e-04). ETA=17:56:27, max mem: 15.9 GB 
[10/28 17:38:46 visual_prompt]: 	Training 400/1106. train loss: 0.1734,	0.6195 s / batch. (data: 2.77e-04). ETA=17:15:06, max mem: 15.9 GB 
[10/28 17:39:49 visual_prompt]: 	Training 500/1106. train loss: 0.7915,	0.6308 s / batch. (data: 7.32e-04). ETA=17:32:51, max mem: 15.9 GB 
[10/28 17:40:52 visual_prompt]: 	Training 600/1106. train loss: 9.1805,	0.6520 s / batch. (data: 8.00e-04). ETA=18:07:11, max mem: 15.9 GB 
[10/28 17:41:55 visual_prompt]: 	Training 700/1106. train loss: 23.0360,	0.6303 s / batch. (data: 3.25e-04). ETA=17:29:59, max mem: 15.9 GB 
[10/28 17:42:59 visual_prompt]: 	Training 800/1106. train loss: 0.8565,	0.6508 s / batch. (data: 5.44e-03). ETA=18:03:02, max mem: 15.9 GB 
[10/28 17:44:02 visual_prompt]: 	Training 900/1106. train loss: 2.5284,	0.6454 s / batch. (data: 1.10e-02). ETA=17:53:00, max mem: 15.9 GB 
[10/28 17:45:05 visual_prompt]: 	Training 1000/1106. train loss: 0.7152,	0.6440 s / batch. (data: 3.05e-04). ETA=17:49:32, max mem: 15.9 GB 
[10/28 17:46:09 visual_prompt]: 	Training 1100/1106. train loss: 0.9655,	0.6177 s / batch. (data: 1.50e-04). ETA=17:04:44, max mem: 15.9 GB 
[10/28 17:46:12 visual_prompt]: Epoch 10 / 100: avg data time: 3.65e-03, avg batch time: 0.6339, average train loss: 2.0237
[10/28 17:47:03 visual_prompt]: 	Test 100/123. loss: 2.018, 0.2260 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[10/28 17:47:13 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2355, average loss: 1.8288
[10/28 17:47:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.88	
[10/28 17:47:13 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/28 17:48:18 visual_prompt]: 	Training 100/1106. train loss: 1.7894,	0.6304 s / batch. (data: 3.27e-04). ETA=17:24:42, max mem: 15.9 GB 
[10/28 17:49:21 visual_prompt]: 	Training 200/1106. train loss: 1.7064,	0.6373 s / batch. (data: 8.53e-04). ETA=17:35:09, max mem: 15.9 GB 
[10/28 17:50:25 visual_prompt]: 	Training 300/1106. train loss: 1.4927,	0.6192 s / batch. (data: 4.38e-04). ETA=17:04:05, max mem: 15.9 GB 
[10/28 17:51:28 visual_prompt]: 	Training 400/1106. train loss: 0.6563,	0.6188 s / batch. (data: 3.29e-04). ETA=17:02:24, max mem: 15.9 GB 
[10/28 17:52:31 visual_prompt]: 	Training 500/1106. train loss: 0.8641,	0.6415 s / batch. (data: 7.49e-04). ETA=17:38:58, max mem: 15.9 GB 
[10/28 17:53:35 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6454 s / batch. (data: 8.03e-04). ETA=17:44:13, max mem: 15.9 GB 
[10/28 17:54:38 visual_prompt]: 	Training 700/1106. train loss: 0.3729,	0.6255 s / batch. (data: 3.05e-04). ETA=17:10:23, max mem: 15.9 GB 
[10/28 17:55:41 visual_prompt]: 	Training 800/1106. train loss: 2.4160,	0.6313 s / batch. (data: 1.13e-02). ETA=17:18:53, max mem: 15.9 GB 
[10/28 17:56:45 visual_prompt]: 	Training 900/1106. train loss: 0.5060,	0.6182 s / batch. (data: 3.20e-04). ETA=16:56:16, max mem: 15.9 GB 
[10/28 17:57:48 visual_prompt]: 	Training 1000/1106. train loss: 0.0001,	0.6189 s / batch. (data: 2.79e-04). ETA=16:56:22, max mem: 15.9 GB 
[10/28 17:58:51 visual_prompt]: 	Training 1100/1106. train loss: 0.7159,	0.6169 s / batch. (data: 1.42e-04). ETA=16:52:11, max mem: 15.9 GB 
[10/28 17:58:55 visual_prompt]: Epoch 11 / 100: avg data time: 4.42e-03, avg batch time: 0.6348, average train loss: 2.0473
[10/28 17:59:45 visual_prompt]: 	Test 100/123. loss: 1.201, 0.2262 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[10/28 17:59:55 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2319, average loss: 1.0964
[10/28 17:59:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.83	
[10/28 17:59:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/28 18:01:02 visual_prompt]: 	Training 100/1106. train loss: 3.9431,	0.6310 s / batch. (data: 7.75e-04). ETA=17:14:13, max mem: 15.9 GB 
[10/28 18:02:05 visual_prompt]: 	Training 200/1106. train loss: 1.3744,	0.6213 s / batch. (data: 2.87e-04). ETA=16:57:08, max mem: 15.9 GB 
[10/28 18:03:08 visual_prompt]: 	Training 300/1106. train loss: 0.0673,	0.6340 s / batch. (data: 7.88e-04). ETA=17:16:53, max mem: 15.9 GB 
[10/28 18:04:11 visual_prompt]: 	Training 400/1106. train loss: 1.9277,	0.6341 s / batch. (data: 8.24e-04). ETA=17:16:01, max mem: 15.9 GB 
[10/28 18:05:15 visual_prompt]: 	Training 500/1106. train loss: 0.0712,	0.6385 s / batch. (data: 8.65e-04). ETA=17:22:15, max mem: 15.9 GB 
[10/28 18:06:18 visual_prompt]: 	Training 600/1106. train loss: 0.0006,	0.6197 s / batch. (data: 3.24e-04). ETA=16:50:28, max mem: 15.9 GB 
[10/28 18:07:22 visual_prompt]: 	Training 700/1106. train loss: 0.7036,	0.6194 s / batch. (data: 3.21e-04). ETA=16:49:00, max mem: 15.9 GB 
[10/28 18:08:25 visual_prompt]: 	Training 800/1106. train loss: 0.7434,	0.6316 s / batch. (data: 8.90e-04). ETA=17:07:45, max mem: 15.9 GB 
[10/28 18:09:28 visual_prompt]: 	Training 900/1106. train loss: 0.1940,	0.6343 s / batch. (data: 3.13e-04). ETA=17:11:06, max mem: 15.9 GB 
[10/28 18:10:32 visual_prompt]: 	Training 1000/1106. train loss: 0.6822,	0.6333 s / batch. (data: 7.99e-04). ETA=17:08:23, max mem: 15.9 GB 
[10/28 18:11:35 visual_prompt]: 	Training 1100/1106. train loss: 0.8982,	0.6190 s / batch. (data: 1.38e-04). ETA=16:44:09, max mem: 15.9 GB 
[10/28 18:11:38 visual_prompt]: Epoch 12 / 100: avg data time: 5.15e-03, avg batch time: 0.6356, average train loss: 2.2878
[10/28 18:12:28 visual_prompt]: 	Test 100/123. loss: 0.829, 0.2254 s / batch. (data: 3.39e-05)max mem: 15.94594 GB 
[10/28 18:12:39 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2315, average loss: 0.8886
[10/28 18:12:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.82	
[10/28 18:12:39 visual_prompt]: Best epoch 12: best metric: -0.889
[10/28 18:12:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/28 18:13:44 visual_prompt]: 	Training 100/1106. train loss: 0.3813,	0.6181 s / batch. (data: 3.38e-04). ETA=16:41:34, max mem: 15.9 GB 
[10/28 18:14:47 visual_prompt]: 	Training 200/1106. train loss: 2.8917,	0.6609 s / batch. (data: 8.07e-04). ETA=17:49:48, max mem: 15.9 GB 
[10/28 18:15:51 visual_prompt]: 	Training 300/1106. train loss: 0.0295,	0.6316 s / batch. (data: 7.93e-04). ETA=17:01:25, max mem: 15.9 GB 
[10/28 18:16:54 visual_prompt]: 	Training 400/1106. train loss: 2.7692,	0.6306 s / batch. (data: 8.31e-04). ETA=16:58:43, max mem: 15.9 GB 
[10/28 18:17:57 visual_prompt]: 	Training 500/1106. train loss: 0.0008,	0.6320 s / batch. (data: 7.50e-04). ETA=16:59:55, max mem: 15.9 GB 
[10/28 18:19:00 visual_prompt]: 	Training 600/1106. train loss: 1.0422,	0.6187 s / batch. (data: 3.21e-04). ETA=16:37:26, max mem: 15.9 GB 
[10/28 18:20:03 visual_prompt]: 	Training 700/1106. train loss: 0.1823,	0.6178 s / batch. (data: 2.91e-04). ETA=16:34:55, max mem: 15.9 GB 
[10/28 18:21:07 visual_prompt]: 	Training 800/1106. train loss: 0.8143,	0.6172 s / batch. (data: 3.03e-04). ETA=16:32:55, max mem: 15.9 GB 
[10/28 18:22:10 visual_prompt]: 	Training 900/1106. train loss: 1.9289,	0.6279 s / batch. (data: 3.04e-04). ETA=16:49:02, max mem: 15.9 GB 
[10/28 18:23:13 visual_prompt]: 	Training 1000/1106. train loss: 0.2339,	0.6329 s / batch. (data: 7.84e-04). ETA=16:56:02, max mem: 15.9 GB 
[10/28 18:24:17 visual_prompt]: 	Training 1100/1106. train loss: 1.0053,	0.6184 s / batch. (data: 1.66e-04). ETA=16:31:49, max mem: 15.9 GB 
[10/28 18:24:21 visual_prompt]: Epoch 13 / 100: avg data time: 4.12e-03, avg batch time: 0.6344, average train loss: 2.0452
[10/28 18:25:10 visual_prompt]: 	Test 100/123. loss: 1.272, 0.2397 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/28 18:25:21 visual_prompt]: Inference (val):avg data time: 1.00e-04, avg batch time: 0.2323, average loss: 1.1548
[10/28 18:25:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.73	
[10/28 18:25:21 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/28 18:26:26 visual_prompt]: 	Training 100/1106. train loss: 3.3703,	0.6440 s / batch. (data: 8.35e-04). ETA=17:11:44, max mem: 15.9 GB 
[10/28 18:27:29 visual_prompt]: 	Training 200/1106. train loss: 2.4014,	0.6406 s / batch. (data: 8.03e-04). ETA=17:05:10, max mem: 15.9 GB 
[10/28 18:28:33 visual_prompt]: 	Training 300/1106. train loss: 2.7517,	0.6425 s / batch. (data: 9.10e-04). ETA=17:07:06, max mem: 15.9 GB 
[10/28 18:29:36 visual_prompt]: 	Training 400/1106. train loss: 0.0820,	0.6334 s / batch. (data: 7.99e-04). ETA=16:51:33, max mem: 15.9 GB 
[10/28 18:30:39 visual_prompt]: 	Training 500/1106. train loss: 0.8144,	0.6184 s / batch. (data: 3.03e-04). ETA=16:26:34, max mem: 15.9 GB 
[10/28 18:31:43 visual_prompt]: 	Training 600/1106. train loss: 1.0162,	0.6445 s / batch. (data: 7.56e-04). ETA=17:07:06, max mem: 15.9 GB 
[10/28 18:32:46 visual_prompt]: 	Training 700/1106. train loss: 0.3885,	0.6644 s / batch. (data: 7.97e-04). ETA=17:37:41, max mem: 15.9 GB 
[10/28 18:33:49 visual_prompt]: 	Training 800/1106. train loss: 4.2563,	0.6336 s / batch. (data: 8.68e-04). ETA=16:47:43, max mem: 15.9 GB 
[10/28 18:34:53 visual_prompt]: 	Training 900/1106. train loss: 2.4954,	0.6456 s / batch. (data: 9.14e-04). ETA=17:05:38, max mem: 15.9 GB 
[10/28 18:35:56 visual_prompt]: 	Training 1000/1106. train loss: 0.0713,	0.6444 s / batch. (data: 1.03e-03). ETA=17:02:41, max mem: 15.9 GB 
[10/28 18:36:59 visual_prompt]: 	Training 1100/1106. train loss: 1.7063,	0.6183 s / batch. (data: 1.51e-04). ETA=16:20:15, max mem: 15.9 GB 
[10/28 18:37:03 visual_prompt]: Epoch 14 / 100: avg data time: 4.36e-03, avg batch time: 0.6345, average train loss: 2.2020
[10/28 18:37:52 visual_prompt]: 	Test 100/123. loss: 3.381, 0.2317 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/28 18:38:03 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2329, average loss: 3.0473
[10/28 18:38:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.57	
[10/28 18:38:03 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/28 18:39:08 visual_prompt]: 	Training 100/1106. train loss: 4.9355,	0.6340 s / batch. (data: 3.46e-04). ETA=16:43:57, max mem: 15.9 GB 
[10/28 18:40:11 visual_prompt]: 	Training 200/1106. train loss: 3.6876,	0.6449 s / batch. (data: 7.58e-04). ETA=17:00:14, max mem: 15.9 GB 
[10/28 18:41:14 visual_prompt]: 	Training 300/1106. train loss: 1.5619,	0.6396 s / batch. (data: 5.92e-03). ETA=16:50:46, max mem: 15.9 GB 
[10/28 18:42:17 visual_prompt]: 	Training 400/1106. train loss: 0.0965,	0.6413 s / batch. (data: 8.10e-04). ETA=16:52:21, max mem: 15.9 GB 
[10/28 18:43:21 visual_prompt]: 	Training 500/1106. train loss: 3.4968,	0.6307 s / batch. (data: 2.78e-04). ETA=16:34:36, max mem: 15.9 GB 
[10/28 18:44:24 visual_prompt]: 	Training 600/1106. train loss: 1.0922,	0.6395 s / batch. (data: 1.02e-03). ETA=16:47:18, max mem: 15.9 GB 
[10/28 18:45:27 visual_prompt]: 	Training 700/1106. train loss: 2.1313,	0.6215 s / batch. (data: 2.81e-04). ETA=16:18:00, max mem: 15.9 GB 
[10/28 18:46:30 visual_prompt]: 	Training 800/1106. train loss: 2.0926,	0.6401 s / batch. (data: 5.45e-03). ETA=16:46:11, max mem: 15.9 GB 
[10/28 18:47:34 visual_prompt]: 	Training 900/1106. train loss: 5.6880,	0.6274 s / batch. (data: 8.23e-04). ETA=16:25:15, max mem: 15.9 GB 
[10/28 18:48:37 visual_prompt]: 	Training 1000/1106. train loss: 0.7595,	0.6267 s / batch. (data: 3.04e-04). ETA=16:23:03, max mem: 15.9 GB 
[10/28 18:49:40 visual_prompt]: 	Training 1100/1106. train loss: 0.9430,	0.6174 s / batch. (data: 1.36e-04). ETA=16:07:27, max mem: 15.9 GB 
[10/28 18:49:44 visual_prompt]: Epoch 15 / 100: avg data time: 3.56e-03, avg batch time: 0.6337, average train loss: 2.3517
[10/28 18:50:34 visual_prompt]: 	Test 100/123. loss: 1.117, 0.2355 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/28 18:50:44 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2316, average loss: 1.0125
[10/28 18:50:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.84	
[10/28 18:50:44 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/28 18:51:49 visual_prompt]: 	Training 100/1106. train loss: 0.0011,	0.6440 s / batch. (data: 7.77e-04). ETA=16:47:58, max mem: 15.9 GB 
[10/28 18:52:52 visual_prompt]: 	Training 200/1106. train loss: 0.6721,	0.6392 s / batch. (data: 9.68e-04). ETA=16:39:19, max mem: 15.9 GB 
[10/28 18:53:55 visual_prompt]: 	Training 300/1106. train loss: 3.7817,	0.6194 s / batch. (data: 3.33e-04). ETA=16:07:27, max mem: 15.9 GB 
[10/28 18:54:59 visual_prompt]: 	Training 400/1106. train loss: 2.7516,	0.6321 s / batch. (data: 8.56e-04). ETA=16:26:08, max mem: 15.9 GB 
[10/28 18:56:02 visual_prompt]: 	Training 500/1106. train loss: 17.5478,	0.6402 s / batch. (data: 1.05e-02). ETA=16:37:46, max mem: 15.9 GB 
[10/28 18:57:05 visual_prompt]: 	Training 600/1106. train loss: 0.4054,	0.6195 s / batch. (data: 3.04e-04). ETA=16:04:31, max mem: 15.9 GB 
[10/28 18:58:09 visual_prompt]: 	Training 700/1106. train loss: 0.7163,	0.6489 s / batch. (data: 8.20e-04). ETA=16:49:04, max mem: 15.9 GB 
[10/28 18:59:12 visual_prompt]: 	Training 800/1106. train loss: 0.7261,	0.6197 s / batch. (data: 3.31e-04). ETA=16:02:38, max mem: 15.9 GB 
[10/28 19:00:15 visual_prompt]: 	Training 900/1106. train loss: 1.5644,	0.6398 s / batch. (data: 8.00e-04). ETA=16:32:54, max mem: 15.9 GB 
[10/28 19:01:19 visual_prompt]: 	Training 1000/1106. train loss: 0.0023,	0.6337 s / batch. (data: 8.16e-04). ETA=16:22:22, max mem: 15.9 GB 
[10/28 19:02:22 visual_prompt]: 	Training 1100/1106. train loss: 10.5563,	0.6187 s / batch. (data: 1.65e-04). ETA=15:58:02, max mem: 15.9 GB 
[10/28 19:02:26 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-03, avg batch time: 0.6344, average train loss: 2.4473
[10/28 19:03:17 visual_prompt]: 	Test 100/123. loss: 0.909, 0.2253 s / batch. (data: 3.58e-05)max mem: 15.94594 GB 
[10/28 19:03:26 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.2332, average loss: 0.9977
[10/28 19:03:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.98	
[10/28 19:03:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/28 19:04:32 visual_prompt]: 	Training 100/1106. train loss: 0.0927,	0.6330 s / batch. (data: 1.05e-02). ETA=16:19:09, max mem: 15.9 GB 
[10/28 19:05:35 visual_prompt]: 	Training 200/1106. train loss: 5.0603,	0.6223 s / batch. (data: 2.95e-04). ETA=16:01:27, max mem: 15.9 GB 
[10/28 19:06:38 visual_prompt]: 	Training 300/1106. train loss: 3.6728,	0.6183 s / batch. (data: 3.19e-04). ETA=15:54:21, max mem: 15.9 GB 
[10/28 19:07:42 visual_prompt]: 	Training 400/1106. train loss: 4.0731,	0.6281 s / batch. (data: 3.40e-04). ETA=16:08:18, max mem: 15.9 GB 
[10/28 19:08:45 visual_prompt]: 	Training 500/1106. train loss: 0.0038,	0.6208 s / batch. (data: 3.08e-04). ETA=15:56:02, max mem: 15.9 GB 
[10/28 19:09:48 visual_prompt]: 	Training 600/1106. train loss: 2.6414,	0.6197 s / batch. (data: 4.13e-04). ETA=15:53:22, max mem: 15.9 GB 
[10/28 19:10:51 visual_prompt]: 	Training 700/1106. train loss: 0.8054,	0.6342 s / batch. (data: 8.20e-04). ETA=16:14:37, max mem: 15.9 GB 
[10/28 19:11:55 visual_prompt]: 	Training 800/1106. train loss: 2.2700,	0.6185 s / batch. (data: 4.52e-04). ETA=15:49:22, max mem: 15.9 GB 
[10/28 19:12:58 visual_prompt]: 	Training 900/1106. train loss: 0.0181,	0.6324 s / batch. (data: 1.05e-02). ETA=16:09:46, max mem: 15.9 GB 
[10/28 19:14:01 visual_prompt]: 	Training 1000/1106. train loss: 1.8903,	0.6327 s / batch. (data: 7.63e-04). ETA=16:09:10, max mem: 15.9 GB 
[10/28 19:15:05 visual_prompt]: 	Training 1100/1106. train loss: 0.0006,	0.6197 s / batch. (data: 1.62e-04). ETA=15:48:10, max mem: 15.9 GB 
[10/28 19:15:08 visual_prompt]: Epoch 17 / 100: avg data time: 4.42e-03, avg batch time: 0.6346, average train loss: 2.1525
[10/28 19:15:59 visual_prompt]: 	Test 100/123. loss: 1.482, 0.2259 s / batch. (data: 3.70e-05)max mem: 15.94594 GB 
[10/28 19:16:09 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2327, average loss: 1.6292
[10/28 19:16:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.05	
[10/28 19:16:09 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/28 19:17:14 visual_prompt]: 	Training 100/1106. train loss: 24.5831,	0.6171 s / batch. (data: 2.93e-04). ETA=15:43:08, max mem: 15.9 GB 
[10/28 19:18:17 visual_prompt]: 	Training 200/1106. train loss: 4.5021,	0.6321 s / batch. (data: 8.27e-04). ETA=16:04:55, max mem: 15.9 GB 
[10/28 19:19:21 visual_prompt]: 	Training 300/1106. train loss: 6.9528,	0.6337 s / batch. (data: 3.05e-04). ETA=16:06:18, max mem: 15.9 GB 
[10/28 19:20:24 visual_prompt]: 	Training 400/1106. train loss: 4.4499,	0.6338 s / batch. (data: 3.48e-04). ETA=16:05:28, max mem: 15.9 GB 
[10/28 19:21:28 visual_prompt]: 	Training 500/1106. train loss: 0.0072,	0.6600 s / batch. (data: 8.12e-04). ETA=16:44:18, max mem: 15.9 GB 
[10/28 19:22:31 visual_prompt]: 	Training 600/1106. train loss: 4.7622,	0.6331 s / batch. (data: 8.10e-04). ETA=16:02:21, max mem: 15.9 GB 
[10/28 19:23:34 visual_prompt]: 	Training 700/1106. train loss: 0.0000,	0.6185 s / batch. (data: 7.51e-04). ETA=15:39:04, max mem: 15.9 GB 
[10/28 19:24:37 visual_prompt]: 	Training 800/1106. train loss: 0.7345,	0.6311 s / batch. (data: 3.55e-04). ETA=15:57:07, max mem: 15.9 GB 
[10/28 19:25:41 visual_prompt]: 	Training 900/1106. train loss: 2.7051,	0.6320 s / batch. (data: 3.19e-04). ETA=15:57:27, max mem: 15.9 GB 
[10/28 19:26:44 visual_prompt]: 	Training 1000/1106. train loss: 3.2509,	0.6295 s / batch. (data: 2.99e-04). ETA=15:52:41, max mem: 15.9 GB 
[10/28 19:27:47 visual_prompt]: 	Training 1100/1106. train loss: 2.6979,	0.6192 s / batch. (data: 1.64e-04). ETA=15:35:59, max mem: 15.9 GB 
[10/28 19:27:51 visual_prompt]: Epoch 18 / 100: avg data time: 4.30e-03, avg batch time: 0.6346, average train loss: 2.3364
[10/28 19:28:41 visual_prompt]: 	Test 100/123. loss: 1.520, 0.2350 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[10/28 19:28:52 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.2341, average loss: 1.3651
[10/28 19:28:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.75	
[10/28 19:28:52 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/28 19:29:57 visual_prompt]: 	Training 100/1106. train loss: 0.3248,	0.6185 s / batch. (data: 3.24e-04). ETA=15:33:49, max mem: 15.9 GB 
[10/28 19:31:00 visual_prompt]: 	Training 200/1106. train loss: 2.9132,	0.6187 s / batch. (data: 3.60e-04). ETA=15:33:08, max mem: 15.9 GB 
[10/28 19:32:04 visual_prompt]: 	Training 300/1106. train loss: 0.4280,	0.6436 s / batch. (data: 1.10e-02). ETA=16:09:36, max mem: 15.9 GB 
[10/28 19:33:07 visual_prompt]: 	Training 400/1106. train loss: 1.7697,	0.6319 s / batch. (data: 7.66e-04). ETA=15:50:54, max mem: 15.9 GB 
[10/28 19:34:11 visual_prompt]: 	Training 500/1106. train loss: 0.3047,	0.6183 s / batch. (data: 3.37e-04). ETA=15:29:23, max mem: 15.9 GB 
[10/28 19:35:14 visual_prompt]: 	Training 600/1106. train loss: 0.0992,	0.6338 s / batch. (data: 8.00e-04). ETA=15:51:37, max mem: 15.9 GB 
[10/28 19:36:17 visual_prompt]: 	Training 700/1106. train loss: 0.7024,	0.6201 s / batch. (data: 4.38e-04). ETA=15:30:03, max mem: 15.9 GB 
[10/28 19:37:21 visual_prompt]: 	Training 800/1106. train loss: 1.3453,	0.6336 s / batch. (data: 8.16e-04). ETA=15:49:16, max mem: 15.9 GB 
[10/28 19:38:24 visual_prompt]: 	Training 900/1106. train loss: 0.0391,	0.6199 s / batch. (data: 3.27e-04). ETA=15:27:38, max mem: 15.9 GB 
[10/28 19:39:27 visual_prompt]: 	Training 1000/1106. train loss: 0.4445,	0.6336 s / batch. (data: 7.59e-04). ETA=15:47:10, max mem: 15.9 GB 
[10/28 19:40:31 visual_prompt]: 	Training 1100/1106. train loss: 0.0001,	0.6195 s / batch. (data: 1.71e-04). ETA=15:24:58, max mem: 15.9 GB 
[10/28 19:40:34 visual_prompt]: Epoch 19 / 100: avg data time: 4.61e-03, avg batch time: 0.6354, average train loss: 2.0534
[10/28 19:41:24 visual_prompt]: 	Test 100/123. loss: 3.359, 0.2326 s / batch. (data: 4.08e-05)max mem: 15.94594 GB 
[10/28 19:41:35 visual_prompt]: Inference (val):avg data time: 2.23e-04, avg batch time: 0.2336, average loss: 5.0487
[10/28 19:41:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.57	
[10/28 19:41:35 visual_prompt]: Stopping early.
[10/28 19:41:35 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 19:41:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 19:41:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 19:41:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 19:41:35 visual_prompt]: Training with config:
[10/28 19:41:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 19:41:35 visual_prompt]: Loading training data...
[10/28 19:41:35 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 19:41:35 visual_prompt]: Loading validation data...
[10/28 19:41:35 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 19:41:35 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/28 19:41:38 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/28 19:41:38 visual_prompt]: tuned percent:0.522
[10/28 19:41:38 visual_prompt]: Device used for model: 0
[10/28 19:41:38 visual_prompt]: Setting up Evaluator...
[10/28 19:41:38 visual_prompt]: Setting up Trainer...
[10/28 19:41:38 visual_prompt]: 	Setting up the optimizer...
[10/28 19:41:38 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 19:42:43 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6313 s / batch. (data: 9.55e-04). ETA=19:22:40, max mem: 15.9 GB 
[10/28 19:43:46 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6323 s / batch. (data: 2.95e-04). ETA=19:23:27, max mem: 15.9 GB 
[10/28 19:44:50 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6179 s / batch. (data: 3.28e-04). ETA=18:55:51, max mem: 15.9 GB 
[10/28 19:45:53 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6418 s / batch. (data: 8.21e-04). ETA=19:38:48, max mem: 15.9 GB 
[10/28 19:46:56 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6526 s / batch. (data: 1.26e-02). ETA=19:57:32, max mem: 15.9 GB 
[10/28 19:48:00 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6401 s / batch. (data: 7.34e-04). ETA=19:33:30, max mem: 15.9 GB 
[10/28 19:49:03 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6312 s / batch. (data: 6.98e-04). ETA=19:16:12, max mem: 15.9 GB 
[10/28 19:50:06 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6182 s / batch. (data: 3.36e-04). ETA=18:51:19, max mem: 15.9 GB 
[10/28 19:51:10 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6404 s / batch. (data: 1.05e-02). ETA=19:30:49, max mem: 15.9 GB 
[10/28 19:52:13 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6314 s / batch. (data: 3.12e-04). ETA=19:13:26, max mem: 15.9 GB 
[10/28 19:53:16 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6183 s / batch. (data: 1.50e-04). ETA=18:48:20, max mem: 15.9 GB 
[10/28 19:53:20 visual_prompt]: Epoch 1 / 100: avg data time: 4.49e-03, avg batch time: 0.6351, average train loss: 1.4028
[10/28 19:54:10 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2251 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[10/28 19:54:21 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.2321, average loss: 1.3505
[10/28 19:54:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/28 19:54:21 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/28 19:55:26 visual_prompt]: 	Training 100/1106. train loss: 1.0409,	0.6214 s / batch. (data: 5.79e-03). ETA=18:52:53, max mem: 15.9 GB 
[10/28 19:56:29 visual_prompt]: 	Training 200/1106. train loss: 0.8547,	0.6371 s / batch. (data: 3.43e-04). ETA=19:20:26, max mem: 15.9 GB 
[10/28 19:57:32 visual_prompt]: 	Training 300/1106. train loss: 0.4551,	0.6481 s / batch. (data: 6.95e-03). ETA=19:39:29, max mem: 15.9 GB 
[10/28 19:58:35 visual_prompt]: 	Training 400/1106. train loss: 0.0774,	0.6305 s / batch. (data: 7.92e-04). ETA=19:06:27, max mem: 15.9 GB 
[10/28 19:59:39 visual_prompt]: 	Training 500/1106. train loss: 1.0522,	0.6200 s / batch. (data: 3.34e-04). ETA=18:46:14, max mem: 15.9 GB 
[10/28 20:00:42 visual_prompt]: 	Training 600/1106. train loss: 0.6129,	0.6352 s / batch. (data: 7.87e-04). ETA=19:12:47, max mem: 15.9 GB 
[10/28 20:01:45 visual_prompt]: 	Training 700/1106. train loss: 0.9307,	0.6189 s / batch. (data: 3.38e-04). ETA=18:42:17, max mem: 15.9 GB 
[10/28 20:02:48 visual_prompt]: 	Training 800/1106. train loss: 0.7900,	0.6352 s / batch. (data: 8.44e-04). ETA=19:10:41, max mem: 15.9 GB 
[10/28 20:03:52 visual_prompt]: 	Training 900/1106. train loss: 1.1241,	0.6449 s / batch. (data: 7.10e-04). ETA=19:27:17, max mem: 15.9 GB 
[10/28 20:04:55 visual_prompt]: 	Training 1000/1106. train loss: 0.2410,	0.6266 s / batch. (data: 3.09e-04). ETA=18:52:57, max mem: 15.9 GB 
[10/28 20:05:58 visual_prompt]: 	Training 1100/1106. train loss: 0.3170,	0.6173 s / batch. (data: 1.44e-04). ETA=18:35:09, max mem: 15.9 GB 
[10/28 20:06:02 visual_prompt]: Epoch 2 / 100: avg data time: 4.18e-03, avg batch time: 0.6341, average train loss: 0.9030
[10/28 20:06:52 visual_prompt]: 	Test 100/123. loss: 1.499, 0.2359 s / batch. (data: 2.50e-05)max mem: 15.94594 GB 
[10/28 20:07:02 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.2331, average loss: 1.3666
[10/28 20:07:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.49	
[10/28 20:07:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/28 20:08:08 visual_prompt]: 	Training 100/1106. train loss: 0.7806,	0.6311 s / batch. (data: 7.64e-04). ETA=18:58:55, max mem: 15.9 GB 
[10/28 20:09:11 visual_prompt]: 	Training 200/1106. train loss: 0.2744,	0.6211 s / batch. (data: 3.28e-04). ETA=18:40:00, max mem: 15.9 GB 
[10/28 20:10:15 visual_prompt]: 	Training 300/1106. train loss: 0.2995,	0.6325 s / batch. (data: 7.38e-04). ETA=18:59:26, max mem: 15.9 GB 
[10/28 20:11:18 visual_prompt]: 	Training 400/1106. train loss: 0.8756,	0.6536 s / batch. (data: 7.89e-04). ETA=19:36:18, max mem: 15.9 GB 
[10/28 20:12:21 visual_prompt]: 	Training 500/1106. train loss: 1.2498,	0.6184 s / batch. (data: 3.31e-04). ETA=18:31:53, max mem: 15.9 GB 
[10/28 20:13:24 visual_prompt]: 	Training 600/1106. train loss: 0.7401,	0.6298 s / batch. (data: 3.65e-04). ETA=18:51:29, max mem: 15.9 GB 
[10/28 20:14:27 visual_prompt]: 	Training 700/1106. train loss: 2.0586,	0.6260 s / batch. (data: 3.15e-04). ETA=18:43:34, max mem: 15.9 GB 
[10/28 20:15:31 visual_prompt]: 	Training 800/1106. train loss: 0.4488,	0.6403 s / batch. (data: 2.11e-02). ETA=19:08:05, max mem: 15.9 GB 
[10/28 20:16:34 visual_prompt]: 	Training 900/1106. train loss: 3.1224,	0.6407 s / batch. (data: 1.05e-02). ETA=19:07:46, max mem: 15.9 GB 
[10/28 20:17:37 visual_prompt]: 	Training 1000/1106. train loss: 0.6949,	0.6322 s / batch. (data: 7.94e-04). ETA=18:51:31, max mem: 15.9 GB 
[10/28 20:18:40 visual_prompt]: 	Training 1100/1106. train loss: 0.7988,	0.6171 s / batch. (data: 1.67e-04). ETA=18:23:29, max mem: 15.9 GB 
[10/28 20:18:44 visual_prompt]: Epoch 3 / 100: avg data time: 4.86e-03, avg batch time: 0.6345, average train loss: 0.9878
[10/28 20:19:34 visual_prompt]: 	Test 100/123. loss: 1.072, 0.2473 s / batch. (data: 4.22e-05)max mem: 15.94594 GB 
[10/28 20:19:44 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.2318, average loss: 1.1680
[10/28 20:19:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.84	
[10/28 20:19:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/28 20:20:50 visual_prompt]: 	Training 100/1106. train loss: 0.6887,	0.6400 s / batch. (data: 7.95e-04). ETA=19:03:19, max mem: 15.9 GB 
[10/28 20:21:53 visual_prompt]: 	Training 200/1106. train loss: 2.7138,	0.6460 s / batch. (data: 1.61e-02). ETA=19:12:55, max mem: 15.9 GB 
[10/28 20:22:56 visual_prompt]: 	Training 300/1106. train loss: 1.0625,	0.6189 s / batch. (data: 3.18e-04). ETA=18:23:32, max mem: 15.9 GB 
[10/28 20:24:00 visual_prompt]: 	Training 400/1106. train loss: 1.0373,	0.6206 s / batch. (data: 3.27e-04). ETA=18:25:36, max mem: 15.9 GB 
[10/28 20:25:03 visual_prompt]: 	Training 500/1106. train loss: 2.1465,	0.6395 s / batch. (data: 7.85e-04). ETA=18:58:09, max mem: 15.9 GB 
[10/28 20:26:06 visual_prompt]: 	Training 600/1106. train loss: 0.1836,	0.6418 s / batch. (data: 8.12e-04). ETA=19:01:10, max mem: 15.9 GB 
[10/28 20:27:10 visual_prompt]: 	Training 700/1106. train loss: 1.7855,	0.6330 s / batch. (data: 7.75e-04). ETA=18:44:23, max mem: 15.9 GB 
[10/28 20:28:13 visual_prompt]: 	Training 800/1106. train loss: 0.6371,	0.6468 s / batch. (data: 7.76e-04). ETA=19:07:47, max mem: 15.9 GB 
[10/28 20:29:16 visual_prompt]: 	Training 900/1106. train loss: 1.3757,	0.6204 s / batch. (data: 7.85e-04). ETA=18:19:54, max mem: 15.9 GB 
[10/28 20:30:19 visual_prompt]: 	Training 1000/1106. train loss: 0.9166,	0.6340 s / batch. (data: 7.80e-04). ETA=18:42:57, max mem: 15.9 GB 
[10/28 20:31:23 visual_prompt]: 	Training 1100/1106. train loss: 1.6229,	0.6176 s / batch. (data: 1.39e-04). ETA=18:12:59, max mem: 15.9 GB 
[10/28 20:31:27 visual_prompt]: Epoch 4 / 100: avg data time: 4.17e-03, avg batch time: 0.6349, average train loss: 0.9442
[10/28 20:32:17 visual_prompt]: 	Test 100/123. loss: 0.697, 0.2318 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/28 20:32:27 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2324, average loss: 0.7063
[10/28 20:32:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.92	
[10/28 20:32:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/28 20:33:33 visual_prompt]: 	Training 100/1106. train loss: 0.7217,	0.6290 s / batch. (data: 8.03e-04). ETA=18:32:04, max mem: 15.9 GB 
[10/28 20:34:36 visual_prompt]: 	Training 200/1106. train loss: 0.4393,	0.6208 s / batch. (data: 3.15e-04). ETA=18:16:27, max mem: 15.9 GB 
[10/28 20:35:39 visual_prompt]: 	Training 300/1106. train loss: 0.6716,	0.6190 s / batch. (data: 3.25e-04). ETA=18:12:14, max mem: 15.9 GB 
[10/28 20:36:42 visual_prompt]: 	Training 400/1106. train loss: 3.1893,	0.6379 s / batch. (data: 8.37e-04). ETA=18:44:32, max mem: 15.9 GB 
[10/28 20:37:46 visual_prompt]: 	Training 500/1106. train loss: 0.0454,	0.6338 s / batch. (data: 7.79e-04). ETA=18:36:17, max mem: 15.9 GB 
[10/28 20:38:49 visual_prompt]: 	Training 600/1106. train loss: 2.5443,	0.6291 s / batch. (data: 3.42e-04). ETA=18:26:53, max mem: 15.9 GB 
[10/28 20:39:52 visual_prompt]: 	Training 700/1106. train loss: 1.2619,	0.6340 s / batch. (data: 8.12e-04). ETA=18:34:27, max mem: 15.9 GB 
[10/28 20:40:55 visual_prompt]: 	Training 800/1106. train loss: 0.5549,	0.6439 s / batch. (data: 7.85e-04). ETA=18:50:50, max mem: 15.9 GB 
[10/28 20:41:59 visual_prompt]: 	Training 900/1106. train loss: 1.0285,	0.6388 s / batch. (data: 5.38e-03). ETA=18:40:52, max mem: 15.9 GB 
[10/28 20:43:02 visual_prompt]: 	Training 1000/1106. train loss: 2.1815,	0.6189 s / batch. (data: 3.18e-04). ETA=18:04:57, max mem: 15.9 GB 
[10/28 20:44:05 visual_prompt]: 	Training 1100/1106. train loss: 1.0890,	0.6181 s / batch. (data: 2.12e-04). ETA=18:02:24, max mem: 15.9 GB 
[10/28 20:44:09 visual_prompt]: Epoch 5 / 100: avg data time: 4.46e-03, avg batch time: 0.6345, average train loss: 1.1823
[10/28 20:44:59 visual_prompt]: 	Test 100/123. loss: 0.693, 0.2620 s / batch. (data: 3.62e-05)max mem: 15.94594 GB 
[10/28 20:45:09 visual_prompt]: Inference (val):avg data time: 9.64e-05, avg batch time: 0.2324, average loss: 0.6898
[10/28 20:45:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.39	
[10/28 20:45:09 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/28 20:46:14 visual_prompt]: 	Training 100/1106. train loss: 2.3769,	0.6242 s / batch. (data: 3.26e-04). ETA=18:12:03, max mem: 15.9 GB 
[10/28 20:47:18 visual_prompt]: 	Training 200/1106. train loss: 0.0328,	0.6244 s / batch. (data: 4.38e-04). ETA=18:11:25, max mem: 15.9 GB 
[10/28 20:48:21 visual_prompt]: 	Training 300/1106. train loss: 0.0149,	0.6440 s / batch. (data: 8.14e-04). ETA=18:44:32, max mem: 15.9 GB 
[10/28 20:49:24 visual_prompt]: 	Training 400/1106. train loss: 1.2280,	0.6245 s / batch. (data: 5.46e-03). ETA=18:09:22, max mem: 15.9 GB 
[10/28 20:50:27 visual_prompt]: 	Training 500/1106. train loss: 0.7141,	0.6523 s / batch. (data: 8.01e-04). ETA=18:56:54, max mem: 15.9 GB 
[10/28 20:51:31 visual_prompt]: 	Training 600/1106. train loss: 0.2746,	0.6383 s / batch. (data: 8.33e-04). ETA=18:31:18, max mem: 15.9 GB 
[10/28 20:52:34 visual_prompt]: 	Training 700/1106. train loss: 0.7854,	0.6652 s / batch. (data: 8.02e-04). ETA=19:17:09, max mem: 15.9 GB 
[10/28 20:53:37 visual_prompt]: 	Training 800/1106. train loss: 2.3779,	0.6281 s / batch. (data: 3.17e-04). ETA=18:11:29, max mem: 15.9 GB 
[10/28 20:54:40 visual_prompt]: 	Training 900/1106. train loss: 1.0538,	0.6560 s / batch. (data: 7.72e-04). ETA=18:58:54, max mem: 15.9 GB 
[10/28 20:55:44 visual_prompt]: 	Training 1000/1106. train loss: 0.1240,	0.6407 s / batch. (data: 1.27e-02). ETA=18:31:19, max mem: 15.9 GB 
[10/28 20:56:47 visual_prompt]: 	Training 1100/1106. train loss: 0.0491,	0.6177 s / batch. (data: 1.55e-04). ETA=17:50:17, max mem: 15.9 GB 
[10/28 20:56:51 visual_prompt]: Epoch 6 / 100: avg data time: 4.08e-03, avg batch time: 0.6342, average train loss: 1.2710
[10/28 20:57:41 visual_prompt]: 	Test 100/123. loss: 1.103, 0.2248 s / batch. (data: 2.65e-05)max mem: 15.94594 GB 
[10/28 20:57:51 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2319, average loss: 1.2005
[10/28 20:57:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.47	
[10/28 20:57:51 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/28 20:58:56 visual_prompt]: 	Training 100/1106. train loss: 1.2018,	0.6373 s / batch. (data: 8.51e-04). ETA=18:23:12, max mem: 15.9 GB 
[10/28 21:00:00 visual_prompt]: 	Training 200/1106. train loss: 2.3613,	0.6248 s / batch. (data: 2.66e-04). ETA=18:00:30, max mem: 15.9 GB 
[10/28 21:01:03 visual_prompt]: 	Training 300/1106. train loss: 0.1169,	0.6184 s / batch. (data: 2.66e-04). ETA=17:48:27, max mem: 15.9 GB 
[10/28 21:02:06 visual_prompt]: 	Training 400/1106. train loss: 0.9336,	0.6193 s / batch. (data: 3.42e-04). ETA=17:48:57, max mem: 15.9 GB 
[10/28 21:03:10 visual_prompt]: 	Training 500/1106. train loss: 1.6928,	0.6306 s / batch. (data: 1.34e-02). ETA=18:07:23, max mem: 15.9 GB 
[10/28 21:04:13 visual_prompt]: 	Training 600/1106. train loss: 3.0232,	0.6241 s / batch. (data: 3.28e-04). ETA=17:55:09, max mem: 15.9 GB 
[10/28 21:05:16 visual_prompt]: 	Training 700/1106. train loss: 2.4158,	0.6590 s / batch. (data: 4.09e-02). ETA=18:54:09, max mem: 15.9 GB 
[10/28 21:06:19 visual_prompt]: 	Training 800/1106. train loss: 0.0339,	0.6179 s / batch. (data: 3.25e-04). ETA=17:42:25, max mem: 15.9 GB 
[10/28 21:07:23 visual_prompt]: 	Training 900/1106. train loss: 0.9512,	0.6488 s / batch. (data: 7.60e-04). ETA=18:34:31, max mem: 15.9 GB 
[10/28 21:08:26 visual_prompt]: 	Training 1000/1106. train loss: 1.1110,	0.6176 s / batch. (data: 3.47e-04). ETA=17:39:52, max mem: 15.9 GB 
[10/28 21:09:29 visual_prompt]: 	Training 1100/1106. train loss: 2.0817,	0.6179 s / batch. (data: 1.49e-04). ETA=17:39:21, max mem: 15.9 GB 
[10/28 21:09:33 visual_prompt]: Epoch 7 / 100: avg data time: 3.77e-03, avg batch time: 0.6338, average train loss: 1.3029
[10/28 21:10:22 visual_prompt]: 	Test 100/123. loss: 1.553, 0.2397 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[10/28 21:10:33 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.2328, average loss: 1.4063
[10/28 21:10:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.37	
[10/28 21:10:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/28 21:11:38 visual_prompt]: 	Training 100/1106. train loss: 1.4127,	0.6305 s / batch. (data: 7.82e-04). ETA=17:59:53, max mem: 15.9 GB 
[10/28 21:12:41 visual_prompt]: 	Training 200/1106. train loss: 0.9483,	0.6350 s / batch. (data: 8.15e-04). ETA=18:06:28, max mem: 15.9 GB 
[10/28 21:13:44 visual_prompt]: 	Training 300/1106. train loss: 0.0647,	0.6346 s / batch. (data: 3.07e-04). ETA=18:04:46, max mem: 15.9 GB 
[10/28 21:14:48 visual_prompt]: 	Training 400/1106. train loss: 8.5907,	0.6179 s / batch. (data: 2.24e-04). ETA=17:35:09, max mem: 15.9 GB 
[10/28 21:15:51 visual_prompt]: 	Training 500/1106. train loss: 0.0974,	0.6447 s / batch. (data: 5.89e-03). ETA=18:19:47, max mem: 15.9 GB 
[10/28 21:16:54 visual_prompt]: 	Training 600/1106. train loss: 0.0299,	0.6489 s / batch. (data: 5.93e-03). ETA=18:25:54, max mem: 15.9 GB 
[10/28 21:17:58 visual_prompt]: 	Training 700/1106. train loss: 1.0372,	0.6461 s / batch. (data: 7.99e-04). ETA=18:20:01, max mem: 15.9 GB 
[10/28 21:19:01 visual_prompt]: 	Training 800/1106. train loss: 1.0352,	0.6370 s / batch. (data: 7.63e-04). ETA=18:03:29, max mem: 15.9 GB 
[10/28 21:20:04 visual_prompt]: 	Training 900/1106. train loss: 0.0647,	0.6492 s / batch. (data: 7.10e-04). ETA=18:23:12, max mem: 15.9 GB 
[10/28 21:21:07 visual_prompt]: 	Training 1000/1106. train loss: 0.8620,	0.6190 s / batch. (data: 3.33e-04). ETA=17:30:54, max mem: 15.9 GB 
[10/28 21:22:11 visual_prompt]: 	Training 1100/1106. train loss: 1.5163,	0.6176 s / batch. (data: 1.49e-04). ETA=17:27:22, max mem: 15.9 GB 
[10/28 21:22:14 visual_prompt]: Epoch 8 / 100: avg data time: 4.22e-03, avg batch time: 0.6343, average train loss: 1.4351
[10/28 21:23:04 visual_prompt]: 	Test 100/123. loss: 0.788, 0.2504 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[10/28 21:23:15 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2316, average loss: 0.7460
[10/28 21:23:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.77	
[10/28 21:23:15 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/28 21:24:21 visual_prompt]: 	Training 100/1106. train loss: 3.7750,	0.6372 s / batch. (data: 7.66e-04). ETA=17:59:36, max mem: 15.9 GB 
[10/28 21:25:24 visual_prompt]: 	Training 200/1106. train loss: 0.0066,	0.6720 s / batch. (data: 7.90e-04). ETA=18:57:21, max mem: 15.9 GB 
[10/28 21:26:27 visual_prompt]: 	Training 300/1106. train loss: 0.8247,	0.6194 s / batch. (data: 3.27e-04). ETA=17:27:24, max mem: 15.9 GB 
[10/28 21:27:30 visual_prompt]: 	Training 400/1106. train loss: 0.0715,	0.6367 s / batch. (data: 5.41e-03). ETA=17:55:27, max mem: 15.9 GB 
[10/28 21:28:34 visual_prompt]: 	Training 500/1106. train loss: 6.0167,	0.6519 s / batch. (data: 8.30e-04). ETA=18:20:10, max mem: 15.9 GB 
[10/28 21:29:37 visual_prompt]: 	Training 600/1106. train loss: 0.9354,	0.6393 s / batch. (data: 7.70e-04). ETA=17:57:48, max mem: 15.9 GB 
[10/28 21:30:40 visual_prompt]: 	Training 700/1106. train loss: 0.1988,	0.6445 s / batch. (data: 8.36e-04). ETA=18:05:25, max mem: 15.9 GB 
[10/28 21:31:44 visual_prompt]: 	Training 800/1106. train loss: 0.9805,	0.6402 s / batch. (data: 3.23e-04). ETA=17:57:12, max mem: 15.9 GB 
[10/28 21:32:47 visual_prompt]: 	Training 900/1106. train loss: 0.0146,	0.6573 s / batch. (data: 2.53e-02). ETA=18:24:45, max mem: 15.9 GB 
[10/28 21:33:51 visual_prompt]: 	Training 1000/1106. train loss: 0.2548,	0.6185 s / batch. (data: 3.20e-04). ETA=17:18:34, max mem: 15.9 GB 
[10/28 21:34:54 visual_prompt]: 	Training 1100/1106. train loss: 0.6895,	0.6177 s / batch. (data: 1.70e-04). ETA=17:16:13, max mem: 15.9 GB 
[10/28 21:34:58 visual_prompt]: Epoch 9 / 100: avg data time: 4.71e-03, avg batch time: 0.6354, average train loss: 1.2667
[10/28 21:35:48 visual_prompt]: 	Test 100/123. loss: 1.153, 0.2252 s / batch. (data: 2.48e-05)max mem: 15.94594 GB 
[10/28 21:35:58 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2331, average loss: 1.0548
[10/28 21:35:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.16	
[10/28 21:35:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/28 21:37:03 visual_prompt]: 	Training 100/1106. train loss: 2.7076,	0.6353 s / batch. (data: 7.71e-04). ETA=17:44:39, max mem: 15.9 GB 
[10/28 21:38:06 visual_prompt]: 	Training 200/1106. train loss: 13.2166,	0.6512 s / batch. (data: 8.26e-04). ETA=18:10:08, max mem: 15.9 GB 
[10/28 21:39:09 visual_prompt]: 	Training 300/1106. train loss: 2.8747,	0.6317 s / batch. (data: 3.17e-04). ETA=17:36:28, max mem: 15.9 GB 
[10/28 21:40:12 visual_prompt]: 	Training 400/1106. train loss: 0.1393,	0.6320 s / batch. (data: 7.64e-04). ETA=17:35:53, max mem: 15.9 GB 
[10/28 21:41:16 visual_prompt]: 	Training 500/1106. train loss: 1.2922,	0.6672 s / batch. (data: 3.10e-02). ETA=18:33:33, max mem: 15.9 GB 
[10/28 21:42:19 visual_prompt]: 	Training 600/1106. train loss: 6.0538,	0.6566 s / batch. (data: 1.65e-02). ETA=18:14:45, max mem: 15.9 GB 
[10/28 21:43:22 visual_prompt]: 	Training 700/1106. train loss: 4.2101,	0.6350 s / batch. (data: 8.40e-04). ETA=17:37:40, max mem: 15.9 GB 
[10/28 21:44:26 visual_prompt]: 	Training 800/1106. train loss: 2.2905,	0.6447 s / batch. (data: 8.03e-04). ETA=17:52:48, max mem: 15.9 GB 
[10/28 21:45:29 visual_prompt]: 	Training 900/1106. train loss: 2.5111,	0.6347 s / batch. (data: 2.90e-04). ETA=17:35:12, max mem: 15.9 GB 
[10/28 21:46:32 visual_prompt]: 	Training 1000/1106. train loss: 0.0005,	0.6316 s / batch. (data: 9.94e-03). ETA=17:28:59, max mem: 15.9 GB 
[10/28 21:47:36 visual_prompt]: 	Training 1100/1106. train loss: 0.0000,	0.6183 s / batch. (data: 1.37e-04). ETA=17:05:49, max mem: 15.9 GB 
[10/28 21:47:40 visual_prompt]: Epoch 10 / 100: avg data time: 3.93e-03, avg batch time: 0.6344, average train loss: 2.2191
[10/28 21:48:29 visual_prompt]: 	Test 100/123. loss: 4.717, 0.2251 s / batch. (data: 5.46e-05)max mem: 15.94594 GB 
[10/28 21:48:40 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2329, average loss: 4.2373
[10/28 21:48:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.88	
[10/28 21:48:40 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/28 21:49:45 visual_prompt]: 	Training 100/1106. train loss: 0.0032,	0.6415 s / batch. (data: 7.96e-04). ETA=17:43:13, max mem: 15.9 GB 
[10/28 21:50:49 visual_prompt]: 	Training 200/1106. train loss: 1.0482,	0.6172 s / batch. (data: 2.68e-04). ETA=17:01:49, max mem: 15.9 GB 
[10/28 21:51:52 visual_prompt]: 	Training 300/1106. train loss: 5.6663,	0.6302 s / batch. (data: 8.28e-04). ETA=17:22:18, max mem: 15.9 GB 
[10/28 21:52:55 visual_prompt]: 	Training 400/1106. train loss: 0.2083,	0.6395 s / batch. (data: 8.02e-04). ETA=17:36:36, max mem: 15.9 GB 
[10/28 21:53:59 visual_prompt]: 	Training 500/1106. train loss: 0.6669,	0.6184 s / batch. (data: 3.07e-04). ETA=17:00:45, max mem: 15.9 GB 
[10/28 21:55:02 visual_prompt]: 	Training 600/1106. train loss: 0.0000,	0.6545 s / batch. (data: 7.84e-04). ETA=17:59:21, max mem: 15.9 GB 
[10/28 21:56:05 visual_prompt]: 	Training 700/1106. train loss: 1.6377,	0.6632 s / batch. (data: 3.35e-02). ETA=18:12:35, max mem: 15.9 GB 
[10/28 21:57:09 visual_prompt]: 	Training 800/1106. train loss: 6.2017,	0.6360 s / batch. (data: 8.07e-04). ETA=17:26:38, max mem: 15.9 GB 
[10/28 21:58:12 visual_prompt]: 	Training 900/1106. train loss: 0.1047,	0.6344 s / batch. (data: 3.33e-04). ETA=17:22:59, max mem: 15.9 GB 
[10/28 21:59:15 visual_prompt]: 	Training 1000/1106. train loss: 0.0007,	0.6325 s / batch. (data: 8.23e-04). ETA=17:18:51, max mem: 15.9 GB 
[10/28 22:00:18 visual_prompt]: 	Training 1100/1106. train loss: 1.2528,	0.6177 s / batch. (data: 1.36e-04). ETA=16:53:30, max mem: 15.9 GB 
[10/28 22:00:22 visual_prompt]: Epoch 11 / 100: avg data time: 4.34e-03, avg batch time: 0.6347, average train loss: 2.0673
[10/28 22:01:12 visual_prompt]: 	Test 100/123. loss: 0.783, 0.2400 s / batch. (data: 4.34e-05)max mem: 15.94594 GB 
[10/28 22:01:22 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.2316, average loss: 0.7404
[10/28 22:01:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.34	
[10/28 22:01:22 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/28 22:02:29 visual_prompt]: 	Training 100/1106. train loss: 0.6058,	0.6436 s / batch. (data: 8.62e-04). ETA=17:34:46, max mem: 15.9 GB 
[10/28 22:03:33 visual_prompt]: 	Training 200/1106. train loss: 0.7149,	0.6340 s / batch. (data: 8.30e-04). ETA=17:17:58, max mem: 15.9 GB 
[10/28 22:04:36 visual_prompt]: 	Training 300/1106. train loss: 2.8198,	0.6355 s / batch. (data: 1.58e-02). ETA=17:19:23, max mem: 15.9 GB 
[10/28 22:05:39 visual_prompt]: 	Training 400/1106. train loss: 0.0540,	0.6202 s / batch. (data: 3.11e-04). ETA=16:53:20, max mem: 15.9 GB 
[10/28 22:06:43 visual_prompt]: 	Training 500/1106. train loss: 0.1358,	0.6364 s / batch. (data: 7.88e-04). ETA=17:18:48, max mem: 15.9 GB 
[10/28 22:07:46 visual_prompt]: 	Training 600/1106. train loss: 0.2335,	0.6352 s / batch. (data: 2.83e-04). ETA=17:15:42, max mem: 15.9 GB 
[10/28 22:08:50 visual_prompt]: 	Training 700/1106. train loss: 1.7902,	0.6207 s / batch. (data: 2.21e-04). ETA=16:51:00, max mem: 15.9 GB 
[10/28 22:09:53 visual_prompt]: 	Training 800/1106. train loss: 1.8791,	0.6185 s / batch. (data: 3.15e-04). ETA=16:46:30, max mem: 15.9 GB 
[10/28 22:10:56 visual_prompt]: 	Training 900/1106. train loss: 2.1756,	0.6812 s / batch. (data: 5.00e-02). ETA=18:27:18, max mem: 15.9 GB 
[10/28 22:11:59 visual_prompt]: 	Training 1000/1106. train loss: 1.5349,	0.6326 s / batch. (data: 8.16e-04). ETA=17:07:21, max mem: 15.9 GB 
[10/28 22:13:03 visual_prompt]: 	Training 1100/1106. train loss: 7.2217,	0.6187 s / batch. (data: 1.25e-04). ETA=16:43:44, max mem: 15.9 GB 
[10/28 22:13:07 visual_prompt]: Epoch 12 / 100: avg data time: 5.82e-03, avg batch time: 0.6368, average train loss: 2.1015
[10/28 22:13:56 visual_prompt]: 	Test 100/123. loss: 1.033, 0.2261 s / batch. (data: 2.72e-05)max mem: 15.94594 GB 
[10/28 22:14:07 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2318, average loss: 0.9461
[10/28 22:14:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.33	
[10/28 22:14:07 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/28 22:15:12 visual_prompt]: 	Training 100/1106. train loss: 0.3523,	0.6319 s / batch. (data: 3.14e-04). ETA=17:03:53, max mem: 15.9 GB 
[10/28 22:16:16 visual_prompt]: 	Training 200/1106. train loss: 0.7034,	0.6292 s / batch. (data: 2.68e-04). ETA=16:58:32, max mem: 15.9 GB 
[10/28 22:17:19 visual_prompt]: 	Training 300/1106. train loss: 3.1854,	0.6190 s / batch. (data: 3.32e-04). ETA=16:40:57, max mem: 15.9 GB 
[10/28 22:18:22 visual_prompt]: 	Training 400/1106. train loss: 3.0419,	0.6294 s / batch. (data: 8.23e-04). ETA=16:56:43, max mem: 15.9 GB 
[10/28 22:19:25 visual_prompt]: 	Training 500/1106. train loss: 0.0503,	0.6317 s / batch. (data: 2.93e-04). ETA=16:59:30, max mem: 15.9 GB 
[10/28 22:20:28 visual_prompt]: 	Training 600/1106. train loss: 5.1426,	0.6445 s / batch. (data: 8.75e-04). ETA=17:19:01, max mem: 15.9 GB 
[10/28 22:21:32 visual_prompt]: 	Training 700/1106. train loss: 0.1257,	0.6181 s / batch. (data: 3.32e-04). ETA=16:35:23, max mem: 15.9 GB 
[10/28 22:22:35 visual_prompt]: 	Training 800/1106. train loss: 3.2278,	0.6332 s / batch. (data: 3.22e-04). ETA=16:58:41, max mem: 15.9 GB 
[10/28 22:23:38 visual_prompt]: 	Training 900/1106. train loss: 0.0013,	0.6312 s / batch. (data: 1.27e-02). ETA=16:54:23, max mem: 15.9 GB 
[10/28 22:24:41 visual_prompt]: 	Training 1000/1106. train loss: 0.0004,	0.6329 s / batch. (data: 7.97e-04). ETA=16:56:04, max mem: 15.9 GB 
[10/28 22:25:45 visual_prompt]: 	Training 1100/1106. train loss: 1.7218,	0.6189 s / batch. (data: 2.56e-04). ETA=16:32:32, max mem: 15.9 GB 
[10/28 22:25:49 visual_prompt]: Epoch 13 / 100: avg data time: 4.31e-03, avg batch time: 0.6342, average train loss: 1.8972
[10/28 22:26:38 visual_prompt]: 	Test 100/123. loss: 0.720, 0.2277 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/28 22:26:49 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.2324, average loss: 0.6996
[10/28 22:26:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.26	
[10/28 22:26:49 visual_prompt]: Best epoch 13: best metric: -0.700
[10/28 22:26:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/28 22:27:55 visual_prompt]: 	Training 100/1106. train loss: 1.3153,	0.6167 s / batch. (data: 3.32e-04). ETA=16:28:00, max mem: 15.9 GB 
[10/28 22:28:58 visual_prompt]: 	Training 200/1106. train loss: 6.3511,	0.6184 s / batch. (data: 3.46e-04). ETA=16:29:42, max mem: 15.9 GB 
[10/28 22:30:01 visual_prompt]: 	Training 300/1106. train loss: 3.1502,	0.6341 s / batch. (data: 3.85e-04). ETA=16:53:44, max mem: 15.9 GB 
[10/28 22:31:04 visual_prompt]: 	Training 400/1106. train loss: 0.0003,	0.6304 s / batch. (data: 7.28e-04). ETA=16:46:49, max mem: 15.9 GB 
[10/28 22:32:08 visual_prompt]: 	Training 500/1106. train loss: 2.8850,	0.6204 s / batch. (data: 1.04e-03). ETA=16:29:42, max mem: 15.9 GB 
[10/28 22:33:11 visual_prompt]: 	Training 600/1106. train loss: 0.6341,	0.6324 s / batch. (data: 4.89e-04). ETA=16:47:49, max mem: 15.9 GB 
[10/28 22:34:15 visual_prompt]: 	Training 700/1106. train loss: 0.2914,	0.6529 s / batch. (data: 1.29e-02). ETA=17:19:27, max mem: 15.9 GB 
[10/28 22:35:18 visual_prompt]: 	Training 800/1106. train loss: 3.3590,	0.6336 s / batch. (data: 8.17e-04). ETA=16:47:35, max mem: 15.9 GB 
[10/28 22:36:21 visual_prompt]: 	Training 900/1106. train loss: 1.4958,	0.6332 s / batch. (data: 8.29e-04). ETA=16:45:56, max mem: 15.9 GB 
[10/28 22:37:24 visual_prompt]: 	Training 1000/1106. train loss: 1.3134,	0.6181 s / batch. (data: 3.19e-04). ETA=16:20:52, max mem: 15.9 GB 
[10/28 22:38:27 visual_prompt]: 	Training 1100/1106. train loss: 1.1257,	0.6190 s / batch. (data: 1.49e-04). ETA=16:21:20, max mem: 15.9 GB 
[10/28 22:38:31 visual_prompt]: Epoch 14 / 100: avg data time: 4.27e-03, avg batch time: 0.6347, average train loss: 2.1789
[10/28 22:39:21 visual_prompt]: 	Test 100/123. loss: 1.530, 0.2316 s / batch. (data: 5.08e-05)max mem: 15.94594 GB 
[10/28 22:39:31 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2320, average loss: 1.3824
[10/28 22:39:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.44	
[10/28 22:39:31 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/28 22:40:36 visual_prompt]: 	Training 100/1106. train loss: 3.7171,	0.6335 s / batch. (data: 2.99e-04). ETA=16:43:17, max mem: 15.9 GB 
[10/28 22:41:40 visual_prompt]: 	Training 200/1106. train loss: 2.9904,	0.6245 s / batch. (data: 3.12e-04). ETA=16:27:55, max mem: 15.9 GB 
[10/28 22:42:43 visual_prompt]: 	Training 300/1106. train loss: 5.3345,	0.6524 s / batch. (data: 8.03e-04). ETA=17:10:57, max mem: 15.9 GB 
[10/28 22:43:46 visual_prompt]: 	Training 400/1106. train loss: 22.2763,	0.6420 s / batch. (data: 7.46e-04). ETA=16:53:30, max mem: 15.9 GB 
[10/28 22:44:50 visual_prompt]: 	Training 500/1106. train loss: 2.5346,	0.6269 s / batch. (data: 3.28e-04). ETA=16:28:37, max mem: 15.9 GB 
[10/28 22:45:53 visual_prompt]: 	Training 600/1106. train loss: 1.3442,	0.6317 s / batch. (data: 7.76e-04). ETA=16:35:06, max mem: 15.9 GB 
[10/28 22:46:56 visual_prompt]: 	Training 700/1106. train loss: 2.7923,	0.6272 s / batch. (data: 2.91e-04). ETA=16:26:56, max mem: 15.9 GB 
[10/28 22:47:59 visual_prompt]: 	Training 800/1106. train loss: 0.6523,	0.6193 s / batch. (data: 3.33e-04). ETA=16:13:32, max mem: 15.9 GB 
[10/28 22:49:03 visual_prompt]: 	Training 900/1106. train loss: 4.4683,	0.6476 s / batch. (data: 3.51e-04). ETA=16:56:51, max mem: 15.9 GB 
[10/28 22:50:06 visual_prompt]: 	Training 1000/1106. train loss: 1.6101,	0.6331 s / batch. (data: 1.52e-02). ETA=16:33:04, max mem: 15.9 GB 
[10/28 22:51:10 visual_prompt]: 	Training 1100/1106. train loss: 4.1407,	0.6186 s / batch. (data: 1.36e-04). ETA=16:09:18, max mem: 15.9 GB 
[10/28 22:51:13 visual_prompt]: Epoch 15 / 100: avg data time: 3.48e-03, avg batch time: 0.6347, average train loss: 2.5590
[10/28 22:52:04 visual_prompt]: 	Test 100/123. loss: 1.617, 0.2254 s / batch. (data: 4.22e-05)max mem: 15.94594 GB 
[10/28 22:52:14 visual_prompt]: Inference (val):avg data time: 9.28e-05, avg batch time: 0.2324, average loss: 1.7767
[10/28 22:52:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.84	
[10/28 22:52:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/28 22:53:19 visual_prompt]: 	Training 100/1106. train loss: 0.0045,	0.6240 s / batch. (data: 3.08e-04). ETA=16:16:40, max mem: 15.9 GB 
[10/28 22:54:22 visual_prompt]: 	Training 200/1106. train loss: 2.0416,	0.6192 s / batch. (data: 3.12e-04). ETA=16:08:05, max mem: 15.9 GB 
[10/28 22:55:26 visual_prompt]: 	Training 300/1106. train loss: 0.0032,	0.6683 s / batch. (data: 8.23e-04). ETA=17:23:44, max mem: 15.9 GB 
[10/28 22:56:29 visual_prompt]: 	Training 400/1106. train loss: 6.2225,	0.6309 s / batch. (data: 8.40e-04). ETA=16:24:22, max mem: 15.9 GB 
[10/28 22:57:32 visual_prompt]: 	Training 500/1106. train loss: 3.7405,	0.6348 s / batch. (data: 3.09e-04). ETA=16:29:21, max mem: 15.9 GB 
[10/28 22:58:35 visual_prompt]: 	Training 600/1106. train loss: 0.9931,	0.6389 s / batch. (data: 9.20e-04). ETA=16:34:36, max mem: 15.9 GB 
[10/28 22:59:38 visual_prompt]: 	Training 700/1106. train loss: 2.7807,	0.6187 s / batch. (data: 3.40e-04). ETA=16:02:10, max mem: 15.9 GB 
[10/28 23:00:42 visual_prompt]: 	Training 800/1106. train loss: 1.9996,	0.6338 s / batch. (data: 2.64e-04). ETA=16:24:35, max mem: 15.9 GB 
[10/28 23:01:45 visual_prompt]: 	Training 900/1106. train loss: 0.8242,	0.6377 s / batch. (data: 7.85e-04). ETA=16:29:33, max mem: 15.9 GB 
[10/28 23:02:48 visual_prompt]: 	Training 1000/1106. train loss: 4.9615,	0.6382 s / batch. (data: 4.10e-03). ETA=16:29:15, max mem: 15.9 GB 
[10/28 23:03:52 visual_prompt]: 	Training 1100/1106. train loss: 0.0337,	0.6182 s / batch. (data: 1.44e-04). ETA=15:57:19, max mem: 15.9 GB 
[10/28 23:03:56 visual_prompt]: Epoch 16 / 100: avg data time: 3.91e-03, avg batch time: 0.6343, average train loss: 2.2271
[10/28 23:04:46 visual_prompt]: 	Test 100/123. loss: 1.567, 0.2258 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[10/28 23:04:56 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.2313, average loss: 1.4114
[10/28 23:04:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.57	
[10/28 23:04:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/28 23:06:02 visual_prompt]: 	Training 100/1106. train loss: 0.0982,	0.6575 s / batch. (data: 1.34e-02). ETA=16:56:54, max mem: 15.9 GB 
[10/28 23:07:05 visual_prompt]: 	Training 200/1106. train loss: 0.6638,	0.6325 s / batch. (data: 3.21e-04). ETA=16:17:13, max mem: 15.9 GB 
[10/28 23:08:08 visual_prompt]: 	Training 300/1106. train loss: 0.3807,	0.6208 s / batch. (data: 7.79e-04). ETA=15:58:08, max mem: 15.9 GB 
[10/28 23:09:12 visual_prompt]: 	Training 400/1106. train loss: 0.0943,	0.6305 s / batch. (data: 8.47e-04). ETA=16:12:07, max mem: 15.9 GB 
[10/28 23:10:15 visual_prompt]: 	Training 500/1106. train loss: 0.0079,	0.6370 s / batch. (data: 7.93e-04). ETA=16:20:57, max mem: 15.9 GB 
[10/28 23:11:18 visual_prompt]: 	Training 600/1106. train loss: 1.6207,	0.6631 s / batch. (data: 3.19e-02). ETA=17:00:08, max mem: 15.9 GB 
[10/28 23:12:22 visual_prompt]: 	Training 700/1106. train loss: 1.2343,	0.6239 s / batch. (data: 2.99e-04). ETA=15:58:45, max mem: 15.9 GB 
[10/28 23:13:25 visual_prompt]: 	Training 800/1106. train loss: 3.0629,	0.6291 s / batch. (data: 3.51e-04). ETA=16:05:44, max mem: 15.9 GB 
[10/28 23:14:28 visual_prompt]: 	Training 900/1106. train loss: 3.2783,	0.6379 s / batch. (data: 1.39e-02). ETA=16:18:08, max mem: 15.9 GB 
[10/28 23:15:31 visual_prompt]: 	Training 1000/1106. train loss: 0.0500,	0.6459 s / batch. (data: 7.82e-04). ETA=16:29:17, max mem: 15.9 GB 
[10/28 23:16:35 visual_prompt]: 	Training 1100/1106. train loss: 0.0044,	0.6193 s / batch. (data: 1.52e-04). ETA=15:47:30, max mem: 15.9 GB 
[10/28 23:16:39 visual_prompt]: Epoch 17 / 100: avg data time: 4.39e-03, avg batch time: 0.6351, average train loss: 1.9155
[10/28 23:17:28 visual_prompt]: 	Test 100/123. loss: 2.272, 0.2389 s / batch. (data: 5.29e-05)max mem: 15.94594 GB 
[10/28 23:17:39 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2329, average loss: 2.4930
[10/28 23:17:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.01	
[10/28 23:17:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/28 23:18:44 visual_prompt]: 	Training 100/1106. train loss: 0.0000,	0.6302 s / batch. (data: 3.25e-04). ETA=16:03:06, max mem: 15.9 GB 
[10/28 23:19:48 visual_prompt]: 	Training 200/1106. train loss: 9.5798,	0.6317 s / batch. (data: 8.02e-04). ETA=16:04:24, max mem: 15.9 GB 
[10/28 23:20:51 visual_prompt]: 	Training 300/1106. train loss: 11.1166,	0.6270 s / batch. (data: 3.44e-04). ETA=15:56:12, max mem: 15.9 GB 
[10/28 23:21:55 visual_prompt]: 	Training 400/1106. train loss: 4.9663,	0.6314 s / batch. (data: 7.69e-04). ETA=16:01:48, max mem: 15.9 GB 
[10/28 23:22:58 visual_prompt]: 	Training 500/1106. train loss: 0.0000,	0.6243 s / batch. (data: 3.21e-04). ETA=15:49:58, max mem: 15.9 GB 
[10/28 23:24:01 visual_prompt]: 	Training 600/1106. train loss: 0.0947,	0.6497 s / batch. (data: 4.50e-04). ETA=16:27:32, max mem: 15.9 GB 
[10/28 23:25:04 visual_prompt]: 	Training 700/1106. train loss: 0.0293,	0.6290 s / batch. (data: 3.27e-04). ETA=15:55:01, max mem: 15.9 GB 
[10/28 23:26:08 visual_prompt]: 	Training 800/1106. train loss: 1.3220,	0.6307 s / batch. (data: 9.59e-04). ETA=15:56:35, max mem: 15.9 GB 
[10/28 23:27:11 visual_prompt]: 	Training 900/1106. train loss: 7.4184,	0.6246 s / batch. (data: 5.46e-03). ETA=15:46:13, max mem: 15.9 GB 
[10/28 23:28:14 visual_prompt]: 	Training 1000/1106. train loss: 1.6034,	0.6197 s / batch. (data: 3.36e-04). ETA=15:37:49, max mem: 15.9 GB 
[10/28 23:29:17 visual_prompt]: 	Training 1100/1106. train loss: 0.0001,	0.6185 s / batch. (data: 1.44e-04). ETA=15:35:01, max mem: 15.9 GB 
[10/28 23:29:21 visual_prompt]: Epoch 18 / 100: avg data time: 4.28e-03, avg batch time: 0.6348, average train loss: 2.5334
[10/28 23:30:11 visual_prompt]: 	Test 100/123. loss: 1.311, 0.2264 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/28 23:30:21 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2333, average loss: 1.4359
[10/28 23:30:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.28	
[10/28 23:30:21 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/28 23:31:27 visual_prompt]: 	Training 100/1106. train loss: 0.1340,	0.6292 s / batch. (data: 3.16e-04). ETA=15:50:01, max mem: 15.9 GB 
[10/28 23:32:30 visual_prompt]: 	Training 200/1106. train loss: 2.5637,	0.6413 s / batch. (data: 8.14e-04). ETA=16:07:11, max mem: 15.9 GB 
[10/28 23:33:33 visual_prompt]: 	Training 300/1106. train loss: 0.0384,	0.6341 s / batch. (data: 3.21e-04). ETA=15:55:17, max mem: 15.9 GB 
[10/28 23:34:37 visual_prompt]: 	Training 400/1106. train loss: 1.0045,	0.6331 s / batch. (data: 7.83e-04). ETA=15:52:42, max mem: 15.9 GB 
[10/28 23:35:40 visual_prompt]: 	Training 500/1106. train loss: 0.8133,	0.6514 s / batch. (data: 7.50e-04). ETA=16:19:12, max mem: 15.9 GB 
[10/28 23:36:43 visual_prompt]: 	Training 600/1106. train loss: 0.3121,	0.6192 s / batch. (data: 3.17e-04). ETA=15:29:41, max mem: 15.9 GB 
[10/28 23:37:47 visual_prompt]: 	Training 700/1106. train loss: 1.0269,	0.6373 s / batch. (data: 7.86e-04). ETA=15:55:51, max mem: 15.9 GB 
[10/28 23:38:50 visual_prompt]: 	Training 800/1106. train loss: 0.0080,	0.6332 s / batch. (data: 2.81e-04). ETA=15:48:39, max mem: 15.9 GB 
[10/28 23:39:53 visual_prompt]: 	Training 900/1106. train loss: 0.2401,	0.6298 s / batch. (data: 3.15e-04). ETA=15:42:32, max mem: 15.9 GB 
[10/28 23:40:56 visual_prompt]: 	Training 1000/1106. train loss: 0.0002,	0.6479 s / batch. (data: 7.52e-04). ETA=16:08:35, max mem: 15.9 GB 
[10/28 23:42:00 visual_prompt]: 	Training 1100/1106. train loss: 7.9721,	0.6185 s / batch. (data: 1.74e-04). ETA=15:23:30, max mem: 15.9 GB 
[10/28 23:42:04 visual_prompt]: Epoch 19 / 100: avg data time: 4.32e-03, avg batch time: 0.6349, average train loss: 2.2048
[10/28 23:42:54 visual_prompt]: 	Test 100/123. loss: 11.016, 0.2325 s / batch. (data: 4.29e-05)max mem: 15.94594 GB 
[10/28 23:43:04 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2331, average loss: 9.9366
[10/28 23:43:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.58	
[10/28 23:43:04 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[10/28 23:44:10 visual_prompt]: 	Training 100/1106. train loss: 2.2060,	0.6364 s / batch. (data: 5.89e-03). ETA=15:49:09, max mem: 15.9 GB 
[10/28 23:45:13 visual_prompt]: 	Training 200/1106. train loss: 1.6018,	0.6193 s / batch. (data: 3.26e-04). ETA=15:22:35, max mem: 15.9 GB 
[10/28 23:46:17 visual_prompt]: 	Training 300/1106. train loss: 1.2353,	0.6469 s / batch. (data: 3.36e-04). ETA=16:02:39, max mem: 15.9 GB 
[10/28 23:47:20 visual_prompt]: 	Training 400/1106. train loss: 0.6808,	0.6186 s / batch. (data: 3.29e-04). ETA=15:19:26, max mem: 15.9 GB 
[10/28 23:48:23 visual_prompt]: 	Training 500/1106. train loss: 0.7397,	0.6319 s / batch. (data: 5.00e-04). ETA=15:38:16, max mem: 15.9 GB 
[10/28 23:49:26 visual_prompt]: 	Training 600/1106. train loss: 2.6039,	0.6181 s / batch. (data: 3.11e-04). ETA=15:16:43, max mem: 15.9 GB 
[10/28 23:50:30 visual_prompt]: 	Training 700/1106. train loss: 1.5588,	0.6666 s / batch. (data: 3.74e-02). ETA=16:27:32, max mem: 15.9 GB 
[10/28 23:51:33 visual_prompt]: 	Training 800/1106. train loss: 0.1803,	0.6206 s / batch. (data: 7.51e-04). ETA=15:18:21, max mem: 15.9 GB 
[10/28 23:52:36 visual_prompt]: 	Training 900/1106. train loss: 4.5478,	0.6454 s / batch. (data: 7.87e-04). ETA=15:53:56, max mem: 15.9 GB 
[10/28 23:53:39 visual_prompt]: 	Training 1000/1106. train loss: 0.1848,	0.6471 s / batch. (data: 8.82e-04). ETA=15:55:23, max mem: 15.9 GB 
[10/28 23:54:42 visual_prompt]: 	Training 1100/1106. train loss: 1.7854,	0.6179 s / batch. (data: 1.39e-04). ETA=15:11:11, max mem: 15.9 GB 
[10/28 23:54:46 visual_prompt]: Epoch 20 / 100: avg data time: 4.48e-03, avg batch time: 0.6343, average train loss: 1.5617
[10/28 23:55:36 visual_prompt]: 	Test 100/123. loss: 0.692, 0.2326 s / batch. (data: 4.67e-05)max mem: 15.94594 GB 
[10/28 23:55:46 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.2331, average loss: 0.7107
[10/28 23:55:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.23	
[10/28 23:55:46 visual_prompt]: Stopping early.
[10/28 23:55:46 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 23:55:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 23:55:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 23:55:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 23:55:46 visual_prompt]: Training with config:
[10/28 23:55:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 23:55:46 visual_prompt]: Loading training data...
[10/28 23:55:46 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 23:55:46 visual_prompt]: Loading validation data...
[10/28 23:55:46 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 23:55:46 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/28 23:55:49 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/28 23:55:49 visual_prompt]: tuned percent:0.522
[10/28 23:55:49 visual_prompt]: Device used for model: 0
[10/28 23:55:49 visual_prompt]: Setting up Evaluator...
[10/28 23:55:49 visual_prompt]: Setting up Trainer...
[10/28 23:55:49 visual_prompt]: 	Setting up the optimizer...
[10/28 23:55:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 23:56:54 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6305 s / batch. (data: 3.28e-04). ETA=19:21:06, max mem: 15.9 GB 
[10/28 23:57:58 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6344 s / batch. (data: 7.87e-04). ETA=19:27:19, max mem: 15.9 GB 
[10/28 23:59:01 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6354 s / batch. (data: 8.45e-04). ETA=19:28:02, max mem: 15.9 GB 
[10/29 00:00:04 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6356 s / batch. (data: 7.87e-04). ETA=19:27:21, max mem: 15.9 GB 
[10/29 00:01:07 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6275 s / batch. (data: 3.14e-04). ETA=19:11:25, max mem: 15.9 GB 
[10/29 00:02:11 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6183 s / batch. (data: 2.79e-04). ETA=18:53:30, max mem: 15.9 GB 
[10/29 00:03:14 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6340 s / batch. (data: 3.29e-04). ETA=19:21:12, max mem: 15.9 GB 
[10/29 00:04:17 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6444 s / batch. (data: 7.85e-04). ETA=19:39:20, max mem: 15.9 GB 
[10/29 00:05:21 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6394 s / batch. (data: 8.07e-04). ETA=19:29:06, max mem: 15.9 GB 
[10/29 00:06:24 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6429 s / batch. (data: 8.37e-04). ETA=19:34:17, max mem: 15.9 GB 
[10/29 00:07:27 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6177 s / batch. (data: 1.44e-04). ETA=18:47:14, max mem: 15.9 GB 
[10/29 00:07:31 visual_prompt]: Epoch 1 / 100: avg data time: 4.45e-03, avg batch time: 0.6346, average train loss: 1.4028
[10/29 00:08:21 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2256 s / batch. (data: 5.58e-05)max mem: 15.94594 GB 
[10/29 00:08:31 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2327, average loss: 1.3505
[10/29 00:08:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/29 00:08:31 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/29 00:09:36 visual_prompt]: 	Training 100/1106. train loss: 0.9820,	0.6400 s / batch. (data: 8.53e-04). ETA=19:26:56, max mem: 15.9 GB 
[10/29 00:10:39 visual_prompt]: 	Training 200/1106. train loss: 0.8091,	0.6393 s / batch. (data: 7.64e-04). ETA=19:24:32, max mem: 15.9 GB 
[10/29 00:11:43 visual_prompt]: 	Training 300/1106. train loss: 0.4332,	0.6432 s / batch. (data: 7.82e-04). ETA=19:30:34, max mem: 15.9 GB 
[10/29 00:12:46 visual_prompt]: 	Training 400/1106. train loss: 0.0725,	0.6204 s / batch. (data: 3.32e-04). ETA=18:48:05, max mem: 15.9 GB 
[10/29 00:13:49 visual_prompt]: 	Training 500/1106. train loss: 1.0456,	0.6198 s / batch. (data: 3.29e-04). ETA=18:45:49, max mem: 15.9 GB 
[10/29 00:14:53 visual_prompt]: 	Training 600/1106. train loss: 0.5826,	0.6317 s / batch. (data: 7.86e-04). ETA=19:06:27, max mem: 15.9 GB 
[10/29 00:15:56 visual_prompt]: 	Training 700/1106. train loss: 0.9912,	0.6371 s / batch. (data: 3.22e-04). ETA=19:15:15, max mem: 15.9 GB 
[10/29 00:16:59 visual_prompt]: 	Training 800/1106. train loss: 0.7769,	0.6233 s / batch. (data: 3.28e-04). ETA=18:49:06, max mem: 15.9 GB 
[10/29 00:18:02 visual_prompt]: 	Training 900/1106. train loss: 1.1257,	0.6399 s / batch. (data: 8.20e-04). ETA=19:18:14, max mem: 15.9 GB 
[10/29 00:19:06 visual_prompt]: 	Training 1000/1106. train loss: 0.2383,	0.6283 s / batch. (data: 1.05e-02). ETA=18:56:01, max mem: 15.9 GB 
[10/29 00:20:09 visual_prompt]: 	Training 1100/1106. train loss: 0.3425,	0.6181 s / batch. (data: 1.57e-04). ETA=18:36:34, max mem: 15.9 GB 
[10/29 00:20:13 visual_prompt]: Epoch 2 / 100: avg data time: 4.13e-03, avg batch time: 0.6339, average train loss: 0.9134
[10/29 00:21:02 visual_prompt]: 	Test 100/123. loss: 1.445, 0.2365 s / batch. (data: 4.48e-05)max mem: 15.94594 GB 
[10/29 00:21:13 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2317, average loss: 1.3347
[10/29 00:21:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.11	
[10/29 00:21:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/29 00:22:19 visual_prompt]: 	Training 100/1106. train loss: 1.1596,	0.6260 s / batch. (data: 3.37e-04). ETA=18:49:51, max mem: 15.9 GB 
[10/29 00:23:23 visual_prompt]: 	Training 200/1106. train loss: 0.2171,	0.6419 s / batch. (data: 8.76e-03). ETA=19:17:29, max mem: 15.9 GB 
[10/29 00:24:26 visual_prompt]: 	Training 300/1106. train loss: 0.3154,	0.6268 s / batch. (data: 3.35e-04). ETA=18:49:12, max mem: 15.9 GB 
[10/29 00:25:29 visual_prompt]: 	Training 400/1106. train loss: 1.0887,	0.6333 s / batch. (data: 8.38e-04). ETA=18:59:47, max mem: 15.9 GB 
[10/29 00:26:33 visual_prompt]: 	Training 500/1106. train loss: 1.4013,	0.6174 s / batch. (data: 5.09e-04). ETA=18:30:08, max mem: 15.9 GB 
[10/29 00:27:36 visual_prompt]: 	Training 600/1106. train loss: 0.5408,	0.6334 s / batch. (data: 8.03e-04). ETA=18:57:53, max mem: 15.9 GB 
[10/29 00:28:39 visual_prompt]: 	Training 700/1106. train loss: 1.6230,	0.6321 s / batch. (data: 3.26e-04). ETA=18:54:25, max mem: 15.9 GB 
[10/29 00:29:42 visual_prompt]: 	Training 800/1106. train loss: 0.2189,	0.6338 s / batch. (data: 1.21e-03). ETA=18:56:23, max mem: 15.9 GB 
[10/29 00:30:46 visual_prompt]: 	Training 900/1106. train loss: 4.3543,	0.6378 s / batch. (data: 2.06e-02). ETA=19:02:38, max mem: 15.9 GB 
[10/29 00:31:49 visual_prompt]: 	Training 1000/1106. train loss: 0.6540,	0.6355 s / batch. (data: 8.07e-04). ETA=18:57:28, max mem: 15.9 GB 
[10/29 00:32:52 visual_prompt]: 	Training 1100/1106. train loss: 0.8541,	0.6177 s / batch. (data: 1.39e-04). ETA=18:24:32, max mem: 15.9 GB 
[10/29 00:32:56 visual_prompt]: Epoch 3 / 100: avg data time: 5.61e-03, avg batch time: 0.6355, average train loss: 1.0783
[10/29 00:33:45 visual_prompt]: 	Test 100/123. loss: 1.757, 0.2316 s / batch. (data: 2.77e-05)max mem: 15.94594 GB 
[10/29 00:33:56 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.2312, average loss: 1.9067
[10/29 00:33:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.86	
[10/29 00:33:56 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/29 00:35:02 visual_prompt]: 	Training 100/1106. train loss: 0.1815,	0.6280 s / batch. (data: 3.14e-04). ETA=18:41:47, max mem: 15.9 GB 
[10/29 00:36:05 visual_prompt]: 	Training 200/1106. train loss: 4.4267,	0.6448 s / batch. (data: 1.41e-02). ETA=19:10:46, max mem: 15.9 GB 
[10/29 00:37:08 visual_prompt]: 	Training 300/1106. train loss: 0.6551,	0.6344 s / batch. (data: 8.44e-04). ETA=18:51:06, max mem: 15.9 GB 
[10/29 00:38:12 visual_prompt]: 	Training 400/1106. train loss: 0.8583,	0.6434 s / batch. (data: 8.10e-04). ETA=19:06:12, max mem: 15.9 GB 
[10/29 00:39:15 visual_prompt]: 	Training 500/1106. train loss: 0.9071,	0.6400 s / batch. (data: 7.72e-04). ETA=18:59:05, max mem: 15.9 GB 
[10/29 00:40:18 visual_prompt]: 	Training 600/1106. train loss: 0.1409,	0.6421 s / batch. (data: 8.03e-04). ETA=19:01:38, max mem: 15.9 GB 
[10/29 00:41:21 visual_prompt]: 	Training 700/1106. train loss: 3.0129,	0.6185 s / batch. (data: 3.35e-04). ETA=18:18:39, max mem: 15.9 GB 
[10/29 00:42:25 visual_prompt]: 	Training 800/1106. train loss: 1.2964,	0.6317 s / batch. (data: 7.89e-04). ETA=18:41:04, max mem: 15.9 GB 
[10/29 00:43:28 visual_prompt]: 	Training 900/1106. train loss: 2.0467,	0.6590 s / batch. (data: 2.70e-02). ETA=19:28:27, max mem: 15.9 GB 
[10/29 00:44:31 visual_prompt]: 	Training 1000/1106. train loss: 1.0464,	0.6265 s / batch. (data: 3.31e-04). ETA=18:29:44, max mem: 15.9 GB 
[10/29 00:45:35 visual_prompt]: 	Training 1100/1106. train loss: 1.8365,	0.6175 s / batch. (data: 1.65e-04). ETA=18:12:51, max mem: 15.9 GB 
[10/29 00:45:39 visual_prompt]: Epoch 4 / 100: avg data time: 4.83e-03, avg batch time: 0.6347, average train loss: 1.1551
[10/29 00:46:28 visual_prompt]: 	Test 100/123. loss: 0.694, 0.2486 s / batch. (data: 4.36e-05)max mem: 15.94594 GB 
[10/29 00:46:39 visual_prompt]: Inference (val):avg data time: 1.26e-04, avg batch time: 0.2332, average loss: 0.7136
[10/29 00:46:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.13	
[10/29 00:46:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/29 00:47:44 visual_prompt]: 	Training 100/1106. train loss: 0.9047,	0.6211 s / batch. (data: 3.27e-04). ETA=18:18:04, max mem: 15.9 GB 
[10/29 00:48:47 visual_prompt]: 	Training 200/1106. train loss: 0.0439,	0.6385 s / batch. (data: 8.20e-04). ETA=18:47:40, max mem: 15.9 GB 
[10/29 00:49:51 visual_prompt]: 	Training 300/1106. train loss: 1.1250,	0.6346 s / batch. (data: 3.16e-04). ETA=18:39:46, max mem: 15.9 GB 
[10/29 00:50:54 visual_prompt]: 	Training 400/1106. train loss: 3.0378,	0.6196 s / batch. (data: 3.27e-04). ETA=18:12:17, max mem: 15.9 GB 
[10/29 00:51:57 visual_prompt]: 	Training 500/1106. train loss: 0.0386,	0.6208 s / batch. (data: 3.17e-04). ETA=18:13:27, max mem: 15.9 GB 
[10/29 00:53:01 visual_prompt]: 	Training 600/1106. train loss: 2.4878,	0.6294 s / batch. (data: 2.97e-04). ETA=18:27:32, max mem: 15.9 GB 
[10/29 00:54:04 visual_prompt]: 	Training 700/1106. train loss: 1.4022,	0.6309 s / batch. (data: 8.11e-04). ETA=18:29:09, max mem: 15.9 GB 
[10/29 00:55:07 visual_prompt]: 	Training 800/1106. train loss: 0.5186,	0.6309 s / batch. (data: 3.21e-04). ETA=18:28:01, max mem: 15.9 GB 
[10/29 00:56:11 visual_prompt]: 	Training 900/1106. train loss: 1.1092,	0.6398 s / batch. (data: 7.41e-04). ETA=18:42:33, max mem: 15.9 GB 
[10/29 00:57:14 visual_prompt]: 	Training 1000/1106. train loss: 1.8275,	0.6359 s / batch. (data: 3.22e-04). ETA=18:34:40, max mem: 15.9 GB 
[10/29 00:58:17 visual_prompt]: 	Training 1100/1106. train loss: 0.9258,	0.6174 s / batch. (data: 1.98e-04). ETA=18:01:09, max mem: 15.9 GB 
[10/29 00:58:21 visual_prompt]: Epoch 5 / 100: avg data time: 3.97e-03, avg batch time: 0.6347, average train loss: 1.2947
[10/29 00:59:11 visual_prompt]: 	Test 100/123. loss: 0.865, 0.2631 s / batch. (data: 3.81e-05)max mem: 15.94594 GB 
[10/29 00:59:21 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2334, average loss: 0.9268
[10/29 00:59:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.04	
[10/29 00:59:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/29 01:00:27 visual_prompt]: 	Training 100/1106. train loss: 0.7770,	0.6178 s / batch. (data: 3.30e-04). ETA=18:00:54, max mem: 15.9 GB 
[10/29 01:01:30 visual_prompt]: 	Training 200/1106. train loss: 0.1199,	0.6196 s / batch. (data: 3.05e-04). ETA=18:02:55, max mem: 15.9 GB 
[10/29 01:02:33 visual_prompt]: 	Training 300/1106. train loss: 0.1208,	0.6544 s / batch. (data: 7.94e-04). ETA=19:02:39, max mem: 15.9 GB 
[10/29 01:03:37 visual_prompt]: 	Training 400/1106. train loss: 0.1185,	0.6426 s / batch. (data: 1.46e-02). ETA=18:41:00, max mem: 15.9 GB 
[10/29 01:04:40 visual_prompt]: 	Training 500/1106. train loss: 0.8990,	0.6247 s / batch. (data: 7.98e-04). ETA=18:08:47, max mem: 15.9 GB 
[10/29 01:05:43 visual_prompt]: 	Training 600/1106. train loss: 0.2580,	0.6356 s / batch. (data: 8.60e-04). ETA=18:26:36, max mem: 15.9 GB 
[10/29 01:06:47 visual_prompt]: 	Training 700/1106. train loss: 1.4278,	0.6503 s / batch. (data: 8.08e-04). ETA=18:51:09, max mem: 15.9 GB 
[10/29 01:07:50 visual_prompt]: 	Training 800/1106. train loss: 1.9803,	0.6349 s / batch. (data: 8.38e-04). ETA=18:23:25, max mem: 15.9 GB 
[10/29 01:08:54 visual_prompt]: 	Training 900/1106. train loss: 1.6568,	0.6286 s / batch. (data: 3.05e-04). ETA=18:11:25, max mem: 15.9 GB 
[10/29 01:09:57 visual_prompt]: 	Training 1000/1106. train loss: 0.3358,	0.6205 s / batch. (data: 2.77e-04). ETA=17:56:20, max mem: 15.9 GB 
[10/29 01:11:00 visual_prompt]: 	Training 1100/1106. train loss: 0.0759,	0.6187 s / batch. (data: 1.52e-04). ETA=17:52:07, max mem: 15.9 GB 
[10/29 01:11:04 visual_prompt]: Epoch 6 / 100: avg data time: 4.34e-03, avg batch time: 0.6353, average train loss: 1.1405
[10/29 01:11:54 visual_prompt]: 	Test 100/123. loss: 1.131, 0.2346 s / batch. (data: 4.01e-05)max mem: 15.94594 GB 
[10/29 01:12:04 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2318, average loss: 1.2749
[10/29 01:12:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.47	
[10/29 01:12:04 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/29 01:13:09 visual_prompt]: 	Training 100/1106. train loss: 1.4475,	0.6453 s / batch. (data: 8.15e-04). ETA=18:37:04, max mem: 15.9 GB 
[10/29 01:14:13 visual_prompt]: 	Training 200/1106. train loss: 1.9655,	0.6388 s / batch. (data: 7.69e-04). ETA=18:24:47, max mem: 15.9 GB 
[10/29 01:15:16 visual_prompt]: 	Training 300/1106. train loss: 0.0216,	0.6444 s / batch. (data: 2.60e-02). ETA=18:33:20, max mem: 15.9 GB 
[10/29 01:16:19 visual_prompt]: 	Training 400/1106. train loss: 1.0864,	0.6502 s / batch. (data: 1.02e-03). ETA=18:42:15, max mem: 15.9 GB 
[10/29 01:17:22 visual_prompt]: 	Training 500/1106. train loss: 2.5464,	0.6209 s / batch. (data: 2.90e-04). ETA=17:50:39, max mem: 15.9 GB 
[10/29 01:18:26 visual_prompt]: 	Training 600/1106. train loss: 2.1243,	0.6336 s / batch. (data: 3.27e-04). ETA=18:11:27, max mem: 15.9 GB 
[10/29 01:19:29 visual_prompt]: 	Training 700/1106. train loss: 2.7812,	0.6266 s / batch. (data: 4.41e-04). ETA=17:58:23, max mem: 15.9 GB 
[10/29 01:20:33 visual_prompt]: 	Training 800/1106. train loss: 0.3597,	0.6329 s / batch. (data: 7.88e-04). ETA=18:08:15, max mem: 15.9 GB 
[10/29 01:21:36 visual_prompt]: 	Training 900/1106. train loss: 0.9108,	0.6184 s / batch. (data: 3.33e-04). ETA=17:42:18, max mem: 15.9 GB 
[10/29 01:22:39 visual_prompt]: 	Training 1000/1106. train loss: 0.8176,	0.6315 s / batch. (data: 8.20e-04). ETA=18:03:46, max mem: 15.9 GB 
[10/29 01:23:42 visual_prompt]: 	Training 1100/1106. train loss: 0.8247,	0.6182 s / batch. (data: 1.52e-04). ETA=17:39:51, max mem: 15.9 GB 
[10/29 01:23:46 visual_prompt]: Epoch 7 / 100: avg data time: 4.17e-03, avg batch time: 0.6344, average train loss: 1.1484
[10/29 01:24:36 visual_prompt]: 	Test 100/123. loss: 1.268, 0.2398 s / batch. (data: 2.43e-05)max mem: 15.94594 GB 
[10/29 01:24:47 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2314, average loss: 1.1449
[10/29 01:24:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.11	
[10/29 01:24:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/29 01:25:52 visual_prompt]: 	Training 100/1106. train loss: 0.1063,	0.6405 s / batch. (data: 1.24e-02). ETA=18:16:51, max mem: 15.9 GB 
[10/29 01:26:55 visual_prompt]: 	Training 200/1106. train loss: 0.6998,	0.6312 s / batch. (data: 7.51e-04). ETA=17:59:55, max mem: 15.9 GB 
[10/29 01:27:58 visual_prompt]: 	Training 300/1106. train loss: 0.0088,	0.6240 s / batch. (data: 5.42e-03). ETA=17:46:31, max mem: 15.9 GB 
[10/29 01:29:02 visual_prompt]: 	Training 400/1106. train loss: 0.1597,	0.6192 s / batch. (data: 2.62e-04). ETA=17:37:26, max mem: 15.9 GB 
[10/29 01:30:05 visual_prompt]: 	Training 500/1106. train loss: 0.0422,	0.6304 s / batch. (data: 3.22e-04). ETA=17:55:24, max mem: 15.9 GB 
[10/29 01:31:09 visual_prompt]: 	Training 600/1106. train loss: 0.0601,	0.6301 s / batch. (data: 3.20e-04). ETA=17:53:57, max mem: 15.9 GB 
[10/29 01:32:12 visual_prompt]: 	Training 700/1106. train loss: 1.3530,	0.6299 s / batch. (data: 1.20e-02). ETA=17:52:27, max mem: 15.9 GB 
[10/29 01:33:15 visual_prompt]: 	Training 800/1106. train loss: 1.6917,	0.6475 s / batch. (data: 4.26e-04). ETA=18:21:21, max mem: 15.9 GB 
[10/29 01:34:18 visual_prompt]: 	Training 900/1106. train loss: 0.0609,	0.6194 s / batch. (data: 3.28e-04). ETA=17:32:28, max mem: 15.9 GB 
[10/29 01:35:22 visual_prompt]: 	Training 1000/1106. train loss: 1.1412,	0.6319 s / batch. (data: 3.17e-04). ETA=17:52:43, max mem: 15.9 GB 
[10/29 01:36:25 visual_prompt]: 	Training 1100/1106. train loss: 0.2105,	0.6188 s / batch. (data: 1.20e-04). ETA=17:29:22, max mem: 15.9 GB 
[10/29 01:36:29 visual_prompt]: Epoch 8 / 100: avg data time: 4.13e-03, avg batch time: 0.6345, average train loss: 1.3367
[10/29 01:37:19 visual_prompt]: 	Test 100/123. loss: 0.688, 0.2249 s / batch. (data: 4.63e-05)max mem: 15.94594 GB 
[10/29 01:37:29 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2329, average loss: 0.6972
[10/29 01:37:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 54.13	
[10/29 01:37:29 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/29 01:38:35 visual_prompt]: 	Training 100/1106. train loss: 5.7525,	0.6213 s / batch. (data: 3.11e-04). ETA=17:32:37, max mem: 15.9 GB 
[10/29 01:39:38 visual_prompt]: 	Training 200/1106. train loss: 0.3378,	0.6404 s / batch. (data: 8.84e-04). ETA=18:03:54, max mem: 15.9 GB 
[10/29 01:40:42 visual_prompt]: 	Training 300/1106. train loss: 1.2454,	0.6195 s / batch. (data: 3.22e-04). ETA=17:27:31, max mem: 15.9 GB 
[10/29 01:41:45 visual_prompt]: 	Training 400/1106. train loss: 0.3937,	0.6361 s / batch. (data: 8.03e-04). ETA=17:54:28, max mem: 15.9 GB 
[10/29 01:42:48 visual_prompt]: 	Training 500/1106. train loss: 3.9884,	0.6440 s / batch. (data: 3.14e-04). ETA=18:06:48, max mem: 15.9 GB 
[10/29 01:43:52 visual_prompt]: 	Training 600/1106. train loss: 0.7185,	0.6421 s / batch. (data: 7.63e-04). ETA=18:02:26, max mem: 15.9 GB 
[10/29 01:44:55 visual_prompt]: 	Training 700/1106. train loss: 0.2971,	0.6188 s / batch. (data: 4.67e-04). ETA=17:22:08, max mem: 15.9 GB 
[10/29 01:45:58 visual_prompt]: 	Training 800/1106. train loss: 0.9759,	0.6242 s / batch. (data: 3.33e-04). ETA=17:30:10, max mem: 15.9 GB 
[10/29 01:47:02 visual_prompt]: 	Training 900/1106. train loss: 0.3617,	0.6584 s / batch. (data: 4.06e-02). ETA=18:26:38, max mem: 15.9 GB 
[10/29 01:48:06 visual_prompt]: 	Training 1000/1106. train loss: 0.2237,	0.6201 s / batch. (data: 4.74e-04). ETA=17:21:13, max mem: 15.9 GB 
[10/29 01:49:09 visual_prompt]: 	Training 1100/1106. train loss: 1.3549,	0.6185 s / batch. (data: 1.54e-04). ETA=17:17:35, max mem: 15.9 GB 
[10/29 01:49:13 visual_prompt]: Epoch 9 / 100: avg data time: 4.52e-03, avg batch time: 0.6358, average train loss: 1.1409
[10/29 01:50:03 visual_prompt]: 	Test 100/123. loss: 0.961, 0.2405 s / batch. (data: 4.91e-05)max mem: 15.94594 GB 
[10/29 01:50:13 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2325, average loss: 1.0551
[10/29 01:50:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.90	
[10/29 01:50:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/29 01:51:19 visual_prompt]: 	Training 100/1106. train loss: 1.3561,	0.6260 s / batch. (data: 2.67e-04). ETA=17:29:03, max mem: 15.9 GB 
[10/29 01:52:22 visual_prompt]: 	Training 200/1106. train loss: 2.2045,	0.6336 s / batch. (data: 3.19e-04). ETA=17:40:46, max mem: 15.9 GB 
[10/29 01:53:25 visual_prompt]: 	Training 300/1106. train loss: 1.1380,	0.6280 s / batch. (data: 2.90e-04). ETA=17:30:20, max mem: 15.9 GB 
[10/29 01:54:28 visual_prompt]: 	Training 400/1106. train loss: 1.3760,	0.6197 s / batch. (data: 3.33e-04). ETA=17:15:20, max mem: 15.9 GB 
[10/29 01:55:32 visual_prompt]: 	Training 500/1106. train loss: 0.9633,	0.6412 s / batch. (data: 8.08e-04). ETA=17:50:10, max mem: 15.9 GB 
[10/29 01:56:35 visual_prompt]: 	Training 600/1106. train loss: 5.5301,	0.6421 s / batch. (data: 8.55e-04). ETA=17:50:42, max mem: 15.9 GB 
[10/29 01:57:38 visual_prompt]: 	Training 700/1106. train loss: 2.4175,	0.6341 s / batch. (data: 8.15e-04). ETA=17:36:14, max mem: 15.9 GB 
[10/29 01:58:42 visual_prompt]: 	Training 800/1106. train loss: 0.7048,	0.6303 s / batch. (data: 3.24e-04). ETA=17:28:57, max mem: 15.9 GB 
[10/29 01:59:45 visual_prompt]: 	Training 900/1106. train loss: 2.8199,	0.6478 s / batch. (data: 1.58e-02). ETA=17:56:58, max mem: 15.9 GB 
[10/29 02:00:48 visual_prompt]: 	Training 1000/1106. train loss: 0.0057,	0.6547 s / batch. (data: 7.34e-04). ETA=18:07:15, max mem: 15.9 GB 
[10/29 02:01:52 visual_prompt]: 	Training 1100/1106. train loss: 0.4975,	0.6183 s / batch. (data: 1.57e-04). ETA=17:05:49, max mem: 15.9 GB 
[10/29 02:01:55 visual_prompt]: Epoch 10 / 100: avg data time: 4.21e-03, avg batch time: 0.6345, average train loss: 1.8223
[10/29 02:02:45 visual_prompt]: 	Test 100/123. loss: 1.181, 0.2357 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/29 02:02:56 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2341, average loss: 1.0731
[10/29 02:02:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.62	
[10/29 02:02:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/29 02:04:02 visual_prompt]: 	Training 100/1106. train loss: 4.7244,	0.6541 s / batch. (data: 7.76e-04). ETA=18:03:58, max mem: 15.9 GB 
[10/29 02:05:05 visual_prompt]: 	Training 200/1106. train loss: 0.9055,	0.6248 s / batch. (data: 3.19e-04). ETA=17:14:29, max mem: 15.9 GB 
[10/29 02:06:08 visual_prompt]: 	Training 300/1106. train loss: 1.8809,	0.6249 s / batch. (data: 4.05e-04). ETA=17:13:35, max mem: 15.9 GB 
[10/29 02:07:11 visual_prompt]: 	Training 400/1106. train loss: 0.4340,	0.6337 s / batch. (data: 8.78e-04). ETA=17:27:05, max mem: 15.9 GB 
[10/29 02:08:15 visual_prompt]: 	Training 500/1106. train loss: 0.6399,	0.6283 s / batch. (data: 7.83e-04). ETA=17:17:04, max mem: 15.9 GB 
[10/29 02:09:18 visual_prompt]: 	Training 600/1106. train loss: 0.0025,	0.6411 s / batch. (data: 8.58e-04). ETA=17:37:13, max mem: 15.9 GB 
[10/29 02:10:21 visual_prompt]: 	Training 700/1106. train loss: 2.2267,	0.6194 s / batch. (data: 3.29e-04). ETA=17:00:17, max mem: 15.9 GB 
[10/29 02:11:24 visual_prompt]: 	Training 800/1106. train loss: 2.4365,	0.6453 s / batch. (data: 7.34e-04). ETA=17:41:54, max mem: 15.9 GB 
[10/29 02:12:27 visual_prompt]: 	Training 900/1106. train loss: 0.2786,	0.6179 s / batch. (data: 3.01e-04). ETA=16:55:45, max mem: 15.9 GB 
[10/29 02:13:31 visual_prompt]: 	Training 1000/1106. train loss: 0.0148,	0.6446 s / batch. (data: 5.93e-03). ETA=17:38:37, max mem: 15.9 GB 
[10/29 02:14:34 visual_prompt]: 	Training 1100/1106. train loss: 1.3488,	0.6183 s / batch. (data: 1.53e-04). ETA=16:54:24, max mem: 15.9 GB 
[10/29 02:14:38 visual_prompt]: Epoch 11 / 100: avg data time: 4.54e-03, avg batch time: 0.6342, average train loss: 1.3232
[10/29 02:15:28 visual_prompt]: 	Test 100/123. loss: 0.771, 0.2446 s / batch. (data: 3.34e-05)max mem: 15.94594 GB 
[10/29 02:15:38 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.2330, average loss: 0.7310
[10/29 02:15:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.32	
[10/29 02:15:38 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/29 02:16:44 visual_prompt]: 	Training 100/1106. train loss: 0.7790,	0.6436 s / batch. (data: 8.21e-04). ETA=17:34:47, max mem: 15.9 GB 
[10/29 02:17:48 visual_prompt]: 	Training 200/1106. train loss: 0.9800,	0.6189 s / batch. (data: 3.28e-04). ETA=16:53:13, max mem: 15.9 GB 
[10/29 02:18:51 visual_prompt]: 	Training 300/1106. train loss: 2.5503,	0.6201 s / batch. (data: 3.31e-04). ETA=16:54:10, max mem: 15.9 GB 
[10/29 02:19:54 visual_prompt]: 	Training 400/1106. train loss: 0.7266,	0.6260 s / batch. (data: 3.31e-04). ETA=17:02:47, max mem: 15.9 GB 
[10/29 02:20:58 visual_prompt]: 	Training 500/1106. train loss: 0.4020,	0.6202 s / batch. (data: 3.32e-04). ETA=16:52:17, max mem: 15.9 GB 
[10/29 02:22:01 visual_prompt]: 	Training 600/1106. train loss: 0.3651,	0.6263 s / batch. (data: 3.01e-04). ETA=17:01:16, max mem: 15.9 GB 
[10/29 02:23:04 visual_prompt]: 	Training 700/1106. train loss: 0.9006,	0.6196 s / batch. (data: 3.07e-04). ETA=16:49:15, max mem: 15.9 GB 
[10/29 02:24:08 visual_prompt]: 	Training 800/1106. train loss: 1.2450,	0.6305 s / batch. (data: 7.71e-04). ETA=17:05:55, max mem: 15.9 GB 
[10/29 02:25:11 visual_prompt]: 	Training 900/1106. train loss: 1.6005,	0.6200 s / batch. (data: 4.42e-04). ETA=16:47:49, max mem: 15.9 GB 
[10/29 02:26:14 visual_prompt]: 	Training 1000/1106. train loss: 1.8035,	0.6328 s / batch. (data: 8.17e-04). ETA=17:07:39, max mem: 15.9 GB 
[10/29 02:27:17 visual_prompt]: 	Training 1100/1106. train loss: 2.0339,	0.6186 s / batch. (data: 1.71e-04). ETA=16:43:31, max mem: 15.9 GB 
[10/29 02:27:21 visual_prompt]: Epoch 12 / 100: avg data time: 4.87e-03, avg batch time: 0.6355, average train loss: 1.4380
[10/29 02:28:11 visual_prompt]: 	Test 100/123. loss: 0.756, 0.2277 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[10/29 02:28:22 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2317, average loss: 0.7163
[10/29 02:28:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.66	
[10/29 02:28:22 visual_prompt]: Best epoch 12: best metric: -0.716
[10/29 02:28:22 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/29 02:29:27 visual_prompt]: 	Training 100/1106. train loss: 0.9443,	0.6314 s / batch. (data: 8.90e-04). ETA=17:03:05, max mem: 15.9 GB 
[10/29 02:30:31 visual_prompt]: 	Training 200/1106. train loss: 1.9576,	0.6312 s / batch. (data: 8.13e-04). ETA=17:01:44, max mem: 15.9 GB 
[10/29 02:31:34 visual_prompt]: 	Training 300/1106. train loss: 0.0109,	0.6183 s / batch. (data: 3.14e-04). ETA=16:39:51, max mem: 15.9 GB 
[10/29 02:32:37 visual_prompt]: 	Training 400/1106. train loss: 2.0910,	0.6329 s / batch. (data: 8.03e-04). ETA=17:02:25, max mem: 15.9 GB 
[10/29 02:33:40 visual_prompt]: 	Training 500/1106. train loss: 0.0436,	0.6194 s / batch. (data: 3.14e-04). ETA=16:39:31, max mem: 15.9 GB 
[10/29 02:34:44 visual_prompt]: 	Training 600/1106. train loss: 0.8006,	0.6305 s / batch. (data: 3.42e-04). ETA=16:56:29, max mem: 15.9 GB 
[10/29 02:35:47 visual_prompt]: 	Training 700/1106. train loss: 1.2093,	0.6426 s / batch. (data: 1.05e-02). ETA=17:14:51, max mem: 15.9 GB 
[10/29 02:36:50 visual_prompt]: 	Training 800/1106. train loss: 1.0339,	0.6305 s / batch. (data: 7.97e-04). ETA=16:54:19, max mem: 15.9 GB 
[10/29 02:37:53 visual_prompt]: 	Training 900/1106. train loss: 1.5809,	0.6657 s / batch. (data: 3.45e-02). ETA=17:49:49, max mem: 15.9 GB 
[10/29 02:38:57 visual_prompt]: 	Training 1000/1106. train loss: 0.0043,	0.6443 s / batch. (data: 7.92e-04). ETA=17:14:26, max mem: 15.9 GB 
[10/29 02:40:00 visual_prompt]: 	Training 1100/1106. train loss: 1.8312,	0.6181 s / batch. (data: 1.66e-04). ETA=16:31:14, max mem: 15.9 GB 
[10/29 02:40:04 visual_prompt]: Epoch 13 / 100: avg data time: 4.82e-03, avg batch time: 0.6348, average train loss: 1.5240
[10/29 02:40:54 visual_prompt]: 	Test 100/123. loss: 0.678, 0.2366 s / batch. (data: 4.29e-05)max mem: 15.94594 GB 
[10/29 02:41:05 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2316, average loss: 0.6950
[10/29 02:41:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 54.60	
[10/29 02:41:05 visual_prompt]: Best epoch 13: best metric: -0.695
[10/29 02:41:05 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/29 02:42:10 visual_prompt]: 	Training 100/1106. train loss: 0.0128,	0.6315 s / batch. (data: 9.17e-04). ETA=16:51:39, max mem: 15.9 GB 
[10/29 02:43:14 visual_prompt]: 	Training 200/1106. train loss: 1.8881,	0.6323 s / batch. (data: 8.69e-04). ETA=16:51:56, max mem: 15.9 GB 
[10/29 02:44:17 visual_prompt]: 	Training 300/1106. train loss: 2.7205,	0.6367 s / batch. (data: 7.28e-04). ETA=16:57:50, max mem: 15.9 GB 
[10/29 02:45:20 visual_prompt]: 	Training 400/1106. train loss: 0.0000,	0.6198 s / batch. (data: 3.33e-04). ETA=16:29:48, max mem: 15.9 GB 
[10/29 02:46:24 visual_prompt]: 	Training 500/1106. train loss: 1.7615,	0.6344 s / batch. (data: 8.40e-04). ETA=16:52:07, max mem: 15.9 GB 
[10/29 02:47:27 visual_prompt]: 	Training 600/1106. train loss: 1.4117,	0.6194 s / batch. (data: 3.39e-04). ETA=16:27:06, max mem: 15.9 GB 
[10/29 02:48:30 visual_prompt]: 	Training 700/1106. train loss: 0.0866,	0.6263 s / batch. (data: 3.22e-04). ETA=16:37:08, max mem: 15.9 GB 
[10/29 02:49:34 visual_prompt]: 	Training 800/1106. train loss: 3.3765,	0.6362 s / batch. (data: 5.46e-03). ETA=16:51:43, max mem: 15.9 GB 
[10/29 02:50:37 visual_prompt]: 	Training 900/1106. train loss: 0.7937,	0.6190 s / batch. (data: 3.18e-04). ETA=16:23:21, max mem: 15.9 GB 
[10/29 02:51:40 visual_prompt]: 	Training 1000/1106. train loss: 1.9919,	0.6292 s / batch. (data: 2.83e-04). ETA=16:38:33, max mem: 15.9 GB 
[10/29 02:52:43 visual_prompt]: 	Training 1100/1106. train loss: 0.8954,	0.6190 s / batch. (data: 1.16e-04). ETA=16:21:21, max mem: 15.9 GB 
[10/29 02:52:47 visual_prompt]: Epoch 14 / 100: avg data time: 4.45e-03, avg batch time: 0.6350, average train loss: 2.1238
[10/29 02:53:37 visual_prompt]: 	Test 100/123. loss: 1.862, 0.2395 s / batch. (data: 4.39e-05)max mem: 15.94594 GB 
[10/29 02:53:48 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2319, average loss: 1.6681
[10/29 02:53:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.34	
[10/29 02:53:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/29 02:54:52 visual_prompt]: 	Training 100/1106. train loss: 5.8302,	0.6362 s / batch. (data: 5.49e-03). ETA=16:47:25, max mem: 15.9 GB 
[10/29 02:55:56 visual_prompt]: 	Training 200/1106. train loss: 1.1252,	0.6336 s / batch. (data: 7.28e-04). ETA=16:42:19, max mem: 15.9 GB 
[10/29 02:56:59 visual_prompt]: 	Training 300/1106. train loss: 4.6115,	0.6546 s / batch. (data: 7.92e-04). ETA=17:14:26, max mem: 15.9 GB 
[10/29 02:58:02 visual_prompt]: 	Training 400/1106. train loss: 12.5293,	0.6252 s / batch. (data: 3.17e-04). ETA=16:26:56, max mem: 15.9 GB 
[10/29 02:59:06 visual_prompt]: 	Training 500/1106. train loss: 0.8904,	0.6414 s / batch. (data: 1.20e-02). ETA=16:51:24, max mem: 15.9 GB 
[10/29 02:00:09 visual_prompt]: 	Training 600/1106. train loss: 0.6718,	0.6240 s / batch. (data: 5.61e-03). ETA=16:22:57, max mem: 15.9 GB 
[10/29 02:01:12 visual_prompt]: 	Training 700/1106. train loss: 2.1243,	0.6314 s / batch. (data: 7.36e-04). ETA=16:33:33, max mem: 15.9 GB 
[10/29 02:02:15 visual_prompt]: 	Training 800/1106. train loss: 1.0612,	0.6440 s / batch. (data: 3.23e-04). ETA=16:52:21, max mem: 15.9 GB 
[10/29 02:03:19 visual_prompt]: 	Training 900/1106. train loss: 2.6549,	0.6444 s / batch. (data: 8.07e-04). ETA=16:51:54, max mem: 15.9 GB 
[10/29 02:04:22 visual_prompt]: 	Training 1000/1106. train loss: 0.9884,	0.6315 s / batch. (data: 2.49e-04). ETA=16:30:34, max mem: 15.9 GB 
[10/29 02:05:25 visual_prompt]: 	Training 1100/1106. train loss: 1.4514,	0.6189 s / batch. (data: 1.61e-04). ETA=16:09:41, max mem: 15.9 GB 
[10/29 02:05:29 visual_prompt]: Epoch 15 / 100: avg data time: 3.83e-03, avg batch time: 0.6339, average train loss: 1.6567
[10/29 02:06:19 visual_prompt]: 	Test 100/123. loss: 0.922, 0.2362 s / batch. (data: 4.10e-05)max mem: 15.94594 GB 
[10/29 02:06:29 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.2323, average loss: 0.9920
[10/29 02:06:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.20	
[10/29 02:06:29 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/29 02:07:34 visual_prompt]: 	Training 100/1106. train loss: 0.4182,	0.6278 s / batch. (data: 8.25e-04). ETA=16:22:41, max mem: 15.9 GB 
[10/29 02:08:37 visual_prompt]: 	Training 200/1106. train loss: 1.0565,	0.6239 s / batch. (data: 3.17e-04). ETA=16:15:28, max mem: 15.9 GB 
[10/29 02:09:40 visual_prompt]: 	Training 300/1106. train loss: 0.0353,	0.6190 s / batch. (data: 3.32e-04). ETA=16:06:42, max mem: 15.9 GB 
[10/29 02:10:44 visual_prompt]: 	Training 400/1106. train loss: 1.1152,	0.6567 s / batch. (data: 8.16e-04). ETA=17:04:30, max mem: 15.9 GB 
[10/29 02:11:47 visual_prompt]: 	Training 500/1106. train loss: 1.7193,	0.6405 s / batch. (data: 8.02e-04). ETA=16:38:13, max mem: 15.9 GB 
[10/29 02:12:50 visual_prompt]: 	Training 600/1106. train loss: 0.4925,	0.6306 s / batch. (data: 8.02e-04). ETA=16:21:40, max mem: 15.9 GB 
[10/29 02:13:53 visual_prompt]: 	Training 700/1106. train loss: 2.1045,	0.6387 s / batch. (data: 8.13e-04). ETA=16:33:17, max mem: 15.9 GB 
[10/29 02:14:57 visual_prompt]: 	Training 800/1106. train loss: 2.7292,	0.6388 s / batch. (data: 1.05e-02). ETA=16:32:19, max mem: 15.9 GB 
[10/29 02:16:00 visual_prompt]: 	Training 900/1106. train loss: 0.8380,	0.6338 s / batch. (data: 8.16e-04). ETA=16:23:37, max mem: 15.9 GB 
[10/29 02:17:03 visual_prompt]: 	Training 1000/1106. train loss: 1.5969,	0.6188 s / batch. (data: 3.37e-04). ETA=15:59:18, max mem: 15.9 GB 
[10/29 02:18:06 visual_prompt]: 	Training 1100/1106. train loss: 0.0465,	0.6181 s / batch. (data: 1.48e-04). ETA=15:57:05, max mem: 15.9 GB 
[10/29 02:18:10 visual_prompt]: Epoch 16 / 100: avg data time: 3.88e-03, avg batch time: 0.6341, average train loss: 1.2785
[10/29 02:19:00 visual_prompt]: 	Test 100/123. loss: 0.733, 0.2257 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/29 02:19:11 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.2325, average loss: 0.7043
[10/29 02:19:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.59	
[10/29 02:19:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/29 02:20:16 visual_prompt]: 	Training 100/1106. train loss: 0.4418,	0.6411 s / batch. (data: 3.07e-04). ETA=16:31:36, max mem: 15.9 GB 
[10/29 02:21:20 visual_prompt]: 	Training 200/1106. train loss: 1.6289,	0.6216 s / batch. (data: 2.71e-04). ETA=16:00:26, max mem: 15.9 GB 
[10/29 02:22:23 visual_prompt]: 	Training 300/1106. train loss: 0.5580,	0.6234 s / batch. (data: 5.44e-03). ETA=16:02:13, max mem: 15.9 GB 
[10/29 02:23:27 visual_prompt]: 	Training 400/1106. train loss: 0.3098,	0.6322 s / batch. (data: 8.31e-04). ETA=16:14:40, max mem: 15.9 GB 
[10/29 02:24:30 visual_prompt]: 	Training 500/1106. train loss: 1.6279,	0.6403 s / batch. (data: 7.76e-04). ETA=16:26:10, max mem: 15.9 GB 
[10/29 02:25:33 visual_prompt]: 	Training 600/1106. train loss: 2.6212,	0.6623 s / batch. (data: 5.45e-03). ETA=16:58:49, max mem: 15.9 GB 
[10/29 02:26:36 visual_prompt]: 	Training 700/1106. train loss: 0.9929,	0.6269 s / batch. (data: 2.82e-04). ETA=16:03:24, max mem: 15.9 GB 
[10/29 02:27:40 visual_prompt]: 	Training 800/1106. train loss: 1.0973,	0.6299 s / batch. (data: 1.24e-02). ETA=16:06:54, max mem: 15.9 GB 
[10/29 02:28:43 visual_prompt]: 	Training 900/1106. train loss: 1.3305,	0.6345 s / batch. (data: 7.68e-04). ETA=16:12:56, max mem: 15.9 GB 
[10/29 02:29:46 visual_prompt]: 	Training 1000/1106. train loss: 0.6379,	0.6313 s / batch. (data: 8.33e-04). ETA=16:07:00, max mem: 15.9 GB 
[10/29 02:30:49 visual_prompt]: 	Training 1100/1106. train loss: 0.2087,	0.6177 s / batch. (data: 1.43e-04). ETA=15:45:08, max mem: 15.9 GB 
[10/29 02:30:53 visual_prompt]: Epoch 17 / 100: avg data time: 4.61e-03, avg batch time: 0.6351, average train loss: 1.6601
[10/29 02:31:43 visual_prompt]: 	Test 100/123. loss: 1.072, 0.2254 s / batch. (data: 4.36e-05)max mem: 15.94594 GB 
[10/29 02:31:54 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.2332, average loss: 1.1752
[10/29 02:31:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.52	
[10/29 02:31:54 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/29 02:32:59 visual_prompt]: 	Training 100/1106. train loss: 0.0284,	0.6178 s / batch. (data: 2.88e-04). ETA=15:44:10, max mem: 15.9 GB 
[10/29 02:34:02 visual_prompt]: 	Training 200/1106. train loss: 3.6266,	0.6417 s / batch. (data: 8.32e-04). ETA=16:19:40, max mem: 15.9 GB 
[10/29 02:35:06 visual_prompt]: 	Training 300/1106. train loss: 5.9419,	0.6427 s / batch. (data: 8.03e-04). ETA=16:20:02, max mem: 15.9 GB 
[10/29 02:36:09 visual_prompt]: 	Training 400/1106. train loss: 1.8878,	0.6680 s / batch. (data: 3.15e-04). ETA=16:57:32, max mem: 15.9 GB 
[10/29 02:37:12 visual_prompt]: 	Training 500/1106. train loss: 0.0086,	0.6358 s / batch. (data: 8.82e-04). ETA=16:07:26, max mem: 15.9 GB 
[10/29 02:38:15 visual_prompt]: 	Training 600/1106. train loss: 0.2824,	0.6184 s / batch. (data: 4.85e-04). ETA=15:40:00, max mem: 15.9 GB 
[10/29 02:39:18 visual_prompt]: 	Training 700/1106. train loss: 0.6607,	0.6331 s / batch. (data: 2.96e-04). ETA=16:01:12, max mem: 15.9 GB 
[10/29 02:40:22 visual_prompt]: 	Training 800/1106. train loss: 0.7203,	0.6438 s / batch. (data: 7.92e-04). ETA=16:16:28, max mem: 15.9 GB 
[10/29 02:41:25 visual_prompt]: 	Training 900/1106. train loss: 3.8344,	0.6320 s / batch. (data: 8.12e-04). ETA=15:57:30, max mem: 15.9 GB 
[10/29 02:42:28 visual_prompt]: 	Training 1000/1106. train loss: 1.1901,	0.6201 s / batch. (data: 3.23e-04). ETA=15:38:19, max mem: 15.9 GB 
[10/29 02:43:32 visual_prompt]: 	Training 1100/1106. train loss: 0.0104,	0.6175 s / batch. (data: 1.41e-04). ETA=15:33:27, max mem: 15.9 GB 
[10/29 02:43:35 visual_prompt]: Epoch 18 / 100: avg data time: 4.50e-03, avg batch time: 0.6345, average train loss: 1.4222
[10/29 02:44:25 visual_prompt]: 	Test 100/123. loss: 0.962, 0.2259 s / batch. (data: 2.67e-05)max mem: 15.94594 GB 
[10/29 02:44:36 visual_prompt]: Inference (val):avg data time: 9.65e-05, avg batch time: 0.2320, average loss: 1.0549
[10/29 02:44:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.19	
[10/29 02:44:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/29 02:45:41 visual_prompt]: 	Training 100/1106. train loss: 0.2740,	0.6425 s / batch. (data: 2.55e-02). ETA=16:10:09, max mem: 15.9 GB 
[10/29 02:46:45 visual_prompt]: 	Training 200/1106. train loss: 1.8059,	0.6318 s / batch. (data: 7.55e-04). ETA=15:52:49, max mem: 15.9 GB 
[10/29 02:47:48 visual_prompt]: 	Training 300/1106. train loss: 0.4184,	0.6442 s / batch. (data: 9.17e-04). ETA=16:10:31, max mem: 15.9 GB 
[10/29 02:48:51 visual_prompt]: 	Training 400/1106. train loss: 1.2790,	0.6188 s / batch. (data: 2.72e-04). ETA=15:31:14, max mem: 15.9 GB 
[10/29 02:49:54 visual_prompt]: 	Training 500/1106. train loss: 0.3693,	0.6193 s / batch. (data: 7.92e-04). ETA=15:30:54, max mem: 15.9 GB 
[10/29 02:50:58 visual_prompt]: 	Training 600/1106. train loss: 0.1161,	0.6308 s / batch. (data: 8.02e-04). ETA=15:47:10, max mem: 15.9 GB 
[10/29 02:52:01 visual_prompt]: 	Training 700/1106. train loss: 0.8834,	0.6275 s / batch. (data: 2.98e-04). ETA=15:41:11, max mem: 15.9 GB 
[10/29 02:53:04 visual_prompt]: 	Training 800/1106. train loss: 0.0060,	0.6341 s / batch. (data: 2.86e-04). ETA=15:49:59, max mem: 15.9 GB 
[10/29 02:54:07 visual_prompt]: 	Training 900/1106. train loss: 1.1452,	0.6365 s / batch. (data: 7.75e-04). ETA=15:52:28, max mem: 15.9 GB 
[10/29 02:55:10 visual_prompt]: 	Training 1000/1106. train loss: 0.0602,	0.6580 s / batch. (data: 8.13e-04). ETA=16:23:39, max mem: 15.9 GB 
[10/29 02:56:14 visual_prompt]: 	Training 1100/1106. train loss: 3.3521,	0.6174 s / batch. (data: 1.61e-04). ETA=15:21:57, max mem: 15.9 GB 
[10/29 02:56:18 visual_prompt]: Epoch 19 / 100: avg data time: 4.03e-03, avg batch time: 0.6343, average train loss: 1.3812
[10/29 02:57:07 visual_prompt]: 	Test 100/123. loss: 8.771, 0.2258 s / batch. (data: 4.43e-05)max mem: 15.94594 GB 
[10/29 02:57:18 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.2332, average loss: 7.9422
[10/29 02:57:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.19	
[10/29 02:57:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[10/29 02:58:24 visual_prompt]: 	Training 100/1106. train loss: 2.4794,	0.6169 s / batch. (data: 3.19e-04). ETA=15:20:03, max mem: 15.9 GB 
[10/29 02:59:27 visual_prompt]: 	Training 200/1106. train loss: 1.3811,	0.6179 s / batch. (data: 3.30e-04). ETA=15:20:33, max mem: 15.9 GB 
[10/29 03:00:30 visual_prompt]: 	Training 300/1106. train loss: 2.1391,	0.6509 s / batch. (data: 8.09e-04). ETA=16:08:33, max mem: 15.9 GB 
[10/29 03:01:33 visual_prompt]: 	Training 400/1106. train loss: 0.9276,	0.6192 s / batch. (data: 2.97e-04). ETA=15:20:22, max mem: 15.9 GB 
[10/29 03:02:37 visual_prompt]: 	Training 500/1106. train loss: 1.5819,	0.6329 s / batch. (data: 8.06e-04). ETA=15:39:43, max mem: 15.9 GB 
[10/29 03:03:40 visual_prompt]: 	Training 600/1106. train loss: 1.6150,	0.6323 s / batch. (data: 7.57e-04). ETA=15:37:49, max mem: 15.9 GB 
[10/29 03:04:43 visual_prompt]: 	Training 700/1106. train loss: 0.7601,	0.6573 s / batch. (data: 7.74e-04). ETA=16:13:48, max mem: 15.9 GB 
[10/29 03:05:47 visual_prompt]: 	Training 800/1106. train loss: 0.0639,	0.6186 s / batch. (data: 2.81e-04). ETA=15:15:20, max mem: 15.9 GB 
[10/29 03:06:50 visual_prompt]: 	Training 900/1106. train loss: 3.5583,	0.6301 s / batch. (data: 8.18e-04). ETA=15:31:17, max mem: 15.9 GB 
[10/29 03:07:53 visual_prompt]: 	Training 1000/1106. train loss: 0.0282,	0.6443 s / batch. (data: 1.30e-02). ETA=15:51:15, max mem: 15.9 GB 
[10/29 03:08:56 visual_prompt]: 	Training 1100/1106. train loss: 0.4670,	0.6187 s / batch. (data: 1.44e-04). ETA=15:12:28, max mem: 15.9 GB 
[10/29 03:09:00 visual_prompt]: Epoch 20 / 100: avg data time: 4.79e-03, avg batch time: 0.6349, average train loss: 1.4426
[10/29 03:09:50 visual_prompt]: 	Test 100/123. loss: 0.694, 0.2523 s / batch. (data: 4.15e-05)max mem: 15.94594 GB 
[10/29 03:10:01 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2321, average loss: 0.7201
[10/29 03:10:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.63	
[10/29 03:10:01 visual_prompt]: Stopping early.
[10/29 03:10:01 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 03:10:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 03:10:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 03:10:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 03:10:01 visual_prompt]: Training with config:
[10/29 03:10:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 03:10:01 visual_prompt]: Loading training data...
[10/29 03:10:01 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 03:10:01 visual_prompt]: Loading validation data...
[10/29 03:10:01 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 03:10:01 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/29 03:10:03 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/29 03:10:03 visual_prompt]: tuned percent:0.522
[10/29 03:10:03 visual_prompt]: Device used for model: 0
[10/29 03:10:03 visual_prompt]: Setting up Evaluator...
[10/29 03:10:03 visual_prompt]: Setting up Trainer...
[10/29 03:10:03 visual_prompt]: 	Setting up the optimizer...
[10/29 03:10:03 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 03:11:09 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6183 s / batch. (data: 3.39e-04). ETA=18:58:37, max mem: 15.9 GB 
[10/29 03:12:12 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6493 s / batch. (data: 8.23e-04). ETA=19:54:39, max mem: 15.9 GB 
[10/29 03:13:16 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6314 s / batch. (data: 8.22e-04). ETA=19:20:44, max mem: 15.9 GB 
[10/29 03:14:19 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6317 s / batch. (data: 8.24e-04). ETA=19:20:14, max mem: 15.9 GB 
[10/29 03:15:22 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6520 s / batch. (data: 1.20e-02). ETA=19:56:23, max mem: 15.9 GB 
[10/29 03:16:26 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6486 s / batch. (data: 7.46e-04). ETA=19:49:01, max mem: 15.9 GB 
[10/29 03:17:29 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6278 s / batch. (data: 8.63e-03). ETA=19:09:57, max mem: 15.9 GB 
[10/29 03:18:32 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6331 s / batch. (data: 5.44e-03). ETA=19:18:36, max mem: 15.9 GB 
[10/29 03:19:36 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6430 s / batch. (data: 1.05e-02). ETA=19:35:42, max mem: 15.9 GB 
[10/29 03:20:39 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6189 s / batch. (data: 3.20e-04). ETA=18:50:36, max mem: 15.9 GB 
[10/29 03:21:42 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6188 s / batch. (data: 1.47e-04). ETA=18:49:15, max mem: 15.9 GB 
[10/29 03:21:46 visual_prompt]: Epoch 1 / 100: avg data time: 4.93e-03, avg batch time: 0.6352, average train loss: 1.4028
[10/29 03:22:36 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2366 s / batch. (data: 4.29e-05)max mem: 15.94594 GB 
[10/29 03:22:46 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2330, average loss: 1.3505
[10/29 03:22:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/29 03:22:46 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/29 03:23:51 visual_prompt]: 	Training 100/1106. train loss: 0.9805,	0.6259 s / batch. (data: 3.04e-04). ETA=19:01:12, max mem: 15.9 GB 
[10/29 03:24:54 visual_prompt]: 	Training 200/1106. train loss: 0.7990,	0.6186 s / batch. (data: 3.22e-04). ETA=18:46:49, max mem: 15.9 GB 
[10/29 03:25:58 visual_prompt]: 	Training 300/1106. train loss: 0.4311,	0.6551 s / batch. (data: 7.81e-04). ETA=19:52:09, max mem: 15.9 GB 
[10/29 03:27:01 visual_prompt]: 	Training 400/1106. train loss: 0.0793,	0.6346 s / batch. (data: 8.07e-04). ETA=19:13:55, max mem: 15.9 GB 
[10/29 03:28:04 visual_prompt]: 	Training 500/1106. train loss: 1.0731,	0.6766 s / batch. (data: 6.02e-03). ETA=20:29:04, max mem: 15.9 GB 
[10/29 03:29:08 visual_prompt]: 	Training 600/1106. train loss: 0.5750,	0.6527 s / batch. (data: 2.87e-02). ETA=19:44:31, max mem: 15.9 GB 
[10/29 03:30:11 visual_prompt]: 	Training 700/1106. train loss: 0.9916,	0.6195 s / batch. (data: 3.06e-04). ETA=18:43:14, max mem: 15.9 GB 
[10/29 03:31:14 visual_prompt]: 	Training 800/1106. train loss: 0.7776,	0.6286 s / batch. (data: 3.29e-04). ETA=18:58:46, max mem: 15.9 GB 
[10/29 03:32:17 visual_prompt]: 	Training 900/1106. train loss: 1.1370,	0.6186 s / batch. (data: 3.53e-04). ETA=18:39:36, max mem: 15.9 GB 
[10/29 03:33:21 visual_prompt]: 	Training 1000/1106. train loss: 0.2353,	0.6304 s / batch. (data: 8.09e-04). ETA=18:59:51, max mem: 15.9 GB 
[10/29 03:34:24 visual_prompt]: 	Training 1100/1106. train loss: 0.3503,	0.6186 s / batch. (data: 2.18e-04). ETA=18:37:27, max mem: 15.9 GB 
[10/29 03:34:28 visual_prompt]: Epoch 2 / 100: avg data time: 4.11e-03, avg batch time: 0.6340, average train loss: 0.9141
[10/29 03:35:18 visual_prompt]: 	Test 100/123. loss: 1.443, 0.2249 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/29 03:35:28 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2322, average loss: 1.3364
[10/29 03:35:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.15	
[10/29 03:35:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/29 03:36:34 visual_prompt]: 	Training 100/1106. train loss: 1.1802,	0.6473 s / batch. (data: 8.04e-04). ETA=19:28:15, max mem: 15.9 GB 
[10/29 03:37:38 visual_prompt]: 	Training 200/1106. train loss: 0.2094,	0.6378 s / batch. (data: 8.02e-04). ETA=19:10:07, max mem: 15.9 GB 
[10/29 03:38:41 visual_prompt]: 	Training 300/1106. train loss: 0.3099,	0.6259 s / batch. (data: 3.17e-04). ETA=18:47:36, max mem: 15.9 GB 
[10/29 03:39:44 visual_prompt]: 	Training 400/1106. train loss: 1.1157,	0.6548 s / batch. (data: 7.87e-04). ETA=19:38:26, max mem: 15.9 GB 
[10/29 03:40:47 visual_prompt]: 	Training 500/1106. train loss: 1.4011,	0.6185 s / batch. (data: 3.23e-04). ETA=18:32:06, max mem: 15.9 GB 
[10/29 03:41:51 visual_prompt]: 	Training 600/1106. train loss: 0.5173,	0.6564 s / batch. (data: 3.84e-02). ETA=19:39:16, max mem: 15.9 GB 
[10/29 03:42:54 visual_prompt]: 	Training 700/1106. train loss: 1.5944,	0.6434 s / batch. (data: 8.08e-04). ETA=19:14:42, max mem: 15.9 GB 
[10/29 03:43:57 visual_prompt]: 	Training 800/1106. train loss: 0.2164,	0.6299 s / batch. (data: 3.04e-04). ETA=18:49:35, max mem: 15.9 GB 
[10/29 03:45:01 visual_prompt]: 	Training 900/1106. train loss: 4.4161,	0.6404 s / batch. (data: 2.09e-02). ETA=19:07:19, max mem: 15.9 GB 
[10/29 03:46:04 visual_prompt]: 	Training 1000/1106. train loss: 0.6428,	0.6191 s / batch. (data: 3.03e-04). ETA=18:28:08, max mem: 15.9 GB 
[10/29 03:47:07 visual_prompt]: 	Training 1100/1106. train loss: 0.8300,	0.6194 s / batch. (data: 1.44e-04). ETA=18:27:39, max mem: 15.9 GB 
[10/29 03:47:11 visual_prompt]: Epoch 3 / 100: avg data time: 4.47e-03, avg batch time: 0.6352, average train loss: 1.0869
[10/29 03:48:01 visual_prompt]: 	Test 100/123. loss: 1.800, 0.2368 s / batch. (data: 2.60e-05)max mem: 15.94594 GB 
[10/29 03:48:11 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2322, average loss: 1.9475
[10/29 03:48:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.76	
[10/29 03:48:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/29 03:49:17 visual_prompt]: 	Training 100/1106. train loss: 0.1600,	0.6269 s / batch. (data: 1.05e-02). ETA=18:39:52, max mem: 15.9 GB 
[10/29 03:50:20 visual_prompt]: 	Training 200/1106. train loss: 4.5664,	0.6377 s / batch. (data: 5.94e-03). ETA=18:58:02, max mem: 15.9 GB 
[10/29 03:51:23 visual_prompt]: 	Training 300/1106. train loss: 0.6426,	0.6297 s / batch. (data: 8.18e-04). ETA=18:42:49, max mem: 15.9 GB 
[10/29 03:52:27 visual_prompt]: 	Training 400/1106. train loss: 0.8697,	0.6311 s / batch. (data: 3.27e-04). ETA=18:44:10, max mem: 15.9 GB 
[10/29 03:53:30 visual_prompt]: 	Training 500/1106. train loss: 0.9143,	0.6360 s / batch. (data: 1.20e-02). ETA=18:51:51, max mem: 15.9 GB 
[10/29 03:54:33 visual_prompt]: 	Training 600/1106. train loss: 0.1217,	0.6173 s / batch. (data: 2.92e-04). ETA=18:17:30, max mem: 15.9 GB 
[10/29 03:55:36 visual_prompt]: 	Training 700/1106. train loss: 3.1528,	0.6317 s / batch. (data: 8.03e-04). ETA=18:42:05, max mem: 15.9 GB 
[10/29 03:56:39 visual_prompt]: 	Training 800/1106. train loss: 1.4333,	0.6423 s / batch. (data: 8.02e-04). ETA=18:59:58, max mem: 15.9 GB 
[10/29 03:57:42 visual_prompt]: 	Training 900/1106. train loss: 2.3799,	0.6481 s / batch. (data: 8.77e-04). ETA=19:09:09, max mem: 15.9 GB 
[10/29 03:58:45 visual_prompt]: 	Training 1000/1106. train loss: 0.7169,	0.6352 s / batch. (data: 5.43e-03). ETA=18:45:15, max mem: 15.9 GB 
[10/29 03:59:49 visual_prompt]: 	Training 1100/1106. train loss: 2.0517,	0.6184 s / batch. (data: 1.54e-04). ETA=18:14:17, max mem: 15.9 GB 
[10/29 03:59:53 visual_prompt]: Epoch 4 / 100: avg data time: 4.44e-03, avg batch time: 0.6339, average train loss: 1.1774
[10/29 04:00:42 visual_prompt]: 	Test 100/123. loss: 0.681, 0.2286 s / batch. (data: 4.27e-05)max mem: 15.94594 GB 
[10/29 04:00:53 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2327, average loss: 0.7119
[10/29 04:00:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.24	
[10/29 04:00:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/29 04:01:58 visual_prompt]: 	Training 100/1106. train loss: 0.8195,	0.6289 s / batch. (data: 8.73e-04). ETA=18:31:51, max mem: 15.9 GB 
[10/29 04:03:01 visual_prompt]: 	Training 200/1106. train loss: 0.0310,	0.6493 s / batch. (data: 5.91e-03). ETA=19:06:53, max mem: 15.9 GB 
[10/29 04:04:04 visual_prompt]: 	Training 300/1106. train loss: 1.5244,	0.6353 s / batch. (data: 8.19e-04). ETA=18:40:59, max mem: 15.9 GB 
[10/29 04:05:07 visual_prompt]: 	Training 400/1106. train loss: 3.0840,	0.6400 s / batch. (data: 3.08e-04). ETA=18:48:17, max mem: 15.9 GB 
[10/29 04:06:11 visual_prompt]: 	Training 500/1106. train loss: 0.2170,	0.6193 s / batch. (data: 2.89e-04). ETA=18:10:40, max mem: 15.9 GB 
[10/29 04:07:14 visual_prompt]: 	Training 600/1106. train loss: 2.1354,	0.6423 s / batch. (data: 8.10e-04). ETA=18:50:11, max mem: 15.9 GB 
[10/29 04:08:17 visual_prompt]: 	Training 700/1106. train loss: 1.1259,	0.6322 s / batch. (data: 7.93e-04). ETA=18:31:24, max mem: 15.9 GB 
[10/29 04:09:21 visual_prompt]: 	Training 800/1106. train loss: 0.6415,	0.6177 s / batch. (data: 3.08e-04). ETA=18:04:49, max mem: 15.9 GB 
[10/29 04:10:24 visual_prompt]: 	Training 900/1106. train loss: 1.3171,	0.6321 s / batch. (data: 8.17e-04). ETA=18:29:03, max mem: 15.9 GB 
[10/29 04:11:27 visual_prompt]: 	Training 1000/1106. train loss: 0.6098,	0.6298 s / batch. (data: 8.34e-04). ETA=18:23:55, max mem: 15.9 GB 
[10/29 04:12:30 visual_prompt]: 	Training 1100/1106. train loss: 1.0885,	0.6181 s / batch. (data: 1.87e-04). ETA=18:02:31, max mem: 15.9 GB 
[10/29 04:12:34 visual_prompt]: Epoch 5 / 100: avg data time: 4.11e-03, avg batch time: 0.6340, average train loss: 1.2946
[10/29 04:13:24 visual_prompt]: 	Test 100/123. loss: 0.712, 0.2254 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[10/29 04:13:34 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2334, average loss: 0.7348
[10/29 04:13:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.26	
[10/29 04:13:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/29 04:14:40 visual_prompt]: 	Training 100/1106. train loss: 0.8406,	0.6281 s / batch. (data: 1.10e-02). ETA=18:18:55, max mem: 15.9 GB 
[10/29 04:15:43 visual_prompt]: 	Training 200/1106. train loss: 0.1123,	0.6180 s / batch. (data: 3.22e-04). ETA=18:00:08, max mem: 15.9 GB 
[10/29 04:16:46 visual_prompt]: 	Training 300/1106. train loss: 0.4598,	0.6354 s / batch. (data: 7.88e-04). ETA=18:29:35, max mem: 15.9 GB 
[10/29 04:17:50 visual_prompt]: 	Training 400/1106. train loss: 0.6476,	0.6440 s / batch. (data: 7.98e-04). ETA=18:43:27, max mem: 15.9 GB 
[10/29 04:18:53 visual_prompt]: 	Training 500/1106. train loss: 0.8187,	0.6360 s / batch. (data: 2.66e-04). ETA=18:28:25, max mem: 15.9 GB 
[10/29 04:19:56 visual_prompt]: 	Training 600/1106. train loss: 0.4932,	0.6303 s / batch. (data: 1.20e-02). ETA=18:17:31, max mem: 15.9 GB 
[10/29 04:20:59 visual_prompt]: 	Training 700/1106. train loss: 1.5074,	0.6404 s / batch. (data: 5.48e-03). ETA=18:33:58, max mem: 15.9 GB 
[10/29 04:22:03 visual_prompt]: 	Training 800/1106. train loss: 2.1896,	0.6191 s / batch. (data: 3.01e-04). ETA=17:55:55, max mem: 15.9 GB 
[10/29 04:23:06 visual_prompt]: 	Training 900/1106. train loss: 2.5114,	0.6326 s / batch. (data: 7.90e-04). ETA=18:18:16, max mem: 15.9 GB 
[10/29 04:24:09 visual_prompt]: 	Training 1000/1106. train loss: 0.1620,	0.6285 s / batch. (data: 3.25e-04). ETA=18:10:03, max mem: 15.9 GB 
[10/29 04:25:12 visual_prompt]: 	Training 1100/1106. train loss: 0.0077,	0.6188 s / batch. (data: 1.54e-04). ETA=17:52:19, max mem: 15.9 GB 
[10/29 04:25:16 visual_prompt]: Epoch 6 / 100: avg data time: 4.34e-03, avg batch time: 0.6344, average train loss: 1.2793
[10/29 04:26:06 visual_prompt]: 	Test 100/123. loss: 1.625, 0.2260 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/29 04:26:16 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.2325, average loss: 1.9114
[10/29 04:26:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.97	
[10/29 04:26:16 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/29 04:27:21 visual_prompt]: 	Training 100/1106. train loss: 1.9162,	0.6295 s / batch. (data: 3.45e-04). ETA=18:09:44, max mem: 15.9 GB 
[10/29 04:28:25 visual_prompt]: 	Training 200/1106. train loss: 2.5883,	0.6204 s / batch. (data: 2.87e-04). ETA=17:52:51, max mem: 15.9 GB 
[10/29 04:29:28 visual_prompt]: 	Training 300/1106. train loss: 0.6793,	0.6184 s / batch. (data: 3.19e-04). ETA=17:48:22, max mem: 15.9 GB 
[10/29 04:30:31 visual_prompt]: 	Training 400/1106. train loss: 1.3435,	0.6310 s / batch. (data: 3.29e-04). ETA=18:09:09, max mem: 15.9 GB 
[10/29 04:31:35 visual_prompt]: 	Training 500/1106. train loss: 2.8633,	0.6398 s / batch. (data: 2.88e-04). ETA=18:23:13, max mem: 15.9 GB 
[10/29 04:32:38 visual_prompt]: 	Training 600/1106. train loss: 3.0063,	0.6194 s / batch. (data: 3.26e-04). ETA=17:46:59, max mem: 15.9 GB 
[10/29 04:33:41 visual_prompt]: 	Training 700/1106. train loss: 4.1753,	0.6519 s / batch. (data: 3.07e-04). ETA=18:41:59, max mem: 15.9 GB 
[10/29 04:34:44 visual_prompt]: 	Training 800/1106. train loss: 0.8400,	0.6189 s / batch. (data: 3.26e-04). ETA=17:44:04, max mem: 15.9 GB 
[10/29 04:35:48 visual_prompt]: 	Training 900/1106. train loss: 0.8608,	0.6342 s / batch. (data: 3.17e-04). ETA=18:09:18, max mem: 15.9 GB 
[10/29 04:36:51 visual_prompt]: 	Training 1000/1106. train loss: 1.3661,	0.6548 s / batch. (data: 7.95e-04). ETA=18:43:37, max mem: 15.9 GB 
[10/29 04:37:54 visual_prompt]: 	Training 1100/1106. train loss: 2.5803,	0.6186 s / batch. (data: 1.54e-04). ETA=17:40:34, max mem: 15.9 GB 
[10/29 04:37:57 visual_prompt]: Epoch 7 / 100: avg data time: 4.32e-03, avg batch time: 0.6338, average train loss: 1.3263
[10/29 04:38:47 visual_prompt]: 	Test 100/123. loss: 1.712, 0.2415 s / batch. (data: 4.24e-05)max mem: 15.94594 GB 
[10/29 04:38:58 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.2341, average loss: 1.5099
[10/29 04:38:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.35	
[10/29 04:38:58 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/29 04:40:03 visual_prompt]: 	Training 100/1106. train loss: 3.4837,	0.6453 s / batch. (data: 3.01e-04). ETA=18:25:11, max mem: 15.9 GB 
[10/29 04:41:06 visual_prompt]: 	Training 200/1106. train loss: 1.2811,	0.6172 s / batch. (data: 3.23e-04). ETA=17:36:00, max mem: 15.9 GB 
[10/29 04:42:09 visual_prompt]: 	Training 300/1106. train loss: 0.0015,	0.6732 s / batch. (data: 3.72e-02). ETA=19:10:43, max mem: 15.9 GB 
[10/29 04:43:13 visual_prompt]: 	Training 400/1106. train loss: 0.1022,	0.6187 s / batch. (data: 2.41e-04). ETA=17:36:34, max mem: 15.9 GB 
[10/29 04:44:16 visual_prompt]: 	Training 500/1106. train loss: 0.1024,	0.6518 s / batch. (data: 8.86e-04). ETA=18:31:59, max mem: 15.9 GB 
[10/29 04:45:19 visual_prompt]: 	Training 600/1106. train loss: 0.0263,	0.6271 s / batch. (data: 7.36e-04). ETA=17:48:42, max mem: 15.9 GB 
[10/29 04:46:22 visual_prompt]: 	Training 700/1106. train loss: 0.9825,	0.6186 s / batch. (data: 2.62e-04). ETA=17:33:17, max mem: 15.9 GB 
[10/29 04:47:25 visual_prompt]: 	Training 800/1106. train loss: 2.2249,	0.6338 s / batch. (data: 8.58e-04). ETA=17:58:02, max mem: 15.9 GB 
[10/29 04:48:29 visual_prompt]: 	Training 900/1106. train loss: 0.1251,	0.6459 s / batch. (data: 7.94e-04). ETA=18:17:39, max mem: 15.9 GB 
[10/29 04:49:32 visual_prompt]: 	Training 1000/1106. train loss: 1.1067,	0.6180 s / batch. (data: 3.21e-04). ETA=17:29:08, max mem: 15.9 GB 
[10/29 04:50:35 visual_prompt]: 	Training 1100/1106. train loss: 0.2817,	0.6169 s / batch. (data: 1.59e-04). ETA=17:26:10, max mem: 15.9 GB 
[10/29 04:50:39 visual_prompt]: Epoch 8 / 100: avg data time: 3.85e-03, avg batch time: 0.6337, average train loss: 1.3971
[10/29 04:51:28 visual_prompt]: 	Test 100/123. loss: 0.703, 0.2286 s / batch. (data: 4.27e-05)max mem: 15.94594 GB 
[10/29 04:51:39 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2338, average loss: 0.7537
[10/29 04:51:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.34	
[10/29 04:51:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/29 04:52:45 visual_prompt]: 	Training 100/1106. train loss: 5.9521,	0.6340 s / batch. (data: 7.23e-04). ETA=17:54:02, max mem: 15.9 GB 
[10/29 04:53:48 visual_prompt]: 	Training 200/1106. train loss: 0.6598,	0.6517 s / batch. (data: 8.20e-04). ETA=18:23:02, max mem: 15.9 GB 
[10/29 04:54:51 visual_prompt]: 	Training 300/1106. train loss: 1.8401,	0.6319 s / batch. (data: 7.79e-04). ETA=17:48:27, max mem: 15.9 GB 
[10/29 04:55:54 visual_prompt]: 	Training 400/1106. train loss: 0.3367,	0.6370 s / batch. (data: 2.70e-04). ETA=17:56:02, max mem: 15.9 GB 
[10/29 04:56:58 visual_prompt]: 	Training 500/1106. train loss: 4.9003,	0.6578 s / batch. (data: 1.56e-02). ETA=18:30:01, max mem: 15.9 GB 
[10/29 04:58:01 visual_prompt]: 	Training 600/1106. train loss: 0.8878,	0.6669 s / batch. (data: 7.38e-04). ETA=18:44:13, max mem: 15.9 GB 
[10/29 04:59:04 visual_prompt]: 	Training 700/1106. train loss: 0.4367,	0.6345 s / batch. (data: 7.89e-04). ETA=17:48:39, max mem: 15.9 GB 
[10/29 05:00:08 visual_prompt]: 	Training 800/1106. train loss: 1.0700,	0.6187 s / batch. (data: 7.09e-04). ETA=17:20:59, max mem: 15.9 GB 
[10/29 05:01:11 visual_prompt]: 	Training 900/1106. train loss: 1.0717,	0.6374 s / batch. (data: 3.14e-04). ETA=17:51:19, max mem: 15.9 GB 
[10/29 05:02:15 visual_prompt]: 	Training 1000/1106. train loss: 0.2866,	0.6415 s / batch. (data: 8.13e-04). ETA=17:57:15, max mem: 15.9 GB 
[10/29 05:03:18 visual_prompt]: 	Training 1100/1106. train loss: 0.7408,	0.6180 s / batch. (data: 1.63e-04). ETA=17:16:47, max mem: 15.9 GB 
[10/29 05:03:22 visual_prompt]: Epoch 9 / 100: avg data time: 4.66e-03, avg batch time: 0.6352, average train loss: 1.2340
[10/29 05:04:11 visual_prompt]: 	Test 100/123. loss: 0.890, 0.2253 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[10/29 05:04:22 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2338, average loss: 0.7926
[10/29 05:04:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.44	
[10/29 05:04:22 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/29 05:05:27 visual_prompt]: 	Training 100/1106. train loss: 1.1357,	0.6370 s / batch. (data: 1.30e-02). ETA=17:47:31, max mem: 15.9 GB 
[10/29 05:06:31 visual_prompt]: 	Training 200/1106. train loss: 6.4724,	0.6505 s / batch. (data: 8.04e-04). ETA=18:08:56, max mem: 15.9 GB 
[10/29 05:07:34 visual_prompt]: 	Training 300/1106. train loss: 3.0486,	0.6200 s / batch. (data: 3.15e-04). ETA=17:16:56, max mem: 15.9 GB 
[10/29 05:08:37 visual_prompt]: 	Training 400/1106. train loss: 2.2484,	0.6208 s / batch. (data: 3.10e-04). ETA=17:17:08, max mem: 15.9 GB 
[10/29 05:09:41 visual_prompt]: 	Training 500/1106. train loss: 0.6421,	0.6223 s / batch. (data: 3.04e-04). ETA=17:18:37, max mem: 15.9 GB 
[10/29 05:10:44 visual_prompt]: 	Training 600/1106. train loss: 2.7564,	0.6307 s / batch. (data: 1.23e-02). ETA=17:31:34, max mem: 15.9 GB 
[10/29 05:11:47 visual_prompt]: 	Training 700/1106. train loss: 1.1198,	0.6345 s / batch. (data: 8.11e-04). ETA=17:36:52, max mem: 15.9 GB 
[10/29 05:12:50 visual_prompt]: 	Training 800/1106. train loss: 1.1658,	0.6346 s / batch. (data: 3.00e-04). ETA=17:35:59, max mem: 15.9 GB 
[10/29 05:13:53 visual_prompt]: 	Training 900/1106. train loss: 2.7638,	0.6334 s / batch. (data: 7.76e-04). ETA=17:33:01, max mem: 15.9 GB 
[10/29 05:14:57 visual_prompt]: 	Training 1000/1106. train loss: 0.0016,	0.6325 s / batch. (data: 5.39e-03). ETA=17:30:23, max mem: 15.9 GB 
[10/29 05:16:00 visual_prompt]: 	Training 1100/1106. train loss: 0.2649,	0.6189 s / batch. (data: 1.60e-04). ETA=17:06:45, max mem: 15.9 GB 
[10/29 05:16:04 visual_prompt]: Epoch 10 / 100: avg data time: 4.19e-03, avg batch time: 0.6344, average train loss: 1.7543
[10/29 05:16:54 visual_prompt]: 	Test 100/123. loss: 1.227, 0.2247 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/29 05:17:04 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2321, average loss: 1.1379
[10/29 05:17:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.37	
[10/29 05:17:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/29 05:18:10 visual_prompt]: 	Training 100/1106. train loss: 8.4313,	0.6360 s / batch. (data: 7.73e-04). ETA=17:34:03, max mem: 15.9 GB 
[10/29 05:19:13 visual_prompt]: 	Training 200/1106. train loss: 0.7922,	0.6422 s / batch. (data: 3.34e-04). ETA=17:43:17, max mem: 15.9 GB 
[10/29 05:20:16 visual_prompt]: 	Training 300/1106. train loss: 1.2809,	0.6397 s / batch. (data: 8.07e-04). ETA=17:38:02, max mem: 15.9 GB 
[10/29 05:21:19 visual_prompt]: 	Training 400/1106. train loss: 1.1406,	0.6197 s / batch. (data: 4.68e-04). ETA=17:03:54, max mem: 15.9 GB 
[10/29 05:22:23 visual_prompt]: 	Training 500/1106. train loss: 2.4954,	0.6189 s / batch. (data: 3.21e-04). ETA=17:01:31, max mem: 15.9 GB 
[10/29 05:23:26 visual_prompt]: 	Training 600/1106. train loss: 0.0598,	0.6360 s / batch. (data: 8.98e-04). ETA=17:28:43, max mem: 15.9 GB 
[10/29 05:24:29 visual_prompt]: 	Training 700/1106. train loss: 0.1795,	0.6452 s / batch. (data: 7.46e-04). ETA=17:42:56, max mem: 15.9 GB 
[10/29 05:25:33 visual_prompt]: 	Training 800/1106. train loss: 3.0219,	0.6400 s / batch. (data: 7.35e-04). ETA=17:33:12, max mem: 15.9 GB 
[10/29 05:26:36 visual_prompt]: 	Training 900/1106. train loss: 0.1130,	0.6199 s / batch. (data: 3.66e-04). ETA=16:59:07, max mem: 15.9 GB 
[10/29 05:27:39 visual_prompt]: 	Training 1000/1106. train loss: 0.0156,	0.6279 s / batch. (data: 2.97e-04). ETA=17:11:09, max mem: 15.9 GB 
[10/29 05:28:43 visual_prompt]: 	Training 1100/1106. train loss: 1.0847,	0.6186 s / batch. (data: 1.70e-04). ETA=16:54:58, max mem: 15.9 GB 
[10/29 05:28:46 visual_prompt]: Epoch 11 / 100: avg data time: 4.37e-03, avg batch time: 0.6349, average train loss: 1.5227
[10/29 05:29:36 visual_prompt]: 	Test 100/123. loss: 1.087, 0.2277 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/29 05:29:47 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2314, average loss: 1.3298
[10/29 05:29:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.93	
[10/29 05:29:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/29 05:30:54 visual_prompt]: 	Training 100/1106. train loss: 0.5055,	0.6344 s / batch. (data: 8.16e-04). ETA=17:19:47, max mem: 15.9 GB 
[10/29 05:31:57 visual_prompt]: 	Training 200/1106. train loss: 0.7289,	0.6204 s / batch. (data: 3.31e-04). ETA=16:55:48, max mem: 15.9 GB 
[10/29 05:33:00 visual_prompt]: 	Training 300/1106. train loss: 3.0639,	0.6356 s / batch. (data: 8.03e-04). ETA=17:19:33, max mem: 15.9 GB 
[10/29 05:34:04 visual_prompt]: 	Training 400/1106. train loss: 0.1333,	0.6338 s / batch. (data: 7.72e-04). ETA=17:15:35, max mem: 15.9 GB 
[10/29 05:35:07 visual_prompt]: 	Training 500/1106. train loss: 1.0765,	0.6256 s / batch. (data: 3.20e-04). ETA=17:01:09, max mem: 15.9 GB 
[10/29 05:36:10 visual_prompt]: 	Training 600/1106. train loss: 0.7363,	0.6330 s / batch. (data: 7.95e-04). ETA=17:12:11, max mem: 15.9 GB 
[10/29 05:37:14 visual_prompt]: 	Training 700/1106. train loss: 1.0387,	0.6323 s / batch. (data: 7.24e-04). ETA=17:09:54, max mem: 15.9 GB 
[10/29 05:38:17 visual_prompt]: 	Training 800/1106. train loss: 1.2915,	0.6473 s / batch. (data: 7.38e-04). ETA=17:33:18, max mem: 15.9 GB 
[10/29 05:39:20 visual_prompt]: 	Training 900/1106. train loss: 1.7277,	0.6239 s / batch. (data: 7.43e-04). ETA=16:54:08, max mem: 15.9 GB 
[10/29 05:40:24 visual_prompt]: 	Training 1000/1106. train loss: 0.4220,	0.6224 s / batch. (data: 3.17e-04). ETA=16:50:45, max mem: 15.9 GB 
[10/29 05:41:27 visual_prompt]: 	Training 1100/1106. train loss: 1.5815,	0.6182 s / batch. (data: 1.49e-04). ETA=16:42:55, max mem: 15.9 GB 
[10/29 05:41:31 visual_prompt]: Epoch 12 / 100: avg data time: 5.34e-03, avg batch time: 0.6362, average train loss: 1.5203
[10/29 05:42:21 visual_prompt]: 	Test 100/123. loss: 0.673, 0.2246 s / batch. (data: 5.01e-05)max mem: 15.94594 GB 
[10/29 05:42:32 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.2322, average loss: 1.1081
[10/29 05:42:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.31	
[10/29 05:42:32 visual_prompt]: Best epoch 12: best metric: -1.108
[10/29 05:42:32 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/29 05:43:37 visual_prompt]: 	Training 100/1106. train loss: 6.3399,	0.6184 s / batch. (data: 3.23e-04). ETA=16:42:02, max mem: 15.9 GB 
[10/29 05:44:41 visual_prompt]: 	Training 200/1106. train loss: 1.9896,	0.6383 s / batch. (data: 8.05e-04). ETA=17:13:12, max mem: 15.9 GB 
[10/29 05:45:44 visual_prompt]: 	Training 300/1106. train loss: 0.0025,	0.6191 s / batch. (data: 3.16e-04). ETA=16:41:05, max mem: 15.9 GB 
[10/29 05:46:47 visual_prompt]: 	Training 400/1106. train loss: 3.9235,	0.6332 s / batch. (data: 8.47e-04). ETA=17:02:54, max mem: 15.9 GB 
[10/29 05:47:50 visual_prompt]: 	Training 500/1106. train loss: 0.5220,	0.6216 s / batch. (data: 3.02e-04). ETA=16:43:06, max mem: 15.9 GB 
[10/29 05:48:54 visual_prompt]: 	Training 600/1106. train loss: 0.9986,	0.6178 s / batch. (data: 3.38e-04). ETA=16:35:55, max mem: 15.9 GB 
[10/29 05:49:57 visual_prompt]: 	Training 700/1106. train loss: 1.9575,	0.6339 s / batch. (data: 8.36e-04). ETA=17:00:56, max mem: 15.9 GB 
[10/29 05:51:01 visual_prompt]: 	Training 800/1106. train loss: 1.2752,	0.6372 s / batch. (data: 3.16e-04). ETA=17:05:06, max mem: 15.9 GB 
[10/29 05:52:04 visual_prompt]: 	Training 900/1106. train loss: 3.9182,	0.6419 s / batch. (data: 8.04e-04). ETA=17:11:35, max mem: 15.9 GB 
[10/29 05:53:07 visual_prompt]: 	Training 1000/1106. train loss: 0.0229,	0.6306 s / batch. (data: 8.01e-04). ETA=16:52:22, max mem: 15.9 GB 
[10/29 05:54:11 visual_prompt]: 	Training 1100/1106. train loss: 0.6923,	0.6180 s / batch. (data: 1.84e-04). ETA=16:31:05, max mem: 15.9 GB 
[10/29 05:54:15 visual_prompt]: Epoch 13 / 100: avg data time: 4.71e-03, avg batch time: 0.6356, average train loss: 1.6478
[10/29 05:55:05 visual_prompt]: 	Test 100/123. loss: 0.787, 0.2382 s / batch. (data: 3.43e-05)max mem: 15.94594 GB 
[10/29 05:55:15 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.2320, average loss: 0.7978
[10/29 05:55:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 58.07	
[10/29 05:55:15 visual_prompt]: Best epoch 13: best metric: -0.798
[10/29 05:55:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/29 05:56:21 visual_prompt]: 	Training 100/1106. train loss: 0.0101,	0.6448 s / batch. (data: 8.73e-04). ETA=17:12:56, max mem: 15.9 GB 
[10/29 05:57:24 visual_prompt]: 	Training 200/1106. train loss: 2.6350,	0.6304 s / batch. (data: 3.32e-04). ETA=16:48:55, max mem: 15.9 GB 
[10/29 05:58:28 visual_prompt]: 	Training 300/1106. train loss: 1.4548,	0.6485 s / batch. (data: 8.36e-04). ETA=17:16:50, max mem: 15.9 GB 
[10/29 05:59:31 visual_prompt]: 	Training 400/1106. train loss: 0.0036,	0.6409 s / batch. (data: 8.07e-04). ETA=17:03:27, max mem: 15.9 GB 
[10/29 06:00:35 visual_prompt]: 	Training 500/1106. train loss: 1.0142,	0.6371 s / batch. (data: 8.11e-04). ETA=16:56:21, max mem: 15.9 GB 
[10/29 06:01:38 visual_prompt]: 	Training 600/1106. train loss: 1.4230,	0.6194 s / batch. (data: 3.06e-04). ETA=16:27:03, max mem: 15.9 GB 
[10/29 06:02:42 visual_prompt]: 	Training 700/1106. train loss: 1.6258,	0.6208 s / batch. (data: 3.07e-04). ETA=16:28:21, max mem: 15.9 GB 
[10/29 06:03:45 visual_prompt]: 	Training 800/1106. train loss: 1.1176,	0.6325 s / batch. (data: 4.52e-04). ETA=16:45:50, max mem: 15.9 GB 
[10/29 06:04:48 visual_prompt]: 	Training 900/1106. train loss: 1.0086,	0.6345 s / batch. (data: 7.67e-04). ETA=16:47:57, max mem: 15.9 GB 
[10/29 06:05:52 visual_prompt]: 	Training 1000/1106. train loss: 0.4069,	0.6343 s / batch. (data: 8.04e-04). ETA=16:46:38, max mem: 15.9 GB 
[10/29 06:06:55 visual_prompt]: 	Training 1100/1106. train loss: 0.8528,	0.6190 s / batch. (data: 1.58e-04). ETA=16:21:21, max mem: 15.9 GB 
[10/29 06:06:59 visual_prompt]: Epoch 14 / 100: avg data time: 4.38e-03, avg batch time: 0.6357, average train loss: 1.5225
[10/29 06:07:48 visual_prompt]: 	Test 100/123. loss: 1.087, 0.2298 s / batch. (data: 3.74e-05)max mem: 15.94594 GB 
[10/29 06:07:59 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.2320, average loss: 1.0238
[10/29 06:07:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.82	
[10/29 06:07:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/29 06:09:04 visual_prompt]: 	Training 100/1106. train loss: 4.3629,	0.6464 s / batch. (data: 8.08e-04). ETA=17:03:41, max mem: 15.9 GB 
[10/29 06:10:07 visual_prompt]: 	Training 200/1106. train loss: 2.0915,	0.6321 s / batch. (data: 8.90e-04). ETA=16:39:59, max mem: 15.9 GB 
[10/29 06:11:10 visual_prompt]: 	Training 300/1106. train loss: 3.7653,	0.6423 s / batch. (data: 7.68e-04). ETA=16:55:01, max mem: 15.9 GB 
[10/29 06:12:14 visual_prompt]: 	Training 400/1106. train loss: 5.6109,	0.6328 s / batch. (data: 3.17e-04). ETA=16:38:55, max mem: 15.9 GB 
[10/29 06:13:17 visual_prompt]: 	Training 500/1106. train loss: 0.5071,	0.6408 s / batch. (data: 8.13e-04). ETA=16:50:26, max mem: 15.9 GB 
[10/29 06:14:20 visual_prompt]: 	Training 600/1106. train loss: 0.6718,	0.6217 s / batch. (data: 3.07e-04). ETA=16:19:23, max mem: 15.9 GB 
[10/29 06:15:24 visual_prompt]: 	Training 700/1106. train loss: 0.7753,	0.6470 s / batch. (data: 8.24e-04). ETA=16:58:08, max mem: 15.9 GB 
[10/29 06:16:27 visual_prompt]: 	Training 800/1106. train loss: 0.8462,	0.6247 s / batch. (data: 4.70e-04). ETA=16:21:55, max mem: 15.9 GB 
[10/29 06:17:30 visual_prompt]: 	Training 900/1106. train loss: 3.0318,	0.6192 s / batch. (data: 3.11e-04). ETA=16:12:15, max mem: 15.9 GB 
[10/29 06:18:33 visual_prompt]: 	Training 1000/1106. train loss: 0.8164,	0.6320 s / batch. (data: 1.20e-02). ETA=16:31:24, max mem: 15.9 GB 
[10/29 06:19:36 visual_prompt]: 	Training 1100/1106. train loss: 1.1506,	0.6187 s / batch. (data: 1.64e-04). ETA=16:09:25, max mem: 15.9 GB 
[10/29 06:19:40 visual_prompt]: Epoch 15 / 100: avg data time: 3.99e-03, avg batch time: 0.6342, average train loss: 1.5552
[10/29 06:20:30 visual_prompt]: 	Test 100/123. loss: 1.342, 0.2388 s / batch. (data: 3.72e-05)max mem: 15.94594 GB 
[10/29 06:20:41 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2332, average loss: 1.7186
[10/29 06:20:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.61	
[10/29 06:20:41 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/29 06:21:46 visual_prompt]: 	Training 100/1106. train loss: 1.1732,	0.6505 s / batch. (data: 7.83e-04). ETA=16:58:08, max mem: 15.9 GB 
[10/29 06:22:49 visual_prompt]: 	Training 200/1106. train loss: 0.9230,	0.6462 s / batch. (data: 8.89e-04). ETA=16:50:20, max mem: 15.9 GB 
[10/29 06:23:52 visual_prompt]: 	Training 300/1106. train loss: 0.0610,	0.6390 s / batch. (data: 8.27e-04). ETA=16:38:04, max mem: 15.9 GB 
[10/29 06:24:56 visual_prompt]: 	Training 400/1106. train loss: 2.7207,	0.6180 s / batch. (data: 3.19e-04). ETA=16:04:07, max mem: 15.9 GB 
[10/29 06:25:59 visual_prompt]: 	Training 500/1106. train loss: 1.3816,	0.6185 s / batch. (data: 3.12e-04). ETA=16:03:55, max mem: 15.9 GB 
[10/29 06:27:02 visual_prompt]: 	Training 600/1106. train loss: 0.8507,	0.6447 s / batch. (data: 7.58e-04). ETA=16:43:45, max mem: 15.9 GB 
[10/29 06:28:05 visual_prompt]: 	Training 700/1106. train loss: 2.4288,	0.6350 s / batch. (data: 3.21e-04). ETA=16:27:28, max mem: 15.9 GB 
[10/29 06:29:09 visual_prompt]: 	Training 800/1106. train loss: 2.8437,	0.6364 s / batch. (data: 5.89e-03). ETA=16:28:34, max mem: 15.9 GB 
[10/29 06:30:12 visual_prompt]: 	Training 900/1106. train loss: 0.4699,	0.6211 s / batch. (data: 3.16e-04). ETA=16:03:55, max mem: 15.9 GB 
[10/29 06:31:15 visual_prompt]: 	Training 1000/1106. train loss: 2.5552,	0.6317 s / batch. (data: 7.76e-04). ETA=16:19:17, max mem: 15.9 GB 
[10/29 06:32:19 visual_prompt]: 	Training 1100/1106. train loss: 0.0782,	0.6175 s / batch. (data: 1.49e-04). ETA=15:56:16, max mem: 15.9 GB 
[10/29 06:32:23 visual_prompt]: Epoch 16 / 100: avg data time: 4.18e-03, avg batch time: 0.6346, average train loss: 1.5217
[10/29 06:33:13 visual_prompt]: 	Test 100/123. loss: 0.774, 0.2251 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/29 06:33:23 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.2321, average loss: 0.7697
[10/29 06:33:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 60.02	
[10/29 06:33:23 visual_prompt]: Best epoch 16: best metric: -0.770
[10/29 06:33:23 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/29 06:34:29 visual_prompt]: 	Training 100/1106. train loss: 0.4447,	0.6172 s / batch. (data: 3.14e-04). ETA=15:54:41, max mem: 15.9 GB 
[10/29 06:35:32 visual_prompt]: 	Training 200/1106. train loss: 0.4493,	0.6606 s / batch. (data: 8.38e-04). ETA=17:00:42, max mem: 15.9 GB 
[10/29 06:36:35 visual_prompt]: 	Training 300/1106. train loss: 0.6145,	0.6341 s / batch. (data: 3.24e-04). ETA=16:18:40, max mem: 15.9 GB 
[10/29 06:37:39 visual_prompt]: 	Training 400/1106. train loss: 10.6493,	0.6366 s / batch. (data: 7.83e-04). ETA=16:21:29, max mem: 15.9 GB 
[10/29 06:38:42 visual_prompt]: 	Training 500/1106. train loss: 0.0021,	0.6342 s / batch. (data: 7.46e-04). ETA=16:16:45, max mem: 15.9 GB 
[10/29 06:39:45 visual_prompt]: 	Training 600/1106. train loss: 1.0680,	0.6348 s / batch. (data: 8.22e-04). ETA=16:16:37, max mem: 15.9 GB 
[10/29 06:40:48 visual_prompt]: 	Training 700/1106. train loss: 1.0688,	0.6487 s / batch. (data: 5.85e-03). ETA=16:36:53, max mem: 15.9 GB 
[10/29 06:41:52 visual_prompt]: 	Training 800/1106. train loss: 1.6515,	0.6212 s / batch. (data: 5.07e-04). ETA=15:53:35, max mem: 15.9 GB 
[10/29 06:42:55 visual_prompt]: 	Training 900/1106. train loss: 2.5569,	0.6440 s / batch. (data: 7.71e-04). ETA=16:27:27, max mem: 15.9 GB 
[10/29 06:43:58 visual_prompt]: 	Training 1000/1106. train loss: 0.7858,	0.6393 s / batch. (data: 8.16e-04). ETA=16:19:12, max mem: 15.9 GB 
[10/29 06:45:01 visual_prompt]: 	Training 1100/1106. train loss: 0.0033,	0.6186 s / batch. (data: 1.35e-04). ETA=15:46:31, max mem: 15.9 GB 
[10/29 06:45:05 visual_prompt]: Epoch 17 / 100: avg data time: 4.05e-03, avg batch time: 0.6345, average train loss: 1.5510
[10/29 06:45:55 visual_prompt]: 	Test 100/123. loss: 1.770, 0.2248 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[10/29 06:46:06 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2329, average loss: 2.2744
[10/29 06:46:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.45	
[10/29 06:46:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/29 06:47:11 visual_prompt]: 	Training 100/1106. train loss: 0.0054,	0.6351 s / batch. (data: 7.93e-04). ETA=16:10:36, max mem: 15.9 GB 
[10/29 06:48:15 visual_prompt]: 	Training 200/1106. train loss: 6.7150,	0.6366 s / batch. (data: 8.07e-04). ETA=16:11:50, max mem: 15.9 GB 
[10/29 06:49:18 visual_prompt]: 	Training 300/1106. train loss: 8.4453,	0.6340 s / batch. (data: 7.72e-04). ETA=16:06:50, max mem: 15.9 GB 
[10/29 06:50:21 visual_prompt]: 	Training 400/1106. train loss: 1.1156,	0.6196 s / batch. (data: 2.47e-04). ETA=15:43:48, max mem: 15.9 GB 
[10/29 06:51:25 visual_prompt]: 	Training 500/1106. train loss: 0.0030,	0.6330 s / batch. (data: 2.95e-04). ETA=16:03:15, max mem: 15.9 GB 
[10/29 06:52:28 visual_prompt]: 	Training 600/1106. train loss: 0.2320,	0.6310 s / batch. (data: 7.29e-04). ETA=15:59:04, max mem: 15.9 GB 
[10/29 06:53:31 visual_prompt]: 	Training 700/1106. train loss: 0.2624,	0.6330 s / batch. (data: 8.35e-04). ETA=16:01:02, max mem: 15.9 GB 
[10/29 06:54:35 visual_prompt]: 	Training 800/1106. train loss: 1.0432,	0.6334 s / batch. (data: 8.16e-04). ETA=16:00:35, max mem: 15.9 GB 
[10/29 06:55:38 visual_prompt]: 	Training 900/1106. train loss: 5.4624,	0.6190 s / batch. (data: 2.56e-04). ETA=15:37:41, max mem: 15.9 GB 
[10/29 06:56:41 visual_prompt]: 	Training 1000/1106. train loss: 1.1046,	0.6324 s / batch. (data: 3.20e-04). ETA=15:56:56, max mem: 15.9 GB 
[10/29 06:57:44 visual_prompt]: 	Training 1100/1106. train loss: 0.0009,	0.6195 s / batch. (data: 1.41e-04). ETA=15:36:30, max mem: 15.9 GB 
[10/29 06:57:48 visual_prompt]: Epoch 18 / 100: avg data time: 4.45e-03, avg batch time: 0.6352, average train loss: 1.6648
[10/29 06:58:38 visual_prompt]: 	Test 100/123. loss: 0.500, 0.2397 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/29 06:58:49 visual_prompt]: Inference (val):avg data time: 1.77e-04, avg batch time: 0.2318, average loss: 0.8730
[10/29 06:58:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 57.13	
[10/29 06:58:49 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/29 06:59:54 visual_prompt]: 	Training 100/1106. train loss: 0.0327,	0.6280 s / batch. (data: 1.14e-02). ETA=15:48:16, max mem: 15.9 GB 
[10/29 07:00:57 visual_prompt]: 	Training 200/1106. train loss: 3.3201,	0.6185 s / batch. (data: 2.71e-04). ETA=15:32:48, max mem: 15.9 GB 
[10/29 07:02:00 visual_prompt]: 	Training 300/1106. train loss: 0.6869,	0.6213 s / batch. (data: 3.11e-04). ETA=15:35:59, max mem: 15.9 GB 
[10/29 07:03:04 visual_prompt]: 	Training 400/1106. train loss: 0.4634,	0.6179 s / batch. (data: 3.08e-04). ETA=15:29:52, max mem: 15.9 GB 
[10/29 07:04:07 visual_prompt]: 	Training 500/1106. train loss: 0.9788,	0.6321 s / batch. (data: 7.73e-04). ETA=15:50:10, max mem: 15.9 GB 
[10/29 07:05:10 visual_prompt]: 	Training 600/1106. train loss: 0.0772,	0.6318 s / batch. (data: 8.18e-04). ETA=15:48:44, max mem: 15.9 GB 
[10/29 07:06:13 visual_prompt]: 	Training 700/1106. train loss: 0.3711,	0.6180 s / batch. (data: 3.08e-04). ETA=15:26:52, max mem: 15.9 GB 
[10/29 07:07:17 visual_prompt]: 	Training 800/1106. train loss: 0.0162,	0.6315 s / batch. (data: 8.56e-04). ETA=15:46:04, max mem: 15.9 GB 
[10/29 07:08:20 visual_prompt]: 	Training 900/1106. train loss: 1.7514,	0.6189 s / batch. (data: 7.26e-04). ETA=15:26:10, max mem: 15.9 GB 
[10/29 07:09:23 visual_prompt]: 	Training 1000/1106. train loss: 0.0646,	0.6218 s / batch. (data: 3.29e-04). ETA=15:29:30, max mem: 15.9 GB 
[10/29 07:10:26 visual_prompt]: 	Training 1100/1106. train loss: 3.3143,	0.6185 s / batch. (data: 1.92e-04). ETA=15:23:31, max mem: 15.9 GB 
[10/29 07:10:30 visual_prompt]: Epoch 19 / 100: avg data time: 3.99e-03, avg batch time: 0.6341, average train loss: 1.5627
[10/29 07:11:20 visual_prompt]: 	Test 100/123. loss: 7.048, 0.2374 s / batch. (data: 3.29e-05)max mem: 15.94594 GB 
[10/29 07:11:30 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2329, average loss: 6.0657
[10/29 07:11:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.23	
[10/29 07:11:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[10/29 07:12:36 visual_prompt]: 	Training 100/1106. train loss: 0.8022,	0.6189 s / batch. (data: 3.13e-04). ETA=15:23:07, max mem: 15.9 GB 
[10/29 07:13:39 visual_prompt]: 	Training 200/1106. train loss: 0.6723,	0.6516 s / batch. (data: 8.10e-04). ETA=16:10:39, max mem: 15.9 GB 
[10/29 07:14:42 visual_prompt]: 	Training 300/1106. train loss: 1.3189,	0.6344 s / batch. (data: 8.09e-04). ETA=15:44:05, max mem: 15.9 GB 
[10/29 07:15:45 visual_prompt]: 	Training 400/1106. train loss: 0.3949,	0.6328 s / batch. (data: 3.10e-04). ETA=15:40:37, max mem: 15.9 GB 
[10/29 07:16:49 visual_prompt]: 	Training 500/1106. train loss: 1.1064,	0.6302 s / batch. (data: 3.16e-04). ETA=15:35:44, max mem: 15.9 GB 
[10/29 07:17:52 visual_prompt]: 	Training 600/1106. train loss: 0.7816,	0.6233 s / batch. (data: 3.33e-04). ETA=15:24:22, max mem: 15.9 GB 
[10/29 07:18:55 visual_prompt]: 	Training 700/1106. train loss: 1.6068,	0.6246 s / batch. (data: 2.87e-04). ETA=15:25:20, max mem: 15.9 GB 
[10/29 07:19:58 visual_prompt]: 	Training 800/1106. train loss: 0.2219,	0.6377 s / batch. (data: 7.94e-04). ETA=15:43:41, max mem: 15.9 GB 
[10/29 07:21:02 visual_prompt]: 	Training 900/1106. train loss: 8.3314,	0.6403 s / batch. (data: 2.91e-04). ETA=15:46:25, max mem: 15.9 GB 
[10/29 07:22:05 visual_prompt]: 	Training 1000/1106. train loss: 0.0437,	0.6381 s / batch. (data: 8.85e-04). ETA=15:42:09, max mem: 15.9 GB 
[10/29 07:23:08 visual_prompt]: 	Training 1100/1106. train loss: 1.7836,	0.6168 s / batch. (data: 1.40e-04). ETA=15:09:41, max mem: 15.9 GB 
[10/29 07:23:12 visual_prompt]: Epoch 20 / 100: avg data time: 4.57e-03, avg batch time: 0.6345, average train loss: 1.4424
[10/29 07:24:02 visual_prompt]: 	Test 100/123. loss: 0.695, 0.2275 s / batch. (data: 4.12e-05)max mem: 15.94594 GB 
[10/29 07:24:13 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2325, average loss: 1.0530
[10/29 07:24:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.13	
[10/29 07:24:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[10/29 07:25:18 visual_prompt]: 	Training 100/1106. train loss: 1.0863,	0.6697 s / batch. (data: 8.17e-04). ETA=16:26:31, max mem: 15.9 GB 
[10/29 07:26:22 visual_prompt]: 	Training 200/1106. train loss: 5.2180,	0.6438 s / batch. (data: 9.75e-04). ETA=15:47:17, max mem: 15.9 GB 
[10/29 07:27:25 visual_prompt]: 	Training 300/1106. train loss: 1.5451,	0.6188 s / batch. (data: 3.09e-04). ETA=15:09:22, max mem: 15.9 GB 
[10/29 07:28:28 visual_prompt]: 	Training 400/1106. train loss: 2.5476,	0.6366 s / batch. (data: 1.25e-02). ETA=15:34:30, max mem: 15.9 GB 
[10/29 07:29:31 visual_prompt]: 	Training 500/1106. train loss: 0.2296,	0.6421 s / batch. (data: 8.11e-04). ETA=15:41:30, max mem: 15.9 GB 
[10/29 07:30:34 visual_prompt]: 	Training 600/1106. train loss: 1.2503,	0.6280 s / batch. (data: 3.11e-04). ETA=15:19:44, max mem: 15.9 GB 
[10/29 07:31:38 visual_prompt]: 	Training 700/1106. train loss: 3.4653,	0.6338 s / batch. (data: 8.10e-04). ETA=15:27:12, max mem: 15.9 GB 
[10/29 07:32:41 visual_prompt]: 	Training 800/1106. train loss: 1.3301,	0.6440 s / batch. (data: 1.20e-02). ETA=15:41:06, max mem: 15.9 GB 
[10/29 07:33:44 visual_prompt]: 	Training 900/1106. train loss: 0.0000,	0.6403 s / batch. (data: 9.13e-04). ETA=15:34:35, max mem: 15.9 GB 
[10/29 07:34:48 visual_prompt]: 	Training 1000/1106. train loss: 1.0510,	0.6179 s / batch. (data: 3.17e-04). ETA=15:00:54, max mem: 15.9 GB 
[10/29 07:35:51 visual_prompt]: 	Training 1100/1106. train loss: 1.6156,	0.6188 s / batch. (data: 1.38e-04). ETA=15:01:11, max mem: 15.9 GB 
[10/29 07:35:54 visual_prompt]: Epoch 21 / 100: avg data time: 4.38e-03, avg batch time: 0.6345, average train loss: 1.6651
[10/29 07:36:44 visual_prompt]: 	Test 100/123. loss: 0.851, 0.2463 s / batch. (data: 4.10e-05)max mem: 15.94594 GB 
[10/29 07:36:55 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2322, average loss: 0.8595
[10/29 07:36:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.24	
[10/29 07:36:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[10/29 07:38:00 visual_prompt]: 	Training 100/1106. train loss: 4.2268,	0.6409 s / batch. (data: 1.56e-02). ETA=15:32:16, max mem: 15.9 GB 
[10/29 07:39:03 visual_prompt]: 	Training 200/1106. train loss: 0.0423,	0.6326 s / batch. (data: 7.91e-04). ETA=15:19:08, max mem: 15.9 GB 
[10/29 07:40:07 visual_prompt]: 	Training 300/1106. train loss: 2.3331,	0.6241 s / batch. (data: 3.17e-04). ETA=15:05:46, max mem: 15.9 GB 
[10/29 07:41:10 visual_prompt]: 	Training 400/1106. train loss: 1.8785,	0.6225 s / batch. (data: 3.41e-04). ETA=15:02:18, max mem: 15.9 GB 
[10/29 07:42:13 visual_prompt]: 	Training 500/1106. train loss: 1.5901,	0.6402 s / batch. (data: 8.20e-04). ETA=15:26:56, max mem: 15.9 GB 
[10/29 07:43:16 visual_prompt]: 	Training 600/1106. train loss: 0.0397,	0.6289 s / batch. (data: 3.07e-04). ETA=15:09:29, max mem: 15.9 GB 
[10/29 07:44:19 visual_prompt]: 	Training 700/1106. train loss: 0.5590,	0.6383 s / batch. (data: 7.37e-04). ETA=15:22:07, max mem: 15.9 GB 
[10/29 07:45:23 visual_prompt]: 	Training 800/1106. train loss: 1.6280,	0.6320 s / batch. (data: 8.14e-04). ETA=15:11:58, max mem: 15.9 GB 
[10/29 07:46:26 visual_prompt]: 	Training 900/1106. train loss: 1.1789,	0.6328 s / batch. (data: 7.67e-04). ETA=15:12:02, max mem: 15.9 GB 
[10/29 07:47:30 visual_prompt]: 	Training 1000/1106. train loss: 1.1852,	0.6311 s / batch. (data: 1.32e-02). ETA=15:08:34, max mem: 15.9 GB 
[10/29 07:48:33 visual_prompt]: 	Training 1100/1106. train loss: 2.7787,	0.6188 s / batch. (data: 1.49e-04). ETA=14:49:46, max mem: 15.9 GB 
[10/29 07:48:37 visual_prompt]: Epoch 22 / 100: avg data time: 4.15e-03, avg batch time: 0.6345, average train loss: 1.3578
[10/29 07:49:26 visual_prompt]: 	Test 100/123. loss: 1.032, 0.2354 s / batch. (data: 3.31e-05)max mem: 15.94594 GB 
[10/29 07:49:37 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2327, average loss: 0.9098
[10/29 07:49:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 58.09	
[10/29 07:49:37 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[10/29 07:50:43 visual_prompt]: 	Training 100/1106. train loss: 1.0463,	0.6536 s / batch. (data: 3.78e-02). ETA=15:38:41, max mem: 15.9 GB 
[10/29 07:51:46 visual_prompt]: 	Training 200/1106. train loss: 1.2483,	0.6495 s / batch. (data: 7.84e-04). ETA=15:31:43, max mem: 15.9 GB 
[10/29 07:52:50 visual_prompt]: 	Training 300/1106. train loss: 0.2821,	0.6520 s / batch. (data: 7.83e-04). ETA=15:34:09, max mem: 15.9 GB 
[10/29 07:53:53 visual_prompt]: 	Training 400/1106. train loss: 1.6630,	0.6167 s / batch. (data: 2.90e-04). ETA=14:42:36, max mem: 15.9 GB 
[10/29 07:54:56 visual_prompt]: 	Training 500/1106. train loss: 1.0889,	0.6459 s / batch. (data: 2.82e-02). ETA=15:23:17, max mem: 15.9 GB 
[10/29 07:55:59 visual_prompt]: 	Training 600/1106. train loss: 0.0160,	0.6333 s / batch. (data: 3.29e-04). ETA=15:04:13, max mem: 15.9 GB 
[10/29 07:57:02 visual_prompt]: 	Training 700/1106. train loss: 0.5691,	0.6175 s / batch. (data: 3.01e-04). ETA=14:40:34, max mem: 15.9 GB 
[10/29 07:58:06 visual_prompt]: 	Training 800/1106. train loss: 0.8840,	0.6310 s / batch. (data: 7.65e-04). ETA=14:58:54, max mem: 15.9 GB 
[10/29 07:59:09 visual_prompt]: 	Training 900/1106. train loss: 0.7758,	0.6323 s / batch. (data: 3.48e-04). ETA=14:59:38, max mem: 15.9 GB 
[10/29 08:00:12 visual_prompt]: 	Training 1000/1106. train loss: 0.0502,	0.6515 s / batch. (data: 7.49e-04). ETA=15:25:54, max mem: 15.9 GB 
[10/29 08:01:15 visual_prompt]: 	Training 1100/1106. train loss: 0.8387,	0.6179 s / batch. (data: 1.50e-04). ETA=14:37:01, max mem: 15.9 GB 
[10/29 08:01:19 visual_prompt]: Epoch 23 / 100: avg data time: 4.92e-03, avg batch time: 0.6349, average train loss: 1.4317
[10/29 08:02:09 visual_prompt]: 	Test 100/123. loss: 0.723, 0.2344 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/29 08:02:20 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2318, average loss: 0.7027
[10/29 08:02:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 58.79	
[10/29 08:02:20 visual_prompt]: Best epoch 23: best metric: -0.703
[10/29 08:02:20 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[10/29 08:03:25 visual_prompt]: 	Training 100/1106. train loss: 3.0370,	0.6168 s / batch. (data: 4.47e-04). ETA=14:34:28, max mem: 15.9 GB 
[10/29 08:04:28 visual_prompt]: 	Training 200/1106. train loss: 2.6225,	0.6322 s / batch. (data: 8.90e-04). ETA=14:55:12, max mem: 15.9 GB 
[10/29 08:05:31 visual_prompt]: 	Training 300/1106. train loss: 0.2775,	0.6489 s / batch. (data: 8.23e-04). ETA=15:17:45, max mem: 15.9 GB 
[10/29 08:06:35 visual_prompt]: 	Training 400/1106. train loss: 0.9944,	0.6182 s / batch. (data: 3.31e-04). ETA=14:33:21, max mem: 15.9 GB 
[10/29 08:07:38 visual_prompt]: 	Training 500/1106. train loss: 3.6318,	0.6317 s / batch. (data: 8.03e-04). ETA=14:51:19, max mem: 15.9 GB 
[10/29 08:08:41 visual_prompt]: 	Training 600/1106. train loss: 1.1808,	0.6172 s / batch. (data: 3.00e-04). ETA=14:29:48, max mem: 15.9 GB 
[10/29 08:09:45 visual_prompt]: 	Training 700/1106. train loss: 1.7143,	0.6314 s / batch. (data: 7.32e-04). ETA=14:48:46, max mem: 15.9 GB 
[10/29 08:10:48 visual_prompt]: 	Training 800/1106. train loss: 1.2386,	0.6209 s / batch. (data: 3.22e-04). ETA=14:33:03, max mem: 15.9 GB 
[10/29 08:11:52 visual_prompt]: 	Training 900/1106. train loss: 0.0538,	0.6337 s / batch. (data: 8.18e-04). ETA=14:49:58, max mem: 15.9 GB 
[10/29 08:12:55 visual_prompt]: 	Training 1000/1106. train loss: 0.3792,	0.6186 s / batch. (data: 3.36e-04). ETA=14:27:45, max mem: 15.9 GB 
[10/29 08:13:58 visual_prompt]: 	Training 1100/1106. train loss: 0.8125,	0.6173 s / batch. (data: 1.57e-04). ETA=14:24:55, max mem: 15.9 GB 
[10/29 08:14:02 visual_prompt]: Epoch 24 / 100: avg data time: 4.50e-03, avg batch time: 0.6349, average train loss: 1.5369
[10/29 08:14:51 visual_prompt]: 	Test 100/123. loss: 0.469, 0.2354 s / batch. (data: 3.93e-05)max mem: 15.94594 GB 
[10/29 08:15:02 visual_prompt]: Inference (val):avg data time: 9.81e-05, avg batch time: 0.2326, average loss: 0.7587
[10/29 08:15:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 60.84	
[10/29 08:15:02 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[10/29 08:16:08 visual_prompt]: 	Training 100/1106. train loss: 2.3820,	0.6307 s / batch. (data: 3.45e-04). ETA=14:42:29, max mem: 15.9 GB 
[10/29 08:17:11 visual_prompt]: 	Training 200/1106. train loss: 0.4424,	0.6301 s / batch. (data: 3.26e-04). ETA=14:40:37, max mem: 15.9 GB 
[10/29 08:18:15 visual_prompt]: 	Training 300/1106. train loss: 1.3675,	0.6404 s / batch. (data: 8.04e-04). ETA=14:53:58, max mem: 15.9 GB 
[10/29 08:19:18 visual_prompt]: 	Training 400/1106. train loss: 2.7940,	0.6308 s / batch. (data: 2.53e-04). ETA=14:39:32, max mem: 15.9 GB 
[10/29 08:20:21 visual_prompt]: 	Training 500/1106. train loss: 1.0880,	0.6313 s / batch. (data: 8.64e-04). ETA=14:39:11, max mem: 15.9 GB 
[10/29 08:21:24 visual_prompt]: 	Training 600/1106. train loss: 2.5917,	0.6313 s / batch. (data: 8.00e-04). ETA=14:38:07, max mem: 15.9 GB 
[10/29 08:22:28 visual_prompt]: 	Training 700/1106. train loss: 1.1964,	0.6765 s / batch. (data: 5.93e-02). ETA=15:39:50, max mem: 15.9 GB 
[10/29 08:23:31 visual_prompt]: 	Training 800/1106. train loss: 0.0974,	0.6261 s / batch. (data: 2.97e-04). ETA=14:28:49, max mem: 15.9 GB 
[10/29 08:24:34 visual_prompt]: 	Training 900/1106. train loss: 1.2260,	0.6308 s / batch. (data: 8.06e-04). ETA=14:34:16, max mem: 15.9 GB 
[10/29 08:25:37 visual_prompt]: 	Training 1000/1106. train loss: 1.7287,	0.6188 s / batch. (data: 3.00e-04). ETA=14:16:35, max mem: 15.9 GB 
[10/29 08:26:41 visual_prompt]: 	Training 1100/1106. train loss: 0.0432,	0.6179 s / batch. (data: 1.50e-04). ETA=14:14:21, max mem: 15.9 GB 
[10/29 08:26:44 visual_prompt]: Epoch 25 / 100: avg data time: 4.76e-03, avg batch time: 0.6350, average train loss: 1.3900
[10/29 08:27:34 visual_prompt]: 	Test 100/123. loss: 1.176, 0.2319 s / batch. (data: 4.55e-05)max mem: 15.94594 GB 
[10/29 08:27:45 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.2336, average loss: 0.9401
[10/29 08:27:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 58.61	
[10/29 08:27:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[10/29 08:28:50 visual_prompt]: 	Training 100/1106. train loss: 1.0057,	0.6325 s / batch. (data: 1.60e-02). ETA=14:33:23, max mem: 15.9 GB 
[10/29 08:29:53 visual_prompt]: 	Training 200/1106. train loss: 2.0037,	0.6457 s / batch. (data: 8.14e-04). ETA=14:50:28, max mem: 15.9 GB 
[10/29 08:30:57 visual_prompt]: 	Training 300/1106. train loss: 0.0427,	0.6542 s / batch. (data: 7.32e-04). ETA=15:01:07, max mem: 15.9 GB 
[10/29 08:32:00 visual_prompt]: 	Training 400/1106. train loss: 0.4415,	0.6333 s / batch. (data: 8.12e-04). ETA=14:31:20, max mem: 15.9 GB 
[10/29 08:33:03 visual_prompt]: 	Training 500/1106. train loss: 1.4727,	0.6355 s / batch. (data: 4.50e-04). ETA=14:33:14, max mem: 15.9 GB 
[10/29 08:34:07 visual_prompt]: 	Training 600/1106. train loss: 0.6974,	0.6300 s / batch. (data: 3.30e-04). ETA=14:24:40, max mem: 15.9 GB 
[10/29 08:35:10 visual_prompt]: 	Training 700/1106. train loss: 2.8313,	0.6420 s / batch. (data: 7.57e-04). ETA=14:40:04, max mem: 15.9 GB 
[10/29 08:36:13 visual_prompt]: 	Training 800/1106. train loss: 0.6433,	0.6246 s / batch. (data: 7.99e-04). ETA=14:15:10, max mem: 15.9 GB 
[10/29 08:37:16 visual_prompt]: 	Training 900/1106. train loss: 0.2527,	0.6319 s / batch. (data: 7.36e-04). ETA=14:24:08, max mem: 15.9 GB 
[10/29 08:38:20 visual_prompt]: 	Training 1000/1106. train loss: 4.2619,	0.6341 s / batch. (data: 8.24e-04). ETA=14:26:04, max mem: 15.9 GB 
[10/29 08:39:23 visual_prompt]: 	Training 1100/1106. train loss: 0.3718,	0.6188 s / batch. (data: 1.49e-04). ETA=14:04:11, max mem: 15.9 GB 
[10/29 08:39:27 visual_prompt]: Epoch 26 / 100: avg data time: 3.99e-03, avg batch time: 0.6350, average train loss: 1.3662
[10/29 08:40:17 visual_prompt]: 	Test 100/123. loss: 1.216, 0.2436 s / batch. (data: 2.60e-05)max mem: 15.94594 GB 
[10/29 08:40:28 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.2323, average loss: 1.1118
[10/29 08:40:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.70	
[10/29 08:40:28 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[10/29 08:41:33 visual_prompt]: 	Training 100/1106. train loss: 0.0045,	0.6313 s / batch. (data: 3.15e-04). ETA=14:20:04, max mem: 15.9 GB 
[10/29 08:42:36 visual_prompt]: 	Training 200/1106. train loss: 1.0396,	0.6332 s / batch. (data: 7.47e-04). ETA=14:21:39, max mem: 15.9 GB 
[10/29 08:43:39 visual_prompt]: 	Training 300/1106. train loss: 1.4388,	0.6323 s / batch. (data: 7.75e-04). ETA=14:19:16, max mem: 15.9 GB 
[10/29 08:44:43 visual_prompt]: 	Training 400/1106. train loss: 4.7360,	0.6434 s / batch. (data: 7.63e-04). ETA=14:33:22, max mem: 15.9 GB 
[10/29 08:45:46 visual_prompt]: 	Training 500/1106. train loss: 0.1536,	0.6330 s / batch. (data: 8.05e-04). ETA=14:18:11, max mem: 15.9 GB 
[10/29 08:46:49 visual_prompt]: 	Training 600/1106. train loss: 3.9393,	0.6498 s / batch. (data: 5.91e-03). ETA=14:39:50, max mem: 15.9 GB 
[10/29 08:47:53 visual_prompt]: 	Training 700/1106. train loss: 1.0650,	0.6189 s / batch. (data: 3.25e-04). ETA=13:56:59, max mem: 15.9 GB 
[10/29 08:48:56 visual_prompt]: 	Training 800/1106. train loss: 1.5500,	0.6438 s / batch. (data: 7.72e-04). ETA=14:29:36, max mem: 15.9 GB 
[10/29 08:49:59 visual_prompt]: 	Training 900/1106. train loss: 2.4032,	0.6177 s / batch. (data: 3.45e-04). ETA=13:53:22, max mem: 15.9 GB 
[10/29 08:51:03 visual_prompt]: 	Training 1000/1106. train loss: 0.3546,	0.6312 s / batch. (data: 3.17e-04). ETA=14:10:26, max mem: 15.9 GB 
[10/29 08:52:06 visual_prompt]: 	Training 1100/1106. train loss: 0.0752,	0.6181 s / batch. (data: 1.52e-04). ETA=13:51:50, max mem: 15.9 GB 
[10/29 08:52:10 visual_prompt]: Epoch 27 / 100: avg data time: 4.36e-03, avg batch time: 0.6346, average train loss: 1.4653
[10/29 08:52:59 visual_prompt]: 	Test 100/123. loss: 3.272, 0.2437 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/29 08:53:10 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2312, average loss: 4.0319
[10/29 08:53:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.91	
[10/29 08:53:10 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[10/29 08:54:15 visual_prompt]: 	Training 100/1106. train loss: 0.0508,	0.6333 s / batch. (data: 8.13e-04). ETA=14:11:06, max mem: 15.9 GB 
[10/29 08:55:18 visual_prompt]: 	Training 200/1106. train loss: 0.0331,	0.6305 s / batch. (data: 2.81e-04). ETA=14:06:19, max mem: 15.9 GB 
[10/29 08:56:22 visual_prompt]: 	Training 300/1106. train loss: 0.2802,	0.6334 s / batch. (data: 3.31e-04). ETA=14:09:06, max mem: 15.9 GB 
[10/29 08:57:25 visual_prompt]: 	Training 400/1106. train loss: 0.9980,	0.6409 s / batch. (data: 1.55e-02). ETA=14:18:09, max mem: 15.9 GB 
[10/29 08:58:29 visual_prompt]: 	Training 500/1106. train loss: 1.3038,	0.6402 s / batch. (data: 3.20e-04). ETA=14:16:12, max mem: 15.9 GB 
[10/29 08:59:32 visual_prompt]: 	Training 600/1106. train loss: 1.0432,	0.6339 s / batch. (data: 7.83e-04). ETA=14:06:37, max mem: 15.9 GB 
[10/29 09:00:35 visual_prompt]: 	Training 700/1106. train loss: 0.1335,	0.6203 s / batch. (data: 3.40e-04). ETA=13:47:27, max mem: 15.9 GB 
[10/29 09:01:38 visual_prompt]: 	Training 800/1106. train loss: 4.6686,	0.6439 s / batch. (data: 3.22e-04). ETA=14:17:53, max mem: 15.9 GB 
[10/29 09:02:42 visual_prompt]: 	Training 900/1106. train loss: 3.3196,	0.6260 s / batch. (data: 3.23e-04). ETA=13:52:55, max mem: 15.9 GB 
[10/29 09:03:45 visual_prompt]: 	Training 1000/1106. train loss: 3.4266,	0.6480 s / batch. (data: 3.27e-04). ETA=14:21:08, max mem: 15.9 GB 
[10/29 09:04:49 visual_prompt]: 	Training 1100/1106. train loss: 1.8053,	0.6188 s / batch. (data: 1.34e-04). ETA=13:41:18, max mem: 15.9 GB 
[10/29 09:04:52 visual_prompt]: Epoch 28 / 100: avg data time: 4.41e-03, avg batch time: 0.6351, average train loss: 1.5860
[10/29 09:05:42 visual_prompt]: 	Test 100/123. loss: 1.244, 0.2357 s / batch. (data: 5.05e-05)max mem: 15.94594 GB 
[10/29 09:05:53 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.2322, average loss: 1.2174
[10/29 09:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 62.46	
[10/29 09:05:53 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[10/29 09:06:58 visual_prompt]: 	Training 100/1106. train loss: 2.7821,	0.6423 s / batch. (data: 8.15e-04). ETA=14:11:19, max mem: 15.9 GB 
[10/29 09:08:01 visual_prompt]: 	Training 200/1106. train loss: 1.9076,	0.6303 s / batch. (data: 2.94e-04). ETA=13:54:28, max mem: 15.9 GB 
[10/29 09:09:04 visual_prompt]: 	Training 300/1106. train loss: 0.9831,	0.6358 s / batch. (data: 8.69e-04). ETA=14:00:36, max mem: 15.9 GB 
[10/29 09:10:07 visual_prompt]: 	Training 400/1106. train loss: 2.3677,	0.6451 s / batch. (data: 8.04e-04). ETA=14:11:55, max mem: 15.9 GB 
[10/29 09:11:11 visual_prompt]: 	Training 500/1106. train loss: 0.4713,	0.6219 s / batch. (data: 7.69e-04). ETA=13:40:10, max mem: 15.9 GB 
[10/29 09:12:13 visual_prompt]: 	Training 600/1106. train loss: 0.7478,	0.6457 s / batch. (data: 1.55e-02). ETA=14:10:30, max mem: 15.9 GB 
[10/29 09:13:16 visual_prompt]: 	Training 700/1106. train loss: 3.3759,	0.6227 s / batch. (data: 3.03e-04). ETA=13:39:14, max mem: 15.9 GB 
[10/29 09:14:20 visual_prompt]: 	Training 800/1106. train loss: 0.0000,	0.6440 s / batch. (data: 7.66e-04). ETA=14:06:05, max mem: 15.9 GB 
[10/29 09:15:23 visual_prompt]: 	Training 900/1106. train loss: 3.1035,	0.6221 s / batch. (data: 3.30e-04). ETA=13:36:18, max mem: 15.9 GB 
[10/29 09:16:27 visual_prompt]: 	Training 1000/1106. train loss: 1.3724,	0.6359 s / batch. (data: 7.99e-03). ETA=13:53:25, max mem: 15.9 GB 
[10/29 09:17:30 visual_prompt]: 	Training 1100/1106. train loss: 0.2906,	0.6174 s / batch. (data: 1.70e-04). ETA=13:28:06, max mem: 15.9 GB 
[10/29 09:17:33 visual_prompt]: Epoch 29 / 100: avg data time: 4.03e-03, avg batch time: 0.6336, average train loss: 1.5177
[10/29 09:18:23 visual_prompt]: 	Test 100/123. loss: 1.857, 0.2287 s / batch. (data: 3.86e-05)max mem: 15.94594 GB 
[10/29 09:18:34 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2326, average loss: 1.6569
[10/29 09:18:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.33	
[10/29 09:18:34 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[10/29 09:19:39 visual_prompt]: 	Training 100/1106. train loss: 1.8601,	0.6166 s / batch. (data: 2.82e-04). ETA=13:25:59, max mem: 15.9 GB 
[10/29 09:20:42 visual_prompt]: 	Training 200/1106. train loss: 0.7026,	0.6420 s / batch. (data: 8.18e-04). ETA=13:58:07, max mem: 15.9 GB 
[10/29 09:21:45 visual_prompt]: 	Training 300/1106. train loss: 1.5285,	0.6177 s / batch. (data: 3.04e-04). ETA=13:25:16, max mem: 15.9 GB 
[10/29 09:22:49 visual_prompt]: 	Training 400/1106. train loss: 1.0934,	0.6314 s / batch. (data: 7.41e-04). ETA=13:42:12, max mem: 15.9 GB 
[10/29 09:23:52 visual_prompt]: 	Training 500/1106. train loss: 1.0249,	0.6408 s / batch. (data: 7.93e-04). ETA=13:53:16, max mem: 15.9 GB 
[10/29 09:24:55 visual_prompt]: 	Training 600/1106. train loss: 0.0812,	0.6181 s / batch. (data: 3.19e-04). ETA=13:22:49, max mem: 15.9 GB 
[10/29 09:25:59 visual_prompt]: 	Training 700/1106. train loss: 1.2326,	0.6563 s / batch. (data: 2.03e-02). ETA=14:11:15, max mem: 15.9 GB 
[10/29 09:27:02 visual_prompt]: 	Training 800/1106. train loss: 0.8570,	0.6337 s / batch. (data: 1.37e-02). ETA=13:40:58, max mem: 15.9 GB 
[10/29 09:28:05 visual_prompt]: 	Training 900/1106. train loss: 1.2292,	0.6222 s / batch. (data: 3.12e-04). ETA=13:24:58, max mem: 15.9 GB 
[10/29 09:29:08 visual_prompt]: 	Training 1000/1106. train loss: 4.0481,	0.6531 s / batch. (data: 5.92e-03). ETA=14:03:53, max mem: 15.9 GB 
[10/29 09:30:12 visual_prompt]: 	Training 1100/1106. train loss: 1.2878,	0.6175 s / batch. (data: 1.32e-04). ETA=13:16:52, max mem: 15.9 GB 
[10/29 09:30:16 visual_prompt]: Epoch 30 / 100: avg data time: 4.43e-03, avg batch time: 0.6346, average train loss: 1.2102
[10/29 09:31:05 visual_prompt]: 	Test 100/123. loss: 0.753, 0.2255 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/29 09:31:16 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.2333, average loss: 0.8562
[10/29 09:31:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.55	
[10/29 09:31:16 visual_prompt]: Stopping early.
[10/29 09:31:16 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 09:31:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 09:31:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 09:31:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 09:31:16 visual_prompt]: Training with config:
[10/29 09:31:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.25_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 09:31:16 visual_prompt]: Loading training data...
[10/29 09:31:16 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 09:31:16 visual_prompt]: Loading validation data...
[10/29 09:31:16 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 09:31:16 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/29 09:31:18 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/29 09:31:18 visual_prompt]: tuned percent:0.522
[10/29 09:31:19 visual_prompt]: Device used for model: 0
[10/29 09:31:19 visual_prompt]: Setting up Evaluator...
[10/29 09:31:19 visual_prompt]: Setting up Trainer...
[10/29 09:31:19 visual_prompt]: 	Setting up the optimizer...
[10/29 09:31:19 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 09:32:24 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6372 s / batch. (data: 8.32e-04). ETA=19:33:30, max mem: 15.9 GB 
[10/29 09:33:27 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6484 s / batch. (data: 8.31e-04). ETA=19:53:02, max mem: 15.9 GB 
[10/29 09:34:31 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6412 s / batch. (data: 7.52e-04). ETA=19:38:39, max mem: 15.9 GB 
[10/29 09:35:34 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6307 s / batch. (data: 3.26e-04). ETA=19:18:19, max mem: 15.9 GB 
[10/29 09:36:37 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6405 s / batch. (data: 1.20e-02). ETA=19:35:14, max mem: 15.9 GB 
[10/29 09:37:40 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6195 s / batch. (data: 2.93e-04). ETA=18:55:39, max mem: 15.9 GB 
[10/29 09:38:44 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6338 s / batch. (data: 3.18e-04). ETA=19:20:50, max mem: 15.9 GB 
[10/29 09:39:47 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6480 s / batch. (data: 1.20e-02). ETA=19:45:51, max mem: 15.9 GB 
[10/29 09:40:50 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6400 s / batch. (data: 8.22e-04). ETA=19:30:08, max mem: 15.9 GB 
[10/29 09:41:53 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6210 s / batch. (data: 3.29e-04). ETA=18:54:16, max mem: 15.9 GB 
[10/29 09:42:57 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6184 s / batch. (data: 1.38e-04). ETA=18:48:39, max mem: 15.9 GB 
[10/29 09:43:01 visual_prompt]: Epoch 1 / 100: avg data time: 4.38e-03, avg batch time: 0.6346, average train loss: 1.4028
[10/29 09:43:51 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2536 s / batch. (data: 3.84e-05)max mem: 15.94594 GB 
[10/29 09:44:01 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.2319, average loss: 1.3505
[10/29 09:44:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/29 09:44:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/29 09:45:06 visual_prompt]: 	Training 100/1106. train loss: 0.6786,	0.6247 s / batch. (data: 3.11e-04). ETA=18:58:54, max mem: 15.9 GB 
[10/29 09:46:09 visual_prompt]: 	Training 200/1106. train loss: 1.0157,	0.6173 s / batch. (data: 2.94e-04). ETA=18:44:22, max mem: 15.9 GB 
[10/29 09:47:12 visual_prompt]: 	Training 300/1106. train loss: 1.2786,	0.6312 s / batch. (data: 5.44e-03). ETA=19:08:45, max mem: 15.9 GB 
[10/29 09:48:15 visual_prompt]: 	Training 400/1106. train loss: 0.1657,	0.6178 s / batch. (data: 3.22e-04). ETA=18:43:17, max mem: 15.9 GB 
[10/29 09:49:19 visual_prompt]: 	Training 500/1106. train loss: 0.7228,	0.6490 s / batch. (data: 1.31e-02). ETA=19:39:02, max mem: 15.9 GB 
[10/29 09:50:22 visual_prompt]: 	Training 600/1106. train loss: 0.7002,	0.6198 s / batch. (data: 3.32e-04). ETA=18:44:47, max mem: 15.9 GB 
[10/29 09:51:25 visual_prompt]: 	Training 700/1106. train loss: 0.9986,	0.6546 s / batch. (data: 2.66e-02). ETA=19:46:57, max mem: 15.9 GB 
[10/29 09:52:29 visual_prompt]: 	Training 800/1106. train loss: 0.7137,	0.6608 s / batch. (data: 4.08e-02). ETA=19:57:03, max mem: 15.9 GB 
[10/29 09:53:32 visual_prompt]: 	Training 900/1106. train loss: 0.7605,	0.6529 s / batch. (data: 7.53e-04). ETA=19:41:43, max mem: 15.9 GB 
[10/29 09:54:35 visual_prompt]: 	Training 1000/1106. train loss: 0.5445,	0.6467 s / batch. (data: 7.21e-04). ETA=19:29:27, max mem: 15.9 GB 
[10/29 09:55:38 visual_prompt]: 	Training 1100/1106. train loss: 0.6415,	0.6183 s / batch. (data: 1.32e-04). ETA=18:37:02, max mem: 15.9 GB 
[10/29 09:55:42 visual_prompt]: Epoch 2 / 100: avg data time: 3.66e-03, avg batch time: 0.6338, average train loss: 0.8161
[10/29 09:56:32 visual_prompt]: 	Test 100/123. loss: 1.092, 0.2486 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/29 09:56:43 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2325, average loss: 0.9989
[10/29 09:56:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.36	
[10/29 09:56:43 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/29 09:57:49 visual_prompt]: 	Training 100/1106. train loss: 0.5092,	0.6323 s / batch. (data: 3.50e-04). ETA=19:01:09, max mem: 15.9 GB 
[10/29 09:58:52 visual_prompt]: 	Training 200/1106. train loss: 0.3936,	0.6198 s / batch. (data: 3.05e-04). ETA=18:37:39, max mem: 15.9 GB 
[10/29 09:59:55 visual_prompt]: 	Training 300/1106. train loss: 0.4224,	0.6234 s / batch. (data: 3.06e-04). ETA=18:43:03, max mem: 15.9 GB 
[10/29 10:00:59 visual_prompt]: 	Training 400/1106. train loss: 0.7665,	0.6413 s / batch. (data: 7.98e-04). ETA=19:14:13, max mem: 15.9 GB 
[10/29 10:02:02 visual_prompt]: 	Training 500/1106. train loss: 1.1549,	0.6187 s / batch. (data: 3.18e-04). ETA=18:32:28, max mem: 15.9 GB 
[10/29 10:03:05 visual_prompt]: 	Training 600/1106. train loss: 0.7103,	0.6303 s / batch. (data: 7.79e-04). ETA=18:52:14, max mem: 15.9 GB 
[10/29 10:04:08 visual_prompt]: 	Training 700/1106. train loss: 1.4557,	0.6583 s / batch. (data: 8.10e-04). ETA=19:41:26, max mem: 15.9 GB 
[10/29 10:05:11 visual_prompt]: 	Training 800/1106. train loss: 0.8767,	0.6213 s / batch. (data: 2.79e-04). ETA=18:34:01, max mem: 15.9 GB 
[10/29 10:06:15 visual_prompt]: 	Training 900/1106. train loss: 1.9625,	0.6293 s / batch. (data: 3.18e-04). ETA=18:47:19, max mem: 15.9 GB 
[10/29 10:07:18 visual_prompt]: 	Training 1000/1106. train loss: 0.7037,	0.6492 s / batch. (data: 1.62e-02). ETA=19:21:51, max mem: 15.9 GB 
[10/29 10:08:21 visual_prompt]: 	Training 1100/1106. train loss: 0.6967,	0.6178 s / batch. (data: 1.34e-04). ETA=18:24:44, max mem: 15.9 GB 
[10/29 10:08:25 visual_prompt]: Epoch 3 / 100: avg data time: 5.21e-03, avg batch time: 0.6349, average train loss: 0.7942
[10/29 10:09:15 visual_prompt]: 	Test 100/123. loss: 0.764, 0.2297 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/29 10:09:25 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2318, average loss: 0.8016
[10/29 10:09:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.03	
[10/29 10:09:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/29 10:10:31 visual_prompt]: 	Training 100/1106. train loss: 0.6441,	0.6320 s / batch. (data: 3.35e-04). ETA=18:48:59, max mem: 15.9 GB 
[10/29 10:11:34 visual_prompt]: 	Training 200/1106. train loss: 1.1504,	0.6333 s / batch. (data: 8.18e-04). ETA=18:50:16, max mem: 15.9 GB 
[10/29 10:12:37 visual_prompt]: 	Training 300/1106. train loss: 1.0814,	0.6184 s / batch. (data: 3.31e-04). ETA=18:22:37, max mem: 15.9 GB 
[10/29 10:13:41 visual_prompt]: 	Training 400/1106. train loss: 0.9012,	0.6320 s / batch. (data: 4.02e-03). ETA=18:45:48, max mem: 15.9 GB 
[10/29 10:14:44 visual_prompt]: 	Training 500/1106. train loss: 1.9651,	0.6311 s / batch. (data: 1.25e-02). ETA=18:43:07, max mem: 15.9 GB 
[10/29 10:15:47 visual_prompt]: 	Training 600/1106. train loss: 0.2879,	0.6490 s / batch. (data: 2.64e-02). ETA=19:14:00, max mem: 15.9 GB 
[10/29 10:16:50 visual_prompt]: 	Training 700/1106. train loss: 1.5208,	0.6204 s / batch. (data: 7.71e-04). ETA=18:22:03, max mem: 15.9 GB 
[10/29 10:17:53 visual_prompt]: 	Training 800/1106. train loss: 0.3716,	0.6258 s / batch. (data: 3.17e-04). ETA=18:30:39, max mem: 15.9 GB 
[10/29 10:18:57 visual_prompt]: 	Training 900/1106. train loss: 0.9492,	0.6335 s / batch. (data: 1.21e-03). ETA=18:43:08, max mem: 15.9 GB 
[10/29 10:20:00 visual_prompt]: 	Training 1000/1106. train loss: 1.7833,	0.6383 s / batch. (data: 3.09e-04). ETA=18:50:41, max mem: 15.9 GB 
[10/29 10:21:03 visual_prompt]: 	Training 1100/1106. train loss: 1.5471,	0.6174 s / batch. (data: 1.38e-04). ETA=18:12:35, max mem: 15.9 GB 
[10/29 10:21:07 visual_prompt]: Epoch 4 / 100: avg data time: 4.74e-03, avg batch time: 0.6343, average train loss: 0.8362
[10/29 10:21:57 visual_prompt]: 	Test 100/123. loss: 0.722, 0.2324 s / batch. (data: 4.22e-05)max mem: 15.94594 GB 
[10/29 10:22:07 visual_prompt]: Inference (val):avg data time: 2.33e-04, avg batch time: 0.2322, average loss: 0.6985
[10/29 10:22:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.86	
[10/29 10:22:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/29 10:23:12 visual_prompt]: 	Training 100/1106. train loss: 0.7190,	0.6250 s / batch. (data: 3.20e-04). ETA=18:24:59, max mem: 15.9 GB 
[10/29 10:24:15 visual_prompt]: 	Training 200/1106. train loss: 0.6918,	0.6533 s / batch. (data: 1.10e-02). ETA=19:13:54, max mem: 15.9 GB 
[10/29 10:25:18 visual_prompt]: 	Training 300/1106. train loss: 0.9127,	0.6171 s / batch. (data: 3.16e-04). ETA=18:08:53, max mem: 15.9 GB 
[10/29 10:26:22 visual_prompt]: 	Training 400/1106. train loss: 1.4382,	0.6320 s / batch. (data: 3.01e-04). ETA=18:34:13, max mem: 15.9 GB 
[10/29 10:27:25 visual_prompt]: 	Training 500/1106. train loss: 0.0774,	0.6192 s / batch. (data: 4.17e-04). ETA=18:10:31, max mem: 15.9 GB 
[10/29 10:28:28 visual_prompt]: 	Training 600/1106. train loss: 2.3689,	0.6351 s / batch. (data: 8.18e-04). ETA=18:37:30, max mem: 15.9 GB 
[10/29 10:29:31 visual_prompt]: 	Training 700/1106. train loss: 0.7055,	0.6306 s / batch. (data: 8.54e-04). ETA=18:28:36, max mem: 15.9 GB 
[10/29 10:30:35 visual_prompt]: 	Training 800/1106. train loss: 0.6992,	0.6179 s / batch. (data: 3.03e-04). ETA=18:05:13, max mem: 15.9 GB 
[10/29 10:31:38 visual_prompt]: 	Training 900/1106. train loss: 0.8828,	0.6184 s / batch. (data: 4.30e-04). ETA=18:05:06, max mem: 15.9 GB 
[10/29 10:32:41 visual_prompt]: 	Training 1000/1106. train loss: 0.8958,	0.6178 s / batch. (data: 3.49e-04). ETA=18:02:54, max mem: 15.9 GB 
[10/29 10:33:45 visual_prompt]: 	Training 1100/1106. train loss: 0.6888,	0.6187 s / batch. (data: 2.66e-04). ETA=18:03:28, max mem: 15.9 GB 
[10/29 10:33:49 visual_prompt]: Epoch 5 / 100: avg data time: 4.29e-03, avg batch time: 0.6340, average train loss: 0.9145
[10/29 10:34:39 visual_prompt]: 	Test 100/123. loss: 1.316, 0.2250 s / batch. (data: 4.20e-05)max mem: 15.94594 GB 
[10/29 10:34:49 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2333, average loss: 1.4376
[10/29 10:34:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.33	
[10/29 10:34:49 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/29 10:35:55 visual_prompt]: 	Training 100/1106. train loss: 2.6693,	0.6446 s / batch. (data: 8.55e-04). ETA=18:47:41, max mem: 15.9 GB 
[10/29 10:36:58 visual_prompt]: 	Training 200/1106. train loss: 0.0728,	0.6300 s / batch. (data: 8.11e-04). ETA=18:21:06, max mem: 15.9 GB 
[10/29 10:38:01 visual_prompt]: 	Training 300/1106. train loss: 0.0410,	0.6402 s / batch. (data: 8.32e-04). ETA=18:37:51, max mem: 15.9 GB 
[10/29 10:39:04 visual_prompt]: 	Training 400/1106. train loss: 0.5496,	0.6337 s / batch. (data: 7.89e-04). ETA=18:25:25, max mem: 15.9 GB 
[10/29 10:40:07 visual_prompt]: 	Training 500/1106. train loss: 0.9639,	0.6360 s / batch. (data: 3.24e-04). ETA=18:28:28, max mem: 15.9 GB 
[10/29 10:41:11 visual_prompt]: 	Training 600/1106. train loss: 0.3630,	0.6363 s / batch. (data: 5.98e-03). ETA=18:27:52, max mem: 15.9 GB 
[10/29 10:42:14 visual_prompt]: 	Training 700/1106. train loss: 1.1617,	0.6416 s / batch. (data: 3.42e-04). ETA=18:36:08, max mem: 15.9 GB 
[10/29 10:43:17 visual_prompt]: 	Training 800/1106. train loss: 2.1341,	0.6192 s / batch. (data: 4.14e-04). ETA=17:56:01, max mem: 15.9 GB 
[10/29 10:44:21 visual_prompt]: 	Training 900/1106. train loss: 1.1663,	0.6320 s / batch. (data: 7.10e-04). ETA=18:17:18, max mem: 15.9 GB 
[10/29 10:45:24 visual_prompt]: 	Training 1000/1106. train loss: 1.7726,	0.6430 s / batch. (data: 8.48e-04). ETA=18:35:16, max mem: 15.9 GB 
[10/29 10:46:27 visual_prompt]: 	Training 1100/1106. train loss: 0.0477,	0.6179 s / batch. (data: 1.64e-04). ETA=17:50:38, max mem: 15.9 GB 
[10/29 10:46:31 visual_prompt]: Epoch 6 / 100: avg data time: 4.37e-03, avg batch time: 0.6339, average train loss: 1.0281
[10/29 10:47:21 visual_prompt]: 	Test 100/123. loss: 0.973, 0.2486 s / batch. (data: 4.65e-05)max mem: 15.94594 GB 
[10/29 10:47:31 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.2329, average loss: 1.0499
[10/29 10:47:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.45	
[10/29 10:47:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/29 10:48:36 visual_prompt]: 	Training 100/1106. train loss: 2.3230,	0.6226 s / batch. (data: 3.41e-04). ETA=17:57:42, max mem: 15.9 GB 
[10/29 10:49:40 visual_prompt]: 	Training 200/1106. train loss: 1.6585,	0.6431 s / batch. (data: 8.13e-04). ETA=18:32:07, max mem: 15.9 GB 
[10/29 10:50:43 visual_prompt]: 	Training 300/1106. train loss: 0.5200,	0.6198 s / batch. (data: 3.29e-04). ETA=17:50:52, max mem: 15.9 GB 
[10/29 10:51:46 visual_prompt]: 	Training 400/1106. train loss: 0.8415,	0.6185 s / batch. (data: 3.54e-04). ETA=17:47:32, max mem: 15.9 GB 
[10/29 10:52:49 visual_prompt]: 	Training 500/1106. train loss: 2.2814,	0.6284 s / batch. (data: 3.42e-04). ETA=18:03:41, max mem: 15.9 GB 
[10/29 10:53:53 visual_prompt]: 	Training 600/1106. train loss: 2.8563,	0.6228 s / batch. (data: 3.12e-04). ETA=17:52:59, max mem: 15.9 GB 
[10/29 10:54:56 visual_prompt]: 	Training 700/1106. train loss: 3.2390,	0.6480 s / batch. (data: 1.20e-02). ETA=18:35:11, max mem: 15.9 GB 
[10/29 10:55:59 visual_prompt]: 	Training 800/1106. train loss: 0.6635,	0.6314 s / batch. (data: 7.95e-04). ETA=18:05:41, max mem: 15.9 GB 
[10/29 10:57:03 visual_prompt]: 	Training 900/1106. train loss: 0.6575,	0.6232 s / batch. (data: 2.97e-04). ETA=17:50:27, max mem: 15.9 GB 
[10/29 10:58:06 visual_prompt]: 	Training 1000/1106. train loss: 0.7007,	0.6272 s / batch. (data: 2.88e-04). ETA=17:56:21, max mem: 15.9 GB 
[10/29 10:59:09 visual_prompt]: 	Training 1100/1106. train loss: 0.8498,	0.6194 s / batch. (data: 1.49e-04). ETA=17:41:53, max mem: 15.9 GB 
[10/29 10:59:13 visual_prompt]: Epoch 7 / 100: avg data time: 4.11e-03, avg batch time: 0.6344, average train loss: 1.1819
[10/29 11:00:04 visual_prompt]: 	Test 100/123. loss: 0.971, 0.2326 s / batch. (data: 4.79e-05)max mem: 15.94594 GB 
[10/29 11:00:14 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2326, average loss: 0.8967
[10/29 11:00:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.50	
[10/29 11:00:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/29 11:01:19 visual_prompt]: 	Training 100/1106. train loss: 1.3993,	0.6534 s / batch. (data: 1.18e-03). ETA=18:39:02, max mem: 15.9 GB 
[10/29 11:02:22 visual_prompt]: 	Training 200/1106. train loss: 0.9468,	0.6284 s / batch. (data: 8.31e-04). ETA=17:55:13, max mem: 15.9 GB 
[10/29 11:03:25 visual_prompt]: 	Training 300/1106. train loss: 0.0627,	0.6477 s / batch. (data: 8.09e-04). ETA=18:27:06, max mem: 15.9 GB 
[10/29 11:04:29 visual_prompt]: 	Training 400/1106. train loss: 1.1951,	0.6181 s / batch. (data: 2.49e-04). ETA=17:35:25, max mem: 15.9 GB 
[10/29 11:05:32 visual_prompt]: 	Training 500/1106. train loss: 0.0658,	0.6477 s / batch. (data: 9.58e-04). ETA=18:24:57, max mem: 15.9 GB 
[10/29 11:06:35 visual_prompt]: 	Training 600/1106. train loss: 0.1146,	0.6199 s / batch. (data: 3.30e-04). ETA=17:36:29, max mem: 15.9 GB 
[10/29 11:07:39 visual_prompt]: 	Training 700/1106. train loss: 0.8911,	0.6332 s / batch. (data: 3.23e-04). ETA=17:58:05, max mem: 15.9 GB 
[10/29 11:08:42 visual_prompt]: 	Training 800/1106. train loss: 1.0544,	0.6182 s / batch. (data: 3.06e-04). ETA=17:31:36, max mem: 15.9 GB 
[10/29 11:09:45 visual_prompt]: 	Training 900/1106. train loss: 0.1290,	0.6296 s / batch. (data: 7.56e-04). ETA=17:49:53, max mem: 15.9 GB 
[10/29 11:10:48 visual_prompt]: 	Training 1000/1106. train loss: 0.8462,	0.6291 s / batch. (data: 3.20e-04). ETA=17:48:01, max mem: 15.9 GB 
[10/29 11:11:51 visual_prompt]: 	Training 1100/1106. train loss: 1.4405,	0.6165 s / batch. (data: 1.67e-04). ETA=17:25:32, max mem: 15.9 GB 
[10/29 11:11:55 visual_prompt]: Epoch 8 / 100: avg data time: 4.12e-03, avg batch time: 0.6339, average train loss: 1.1477
[10/29 11:12:45 visual_prompt]: 	Test 100/123. loss: 0.770, 0.2478 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/29 11:12:56 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2320, average loss: 0.7314
[10/29 11:12:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.89	
[10/29 11:12:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/29 11:14:01 visual_prompt]: 	Training 100/1106. train loss: 3.8251,	0.6446 s / batch. (data: 2.18e-02). ETA=18:12:00, max mem: 15.9 GB 
[10/29 11:15:04 visual_prompt]: 	Training 200/1106. train loss: 0.0707,	0.6377 s / batch. (data: 7.94e-04). ETA=17:59:19, max mem: 15.9 GB 
[10/29 11:16:08 visual_prompt]: 	Training 300/1106. train loss: 0.8604,	0.6312 s / batch. (data: 8.81e-04). ETA=17:47:20, max mem: 15.9 GB 
[10/29 11:17:11 visual_prompt]: 	Training 400/1106. train loss: 0.6575,	0.6166 s / batch. (data: 2.89e-04). ETA=17:21:37, max mem: 15.9 GB 
[10/29 11:18:14 visual_prompt]: 	Training 500/1106. train loss: 3.6022,	0.6201 s / batch. (data: 3.17e-04). ETA=17:26:24, max mem: 15.9 GB 
[10/29 11:19:17 visual_prompt]: 	Training 600/1106. train loss: 0.7383,	0.6421 s / batch. (data: 1.10e-02). ETA=18:02:27, max mem: 15.9 GB 
[10/29 11:20:21 visual_prompt]: 	Training 700/1106. train loss: 0.1987,	0.6443 s / batch. (data: 7.96e-04). ETA=18:05:11, max mem: 15.9 GB 
[10/29 11:21:24 visual_prompt]: 	Training 800/1106. train loss: 0.6970,	0.6185 s / batch. (data: 2.97e-04). ETA=17:20:40, max mem: 15.9 GB 
[10/29 11:22:28 visual_prompt]: 	Training 900/1106. train loss: 0.1299,	0.6644 s / batch. (data: 5.79e-03). ETA=18:36:41, max mem: 15.9 GB 
[10/29 11:23:31 visual_prompt]: 	Training 1000/1106. train loss: 0.3564,	0.6407 s / batch. (data: 3.37e-04). ETA=17:55:55, max mem: 15.9 GB 
[10/29 11:24:34 visual_prompt]: 	Training 1100/1106. train loss: 0.7050,	0.6180 s / batch. (data: 1.85e-04). ETA=17:16:37, max mem: 15.9 GB 
[10/29 11:24:38 visual_prompt]: Epoch 9 / 100: avg data time: 4.60e-03, avg batch time: 0.6350, average train loss: 1.0831
[10/29 11:25:28 visual_prompt]: 	Test 100/123. loss: 0.712, 0.2246 s / batch. (data: 4.03e-05)max mem: 15.94594 GB 
[10/29 11:25:39 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.2318, average loss: 0.6928
[10/29 11:25:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[10/29 11:25:39 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/29 11:26:44 visual_prompt]: 	Training 100/1106. train loss: 0.7556,	0.6171 s / batch. (data: 3.12e-04). ETA=17:14:02, max mem: 15.9 GB 
[10/29 11:27:47 visual_prompt]: 	Training 200/1106. train loss: 1.0793,	0.6373 s / batch. (data: 5.89e-03). ETA=17:46:57, max mem: 15.9 GB 
[10/29 11:28:50 visual_prompt]: 	Training 300/1106. train loss: 0.7706,	0.6233 s / batch. (data: 5.45e-03). ETA=17:22:22, max mem: 15.9 GB 
[10/29 11:29:53 visual_prompt]: 	Training 400/1106. train loss: 0.0161,	0.6195 s / batch. (data: 3.31e-04). ETA=17:15:06, max mem: 15.9 GB 
[10/29 11:30:56 visual_prompt]: 	Training 500/1106. train loss: 0.7959,	0.6376 s / batch. (data: 7.86e-04). ETA=17:44:10, max mem: 15.9 GB 
[10/29 11:32:00 visual_prompt]: 	Training 600/1106. train loss: 5.1464,	0.6184 s / batch. (data: 3.49e-04). ETA=17:11:08, max mem: 15.9 GB 
[10/29 11:33:03 visual_prompt]: 	Training 700/1106. train loss: 3.5525,	0.6339 s / batch. (data: 7.85e-04). ETA=17:35:59, max mem: 15.9 GB 
[10/29 11:34:06 visual_prompt]: 	Training 800/1106. train loss: 1.0717,	0.6440 s / batch. (data: 7.89e-04). ETA=17:51:38, max mem: 15.9 GB 
[10/29 11:35:09 visual_prompt]: 	Training 900/1106. train loss: 1.4419,	0.6290 s / batch. (data: 7.78e-04). ETA=17:25:39, max mem: 15.9 GB 
[10/29 11:36:13 visual_prompt]: 	Training 1000/1106. train loss: 0.0139,	0.6175 s / batch. (data: 3.13e-04). ETA=17:05:35, max mem: 15.9 GB 
[10/29 11:37:16 visual_prompt]: 	Training 1100/1106. train loss: 0.6967,	0.6182 s / batch. (data: 1.41e-04). ETA=17:05:40, max mem: 15.9 GB 
[10/29 11:37:20 visual_prompt]: Epoch 10 / 100: avg data time: 3.99e-03, avg batch time: 0.6338, average train loss: 1.2616
[10/29 11:38:09 visual_prompt]: 	Test 100/123. loss: 0.994, 0.2398 s / batch. (data: 2.55e-05)max mem: 15.94594 GB 
[10/29 11:38:20 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2316, average loss: 0.9132
[10/29 11:38:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.08	
[10/29 11:38:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/29 11:39:25 visual_prompt]: 	Training 100/1106. train loss: 1.2023,	0.6263 s / batch. (data: 3.30e-04). ETA=17:18:03, max mem: 15.9 GB 
[10/29 11:40:29 visual_prompt]: 	Training 200/1106. train loss: 1.2817,	0.6450 s / batch. (data: 8.58e-04). ETA=17:47:51, max mem: 15.9 GB 
[10/29 11:41:32 visual_prompt]: 	Training 300/1106. train loss: 1.4491,	0.6190 s / batch. (data: 3.45e-04). ETA=17:03:46, max mem: 15.9 GB 
[10/29 11:42:35 visual_prompt]: 	Training 400/1106. train loss: 0.3824,	0.6331 s / batch. (data: 8.17e-04). ETA=17:26:02, max mem: 15.9 GB 
[10/29 11:43:38 visual_prompt]: 	Training 500/1106. train loss: 0.8116,	0.6472 s / batch. (data: 7.33e-04). ETA=17:48:17, max mem: 15.9 GB 
[10/29 11:44:42 visual_prompt]: 	Training 600/1106. train loss: 0.0069,	0.6313 s / batch. (data: 8.04e-04). ETA=17:21:03, max mem: 15.9 GB 
[10/29 11:45:45 visual_prompt]: 	Training 700/1106. train loss: 1.2342,	0.6244 s / batch. (data: 2.75e-04). ETA=17:08:39, max mem: 15.9 GB 
[10/29 11:46:48 visual_prompt]: 	Training 800/1106. train loss: 2.0458,	0.6510 s / batch. (data: 8.06e-04). ETA=17:51:16, max mem: 15.9 GB 
[10/29 11:47:51 visual_prompt]: 	Training 900/1106. train loss: 0.1646,	0.6300 s / batch. (data: 3.43e-04). ETA=17:15:45, max mem: 15.9 GB 
[10/29 11:48:54 visual_prompt]: 	Training 1000/1106. train loss: 0.0361,	0.6486 s / batch. (data: 7.85e-04). ETA=17:45:12, max mem: 15.9 GB 
[10/29 11:49:58 visual_prompt]: 	Training 1100/1106. train loss: 0.7257,	0.6173 s / batch. (data: 1.51e-04). ETA=16:52:43, max mem: 15.9 GB 
[10/29 11:50:01 visual_prompt]: Epoch 11 / 100: avg data time: 4.58e-03, avg batch time: 0.6341, average train loss: 1.4021
[10/29 11:50:52 visual_prompt]: 	Test 100/123. loss: 1.013, 0.2248 s / batch. (data: 2.72e-05)max mem: 15.94594 GB 
[10/29 11:51:02 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2320, average loss: 0.9301
[10/29 11:51:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.80	
[10/29 11:51:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/29 11:52:08 visual_prompt]: 	Training 100/1106. train loss: 0.8278,	0.6424 s / batch. (data: 8.42e-04). ETA=17:32:48, max mem: 15.9 GB 
[10/29 11:53:11 visual_prompt]: 	Training 200/1106. train loss: 0.7437,	0.6344 s / batch. (data: 3.49e-04). ETA=17:18:35, max mem: 15.9 GB 
[10/29 11:54:14 visual_prompt]: 	Training 300/1106. train loss: 5.0324,	0.6423 s / batch. (data: 7.89e-04). ETA=17:30:27, max mem: 15.9 GB 
[10/29 11:55:18 visual_prompt]: 	Training 400/1106. train loss: 0.1190,	0.6172 s / batch. (data: 3.36e-04). ETA=16:48:28, max mem: 15.9 GB 
[10/29 11:56:21 visual_prompt]: 	Training 500/1106. train loss: 1.7773,	0.6309 s / batch. (data: 8.14e-04). ETA=17:09:43, max mem: 15.9 GB 
[10/29 11:57:24 visual_prompt]: 	Training 600/1106. train loss: 0.1048,	0.6320 s / batch. (data: 3.05e-04). ETA=17:10:32, max mem: 15.9 GB 
[10/29 11:58:28 visual_prompt]: 	Training 700/1106. train loss: 0.7346,	0.6188 s / batch. (data: 8.17e-04). ETA=16:47:56, max mem: 15.9 GB 
[10/29 11:59:31 visual_prompt]: 	Training 800/1106. train loss: 1.6298,	0.6470 s / batch. (data: 7.66e-04). ETA=17:32:44, max mem: 15.9 GB 
[10/29 12:00:34 visual_prompt]: 	Training 900/1106. train loss: 0.8107,	0.6402 s / batch. (data: 8.24e-04). ETA=17:20:41, max mem: 15.9 GB 
[10/29 12:01:37 visual_prompt]: 	Training 1000/1106. train loss: 1.1408,	0.6190 s / batch. (data: 3.12e-04). ETA=16:45:14, max mem: 15.9 GB 
[10/29 12:02:40 visual_prompt]: 	Training 1100/1106. train loss: 3.3146,	0.6178 s / batch. (data: 1.74e-04). ETA=16:42:15, max mem: 15.9 GB 
[10/29 12:02:44 visual_prompt]: Epoch 12 / 100: avg data time: 5.09e-03, avg batch time: 0.6349, average train loss: 1.4306
[10/29 12:03:34 visual_prompt]: 	Test 100/123. loss: 1.050, 0.2253 s / batch. (data: 3.58e-05)max mem: 15.94594 GB 
[10/29 12:03:45 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2321, average loss: 0.9594
[10/29 12:03:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.56	
[10/29 12:03:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/29 12:04:50 visual_prompt]: 	Training 100/1106. train loss: 0.5599,	0.6357 s / batch. (data: 8.20e-04). ETA=17:10:12, max mem: 15.9 GB 
[10/29 12:05:54 visual_prompt]: 	Training 200/1106. train loss: 1.9218,	0.6295 s / batch. (data: 2.59e-04). ETA=16:59:02, max mem: 15.9 GB 
[10/29 12:06:57 visual_prompt]: 	Training 300/1106. train loss: 0.1034,	0.6180 s / batch. (data: 2.98e-04). ETA=16:39:18, max mem: 15.9 GB 
[10/29 12:08:00 visual_prompt]: 	Training 400/1106. train loss: 1.2473,	0.6423 s / batch. (data: 7.95e-04). ETA=17:17:39, max mem: 15.9 GB 
[10/29 12:09:03 visual_prompt]: 	Training 500/1106. train loss: 0.0329,	0.6321 s / batch. (data: 7.95e-04). ETA=17:00:07, max mem: 15.9 GB 
[10/29 12:10:07 visual_prompt]: 	Training 600/1106. train loss: 0.8404,	0.6210 s / batch. (data: 3.01e-04). ETA=16:41:11, max mem: 15.9 GB 
[10/29 12:11:10 visual_prompt]: 	Training 700/1106. train loss: 0.2899,	0.6450 s / batch. (data: 1.29e-02). ETA=17:18:41, max mem: 15.9 GB 
[10/29 12:12:13 visual_prompt]: 	Training 800/1106. train loss: 1.0143,	0.6192 s / batch. (data: 3.21e-04). ETA=16:36:09, max mem: 15.9 GB 
[10/29 12:13:16 visual_prompt]: 	Training 900/1106. train loss: 0.5268,	0.6406 s / batch. (data: 7.70e-04). ETA=17:09:34, max mem: 15.9 GB 
[10/29 12:14:20 visual_prompt]: 	Training 1000/1106. train loss: 0.2917,	0.6177 s / batch. (data: 3.28e-04). ETA=16:31:41, max mem: 15.9 GB 
[10/29 12:15:23 visual_prompt]: 	Training 1100/1106. train loss: 2.8810,	0.6174 s / batch. (data: 1.80e-04). ETA=16:30:08, max mem: 15.9 GB 
[10/29 12:15:27 visual_prompt]: Epoch 13 / 100: avg data time: 4.54e-03, avg batch time: 0.6344, average train loss: 1.3475
[10/29 12:16:16 visual_prompt]: 	Test 100/123. loss: 0.705, 0.2440 s / batch. (data: 3.98e-05)max mem: 15.94594 GB 
[10/29 12:16:27 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.2318, average loss: 0.6897
[10/29 12:16:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.36	
[10/29 12:16:27 visual_prompt]: Best epoch 13: best metric: -0.690
[10/29 12:16:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/29 12:17:33 visual_prompt]: 	Training 100/1106. train loss: 0.4413,	0.6307 s / batch. (data: 3.18e-04). ETA=16:50:27, max mem: 15.9 GB 
[10/29 12:18:36 visual_prompt]: 	Training 200/1106. train loss: 0.8085,	0.6175 s / batch. (data: 3.35e-04). ETA=16:28:15, max mem: 15.9 GB 
[10/29 12:19:39 visual_prompt]: 	Training 300/1106. train loss: 0.9643,	0.6504 s / batch. (data: 1.45e-02). ETA=17:19:49, max mem: 15.9 GB 
[10/29 12:20:42 visual_prompt]: 	Training 400/1106. train loss: 0.0081,	0.6330 s / batch. (data: 8.15e-04). ETA=16:50:52, max mem: 15.9 GB 
[10/29 12:21:46 visual_prompt]: 	Training 500/1106. train loss: 1.3572,	0.6256 s / batch. (data: 4.78e-04). ETA=16:38:06, max mem: 15.9 GB 
[10/29 12:22:49 visual_prompt]: 	Training 600/1106. train loss: 0.9365,	0.6448 s / batch. (data: 7.95e-04). ETA=17:07:38, max mem: 15.9 GB 
[10/29 12:23:52 visual_prompt]: 	Training 700/1106. train loss: 1.6951,	0.6190 s / batch. (data: 3.47e-04). ETA=16:25:29, max mem: 15.9 GB 
[10/29 12:24:55 visual_prompt]: 	Training 800/1106. train loss: 1.8280,	0.6172 s / batch. (data: 3.02e-04). ETA=16:21:31, max mem: 15.9 GB 
[10/29 12:25:58 visual_prompt]: 	Training 900/1106. train loss: 0.7577,	0.6437 s / batch. (data: 5.96e-03). ETA=17:02:40, max mem: 15.9 GB 
[10/29 12:27:01 visual_prompt]: 	Training 1000/1106. train loss: 1.4427,	0.6306 s / batch. (data: 2.93e-04). ETA=16:40:50, max mem: 15.9 GB 
[10/29 12:28:04 visual_prompt]: 	Training 1100/1106. train loss: 0.7677,	0.6185 s / batch. (data: 1.39e-04). ETA=16:20:37, max mem: 15.9 GB 
[10/29 12:28:08 visual_prompt]: Epoch 14 / 100: avg data time: 4.05e-03, avg batch time: 0.6339, average train loss: 1.2378
[10/29 12:28:58 visual_prompt]: 	Test 100/123. loss: 1.367, 0.2249 s / batch. (data: 3.31e-05)max mem: 15.94594 GB 
[10/29 12:29:09 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2320, average loss: 1.2401
[10/29 12:29:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.94	
[10/29 12:29:09 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/29 12:30:14 visual_prompt]: 	Training 100/1106. train loss: 4.3615,	0.6294 s / batch. (data: 1.28e-02). ETA=16:36:38, max mem: 15.9 GB 
[10/29 12:31:17 visual_prompt]: 	Training 200/1106. train loss: 1.4409,	0.6289 s / batch. (data: 2.68e-04). ETA=16:34:48, max mem: 15.9 GB 
[10/29 12:32:20 visual_prompt]: 	Training 300/1106. train loss: 1.4405,	0.6388 s / batch. (data: 8.00e-04). ETA=16:49:32, max mem: 15.9 GB 
[10/29 12:33:23 visual_prompt]: 	Training 400/1106. train loss: 2.5751,	0.6301 s / batch. (data: 3.09e-04). ETA=16:34:41, max mem: 15.9 GB 
[10/29 12:34:27 visual_prompt]: 	Training 500/1106. train loss: 0.3073,	0.6327 s / batch. (data: 8.13e-04). ETA=16:37:41, max mem: 15.9 GB 
[10/29 12:35:30 visual_prompt]: 	Training 600/1106. train loss: 1.2194,	0.6188 s / batch. (data: 3.08e-04). ETA=16:14:45, max mem: 15.9 GB 
[10/29 12:36:33 visual_prompt]: 	Training 700/1106. train loss: 1.0511,	0.6264 s / batch. (data: 2.60e-04). ETA=16:25:37, max mem: 15.9 GB 
[10/29 12:37:36 visual_prompt]: 	Training 800/1106. train loss: 0.8106,	0.6442 s / batch. (data: 7.87e-04). ETA=16:52:38, max mem: 15.9 GB 
[10/29 12:38:40 visual_prompt]: 	Training 900/1106. train loss: 2.6714,	0.6234 s / batch. (data: 7.70e-04). ETA=16:18:53, max mem: 15.9 GB 
[10/29 12:39:43 visual_prompt]: 	Training 1000/1106. train loss: 0.9161,	0.6192 s / batch. (data: 3.32e-04). ETA=16:11:12, max mem: 15.9 GB 
[10/29 12:40:46 visual_prompt]: 	Training 1100/1106. train loss: 1.8073,	0.6182 s / batch. (data: 1.66e-04). ETA=16:08:44, max mem: 15.9 GB 
[10/29 12:40:50 visual_prompt]: Epoch 15 / 100: avg data time: 4.16e-03, avg batch time: 0.6338, average train loss: 1.5857
[10/29 12:41:39 visual_prompt]: 	Test 100/123. loss: 1.962, 0.2357 s / batch. (data: 2.77e-05)max mem: 15.94594 GB 
[10/29 12:41:50 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.2320, average loss: 1.7709
[10/29 12:41:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.46	
[10/29 12:41:50 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/29 12:42:55 visual_prompt]: 	Training 100/1106. train loss: 0.0369,	0.6316 s / batch. (data: 7.60e-04). ETA=16:28:37, max mem: 15.9 GB 
[10/29 12:43:58 visual_prompt]: 	Training 200/1106. train loss: 3.0288,	0.6490 s / batch. (data: 8.58e-04). ETA=16:54:44, max mem: 15.9 GB 
[10/29 12:45:01 visual_prompt]: 	Training 300/1106. train loss: 0.8694,	0.6324 s / batch. (data: 8.18e-04). ETA=16:27:38, max mem: 15.9 GB 
[10/29 12:46:05 visual_prompt]: 	Training 400/1106. train loss: 2.2160,	0.6336 s / batch. (data: 8.27e-04). ETA=16:28:28, max mem: 15.9 GB 
[10/29 12:47:08 visual_prompt]: 	Training 500/1106. train loss: 0.6932,	0.6580 s / batch. (data: 4.10e-02). ETA=17:05:30, max mem: 15.9 GB 
[10/29 12:48:11 visual_prompt]: 	Training 600/1106. train loss: 0.3550,	0.6328 s / batch. (data: 8.29e-04). ETA=16:25:09, max mem: 15.9 GB 
[10/29 12:49:14 visual_prompt]: 	Training 700/1106. train loss: 1.0048,	0.6283 s / batch. (data: 3.25e-04). ETA=16:17:04, max mem: 15.9 GB 
[10/29 12:50:18 visual_prompt]: 	Training 800/1106. train loss: 1.1993,	0.6179 s / batch. (data: 4.19e-04). ETA=15:59:57, max mem: 15.9 GB 
[10/29 12:51:21 visual_prompt]: 	Training 900/1106. train loss: 1.3858,	0.6329 s / batch. (data: 8.24e-04). ETA=16:22:04, max mem: 15.9 GB 
[10/29 12:52:24 visual_prompt]: 	Training 1000/1106. train loss: 1.8299,	0.6454 s / batch. (data: 7.81e-04). ETA=16:40:24, max mem: 15.9 GB 
[10/29 12:53:27 visual_prompt]: 	Training 1100/1106. train loss: 1.1494,	0.6176 s / batch. (data: 1.55e-04). ETA=15:56:25, max mem: 15.9 GB 
[10/29 12:53:31 visual_prompt]: Epoch 16 / 100: avg data time: 4.12e-03, avg batch time: 0.6337, average train loss: 1.3050
[10/29 12:54:21 visual_prompt]: 	Test 100/123. loss: 0.923, 0.2400 s / batch. (data: 3.58e-05)max mem: 15.94594 GB 
[10/29 12:54:32 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.2318, average loss: 0.8533
[10/29 12:54:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.61	
[10/29 12:54:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/29 12:55:37 visual_prompt]: 	Training 100/1106. train loss: 0.0967,	0.6377 s / batch. (data: 1.10e-02). ETA=16:26:22, max mem: 15.9 GB 
[10/29 12:56:40 visual_prompt]: 	Training 200/1106. train loss: 0.9997,	0.6444 s / batch. (data: 7.56e-04). ETA=16:35:39, max mem: 15.9 GB 
[10/29 12:57:44 visual_prompt]: 	Training 300/1106. train loss: 2.1897,	0.6188 s / batch. (data: 2.91e-04). ETA=15:55:07, max mem: 15.9 GB 
[10/29 12:58:47 visual_prompt]: 	Training 400/1106. train loss: 0.3395,	0.6440 s / batch. (data: 7.80e-04). ETA=16:32:54, max mem: 15.9 GB 
[10/29 12:59:51 visual_prompt]: 	Training 500/1106. train loss: 0.9139,	0.6329 s / batch. (data: 3.22e-04). ETA=16:14:42, max mem: 15.9 GB 
[10/29 13:00:54 visual_prompt]: 	Training 600/1106. train loss: 1.1051,	0.6198 s / batch. (data: 1.40e-03). ETA=15:53:28, max mem: 15.9 GB 
[10/29 13:01:57 visual_prompt]: 	Training 700/1106. train loss: 1.0791,	0.6197 s / batch. (data: 2.92e-04). ETA=15:52:18, max mem: 15.9 GB 
[10/29 13:03:00 visual_prompt]: 	Training 800/1106. train loss: 2.3888,	0.6238 s / batch. (data: 3.36e-04). ETA=15:57:37, max mem: 15.9 GB 
[10/29 13:04:03 visual_prompt]: 	Training 900/1106. train loss: 0.1850,	0.6359 s / batch. (data: 7.49e-04). ETA=16:15:03, max mem: 15.9 GB 
[10/29 13:05:06 visual_prompt]: 	Training 1000/1106. train loss: 2.2087,	0.6316 s / batch. (data: 3.23e-04). ETA=16:07:22, max mem: 15.9 GB 
[10/29 13:06:10 visual_prompt]: 	Training 1100/1106. train loss: 0.0021,	0.6184 s / batch. (data: 1.54e-04). ETA=15:46:08, max mem: 15.9 GB 
[10/29 13:06:13 visual_prompt]: Epoch 17 / 100: avg data time: 4.33e-03, avg batch time: 0.6346, average train loss: 1.3903
[10/29 13:07:03 visual_prompt]: 	Test 100/123. loss: 2.332, 0.2403 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/29 13:07:14 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2325, average loss: 2.5607
[10/29 13:07:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.59	
[10/29 13:07:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/29 13:08:19 visual_prompt]: 	Training 100/1106. train loss: 0.0029,	0.6179 s / batch. (data: 3.21e-04). ETA=15:44:19, max mem: 15.9 GB 
[10/29 13:09:22 visual_prompt]: 	Training 200/1106. train loss: 5.1135,	0.6240 s / batch. (data: 2.99e-04). ETA=15:52:37, max mem: 15.9 GB 
[10/29 13:10:26 visual_prompt]: 	Training 300/1106. train loss: 11.7833,	0.6394 s / batch. (data: 1.21e-02). ETA=16:15:00, max mem: 15.9 GB 
[10/29 13:11:29 visual_prompt]: 	Training 400/1106. train loss: 2.6270,	0.6248 s / batch. (data: 3.29e-04). ETA=15:51:44, max mem: 15.9 GB 
[10/29 13:12:32 visual_prompt]: 	Training 500/1106. train loss: 0.0289,	0.6304 s / batch. (data: 8.08e-04). ETA=15:59:12, max mem: 15.9 GB 
[10/29 13:13:36 visual_prompt]: 	Training 600/1106. train loss: 0.8503,	0.6191 s / batch. (data: 3.27e-04). ETA=15:40:57, max mem: 15.9 GB 
[10/29 13:14:39 visual_prompt]: 	Training 700/1106. train loss: 0.3961,	0.6568 s / batch. (data: 4.09e-02). ETA=16:37:15, max mem: 15.9 GB 
[10/29 13:15:42 visual_prompt]: 	Training 800/1106. train loss: 0.7860,	0.6436 s / batch. (data: 8.13e-04). ETA=16:16:09, max mem: 15.9 GB 
[10/29 13:16:45 visual_prompt]: 	Training 900/1106. train loss: 1.6112,	0.6437 s / batch. (data: 5.88e-03). ETA=16:15:15, max mem: 15.9 GB 
[10/29 13:17:48 visual_prompt]: 	Training 1000/1106. train loss: 1.1432,	0.6242 s / batch. (data: 3.25e-04). ETA=15:44:35, max mem: 15.9 GB 
[10/29 13:18:52 visual_prompt]: 	Training 1100/1106. train loss: 11.2246,	0.6182 s / batch. (data: 1.71e-04). ETA=15:34:25, max mem: 15.9 GB 
[10/29 13:18:56 visual_prompt]: Epoch 18 / 100: avg data time: 4.34e-03, avg batch time: 0.6344, average train loss: 1.6590
[10/29 13:19:46 visual_prompt]: 	Test 100/123. loss: 13.989, 0.2435 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[10/29 13:19:56 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2318, average loss: 14.7164
[10/29 13:19:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.73	
[10/29 13:19:56 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/29 13:21:02 visual_prompt]: 	Training 100/1106. train loss: 2.5665,	0.6171 s / batch. (data: 3.05e-04). ETA=15:31:39, max mem: 15.9 GB 
[10/29 13:22:05 visual_prompt]: 	Training 200/1106. train loss: 1.5364,	0.6438 s / batch. (data: 7.80e-04). ETA=16:10:57, max mem: 15.9 GB 
[10/29 13:23:09 visual_prompt]: 	Training 300/1106. train loss: 0.1445,	0.6224 s / batch. (data: 2.96e-04). ETA=15:37:35, max mem: 15.9 GB 
[10/29 13:24:12 visual_prompt]: 	Training 400/1106. train loss: 0.6817,	0.6183 s / batch. (data: 3.19e-04). ETA=15:30:30, max mem: 15.9 GB 
[10/29 13:25:15 visual_prompt]: 	Training 500/1106. train loss: 0.9153,	0.6569 s / batch. (data: 5.93e-03). ETA=16:27:25, max mem: 15.9 GB 
[10/29 13:26:18 visual_prompt]: 	Training 600/1106. train loss: 0.7520,	0.6176 s / batch. (data: 3.22e-04). ETA=15:27:21, max mem: 15.9 GB 
[10/29 13:27:22 visual_prompt]: 	Training 700/1106. train loss: 0.7331,	0.6190 s / batch. (data: 3.29e-04). ETA=15:28:26, max mem: 15.9 GB 
[10/29 13:28:25 visual_prompt]: 	Training 800/1106. train loss: 0.0175,	0.6395 s / batch. (data: 8.12e-04). ETA=15:58:09, max mem: 15.9 GB 
[10/29 13:29:28 visual_prompt]: 	Training 900/1106. train loss: 0.6107,	0.6321 s / batch. (data: 8.28e-04). ETA=15:45:53, max mem: 15.9 GB 
[10/29 13:30:31 visual_prompt]: 	Training 1000/1106. train loss: 0.0163,	0.6353 s / batch. (data: 8.09e-04). ETA=15:49:41, max mem: 15.9 GB 
[10/29 13:31:34 visual_prompt]: 	Training 1100/1106. train loss: 0.0456,	0.6225 s / batch. (data: 1.70e-04). ETA=15:29:27, max mem: 15.9 GB 
[10/29 13:31:38 visual_prompt]: Epoch 19 / 100: avg data time: 4.37e-03, avg batch time: 0.6345, average train loss: 1.4310
[10/29 13:32:28 visual_prompt]: 	Test 100/123. loss: 2.042, 0.2307 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[10/29 13:32:39 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2327, average loss: 1.8310
[10/29 13:32:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.80	
[10/29 13:32:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/29 13:33:44 visual_prompt]: 	Training 100/1106. train loss: 0.7191,	0.6313 s / batch. (data: 7.72e-04). ETA=15:41:30, max mem: 15.9 GB 
[10/29 13:34:47 visual_prompt]: 	Training 200/1106. train loss: 0.8069,	0.6233 s / batch. (data: 5.65e-03). ETA=15:28:35, max mem: 15.9 GB 
[10/29 13:35:50 visual_prompt]: 	Training 300/1106. train loss: 1.4251,	0.6394 s / batch. (data: 4.73e-04). ETA=15:51:28, max mem: 15.9 GB 
[10/29 13:36:54 visual_prompt]: 	Training 400/1106. train loss: 1.4661,	0.6318 s / batch. (data: 3.25e-04). ETA=15:39:04, max mem: 15.9 GB 
[10/29 13:37:57 visual_prompt]: 	Training 500/1106. train loss: 0.7600,	0.6493 s / batch. (data: 5.92e-03). ETA=16:04:05, max mem: 15.9 GB 
[10/29 13:39:00 visual_prompt]: 	Training 600/1106. train loss: 1.1529,	0.6445 s / batch. (data: 1.77e-02). ETA=15:55:47, max mem: 15.9 GB 
[10/29 13:40:03 visual_prompt]: 	Training 700/1106. train loss: 1.3264,	0.6426 s / batch. (data: 8.10e-04). ETA=15:51:54, max mem: 15.9 GB 
[10/29 13:41:06 visual_prompt]: 	Training 800/1106. train loss: 0.0326,	0.6313 s / batch. (data: 7.96e-04). ETA=15:34:07, max mem: 15.9 GB 
[10/29 13:42:10 visual_prompt]: 	Training 900/1106. train loss: 2.9376,	0.6177 s / batch. (data: 3.17e-04). ETA=15:12:57, max mem: 15.9 GB 
[10/29 13:43:13 visual_prompt]: 	Training 1000/1106. train loss: 1.8561,	0.6249 s / batch. (data: 3.06e-04). ETA=15:22:38, max mem: 15.9 GB 
[10/29 13:44:16 visual_prompt]: 	Training 1100/1106. train loss: 0.8755,	0.6177 s / batch. (data: 2.03e-04). ETA=15:11:00, max mem: 15.9 GB 
[10/29 13:44:20 visual_prompt]: Epoch 20 / 100: avg data time: 4.47e-03, avg batch time: 0.6339, average train loss: 1.2778
[10/29 13:45:09 visual_prompt]: 	Test 100/123. loss: 0.753, 0.2317 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/29 13:45:20 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2329, average loss: 0.7179
[10/29 13:45:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.59	
[10/29 13:45:20 visual_prompt]: Stopping early.
[10/29 13:45:20 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 13:45:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 13:45:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 13:45:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 13:45:20 visual_prompt]: Training with config:
[10/29 13:45:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 13:45:20 visual_prompt]: Loading training data...
[10/29 13:45:20 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 13:45:20 visual_prompt]: Loading validation data...
[10/29 13:45:20 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 13:45:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/29 13:45:23 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/29 13:45:23 visual_prompt]: tuned percent:0.522
[10/29 13:45:23 visual_prompt]: Device used for model: 0
[10/29 13:45:23 visual_prompt]: Setting up Evaluator...
[10/29 13:45:23 visual_prompt]: Setting up Trainer...
[10/29 13:45:23 visual_prompt]: 	Setting up the optimizer...
[10/29 13:45:23 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 13:46:28 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6396 s / batch. (data: 5.95e-03). ETA=19:37:56, max mem: 15.9 GB 
[10/29 13:47:32 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6410 s / batch. (data: 7.26e-04). ETA=19:39:21, max mem: 15.9 GB 
[10/29 13:48:35 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6306 s / batch. (data: 8.18e-04). ETA=19:19:12, max mem: 15.9 GB 
[10/29 13:49:38 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6180 s / batch. (data: 3.37e-04). ETA=18:55:03, max mem: 15.9 GB 
[10/29 13:50:42 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6360 s / batch. (data: 2.83e-04). ETA=19:27:04, max mem: 15.9 GB 
[10/29 13:51:45 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6175 s / batch. (data: 2.76e-04). ETA=18:52:05, max mem: 15.9 GB 
[10/29 13:52:48 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6182 s / batch. (data: 3.21e-04). ETA=18:52:19, max mem: 15.9 GB 
[10/29 13:53:51 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6399 s / batch. (data: 8.07e-04). ETA=19:31:01, max mem: 15.9 GB 
[10/29 13:54:55 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6400 s / batch. (data: 8.63e-04). ETA=19:30:05, max mem: 15.9 GB 
[10/29 13:55:58 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6189 s / batch. (data: 3.65e-04). ETA=18:50:35, max mem: 15.9 GB 
[10/29 13:57:01 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6185 s / batch. (data: 1.64e-04). ETA=18:48:49, max mem: 15.9 GB 
[10/29 13:57:05 visual_prompt]: Epoch 1 / 100: avg data time: 4.35e-03, avg batch time: 0.6348, average train loss: 1.4028
[10/29 13:57:55 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2437 s / batch. (data: 3.63e-03)max mem: 15.94594 GB 
[10/29 13:58:06 visual_prompt]: Inference (val):avg data time: 2.86e-04, avg batch time: 0.2309, average loss: 1.3505
[10/29 13:58:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/29 13:58:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/29 13:59:11 visual_prompt]: 	Training 100/1106. train loss: 0.7087,	0.6496 s / batch. (data: 2.24e-02). ETA=19:44:21, max mem: 15.9 GB 
[10/29 14:00:14 visual_prompt]: 	Training 200/1106. train loss: 0.7849,	0.6179 s / batch. (data: 2.95e-04). ETA=18:45:36, max mem: 15.9 GB 
[10/29 14:01:17 visual_prompt]: 	Training 300/1106. train loss: 0.9198,	0.6356 s / batch. (data: 8.03e-04). ETA=19:16:40, max mem: 15.9 GB 
[10/29 14:02:20 visual_prompt]: 	Training 400/1106. train loss: 0.3144,	0.6338 s / batch. (data: 8.00e-04). ETA=19:12:28, max mem: 15.9 GB 
[10/29 14:03:24 visual_prompt]: 	Training 500/1106. train loss: 0.5913,	0.6240 s / batch. (data: 3.29e-04). ETA=18:53:30, max mem: 15.9 GB 
[10/29 14:04:27 visual_prompt]: 	Training 600/1106. train loss: 0.6461,	0.6372 s / batch. (data: 2.88e-04). ETA=19:16:22, max mem: 15.9 GB 
[10/29 14:05:30 visual_prompt]: 	Training 700/1106. train loss: 1.0472,	0.6589 s / batch. (data: 2.92e-02). ETA=19:54:46, max mem: 15.9 GB 
[10/29 14:06:34 visual_prompt]: 	Training 800/1106. train loss: 0.7124,	0.6563 s / batch. (data: 7.92e-04). ETA=19:48:59, max mem: 15.9 GB 
[10/29 14:07:37 visual_prompt]: 	Training 900/1106. train loss: 0.9179,	0.6435 s / batch. (data: 8.09e-04). ETA=19:24:41, max mem: 15.9 GB 
[10/29 14:08:40 visual_prompt]: 	Training 1000/1106. train loss: 0.5996,	0.6320 s / batch. (data: 3.06e-04). ETA=19:02:45, max mem: 15.9 GB 
[10/29 14:09:43 visual_prompt]: 	Training 1100/1106. train loss: 0.4731,	0.6175 s / batch. (data: 1.37e-04). ETA=18:35:29, max mem: 15.9 GB 
[10/29 14:09:47 visual_prompt]: Epoch 2 / 100: avg data time: 3.91e-03, avg batch time: 0.6339, average train loss: 0.8610
[10/29 14:10:37 visual_prompt]: 	Test 100/123. loss: 1.535, 0.2393 s / batch. (data: 3.65e-05)max mem: 15.94594 GB 
[10/29 14:10:47 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2324, average loss: 1.4059
[10/29 14:10:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.08	
[10/29 14:10:47 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/29 14:11:54 visual_prompt]: 	Training 100/1106. train loss: 0.3327,	0.6300 s / batch. (data: 8.59e-04). ETA=18:56:59, max mem: 15.9 GB 
[10/29 14:12:57 visual_prompt]: 	Training 200/1106. train loss: 0.5051,	0.6428 s / batch. (data: 7.61e-04). ETA=19:19:04, max mem: 15.9 GB 
[10/29 14:14:00 visual_prompt]: 	Training 300/1106. train loss: 0.2238,	0.6631 s / batch. (data: 1.14e-02). ETA=19:54:29, max mem: 15.9 GB 
[10/29 14:15:04 visual_prompt]: 	Training 400/1106. train loss: 0.6736,	0.6507 s / batch. (data: 3.26e-04). ETA=19:31:04, max mem: 15.9 GB 
[10/29 14:16:07 visual_prompt]: 	Training 500/1106. train loss: 1.3079,	0.6188 s / batch. (data: 3.27e-04). ETA=18:32:36, max mem: 15.9 GB 
[10/29 14:17:10 visual_prompt]: 	Training 600/1106. train loss: 0.7421,	0.6306 s / batch. (data: 3.14e-04). ETA=18:52:49, max mem: 15.9 GB 
[10/29 14:18:14 visual_prompt]: 	Training 700/1106. train loss: 1.5472,	0.6470 s / batch. (data: 7.57e-04). ETA=19:21:16, max mem: 15.9 GB 
[10/29 14:19:17 visual_prompt]: 	Training 800/1106. train loss: 1.2901,	0.6360 s / batch. (data: 7.51e-04). ETA=19:00:24, max mem: 15.9 GB 
[10/29 14:20:20 visual_prompt]: 	Training 900/1106. train loss: 1.7713,	0.6366 s / batch. (data: 5.43e-03). ETA=19:00:32, max mem: 15.9 GB 
[10/29 14:21:23 visual_prompt]: 	Training 1000/1106. train loss: 0.7219,	0.6440 s / batch. (data: 2.99e-04). ETA=19:12:39, max mem: 15.9 GB 
[10/29 14:22:26 visual_prompt]: 	Training 1100/1106. train loss: 0.6789,	0.6186 s / batch. (data: 1.64e-04). ETA=18:26:04, max mem: 15.9 GB 
[10/29 14:22:30 visual_prompt]: Epoch 3 / 100: avg data time: 5.39e-03, avg batch time: 0.6356, average train loss: 0.8751
[10/29 14:23:20 visual_prompt]: 	Test 100/123. loss: 0.817, 0.2440 s / batch. (data: 4.24e-05)max mem: 15.94594 GB 
[10/29 14:23:31 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2314, average loss: 0.8650
[10/29 14:23:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.32	
[10/29 14:23:31 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/29 14:24:36 visual_prompt]: 	Training 100/1106. train loss: 0.9903,	0.6278 s / batch. (data: 7.82e-04). ETA=18:41:29, max mem: 15.9 GB 
[10/29 14:25:39 visual_prompt]: 	Training 200/1106. train loss: 1.3786,	0.6320 s / batch. (data: 3.31e-04). ETA=18:47:51, max mem: 15.9 GB 
[10/29 14:26:42 visual_prompt]: 	Training 300/1106. train loss: 1.0160,	0.6306 s / batch. (data: 8.27e-04). ETA=18:44:18, max mem: 15.9 GB 
[10/29 14:27:46 visual_prompt]: 	Training 400/1106. train loss: 0.9948,	0.6585 s / batch. (data: 5.94e-03). ETA=19:33:04, max mem: 15.9 GB 
[10/29 14:28:49 visual_prompt]: 	Training 500/1106. train loss: 2.3093,	0.6342 s / batch. (data: 7.84e-04). ETA=18:48:38, max mem: 15.9 GB 
[10/29 14:29:52 visual_prompt]: 	Training 600/1106. train loss: 0.2714,	0.6277 s / batch. (data: 1.06e-02). ETA=18:36:01, max mem: 15.9 GB 
[10/29 14:30:55 visual_prompt]: 	Training 700/1106. train loss: 1.6417,	0.6173 s / batch. (data: 3.30e-04). ETA=18:16:29, max mem: 15.9 GB 
[10/29 14:31:58 visual_prompt]: 	Training 800/1106. train loss: 0.0653,	0.6184 s / batch. (data: 3.38e-04). ETA=18:17:30, max mem: 15.9 GB 
[10/29 14:33:02 visual_prompt]: 	Training 900/1106. train loss: 0.6972,	0.6344 s / batch. (data: 1.43e-02). ETA=18:44:46, max mem: 15.9 GB 
[10/29 14:34:05 visual_prompt]: 	Training 1000/1106. train loss: 1.1315,	0.6304 s / batch. (data: 5.40e-03). ETA=18:36:43, max mem: 15.9 GB 
[10/29 14:35:08 visual_prompt]: 	Training 1100/1106. train loss: 1.3932,	0.6169 s / batch. (data: 1.78e-04). ETA=18:11:48, max mem: 15.9 GB 
[10/29 14:35:12 visual_prompt]: Epoch 4 / 100: avg data time: 4.46e-03, avg batch time: 0.6342, average train loss: 0.8724
[10/29 14:36:03 visual_prompt]: 	Test 100/123. loss: 0.814, 0.2385 s / batch. (data: 4.46e-05)max mem: 15.94594 GB 
[10/29 14:36:12 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.2340, average loss: 0.7634
[10/29 14:36:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.42	
[10/29 14:36:12 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/29 14:37:17 visual_prompt]: 	Training 100/1106. train loss: 1.3510,	0.6178 s / batch. (data: 3.66e-04). ETA=18:12:16, max mem: 15.9 GB 
[10/29 14:38:20 visual_prompt]: 	Training 200/1106. train loss: 1.4294,	0.6544 s / batch. (data: 4.04e-04). ETA=19:15:53, max mem: 15.9 GB 
[10/29 14:39:23 visual_prompt]: 	Training 300/1106. train loss: 0.8143,	0.6312 s / batch. (data: 8.28e-04). ETA=18:33:45, max mem: 15.9 GB 
[10/29 14:40:27 visual_prompt]: 	Training 400/1106. train loss: 1.0911,	0.6191 s / batch. (data: 8.50e-04). ETA=18:11:27, max mem: 15.9 GB 
[10/29 14:41:30 visual_prompt]: 	Training 500/1106. train loss: 0.0852,	0.6189 s / batch. (data: 3.38e-04). ETA=18:10:06, max mem: 15.9 GB 
[10/29 14:42:33 visual_prompt]: 	Training 600/1106. train loss: 2.4550,	0.6290 s / batch. (data: 3.37e-04). ETA=18:26:48, max mem: 15.9 GB 
[10/29 14:43:37 visual_prompt]: 	Training 700/1106. train loss: 0.6895,	0.6175 s / batch. (data: 3.29e-04). ETA=18:05:34, max mem: 15.9 GB 
[10/29 14:44:40 visual_prompt]: 	Training 800/1106. train loss: 1.6468,	0.6183 s / batch. (data: 3.01e-04). ETA=18:05:50, max mem: 15.9 GB 
[10/29 14:45:43 visual_prompt]: 	Training 900/1106. train loss: 0.7395,	0.6403 s / batch. (data: 3.46e-04). ETA=18:43:31, max mem: 15.9 GB 
[10/29 14:46:46 visual_prompt]: 	Training 1000/1106. train loss: 0.3968,	0.6342 s / batch. (data: 8.74e-04). ETA=18:31:39, max mem: 15.9 GB 
[10/29 14:47:50 visual_prompt]: 	Training 1100/1106. train loss: 0.7085,	0.6176 s / batch. (data: 1.91e-04). ETA=18:01:34, max mem: 15.9 GB 
[10/29 14:47:53 visual_prompt]: Epoch 5 / 100: avg data time: 3.90e-03, avg batch time: 0.6338, average train loss: 0.8603
[10/29 14:48:43 visual_prompt]: 	Test 100/123. loss: 1.123, 0.2520 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[10/29 14:48:54 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.2319, average loss: 1.2182
[10/29 14:48:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.12	
[10/29 14:48:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/29 14:49:59 visual_prompt]: 	Training 100/1106. train loss: 0.9647,	0.6566 s / batch. (data: 2.47e-02). ETA=19:08:40, max mem: 15.9 GB 
[10/29 14:51:02 visual_prompt]: 	Training 200/1106. train loss: 0.0210,	0.6172 s / batch. (data: 3.23e-04). ETA=17:58:42, max mem: 15.9 GB 
[10/29 14:52:05 visual_prompt]: 	Training 300/1106. train loss: 0.0207,	0.6168 s / batch. (data: 3.04e-04). ETA=17:57:06, max mem: 15.9 GB 
[10/29 14:53:08 visual_prompt]: 	Training 400/1106. train loss: 0.3132,	0.6280 s / batch. (data: 4.06e-03). ETA=18:15:32, max mem: 15.9 GB 
[10/29 14:54:11 visual_prompt]: 	Training 500/1106. train loss: 0.8568,	0.6582 s / batch. (data: 7.00e-04). ETA=19:07:09, max mem: 15.9 GB 
[10/29 14:55:15 visual_prompt]: 	Training 600/1106. train loss: 0.3941,	0.6391 s / batch. (data: 5.44e-03). ETA=18:32:48, max mem: 15.9 GB 
[10/29 14:56:18 visual_prompt]: 	Training 700/1106. train loss: 1.2292,	0.6460 s / batch. (data: 8.02e-04). ETA=18:43:46, max mem: 15.9 GB 
[10/29 14:57:21 visual_prompt]: 	Training 800/1106. train loss: 0.8697,	0.6178 s / batch. (data: 3.05e-04). ETA=17:53:42, max mem: 15.9 GB 
[10/29 14:58:25 visual_prompt]: 	Training 900/1106. train loss: 0.7719,	0.6349 s / batch. (data: 7.55e-04). ETA=18:22:15, max mem: 15.9 GB 
[10/29 14:59:28 visual_prompt]: 	Training 1000/1106. train loss: 1.4985,	0.6325 s / batch. (data: 1.59e-02). ETA=18:17:04, max mem: 15.9 GB 
[10/29 15:00:31 visual_prompt]: 	Training 1100/1106. train loss: 0.2234,	0.6181 s / batch. (data: 1.38e-04). ETA=17:51:04, max mem: 15.9 GB 
[10/29 15:00:35 visual_prompt]: Epoch 6 / 100: avg data time: 4.04e-03, avg batch time: 0.6338, average train loss: 0.9017
[10/29 15:01:25 visual_prompt]: 	Test 100/123. loss: 0.855, 0.2406 s / batch. (data: 5.27e-05)max mem: 15.94594 GB 
[10/29 15:01:35 visual_prompt]: Inference (val):avg data time: 1.31e-04, avg batch time: 0.2317, average loss: 0.7979
[10/29 15:01:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.71	
[10/29 15:01:35 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/29 15:02:40 visual_prompt]: 	Training 100/1106. train loss: 0.8995,	0.6317 s / batch. (data: 8.33e-04). ETA=18:13:33, max mem: 15.9 GB 
[10/29 15:03:43 visual_prompt]: 	Training 200/1106. train loss: 1.6555,	0.6243 s / batch. (data: 3.09e-04). ETA=17:59:37, max mem: 15.9 GB 
[10/29 15:04:46 visual_prompt]: 	Training 300/1106. train loss: 0.1183,	0.6295 s / batch. (data: 3.14e-04). ETA=18:07:38, max mem: 15.9 GB 
[10/29 15:05:49 visual_prompt]: 	Training 400/1106. train loss: 0.9308,	0.6191 s / batch. (data: 4.27e-04). ETA=17:48:31, max mem: 15.9 GB 
[10/29 15:06:52 visual_prompt]: 	Training 500/1106. train loss: 0.9309,	0.6321 s / batch. (data: 3.20e-04). ETA=18:09:55, max mem: 15.9 GB 
[10/29 15:07:56 visual_prompt]: 	Training 600/1106. train loss: 2.6555,	0.6274 s / batch. (data: 3.05e-04). ETA=18:00:53, max mem: 15.9 GB 
[10/29 15:08:59 visual_prompt]: 	Training 700/1106. train loss: 2.4775,	0.6172 s / batch. (data: 3.10e-04). ETA=17:42:14, max mem: 15.9 GB 
[10/29 15:10:02 visual_prompt]: 	Training 800/1106. train loss: 0.2367,	0.6316 s / batch. (data: 7.73e-04). ETA=18:05:53, max mem: 15.9 GB 
[10/29 15:11:05 visual_prompt]: 	Training 900/1106. train loss: 0.8000,	0.6192 s / batch. (data: 3.79e-04). ETA=17:43:37, max mem: 15.9 GB 
[10/29 15:12:08 visual_prompt]: 	Training 1000/1106. train loss: 0.6978,	0.6442 s / batch. (data: 7.92e-04). ETA=18:25:31, max mem: 15.9 GB 
[10/29 15:13:11 visual_prompt]: 	Training 1100/1106. train loss: 0.5075,	0.6157 s / batch. (data: 1.54e-04). ETA=17:35:31, max mem: 15.9 GB 
[10/29 15:13:15 visual_prompt]: Epoch 7 / 100: avg data time: 3.82e-03, avg batch time: 0.6327, average train loss: 0.9301
[10/29 15:14:04 visual_prompt]: 	Test 100/123. loss: 0.806, 0.2357 s / batch. (data: 4.98e-05)max mem: 15.94594 GB 
[10/29 15:14:15 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2319, average loss: 0.7588
[10/29 15:14:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.45	
[10/29 15:14:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/29 15:15:20 visual_prompt]: 	Training 100/1106. train loss: 0.4223,	0.6181 s / batch. (data: 3.35e-04). ETA=17:38:32, max mem: 15.9 GB 
[10/29 15:16:23 visual_prompt]: 	Training 200/1106. train loss: 0.9842,	0.6161 s / batch. (data: 3.23e-04). ETA=17:34:08, max mem: 15.9 GB 
[10/29 15:17:26 visual_prompt]: 	Training 300/1106. train loss: 0.0778,	0.6415 s / batch. (data: 1.29e-02). ETA=18:16:34, max mem: 15.9 GB 
[10/29 15:18:30 visual_prompt]: 	Training 400/1106. train loss: 0.2885,	0.6171 s / batch. (data: 2.31e-04). ETA=17:33:44, max mem: 15.9 GB 
[10/29 15:19:33 visual_prompt]: 	Training 500/1106. train loss: 0.1990,	0.6223 s / batch. (data: 3.24e-04). ETA=17:41:40, max mem: 15.9 GB 
[10/29 15:20:36 visual_prompt]: 	Training 600/1106. train loss: 0.3607,	0.6165 s / batch. (data: 3.18e-04). ETA=17:30:39, max mem: 15.9 GB 
[10/29 15:21:39 visual_prompt]: 	Training 700/1106. train loss: 0.8509,	0.6319 s / batch. (data: 7.90e-04). ETA=17:55:53, max mem: 15.9 GB 
[10/29 15:22:42 visual_prompt]: 	Training 800/1106. train loss: 2.1139,	0.6556 s / batch. (data: 1.10e-02). ETA=18:35:07, max mem: 15.9 GB 
[10/29 15:23:45 visual_prompt]: 	Training 900/1106. train loss: 0.0483,	0.6222 s / batch. (data: 3.25e-04). ETA=17:37:22, max mem: 15.9 GB 
[10/29 15:24:49 visual_prompt]: 	Training 1000/1106. train loss: 0.9620,	0.6314 s / batch. (data: 8.06e-04). ETA=17:51:50, max mem: 15.9 GB 
[10/29 15:25:52 visual_prompt]: 	Training 1100/1106. train loss: 1.2072,	0.6175 s / batch. (data: 1.53e-04). ETA=17:27:18, max mem: 15.9 GB 
[10/29 15:25:56 visual_prompt]: Epoch 8 / 100: avg data time: 4.10e-03, avg batch time: 0.6335, average train loss: 1.0034
[10/29 15:26:45 visual_prompt]: 	Test 100/123. loss: 0.789, 0.2245 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/29 15:26:56 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.2321, average loss: 0.7463
[10/29 15:26:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.77	
[10/29 15:26:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/29 15:28:02 visual_prompt]: 	Training 100/1106. train loss: 6.4866,	0.6318 s / batch. (data: 7.60e-04). ETA=17:50:23, max mem: 15.9 GB 
[10/29 15:29:05 visual_prompt]: 	Training 200/1106. train loss: 0.4271,	0.6404 s / batch. (data: 7.87e-04). ETA=18:03:56, max mem: 15.9 GB 
[10/29 15:30:08 visual_prompt]: 	Training 300/1106. train loss: 0.8990,	0.6184 s / batch. (data: 3.23e-04). ETA=17:25:42, max mem: 15.9 GB 
[10/29 15:31:11 visual_prompt]: 	Training 400/1106. train loss: 1.4124,	0.6249 s / batch. (data: 3.08e-04). ETA=17:35:39, max mem: 15.9 GB 
[10/29 15:32:15 visual_prompt]: 	Training 500/1106. train loss: 1.5584,	0.6411 s / batch. (data: 1.10e-02). ETA=18:01:57, max mem: 15.9 GB 
[10/29 15:33:18 visual_prompt]: 	Training 600/1106. train loss: 0.7079,	0.6337 s / batch. (data: 7.21e-04). ETA=17:48:22, max mem: 15.9 GB 
[10/29 15:34:21 visual_prompt]: 	Training 700/1106. train loss: 0.5564,	0.6340 s / batch. (data: 7.51e-04). ETA=17:47:43, max mem: 15.9 GB 
[10/29 15:35:25 visual_prompt]: 	Training 800/1106. train loss: 0.8980,	0.6319 s / batch. (data: 7.68e-04). ETA=17:43:12, max mem: 15.9 GB 
[10/29 15:36:28 visual_prompt]: 	Training 900/1106. train loss: 0.3534,	0.6399 s / batch. (data: 1.09e-03). ETA=17:55:38, max mem: 15.9 GB 
[10/29 15:37:32 visual_prompt]: 	Training 1000/1106. train loss: 0.7299,	0.6455 s / batch. (data: 8.24e-04). ETA=18:03:58, max mem: 15.9 GB 
[10/29 15:38:35 visual_prompt]: 	Training 1100/1106. train loss: 0.9130,	0.6178 s / batch. (data: 1.46e-04). ETA=17:16:20, max mem: 15.9 GB 
[10/29 15:38:39 visual_prompt]: Epoch 9 / 100: avg data time: 4.55e-03, avg batch time: 0.6352, average train loss: 1.0422
[10/29 15:39:28 visual_prompt]: 	Test 100/123. loss: 0.778, 0.2256 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/29 15:39:39 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2329, average loss: 0.8222
[10/29 15:39:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.53	
[10/29 15:39:39 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/29 15:40:45 visual_prompt]: 	Training 100/1106. train loss: 1.3904,	0.6173 s / batch. (data: 3.01e-04). ETA=17:14:25, max mem: 15.9 GB 
[10/29 15:41:48 visual_prompt]: 	Training 200/1106. train loss: 4.4684,	0.6225 s / batch. (data: 2.96e-04). ETA=17:22:03, max mem: 15.9 GB 
[10/29 15:42:51 visual_prompt]: 	Training 300/1106. train loss: 1.5119,	0.6455 s / batch. (data: 7.77e-04). ETA=17:59:31, max mem: 15.9 GB 
[10/29 15:43:54 visual_prompt]: 	Training 400/1106. train loss: 1.5915,	0.6179 s / batch. (data: 3.21e-04). ETA=17:12:19, max mem: 15.9 GB 
[10/29 15:44:58 visual_prompt]: 	Training 500/1106. train loss: 0.7857,	0.6560 s / batch. (data: 8.08e-04). ETA=18:14:56, max mem: 15.9 GB 
[10/29 15:46:01 visual_prompt]: 	Training 600/1106. train loss: 1.6159,	0.6360 s / batch. (data: 3.02e-04). ETA=17:40:28, max mem: 15.9 GB 
[10/29 15:47:05 visual_prompt]: 	Training 700/1106. train loss: 0.5126,	0.6212 s / batch. (data: 3.16e-04). ETA=17:14:46, max mem: 15.9 GB 
[10/29 15:48:08 visual_prompt]: 	Training 800/1106. train loss: 0.7090,	0.6520 s / batch. (data: 7.67e-04). ETA=18:04:58, max mem: 15.9 GB 
[10/29 15:49:11 visual_prompt]: 	Training 900/1106. train loss: 0.7832,	0.6450 s / batch. (data: 2.55e-02). ETA=17:52:18, max mem: 15.9 GB 
[10/29 15:50:14 visual_prompt]: 	Training 1000/1106. train loss: 0.2110,	0.6342 s / batch. (data: 7.97e-04). ETA=17:33:19, max mem: 15.9 GB 
[10/29 15:51:18 visual_prompt]: 	Training 1100/1106. train loss: 1.5347,	0.6186 s / batch. (data: 1.48e-04). ETA=17:06:18, max mem: 15.9 GB 
[10/29 15:51:21 visual_prompt]: Epoch 10 / 100: avg data time: 3.86e-03, avg batch time: 0.6348, average train loss: 1.1190
[10/29 15:52:11 visual_prompt]: 	Test 100/123. loss: 0.692, 0.2405 s / batch. (data: 5.41e-05)max mem: 15.94594 GB 
[10/29 15:52:22 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.2314, average loss: 0.6883
[10/29 15:52:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.03	
[10/29 15:52:22 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/29 15:53:27 visual_prompt]: 	Training 100/1106. train loss: 2.6007,	0.6374 s / batch. (data: 3.06e-04). ETA=17:36:22, max mem: 15.9 GB 
[10/29 15:54:30 visual_prompt]: 	Training 200/1106. train loss: 0.7453,	0.6431 s / batch. (data: 8.43e-04). ETA=17:44:47, max mem: 15.9 GB 
[10/29 15:55:33 visual_prompt]: 	Training 300/1106. train loss: 0.7753,	0.6187 s / batch. (data: 3.07e-04). ETA=17:03:17, max mem: 15.9 GB 
[10/29 15:56:37 visual_prompt]: 	Training 400/1106. train loss: 0.5588,	0.6344 s / batch. (data: 8.58e-04). ETA=17:28:16, max mem: 15.9 GB 
[10/29 15:57:40 visual_prompt]: 	Training 500/1106. train loss: 0.7517,	0.6336 s / batch. (data: 3.27e-04). ETA=17:25:53, max mem: 15.9 GB 
[10/29 15:58:44 visual_prompt]: 	Training 600/1106. train loss: 0.0555,	0.6213 s / batch. (data: 2.80e-04). ETA=17:04:28, max mem: 15.9 GB 
[10/29 15:59:47 visual_prompt]: 	Training 700/1106. train loss: 0.3498,	0.6195 s / batch. (data: 2.81e-04). ETA=17:00:34, max mem: 15.9 GB 
[10/29 16:00:50 visual_prompt]: 	Training 800/1106. train loss: 0.5428,	0.6589 s / batch. (data: 8.18e-04). ETA=18:04:15, max mem: 15.9 GB 
[10/29 16:01:53 visual_prompt]: 	Training 900/1106. train loss: 0.1383,	0.6182 s / batch. (data: 3.15e-04). ETA=16:56:18, max mem: 15.9 GB 
[10/29 16:02:56 visual_prompt]: 	Training 1000/1106. train loss: 0.0168,	0.6201 s / batch. (data: 3.29e-04). ETA=16:58:29, max mem: 15.9 GB 
[10/29 16:04:00 visual_prompt]: 	Training 1100/1106. train loss: 1.0619,	0.6189 s / batch. (data: 1.57e-04). ETA=16:55:25, max mem: 15.9 GB 
[10/29 16:04:03 visual_prompt]: Epoch 11 / 100: avg data time: 4.11e-03, avg batch time: 0.6342, average train loss: 1.1551
[10/29 16:04:54 visual_prompt]: 	Test 100/123. loss: 1.260, 0.2542 s / batch. (data: 3.86e-05)max mem: 15.94594 GB 
[10/29 16:05:04 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2331, average loss: 1.1487
[10/29 16:05:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[10/29 16:05:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/29 16:06:10 visual_prompt]: 	Training 100/1106. train loss: 1.0983,	0.6315 s / batch. (data: 7.71e-04). ETA=17:14:57, max mem: 15.9 GB 
[10/29 16:07:14 visual_prompt]: 	Training 200/1106. train loss: 0.8042,	0.6324 s / batch. (data: 8.25e-04). ETA=17:15:19, max mem: 15.9 GB 
[10/29 16:08:17 visual_prompt]: 	Training 300/1106. train loss: 2.5700,	0.6241 s / batch. (data: 5.54e-03). ETA=17:00:45, max mem: 15.9 GB 
[10/29 16:09:20 visual_prompt]: 	Training 400/1106. train loss: 0.0833,	0.6714 s / batch. (data: 3.90e-02). ETA=18:16:56, max mem: 15.9 GB 
[10/29 16:10:23 visual_prompt]: 	Training 500/1106. train loss: 5.6291,	0.6306 s / batch. (data: 3.05e-04). ETA=17:09:16, max mem: 15.9 GB 
[10/29 16:11:26 visual_prompt]: 	Training 600/1106. train loss: 0.9858,	0.6229 s / batch. (data: 2.66e-04). ETA=16:55:41, max mem: 15.9 GB 
[10/29 16:12:30 visual_prompt]: 	Training 700/1106. train loss: 0.7247,	0.6309 s / batch. (data: 7.52e-04). ETA=17:07:42, max mem: 15.9 GB 
[10/29 16:13:33 visual_prompt]: 	Training 800/1106. train loss: 0.7450,	0.6340 s / batch. (data: 7.30e-04). ETA=17:11:43, max mem: 15.9 GB 
[10/29 16:14:36 visual_prompt]: 	Training 900/1106. train loss: 1.0042,	0.6314 s / batch. (data: 8.21e-04). ETA=17:06:27, max mem: 15.9 GB 
[10/29 16:15:40 visual_prompt]: 	Training 1000/1106. train loss: 0.8350,	0.6193 s / batch. (data: 3.19e-04). ETA=16:45:41, max mem: 15.9 GB 
[10/29 16:16:43 visual_prompt]: 	Training 1100/1106. train loss: 2.6055,	0.6173 s / batch. (data: 1.58e-04). ETA=16:41:19, max mem: 15.9 GB 
[10/29 16:16:47 visual_prompt]: Epoch 12 / 100: avg data time: 5.21e-03, avg batch time: 0.6352, average train loss: 1.2138
[10/29 16:17:37 visual_prompt]: 	Test 100/123. loss: 0.693, 0.2272 s / batch. (data: 3.86e-05)max mem: 15.94594 GB 
[10/29 16:17:47 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2320, average loss: 0.6964
[10/29 16:17:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.67	
[10/29 16:17:47 visual_prompt]: Best epoch 12: best metric: -0.696
[10/29 16:17:47 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/29 16:18:53 visual_prompt]: 	Training 100/1106. train loss: 3.4373,	0.6304 s / batch. (data: 8.26e-04). ETA=17:01:31, max mem: 15.9 GB 
[10/29 16:19:56 visual_prompt]: 	Training 200/1106. train loss: 1.2439,	0.6261 s / batch. (data: 4.78e-04). ETA=16:53:31, max mem: 15.9 GB 
[10/29 16:20:59 visual_prompt]: 	Training 300/1106. train loss: 0.0907,	0.6317 s / batch. (data: 8.12e-04). ETA=17:01:31, max mem: 15.9 GB 
[10/29 16:22:02 visual_prompt]: 	Training 400/1106. train loss: 1.5865,	0.6187 s / batch. (data: 3.14e-04). ETA=16:39:25, max mem: 15.9 GB 
[10/29 16:23:06 visual_prompt]: 	Training 500/1106. train loss: 0.6865,	0.6199 s / batch. (data: 3.38e-04). ETA=16:40:22, max mem: 15.9 GB 
[10/29 16:24:09 visual_prompt]: 	Training 600/1106. train loss: 2.3904,	0.6263 s / batch. (data: 2.96e-04). ETA=16:49:40, max mem: 15.9 GB 
[10/29 16:25:12 visual_prompt]: 	Training 700/1106. train loss: 0.3353,	0.6354 s / batch. (data: 3.09e-04). ETA=17:03:13, max mem: 15.9 GB 
[10/29 16:26:16 visual_prompt]: 	Training 800/1106. train loss: 1.2537,	0.6550 s / batch. (data: 8.00e-04). ETA=17:33:48, max mem: 15.9 GB 
[10/29 16:27:19 visual_prompt]: 	Training 900/1106. train loss: 3.4873,	0.6432 s / batch. (data: 1.20e-02). ETA=17:13:39, max mem: 15.9 GB 
[10/29 16:28:22 visual_prompt]: 	Training 1000/1106. train loss: 0.0054,	0.6350 s / batch. (data: 8.37e-04). ETA=16:59:30, max mem: 15.9 GB 
[10/29 16:29:26 visual_prompt]: 	Training 1100/1106. train loss: 1.3018,	0.6192 s / batch. (data: 1.85e-04). ETA=16:33:06, max mem: 15.9 GB 
[10/29 16:29:29 visual_prompt]: Epoch 13 / 100: avg data time: 4.66e-03, avg batch time: 0.6346, average train loss: 1.4126
[10/29 16:30:19 visual_prompt]: 	Test 100/123. loss: 0.699, 0.2436 s / batch. (data: 1.11e-02)max mem: 15.94594 GB 
[10/29 16:30:30 visual_prompt]: Inference (val):avg data time: 1.88e-04, avg batch time: 0.2328, average loss: 0.7179
[10/29 16:30:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.16	
[10/29 16:30:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/29 16:31:36 visual_prompt]: 	Training 100/1106. train loss: 0.0715,	0.6276 s / batch. (data: 3.02e-04). ETA=16:45:21, max mem: 15.9 GB 
[10/29 16:32:39 visual_prompt]: 	Training 200/1106. train loss: 0.7057,	0.6467 s / batch. (data: 9.50e-04). ETA=17:14:58, max mem: 15.9 GB 
[10/29 16:33:42 visual_prompt]: 	Training 300/1106. train loss: 0.7602,	0.6447 s / batch. (data: 7.56e-04). ETA=17:10:41, max mem: 15.9 GB 
[10/29 16:34:46 visual_prompt]: 	Training 400/1106. train loss: 0.0625,	0.6328 s / batch. (data: 7.79e-04). ETA=16:50:33, max mem: 15.9 GB 
[10/29 16:35:49 visual_prompt]: 	Training 500/1106. train loss: 1.2201,	0.6339 s / batch. (data: 8.10e-04). ETA=16:51:20, max mem: 15.9 GB 
[10/29 16:36:52 visual_prompt]: 	Training 600/1106. train loss: 0.8912,	0.6181 s / batch. (data: 3.15e-04). ETA=16:25:05, max mem: 15.9 GB 
[10/29 16:37:56 visual_prompt]: 	Training 700/1106. train loss: 2.0349,	0.6331 s / batch. (data: 8.03e-04). ETA=16:47:53, max mem: 15.9 GB 
[10/29 16:38:59 visual_prompt]: 	Training 800/1106. train loss: 1.2250,	0.6336 s / batch. (data: 3.35e-04). ETA=16:47:42, max mem: 15.9 GB 
[10/29 16:40:03 visual_prompt]: 	Training 900/1106. train loss: 0.7405,	0.6307 s / batch. (data: 7.19e-04). ETA=16:42:00, max mem: 15.9 GB 
[10/29 16:41:06 visual_prompt]: 	Training 1000/1106. train loss: 0.1197,	0.6400 s / batch. (data: 7.98e-03). ETA=16:55:41, max mem: 15.9 GB 
[10/29 16:42:09 visual_prompt]: 	Training 1100/1106. train loss: 0.7398,	0.6178 s / batch. (data: 1.47e-04). ETA=16:19:26, max mem: 15.9 GB 
[10/29 16:42:13 visual_prompt]: Epoch 14 / 100: avg data time: 4.32e-03, avg batch time: 0.6352, average train loss: 1.1657
[10/29 16:43:03 visual_prompt]: 	Test 100/123. loss: 1.451, 0.2294 s / batch. (data: 4.86e-05)max mem: 15.94594 GB 
[10/29 16:43:13 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2322, average loss: 1.3085
[10/29 16:43:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.93	
[10/29 16:43:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/29 16:44:18 visual_prompt]: 	Training 100/1106. train loss: 3.5826,	0.6376 s / batch. (data: 7.48e-04). ETA=16:49:41, max mem: 15.9 GB 
[10/29 16:45:22 visual_prompt]: 	Training 200/1106. train loss: 0.7338,	0.6193 s / batch. (data: 4.34e-04). ETA=16:19:42, max mem: 15.9 GB 
[10/29 16:46:25 visual_prompt]: 	Training 300/1106. train loss: 4.3839,	0.6321 s / batch. (data: 3.31e-04). ETA=16:38:50, max mem: 15.9 GB 
[10/29 16:47:28 visual_prompt]: 	Training 400/1106. train loss: 13.2085,	0.6189 s / batch. (data: 3.09e-04). ETA=16:17:01, max mem: 15.9 GB 
[10/29 16:48:31 visual_prompt]: 	Training 500/1106. train loss: 0.6386,	0.6480 s / batch. (data: 7.73e-04). ETA=17:01:51, max mem: 15.9 GB 
[10/29 16:49:35 visual_prompt]: 	Training 600/1106. train loss: 1.1494,	0.6274 s / batch. (data: 2.92e-04). ETA=16:28:18, max mem: 15.9 GB 
[10/29 16:50:38 visual_prompt]: 	Training 700/1106. train loss: 2.1855,	0.6332 s / batch. (data: 7.91e-04). ETA=16:36:20, max mem: 15.9 GB 
[10/29 16:51:41 visual_prompt]: 	Training 800/1106. train loss: 1.0594,	0.6448 s / batch. (data: 7.75e-04). ETA=16:53:30, max mem: 15.9 GB 
[10/29 16:52:44 visual_prompt]: 	Training 900/1106. train loss: 2.3275,	0.6495 s / batch. (data: 1.68e-02). ETA=16:59:53, max mem: 15.9 GB 
[10/29 16:53:48 visual_prompt]: 	Training 1000/1106. train loss: 0.8060,	0.6182 s / batch. (data: 2.98e-04). ETA=16:09:46, max mem: 15.9 GB 
[10/29 16:54:51 visual_prompt]: 	Training 1100/1106. train loss: 1.7634,	0.6191 s / batch. (data: 1.65e-04). ETA=16:10:01, max mem: 15.9 GB 
[10/29 16:54:55 visual_prompt]: Epoch 15 / 100: avg data time: 4.17e-03, avg batch time: 0.6341, average train loss: 1.6137
[10/29 16:55:44 visual_prompt]: 	Test 100/123. loss: 1.186, 0.2266 s / batch. (data: 4.01e-05)max mem: 15.94594 GB 
[10/29 16:55:55 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2333, average loss: 1.2922
[10/29 16:55:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.10	
[10/29 16:55:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/29 16:57:00 visual_prompt]: 	Training 100/1106. train loss: 0.5137,	0.6456 s / batch. (data: 8.74e-04). ETA=16:50:31, max mem: 15.9 GB 
[10/29 16:58:04 visual_prompt]: 	Training 200/1106. train loss: 0.8852,	0.6183 s / batch. (data: 3.15e-04). ETA=16:06:46, max mem: 15.9 GB 
[10/29 16:59:07 visual_prompt]: 	Training 300/1106. train loss: 0.3373,	0.6312 s / batch. (data: 3.40e-04). ETA=16:25:49, max mem: 15.9 GB 
[10/29 17:00:10 visual_prompt]: 	Training 400/1106. train loss: 1.0345,	0.6191 s / batch. (data: 3.27e-04). ETA=16:05:50, max mem: 15.9 GB 
[10/29 17:01:13 visual_prompt]: 	Training 500/1106. train loss: 0.9764,	0.6472 s / batch. (data: 8.16e-04). ETA=16:48:35, max mem: 15.9 GB 
[10/29 17:02:17 visual_prompt]: 	Training 600/1106. train loss: 0.8761,	0.6177 s / batch. (data: 3.52e-04). ETA=16:01:42, max mem: 15.9 GB 
[10/29 17:03:20 visual_prompt]: 	Training 700/1106. train loss: 1.0147,	0.6195 s / batch. (data: 2.98e-04). ETA=16:03:22, max mem: 15.9 GB 
[10/29 17:04:24 visual_prompt]: 	Training 800/1106. train loss: 0.6921,	0.6281 s / batch. (data: 4.49e-04). ETA=16:15:42, max mem: 15.9 GB 
[10/29 17:05:27 visual_prompt]: 	Training 900/1106. train loss: 0.8490,	0.6316 s / batch. (data: 8.44e-04). ETA=16:20:04, max mem: 15.9 GB 
[10/29 17:06:30 visual_prompt]: 	Training 1000/1106. train loss: 1.0891,	0.6353 s / batch. (data: 7.43e-04). ETA=16:24:46, max mem: 15.9 GB 
[10/29 17:07:34 visual_prompt]: 	Training 1100/1106. train loss: 0.0683,	0.6176 s / batch. (data: 1.39e-04). ETA=15:56:25, max mem: 15.9 GB 
[10/29 17:07:37 visual_prompt]: Epoch 16 / 100: avg data time: 3.84e-03, avg batch time: 0.6348, average train loss: 1.0738
[10/29 17:08:27 visual_prompt]: 	Test 100/123. loss: 0.712, 0.2317 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[10/29 17:08:38 visual_prompt]: Inference (val):avg data time: 1.20e-04, avg batch time: 0.2328, average loss: 0.6929
[10/29 17:08:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.26	
[10/29 17:08:38 visual_prompt]: Best epoch 16: best metric: -0.693
[10/29 17:08:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/29 17:09:43 visual_prompt]: 	Training 100/1106. train loss: 0.0615,	0.6257 s / batch. (data: 7.81e-04). ETA=16:07:43, max mem: 15.9 GB 
[10/29 17:10:46 visual_prompt]: 	Training 200/1106. train loss: 0.8937,	0.6406 s / batch. (data: 2.88e-04). ETA=16:29:47, max mem: 15.9 GB 
[10/29 17:11:49 visual_prompt]: 	Training 300/1106. train loss: 0.5953,	0.6305 s / batch. (data: 1.26e-02). ETA=16:13:10, max mem: 15.9 GB 
[10/29 17:12:53 visual_prompt]: 	Training 400/1106. train loss: 0.3177,	0.6381 s / batch. (data: 3.65e-04). ETA=16:23:50, max mem: 15.9 GB 
[10/29 17:13:56 visual_prompt]: 	Training 500/1106. train loss: 0.0651,	0.6350 s / batch. (data: 8.55e-04). ETA=16:17:58, max mem: 15.9 GB 
[10/29 17:14:59 visual_prompt]: 	Training 600/1106. train loss: 0.7710,	0.6467 s / batch. (data: 8.16e-04). ETA=16:34:52, max mem: 15.9 GB 
[10/29 17:16:03 visual_prompt]: 	Training 700/1106. train loss: 1.7118,	0.6181 s / batch. (data: 7.74e-04). ETA=15:49:53, max mem: 15.9 GB 
[10/29 17:17:06 visual_prompt]: 	Training 800/1106. train loss: 0.4824,	0.6222 s / batch. (data: 2.97e-04). ETA=15:55:03, max mem: 15.9 GB 
[10/29 17:18:09 visual_prompt]: 	Training 900/1106. train loss: 0.2761,	0.6332 s / batch. (data: 8.08e-04). ETA=16:10:59, max mem: 15.9 GB 
[10/29 17:19:12 visual_prompt]: 	Training 1000/1106. train loss: 0.0909,	0.6194 s / batch. (data: 5.96e-04). ETA=15:48:43, max mem: 15.9 GB 
[10/29 17:20:16 visual_prompt]: 	Training 1100/1106. train loss: 0.1112,	0.6194 s / batch. (data: 1.55e-04). ETA=15:47:43, max mem: 15.9 GB 
[10/29 17:20:19 visual_prompt]: Epoch 17 / 100: avg data time: 4.24e-03, avg batch time: 0.6345, average train loss: 1.1513
[10/29 17:21:09 visual_prompt]: 	Test 100/123. loss: 0.963, 0.2467 s / batch. (data: 2.86e-05)max mem: 15.94594 GB 
[10/29 17:21:20 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2320, average loss: 1.0409
[10/29 17:21:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.31	
[10/29 17:21:20 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/29 17:22:25 visual_prompt]: 	Training 100/1106. train loss: 0.2809,	0.6328 s / batch. (data: 9.95e-04). ETA=16:07:04, max mem: 15.9 GB 
[10/29 17:23:28 visual_prompt]: 	Training 200/1106. train loss: 1.1471,	0.6444 s / batch. (data: 7.97e-04). ETA=16:23:41, max mem: 15.9 GB 
[10/29 17:24:31 visual_prompt]: 	Training 300/1106. train loss: 7.9690,	0.6184 s / batch. (data: 3.12e-04). ETA=15:43:03, max mem: 15.9 GB 
[10/29 17:25:34 visual_prompt]: 	Training 400/1106. train loss: 1.2334,	0.6336 s / batch. (data: 8.24e-04). ETA=16:05:06, max mem: 15.9 GB 
[10/29 17:26:38 visual_prompt]: 	Training 500/1106. train loss: 0.0064,	0.6349 s / batch. (data: 3.19e-04). ETA=16:06:04, max mem: 15.9 GB 
[10/29 17:27:41 visual_prompt]: 	Training 600/1106. train loss: 0.3081,	0.6332 s / batch. (data: 1.18e-03). ETA=16:02:27, max mem: 15.9 GB 
[10/29 17:28:44 visual_prompt]: 	Training 700/1106. train loss: 0.8447,	0.6191 s / batch. (data: 3.21e-04). ETA=15:39:58, max mem: 15.9 GB 
[10/29 17:29:47 visual_prompt]: 	Training 800/1106. train loss: 0.7591,	0.6342 s / batch. (data: 3.20e-04). ETA=16:01:48, max mem: 15.9 GB 
[10/29 17:30:51 visual_prompt]: 	Training 900/1106. train loss: 4.8462,	0.6402 s / batch. (data: 5.93e-03). ETA=16:09:52, max mem: 15.9 GB 
[10/29 17:31:54 visual_prompt]: 	Training 1000/1106. train loss: 0.8738,	0.6192 s / batch. (data: 3.02e-04). ETA=15:37:01, max mem: 15.9 GB 
[10/29 17:32:57 visual_prompt]: 	Training 1100/1106. train loss: 0.0212,	0.6180 s / batch. (data: 2.06e-04). ETA=15:34:12, max mem: 15.9 GB 
[10/29 17:33:01 visual_prompt]: Epoch 18 / 100: avg data time: 4.45e-03, avg batch time: 0.6340, average train loss: 1.2621
[10/29 17:33:51 visual_prompt]: 	Test 100/123. loss: 0.874, 0.2406 s / batch. (data: 4.41e-05)max mem: 15.94594 GB 
[10/29 17:34:02 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.2318, average loss: 0.9393
[10/29 17:34:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.32	
[10/29 17:34:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/29 17:35:07 visual_prompt]: 	Training 100/1106. train loss: 0.3289,	0.6327 s / batch. (data: 8.65e-04). ETA=15:55:16, max mem: 15.9 GB 
[10/29 17:36:10 visual_prompt]: 	Training 200/1106. train loss: 1.5993,	0.6190 s / batch. (data: 3.23e-04). ETA=15:33:35, max mem: 15.9 GB 
[10/29 17:37:14 visual_prompt]: 	Training 300/1106. train loss: 0.4048,	0.6393 s / batch. (data: 7.88e-04). ETA=16:03:07, max mem: 15.9 GB 
[10/29 17:38:17 visual_prompt]: 	Training 400/1106. train loss: 0.7116,	0.6419 s / batch. (data: 1.17e-02). ETA=16:05:58, max mem: 15.9 GB 
[10/29 17:39:20 visual_prompt]: 	Training 500/1106. train loss: 0.1223,	0.6560 s / batch. (data: 1.60e-02). ETA=16:26:04, max mem: 15.9 GB 
[10/29 17:40:23 visual_prompt]: 	Training 600/1106. train loss: 0.1821,	0.6189 s / batch. (data: 3.18e-04). ETA=15:29:14, max mem: 15.9 GB 
[10/29 17:41:27 visual_prompt]: 	Training 700/1106. train loss: 0.7402,	0.6181 s / batch. (data: 3.26e-04). ETA=15:27:08, max mem: 15.9 GB 
[10/29 17:42:30 visual_prompt]: 	Training 800/1106. train loss: 0.0087,	0.6341 s / batch. (data: 8.22e-04). ETA=15:49:58, max mem: 15.9 GB 
[10/29 17:43:33 visual_prompt]: 	Training 900/1106. train loss: 1.8097,	0.6320 s / batch. (data: 3.32e-04). ETA=15:45:49, max mem: 15.9 GB 
[10/29 17:44:37 visual_prompt]: 	Training 1000/1106. train loss: 0.0111,	0.6398 s / batch. (data: 7.45e-04). ETA=15:56:25, max mem: 15.9 GB 
[10/29 17:45:40 visual_prompt]: 	Training 1100/1106. train loss: 4.8669,	0.6322 s / batch. (data: 1.80e-04). ETA=15:43:55, max mem: 15.9 GB 
[10/29 17:45:44 visual_prompt]: Epoch 19 / 100: avg data time: 4.19e-03, avg batch time: 0.6348, average train loss: 1.2148
[10/29 17:46:35 visual_prompt]: 	Test 100/123. loss: 5.054, 0.2315 s / batch. (data: 4.41e-05)max mem: 15.94594 GB 
[10/29 17:46:44 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2325, average loss: 4.5580
[10/29 17:46:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.01	
[10/29 17:46:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/29 17:47:50 visual_prompt]: 	Training 100/1106. train loss: 1.0958,	0.6269 s / batch. (data: 2.54e-04). ETA=15:34:55, max mem: 15.9 GB 
[10/29 17:48:53 visual_prompt]: 	Training 200/1106. train loss: 0.4067,	0.6337 s / batch. (data: 1.60e-02). ETA=15:44:03, max mem: 15.9 GB 
[10/29 17:49:56 visual_prompt]: 	Training 300/1106. train loss: 0.9096,	0.6280 s / batch. (data: 7.32e-04). ETA=15:34:31, max mem: 15.9 GB 
[10/29 17:51:00 visual_prompt]: 	Training 400/1106. train loss: 1.1788,	0.6179 s / batch. (data: 2.67e-04). ETA=15:18:31, max mem: 15.9 GB 
[10/29 17:52:03 visual_prompt]: 	Training 500/1106. train loss: 0.8792,	0.6205 s / batch. (data: 3.28e-04). ETA=15:21:15, max mem: 15.9 GB 
[10/29 17:53:06 visual_prompt]: 	Training 600/1106. train loss: 0.8999,	0.6185 s / batch. (data: 3.75e-04). ETA=15:17:14, max mem: 15.9 GB 
[10/29 17:54:09 visual_prompt]: 	Training 700/1106. train loss: 1.0747,	0.6537 s / batch. (data: 3.39e-02). ETA=16:08:24, max mem: 15.9 GB 
[10/29 17:55:13 visual_prompt]: 	Training 800/1106. train loss: 0.1265,	0.6301 s / batch. (data: 2.95e-04). ETA=15:32:22, max mem: 15.9 GB 
[10/29 17:56:16 visual_prompt]: 	Training 900/1106. train loss: 1.5754,	0.6311 s / batch. (data: 7.75e-04). ETA=15:32:52, max mem: 15.9 GB 
[10/29 17:57:19 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6469 s / batch. (data: 5.88e-03). ETA=15:55:08, max mem: 15.9 GB 
[10/29 17:58:23 visual_prompt]: 	Training 1100/1106. train loss: 0.1936,	0.6184 s / batch. (data: 1.45e-04). ETA=15:12:02, max mem: 15.9 GB 
[10/29 17:58:27 visual_prompt]: Epoch 20 / 100: avg data time: 4.75e-03, avg batch time: 0.6350, average train loss: 1.1390
[10/29 17:59:17 visual_prompt]: 	Test 100/123. loss: 0.698, 0.2441 s / batch. (data: 4.36e-05)max mem: 15.94594 GB 
[10/29 17:59:27 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2316, average loss: 0.6885
[10/29 17:59:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.21	
[10/29 17:59:27 visual_prompt]: Best epoch 20: best metric: -0.689
[10/29 17:59:27 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[10/29 18:00:33 visual_prompt]: 	Training 100/1106. train loss: 0.1459,	0.6566 s / batch. (data: 1.10e-02). ETA=16:07:06, max mem: 15.9 GB 
[10/29 18:01:37 visual_prompt]: 	Training 200/1106. train loss: 1.0916,	0.6560 s / batch. (data: 3.11e-04). ETA=16:05:09, max mem: 15.9 GB 
[10/29 18:02:40 visual_prompt]: 	Training 300/1106. train loss: 0.8908,	0.6304 s / batch. (data: 2.64e-04). ETA=15:26:31, max mem: 15.9 GB 
[10/29 18:03:43 visual_prompt]: 	Training 400/1106. train loss: 1.1146,	0.6466 s / batch. (data: 7.61e-04). ETA=15:49:09, max mem: 15.9 GB 
[10/29 18:04:46 visual_prompt]: 	Training 500/1106. train loss: 0.0851,	0.6210 s / batch. (data: 3.22e-04). ETA=15:10:37, max mem: 15.9 GB 
[10/29 18:05:49 visual_prompt]: 	Training 600/1106. train loss: 1.1958,	0.6179 s / batch. (data: 3.22e-04). ETA=15:05:05, max mem: 15.9 GB 
[10/29 18:06:53 visual_prompt]: 	Training 700/1106. train loss: 1.8426,	0.6195 s / batch. (data: 2.87e-04). ETA=15:06:17, max mem: 15.9 GB 
[10/29 18:07:56 visual_prompt]: 	Training 800/1106. train loss: 0.7581,	0.6180 s / batch. (data: 2.93e-04). ETA=15:03:06, max mem: 15.9 GB 
[10/29 18:08:59 visual_prompt]: 	Training 900/1106. train loss: 0.0005,	0.6400 s / batch. (data: 7.97e-04). ETA=15:34:14, max mem: 15.9 GB 
[10/29 18:10:03 visual_prompt]: 	Training 1000/1106. train loss: 0.7111,	0.6197 s / batch. (data: 3.26e-04). ETA=15:03:33, max mem: 15.9 GB 
[10/29 18:11:06 visual_prompt]: 	Training 1100/1106. train loss: 1.4527,	0.6186 s / batch. (data: 1.53e-04). ETA=15:00:57, max mem: 15.9 GB 
[10/29 18:11:09 visual_prompt]: Epoch 21 / 100: avg data time: 4.83e-03, avg batch time: 0.6347, average train loss: 1.2074
[10/29 18:12:00 visual_prompt]: 	Test 100/123. loss: 0.762, 0.2486 s / batch. (data: 4.48e-05)max mem: 15.94594 GB 
[10/29 18:12:10 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2335, average loss: 0.7263
[10/29 18:12:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.34	
[10/29 18:12:10 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[10/29 18:13:16 visual_prompt]: 	Training 100/1106. train loss: 1.0790,	0.6321 s / batch. (data: 8.18e-04). ETA=15:19:23, max mem: 15.9 GB 
[10/29 18:14:19 visual_prompt]: 	Training 200/1106. train loss: 0.0448,	0.6200 s / batch. (data: 4.59e-04). ETA=15:00:49, max mem: 15.9 GB 
[10/29 18:15:22 visual_prompt]: 	Training 300/1106. train loss: 0.5267,	0.6194 s / batch. (data: 3.29e-04). ETA=14:58:55, max mem: 15.9 GB 
[10/29 18:16:25 visual_prompt]: 	Training 400/1106. train loss: 1.1254,	0.6204 s / batch. (data: 3.11e-04). ETA=14:59:15, max mem: 15.9 GB 
[10/29 18:17:28 visual_prompt]: 	Training 500/1106. train loss: 1.9999,	0.6326 s / batch. (data: 3.02e-04). ETA=15:15:53, max mem: 15.9 GB 
[10/29 18:18:32 visual_prompt]: 	Training 600/1106. train loss: 0.0489,	0.6430 s / batch. (data: 8.30e-04). ETA=15:29:53, max mem: 15.9 GB 
[10/29 18:19:35 visual_prompt]: 	Training 700/1106. train loss: 1.5106,	0.6384 s / batch. (data: 8.14e-04). ETA=15:22:15, max mem: 15.9 GB 
[10/29 18:20:39 visual_prompt]: 	Training 800/1106. train loss: 4.4337,	0.6498 s / batch. (data: 7.91e-04). ETA=15:37:38, max mem: 15.9 GB 
[10/29 18:21:42 visual_prompt]: 	Training 900/1106. train loss: 1.5603,	0.6324 s / batch. (data: 4.63e-04). ETA=15:11:22, max mem: 15.9 GB 
[10/29 18:22:46 visual_prompt]: 	Training 1000/1106. train loss: 1.5799,	0.6207 s / batch. (data: 2.55e-04). ETA=14:53:30, max mem: 15.9 GB 
[10/29 18:23:49 visual_prompt]: 	Training 1100/1106. train loss: 3.7574,	0.6187 s / batch. (data: 1.59e-04). ETA=14:49:40, max mem: 15.9 GB 
[10/29 18:23:53 visual_prompt]: Epoch 22 / 100: avg data time: 4.11e-03, avg batch time: 0.6351, average train loss: 1.1727
[10/29 18:24:43 visual_prompt]: 	Test 100/123. loss: 1.449, 0.2251 s / batch. (data: 4.05e-05)max mem: 15.94594 GB 
[10/29 18:24:54 visual_prompt]: Inference (val):avg data time: 1.15e-04, avg batch time: 0.2312, average loss: 1.3149
[10/29 18:24:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.11	
[10/29 18:24:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[10/29 18:26:00 visual_prompt]: 	Training 100/1106. train loss: 0.6219,	0.6278 s / batch. (data: 7.84e-04). ETA=15:01:36, max mem: 15.9 GB 
[10/29 18:27:03 visual_prompt]: 	Training 200/1106. train loss: 1.1511,	0.6180 s / batch. (data: 3.15e-04). ETA=14:46:26, max mem: 15.9 GB 
[10/29 18:28:06 visual_prompt]: 	Training 300/1106. train loss: 0.1870,	0.6444 s / batch. (data: 7.81e-04). ETA=15:23:14, max mem: 15.9 GB 
[10/29 18:29:10 visual_prompt]: 	Training 400/1106. train loss: 0.7355,	0.6370 s / batch. (data: 7.91e-04). ETA=15:11:41, max mem: 15.9 GB 
[10/29 18:30:13 visual_prompt]: 	Training 500/1106. train loss: 0.6999,	0.6440 s / batch. (data: 1.61e-02). ETA=15:20:34, max mem: 15.9 GB 
[10/29 18:31:16 visual_prompt]: 	Training 600/1106. train loss: 0.1449,	0.6334 s / batch. (data: 8.23e-04). ETA=15:04:19, max mem: 15.9 GB 
[10/29 18:32:20 visual_prompt]: 	Training 700/1106. train loss: 1.3020,	0.6208 s / batch. (data: 3.02e-04). ETA=14:45:19, max mem: 15.9 GB 
[10/29 18:33:23 visual_prompt]: 	Training 800/1106. train loss: 0.9708,	0.6216 s / batch. (data: 3.20e-04). ETA=14:45:30, max mem: 15.9 GB 
[10/29 18:34:26 visual_prompt]: 	Training 900/1106. train loss: 0.7891,	0.6496 s / batch. (data: 5.91e-03). ETA=15:24:14, max mem: 15.9 GB 
[10/29 18:35:29 visual_prompt]: 	Training 1000/1106. train loss: 0.0393,	0.6401 s / batch. (data: 7.28e-04). ETA=15:09:41, max mem: 15.9 GB 
[10/29 18:36:32 visual_prompt]: 	Training 1100/1106. train loss: 1.0207,	0.6166 s / batch. (data: 1.59e-04). ETA=14:35:11, max mem: 15.9 GB 
[10/29 18:36:36 visual_prompt]: Epoch 23 / 100: avg data time: 4.90e-03, avg batch time: 0.6351, average train loss: 1.0752
[10/29 18:37:26 visual_prompt]: 	Test 100/123. loss: 0.784, 0.2458 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[10/29 18:37:36 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2322, average loss: 0.7413
[10/29 18:37:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.62	
[10/29 18:37:36 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[10/29 18:38:42 visual_prompt]: 	Training 100/1106. train loss: 3.2735,	0.6309 s / batch. (data: 8.66e-04). ETA=14:54:24, max mem: 15.9 GB 
[10/29 18:39:45 visual_prompt]: 	Training 200/1106. train loss: 1.2979,	0.6327 s / batch. (data: 7.82e-04). ETA=14:55:57, max mem: 15.9 GB 
[10/29 18:40:48 visual_prompt]: 	Training 300/1106. train loss: 0.6687,	0.6187 s / batch. (data: 3.21e-04). ETA=14:35:05, max mem: 15.9 GB 
[10/29 18:41:51 visual_prompt]: 	Training 400/1106. train loss: 0.7726,	0.6192 s / batch. (data: 2.64e-04). ETA=14:34:43, max mem: 15.9 GB 
[10/29 18:42:55 visual_prompt]: 	Training 500/1106. train loss: 2.1507,	0.6287 s / batch. (data: 3.18e-04). ETA=14:47:10, max mem: 15.9 GB 
[10/29 18:43:58 visual_prompt]: 	Training 600/1106. train loss: 1.2879,	0.6253 s / batch. (data: 3.26e-04). ETA=14:41:18, max mem: 15.9 GB 
[10/29 18:45:01 visual_prompt]: 	Training 700/1106. train loss: 2.0860,	0.6190 s / batch. (data: 2.84e-04). ETA=14:31:19, max mem: 15.9 GB 
[10/29 18:46:04 visual_prompt]: 	Training 800/1106. train loss: 0.8494,	0.6310 s / batch. (data: 1.25e-02). ETA=14:47:16, max mem: 15.9 GB 
[10/29 18:47:08 visual_prompt]: 	Training 900/1106. train loss: 0.8039,	0.6194 s / batch. (data: 2.93e-04). ETA=14:29:54, max mem: 15.9 GB 
[10/29 18:48:11 visual_prompt]: 	Training 1000/1106. train loss: 0.6994,	0.6444 s / batch. (data: 8.16e-04). ETA=15:03:54, max mem: 15.9 GB 
[10/29 18:49:14 visual_prompt]: 	Training 1100/1106. train loss: 1.0052,	0.6177 s / batch. (data: 1.54e-04). ETA=14:25:24, max mem: 15.9 GB 
[10/29 18:49:18 visual_prompt]: Epoch 24 / 100: avg data time: 4.58e-03, avg batch time: 0.6345, average train loss: 1.1698
[10/29 18:50:08 visual_prompt]: 	Test 100/123. loss: 1.439, 0.2397 s / batch. (data: 5.44e-05)max mem: 15.94594 GB 
[10/29 18:50:18 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2341, average loss: 1.3035
[10/29 18:50:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.77	
[10/29 18:50:18 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[10/29 18:51:24 visual_prompt]: 	Training 100/1106. train loss: 1.8922,	0.6349 s / batch. (data: 8.02e-04). ETA=14:48:19, max mem: 15.9 GB 
[10/29 18:52:27 visual_prompt]: 	Training 200/1106. train loss: 0.7770,	0.6489 s / batch. (data: 2.11e-02). ETA=15:06:56, max mem: 15.9 GB 
[10/29 18:53:31 visual_prompt]: 	Training 300/1106. train loss: 1.0031,	0.6554 s / batch. (data: 1.10e-02). ETA=15:14:51, max mem: 15.9 GB 
[10/29 18:54:34 visual_prompt]: 	Training 400/1106. train loss: 1.5025,	0.6283 s / batch. (data: 3.04e-04). ETA=14:36:00, max mem: 15.9 GB 
[10/29 18:55:37 visual_prompt]: 	Training 500/1106. train loss: 0.7414,	0.6194 s / batch. (data: 2.92e-04). ETA=14:22:34, max mem: 15.9 GB 
[10/29 18:56:40 visual_prompt]: 	Training 600/1106. train loss: 2.5814,	0.6190 s / batch. (data: 3.12e-04). ETA=14:20:57, max mem: 15.9 GB 
[10/29 18:57:44 visual_prompt]: 	Training 700/1106. train loss: 1.1119,	0.6440 s / batch. (data: 3.19e-04). ETA=14:54:38, max mem: 15.9 GB 
[10/29 18:58:47 visual_prompt]: 	Training 800/1106. train loss: 0.2989,	0.6294 s / batch. (data: 1.02e-03). ETA=14:33:25, max mem: 15.9 GB 
[10/29 18:59:50 visual_prompt]: 	Training 900/1106. train loss: 0.7138,	0.6191 s / batch. (data: 6.11e-04). ETA=14:18:02, max mem: 15.9 GB 
[10/29 19:00:54 visual_prompt]: 	Training 1000/1106. train loss: 0.6983,	0.6422 s / batch. (data: 8.72e-04). ETA=14:49:01, max mem: 15.9 GB 
[10/29 19:01:57 visual_prompt]: 	Training 1100/1106. train loss: 0.0264,	0.6169 s / batch. (data: 1.55e-04). ETA=14:12:59, max mem: 15.9 GB 
[10/29 19:02:01 visual_prompt]: Epoch 25 / 100: avg data time: 4.30e-03, avg batch time: 0.6351, average train loss: 1.1619
[10/29 19:02:50 visual_prompt]: 	Test 100/123. loss: 0.919, 0.2381 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/29 19:03:01 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2321, average loss: 0.8500
[10/29 19:03:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.79	
[10/29 19:03:01 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[10/29 19:04:06 visual_prompt]: 	Training 100/1106. train loss: 0.8060,	0.6256 s / batch. (data: 3.13e-04). ETA=14:23:49, max mem: 15.9 GB 
[10/29 19:05:10 visual_prompt]: 	Training 200/1106. train loss: 1.2570,	0.6459 s / batch. (data: 7.86e-04). ETA=14:50:45, max mem: 15.9 GB 
[10/29 19:06:13 visual_prompt]: 	Training 300/1106. train loss: 0.2002,	0.6327 s / batch. (data: 8.08e-04). ETA=14:31:35, max mem: 15.9 GB 
[10/29 19:07:16 visual_prompt]: 	Training 400/1106. train loss: 0.7533,	0.6310 s / batch. (data: 3.04e-04). ETA=14:28:10, max mem: 15.9 GB 
[10/29 19:08:19 visual_prompt]: 	Training 500/1106. train loss: 1.5351,	0.6191 s / batch. (data: 3.23e-04). ETA=14:10:44, max mem: 15.9 GB 
[10/29 19:09:22 visual_prompt]: 	Training 600/1106. train loss: 0.7883,	0.6456 s / batch. (data: 2.95e-04). ETA=14:46:01, max mem: 15.9 GB 
[10/29 19:10:26 visual_prompt]: 	Training 700/1106. train loss: 1.2775,	0.6214 s / batch. (data: 3.18e-04). ETA=14:11:49, max mem: 15.9 GB 
[10/29 19:11:29 visual_prompt]: 	Training 800/1106. train loss: 1.6015,	0.6289 s / batch. (data: 2.65e-04). ETA=14:21:03, max mem: 15.9 GB 
[10/29 19:12:32 visual_prompt]: 	Training 900/1106. train loss: 2.5353,	0.6342 s / batch. (data: 7.92e-04). ETA=14:27:13, max mem: 15.9 GB 
[10/29 19:13:35 visual_prompt]: 	Training 1000/1106. train loss: 1.4702,	0.6441 s / batch. (data: 8.82e-03). ETA=14:39:46, max mem: 15.9 GB 
[10/29 19:14:39 visual_prompt]: 	Training 1100/1106. train loss: 0.0986,	0.6187 s / batch. (data: 1.32e-04). ETA=14:04:04, max mem: 15.9 GB 
[10/29 19:14:42 visual_prompt]: Epoch 26 / 100: avg data time: 4.29e-03, avg batch time: 0.6338, average train loss: 1.1099
[10/29 19:15:32 visual_prompt]: 	Test 100/123. loss: 1.201, 0.2250 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/29 19:15:43 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2324, average loss: 1.0895
[10/29 19:15:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.64	
[10/29 19:15:43 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[10/29 19:16:48 visual_prompt]: 	Training 100/1106. train loss: 0.2014,	0.6395 s / batch. (data: 8.67e-04). ETA=14:31:16, max mem: 15.9 GB 
[10/29 19:17:51 visual_prompt]: 	Training 200/1106. train loss: 1.2811,	0.6189 s / batch. (data: 3.05e-04). ETA=14:02:07, max mem: 15.9 GB 
[10/29 19:18:54 visual_prompt]: 	Training 300/1106. train loss: 1.8196,	0.6310 s / batch. (data: 7.87e-04). ETA=14:17:36, max mem: 15.9 GB 
[10/29 19:19:58 visual_prompt]: 	Training 400/1106. train loss: 6.3940,	0.6191 s / batch. (data: 3.28e-04). ETA=14:00:21, max mem: 15.9 GB 
[10/29 19:21:01 visual_prompt]: 	Training 500/1106. train loss: 0.5254,	0.6383 s / batch. (data: 3.35e-04). ETA=14:25:25, max mem: 15.9 GB 
[10/29 19:22:04 visual_prompt]: 	Training 600/1106. train loss: 2.5616,	0.6270 s / batch. (data: 3.29e-04). ETA=14:08:59, max mem: 15.9 GB 
[10/29 19:23:08 visual_prompt]: 	Training 700/1106. train loss: 1.3425,	0.6313 s / batch. (data: 7.93e-04). ETA=14:13:43, max mem: 15.9 GB 
[10/29 19:24:11 visual_prompt]: 	Training 800/1106. train loss: 0.7859,	0.6376 s / batch. (data: 5.89e-03). ETA=14:21:14, max mem: 15.9 GB 
[10/29 19:25:14 visual_prompt]: 	Training 900/1106. train loss: 1.3033,	0.6317 s / batch. (data: 7.98e-04). ETA=14:12:11, max mem: 15.9 GB 
[10/29 19:26:17 visual_prompt]: 	Training 1000/1106. train loss: 0.7047,	0.6318 s / batch. (data: 3.14e-04). ETA=14:11:19, max mem: 15.9 GB 
[10/29 19:27:21 visual_prompt]: 	Training 1100/1106. train loss: 0.3984,	0.6182 s / batch. (data: 1.57e-04). ETA=13:51:55, max mem: 15.9 GB 
[10/29 19:27:24 visual_prompt]: Epoch 27 / 100: avg data time: 4.56e-03, avg batch time: 0.6344, average train loss: 1.1024
[10/29 19:28:14 visual_prompt]: 	Test 100/123. loss: 2.026, 0.2326 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/29 19:28:25 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2326, average loss: 2.2258
[10/29 19:28:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.09	
[10/29 19:28:25 visual_prompt]: Stopping early.
[10/29 19:28:25 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 19:28:25 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 19:28:25 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 19:28:25 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 19:28:25 visual_prompt]: Training with config:
[10/29 19:28:25 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.25_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 19:28:25 visual_prompt]: Loading training data...
[10/29 19:28:25 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 19:28:25 visual_prompt]: Loading validation data...
[10/29 19:28:25 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 19:28:25 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/29 19:28:27 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/29 19:28:27 visual_prompt]: tuned percent:0.522
[10/29 19:28:27 visual_prompt]: Device used for model: 0
[10/29 19:28:27 visual_prompt]: Setting up Evaluator...
[10/29 19:28:27 visual_prompt]: Setting up Trainer...
[10/29 19:28:27 visual_prompt]: 	Setting up the optimizer...
[10/29 19:28:27 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 19:29:33 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6336 s / batch. (data: 1.20e-02). ETA=19:26:50, max mem: 15.9 GB 
[10/29 19:30:36 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6300 s / batch. (data: 8.55e-04). ETA=19:19:07, max mem: 15.9 GB 
[10/29 19:31:40 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6189 s / batch. (data: 3.32e-04). ETA=18:57:44, max mem: 15.9 GB 
[10/29 19:32:43 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6193 s / batch. (data: 3.07e-04). ETA=18:57:27, max mem: 15.9 GB 
[10/29 19:33:46 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6275 s / batch. (data: 3.18e-04). ETA=19:11:22, max mem: 15.9 GB 
[10/29 19:34:50 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6298 s / batch. (data: 3.80e-04). ETA=19:14:41, max mem: 15.9 GB 
[10/29 19:35:53 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6324 s / batch. (data: 3.19e-04). ETA=19:18:21, max mem: 15.9 GB 
[10/29 19:36:56 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6300 s / batch. (data: 2.86e-04). ETA=19:12:50, max mem: 15.9 GB 
[10/29 19:38:00 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6440 s / batch. (data: 7.76e-04). ETA=19:37:21, max mem: 15.9 GB 
[10/29 19:39:03 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6338 s / batch. (data: 8.22e-04). ETA=19:17:43, max mem: 15.9 GB 
[10/29 19:40:06 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6192 s / batch. (data: 1.44e-04). ETA=18:50:06, max mem: 15.9 GB 
[10/29 19:40:10 visual_prompt]: Epoch 1 / 100: avg data time: 4.55e-03, avg batch time: 0.6352, average train loss: 1.4028
[10/29 19:41:00 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2397 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/29 19:41:10 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2322, average loss: 1.3505
[10/29 19:41:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/29 19:41:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/29 19:42:15 visual_prompt]: 	Training 100/1106. train loss: 0.7252,	0.6263 s / batch. (data: 1.05e-02). ETA=19:01:52, max mem: 15.9 GB 
[10/29 19:43:19 visual_prompt]: 	Training 200/1106. train loss: 0.7488,	0.6216 s / batch. (data: 3.03e-04). ETA=18:52:14, max mem: 15.9 GB 
[10/29 19:44:22 visual_prompt]: 	Training 300/1106. train loss: 0.8770,	0.6181 s / batch. (data: 3.56e-04). ETA=18:44:48, max mem: 15.9 GB 
[10/29 19:45:25 visual_prompt]: 	Training 400/1106. train loss: 0.3529,	0.6264 s / batch. (data: 1.14e-03). ETA=18:58:57, max mem: 15.9 GB 
[10/29 19:46:29 visual_prompt]: 	Training 500/1106. train loss: 0.5852,	0.6290 s / batch. (data: 5.27e-04). ETA=19:02:35, max mem: 15.9 GB 
[10/29 19:47:32 visual_prompt]: 	Training 600/1106. train loss: 0.5774,	0.6231 s / batch. (data: 3.14e-04). ETA=18:50:50, max mem: 15.9 GB 
[10/29 19:48:35 visual_prompt]: 	Training 700/1106. train loss: 1.0693,	0.6173 s / batch. (data: 3.25e-04). ETA=18:39:15, max mem: 15.9 GB 
[10/29 19:49:38 visual_prompt]: 	Training 800/1106. train loss: 0.7473,	0.6391 s / batch. (data: 8.22e-04). ETA=19:17:49, max mem: 15.9 GB 
[10/29 19:50:41 visual_prompt]: 	Training 900/1106. train loss: 0.9020,	0.6446 s / batch. (data: 7.97e-04). ETA=19:26:34, max mem: 15.9 GB 
[10/29 19:51:45 visual_prompt]: 	Training 1000/1106. train loss: 0.5549,	0.6571 s / batch. (data: 7.59e-04). ETA=19:48:06, max mem: 15.9 GB 
[10/29 19:52:48 visual_prompt]: 	Training 1100/1106. train loss: 0.5189,	0.6186 s / batch. (data: 1.58e-04). ETA=18:37:28, max mem: 15.9 GB 
[10/29 19:52:52 visual_prompt]: Epoch 2 / 100: avg data time: 3.99e-03, avg batch time: 0.6340, average train loss: 0.8657
[10/29 19:53:41 visual_prompt]: 	Test 100/123. loss: 1.582, 0.2356 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/29 19:53:52 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.2308, average loss: 1.4565
[10/29 19:53:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.23	
[10/29 19:53:52 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/29 19:54:58 visual_prompt]: 	Training 100/1106. train loss: 0.3413,	0.6400 s / batch. (data: 7.75e-04). ETA=19:15:00, max mem: 15.9 GB 
[10/29 19:56:02 visual_prompt]: 	Training 200/1106. train loss: 0.5455,	0.6307 s / batch. (data: 7.41e-04). ETA=18:57:19, max mem: 15.9 GB 
[10/29 19:57:05 visual_prompt]: 	Training 300/1106. train loss: 0.2009,	0.6473 s / batch. (data: 7.98e-04). ETA=19:26:06, max mem: 15.9 GB 
[10/29 19:58:08 visual_prompt]: 	Training 400/1106. train loss: 0.6459,	0.6480 s / batch. (data: 8.10e-04). ETA=19:26:17, max mem: 15.9 GB 
[10/29 19:59:11 visual_prompt]: 	Training 500/1106. train loss: 1.3303,	0.6184 s / batch. (data: 3.38e-04). ETA=18:32:00, max mem: 15.9 GB 
[10/29 20:00:15 visual_prompt]: 	Training 600/1106. train loss: 0.7441,	0.6195 s / batch. (data: 3.59e-04). ETA=18:32:51, max mem: 15.9 GB 
[10/29 20:01:18 visual_prompt]: 	Training 700/1106. train loss: 1.5806,	0.6463 s / batch. (data: 7.87e-04). ETA=19:19:57, max mem: 15.9 GB 
[10/29 20:02:21 visual_prompt]: 	Training 800/1106. train loss: 0.8765,	0.6240 s / batch. (data: 3.03e-04). ETA=18:38:55, max mem: 15.9 GB 
[10/29 20:03:25 visual_prompt]: 	Training 900/1106. train loss: 2.3258,	0.6215 s / batch. (data: 2.79e-04). ETA=18:33:20, max mem: 15.9 GB 
[10/29 20:04:28 visual_prompt]: 	Training 1000/1106. train loss: 0.6821,	0.6610 s / batch. (data: 1.13e-03). ETA=19:42:59, max mem: 15.9 GB 
[10/29 20:05:31 visual_prompt]: 	Training 1100/1106. train loss: 0.6118,	0.6191 s / batch. (data: 1.34e-04). ETA=18:26:56, max mem: 15.9 GB 
[10/29 20:05:35 visual_prompt]: Epoch 3 / 100: avg data time: 5.07e-03, avg batch time: 0.6354, average train loss: 0.8992
[10/29 20:06:25 visual_prompt]: 	Test 100/123. loss: 0.934, 0.2247 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/29 20:06:35 visual_prompt]: Inference (val):avg data time: 3.16e-04, avg batch time: 0.2329, average loss: 0.9717
[10/29 20:06:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.28	
[10/29 20:06:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/29 20:07:41 visual_prompt]: 	Training 100/1106. train loss: 0.5970,	0.6303 s / batch. (data: 1.34e-02). ETA=18:45:57, max mem: 15.9 GB 
[10/29 20:08:44 visual_prompt]: 	Training 200/1106. train loss: 1.9715,	0.6288 s / batch. (data: 8.03e-04). ETA=18:42:10, max mem: 15.9 GB 
[10/29 20:09:48 visual_prompt]: 	Training 300/1106. train loss: 1.0979,	0.6290 s / batch. (data: 7.77e-04). ETA=18:41:33, max mem: 15.9 GB 
[10/29 20:10:51 visual_prompt]: 	Training 400/1106. train loss: 0.9273,	0.6207 s / batch. (data: 2.56e-04). ETA=18:25:37, max mem: 15.9 GB 
[10/29 20:11:54 visual_prompt]: 	Training 500/1106. train loss: 1.8809,	0.6326 s / batch. (data: 8.22e-04). ETA=18:45:51, max mem: 15.9 GB 
[10/29 20:12:58 visual_prompt]: 	Training 600/1106. train loss: 0.1378,	0.6313 s / batch. (data: 3.08e-04). ETA=18:42:28, max mem: 15.9 GB 
[10/29 20:14:01 visual_prompt]: 	Training 700/1106. train loss: 1.7566,	0.6192 s / batch. (data: 3.12e-04). ETA=18:19:55, max mem: 15.9 GB 
[10/29 20:15:04 visual_prompt]: 	Training 800/1106. train loss: 0.4929,	0.6296 s / batch. (data: 3.20e-04). ETA=18:37:22, max mem: 15.9 GB 
[10/29 20:16:07 visual_prompt]: 	Training 900/1106. train loss: 0.7590,	0.6441 s / batch. (data: 3.03e-04). ETA=19:02:01, max mem: 15.9 GB 
[10/29 20:17:11 visual_prompt]: 	Training 1000/1106. train loss: 2.0768,	0.6339 s / batch. (data: 7.29e-04). ETA=18:42:54, max mem: 15.9 GB 
[10/29 20:18:14 visual_prompt]: 	Training 1100/1106. train loss: 1.7261,	0.6185 s / batch. (data: 1.28e-04). ETA=18:14:31, max mem: 15.9 GB 
[10/29 20:18:18 visual_prompt]: Epoch 4 / 100: avg data time: 4.45e-03, avg batch time: 0.6351, average train loss: 0.9309
[10/29 20:19:08 visual_prompt]: 	Test 100/123. loss: 0.699, 0.2341 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/29 20:19:18 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2322, average loss: 0.6852
[10/29 20:19:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 56.00	
[10/29 20:19:18 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/29 20:20:24 visual_prompt]: 	Training 100/1106. train loss: 1.5340,	0.6181 s / batch. (data: 4.08e-04). ETA=18:12:42, max mem: 15.9 GB 
[10/29 20:21:27 visual_prompt]: 	Training 200/1106. train loss: 0.8354,	0.6209 s / batch. (data: 3.35e-04). ETA=18:16:37, max mem: 15.9 GB 
[10/29 20:22:30 visual_prompt]: 	Training 300/1106. train loss: 0.8630,	0.6199 s / batch. (data: 3.25e-04). ETA=18:13:51, max mem: 15.9 GB 
[10/29 20:23:33 visual_prompt]: 	Training 400/1106. train loss: 1.0292,	0.6476 s / batch. (data: 2.08e-02). ETA=19:01:45, max mem: 15.9 GB 
[10/29 20:24:36 visual_prompt]: 	Training 500/1106. train loss: 0.2338,	0.6317 s / batch. (data: 4.59e-04). ETA=18:32:35, max mem: 15.9 GB 
[10/29 20:25:39 visual_prompt]: 	Training 600/1106. train loss: 2.0918,	0.6298 s / batch. (data: 3.14e-04). ETA=18:28:11, max mem: 15.9 GB 
[10/29 20:26:42 visual_prompt]: 	Training 700/1106. train loss: 0.8209,	0.6182 s / batch. (data: 3.23e-04). ETA=18:06:46, max mem: 15.9 GB 
[10/29 20:27:46 visual_prompt]: 	Training 800/1106. train loss: 1.3935,	0.6305 s / batch. (data: 2.94e-04). ETA=18:27:19, max mem: 15.9 GB 
[10/29 20:28:49 visual_prompt]: 	Training 900/1106. train loss: 0.8533,	0.6189 s / batch. (data: 2.91e-04). ETA=18:05:51, max mem: 15.9 GB 
[10/29 20:29:52 visual_prompt]: 	Training 1000/1106. train loss: 1.1126,	0.6416 s / batch. (data: 7.67e-04). ETA=18:44:39, max mem: 15.9 GB 
[10/29 20:30:56 visual_prompt]: 	Training 1100/1106. train loss: 0.8386,	0.6183 s / batch. (data: 1.42e-04). ETA=18:02:46, max mem: 15.9 GB 
[10/29 20:30:59 visual_prompt]: Epoch 5 / 100: avg data time: 4.31e-03, avg batch time: 0.6339, average train loss: 1.0189
[10/29 20:31:50 visual_prompt]: 	Test 100/123. loss: 1.432, 0.2697 s / batch. (data: 3.62e-05)max mem: 15.94594 GB 
[10/29 20:32:00 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.2335, average loss: 1.5460
[10/29 20:32:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.49	
[10/29 20:32:00 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/29 20:33:05 visual_prompt]: 	Training 100/1106. train loss: 1.1821,	0.6440 s / batch. (data: 2.96e-04). ETA=18:46:42, max mem: 15.9 GB 
[10/29 20:34:08 visual_prompt]: 	Training 200/1106. train loss: 0.0250,	0.6244 s / batch. (data: 2.88e-04). ETA=18:11:19, max mem: 15.9 GB 
[10/29 20:35:11 visual_prompt]: 	Training 300/1106. train loss: 0.0197,	0.6329 s / batch. (data: 8.23e-04). ETA=18:25:07, max mem: 15.9 GB 
[10/29 20:36:15 visual_prompt]: 	Training 400/1106. train loss: 1.3972,	0.6377 s / batch. (data: 8.01e-04). ETA=18:32:29, max mem: 15.9 GB 
[10/29 20:37:18 visual_prompt]: 	Training 500/1106. train loss: 0.8402,	0.6314 s / batch. (data: 3.13e-04). ETA=18:20:26, max mem: 15.9 GB 
[10/29 20:38:21 visual_prompt]: 	Training 600/1106. train loss: 0.2085,	0.6486 s / batch. (data: 8.05e-04). ETA=18:49:17, max mem: 15.9 GB 
[10/29 20:39:24 visual_prompt]: 	Training 700/1106. train loss: 1.9776,	0.6839 s / batch. (data: 2.79e-02). ETA=19:49:42, max mem: 15.9 GB 
[10/29 20:40:28 visual_prompt]: 	Training 800/1106. train loss: 1.6015,	0.6190 s / batch. (data: 3.23e-04). ETA=17:55:38, max mem: 15.9 GB 
[10/29 20:41:31 visual_prompt]: 	Training 900/1106. train loss: 1.5565,	0.6184 s / batch. (data: 3.22e-04). ETA=17:53:42, max mem: 15.9 GB 
[10/29 20:42:34 visual_prompt]: 	Training 1000/1106. train loss: 0.9933,	0.6360 s / batch. (data: 2.97e-04). ETA=18:23:08, max mem: 15.9 GB 
[10/29 20:43:37 visual_prompt]: 	Training 1100/1106. train loss: 0.0295,	0.6172 s / batch. (data: 1.58e-04). ETA=17:49:35, max mem: 15.9 GB 
[10/29 20:43:41 visual_prompt]: Epoch 6 / 100: avg data time: 4.28e-03, avg batch time: 0.6340, average train loss: 0.9811
[10/29 20:44:32 visual_prompt]: 	Test 100/123. loss: 0.768, 0.2483 s / batch. (data: 3.77e-05)max mem: 15.94594 GB 
[10/29 20:44:43 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.2329, average loss: 0.8029
[10/29 20:44:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.62	
[10/29 20:44:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/29 20:45:48 visual_prompt]: 	Training 100/1106. train loss: 1.0694,	0.6402 s / batch. (data: 8.05e-04). ETA=18:28:13, max mem: 15.9 GB 
[10/29 20:46:51 visual_prompt]: 	Training 200/1106. train loss: 1.5833,	0.6310 s / batch. (data: 1.27e-02). ETA=18:11:19, max mem: 15.9 GB 
[10/29 20:47:54 visual_prompt]: 	Training 300/1106. train loss: 0.1247,	0.6173 s / batch. (data: 3.11e-04). ETA=17:46:27, max mem: 15.9 GB 
[10/29 20:48:58 visual_prompt]: 	Training 400/1106. train loss: 0.9600,	0.6173 s / batch. (data: 3.34e-04). ETA=17:45:32, max mem: 15.9 GB 
[10/29 20:50:01 visual_prompt]: 	Training 500/1106. train loss: 1.8718,	0.6232 s / batch. (data: 5.45e-03). ETA=17:54:35, max mem: 15.9 GB 
[10/29 20:51:04 visual_prompt]: 	Training 600/1106. train loss: 2.0658,	0.6349 s / batch. (data: 7.88e-04). ETA=18:13:40, max mem: 15.9 GB 
[10/29 20:52:08 visual_prompt]: 	Training 700/1106. train loss: 2.1231,	0.6191 s / batch. (data: 2.83e-04). ETA=17:45:29, max mem: 15.9 GB 
[10/29 20:53:11 visual_prompt]: 	Training 800/1106. train loss: 0.1703,	0.6336 s / batch. (data: 1.01e-03). ETA=18:09:22, max mem: 15.9 GB 
[10/29 20:54:14 visual_prompt]: 	Training 900/1106. train loss: 0.8493,	0.6294 s / batch. (data: 3.29e-04). ETA=18:01:12, max mem: 15.9 GB 
[10/29 20:55:18 visual_prompt]: 	Training 1000/1106. train loss: 0.6812,	0.6197 s / batch. (data: 3.15e-04). ETA=17:43:30, max mem: 15.9 GB 
[10/29 20:56:21 visual_prompt]: 	Training 1100/1106. train loss: 0.4453,	0.6184 s / batch. (data: 1.52e-04). ETA=17:40:13, max mem: 15.9 GB 
[10/29 20:56:25 visual_prompt]: Epoch 7 / 100: avg data time: 4.34e-03, avg batch time: 0.6346, average train loss: 0.9709
[10/29 20:57:15 visual_prompt]: 	Test 100/123. loss: 0.783, 0.2254 s / batch. (data: 6.18e-05)max mem: 15.94594 GB 
[10/29 20:57:25 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.2324, average loss: 0.7404
[10/29 20:57:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.77	
[10/29 20:57:25 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/29 20:58:30 visual_prompt]: 	Training 100/1106. train loss: 0.2398,	0.6279 s / batch. (data: 3.94e-03). ETA=17:55:16, max mem: 15.9 GB 
[10/29 20:59:33 visual_prompt]: 	Training 200/1106. train loss: 0.7272,	0.6171 s / batch. (data: 2.52e-04). ETA=17:35:46, max mem: 15.9 GB 
[10/29 21:00:37 visual_prompt]: 	Training 300/1106. train loss: 0.1126,	0.6317 s / batch. (data: 7.88e-04). ETA=17:59:42, max mem: 15.9 GB 
[10/29 21:01:40 visual_prompt]: 	Training 400/1106. train loss: 0.3959,	0.6188 s / batch. (data: 2.71e-04). ETA=17:36:37, max mem: 15.9 GB 
[10/29 21:02:44 visual_prompt]: 	Training 500/1106. train loss: 0.1477,	0.6183 s / batch. (data: 2.43e-04). ETA=17:34:43, max mem: 15.9 GB 
[10/29 21:03:47 visual_prompt]: 	Training 600/1106. train loss: 0.0898,	0.6445 s / batch. (data: 8.34e-04). ETA=18:18:29, max mem: 15.9 GB 
[10/29 21:04:50 visual_prompt]: 	Training 700/1106. train loss: 0.8443,	0.6469 s / batch. (data: 2.73e-02). ETA=18:21:23, max mem: 15.9 GB 
[10/29 21:05:53 visual_prompt]: 	Training 800/1106. train loss: 0.9632,	0.6398 s / batch. (data: 8.34e-04). ETA=18:08:14, max mem: 15.9 GB 
[10/29 21:06:57 visual_prompt]: 	Training 900/1106. train loss: 0.0519,	0.6262 s / batch. (data: 1.28e-03). ETA=17:44:05, max mem: 15.9 GB 
[10/29 21:08:00 visual_prompt]: 	Training 1000/1106. train loss: 2.6873,	0.6330 s / batch. (data: 3.30e-04). ETA=17:54:37, max mem: 15.9 GB 
[10/29 21:09:03 visual_prompt]: 	Training 1100/1106. train loss: 1.9243,	0.6187 s / batch. (data: 1.67e-04). ETA=17:29:13, max mem: 15.9 GB 
[10/29 21:09:07 visual_prompt]: Epoch 8 / 100: avg data time: 4.15e-03, avg batch time: 0.6347, average train loss: 1.0175
[10/29 21:09:57 visual_prompt]: 	Test 100/123. loss: 0.885, 0.2398 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/29 21:10:08 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2311, average loss: 0.8293
[10/29 21:10:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.02	
[10/29 21:10:08 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/29 21:11:13 visual_prompt]: 	Training 100/1106. train loss: 1.7322,	0.6416 s / batch. (data: 8.02e-04). ETA=18:07:03, max mem: 15.9 GB 
[10/29 21:12:17 visual_prompt]: 	Training 200/1106. train loss: 0.4722,	0.6524 s / batch. (data: 7.58e-04). ETA=18:24:14, max mem: 15.9 GB 
[10/29 21:13:20 visual_prompt]: 	Training 300/1106. train loss: 1.1500,	0.6190 s / batch. (data: 3.22e-04). ETA=17:26:42, max mem: 15.9 GB 
[10/29 21:14:23 visual_prompt]: 	Training 400/1106. train loss: 0.5571,	0.6183 s / batch. (data: 3.11e-04). ETA=17:24:28, max mem: 15.9 GB 
[10/29 21:15:26 visual_prompt]: 	Training 500/1106. train loss: 2.0542,	0.6192 s / batch. (data: 3.13e-04). ETA=17:24:55, max mem: 15.9 GB 
[10/29 21:16:30 visual_prompt]: 	Training 600/1106. train loss: 0.8211,	0.6253 s / batch. (data: 3.56e-04). ETA=17:34:11, max mem: 15.9 GB 
[10/29 21:17:33 visual_prompt]: 	Training 700/1106. train loss: 0.7932,	0.6201 s / batch. (data: 3.39e-04). ETA=17:24:17, max mem: 15.9 GB 
[10/29 21:18:36 visual_prompt]: 	Training 800/1106. train loss: 0.9347,	0.6196 s / batch. (data: 3.12e-04). ETA=17:22:25, max mem: 15.9 GB 
[10/29 21:19:39 visual_prompt]: 	Training 900/1106. train loss: 0.5436,	0.6475 s / batch. (data: 7.69e-04). ETA=18:08:26, max mem: 15.9 GB 
[10/29 21:20:43 visual_prompt]: 	Training 1000/1106. train loss: 0.3299,	0.6451 s / batch. (data: 7.94e-04). ETA=18:03:12, max mem: 15.9 GB 
[10/29 21:21:46 visual_prompt]: 	Training 1100/1106. train loss: 0.8270,	0.6177 s / batch. (data: 1.63e-04). ETA=17:16:14, max mem: 15.9 GB 
[10/29 21:21:50 visual_prompt]: Epoch 9 / 100: avg data time: 4.42e-03, avg batch time: 0.6345, average train loss: 0.9266
[10/29 21:22:39 visual_prompt]: 	Test 100/123. loss: 0.726, 0.2351 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/29 21:22:50 visual_prompt]: Inference (val):avg data time: 5.12e-05, avg batch time: 0.2324, average loss: 0.7602
[10/29 21:22:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.63	
[10/29 21:22:50 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/29 21:23:55 visual_prompt]: 	Training 100/1106. train loss: 0.7262,	0.6173 s / batch. (data: 2.95e-04). ETA=17:14:25, max mem: 15.9 GB 
[10/29 21:24:58 visual_prompt]: 	Training 200/1106. train loss: 3.2501,	0.6192 s / batch. (data: 3.14e-04). ETA=17:16:40, max mem: 15.9 GB 
[10/29 21:26:02 visual_prompt]: 	Training 300/1106. train loss: 1.0311,	0.6283 s / batch. (data: 3.43e-04). ETA=17:30:50, max mem: 15.9 GB 
[10/29 21:27:05 visual_prompt]: 	Training 400/1106. train loss: 1.6007,	0.6275 s / batch. (data: 3.28e-04). ETA=17:28:24, max mem: 15.9 GB 
[10/29 21:28:08 visual_prompt]: 	Training 500/1106. train loss: 0.8316,	0.6509 s / batch. (data: 5.94e-03). ETA=18:06:25, max mem: 15.9 GB 
[10/29 21:29:11 visual_prompt]: 	Training 600/1106. train loss: 1.6392,	0.6481 s / batch. (data: 8.52e-04). ETA=18:00:41, max mem: 15.9 GB 
[10/29 21:30:15 visual_prompt]: 	Training 700/1106. train loss: 0.5234,	0.6333 s / batch. (data: 4.65e-04). ETA=17:34:52, max mem: 15.9 GB 
[10/29 21:31:18 visual_prompt]: 	Training 800/1106. train loss: 0.6954,	0.6480 s / batch. (data: 2.51e-04). ETA=17:58:17, max mem: 15.9 GB 
[10/29 21:32:21 visual_prompt]: 	Training 900/1106. train loss: 0.8215,	0.6312 s / batch. (data: 3.11e-04). ETA=17:29:17, max mem: 15.9 GB 
[10/29 21:33:25 visual_prompt]: 	Training 1000/1106. train loss: 0.1376,	0.6497 s / batch. (data: 8.21e-04). ETA=17:59:00, max mem: 15.9 GB 
[10/29 21:34:28 visual_prompt]: 	Training 1100/1106. train loss: 1.4920,	0.6179 s / batch. (data: 1.52e-04). ETA=17:05:04, max mem: 15.9 GB 
[10/29 21:34:31 visual_prompt]: Epoch 10 / 100: avg data time: 4.05e-03, avg batch time: 0.6341, average train loss: 1.0994
[10/29 21:35:22 visual_prompt]: 	Test 100/123. loss: 0.734, 0.2247 s / batch. (data: 4.22e-05)max mem: 15.94594 GB 
[10/29 21:35:32 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2323, average loss: 0.7635
[10/29 21:35:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.93	
[10/29 21:35:32 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/29 21:36:37 visual_prompt]: 	Training 100/1106. train loss: 2.9686,	0.6373 s / batch. (data: 2.76e-04). ETA=17:36:12, max mem: 15.9 GB 
[10/29 21:37:40 visual_prompt]: 	Training 200/1106. train loss: 0.8715,	0.6499 s / batch. (data: 1.10e-02). ETA=17:55:57, max mem: 15.9 GB 
[10/29 21:38:44 visual_prompt]: 	Training 300/1106. train loss: 1.4283,	0.6240 s / batch. (data: 2.92e-04). ETA=17:12:07, max mem: 15.9 GB 
[10/29 21:39:47 visual_prompt]: 	Training 400/1106. train loss: 1.0258,	0.6187 s / batch. (data: 3.20e-04). ETA=17:02:16, max mem: 15.9 GB 
[10/29 21:40:50 visual_prompt]: 	Training 500/1106. train loss: 0.7741,	0.6177 s / batch. (data: 3.12e-04). ETA=16:59:39, max mem: 15.9 GB 
[10/29 21:41:54 visual_prompt]: 	Training 600/1106. train loss: 0.0049,	0.6451 s / batch. (data: 1.31e-02). ETA=17:43:47, max mem: 15.9 GB 
[10/29 21:42:57 visual_prompt]: 	Training 700/1106. train loss: 1.9236,	0.6193 s / batch. (data: 3.48e-04). ETA=17:00:12, max mem: 15.9 GB 
[10/29 21:44:00 visual_prompt]: 	Training 800/1106. train loss: 1.2588,	0.6666 s / batch. (data: 7.09e-04). ETA=18:16:58, max mem: 15.9 GB 
[10/29 21:45:03 visual_prompt]: 	Training 900/1106. train loss: 1.2148,	0.6388 s / batch. (data: 9.19e-04). ETA=17:30:14, max mem: 15.9 GB 
[10/29 21:46:06 visual_prompt]: 	Training 1000/1106. train loss: 0.4463,	0.6495 s / batch. (data: 7.81e-04). ETA=17:46:41, max mem: 15.9 GB 
[10/29 21:47:09 visual_prompt]: 	Training 1100/1106. train loss: 0.7096,	0.6175 s / batch. (data: 1.76e-04). ETA=16:53:09, max mem: 15.9 GB 
[10/29 21:47:13 visual_prompt]: Epoch 11 / 100: avg data time: 4.23e-03, avg batch time: 0.6341, average train loss: 1.1338
[10/29 21:48:03 visual_prompt]: 	Test 100/123. loss: 0.693, 0.2262 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[10/29 21:48:14 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.2330, average loss: 0.7003
[10/29 21:48:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.84	
[10/29 21:48:14 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/29 21:49:20 visual_prompt]: 	Training 100/1106. train loss: 0.7200,	0.6181 s / batch. (data: 3.31e-04). ETA=16:52:58, max mem: 15.9 GB 
[10/29 21:50:24 visual_prompt]: 	Training 200/1106. train loss: 0.7071,	0.6183 s / batch. (data: 3.27e-04). ETA=16:52:22, max mem: 15.9 GB 
[10/29 21:51:27 visual_prompt]: 	Training 300/1106. train loss: 2.1699,	0.6594 s / batch. (data: 8.22e-04). ETA=17:58:24, max mem: 15.9 GB 
[10/29 21:52:30 visual_prompt]: 	Training 400/1106. train loss: 1.0024,	0.6277 s / batch. (data: 3.17e-04). ETA=17:05:40, max mem: 15.9 GB 
[10/29 21:53:33 visual_prompt]: 	Training 500/1106. train loss: 1.3730,	0.6313 s / batch. (data: 3.25e-04). ETA=17:10:27, max mem: 15.9 GB 
[10/29 21:54:37 visual_prompt]: 	Training 600/1106. train loss: 1.0006,	0.6300 s / batch. (data: 3.35e-04). ETA=17:07:17, max mem: 15.9 GB 
[10/29 21:55:40 visual_prompt]: 	Training 700/1106. train loss: 0.7026,	0.6315 s / batch. (data: 8.48e-04). ETA=17:08:39, max mem: 15.9 GB 
[10/29 21:56:43 visual_prompt]: 	Training 800/1106. train loss: 0.6738,	0.6193 s / batch. (data: 3.03e-04). ETA=16:47:41, max mem: 15.9 GB 
[10/29 21:57:47 visual_prompt]: 	Training 900/1106. train loss: 1.1665,	0.6328 s / batch. (data: 8.56e-04). ETA=17:08:41, max mem: 15.9 GB 
[10/29 21:58:50 visual_prompt]: 	Training 1000/1106. train loss: 0.7956,	0.6297 s / batch. (data: 7.55e-04). ETA=17:02:37, max mem: 15.9 GB 
[10/29 21:59:53 visual_prompt]: 	Training 1100/1106. train loss: 2.0687,	0.6185 s / batch. (data: 1.52e-04). ETA=16:43:21, max mem: 15.9 GB 
[10/29 21:59:57 visual_prompt]: Epoch 12 / 100: avg data time: 5.38e-03, avg batch time: 0.6356, average train loss: 1.0816
[10/29 22:00:47 visual_prompt]: 	Test 100/123. loss: 1.052, 0.2256 s / batch. (data: 4.20e-05)max mem: 15.94594 GB 
[10/29 22:00:57 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.2334, average loss: 1.1809
[10/29 22:00:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.80	
[10/29 22:00:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/29 22:02:03 visual_prompt]: 	Training 100/1106. train loss: 2.6433,	0.6172 s / batch. (data: 3.10e-04). ETA=16:40:13, max mem: 15.9 GB 
[10/29 22:03:06 visual_prompt]: 	Training 200/1106. train loss: 1.0040,	0.6446 s / batch. (data: 3.26e-04). ETA=17:23:33, max mem: 15.9 GB 
[10/29 22:04:09 visual_prompt]: 	Training 300/1106. train loss: 0.1073,	0.6332 s / batch. (data: 7.95e-04). ETA=17:03:57, max mem: 15.9 GB 
[10/29 22:05:12 visual_prompt]: 	Training 400/1106. train loss: 1.4593,	0.6297 s / batch. (data: 7.86e-04). ETA=16:57:16, max mem: 15.9 GB 
[10/29 22:06:15 visual_prompt]: 	Training 500/1106. train loss: 0.6558,	0.6178 s / batch. (data: 3.13e-04). ETA=16:37:03, max mem: 15.9 GB 
[10/29 22:07:18 visual_prompt]: 	Training 600/1106. train loss: 2.2036,	0.6439 s / batch. (data: 8.35e-04). ETA=17:18:03, max mem: 15.9 GB 
[10/29 22:08:22 visual_prompt]: 	Training 700/1106. train loss: 0.8442,	0.6192 s / batch. (data: 7.90e-04). ETA=16:37:13, max mem: 15.9 GB 
[10/29 22:09:25 visual_prompt]: 	Training 800/1106. train loss: 0.7131,	0.6338 s / batch. (data: 7.81e-04). ETA=16:59:38, max mem: 15.9 GB 
[10/29 22:10:28 visual_prompt]: 	Training 900/1106. train loss: 4.7161,	0.6346 s / batch. (data: 8.09e-04). ETA=16:59:54, max mem: 15.9 GB 
[10/29 22:11:32 visual_prompt]: 	Training 1000/1106. train loss: 0.0002,	0.6321 s / batch. (data: 8.18e-04). ETA=16:54:51, max mem: 15.9 GB 
[10/29 22:12:35 visual_prompt]: 	Training 1100/1106. train loss: 1.6744,	0.6186 s / batch. (data: 1.67e-04). ETA=16:32:06, max mem: 15.9 GB 
[10/29 22:12:39 visual_prompt]: Epoch 13 / 100: avg data time: 4.28e-03, avg batch time: 0.6341, average train loss: 1.1531
[10/29 22:13:28 visual_prompt]: 	Test 100/123. loss: 0.691, 0.2325 s / batch. (data: 4.24e-05)max mem: 15.94594 GB 
[10/29 22:13:39 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2336, average loss: 0.6928
[10/29 22:13:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 55.94	
[10/29 22:13:39 visual_prompt]: Best epoch 13: best metric: -0.693
[10/29 22:13:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/29 22:14:45 visual_prompt]: 	Training 100/1106. train loss: 0.0379,	0.6299 s / batch. (data: 3.48e-04). ETA=16:49:03, max mem: 15.9 GB 
[10/29 22:15:48 visual_prompt]: 	Training 200/1106. train loss: 0.9686,	0.6182 s / batch. (data: 3.42e-04). ETA=16:29:25, max mem: 15.9 GB 
[10/29 22:16:51 visual_prompt]: 	Training 300/1106. train loss: 0.8389,	0.6501 s / batch. (data: 8.12e-04). ETA=17:19:22, max mem: 15.9 GB 
[10/29 22:17:54 visual_prompt]: 	Training 400/1106. train loss: 0.0719,	0.6471 s / batch. (data: 7.67e-04). ETA=17:13:23, max mem: 15.9 GB 
[10/29 22:18:58 visual_prompt]: 	Training 500/1106. train loss: 0.7516,	0.6327 s / batch. (data: 9.08e-04). ETA=16:49:21, max mem: 15.9 GB 
[10/29 22:20:01 visual_prompt]: 	Training 600/1106. train loss: 0.7337,	0.6219 s / batch. (data: 3.44e-04). ETA=16:31:09, max mem: 15.9 GB 
[10/29 22:21:05 visual_prompt]: 	Training 700/1106. train loss: 0.8492,	0.6308 s / batch. (data: 7.72e-04). ETA=16:44:16, max mem: 15.9 GB 
[10/29 22:22:08 visual_prompt]: 	Training 800/1106. train loss: 1.3481,	0.6189 s / batch. (data: 3.59e-04). ETA=16:24:12, max mem: 15.9 GB 
[10/29 22:23:11 visual_prompt]: 	Training 900/1106. train loss: 1.3864,	0.6197 s / batch. (data: 4.69e-04). ETA=16:24:33, max mem: 15.9 GB 
[10/29 22:24:14 visual_prompt]: 	Training 1000/1106. train loss: 0.1803,	0.6617 s / batch. (data: 4.49e-02). ETA=17:30:06, max mem: 15.9 GB 
[10/29 22:25:17 visual_prompt]: 	Training 1100/1106. train loss: 0.5635,	0.6185 s / batch. (data: 1.48e-04). ETA=16:20:32, max mem: 15.9 GB 
[10/29 22:25:21 visual_prompt]: Epoch 14 / 100: avg data time: 4.38e-03, avg batch time: 0.6347, average train loss: 1.1283
[10/29 22:26:11 visual_prompt]: 	Test 100/123. loss: 0.760, 0.2397 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[10/29 22:26:22 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.2320, average loss: 0.7278
[10/29 22:26:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.72	
[10/29 22:26:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/29 22:27:26 visual_prompt]: 	Training 100/1106. train loss: 1.9007,	0.6436 s / batch. (data: 8.79e-04). ETA=16:59:13, max mem: 15.9 GB 
[10/29 22:28:30 visual_prompt]: 	Training 200/1106. train loss: 1.0642,	0.6235 s / batch. (data: 7.43e-04). ETA=16:26:21, max mem: 15.9 GB 
[10/29 22:29:33 visual_prompt]: 	Training 300/1106. train loss: 1.3642,	0.6313 s / batch. (data: 8.07e-04). ETA=16:37:37, max mem: 15.9 GB 
[10/29 22:30:36 visual_prompt]: 	Training 400/1106. train loss: 3.9761,	0.6469 s / batch. (data: 8.22e-04). ETA=17:01:08, max mem: 15.9 GB 
[10/29 22:31:39 visual_prompt]: 	Training 500/1106. train loss: 0.8662,	0.6424 s / batch. (data: 8.17e-04). ETA=16:52:59, max mem: 15.9 GB 
[10/29 22:32:42 visual_prompt]: 	Training 600/1106. train loss: 1.1278,	0.6313 s / batch. (data: 3.00e-04). ETA=16:34:25, max mem: 15.9 GB 
[10/29 22:33:45 visual_prompt]: 	Training 700/1106. train loss: 0.9818,	0.6373 s / batch. (data: 1.04e-03). ETA=16:42:55, max mem: 15.9 GB 
[10/29 22:34:49 visual_prompt]: 	Training 800/1106. train loss: 0.7278,	0.6347 s / batch. (data: 1.60e-02). ETA=16:37:40, max mem: 15.9 GB 
[10/29 22:35:52 visual_prompt]: 	Training 900/1106. train loss: 1.6011,	0.6179 s / batch. (data: 3.13e-04). ETA=16:10:18, max mem: 15.9 GB 
[10/29 22:36:55 visual_prompt]: 	Training 1000/1106. train loss: 0.9833,	0.6310 s / batch. (data: 1.68e-03). ETA=16:29:46, max mem: 15.9 GB 
[10/29 22:37:58 visual_prompt]: 	Training 1100/1106. train loss: 1.9685,	0.6186 s / batch. (data: 1.46e-04). ETA=16:09:15, max mem: 15.9 GB 
[10/29 22:38:02 visual_prompt]: Epoch 15 / 100: avg data time: 3.79e-03, avg batch time: 0.6333, average train loss: 1.0641
[10/29 22:38:52 visual_prompt]: 	Test 100/123. loss: 1.195, 0.2257 s / batch. (data: 3.31e-05)max mem: 15.94594 GB 
[10/29 22:39:02 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2319, average loss: 1.3384
[10/29 22:39:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.33	
[10/29 22:39:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/29 22:40:08 visual_prompt]: 	Training 100/1106. train loss: 0.7091,	0.6411 s / batch. (data: 7.82e-04). ETA=16:43:24, max mem: 15.9 GB 
[10/29 22:41:11 visual_prompt]: 	Training 200/1106. train loss: 1.6173,	0.6326 s / batch. (data: 7.98e-04). ETA=16:29:03, max mem: 15.9 GB 
[10/29 22:42:14 visual_prompt]: 	Training 300/1106. train loss: 0.3419,	0.6191 s / batch. (data: 3.57e-04). ETA=16:06:56, max mem: 15.9 GB 
[10/29 22:43:18 visual_prompt]: 	Training 400/1106. train loss: 0.6837,	0.6182 s / batch. (data: 3.19e-04). ETA=16:04:33, max mem: 15.9 GB 
[10/29 22:44:21 visual_prompt]: 	Training 500/1106. train loss: 0.7904,	0.6330 s / batch. (data: 7.95e-04). ETA=16:26:32, max mem: 15.9 GB 
[10/29 22:45:24 visual_prompt]: 	Training 600/1106. train loss: 0.9889,	0.6191 s / batch. (data: 4.81e-04). ETA=16:03:47, max mem: 15.9 GB 
[10/29 22:46:27 visual_prompt]: 	Training 700/1106. train loss: 1.2846,	0.6292 s / batch. (data: 3.10e-04). ETA=16:18:28, max mem: 15.9 GB 
[10/29 22:47:31 visual_prompt]: 	Training 800/1106. train loss: 1.9412,	0.6202 s / batch. (data: 3.96e-04). ETA=16:03:31, max mem: 15.9 GB 
[10/29 22:48:34 visual_prompt]: 	Training 900/1106. train loss: 0.7433,	0.6194 s / batch. (data: 3.53e-04). ETA=16:01:09, max mem: 15.9 GB 
[10/29 22:49:37 visual_prompt]: 	Training 1000/1106. train loss: 0.5034,	0.6191 s / batch. (data: 3.19e-04). ETA=15:59:38, max mem: 15.9 GB 
[10/29 22:50:41 visual_prompt]: 	Training 1100/1106. train loss: 0.1622,	0.6177 s / batch. (data: 1.39e-04). ETA=15:56:28, max mem: 15.9 GB 
[10/29 22:50:44 visual_prompt]: Epoch 16 / 100: avg data time: 3.82e-03, avg batch time: 0.6345, average train loss: 0.9792
[10/29 22:51:34 visual_prompt]: 	Test 100/123. loss: 0.687, 0.2251 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/29 22:51:45 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2312, average loss: 0.7092
[10/29 22:51:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.43	
[10/29 22:51:45 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/29 22:52:50 visual_prompt]: 	Training 100/1106. train loss: 0.7197,	0.6476 s / batch. (data: 2.49e-02). ETA=16:41:38, max mem: 15.9 GB 
[10/29 22:53:53 visual_prompt]: 	Training 200/1106. train loss: 0.8658,	0.6256 s / batch. (data: 2.77e-04). ETA=16:06:31, max mem: 15.9 GB 
[10/29 22:54:57 visual_prompt]: 	Training 300/1106. train loss: 1.4739,	0.6388 s / batch. (data: 1.05e-02). ETA=16:25:59, max mem: 15.9 GB 
[10/29 22:56:00 visual_prompt]: 	Training 400/1106. train loss: 4.9688,	0.6321 s / batch. (data: 7.98e-04). ETA=16:14:28, max mem: 15.9 GB 
[10/29 22:57:03 visual_prompt]: 	Training 500/1106. train loss: 0.0529,	0.6254 s / batch. (data: 3.81e-04). ETA=16:03:13, max mem: 15.9 GB 
[10/29 22:58:06 visual_prompt]: 	Training 600/1106. train loss: 0.8110,	0.6440 s / batch. (data: 1.20e-02). ETA=16:30:42, max mem: 15.9 GB 
[10/29 22:59:10 visual_prompt]: 	Training 700/1106. train loss: 1.0545,	0.6445 s / batch. (data: 8.29e-04). ETA=16:30:22, max mem: 15.9 GB 
[10/29 23:00:13 visual_prompt]: 	Training 800/1106. train loss: 1.1939,	0.6189 s / batch. (data: 2.90e-04). ETA=15:50:04, max mem: 15.9 GB 
[10/29 23:01:16 visual_prompt]: 	Training 900/1106. train loss: 1.4304,	0.6191 s / batch. (data: 2.92e-04). ETA=15:49:23, max mem: 15.9 GB 
[10/29 23:02:20 visual_prompt]: 	Training 1000/1106. train loss: 0.1151,	0.6185 s / batch. (data: 2.90e-04). ETA=15:47:18, max mem: 15.9 GB 
[10/29 23:03:23 visual_prompt]: 	Training 1100/1106. train loss: 0.1267,	0.6174 s / batch. (data: 1.56e-04). ETA=15:44:38, max mem: 15.9 GB 
[10/29 23:03:27 visual_prompt]: Epoch 17 / 100: avg data time: 4.78e-03, avg batch time: 0.6347, average train loss: 1.0626
[10/29 23:04:16 visual_prompt]: 	Test 100/123. loss: 0.863, 0.2253 s / batch. (data: 3.31e-05)max mem: 15.94594 GB 
[10/29 23:04:27 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2325, average loss: 0.9480
[10/29 23:04:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.60	
[10/29 23:04:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/29 23:05:32 visual_prompt]: 	Training 100/1106. train loss: 0.1931,	0.6247 s / batch. (data: 2.98e-04). ETA=15:54:46, max mem: 15.9 GB 
[10/29 23:06:36 visual_prompt]: 	Training 200/1106. train loss: 2.0923,	0.6339 s / batch. (data: 8.22e-04). ETA=16:07:41, max mem: 15.9 GB 
[10/29 23:07:39 visual_prompt]: 	Training 300/1106. train loss: 4.2177,	0.6441 s / batch. (data: 7.54e-04). ETA=16:22:18, max mem: 15.9 GB 
[10/29 23:08:42 visual_prompt]: 	Training 400/1106. train loss: 0.6571,	0.6279 s / batch. (data: 7.69e-04). ETA=15:56:24, max mem: 15.9 GB 
[10/29 23:09:46 visual_prompt]: 	Training 500/1106. train loss: 0.2043,	0.6236 s / batch. (data: 5.47e-03). ETA=15:48:54, max mem: 15.9 GB 
[10/29 23:10:49 visual_prompt]: 	Training 600/1106. train loss: 0.2264,	0.6490 s / batch. (data: 8.49e-04). ETA=16:26:28, max mem: 15.9 GB 
[10/29 23:11:52 visual_prompt]: 	Training 700/1106. train loss: 0.7281,	0.6485 s / batch. (data: 8.19e-04). ETA=16:24:32, max mem: 15.9 GB 
[10/29 23:12:56 visual_prompt]: 	Training 800/1106. train loss: 0.6367,	0.6214 s / batch. (data: 2.83e-04). ETA=15:42:22, max mem: 15.9 GB 
[10/29 23:13:59 visual_prompt]: 	Training 900/1106. train loss: 1.9429,	0.6202 s / batch. (data: 7.17e-04). ETA=15:39:31, max mem: 15.9 GB 
[10/29 23:15:02 visual_prompt]: 	Training 1000/1106. train loss: 0.7746,	0.6266 s / batch. (data: 2.82e-04). ETA=15:48:10, max mem: 15.9 GB 
[10/29 23:16:05 visual_prompt]: 	Training 1100/1106. train loss: 0.1761,	0.6174 s / batch. (data: 1.63e-04). ETA=15:33:19, max mem: 15.9 GB 
[10/29 23:16:09 visual_prompt]: Epoch 18 / 100: avg data time: 4.10e-03, avg batch time: 0.6348, average train loss: 1.0130
[10/29 23:16:59 visual_prompt]: 	Test 100/123. loss: 0.761, 0.2437 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[10/29 23:17:10 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2334, average loss: 0.7275
[10/29 23:17:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.20	
[10/29 23:17:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/29 23:18:15 visual_prompt]: 	Training 100/1106. train loss: 0.7169,	0.6178 s / batch. (data: 2.91e-04). ETA=15:32:52, max mem: 15.9 GB 
[10/29 23:19:18 visual_prompt]: 	Training 200/1106. train loss: 0.9937,	0.6312 s / batch. (data: 1.21e-03). ETA=15:51:57, max mem: 15.9 GB 
[10/29 23:20:21 visual_prompt]: 	Training 300/1106. train loss: 0.3545,	0.6175 s / batch. (data: 2.76e-04). ETA=15:30:17, max mem: 15.9 GB 
[10/29 23:21:25 visual_prompt]: 	Training 400/1106. train loss: 0.8006,	0.6477 s / batch. (data: 1.61e-02). ETA=16:14:43, max mem: 15.9 GB 
[10/29 23:22:28 visual_prompt]: 	Training 500/1106. train loss: 0.1679,	0.6211 s / batch. (data: 2.85e-04). ETA=15:33:35, max mem: 15.9 GB 
[10/29 23:23:31 visual_prompt]: 	Training 600/1106. train loss: 0.4043,	0.6192 s / batch. (data: 3.28e-04). ETA=15:29:45, max mem: 15.9 GB 
[10/29 23:24:34 visual_prompt]: 	Training 700/1106. train loss: 0.7527,	0.6258 s / batch. (data: 2.97e-04). ETA=15:38:39, max mem: 15.9 GB 
[10/29 23:25:38 visual_prompt]: 	Training 800/1106. train loss: 0.2538,	0.6255 s / batch. (data: 1.25e-03). ETA=15:37:06, max mem: 15.9 GB 
[10/29 23:26:41 visual_prompt]: 	Training 900/1106. train loss: 1.5949,	0.6184 s / batch. (data: 3.38e-04). ETA=15:25:27, max mem: 15.9 GB 
[10/29 23:27:44 visual_prompt]: 	Training 1000/1106. train loss: 0.4727,	0.6265 s / batch. (data: 3.21e-04). ETA=15:36:33, max mem: 15.9 GB 
[10/29 23:28:47 visual_prompt]: 	Training 1100/1106. train loss: 2.5196,	0.6251 s / batch. (data: 1.92e-04). ETA=15:33:23, max mem: 15.9 GB 
[10/29 23:28:51 visual_prompt]: Epoch 19 / 100: avg data time: 4.32e-03, avg batch time: 0.6340, average train loss: 0.9817
[10/29 23:29:40 visual_prompt]: 	Test 100/123. loss: 2.982, 0.2425 s / batch. (data: 4.39e-05)max mem: 15.94594 GB 
[10/29 23:29:51 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2335, average loss: 2.6926
[10/29 23:29:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.93	
[10/29 23:29:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/29 23:30:57 visual_prompt]: 	Training 100/1106. train loss: 0.7388,	0.6316 s / batch. (data: 7.61e-04). ETA=15:41:58, max mem: 15.9 GB 
[10/29 23:32:00 visual_prompt]: 	Training 200/1106. train loss: 0.3237,	0.6185 s / batch. (data: 2.95e-04). ETA=15:21:26, max mem: 15.9 GB 
[10/29 23:33:03 visual_prompt]: 	Training 300/1106. train loss: 0.7105,	0.6333 s / batch. (data: 3.35e-04). ETA=15:42:22, max mem: 15.9 GB 
[10/29 23:34:06 visual_prompt]: 	Training 400/1106. train loss: 0.5669,	0.6573 s / batch. (data: 8.21e-04). ETA=16:16:57, max mem: 15.9 GB 
[10/29 23:35:10 visual_prompt]: 	Training 500/1106. train loss: 1.0770,	0.6185 s / batch. (data: 4.81e-04). ETA=15:18:23, max mem: 15.9 GB 
[10/29 23:36:13 visual_prompt]: 	Training 600/1106. train loss: 0.7077,	0.6183 s / batch. (data: 3.36e-04). ETA=15:17:04, max mem: 15.9 GB 
[10/29 23:37:16 visual_prompt]: 	Training 700/1106. train loss: 0.7909,	0.6330 s / batch. (data: 8.45e-04). ETA=15:37:40, max mem: 15.9 GB 
[10/29 23:38:19 visual_prompt]: 	Training 800/1106. train loss: 0.1097,	0.6177 s / batch. (data: 3.16e-04). ETA=15:14:04, max mem: 15.9 GB 
[10/29 23:39:23 visual_prompt]: 	Training 900/1106. train loss: 1.8200,	0.6336 s / batch. (data: 7.96e-04). ETA=15:36:27, max mem: 15.9 GB 
[10/29 23:40:26 visual_prompt]: 	Training 1000/1106. train loss: 0.5270,	0.6342 s / batch. (data: 1.41e-02). ETA=15:36:18, max mem: 15.9 GB 
[10/29 23:41:29 visual_prompt]: 	Training 1100/1106. train loss: 0.2997,	0.6174 s / batch. (data: 1.69e-04). ETA=15:10:34, max mem: 15.9 GB 
[10/29 23:41:33 visual_prompt]: Epoch 20 / 100: avg data time: 4.59e-03, avg batch time: 0.6342, average train loss: 0.9365
[10/29 23:42:23 visual_prompt]: 	Test 100/123. loss: 0.689, 0.2361 s / batch. (data: 3.98e-05)max mem: 15.94594 GB 
[10/29 23:42:33 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2323, average loss: 0.7620
[10/29 23:42:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.12	
[10/29 23:42:33 visual_prompt]: Stopping early.
[10/29 23:42:33 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 23:42:33 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 23:42:33 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 23:42:33 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 23:42:33 visual_prompt]: Training with config:
[10/29 23:42:33 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 23:42:33 visual_prompt]: Loading training data...
[10/29 23:42:33 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 23:42:33 visual_prompt]: Loading validation data...
[10/29 23:42:33 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 23:42:33 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/29 23:42:36 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/29 23:42:36 visual_prompt]: tuned percent:0.522
[10/29 23:42:36 visual_prompt]: Device used for model: 0
[10/29 23:42:36 visual_prompt]: Setting up Evaluator...
[10/29 23:42:36 visual_prompt]: Setting up Trainer...
[10/29 23:42:36 visual_prompt]: 	Setting up the optimizer...
[10/29 23:42:36 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 23:43:42 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6440 s / batch. (data: 1.20e-02). ETA=19:45:59, max mem: 15.9 GB 
[10/29 23:44:45 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6183 s / batch. (data: 2.85e-04). ETA=18:57:35, max mem: 15.9 GB 
[10/29 23:45:48 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6480 s / batch. (data: 9.53e-04). ETA=19:51:12, max mem: 15.9 GB 
[10/29 23:46:51 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6463 s / batch. (data: 7.46e-04). ETA=19:46:57, max mem: 15.9 GB 
[10/29 23:47:55 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6320 s / batch. (data: 2.88e-04). ETA=19:19:45, max mem: 15.9 GB 
[10/29 23:48:58 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6306 s / batch. (data: 3.03e-04). ETA=19:16:09, max mem: 15.9 GB 
[10/29 23:50:02 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6204 s / batch. (data: 3.03e-04). ETA=18:56:18, max mem: 15.9 GB 
[10/29 23:51:05 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6578 s / batch. (data: 1.61e-02). ETA=20:03:41, max mem: 15.9 GB 
[10/29 23:52:08 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6600 s / batch. (data: 8.14e-04). ETA=20:06:37, max mem: 15.9 GB 
[10/29 23:53:12 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6424 s / batch. (data: 8.62e-04). ETA=19:33:25, max mem: 15.9 GB 
[10/29 23:54:15 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6185 s / batch. (data: 1.51e-04). ETA=18:48:45, max mem: 15.9 GB 
[10/29 23:54:19 visual_prompt]: Epoch 1 / 100: avg data time: 4.69e-03, avg batch time: 0.6353, average train loss: 1.4028
[10/29 23:55:09 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2281 s / batch. (data: 3.91e-05)max mem: 15.94594 GB 
[10/29 23:55:19 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2325, average loss: 1.3505
[10/29 23:55:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/29 23:55:19 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/29 23:56:24 visual_prompt]: 	Training 100/1106. train loss: 0.7292,	0.6417 s / batch. (data: 3.74e-04). ETA=19:29:55, max mem: 15.9 GB 
[10/29 23:57:28 visual_prompt]: 	Training 200/1106. train loss: 0.7610,	0.6448 s / batch. (data: 1.00e-03). ETA=19:34:28, max mem: 15.9 GB 
[10/29 23:58:31 visual_prompt]: 	Training 300/1106. train loss: 0.7158,	0.6371 s / batch. (data: 5.41e-03). ETA=19:19:30, max mem: 15.9 GB 
[10/29 23:59:34 visual_prompt]: 	Training 400/1106. train loss: 0.3557,	0.6327 s / batch. (data: 8.24e-04). ETA=19:10:19, max mem: 15.9 GB 
[10/30 00:00:37 visual_prompt]: 	Training 500/1106. train loss: 0.5810,	0.6600 s / batch. (data: 8.26e-04). ETA=19:58:53, max mem: 15.9 GB 
[10/30 00:01:41 visual_prompt]: 	Training 600/1106. train loss: 0.6091,	0.6424 s / batch. (data: 1.10e-02). ETA=19:25:53, max mem: 15.9 GB 
[10/30 00:02:44 visual_prompt]: 	Training 700/1106. train loss: 1.0866,	0.6440 s / batch. (data: 7.85e-04). ETA=19:27:45, max mem: 15.9 GB 
[10/30 00:03:47 visual_prompt]: 	Training 800/1106. train loss: 0.7527,	0.6190 s / batch. (data: 3.27e-04). ETA=18:41:19, max mem: 15.9 GB 
[10/30 00:04:51 visual_prompt]: 	Training 900/1106. train loss: 0.9438,	0.6196 s / batch. (data: 3.36e-04). ETA=18:41:25, max mem: 15.9 GB 
[10/30 00:05:54 visual_prompt]: 	Training 1000/1106. train loss: 0.5298,	0.6333 s / batch. (data: 7.73e-04). ETA=19:05:08, max mem: 15.9 GB 
[10/30 00:06:57 visual_prompt]: 	Training 1100/1106. train loss: 0.4876,	0.6190 s / batch. (data: 1.38e-04). ETA=18:38:11, max mem: 15.9 GB 
[10/30 00:07:01 visual_prompt]: Epoch 2 / 100: avg data time: 3.91e-03, avg batch time: 0.6344, average train loss: 0.8691
[10/30 00:07:51 visual_prompt]: 	Test 100/123. loss: 1.612, 0.2478 s / batch. (data: 4.84e-05)max mem: 15.94594 GB 
[10/30 00:08:02 visual_prompt]: Inference (val):avg data time: 1.30e-04, avg batch time: 0.2326, average loss: 1.4749
[10/30 00:08:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.95	
[10/30 00:08:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/30 00:09:08 visual_prompt]: 	Training 100/1106. train loss: 0.3443,	0.6302 s / batch. (data: 8.04e-04). ETA=18:57:24, max mem: 15.9 GB 
[10/30 00:10:11 visual_prompt]: 	Training 200/1106. train loss: 0.6004,	0.6464 s / batch. (data: 8.11e-04). ETA=19:25:27, max mem: 15.9 GB 
[10/30 00:11:14 visual_prompt]: 	Training 300/1106. train loss: 0.2009,	0.6531 s / batch. (data: 7.25e-04). ETA=19:36:33, max mem: 15.9 GB 
[10/30 00:12:17 visual_prompt]: 	Training 400/1106. train loss: 0.6267,	0.6335 s / batch. (data: 7.84e-04). ETA=19:00:14, max mem: 15.9 GB 
[10/30 00:13:21 visual_prompt]: 	Training 500/1106. train loss: 1.3244,	0.6291 s / batch. (data: 8.98e-04). ETA=18:51:17, max mem: 15.9 GB 
[10/30 00:14:24 visual_prompt]: 	Training 600/1106. train loss: 0.7322,	0.6189 s / batch. (data: 2.91e-04). ETA=18:31:45, max mem: 15.9 GB 
[10/30 00:15:27 visual_prompt]: 	Training 700/1106. train loss: 1.5857,	0.6192 s / batch. (data: 3.07e-04). ETA=18:31:20, max mem: 15.9 GB 
[10/30 00:16:30 visual_prompt]: 	Training 800/1106. train loss: 0.8584,	0.6192 s / batch. (data: 3.02e-04). ETA=18:30:22, max mem: 15.9 GB 
[10/30 00:17:34 visual_prompt]: 	Training 900/1106. train loss: 2.3534,	0.6309 s / batch. (data: 2.90e-04). ETA=18:50:13, max mem: 15.9 GB 
[10/30 00:18:37 visual_prompt]: 	Training 1000/1106. train loss: 0.6734,	0.6347 s / batch. (data: 8.15e-04). ETA=18:56:00, max mem: 15.9 GB 
[10/30 00:19:40 visual_prompt]: 	Training 1100/1106. train loss: 0.6391,	0.6192 s / batch. (data: 1.41e-04). ETA=18:27:11, max mem: 15.9 GB 
[10/30 00:19:44 visual_prompt]: Epoch 3 / 100: avg data time: 5.05e-03, avg batch time: 0.6350, average train loss: 0.9029
[10/30 00:20:34 visual_prompt]: 	Test 100/123. loss: 0.951, 0.2397 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/30 00:20:44 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2338, average loss: 0.9963
[10/30 00:20:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.31	
[10/30 00:20:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/30 00:21:50 visual_prompt]: 	Training 100/1106. train loss: 0.5269,	0.6469 s / batch. (data: 8.13e-04). ETA=19:15:37, max mem: 15.9 GB 
[10/30 00:22:53 visual_prompt]: 	Training 200/1106. train loss: 2.0856,	0.6376 s / batch. (data: 1.35e-02). ETA=18:57:55, max mem: 15.9 GB 
[10/30 00:23:56 visual_prompt]: 	Training 300/1106. train loss: 1.1138,	0.6327 s / batch. (data: 8.02e-04). ETA=18:48:10, max mem: 15.9 GB 
[10/30 00:25:00 visual_prompt]: 	Training 400/1106. train loss: 0.9071,	0.6183 s / batch. (data: 7.81e-04). ETA=18:21:22, max mem: 15.9 GB 
[10/30 00:26:03 visual_prompt]: 	Training 500/1106. train loss: 1.7991,	0.6331 s / batch. (data: 8.13e-04). ETA=18:46:42, max mem: 15.9 GB 
[10/30 00:27:06 visual_prompt]: 	Training 600/1106. train loss: 0.1303,	0.6470 s / batch. (data: 7.99e-04). ETA=19:10:26, max mem: 15.9 GB 
[10/30 00:28:09 visual_prompt]: 	Training 700/1106. train loss: 1.8217,	0.6182 s / batch. (data: 3.03e-04). ETA=18:18:04, max mem: 15.9 GB 
[10/30 00:29:12 visual_prompt]: 	Training 800/1106. train loss: 0.5423,	0.6342 s / batch. (data: 3.46e-04). ETA=18:45:34, max mem: 15.9 GB 
[10/30 00:30:16 visual_prompt]: 	Training 900/1106. train loss: 0.8199,	0.6338 s / batch. (data: 7.97e-04). ETA=18:43:44, max mem: 15.9 GB 
[10/30 00:31:19 visual_prompt]: 	Training 1000/1106. train loss: 2.1389,	0.6521 s / batch. (data: 1.69e-02). ETA=19:15:01, max mem: 15.9 GB 
[10/30 00:32:22 visual_prompt]: 	Training 1100/1106. train loss: 1.8154,	0.6174 s / batch. (data: 1.56e-04). ETA=18:12:34, max mem: 15.9 GB 
[10/30 00:32:26 visual_prompt]: Epoch 4 / 100: avg data time: 4.47e-03, avg batch time: 0.6344, average train loss: 0.9413
[10/30 00:33:16 visual_prompt]: 	Test 100/123. loss: 0.703, 0.2243 s / batch. (data: 3.15e-05)max mem: 15.94594 GB 
[10/30 00:33:27 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2315, average loss: 0.6864
[10/30 00:33:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.77	
[10/30 00:33:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/30 00:34:32 visual_prompt]: 	Training 100/1106. train loss: 1.6084,	0.6171 s / batch. (data: 3.17e-04). ETA=18:10:55, max mem: 15.9 GB 
[10/30 00:35:35 visual_prompt]: 	Training 200/1106. train loss: 0.8231,	0.6400 s / batch. (data: 7.61e-04). ETA=18:50:23, max mem: 15.9 GB 
[10/30 00:36:38 visual_prompt]: 	Training 300/1106. train loss: 0.9178,	0.6238 s / batch. (data: 3.31e-04). ETA=18:20:41, max mem: 15.9 GB 
[10/30 00:37:41 visual_prompt]: 	Training 400/1106. train loss: 1.0749,	0.6298 s / batch. (data: 3.35e-04). ETA=18:30:22, max mem: 15.9 GB 
[10/30 00:38:45 visual_prompt]: 	Training 500/1106. train loss: 0.2648,	0.6339 s / batch. (data: 8.13e-04). ETA=18:36:28, max mem: 15.9 GB 
[10/30 00:39:48 visual_prompt]: 	Training 600/1106. train loss: 1.9025,	0.6460 s / batch. (data: 8.23e-04). ETA=18:56:45, max mem: 15.9 GB 
[10/30 00:40:51 visual_prompt]: 	Training 700/1106. train loss: 0.8172,	0.6250 s / batch. (data: 3.26e-04). ETA=18:18:39, max mem: 15.9 GB 
[10/30 00:41:54 visual_prompt]: 	Training 800/1106. train loss: 1.3836,	0.6176 s / batch. (data: 2.77e-04). ETA=18:04:44, max mem: 15.9 GB 
[10/30 00:42:58 visual_prompt]: 	Training 900/1106. train loss: 0.8703,	0.6401 s / batch. (data: 1.21e-02). ETA=18:43:07, max mem: 15.9 GB 
[10/30 00:44:01 visual_prompt]: 	Training 1000/1106. train loss: 1.3845,	0.6192 s / batch. (data: 3.21e-04). ETA=18:05:21, max mem: 15.9 GB 
[10/30 00:45:04 visual_prompt]: 	Training 1100/1106. train loss: 0.8479,	0.6171 s / batch. (data: 1.87e-04). ETA=18:00:41, max mem: 15.9 GB 
[10/30 00:45:08 visual_prompt]: Epoch 5 / 100: avg data time: 4.14e-03, avg batch time: 0.6341, average train loss: 1.0455
[10/30 00:45:58 visual_prompt]: 	Test 100/123. loss: 1.606, 0.2252 s / batch. (data: 4.15e-05)max mem: 15.94594 GB 
[10/30 00:46:09 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2331, average loss: 1.7110
[10/30 00:46:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.21	
[10/30 00:46:09 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/30 00:47:14 visual_prompt]: 	Training 100/1106. train loss: 1.4673,	0.6322 s / batch. (data: 8.36e-04). ETA=18:26:01, max mem: 15.9 GB 
[10/30 00:48:18 visual_prompt]: 	Training 200/1106. train loss: 0.0212,	0.6429 s / batch. (data: 8.05e-04). ETA=18:43:45, max mem: 15.9 GB 
[10/30 00:49:21 visual_prompt]: 	Training 300/1106. train loss: 0.0194,	0.6188 s / batch. (data: 3.07e-04). ETA=18:00:30, max mem: 15.9 GB 
[10/30 00:50:24 visual_prompt]: 	Training 400/1106. train loss: 1.2145,	0.6187 s / batch. (data: 3.24e-04). ETA=17:59:22, max mem: 15.9 GB 
[10/30 00:51:27 visual_prompt]: 	Training 500/1106. train loss: 0.8448,	0.6460 s / batch. (data: 7.61e-04). ETA=18:45:53, max mem: 15.9 GB 
[10/30 00:52:31 visual_prompt]: 	Training 600/1106. train loss: 0.1944,	0.6404 s / batch. (data: 7.99e-04). ETA=18:35:02, max mem: 15.9 GB 
[10/30 00:53:34 visual_prompt]: 	Training 700/1106. train loss: 1.9358,	0.6724 s / batch. (data: 1.11e-02). ETA=19:29:37, max mem: 15.9 GB 
[10/30 00:54:37 visual_prompt]: 	Training 800/1106. train loss: 1.8033,	0.6438 s / batch. (data: 8.11e-04). ETA=18:38:52, max mem: 15.9 GB 
[10/30 00:55:41 visual_prompt]: 	Training 900/1106. train loss: 1.7638,	0.6320 s / batch. (data: 3.02e-04). ETA=18:17:15, max mem: 15.9 GB 
[10/30 00:56:44 visual_prompt]: 	Training 1000/1106. train loss: 0.7609,	0.6185 s / batch. (data: 3.16e-04). ETA=17:52:49, max mem: 15.9 GB 
[10/30 00:57:47 visual_prompt]: 	Training 1100/1106. train loss: 0.0261,	0.6175 s / batch. (data: 1.52e-04). ETA=17:50:03, max mem: 15.9 GB 
[10/30 00:57:51 visual_prompt]: Epoch 6 / 100: avg data time: 4.16e-03, avg batch time: 0.6345, average train loss: 1.0021
[10/30 00:58:41 visual_prompt]: 	Test 100/123. loss: 0.933, 0.2359 s / batch. (data: 2.74e-05)max mem: 15.94594 GB 
[10/30 00:58:52 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2313, average loss: 1.0091
[10/30 00:58:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[10/30 00:58:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/30 00:59:57 visual_prompt]: 	Training 100/1106. train loss: 1.1306,	0.6187 s / batch. (data: 3.13e-04). ETA=17:51:00, max mem: 15.9 GB 
[10/30 01:01:00 visual_prompt]: 	Training 200/1106. train loss: 1.6051,	0.6331 s / batch. (data: 7.57e-04). ETA=18:14:56, max mem: 15.9 GB 
[10/30 01:02:03 visual_prompt]: 	Training 300/1106. train loss: 0.1130,	0.6299 s / batch. (data: 7.64e-04). ETA=18:08:16, max mem: 15.9 GB 
[10/30 01:03:06 visual_prompt]: 	Training 400/1106. train loss: 0.9694,	0.6375 s / batch. (data: 3.43e-04). ETA=18:20:18, max mem: 15.9 GB 
[10/30 01:04:10 visual_prompt]: 	Training 500/1106. train loss: 2.1011,	0.6291 s / batch. (data: 2.71e-04). ETA=18:04:50, max mem: 15.9 GB 
[10/30 01:05:13 visual_prompt]: 	Training 600/1106. train loss: 2.1447,	0.6372 s / batch. (data: 3.36e-04). ETA=18:17:39, max mem: 15.9 GB 
[10/30 01:06:16 visual_prompt]: 	Training 700/1106. train loss: 2.7702,	0.6600 s / batch. (data: 1.20e-02). ETA=18:55:54, max mem: 15.9 GB 
[10/30 01:07:20 visual_prompt]: 	Training 800/1106. train loss: 0.2288,	0.6304 s / batch. (data: 9.74e-04). ETA=18:03:51, max mem: 15.9 GB 
[10/30 01:08:23 visual_prompt]: 	Training 900/1106. train loss: 0.8393,	0.6336 s / batch. (data: 7.59e-04). ETA=18:08:23, max mem: 15.9 GB 
[10/30 01:09:26 visual_prompt]: 	Training 1000/1106. train loss: 0.7480,	0.6190 s / batch. (data: 3.03e-04). ETA=17:42:09, max mem: 15.9 GB 
[10/30 01:10:30 visual_prompt]: 	Training 1100/1106. train loss: 0.4787,	0.6181 s / batch. (data: 1.47e-04). ETA=17:39:41, max mem: 15.9 GB 
[10/30 01:10:33 visual_prompt]: Epoch 7 / 100: avg data time: 4.05e-03, avg batch time: 0.6343, average train loss: 1.0353
[10/30 01:11:23 visual_prompt]: 	Test 100/123. loss: 0.829, 0.2346 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/30 01:11:34 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2320, average loss: 0.7742
[10/30 01:11:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.61	
[10/30 01:11:34 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/30 01:12:40 visual_prompt]: 	Training 100/1106. train loss: 0.1895,	0.6296 s / batch. (data: 3.00e-04). ETA=17:58:16, max mem: 15.9 GB 
[10/30 01:13:43 visual_prompt]: 	Training 200/1106. train loss: 0.7469,	0.6456 s / batch. (data: 8.32e-04). ETA=18:24:35, max mem: 15.9 GB 
[10/30 01:14:46 visual_prompt]: 	Training 300/1106. train loss: 0.0415,	0.6461 s / batch. (data: 7.75e-04). ETA=18:24:25, max mem: 15.9 GB 
[10/30 01:15:49 visual_prompt]: 	Training 400/1106. train loss: 0.3209,	0.6194 s / batch. (data: 2.57e-04). ETA=17:37:42, max mem: 15.9 GB 
[10/30 01:16:53 visual_prompt]: 	Training 500/1106. train loss: 0.0741,	0.6323 s / batch. (data: 3.29e-04). ETA=17:58:45, max mem: 15.9 GB 
[10/30 01:17:56 visual_prompt]: 	Training 600/1106. train loss: 0.0955,	0.6245 s / batch. (data: 3.47e-04). ETA=17:44:16, max mem: 15.9 GB 
[10/30 01:18:59 visual_prompt]: 	Training 700/1106. train loss: 0.9273,	0.6286 s / batch. (data: 7.82e-04). ETA=17:50:14, max mem: 15.9 GB 
[10/30 01:20:03 visual_prompt]: 	Training 800/1106. train loss: 1.5733,	0.6376 s / batch. (data: 7.93e-04). ETA=18:04:29, max mem: 15.9 GB 
[10/30 01:21:06 visual_prompt]: 	Training 900/1106. train loss: 0.0253,	0.6402 s / batch. (data: 3.10e-04). ETA=18:07:53, max mem: 15.9 GB 
[10/30 01:22:09 visual_prompt]: 	Training 1000/1106. train loss: 2.2040,	0.6360 s / batch. (data: 8.19e-04). ETA=17:59:37, max mem: 15.9 GB 
[10/30 01:23:13 visual_prompt]: 	Training 1100/1106. train loss: 1.0819,	0.6188 s / batch. (data: 1.49e-04). ETA=17:29:26, max mem: 15.9 GB 
[10/30 01:23:16 visual_prompt]: Epoch 8 / 100: avg data time: 4.21e-03, avg batch time: 0.6347, average train loss: 1.1373
[10/30 01:24:06 visual_prompt]: 	Test 100/123. loss: 0.761, 0.2436 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/30 01:24:17 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2348, average loss: 0.7150
[10/30 01:24:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.63	
[10/30 01:24:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/30 01:25:23 visual_prompt]: 	Training 100/1106. train loss: 3.0230,	0.6184 s / batch. (data: 2.99e-04). ETA=17:27:44, max mem: 15.9 GB 
[10/30 01:26:26 visual_prompt]: 	Training 200/1106. train loss: 0.7277,	0.6246 s / batch. (data: 4.46e-04). ETA=17:37:07, max mem: 15.9 GB 
[10/30 01:27:30 visual_prompt]: 	Training 300/1106. train loss: 1.0579,	0.6325 s / batch. (data: 8.49e-04). ETA=17:49:31, max mem: 15.9 GB 
[10/30 01:28:33 visual_prompt]: 	Training 400/1106. train loss: 1.0123,	0.6318 s / batch. (data: 7.98e-04). ETA=17:47:14, max mem: 15.9 GB 
[10/30 01:29:36 visual_prompt]: 	Training 500/1106. train loss: 1.9255,	0.6668 s / batch. (data: 1.11e-02). ETA=18:45:18, max mem: 15.9 GB 
[10/30 01:30:40 visual_prompt]: 	Training 600/1106. train loss: 0.7624,	0.6583 s / batch. (data: 8.38e-04). ETA=18:29:50, max mem: 15.9 GB 
[10/30 01:31:43 visual_prompt]: 	Training 700/1106. train loss: 1.1181,	0.6200 s / batch. (data: 3.35e-04). ETA=17:24:11, max mem: 15.9 GB 
[10/30 01:32:47 visual_prompt]: 	Training 800/1106. train loss: 1.2875,	0.6212 s / batch. (data: 2.85e-04). ETA=17:25:13, max mem: 15.9 GB 
[10/30 01:33:50 visual_prompt]: 	Training 900/1106. train loss: 0.3925,	0.6322 s / batch. (data: 8.08e-04). ETA=17:42:38, max mem: 15.9 GB 
[10/30 01:34:53 visual_prompt]: 	Training 1000/1106. train loss: 1.0218,	0.6321 s / batch. (data: 7.54e-04). ETA=17:41:23, max mem: 15.9 GB 
[10/30 01:35:56 visual_prompt]: 	Training 1100/1106. train loss: 1.0420,	0.6183 s / batch. (data: 1.56e-04). ETA=17:17:08, max mem: 15.9 GB 
[10/30 01:36:00 visual_prompt]: Epoch 9 / 100: avg data time: 4.85e-03, avg batch time: 0.6354, average train loss: 1.0006
[10/30 01:36:51 visual_prompt]: 	Test 100/123. loss: 0.827, 0.2248 s / batch. (data: 4.84e-05)max mem: 15.94594 GB 
[10/30 01:37:01 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2327, average loss: 0.8450
[10/30 01:37:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.95	
[10/30 01:37:01 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/30 01:38:06 visual_prompt]: 	Training 100/1106. train loss: 0.7964,	0.6174 s / batch. (data: 3.19e-04). ETA=17:14:40, max mem: 15.9 GB 
[10/30 01:39:09 visual_prompt]: 	Training 200/1106. train loss: 3.8895,	0.6188 s / batch. (data: 3.22e-04). ETA=17:15:56, max mem: 15.9 GB 
[10/30 01:40:12 visual_prompt]: 	Training 300/1106. train loss: 1.6449,	0.6372 s / batch. (data: 9.20e-03). ETA=17:45:37, max mem: 15.9 GB 
[10/30 01:41:16 visual_prompt]: 	Training 400/1106. train loss: 1.6271,	0.6247 s / batch. (data: 3.43e-04). ETA=17:23:46, max mem: 15.9 GB 
[10/30 01:42:19 visual_prompt]: 	Training 500/1106. train loss: 0.9588,	0.6323 s / batch. (data: 3.17e-04). ETA=17:35:25, max mem: 15.9 GB 
[10/30 01:43:22 visual_prompt]: 	Training 600/1106. train loss: 2.2820,	0.6317 s / batch. (data: 8.20e-04). ETA=17:33:18, max mem: 15.9 GB 
[10/30 01:44:25 visual_prompt]: 	Training 700/1106. train loss: 0.5062,	0.6194 s / batch. (data: 3.13e-04). ETA=17:11:44, max mem: 15.9 GB 
[10/30 01:45:29 visual_prompt]: 	Training 800/1106. train loss: 0.8259,	0.6312 s / batch. (data: 7.98e-04). ETA=17:30:26, max mem: 15.9 GB 
[10/30 01:46:32 visual_prompt]: 	Training 900/1106. train loss: 1.3868,	0.6271 s / batch. (data: 3.25e-04). ETA=17:22:31, max mem: 15.9 GB 
[10/30 01:47:35 visual_prompt]: 	Training 1000/1106. train loss: 0.0593,	0.6402 s / batch. (data: 8.26e-04). ETA=17:43:18, max mem: 15.9 GB 
[10/30 01:48:39 visual_prompt]: 	Training 1100/1106. train loss: 1.5972,	0.6172 s / batch. (data: 1.61e-04). ETA=17:03:59, max mem: 15.9 GB 
[10/30 01:48:42 visual_prompt]: Epoch 10 / 100: avg data time: 4.12e-03, avg batch time: 0.6343, average train loss: 1.2090
[10/30 01:49:33 visual_prompt]: 	Test 100/123. loss: 0.729, 0.2599 s / batch. (data: 3.89e-05)max mem: 15.94594 GB 
[10/30 01:49:43 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2328, average loss: 0.7572
[10/30 01:49:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.61	
[10/30 01:49:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/30 01:50:48 visual_prompt]: 	Training 100/1106. train loss: 4.4278,	0.6185 s / batch. (data: 3.37e-04). ETA=17:05:04, max mem: 15.9 GB 
[10/30 01:51:52 visual_prompt]: 	Training 200/1106. train loss: 0.7302,	0.6292 s / batch. (data: 3.15e-04). ETA=17:21:42, max mem: 15.9 GB 
[10/30 01:52:55 visual_prompt]: 	Training 300/1106. train loss: 0.7509,	0.6379 s / batch. (data: 8.08e-04). ETA=17:35:05, max mem: 15.9 GB 
[10/30 01:53:59 visual_prompt]: 	Training 400/1106. train loss: 0.7802,	0.6334 s / batch. (data: 8.07e-04). ETA=17:26:35, max mem: 15.9 GB 
[10/30 01:55:02 visual_prompt]: 	Training 500/1106. train loss: 1.2296,	0.6189 s / batch. (data: 2.80e-04). ETA=17:01:31, max mem: 15.9 GB 
[10/30 01:56:05 visual_prompt]: 	Training 600/1106. train loss: 0.0206,	0.6486 s / batch. (data: 7.99e-04). ETA=17:49:34, max mem: 15.9 GB 
[10/30 01:57:08 visual_prompt]: 	Training 700/1106. train loss: 0.2148,	0.6253 s / batch. (data: 6.34e-03). ETA=17:10:09, max mem: 15.9 GB 
[10/30 01:58:12 visual_prompt]: 	Training 800/1106. train loss: 1.8310,	0.6440 s / batch. (data: 9.59e-04). ETA=17:39:43, max mem: 15.9 GB 
[10/30 01:59:15 visual_prompt]: 	Training 900/1106. train loss: 1.2426,	0.6338 s / batch. (data: 8.15e-04). ETA=17:22:02, max mem: 15.9 GB 
[10/30 02:00:18 visual_prompt]: 	Training 1000/1106. train loss: 0.6859,	0.6197 s / batch. (data: 3.34e-04). ETA=16:57:42, max mem: 15.9 GB 
[10/30 02:01:21 visual_prompt]: 	Training 1100/1106. train loss: 0.7974,	0.6194 s / batch. (data: 1.94e-04). ETA=16:56:10, max mem: 15.9 GB 
[10/30 02:01:25 visual_prompt]: Epoch 11 / 100: avg data time: 4.22e-03, avg batch time: 0.6348, average train loss: 1.0856
[10/30 02:02:15 visual_prompt]: 	Test 100/123. loss: 0.675, 0.2247 s / batch. (data: 4.79e-05)max mem: 15.94594 GB 
[10/30 02:02:26 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2317, average loss: 0.6717
[10/30 02:02:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 59.61	
[10/30 02:02:26 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/30 02:03:32 visual_prompt]: 	Training 100/1106. train loss: 0.4592,	0.6447 s / batch. (data: 1.13e-03). ETA=17:36:40, max mem: 15.9 GB 
[10/30 02:04:35 visual_prompt]: 	Training 200/1106. train loss: 0.6455,	0.6336 s / batch. (data: 8.25e-04). ETA=17:17:24, max mem: 15.9 GB 
[10/30 02:05:38 visual_prompt]: 	Training 300/1106. train loss: 2.0134,	0.6192 s / batch. (data: 2.99e-04). ETA=16:52:42, max mem: 15.9 GB 
[10/30 02:06:42 visual_prompt]: 	Training 400/1106. train loss: 1.2455,	0.6307 s / batch. (data: 3.45e-04). ETA=17:10:31, max mem: 15.9 GB 
[10/30 02:07:45 visual_prompt]: 	Training 500/1106. train loss: 1.0988,	0.6416 s / batch. (data: 7.67e-04). ETA=17:27:10, max mem: 15.9 GB 
[10/30 02:08:48 visual_prompt]: 	Training 600/1106. train loss: 0.9119,	0.6328 s / batch. (data: 8.13e-04). ETA=17:11:51, max mem: 15.9 GB 
[10/30 02:09:51 visual_prompt]: 	Training 700/1106. train loss: 0.6741,	0.6457 s / batch. (data: 7.55e-04). ETA=17:31:43, max mem: 15.9 GB 
[10/30 02:10:54 visual_prompt]: 	Training 800/1106. train loss: 0.6527,	0.6188 s / batch. (data: 3.04e-04). ETA=16:46:52, max mem: 15.9 GB 
[10/30 02:11:58 visual_prompt]: 	Training 900/1106. train loss: 0.9362,	0.6443 s / batch. (data: 8.18e-04). ETA=17:27:17, max mem: 15.9 GB 
[10/30 02:13:01 visual_prompt]: 	Training 1000/1106. train loss: 0.8417,	0.6318 s / batch. (data: 8.22e-04). ETA=17:05:57, max mem: 15.9 GB 
[10/30 02:14:04 visual_prompt]: 	Training 1100/1106. train loss: 2.3923,	0.6175 s / batch. (data: 1.55e-04). ETA=16:41:42, max mem: 15.9 GB 
[10/30 02:14:08 visual_prompt]: Epoch 12 / 100: avg data time: 5.03e-03, avg batch time: 0.6348, average train loss: 1.0595
[10/30 02:14:57 visual_prompt]: 	Test 100/123. loss: 1.004, 0.2257 s / batch. (data: 3.65e-05)max mem: 15.94594 GB 
[10/30 02:15:08 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.2328, average loss: 1.1090
[10/30 02:15:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.85	
[10/30 02:15:08 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/30 02:16:14 visual_prompt]: 	Training 100/1106. train loss: 3.4379,	0.6189 s / batch. (data: 3.38e-04). ETA=16:42:50, max mem: 15.9 GB 
[10/30 02:17:17 visual_prompt]: 	Training 200/1106. train loss: 1.3422,	0.6313 s / batch. (data: 7.54e-04). ETA=17:01:56, max mem: 15.9 GB 
[10/30 02:18:21 visual_prompt]: 	Training 300/1106. train loss: 0.1531,	0.6300 s / batch. (data: 8.13e-04). ETA=16:58:47, max mem: 15.9 GB 
[10/30 02:19:24 visual_prompt]: 	Training 400/1106. train loss: 1.7649,	0.6287 s / batch. (data: 3.21e-04). ETA=16:55:34, max mem: 15.9 GB 
[10/30 02:20:27 visual_prompt]: 	Training 500/1106. train loss: 0.6568,	0.6301 s / batch. (data: 7.52e-04). ETA=16:56:53, max mem: 15.9 GB 
[10/30 02:21:31 visual_prompt]: 	Training 600/1106. train loss: 2.1279,	0.6210 s / batch. (data: 3.24e-04). ETA=16:41:06, max mem: 15.9 GB 
[10/30 02:22:34 visual_prompt]: 	Training 700/1106. train loss: 1.3514,	0.6370 s / batch. (data: 5.40e-03). ETA=17:05:53, max mem: 15.9 GB 
[10/30 02:23:37 visual_prompt]: 	Training 800/1106. train loss: 1.0673,	0.6236 s / batch. (data: 3.78e-04). ETA=16:43:10, max mem: 15.9 GB 
[10/30 02:24:41 visual_prompt]: 	Training 900/1106. train loss: 2.6892,	0.6450 s / batch. (data: 8.09e-04). ETA=17:16:32, max mem: 15.9 GB 
[10/30 02:25:44 visual_prompt]: 	Training 1000/1106. train loss: 0.0181,	0.6318 s / batch. (data: 8.04e-04). ETA=16:54:18, max mem: 15.9 GB 
[10/30 02:26:48 visual_prompt]: 	Training 1100/1106. train loss: 1.8314,	0.6188 s / batch. (data: 1.98e-04). ETA=16:32:25, max mem: 15.9 GB 
[10/30 02:26:51 visual_prompt]: Epoch 13 / 100: avg data time: 4.39e-03, avg batch time: 0.6355, average train loss: 1.0470
[10/30 02:27:41 visual_prompt]: 	Test 100/123. loss: 0.859, 0.2251 s / batch. (data: 2.67e-05)max mem: 15.94594 GB 
[10/30 02:27:52 visual_prompt]: Inference (val):avg data time: 1.02e-04, avg batch time: 0.2326, average loss: 0.9667
[10/30 02:27:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.26	
[10/30 02:27:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/30 02:28:58 visual_prompt]: 	Training 100/1106. train loss: 0.2061,	0.6362 s / batch. (data: 7.91e-04). ETA=16:59:12, max mem: 15.9 GB 
[10/30 02:30:01 visual_prompt]: 	Training 200/1106. train loss: 1.3894,	0.6185 s / batch. (data: 3.37e-04). ETA=16:29:48, max mem: 15.9 GB 
[10/30 02:31:05 visual_prompt]: 	Training 300/1106. train loss: 0.9230,	0.6183 s / batch. (data: 3.25e-04). ETA=16:28:25, max mem: 15.9 GB 
[10/30 02:32:08 visual_prompt]: 	Training 400/1106. train loss: 0.1379,	0.6336 s / batch. (data: 3.72e-04). ETA=16:51:49, max mem: 15.9 GB 
[10/30 02:33:11 visual_prompt]: 	Training 500/1106. train loss: 0.9897,	0.6177 s / batch. (data: 3.16e-04). ETA=16:25:27, max mem: 15.9 GB 
[10/30 02:34:14 visual_prompt]: 	Training 600/1106. train loss: 0.9874,	0.6248 s / batch. (data: 3.14e-04). ETA=16:35:46, max mem: 15.9 GB 
[10/30 02:35:18 visual_prompt]: 	Training 700/1106. train loss: 0.9307,	0.6360 s / batch. (data: 3.20e-04). ETA=16:52:30, max mem: 15.9 GB 
[10/30 02:36:21 visual_prompt]: 	Training 800/1106. train loss: 1.2633,	0.6336 s / batch. (data: 3.18e-04). ETA=16:47:37, max mem: 15.9 GB 
[10/30 02:37:24 visual_prompt]: 	Training 900/1106. train loss: 1.6709,	0.6194 s / batch. (data: 3.16e-04). ETA=16:24:01, max mem: 15.9 GB 
[10/30 02:38:27 visual_prompt]: 	Training 1000/1106. train loss: 0.1365,	0.6332 s / batch. (data: 7.90e-04). ETA=16:44:56, max mem: 15.9 GB 
[10/30 02:39:30 visual_prompt]: 	Training 1100/1106. train loss: 0.8349,	0.6175 s / batch. (data: 1.61e-04). ETA=16:18:56, max mem: 15.9 GB 
[10/30 02:39:34 visual_prompt]: Epoch 14 / 100: avg data time: 5.11e-03, avg batch time: 0.6346, average train loss: 0.9914
[10/30 02:40:24 visual_prompt]: 	Test 100/123. loss: 0.683, 0.2356 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[10/30 02:40:35 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.2323, average loss: 0.6840
[10/30 02:40:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.52	
[10/30 02:40:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/30 02:41:40 visual_prompt]: 	Training 100/1106. train loss: 2.2907,	0.6176 s / batch. (data: 3.36e-04). ETA=16:18:01, max mem: 15.9 GB 
[10/30 02:42:43 visual_prompt]: 	Training 200/1106. train loss: 1.2505,	0.6182 s / batch. (data: 2.76e-04). ETA=16:18:01, max mem: 15.9 GB 
[10/30 02:43:46 visual_prompt]: 	Training 300/1106. train loss: 1.2450,	0.6193 s / batch. (data: 3.12e-04). ETA=16:18:39, max mem: 15.9 GB 
[10/30 02:44:49 visual_prompt]: 	Training 400/1106. train loss: 4.0939,	0.6324 s / batch. (data: 3.26e-04). ETA=16:38:22, max mem: 15.9 GB 
[10/30 02:45:53 visual_prompt]: 	Training 500/1106. train loss: 0.7944,	0.6369 s / batch. (data: 3.11e-04). ETA=16:44:22, max mem: 15.9 GB 
[10/30 02:46:56 visual_prompt]: 	Training 600/1106. train loss: 0.9090,	0.6194 s / batch. (data: 7.67e-04). ETA=16:15:44, max mem: 15.9 GB 
[10/30 02:47:59 visual_prompt]: 	Training 700/1106. train loss: 1.1549,	0.6415 s / batch. (data: 7.34e-04). ETA=16:49:26, max mem: 15.9 GB 
[10/30 02:49:02 visual_prompt]: 	Training 800/1106. train loss: 0.6298,	0.6210 s / batch. (data: 3.29e-04). ETA=16:16:13, max mem: 15.9 GB 
[10/30 02:50:06 visual_prompt]: 	Training 900/1106. train loss: 0.9212,	0.6385 s / batch. (data: 1.47e-02). ETA=16:42:35, max mem: 15.9 GB 
[10/30 02:51:09 visual_prompt]: 	Training 1000/1106. train loss: 0.8899,	0.6308 s / batch. (data: 2.72e-04). ETA=16:29:23, max mem: 15.9 GB 
[10/30 02:52:12 visual_prompt]: 	Training 1100/1106. train loss: 1.9882,	0.6177 s / batch. (data: 1.42e-04). ETA=16:07:51, max mem: 15.9 GB 
[10/30 02:52:16 visual_prompt]: Epoch 15 / 100: avg data time: 3.66e-03, avg batch time: 0.6341, average train loss: 1.1086
[10/30 02:53:07 visual_prompt]: 	Test 100/123. loss: 1.396, 0.2252 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/30 02:53:17 visual_prompt]: Inference (val):avg data time: 9.68e-05, avg batch time: 0.2322, average loss: 1.5470
[10/30 02:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.54	
[10/30 02:53:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/30 02:54:22 visual_prompt]: 	Training 100/1106. train loss: 0.4090,	0.6380 s / batch. (data: 8.31e-04). ETA=16:38:33, max mem: 15.9 GB 
[10/30 02:55:26 visual_prompt]: 	Training 200/1106. train loss: 1.2700,	0.6197 s / batch. (data: 3.15e-04). ETA=16:08:57, max mem: 15.9 GB 
[10/30 02:56:29 visual_prompt]: 	Training 300/1106. train loss: 0.7057,	0.6200 s / batch. (data: 3.56e-04). ETA=16:08:15, max mem: 15.9 GB 
[10/30 02:57:32 visual_prompt]: 	Training 400/1106. train loss: 0.7800,	0.6306 s / batch. (data: 8.15e-04). ETA=16:23:50, max mem: 15.9 GB 
[10/30 02:58:35 visual_prompt]: 	Training 500/1106. train loss: 0.9253,	0.6351 s / batch. (data: 3.45e-04). ETA=16:29:47, max mem: 15.9 GB 
[10/30 02:59:39 visual_prompt]: 	Training 600/1106. train loss: 0.9264,	0.6193 s / batch. (data: 3.22e-04). ETA=16:04:05, max mem: 15.9 GB 
[10/30 03:00:42 visual_prompt]: 	Training 700/1106. train loss: 2.3162,	0.6197 s / batch. (data: 3.29e-04). ETA=16:03:45, max mem: 15.9 GB 
[10/30 03:01:45 visual_prompt]: 	Training 800/1106. train loss: 2.2376,	0.6478 s / batch. (data: 1.53e-02). ETA=16:46:18, max mem: 15.9 GB 
[10/30 03:02:49 visual_prompt]: 	Training 900/1106. train loss: 0.5789,	0.6344 s / batch. (data: 8.25e-04). ETA=16:24:29, max mem: 15.9 GB 
[10/30 03:03:52 visual_prompt]: 	Training 1000/1106. train loss: 0.7900,	0.6200 s / batch. (data: 3.21e-04). ETA=16:01:02, max mem: 15.9 GB 
[10/30 03:04:55 visual_prompt]: 	Training 1100/1106. train loss: 0.0719,	0.6177 s / batch. (data: 2.95e-04). ETA=15:56:31, max mem: 15.9 GB 
[10/30 03:04:59 visual_prompt]: Epoch 16 / 100: avg data time: 3.94e-03, avg batch time: 0.6348, average train loss: 1.0027
[10/30 03:05:49 visual_prompt]: 	Test 100/123. loss: 0.741, 0.2277 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/30 03:06:00 visual_prompt]: Inference (val):avg data time: 1.31e-04, avg batch time: 0.2325, average loss: 0.7648
[10/30 03:06:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.62	
[10/30 03:06:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/30 03:07:06 visual_prompt]: 	Training 100/1106. train loss: 0.8502,	0.6482 s / batch. (data: 1.00e-02). ETA=16:42:35, max mem: 15.9 GB 
[10/30 03:08:09 visual_prompt]: 	Training 200/1106. train loss: 0.7281,	0.6186 s / batch. (data: 3.00e-04). ETA=15:55:44, max mem: 15.9 GB 
[10/30 03:09:12 visual_prompt]: 	Training 300/1106. train loss: 1.2534,	0.6181 s / batch. (data: 3.14e-04). ETA=15:53:57, max mem: 15.9 GB 
[10/30 03:10:15 visual_prompt]: 	Training 400/1106. train loss: 6.0134,	0.6319 s / batch. (data: 8.03e-04). ETA=16:14:13, max mem: 15.9 GB 
[10/30 03:11:19 visual_prompt]: 	Training 500/1106. train loss: 0.0433,	0.6310 s / batch. (data: 4.81e-04). ETA=16:11:42, max mem: 15.9 GB 
[10/30 03:12:22 visual_prompt]: 	Training 600/1106. train loss: 0.7600,	0.6353 s / batch. (data: 7.69e-04). ETA=16:17:16, max mem: 15.9 GB 
[10/30 03:13:25 visual_prompt]: 	Training 700/1106. train loss: 1.1790,	0.6329 s / batch. (data: 8.02e-04). ETA=16:12:32, max mem: 15.9 GB 
[10/30 03:14:29 visual_prompt]: 	Training 800/1106. train loss: 1.2343,	0.6488 s / batch. (data: 1.41e-02). ETA=16:35:54, max mem: 15.9 GB 
[10/30 03:15:32 visual_prompt]: 	Training 900/1106. train loss: 1.8057,	0.6600 s / batch. (data: 2.80e-02). ETA=16:51:58, max mem: 15.9 GB 
[10/30 03:16:35 visual_prompt]: 	Training 1000/1106. train loss: 0.1437,	0.6304 s / batch. (data: 3.60e-04). ETA=16:05:39, max mem: 15.9 GB 
[10/30 03:17:39 visual_prompt]: 	Training 1100/1106. train loss: 0.0503,	0.6181 s / batch. (data: 1.32e-04). ETA=15:45:40, max mem: 15.9 GB 
[10/30 03:17:43 visual_prompt]: Epoch 17 / 100: avg data time: 4.41e-03, avg batch time: 0.6351, average train loss: 1.0966
[10/30 03:18:33 visual_prompt]: 	Test 100/123. loss: 0.919, 0.2252 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/30 03:18:43 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.2326, average loss: 1.0258
[10/30 03:18:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.22	
[10/30 03:18:43 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/30 03:19:48 visual_prompt]: 	Training 100/1106. train loss: 0.2513,	0.6181 s / batch. (data: 3.31e-04). ETA=15:44:34, max mem: 15.9 GB 
[10/30 03:20:51 visual_prompt]: 	Training 200/1106. train loss: 2.3973,	0.6406 s / batch. (data: 8.22e-04). ETA=16:17:56, max mem: 15.9 GB 
[10/30 03:21:55 visual_prompt]: 	Training 300/1106. train loss: 5.2892,	0.6174 s / batch. (data: 2.77e-04). ETA=15:41:35, max mem: 15.9 GB 
[10/30 03:22:58 visual_prompt]: 	Training 400/1106. train loss: 0.8150,	0.6240 s / batch. (data: 4.28e-04). ETA=15:50:36, max mem: 15.9 GB 
[10/30 03:24:01 visual_prompt]: 	Training 500/1106. train loss: 0.0108,	0.6352 s / batch. (data: 3.18e-04). ETA=16:06:30, max mem: 15.9 GB 
[10/30 03:25:05 visual_prompt]: 	Training 600/1106. train loss: 0.2571,	0.6368 s / batch. (data: 1.24e-03). ETA=16:07:50, max mem: 15.9 GB 
[10/30 03:26:08 visual_prompt]: 	Training 700/1106. train loss: 0.3797,	0.6332 s / batch. (data: 5.44e-03). ETA=16:01:25, max mem: 15.9 GB 
[10/30 03:27:11 visual_prompt]: 	Training 800/1106. train loss: 0.5912,	0.6185 s / batch. (data: 3.34e-04). ETA=15:38:00, max mem: 15.9 GB 
[10/30 03:28:14 visual_prompt]: 	Training 900/1106. train loss: 2.4390,	0.6313 s / batch. (data: 4.54e-04). ETA=15:56:27, max mem: 15.9 GB 
[10/30 03:29:17 visual_prompt]: 	Training 1000/1106. train loss: 0.7138,	0.6183 s / batch. (data: 3.32e-04). ETA=15:35:36, max mem: 15.9 GB 
[10/30 03:30:21 visual_prompt]: 	Training 1100/1106. train loss: 0.0913,	0.6189 s / batch. (data: 1.99e-04). ETA=15:35:28, max mem: 15.9 GB 
[10/30 03:30:24 visual_prompt]: Epoch 18 / 100: avg data time: 4.42e-03, avg batch time: 0.6341, average train loss: 1.1093
[10/30 03:31:15 visual_prompt]: 	Test 100/123. loss: 0.674, 0.2317 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[10/30 03:31:25 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2326, average loss: 0.7012
[10/30 03:31:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.70	
[10/30 03:31:25 visual_prompt]: Stopping early.
[10/30 03:31:25 visual_prompt]: Rank of current process: 0. World size: 1
[10/30 03:31:25 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/30 03:31:25 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/30 03:31:25 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/30 03:31:25 visual_prompt]: Training with config:
[10/30 03:31:25 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.1_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/30 03:31:25 visual_prompt]: Loading training data...
[10/30 03:31:25 visual_prompt]: Constructing mammo-cbis dataset train...
[10/30 03:31:25 visual_prompt]: Loading validation data...
[10/30 03:31:25 visual_prompt]: Constructing mammo-cbis dataset val...
[10/30 03:31:25 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/30 03:31:28 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/30 03:31:28 visual_prompt]: tuned percent:0.522
[10/30 03:31:28 visual_prompt]: Device used for model: 0
[10/30 03:31:28 visual_prompt]: Setting up Evaluator...
[10/30 03:31:28 visual_prompt]: Setting up Trainer...
[10/30 03:31:28 visual_prompt]: 	Setting up the optimizer...
[10/30 03:31:28 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/30 03:32:34 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6294 s / batch. (data: 3.16e-04). ETA=19:19:08, max mem: 15.9 GB 
[10/30 03:33:37 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6177 s / batch. (data: 3.11e-04). ETA=18:56:31, max mem: 15.9 GB 
[10/30 03:34:40 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6390 s / batch. (data: 8.59e-04). ETA=19:34:40, max mem: 15.9 GB 
[10/30 03:35:43 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6323 s / batch. (data: 3.43e-04). ETA=19:21:24, max mem: 15.9 GB 
[10/30 03:36:47 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6244 s / batch. (data: 5.48e-03). ETA=19:05:46, max mem: 15.9 GB 
[10/30 03:37:50 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6186 s / batch. (data: 3.17e-04). ETA=18:54:05, max mem: 15.9 GB 
[10/30 03:38:53 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6362 s / batch. (data: 8.40e-04). ETA=19:25:20, max mem: 15.9 GB 
[10/30 03:39:57 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6450 s / batch. (data: 1.10e-02). ETA=19:40:16, max mem: 15.9 GB 
[10/30 03:41:00 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6680 s / batch. (data: 7.91e-04). ETA=20:21:20, max mem: 15.9 GB 
[10/30 03:42:03 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6273 s / batch. (data: 3.28e-04). ETA=19:05:56, max mem: 15.9 GB 
[10/30 03:43:06 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6184 s / batch. (data: 1.55e-04). ETA=18:48:37, max mem: 15.9 GB 
[10/30 03:43:10 visual_prompt]: Epoch 1 / 100: avg data time: 4.52e-03, avg batch time: 0.6349, average train loss: 1.4028
[10/30 03:44:00 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2251 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[10/30 03:44:10 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.2334, average loss: 1.3505
[10/30 03:44:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/30 03:44:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/30 03:45:15 visual_prompt]: 	Training 100/1106. train loss: 0.6931,	0.6306 s / batch. (data: 1.20e-02). ETA=19:09:44, max mem: 15.9 GB 
[10/30 03:46:19 visual_prompt]: 	Training 200/1106. train loss: 0.7104,	0.6320 s / batch. (data: 8.82e-04). ETA=19:11:09, max mem: 15.9 GB 
[10/30 03:47:22 visual_prompt]: 	Training 300/1106. train loss: 1.0737,	0.6400 s / batch. (data: 7.86e-04). ETA=19:24:38, max mem: 15.9 GB 
[10/30 03:48:25 visual_prompt]: 	Training 400/1106. train loss: 0.0733,	0.6332 s / batch. (data: 8.16e-04). ETA=19:11:16, max mem: 15.9 GB 
[10/30 03:49:28 visual_prompt]: 	Training 500/1106. train loss: 0.7775,	0.6300 s / batch. (data: 1.20e-02). ETA=19:04:31, max mem: 15.9 GB 
[10/30 03:50:32 visual_prompt]: 	Training 600/1106. train loss: 0.6214,	0.6280 s / batch. (data: 3.39e-04). ETA=18:59:43, max mem: 15.9 GB 
[10/30 03:51:35 visual_prompt]: 	Training 700/1106. train loss: 0.8086,	0.6443 s / batch. (data: 2.43e-02). ETA=19:28:18, max mem: 15.9 GB 
[10/30 03:52:39 visual_prompt]: 	Training 800/1106. train loss: 0.7663,	0.6417 s / batch. (data: 7.88e-04). ETA=19:22:26, max mem: 15.9 GB 
[10/30 03:53:42 visual_prompt]: 	Training 900/1106. train loss: 0.6862,	0.6479 s / batch. (data: 7.94e-04). ETA=19:32:36, max mem: 15.9 GB 
[10/30 03:54:45 visual_prompt]: 	Training 1000/1106. train loss: 0.5473,	0.6646 s / batch. (data: 3.39e-02). ETA=20:01:42, max mem: 15.9 GB 
[10/30 03:55:48 visual_prompt]: 	Training 1100/1106. train loss: 0.6963,	0.6176 s / batch. (data: 1.60e-04). ETA=18:35:47, max mem: 15.9 GB 
[10/30 03:55:52 visual_prompt]: Epoch 2 / 100: avg data time: 3.84e-03, avg batch time: 0.6343, average train loss: 0.7860
[10/30 03:56:41 visual_prompt]: 	Test 100/123. loss: 0.982, 0.2248 s / batch. (data: 4.41e-05)max mem: 15.94594 GB 
[10/30 03:56:52 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2319, average loss: 0.9069
[10/30 03:56:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.04	
[10/30 03:56:52 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/30 03:57:58 visual_prompt]: 	Training 100/1106. train loss: 0.5996,	0.6321 s / batch. (data: 7.58e-04). ETA=19:00:51, max mem: 15.9 GB 
[10/30 03:59:02 visual_prompt]: 	Training 200/1106. train loss: 0.3232,	0.6174 s / batch. (data: 3.12e-04). ETA=18:33:18, max mem: 15.9 GB 
[10/30 04:00:05 visual_prompt]: 	Training 300/1106. train loss: 0.4093,	0.6458 s / batch. (data: 3.22e-04). ETA=19:23:22, max mem: 15.9 GB 
[10/30 04:01:08 visual_prompt]: 	Training 400/1106. train loss: 0.7008,	0.6406 s / batch. (data: 1.32e-02). ETA=19:13:02, max mem: 15.9 GB 
[10/30 04:02:11 visual_prompt]: 	Training 500/1106. train loss: 1.3310,	0.6311 s / batch. (data: 8.24e-04). ETA=18:54:45, max mem: 15.9 GB 
[10/30 04:03:15 visual_prompt]: 	Training 600/1106. train loss: 0.6153,	0.6428 s / batch. (data: 8.25e-04). ETA=19:14:46, max mem: 15.9 GB 
[10/30 04:04:18 visual_prompt]: 	Training 700/1106. train loss: 0.7088,	0.6304 s / batch. (data: 8.02e-04). ETA=18:51:22, max mem: 15.9 GB 
[10/30 04:05:21 visual_prompt]: 	Training 800/1106. train loss: 0.8488,	0.6398 s / batch. (data: 8.08e-04). ETA=19:07:20, max mem: 15.9 GB 
[10/30 04:06:24 visual_prompt]: 	Training 900/1106. train loss: 0.9701,	0.6423 s / batch. (data: 1.20e-02). ETA=19:10:41, max mem: 15.9 GB 
[10/30 04:07:28 visual_prompt]: 	Training 1000/1106. train loss: 0.7234,	0.6414 s / batch. (data: 7.81e-04). ETA=19:08:02, max mem: 15.9 GB 
[10/30 04:08:31 visual_prompt]: 	Training 1100/1106. train loss: 0.6965,	0.6173 s / batch. (data: 1.65e-04). ETA=18:23:44, max mem: 15.9 GB 
[10/30 04:08:35 visual_prompt]: Epoch 3 / 100: avg data time: 4.93e-03, avg batch time: 0.6348, average train loss: 0.7481
[10/30 04:09:24 visual_prompt]: 	Test 100/123. loss: 0.705, 0.2397 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/30 04:09:35 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2312, average loss: 0.7196
[10/30 04:09:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.40	
[10/30 04:09:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/30 04:10:40 visual_prompt]: 	Training 100/1106. train loss: 1.2038,	0.6413 s / batch. (data: 8.27e-04). ETA=19:05:37, max mem: 15.9 GB 
[10/30 04:11:44 visual_prompt]: 	Training 200/1106. train loss: 0.9691,	0.6507 s / batch. (data: 1.10e-02). ETA=19:21:13, max mem: 15.9 GB 
[10/30 04:12:47 visual_prompt]: 	Training 300/1106. train loss: 0.6904,	0.6335 s / batch. (data: 8.02e-04). ETA=18:49:33, max mem: 15.9 GB 
[10/30 04:13:50 visual_prompt]: 	Training 400/1106. train loss: 0.6930,	0.6307 s / batch. (data: 3.19e-04). ETA=18:43:25, max mem: 15.9 GB 
[10/30 04:14:54 visual_prompt]: 	Training 500/1106. train loss: 2.1873,	0.6320 s / batch. (data: 3.19e-04). ETA=18:44:43, max mem: 15.9 GB 
[10/30 04:15:57 visual_prompt]: 	Training 600/1106. train loss: 0.2720,	0.6359 s / batch. (data: 8.67e-04). ETA=18:50:43, max mem: 15.9 GB 
[10/30 04:17:00 visual_prompt]: 	Training 700/1106. train loss: 1.1531,	0.6485 s / batch. (data: 7.93e-04). ETA=19:11:58, max mem: 15.9 GB 
[10/30 04:18:03 visual_prompt]: 	Training 800/1106. train loss: 0.2260,	0.6344 s / batch. (data: 8.23e-04). ETA=18:45:54, max mem: 15.9 GB 
[10/30 04:19:07 visual_prompt]: 	Training 900/1106. train loss: 0.9109,	0.6313 s / batch. (data: 5.05e-04). ETA=18:39:17, max mem: 15.9 GB 
[10/30 04:20:10 visual_prompt]: 	Training 1000/1106. train loss: 0.6055,	0.6281 s / batch. (data: 8.25e-04). ETA=18:32:35, max mem: 15.9 GB 
[10/30 04:21:13 visual_prompt]: 	Training 1100/1106. train loss: 0.7585,	0.6177 s / batch. (data: 1.25e-04). ETA=18:13:13, max mem: 15.9 GB 
[10/30 04:21:17 visual_prompt]: Epoch 4 / 100: avg data time: 4.30e-03, avg batch time: 0.6350, average train loss: 0.7537
[10/30 04:22:07 visual_prompt]: 	Test 100/123. loss: 0.835, 0.2346 s / batch. (data: 3.96e-05)max mem: 15.94594 GB 
[10/30 04:22:17 visual_prompt]: Inference (val):avg data time: 9.96e-05, avg batch time: 0.2321, average loss: 0.7821
[10/30 04:22:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.24	
[10/30 04:22:17 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/30 04:23:22 visual_prompt]: 	Training 100/1106. train loss: 0.9613,	0.6324 s / batch. (data: 7.89e-04). ETA=18:37:58, max mem: 15.9 GB 
[10/30 04:24:25 visual_prompt]: 	Training 200/1106. train loss: 1.2975,	0.6313 s / batch. (data: 8.46e-04). ETA=18:34:58, max mem: 15.9 GB 
[10/30 04:25:28 visual_prompt]: 	Training 300/1106. train loss: 0.7245,	0.6330 s / batch. (data: 8.46e-04). ETA=18:37:02, max mem: 15.9 GB 
[10/30 04:26:32 visual_prompt]: 	Training 400/1106. train loss: 0.9419,	0.6442 s / batch. (data: 7.90e-04). ETA=18:55:44, max mem: 15.9 GB 
[10/30 04:27:35 visual_prompt]: 	Training 500/1106. train loss: 0.1047,	0.6185 s / batch. (data: 2.96e-04). ETA=18:09:21, max mem: 15.9 GB 
[10/30 04:28:38 visual_prompt]: 	Training 600/1106. train loss: 1.5907,	0.6176 s / batch. (data: 3.23e-04). ETA=18:06:38, max mem: 15.9 GB 
[10/30 04:29:42 visual_prompt]: 	Training 700/1106. train loss: 0.6947,	0.6296 s / batch. (data: 8.09e-04). ETA=18:26:44, max mem: 15.9 GB 
[10/30 04:30:45 visual_prompt]: 	Training 800/1106. train loss: 1.3348,	0.6198 s / batch. (data: 3.12e-04). ETA=18:08:29, max mem: 15.9 GB 
[10/30 04:31:48 visual_prompt]: 	Training 900/1106. train loss: 0.7459,	0.6361 s / batch. (data: 1.63e-02). ETA=18:36:01, max mem: 15.9 GB 
[10/30 04:32:51 visual_prompt]: 	Training 1000/1106. train loss: 0.3454,	0.6218 s / batch. (data: 2.78e-04). ETA=18:09:56, max mem: 15.9 GB 
[10/30 04:33:55 visual_prompt]: 	Training 1100/1106. train loss: 0.7384,	0.6183 s / batch. (data: 1.62e-04). ETA=18:02:46, max mem: 15.9 GB 
[10/30 04:33:58 visual_prompt]: Epoch 5 / 100: avg data time: 4.04e-03, avg batch time: 0.6338, average train loss: 0.7666
[10/30 04:34:49 visual_prompt]: 	Test 100/123. loss: 0.954, 0.2671 s / batch. (data: 5.67e-05)max mem: 15.94594 GB 
[10/30 04:34:59 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2328, average loss: 1.0278
[10/30 04:34:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.30	
[10/30 04:34:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/30 04:36:04 visual_prompt]: 	Training 100/1106. train loss: 0.9434,	0.6440 s / batch. (data: 8.62e-04). ETA=18:46:39, max mem: 15.9 GB 
[10/30 04:37:08 visual_prompt]: 	Training 200/1106. train loss: 0.4399,	0.6221 s / batch. (data: 3.47e-04). ETA=18:07:18, max mem: 15.9 GB 
[10/30 04:38:11 visual_prompt]: 	Training 300/1106. train loss: 0.1155,	0.6340 s / batch. (data: 8.17e-04). ETA=18:27:09, max mem: 15.9 GB 
[10/30 04:39:14 visual_prompt]: 	Training 400/1106. train loss: 0.8564,	0.6485 s / batch. (data: 8.27e-04). ETA=18:51:15, max mem: 15.9 GB 
[10/30 04:40:17 visual_prompt]: 	Training 500/1106. train loss: 0.7406,	0.6349 s / batch. (data: 2.94e-04). ETA=18:26:31, max mem: 15.9 GB 
[10/30 04:41:21 visual_prompt]: 	Training 600/1106. train loss: 0.8407,	0.6181 s / batch. (data: 2.74e-04). ETA=17:56:18, max mem: 15.9 GB 
[10/30 04:42:24 visual_prompt]: 	Training 700/1106. train loss: 0.9719,	0.6335 s / batch. (data: 1.56e-02). ETA=18:21:56, max mem: 15.9 GB 
[10/30 04:43:27 visual_prompt]: 	Training 800/1106. train loss: 0.6976,	0.6468 s / batch. (data: 8.05e-04). ETA=18:44:04, max mem: 15.9 GB 
[10/30 04:44:31 visual_prompt]: 	Training 900/1106. train loss: 0.4162,	0.6329 s / batch. (data: 1.37e-02). ETA=18:18:54, max mem: 15.9 GB 
[10/30 04:45:34 visual_prompt]: 	Training 1000/1106. train loss: 1.3885,	0.6240 s / batch. (data: 3.02e-04). ETA=18:02:16, max mem: 15.9 GB 
[10/30 04:46:37 visual_prompt]: 	Training 1100/1106. train loss: 0.4697,	0.6180 s / batch. (data: 1.41e-04). ETA=17:50:52, max mem: 15.9 GB 
[10/30 04:46:41 visual_prompt]: Epoch 6 / 100: avg data time: 4.32e-03, avg batch time: 0.6344, average train loss: 0.7811
[10/30 04:47:31 visual_prompt]: 	Test 100/123. loss: 0.759, 0.2245 s / batch. (data: 2.31e-05)max mem: 15.94594 GB 
[10/30 04:47:41 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.2321, average loss: 0.7232
[10/30 04:47:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.93	
[10/30 04:47:42 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/30 04:48:47 visual_prompt]: 	Training 100/1106. train loss: 0.9229,	0.6178 s / batch. (data: 3.43e-04). ETA=17:49:32, max mem: 15.9 GB 
[10/30 04:49:50 visual_prompt]: 	Training 200/1106. train loss: 1.2931,	0.6189 s / batch. (data: 7.87e-04). ETA=17:50:23, max mem: 15.9 GB 
[10/30 04:50:53 visual_prompt]: 	Training 300/1106. train loss: 0.2620,	0.6309 s / batch. (data: 3.15e-04). ETA=18:09:59, max mem: 15.9 GB 
[10/30 04:51:56 visual_prompt]: 	Training 400/1106. train loss: 1.0029,	0.6479 s / batch. (data: 9.32e-04). ETA=18:38:19, max mem: 15.9 GB 
[10/30 04:53:00 visual_prompt]: 	Training 500/1106. train loss: 0.7874,	0.6200 s / batch. (data: 3.08e-04). ETA=17:49:07, max mem: 15.9 GB 
[10/30 04:54:03 visual_prompt]: 	Training 600/1106. train loss: 0.8728,	0.6181 s / batch. (data: 3.11e-04). ETA=17:44:45, max mem: 15.9 GB 
[10/30 04:55:06 visual_prompt]: 	Training 700/1106. train loss: 0.4622,	0.6256 s / batch. (data: 3.23e-04). ETA=17:56:38, max mem: 15.9 GB 
[10/30 04:56:10 visual_prompt]: 	Training 800/1106. train loss: 0.3305,	0.6195 s / batch. (data: 3.30e-04). ETA=17:45:09, max mem: 15.9 GB 
[10/30 04:57:13 visual_prompt]: 	Training 900/1106. train loss: 0.7337,	0.6320 s / batch. (data: 3.26e-04). ETA=18:05:34, max mem: 15.9 GB 
[10/30 04:58:16 visual_prompt]: 	Training 1000/1106. train loss: 0.9142,	0.6536 s / batch. (data: 7.93e-04). ETA=18:41:40, max mem: 15.9 GB 
[10/30 04:59:19 visual_prompt]: 	Training 1100/1106. train loss: 0.3430,	0.6179 s / batch. (data: 1.47e-04). ETA=17:39:14, max mem: 15.9 GB 
[10/30 04:59:23 visual_prompt]: Epoch 7 / 100: avg data time: 4.09e-03, avg batch time: 0.6344, average train loss: 0.7959
[10/30 05:00:13 visual_prompt]: 	Test 100/123. loss: 0.733, 0.2466 s / batch. (data: 3.77e-05)max mem: 15.94594 GB 
[10/30 05:00:24 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2319, average loss: 0.7049
[10/30 05:00:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.71	
[10/30 05:00:24 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/30 05:01:29 visual_prompt]: 	Training 100/1106. train loss: 0.5795,	0.6298 s / batch. (data: 8.08e-04). ETA=17:58:34, max mem: 15.9 GB 
[10/30 05:02:32 visual_prompt]: 	Training 200/1106. train loss: 0.7584,	0.6317 s / batch. (data: 3.15e-04). ETA=18:00:45, max mem: 15.9 GB 
[10/30 05:03:35 visual_prompt]: 	Training 300/1106. train loss: 0.1665,	0.6688 s / batch. (data: 3.27e-02). ETA=19:03:07, max mem: 15.9 GB 
[10/30 05:04:38 visual_prompt]: 	Training 400/1106. train loss: 0.7442,	0.6180 s / batch. (data: 2.66e-04). ETA=17:35:22, max mem: 15.9 GB 
[10/30 05:05:42 visual_prompt]: 	Training 500/1106. train loss: 0.2828,	0.6467 s / batch. (data: 8.16e-04). ETA=18:23:19, max mem: 15.9 GB 
[10/30 05:06:45 visual_prompt]: 	Training 600/1106. train loss: 0.1963,	0.6229 s / batch. (data: 7.64e-04). ETA=17:41:33, max mem: 15.9 GB 
[10/30 05:07:48 visual_prompt]: 	Training 700/1106. train loss: 0.7497,	0.6286 s / batch. (data: 3.71e-04). ETA=17:50:12, max mem: 15.9 GB 
[10/30 05:08:52 visual_prompt]: 	Training 800/1106. train loss: 0.9513,	0.6326 s / batch. (data: 3.50e-04). ETA=17:56:01, max mem: 15.9 GB 
[10/30 05:09:55 visual_prompt]: 	Training 900/1106. train loss: 0.1084,	0.6240 s / batch. (data: 3.50e-04). ETA=17:40:19, max mem: 15.9 GB 
[10/30 05:10:58 visual_prompt]: 	Training 1000/1106. train loss: 1.5877,	0.6331 s / batch. (data: 8.14e-04). ETA=17:54:50, max mem: 15.9 GB 
[10/30 05:12:02 visual_prompt]: 	Training 1100/1106. train loss: 0.8588,	0.6167 s / batch. (data: 1.21e-04). ETA=17:25:52, max mem: 15.9 GB 
[10/30 05:12:05 visual_prompt]: Epoch 8 / 100: avg data time: 4.17e-03, avg batch time: 0.6345, average train loss: 0.8649
[10/30 05:12:55 visual_prompt]: 	Test 100/123. loss: 0.713, 0.2361 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/30 05:13:06 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2311, average loss: 0.6934
[10/30 05:13:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.61	
[10/30 05:13:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/30 05:14:12 visual_prompt]: 	Training 100/1106. train loss: 0.6255,	0.6177 s / batch. (data: 3.36e-04). ETA=17:26:30, max mem: 15.9 GB 
[10/30 05:15:15 visual_prompt]: 	Training 200/1106. train loss: 0.2344,	0.6335 s / batch. (data: 3.19e-04). ETA=17:52:18, max mem: 15.9 GB 
[10/30 05:16:18 visual_prompt]: 	Training 300/1106. train loss: 0.9705,	0.6420 s / batch. (data: 8.42e-04). ETA=18:05:27, max mem: 15.9 GB 
[10/30 05:17:21 visual_prompt]: 	Training 400/1106. train loss: 0.7691,	0.6193 s / batch. (data: 7.69e-04). ETA=17:26:08, max mem: 15.9 GB 
[10/30 05:18:24 visual_prompt]: 	Training 500/1106. train loss: 0.7544,	0.6337 s / batch. (data: 7.78e-04). ETA=17:49:26, max mem: 15.9 GB 
[10/30 05:19:28 visual_prompt]: 	Training 600/1106. train loss: 0.7197,	0.6178 s / batch. (data: 2.83e-04). ETA=17:21:29, max mem: 15.9 GB 
[10/30 05:20:31 visual_prompt]: 	Training 700/1106. train loss: 0.9353,	0.6313 s / batch. (data: 3.29e-04). ETA=17:43:15, max mem: 15.9 GB 
[10/30 05:21:34 visual_prompt]: 	Training 800/1106. train loss: 0.6935,	0.6280 s / batch. (data: 2.89e-04). ETA=17:36:37, max mem: 15.9 GB 
[10/30 05:22:38 visual_prompt]: 	Training 900/1106. train loss: 0.7335,	0.6180 s / batch. (data: 2.91e-04). ETA=17:18:46, max mem: 15.9 GB 
[10/30 05:23:41 visual_prompt]: 	Training 1000/1106. train loss: 0.7647,	0.6280 s / batch. (data: 4.11e-04). ETA=17:34:31, max mem: 15.9 GB 
[10/30 05:24:44 visual_prompt]: 	Training 1100/1106. train loss: 0.7968,	0.6177 s / batch. (data: 1.56e-04). ETA=17:16:17, max mem: 15.9 GB 
[10/30 05:24:48 visual_prompt]: Epoch 9 / 100: avg data time: 4.30e-03, avg batch time: 0.6345, average train loss: 0.8046
[10/30 05:25:38 visual_prompt]: 	Test 100/123. loss: 0.748, 0.2448 s / batch. (data: 4.29e-05)max mem: 15.94594 GB 
[10/30 05:25:49 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2336, average loss: 0.7832
[10/30 05:25:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.54	
[10/30 05:25:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/30 05:26:54 visual_prompt]: 	Training 100/1106. train loss: 0.8584,	0.6308 s / batch. (data: 3.27e-04). ETA=17:37:02, max mem: 15.9 GB 
[10/30 05:27:57 visual_prompt]: 	Training 200/1106. train loss: 2.0312,	0.6169 s / batch. (data: 3.24e-04). ETA=17:12:43, max mem: 15.9 GB 
[10/30 05:29:00 visual_prompt]: 	Training 300/1106. train loss: 0.7824,	0.6365 s / batch. (data: 5.43e-03). ETA=17:44:32, max mem: 15.9 GB 
[10/30 05:30:04 visual_prompt]: 	Training 400/1106. train loss: 0.9002,	0.6324 s / batch. (data: 8.07e-04). ETA=17:36:33, max mem: 15.9 GB 
[10/30 05:31:07 visual_prompt]: 	Training 500/1106. train loss: 0.7562,	0.6393 s / batch. (data: 2.19e-02). ETA=17:47:05, max mem: 15.9 GB 
[10/30 05:32:10 visual_prompt]: 	Training 600/1106. train loss: 1.9473,	0.6271 s / batch. (data: 3.37e-04). ETA=17:25:41, max mem: 15.9 GB 
[10/30 05:33:14 visual_prompt]: 	Training 700/1106. train loss: 0.5177,	0.6184 s / batch. (data: 4.64e-04). ETA=17:10:05, max mem: 15.9 GB 
[10/30 05:34:17 visual_prompt]: 	Training 800/1106. train loss: 0.6973,	0.6231 s / batch. (data: 3.03e-04). ETA=17:16:52, max mem: 15.9 GB 
[10/30 05:35:20 visual_prompt]: 	Training 900/1106. train loss: 0.7087,	0.6268 s / batch. (data: 2.90e-04). ETA=17:22:03, max mem: 15.9 GB 
[10/30 05:36:23 visual_prompt]: 	Training 1000/1106. train loss: 0.5444,	0.6187 s / batch. (data: 3.27e-04). ETA=17:07:26, max mem: 15.9 GB 
[10/30 05:37:26 visual_prompt]: 	Training 1100/1106. train loss: 1.2346,	0.6183 s / batch. (data: 1.44e-04). ETA=17:05:47, max mem: 15.9 GB 
[10/30 05:37:30 visual_prompt]: Epoch 10 / 100: avg data time: 3.91e-03, avg batch time: 0.6341, average train loss: 0.9028
[10/30 05:38:20 visual_prompt]: 	Test 100/123. loss: 0.704, 0.2386 s / batch. (data: 3.89e-05)max mem: 15.94594 GB 
[10/30 05:38:31 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2327, average loss: 0.7194
[10/30 05:38:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.79	
[10/30 05:38:31 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/30 05:39:36 visual_prompt]: 	Training 100/1106. train loss: 2.1675,	0.6200 s / batch. (data: 7.99e-04). ETA=17:07:30, max mem: 15.9 GB 
[10/30 05:40:40 visual_prompt]: 	Training 200/1106. train loss: 0.7812,	0.6298 s / batch. (data: 2.90e-04). ETA=17:22:40, max mem: 15.9 GB 
[10/30 05:41:43 visual_prompt]: 	Training 300/1106. train loss: 0.6969,	0.6167 s / batch. (data: 3.59e-04). ETA=17:00:06, max mem: 15.9 GB 
[10/30 05:42:46 visual_prompt]: 	Training 400/1106. train loss: 0.6399,	0.6425 s / batch. (data: 8.21e-04). ETA=17:41:40, max mem: 15.9 GB 
[10/30 05:43:50 visual_prompt]: 	Training 500/1106. train loss: 0.7715,	0.6464 s / batch. (data: 7.72e-04). ETA=17:47:02, max mem: 15.9 GB 
[10/30 05:44:53 visual_prompt]: 	Training 600/1106. train loss: 0.0700,	0.6393 s / batch. (data: 9.98e-04). ETA=17:34:11, max mem: 15.9 GB 
[10/30 05:45:56 visual_prompt]: 	Training 700/1106. train loss: 0.8272,	0.6332 s / batch. (data: 7.81e-04). ETA=17:23:05, max mem: 15.9 GB 
[10/30 05:46:59 visual_prompt]: 	Training 800/1106. train loss: 0.9566,	0.6726 s / batch. (data: 1.14e-02). ETA=18:26:48, max mem: 15.9 GB 
[10/30 05:48:02 visual_prompt]: 	Training 900/1106. train loss: 0.2710,	0.6183 s / batch. (data: 3.27e-04). ETA=16:56:33, max mem: 15.9 GB 
[10/30 05:49:05 visual_prompt]: 	Training 1000/1106. train loss: 0.3986,	0.6309 s / batch. (data: 7.90e-04). ETA=17:16:09, max mem: 15.9 GB 
[10/30 05:50:08 visual_prompt]: 	Training 1100/1106. train loss: 0.7293,	0.6187 s / batch. (data: 1.47e-04). ETA=16:55:03, max mem: 15.9 GB 
[10/30 05:50:12 visual_prompt]: Epoch 11 / 100: avg data time: 4.33e-03, avg batch time: 0.6341, average train loss: 0.8983
[10/30 05:51:03 visual_prompt]: 	Test 100/123. loss: 0.740, 0.2252 s / batch. (data: 4.63e-05)max mem: 15.94594 GB 
[10/30 05:51:13 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2324, average loss: 0.7094
[10/30 05:51:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.22	
[10/30 05:51:13 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/30 05:52:19 visual_prompt]: 	Training 100/1106. train loss: 0.2391,	0.6482 s / batch. (data: 2.93e-02). ETA=17:42:17, max mem: 15.9 GB 
[10/30 05:53:22 visual_prompt]: 	Training 200/1106. train loss: 0.7562,	0.6203 s / batch. (data: 3.01e-04). ETA=16:55:36, max mem: 15.9 GB 
[10/30 05:54:26 visual_prompt]: 	Training 300/1106. train loss: 0.5061,	0.6197 s / batch. (data: 7.70e-04). ETA=16:53:31, max mem: 15.9 GB 
[10/30 05:55:29 visual_prompt]: 	Training 400/1106. train loss: 0.5414,	0.6303 s / batch. (data: 6.96e-04). ETA=17:09:50, max mem: 15.9 GB 
[10/30 05:56:32 visual_prompt]: 	Training 500/1106. train loss: 1.1834,	0.6186 s / batch. (data: 3.26e-04). ETA=16:49:44, max mem: 15.9 GB 
[10/30 05:57:36 visual_prompt]: 	Training 600/1106. train loss: 1.0993,	0.6371 s / batch. (data: 3.19e-04). ETA=17:18:50, max mem: 15.9 GB 
[10/30 05:58:39 visual_prompt]: 	Training 700/1106. train loss: 0.6941,	0.6299 s / batch. (data: 2.73e-04). ETA=17:05:58, max mem: 15.9 GB 
[10/30 05:59:42 visual_prompt]: 	Training 800/1106. train loss: 0.7082,	0.6342 s / batch. (data: 7.47e-04). ETA=17:11:57, max mem: 15.9 GB 
[10/30 06:00:45 visual_prompt]: 	Training 900/1106. train loss: 0.8000,	0.6400 s / batch. (data: 3.29e-04). ETA=17:20:20, max mem: 15.9 GB 
[10/30 06:01:48 visual_prompt]: 	Training 1000/1106. train loss: 0.9103,	0.6191 s / batch. (data: 3.08e-04). ETA=16:45:18, max mem: 15.9 GB 
[10/30 06:02:52 visual_prompt]: 	Training 1100/1106. train loss: 1.3425,	0.6165 s / batch. (data: 1.57e-04). ETA=16:40:03, max mem: 15.9 GB 
[10/30 06:02:56 visual_prompt]: Epoch 12 / 100: avg data time: 5.28e-03, avg batch time: 0.6355, average train loss: 0.9083
[10/30 06:03:45 visual_prompt]: 	Test 100/123. loss: 1.137, 0.2255 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/30 06:03:56 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2329, average loss: 1.2329
[10/30 06:03:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.92	
[10/30 06:03:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/30 06:05:02 visual_prompt]: 	Training 100/1106. train loss: 1.6317,	0.6284 s / batch. (data: 8.34e-04). ETA=16:58:15, max mem: 15.9 GB 
[10/30 06:06:05 visual_prompt]: 	Training 200/1106. train loss: 1.1748,	0.6250 s / batch. (data: 4.61e-04). ETA=16:51:46, max mem: 15.9 GB 
[10/30 06:07:08 visual_prompt]: 	Training 300/1106. train loss: 0.1939,	0.6172 s / batch. (data: 3.13e-04). ETA=16:38:04, max mem: 15.9 GB 
[10/30 06:08:11 visual_prompt]: 	Training 400/1106. train loss: 1.0006,	0.6375 s / batch. (data: 7.99e-04). ETA=17:09:55, max mem: 15.9 GB 
[10/30 06:09:15 visual_prompt]: 	Training 500/1106. train loss: 0.6098,	0.6452 s / batch. (data: 7.67e-04). ETA=17:21:10, max mem: 15.9 GB 
[10/30 06:10:18 visual_prompt]: 	Training 600/1106. train loss: 1.5610,	0.6521 s / batch. (data: 2.30e-02). ETA=17:31:17, max mem: 15.9 GB 
[10/30 06:11:21 visual_prompt]: 	Training 700/1106. train loss: 0.7692,	0.6473 s / batch. (data: 3.34e-04). ETA=17:22:28, max mem: 15.9 GB 
[10/30 06:12:24 visual_prompt]: 	Training 800/1106. train loss: 0.7619,	0.6265 s / batch. (data: 3.33e-04). ETA=16:47:56, max mem: 15.9 GB 
[10/30 06:13:28 visual_prompt]: 	Training 900/1106. train loss: 1.8001,	0.6366 s / batch. (data: 7.57e-04). ETA=17:03:01, max mem: 15.9 GB 
[10/30 06:14:31 visual_prompt]: 	Training 1000/1106. train loss: 0.2603,	0.6327 s / batch. (data: 8.06e-04). ETA=16:55:43, max mem: 15.9 GB 
[10/30 06:15:35 visual_prompt]: 	Training 1100/1106. train loss: 1.3496,	0.6191 s / batch. (data: 1.96e-04). ETA=16:32:54, max mem: 15.9 GB 
[10/30 06:15:38 visual_prompt]: Epoch 13 / 100: avg data time: 4.68e-03, avg batch time: 0.6348, average train loss: 0.8874
[10/30 06:16:28 visual_prompt]: 	Test 100/123. loss: 0.721, 0.2308 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/30 06:16:39 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.2327, average loss: 0.7449
[10/30 06:16:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.11	
[10/30 06:16:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/30 06:17:44 visual_prompt]: 	Training 100/1106. train loss: 0.2168,	0.6323 s / batch. (data: 8.40e-04). ETA=16:52:58, max mem: 15.9 GB 
[10/30 06:18:47 visual_prompt]: 	Training 200/1106. train loss: 0.7475,	0.6339 s / batch. (data: 7.89e-04). ETA=16:54:24, max mem: 15.9 GB 
[10/30 06:19:51 visual_prompt]: 	Training 300/1106. train loss: 0.6901,	0.6309 s / batch. (data: 8.27e-04). ETA=16:48:36, max mem: 15.9 GB 
[10/30 06:20:54 visual_prompt]: 	Training 400/1106. train loss: 0.1092,	0.6191 s / batch. (data: 3.17e-04). ETA=16:28:46, max mem: 15.9 GB 
[10/30 06:21:57 visual_prompt]: 	Training 500/1106. train loss: 0.7132,	0.6397 s / batch. (data: 8.28e-04). ETA=17:00:31, max mem: 15.9 GB 
[10/30 06:23:01 visual_prompt]: 	Training 600/1106. train loss: 0.6915,	0.6353 s / batch. (data: 8.19e-04). ETA=16:52:24, max mem: 15.9 GB 
[10/30 06:24:04 visual_prompt]: 	Training 700/1106. train loss: 0.7744,	0.6356 s / batch. (data: 1.20e-02). ETA=16:51:49, max mem: 15.9 GB 
[10/30 06:25:07 visual_prompt]: 	Training 800/1106. train loss: 1.2327,	0.6316 s / batch. (data: 1.45e-02). ETA=16:44:32, max mem: 15.9 GB 
[10/30 06:26:10 visual_prompt]: 	Training 900/1106. train loss: 0.8897,	0.6496 s / batch. (data: 5.91e-03). ETA=17:11:57, max mem: 15.9 GB 
[10/30 06:27:14 visual_prompt]: 	Training 1000/1106. train loss: 0.1937,	0.6330 s / batch. (data: 7.93e-04). ETA=16:44:31, max mem: 15.9 GB 
[10/30 06:28:17 visual_prompt]: 	Training 1100/1106. train loss: 0.7015,	0.6186 s / batch. (data: 1.58e-04). ETA=16:20:44, max mem: 15.9 GB 
[10/30 06:28:20 visual_prompt]: Epoch 14 / 100: avg data time: 4.63e-03, avg batch time: 0.6343, average train loss: 0.8922
[10/30 06:29:10 visual_prompt]: 	Test 100/123. loss: 0.763, 0.2540 s / batch. (data: 3.70e-05)max mem: 15.94594 GB 
[10/30 06:29:21 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2325, average loss: 0.7240
[10/30 06:29:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.49	
[10/30 06:29:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/30 06:30:26 visual_prompt]: 	Training 100/1106. train loss: 1.2225,	0.6468 s / batch. (data: 3.37e-04). ETA=17:04:19, max mem: 15.9 GB 
[10/30 06:31:29 visual_prompt]: 	Training 200/1106. train loss: 0.8705,	0.6476 s / batch. (data: 8.20e-04). ETA=17:04:30, max mem: 15.9 GB 
[10/30 06:32:32 visual_prompt]: 	Training 300/1106. train loss: 0.7482,	0.6439 s / batch. (data: 1.33e-02). ETA=16:57:33, max mem: 15.9 GB 
[10/30 06:33:35 visual_prompt]: 	Training 400/1106. train loss: 5.3381,	0.6186 s / batch. (data: 3.21e-04). ETA=16:16:35, max mem: 15.9 GB 
[10/30 06:34:39 visual_prompt]: 	Training 500/1106. train loss: 1.0460,	0.6525 s / batch. (data: 1.25e-02). ETA=17:08:56, max mem: 15.9 GB 
[10/30 06:35:42 visual_prompt]: 	Training 600/1106. train loss: 1.0026,	0.6432 s / batch. (data: 7.90e-04). ETA=16:53:10, max mem: 15.9 GB 
[10/30 06:36:45 visual_prompt]: 	Training 700/1106. train loss: 0.8168,	0.6181 s / batch. (data: 2.93e-04). ETA=16:12:35, max mem: 15.9 GB 
[10/30 06:37:48 visual_prompt]: 	Training 800/1106. train loss: 0.6850,	0.6220 s / batch. (data: 3.22e-04). ETA=16:17:41, max mem: 15.9 GB 
[10/30 06:38:51 visual_prompt]: 	Training 900/1106. train loss: 1.3699,	0.6452 s / batch. (data: 1.48e-02). ETA=16:53:09, max mem: 15.9 GB 
[10/30 06:39:55 visual_prompt]: 	Training 1000/1106. train loss: 0.8092,	0.6265 s / batch. (data: 7.65e-04). ETA=16:22:46, max mem: 15.9 GB 
[10/30 06:40:58 visual_prompt]: 	Training 1100/1106. train loss: 1.5286,	0.6190 s / batch. (data: 1.53e-04). ETA=16:09:56, max mem: 15.9 GB 
[10/30 06:41:02 visual_prompt]: Epoch 15 / 100: avg data time: 3.75e-03, avg batch time: 0.6336, average train loss: 0.9586
[10/30 06:41:51 visual_prompt]: 	Test 100/123. loss: 1.068, 0.2309 s / batch. (data: 4.46e-05)max mem: 15.94594 GB 
[10/30 06:42:02 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.2330, average loss: 1.1572
[10/30 06:42:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.69	
[10/30 06:42:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/30 06:43:07 visual_prompt]: 	Training 100/1106. train loss: 0.6701,	0.6341 s / batch. (data: 7.80e-04). ETA=16:32:26, max mem: 15.9 GB 
[10/30 06:44:10 visual_prompt]: 	Training 200/1106. train loss: 0.7719,	0.6306 s / batch. (data: 8.03e-04). ETA=16:25:53, max mem: 15.9 GB 
[10/30 06:45:13 visual_prompt]: 	Training 300/1106. train loss: 0.3212,	0.6201 s / batch. (data: 3.19e-04). ETA=16:08:32, max mem: 15.9 GB 
[10/30 06:46:17 visual_prompt]: 	Training 400/1106. train loss: 1.2950,	0.6231 s / batch. (data: 3.25e-04). ETA=16:12:10, max mem: 15.9 GB 
[10/30 06:47:20 visual_prompt]: 	Training 500/1106. train loss: 0.7511,	0.6316 s / batch. (data: 1.02e-03). ETA=16:24:19, max mem: 15.9 GB 
[10/30 06:48:23 visual_prompt]: 	Training 600/1106. train loss: 0.6576,	0.6170 s / batch. (data: 3.08e-04). ETA=16:00:32, max mem: 15.9 GB 
[10/30 06:49:26 visual_prompt]: 	Training 700/1106. train loss: 0.9839,	0.6304 s / batch. (data: 3.23e-04). ETA=16:20:18, max mem: 15.9 GB 
[10/30 06:50:29 visual_prompt]: 	Training 800/1106. train loss: 0.6734,	0.6362 s / batch. (data: 1.30e-02). ETA=16:28:23, max mem: 15.9 GB 
[10/30 06:51:32 visual_prompt]: 	Training 900/1106. train loss: 0.7552,	0.6335 s / batch. (data: 3.17e-04). ETA=16:23:01, max mem: 15.9 GB 
[10/30 06:52:36 visual_prompt]: 	Training 1000/1106. train loss: 0.8999,	0.6181 s / batch. (data: 2.84e-04). ETA=15:58:08, max mem: 15.9 GB 
[10/30 06:53:39 visual_prompt]: 	Training 1100/1106. train loss: 0.2887,	0.6189 s / batch. (data: 1.43e-04). ETA=15:58:20, max mem: 15.9 GB 
[10/30 06:53:43 visual_prompt]: Epoch 16 / 100: avg data time: 3.88e-03, avg batch time: 0.6337, average train loss: 0.8881
[10/30 06:54:32 visual_prompt]: 	Test 100/123. loss: 0.714, 0.2252 s / batch. (data: 2.48e-05)max mem: 15.94594 GB 
[10/30 06:54:43 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.2322, average loss: 0.6939
[10/30 06:54:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.06	
[10/30 06:54:43 visual_prompt]: Best epoch 16: best metric: -0.694
[10/30 06:54:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/30 06:55:49 visual_prompt]: 	Training 100/1106. train loss: 0.8060,	0.6409 s / batch. (data: 3.05e-04). ETA=16:31:14, max mem: 15.9 GB 
[10/30 06:56:52 visual_prompt]: 	Training 200/1106. train loss: 0.8755,	0.6176 s / batch. (data: 2.99e-04). ETA=15:54:16, max mem: 15.9 GB 
[10/30 06:57:55 visual_prompt]: 	Training 300/1106. train loss: 0.9793,	0.6326 s / batch. (data: 8.06e-04). ETA=16:16:17, max mem: 15.9 GB 
[10/30 06:58:58 visual_prompt]: 	Training 400/1106. train loss: 3.3205,	0.6288 s / batch. (data: 2.98e-04). ETA=16:09:26, max mem: 15.9 GB 
[10/30 07:00:02 visual_prompt]: 	Training 500/1106. train loss: 0.0942,	0.6382 s / batch. (data: 7.92e-04). ETA=16:22:50, max mem: 15.9 GB 
[10/30 07:01:05 visual_prompt]: 	Training 600/1106. train loss: 0.7604,	0.6167 s / batch. (data: 3.22e-04). ETA=15:48:46, max mem: 15.9 GB 
[10/30 07:02:08 visual_prompt]: 	Training 700/1106. train loss: 0.8342,	0.6283 s / batch. (data: 3.32e-04). ETA=16:05:32, max mem: 15.9 GB 
[10/30 07:03:11 visual_prompt]: 	Training 800/1106. train loss: 0.9031,	0.6451 s / batch. (data: 7.46e-04). ETA=16:30:20, max mem: 15.9 GB 
[10/30 07:04:14 visual_prompt]: 	Training 900/1106. train loss: 1.4372,	0.6662 s / batch. (data: 3.60e-02). ETA=17:01:32, max mem: 15.9 GB 
[10/30 07:05:17 visual_prompt]: 	Training 1000/1106. train loss: 0.0525,	0.6314 s / batch. (data: 8.29e-04). ETA=16:07:05, max mem: 15.9 GB 
[10/30 07:06:21 visual_prompt]: 	Training 1100/1106. train loss: 0.2501,	0.6168 s / batch. (data: 1.51e-04). ETA=15:43:47, max mem: 15.9 GB 
[10/30 07:06:25 visual_prompt]: Epoch 17 / 100: avg data time: 4.57e-03, avg batch time: 0.6342, average train loss: 0.9417
[10/30 07:07:14 visual_prompt]: 	Test 100/123. loss: 0.692, 0.2264 s / batch. (data: 2.86e-05)max mem: 15.94594 GB 
[10/30 07:07:25 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.2314, average loss: 0.6921
[10/30 07:07:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 51.90	
[10/30 07:07:25 visual_prompt]: Best epoch 17: best metric: -0.692
[10/30 07:07:25 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/30 07:08:30 visual_prompt]: 	Training 100/1106. train loss: 0.2059,	0.6173 s / batch. (data: 2.76e-04). ETA=15:43:21, max mem: 15.9 GB 
[10/30 07:09:33 visual_prompt]: 	Training 200/1106. train loss: 1.5670,	0.6184 s / batch. (data: 2.94e-04). ETA=15:44:05, max mem: 15.9 GB 
[10/30 07:10:37 visual_prompt]: 	Training 300/1106. train loss: 3.6261,	0.6296 s / batch. (data: 7.91e-04). ETA=16:00:05, max mem: 15.9 GB 
[10/30 07:11:40 visual_prompt]: 	Training 400/1106. train loss: 0.7797,	0.6307 s / batch. (data: 1.26e-02). ETA=16:00:42, max mem: 15.9 GB 
[10/30 07:12:43 visual_prompt]: 	Training 500/1106. train loss: 0.2691,	0.6330 s / batch. (data: 7.86e-04). ETA=16:03:10, max mem: 15.9 GB 
[10/30 07:13:46 visual_prompt]: 	Training 600/1106. train loss: 0.3198,	0.6176 s / batch. (data: 3.74e-04). ETA=15:38:47, max mem: 15.9 GB 
[10/30 07:14:49 visual_prompt]: 	Training 700/1106. train loss: 0.7260,	0.6404 s / batch. (data: 7.73e-04). ETA=16:12:14, max mem: 15.9 GB 
[10/30 07:15:53 visual_prompt]: 	Training 800/1106. train loss: 0.7126,	0.6333 s / batch. (data: 8.06e-04). ETA=16:00:27, max mem: 15.9 GB 
[10/30 07:16:56 visual_prompt]: 	Training 900/1106. train loss: 1.3423,	0.6491 s / batch. (data: 2.61e-02). ETA=16:23:23, max mem: 15.9 GB 
[10/30 07:17:59 visual_prompt]: 	Training 1000/1106. train loss: 0.6878,	0.6364 s / batch. (data: 8.07e-04). ETA=16:03:04, max mem: 15.9 GB 
[10/30 07:19:02 visual_prompt]: 	Training 1100/1106. train loss: 0.5965,	0.6181 s / batch. (data: 1.54e-04). ETA=15:34:19, max mem: 15.9 GB 
[10/30 07:19:06 visual_prompt]: Epoch 18 / 100: avg data time: 4.16e-03, avg batch time: 0.6339, average train loss: 0.9792
[10/30 07:19:56 visual_prompt]: 	Test 100/123. loss: 0.764, 0.2441 s / batch. (data: 3.55e-05)max mem: 15.94594 GB 
[10/30 07:20:06 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.2320, average loss: 0.7258
[10/30 07:20:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.63	
[10/30 07:20:06 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/30 07:21:12 visual_prompt]: 	Training 100/1106. train loss: 0.5115,	0.6167 s / batch. (data: 3.24e-04). ETA=15:31:10, max mem: 15.9 GB 
[10/30 07:22:15 visual_prompt]: 	Training 200/1106. train loss: 0.9835,	0.6246 s / batch. (data: 7.51e-04). ETA=15:41:58, max mem: 15.9 GB 
[10/30 07:23:18 visual_prompt]: 	Training 300/1106. train loss: 0.2534,	0.6431 s / batch. (data: 8.84e-04). ETA=16:08:49, max mem: 15.9 GB 
[10/30 07:24:21 visual_prompt]: 	Training 400/1106. train loss: 0.9113,	0.6430 s / batch. (data: 7.70e-04). ETA=16:07:38, max mem: 15.9 GB 
[10/30 07:25:24 visual_prompt]: 	Training 500/1106. train loss: 0.2145,	0.6398 s / batch. (data: 7.88e-04). ETA=16:01:46, max mem: 15.9 GB 
[10/30 07:26:27 visual_prompt]: 	Training 600/1106. train loss: 0.6512,	0.6184 s / batch. (data: 2.97e-04). ETA=15:28:32, max mem: 15.9 GB 
[10/30 07:27:31 visual_prompt]: 	Training 700/1106. train loss: 0.7605,	0.6429 s / batch. (data: 8.00e-04). ETA=16:04:13, max mem: 15.9 GB 
[10/30 07:28:34 visual_prompt]: 	Training 800/1106. train loss: 0.2528,	0.6215 s / batch. (data: 3.15e-04). ETA=15:31:08, max mem: 15.9 GB 
[10/30 07:29:37 visual_prompt]: 	Training 900/1106. train loss: 1.3240,	0.6320 s / batch. (data: 8.07e-04). ETA=15:45:52, max mem: 15.9 GB 
[10/30 07:30:40 visual_prompt]: 	Training 1000/1106. train loss: 0.5474,	0.6333 s / batch. (data: 3.03e-04). ETA=15:46:44, max mem: 15.9 GB 
[10/30 07:31:44 visual_prompt]: 	Training 1100/1106. train loss: 1.1577,	0.6183 s / batch. (data: 1.70e-04). ETA=15:23:12, max mem: 15.9 GB 
[10/30 07:31:47 visual_prompt]: Epoch 19 / 100: avg data time: 4.03e-03, avg batch time: 0.6339, average train loss: 0.9200
[10/30 07:32:37 visual_prompt]: 	Test 100/123. loss: 2.843, 0.2252 s / batch. (data: 4.17e-05)max mem: 15.94594 GB 
[10/30 07:32:48 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2318, average loss: 2.5666
[10/30 07:32:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.38	
[10/30 07:32:48 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/30 07:33:53 visual_prompt]: 	Training 100/1106. train loss: 0.7310,	0.6502 s / batch. (data: 8.59e-04). ETA=16:09:39, max mem: 15.9 GB 
[10/30 07:34:56 visual_prompt]: 	Training 200/1106. train loss: 0.4882,	0.6210 s / batch. (data: 2.17e-04). ETA=15:25:04, max mem: 15.9 GB 
[10/30 07:36:00 visual_prompt]: 	Training 300/1106. train loss: 0.6902,	0.6491 s / batch. (data: 8.04e-04). ETA=16:05:56, max mem: 15.9 GB 
[10/30 07:37:03 visual_prompt]: 	Training 400/1106. train loss: 0.6946,	0.6480 s / batch. (data: 7.47e-04). ETA=16:03:13, max mem: 15.9 GB 
[10/30 07:38:06 visual_prompt]: 	Training 500/1106. train loss: 0.7121,	0.6233 s / batch. (data: 3.18e-04). ETA=15:25:30, max mem: 15.9 GB 
[10/30 07:39:10 visual_prompt]: 	Training 600/1106. train loss: 0.6628,	0.6260 s / batch. (data: 3.13e-04). ETA=15:28:26, max mem: 15.9 GB 
[10/30 07:40:13 visual_prompt]: 	Training 700/1106. train loss: 0.5824,	0.6389 s / batch. (data: 1.20e-02). ETA=15:46:30, max mem: 15.9 GB 
[10/30 07:41:16 visual_prompt]: 	Training 800/1106. train loss: 0.1145,	0.6336 s / batch. (data: 7.48e-04). ETA=15:37:32, max mem: 15.9 GB 
[10/30 07:42:19 visual_prompt]: 	Training 900/1106. train loss: 1.5077,	0.6475 s / batch. (data: 3.36e-04). ETA=15:57:00, max mem: 15.9 GB 
[10/30 07:43:23 visual_prompt]: 	Training 1000/1106. train loss: 0.3190,	0.6339 s / batch. (data: 7.72e-04). ETA=15:35:57, max mem: 15.9 GB 
[10/30 07:44:26 visual_prompt]: 	Training 1100/1106. train loss: 0.4199,	0.6186 s / batch. (data: 1.58e-04). ETA=15:12:14, max mem: 15.9 GB 
[10/30 07:44:30 visual_prompt]: Epoch 20 / 100: avg data time: 4.58e-03, avg batch time: 0.6346, average train loss: 0.8966
[10/30 07:45:19 visual_prompt]: 	Test 100/123. loss: 0.796, 0.2322 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/30 07:45:30 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2308, average loss: 0.8417
[10/30 07:45:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.05	
[10/30 07:45:30 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/30 07:46:36 visual_prompt]: 	Training 100/1106. train loss: 0.4095,	0.6320 s / batch. (data: 2.96e-04). ETA=15:30:57, max mem: 15.9 GB 
[10/30 07:47:39 visual_prompt]: 	Training 200/1106. train loss: 1.2218,	0.6181 s / batch. (data: 3.26e-04). ETA=15:09:28, max mem: 15.9 GB 
[10/30 07:48:42 visual_prompt]: 	Training 300/1106. train loss: 1.5481,	0.6323 s / batch. (data: 7.98e-04). ETA=15:29:15, max mem: 15.9 GB 
[10/30 07:49:45 visual_prompt]: 	Training 400/1106. train loss: 1.1576,	0.6327 s / batch. (data: 7.59e-04). ETA=15:28:44, max mem: 15.9 GB 
[10/30 07:50:49 visual_prompt]: 	Training 500/1106. train loss: 0.9622,	0.6470 s / batch. (data: 8.22e-04). ETA=15:48:43, max mem: 15.9 GB 
[10/30 07:51:52 visual_prompt]: 	Training 600/1106. train loss: 3.0703,	0.6302 s / batch. (data: 8.09e-04). ETA=15:23:01, max mem: 15.9 GB 
[10/30 07:52:55 visual_prompt]: 	Training 700/1106. train loss: 1.4649,	0.6447 s / batch. (data: 8.44e-04). ETA=15:43:09, max mem: 15.9 GB 
[10/30 07:53:58 visual_prompt]: 	Training 800/1106. train loss: 0.8109,	0.6400 s / batch. (data: 7.80e-04). ETA=15:35:14, max mem: 15.9 GB 
[10/30 07:55:02 visual_prompt]: 	Training 900/1106. train loss: 0.0147,	0.6223 s / batch. (data: 3.21e-04). ETA=15:08:17, max mem: 15.9 GB 
[10/30 07:56:05 visual_prompt]: 	Training 1000/1106. train loss: 0.7035,	0.6383 s / batch. (data: 8.07e-04). ETA=15:30:35, max mem: 15.9 GB 
[10/30 07:57:08 visual_prompt]: 	Training 1100/1106. train loss: 1.5712,	0.6175 s / batch. (data: 1.46e-04). ETA=14:59:21, max mem: 15.9 GB 
[10/30 07:57:12 visual_prompt]: Epoch 21 / 100: avg data time: 4.78e-03, avg batch time: 0.6344, average train loss: 0.8938
[10/30 07:58:02 visual_prompt]: 	Test 100/123. loss: 0.790, 0.2251 s / batch. (data: 4.15e-05)max mem: 15.94594 GB 
[10/30 07:58:12 visual_prompt]: Inference (val):avg data time: 9.73e-05, avg batch time: 0.2325, average loss: 0.7461
[10/30 07:58:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.38	
[10/30 07:58:12 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/30 07:59:17 visual_prompt]: 	Training 100/1106. train loss: 0.7808,	0.6169 s / batch. (data: 3.01e-04). ETA=14:57:21, max mem: 15.9 GB 
[10/30 08:00:21 visual_prompt]: 	Training 200/1106. train loss: 0.1122,	0.6334 s / batch. (data: 3.15e-04). ETA=15:20:12, max mem: 15.9 GB 
[10/30 08:01:24 visual_prompt]: 	Training 300/1106. train loss: 0.7711,	0.6201 s / batch. (data: 2.01e-03). ETA=14:59:58, max mem: 15.9 GB 
[10/30 08:02:27 visual_prompt]: 	Training 400/1106. train loss: 0.4723,	0.6203 s / batch. (data: 2.71e-04). ETA=14:59:07, max mem: 15.9 GB 
[10/30 08:03:30 visual_prompt]: 	Training 500/1106. train loss: 0.4919,	0.6178 s / batch. (data: 3.11e-04). ETA=14:54:34, max mem: 15.9 GB 
[10/30 08:04:33 visual_prompt]: 	Training 600/1106. train loss: 0.1115,	0.6177 s / batch. (data: 3.00e-04). ETA=14:53:19, max mem: 15.9 GB 
[10/30 08:05:36 visual_prompt]: 	Training 700/1106. train loss: 0.9544,	0.6427 s / batch. (data: 7.52e-04). ETA=15:28:27, max mem: 15.9 GB 
[10/30 08:06:39 visual_prompt]: 	Training 800/1106. train loss: 2.1234,	0.6278 s / batch. (data: 8.75e-04). ETA=15:05:49, max mem: 15.9 GB 
[10/30 08:07:43 visual_prompt]: 	Training 900/1106. train loss: 0.7019,	0.6459 s / batch. (data: 7.61e-04). ETA=15:30:54, max mem: 15.9 GB 
[10/30 08:08:46 visual_prompt]: 	Training 1000/1106. train loss: 0.9349,	0.6176 s / batch. (data: 3.18e-04). ETA=14:49:04, max mem: 15.9 GB 
[10/30 08:09:49 visual_prompt]: 	Training 1100/1106. train loss: 1.0774,	0.6183 s / batch. (data: 1.56e-04). ETA=14:48:59, max mem: 15.9 GB 
[10/30 08:09:53 visual_prompt]: Epoch 22 / 100: avg data time: 4.02e-03, avg batch time: 0.6339, average train loss: 0.8602
[10/30 08:10:44 visual_prompt]: 	Test 100/123. loss: 0.693, 0.2246 s / batch. (data: 4.70e-05)max mem: 15.94594 GB 
[10/30 08:10:54 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2323, average loss: 0.6958
[10/30 08:10:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.51	
[10/30 08:10:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/30 08:12:00 visual_prompt]: 	Training 100/1106. train loss: 0.8699,	0.6461 s / batch. (data: 1.41e-02). ETA=15:27:54, max mem: 15.9 GB 
[10/30 08:13:03 visual_prompt]: 	Training 200/1106. train loss: 0.9941,	0.6400 s / batch. (data: 3.18e-04). ETA=15:18:06, max mem: 15.9 GB 
[10/30 08:14:07 visual_prompt]: 	Training 300/1106. train loss: 0.5980,	0.6294 s / batch. (data: 3.14e-04). ETA=15:01:48, max mem: 15.9 GB 
[10/30 08:15:10 visual_prompt]: 	Training 400/1106. train loss: 1.9925,	0.6392 s / batch. (data: 8.11e-04). ETA=15:14:45, max mem: 15.9 GB 
[10/30 08:16:14 visual_prompt]: 	Training 500/1106. train loss: 0.7331,	0.6238 s / batch. (data: 3.20e-04). ETA=14:51:45, max mem: 15.9 GB 
[10/30 08:17:17 visual_prompt]: 	Training 600/1106. train loss: 0.3643,	0.6403 s / batch. (data: 8.04e-04). ETA=15:14:12, max mem: 15.9 GB 
[10/30 08:18:20 visual_prompt]: 	Training 700/1106. train loss: 1.0090,	0.6238 s / batch. (data: 3.18e-04). ETA=14:49:39, max mem: 15.9 GB 
[10/30 08:19:23 visual_prompt]: 	Training 800/1106. train loss: 0.8542,	0.6405 s / batch. (data: 7.65e-04). ETA=15:12:22, max mem: 15.9 GB 
[10/30 08:20:26 visual_prompt]: 	Training 900/1106. train loss: 0.7714,	0.6203 s / batch. (data: 7.59e-04). ETA=14:42:37, max mem: 15.9 GB 
[10/30 08:21:30 visual_prompt]: 	Training 1000/1106. train loss: 0.1414,	0.6809 s / batch. (data: 3.69e-02). ETA=16:07:39, max mem: 15.9 GB 
[10/30 08:22:33 visual_prompt]: 	Training 1100/1106. train loss: 0.7507,	0.6182 s / batch. (data: 1.56e-04). ETA=14:37:31, max mem: 15.9 GB 
[10/30 08:22:37 visual_prompt]: Epoch 23 / 100: avg data time: 4.98e-03, avg batch time: 0.6351, average train loss: 0.8995
[10/30 08:23:26 visual_prompt]: 	Test 100/123. loss: 0.858, 0.2399 s / batch. (data: 3.70e-05)max mem: 15.94594 GB 
[10/30 08:23:37 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2329, average loss: 0.8000
[10/30 08:23:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.27	
[10/30 08:23:37 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/30 08:24:42 visual_prompt]: 	Training 100/1106. train loss: 2.0658,	0.6314 s / batch. (data: 8.47e-04). ETA=14:55:06, max mem: 15.9 GB 
[10/30 08:25:45 visual_prompt]: 	Training 200/1106. train loss: 1.3401,	0.6178 s / batch. (data: 2.98e-04). ETA=14:34:52, max mem: 15.9 GB 
[10/30 08:26:48 visual_prompt]: 	Training 300/1106. train loss: 1.2317,	0.6218 s / batch. (data: 2.45e-04). ETA=14:39:25, max mem: 15.9 GB 
[10/30 08:27:52 visual_prompt]: 	Training 400/1106. train loss: 0.6919,	0.6256 s / batch. (data: 6.77e-03). ETA=14:43:43, max mem: 15.9 GB 
[10/30 08:28:55 visual_prompt]: 	Training 500/1106. train loss: 1.7804,	0.6290 s / batch. (data: 9.38e-04). ETA=14:47:31, max mem: 15.9 GB 
[10/30 08:29:58 visual_prompt]: 	Training 600/1106. train loss: 0.7585,	0.6408 s / batch. (data: 2.93e-04). ETA=15:03:07, max mem: 15.9 GB 
[10/30 08:31:01 visual_prompt]: 	Training 700/1106. train loss: 1.9825,	0.6273 s / batch. (data: 3.29e-04). ETA=14:43:06, max mem: 15.9 GB 
[10/30 08:32:04 visual_prompt]: 	Training 800/1106. train loss: 0.8106,	0.6177 s / batch. (data: 3.33e-04). ETA=14:28:34, max mem: 15.9 GB 
[10/30 08:33:08 visual_prompt]: 	Training 900/1106. train loss: 0.7713,	0.6188 s / batch. (data: 7.67e-04). ETA=14:28:59, max mem: 15.9 GB 
[10/30 08:34:11 visual_prompt]: 	Training 1000/1106. train loss: 0.6929,	0.6186 s / batch. (data: 3.09e-04). ETA=14:27:46, max mem: 15.9 GB 
[10/30 08:35:15 visual_prompt]: 	Training 1100/1106. train loss: 0.8422,	0.6170 s / batch. (data: 1.49e-04). ETA=14:24:28, max mem: 15.9 GB 
[10/30 08:35:19 visual_prompt]: Epoch 24 / 100: avg data time: 5.06e-03, avg batch time: 0.6345, average train loss: 0.8919
[10/30 08:36:12 visual_prompt]: 	Test 100/123. loss: 0.760, 0.2250 s / batch. (data: 4.03e-05)max mem: 15.94594 GB 
[10/30 08:36:23 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2311, average loss: 0.7214
[10/30 08:36:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.61	
[10/30 08:36:23 visual_prompt]: Stopping early.
[10/30 08:36:23 visual_prompt]: Rank of current process: 0. World size: 1
[10/30 08:36:23 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/30 08:36:23 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/30 08:36:23 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/30 08:36:23 visual_prompt]: Training with config:
[10/30 08:36:23 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.1_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/30 08:36:23 visual_prompt]: Loading training data...
[10/30 08:36:23 visual_prompt]: Constructing mammo-cbis dataset train...
[10/30 08:36:23 visual_prompt]: Loading validation data...
[10/30 08:36:23 visual_prompt]: Constructing mammo-cbis dataset val...
[10/30 08:36:23 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/30 08:36:29 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/30 08:36:29 visual_prompt]: tuned percent:0.522
[10/30 08:36:29 visual_prompt]: Device used for model: 0
[10/30 08:36:29 visual_prompt]: Setting up Evaluator...
[10/30 08:36:29 visual_prompt]: Setting up Trainer...
[10/30 08:36:29 visual_prompt]: 	Setting up the optimizer...
[10/30 08:36:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/30 08:37:34 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6319 s / batch. (data: 7.96e-04). ETA=19:23:41, max mem: 15.9 GB 
[10/30 08:38:38 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6407 s / batch. (data: 7.87e-04). ETA=19:38:58, max mem: 15.9 GB 
[10/30 08:39:42 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6448 s / batch. (data: 7.67e-04). ETA=19:45:17, max mem: 15.9 GB 
[10/30 08:40:45 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6246 s / batch. (data: 3.40e-04). ETA=19:07:07, max mem: 15.9 GB 
[10/30 08:41:49 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6311 s / batch. (data: 3.44e-04). ETA=19:18:06, max mem: 15.9 GB 
[10/30 08:42:53 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6224 s / batch. (data: 3.35e-04). ETA=19:00:59, max mem: 15.9 GB 
[10/30 08:43:56 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6349 s / batch. (data: 1.09e-03). ETA=19:22:57, max mem: 15.9 GB 
[10/30 08:45:00 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6680 s / batch. (data: 7.97e-04). ETA=20:22:25, max mem: 15.9 GB 
[10/30 08:46:04 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6280 s / batch. (data: 3.35e-04). ETA=19:08:11, max mem: 15.9 GB 
[10/30 08:47:07 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6196 s / batch. (data: 2.94e-04). ETA=18:51:50, max mem: 15.9 GB 
[10/30 08:48:11 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6183 s / batch. (data: 1.48e-04). ETA=18:48:18, max mem: 15.9 GB 
[10/30 08:48:14 visual_prompt]: Epoch 1 / 100: avg data time: 5.40e-03, avg batch time: 0.6380, average train loss: 1.4028
[10/30 08:49:06 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2345 s / batch. (data: 4.29e-05)max mem: 15.94594 GB 
[10/30 08:49:17 visual_prompt]: Inference (val):avg data time: 1.19e-04, avg batch time: 0.2314, average loss: 1.3505
[10/30 08:49:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/30 08:49:17 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/30 08:50:22 visual_prompt]: 	Training 100/1106. train loss: 0.6863,	0.6279 s / batch. (data: 2.96e-04). ETA=19:04:49, max mem: 15.9 GB 
[10/30 08:51:26 visual_prompt]: 	Training 200/1106. train loss: 0.6684,	0.6331 s / batch. (data: 7.76e-04). ETA=19:13:16, max mem: 15.9 GB 
[10/30 08:52:30 visual_prompt]: 	Training 300/1106. train loss: 1.1519,	0.6264 s / batch. (data: 3.08e-04). ETA=18:59:57, max mem: 15.9 GB 
[10/30 08:53:33 visual_prompt]: 	Training 400/1106. train loss: 0.0615,	0.6321 s / batch. (data: 2.98e-04). ETA=19:09:18, max mem: 15.9 GB 
[10/30 08:54:37 visual_prompt]: 	Training 500/1106. train loss: 0.7475,	0.6240 s / batch. (data: 3.36e-04). ETA=18:53:29, max mem: 15.9 GB 
[10/30 08:55:41 visual_prompt]: 	Training 600/1106. train loss: 0.5451,	0.6181 s / batch. (data: 3.37e-04). ETA=18:41:52, max mem: 15.9 GB 
[10/30 08:56:44 visual_prompt]: 	Training 700/1106. train loss: 0.8275,	0.6581 s / batch. (data: 8.73e-04). ETA=19:53:15, max mem: 15.9 GB 
[10/30 08:57:48 visual_prompt]: 	Training 800/1106. train loss: 0.7598,	0.6193 s / batch. (data: 3.47e-04). ETA=18:41:52, max mem: 15.9 GB 
[10/30 08:58:52 visual_prompt]: 	Training 900/1106. train loss: 0.6270,	0.6525 s / batch. (data: 5.92e-03). ETA=19:41:01, max mem: 15.9 GB 
[10/30 08:59:56 visual_prompt]: 	Training 1000/1106. train loss: 0.5351,	0.6193 s / batch. (data: 3.32e-04). ETA=18:39:51, max mem: 15.9 GB 
[10/30 09:00:59 visual_prompt]: 	Training 1100/1106. train loss: 0.7050,	0.6173 s / batch. (data: 1.74e-04). ETA=18:35:07, max mem: 15.9 GB 
[10/30 09:01:03 visual_prompt]: Epoch 2 / 100: avg data time: 5.76e-03, avg batch time: 0.6382, average train loss: 0.7979
[10/30 09:01:57 visual_prompt]: 	Test 100/123. loss: 1.130, 0.2479 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[10/30 09:02:09 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.2324, average loss: 1.0129
[10/30 09:02:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[10/30 09:02:09 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/30 09:03:15 visual_prompt]: 	Training 100/1106. train loss: 0.6364,	0.6207 s / batch. (data: 2.32e-04). ETA=18:40:09, max mem: 15.9 GB 
[10/30 09:04:19 visual_prompt]: 	Training 200/1106. train loss: 0.2529,	0.6182 s / batch. (data: 8.04e-04). ETA=18:34:47, max mem: 15.9 GB 
[10/30 09:05:23 visual_prompt]: 	Training 300/1106. train loss: 0.3682,	0.6334 s / batch. (data: 3.60e-04). ETA=19:01:00, max mem: 15.9 GB 
[10/30 09:06:26 visual_prompt]: 	Training 400/1106. train loss: 0.8308,	0.6364 s / batch. (data: 5.45e-03). ETA=19:05:21, max mem: 15.9 GB 
[10/30 09:07:30 visual_prompt]: 	Training 500/1106. train loss: 1.2533,	0.6170 s / batch. (data: 3.19e-04). ETA=18:29:27, max mem: 15.9 GB 
[10/30 09:08:34 visual_prompt]: 	Training 600/1106. train loss: 0.6732,	0.6419 s / batch. (data: 7.76e-04). ETA=19:13:09, max mem: 15.9 GB 
[10/30 09:09:37 visual_prompt]: 	Training 700/1106. train loss: 1.0012,	0.6195 s / batch. (data: 3.14e-04). ETA=18:31:52, max mem: 15.9 GB 
[10/30 09:10:41 visual_prompt]: 	Training 800/1106. train loss: 1.6428,	0.6284 s / batch. (data: 3.09e-04). ETA=18:46:48, max mem: 15.9 GB 
[10/30 09:11:45 visual_prompt]: 	Training 900/1106. train loss: 1.3696,	0.6443 s / batch. (data: 3.51e-04). ETA=19:14:13, max mem: 15.9 GB 
[10/30 09:12:49 visual_prompt]: 	Training 1000/1106. train loss: 0.6975,	0.6401 s / batch. (data: 8.05e-04). ETA=19:05:36, max mem: 15.9 GB 
[10/30 09:13:53 visual_prompt]: 	Training 1100/1106. train loss: 0.5853,	0.6178 s / batch. (data: 1.93e-04). ETA=18:24:44, max mem: 15.9 GB 
[10/30 09:13:57 visual_prompt]: Epoch 3 / 100: avg data time: 6.51e-03, avg batch time: 0.6400, average train loss: 0.7876
[10/30 09:14:51 visual_prompt]: 	Test 100/123. loss: 0.812, 0.2397 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[10/30 09:15:02 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2336, average loss: 0.8136
[10/30 09:15:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.63	
[10/30 09:15:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/30 09:16:08 visual_prompt]: 	Training 100/1106. train loss: 1.2482,	0.6273 s / batch. (data: 3.84e-04). ETA=18:40:35, max mem: 15.9 GB 
[10/30 09:17:11 visual_prompt]: 	Training 200/1106. train loss: 1.0650,	0.6306 s / batch. (data: 3.27e-04). ETA=18:45:23, max mem: 15.9 GB 
[10/30 09:18:14 visual_prompt]: 	Training 300/1106. train loss: 0.8702,	0.6179 s / batch. (data: 3.37e-04). ETA=18:21:48, max mem: 15.9 GB 
[10/30 09:19:18 visual_prompt]: 	Training 400/1106. train loss: 0.7900,	0.6232 s / batch. (data: 3.25e-04). ETA=18:30:14, max mem: 15.9 GB 
[10/30 09:20:21 visual_prompt]: 	Training 500/1106. train loss: 2.5570,	0.6181 s / batch. (data: 8.14e-04). ETA=18:20:03, max mem: 15.9 GB 
[10/30 09:21:25 visual_prompt]: 	Training 600/1106. train loss: 0.1668,	0.6272 s / batch. (data: 3.25e-04). ETA=18:35:11, max mem: 15.9 GB 
[10/30 09:22:28 visual_prompt]: 	Training 700/1106. train loss: 1.4776,	0.6316 s / batch. (data: 3.19e-04). ETA=18:41:55, max mem: 15.9 GB 
[10/30 09:23:31 visual_prompt]: 	Training 800/1106. train loss: 0.0581,	0.6430 s / batch. (data: 8.18e-04). ETA=19:01:03, max mem: 15.9 GB 
[10/30 09:24:35 visual_prompt]: 	Training 900/1106. train loss: 1.0510,	0.6446 s / batch. (data: 2.91e-04). ETA=19:02:51, max mem: 15.9 GB 
[10/30 09:25:38 visual_prompt]: 	Training 1000/1106. train loss: 0.5730,	0.6340 s / batch. (data: 1.56e-02). ETA=18:43:06, max mem: 15.9 GB 
[10/30 09:26:41 visual_prompt]: 	Training 1100/1106. train loss: 0.9044,	0.6183 s / batch. (data: 1.52e-04). ETA=18:14:12, max mem: 15.9 GB 
[10/30 09:26:45 visual_prompt]: Epoch 4 / 100: avg data time: 4.88e-03, avg batch time: 0.6359, average train loss: 0.8106
[10/30 09:27:35 visual_prompt]: 	Test 100/123. loss: 1.062, 0.2449 s / batch. (data: 4.22e-05)max mem: 15.94594 GB 
[10/30 09:27:46 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.2322, average loss: 0.9619
[10/30 09:27:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.54	
[10/30 09:27:46 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/30 09:28:51 visual_prompt]: 	Training 100/1106. train loss: 1.0318,	0.6176 s / batch. (data: 3.32e-04). ETA=18:11:55, max mem: 15.9 GB 
[10/30 09:29:54 visual_prompt]: 	Training 200/1106. train loss: 1.5361,	0.6440 s / batch. (data: 8.35e-04). ETA=18:57:26, max mem: 15.9 GB 
[10/30 09:30:58 visual_prompt]: 	Training 300/1106. train loss: 0.7970,	0.6313 s / batch. (data: 8.22e-04). ETA=18:33:58, max mem: 15.9 GB 
[10/30 09:32:01 visual_prompt]: 	Training 400/1106. train loss: 1.0483,	0.6327 s / batch. (data: 7.76e-04). ETA=18:35:29, max mem: 15.9 GB 
[10/30 09:33:04 visual_prompt]: 	Training 500/1106. train loss: 0.0673,	0.6415 s / batch. (data: 8.22e-04). ETA=18:49:47, max mem: 15.9 GB 
[10/30 09:34:08 visual_prompt]: 	Training 600/1106. train loss: 1.9526,	0.6371 s / batch. (data: 7.67e-04). ETA=18:41:03, max mem: 15.9 GB 
[10/30 09:35:11 visual_prompt]: 	Training 700/1106. train loss: 0.6671,	0.6344 s / batch. (data: 7.94e-04). ETA=18:35:16, max mem: 15.9 GB 
[10/30 09:36:14 visual_prompt]: 	Training 800/1106. train loss: 1.4336,	0.6222 s / batch. (data: 2.82e-04). ETA=18:12:40, max mem: 15.9 GB 
[10/30 09:37:17 visual_prompt]: 	Training 900/1106. train loss: 0.7291,	0.6478 s / batch. (data: 5.85e-03). ETA=18:56:36, max mem: 15.9 GB 
[10/30 09:38:20 visual_prompt]: 	Training 1000/1106. train loss: 0.3338,	0.6308 s / batch. (data: 3.22e-04). ETA=18:25:43, max mem: 15.9 GB 
[10/30 09:39:24 visual_prompt]: 	Training 1100/1106. train loss: 0.7355,	0.6173 s / batch. (data: 1.90e-04). ETA=18:01:04, max mem: 15.9 GB 
[10/30 09:39:27 visual_prompt]: Epoch 5 / 100: avg data time: 4.24e-03, avg batch time: 0.6342, average train loss: 0.8124
[10/30 09:40:17 visual_prompt]: 	Test 100/123. loss: 0.975, 0.2358 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[10/30 09:40:28 visual_prompt]: Inference (val):avg data time: 1.21e-04, avg batch time: 0.2324, average loss: 1.0462
[10/30 09:40:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.97	
[10/30 09:40:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/30 09:41:33 visual_prompt]: 	Training 100/1106. train loss: 1.0600,	0.6472 s / batch. (data: 3.49e-04). ETA=18:52:21, max mem: 15.9 GB 
[10/30 09:42:37 visual_prompt]: 	Training 200/1106. train loss: 0.4057,	0.6188 s / batch. (data: 3.18e-04). ETA=18:01:28, max mem: 15.9 GB 
[10/30 09:43:40 visual_prompt]: 	Training 300/1106. train loss: 0.0518,	0.6183 s / batch. (data: 3.16e-04). ETA=17:59:39, max mem: 15.9 GB 
[10/30 09:44:43 visual_prompt]: 	Training 400/1106. train loss: 1.4263,	0.6320 s / batch. (data: 8.27e-04). ETA=18:22:36, max mem: 15.9 GB 
[10/30 09:45:46 visual_prompt]: 	Training 500/1106. train loss: 0.7379,	0.6389 s / batch. (data: 1.49e-02). ETA=18:33:30, max mem: 15.9 GB 
[10/30 09:46:50 visual_prompt]: 	Training 600/1106. train loss: 0.8129,	0.6262 s / batch. (data: 5.57e-03). ETA=18:10:20, max mem: 15.9 GB 
[10/30 09:47:53 visual_prompt]: 	Training 700/1106. train loss: 0.9052,	0.6303 s / batch. (data: 3.21e-04). ETA=18:16:21, max mem: 15.9 GB 
[10/30 09:48:56 visual_prompt]: 	Training 800/1106. train loss: 0.7069,	0.6344 s / batch. (data: 9.06e-04). ETA=18:22:26, max mem: 15.9 GB 
[10/30 09:50:00 visual_prompt]: 	Training 900/1106. train loss: 0.4007,	0.6567 s / batch. (data: 8.59e-04). ETA=19:00:09, max mem: 15.9 GB 
[10/30 09:51:03 visual_prompt]: 	Training 1000/1106. train loss: 1.7346,	0.6400 s / batch. (data: 1.20e-02). ETA=18:30:05, max mem: 15.9 GB 
[10/30 09:52:06 visual_prompt]: 	Training 1100/1106. train loss: 0.4831,	0.6176 s / batch. (data: 1.45e-04). ETA=17:50:08, max mem: 15.9 GB 
[10/30 09:52:10 visual_prompt]: Epoch 6 / 100: avg data time: 4.38e-03, avg batch time: 0.6348, average train loss: 0.8236
[10/30 09:53:00 visual_prompt]: 	Test 100/123. loss: 0.848, 0.2358 s / batch. (data: 2.86e-05)max mem: 15.94594 GB 
[10/30 09:53:11 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2323, average loss: 0.7907
[10/30 09:53:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.48	
[10/30 09:53:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/30 09:54:16 visual_prompt]: 	Training 100/1106. train loss: 0.7657,	0.6295 s / batch. (data: 3.33e-04). ETA=18:09:45, max mem: 15.9 GB 
[10/30 09:55:19 visual_prompt]: 	Training 200/1106. train loss: 1.5482,	0.6183 s / batch. (data: 2.94e-04). ETA=17:49:17, max mem: 15.9 GB 
[10/30 09:56:22 visual_prompt]: 	Training 300/1106. train loss: 0.2373,	0.6560 s / batch. (data: 3.79e-02). ETA=18:53:18, max mem: 15.9 GB 
[10/30 09:57:25 visual_prompt]: 	Training 400/1106. train loss: 1.0101,	0.6176 s / batch. (data: 3.43e-04). ETA=17:46:04, max mem: 15.9 GB 
[10/30 09:58:29 visual_prompt]: 	Training 500/1106. train loss: 0.6893,	0.6172 s / batch. (data: 3.03e-04). ETA=17:44:15, max mem: 15.9 GB 
[10/30 09:59:32 visual_prompt]: 	Training 600/1106. train loss: 0.7141,	0.6182 s / batch. (data: 3.32e-04). ETA=17:45:01, max mem: 15.9 GB 
[10/30 10:00:35 visual_prompt]: 	Training 700/1106. train loss: 0.4667,	0.6445 s / batch. (data: 8.15e-04). ETA=18:29:09, max mem: 15.9 GB 
[10/30 10:01:39 visual_prompt]: 	Training 800/1106. train loss: 0.3860,	0.6186 s / batch. (data: 3.20e-04). ETA=17:43:37, max mem: 15.9 GB 
[10/30 10:02:42 visual_prompt]: 	Training 900/1106. train loss: 0.6945,	0.6430 s / batch. (data: 7.52e-04). ETA=18:24:30, max mem: 15.9 GB 
[10/30 10:03:45 visual_prompt]: 	Training 1000/1106. train loss: 0.9605,	0.6203 s / batch. (data: 3.24e-04). ETA=17:44:28, max mem: 15.9 GB 
[10/30 10:04:49 visual_prompt]: 	Training 1100/1106. train loss: 0.3315,	0.6175 s / batch. (data: 1.41e-04). ETA=17:38:36, max mem: 15.9 GB 
[10/30 10:04:52 visual_prompt]: Epoch 7 / 100: avg data time: 3.97e-03, avg batch time: 0.6343, average train loss: 0.7800
[10/30 10:05:43 visual_prompt]: 	Test 100/123. loss: 0.770, 0.2253 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/30 10:05:53 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2328, average loss: 0.7310
[10/30 10:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.23	
[10/30 10:05:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/30 10:06:58 visual_prompt]: 	Training 100/1106. train loss: 0.6926,	0.6318 s / batch. (data: 8.03e-04). ETA=18:02:02, max mem: 15.9 GB 
[10/30 10:08:01 visual_prompt]: 	Training 200/1106. train loss: 1.0058,	0.6362 s / batch. (data: 8.11e-04). ETA=18:08:28, max mem: 15.9 GB 
[10/30 10:09:04 visual_prompt]: 	Training 300/1106. train loss: 0.2133,	0.6184 s / batch. (data: 3.07e-04). ETA=17:36:59, max mem: 15.9 GB 
[10/30 10:10:07 visual_prompt]: 	Training 400/1106. train loss: 0.5991,	0.6177 s / batch. (data: 2.34e-04). ETA=17:34:44, max mem: 15.9 GB 
[10/30 10:11:11 visual_prompt]: 	Training 500/1106. train loss: 0.1870,	0.6175 s / batch. (data: 4.09e-04). ETA=17:33:27, max mem: 15.9 GB 
[10/30 10:12:14 visual_prompt]: 	Training 600/1106. train loss: 0.6398,	0.6320 s / batch. (data: 7.73e-04). ETA=17:57:04, max mem: 15.9 GB 
[10/30 10:13:17 visual_prompt]: 	Training 700/1106. train loss: 0.7179,	0.6470 s / batch. (data: 7.86e-04). ETA=18:21:35, max mem: 15.9 GB 
[10/30 10:14:20 visual_prompt]: 	Training 800/1106. train loss: 0.8612,	0.6342 s / batch. (data: 7.59e-04). ETA=17:58:45, max mem: 15.9 GB 
[10/30 10:15:24 visual_prompt]: 	Training 900/1106. train loss: 0.1260,	0.6315 s / batch. (data: 7.23e-04). ETA=17:53:02, max mem: 15.9 GB 
[10/30 10:16:27 visual_prompt]: 	Training 1000/1106. train loss: 1.9471,	0.6324 s / batch. (data: 1.12e-03). ETA=17:53:31, max mem: 15.9 GB 
[10/30 10:17:30 visual_prompt]: 	Training 1100/1106. train loss: 1.0546,	0.6180 s / batch. (data: 1.55e-04). ETA=17:28:11, max mem: 15.9 GB 
[10/30 10:17:34 visual_prompt]: Epoch 8 / 100: avg data time: 3.84e-03, avg batch time: 0.6339, average train loss: 0.9146
[10/30 10:18:25 visual_prompt]: 	Test 100/123. loss: 0.712, 0.2446 s / batch. (data: 3.60e-05)max mem: 15.94594 GB 
[10/30 10:18:35 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.2321, average loss: 0.7277
[10/30 10:18:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.84	
[10/30 10:18:35 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/30 10:19:40 visual_prompt]: 	Training 100/1106. train loss: 0.8051,	0.6380 s / batch. (data: 7.38e-04). ETA=18:00:51, max mem: 15.9 GB 
[10/30 10:20:43 visual_prompt]: 	Training 200/1106. train loss: 0.3334,	0.6296 s / batch. (data: 8.04e-04). ETA=17:45:39, max mem: 15.9 GB 
[10/30 10:21:47 visual_prompt]: 	Training 300/1106. train loss: 0.9432,	0.6181 s / batch. (data: 3.23e-04). ETA=17:25:08, max mem: 15.9 GB 
[10/30 10:22:50 visual_prompt]: 	Training 400/1106. train loss: 0.8593,	0.6198 s / batch. (data: 8.32e-04). ETA=17:27:02, max mem: 15.9 GB 
[10/30 10:23:53 visual_prompt]: 	Training 500/1106. train loss: 0.8909,	0.6700 s / batch. (data: 5.93e-03). ETA=18:50:34, max mem: 15.9 GB 
[10/30 10:24:57 visual_prompt]: 	Training 600/1106. train loss: 0.7534,	0.6432 s / batch. (data: 7.85e-04). ETA=18:04:18, max mem: 15.9 GB 
[10/30 10:26:00 visual_prompt]: 	Training 700/1106. train loss: 1.0957,	0.6188 s / batch. (data: 3.21e-04). ETA=17:22:08, max mem: 15.9 GB 
[10/30 10:27:03 visual_prompt]: 	Training 800/1106. train loss: 0.6822,	0.6186 s / batch. (data: 2.97e-04). ETA=17:20:48, max mem: 15.9 GB 
[10/30 10:28:07 visual_prompt]: 	Training 900/1106. train loss: 0.6951,	0.6178 s / batch. (data: 2.73e-04). ETA=17:18:29, max mem: 15.9 GB 
[10/30 10:29:10 visual_prompt]: 	Training 1000/1106. train loss: 0.8430,	0.6316 s / batch. (data: 7.62e-04). ETA=17:40:33, max mem: 15.9 GB 
[10/30 10:30:13 visual_prompt]: 	Training 1100/1106. train loss: 0.6739,	0.6185 s / batch. (data: 1.51e-04). ETA=17:17:33, max mem: 15.9 GB 
[10/30 10:30:17 visual_prompt]: Epoch 9 / 100: avg data time: 4.66e-03, avg batch time: 0.6347, average train loss: 0.8070
[10/30 10:31:07 visual_prompt]: 	Test 100/123. loss: 0.696, 0.2427 s / batch. (data: 3.84e-05)max mem: 15.94594 GB 
[10/30 10:31:17 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2312, average loss: 0.7038
[10/30 10:31:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.67	
[10/30 10:31:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/30 10:32:23 visual_prompt]: 	Training 100/1106. train loss: 0.7031,	0.6437 s / batch. (data: 7.77e-04). ETA=17:58:44, max mem: 15.9 GB 
[10/30 10:33:26 visual_prompt]: 	Training 200/1106. train loss: 1.8026,	0.6267 s / batch. (data: 3.09e-04). ETA=17:29:10, max mem: 15.9 GB 
[10/30 10:34:29 visual_prompt]: 	Training 300/1106. train loss: 0.7186,	0.6255 s / batch. (data: 4.75e-04). ETA=17:26:05, max mem: 15.9 GB 
[10/30 10:35:32 visual_prompt]: 	Training 400/1106. train loss: 0.8302,	0.6320 s / batch. (data: 8.45e-04). ETA=17:35:54, max mem: 15.9 GB 
[10/30 10:36:35 visual_prompt]: 	Training 500/1106. train loss: 0.7925,	0.6520 s / batch. (data: 1.20e-02). ETA=18:08:15, max mem: 15.9 GB 
[10/30 10:37:38 visual_prompt]: 	Training 600/1106. train loss: 0.4156,	0.6332 s / batch. (data: 3.36e-04). ETA=17:35:51, max mem: 15.9 GB 
[10/30 10:38:41 visual_prompt]: 	Training 700/1106. train loss: 4.1931,	0.6304 s / batch. (data: 2.80e-04). ETA=17:30:09, max mem: 15.9 GB 
[10/30 10:39:45 visual_prompt]: 	Training 800/1106. train loss: 0.7317,	0.6515 s / batch. (data: 1.59e-02). ETA=18:04:09, max mem: 15.9 GB 
[10/30 10:40:48 visual_prompt]: 	Training 900/1106. train loss: 0.6824,	0.6348 s / batch. (data: 7.40e-04). ETA=17:35:15, max mem: 15.9 GB 
[10/30 10:41:51 visual_prompt]: 	Training 1000/1106. train loss: 0.5177,	0.6267 s / batch. (data: 8.00e-03). ETA=17:20:52, max mem: 15.9 GB 
[10/30 10:42:55 visual_prompt]: 	Training 1100/1106. train loss: 0.8830,	0.6177 s / batch. (data: 1.56e-04). ETA=17:04:47, max mem: 15.9 GB 
[10/30 10:42:58 visual_prompt]: Epoch 10 / 100: avg data time: 3.79e-03, avg batch time: 0.6336, average train loss: 0.9640
[10/30 10:43:48 visual_prompt]: 	Test 100/123. loss: 0.694, 0.2382 s / batch. (data: 3.62e-05)max mem: 15.94594 GB 
[10/30 10:43:59 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2319, average loss: 0.6902
[10/30 10:43:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.56	
[10/30 10:43:59 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/30 10:45:04 visual_prompt]: 	Training 100/1106. train loss: 1.5722,	0.6425 s / batch. (data: 1.44e-02). ETA=17:44:45, max mem: 15.9 GB 
[10/30 10:46:08 visual_prompt]: 	Training 200/1106. train loss: 0.7587,	0.6177 s / batch. (data: 2.93e-04). ETA=17:02:40, max mem: 15.9 GB 
[10/30 10:47:11 visual_prompt]: 	Training 300/1106. train loss: 0.7064,	0.6435 s / batch. (data: 8.40e-04). ETA=17:44:23, max mem: 15.9 GB 
[10/30 10:48:14 visual_prompt]: 	Training 400/1106. train loss: 1.1509,	0.6255 s / batch. (data: 3.22e-04). ETA=17:13:35, max mem: 15.9 GB 
[10/30 10:49:17 visual_prompt]: 	Training 500/1106. train loss: 1.7412,	0.6188 s / batch. (data: 7.87e-04). ETA=17:01:30, max mem: 15.9 GB 
[10/30 10:50:20 visual_prompt]: 	Training 600/1106. train loss: 0.1436,	0.6406 s / batch. (data: 8.20e-04). ETA=17:36:23, max mem: 15.9 GB 
[10/30 10:51:23 visual_prompt]: 	Training 700/1106. train loss: 0.4564,	0.6474 s / batch. (data: 8.33e-04). ETA=17:46:29, max mem: 15.9 GB 
[10/30 10:52:26 visual_prompt]: 	Training 800/1106. train loss: 0.8211,	0.6363 s / batch. (data: 1.05e-02). ETA=17:27:09, max mem: 15.9 GB 
[10/30 10:53:30 visual_prompt]: 	Training 900/1106. train loss: 0.3213,	0.6342 s / batch. (data: 1.12e-03). ETA=17:22:41, max mem: 15.9 GB 
[10/30 10:54:33 visual_prompt]: 	Training 1000/1106. train loss: 0.3877,	0.6239 s / batch. (data: 2.79e-04). ETA=17:04:40, max mem: 15.9 GB 
[10/30 10:55:36 visual_prompt]: 	Training 1100/1106. train loss: 0.7109,	0.6182 s / batch. (data: 1.49e-04). ETA=16:54:17, max mem: 15.9 GB 
[10/30 10:55:40 visual_prompt]: Epoch 11 / 100: avg data time: 4.47e-03, avg batch time: 0.6338, average train loss: 0.8372
[10/30 10:56:30 visual_prompt]: 	Test 100/123. loss: 0.730, 0.2255 s / batch. (data: 3.62e-05)max mem: 15.94594 GB 
[10/30 10:56:41 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.2310, average loss: 0.7041
[10/30 10:56:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.37	
[10/30 10:56:41 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/30 10:57:47 visual_prompt]: 	Training 100/1106. train loss: 0.2063,	0.6362 s / batch. (data: 8.41e-04). ETA=17:22:40, max mem: 15.9 GB 
[10/30 10:58:51 visual_prompt]: 	Training 200/1106. train loss: 0.7674,	0.6227 s / batch. (data: 3.07e-04). ETA=16:59:34, max mem: 15.9 GB 
[10/30 10:59:54 visual_prompt]: 	Training 300/1106. train loss: 0.6684,	0.6218 s / batch. (data: 3.42e-04). ETA=16:57:00, max mem: 15.9 GB 
[10/30 11:00:58 visual_prompt]: 	Training 400/1106. train loss: 0.5208,	0.6338 s / batch. (data: 7.79e-04). ETA=17:15:36, max mem: 15.9 GB 
[10/30 11:02:01 visual_prompt]: 	Training 500/1106. train loss: 2.1155,	0.6205 s / batch. (data: 2.99e-04). ETA=16:52:49, max mem: 15.9 GB 
[10/30 11:03:04 visual_prompt]: 	Training 600/1106. train loss: 1.1880,	0.6250 s / batch. (data: 3.19e-04). ETA=16:59:06, max mem: 15.9 GB 
[10/30 11:04:08 visual_prompt]: 	Training 700/1106. train loss: 0.7443,	0.6415 s / batch. (data: 1.42e-02). ETA=17:24:52, max mem: 15.9 GB 
[10/30 11:05:11 visual_prompt]: 	Training 800/1106. train loss: 0.7625,	0.6337 s / batch. (data: 3.06e-04). ETA=17:11:11, max mem: 15.9 GB 
[10/30 11:06:14 visual_prompt]: 	Training 900/1106. train loss: 0.2206,	0.6199 s / batch. (data: 3.25e-04). ETA=16:47:41, max mem: 15.9 GB 
[10/30 11:07:18 visual_prompt]: 	Training 1000/1106. train loss: 0.9328,	0.6387 s / batch. (data: 7.82e-04). ETA=17:17:14, max mem: 15.9 GB 
[10/30 11:08:21 visual_prompt]: 	Training 1100/1106. train loss: 0.7826,	0.6178 s / batch. (data: 1.48e-04). ETA=16:42:11, max mem: 15.9 GB 
[10/30 11:08:25 visual_prompt]: Epoch 12 / 100: avg data time: 5.41e-03, avg batch time: 0.6365, average train loss: 0.8454
[10/30 11:09:15 visual_prompt]: 	Test 100/123. loss: 1.347, 0.2405 s / batch. (data: 3.31e-05)max mem: 15.94594 GB 
[10/30 11:09:26 visual_prompt]: Inference (val):avg data time: 1.08e-04, avg batch time: 0.2317, average loss: 1.4683
[10/30 11:09:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.28	
[10/30 11:09:26 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/30 11:10:32 visual_prompt]: 	Training 100/1106. train loss: 2.1951,	0.6193 s / batch. (data: 3.12e-04). ETA=16:43:33, max mem: 15.9 GB 
[10/30 11:11:35 visual_prompt]: 	Training 200/1106. train loss: 0.7656,	0.6282 s / batch. (data: 3.40e-04). ETA=16:56:59, max mem: 15.9 GB 
[10/30 11:12:38 visual_prompt]: 	Training 300/1106. train loss: 0.2635,	0.6312 s / batch. (data: 8.12e-04). ETA=17:00:41, max mem: 15.9 GB 
[10/30 11:13:41 visual_prompt]: 	Training 400/1106. train loss: 0.8264,	0.6197 s / batch. (data: 6.34e-04). ETA=16:41:03, max mem: 15.9 GB 
[10/30 11:14:45 visual_prompt]: 	Training 500/1106. train loss: 0.1793,	0.6186 s / batch. (data: 2.50e-04). ETA=16:38:17, max mem: 15.9 GB 
[10/30 11:15:48 visual_prompt]: 	Training 600/1106. train loss: 0.9525,	0.6315 s / batch. (data: 7.83e-04). ETA=16:57:59, max mem: 15.9 GB 
[10/30 11:16:51 visual_prompt]: 	Training 700/1106. train loss: 0.7515,	0.6200 s / batch. (data: 2.67e-04). ETA=16:38:28, max mem: 15.9 GB 
[10/30 11:17:55 visual_prompt]: 	Training 800/1106. train loss: 1.2178,	0.6416 s / batch. (data: 3.21e-04). ETA=17:12:08, max mem: 15.9 GB 
[10/30 11:18:58 visual_prompt]: 	Training 900/1106. train loss: 0.9458,	0.6446 s / batch. (data: 2.09e-02). ETA=17:15:54, max mem: 15.9 GB 
[10/30 11:20:01 visual_prompt]: 	Training 1000/1106. train loss: 0.3566,	0.6327 s / batch. (data: 8.38e-04). ETA=16:55:47, max mem: 15.9 GB 
[10/30 11:21:05 visual_prompt]: 	Training 1100/1106. train loss: 0.6978,	0.6198 s / batch. (data: 1.86e-04). ETA=16:34:01, max mem: 15.9 GB 
[10/30 11:21:09 visual_prompt]: Epoch 13 / 100: avg data time: 4.58e-03, avg batch time: 0.6354, average train loss: 0.8409
[10/30 11:21:59 visual_prompt]: 	Test 100/123. loss: 0.903, 0.2315 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[10/30 11:22:09 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.2323, average loss: 0.8391
[10/30 11:22:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.15	
[10/30 11:22:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/30 11:23:15 visual_prompt]: 	Training 100/1106. train loss: 0.8695,	0.6389 s / batch. (data: 7.27e-04). ETA=17:03:33, max mem: 15.9 GB 
[10/30 11:24:18 visual_prompt]: 	Training 200/1106. train loss: 0.6896,	0.6196 s / batch. (data: 3.23e-04). ETA=16:31:33, max mem: 15.9 GB 
[10/30 11:25:22 visual_prompt]: 	Training 300/1106. train loss: 0.7362,	0.6446 s / batch. (data: 5.95e-03). ETA=17:10:33, max mem: 15.9 GB 
[10/30 11:26:25 visual_prompt]: 	Training 400/1106. train loss: 0.3013,	0.6186 s / batch. (data: 3.17e-04). ETA=16:27:54, max mem: 15.9 GB 
[10/30 11:27:28 visual_prompt]: 	Training 500/1106. train loss: 0.7431,	0.6402 s / batch. (data: 7.75e-04). ETA=17:01:24, max mem: 15.9 GB 
[10/30 11:28:32 visual_prompt]: 	Training 600/1106. train loss: 0.6790,	0.6200 s / batch. (data: 3.32e-04). ETA=16:28:06, max mem: 15.9 GB 
[10/30 11:29:35 visual_prompt]: 	Training 700/1106. train loss: 0.3775,	0.6195 s / batch. (data: 3.66e-04). ETA=16:26:12, max mem: 15.9 GB 
[10/30 11:30:39 visual_prompt]: 	Training 800/1106. train loss: 0.7472,	0.6200 s / batch. (data: 2.93e-04). ETA=16:26:04, max mem: 15.9 GB 
[10/30 11:31:42 visual_prompt]: 	Training 900/1106. train loss: 0.7057,	0.6323 s / batch. (data: 8.15e-04). ETA=16:44:35, max mem: 15.9 GB 
[10/30 11:32:45 visual_prompt]: 	Training 1000/1106. train loss: 0.1036,	0.6200 s / batch. (data: 2.92e-04). ETA=16:23:58, max mem: 15.9 GB 
[10/30 11:33:48 visual_prompt]: 	Training 1100/1106. train loss: 0.7569,	0.6183 s / batch. (data: 1.63e-04). ETA=16:20:15, max mem: 15.9 GB 
[10/30 11:33:52 visual_prompt]: Epoch 14 / 100: avg data time: 4.49e-03, avg batch time: 0.6352, average train loss: 0.8049
[10/30 11:34:43 visual_prompt]: 	Test 100/123. loss: 0.695, 0.2393 s / batch. (data: 3.65e-05)max mem: 15.94594 GB 
[10/30 11:34:53 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2324, average loss: 0.6890
[10/30 11:34:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.40	
[10/30 11:34:53 visual_prompt]: Best epoch 14: best metric: -0.689
[10/30 11:34:53 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/30 11:35:58 visual_prompt]: 	Training 100/1106. train loss: 0.8000,	0.6518 s / batch. (data: 8.22e-04). ETA=17:12:12, max mem: 15.9 GB 
[10/30 11:37:01 visual_prompt]: 	Training 200/1106. train loss: 0.7070,	0.6195 s / batch. (data: 2.80e-04). ETA=16:20:04, max mem: 15.9 GB 
[10/30 11:38:04 visual_prompt]: 	Training 300/1106. train loss: 0.7054,	0.6234 s / batch. (data: 5.44e-03). ETA=16:25:12, max mem: 15.9 GB 
[10/30 11:39:08 visual_prompt]: 	Training 400/1106. train loss: 3.8444,	0.6184 s / batch. (data: 4.93e-04). ETA=16:16:08, max mem: 15.9 GB 
[10/30 11:40:11 visual_prompt]: 	Training 500/1106. train loss: 0.8018,	0.6531 s / batch. (data: 1.56e-02). ETA=17:09:56, max mem: 15.9 GB 
[10/30 11:41:15 visual_prompt]: 	Training 600/1106. train loss: 0.7664,	0.6440 s / batch. (data: 1.20e-02). ETA=16:54:30, max mem: 15.9 GB 
[10/30 11:42:18 visual_prompt]: 	Training 700/1106. train loss: 0.6917,	0.6319 s / batch. (data: 3.18e-04). ETA=16:34:23, max mem: 15.9 GB 
[10/30 11:43:21 visual_prompt]: 	Training 800/1106. train loss: 0.6941,	0.6402 s / batch. (data: 1.04e-02). ETA=16:46:20, max mem: 15.9 GB 
[10/30 11:44:25 visual_prompt]: 	Training 900/1106. train loss: 0.8587,	0.6423 s / batch. (data: 7.62e-04). ETA=16:48:36, max mem: 15.9 GB 
[10/30 11:45:28 visual_prompt]: 	Training 1000/1106. train loss: 0.9790,	0.6318 s / batch. (data: 8.33e-04). ETA=16:31:06, max mem: 15.9 GB 
[10/30 11:46:32 visual_prompt]: 	Training 1100/1106. train loss: 1.3589,	0.6196 s / batch. (data: 1.60e-04). ETA=16:10:54, max mem: 15.9 GB 
[10/30 11:46:35 visual_prompt]: Epoch 15 / 100: avg data time: 4.22e-03, avg batch time: 0.6351, average train loss: 0.8721
[10/30 11:47:26 visual_prompt]: 	Test 100/123. loss: 0.841, 0.2329 s / batch. (data: 4.36e-05)max mem: 15.94594 GB 
[10/30 11:47:36 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2330, average loss: 0.7862
[10/30 11:47:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.55	
[10/30 11:47:36 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/30 11:48:42 visual_prompt]: 	Training 100/1106. train loss: 0.4396,	0.6333 s / batch. (data: 7.98e-04). ETA=16:31:14, max mem: 15.9 GB 
[10/30 11:49:45 visual_prompt]: 	Training 200/1106. train loss: 1.0022,	0.6239 s / batch. (data: 3.12e-04). ETA=16:15:25, max mem: 15.9 GB 
[10/30 11:50:48 visual_prompt]: 	Training 300/1106. train loss: 0.3917,	0.6258 s / batch. (data: 3.24e-04). ETA=16:17:27, max mem: 15.9 GB 
[10/30 11:51:52 visual_prompt]: 	Training 400/1106. train loss: 0.7910,	0.6341 s / batch. (data: 8.49e-04). ETA=16:29:15, max mem: 15.9 GB 
[10/30 11:52:55 visual_prompt]: 	Training 500/1106. train loss: 0.7240,	0.6334 s / batch. (data: 8.06e-04). ETA=16:27:13, max mem: 15.9 GB 
[10/30 11:53:59 visual_prompt]: 	Training 600/1106. train loss: 1.0607,	0.6195 s / batch. (data: 3.09e-04). ETA=16:04:31, max mem: 15.9 GB 
[10/30 11:55:02 visual_prompt]: 	Training 700/1106. train loss: 0.7123,	0.6356 s / batch. (data: 2.93e-04). ETA=16:28:30, max mem: 15.9 GB 
[10/30 11:56:06 visual_prompt]: 	Training 800/1106. train loss: 0.7917,	0.6295 s / batch. (data: 1.05e-02). ETA=16:17:58, max mem: 15.9 GB 
[10/30 11:57:09 visual_prompt]: 	Training 900/1106. train loss: 0.7097,	0.6372 s / batch. (data: 8.55e-04). ETA=16:28:52, max mem: 15.9 GB 
[10/30 11:58:12 visual_prompt]: 	Training 1000/1106. train loss: 0.8833,	0.6322 s / batch. (data: 8.38e-04). ETA=16:19:59, max mem: 15.9 GB 
[10/30 11:59:16 visual_prompt]: 	Training 1100/1106. train loss: 0.1818,	0.6181 s / batch. (data: 1.52e-04). ETA=15:57:04, max mem: 15.9 GB 
[10/30 11:59:20 visual_prompt]: Epoch 16 / 100: avg data time: 4.07e-03, avg batch time: 0.6359, average train loss: 0.8061
[10/30 12:00:10 visual_prompt]: 	Test 100/123. loss: 0.693, 0.2247 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/30 12:00:21 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2319, average loss: 0.6907
[10/30 12:00:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.17	
[10/30 12:00:21 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/30 12:01:26 visual_prompt]: 	Training 100/1106. train loss: 0.7903,	0.6264 s / batch. (data: 2.96e-04). ETA=16:08:55, max mem: 15.9 GB 
[10/30 12:02:29 visual_prompt]: 	Training 200/1106. train loss: 0.7052,	0.6330 s / batch. (data: 8.54e-04). ETA=16:18:05, max mem: 15.9 GB 
[10/30 12:03:33 visual_prompt]: 	Training 300/1106. train loss: 0.8618,	0.6242 s / batch. (data: 3.26e-04). ETA=16:03:23, max mem: 15.9 GB 
[10/30 12:04:36 visual_prompt]: 	Training 400/1106. train loss: 2.5285,	0.6184 s / batch. (data: 3.33e-04). ETA=15:53:24, max mem: 15.9 GB 
[10/30 12:05:39 visual_prompt]: 	Training 500/1106. train loss: 0.1607,	0.6185 s / batch. (data: 2.94e-04). ETA=15:52:33, max mem: 15.9 GB 
[10/30 12:06:43 visual_prompt]: 	Training 600/1106. train loss: 0.9152,	0.6534 s / batch. (data: 5.90e-03). ETA=16:45:09, max mem: 15.9 GB 
[10/30 12:07:46 visual_prompt]: 	Training 700/1106. train loss: 0.7058,	0.6486 s / batch. (data: 7.84e-04). ETA=16:36:45, max mem: 15.9 GB 
[10/30 12:08:49 visual_prompt]: 	Training 800/1106. train loss: 1.3314,	0.6314 s / batch. (data: 3.25e-04). ETA=16:09:11, max mem: 15.9 GB 
[10/30 12:09:52 visual_prompt]: 	Training 900/1106. train loss: 0.6862,	0.6178 s / batch. (data: 2.96e-04). ETA=15:47:20, max mem: 15.9 GB 
[10/30 12:10:56 visual_prompt]: 	Training 1000/1106. train loss: 0.0776,	0.6442 s / batch. (data: 8.39e-04). ETA=16:26:42, max mem: 15.9 GB 
[10/30 12:11:59 visual_prompt]: 	Training 1100/1106. train loss: 0.2449,	0.6179 s / batch. (data: 1.37e-04). ETA=15:45:26, max mem: 15.9 GB 
[10/30 12:12:03 visual_prompt]: Epoch 17 / 100: avg data time: 4.79e-03, avg batch time: 0.6349, average train loss: 0.8692
[10/30 12:12:53 visual_prompt]: 	Test 100/123. loss: 0.695, 0.2254 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[10/30 12:13:04 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2334, average loss: 0.6890
[10/30 12:13:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.50	
[10/30 12:13:04 visual_prompt]: Best epoch 17: best metric: -0.689
[10/30 12:13:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/30 12:14:09 visual_prompt]: 	Training 100/1106. train loss: 0.6054,	0.6244 s / batch. (data: 2.74e-04). ETA=15:54:13, max mem: 15.9 GB 
[10/30 12:15:12 visual_prompt]: 	Training 200/1106. train loss: 0.5784,	0.6301 s / batch. (data: 3.00e-04). ETA=16:01:59, max mem: 15.9 GB 
[10/30 12:16:15 visual_prompt]: 	Training 300/1106. train loss: 1.4294,	0.6309 s / batch. (data: 8.40e-04). ETA=16:02:02, max mem: 15.9 GB 
[10/30 12:17:19 visual_prompt]: 	Training 400/1106. train loss: 0.8250,	0.6576 s / batch. (data: 7.52e-04). ETA=16:41:41, max mem: 15.9 GB 
[10/30 12:18:22 visual_prompt]: 	Training 500/1106. train loss: 0.3528,	0.6239 s / batch. (data: 5.40e-03). ETA=15:49:24, max mem: 15.9 GB 
[10/30 12:19:25 visual_prompt]: 	Training 600/1106. train loss: 0.3837,	0.6333 s / batch. (data: 1.18e-03). ETA=16:02:34, max mem: 15.9 GB 
[10/30 12:20:28 visual_prompt]: 	Training 700/1106. train loss: 0.8938,	0.6333 s / batch. (data: 7.84e-04). ETA=16:01:28, max mem: 15.9 GB 
[10/30 12:21:32 visual_prompt]: 	Training 800/1106. train loss: 0.8265,	0.6189 s / batch. (data: 3.02e-04). ETA=15:38:34, max mem: 15.9 GB 
[10/30 12:22:35 visual_prompt]: 	Training 900/1106. train loss: 1.1735,	0.6217 s / batch. (data: 2.75e-04). ETA=15:41:54, max mem: 15.9 GB 
[10/30 12:23:38 visual_prompt]: 	Training 1000/1106. train loss: 0.7513,	0.6316 s / batch. (data: 2.93e-04). ETA=15:55:45, max mem: 15.9 GB 
[10/30 12:24:41 visual_prompt]: 	Training 1100/1106. train loss: 0.3370,	0.6182 s / batch. (data: 1.54e-04). ETA=15:34:30, max mem: 15.9 GB 
[10/30 12:24:45 visual_prompt]: Epoch 18 / 100: avg data time: 4.33e-03, avg batch time: 0.6343, average train loss: 0.8833
[10/30 12:25:35 visual_prompt]: 	Test 100/123. loss: 0.868, 0.2248 s / batch. (data: 4.51e-05)max mem: 15.94594 GB 
[10/30 12:25:46 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.2337, average loss: 0.8087
[10/30 12:25:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.24	
[10/30 12:25:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/30 12:26:52 visual_prompt]: 	Training 100/1106. train loss: 0.2666,	0.6465 s / batch. (data: 7.98e-04). ETA=16:16:04, max mem: 15.9 GB 
[10/30 12:27:55 visual_prompt]: 	Training 200/1106. train loss: 0.7919,	0.6331 s / batch. (data: 3.13e-04). ETA=15:54:47, max mem: 15.9 GB 
[10/30 12:28:59 visual_prompt]: 	Training 300/1106. train loss: 0.4313,	0.6539 s / batch. (data: 1.10e-02). ETA=16:25:06, max mem: 15.9 GB 
[10/30 12:30:02 visual_prompt]: 	Training 400/1106. train loss: 0.9051,	0.6201 s / batch. (data: 7.62e-04). ETA=15:33:09, max mem: 15.9 GB 
[10/30 12:31:05 visual_prompt]: 	Training 500/1106. train loss: 0.4138,	0.6185 s / batch. (data: 3.23e-04). ETA=15:29:48, max mem: 15.9 GB 
[10/30 12:32:09 visual_prompt]: 	Training 600/1106. train loss: 1.2703,	0.6202 s / batch. (data: 6.51e-04). ETA=15:31:10, max mem: 15.9 GB 
[10/30 12:33:12 visual_prompt]: 	Training 700/1106. train loss: 0.7404,	0.6309 s / batch. (data: 7.46e-04). ETA=15:46:19, max mem: 15.9 GB 
[10/30 12:34:16 visual_prompt]: 	Training 800/1106. train loss: 0.1726,	0.6348 s / batch. (data: 8.11e-04). ETA=15:51:04, max mem: 15.9 GB 
[10/30 12:35:19 visual_prompt]: 	Training 900/1106. train loss: 0.6296,	0.6312 s / batch. (data: 8.21e-04). ETA=15:44:39, max mem: 15.9 GB 
[10/30 12:36:22 visual_prompt]: 	Training 1000/1106. train loss: 0.6098,	0.6372 s / batch. (data: 2.95e-04). ETA=15:52:28, max mem: 15.9 GB 
[10/30 12:37:26 visual_prompt]: 	Training 1100/1106. train loss: 1.2723,	0.6317 s / batch. (data: 1.72e-04). ETA=15:43:11, max mem: 15.9 GB 
[10/30 12:37:29 visual_prompt]: Epoch 19 / 100: avg data time: 4.18e-03, avg batch time: 0.6358, average train loss: 0.7761
[10/30 12:38:20 visual_prompt]: 	Test 100/123. loss: 1.242, 0.2356 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/30 12:38:30 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2332, average loss: 1.1304
[10/30 12:38:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.04	
[10/30 12:38:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/30 12:39:36 visual_prompt]: 	Training 100/1106. train loss: 0.6851,	0.6387 s / batch. (data: 8.85e-03). ETA=15:52:34, max mem: 15.9 GB 
[10/30 12:40:39 visual_prompt]: 	Training 200/1106. train loss: 0.5547,	0.6457 s / batch. (data: 8.29e-04). ETA=16:01:52, max mem: 15.9 GB 
[10/30 12:41:43 visual_prompt]: 	Training 300/1106. train loss: 0.7552,	0.6485 s / batch. (data: 8.08e-04). ETA=16:05:00, max mem: 15.9 GB 
[10/30 12:42:46 visual_prompt]: 	Training 400/1106. train loss: 0.7860,	0.6503 s / batch. (data: 1.10e-02). ETA=16:06:35, max mem: 15.9 GB 
[10/30 12:43:50 visual_prompt]: 	Training 500/1106. train loss: 0.7005,	0.6481 s / batch. (data: 3.34e-04). ETA=16:02:13, max mem: 15.9 GB 
[10/30 12:44:53 visual_prompt]: 	Training 600/1106. train loss: 0.8651,	0.6230 s / batch. (data: 7.73e-04). ETA=15:23:57, max mem: 15.9 GB 
[10/30 12:45:56 visual_prompt]: 	Training 700/1106. train loss: 0.4428,	0.6435 s / batch. (data: 1.54e-02). ETA=15:53:14, max mem: 15.9 GB 
[10/30 12:47:00 visual_prompt]: 	Training 800/1106. train loss: 0.3760,	0.6193 s / batch. (data: 3.13e-04). ETA=15:16:26, max mem: 15.9 GB 
[10/30 12:48:03 visual_prompt]: 	Training 900/1106. train loss: 0.5284,	0.6183 s / batch. (data: 3.01e-04). ETA=15:13:53, max mem: 15.9 GB 
[10/30 12:49:06 visual_prompt]: 	Training 1000/1106. train loss: 0.6588,	0.6634 s / batch. (data: 3.63e-02). ETA=16:19:31, max mem: 15.9 GB 
[10/30 12:50:10 visual_prompt]: 	Training 1100/1106. train loss: 0.4628,	0.6178 s / batch. (data: 2.04e-04). ETA=15:11:07, max mem: 15.9 GB 
[10/30 12:50:14 visual_prompt]: Epoch 20 / 100: avg data time: 4.92e-03, avg batch time: 0.6359, average train loss: 0.8006
[10/30 12:51:03 visual_prompt]: 	Test 100/123. loss: 0.927, 0.2256 s / batch. (data: 2.77e-05)max mem: 15.94594 GB 
[10/30 12:51:14 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.2335, average loss: 0.9978
[10/30 12:51:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.14	
[10/30 12:51:14 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/30 12:52:20 visual_prompt]: 	Training 100/1106. train loss: 0.5965,	0.6301 s / batch. (data: 2.97e-04). ETA=15:28:10, max mem: 15.9 GB 
[10/30 12:53:23 visual_prompt]: 	Training 200/1106. train loss: 0.5497,	0.6651 s / batch. (data: 5.86e-03). ETA=16:18:30, max mem: 15.9 GB 
[10/30 12:54:26 visual_prompt]: 	Training 300/1106. train loss: 0.7483,	0.6190 s / batch. (data: 3.24e-04). ETA=15:09:44, max mem: 15.9 GB 
[10/30 12:55:29 visual_prompt]: 	Training 400/1106. train loss: 0.6956,	0.6179 s / batch. (data: 3.12e-04). ETA=15:07:03, max mem: 15.9 GB 
[10/30 12:56:33 visual_prompt]: 	Training 500/1106. train loss: 1.0478,	0.6186 s / batch. (data: 3.53e-04). ETA=15:07:06, max mem: 15.9 GB 
[10/30 12:57:36 visual_prompt]: 	Training 600/1106. train loss: 2.0415,	0.6230 s / batch. (data: 4.16e-04). ETA=15:12:26, max mem: 15.9 GB 
[10/30 12:58:39 visual_prompt]: 	Training 700/1106. train loss: 1.2284,	0.6183 s / batch. (data: 3.08e-04). ETA=15:04:33, max mem: 15.9 GB 
[10/30 12:59:42 visual_prompt]: 	Training 800/1106. train loss: 0.7315,	0.6432 s / batch. (data: 8.08e-04). ETA=15:39:51, max mem: 15.9 GB 
[10/30 13:00:46 visual_prompt]: 	Training 900/1106. train loss: 0.0586,	0.6560 s / batch. (data: 7.95e-04). ETA=15:57:33, max mem: 15.9 GB 
[10/30 13:01:49 visual_prompt]: 	Training 1000/1106. train loss: 0.6977,	0.6177 s / batch. (data: 3.34e-04). ETA=15:00:35, max mem: 15.9 GB 
[10/30 13:02:52 visual_prompt]: 	Training 1100/1106. train loss: 0.9600,	0.6170 s / batch. (data: 1.38e-04). ETA=14:58:35, max mem: 15.9 GB 
[10/30 13:02:56 visual_prompt]: Epoch 21 / 100: avg data time: 4.20e-03, avg batch time: 0.6341, average train loss: 0.7859
[10/30 13:03:46 visual_prompt]: 	Test 100/123. loss: 0.694, 0.2280 s / batch. (data: 2.26e-05)max mem: 15.94594 GB 
[10/30 13:03:56 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.2320, average loss: 0.6987
[10/30 13:03:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.81	
[10/30 13:03:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/30 13:05:01 visual_prompt]: 	Training 100/1106. train loss: 1.3319,	0.6430 s / batch. (data: 8.30e-04). ETA=15:35:20, max mem: 15.9 GB 
[10/30 13:06:04 visual_prompt]: 	Training 200/1106. train loss: 0.5996,	0.6256 s / batch. (data: 2.92e-04). ETA=15:08:52, max mem: 15.9 GB 
[10/30 13:07:08 visual_prompt]: 	Training 300/1106. train loss: 0.6168,	0.6318 s / batch. (data: 8.20e-04). ETA=15:16:56, max mem: 15.9 GB 
[10/30 13:08:11 visual_prompt]: 	Training 400/1106. train loss: 0.1268,	0.6179 s / batch. (data: 2.85e-04). ETA=14:55:38, max mem: 15.9 GB 
[10/30 13:09:14 visual_prompt]: 	Training 500/1106. train loss: 0.3247,	0.6183 s / batch. (data: 3.31e-04). ETA=14:55:15, max mem: 15.9 GB 
[10/30 13:10:17 visual_prompt]: 	Training 600/1106. train loss: 0.3023,	0.6365 s / batch. (data: 6.37e-04). ETA=15:20:33, max mem: 15.9 GB 
[10/30 13:11:21 visual_prompt]: 	Training 700/1106. train loss: 1.2349,	0.6339 s / batch. (data: 8.33e-04). ETA=15:15:40, max mem: 15.9 GB 
[10/30 13:12:24 visual_prompt]: 	Training 800/1106. train loss: 1.0073,	0.6182 s / batch. (data: 3.19e-04). ETA=14:51:57, max mem: 15.9 GB 
[10/30 13:13:27 visual_prompt]: 	Training 900/1106. train loss: 0.7168,	0.6405 s / batch. (data: 7.82e-04). ETA=15:23:04, max mem: 15.9 GB 
[10/30 13:14:30 visual_prompt]: 	Training 1000/1106. train loss: 0.8496,	0.6234 s / batch. (data: 2.93e-04). ETA=14:57:22, max mem: 15.9 GB 
[10/30 13:15:33 visual_prompt]: 	Training 1100/1106. train loss: 0.8635,	0.6170 s / batch. (data: 1.40e-04). ETA=14:47:14, max mem: 15.9 GB 
[10/30 13:15:37 visual_prompt]: Epoch 22 / 100: avg data time: 4.15e-03, avg batch time: 0.6340, average train loss: 0.7853
[10/30 13:16:27 visual_prompt]: 	Test 100/123. loss: 0.730, 0.2402 s / batch. (data: 3.31e-05)max mem: 15.94594 GB 
[10/30 13:16:38 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2323, average loss: 0.7577
[10/30 13:16:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.85	
[10/30 13:16:38 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/30 13:17:44 visual_prompt]: 	Training 100/1106. train loss: 0.5567,	0.6453 s / batch. (data: 7.32e-04). ETA=15:26:41, max mem: 15.9 GB 
[10/30 13:18:47 visual_prompt]: 	Training 200/1106. train loss: 0.8082,	0.6470 s / batch. (data: 8.26e-04). ETA=15:28:02, max mem: 15.9 GB 
[10/30 13:19:51 visual_prompt]: 	Training 300/1106. train loss: 0.7773,	0.6407 s / batch. (data: 8.14e-04). ETA=15:18:00, max mem: 15.9 GB 
[10/30 13:20:54 visual_prompt]: 	Training 400/1106. train loss: 0.7209,	0.6249 s / batch. (data: 3.12e-04). ETA=14:54:22, max mem: 15.9 GB 
[10/30 13:21:57 visual_prompt]: 	Training 500/1106. train loss: 0.7321,	0.6272 s / batch. (data: 1.83e-04). ETA=14:56:36, max mem: 15.9 GB 
[10/30 13:23:00 visual_prompt]: 	Training 600/1106. train loss: 0.6652,	0.6259 s / batch. (data: 2.97e-04). ETA=14:53:38, max mem: 15.9 GB 
[10/30 13:24:04 visual_prompt]: 	Training 700/1106. train loss: 0.8322,	0.6310 s / batch. (data: 7.94e-04). ETA=14:59:54, max mem: 15.9 GB 
[10/30 13:25:07 visual_prompt]: 	Training 800/1106. train loss: 0.8981,	0.6335 s / batch. (data: 7.62e-04). ETA=15:02:24, max mem: 15.9 GB 
[10/30 13:26:10 visual_prompt]: 	Training 900/1106. train loss: 0.7208,	0.6564 s / batch. (data: 3.36e-04). ETA=15:33:54, max mem: 15.9 GB 
[10/30 13:27:13 visual_prompt]: 	Training 1000/1106. train loss: 0.1221,	0.6319 s / batch. (data: 3.14e-04). ETA=14:57:58, max mem: 15.9 GB 
[10/30 13:28:17 visual_prompt]: 	Training 1100/1106. train loss: 0.8900,	0.6190 s / batch. (data: 1.24e-04). ETA=14:38:40, max mem: 15.9 GB 
[10/30 13:28:20 visual_prompt]: Epoch 23 / 100: avg data time: 4.69e-03, avg batch time: 0.6349, average train loss: 0.7971
[10/30 13:29:10 visual_prompt]: 	Test 100/123. loss: 1.015, 0.2249 s / batch. (data: 3.77e-05)max mem: 15.94594 GB 
[10/30 13:29:21 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.2318, average loss: 0.9309
[10/30 13:29:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.60	
[10/30 13:29:21 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/30 13:30:27 visual_prompt]: 	Training 100/1106. train loss: 1.4247,	0.6295 s / batch. (data: 3.29e-04). ETA=14:52:23, max mem: 15.9 GB 
[10/30 13:31:30 visual_prompt]: 	Training 200/1106. train loss: 1.6633,	0.6315 s / batch. (data: 7.97e-04). ETA=14:54:15, max mem: 15.9 GB 
[10/30 13:32:33 visual_prompt]: 	Training 300/1106. train loss: 1.5996,	0.6554 s / batch. (data: 8.17e-04). ETA=15:26:58, max mem: 15.9 GB 
[10/30 13:33:36 visual_prompt]: 	Training 400/1106. train loss: 0.6979,	0.6192 s / batch. (data: 3.20e-04). ETA=14:34:48, max mem: 15.9 GB 
[10/30 13:34:40 visual_prompt]: 	Training 500/1106. train loss: 1.0258,	0.6208 s / batch. (data: 3.03e-04). ETA=14:35:58, max mem: 15.9 GB 
[10/30 13:35:43 visual_prompt]: 	Training 600/1106. train loss: 0.6864,	0.6364 s / batch. (data: 3.17e-04). ETA=14:56:55, max mem: 15.9 GB 
[10/30 13:36:46 visual_prompt]: 	Training 700/1106. train loss: 0.9516,	0.6174 s / batch. (data: 3.00e-04). ETA=14:29:10, max mem: 15.9 GB 
[10/30 13:37:49 visual_prompt]: 	Training 800/1106. train loss: 0.6906,	0.6186 s / batch. (data: 3.14e-04). ETA=14:29:50, max mem: 15.9 GB 
[10/30 13:38:53 visual_prompt]: 	Training 900/1106. train loss: 0.7841,	0.6344 s / batch. (data: 8.15e-04). ETA=14:50:59, max mem: 15.9 GB 
[10/30 13:39:56 visual_prompt]: 	Training 1000/1106. train loss: 0.7754,	0.6196 s / batch. (data: 3.31e-04). ETA=14:29:10, max mem: 15.9 GB 
[10/30 13:40:59 visual_prompt]: 	Training 1100/1106. train loss: 0.9625,	0.6177 s / batch. (data: 1.35e-04). ETA=14:25:24, max mem: 15.9 GB 
[10/30 13:41:03 visual_prompt]: Epoch 24 / 100: avg data time: 4.78e-03, avg batch time: 0.6350, average train loss: 0.9052
[10/30 13:41:53 visual_prompt]: 	Test 100/123. loss: 0.697, 0.2248 s / batch. (data: 4.10e-05)max mem: 15.94594 GB 
[10/30 13:42:04 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.2331, average loss: 0.6882
[10/30 13:42:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.86	
[10/30 13:42:04 visual_prompt]: Best epoch 24: best metric: -0.688
[10/30 13:42:04 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/30 13:43:09 visual_prompt]: 	Training 100/1106. train loss: 1.7401,	0.6301 s / batch. (data: 7.83e-04). ETA=14:41:37, max mem: 15.9 GB 
[10/30 13:44:12 visual_prompt]: 	Training 200/1106. train loss: 0.7514,	0.6464 s / batch. (data: 7.97e-03). ETA=15:03:24, max mem: 15.9 GB 
[10/30 13:45:15 visual_prompt]: 	Training 300/1106. train loss: 1.7281,	0.6331 s / batch. (data: 1.63e-02). ETA=14:43:47, max mem: 15.9 GB 
[10/30 13:46:19 visual_prompt]: 	Training 400/1106. train loss: 0.9807,	0.6336 s / batch. (data: 1.17e-03). ETA=14:43:21, max mem: 15.9 GB 
[10/30 13:47:22 visual_prompt]: 	Training 500/1106. train loss: 1.9911,	0.6176 s / batch. (data: 5.02e-04). ETA=14:20:06, max mem: 15.9 GB 
[10/30 13:48:25 visual_prompt]: 	Training 600/1106. train loss: 0.5365,	0.6181 s / batch. (data: 2.71e-04). ETA=14:19:46, max mem: 15.9 GB 
[10/30 13:49:28 visual_prompt]: 	Training 700/1106. train loss: 0.7002,	0.6240 s / batch. (data: 3.24e-04). ETA=14:26:53, max mem: 15.9 GB 
[10/30 13:50:32 visual_prompt]: 	Training 800/1106. train loss: 0.5222,	0.6215 s / batch. (data: 5.18e-04). ETA=14:22:21, max mem: 15.9 GB 
[10/30 13:51:35 visual_prompt]: 	Training 900/1106. train loss: 0.6834,	0.6176 s / batch. (data: 4.39e-04). ETA=14:15:57, max mem: 15.9 GB 
[10/30 13:52:38 visual_prompt]: 	Training 1000/1106. train loss: 0.7983,	0.6339 s / batch. (data: 7.83e-04). ETA=14:37:31, max mem: 15.9 GB 
[10/30 13:53:42 visual_prompt]: 	Training 1100/1106. train loss: 0.5680,	0.6180 s / batch. (data: 1.42e-04). ETA=14:14:22, max mem: 15.9 GB 
[10/30 13:53:45 visual_prompt]: Epoch 25 / 100: avg data time: 4.43e-03, avg batch time: 0.6342, average train loss: 0.8549
[10/30 13:54:36 visual_prompt]: 	Test 100/123. loss: 1.342, 0.2253 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[10/30 13:54:46 visual_prompt]: Inference (val):avg data time: 3.00e-04, avg batch time: 0.2333, average loss: 1.4661
[10/30 13:54:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.66	
[10/30 13:54:46 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[10/30 13:55:51 visual_prompt]: 	Training 100/1106. train loss: 0.7826,	0.6579 s / batch. (data: 1.61e-02). ETA=15:08:27, max mem: 15.9 GB 
[10/30 13:56:55 visual_prompt]: 	Training 200/1106. train loss: 0.7165,	0.6244 s / batch. (data: 3.04e-04). ETA=14:21:06, max mem: 15.9 GB 
[10/30 13:57:58 visual_prompt]: 	Training 300/1106. train loss: 0.5573,	0.6252 s / batch. (data: 3.13e-04). ETA=14:21:16, max mem: 15.9 GB 
[10/30 13:59:01 visual_prompt]: 	Training 400/1106. train loss: 1.0382,	0.6383 s / batch. (data: 7.67e-04). ETA=14:38:15, max mem: 15.9 GB 
[10/30 14:00:05 visual_prompt]: 	Training 500/1106. train loss: 0.7734,	0.6336 s / batch. (data: 2.49e-04). ETA=14:30:42, max mem: 15.9 GB 
[10/30 14:01:08 visual_prompt]: 	Training 600/1106. train loss: 0.1317,	0.6400 s / batch. (data: 7.79e-04). ETA=14:38:25, max mem: 15.9 GB 
[10/30 14:02:11 visual_prompt]: 	Training 700/1106. train loss: 1.5583,	0.6294 s / batch. (data: 8.21e-04). ETA=14:22:51, max mem: 15.9 GB 
[10/30 14:03:15 visual_prompt]: 	Training 800/1106. train loss: 0.0342,	0.6409 s / batch. (data: 2.99e-04). ETA=14:37:31, max mem: 15.9 GB 
[10/30 14:04:18 visual_prompt]: 	Training 900/1106. train loss: 0.8752,	0.6406 s / batch. (data: 1.12e-03). ETA=14:36:00, max mem: 15.9 GB 
[10/30 14:05:21 visual_prompt]: 	Training 1000/1106. train loss: 1.1907,	0.6309 s / batch. (data: 1.20e-02). ETA=14:21:44, max mem: 15.9 GB 
[10/30 14:06:25 visual_prompt]: 	Training 1100/1106. train loss: 0.6196,	0.6172 s / batch. (data: 1.68e-04). ETA=14:01:59, max mem: 15.9 GB 
[10/30 14:06:28 visual_prompt]: Epoch 26 / 100: avg data time: 4.52e-03, avg batch time: 0.6351, average train loss: 0.8171
[10/30 14:07:18 visual_prompt]: 	Test 100/123. loss: 0.704, 0.2360 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/30 14:07:29 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2325, average loss: 0.7200
[10/30 14:07:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.13	
[10/30 14:07:29 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[10/30 14:08:35 visual_prompt]: 	Training 100/1106. train loss: 0.0772,	0.6216 s / batch. (data: 3.10e-04). ETA=14:06:53, max mem: 15.9 GB 
[10/30 14:09:38 visual_prompt]: 	Training 200/1106. train loss: 0.3487,	0.6472 s / batch. (data: 8.32e-04). ETA=14:40:41, max mem: 15.9 GB 
[10/30 14:10:41 visual_prompt]: 	Training 300/1106. train loss: 1.1822,	0.6332 s / batch. (data: 8.26e-04). ETA=14:20:32, max mem: 15.9 GB 
[10/30 14:11:44 visual_prompt]: 	Training 400/1106. train loss: 2.7233,	0.6350 s / batch. (data: 1.15e-03). ETA=14:21:54, max mem: 15.9 GB 
[10/30 14:12:48 visual_prompt]: 	Training 500/1106. train loss: 1.0521,	0.6390 s / batch. (data: 2.73e-04). ETA=14:26:19, max mem: 15.9 GB 
[10/30 14:13:51 visual_prompt]: 	Training 600/1106. train loss: 0.9972,	0.6229 s / batch. (data: 3.40e-04). ETA=14:03:26, max mem: 15.9 GB 
[10/30 14:14:54 visual_prompt]: 	Training 700/1106. train loss: 1.3912,	0.6301 s / batch. (data: 7.74e-04). ETA=14:12:07, max mem: 15.9 GB 
[10/30 14:15:57 visual_prompt]: 	Training 800/1106. train loss: 0.6956,	0.6588 s / batch. (data: 2.93e-02). ETA=14:49:48, max mem: 15.9 GB 
[10/30 14:17:01 visual_prompt]: 	Training 900/1106. train loss: 0.7497,	0.6181 s / batch. (data: 2.22e-04). ETA=13:53:47, max mem: 15.9 GB 
[10/30 14:18:04 visual_prompt]: 	Training 1000/1106. train loss: 0.4168,	0.6332 s / batch. (data: 7.68e-04). ETA=14:13:10, max mem: 15.9 GB 
[10/30 14:19:07 visual_prompt]: 	Training 1100/1106. train loss: 0.6092,	0.6190 s / batch. (data: 2.52e-04). ETA=13:53:01, max mem: 15.9 GB 
[10/30 14:19:11 visual_prompt]: Epoch 27 / 100: avg data time: 4.85e-03, avg batch time: 0.6345, average train loss: 0.8183
[10/30 14:20:01 visual_prompt]: 	Test 100/123. loss: 0.784, 0.2357 s / batch. (data: 2.77e-05)max mem: 15.94594 GB 
[10/30 14:20:12 visual_prompt]: Inference (val):avg data time: 9.76e-05, avg batch time: 0.2323, average loss: 0.8271
[10/30 14:20:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.25	
[10/30 14:20:12 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[10/30 14:21:17 visual_prompt]: 	Training 100/1106. train loss: 0.9040,	0.6319 s / batch. (data: 7.47e-04). ETA=14:09:13, max mem: 15.9 GB 
[10/30 14:22:20 visual_prompt]: 	Training 200/1106. train loss: 0.0086,	0.6585 s / batch. (data: 8.07e-04). ETA=14:43:51, max mem: 15.9 GB 
[10/30 14:23:23 visual_prompt]: 	Training 300/1106. train loss: 1.1148,	0.6179 s / batch. (data: 2.98e-04). ETA=13:48:25, max mem: 15.9 GB 
[10/30 14:24:26 visual_prompt]: 	Training 400/1106. train loss: 1.2804,	0.6394 s / batch. (data: 5.97e-03). ETA=14:16:05, max mem: 15.9 GB 
[10/30 14:25:30 visual_prompt]: 	Training 500/1106. train loss: 0.7661,	0.6320 s / batch. (data: 8.28e-04). ETA=14:05:12, max mem: 15.9 GB 
[10/30 14:26:33 visual_prompt]: 	Training 600/1106. train loss: 1.0303,	0.6568 s / batch. (data: 5.94e-03). ETA=14:37:11, max mem: 15.9 GB 
[10/30 14:27:36 visual_prompt]: 	Training 700/1106. train loss: 0.4243,	0.6360 s / batch. (data: 8.14e-04). ETA=14:08:22, max mem: 15.9 GB 
[10/30 14:28:39 visual_prompt]: 	Training 800/1106. train loss: 3.0857,	0.6427 s / batch. (data: 8.24e-04). ETA=14:16:14, max mem: 15.9 GB 
[10/30 14:29:42 visual_prompt]: 	Training 900/1106. train loss: 1.1664,	0.6335 s / batch. (data: 8.14e-04). ETA=14:02:54, max mem: 15.9 GB 
[10/30 14:30:45 visual_prompt]: 	Training 1000/1106. train loss: 2.2466,	0.6360 s / batch. (data: 3.33e-04). ETA=14:05:10, max mem: 15.9 GB 
[10/30 14:31:48 visual_prompt]: 	Training 1100/1106. train loss: 0.7414,	0.6172 s / batch. (data: 1.19e-04). ETA=13:39:12, max mem: 15.9 GB 
[10/30 14:31:52 visual_prompt]: Epoch 28 / 100: avg data time: 4.19e-03, avg batch time: 0.6335, average train loss: 0.9717
[10/30 14:32:42 visual_prompt]: 	Test 100/123. loss: 0.699, 0.2319 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/30 14:32:53 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2323, average loss: 0.7135
[10/30 14:32:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.24	
[10/30 14:32:53 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[10/30 14:33:58 visual_prompt]: 	Training 100/1106. train loss: 1.2852,	0.6283 s / batch. (data: 1.20e-02). ETA=13:52:52, max mem: 15.9 GB 
[10/30 14:35:02 visual_prompt]: 	Training 200/1106. train loss: 0.7630,	0.6311 s / batch. (data: 8.08e-04). ETA=13:55:28, max mem: 15.9 GB 
[10/30 14:36:05 visual_prompt]: 	Training 300/1106. train loss: 0.8237,	0.6371 s / batch. (data: 3.32e-04). ETA=14:02:21, max mem: 15.9 GB 
[10/30 14:37:08 visual_prompt]: 	Training 400/1106. train loss: 0.8821,	0.6335 s / batch. (data: 7.66e-04). ETA=13:56:34, max mem: 15.9 GB 
[10/30 14:38:11 visual_prompt]: 	Training 500/1106. train loss: 0.7531,	0.6329 s / batch. (data: 3.66e-04). ETA=13:54:41, max mem: 15.9 GB 
[10/30 14:39:14 visual_prompt]: 	Training 600/1106. train loss: 0.7137,	0.6326 s / batch. (data: 7.73e-04). ETA=13:53:17, max mem: 15.9 GB 
[10/30 14:40:17 visual_prompt]: 	Training 700/1106. train loss: 1.8889,	0.6367 s / batch. (data: 7.74e-04). ETA=13:57:33, max mem: 15.9 GB 
[10/30 14:41:20 visual_prompt]: 	Training 800/1106. train loss: 0.0115,	0.6174 s / batch. (data: 2.85e-04). ETA=13:31:10, max mem: 15.9 GB 
[10/30 14:42:23 visual_prompt]: 	Training 900/1106. train loss: 1.4248,	0.6461 s / batch. (data: 8.62e-04). ETA=14:07:52, max mem: 15.9 GB 
[10/30 14:43:27 visual_prompt]: 	Training 1000/1106. train loss: 1.5401,	0.6180 s / batch. (data: 3.51e-04). ETA=13:29:51, max mem: 15.9 GB 
[10/30 14:44:30 visual_prompt]: 	Training 1100/1106. train loss: 0.7482,	0.6167 s / batch. (data: 1.63e-04). ETA=13:27:09, max mem: 15.9 GB 
[10/30 14:44:34 visual_prompt]: Epoch 29 / 100: avg data time: 3.91e-03, avg batch time: 0.6335, average train loss: 0.9259
[10/30 14:45:23 visual_prompt]: 	Test 100/123. loss: 1.191, 0.2395 s / batch. (data: 3.43e-05)max mem: 15.94594 GB 
[10/30 14:45:34 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2328, average loss: 1.0853
[10/30 14:45:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.59	
[10/30 14:45:34 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[10/30 14:46:40 visual_prompt]: 	Training 100/1106. train loss: 0.6991,	0.6164 s / batch. (data: 4.30e-04). ETA=13:25:45, max mem: 15.9 GB 
[10/30 14:47:43 visual_prompt]: 	Training 200/1106. train loss: 1.1268,	0.6187 s / batch. (data: 2.44e-04). ETA=13:27:37, max mem: 15.9 GB 
[10/30 14:48:46 visual_prompt]: 	Training 300/1106. train loss: 0.7413,	0.6365 s / batch. (data: 3.40e-04). ETA=13:49:49, max mem: 15.9 GB 
[10/30 14:49:50 visual_prompt]: 	Training 400/1106. train loss: 1.1208,	0.6334 s / batch. (data: 1.56e-02). ETA=13:44:43, max mem: 15.9 GB 
[10/30 14:50:53 visual_prompt]: 	Training 500/1106. train loss: 0.7218,	0.6378 s / batch. (data: 5.45e-03). ETA=13:49:23, max mem: 15.9 GB 
[10/30 14:51:56 visual_prompt]: 	Training 600/1106. train loss: 0.2019,	0.6640 s / batch. (data: 7.57e-04). ETA=14:22:25, max mem: 15.9 GB 
[10/30 14:52:59 visual_prompt]: 	Training 700/1106. train loss: 0.8840,	0.6389 s / batch. (data: 7.94e-04). ETA=13:48:42, max mem: 15.9 GB 
[10/30 14:54:03 visual_prompt]: 	Training 800/1106. train loss: 0.6999,	0.6500 s / batch. (data: 5.92e-03). ETA=14:02:03, max mem: 15.9 GB 
[10/30 14:55:06 visual_prompt]: 	Training 900/1106. train loss: 0.7952,	0.6302 s / batch. (data: 3.45e-04). ETA=13:35:18, max mem: 15.9 GB 
[10/30 14:56:09 visual_prompt]: 	Training 1000/1106. train loss: 0.6724,	0.6515 s / batch. (data: 1.10e-02). ETA=14:01:47, max mem: 15.9 GB 
[10/30 14:57:12 visual_prompt]: 	Training 1100/1106. train loss: 0.9236,	0.6174 s / batch. (data: 1.52e-04). ETA=13:16:39, max mem: 15.9 GB 
[10/30 14:57:16 visual_prompt]: Epoch 30 / 100: avg data time: 4.37e-03, avg batch time: 0.6342, average train loss: 0.8506
[10/30 14:58:05 visual_prompt]: 	Test 100/123. loss: 0.716, 0.2251 s / batch. (data: 3.58e-05)max mem: 15.94594 GB 
[10/30 14:58:16 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2331, average loss: 0.7368
[10/30 14:58:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.58	
[10/30 14:58:16 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[10/30 14:59:22 visual_prompt]: 	Training 100/1106. train loss: 1.7099,	0.6222 s / batch. (data: 3.05e-04). ETA=13:21:44, max mem: 15.9 GB 
[10/30 15:00:25 visual_prompt]: 	Training 200/1106. train loss: 1.3106,	0.6294 s / batch. (data: 3.27e-04). ETA=13:29:58, max mem: 15.9 GB 
[10/30 15:01:28 visual_prompt]: 	Training 300/1106. train loss: 0.1160,	0.6252 s / batch. (data: 2.77e-04). ETA=13:23:31, max mem: 15.9 GB 
[10/30 15:02:32 visual_prompt]: 	Training 400/1106. train loss: 0.4227,	0.6178 s / batch. (data: 3.33e-04). ETA=13:13:00, max mem: 15.9 GB 
[10/30 15:03:35 visual_prompt]: 	Training 500/1106. train loss: 1.4038,	0.6187 s / batch. (data: 3.34e-04). ETA=13:13:08, max mem: 15.9 GB 
[10/30 15:04:38 visual_prompt]: 	Training 600/1106. train loss: 2.2244,	0.6280 s / batch. (data: 3.16e-04). ETA=13:24:01, max mem: 15.9 GB 
[10/30 15:05:41 visual_prompt]: 	Training 700/1106. train loss: 2.3721,	0.6335 s / batch. (data: 8.09e-04). ETA=13:30:00, max mem: 15.9 GB 
[10/30 15:06:44 visual_prompt]: 	Training 800/1106. train loss: 0.5629,	0.6185 s / batch. (data: 2.65e-04). ETA=13:09:52, max mem: 15.9 GB 
[10/30 15:07:48 visual_prompt]: 	Training 900/1106. train loss: 0.2561,	0.6297 s / batch. (data: 2.96e-04). ETA=13:23:07, max mem: 15.9 GB 
[10/30 15:08:51 visual_prompt]: 	Training 1000/1106. train loss: 1.0664,	0.6403 s / batch. (data: 1.29e-02). ETA=13:35:34, max mem: 15.9 GB 
[10/30 15:09:54 visual_prompt]: 	Training 1100/1106. train loss: 0.7444,	0.6187 s / batch. (data: 1.55e-04). ETA=13:07:00, max mem: 15.9 GB 
[10/30 15:09:58 visual_prompt]: Epoch 31 / 100: avg data time: 4.90e-03, avg batch time: 0.6344, average train loss: 0.9078
[10/30 15:10:48 visual_prompt]: 	Test 100/123. loss: 1.063, 0.2404 s / batch. (data: 4.32e-05)max mem: 15.94594 GB 
[10/30 15:10:58 visual_prompt]: Inference (val):avg data time: 2.51e-04, avg batch time: 0.2327, average loss: 0.9689
[10/30 15:10:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.25	
[10/30 15:10:58 visual_prompt]: Stopping early.
[10/30 15:10:58 visual_prompt]: Rank of current process: 0. World size: 1
[10/30 15:10:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/30 15:10:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/30 15:10:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/30 15:10:58 visual_prompt]: Training with config:
[10/30 15:10:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/30 15:10:58 visual_prompt]: Loading training data...
[10/30 15:10:58 visual_prompt]: Constructing mammo-cbis dataset train...
[10/30 15:10:58 visual_prompt]: Loading validation data...
[10/30 15:10:58 visual_prompt]: Constructing mammo-cbis dataset val...
[10/30 15:10:58 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/30 15:11:01 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/30 15:11:01 visual_prompt]: tuned percent:0.522
[10/30 15:11:01 visual_prompt]: Device used for model: 0
[10/30 15:11:01 visual_prompt]: Setting up Evaluator...
[10/30 15:11:01 visual_prompt]: Setting up Trainer...
[10/30 15:11:01 visual_prompt]: 	Setting up the optimizer...
[10/30 15:11:01 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/30 15:12:07 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6330 s / batch. (data: 1.47e-02). ETA=19:25:48, max mem: 15.9 GB 
[10/30 15:13:10 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6464 s / batch. (data: 8.61e-04). ETA=19:49:25, max mem: 15.9 GB 
[10/30 15:14:13 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6335 s / batch. (data: 3.19e-04). ETA=19:24:32, max mem: 15.9 GB 
[10/30 15:15:17 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6203 s / batch. (data: 3.14e-04). ETA=18:59:15, max mem: 15.9 GB 
[10/30 15:16:20 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6504 s / batch. (data: 5.43e-03). ETA=19:53:24, max mem: 15.9 GB 
[10/30 15:17:24 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6309 s / batch. (data: 8.08e-04). ETA=19:16:37, max mem: 15.9 GB 
[10/30 15:18:27 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6312 s / batch. (data: 7.84e-04). ETA=19:16:09, max mem: 15.9 GB 
[10/30 15:19:30 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6254 s / batch. (data: 5.45e-03). ETA=19:04:28, max mem: 15.9 GB 
[10/30 15:20:33 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6436 s / batch. (data: 7.72e-04). ETA=19:36:43, max mem: 15.9 GB 
[10/30 15:21:37 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6303 s / batch. (data: 3.32e-04). ETA=19:11:22, max mem: 15.9 GB 
[10/30 15:22:40 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6187 s / batch. (data: 1.46e-04). ETA=18:49:04, max mem: 15.9 GB 
[10/30 15:22:44 visual_prompt]: Epoch 1 / 100: avg data time: 4.88e-03, avg batch time: 0.6354, average train loss: 1.4028
[10/30 15:23:34 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2489 s / batch. (data: 3.70e-05)max mem: 15.94594 GB 
[10/30 15:23:45 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.2331, average loss: 1.3505
[10/30 15:23:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/30 15:23:45 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/30 15:24:50 visual_prompt]: 	Training 100/1106. train loss: 0.6856,	0.6307 s / batch. (data: 3.03e-04). ETA=19:09:55, max mem: 15.9 GB 
[10/30 15:25:53 visual_prompt]: 	Training 200/1106. train loss: 0.6748,	0.6307 s / batch. (data: 3.91e-04). ETA=19:08:55, max mem: 15.9 GB 
[10/30 15:26:57 visual_prompt]: 	Training 300/1106. train loss: 0.9994,	0.6329 s / batch. (data: 5.44e-03). ETA=19:11:48, max mem: 15.9 GB 
[10/30 15:28:00 visual_prompt]: 	Training 400/1106. train loss: 0.1167,	0.6192 s / batch. (data: 3.20e-04). ETA=18:45:55, max mem: 15.9 GB 
[10/30 15:29:03 visual_prompt]: 	Training 500/1106. train loss: 0.6836,	0.6185 s / batch. (data: 3.31e-04). ETA=18:43:35, max mem: 15.9 GB 
[10/30 15:30:07 visual_prompt]: 	Training 600/1106. train loss: 0.5571,	0.6342 s / batch. (data: 8.30e-04). ETA=19:11:05, max mem: 15.9 GB 
[10/30 15:31:10 visual_prompt]: 	Training 700/1106. train loss: 0.9479,	0.6241 s / batch. (data: 3.28e-04). ETA=18:51:42, max mem: 15.9 GB 
[10/30 15:32:13 visual_prompt]: 	Training 800/1106. train loss: 0.7632,	0.6487 s / batch. (data: 5.94e-03). ETA=19:35:06, max mem: 15.9 GB 
[10/30 15:33:16 visual_prompt]: 	Training 900/1106. train loss: 0.6069,	0.6243 s / batch. (data: 3.08e-04). ETA=18:49:54, max mem: 15.9 GB 
[10/30 15:34:19 visual_prompt]: 	Training 1000/1106. train loss: 0.5743,	0.6179 s / batch. (data: 2.84e-04). ETA=18:37:23, max mem: 15.9 GB 
[10/30 15:35:23 visual_prompt]: 	Training 1100/1106. train loss: 0.6026,	0.6188 s / batch. (data: 1.49e-04). ETA=18:37:54, max mem: 15.9 GB 
[10/30 15:35:26 visual_prompt]: Epoch 2 / 100: avg data time: 4.02e-03, avg batch time: 0.6339, average train loss: 0.7981
[10/30 15:36:17 visual_prompt]: 	Test 100/123. loss: 1.128, 0.2399 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/30 15:36:27 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.2334, average loss: 1.0368
[10/30 15:36:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.75	
[10/30 15:36:27 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/30 15:37:33 visual_prompt]: 	Training 100/1106. train loss: 0.7159,	0.6167 s / batch. (data: 3.33e-04). ETA=18:32:59, max mem: 15.9 GB 
[10/30 15:38:36 visual_prompt]: 	Training 200/1106. train loss: 0.2301,	0.6178 s / batch. (data: 8.08e-04). ETA=18:33:56, max mem: 15.9 GB 
[10/30 15:39:40 visual_prompt]: 	Training 300/1106. train loss: 0.4013,	0.6207 s / batch. (data: 3.55e-04). ETA=18:38:04, max mem: 15.9 GB 
[10/30 15:40:43 visual_prompt]: 	Training 400/1106. train loss: 0.8445,	0.6396 s / batch. (data: 8.02e-04). ETA=19:11:12, max mem: 15.9 GB 
[10/30 15:41:47 visual_prompt]: 	Training 500/1106. train loss: 1.0925,	0.6336 s / batch. (data: 1.06e-03). ETA=18:59:18, max mem: 15.9 GB 
[10/30 15:42:50 visual_prompt]: 	Training 600/1106. train loss: 0.6772,	0.6196 s / batch. (data: 3.30e-04). ETA=18:33:09, max mem: 15.9 GB 
[10/30 15:43:53 visual_prompt]: 	Training 700/1106. train loss: 1.0648,	0.6423 s / batch. (data: 8.13e-04). ETA=19:12:44, max mem: 15.9 GB 
[10/30 15:44:56 visual_prompt]: 	Training 800/1106. train loss: 1.8124,	0.6476 s / batch. (data: 8.01e-04). ETA=19:21:12, max mem: 15.9 GB 
[10/30 15:46:00 visual_prompt]: 	Training 900/1106. train loss: 1.4019,	0.6328 s / batch. (data: 1.05e-02). ETA=18:53:41, max mem: 15.9 GB 
[10/30 15:47:03 visual_prompt]: 	Training 1000/1106. train loss: 0.6594,	0.6295 s / batch. (data: 7.23e-04). ETA=18:46:44, max mem: 15.9 GB 
[10/30 15:48:06 visual_prompt]: 	Training 1100/1106. train loss: 0.5393,	0.6182 s / batch. (data: 1.43e-04). ETA=18:25:29, max mem: 15.9 GB 
[10/30 15:48:10 visual_prompt]: Epoch 3 / 100: avg data time: 4.96e-03, avg batch time: 0.6355, average train loss: 0.7993
[10/30 15:49:00 visual_prompt]: 	Test 100/123. loss: 0.833, 0.2372 s / batch. (data: 5.60e-05)max mem: 15.94594 GB 
[10/30 15:49:11 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.2324, average loss: 0.8317
[10/30 15:49:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.59	
[10/30 15:49:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/30 15:50:16 visual_prompt]: 	Training 100/1106. train loss: 1.1998,	0.6212 s / batch. (data: 7.85e-04). ETA=18:29:41, max mem: 15.9 GB 
[10/30 15:51:19 visual_prompt]: 	Training 200/1106. train loss: 1.1017,	0.6357 s / batch. (data: 5.45e-03). ETA=18:54:29, max mem: 15.9 GB 
[10/30 15:52:22 visual_prompt]: 	Training 300/1106. train loss: 0.9857,	0.6183 s / batch. (data: 3.35e-04). ETA=18:22:23, max mem: 15.9 GB 
[10/30 15:53:26 visual_prompt]: 	Training 400/1106. train loss: 0.7679,	0.6491 s / batch. (data: 8.17e-04). ETA=19:16:21, max mem: 15.9 GB 
[10/30 15:54:29 visual_prompt]: 	Training 500/1106. train loss: 2.8117,	0.6229 s / batch. (data: 3.38e-04). ETA=18:28:38, max mem: 15.9 GB 
[10/30 15:55:32 visual_prompt]: 	Training 600/1106. train loss: 0.2116,	0.6400 s / batch. (data: 8.16e-04). ETA=18:57:55, max mem: 15.9 GB 
[10/30 15:56:36 visual_prompt]: 	Training 700/1106. train loss: 1.3647,	0.6200 s / batch. (data: 8.67e-04). ETA=18:21:22, max mem: 15.9 GB 
[10/30 15:57:39 visual_prompt]: 	Training 800/1106. train loss: 0.0443,	0.6442 s / batch. (data: 8.21e-04). ETA=19:03:19, max mem: 15.9 GB 
[10/30 15:58:42 visual_prompt]: 	Training 900/1106. train loss: 1.1425,	0.6439 s / batch. (data: 3.45e-04). ETA=19:01:38, max mem: 15.9 GB 
[10/30 15:59:45 visual_prompt]: 	Training 1000/1106. train loss: 0.6139,	0.6296 s / batch. (data: 1.20e-02). ETA=18:35:19, max mem: 15.9 GB 
[10/30 16:00:49 visual_prompt]: 	Training 1100/1106. train loss: 1.0688,	0.6187 s / batch. (data: 1.22e-04). ETA=18:14:51, max mem: 15.9 GB 
[10/30 16:00:53 visual_prompt]: Epoch 4 / 100: avg data time: 4.34e-03, avg batch time: 0.6345, average train loss: 0.8215
[10/30 16:01:43 visual_prompt]: 	Test 100/123. loss: 1.118, 0.2286 s / batch. (data: 3.70e-05)max mem: 15.94594 GB 
[10/30 16:01:53 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2321, average loss: 1.0080
[10/30 16:01:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.96	
[10/30 16:01:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/30 16:02:58 visual_prompt]: 	Training 100/1106. train loss: 1.0794,	0.6319 s / batch. (data: 8.18e-04). ETA=18:37:04, max mem: 15.9 GB 
[10/30 16:04:02 visual_prompt]: 	Training 200/1106. train loss: 1.5603,	0.6560 s / batch. (data: 1.20e-02). ETA=19:18:38, max mem: 15.9 GB 
[10/30 16:05:05 visual_prompt]: 	Training 300/1106. train loss: 0.7757,	0.6190 s / batch. (data: 3.33e-04). ETA=18:12:14, max mem: 15.9 GB 
[10/30 16:06:08 visual_prompt]: 	Training 400/1106. train loss: 1.1135,	0.6335 s / batch. (data: 2.96e-04). ETA=18:36:44, max mem: 15.9 GB 
[10/30 16:07:12 visual_prompt]: 	Training 500/1106. train loss: 0.0871,	0.6346 s / batch. (data: 8.00e-04). ETA=18:37:43, max mem: 15.9 GB 
[10/30 16:08:15 visual_prompt]: 	Training 600/1106. train loss: 2.1797,	0.6318 s / batch. (data: 8.19e-04). ETA=18:31:39, max mem: 15.9 GB 
[10/30 16:09:18 visual_prompt]: 	Training 700/1106. train loss: 0.6263,	0.6239 s / batch. (data: 3.43e-04). ETA=18:16:51, max mem: 15.9 GB 
[10/30 16:10:22 visual_prompt]: 	Training 800/1106. train loss: 1.7336,	0.6180 s / batch. (data: 2.83e-04). ETA=18:05:24, max mem: 15.9 GB 
[10/30 16:11:25 visual_prompt]: 	Training 900/1106. train loss: 0.7050,	0.6187 s / batch. (data: 3.12e-04). ETA=18:05:30, max mem: 15.9 GB 
[10/30 16:12:28 visual_prompt]: 	Training 1000/1106. train loss: 0.3110,	0.6215 s / batch. (data: 2.84e-04). ETA=18:09:23, max mem: 15.9 GB 
[10/30 16:13:32 visual_prompt]: 	Training 1100/1106. train loss: 0.7715,	0.6178 s / batch. (data: 1.81e-04). ETA=18:01:52, max mem: 15.9 GB 
[10/30 16:13:35 visual_prompt]: Epoch 5 / 100: avg data time: 4.31e-03, avg batch time: 0.6348, average train loss: 0.8295
[10/30 16:14:26 visual_prompt]: 	Test 100/123. loss: 1.026, 0.2438 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/30 16:14:36 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.2319, average loss: 1.0640
[10/30 16:14:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.63	
[10/30 16:14:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/30 16:15:41 visual_prompt]: 	Training 100/1106. train loss: 1.1580,	0.6332 s / batch. (data: 8.24e-04). ETA=18:27:44, max mem: 15.9 GB 
[10/30 16:16:45 visual_prompt]: 	Training 200/1106. train loss: 0.4128,	0.6196 s / batch. (data: 3.35e-04). ETA=18:03:00, max mem: 15.9 GB 
[10/30 16:17:48 visual_prompt]: 	Training 300/1106. train loss: 0.0651,	0.6196 s / batch. (data: 3.17e-04). ETA=18:01:59, max mem: 15.9 GB 
[10/30 16:18:51 visual_prompt]: 	Training 400/1106. train loss: 1.7235,	0.6294 s / batch. (data: 1.05e-02). ETA=18:17:56, max mem: 15.9 GB 
[10/30 16:19:54 visual_prompt]: 	Training 500/1106. train loss: 0.6799,	0.6487 s / batch. (data: 7.42e-04). ETA=18:50:32, max mem: 15.9 GB 
[10/30 16:20:57 visual_prompt]: 	Training 600/1106. train loss: 1.2148,	0.6440 s / batch. (data: 3.33e-04). ETA=18:41:20, max mem: 15.9 GB 
[10/30 16:22:01 visual_prompt]: 	Training 700/1106. train loss: 0.9815,	0.6560 s / batch. (data: 8.38e-04). ETA=19:01:05, max mem: 15.9 GB 
[10/30 16:23:04 visual_prompt]: 	Training 800/1106. train loss: 0.7753,	0.6340 s / batch. (data: 7.90e-04). ETA=18:21:42, max mem: 15.9 GB 
[10/30 16:24:08 visual_prompt]: 	Training 900/1106. train loss: 0.8058,	0.6433 s / batch. (data: 7.92e-04). ETA=18:36:53, max mem: 15.9 GB 
[10/30 16:25:11 visual_prompt]: 	Training 1000/1106. train loss: 2.6943,	0.6370 s / batch. (data: 8.81e-04). ETA=18:24:57, max mem: 15.9 GB 
[10/30 16:26:14 visual_prompt]: 	Training 1100/1106. train loss: 0.2127,	0.6180 s / batch. (data: 1.59e-04). ETA=17:50:49, max mem: 15.9 GB 
[10/30 16:26:18 visual_prompt]: Epoch 6 / 100: avg data time: 3.96e-03, avg batch time: 0.6346, average train loss: 0.8718
[10/30 16:27:08 visual_prompt]: 	Test 100/123. loss: 0.884, 0.2406 s / batch. (data: 4.32e-05)max mem: 15.94594 GB 
[10/30 16:27:19 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.2328, average loss: 0.8242
[10/30 16:27:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.53	
[10/30 16:27:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/30 16:28:24 visual_prompt]: 	Training 100/1106. train loss: 0.7203,	0.6403 s / batch. (data: 8.26e-04). ETA=18:28:26, max mem: 15.9 GB 
[10/30 16:29:27 visual_prompt]: 	Training 200/1106. train loss: 1.4923,	0.6193 s / batch. (data: 3.32e-04). ETA=17:51:00, max mem: 15.9 GB 
[10/30 16:30:30 visual_prompt]: 	Training 300/1106. train loss: 0.1635,	0.6403 s / batch. (data: 7.83e-04). ETA=18:26:20, max mem: 15.9 GB 
[10/30 16:31:33 visual_prompt]: 	Training 400/1106. train loss: 1.0979,	0.6300 s / batch. (data: 8.20e-04). ETA=18:07:25, max mem: 15.9 GB 
[10/30 16:32:37 visual_prompt]: 	Training 500/1106. train loss: 0.7623,	0.6281 s / batch. (data: 2.88e-04). ETA=18:03:02, max mem: 15.9 GB 
[10/30 16:33:40 visual_prompt]: 	Training 600/1106. train loss: 0.7588,	0.6469 s / batch. (data: 8.30e-04). ETA=18:34:29, max mem: 15.9 GB 
[10/30 16:34:44 visual_prompt]: 	Training 700/1106. train loss: 0.6283,	0.6253 s / batch. (data: 3.42e-04). ETA=17:56:12, max mem: 15.9 GB 
[10/30 16:35:47 visual_prompt]: 	Training 800/1106. train loss: 0.2804,	0.6192 s / batch. (data: 3.22e-04). ETA=17:44:42, max mem: 15.9 GB 
[10/30 16:36:50 visual_prompt]: 	Training 900/1106. train loss: 0.7579,	0.6202 s / batch. (data: 3.00e-04). ETA=17:45:25, max mem: 15.9 GB 
[10/30 16:37:53 visual_prompt]: 	Training 1000/1106. train loss: 0.8351,	0.6397 s / batch. (data: 7.75e-04). ETA=18:17:44, max mem: 15.9 GB 
[10/30 16:38:57 visual_prompt]: 	Training 1100/1106. train loss: 0.4420,	0.6169 s / batch. (data: 1.44e-04). ETA=17:37:40, max mem: 15.9 GB 
[10/30 16:39:01 visual_prompt]: Epoch 7 / 100: avg data time: 3.95e-03, avg batch time: 0.6345, average train loss: 0.8544
[10/30 16:39:51 visual_prompt]: 	Test 100/123. loss: 0.735, 0.2265 s / batch. (data: 5.96e-05)max mem: 15.94594 GB 
[10/30 16:40:01 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.2321, average loss: 0.6939
[10/30 16:40:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.67	
[10/30 16:40:01 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/30 16:41:07 visual_prompt]: 	Training 100/1106. train loss: 0.7106,	0.6185 s / batch. (data: 2.94e-04). ETA=17:39:18, max mem: 15.9 GB 
[10/30 16:42:10 visual_prompt]: 	Training 200/1106. train loss: 1.2271,	0.6218 s / batch. (data: 3.09e-04). ETA=17:43:51, max mem: 15.9 GB 
[10/30 16:43:13 visual_prompt]: 	Training 300/1106. train loss: 0.2416,	0.6592 s / batch. (data: 2.32e-02). ETA=18:46:46, max mem: 15.9 GB 
[10/30 16:44:17 visual_prompt]: 	Training 400/1106. train loss: 0.6685,	0.6185 s / batch. (data: 3.33e-04). ETA=17:36:06, max mem: 15.9 GB 
[10/30 16:45:20 visual_prompt]: 	Training 500/1106. train loss: 0.1585,	0.6194 s / batch. (data: 3.16e-04). ETA=17:36:44, max mem: 15.9 GB 
[10/30 16:46:23 visual_prompt]: 	Training 600/1106. train loss: 1.0946,	0.6530 s / batch. (data: 7.93e-04). ETA=18:32:58, max mem: 15.9 GB 
[10/30 16:47:27 visual_prompt]: 	Training 700/1106. train loss: 0.7978,	0.6452 s / batch. (data: 7.47e-04). ETA=18:18:28, max mem: 15.9 GB 
[10/30 16:48:30 visual_prompt]: 	Training 800/1106. train loss: 0.8930,	0.6188 s / batch. (data: 3.32e-04). ETA=17:32:35, max mem: 15.9 GB 
[10/30 16:49:33 visual_prompt]: 	Training 900/1106. train loss: 0.1122,	0.6184 s / batch. (data: 3.09e-04). ETA=17:30:47, max mem: 15.9 GB 
[10/30 16:50:37 visual_prompt]: 	Training 1000/1106. train loss: 2.5286,	0.6417 s / batch. (data: 7.72e-04). ETA=18:09:26, max mem: 15.9 GB 
[10/30 16:51:40 visual_prompt]: 	Training 1100/1106. train loss: 1.4003,	0.6191 s / batch. (data: 1.61e-04). ETA=17:29:55, max mem: 15.9 GB 
[10/30 16:51:44 visual_prompt]: Epoch 8 / 100: avg data time: 4.09e-03, avg batch time: 0.6351, average train loss: 0.9376
[10/30 16:52:34 visual_prompt]: 	Test 100/123. loss: 0.703, 0.2333 s / batch. (data: 4.96e-05)max mem: 15.94594 GB 
[10/30 16:52:45 visual_prompt]: Inference (val):avg data time: 1.24e-04, avg batch time: 0.2315, average loss: 0.6944
[10/30 16:52:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.41	rocauc: 55.46	
[10/30 16:52:45 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/30 16:53:51 visual_prompt]: 	Training 100/1106. train loss: 0.9524,	0.6404 s / batch. (data: 8.22e-04). ETA=18:05:00, max mem: 15.9 GB 
[10/30 16:54:54 visual_prompt]: 	Training 200/1106. train loss: 0.1179,	0.6351 s / batch. (data: 1.51e-02). ETA=17:54:53, max mem: 15.9 GB 
[10/30 16:55:57 visual_prompt]: 	Training 300/1106. train loss: 1.0534,	0.6343 s / batch. (data: 8.21e-04). ETA=17:52:27, max mem: 15.9 GB 
[10/30 16:57:00 visual_prompt]: 	Training 400/1106. train loss: 0.6591,	0.6332 s / batch. (data: 7.72e-04). ETA=17:49:35, max mem: 15.9 GB 
[10/30 16:58:04 visual_prompt]: 	Training 500/1106. train loss: 1.1580,	0.6320 s / batch. (data: 3.09e-04). ETA=17:46:31, max mem: 15.9 GB 
[10/30 16:59:07 visual_prompt]: 	Training 600/1106. train loss: 0.6707,	0.6196 s / batch. (data: 3.06e-04). ETA=17:24:35, max mem: 15.9 GB 
[10/30 17:00:10 visual_prompt]: 	Training 700/1106. train loss: 1.1361,	0.6234 s / batch. (data: 3.16e-04). ETA=17:29:53, max mem: 15.9 GB 
[10/30 17:01:14 visual_prompt]: 	Training 800/1106. train loss: 0.8133,	0.6205 s / batch. (data: 3.18e-04). ETA=17:24:00, max mem: 15.9 GB 
[10/30 17:02:17 visual_prompt]: 	Training 900/1106. train loss: 0.6352,	0.6425 s / batch. (data: 7.71e-04). ETA=18:00:02, max mem: 15.9 GB 
[10/30 17:03:21 visual_prompt]: 	Training 1000/1106. train loss: 1.0038,	0.6185 s / batch. (data: 3.06e-04). ETA=17:18:30, max mem: 15.9 GB 
[10/30 17:04:24 visual_prompt]: 	Training 1100/1106. train loss: 0.7678,	0.6192 s / batch. (data: 1.37e-04). ETA=17:18:47, max mem: 15.9 GB 
[10/30 17:04:28 visual_prompt]: Epoch 9 / 100: avg data time: 5.00e-03, avg batch time: 0.6356, average train loss: 0.8291
[10/30 17:05:18 visual_prompt]: 	Test 100/123. loss: 0.770, 0.2388 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/30 17:05:29 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.2330, average loss: 0.7072
[10/30 17:05:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.09	
[10/30 17:05:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/30 17:06:34 visual_prompt]: 	Training 100/1106. train loss: 0.7673,	0.6188 s / batch. (data: 3.54e-04). ETA=17:16:54, max mem: 15.9 GB 
[10/30 17:07:37 visual_prompt]: 	Training 200/1106. train loss: 1.8319,	0.6275 s / batch. (data: 3.19e-04). ETA=17:30:24, max mem: 15.9 GB 
[10/30 17:08:41 visual_prompt]: 	Training 300/1106. train loss: 0.9251,	0.6303 s / batch. (data: 7.90e-04). ETA=17:34:12, max mem: 15.9 GB 
[10/30 17:09:44 visual_prompt]: 	Training 400/1106. train loss: 1.1329,	0.6402 s / batch. (data: 3.09e-04). ETA=17:49:33, max mem: 15.9 GB 
[10/30 17:10:47 visual_prompt]: 	Training 500/1106. train loss: 0.7253,	0.6569 s / batch. (data: 3.87e-02). ETA=18:16:25, max mem: 15.9 GB 
[10/30 17:11:50 visual_prompt]: 	Training 600/1106. train loss: 0.4371,	0.6476 s / batch. (data: 3.36e-04). ETA=17:59:53, max mem: 15.9 GB 
[10/30 17:12:54 visual_prompt]: 	Training 700/1106. train loss: 0.5905,	0.6452 s / batch. (data: 1.02e-03). ETA=17:54:42, max mem: 15.9 GB 
[10/30 17:13:57 visual_prompt]: 	Training 800/1106. train loss: 0.7004,	0.6197 s / batch. (data: 3.02e-04). ETA=17:11:11, max mem: 15.9 GB 
[10/30 17:15:00 visual_prompt]: 	Training 900/1106. train loss: 1.0629,	0.6310 s / batch. (data: 1.20e-02). ETA=17:29:02, max mem: 15.9 GB 
[10/30 17:16:04 visual_prompt]: 	Training 1000/1106. train loss: 0.4201,	0.6289 s / batch. (data: 5.45e-03). ETA=17:24:31, max mem: 15.9 GB 
[10/30 17:17:07 visual_prompt]: 	Training 1100/1106. train loss: 0.7833,	0.6176 s / batch. (data: 1.64e-04). ETA=17:04:37, max mem: 15.9 GB 
[10/30 17:17:11 visual_prompt]: Epoch 10 / 100: avg data time: 4.21e-03, avg batch time: 0.6350, average train loss: 0.9725
[10/30 17:18:01 visual_prompt]: 	Test 100/123. loss: 0.706, 0.2296 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/30 17:18:12 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2325, average loss: 0.6861
[10/30 17:18:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.40	
[10/30 17:18:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/30 17:19:17 visual_prompt]: 	Training 100/1106. train loss: 1.5751,	0.6411 s / batch. (data: 8.13e-04). ETA=17:42:27, max mem: 15.9 GB 
[10/30 17:20:21 visual_prompt]: 	Training 200/1106. train loss: 0.7790,	0.6527 s / batch. (data: 1.49e-02). ETA=18:00:42, max mem: 15.9 GB 
[10/30 17:21:24 visual_prompt]: 	Training 300/1106. train loss: 0.6649,	0.6334 s / batch. (data: 1.23e-03). ETA=17:27:35, max mem: 15.9 GB 
[10/30 17:22:28 visual_prompt]: 	Training 400/1106. train loss: 1.0431,	0.6289 s / batch. (data: 8.01e-04). ETA=17:19:11, max mem: 15.9 GB 
[10/30 17:23:31 visual_prompt]: 	Training 500/1106. train loss: 1.3804,	0.6178 s / batch. (data: 2.81e-04). ETA=16:59:48, max mem: 15.9 GB 
[10/30 17:24:34 visual_prompt]: 	Training 600/1106. train loss: 0.1545,	0.6188 s / batch. (data: 3.10e-04). ETA=17:00:22, max mem: 15.9 GB 
[10/30 17:25:38 visual_prompt]: 	Training 700/1106. train loss: 0.5047,	0.6340 s / batch. (data: 7.32e-04). ETA=17:24:21, max mem: 15.9 GB 
[10/30 17:26:41 visual_prompt]: 	Training 800/1106. train loss: 0.7700,	0.6327 s / batch. (data: 2.95e-04). ETA=17:21:16, max mem: 15.9 GB 
[10/30 17:27:44 visual_prompt]: 	Training 900/1106. train loss: 0.3289,	0.6301 s / batch. (data: 8.12e-04). ETA=17:15:50, max mem: 15.9 GB 
[10/30 17:28:47 visual_prompt]: 	Training 1000/1106. train loss: 0.3277,	0.6250 s / batch. (data: 7.73e-04). ETA=17:06:28, max mem: 15.9 GB 
[10/30 17:29:51 visual_prompt]: 	Training 1100/1106. train loss: 1.0478,	0.6180 s / batch. (data: 1.97e-04). ETA=16:53:53, max mem: 15.9 GB 
[10/30 17:29:54 visual_prompt]: Epoch 11 / 100: avg data time: 5.07e-03, avg batch time: 0.6352, average train loss: 0.8418
[10/30 17:30:45 visual_prompt]: 	Test 100/123. loss: 0.918, 0.2449 s / batch. (data: 2.86e-05)max mem: 15.94594 GB 
[10/30 17:30:55 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2324, average loss: 0.8205
[10/30 17:30:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.83	
[10/30 17:30:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/30 17:32:02 visual_prompt]: 	Training 100/1106. train loss: 0.1816,	0.6198 s / batch. (data: 3.49e-04). ETA=16:55:43, max mem: 15.9 GB 
[10/30 17:33:05 visual_prompt]: 	Training 200/1106. train loss: 0.7611,	0.6332 s / batch. (data: 7.93e-04). ETA=17:16:43, max mem: 15.9 GB 
[10/30 17:34:09 visual_prompt]: 	Training 300/1106. train loss: 0.1759,	0.6291 s / batch. (data: 3.22e-04). ETA=17:08:56, max mem: 15.9 GB 
[10/30 17:35:12 visual_prompt]: 	Training 400/1106. train loss: 0.4384,	0.6592 s / batch. (data: 1.61e-02). ETA=17:57:00, max mem: 15.9 GB 
[10/30 17:36:15 visual_prompt]: 	Training 500/1106. train loss: 2.8987,	0.6348 s / batch. (data: 7.72e-04). ETA=17:16:06, max mem: 15.9 GB 
[10/30 17:37:19 visual_prompt]: 	Training 600/1106. train loss: 0.8857,	0.6304 s / batch. (data: 3.12e-04). ETA=17:07:58, max mem: 15.9 GB 
[10/30 17:38:22 visual_prompt]: 	Training 700/1106. train loss: 0.7385,	0.6523 s / batch. (data: 7.50e-04). ETA=17:42:32, max mem: 15.9 GB 
[10/30 17:39:25 visual_prompt]: 	Training 800/1106. train loss: 0.9474,	0.6310 s / batch. (data: 7.10e-04). ETA=17:06:44, max mem: 15.9 GB 
[10/30 17:40:29 visual_prompt]: 	Training 900/1106. train loss: 0.1523,	0.6192 s / batch. (data: 3.00e-04). ETA=16:46:36, max mem: 15.9 GB 
[10/30 17:41:32 visual_prompt]: 	Training 1000/1106. train loss: 1.1044,	0.6360 s / batch. (data: 8.14e-04). ETA=17:12:48, max mem: 15.9 GB 
[10/30 17:42:35 visual_prompt]: 	Training 1100/1106. train loss: 0.8794,	0.6179 s / batch. (data: 1.66e-04). ETA=16:42:17, max mem: 15.9 GB 
[10/30 17:42:39 visual_prompt]: Epoch 12 / 100: avg data time: 5.38e-03, avg batch time: 0.6359, average train loss: 0.8861
[10/30 17:43:29 visual_prompt]: 	Test 100/123. loss: 1.605, 0.2257 s / batch. (data: 3.93e-05)max mem: 15.94594 GB 
[10/30 17:43:40 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.2318, average loss: 1.7767
[10/30 17:43:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.75	
[10/30 17:43:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/30 17:44:45 visual_prompt]: 	Training 100/1106. train loss: 0.9748,	0.6181 s / batch. (data: 3.34e-04). ETA=16:41:37, max mem: 15.9 GB 
[10/30 17:45:49 visual_prompt]: 	Training 200/1106. train loss: 0.6739,	0.6493 s / batch. (data: 1.04e-03). ETA=17:31:02, max mem: 15.9 GB 
[10/30 17:46:52 visual_prompt]: 	Training 300/1106. train loss: 0.2771,	0.6318 s / batch. (data: 7.51e-04). ETA=17:01:38, max mem: 15.9 GB 
[10/30 17:47:55 visual_prompt]: 	Training 400/1106. train loss: 0.7545,	0.6336 s / batch. (data: 9.09e-04). ETA=17:03:32, max mem: 15.9 GB 
[10/30 17:48:58 visual_prompt]: 	Training 500/1106. train loss: 0.2363,	0.6194 s / batch. (data: 2.78e-04). ETA=16:39:36, max mem: 15.9 GB 
[10/30 17:50:02 visual_prompt]: 	Training 600/1106. train loss: 1.4227,	0.6390 s / batch. (data: 7.60e-04). ETA=17:10:13, max mem: 15.9 GB 
[10/30 17:51:05 visual_prompt]: 	Training 700/1106. train loss: 0.9133,	0.6589 s / batch. (data: 3.89e-02). ETA=17:41:10, max mem: 15.9 GB 
[10/30 17:52:09 visual_prompt]: 	Training 800/1106. train loss: 1.5273,	0.6409 s / batch. (data: 8.34e-04). ETA=17:11:03, max mem: 15.9 GB 
[10/30 17:53:12 visual_prompt]: 	Training 900/1106. train loss: 1.1645,	0.6594 s / batch. (data: 4.03e-02). ETA=17:39:42, max mem: 15.9 GB 
[10/30 17:54:16 visual_prompt]: 	Training 1000/1106. train loss: 0.2312,	0.6281 s / batch. (data: 3.39e-04). ETA=16:48:24, max mem: 15.9 GB 
[10/30 17:55:19 visual_prompt]: 	Training 1100/1106. train loss: 0.8662,	0.6186 s / batch. (data: 1.92e-04). ETA=16:32:08, max mem: 15.9 GB 
[10/30 17:55:23 visual_prompt]: Epoch 13 / 100: avg data time: 4.62e-03, avg batch time: 0.6358, average train loss: 0.8365
[10/30 17:56:13 visual_prompt]: 	Test 100/123. loss: 0.894, 0.2433 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/30 17:56:24 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.2322, average loss: 0.8242
[10/30 17:56:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.93	
[10/30 17:56:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/30 17:57:30 visual_prompt]: 	Training 100/1106. train loss: 0.4071,	0.6397 s / batch. (data: 8.21e-04). ETA=17:04:51, max mem: 15.9 GB 
[10/30 17:58:33 visual_prompt]: 	Training 200/1106. train loss: 0.9544,	0.6208 s / batch. (data: 3.34e-04). ETA=16:33:27, max mem: 15.9 GB 
[10/30 17:59:37 visual_prompt]: 	Training 300/1106. train loss: 0.7229,	0.6651 s / batch. (data: 8.30e-04). ETA=17:43:18, max mem: 15.9 GB 
[10/30 18:00:40 visual_prompt]: 	Training 400/1106. train loss: 0.1318,	0.6359 s / batch. (data: 7.78e-04). ETA=16:55:29, max mem: 15.9 GB 
[10/30 18:01:44 visual_prompt]: 	Training 500/1106. train loss: 0.7287,	0.6328 s / batch. (data: 8.16e-04). ETA=16:49:33, max mem: 15.9 GB 
[10/30 18:02:47 visual_prompt]: 	Training 600/1106. train loss: 0.6469,	0.6378 s / batch. (data: 3.17e-04). ETA=16:56:31, max mem: 15.9 GB 
[10/30 18:03:50 visual_prompt]: 	Training 700/1106. train loss: 0.3386,	0.6682 s / batch. (data: 2.44e-02). ETA=17:43:52, max mem: 15.9 GB 
[10/30 18:04:54 visual_prompt]: 	Training 800/1106. train loss: 0.7370,	0.6190 s / batch. (data: 3.43e-04). ETA=16:24:25, max mem: 15.9 GB 
[10/30 18:05:57 visual_prompt]: 	Training 900/1106. train loss: 0.8077,	0.6346 s / batch. (data: 1.64e-02). ETA=16:48:13, max mem: 15.9 GB 
[10/30 18:07:00 visual_prompt]: 	Training 1000/1106. train loss: 0.1705,	0.6325 s / batch. (data: 2.99e-04). ETA=16:43:47, max mem: 15.9 GB 
[10/30 18:08:03 visual_prompt]: 	Training 1100/1106. train loss: 0.8356,	0.6183 s / batch. (data: 1.74e-04). ETA=16:20:17, max mem: 15.9 GB 
[10/30 18:08:07 visual_prompt]: Epoch 14 / 100: avg data time: 5.41e-03, avg batch time: 0.6360, average train loss: 0.8366
[10/30 18:08:57 visual_prompt]: 	Test 100/123. loss: 0.661, 0.2249 s / batch. (data: 4.41e-05)max mem: 15.94594 GB 
[10/30 18:09:08 visual_prompt]: Inference (val):avg data time: 1.31e-04, avg batch time: 0.2330, average loss: 0.6725
[10/30 18:09:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.60	
[10/30 18:09:08 visual_prompt]: Best epoch 14: best metric: -0.673
[10/30 18:09:08 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/30 18:10:13 visual_prompt]: 	Training 100/1106. train loss: 1.1108,	0.6318 s / batch. (data: 3.10e-04). ETA=16:40:33, max mem: 15.9 GB 
[10/30 18:11:17 visual_prompt]: 	Training 200/1106. train loss: 0.9046,	0.6303 s / batch. (data: 6.98e-04). ETA=16:37:07, max mem: 15.9 GB 
[10/30 18:12:20 visual_prompt]: 	Training 300/1106. train loss: 0.7211,	0.6187 s / batch. (data: 3.08e-04). ETA=16:17:42, max mem: 15.9 GB 
[10/30 18:13:23 visual_prompt]: 	Training 400/1106. train loss: 1.2004,	0.6183 s / batch. (data: 3.25e-04). ETA=16:16:04, max mem: 15.9 GB 
[10/30 18:14:27 visual_prompt]: 	Training 500/1106. train loss: 0.2119,	0.6441 s / batch. (data: 8.08e-04). ETA=16:55:38, max mem: 15.9 GB 
[10/30 18:15:30 visual_prompt]: 	Training 600/1106. train loss: 1.0562,	0.6302 s / batch. (data: 3.21e-04). ETA=16:32:48, max mem: 15.9 GB 
[10/30 18:16:34 visual_prompt]: 	Training 700/1106. train loss: 0.4559,	0.6271 s / batch. (data: 7.46e-04). ETA=16:26:51, max mem: 15.9 GB 
[10/30 18:17:37 visual_prompt]: 	Training 800/1106. train loss: 0.5685,	0.6360 s / batch. (data: 4.52e-04). ETA=16:39:44, max mem: 15.9 GB 
[10/30 18:18:40 visual_prompt]: 	Training 900/1106. train loss: 0.8647,	0.6435 s / batch. (data: 5.48e-03). ETA=16:50:31, max mem: 15.9 GB 
[10/30 18:19:44 visual_prompt]: 	Training 1000/1106. train loss: 0.6817,	0.6189 s / batch. (data: 3.22e-04). ETA=16:10:45, max mem: 15.9 GB 
[10/30 18:20:47 visual_prompt]: 	Training 1100/1106. train loss: 1.2066,	0.6190 s / batch. (data: 1.54e-04). ETA=16:09:58, max mem: 15.9 GB 
[10/30 18:20:51 visual_prompt]: Epoch 15 / 100: avg data time: 4.29e-03, avg batch time: 0.6349, average train loss: 0.8475
[10/30 18:21:41 visual_prompt]: 	Test 100/123. loss: 0.916, 0.2318 s / batch. (data: 2.38e-05)max mem: 15.94594 GB 
[10/30 18:21:52 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.2329, average loss: 0.8344
[10/30 18:21:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.90	
[10/30 18:21:52 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/30 18:22:57 visual_prompt]: 	Training 100/1106. train loss: 0.3138,	0.6256 s / batch. (data: 3.05e-04). ETA=16:19:08, max mem: 15.9 GB 
[10/30 18:24:00 visual_prompt]: 	Training 200/1106. train loss: 0.8628,	0.6280 s / batch. (data: 3.21e-04). ETA=16:21:51, max mem: 15.9 GB 
[10/30 18:25:03 visual_prompt]: 	Training 300/1106. train loss: 0.3605,	0.6322 s / batch. (data: 8.50e-04). ETA=16:27:23, max mem: 15.9 GB 
[10/30 18:26:07 visual_prompt]: 	Training 400/1106. train loss: 0.8534,	0.6194 s / batch. (data: 3.20e-04). ETA=16:06:18, max mem: 15.9 GB 
[10/30 18:27:10 visual_prompt]: 	Training 500/1106. train loss: 0.7460,	0.6191 s / batch. (data: 3.32e-04). ETA=16:04:53, max mem: 15.9 GB 
[10/30 18:28:13 visual_prompt]: 	Training 600/1106. train loss: 1.2885,	0.6336 s / batch. (data: 7.44e-04). ETA=16:26:21, max mem: 15.9 GB 
[10/30 18:29:17 visual_prompt]: 	Training 700/1106. train loss: 1.1893,	0.6182 s / batch. (data: 3.11e-04). ETA=16:01:27, max mem: 15.9 GB 
[10/30 18:30:20 visual_prompt]: 	Training 800/1106. train loss: 0.8964,	0.6194 s / batch. (data: 3.76e-04). ETA=16:02:15, max mem: 15.9 GB 
[10/30 18:31:24 visual_prompt]: 	Training 900/1106. train loss: 0.5882,	0.6326 s / batch. (data: 8.25e-04). ETA=16:21:45, max mem: 15.9 GB 
[10/30 18:32:27 visual_prompt]: 	Training 1000/1106. train loss: 0.8532,	0.6322 s / batch. (data: 1.41e-02). ETA=16:19:57, max mem: 15.9 GB 
[10/30 18:33:30 visual_prompt]: 	Training 1100/1106. train loss: 0.3463,	0.6188 s / batch. (data: 1.60e-04). ETA=15:58:12, max mem: 15.9 GB 
[10/30 18:33:34 visual_prompt]: Epoch 16 / 100: avg data time: 3.89e-03, avg batch time: 0.6350, average train loss: 0.7915
[10/30 18:34:24 visual_prompt]: 	Test 100/123. loss: 0.696, 0.2478 s / batch. (data: 3.36e-05)max mem: 15.94594 GB 
[10/30 18:34:35 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.2313, average loss: 0.6853
[10/30 18:34:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.69	
[10/30 18:34:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/30 18:35:40 visual_prompt]: 	Training 100/1106. train loss: 1.7639,	0.6451 s / batch. (data: 8.12e-04). ETA=16:37:50, max mem: 15.9 GB 
[10/30 18:36:44 visual_prompt]: 	Training 200/1106. train loss: 0.9439,	0.6481 s / batch. (data: 8.49e-04). ETA=16:41:22, max mem: 15.9 GB 
[10/30 18:37:47 visual_prompt]: 	Training 300/1106. train loss: 1.2084,	0.6273 s / batch. (data: 7.93e-04). ETA=16:08:09, max mem: 15.9 GB 
[10/30 18:38:50 visual_prompt]: 	Training 400/1106. train loss: 4.1002,	0.6388 s / batch. (data: 7.94e-04). ETA=16:24:48, max mem: 15.9 GB 
[10/30 18:39:53 visual_prompt]: 	Training 500/1106. train loss: 0.0819,	0.6261 s / batch. (data: 3.18e-04). ETA=16:04:17, max mem: 15.9 GB 
[10/30 18:40:57 visual_prompt]: 	Training 600/1106. train loss: 0.9892,	0.6336 s / batch. (data: 7.76e-04). ETA=16:14:47, max mem: 15.9 GB 
[10/30 18:42:00 visual_prompt]: 	Training 700/1106. train loss: 0.8027,	0.6280 s / batch. (data: 3.21e-04). ETA=16:04:59, max mem: 15.9 GB 
[10/30 18:43:04 visual_prompt]: 	Training 800/1106. train loss: 0.8694,	0.6205 s / batch. (data: 3.45e-04). ETA=15:52:31, max mem: 15.9 GB 
[10/30 18:44:07 visual_prompt]: 	Training 900/1106. train loss: 1.6820,	0.6337 s / batch. (data: 8.11e-04). ETA=16:11:43, max mem: 15.9 GB 
[10/30 18:45:10 visual_prompt]: 	Training 1000/1106. train loss: 0.0402,	0.6303 s / batch. (data: 3.22e-04). ETA=16:05:28, max mem: 15.9 GB 
[10/30 18:46:14 visual_prompt]: 	Training 1100/1106. train loss: 0.2325,	0.6196 s / batch. (data: 1.35e-04). ETA=15:47:57, max mem: 15.9 GB 
[10/30 18:46:18 visual_prompt]: Epoch 17 / 100: avg data time: 4.54e-03, avg batch time: 0.6355, average train loss: 0.9052
[10/30 18:47:08 visual_prompt]: 	Test 100/123. loss: 0.657, 0.2318 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/30 18:47:18 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2332, average loss: 0.6967
[10/30 18:47:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 59.70	
[10/30 18:47:18 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/30 18:48:24 visual_prompt]: 	Training 100/1106. train loss: 0.3205,	0.6573 s / batch. (data: 4.11e-02). ETA=16:44:33, max mem: 15.9 GB 
[10/30 18:49:27 visual_prompt]: 	Training 200/1106. train loss: 1.0463,	0.6339 s / batch. (data: 7.49e-04). ETA=16:07:43, max mem: 15.9 GB 
[10/30 18:50:31 visual_prompt]: 	Training 300/1106. train loss: 1.6331,	0.6339 s / batch. (data: 2.59e-04). ETA=16:06:41, max mem: 15.9 GB 
[10/30 18:51:34 visual_prompt]: 	Training 400/1106. train loss: 0.6761,	0.6433 s / batch. (data: 8.05e-04). ETA=16:19:59, max mem: 15.9 GB 
[10/30 18:52:37 visual_prompt]: 	Training 500/1106. train loss: 0.2108,	0.6440 s / batch. (data: 7.72e-04). ETA=16:19:56, max mem: 15.9 GB 
[10/30 18:53:41 visual_prompt]: 	Training 600/1106. train loss: 0.5886,	0.6187 s / batch. (data: 3.04e-04). ETA=15:40:22, max mem: 15.9 GB 
[10/30 18:54:44 visual_prompt]: 	Training 700/1106. train loss: 0.5369,	0.6318 s / batch. (data: 1.48e-02). ETA=15:59:15, max mem: 15.9 GB 
[10/30 18:55:47 visual_prompt]: 	Training 800/1106. train loss: 0.4945,	0.6258 s / batch. (data: 3.30e-04). ETA=15:49:05, max mem: 15.9 GB 
[10/30 18:56:50 visual_prompt]: 	Training 900/1106. train loss: 1.2338,	0.6566 s / batch. (data: 3.76e-02). ETA=16:34:42, max mem: 15.9 GB 
[10/30 18:57:54 visual_prompt]: 	Training 1000/1106. train loss: 0.8639,	0.6419 s / batch. (data: 8.97e-04). ETA=16:11:25, max mem: 15.9 GB 
[10/30 18:58:57 visual_prompt]: 	Training 1100/1106. train loss: 0.4733,	0.6182 s / batch. (data: 1.40e-04). ETA=15:34:33, max mem: 15.9 GB 
[10/30 18:59:01 visual_prompt]: Epoch 18 / 100: avg data time: 4.54e-03, avg batch time: 0.6352, average train loss: 0.8467
[10/30 18:59:51 visual_prompt]: 	Test 100/123. loss: 0.659, 0.2251 s / batch. (data: 3.86e-05)max mem: 15.94594 GB 
[10/30 19:00:01 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.2340, average loss: 0.6813
[10/30 19:00:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 59.12	
[10/30 19:00:01 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/30 19:01:07 visual_prompt]: 	Training 100/1106. train loss: 0.2476,	0.6256 s / batch. (data: 7.70e-04). ETA=15:44:30, max mem: 15.9 GB 
[10/30 19:02:10 visual_prompt]: 	Training 200/1106. train loss: 0.7527,	0.6348 s / batch. (data: 7.48e-04). ETA=15:57:23, max mem: 15.9 GB 
[10/30 19:03:13 visual_prompt]: 	Training 300/1106. train loss: 0.3999,	0.6400 s / batch. (data: 3.43e-04). ETA=16:04:10, max mem: 15.9 GB 
[10/30 19:04:17 visual_prompt]: 	Training 400/1106. train loss: 0.9023,	0.6277 s / batch. (data: 8.19e-04). ETA=15:44:34, max mem: 15.9 GB 
[10/30 19:05:20 visual_prompt]: 	Training 500/1106. train loss: 0.5839,	0.6633 s / batch. (data: 7.92e-04). ETA=16:37:03, max mem: 15.9 GB 
[10/30 19:06:23 visual_prompt]: 	Training 600/1106. train loss: 1.3060,	0.6321 s / batch. (data: 8.07e-04). ETA=15:49:04, max mem: 15.9 GB 
[10/30 19:07:26 visual_prompt]: 	Training 700/1106. train loss: 0.7587,	0.6197 s / batch. (data: 2.95e-04). ETA=15:29:26, max mem: 15.9 GB 
[10/30 19:08:30 visual_prompt]: 	Training 800/1106. train loss: 0.1525,	0.6455 s / batch. (data: 8.02e-04). ETA=16:07:05, max mem: 15.9 GB 
[10/30 19:09:33 visual_prompt]: 	Training 900/1106. train loss: 0.4153,	0.6441 s / batch. (data: 9.01e-04). ETA=16:03:59, max mem: 15.9 GB 
[10/30 19:10:36 visual_prompt]: 	Training 1000/1106. train loss: 0.6715,	0.6453 s / batch. (data: 7.83e-04). ETA=16:04:35, max mem: 15.9 GB 
[10/30 19:11:39 visual_prompt]: 	Training 1100/1106. train loss: 1.0950,	0.6188 s / batch. (data: 1.92e-04). ETA=15:23:56, max mem: 15.9 GB 
[10/30 19:11:43 visual_prompt]: Epoch 19 / 100: avg data time: 4.33e-03, avg batch time: 0.6345, average train loss: 0.7732
[10/30 19:12:34 visual_prompt]: 	Test 100/123. loss: 1.315, 0.2432 s / batch. (data: 4.65e-05)max mem: 15.94594 GB 
[10/30 19:12:44 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.2324, average loss: 1.2171
[10/30 19:12:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.29	
[10/30 19:12:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/30 19:13:50 visual_prompt]: 	Training 100/1106. train loss: 0.6097,	0.6444 s / batch. (data: 8.25e-04). ETA=16:01:05, max mem: 15.9 GB 
[10/30 19:14:53 visual_prompt]: 	Training 200/1106. train loss: 0.4648,	0.6521 s / batch. (data: 2.76e-02). ETA=16:11:32, max mem: 15.9 GB 
[10/30 19:15:56 visual_prompt]: 	Training 300/1106. train loss: 0.5809,	0.6560 s / batch. (data: 8.29e-04). ETA=16:16:10, max mem: 15.9 GB 
[10/30 19:17:00 visual_prompt]: 	Training 400/1106. train loss: 0.4863,	0.6324 s / batch. (data: 3.26e-04). ETA=15:39:59, max mem: 15.9 GB 
[10/30 19:18:03 visual_prompt]: 	Training 500/1106. train loss: 0.5821,	0.6184 s / batch. (data: 7.18e-04). ETA=15:18:14, max mem: 15.9 GB 
[10/30 19:19:06 visual_prompt]: 	Training 600/1106. train loss: 1.1582,	0.6184 s / batch. (data: 2.83e-04). ETA=15:17:06, max mem: 15.9 GB 
[10/30 19:20:10 visual_prompt]: 	Training 700/1106. train loss: 0.4371,	0.6687 s / batch. (data: 3.82e-02). ETA=16:30:40, max mem: 15.9 GB 
[10/30 19:21:13 visual_prompt]: 	Training 800/1106. train loss: 0.6124,	0.6177 s / batch. (data: 2.93e-04). ETA=15:14:06, max mem: 15.9 GB 
[10/30 19:22:16 visual_prompt]: 	Training 900/1106. train loss: 0.7276,	0.6460 s / batch. (data: 7.95e-04). ETA=15:54:49, max mem: 15.9 GB 
[10/30 19:23:19 visual_prompt]: 	Training 1000/1106. train loss: 0.7143,	0.6250 s / batch. (data: 3.04e-04). ETA=15:22:49, max mem: 15.9 GB 
[10/30 19:24:23 visual_prompt]: 	Training 1100/1106. train loss: 0.3007,	0.6177 s / batch. (data: 1.40e-04). ETA=15:10:57, max mem: 15.9 GB 
[10/30 19:24:26 visual_prompt]: Epoch 20 / 100: avg data time: 4.55e-03, avg batch time: 0.6351, average train loss: 0.7891
[10/30 19:25:16 visual_prompt]: 	Test 100/123. loss: 0.887, 0.2259 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[10/30 19:25:27 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.2328, average loss: 1.0459
[10/30 19:25:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.87	
[10/30 19:25:27 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/30 19:26:33 visual_prompt]: 	Training 100/1106. train loss: 0.5358,	0.6520 s / batch. (data: 8.15e-04). ETA=16:00:22, max mem: 15.9 GB 
[10/30 19:27:36 visual_prompt]: 	Training 200/1106. train loss: 0.9801,	0.6693 s / batch. (data: 1.56e-02). ETA=16:24:49, max mem: 15.9 GB 
[10/30 19:28:39 visual_prompt]: 	Training 300/1106. train loss: 0.5579,	0.6341 s / batch. (data: 3.73e-04). ETA=15:31:56, max mem: 15.9 GB 
[10/30 19:29:43 visual_prompt]: 	Training 400/1106. train loss: 0.7658,	0.6331 s / batch. (data: 3.00e-04). ETA=15:29:20, max mem: 15.9 GB 
[10/30 19:30:46 visual_prompt]: 	Training 500/1106. train loss: 1.4404,	0.6194 s / batch. (data: 3.15e-04). ETA=15:08:17, max mem: 15.9 GB 
[10/30 19:31:49 visual_prompt]: 	Training 600/1106. train loss: 2.3896,	0.6335 s / batch. (data: 7.96e-04). ETA=15:27:53, max mem: 15.9 GB 
[10/30 19:32:53 visual_prompt]: 	Training 700/1106. train loss: 0.8318,	0.6330 s / batch. (data: 8.12e-04). ETA=15:26:08, max mem: 15.9 GB 
[10/30 19:33:56 visual_prompt]: 	Training 800/1106. train loss: 0.7310,	0.6344 s / batch. (data: 7.54e-04). ETA=15:27:06, max mem: 15.9 GB 
[10/30 19:34:59 visual_prompt]: 	Training 900/1106. train loss: 0.0577,	0.6232 s / batch. (data: 5.44e-03). ETA=15:09:43, max mem: 15.9 GB 
[10/30 19:36:03 visual_prompt]: 	Training 1000/1106. train loss: 0.6760,	0.6330 s / batch. (data: 8.22e-04). ETA=15:22:57, max mem: 15.9 GB 
[10/30 19:37:06 visual_prompt]: 	Training 1100/1106. train loss: 0.9440,	0.6186 s / batch. (data: 1.29e-04). ETA=15:00:55, max mem: 15.9 GB 
[10/30 19:37:10 visual_prompt]: Epoch 21 / 100: avg data time: 4.59e-03, avg batch time: 0.6354, average train loss: 0.7905
[10/30 19:38:00 visual_prompt]: 	Test 100/123. loss: 0.600, 0.2293 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/30 19:38:10 visual_prompt]: Inference (val):avg data time: 9.74e-05, avg batch time: 0.2329, average loss: 0.6683
[10/30 19:38:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 63.36	
[10/30 19:38:10 visual_prompt]: Best epoch 21: best metric: -0.668
[10/30 19:38:10 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/30 19:39:15 visual_prompt]: 	Training 100/1106. train loss: 1.1903,	0.6262 s / batch. (data: 3.03e-04). ETA=15:10:47, max mem: 15.9 GB 
[10/30 19:40:19 visual_prompt]: 	Training 200/1106. train loss: 0.3918,	0.6440 s / batch. (data: 1.20e-02). ETA=15:35:40, max mem: 15.9 GB 
[10/30 19:41:22 visual_prompt]: 	Training 300/1106. train loss: 0.9573,	0.6237 s / batch. (data: 2.95e-04). ETA=15:05:07, max mem: 15.9 GB 
[10/30 19:42:25 visual_prompt]: 	Training 400/1106. train loss: 0.1678,	0.6250 s / batch. (data: 7.74e-04). ETA=15:05:56, max mem: 15.9 GB 
[10/30 19:43:28 visual_prompt]: 	Training 500/1106. train loss: 0.2835,	0.6193 s / batch. (data: 3.35e-04). ETA=14:56:38, max mem: 15.9 GB 
[10/30 19:44:32 visual_prompt]: 	Training 600/1106. train loss: 0.2129,	0.6330 s / batch. (data: 4.11e-04). ETA=15:15:30, max mem: 15.9 GB 
[10/30 19:45:35 visual_prompt]: 	Training 700/1106. train loss: 1.1442,	0.6311 s / batch. (data: 3.12e-04). ETA=15:11:38, max mem: 15.9 GB 
[10/30 19:46:38 visual_prompt]: 	Training 800/1106. train loss: 0.7948,	0.6366 s / batch. (data: 8.26e-04). ETA=15:18:29, max mem: 15.9 GB 
[10/30 19:47:42 visual_prompt]: 	Training 900/1106. train loss: 0.7085,	0.6345 s / batch. (data: 8.48e-04). ETA=15:14:29, max mem: 15.9 GB 
[10/30 19:48:45 visual_prompt]: 	Training 1000/1106. train loss: 0.7322,	0.6284 s / batch. (data: 3.02e-04). ETA=15:04:39, max mem: 15.9 GB 
[10/30 19:49:48 visual_prompt]: 	Training 1100/1106. train loss: 0.7694,	0.6175 s / batch. (data: 1.60e-04). ETA=14:47:51, max mem: 15.9 GB 
[10/30 19:49:52 visual_prompt]: Epoch 22 / 100: avg data time: 4.03e-03, avg batch time: 0.6344, average train loss: 0.7676
[10/30 19:50:42 visual_prompt]: 	Test 100/123. loss: 0.664, 0.2256 s / batch. (data: 2.79e-05)max mem: 15.94594 GB 
[10/30 19:50:53 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.2336, average loss: 0.7632
[10/30 19:50:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.69	
[10/30 19:50:53 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/30 19:51:59 visual_prompt]: 	Training 100/1106. train loss: 0.4967,	0.6344 s / batch. (data: 3.28e-04). ETA=15:11:09, max mem: 15.9 GB 
[10/30 19:53:02 visual_prompt]: 	Training 200/1106. train loss: 0.8599,	0.6430 s / batch. (data: 8.73e-04). ETA=15:22:20, max mem: 15.9 GB 
[10/30 19:54:06 visual_prompt]: 	Training 300/1106. train loss: 0.5978,	0.6337 s / batch. (data: 7.83e-04). ETA=15:08:00, max mem: 15.9 GB 
[10/30 19:55:09 visual_prompt]: 	Training 400/1106. train loss: 0.7331,	0.6174 s / batch. (data: 3.38e-04). ETA=14:43:37, max mem: 15.9 GB 
[10/30 19:56:13 visual_prompt]: 	Training 500/1106. train loss: 0.8270,	0.6315 s / batch. (data: 7.83e-04). ETA=15:02:40, max mem: 15.9 GB 
[10/30 19:57:16 visual_prompt]: 	Training 600/1106. train loss: 0.7044,	0.6195 s / batch. (data: 3.49e-04). ETA=14:44:35, max mem: 15.9 GB 
[10/30 19:58:19 visual_prompt]: 	Training 700/1106. train loss: 0.6400,	0.6472 s / batch. (data: 8.04e-04). ETA=15:22:57, max mem: 15.9 GB 
[10/30 19:59:22 visual_prompt]: 	Training 800/1106. train loss: 0.9015,	0.6181 s / batch. (data: 3.36e-04). ETA=14:40:24, max mem: 15.9 GB 
[10/30 20:00:26 visual_prompt]: 	Training 900/1106. train loss: 0.8094,	0.6331 s / batch. (data: 7.93e-04). ETA=15:00:45, max mem: 15.9 GB 
[10/30 20:01:29 visual_prompt]: 	Training 1000/1106. train loss: 0.6845,	0.6566 s / batch. (data: 8.30e-04). ETA=15:33:02, max mem: 15.9 GB 
[10/30 20:02:32 visual_prompt]: 	Training 1100/1106. train loss: 0.7055,	0.6185 s / batch. (data: 1.53e-04). ETA=14:37:56, max mem: 15.9 GB 
[10/30 20:02:36 visual_prompt]: Epoch 23 / 100: avg data time: 5.13e-03, avg batch time: 0.6356, average train loss: 0.7795
[10/30 20:03:26 visual_prompt]: 	Test 100/123. loss: 0.739, 0.2253 s / batch. (data: 4.24e-05)max mem: 15.94594 GB 
[10/30 20:03:36 visual_prompt]: Inference (val):avg data time: 9.82e-05, avg batch time: 0.2323, average loss: 0.7031
[10/30 20:03:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.81	
[10/30 20:03:36 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/30 20:04:42 visual_prompt]: 	Training 100/1106. train loss: 0.6236,	0.6319 s / batch. (data: 8.50e-04). ETA=14:55:54, max mem: 15.9 GB 
[10/30 20:05:45 visual_prompt]: 	Training 200/1106. train loss: 0.9975,	0.6190 s / batch. (data: 3.46e-04). ETA=14:36:35, max mem: 15.9 GB 
[10/30 20:06:48 visual_prompt]: 	Training 300/1106. train loss: 1.0020,	0.6176 s / batch. (data: 3.19e-04). ETA=14:33:29, max mem: 15.9 GB 
[10/30 20:07:51 visual_prompt]: 	Training 400/1106. train loss: 0.7531,	0.6188 s / batch. (data: 3.53e-04). ETA=14:34:07, max mem: 15.9 GB 
[10/30 20:08:54 visual_prompt]: 	Training 500/1106. train loss: 0.8222,	0.6497 s / batch. (data: 7.37e-04). ETA=15:16:45, max mem: 15.9 GB 
[10/30 20:09:58 visual_prompt]: 	Training 600/1106. train loss: 0.6995,	0.6305 s / batch. (data: 7.37e-04). ETA=14:48:36, max mem: 15.9 GB 
[10/30 20:11:01 visual_prompt]: 	Training 700/1106. train loss: 0.7788,	0.6192 s / batch. (data: 3.44e-04). ETA=14:31:42, max mem: 15.9 GB 
[10/30 20:12:04 visual_prompt]: 	Training 800/1106. train loss: 0.7993,	0.6490 s / batch. (data: 8.04e-04). ETA=15:12:26, max mem: 15.9 GB 
[10/30 20:13:08 visual_prompt]: 	Training 900/1106. train loss: 0.5100,	0.6228 s / batch. (data: 3.20e-04). ETA=14:34:34, max mem: 15.9 GB 
[10/30 20:14:11 visual_prompt]: 	Training 1000/1106. train loss: 0.7122,	0.6250 s / batch. (data: 7.37e-04). ETA=14:36:44, max mem: 15.9 GB 
[10/30 20:15:15 visual_prompt]: 	Training 1100/1106. train loss: 0.7351,	0.6191 s / batch. (data: 1.43e-04). ETA=14:27:24, max mem: 15.9 GB 
[10/30 20:15:19 visual_prompt]: Epoch 24 / 100: avg data time: 4.97e-03, avg batch time: 0.6348, average train loss: 0.7990
[10/30 20:16:09 visual_prompt]: 	Test 100/123. loss: 0.786, 0.2341 s / batch. (data: 3.74e-05)max mem: 15.94594 GB 
[10/30 20:16:19 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.2324, average loss: 0.9178
[10/30 20:16:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.43	
[10/30 20:16:19 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/30 20:17:25 visual_prompt]: 	Training 100/1106. train loss: 1.0248,	0.6360 s / batch. (data: 8.18e-04). ETA=14:49:54, max mem: 15.9 GB 
[10/30 20:18:28 visual_prompt]: 	Training 200/1106. train loss: 0.5950,	0.6311 s / batch. (data: 4.24e-04). ETA=14:42:04, max mem: 15.9 GB 
[10/30 20:19:31 visual_prompt]: 	Training 300/1106. train loss: 1.0028,	0.6338 s / batch. (data: 5.43e-03). ETA=14:44:41, max mem: 15.9 GB 
[10/30 20:20:35 visual_prompt]: 	Training 400/1106. train loss: 0.9398,	0.6183 s / batch. (data: 3.04e-04). ETA=14:22:06, max mem: 15.9 GB 
[10/30 20:21:38 visual_prompt]: 	Training 500/1106. train loss: 0.9589,	0.6191 s / batch. (data: 2.74e-04). ETA=14:22:07, max mem: 15.9 GB 
[10/30 20:22:41 visual_prompt]: 	Training 600/1106. train loss: 1.0622,	0.6447 s / batch. (data: 7.73e-04). ETA=14:56:45, max mem: 15.9 GB 
[10/30 20:23:45 visual_prompt]: 	Training 700/1106. train loss: 0.7745,	0.6294 s / batch. (data: 1.05e-02). ETA=14:34:25, max mem: 15.9 GB 
[10/30 20:24:48 visual_prompt]: 	Training 800/1106. train loss: 0.6726,	0.6197 s / batch. (data: 7.94e-04). ETA=14:19:55, max mem: 15.9 GB 
[10/30 20:25:51 visual_prompt]: 	Training 900/1106. train loss: 0.6902,	0.6355 s / batch. (data: 8.16e-04). ETA=14:40:45, max mem: 15.9 GB 
[10/30 20:26:54 visual_prompt]: 	Training 1000/1106. train loss: 0.8038,	0.6463 s / batch. (data: 8.55e-04). ETA=14:54:40, max mem: 15.9 GB 
[10/30 20:27:58 visual_prompt]: 	Training 1100/1106. train loss: 0.8771,	0.6193 s / batch. (data: 1.49e-04). ETA=14:16:12, max mem: 15.9 GB 
[10/30 20:28:01 visual_prompt]: Epoch 25 / 100: avg data time: 4.38e-03, avg batch time: 0.6349, average train loss: 0.7839
[10/30 20:28:51 visual_prompt]: 	Test 100/123. loss: 1.110, 0.2245 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/30 20:29:02 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.2326, average loss: 1.3260
[10/30 20:29:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.26	
[10/30 20:29:02 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[10/30 20:30:08 visual_prompt]: 	Training 100/1106. train loss: 0.7897,	0.6305 s / batch. (data: 3.22e-04). ETA=14:30:35, max mem: 15.9 GB 
[10/30 20:31:11 visual_prompt]: 	Training 200/1106. train loss: 0.5865,	0.6318 s / batch. (data: 7.85e-04). ETA=14:31:20, max mem: 15.9 GB 
[10/30 20:32:15 visual_prompt]: 	Training 300/1106. train loss: 0.4654,	0.6301 s / batch. (data: 1.13e-02). ETA=14:27:56, max mem: 15.9 GB 
[10/30 20:33:18 visual_prompt]: 	Training 400/1106. train loss: 0.9629,	0.6339 s / batch. (data: 9.49e-04). ETA=14:32:06, max mem: 15.9 GB 
[10/30 20:34:21 visual_prompt]: 	Training 500/1106. train loss: 0.7269,	0.6379 s / batch. (data: 5.93e-03). ETA=14:36:36, max mem: 15.9 GB 
[10/30 20:35:25 visual_prompt]: 	Training 600/1106. train loss: 0.4982,	0.6461 s / batch. (data: 2.08e-02). ETA=14:46:42, max mem: 15.9 GB 
[10/30 20:36:28 visual_prompt]: 	Training 700/1106. train loss: 1.5551,	0.6474 s / batch. (data: 8.46e-04). ETA=14:47:29, max mem: 15.9 GB 
[10/30 20:37:32 visual_prompt]: 	Training 800/1106. train loss: 0.0300,	0.6347 s / batch. (data: 3.13e-04). ETA=14:28:58, max mem: 15.9 GB 
[10/30 20:38:35 visual_prompt]: 	Training 900/1106. train loss: 1.3289,	0.6260 s / batch. (data: 3.01e-04). ETA=14:16:00, max mem: 15.9 GB 
[10/30 20:39:38 visual_prompt]: 	Training 1000/1106. train loss: 0.8508,	0.6300 s / batch. (data: 8.07e-04). ETA=14:20:25, max mem: 15.9 GB 
[10/30 20:40:41 visual_prompt]: 	Training 1100/1106. train loss: 0.4567,	0.6182 s / batch. (data: 1.56e-04). ETA=14:03:23, max mem: 15.9 GB 
[10/30 20:40:45 visual_prompt]: Epoch 26 / 100: avg data time: 4.39e-03, avg batch time: 0.6352, average train loss: 0.7737
[10/30 20:41:35 visual_prompt]: 	Test 100/123. loss: 0.642, 0.2278 s / batch. (data: 2.53e-05)max mem: 15.94594 GB 
[10/30 20:41:46 visual_prompt]: Inference (val):avg data time: 1.66e-04, avg batch time: 0.2317, average loss: 0.6895
[10/30 20:41:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 62.35	
[10/30 20:41:46 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[10/30 20:42:51 visual_prompt]: 	Training 100/1106. train loss: 0.1169,	0.6318 s / batch. (data: 8.13e-04). ETA=14:20:48, max mem: 15.9 GB 
[10/30 20:43:54 visual_prompt]: 	Training 200/1106. train loss: 0.2374,	0.6400 s / batch. (data: 2.82e-04). ETA=14:30:50, max mem: 15.9 GB 
[10/30 20:44:58 visual_prompt]: 	Training 300/1106. train loss: 1.3515,	0.6313 s / batch. (data: 8.38e-04). ETA=14:18:02, max mem: 15.9 GB 
[10/30 20:46:01 visual_prompt]: 	Training 400/1106. train loss: 1.8459,	0.6237 s / batch. (data: 2.92e-04). ETA=14:06:39, max mem: 15.9 GB 
[10/30 20:47:04 visual_prompt]: 	Training 500/1106. train loss: 1.5995,	0.6447 s / batch. (data: 7.92e-04). ETA=14:34:04, max mem: 15.9 GB 
[10/30 20:48:07 visual_prompt]: 	Training 600/1106. train loss: 0.8835,	0.6339 s / batch. (data: 1.61e-02). ETA=14:18:22, max mem: 15.9 GB 
[10/30 20:49:11 visual_prompt]: 	Training 700/1106. train loss: 1.0690,	0.6177 s / batch. (data: 3.25e-04). ETA=13:55:25, max mem: 15.9 GB 
[10/30 20:50:14 visual_prompt]: 	Training 800/1106. train loss: 0.8166,	0.6343 s / batch. (data: 1.02e-03). ETA=14:16:42, max mem: 15.9 GB 
[10/30 20:51:17 visual_prompt]: 	Training 900/1106. train loss: 0.8952,	0.6225 s / batch. (data: 7.41e-04). ETA=13:59:46, max mem: 15.9 GB 
[10/30 20:52:21 visual_prompt]: 	Training 1000/1106. train loss: 0.5381,	0.6177 s / batch. (data: 3.18e-04). ETA=13:52:19, max mem: 15.9 GB 
[10/30 20:53:24 visual_prompt]: 	Training 1100/1106. train loss: 0.5614,	0.6183 s / batch. (data: 1.41e-04). ETA=13:52:02, max mem: 15.9 GB 
[10/30 20:53:28 visual_prompt]: Epoch 27 / 100: avg data time: 4.63e-03, avg batch time: 0.6345, average train loss: 0.7726
[10/30 20:54:17 visual_prompt]: 	Test 100/123. loss: 0.649, 0.2273 s / batch. (data: 3.81e-05)max mem: 15.94594 GB 
[10/30 20:54:28 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.2336, average loss: 0.7430
[10/30 20:54:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.08	
[10/30 20:54:28 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[10/30 20:55:33 visual_prompt]: 	Training 100/1106. train loss: 0.8808,	0.6324 s / batch. (data: 8.32e-04). ETA=14:09:52, max mem: 15.9 GB 
[10/30 20:56:37 visual_prompt]: 	Training 200/1106. train loss: 0.3613,	0.6195 s / batch. (data: 5.45e-04). ETA=13:51:31, max mem: 15.9 GB 
[10/30 20:57:40 visual_prompt]: 	Training 300/1106. train loss: 0.5529,	0.6195 s / batch. (data: 3.08e-04). ETA=13:50:33, max mem: 15.9 GB 
[10/30 20:58:43 visual_prompt]: 	Training 400/1106. train loss: 1.5316,	0.6197 s / batch. (data: 2.78e-04). ETA=13:49:43, max mem: 15.9 GB 
[10/30 20:59:47 visual_prompt]: 	Training 500/1106. train loss: 1.1694,	0.6317 s / batch. (data: 7.56e-04). ETA=14:04:46, max mem: 15.9 GB 
[10/30 21:00:50 visual_prompt]: 	Training 600/1106. train loss: 0.7200,	0.6262 s / batch. (data: 3.09e-04). ETA=13:56:22, max mem: 15.9 GB 
[10/30 21:01:53 visual_prompt]: 	Training 700/1106. train loss: 1.2313,	0.6540 s / batch. (data: 8.23e-04). ETA=14:32:25, max mem: 15.9 GB 
[10/30 21:02:57 visual_prompt]: 	Training 800/1106. train loss: 1.2071,	0.6554 s / batch. (data: 8.24e-04). ETA=14:33:11, max mem: 15.9 GB 
[10/30 21:04:00 visual_prompt]: 	Training 900/1106. train loss: 0.6939,	0.6188 s / batch. (data: 2.94e-04). ETA=13:43:21, max mem: 15.9 GB 
[10/30 21:05:03 visual_prompt]: 	Training 1000/1106. train loss: 0.8470,	0.6520 s / batch. (data: 8.23e-04). ETA=14:26:27, max mem: 15.9 GB 
[10/30 21:06:06 visual_prompt]: 	Training 1100/1106. train loss: 0.5900,	0.6173 s / batch. (data: 1.34e-04). ETA=13:39:23, max mem: 15.9 GB 
[10/30 21:06:10 visual_prompt]: Epoch 28 / 100: avg data time: 4.23e-03, avg batch time: 0.6347, average train loss: 0.7808
[10/30 21:07:00 visual_prompt]: 	Test 100/123. loss: 0.626, 0.2256 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[10/30 21:07:11 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.2326, average loss: 0.7103
[10/30 21:07:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 62.88	
[10/30 21:07:11 visual_prompt]: Stopping early.
[10/30 21:07:11 visual_prompt]: Rank of current process: 0. World size: 1
[10/30 21:07:11 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/30 21:07:11 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/30 21:07:11 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/30 21:07:11 visual_prompt]: Training with config:
[10/30 21:07:11 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.1_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/30 21:07:11 visual_prompt]: Loading training data...
[10/30 21:07:11 visual_prompt]: Constructing mammo-cbis dataset train...
[10/30 21:07:11 visual_prompt]: Loading validation data...
[10/30 21:07:11 visual_prompt]: Constructing mammo-cbis dataset val...
[10/30 21:07:11 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/30 21:07:14 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/30 21:07:14 visual_prompt]: tuned percent:0.522
[10/30 21:07:14 visual_prompt]: Device used for model: 0
[10/30 21:07:14 visual_prompt]: Setting up Evaluator...
[10/30 21:07:14 visual_prompt]: Setting up Trainer...
[10/30 21:07:14 visual_prompt]: 	Setting up the optimizer...
[10/30 21:07:14 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/30 21:08:19 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6241 s / batch. (data: 3.15e-04). ETA=19:09:22, max mem: 15.9 GB 
[10/30 21:09:23 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6326 s / batch. (data: 7.43e-04). ETA=19:24:01, max mem: 15.9 GB 
[10/30 21:10:26 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6189 s / batch. (data: 2.95e-04). ETA=18:57:42, max mem: 15.9 GB 
[10/30 21:11:29 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6322 s / batch. (data: 3.03e-04). ETA=19:21:11, max mem: 15.9 GB 
[10/30 21:12:32 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6360 s / batch. (data: 1.20e-02). ETA=19:27:00, max mem: 15.9 GB 
[10/30 21:13:36 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6327 s / batch. (data: 7.98e-04). ETA=19:19:54, max mem: 15.9 GB 
[10/30 21:14:39 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6568 s / batch. (data: 1.51e-02). ETA=20:02:57, max mem: 15.9 GB 
[10/30 21:15:42 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6452 s / batch. (data: 8.20e-04). ETA=19:40:48, max mem: 15.9 GB 
[10/30 21:16:46 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6560 s / batch. (data: 3.10e-04). ETA=19:59:25, max mem: 15.9 GB 
[10/30 21:17:49 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6329 s / batch. (data: 1.01e-03). ETA=19:16:01, max mem: 15.9 GB 
[10/30 21:18:52 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6185 s / batch. (data: 1.42e-04). ETA=18:48:46, max mem: 15.9 GB 
[10/30 21:18:56 visual_prompt]: Epoch 1 / 100: avg data time: 4.97e-03, avg batch time: 0.6348, average train loss: 1.4028
[10/30 21:19:46 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2252 s / batch. (data: 3.55e-05)max mem: 15.94594 GB 
[10/30 21:19:57 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.2331, average loss: 1.3505
[10/30 21:19:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/30 21:19:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/30 21:21:01 visual_prompt]: 	Training 100/1106. train loss: 0.6855,	0.6542 s / batch. (data: 8.24e-04). ETA=19:52:43, max mem: 15.9 GB 
[10/30 21:22:05 visual_prompt]: 	Training 200/1106. train loss: 0.6756,	0.6189 s / batch. (data: 2.95e-04). ETA=18:47:21, max mem: 15.9 GB 
[10/30 21:23:08 visual_prompt]: 	Training 300/1106. train loss: 0.9982,	0.6308 s / batch. (data: 2.91e-04). ETA=19:08:00, max mem: 15.9 GB 
[10/30 21:24:11 visual_prompt]: 	Training 400/1106. train loss: 0.1167,	0.6440 s / batch. (data: 7.35e-04). ETA=19:31:00, max mem: 15.9 GB 
[10/30 21:25:15 visual_prompt]: 	Training 500/1106. train loss: 0.6787,	0.6371 s / batch. (data: 1.73e-03). ETA=19:17:20, max mem: 15.9 GB 
[10/30 21:26:18 visual_prompt]: 	Training 600/1106. train loss: 0.5638,	0.6446 s / batch. (data: 7.93e-04). ETA=19:29:48, max mem: 15.9 GB 
[10/30 21:27:21 visual_prompt]: 	Training 700/1106. train loss: 0.9454,	0.6486 s / batch. (data: 2.81e-04). ETA=19:36:08, max mem: 15.9 GB 
[10/30 21:28:25 visual_prompt]: 	Training 800/1106. train loss: 0.7623,	0.6610 s / batch. (data: 3.30e-02). ETA=19:57:25, max mem: 15.9 GB 
[10/30 21:29:28 visual_prompt]: 	Training 900/1106. train loss: 0.5964,	0.6367 s / batch. (data: 8.21e-04). ETA=19:12:17, max mem: 15.9 GB 
[10/30 21:30:32 visual_prompt]: 	Training 1000/1106. train loss: 0.5809,	0.6281 s / batch. (data: 9.62e-03). ETA=18:55:46, max mem: 15.9 GB 
[10/30 21:31:35 visual_prompt]: 	Training 1100/1106. train loss: 0.6076,	0.6194 s / batch. (data: 1.38e-04). ETA=18:38:57, max mem: 15.9 GB 
[10/30 21:31:39 visual_prompt]: Epoch 2 / 100: avg data time: 3.92e-03, avg batch time: 0.6345, average train loss: 0.7977
[10/30 21:32:28 visual_prompt]: 	Test 100/123. loss: 1.129, 0.2254 s / batch. (data: 3.96e-05)max mem: 15.94594 GB 
[10/30 21:32:39 visual_prompt]: Inference (val):avg data time: 1.31e-04, avg batch time: 0.2317, average loss: 1.0331
[10/30 21:32:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.55	
[10/30 21:32:39 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/30 21:33:45 visual_prompt]: 	Training 100/1106. train loss: 0.7161,	0.6390 s / batch. (data: 8.84e-04). ETA=19:13:15, max mem: 15.9 GB 
[10/30 21:34:48 visual_prompt]: 	Training 200/1106. train loss: 0.2293,	0.6489 s / batch. (data: 7.97e-04). ETA=19:30:02, max mem: 15.9 GB 
[10/30 21:35:52 visual_prompt]: 	Training 300/1106. train loss: 0.4003,	0.6423 s / batch. (data: 1.10e-02). ETA=19:17:05, max mem: 15.9 GB 
[10/30 21:36:55 visual_prompt]: 	Training 400/1106. train loss: 0.8524,	0.6404 s / batch. (data: 3.51e-04). ETA=19:12:35, max mem: 15.9 GB 
[10/30 21:37:59 visual_prompt]: 	Training 500/1106. train loss: 1.1041,	0.6305 s / batch. (data: 7.84e-04). ETA=18:53:43, max mem: 15.9 GB 
[10/30 21:39:02 visual_prompt]: 	Training 600/1106. train loss: 0.6818,	0.6317 s / batch. (data: 8.43e-04). ETA=18:54:54, max mem: 15.9 GB 
[10/30 21:40:05 visual_prompt]: 	Training 700/1106. train loss: 1.0658,	0.6436 s / batch. (data: 7.56e-04). ETA=19:15:10, max mem: 15.9 GB 
[10/30 21:41:08 visual_prompt]: 	Training 800/1106. train loss: 1.8221,	0.6433 s / batch. (data: 3.16e-04). ETA=19:13:33, max mem: 15.9 GB 
[10/30 21:42:12 visual_prompt]: 	Training 900/1106. train loss: 1.4064,	0.6279 s / batch. (data: 2.97e-04). ETA=18:44:49, max mem: 15.9 GB 
[10/30 21:43:15 visual_prompt]: 	Training 1000/1106. train loss: 0.6584,	0.6341 s / batch. (data: 1.43e-02). ETA=18:54:50, max mem: 15.9 GB 
[10/30 21:44:18 visual_prompt]: 	Training 1100/1106. train loss: 0.5332,	0.6189 s / batch. (data: 1.54e-04). ETA=18:26:39, max mem: 15.9 GB 
[10/30 21:44:22 visual_prompt]: Epoch 3 / 100: avg data time: 5.08e-03, avg batch time: 0.6355, average train loss: 0.8002
[10/30 21:45:12 visual_prompt]: 	Test 100/123. loss: 0.835, 0.2434 s / batch. (data: 3.72e-05)max mem: 15.94594 GB 
[10/30 21:45:22 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.2330, average loss: 0.8335
[10/30 21:45:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.56	
[10/30 21:45:22 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/30 21:46:28 visual_prompt]: 	Training 100/1106. train loss: 1.1685,	0.6305 s / batch. (data: 8.05e-04). ETA=18:46:19, max mem: 15.9 GB 
[10/30 21:47:31 visual_prompt]: 	Training 200/1106. train loss: 1.0986,	0.6287 s / batch. (data: 3.15e-04). ETA=18:42:07, max mem: 15.9 GB 
[10/30 21:48:34 visual_prompt]: 	Training 300/1106. train loss: 0.9981,	0.6192 s / batch. (data: 3.50e-04). ETA=18:24:05, max mem: 15.9 GB 
[10/30 21:49:38 visual_prompt]: 	Training 400/1106. train loss: 0.7798,	0.6436 s / batch. (data: 1.56e-02). ETA=19:06:29, max mem: 15.9 GB 
[10/30 21:50:41 visual_prompt]: 	Training 500/1106. train loss: 2.8343,	0.6480 s / batch. (data: 7.81e-04). ETA=19:13:14, max mem: 15.9 GB 
[10/30 21:51:44 visual_prompt]: 	Training 600/1106. train loss: 0.2169,	0.6205 s / batch. (data: 3.50e-04). ETA=18:23:16, max mem: 15.9 GB 
[10/30 21:52:48 visual_prompt]: 	Training 700/1106. train loss: 1.3361,	0.6508 s / batch. (data: 4.40e-04). ETA=19:15:59, max mem: 15.9 GB 
[10/30 21:53:51 visual_prompt]: 	Training 800/1106. train loss: 0.0443,	0.6479 s / batch. (data: 8.08e-04). ETA=19:09:50, max mem: 15.9 GB 
[10/30 21:54:54 visual_prompt]: 	Training 900/1106. train loss: 1.1524,	0.6465 s / batch. (data: 3.36e-04). ETA=19:06:16, max mem: 15.9 GB 
[10/30 21:55:58 visual_prompt]: 	Training 1000/1106. train loss: 0.6598,	0.6520 s / batch. (data: 7.81e-04). ETA=19:14:54, max mem: 15.9 GB 
[10/30 21:57:01 visual_prompt]: 	Training 1100/1106. train loss: 1.0819,	0.6188 s / batch. (data: 1.60e-04). ETA=18:15:02, max mem: 15.9 GB 
[10/30 21:57:05 visual_prompt]: Epoch 4 / 100: avg data time: 3.77e-03, avg batch time: 0.6353, average train loss: 0.8239
[10/30 21:57:55 visual_prompt]: 	Test 100/123. loss: 1.124, 0.2384 s / batch. (data: 2.72e-05)max mem: 15.94594 GB 
[10/30 21:58:06 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.2323, average loss: 1.0091
[10/30 21:58:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.70	
[10/30 21:58:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/30 21:59:11 visual_prompt]: 	Training 100/1106. train loss: 1.4224,	0.6330 s / batch. (data: 4.41e-04). ETA=18:39:09, max mem: 15.9 GB 
[10/30 22:00:14 visual_prompt]: 	Training 200/1106. train loss: 1.2425,	0.6307 s / batch. (data: 8.14e-04). ETA=18:33:58, max mem: 15.9 GB 
[10/30 22:01:18 visual_prompt]: 	Training 300/1106. train loss: 0.7895,	0.6303 s / batch. (data: 7.90e-04). ETA=18:32:16, max mem: 15.9 GB 
[10/30 22:02:21 visual_prompt]: 	Training 400/1106. train loss: 1.1068,	0.6587 s / batch. (data: 7.89e-04). ETA=19:21:19, max mem: 15.9 GB 
[10/30 22:03:25 visual_prompt]: 	Training 500/1106. train loss: 0.0884,	0.6342 s / batch. (data: 3.27e-04). ETA=18:37:04, max mem: 15.9 GB 
[10/30 22:04:28 visual_prompt]: 	Training 600/1106. train loss: 2.1983,	0.6187 s / batch. (data: 2.99e-04). ETA=18:08:41, max mem: 15.9 GB 
[10/30 22:05:31 visual_prompt]: 	Training 700/1106. train loss: 0.6296,	0.6330 s / batch. (data: 8.01e-04). ETA=18:32:48, max mem: 15.9 GB 
[10/30 22:06:34 visual_prompt]: 	Training 800/1106. train loss: 1.7421,	0.6326 s / batch. (data: 7.75e-04). ETA=18:31:05, max mem: 15.9 GB 
[10/30 22:07:38 visual_prompt]: 	Training 900/1106. train loss: 0.6868,	0.6362 s / batch. (data: 7.23e-04). ETA=18:36:15, max mem: 15.9 GB 
[10/30 22:08:41 visual_prompt]: 	Training 1000/1106. train loss: 0.3081,	0.6336 s / batch. (data: 7.96e-04). ETA=18:30:40, max mem: 15.9 GB 
[10/30 22:09:45 visual_prompt]: 	Training 1100/1106. train loss: 0.7838,	0.6185 s / batch. (data: 1.44e-04). ETA=18:03:07, max mem: 15.9 GB 
[10/30 22:09:49 visual_prompt]: Epoch 5 / 100: avg data time: 4.00e-03, avg batch time: 0.6352, average train loss: 0.8385
[10/30 22:10:38 visual_prompt]: 	Test 100/123. loss: 1.052, 0.2317 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/30 22:10:49 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.2338, average loss: 1.0916
[10/30 22:10:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.83	
[10/30 22:10:49 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/30 22:11:54 visual_prompt]: 	Training 100/1106. train loss: 1.1543,	0.6297 s / batch. (data: 3.27e-04). ETA=18:21:44, max mem: 15.9 GB 
[10/30 22:12:58 visual_prompt]: 	Training 200/1106. train loss: 0.4207,	0.6480 s / batch. (data: 8.03e-04). ETA=18:52:36, max mem: 15.9 GB 
[10/30 22:14:01 visual_prompt]: 	Training 300/1106. train loss: 0.0548,	0.6198 s / batch. (data: 3.20e-04). ETA=18:02:17, max mem: 15.9 GB 
[10/30 22:15:04 visual_prompt]: 	Training 400/1106. train loss: 1.8222,	0.6195 s / batch. (data: 3.06e-04). ETA=18:00:39, max mem: 15.9 GB 
[10/30 22:16:07 visual_prompt]: 	Training 500/1106. train loss: 0.6965,	0.6345 s / batch. (data: 3.20e-04). ETA=18:25:45, max mem: 15.9 GB 
[10/30 22:17:11 visual_prompt]: 	Training 600/1106. train loss: 1.2315,	0.6475 s / batch. (data: 8.32e-04). ETA=18:47:29, max mem: 15.9 GB 
[10/30 22:18:14 visual_prompt]: 	Training 700/1106. train loss: 0.9676,	0.6438 s / batch. (data: 3.21e-04). ETA=18:39:52, max mem: 15.9 GB 
[10/30 22:19:18 visual_prompt]: 	Training 800/1106. train loss: 0.7702,	0.6249 s / batch. (data: 4.60e-03). ETA=18:06:00, max mem: 15.9 GB 
[10/30 22:20:21 visual_prompt]: 	Training 900/1106. train loss: 0.8281,	0.6317 s / batch. (data: 7.28e-04). ETA=18:16:43, max mem: 15.9 GB 
[10/30 22:21:25 visual_prompt]: 	Training 1000/1106. train loss: 2.6296,	0.6306 s / batch. (data: 7.89e-04). ETA=18:13:48, max mem: 15.9 GB 
[10/30 22:22:28 visual_prompt]: 	Training 1100/1106. train loss: 0.1940,	0.6194 s / batch. (data: 1.59e-04). ETA=17:53:14, max mem: 15.9 GB 
[10/30 22:22:32 visual_prompt]: Epoch 6 / 100: avg data time: 4.17e-03, avg batch time: 0.6353, average train loss: 0.8784
[10/30 22:23:22 visual_prompt]: 	Test 100/123. loss: 0.901, 0.2337 s / batch. (data: 2.81e-05)max mem: 15.94594 GB 
[10/30 22:23:32 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.2343, average loss: 0.8299
[10/30 22:23:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.61	
[10/30 22:23:32 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/30 22:24:37 visual_prompt]: 	Training 100/1106. train loss: 0.7484,	0.6374 s / batch. (data: 3.50e-04). ETA=18:23:19, max mem: 15.9 GB 
[10/30 22:25:40 visual_prompt]: 	Training 200/1106. train loss: 1.3123,	0.6315 s / batch. (data: 7.67e-04). ETA=18:12:10, max mem: 15.9 GB 
[10/30 22:26:44 visual_prompt]: 	Training 300/1106. train loss: 0.1308,	0.6477 s / batch. (data: 8.22e-04). ETA=18:39:04, max mem: 15.9 GB 
[10/30 22:27:47 visual_prompt]: 	Training 400/1106. train loss: 1.0706,	0.6295 s / batch. (data: 3.35e-04). ETA=18:06:36, max mem: 15.9 GB 
[10/30 22:28:50 visual_prompt]: 	Training 500/1106. train loss: 0.8292,	0.6332 s / batch. (data: 7.95e-04). ETA=18:11:52, max mem: 15.9 GB 
[10/30 22:29:54 visual_prompt]: 	Training 600/1106. train loss: 0.7807,	0.6191 s / batch. (data: 3.00e-04). ETA=17:46:30, max mem: 15.9 GB 
[10/30 22:30:57 visual_prompt]: 	Training 700/1106. train loss: 0.6807,	0.6531 s / batch. (data: 8.35e-04). ETA=18:44:03, max mem: 15.9 GB 
[10/30 22:32:01 visual_prompt]: 	Training 800/1106. train loss: 0.2565,	0.6305 s / batch. (data: 1.12e-03). ETA=18:04:08, max mem: 15.9 GB 
[10/30 22:33:04 visual_prompt]: 	Training 900/1106. train loss: 0.7633,	0.6266 s / batch. (data: 3.20e-04). ETA=17:56:22, max mem: 15.9 GB 
[10/30 22:34:07 visual_prompt]: 	Training 1000/1106. train loss: 0.8348,	0.6195 s / batch. (data: 3.42e-04). ETA=17:43:01, max mem: 15.9 GB 
[10/30 22:35:10 visual_prompt]: 	Training 1100/1106. train loss: 0.5043,	0.6182 s / batch. (data: 1.53e-04). ETA=17:39:49, max mem: 15.9 GB 
[10/30 22:35:14 visual_prompt]: Epoch 7 / 100: avg data time: 4.19e-03, avg batch time: 0.6348, average train loss: 0.8592
[10/30 22:36:04 visual_prompt]: 	Test 100/123. loss: 0.745, 0.2255 s / batch. (data: 6.06e-05)max mem: 15.94594 GB 
[10/30 22:36:15 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.2317, average loss: 0.7024
[10/30 22:36:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.73	
[10/30 22:36:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/30 22:37:20 visual_prompt]: 	Training 100/1106. train loss: 0.6356,	0.6428 s / batch. (data: 2.00e-02). ETA=18:20:56, max mem: 15.9 GB 
[10/30 22:38:23 visual_prompt]: 	Training 200/1106. train loss: 1.2391,	0.6392 s / batch. (data: 2.81e-04). ETA=18:13:35, max mem: 15.9 GB 
[10/30 22:39:27 visual_prompt]: 	Training 300/1106. train loss: 0.2608,	0.6293 s / batch. (data: 5.43e-03). ETA=17:55:43, max mem: 15.9 GB 
[10/30 22:40:30 visual_prompt]: 	Training 400/1106. train loss: 0.6617,	0.6190 s / batch. (data: 2.39e-04). ETA=17:37:00, max mem: 15.9 GB 
[10/30 22:41:34 visual_prompt]: 	Training 500/1106. train loss: 0.1442,	0.6312 s / batch. (data: 3.32e-04). ETA=17:56:44, max mem: 15.9 GB 
[10/30 22:42:37 visual_prompt]: 	Training 600/1106. train loss: 1.0520,	0.6287 s / batch. (data: 3.15e-04). ETA=17:51:31, max mem: 15.9 GB 
[10/30 22:43:40 visual_prompt]: 	Training 700/1106. train loss: 0.8254,	0.6306 s / batch. (data: 3.16e-04). ETA=17:53:42, max mem: 15.9 GB 
[10/30 22:44:44 visual_prompt]: 	Training 800/1106. train loss: 0.8242,	0.6461 s / batch. (data: 1.32e-02). ETA=18:18:59, max mem: 15.9 GB 
[10/30 22:45:47 visual_prompt]: 	Training 900/1106. train loss: 0.0951,	0.6566 s / batch. (data: 8.10e-04). ETA=18:35:48, max mem: 15.9 GB 
[10/30 22:46:50 visual_prompt]: 	Training 1000/1106. train loss: 2.6390,	0.6322 s / batch. (data: 3.31e-04). ETA=17:53:11, max mem: 15.9 GB 
[10/30 22:47:53 visual_prompt]: 	Training 1100/1106. train loss: 1.5727,	0.6182 s / batch. (data: 1.43e-04). ETA=17:28:29, max mem: 15.9 GB 
[10/30 22:47:57 visual_prompt]: Epoch 8 / 100: avg data time: 3.99e-03, avg batch time: 0.6349, average train loss: 0.9482
[10/30 22:48:47 visual_prompt]: 	Test 100/123. loss: 0.698, 0.2256 s / batch. (data: 2.53e-05)max mem: 15.94594 GB 
[10/30 22:48:58 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.2322, average loss: 0.6984
[10/30 22:48:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 58.07	
[10/30 22:48:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/30 22:50:03 visual_prompt]: 	Training 100/1106. train loss: 0.7863,	0.6263 s / batch. (data: 2.98e-04). ETA=17:41:06, max mem: 15.9 GB 
[10/30 22:51:07 visual_prompt]: 	Training 200/1106. train loss: 0.1103,	0.6468 s / batch. (data: 2.79e-02). ETA=18:14:39, max mem: 15.9 GB 
[10/30 22:52:10 visual_prompt]: 	Training 300/1106. train loss: 1.1674,	0.6332 s / batch. (data: 8.25e-04). ETA=17:50:35, max mem: 15.9 GB 
[10/30 22:53:13 visual_prompt]: 	Training 400/1106. train loss: 0.5722,	0.6410 s / batch. (data: 7.43e-04). ETA=18:02:47, max mem: 15.9 GB 
[10/30 22:54:17 visual_prompt]: 	Training 500/1106. train loss: 1.2297,	0.6328 s / batch. (data: 3.29e-04). ETA=17:47:55, max mem: 15.9 GB 
[10/30 22:55:20 visual_prompt]: 	Training 600/1106. train loss: 0.6234,	0.6300 s / batch. (data: 2.71e-04). ETA=17:42:04, max mem: 15.9 GB 
[10/30 22:56:23 visual_prompt]: 	Training 700/1106. train loss: 1.2051,	0.6189 s / batch. (data: 3.30e-04). ETA=17:22:16, max mem: 15.9 GB 
[10/30 22:57:26 visual_prompt]: 	Training 800/1106. train loss: 0.8410,	0.6209 s / batch. (data: 2.77e-04). ETA=17:24:38, max mem: 15.9 GB 
[10/30 22:58:30 visual_prompt]: 	Training 900/1106. train loss: 0.6052,	0.6335 s / batch. (data: 7.41e-04). ETA=17:44:50, max mem: 15.9 GB 
[10/30 22:59:33 visual_prompt]: 	Training 1000/1106. train loss: 0.8159,	0.6192 s / batch. (data: 2.80e-04). ETA=17:19:42, max mem: 15.9 GB 
[10/30 23:00:36 visual_prompt]: 	Training 1100/1106. train loss: 0.7697,	0.6175 s / batch. (data: 1.40e-04). ETA=17:15:50, max mem: 15.9 GB 
[10/30 23:00:40 visual_prompt]: Epoch 9 / 100: avg data time: 4.35e-03, avg batch time: 0.6344, average train loss: 0.8327
[10/30 23:01:29 visual_prompt]: 	Test 100/123. loss: 0.811, 0.2257 s / batch. (data: 2.72e-05)max mem: 15.94594 GB 
[10/30 23:01:40 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.2313, average loss: 0.7595
[10/30 23:01:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.28	
[10/30 23:01:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/30 23:02:45 visual_prompt]: 	Training 100/1106. train loss: 0.8311,	0.6303 s / batch. (data: 9.08e-04). ETA=17:36:17, max mem: 15.9 GB 
[10/30 23:03:48 visual_prompt]: 	Training 200/1106. train loss: 2.0745,	0.6751 s / batch. (data: 7.89e-04). ETA=18:50:10, max mem: 15.9 GB 
[10/30 23:04:51 visual_prompt]: 	Training 300/1106. train loss: 0.9716,	0.6389 s / batch. (data: 7.27e-04). ETA=17:48:27, max mem: 15.9 GB 
[10/30 23:05:54 visual_prompt]: 	Training 400/1106. train loss: 0.9122,	0.6367 s / batch. (data: 7.30e-04). ETA=17:43:47, max mem: 15.9 GB 
[10/30 23:06:57 visual_prompt]: 	Training 500/1106. train loss: 0.7416,	0.6172 s / batch. (data: 3.22e-04). ETA=17:10:11, max mem: 15.9 GB 
[10/30 23:08:00 visual_prompt]: 	Training 600/1106. train loss: 0.5121,	0.6356 s / batch. (data: 7.69e-04). ETA=17:39:47, max mem: 15.9 GB 
[10/30 23:09:04 visual_prompt]: 	Training 700/1106. train loss: 0.3996,	0.6340 s / batch. (data: 3.11e-04). ETA=17:36:07, max mem: 15.9 GB 
[10/30 23:10:07 visual_prompt]: 	Training 800/1106. train loss: 0.6631,	0.6480 s / batch. (data: 7.61e-04). ETA=17:58:21, max mem: 15.9 GB 
[10/30 23:11:10 visual_prompt]: 	Training 900/1106. train loss: 0.5291,	0.6426 s / batch. (data: 1.14e-03). ETA=17:48:14, max mem: 15.9 GB 
[10/30 23:12:13 visual_prompt]: 	Training 1000/1106. train loss: 0.4459,	0.6627 s / batch. (data: 5.86e-03). ETA=18:20:36, max mem: 15.9 GB 
[10/30 23:13:17 visual_prompt]: 	Training 1100/1106. train loss: 0.7413,	0.6182 s / batch. (data: 1.43e-04). ETA=17:05:39, max mem: 15.9 GB 
[10/30 23:13:20 visual_prompt]: Epoch 10 / 100: avg data time: 3.54e-03, avg batch time: 0.6336, average train loss: 0.9990
[10/30 23:14:10 visual_prompt]: 	Test 100/123. loss: 0.673, 0.2420 s / batch. (data: 4.98e-05)max mem: 15.94594 GB 
[10/30 23:14:20 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.2322, average loss: 0.6849
[10/30 23:14:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.48	
[10/30 23:14:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/30 23:15:26 visual_prompt]: 	Training 100/1106. train loss: 2.1390,	0.6351 s / batch. (data: 7.47e-04). ETA=17:32:36, max mem: 15.9 GB 
[10/30 23:16:29 visual_prompt]: 	Training 200/1106. train loss: 0.8951,	0.6190 s / batch. (data: 3.35e-04). ETA=17:04:49, max mem: 15.9 GB 
[10/30 23:17:33 visual_prompt]: 	Training 300/1106. train loss: 0.6487,	0.6305 s / batch. (data: 4.39e-04). ETA=17:22:47, max mem: 15.9 GB 
[10/30 23:18:36 visual_prompt]: 	Training 400/1106. train loss: 0.8871,	0.6173 s / batch. (data: 3.42e-04). ETA=16:59:56, max mem: 15.9 GB 
[10/30 23:19:39 visual_prompt]: 	Training 500/1106. train loss: 1.2963,	0.6301 s / batch. (data: 3.08e-04). ETA=17:20:05, max mem: 15.9 GB 
[10/30 23:20:42 visual_prompt]: 	Training 600/1106. train loss: 0.1661,	0.6373 s / batch. (data: 8.90e-04). ETA=17:30:54, max mem: 15.9 GB 
[10/30 23:21:46 visual_prompt]: 	Training 700/1106. train loss: 0.4732,	0.6191 s / batch. (data: 2.73e-04). ETA=16:59:53, max mem: 15.9 GB 
[10/30 23:22:49 visual_prompt]: 	Training 800/1106. train loss: 0.7055,	0.6423 s / batch. (data: 7.43e-04). ETA=17:37:03, max mem: 15.9 GB 
[10/30 23:23:52 visual_prompt]: 	Training 900/1106. train loss: 0.3115,	0.6382 s / batch. (data: 8.13e-04). ETA=17:29:12, max mem: 15.9 GB 
[10/30 23:24:55 visual_prompt]: 	Training 1000/1106. train loss: 0.3230,	0.6347 s / batch. (data: 2.97e-04). ETA=17:22:24, max mem: 15.9 GB 
[10/30 23:25:58 visual_prompt]: 	Training 1100/1106. train loss: 0.8983,	0.6178 s / batch. (data: 1.41e-04). ETA=16:53:31, max mem: 15.9 GB 
[10/30 23:26:02 visual_prompt]: Epoch 11 / 100: avg data time: 4.10e-03, avg batch time: 0.6343, average train loss: 0.8584
[10/30 23:26:52 visual_prompt]: 	Test 100/123. loss: 0.812, 0.2263 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/30 23:27:02 visual_prompt]: Inference (val):avg data time: 9.73e-05, avg batch time: 0.2327, average loss: 0.7864
[10/30 23:27:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.12	
[10/30 23:27:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/30 23:28:08 visual_prompt]: 	Training 100/1106. train loss: 0.1941,	0.6257 s / batch. (data: 2.89e-04). ETA=17:05:31, max mem: 15.9 GB 
[10/30 23:29:12 visual_prompt]: 	Training 200/1106. train loss: 0.7910,	0.6188 s / batch. (data: 3.24e-04). ETA=16:53:05, max mem: 15.9 GB 
[10/30 23:30:15 visual_prompt]: 	Training 300/1106. train loss: 0.1369,	0.6320 s / batch. (data: 3.20e-04). ETA=17:13:38, max mem: 15.9 GB 
[10/30 23:31:18 visual_prompt]: 	Training 400/1106. train loss: 0.6074,	0.6178 s / batch. (data: 2.93e-04). ETA=16:49:29, max mem: 15.9 GB 
[10/30 23:32:22 visual_prompt]: 	Training 500/1106. train loss: 3.4560,	0.6190 s / batch. (data: 2.52e-04). ETA=16:50:20, max mem: 15.9 GB 
[10/30 23:33:25 visual_prompt]: 	Training 600/1106. train loss: 1.1937,	0.6193 s / batch. (data: 3.05e-04). ETA=16:49:45, max mem: 15.9 GB 
[10/30 23:34:28 visual_prompt]: 	Training 700/1106. train loss: 0.7053,	0.6344 s / batch. (data: 7.41e-04). ETA=17:13:22, max mem: 15.9 GB 
[10/30 23:35:32 visual_prompt]: 	Training 800/1106. train loss: 1.2037,	0.6318 s / batch. (data: 7.66e-04). ETA=17:08:02, max mem: 15.9 GB 
[10/30 23:36:35 visual_prompt]: 	Training 900/1106. train loss: 0.1786,	0.6339 s / batch. (data: 9.53e-04). ETA=17:10:24, max mem: 15.9 GB 
[10/30 23:37:38 visual_prompt]: 	Training 1000/1106. train loss: 0.9686,	0.6174 s / batch. (data: 3.18e-04). ETA=16:42:38, max mem: 15.9 GB 
[10/30 23:38:41 visual_prompt]: 	Training 1100/1106. train loss: 1.1436,	0.6181 s / batch. (data: 1.25e-04). ETA=16:42:38, max mem: 15.9 GB 
[10/30 23:38:45 visual_prompt]: Epoch 12 / 100: avg data time: 4.98e-03, avg batch time: 0.6352, average train loss: 0.9301
[10/30 23:39:34 visual_prompt]: 	Test 100/123. loss: 1.863, 0.2250 s / batch. (data: 2.26e-05)max mem: 15.94594 GB 
[10/30 23:39:45 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.2316, average loss: 2.0481
[10/30 23:39:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.56	
[10/30 23:39:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/30 23:40:50 visual_prompt]: 	Training 100/1106. train loss: 2.1208,	0.6172 s / batch. (data: 3.12e-04). ETA=16:40:12, max mem: 15.9 GB 
[10/30 23:41:53 visual_prompt]: 	Training 200/1106. train loss: 0.9931,	0.6179 s / batch. (data: 3.63e-04). ETA=16:40:15, max mem: 15.9 GB 
[10/30 23:42:57 visual_prompt]: 	Training 300/1106. train loss: 0.2030,	0.6302 s / batch. (data: 7.88e-04). ETA=16:59:05, max mem: 15.9 GB 
[10/30 23:44:00 visual_prompt]: 	Training 400/1106. train loss: 0.5621,	0.6185 s / batch. (data: 3.12e-04). ETA=16:39:07, max mem: 15.9 GB 
[10/30 23:45:03 visual_prompt]: 	Training 500/1106. train loss: 0.2974,	0.6396 s / batch. (data: 2.73e-04). ETA=17:12:06, max mem: 15.9 GB 
[10/30 23:46:06 visual_prompt]: 	Training 600/1106. train loss: 2.0059,	0.6367 s / batch. (data: 7.65e-04). ETA=17:06:26, max mem: 15.9 GB 
[10/30 23:47:09 visual_prompt]: 	Training 700/1106. train loss: 1.0406,	0.6281 s / batch. (data: 7.04e-04). ETA=16:51:33, max mem: 15.9 GB 
[10/30 23:48:12 visual_prompt]: 	Training 800/1106. train loss: 1.3138,	0.6279 s / batch. (data: 2.80e-04). ETA=16:50:07, max mem: 15.9 GB 
[10/30 23:49:16 visual_prompt]: 	Training 900/1106. train loss: 1.4885,	0.6276 s / batch. (data: 3.13e-04). ETA=16:48:37, max mem: 15.9 GB 
[10/30 23:50:19 visual_prompt]: 	Training 1000/1106. train loss: 0.1013,	0.6286 s / batch. (data: 3.16e-04). ETA=16:49:16, max mem: 15.9 GB 
[10/30 23:51:22 visual_prompt]: 	Training 1100/1106. train loss: 1.2977,	0.6183 s / batch. (data: 2.24e-04). ETA=16:31:42, max mem: 15.9 GB 
[10/30 23:51:26 visual_prompt]: Epoch 13 / 100: avg data time: 4.73e-03, avg batch time: 0.6339, average train loss: 0.9177
[10/30 23:52:15 visual_prompt]: 	Test 100/123. loss: 0.654, 0.2280 s / batch. (data: 2.22e-05)max mem: 15.94594 GB 
[10/30 23:52:26 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.2332, average loss: 0.6796
[10/30 23:52:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.53	
[10/30 23:52:26 visual_prompt]: Best epoch 13: best metric: -0.680
[10/30 23:52:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/30 23:53:32 visual_prompt]: 	Training 100/1106. train loss: 0.2182,	0.6462 s / batch. (data: 7.51e-04). ETA=17:15:11, max mem: 15.9 GB 
[10/30 23:54:35 visual_prompt]: 	Training 200/1106. train loss: 1.1795,	0.6178 s / batch. (data: 3.34e-04). ETA=16:28:43, max mem: 15.9 GB 
[10/30 23:55:38 visual_prompt]: 	Training 300/1106. train loss: 0.7228,	0.6183 s / batch. (data: 3.48e-04). ETA=16:28:25, max mem: 15.9 GB 
[10/30 23:56:41 visual_prompt]: 	Training 400/1106. train loss: 0.1471,	0.6335 s / batch. (data: 3.09e-04). ETA=16:51:42, max mem: 15.9 GB 
[10/30 23:57:44 visual_prompt]: 	Training 500/1106. train loss: 0.7264,	0.6333 s / batch. (data: 9.72e-04). ETA=16:50:21, max mem: 15.9 GB 
[10/30 23:58:48 visual_prompt]: 	Training 600/1106. train loss: 0.5788,	0.6405 s / batch. (data: 8.68e-04). ETA=17:00:45, max mem: 15.9 GB 
[10/30 23:59:51 visual_prompt]: 	Training 700/1106. train loss: 0.3693,	0.6320 s / batch. (data: 7.88e-04). ETA=16:46:06, max mem: 15.9 GB 
[10/31 00:00:54 visual_prompt]: 	Training 800/1106. train loss: 0.8148,	0.6331 s / batch. (data: 7.81e-04). ETA=16:46:51, max mem: 15.9 GB 
[10/31 00:01:57 visual_prompt]: 	Training 900/1106. train loss: 1.0012,	0.6311 s / batch. (data: 7.92e-04). ETA=16:42:41, max mem: 15.9 GB 
[10/31 00:03:00 visual_prompt]: 	Training 1000/1106. train loss: 0.1823,	0.6360 s / batch. (data: 7.67e-04). ETA=16:49:24, max mem: 15.9 GB 
[10/31 00:04:03 visual_prompt]: 	Training 1100/1106. train loss: 0.7471,	0.6180 s / batch. (data: 1.45e-04). ETA=16:19:49, max mem: 15.9 GB 
[10/31 00:04:07 visual_prompt]: Epoch 14 / 100: avg data time: 3.99e-03, avg batch time: 0.6338, average train loss: 0.8564
[10/31 00:04:56 visual_prompt]: 	Test 100/123. loss: 0.659, 0.2397 s / batch. (data: 2.31e-05)max mem: 15.94594 GB 
[10/31 00:05:07 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.2327, average loss: 0.7059
[10/31 00:05:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 60.72	
[10/31 00:05:07 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/31 00:06:12 visual_prompt]: 	Training 100/1106. train loss: 1.2887,	0.6381 s / batch. (data: 2.83e-04). ETA=16:50:27, max mem: 15.9 GB 
[10/31 00:07:15 visual_prompt]: 	Training 200/1106. train loss: 0.9216,	0.6231 s / batch. (data: 4.39e-04). ETA=16:25:44, max mem: 15.9 GB 
[10/31 00:08:18 visual_prompt]: 	Training 300/1106. train loss: 0.9298,	0.6349 s / batch. (data: 9.15e-03). ETA=16:43:22, max mem: 15.9 GB 
[10/31 00:09:22 visual_prompt]: 	Training 400/1106. train loss: 2.3810,	0.6338 s / batch. (data: 3.12e-04). ETA=16:40:29, max mem: 15.9 GB 
[10/31 00:10:25 visual_prompt]: 	Training 500/1106. train loss: 0.4307,	0.6684 s / batch. (data: 3.82e-02). ETA=17:34:05, max mem: 15.9 GB 
[10/31 00:11:28 visual_prompt]: 	Training 600/1106. train loss: 1.2293,	0.6187 s / batch. (data: 3.09e-04). ETA=16:14:33, max mem: 15.9 GB 
[10/31 00:12:31 visual_prompt]: 	Training 700/1106. train loss: 0.4398,	0.6293 s / batch. (data: 1.06e-02). ETA=16:30:13, max mem: 15.9 GB 
[10/31 00:13:34 visual_prompt]: 	Training 800/1106. train loss: 0.5383,	0.6400 s / batch. (data: 7.80e-04). ETA=16:46:00, max mem: 15.9 GB 
[10/31 00:14:38 visual_prompt]: 	Training 900/1106. train loss: 0.4963,	0.6178 s / batch. (data: 3.48e-04). ETA=16:10:07, max mem: 15.9 GB 
[10/31 00:15:41 visual_prompt]: 	Training 1000/1106. train loss: 0.8549,	0.6332 s / batch. (data: 7.64e-04). ETA=16:33:16, max mem: 15.9 GB 
[10/31 00:16:44 visual_prompt]: 	Training 1100/1106. train loss: 1.6767,	0.6181 s / batch. (data: 1.50e-04). ETA=16:08:34, max mem: 15.9 GB 
[10/31 00:16:48 visual_prompt]: Epoch 15 / 100: avg data time: 4.10e-03, avg batch time: 0.6336, average train loss: 0.9188
[10/31 00:17:38 visual_prompt]: 	Test 100/123. loss: 0.660, 0.2248 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/31 00:17:48 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.2321, average loss: 0.6781
[10/31 00:17:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 60.70	
[10/31 00:17:48 visual_prompt]: Best epoch 15: best metric: -0.678
[10/31 00:17:48 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/31 00:18:53 visual_prompt]: 	Training 100/1106. train loss: 0.4630,	0.6169 s / batch. (data: 3.16e-04). ETA=16:05:32, max mem: 15.9 GB 
[10/31 00:19:56 visual_prompt]: 	Training 200/1106. train loss: 0.7618,	0.6606 s / batch. (data: 2.91e-02). ETA=17:12:53, max mem: 15.9 GB 
[10/31 00:21:00 visual_prompt]: 	Training 300/1106. train loss: 0.3432,	0.6315 s / batch. (data: 7.38e-04). ETA=16:26:18, max mem: 15.9 GB 
[10/31 00:22:03 visual_prompt]: 	Training 400/1106. train loss: 0.8702,	0.6313 s / batch. (data: 8.22e-04). ETA=16:24:53, max mem: 15.9 GB 
[10/31 00:23:06 visual_prompt]: 	Training 500/1106. train loss: 0.8594,	0.6403 s / batch. (data: 3.80e-04). ETA=16:37:54, max mem: 15.9 GB 
[10/31 00:24:09 visual_prompt]: 	Training 600/1106. train loss: 1.6710,	0.6320 s / batch. (data: 1.15e-03). ETA=16:23:57, max mem: 15.9 GB 
[10/31 00:25:13 visual_prompt]: 	Training 700/1106. train loss: 1.2883,	0.6186 s / batch. (data: 2.80e-04). ETA=16:01:59, max mem: 15.9 GB 
[10/31 00:26:16 visual_prompt]: 	Training 800/1106. train loss: 1.2048,	0.6230 s / batch. (data: 3.63e-04). ETA=16:07:49, max mem: 15.9 GB 
[10/31 00:27:19 visual_prompt]: 	Training 900/1106. train loss: 0.5415,	0.6352 s / batch. (data: 8.14e-04). ETA=16:25:44, max mem: 15.9 GB 
[10/31 00:28:23 visual_prompt]: 	Training 1000/1106. train loss: 0.5593,	0.6284 s / batch. (data: 8.10e-04). ETA=16:14:10, max mem: 15.9 GB 
[10/31 00:29:26 visual_prompt]: 	Training 1100/1106. train loss: 0.2551,	0.6189 s / batch. (data: 1.46e-04). ETA=15:58:25, max mem: 15.9 GB 
[10/31 00:29:30 visual_prompt]: Epoch 16 / 100: avg data time: 3.90e-03, avg batch time: 0.6345, average train loss: 0.8367
[10/31 00:30:19 visual_prompt]: 	Test 100/123. loss: 0.656, 0.2355 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/31 00:30:30 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.2330, average loss: 0.6679
[10/31 00:30:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 61.23	
[10/31 00:30:30 visual_prompt]: Best epoch 16: best metric: -0.668
[10/31 00:30:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/31 00:31:35 visual_prompt]: 	Training 100/1106. train loss: 1.6550,	0.6220 s / batch. (data: 2.88e-04). ETA=16:02:02, max mem: 15.9 GB 
[10/31 00:32:39 visual_prompt]: 	Training 200/1106. train loss: 0.4927,	0.6302 s / batch. (data: 3.28e-04). ETA=16:13:39, max mem: 15.9 GB 
[10/31 00:33:42 visual_prompt]: 	Training 300/1106. train loss: 1.0356,	0.6292 s / batch. (data: 1.20e-02). ETA=16:11:09, max mem: 15.9 GB 
[10/31 00:34:45 visual_prompt]: 	Training 400/1106. train loss: 3.3631,	0.6435 s / batch. (data: 7.56e-04). ETA=16:32:02, max mem: 15.9 GB 
[10/31 00:35:48 visual_prompt]: 	Training 500/1106. train loss: 0.0786,	0.6318 s / batch. (data: 8.41e-04). ETA=16:12:58, max mem: 15.9 GB 
[10/31 00:36:52 visual_prompt]: 	Training 600/1106. train loss: 1.0833,	0.6280 s / batch. (data: 2.63e-04). ETA=16:06:09, max mem: 15.9 GB 
[10/31 00:37:55 visual_prompt]: 	Training 700/1106. train loss: 0.8057,	0.6463 s / batch. (data: 7.66e-04). ETA=16:33:08, max mem: 15.9 GB 
[10/31 00:38:58 visual_prompt]: 	Training 800/1106. train loss: 0.7279,	0.6319 s / batch. (data: 7.80e-04). ETA=16:09:59, max mem: 15.9 GB 
[10/31 00:40:02 visual_prompt]: 	Training 900/1106. train loss: 2.0594,	0.6439 s / batch. (data: 7.80e-04). ETA=16:27:19, max mem: 15.9 GB 
[10/31 00:41:05 visual_prompt]: 	Training 1000/1106. train loss: 0.0369,	0.6310 s / batch. (data: 7.65e-04). ETA=16:06:29, max mem: 15.9 GB 
[10/31 00:42:08 visual_prompt]: 	Training 1100/1106. train loss: 0.2138,	0.6186 s / batch. (data: 1.41e-04). ETA=15:46:26, max mem: 15.9 GB 
[10/31 00:42:12 visual_prompt]: Epoch 17 / 100: avg data time: 3.56e-03, avg batch time: 0.6346, average train loss: 0.8726
[10/31 00:43:02 visual_prompt]: 	Test 100/123. loss: 0.684, 0.2250 s / batch. (data: 2.50e-05)max mem: 15.94594 GB 
[10/31 00:43:12 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.2330, average loss: 0.6888
[10/31 00:43:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 60.57	
[10/31 00:43:12 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/31 00:44:18 visual_prompt]: 	Training 100/1106. train loss: 0.3419,	0.6318 s / batch. (data: 7.58e-04). ETA=16:05:31, max mem: 15.9 GB 
[10/31 00:45:21 visual_prompt]: 	Training 200/1106. train loss: 1.3434,	0.6182 s / batch. (data: 3.05e-04). ETA=15:43:48, max mem: 15.9 GB 
[10/31 00:46:24 visual_prompt]: 	Training 300/1106. train loss: 2.1430,	0.6327 s / batch. (data: 3.29e-04). ETA=16:04:46, max mem: 15.9 GB 
[10/31 00:47:27 visual_prompt]: 	Training 400/1106. train loss: 0.6366,	0.6396 s / batch. (data: 5.84e-03). ETA=16:14:17, max mem: 15.9 GB 
[10/31 00:48:31 visual_prompt]: 	Training 500/1106. train loss: 0.2622,	0.6441 s / batch. (data: 1.20e-02). ETA=16:20:01, max mem: 15.9 GB 
[10/31 00:49:34 visual_prompt]: 	Training 600/1106. train loss: 0.4932,	0.6312 s / batch. (data: 3.86e-04). ETA=15:59:20, max mem: 15.9 GB 
[10/31 00:50:37 visual_prompt]: 	Training 700/1106. train loss: 0.6587,	0.6281 s / batch. (data: 2.10e-04). ETA=15:53:34, max mem: 15.9 GB 
[10/31 00:51:40 visual_prompt]: 	Training 800/1106. train loss: 0.5178,	0.6190 s / batch. (data: 3.24e-04). ETA=15:38:45, max mem: 15.9 GB 
[10/31 00:52:43 visual_prompt]: 	Training 900/1106. train loss: 0.9802,	0.6348 s / batch. (data: 7.13e-04). ETA=16:01:39, max mem: 15.9 GB 
[10/31 00:53:47 visual_prompt]: 	Training 1000/1106. train loss: 0.8745,	0.6425 s / batch. (data: 8.02e-04). ETA=16:12:13, max mem: 15.9 GB 
[10/31 00:54:50 visual_prompt]: 	Training 1100/1106. train loss: 0.7686,	0.6179 s / batch. (data: 2.35e-04). ETA=15:34:03, max mem: 15.9 GB 
[10/31 00:54:53 visual_prompt]: Epoch 18 / 100: avg data time: 4.29e-03, avg batch time: 0.6340, average train loss: 0.8586
[10/31 00:55:43 visual_prompt]: 	Test 100/123. loss: 0.698, 0.2259 s / batch. (data: 2.55e-05)max mem: 15.94594 GB 
[10/31 00:55:53 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.2330, average loss: 0.7412
[10/31 00:55:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 61.07	
[10/31 00:55:53 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/31 00:56:59 visual_prompt]: 	Training 100/1106. train loss: 0.3924,	0.6302 s / batch. (data: 8.03e-04). ETA=15:51:30, max mem: 15.9 GB 
[10/31 00:58:02 visual_prompt]: 	Training 200/1106. train loss: 0.9411,	0.6433 s / batch. (data: 7.49e-04). ETA=16:10:14, max mem: 15.9 GB 
[10/31 00:59:06 visual_prompt]: 	Training 300/1106. train loss: 0.3754,	0.6474 s / batch. (data: 3.06e-04). ETA=16:15:20, max mem: 15.9 GB 
[10/31 01:00:09 visual_prompt]: 	Training 400/1106. train loss: 0.7465,	0.6361 s / batch. (data: 7.47e-04). ETA=15:57:12, max mem: 15.9 GB 
[10/31 01:01:12 visual_prompt]: 	Training 500/1106. train loss: 0.7220,	0.6179 s / batch. (data: 3.42e-04). ETA=15:28:52, max mem: 15.9 GB 
[10/31 01:02:16 visual_prompt]: 	Training 600/1106. train loss: 1.0315,	0.6442 s / batch. (data: 7.88e-04). ETA=16:07:20, max mem: 15.9 GB 
[10/31 01:03:19 visual_prompt]: 	Training 700/1106. train loss: 0.6385,	0.6305 s / batch. (data: 7.93e-04). ETA=15:45:43, max mem: 15.9 GB 
[10/31 01:04:22 visual_prompt]: 	Training 800/1106. train loss: 0.2465,	0.6363 s / batch. (data: 7.54e-04). ETA=15:53:19, max mem: 15.9 GB 
[10/31 01:05:25 visual_prompt]: 	Training 900/1106. train loss: 0.3604,	0.6184 s / batch. (data: 3.15e-04). ETA=15:25:26, max mem: 15.9 GB 
[10/31 01:06:28 visual_prompt]: 	Training 1000/1106. train loss: 0.5316,	0.6342 s / batch. (data: 7.19e-04). ETA=15:48:02, max mem: 15.9 GB 
[10/31 01:07:32 visual_prompt]: 	Training 1100/1106. train loss: 1.1079,	0.6189 s / batch. (data: 1.68e-04). ETA=15:24:10, max mem: 15.9 GB 
[10/31 01:07:36 visual_prompt]: Epoch 19 / 100: avg data time: 4.02e-03, avg batch time: 0.6349, average train loss: 0.7975
[10/31 01:08:25 visual_prompt]: 	Test 100/123. loss: 1.330, 0.2319 s / batch. (data: 2.15e-05)max mem: 15.94594 GB 
[10/31 01:08:36 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.2322, average loss: 1.3301
[10/31 01:08:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.62	
[10/31 01:08:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/31 01:09:42 visual_prompt]: 	Training 100/1106. train loss: 0.7506,	0.6334 s / batch. (data: 7.66e-04). ETA=15:44:37, max mem: 15.9 GB 
[10/31 01:10:45 visual_prompt]: 	Training 200/1106. train loss: 0.5610,	0.6438 s / batch. (data: 2.52e-02). ETA=15:59:03, max mem: 15.9 GB 
[10/31 01:11:48 visual_prompt]: 	Training 300/1106. train loss: 0.9327,	0.6458 s / batch. (data: 7.10e-04). ETA=16:01:00, max mem: 15.9 GB 
[10/31 01:12:52 visual_prompt]: 	Training 400/1106. train loss: 0.5452,	0.6758 s / batch. (data: 3.43e-02). ETA=16:44:34, max mem: 15.9 GB 
[10/31 01:13:55 visual_prompt]: 	Training 500/1106. train loss: 0.4352,	0.6200 s / batch. (data: 2.70e-04). ETA=15:20:33, max mem: 15.9 GB 
[10/31 01:14:58 visual_prompt]: 	Training 600/1106. train loss: 0.8925,	0.6202 s / batch. (data: 3.68e-04). ETA=15:19:46, max mem: 15.9 GB 
[10/31 01:16:01 visual_prompt]: 	Training 700/1106. train loss: 0.3893,	0.6209 s / batch. (data: 3.29e-04). ETA=15:19:46, max mem: 15.9 GB 
[10/31 01:17:05 visual_prompt]: 	Training 800/1106. train loss: 0.6823,	0.6184 s / batch. (data: 2.80e-04). ETA=15:15:08, max mem: 15.9 GB 
[10/31 01:18:08 visual_prompt]: 	Training 900/1106. train loss: 0.3693,	0.6234 s / batch. (data: 2.83e-04). ETA=15:21:25, max mem: 15.9 GB 
[10/31 01:19:11 visual_prompt]: 	Training 1000/1106. train loss: 0.4770,	0.6520 s / batch. (data: 7.72e-04). ETA=16:02:37, max mem: 15.9 GB 
[10/31 01:20:14 visual_prompt]: 	Training 1100/1106. train loss: 0.2225,	0.6191 s / batch. (data: 1.42e-04). ETA=15:13:06, max mem: 15.9 GB 
[10/31 01:20:18 visual_prompt]: Epoch 20 / 100: avg data time: 4.59e-03, avg batch time: 0.6351, average train loss: 0.8105
[10/31 01:21:08 visual_prompt]: 	Test 100/123. loss: 0.941, 0.2436 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/31 01:21:18 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.2320, average loss: 1.0073
[10/31 01:21:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.16	
[10/31 01:21:18 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/31 01:22:24 visual_prompt]: 	Training 100/1106. train loss: 0.5460,	0.6470 s / batch. (data: 1.09e-02). ETA=15:53:01, max mem: 15.9 GB 
[10/31 01:23:27 visual_prompt]: 	Training 200/1106. train loss: 1.1963,	0.6455 s / batch. (data: 5.92e-03). ETA=15:49:45, max mem: 15.9 GB 
[10/31 01:24:31 visual_prompt]: 	Training 300/1106. train loss: 0.9199,	0.6310 s / batch. (data: 1.41e-02). ETA=15:27:22, max mem: 15.9 GB 
[10/31 01:25:34 visual_prompt]: 	Training 400/1106. train loss: 0.8157,	0.6329 s / batch. (data: 1.47e-02). ETA=15:29:08, max mem: 15.9 GB 
[10/31 01:26:37 visual_prompt]: 	Training 500/1106. train loss: 1.9483,	0.6304 s / batch. (data: 3.21e-04). ETA=15:24:18, max mem: 15.9 GB 
[10/31 01:27:40 visual_prompt]: 	Training 600/1106. train loss: 2.6483,	0.6322 s / batch. (data: 9.84e-04). ETA=15:25:56, max mem: 15.9 GB 
[10/31 01:28:43 visual_prompt]: 	Training 700/1106. train loss: 1.0271,	0.6509 s / batch. (data: 7.84e-04). ETA=15:52:19, max mem: 15.9 GB 
[10/31 01:29:46 visual_prompt]: 	Training 800/1106. train loss: 0.7841,	0.6380 s / batch. (data: 7.77e-04). ETA=15:32:19, max mem: 15.9 GB 
[10/31 01:30:50 visual_prompt]: 	Training 900/1106. train loss: 0.0627,	0.6401 s / batch. (data: 3.14e-04). ETA=15:34:21, max mem: 15.9 GB 
[10/31 01:31:53 visual_prompt]: 	Training 1000/1106. train loss: 0.6198,	0.6302 s / batch. (data: 3.52e-04). ETA=15:18:49, max mem: 15.9 GB 
[10/31 01:32:56 visual_prompt]: 	Training 1100/1106. train loss: 0.8494,	0.6317 s / batch. (data: 1.24e-02). ETA=15:19:53, max mem: 15.9 GB 
[10/31 01:33:00 visual_prompt]: Epoch 21 / 100: avg data time: 4.46e-03, avg batch time: 0.6339, average train loss: 0.8008
[10/31 01:33:53 visual_prompt]: 	Test 100/123. loss: 0.628, 0.2260 s / batch. (data: 3.98e-05)max mem: 15.94594 GB 
[10/31 01:34:04 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.2307, average loss: 0.6814
[10/31 01:34:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 62.86	
[10/31 01:34:04 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/31 01:35:10 visual_prompt]: 	Training 100/1106. train loss: 0.9987,	0.6246 s / batch. (data: 3.24e-04). ETA=15:08:29, max mem: 15.9 GB 
[10/31 01:36:13 visual_prompt]: 	Training 200/1106. train loss: 0.5625,	0.6362 s / batch. (data: 8.06e-04). ETA=15:24:16, max mem: 15.9 GB 
[10/31 01:37:17 visual_prompt]: 	Training 300/1106. train loss: 0.8499,	0.6388 s / batch. (data: 5.92e-03). ETA=15:27:03, max mem: 15.9 GB 
[10/31 01:38:20 visual_prompt]: 	Training 400/1106. train loss: 0.3374,	0.6317 s / batch. (data: 7.97e-04). ETA=15:15:43, max mem: 15.9 GB 
[10/31 01:39:23 visual_prompt]: 	Training 500/1106. train loss: 0.2161,	0.6314 s / batch. (data: 8.75e-04). ETA=15:14:10, max mem: 15.9 GB 
[10/31 01:40:27 visual_prompt]: 	Training 600/1106. train loss: 0.1487,	0.6192 s / batch. (data: 3.22e-04). ETA=14:55:30, max mem: 15.9 GB 
[10/31 01:41:30 visual_prompt]: 	Training 700/1106. train loss: 1.0199,	0.6207 s / batch. (data: 1.07e-03). ETA=14:56:40, max mem: 15.9 GB 
[10/31 01:42:33 visual_prompt]: 	Training 800/1106. train loss: 0.8568,	0.6332 s / batch. (data: 8.00e-04). ETA=15:13:42, max mem: 15.9 GB 
[10/31 01:43:37 visual_prompt]: 	Training 900/1106. train loss: 0.9090,	0.6206 s / batch. (data: 3.50e-04). ETA=14:54:29, max mem: 15.9 GB 
[10/31 01:44:40 visual_prompt]: 	Training 1000/1106. train loss: 0.9826,	0.6239 s / batch. (data: 8.09e-04). ETA=14:58:09, max mem: 15.9 GB 
[10/31 01:45:44 visual_prompt]: 	Training 1100/1106. train loss: 0.8829,	0.6183 s / batch. (data: 2.13e-04). ETA=14:48:59, max mem: 15.9 GB 
[10/31 01:45:47 visual_prompt]: Epoch 22 / 100: avg data time: 5.58e-03, avg batch time: 0.6356, average train loss: 0.7715
[10/31 01:46:40 visual_prompt]: 	Test 100/123. loss: 0.677, 0.2263 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[10/31 01:46:52 visual_prompt]: Inference (val):avg data time: 1.11e-04, avg batch time: 0.2314, average loss: 0.7511
[10/31 01:46:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 61.51	
[10/31 01:46:52 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/31 01:47:58 visual_prompt]: 	Training 100/1106. train loss: 0.2235,	0.6377 s / batch. (data: 2.96e-04). ETA=15:15:48, max mem: 15.9 GB 
[10/31 01:49:01 visual_prompt]: 	Training 200/1106. train loss: 0.9333,	0.6255 s / batch. (data: 3.20e-04). ETA=14:57:17, max mem: 15.9 GB 
[10/31 01:50:05 visual_prompt]: 	Training 300/1106. train loss: 0.5403,	0.6326 s / batch. (data: 1.34e-02). ETA=15:06:27, max mem: 15.9 GB 
[10/31 01:51:08 visual_prompt]: 	Training 400/1106. train loss: 0.8287,	0.6310 s / batch. (data: 8.22e-04). ETA=15:02:59, max mem: 15.9 GB 
[10/31 01:52:12 visual_prompt]: 	Training 500/1106. train loss: 0.7827,	0.6482 s / batch. (data: 3.42e-04). ETA=15:26:37, max mem: 15.9 GB 
[10/31 01:53:15 visual_prompt]: 	Training 600/1106. train loss: 0.8098,	0.6285 s / batch. (data: 3.31e-04). ETA=14:57:25, max mem: 15.9 GB 
[10/31 01:54:18 visual_prompt]: 	Training 700/1106. train loss: 0.5954,	0.6189 s / batch. (data: 3.33e-04). ETA=14:42:37, max mem: 15.9 GB 
[10/31 01:55:22 visual_prompt]: 	Training 800/1106. train loss: 0.9031,	0.6329 s / batch. (data: 7.22e-04). ETA=15:01:36, max mem: 15.9 GB 
[10/31 01:56:25 visual_prompt]: 	Training 900/1106. train loss: 0.7329,	0.6326 s / batch. (data: 6.56e-04). ETA=15:00:00, max mem: 15.9 GB 
[10/31 01:57:28 visual_prompt]: 	Training 1000/1106. train loss: 0.4097,	0.6346 s / batch. (data: 8.25e-04). ETA=15:01:51, max mem: 15.9 GB 
[10/31 01:58:31 visual_prompt]: 	Training 1100/1106. train loss: 0.5810,	0.6180 s / batch. (data: 1.17e-04). ETA=14:37:15, max mem: 15.9 GB 
[10/31 01:58:35 visual_prompt]: Epoch 23 / 100: avg data time: 5.57e-03, avg batch time: 0.6360, average train loss: 0.7852
[10/31 01:59:26 visual_prompt]: 	Test 100/123. loss: 0.734, 0.2258 s / batch. (data: 3.98e-05)max mem: 15.94594 GB 
[10/31 01:59:38 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.2328, average loss: 0.6939
[10/31 01:59:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 61.50	
[10/31 01:59:38 visual_prompt]: Stopping early.
[10/31 01:59:38 visual_prompt]: Rank of current process: 0. World size: 1
[10/31 01:59:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/31 01:59:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/31 01:59:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/31 01:59:38 visual_prompt]: Training with config:
[10/31 01:59:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.05_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/31 01:59:38 visual_prompt]: Loading training data...
[10/31 01:59:38 visual_prompt]: Constructing mammo-cbis dataset train...
[10/31 01:59:38 visual_prompt]: Loading validation data...
[10/31 01:59:38 visual_prompt]: Constructing mammo-cbis dataset val...
[10/31 01:59:38 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/31 01:59:45 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/31 01:59:45 visual_prompt]: tuned percent:0.522
[10/31 01:59:45 visual_prompt]: Device used for model: 0
[10/31 01:59:45 visual_prompt]: Setting up Evaluator...
[10/31 01:59:45 visual_prompt]: Setting up Trainer...
[10/31 01:59:45 visual_prompt]: 	Setting up the optimizer...
[10/31 01:59:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/31 02:00:51 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6419 s / batch. (data: 8.20e-04). ETA=19:42:10, max mem: 15.9 GB 
[10/31 02:01:54 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6288 s / batch. (data: 2.80e-04). ETA=19:17:00, max mem: 15.9 GB 
[10/31 02:02:58 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6319 s / batch. (data: 3.81e-04). ETA=19:21:42, max mem: 15.9 GB 
[10/31 02:04:01 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6349 s / batch. (data: 3.42e-04). ETA=19:26:02, max mem: 15.9 GB 
[10/31 02:05:05 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6480 s / batch. (data: 7.96e-03). ETA=19:49:06, max mem: 15.9 GB 
[10/31 02:06:09 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6369 s / batch. (data: 3.16e-04). ETA=19:27:37, max mem: 15.9 GB 
[10/31 02:07:12 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6184 s / batch. (data: 3.57e-04). ETA=18:52:42, max mem: 15.9 GB 
[10/31 02:08:15 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6460 s / batch. (data: 1.05e-02). ETA=19:42:05, max mem: 15.9 GB 
[10/31 02:09:19 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6520 s / batch. (data: 7.94e-04). ETA=19:52:04, max mem: 15.9 GB 
[10/31 02:10:23 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6200 s / batch. (data: 3.71e-04). ETA=18:52:29, max mem: 15.9 GB 
[10/31 02:11:26 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6178 s / batch. (data: 1.69e-04). ETA=18:47:30, max mem: 15.9 GB 
[10/31 02:11:30 visual_prompt]: Epoch 1 / 100: avg data time: 5.21e-03, avg batch time: 0.6375, average train loss: 1.4028
[10/31 02:12:21 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2254 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/31 02:12:32 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.2324, average loss: 1.3505
[10/31 02:12:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/31 02:12:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/31 02:13:37 visual_prompt]: 	Training 100/1106. train loss: 0.6523,	0.6177 s / batch. (data: 3.33e-04). ETA=18:46:18, max mem: 15.9 GB 
[10/31 02:14:41 visual_prompt]: 	Training 200/1106. train loss: 0.7116,	0.6184 s / batch. (data: 4.60e-04). ETA=18:46:22, max mem: 15.9 GB 
[10/31 02:15:45 visual_prompt]: 	Training 300/1106. train loss: 1.0351,	0.6518 s / batch. (data: 7.97e-03). ETA=19:46:11, max mem: 15.9 GB 
[10/31 02:16:48 visual_prompt]: 	Training 400/1106. train loss: 0.1211,	0.6192 s / batch. (data: 3.13e-04). ETA=18:45:53, max mem: 15.9 GB 
[10/31 02:17:51 visual_prompt]: 	Training 500/1106. train loss: 0.7674,	0.6347 s / batch. (data: 3.18e-04). ETA=19:12:53, max mem: 15.9 GB 
[10/31 02:18:55 visual_prompt]: 	Training 600/1106. train loss: 0.5919,	0.6320 s / batch. (data: 1.35e-03). ETA=19:06:59, max mem: 15.9 GB 
[10/31 02:19:58 visual_prompt]: 	Training 700/1106. train loss: 0.7056,	0.6525 s / batch. (data: 1.10e-02). ETA=19:43:10, max mem: 15.9 GB 
[10/31 02:21:02 visual_prompt]: 	Training 800/1106. train loss: 0.7657,	0.6600 s / batch. (data: 7.77e-04). ETA=19:55:40, max mem: 15.9 GB 
[10/31 02:22:05 visual_prompt]: 	Training 900/1106. train loss: 0.6866,	0.6187 s / batch. (data: 3.45e-04). ETA=18:39:51, max mem: 15.9 GB 
[10/31 02:23:09 visual_prompt]: 	Training 1000/1106. train loss: 0.5107,	0.6374 s / batch. (data: 1.33e-02). ETA=19:12:33, max mem: 15.9 GB 
[10/31 02:24:12 visual_prompt]: 	Training 1100/1106. train loss: 0.7694,	0.6191 s / batch. (data: 1.89e-04). ETA=18:38:31, max mem: 15.9 GB 
[10/31 02:24:16 visual_prompt]: Epoch 2 / 100: avg data time: 4.21e-03, avg batch time: 0.6360, average train loss: 0.7646
[10/31 02:25:07 visual_prompt]: 	Test 100/123. loss: 0.925, 0.2439 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/31 02:25:18 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.2316, average loss: 0.8363
[10/31 02:25:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.96	
[10/31 02:25:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/31 02:26:25 visual_prompt]: 	Training 100/1106. train loss: 0.4507,	0.6415 s / batch. (data: 3.18e-04). ETA=19:17:42, max mem: 15.9 GB 
[10/31 02:27:28 visual_prompt]: 	Training 200/1106. train loss: 0.4160,	0.6298 s / batch. (data: 7.71e-04). ETA=18:55:35, max mem: 15.9 GB 
[10/31 02:28:31 visual_prompt]: 	Training 300/1106. train loss: 0.3526,	0.6299 s / batch. (data: 3.14e-04). ETA=18:54:46, max mem: 15.9 GB 
[10/31 02:29:35 visual_prompt]: 	Training 400/1106. train loss: 0.7036,	0.6194 s / batch. (data: 6.82e-04). ETA=18:34:45, max mem: 15.9 GB 
[10/31 02:30:39 visual_prompt]: 	Training 500/1106. train loss: 1.3721,	0.6182 s / batch. (data: 3.68e-04). ETA=18:31:39, max mem: 15.9 GB 
[10/31 02:31:42 visual_prompt]: 	Training 600/1106. train loss: 0.6174,	0.6400 s / batch. (data: 1.01e-03). ETA=19:09:43, max mem: 15.9 GB 
[10/31 02:32:46 visual_prompt]: 	Training 700/1106. train loss: 0.9933,	0.6370 s / batch. (data: 8.38e-04). ETA=19:03:21, max mem: 15.9 GB 
[10/31 02:33:50 visual_prompt]: 	Training 800/1106. train loss: 0.9978,	0.6529 s / batch. (data: 6.02e-03). ETA=19:30:40, max mem: 15.9 GB 
[10/31 02:34:54 visual_prompt]: 	Training 900/1106. train loss: 1.0081,	0.6319 s / batch. (data: 5.50e-03). ETA=18:52:07, max mem: 15.9 GB 
[10/31 02:35:58 visual_prompt]: 	Training 1000/1106. train loss: 0.6895,	0.6313 s / batch. (data: 1.06e-02). ETA=18:49:49, max mem: 15.9 GB 
[10/31 02:37:02 visual_prompt]: 	Training 1100/1106. train loss: 0.6682,	0.6190 s / batch. (data: 2.23e-04). ETA=18:26:54, max mem: 15.9 GB 
[10/31 02:37:05 visual_prompt]: Epoch 3 / 100: avg data time: 6.38e-03, avg batch time: 0.6394, average train loss: 0.7501
[10/31 02:38:00 visual_prompt]: 	Test 100/123. loss: 0.718, 0.2290 s / batch. (data: 5.01e-05)max mem: 15.94594 GB 
[10/31 02:38:11 visual_prompt]: Inference (val):avg data time: 4.71e-05, avg batch time: 0.2324, average loss: 0.7298
[10/31 02:38:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.46	
[10/31 02:38:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/31 02:39:17 visual_prompt]: 	Training 100/1106. train loss: 0.9940,	0.6584 s / batch. (data: 6.05e-03). ETA=19:36:07, max mem: 15.9 GB 
[10/31 02:40:21 visual_prompt]: 	Training 200/1106. train loss: 0.9807,	0.6201 s / batch. (data: 3.47e-04). ETA=18:26:38, max mem: 15.9 GB 
[10/31 02:41:25 visual_prompt]: 	Training 300/1106. train loss: 0.7030,	0.6311 s / batch. (data: 8.74e-04). ETA=18:45:13, max mem: 15.9 GB 
[10/31 02:42:29 visual_prompt]: 	Training 400/1106. train loss: 0.7064,	0.6561 s / batch. (data: 5.52e-03). ETA=19:28:41, max mem: 15.9 GB 
[10/31 02:43:32 visual_prompt]: 	Training 500/1106. train loss: 1.9739,	0.6437 s / batch. (data: 5.52e-03). ETA=19:05:40, max mem: 15.9 GB 
[10/31 02:44:36 visual_prompt]: 	Training 600/1106. train loss: 0.3928,	0.6315 s / batch. (data: 3.76e-04). ETA=18:42:48, max mem: 15.9 GB 
[10/31 02:45:40 visual_prompt]: 	Training 700/1106. train loss: 0.7212,	0.6401 s / batch. (data: 3.61e-04). ETA=18:56:58, max mem: 15.9 GB 
[10/31 02:46:44 visual_prompt]: 	Training 800/1106. train loss: 0.5407,	0.6376 s / batch. (data: 8.45e-04). ETA=18:51:29, max mem: 15.9 GB 
[10/31 02:47:47 visual_prompt]: 	Training 900/1106. train loss: 0.7805,	0.6297 s / batch. (data: 5.54e-03). ETA=18:36:25, max mem: 15.9 GB 
[10/31 02:48:51 visual_prompt]: 	Training 1000/1106. train loss: 0.5178,	0.6280 s / batch. (data: 3.60e-04). ETA=18:32:25, max mem: 15.9 GB 
[10/31 02:49:55 visual_prompt]: 	Training 1100/1106. train loss: 0.7179,	0.6180 s / batch. (data: 2.08e-04). ETA=18:13:35, max mem: 15.9 GB 
[10/31 02:49:59 visual_prompt]: Epoch 4 / 100: avg data time: 6.21e-03, avg batch time: 0.6397, average train loss: 0.7368
[10/31 02:50:53 visual_prompt]: 	Test 100/123. loss: 0.734, 0.2397 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/31 02:51:05 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.2337, average loss: 0.7058
[10/31 02:51:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.90	
[10/31 02:51:05 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/31 02:52:11 visual_prompt]: 	Training 100/1106. train loss: 0.8390,	0.6252 s / batch. (data: 3.65e-04). ETA=18:25:18, max mem: 15.9 GB 
[10/31 02:53:14 visual_prompt]: 	Training 200/1106. train loss: 0.6035,	0.6488 s / batch. (data: 6.08e-03). ETA=19:05:56, max mem: 15.9 GB 
[10/31 02:54:18 visual_prompt]: 	Training 300/1106. train loss: 0.7103,	0.6185 s / batch. (data: 3.78e-04). ETA=18:11:29, max mem: 15.9 GB 
[10/31 02:55:22 visual_prompt]: 	Training 400/1106. train loss: 0.8267,	0.6181 s / batch. (data: 3.24e-04). ETA=18:09:44, max mem: 15.9 GB 
[10/31 02:56:25 visual_prompt]: 	Training 500/1106. train loss: 0.2611,	0.6480 s / batch. (data: 8.81e-04). ETA=19:01:15, max mem: 15.9 GB 
[10/31 02:57:29 visual_prompt]: 	Training 600/1106. train loss: 1.3399,	0.6338 s / batch. (data: 8.97e-04). ETA=18:35:17, max mem: 15.9 GB 
[10/31 02:58:33 visual_prompt]: 	Training 700/1106. train loss: 0.6938,	0.6398 s / batch. (data: 8.81e-04). ETA=18:44:41, max mem: 15.9 GB 
[10/31 02:59:37 visual_prompt]: 	Training 800/1106. train loss: 0.6818,	0.6440 s / batch. (data: 5.53e-03). ETA=18:51:06, max mem: 15.9 GB 
[10/31 03:00:41 visual_prompt]: 	Training 900/1106. train loss: 0.7621,	0.6489 s / batch. (data: 1.11e-02). ETA=18:58:29, max mem: 15.9 GB 
[10/31 03:01:44 visual_prompt]: 	Training 1000/1106. train loss: 0.5427,	0.6353 s / batch. (data: 3.71e-04). ETA=18:33:33, max mem: 15.9 GB 
[10/31 03:02:48 visual_prompt]: 	Training 1100/1106. train loss: 0.8033,	0.6204 s / batch. (data: 2.05e-04). ETA=18:06:24, max mem: 15.9 GB 
[10/31 03:02:52 visual_prompt]: Epoch 5 / 100: avg data time: 5.80e-03, avg batch time: 0.6394, average train loss: 0.7323
[10/31 03:03:46 visual_prompt]: 	Test 100/123. loss: 0.703, 0.2337 s / batch. (data: 5.34e-05)max mem: 15.94594 GB 
[10/31 03:03:58 visual_prompt]: Inference (val):avg data time: 1.09e-04, avg batch time: 0.2336, average loss: 0.7147
[10/31 03:03:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.53	
[10/31 03:03:58 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/31 03:05:04 visual_prompt]: 	Training 100/1106. train loss: 0.6997,	0.6360 s / batch. (data: 3.67e-04). ETA=18:32:40, max mem: 15.9 GB 
[10/31 03:06:07 visual_prompt]: 	Training 200/1106. train loss: 0.6714,	0.6196 s / batch. (data: 3.64e-04). ETA=18:02:59, max mem: 15.9 GB 
[10/31 03:07:11 visual_prompt]: 	Training 300/1106. train loss: 0.1108,	0.6385 s / batch. (data: 1.11e-02). ETA=18:34:50, max mem: 15.9 GB 
[10/31 03:08:14 visual_prompt]: 	Training 400/1106. train loss: 0.6793,	0.6472 s / batch. (data: 8.70e-04). ETA=18:48:59, max mem: 15.9 GB 
[10/31 03:09:18 visual_prompt]: 	Training 500/1106. train loss: 0.8505,	0.6474 s / batch. (data: 3.80e-04). ETA=18:48:21, max mem: 15.9 GB 
[10/31 03:10:22 visual_prompt]: 	Training 600/1106. train loss: 0.3831,	0.6317 s / batch. (data: 3.50e-04). ETA=18:19:58, max mem: 15.9 GB 
[10/31 03:11:25 visual_prompt]: 	Training 700/1106. train loss: 0.7240,	0.6191 s / batch. (data: 3.29e-04). ETA=17:56:58, max mem: 15.9 GB 
[10/31 03:12:29 visual_prompt]: 	Training 800/1106. train loss: 0.7242,	0.6554 s / batch. (data: 8.64e-04). ETA=18:59:01, max mem: 15.9 GB 
[10/31 03:13:33 visual_prompt]: 	Training 900/1106. train loss: 0.3883,	0.6400 s / batch. (data: 3.60e-04). ETA=18:31:06, max mem: 15.9 GB 
[10/31 03:14:36 visual_prompt]: 	Training 1000/1106. train loss: 1.0914,	0.6461 s / batch. (data: 3.44e-04). ETA=18:40:39, max mem: 15.9 GB 
[10/31 03:15:40 visual_prompt]: 	Training 1100/1106. train loss: 0.4425,	0.6194 s / batch. (data: 2.14e-04). ETA=17:53:14, max mem: 15.9 GB 
[10/31 03:15:44 visual_prompt]: Epoch 6 / 100: avg data time: 5.50e-03, avg batch time: 0.6384, average train loss: 0.7379
[10/31 03:16:39 visual_prompt]: 	Test 100/123. loss: 0.738, 0.2397 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/31 03:16:50 visual_prompt]: Inference (val):avg data time: 4.85e-05, avg batch time: 0.2331, average loss: 0.7090
[10/31 03:16:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.60	
[10/31 03:16:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/31 03:17:55 visual_prompt]: 	Training 100/1106. train loss: 0.7057,	0.6388 s / batch. (data: 8.83e-04). ETA=18:25:45, max mem: 15.9 GB 
[10/31 03:18:59 visual_prompt]: 	Training 200/1106. train loss: 0.7947,	0.6509 s / batch. (data: 9.62e-04). ETA=18:45:44, max mem: 15.9 GB 
[10/31 03:20:03 visual_prompt]: 	Training 300/1106. train loss: 0.8750,	0.6367 s / batch. (data: 1.06e-02). ETA=18:20:01, max mem: 15.9 GB 
[10/31 03:21:06 visual_prompt]: 	Training 400/1106. train loss: 0.9372,	0.6331 s / batch. (data: 8.64e-04). ETA=18:12:49, max mem: 15.9 GB 
[10/31 03:22:10 visual_prompt]: 	Training 500/1106. train loss: 0.6926,	0.6520 s / batch. (data: 3.23e-04). ETA=18:44:21, max mem: 15.9 GB 
[10/31 03:23:14 visual_prompt]: 	Training 600/1106. train loss: 0.6954,	0.6380 s / batch. (data: 8.59e-04). ETA=18:19:01, max mem: 15.9 GB 
[10/31 03:24:18 visual_prompt]: 	Training 700/1106. train loss: 0.5114,	0.6320 s / batch. (data: 3.75e-04). ETA=18:07:42, max mem: 15.9 GB 
[10/31 03:25:22 visual_prompt]: 	Training 800/1106. train loss: 0.3253,	0.6191 s / batch. (data: 3.65e-04). ETA=17:44:24, max mem: 15.9 GB 
[10/31 03:26:25 visual_prompt]: 	Training 900/1106. train loss: 0.7455,	0.6313 s / batch. (data: 8.71e-04). ETA=18:04:22, max mem: 15.9 GB 
[10/31 03:27:29 visual_prompt]: 	Training 1000/1106. train loss: 0.9103,	0.6345 s / batch. (data: 1.06e-02). ETA=18:08:48, max mem: 15.9 GB 
[10/31 03:28:33 visual_prompt]: 	Training 1100/1106. train loss: 0.2726,	0.6177 s / batch. (data: 1.75e-04). ETA=17:39:03, max mem: 15.9 GB 
[10/31 03:28:36 visual_prompt]: Epoch 7 / 100: avg data time: 5.22e-03, avg batch time: 0.6387, average train loss: 0.7374
[10/31 03:29:31 visual_prompt]: 	Test 100/123. loss: 0.801, 0.2317 s / batch. (data: 3.55e-05)max mem: 15.94594 GB 
[10/31 03:29:42 visual_prompt]: Inference (val):avg data time: 2.69e-04, avg batch time: 0.2325, average loss: 0.7552
[10/31 03:29:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.42	
[10/31 03:29:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/31 03:30:49 visual_prompt]: 	Training 100/1106. train loss: 0.5986,	0.6337 s / batch. (data: 3.45e-04). ETA=18:05:20, max mem: 15.9 GB 
[10/31 03:31:52 visual_prompt]: 	Training 200/1106. train loss: 0.6958,	0.6302 s / batch. (data: 3.50e-04). ETA=17:58:14, max mem: 15.9 GB 
[10/31 03:32:56 visual_prompt]: 	Training 300/1106. train loss: 0.2883,	0.6535 s / batch. (data: 1.11e-02). ETA=18:37:03, max mem: 15.9 GB 
[10/31 03:34:00 visual_prompt]: 	Training 400/1106. train loss: 1.3607,	0.6188 s / batch. (data: 8.73e-04). ETA=17:36:44, max mem: 15.9 GB 
[10/31 03:35:04 visual_prompt]: 	Training 500/1106. train loss: 0.6148,	0.6232 s / batch. (data: 3.79e-04). ETA=17:43:08, max mem: 15.9 GB 
[10/31 03:36:07 visual_prompt]: 	Training 600/1106. train loss: 1.1252,	0.6360 s / batch. (data: 8.25e-04). ETA=18:04:00, max mem: 15.9 GB 
[10/31 03:37:11 visual_prompt]: 	Training 700/1106. train loss: 0.8054,	0.6316 s / batch. (data: 1.36e-02). ETA=17:55:19, max mem: 15.9 GB 
[10/31 03:38:15 visual_prompt]: 	Training 800/1106. train loss: 1.0246,	0.6440 s / batch. (data: 8.47e-04). ETA=18:15:26, max mem: 15.9 GB 
[10/31 03:39:18 visual_prompt]: 	Training 900/1106. train loss: 0.3411,	0.6376 s / batch. (data: 6.06e-03). ETA=18:03:30, max mem: 15.9 GB 
[10/31 03:40:22 visual_prompt]: 	Training 1000/1106. train loss: 1.1329,	0.6311 s / batch. (data: 3.96e-04). ETA=17:51:17, max mem: 15.9 GB 
[10/31 03:41:26 visual_prompt]: 	Training 1100/1106. train loss: 0.3906,	0.6180 s / batch. (data: 1.64e-04). ETA=17:28:07, max mem: 15.9 GB 
[10/31 03:41:30 visual_prompt]: Epoch 8 / 100: avg data time: 6.23e-03, avg batch time: 0.6393, average train loss: 0.7651
[10/31 03:42:24 visual_prompt]: 	Test 100/123. loss: 0.718, 0.2254 s / batch. (data: 4.20e-05)max mem: 15.94594 GB 
[10/31 03:42:35 visual_prompt]: Inference (val):avg data time: 4.70e-05, avg batch time: 0.2332, average loss: 0.7403
[10/31 03:42:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.29	
[10/31 03:42:35 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/31 03:43:42 visual_prompt]: 	Training 100/1106. train loss: 0.7591,	0.6445 s / batch. (data: 1.57e-02). ETA=18:11:59, max mem: 15.9 GB 
[10/31 03:44:46 visual_prompt]: 	Training 200/1106. train loss: 0.4532,	0.6285 s / batch. (data: 3.70e-04). ETA=17:43:48, max mem: 15.9 GB 
[10/31 03:45:49 visual_prompt]: 	Training 300/1106. train loss: 0.9180,	0.6186 s / batch. (data: 3.49e-04). ETA=17:25:54, max mem: 15.9 GB 
[10/31 03:46:53 visual_prompt]: 	Training 400/1106. train loss: 0.7625,	0.6360 s / batch. (data: 3.49e-04). ETA=17:54:18, max mem: 15.9 GB 
[10/31 03:47:56 visual_prompt]: 	Training 500/1106. train loss: 1.2788,	0.6494 s / batch. (data: 9.38e-04). ETA=18:15:54, max mem: 15.9 GB 
[10/31 03:49:00 visual_prompt]: 	Training 600/1106. train loss: 0.7024,	0.6600 s / batch. (data: 8.55e-04). ETA=18:32:44, max mem: 15.9 GB 
[10/31 03:50:03 visual_prompt]: 	Training 700/1106. train loss: 1.1531,	0.6320 s / batch. (data: 3.60e-04). ETA=17:44:27, max mem: 15.9 GB 
[10/31 03:51:07 visual_prompt]: 	Training 800/1106. train loss: 0.7551,	0.6531 s / batch. (data: 3.08e-02). ETA=18:18:54, max mem: 15.9 GB 
[10/31 03:52:11 visual_prompt]: 	Training 900/1106. train loss: 0.7085,	0.6389 s / batch. (data: 8.96e-04). ETA=17:53:51, max mem: 15.9 GB 
[10/31 03:53:15 visual_prompt]: 	Training 1000/1106. train loss: 0.7587,	0.6482 s / batch. (data: 1.69e-02). ETA=18:08:28, max mem: 15.9 GB 
[10/31 03:54:18 visual_prompt]: 	Training 1100/1106. train loss: 0.6949,	0.6177 s / batch. (data: 1.76e-04). ETA=17:16:13, max mem: 15.9 GB 
[10/31 03:54:22 visual_prompt]: Epoch 9 / 100: avg data time: 6.27e-03, avg batch time: 0.6386, average train loss: 0.7402
[10/31 03:55:16 visual_prompt]: 	Test 100/123. loss: 0.698, 0.2376 s / batch. (data: 4.55e-05)max mem: 15.94594 GB 
[10/31 03:55:28 visual_prompt]: Inference (val):avg data time: 1.19e-04, avg batch time: 0.2323, average loss: 0.7086
[10/31 03:55:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.68	
[10/31 03:55:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/31 03:56:35 visual_prompt]: 	Training 100/1106. train loss: 0.7066,	0.6330 s / batch. (data: 3.67e-04). ETA=17:40:48, max mem: 15.9 GB 
[10/31 03:57:39 visual_prompt]: 	Training 200/1106. train loss: 1.0864,	0.6293 s / batch. (data: 1.20e-02). ETA=17:33:26, max mem: 15.9 GB 
[10/31 03:58:42 visual_prompt]: 	Training 300/1106. train loss: 0.7165,	0.6546 s / batch. (data: 1.51e-02). ETA=18:14:50, max mem: 15.9 GB 
[10/31 03:59:46 visual_prompt]: 	Training 400/1106. train loss: 0.4873,	0.6300 s / batch. (data: 1.23e-02). ETA=17:32:34, max mem: 15.9 GB 
[10/31 04:00:49 visual_prompt]: 	Training 500/1106. train loss: 0.6981,	0.6279 s / batch. (data: 4.37e-04). ETA=17:28:03, max mem: 15.9 GB 
[10/31 04:01:53 visual_prompt]: 	Training 600/1106. train loss: 0.4556,	0.6335 s / batch. (data: 5.58e-03). ETA=17:36:15, max mem: 15.9 GB 
[10/31 04:02:57 visual_prompt]: 	Training 700/1106. train loss: 0.6936,	0.6427 s / batch. (data: 8.60e-04). ETA=17:50:33, max mem: 15.9 GB 
[10/31 04:04:01 visual_prompt]: 	Training 800/1106. train loss: 0.7297,	0.7082 s / batch. (data: 2.42e-02). ETA=19:38:31, max mem: 15.9 GB 
[10/31 04:05:04 visual_prompt]: 	Training 900/1106. train loss: 0.6936,	0.6614 s / batch. (data: 8.89e-04). ETA=18:19:33, max mem: 15.9 GB 
[10/31 04:06:08 visual_prompt]: 	Training 1000/1106. train loss: 0.8406,	0.6656 s / batch. (data: 6.12e-03). ETA=18:25:23, max mem: 15.9 GB 
[10/31 04:07:12 visual_prompt]: 	Training 1100/1106. train loss: 0.3210,	0.6183 s / batch. (data: 2.33e-04). ETA=17:05:45, max mem: 15.9 GB 
[10/31 04:07:16 visual_prompt]: Epoch 10 / 100: avg data time: 6.34e-03, avg batch time: 0.6401, average train loss: 0.7993
[10/31 04:08:10 visual_prompt]: 	Test 100/123. loss: 0.699, 0.2250 s / batch. (data: 6.94e-05)max mem: 15.94594 GB 
[10/31 04:08:22 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.2337, average loss: 0.6884
[10/31 04:08:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.89	
[10/31 04:08:22 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/31 04:09:29 visual_prompt]: 	Training 100/1106. train loss: 1.3283,	0.6180 s / batch. (data: 3.23e-04). ETA=17:04:09, max mem: 15.9 GB 
[10/31 04:10:32 visual_prompt]: 	Training 200/1106. train loss: 0.9808,	0.6526 s / batch. (data: 1.57e-02). ETA=18:00:28, max mem: 15.9 GB 
[10/31 04:11:36 visual_prompt]: 	Training 300/1106. train loss: 0.6844,	0.6300 s / batch. (data: 1.06e-02). ETA=17:22:03, max mem: 15.9 GB 
[10/31 04:12:40 visual_prompt]: 	Training 400/1106. train loss: 1.2117,	0.6343 s / batch. (data: 9.33e-04). ETA=17:28:07, max mem: 15.9 GB 
[10/31 04:13:43 visual_prompt]: 	Training 500/1106. train loss: 1.4297,	0.6358 s / batch. (data: 5.53e-03). ETA=17:29:32, max mem: 15.9 GB 
[10/31 04:14:47 visual_prompt]: 	Training 600/1106. train loss: 0.2610,	0.6492 s / batch. (data: 9.17e-04). ETA=17:50:30, max mem: 15.9 GB 
[10/31 04:15:51 visual_prompt]: 	Training 700/1106. train loss: 0.8497,	0.6187 s / batch. (data: 4.12e-04). ETA=16:59:13, max mem: 15.9 GB 
[10/31 04:16:54 visual_prompt]: 	Training 800/1106. train loss: 0.4003,	0.6360 s / batch. (data: 3.69e-04). ETA=17:26:37, max mem: 15.9 GB 
[10/31 04:17:58 visual_prompt]: 	Training 900/1106. train loss: 0.3724,	0.6232 s / batch. (data: 3.68e-04). ETA=17:04:30, max mem: 15.9 GB 
[10/31 04:19:01 visual_prompt]: 	Training 1000/1106. train loss: 0.4552,	0.6379 s / batch. (data: 1.57e-02). ETA=17:27:37, max mem: 15.9 GB 
[10/31 04:20:05 visual_prompt]: 	Training 1100/1106. train loss: 0.6962,	0.6173 s / batch. (data: 2.16e-04). ETA=16:52:46, max mem: 15.9 GB 
[10/31 04:20:09 visual_prompt]: Epoch 11 / 100: avg data time: 6.71e-03, avg batch time: 0.6392, average train loss: 0.7717
[10/31 04:21:03 visual_prompt]: 	Test 100/123. loss: 0.717, 0.2367 s / batch. (data: 6.79e-05)max mem: 15.94594 GB 
[10/31 04:21:15 visual_prompt]: Inference (val):avg data time: 4.85e-05, avg batch time: 0.2330, average loss: 0.6963
[10/31 04:21:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.38	
[10/31 04:21:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/31 04:22:22 visual_prompt]: 	Training 100/1106. train loss: 0.1682,	0.6287 s / batch. (data: 8.73e-04). ETA=17:10:21, max mem: 15.9 GB 
[10/31 04:23:26 visual_prompt]: 	Training 200/1106. train loss: 0.7226,	0.6383 s / batch. (data: 9.36e-04). ETA=17:24:58, max mem: 15.9 GB 
[10/31 04:24:30 visual_prompt]: 	Training 300/1106. train loss: 0.1321,	0.6440 s / batch. (data: 8.72e-04). ETA=17:33:18, max mem: 15.9 GB 
[10/31 04:25:34 visual_prompt]: 	Training 400/1106. train loss: 0.7431,	0.6405 s / batch. (data: 1.32e-02). ETA=17:26:35, max mem: 15.9 GB 
[10/31 04:26:37 visual_prompt]: 	Training 500/1106. train loss: 1.6082,	0.6439 s / batch. (data: 8.72e-04). ETA=17:30:58, max mem: 15.9 GB 
[10/31 04:27:41 visual_prompt]: 	Training 600/1106. train loss: 0.6911,	0.6275 s / batch. (data: 1.07e-03). ETA=17:03:12, max mem: 15.9 GB 
[10/31 04:28:45 visual_prompt]: 	Training 700/1106. train loss: 0.6951,	0.6240 s / batch. (data: 4.49e-04). ETA=16:56:23, max mem: 15.9 GB 
[10/31 04:29:49 visual_prompt]: 	Training 800/1106. train loss: 0.7314,	0.6202 s / batch. (data: 3.48e-04). ETA=16:49:09, max mem: 15.9 GB 
[10/31 04:30:52 visual_prompt]: 	Training 900/1106. train loss: 0.2477,	0.6469 s / batch. (data: 9.39e-04). ETA=17:31:30, max mem: 15.9 GB 
[10/31 04:31:56 visual_prompt]: 	Training 1000/1106. train loss: 0.6961,	0.6288 s / batch. (data: 4.79e-04). ETA=17:01:04, max mem: 15.9 GB 
[10/31 04:33:00 visual_prompt]: 	Training 1100/1106. train loss: 0.6732,	0.6188 s / batch. (data: 1.68e-04). ETA=16:43:46, max mem: 15.9 GB 
[10/31 04:33:03 visual_prompt]: Epoch 12 / 100: avg data time: 6.93e-03, avg batch time: 0.6407, average train loss: 0.8018
[10/31 04:33:58 visual_prompt]: 	Test 100/123. loss: 1.767, 0.2357 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/31 04:34:09 visual_prompt]: Inference (val):avg data time: 2.62e-04, avg batch time: 0.2323, average loss: 1.9415
[10/31 04:34:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.20	
[10/31 04:34:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/31 04:35:16 visual_prompt]: 	Training 100/1106. train loss: 1.4542,	0.6349 s / batch. (data: 3.61e-04). ETA=17:08:48, max mem: 15.9 GB 
[10/31 04:36:20 visual_prompt]: 	Training 200/1106. train loss: 1.0629,	0.6560 s / batch. (data: 3.22e-02). ETA=17:41:56, max mem: 15.9 GB 
[10/31 04:37:23 visual_prompt]: 	Training 300/1106. train loss: 0.3069,	0.6195 s / batch. (data: 3.48e-04). ETA=16:41:48, max mem: 15.9 GB 
[10/31 04:38:27 visual_prompt]: 	Training 400/1106. train loss: 0.7644,	0.6189 s / batch. (data: 3.32e-04). ETA=16:39:45, max mem: 15.9 GB 
[10/31 04:39:31 visual_prompt]: 	Training 500/1106. train loss: 0.2213,	0.6448 s / batch. (data: 8.69e-04). ETA=17:20:37, max mem: 15.9 GB 
[10/31 04:40:35 visual_prompt]: 	Training 600/1106. train loss: 0.8889,	0.6440 s / batch. (data: 5.47e-04). ETA=17:18:10, max mem: 15.9 GB 
[10/31 04:41:38 visual_prompt]: 	Training 700/1106. train loss: 0.8197,	0.6520 s / batch. (data: 8.83e-04). ETA=17:30:01, max mem: 15.9 GB 
[10/31 04:42:42 visual_prompt]: 	Training 800/1106. train loss: 0.8186,	0.6435 s / batch. (data: 9.03e-04). ETA=17:15:12, max mem: 15.9 GB 
[10/31 04:43:46 visual_prompt]: 	Training 900/1106. train loss: 0.9672,	0.6400 s / batch. (data: 3.49e-04). ETA=17:08:32, max mem: 15.9 GB 
[10/31 04:44:50 visual_prompt]: 	Training 1000/1106. train loss: 0.4656,	0.6342 s / batch. (data: 8.66e-04). ETA=16:58:11, max mem: 15.9 GB 
[10/31 04:45:54 visual_prompt]: 	Training 1100/1106. train loss: 0.7213,	0.6208 s / batch. (data: 2.31e-04). ETA=16:35:34, max mem: 15.9 GB 
[10/31 04:45:58 visual_prompt]: Epoch 13 / 100: avg data time: 6.15e-03, avg batch time: 0.6403, average train loss: 0.8280
[10/31 04:46:52 visual_prompt]: 	Test 100/123. loss: 0.872, 0.2253 s / batch. (data: 3.60e-05)max mem: 15.94594 GB 
[10/31 04:47:04 visual_prompt]: Inference (val):avg data time: 4.81e-05, avg batch time: 0.2330, average loss: 0.8122
[10/31 04:47:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.82	
[10/31 04:47:04 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/31 04:48:10 visual_prompt]: 	Training 100/1106. train loss: 0.8751,	0.6295 s / batch. (data: 3.71e-04). ETA=16:48:32, max mem: 15.9 GB 
[10/31 04:49:13 visual_prompt]: 	Training 200/1106. train loss: 0.7005,	0.6162 s / batch. (data: 3.70e-04). ETA=16:26:06, max mem: 15.9 GB 
[10/31 04:50:18 visual_prompt]: 	Training 300/1106. train loss: 0.7609,	0.6394 s / batch. (data: 3.87e-04). ETA=17:02:12, max mem: 15.9 GB 
[10/31 04:51:22 visual_prompt]: 	Training 400/1106. train loss: 0.6120,	0.6250 s / batch. (data: 7.96e-03). ETA=16:38:10, max mem: 15.9 GB 
[10/31 04:52:25 visual_prompt]: 	Training 500/1106. train loss: 0.7708,	0.6304 s / batch. (data: 8.76e-04). ETA=16:45:40, max mem: 15.9 GB 
[10/31 04:53:29 visual_prompt]: 	Training 600/1106. train loss: 0.6896,	0.6311 s / batch. (data: 8.81e-04). ETA=16:45:47, max mem: 15.9 GB 
[10/31 04:54:33 visual_prompt]: 	Training 700/1106. train loss: 0.2717,	0.6437 s / batch. (data: 1.09e-02). ETA=17:04:45, max mem: 15.9 GB 
[10/31 04:55:36 visual_prompt]: 	Training 800/1106. train loss: 0.6923,	0.6564 s / batch. (data: 1.25e-03). ETA=17:23:57, max mem: 15.9 GB 
[10/31 04:56:40 visual_prompt]: 	Training 900/1106. train loss: 0.6923,	0.6396 s / batch. (data: 8.50e-04). ETA=16:56:04, max mem: 15.9 GB 
[10/31 04:57:44 visual_prompt]: 	Training 1000/1106. train loss: 0.1385,	0.6373 s / batch. (data: 1.57e-02). ETA=16:51:24, max mem: 15.9 GB 
[10/31 04:58:47 visual_prompt]: 	Training 1100/1106. train loss: 0.7435,	0.6196 s / batch. (data: 1.74e-04). ETA=16:22:13, max mem: 15.9 GB 
[10/31 04:58:51 visual_prompt]: Epoch 14 / 100: avg data time: 6.73e-03, avg batch time: 0.6398, average train loss: 0.7599
[10/31 04:59:45 visual_prompt]: 	Test 100/123. loss: 0.696, 0.2437 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[10/31 04:59:57 visual_prompt]: Inference (val):avg data time: 4.77e-05, avg batch time: 0.2326, average loss: 0.7038
[10/31 04:59:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.64	
[10/31 04:59:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/31 05:01:03 visual_prompt]: 	Training 100/1106. train loss: 0.7777,	0.6362 s / batch. (data: 1.69e-02). ETA=16:47:33, max mem: 15.9 GB 
[10/31 05:02:07 visual_prompt]: 	Training 200/1106. train loss: 0.6946,	0.6361 s / batch. (data: 8.29e-04). ETA=16:46:14, max mem: 15.9 GB 
[10/31 05:03:10 visual_prompt]: 	Training 300/1106. train loss: 0.7688,	0.6196 s / batch. (data: 3.57e-04). ETA=16:19:05, max mem: 15.9 GB 
[10/31 05:04:14 visual_prompt]: 	Training 400/1106. train loss: 2.0989,	0.6463 s / batch. (data: 9.51e-04). ETA=17:00:12, max mem: 15.9 GB 
[10/31 05:05:18 visual_prompt]: 	Training 500/1106. train loss: 0.2338,	0.6310 s / batch. (data: 1.40e-02). ETA=16:34:59, max mem: 15.9 GB 
[10/31 05:06:22 visual_prompt]: 	Training 600/1106. train loss: 0.8119,	0.6224 s / batch. (data: 3.17e-04). ETA=16:20:28, max mem: 15.9 GB 
[10/31 05:07:25 visual_prompt]: 	Training 700/1106. train loss: 0.9799,	0.6303 s / batch. (data: 3.49e-04). ETA=16:31:54, max mem: 15.9 GB 
[10/31 05:08:29 visual_prompt]: 	Training 800/1106. train loss: 0.7566,	0.6475 s / batch. (data: 8.99e-04). ETA=16:57:51, max mem: 15.9 GB 
[10/31 05:09:32 visual_prompt]: 	Training 900/1106. train loss: 0.8372,	0.6430 s / batch. (data: 7.90e-04). ETA=16:49:40, max mem: 15.9 GB 
[10/31 05:10:36 visual_prompt]: 	Training 1000/1106. train loss: 0.6933,	0.6285 s / batch. (data: 3.12e-04). ETA=16:25:54, max mem: 15.9 GB 
[10/31 05:11:40 visual_prompt]: 	Training 1100/1106. train loss: 0.8175,	0.6179 s / batch. (data: 2.15e-04). ETA=16:08:16, max mem: 15.9 GB 
[10/31 05:11:44 visual_prompt]: Epoch 15 / 100: avg data time: 5.37e-03, avg batch time: 0.6386, average train loss: 0.8079
[10/31 05:12:38 visual_prompt]: 	Test 100/123. loss: 0.760, 0.2277 s / batch. (data: 5.22e-05)max mem: 15.94594 GB 
[10/31 05:12:49 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.2322, average loss: 0.7247
[10/31 05:12:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.97	
[10/31 05:12:49 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/31 05:13:55 visual_prompt]: 	Training 100/1106. train loss: 0.6237,	0.6498 s / batch. (data: 1.23e-03). ETA=16:56:59, max mem: 15.9 GB 
[10/31 05:14:58 visual_prompt]: 	Training 200/1106. train loss: 1.0537,	0.6189 s / batch. (data: 3.55e-04). ETA=16:07:39, max mem: 15.9 GB 
[10/31 05:16:02 visual_prompt]: 	Training 300/1106. train loss: 0.2891,	0.6198 s / batch. (data: 3.97e-04). ETA=16:08:04, max mem: 15.9 GB 
[10/31 05:17:06 visual_prompt]: 	Training 400/1106. train loss: 0.8067,	0.6189 s / batch. (data: 3.46e-04). ETA=16:05:31, max mem: 15.9 GB 
[10/31 05:18:09 visual_prompt]: 	Training 500/1106. train loss: 0.7025,	0.6304 s / batch. (data: 3.75e-04). ETA=16:22:24, max mem: 15.9 GB 
[10/31 05:19:13 visual_prompt]: 	Training 600/1106. train loss: 1.1185,	0.6687 s / batch. (data: 4.05e-02). ETA=17:21:06, max mem: 15.9 GB 
[10/31 05:20:16 visual_prompt]: 	Training 700/1106. train loss: 0.7102,	0.6318 s / batch. (data: 3.61e-04). ETA=16:22:34, max mem: 15.9 GB 
[10/31 05:21:20 visual_prompt]: 	Training 800/1106. train loss: 0.7185,	0.6440 s / batch. (data: 3.59e-04). ETA=16:40:26, max mem: 15.9 GB 
[10/31 05:22:24 visual_prompt]: 	Training 900/1106. train loss: 0.7097,	0.6327 s / batch. (data: 8.64e-04). ETA=16:21:52, max mem: 15.9 GB 
[10/31 05:23:27 visual_prompt]: 	Training 1000/1106. train loss: 0.9034,	0.6585 s / batch. (data: 8.52e-03). ETA=17:00:48, max mem: 15.9 GB 
[10/31 05:24:31 visual_prompt]: 	Training 1100/1106. train loss: 0.4147,	0.6189 s / batch. (data: 2.21e-04). ETA=15:58:23, max mem: 15.9 GB 
[10/31 05:24:35 visual_prompt]: Epoch 16 / 100: avg data time: 5.49e-03, avg batch time: 0.6380, average train loss: 0.7816
[10/31 05:25:30 visual_prompt]: 	Test 100/123. loss: 0.734, 0.2359 s / batch. (data: 4.91e-05)max mem: 15.94594 GB 
[10/31 05:25:41 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.2313, average loss: 0.7058
[10/31 05:25:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.81	
[10/31 05:25:41 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/31 05:26:48 visual_prompt]: 	Training 100/1106. train loss: 1.2299,	0.6254 s / batch. (data: 3.49e-04). ETA=16:07:20, max mem: 15.9 GB 
[10/31 05:27:51 visual_prompt]: 	Training 200/1106. train loss: 0.7389,	0.6596 s / batch. (data: 1.62e-02). ETA=16:59:06, max mem: 15.9 GB 
[10/31 05:28:55 visual_prompt]: 	Training 300/1106. train loss: 0.6585,	0.6184 s / batch. (data: 3.69e-04). ETA=15:54:28, max mem: 15.9 GB 
[10/31 05:29:59 visual_prompt]: 	Training 400/1106. train loss: 2.3215,	0.6360 s / batch. (data: 3.70e-04). ETA=16:20:32, max mem: 15.9 GB 
[10/31 05:31:03 visual_prompt]: 	Training 500/1106. train loss: 0.2902,	0.6419 s / batch. (data: 5.53e-03). ETA=16:28:34, max mem: 15.9 GB 
[10/31 05:32:06 visual_prompt]: 	Training 600/1106. train loss: 0.9109,	0.6216 s / batch. (data: 3.66e-04). ETA=15:56:19, max mem: 15.9 GB 
[10/31 05:33:10 visual_prompt]: 	Training 700/1106. train loss: 0.6950,	0.6467 s / batch. (data: 8.65e-04). ETA=16:33:52, max mem: 15.9 GB 
[10/31 05:34:14 visual_prompt]: 	Training 800/1106. train loss: 1.1577,	0.6591 s / batch. (data: 8.87e-04). ETA=16:51:42, max mem: 15.9 GB 
[10/31 05:35:17 visual_prompt]: 	Training 900/1106. train loss: 0.6094,	0.6373 s / batch. (data: 6.10e-03). ETA=16:17:15, max mem: 15.9 GB 
[10/31 05:36:21 visual_prompt]: 	Training 1000/1106. train loss: 0.0917,	0.6387 s / batch. (data: 8.85e-04). ETA=16:18:20, max mem: 15.9 GB 
[10/31 05:37:25 visual_prompt]: 	Training 1100/1106. train loss: 0.1611,	0.6178 s / batch. (data: 2.30e-04). ETA=15:45:17, max mem: 15.9 GB 
[10/31 05:37:28 visual_prompt]: Epoch 17 / 100: avg data time: 5.87e-03, avg batch time: 0.6392, average train loss: 0.7926
[10/31 05:38:22 visual_prompt]: 	Test 100/123. loss: 0.874, 0.2245 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/31 05:38:34 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.2323, average loss: 0.9256
[10/31 05:38:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.92	
[10/31 05:38:34 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/31 05:39:40 visual_prompt]: 	Training 100/1106. train loss: 0.3522,	0.6456 s / batch. (data: 9.01e-04). ETA=16:26:42, max mem: 15.9 GB 
[10/31 05:40:44 visual_prompt]: 	Training 200/1106. train loss: 0.7725,	0.6317 s / batch. (data: 8.84e-04). ETA=16:04:18, max mem: 15.9 GB 
[10/31 05:41:48 visual_prompt]: 	Training 300/1106. train loss: 2.2789,	0.6461 s / batch. (data: 5.58e-03). ETA=16:25:16, max mem: 15.9 GB 
[10/31 05:42:52 visual_prompt]: 	Training 400/1106. train loss: 0.6955,	0.6280 s / batch. (data: 3.67e-04). ETA=15:56:37, max mem: 15.9 GB 
[10/31 05:43:55 visual_prompt]: 	Training 500/1106. train loss: 0.2032,	0.6480 s / batch. (data: 8.44e-04). ETA=16:26:00, max mem: 15.9 GB 
[10/31 05:44:59 visual_prompt]: 	Training 600/1106. train loss: 0.5822,	0.6300 s / batch. (data: 1.20e-02). ETA=15:57:37, max mem: 15.9 GB 
[10/31 05:46:03 visual_prompt]: 	Training 700/1106. train loss: 0.6653,	0.6423 s / batch. (data: 8.85e-04). ETA=16:15:14, max mem: 15.9 GB 
[10/31 05:47:07 visual_prompt]: 	Training 800/1106. train loss: 0.7382,	0.6347 s / batch. (data: 8.99e-04). ETA=16:02:38, max mem: 15.9 GB 
[10/31 05:48:10 visual_prompt]: 	Training 900/1106. train loss: 1.3609,	0.6285 s / batch. (data: 8.49e-04). ETA=15:52:09, max mem: 15.9 GB 
[10/31 05:49:14 visual_prompt]: 	Training 1000/1106. train loss: 0.7564,	0.6445 s / batch. (data: 9.53e-04). ETA=16:15:15, max mem: 15.9 GB 
[10/31 05:50:18 visual_prompt]: 	Training 1100/1106. train loss: 0.6957,	0.6196 s / batch. (data: 2.16e-04). ETA=15:36:40, max mem: 15.9 GB 
[10/31 05:50:22 visual_prompt]: Epoch 18 / 100: avg data time: 5.67e-03, avg batch time: 0.6397, average train loss: 0.8319
[10/31 05:51:17 visual_prompt]: 	Test 100/123. loss: 0.815, 0.2256 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/31 05:51:28 visual_prompt]: Inference (val):avg data time: 2.51e-04, avg batch time: 0.2336, average loss: 0.7663
[10/31 05:51:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.55	
[10/31 05:51:28 visual_prompt]: Stopping early.
[10/31 05:51:28 visual_prompt]: Rank of current process: 0. World size: 1
[10/31 05:51:28 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/31 05:51:28 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/31 05:51:28 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/31 05:51:28 visual_prompt]: Training with config:
[10/31 05:51:28 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.05_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/31 05:51:28 visual_prompt]: Loading training data...
[10/31 05:51:28 visual_prompt]: Constructing mammo-cbis dataset train...
[10/31 05:51:28 visual_prompt]: Loading validation data...
[10/31 05:51:28 visual_prompt]: Constructing mammo-cbis dataset val...
[10/31 05:51:28 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/31 05:51:34 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/31 05:51:34 visual_prompt]: tuned percent:0.522
[10/31 05:51:34 visual_prompt]: Device used for model: 0
[10/31 05:51:34 visual_prompt]: Setting up Evaluator...
[10/31 05:51:34 visual_prompt]: Setting up Trainer...
[10/31 05:51:34 visual_prompt]: 	Setting up the optimizer...
[10/31 05:51:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/31 05:52:41 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6322 s / batch. (data: 9.36e-04). ETA=19:24:19, max mem: 15.9 GB 
[10/31 05:53:45 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6440 s / batch. (data: 3.63e-04). ETA=19:44:55, max mem: 15.9 GB 
[10/31 05:54:48 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6515 s / batch. (data: 6.09e-03). ETA=19:57:38, max mem: 15.9 GB 
[10/31 05:55:52 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6286 s / batch. (data: 3.24e-04). ETA=19:14:36, max mem: 15.9 GB 
[10/31 05:56:56 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6520 s / batch. (data: 7.97e-03). ETA=19:56:27, max mem: 15.9 GB 
[10/31 05:57:59 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6332 s / batch. (data: 8.98e-04). ETA=19:20:47, max mem: 15.9 GB 
[10/31 05:59:03 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6197 s / batch. (data: 3.75e-04). ETA=18:55:01, max mem: 15.9 GB 
[10/31 06:00:07 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6400 s / batch. (data: 3.30e-04). ETA=19:31:11, max mem: 15.9 GB 
[10/31 06:01:11 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6650 s / batch. (data: 1.70e-02). ETA=20:15:47, max mem: 15.9 GB 
[10/31 06:02:15 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6327 s / batch. (data: 3.66e-04). ETA=19:15:38, max mem: 15.9 GB 
[10/31 06:03:18 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6180 s / batch. (data: 3.51e-04). ETA=18:47:49, max mem: 15.9 GB 
[10/31 06:03:22 visual_prompt]: Epoch 1 / 100: avg data time: 6.14e-03, avg batch time: 0.6398, average train loss: 1.4028
[10/31 06:04:16 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2258 s / batch. (data: 5.27e-05)max mem: 15.94594 GB 
[10/31 06:04:28 visual_prompt]: Inference (val):avg data time: 4.76e-05, avg batch time: 0.2309, average loss: 1.3505
[10/31 06:04:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/31 06:04:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/31 06:05:34 visual_prompt]: 	Training 100/1106. train loss: 0.6522,	0.6326 s / batch. (data: 1.57e-02). ETA=19:13:19, max mem: 15.9 GB 
[10/31 06:06:38 visual_prompt]: 	Training 200/1106. train loss: 0.7171,	0.6550 s / batch. (data: 9.37e-04). ETA=19:53:06, max mem: 15.9 GB 
[10/31 06:07:41 visual_prompt]: 	Training 300/1106. train loss: 0.9991,	0.6400 s / batch. (data: 3.27e-04). ETA=19:24:45, max mem: 15.9 GB 
[10/31 06:08:44 visual_prompt]: 	Training 400/1106. train loss: 0.1179,	0.6191 s / batch. (data: 3.91e-04). ETA=18:45:37, max mem: 15.9 GB 
[10/31 06:09:48 visual_prompt]: 	Training 500/1106. train loss: 0.7339,	0.6360 s / batch. (data: 3.94e-04). ETA=19:15:21, max mem: 15.9 GB 
[10/31 06:10:52 visual_prompt]: 	Training 600/1106. train loss: 0.5482,	0.6630 s / batch. (data: 3.23e-02). ETA=20:03:18, max mem: 15.9 GB 
[10/31 06:11:56 visual_prompt]: 	Training 700/1106. train loss: 0.7473,	0.6480 s / batch. (data: 8.92e-04). ETA=19:34:57, max mem: 15.9 GB 
[10/31 06:13:00 visual_prompt]: 	Training 800/1106. train loss: 0.7671,	0.6389 s / batch. (data: 3.60e-04). ETA=19:17:29, max mem: 15.9 GB 
[10/31 06:14:03 visual_prompt]: 	Training 900/1106. train loss: 0.6684,	0.6403 s / batch. (data: 3.68e-04). ETA=19:18:49, max mem: 15.9 GB 
[10/31 06:15:07 visual_prompt]: 	Training 1000/1106. train loss: 0.4772,	0.6242 s / batch. (data: 3.84e-04). ETA=18:48:44, max mem: 15.9 GB 
[10/31 06:16:11 visual_prompt]: 	Training 1100/1106. train loss: 0.7437,	0.6190 s / batch. (data: 2.05e-04). ETA=18:38:14, max mem: 15.9 GB 
[10/31 06:16:14 visual_prompt]: Epoch 2 / 100: avg data time: 5.75e-03, avg batch time: 0.6385, average train loss: 0.7710
[10/31 06:17:09 visual_prompt]: 	Test 100/123. loss: 1.042, 0.2287 s / batch. (data: 6.99e-05)max mem: 15.94594 GB 
[10/31 06:17:20 visual_prompt]: Inference (val):avg data time: 1.84e-04, avg batch time: 0.2329, average loss: 0.9170
[10/31 06:17:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.64	
[10/31 06:17:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/31 06:18:28 visual_prompt]: 	Training 100/1106. train loss: 0.6421,	0.6174 s / batch. (data: 3.11e-04). ETA=18:34:12, max mem: 15.9 GB 
[10/31 06:19:31 visual_prompt]: 	Training 200/1106. train loss: 0.3309,	0.6353 s / batch. (data: 8.90e-04). ETA=19:05:36, max mem: 15.9 GB 
[10/31 06:20:35 visual_prompt]: 	Training 300/1106. train loss: 0.3415,	0.6304 s / batch. (data: 3.59e-04). ETA=18:55:38, max mem: 15.9 GB 
[10/31 06:21:39 visual_prompt]: 	Training 400/1106. train loss: 0.7547,	0.6440 s / batch. (data: 7.87e-04). ETA=19:19:02, max mem: 15.9 GB 
[10/31 06:22:43 visual_prompt]: 	Training 500/1106. train loss: 1.3721,	0.6451 s / batch. (data: 9.12e-04). ETA=19:19:56, max mem: 15.9 GB 
[10/31 06:23:46 visual_prompt]: 	Training 600/1106. train loss: 0.6510,	0.6340 s / batch. (data: 1.04e-02). ETA=18:58:55, max mem: 15.9 GB 
[10/31 06:24:50 visual_prompt]: 	Training 700/1106. train loss: 0.8077,	0.6433 s / batch. (data: 1.06e-02). ETA=19:14:34, max mem: 15.9 GB 
[10/31 06:25:54 visual_prompt]: 	Training 800/1106. train loss: 1.2838,	0.6568 s / batch. (data: 3.93e-02). ETA=19:37:42, max mem: 15.9 GB 
[10/31 06:26:57 visual_prompt]: 	Training 900/1106. train loss: 1.1728,	0.6319 s / batch. (data: 5.06e-04). ETA=18:52:04, max mem: 15.9 GB 
[10/31 06:28:01 visual_prompt]: 	Training 1000/1106. train loss: 0.6825,	0.6400 s / batch. (data: 3.30e-04). ETA=19:05:26, max mem: 15.9 GB 
[10/31 06:29:05 visual_prompt]: 	Training 1100/1106. train loss: 0.5686,	0.6185 s / batch. (data: 2.15e-04). ETA=18:25:59, max mem: 15.9 GB 
[10/31 06:29:09 visual_prompt]: Epoch 3 / 100: avg data time: 7.30e-03, avg batch time: 0.6404, average train loss: 0.7643
[10/31 06:30:03 visual_prompt]: 	Test 100/123. loss: 0.797, 0.2336 s / batch. (data: 3.15e-05)max mem: 15.94594 GB 
[10/31 06:30:15 visual_prompt]: Inference (val):avg data time: 5.27e-05, avg batch time: 0.2333, average loss: 0.7726
[10/31 06:30:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.68	
[10/31 06:30:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/31 06:31:21 visual_prompt]: 	Training 100/1106. train loss: 1.2305,	0.6522 s / batch. (data: 8.85e-04). ETA=19:25:00, max mem: 15.9 GB 
[10/31 06:32:24 visual_prompt]: 	Training 200/1106. train loss: 1.0817,	0.6644 s / batch. (data: 2.44e-02). ETA=19:45:43, max mem: 15.9 GB 
[10/31 06:33:28 visual_prompt]: 	Training 300/1106. train loss: 0.6874,	0.6243 s / batch. (data: 3.43e-04). ETA=18:33:11, max mem: 15.9 GB 
[10/31 06:34:32 visual_prompt]: 	Training 400/1106. train loss: 0.6526,	0.6456 s / batch. (data: 5.55e-03). ETA=19:10:04, max mem: 15.9 GB 
[10/31 06:35:36 visual_prompt]: 	Training 500/1106. train loss: 2.2350,	0.6278 s / batch. (data: 5.57e-03). ETA=18:37:18, max mem: 15.9 GB 
[10/31 06:36:40 visual_prompt]: 	Training 600/1106. train loss: 0.1682,	0.6251 s / batch. (data: 5.59e-03). ETA=18:31:29, max mem: 15.9 GB 
[10/31 06:37:44 visual_prompt]: 	Training 700/1106. train loss: 1.2584,	0.6321 s / batch. (data: 3.67e-04). ETA=18:42:48, max mem: 15.9 GB 
[10/31 06:38:47 visual_prompt]: 	Training 800/1106. train loss: 0.1618,	0.6335 s / batch. (data: 9.13e-04). ETA=18:44:14, max mem: 15.9 GB 
[10/31 06:39:51 visual_prompt]: 	Training 900/1106. train loss: 0.9113,	0.6360 s / batch. (data: 9.25e-04). ETA=18:47:38, max mem: 15.9 GB 
[10/31 06:40:55 visual_prompt]: 	Training 1000/1106. train loss: 0.5772,	0.6400 s / batch. (data: 1.20e-02). ETA=18:53:40, max mem: 15.9 GB 
[10/31 06:41:59 visual_prompt]: 	Training 1100/1106. train loss: 0.7243,	0.6193 s / batch. (data: 2.15e-04). ETA=18:15:58, max mem: 15.9 GB 
[10/31 06:42:03 visual_prompt]: Epoch 4 / 100: avg data time: 6.18e-03, avg batch time: 0.6404, average train loss: 0.7788
[10/31 06:42:58 visual_prompt]: 	Test 100/123. loss: 0.908, 0.2399 s / batch. (data: 3.93e-05)max mem: 15.94594 GB 
[10/31 06:43:09 visual_prompt]: Inference (val):avg data time: 4.71e-05, avg batch time: 0.2322, average loss: 0.8194
[10/31 06:43:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.07	
[10/31 06:43:09 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/31 06:44:15 visual_prompt]: 	Training 100/1106. train loss: 1.1617,	0.6239 s / batch. (data: 3.32e-04). ETA=18:23:05, max mem: 15.9 GB 
[10/31 06:45:19 visual_prompt]: 	Training 200/1106. train loss: 0.9633,	0.6189 s / batch. (data: 3.15e-04). ETA=18:13:09, max mem: 15.9 GB 
[10/31 06:46:23 visual_prompt]: 	Training 300/1106. train loss: 0.7414,	0.6338 s / batch. (data: 8.54e-04). ETA=18:38:28, max mem: 15.9 GB 
[10/31 06:47:26 visual_prompt]: 	Training 400/1106. train loss: 0.8527,	0.6557 s / batch. (data: 8.36e-04). ETA=19:15:57, max mem: 15.9 GB 
[10/31 06:48:30 visual_prompt]: 	Training 500/1106. train loss: 0.1497,	0.6356 s / batch. (data: 3.63e-04). ETA=18:39:26, max mem: 15.9 GB 
[10/31 06:49:34 visual_prompt]: 	Training 600/1106. train loss: 1.7012,	0.6187 s / batch. (data: 3.78e-04). ETA=18:08:37, max mem: 15.9 GB 
[10/31 06:50:38 visual_prompt]: 	Training 700/1106. train loss: 0.6551,	0.6198 s / batch. (data: 3.78e-04). ETA=18:09:30, max mem: 15.9 GB 
[10/31 06:51:41 visual_prompt]: 	Training 800/1106. train loss: 1.1263,	0.6499 s / batch. (data: 3.44e-04). ETA=19:01:23, max mem: 15.9 GB 
[10/31 06:52:45 visual_prompt]: 	Training 900/1106. train loss: 0.7403,	0.6440 s / batch. (data: 8.53e-04). ETA=18:49:58, max mem: 15.9 GB 
[10/31 06:53:49 visual_prompt]: 	Training 1000/1106. train loss: 0.3388,	0.6401 s / batch. (data: 8.84e-04). ETA=18:41:58, max mem: 15.9 GB 
[10/31 06:54:53 visual_prompt]: 	Training 1100/1106. train loss: 0.8461,	0.6194 s / batch. (data: 2.02e-04). ETA=18:04:40, max mem: 15.9 GB 
[10/31 06:54:57 visual_prompt]: Epoch 5 / 100: avg data time: 5.57e-03, avg batch time: 0.6396, average train loss: 0.7889
[10/31 06:55:51 visual_prompt]: 	Test 100/123. loss: 0.980, 0.2264 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[10/31 06:56:03 visual_prompt]: Inference (val):avg data time: 4.93e-05, avg batch time: 0.2327, average loss: 1.0463
[10/31 06:56:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.64	
[10/31 06:56:03 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/31 06:57:08 visual_prompt]: 	Training 100/1106. train loss: 0.8615,	0.6200 s / batch. (data: 3.76e-04). ETA=18:04:41, max mem: 15.9 GB 
[10/31 06:58:12 visual_prompt]: 	Training 200/1106. train loss: 0.4283,	0.6335 s / batch. (data: 9.02e-04). ETA=18:27:12, max mem: 15.9 GB 
[10/31 06:59:16 visual_prompt]: 	Training 300/1106. train loss: 0.0978,	0.6430 s / batch. (data: 8.80e-04). ETA=18:42:42, max mem: 15.9 GB 
[10/31 07:00:19 visual_prompt]: 	Training 400/1106. train loss: 0.8176,	0.6376 s / batch. (data: 8.72e-04). ETA=18:32:19, max mem: 15.9 GB 
[10/31 07:01:23 visual_prompt]: 	Training 500/1106. train loss: 0.8114,	0.6192 s / batch. (data: 3.50e-04). ETA=17:59:13, max mem: 15.9 GB 
[10/31 07:02:27 visual_prompt]: 	Training 600/1106. train loss: 0.5220,	0.6216 s / batch. (data: 3.83e-04). ETA=18:02:14, max mem: 15.9 GB 
[10/31 07:03:31 visual_prompt]: 	Training 700/1106. train loss: 0.7843,	0.6479 s / batch. (data: 1.06e-02). ETA=18:47:02, max mem: 15.9 GB 
[10/31 07:04:34 visual_prompt]: 	Training 800/1106. train loss: 0.7369,	0.6192 s / batch. (data: 3.40e-04). ETA=17:56:06, max mem: 15.9 GB 
[10/31 07:05:38 visual_prompt]: 	Training 900/1106. train loss: 0.3828,	0.6602 s / batch. (data: 3.64e-02). ETA=19:06:15, max mem: 15.9 GB 
[10/31 07:06:42 visual_prompt]: 	Training 1000/1106. train loss: 1.9345,	0.6296 s / batch. (data: 1.19e-02). ETA=18:12:04, max mem: 15.9 GB 
[10/31 07:07:46 visual_prompt]: 	Training 1100/1106. train loss: 0.6191,	0.6183 s / batch. (data: 2.28e-04). ETA=17:51:25, max mem: 15.9 GB 
[10/31 07:07:49 visual_prompt]: Epoch 6 / 100: avg data time: 5.61e-03, avg batch time: 0.6390, average train loss: 0.7680
[10/31 07:08:43 visual_prompt]: 	Test 100/123. loss: 0.801, 0.2270 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/31 07:08:55 visual_prompt]: Inference (val):avg data time: 4.72e-05, avg batch time: 0.2325, average loss: 0.7420
[10/31 07:08:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.29	
[10/31 07:08:55 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/31 07:10:01 visual_prompt]: 	Training 100/1106. train loss: 0.7055,	0.6297 s / batch. (data: 3.50e-04). ETA=18:10:07, max mem: 15.9 GB 
[10/31 07:11:05 visual_prompt]: 	Training 200/1106. train loss: 1.0370,	0.6471 s / batch. (data: 1.63e-02). ETA=18:39:06, max mem: 15.9 GB 
[10/31 07:12:09 visual_prompt]: 	Training 300/1106. train loss: 0.8170,	0.6487 s / batch. (data: 5.57e-03). ETA=18:40:44, max mem: 15.9 GB 
[10/31 07:13:12 visual_prompt]: 	Training 400/1106. train loss: 1.0109,	0.6334 s / batch. (data: 8.33e-04). ETA=18:13:22, max mem: 15.9 GB 
[10/31 07:14:16 visual_prompt]: 	Training 500/1106. train loss: 0.6698,	0.6400 s / batch. (data: 3.31e-04). ETA=18:23:37, max mem: 15.9 GB 
[10/31 07:15:20 visual_prompt]: 	Training 600/1106. train loss: 0.6903,	0.6413 s / batch. (data: 9.29e-04). ETA=18:24:47, max mem: 15.9 GB 
[10/31 07:16:24 visual_prompt]: 	Training 700/1106. train loss: 0.4631,	0.6276 s / batch. (data: 3.75e-04). ETA=18:00:12, max mem: 15.9 GB 
[10/31 07:17:27 visual_prompt]: 	Training 800/1106. train loss: 0.3840,	0.6323 s / batch. (data: 8.63e-04). ETA=18:07:11, max mem: 15.9 GB 
[10/31 07:18:31 visual_prompt]: 	Training 900/1106. train loss: 0.7429,	0.6336 s / batch. (data: 8.95e-04). ETA=18:08:20, max mem: 15.9 GB 
[10/31 07:19:35 visual_prompt]: 	Training 1000/1106. train loss: 0.9308,	0.6619 s / batch. (data: 6.07e-03). ETA=18:55:47, max mem: 15.9 GB 
[10/31 07:20:38 visual_prompt]: 	Training 1100/1106. train loss: 0.3235,	0.6177 s / batch. (data: 2.05e-04). ETA=17:39:03, max mem: 15.9 GB 
[10/31 07:20:42 visual_prompt]: Epoch 7 / 100: avg data time: 5.19e-03, avg batch time: 0.6387, average train loss: 0.7592
[10/31 07:21:36 visual_prompt]: 	Test 100/123. loss: 0.766, 0.2347 s / batch. (data: 5.72e-05)max mem: 15.94594 GB 
[10/31 07:21:48 visual_prompt]: Inference (val):avg data time: 1.09e-04, avg batch time: 0.2323, average loss: 0.7231
[10/31 07:21:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.89	
[10/31 07:21:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/31 07:22:54 visual_prompt]: 	Training 100/1106. train loss: 0.5209,	0.6733 s / batch. (data: 9.11e-04). ETA=19:13:04, max mem: 15.9 GB 
[10/31 07:23:58 visual_prompt]: 	Training 200/1106. train loss: 0.7242,	0.6195 s / batch. (data: 3.59e-04). ETA=17:40:01, max mem: 15.9 GB 
[10/31 07:25:02 visual_prompt]: 	Training 300/1106. train loss: 0.2472,	0.6568 s / batch. (data: 2.08e-02). ETA=18:42:41, max mem: 15.9 GB 
[10/31 07:26:06 visual_prompt]: 	Training 400/1106. train loss: 1.3748,	0.6243 s / batch. (data: 3.52e-04). ETA=17:46:01, max mem: 15.9 GB 
[10/31 07:27:09 visual_prompt]: 	Training 500/1106. train loss: 0.5124,	0.6345 s / batch. (data: 1.62e-02). ETA=18:02:29, max mem: 15.9 GB 
[10/31 07:28:13 visual_prompt]: 	Training 600/1106. train loss: 1.0742,	0.6202 s / batch. (data: 4.03e-04). ETA=17:36:58, max mem: 15.9 GB 
[10/31 07:29:17 visual_prompt]: 	Training 700/1106. train loss: 0.8562,	0.6311 s / batch. (data: 8.60e-04). ETA=17:54:33, max mem: 15.9 GB 
[10/31 07:30:21 visual_prompt]: 	Training 800/1106. train loss: 1.0770,	0.6385 s / batch. (data: 8.95e-04). ETA=18:06:02, max mem: 15.9 GB 
[10/31 07:31:24 visual_prompt]: 	Training 900/1106. train loss: 0.3521,	0.6399 s / batch. (data: 8.46e-04). ETA=18:07:23, max mem: 15.9 GB 
[10/31 07:32:28 visual_prompt]: 	Training 1000/1106. train loss: 1.0671,	0.6352 s / batch. (data: 9.34e-04). ETA=17:58:21, max mem: 15.9 GB 
[10/31 07:33:32 visual_prompt]: 	Training 1100/1106. train loss: 0.3637,	0.6176 s / batch. (data: 1.78e-04). ETA=17:27:26, max mem: 15.9 GB 
[10/31 07:33:36 visual_prompt]: Epoch 8 / 100: avg data time: 5.67e-03, avg batch time: 0.6397, average train loss: 0.7802
[10/31 07:34:30 visual_prompt]: 	Test 100/123. loss: 0.787, 0.2252 s / batch. (data: 5.48e-05)max mem: 15.94594 GB 
[10/31 07:34:42 visual_prompt]: Inference (val):avg data time: 1.22e-04, avg batch time: 0.2327, average loss: 0.8337
[10/31 07:34:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.25	
[10/31 07:34:42 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/31 07:35:48 visual_prompt]: 	Training 100/1106. train loss: 0.4176,	0.6480 s / batch. (data: 1.62e-02). ETA=18:17:49, max mem: 15.9 GB 
[10/31 07:36:52 visual_prompt]: 	Training 200/1106. train loss: 0.3302,	0.6520 s / batch. (data: 3.65e-04). ETA=18:23:32, max mem: 15.9 GB 
[10/31 07:37:56 visual_prompt]: 	Training 300/1106. train loss: 0.9403,	0.6189 s / batch. (data: 3.46e-04). ETA=17:26:29, max mem: 15.9 GB 
[10/31 07:39:00 visual_prompt]: 	Training 400/1106. train loss: 0.7562,	0.6425 s / batch. (data: 1.11e-02). ETA=18:05:20, max mem: 15.9 GB 
[10/31 07:40:04 visual_prompt]: 	Training 500/1106. train loss: 0.4362,	0.6440 s / batch. (data: 3.61e-04). ETA=18:06:45, max mem: 15.9 GB 
[10/31 07:41:07 visual_prompt]: 	Training 600/1106. train loss: 0.7291,	0.6320 s / batch. (data: 3.47e-04). ETA=17:45:32, max mem: 15.9 GB 
[10/31 07:42:11 visual_prompt]: 	Training 700/1106. train loss: 1.0286,	0.6266 s / batch. (data: 7.98e-03). ETA=17:35:20, max mem: 15.9 GB 
[10/31 07:43:15 visual_prompt]: 	Training 800/1106. train loss: 0.8375,	0.6430 s / batch. (data: 1.20e-02). ETA=18:01:56, max mem: 15.9 GB 
[10/31 07:44:19 visual_prompt]: 	Training 900/1106. train loss: 0.7265,	0.6289 s / batch. (data: 9.11e-04). ETA=17:37:04, max mem: 15.9 GB 
[10/31 07:45:23 visual_prompt]: 	Training 1000/1106. train loss: 0.9124,	0.6400 s / batch. (data: 1.59e-02). ETA=17:54:40, max mem: 15.9 GB 
[10/31 07:46:26 visual_prompt]: 	Training 1100/1106. train loss: 0.7326,	0.6174 s / batch. (data: 1.83e-04). ETA=17:15:37, max mem: 15.9 GB 
[10/31 07:46:30 visual_prompt]: Epoch 9 / 100: avg data time: 6.37e-03, avg batch time: 0.6401, average train loss: 0.7483
[10/31 07:47:25 visual_prompt]: 	Test 100/123. loss: 0.707, 0.2437 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[10/31 07:47:36 visual_prompt]: Inference (val):avg data time: 3.05e-04, avg batch time: 0.2345, average loss: 0.7269
[10/31 07:47:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.38	
[10/31 07:47:36 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/31 07:48:43 visual_prompt]: 	Training 100/1106. train loss: 0.7179,	0.6200 s / batch. (data: 3.68e-04). ETA=17:18:56, max mem: 15.9 GB 
[10/31 07:49:47 visual_prompt]: 	Training 200/1106. train loss: 0.7380,	0.6225 s / batch. (data: 3.48e-04). ETA=17:22:03, max mem: 15.9 GB 
[10/31 07:50:50 visual_prompt]: 	Training 300/1106. train loss: 0.7519,	0.6188 s / batch. (data: 3.48e-04). ETA=17:14:52, max mem: 15.9 GB 
[10/31 07:51:54 visual_prompt]: 	Training 400/1106. train loss: 0.5335,	0.6358 s / batch. (data: 5.50e-03). ETA=17:42:20, max mem: 15.9 GB 
[10/31 07:52:58 visual_prompt]: 	Training 500/1106. train loss: 0.7523,	0.6339 s / batch. (data: 3.27e-04). ETA=17:38:01, max mem: 15.9 GB 
[10/31 07:54:01 visual_prompt]: 	Training 600/1106. train loss: 0.6000,	0.6524 s / batch. (data: 8.68e-04). ETA=18:07:48, max mem: 15.9 GB 
[10/31 07:55:05 visual_prompt]: 	Training 700/1106. train loss: 0.7062,	0.6308 s / batch. (data: 3.66e-04). ETA=17:30:45, max mem: 15.9 GB 
[10/31 07:56:09 visual_prompt]: 	Training 800/1106. train loss: 0.7031,	0.6464 s / batch. (data: 8.21e-04). ETA=17:55:36, max mem: 15.9 GB 
[10/31 07:57:12 visual_prompt]: 	Training 900/1106. train loss: 0.6864,	0.6434 s / batch. (data: 8.80e-04). ETA=17:49:33, max mem: 15.9 GB 
[10/31 07:58:16 visual_prompt]: 	Training 1000/1106. train loss: 1.3664,	0.6400 s / batch. (data: 3.66e-04). ETA=17:42:52, max mem: 15.9 GB 
[10/31 07:59:20 visual_prompt]: 	Training 1100/1106. train loss: 0.4101,	0.6174 s / batch. (data: 1.85e-04). ETA=17:04:21, max mem: 15.9 GB 
[10/31 07:59:24 visual_prompt]: Epoch 10 / 100: avg data time: 6.81e-03, avg batch time: 0.6399, average train loss: 0.7805
[10/31 08:00:18 visual_prompt]: 	Test 100/123. loss: 0.779, 0.2309 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/31 08:00:30 visual_prompt]: Inference (val):avg data time: 4.60e-05, avg batch time: 0.2324, average loss: 0.8191
[10/31 08:00:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.07	
[10/31 08:00:30 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/31 08:01:36 visual_prompt]: 	Training 100/1106. train loss: 0.5073,	0.6431 s / batch. (data: 8.80e-04). ETA=17:45:45, max mem: 15.9 GB 
[10/31 08:02:40 visual_prompt]: 	Training 200/1106. train loss: 0.6974,	0.6305 s / batch. (data: 9.54e-04). ETA=17:23:53, max mem: 15.9 GB 
[10/31 08:03:44 visual_prompt]: 	Training 300/1106. train loss: 0.7115,	0.6200 s / batch. (data: 5.21e-04). ETA=17:05:24, max mem: 15.9 GB 
[10/31 08:04:47 visual_prompt]: 	Training 400/1106. train loss: 1.2509,	0.6470 s / batch. (data: 8.92e-04). ETA=17:49:06, max mem: 15.9 GB 
[10/31 08:05:51 visual_prompt]: 	Training 500/1106. train loss: 0.8257,	0.6399 s / batch. (data: 8.53e-04). ETA=17:36:19, max mem: 15.9 GB 
[10/31 08:06:55 visual_prompt]: 	Training 600/1106. train loss: 0.5519,	0.6291 s / batch. (data: 5.60e-03). ETA=17:17:22, max mem: 15.9 GB 
[10/31 08:07:59 visual_prompt]: 	Training 700/1106. train loss: 0.8860,	0.6352 s / batch. (data: 3.65e-04). ETA=17:26:23, max mem: 15.9 GB 
[10/31 08:09:03 visual_prompt]: 	Training 800/1106. train loss: 0.6678,	0.6320 s / batch. (data: 3.76e-04). ETA=17:20:05, max mem: 15.9 GB 
[10/31 08:10:06 visual_prompt]: 	Training 900/1106. train loss: 0.6315,	0.6451 s / batch. (data: 2.15e-02). ETA=17:40:34, max mem: 15.9 GB 
[10/31 08:11:10 visual_prompt]: 	Training 1000/1106. train loss: 0.4706,	0.6447 s / batch. (data: 9.39e-04). ETA=17:38:45, max mem: 15.9 GB 
[10/31 08:12:13 visual_prompt]: 	Training 1100/1106. train loss: 0.7062,	0.6183 s / batch. (data: 2.30e-04). ETA=16:54:21, max mem: 15.9 GB 
[10/31 08:12:17 visual_prompt]: Epoch 11 / 100: avg data time: 6.36e-03, avg batch time: 0.6397, average train loss: 0.7418
[10/31 08:13:12 visual_prompt]: 	Test 100/123. loss: 0.701, 0.2341 s / batch. (data: 5.10e-05)max mem: 15.94594 GB 
[10/31 08:13:23 visual_prompt]: Inference (val):avg data time: 4.64e-05, avg batch time: 0.2333, average loss: 0.7130
[10/31 08:13:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.41	
[10/31 08:13:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/31 08:14:31 visual_prompt]: 	Training 100/1106. train loss: 0.4466,	0.6446 s / batch. (data: 1.30e-03). ETA=17:36:21, max mem: 15.9 GB 
[10/31 08:15:35 visual_prompt]: 	Training 200/1106. train loss: 0.8120,	0.6445 s / batch. (data: 9.28e-04). ETA=17:35:10, max mem: 15.9 GB 
[10/31 08:16:39 visual_prompt]: 	Training 300/1106. train loss: 0.1892,	0.6440 s / batch. (data: 8.51e-04). ETA=17:33:19, max mem: 15.9 GB 
[10/31 08:17:42 visual_prompt]: 	Training 400/1106. train loss: 0.5662,	0.6320 s / batch. (data: 3.61e-04). ETA=17:12:40, max mem: 15.9 GB 
[10/31 08:18:46 visual_prompt]: 	Training 500/1106. train loss: 0.6851,	0.6403 s / batch. (data: 6.07e-03). ETA=17:25:04, max mem: 15.9 GB 
[10/31 08:19:50 visual_prompt]: 	Training 600/1106. train loss: 0.7316,	0.6239 s / batch. (data: 3.82e-04). ETA=16:57:19, max mem: 15.9 GB 
[10/31 08:20:54 visual_prompt]: 	Training 700/1106. train loss: 0.7018,	0.6320 s / batch. (data: 3.38e-04). ETA=17:09:28, max mem: 15.9 GB 
[10/31 08:21:57 visual_prompt]: 	Training 800/1106. train loss: 0.7579,	0.6245 s / batch. (data: 3.83e-04). ETA=16:56:10, max mem: 15.9 GB 
[10/31 08:23:01 visual_prompt]: 	Training 900/1106. train loss: 0.4901,	0.6384 s / batch. (data: 3.59e-04). ETA=17:17:46, max mem: 15.9 GB 
[10/31 08:24:05 visual_prompt]: 	Training 1000/1106. train loss: 0.9693,	0.6260 s / batch. (data: 3.96e-04). ETA=16:56:38, max mem: 15.9 GB 
[10/31 08:25:08 visual_prompt]: 	Training 1100/1106. train loss: 0.7177,	0.6193 s / batch. (data: 1.65e-04). ETA=16:44:42, max mem: 15.9 GB 
[10/31 08:25:12 visual_prompt]: Epoch 12 / 100: avg data time: 7.47e-03, avg batch time: 0.6408, average train loss: 0.7506
[10/31 08:26:07 visual_prompt]: 	Test 100/123. loss: 0.852, 0.2557 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/31 08:26:18 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.2333, average loss: 0.9101
[10/31 08:26:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.54	
[10/31 08:26:18 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/31 08:27:24 visual_prompt]: 	Training 100/1106. train loss: 1.3281,	0.6264 s / batch. (data: 3.72e-04). ETA=16:54:59, max mem: 15.9 GB 
[10/31 08:28:28 visual_prompt]: 	Training 200/1106. train loss: 0.6929,	0.6395 s / batch. (data: 8.60e-04). ETA=17:15:14, max mem: 15.9 GB 
[10/31 08:29:32 visual_prompt]: 	Training 300/1106. train loss: 0.3727,	0.6317 s / batch. (data: 8.99e-04). ETA=17:01:31, max mem: 15.9 GB 
[10/31 08:30:36 visual_prompt]: 	Training 400/1106. train loss: 0.8624,	0.6302 s / batch. (data: 8.80e-04). ETA=16:58:03, max mem: 15.9 GB 
[10/31 08:31:39 visual_prompt]: 	Training 500/1106. train loss: 0.1483,	0.6287 s / batch. (data: 8.56e-04). ETA=16:54:33, max mem: 15.9 GB 
[10/31 08:32:43 visual_prompt]: 	Training 600/1106. train loss: 0.7012,	0.6218 s / batch. (data: 3.54e-04). ETA=16:42:28, max mem: 15.9 GB 
[10/31 08:33:47 visual_prompt]: 	Training 700/1106. train loss: 0.4809,	0.6473 s / batch. (data: 3.55e-04). ETA=17:22:30, max mem: 15.9 GB 
[10/31 08:34:51 visual_prompt]: 	Training 800/1106. train loss: 0.7774,	0.6390 s / batch. (data: 8.94e-04). ETA=17:08:02, max mem: 15.9 GB 
[10/31 08:35:54 visual_prompt]: 	Training 900/1106. train loss: 0.6775,	0.6406 s / batch. (data: 5.51e-03). ETA=17:09:27, max mem: 15.9 GB 
[10/31 08:36:58 visual_prompt]: 	Training 1000/1106. train loss: 0.5333,	0.6373 s / batch. (data: 8.93e-04). ETA=17:03:10, max mem: 15.9 GB 
[10/31 08:38:02 visual_prompt]: 	Training 1100/1106. train loss: 0.6897,	0.6200 s / batch. (data: 2.43e-04). ETA=16:34:19, max mem: 15.9 GB 
[10/31 08:38:06 visual_prompt]: Epoch 13 / 100: avg data time: 6.06e-03, avg batch time: 0.6397, average train loss: 0.7418
[10/31 08:39:01 visual_prompt]: 	Test 100/123. loss: 0.722, 0.2295 s / batch. (data: 5.56e-05)max mem: 15.94594 GB 
[10/31 08:39:12 visual_prompt]: Inference (val):avg data time: 4.94e-05, avg batch time: 0.2320, average loss: 0.6983
[10/31 08:39:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.33	
[10/31 08:39:12 visual_prompt]: Best epoch 13: best metric: -0.698
[10/31 08:39:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/31 08:40:19 visual_prompt]: 	Training 100/1106. train loss: 1.2705,	0.6421 s / batch. (data: 9.54e-04). ETA=17:08:38, max mem: 15.9 GB 
[10/31 08:41:23 visual_prompt]: 	Training 200/1106. train loss: 0.7040,	0.6593 s / batch. (data: 6.76e-03). ETA=17:35:07, max mem: 15.9 GB 
[10/31 08:42:27 visual_prompt]: 	Training 300/1106. train loss: 0.8141,	0.6304 s / batch. (data: 1.27e-02). ETA=16:47:50, max mem: 15.9 GB 
[10/31 08:43:30 visual_prompt]: 	Training 400/1106. train loss: 0.6753,	0.6307 s / batch. (data: 3.55e-04). ETA=16:47:16, max mem: 15.9 GB 
[10/31 08:44:34 visual_prompt]: 	Training 500/1106. train loss: 0.7997,	0.6240 s / batch. (data: 3.27e-04). ETA=16:35:30, max mem: 15.9 GB 
[10/31 08:45:38 visual_prompt]: 	Training 600/1106. train loss: 0.7137,	0.6443 s / batch. (data: 8.77e-04). ETA=17:06:50, max mem: 15.9 GB 
[10/31 08:46:42 visual_prompt]: 	Training 700/1106. train loss: 0.4018,	0.6372 s / batch. (data: 3.70e-04). ETA=16:54:26, max mem: 15.9 GB 
[10/31 08:47:45 visual_prompt]: 	Training 800/1106. train loss: 0.6951,	0.6242 s / batch. (data: 1.74e-03). ETA=16:32:40, max mem: 15.9 GB 
[10/31 08:48:49 visual_prompt]: 	Training 900/1106. train loss: 0.7527,	0.6315 s / batch. (data: 8.80e-04). ETA=16:43:19, max mem: 15.9 GB 
[10/31 08:49:53 visual_prompt]: 	Training 1000/1106. train loss: 0.2232,	0.6470 s / batch. (data: 5.51e-03). ETA=17:06:49, max mem: 15.9 GB 
[10/31 08:50:56 visual_prompt]: 	Training 1100/1106. train loss: 0.7285,	0.6182 s / batch. (data: 1.86e-04). ETA=16:20:07, max mem: 15.9 GB 
[10/31 08:51:00 visual_prompt]: Epoch 14 / 100: avg data time: 6.96e-03, avg batch time: 0.6402, average train loss: 0.7253
[10/31 08:51:55 visual_prompt]: 	Test 100/123. loss: 0.700, 0.2277 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[10/31 08:52:06 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.2328, average loss: 0.7130
[10/31 08:52:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.87	
[10/31 08:52:06 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/31 08:53:12 visual_prompt]: 	Training 100/1106. train loss: 0.6983,	0.6189 s / batch. (data: 8.45e-04). ETA=16:20:09, max mem: 15.9 GB 
[10/31 08:54:16 visual_prompt]: 	Training 200/1106. train loss: 0.7181,	0.6549 s / batch. (data: 1.03e-03). ETA=17:15:56, max mem: 15.9 GB 
[10/31 08:55:19 visual_prompt]: 	Training 300/1106. train loss: 0.7891,	0.6570 s / batch. (data: 1.02e-03). ETA=17:18:13, max mem: 15.9 GB 
[10/31 08:56:23 visual_prompt]: 	Training 400/1106. train loss: 1.4899,	0.6252 s / batch. (data: 3.55e-04). ETA=16:26:58, max mem: 15.9 GB 
[10/31 08:57:27 visual_prompt]: 	Training 500/1106. train loss: 0.3083,	0.6440 s / batch. (data: 1.20e-02). ETA=16:55:32, max mem: 15.9 GB 
[10/31 08:58:31 visual_prompt]: 	Training 600/1106. train loss: 0.7817,	0.6369 s / batch. (data: 1.72e-02). ETA=16:43:12, max mem: 15.9 GB 
[10/31 08:59:35 visual_prompt]: 	Training 700/1106. train loss: 1.0621,	0.6479 s / batch. (data: 2.46e-02). ETA=16:59:36, max mem: 15.9 GB 
[10/31 09:00:38 visual_prompt]: 	Training 800/1106. train loss: 0.7410,	0.6573 s / batch. (data: 1.11e-02). ETA=17:13:14, max mem: 15.9 GB 
[10/31 09:01:42 visual_prompt]: 	Training 900/1106. train loss: 0.7290,	0.6427 s / batch. (data: 1.02e-03). ETA=16:49:13, max mem: 15.9 GB 
[10/31 09:02:46 visual_prompt]: 	Training 1000/1106. train loss: 0.7305,	0.6520 s / batch. (data: 3.52e-04). ETA=17:02:43, max mem: 15.9 GB 
[10/31 09:03:50 visual_prompt]: 	Training 1100/1106. train loss: 0.6994,	0.6181 s / batch. (data: 2.27e-04). ETA=16:08:29, max mem: 15.9 GB 
[10/31 09:03:53 visual_prompt]: Epoch 15 / 100: avg data time: 5.91e-03, avg batch time: 0.6393, average train loss: 0.7532
[10/31 09:04:48 visual_prompt]: 	Test 100/123. loss: 0.757, 0.2397 s / batch. (data: 3.96e-05)max mem: 15.94594 GB 
[10/31 09:04:59 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.2337, average loss: 0.7217
[10/31 09:04:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.07	
[10/31 09:04:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/31 09:06:05 visual_prompt]: 	Training 100/1106. train loss: 1.1263,	0.6325 s / batch. (data: 3.87e-04). ETA=16:29:53, max mem: 15.9 GB 
[10/31 09:07:09 visual_prompt]: 	Training 200/1106. train loss: 0.7866,	0.6379 s / batch. (data: 9.86e-04). ETA=16:37:24, max mem: 15.9 GB 
[10/31 09:08:12 visual_prompt]: 	Training 300/1106. train loss: 0.9665,	0.6400 s / batch. (data: 9.27e-04). ETA=16:39:32, max mem: 15.9 GB 
[10/31 09:09:16 visual_prompt]: 	Training 400/1106. train loss: 0.7550,	0.6503 s / batch. (data: 2.70e-02). ETA=16:54:33, max mem: 15.9 GB 
[10/31 09:10:19 visual_prompt]: 	Training 500/1106. train loss: 0.6965,	0.6259 s / batch. (data: 3.45e-04). ETA=16:15:31, max mem: 15.9 GB 
[10/31 09:11:23 visual_prompt]: 	Training 600/1106. train loss: 1.1778,	0.6302 s / batch. (data: 1.20e-02). ETA=16:21:10, max mem: 15.9 GB 
[10/31 09:12:27 visual_prompt]: 	Training 700/1106. train loss: 0.6992,	0.6519 s / batch. (data: 3.36e-02). ETA=16:53:51, max mem: 15.9 GB 
[10/31 09:13:30 visual_prompt]: 	Training 800/1106. train loss: 0.7398,	0.6360 s / batch. (data: 3.76e-04). ETA=16:27:58, max mem: 15.9 GB 
[10/31 09:14:34 visual_prompt]: 	Training 900/1106. train loss: 0.7087,	0.6359 s / batch. (data: 8.69e-04). ETA=16:26:49, max mem: 15.9 GB 
[10/31 09:15:38 visual_prompt]: 	Training 1000/1106. train loss: 1.1233,	0.6404 s / batch. (data: 1.11e-03). ETA=16:32:40, max mem: 15.9 GB 
[10/31 09:16:42 visual_prompt]: 	Training 1100/1106. train loss: 0.3451,	0.6177 s / batch. (data: 2.52e-04). ETA=15:56:31, max mem: 15.9 GB 
[10/31 09:16:46 visual_prompt]: Epoch 16 / 100: avg data time: 5.56e-03, avg batch time: 0.6387, average train loss: 0.7272
[10/31 09:17:41 visual_prompt]: 	Test 100/123. loss: 0.729, 0.2365 s / batch. (data: 5.79e-05)max mem: 15.94594 GB 
[10/31 09:17:52 visual_prompt]: Inference (val):avg data time: 5.07e-05, avg batch time: 0.2326, average loss: 0.7030
[10/31 09:17:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.98	
[10/31 09:17:52 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/31 09:18:58 visual_prompt]: 	Training 100/1106. train loss: 0.6168,	0.6622 s / batch. (data: 2.20e-02). ETA=17:04:15, max mem: 15.9 GB 
[10/31 09:20:02 visual_prompt]: 	Training 200/1106. train loss: 0.7672,	0.6506 s / batch. (data: 9.42e-04). ETA=16:45:15, max mem: 15.9 GB 
[10/31 09:21:06 visual_prompt]: 	Training 300/1106. train loss: 0.7754,	0.6504 s / batch. (data: 8.89e-04). ETA=16:43:53, max mem: 15.9 GB 
[10/31 09:22:09 visual_prompt]: 	Training 400/1106. train loss: 1.0011,	0.6360 s / batch. (data: 3.54e-04). ETA=16:20:31, max mem: 15.9 GB 
[10/31 09:23:13 visual_prompt]: 	Training 500/1106. train loss: 0.4145,	0.6440 s / batch. (data: 8.92e-04). ETA=16:31:47, max mem: 15.9 GB 
[10/31 09:24:17 visual_prompt]: 	Training 600/1106. train loss: 0.8520,	0.6400 s / batch. (data: 3.52e-04). ETA=16:24:33, max mem: 15.9 GB 
[10/31 09:25:20 visual_prompt]: 	Training 700/1106. train loss: 0.7040,	0.6455 s / batch. (data: 8.95e-04). ETA=16:32:01, max mem: 15.9 GB 
[10/31 09:26:24 visual_prompt]: 	Training 800/1106. train loss: 0.7812,	0.6304 s / batch. (data: 5.54e-03). ETA=16:07:41, max mem: 15.9 GB 
[10/31 09:27:28 visual_prompt]: 	Training 900/1106. train loss: 0.3241,	0.6340 s / batch. (data: 3.54e-04). ETA=16:12:07, max mem: 15.9 GB 
[10/31 09:28:31 visual_prompt]: 	Training 1000/1106. train loss: 0.2552,	0.6522 s / batch. (data: 6.08e-03). ETA=16:38:57, max mem: 15.9 GB 
[10/31 09:29:35 visual_prompt]: 	Training 1100/1106. train loss: 0.2722,	0.6193 s / batch. (data: 2.26e-04). ETA=15:47:31, max mem: 15.9 GB 
[10/31 09:29:39 visual_prompt]: Epoch 17 / 100: avg data time: 6.02e-03, avg batch time: 0.6392, average train loss: 0.7270
[10/31 09:30:33 visual_prompt]: 	Test 100/123. loss: 0.748, 0.2326 s / batch. (data: 5.89e-05)max mem: 15.94594 GB 
[10/31 09:30:45 visual_prompt]: Inference (val):avg data time: 1.13e-04, avg batch time: 0.2332, average loss: 0.7154
[10/31 09:30:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.63	
[10/31 09:30:45 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/31 09:31:52 visual_prompt]: 	Training 100/1106. train loss: 0.4014,	0.6465 s / batch. (data: 1.11e-03). ETA=16:27:59, max mem: 15.9 GB 
[10/31 09:32:55 visual_prompt]: 	Training 200/1106. train loss: 0.8431,	0.6182 s / batch. (data: 3.46e-04). ETA=15:43:50, max mem: 15.9 GB 
[10/31 09:33:59 visual_prompt]: 	Training 300/1106. train loss: 1.7478,	0.6600 s / batch. (data: 8.65e-04). ETA=16:46:29, max mem: 15.9 GB 
[10/31 09:35:03 visual_prompt]: 	Training 400/1106. train loss: 0.7209,	0.6400 s / batch. (data: 3.54e-04). ETA=16:14:55, max mem: 15.9 GB 
[10/31 09:36:06 visual_prompt]: 	Training 500/1106. train loss: 0.1966,	0.6382 s / batch. (data: 8.54e-04). ETA=16:11:02, max mem: 15.9 GB 
[10/31 09:37:10 visual_prompt]: 	Training 600/1106. train loss: 0.4794,	0.6406 s / batch. (data: 1.26e-02). ETA=16:13:43, max mem: 15.9 GB 
[10/31 09:38:14 visual_prompt]: 	Training 700/1106. train loss: 0.6493,	0.6549 s / batch. (data: 8.55e-04). ETA=16:34:21, max mem: 15.9 GB 
[10/31 09:39:17 visual_prompt]: 	Training 800/1106. train loss: 0.7003,	0.6589 s / batch. (data: 5.94e-03). ETA=16:39:18, max mem: 15.9 GB 
[10/31 09:40:21 visual_prompt]: 	Training 900/1106. train loss: 1.2688,	0.6400 s / batch. (data: 8.31e-04). ETA=16:09:34, max mem: 15.9 GB 
[10/31 09:41:25 visual_prompt]: 	Training 1000/1106. train loss: 0.7985,	0.6346 s / batch. (data: 8.54e-04). ETA=16:00:19, max mem: 15.9 GB 
[10/31 09:42:29 visual_prompt]: 	Training 1100/1106. train loss: 0.5763,	0.6181 s / batch. (data: 2.08e-04). ETA=15:34:24, max mem: 15.9 GB 
[10/31 09:42:32 visual_prompt]: Epoch 18 / 100: avg data time: 6.27e-03, avg batch time: 0.6396, average train loss: 0.7903
[10/31 09:43:27 visual_prompt]: 	Test 100/123. loss: 0.743, 0.2284 s / batch. (data: 5.32e-05)max mem: 15.94594 GB 
[10/31 09:43:39 visual_prompt]: Inference (val):avg data time: 5.61e-05, avg batch time: 0.2324, average loss: 0.7119
[10/31 09:43:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.24	
[10/31 09:43:39 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[10/31 09:44:46 visual_prompt]: 	Training 100/1106. train loss: 0.1740,	0.6343 s / batch. (data: 1.07e-02). ETA=15:57:38, max mem: 15.9 GB 
[10/31 09:45:49 visual_prompt]: 	Training 200/1106. train loss: 0.7402,	0.6273 s / batch. (data: 3.62e-04). ETA=15:46:07, max mem: 15.9 GB 
[10/31 09:46:53 visual_prompt]: 	Training 300/1106. train loss: 0.4750,	0.6304 s / batch. (data: 3.48e-04). ETA=15:49:42, max mem: 15.9 GB 
[10/31 09:47:57 visual_prompt]: 	Training 400/1106. train loss: 0.7317,	0.6192 s / batch. (data: 3.78e-04). ETA=15:31:47, max mem: 15.9 GB 
[10/31 09:49:00 visual_prompt]: 	Training 500/1106. train loss: 0.3491,	0.6278 s / batch. (data: 1.02e-02). ETA=15:43:44, max mem: 15.9 GB 
[10/31 09:50:04 visual_prompt]: 	Training 600/1106. train loss: 0.8797,	0.6485 s / batch. (data: 3.44e-04). ETA=16:13:42, max mem: 15.9 GB 
[10/31 09:51:08 visual_prompt]: 	Training 700/1106. train loss: 0.7057,	0.6395 s / batch. (data: 8.42e-04). ETA=15:59:07, max mem: 15.9 GB 
[10/31 09:52:11 visual_prompt]: 	Training 800/1106. train loss: 0.2182,	0.6327 s / batch. (data: 8.61e-04). ETA=15:47:55, max mem: 15.9 GB 
[10/31 09:53:15 visual_prompt]: 	Training 900/1106. train loss: 0.4606,	0.6281 s / batch. (data: 3.55e-04). ETA=15:39:59, max mem: 15.9 GB 
[10/31 09:54:18 visual_prompt]: 	Training 1000/1106. train loss: 1.0317,	0.6479 s / batch. (data: 1.79e-02). ETA=16:08:33, max mem: 15.9 GB 
[10/31 09:55:22 visual_prompt]: 	Training 1100/1106. train loss: 1.1101,	0.6427 s / batch. (data: 2.15e-04). ETA=15:59:40, max mem: 15.9 GB 
[10/31 09:55:26 visual_prompt]: Epoch 19 / 100: avg data time: 7.74e-03, avg batch time: 0.6394, average train loss: 0.7434
[10/31 09:56:20 visual_prompt]: 	Test 100/123. loss: 0.837, 0.2287 s / batch. (data: 5.87e-05)max mem: 15.94594 GB 
[10/31 09:56:32 visual_prompt]: Inference (val):avg data time: 5.05e-05, avg batch time: 0.2337, average loss: 0.7837
[10/31 09:56:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.35	
[10/31 09:56:32 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[10/31 09:57:39 visual_prompt]: 	Training 100/1106. train loss: 0.7002,	0.6384 s / batch. (data: 1.20e-02). ETA=15:52:11, max mem: 15.9 GB 
[10/31 09:58:43 visual_prompt]: 	Training 200/1106. train loss: 0.5773,	0.6534 s / batch. (data: 6.00e-03). ETA=16:13:29, max mem: 15.9 GB 
[10/31 09:59:46 visual_prompt]: 	Training 300/1106. train loss: 0.7653,	0.6360 s / batch. (data: 3.57e-04). ETA=15:46:27, max mem: 15.9 GB 
[10/31 10:00:50 visual_prompt]: 	Training 400/1106. train loss: 0.7605,	0.6195 s / batch. (data: 3.68e-04). ETA=15:20:50, max mem: 15.9 GB 
[10/31 10:01:54 visual_prompt]: 	Training 500/1106. train loss: 0.7629,	0.6490 s / batch. (data: 1.38e-02). ETA=16:03:38, max mem: 15.9 GB 
[10/31 10:02:58 visual_prompt]: 	Training 600/1106. train loss: 0.7318,	0.6453 s / batch. (data: 8.69e-04). ETA=15:57:00, max mem: 15.9 GB 
[10/31 10:04:01 visual_prompt]: 	Training 700/1106. train loss: 0.5542,	0.6560 s / batch. (data: 6.48e-03). ETA=16:11:45, max mem: 15.9 GB 
[10/31 10:05:05 visual_prompt]: 	Training 800/1106. train loss: 0.2829,	0.6192 s / batch. (data: 8.44e-04). ETA=15:16:17, max mem: 15.9 GB 
[10/31 10:06:08 visual_prompt]: 	Training 900/1106. train loss: 0.8548,	0.6320 s / batch. (data: 3.55e-04). ETA=15:34:08, max mem: 15.9 GB 
[10/31 10:07:12 visual_prompt]: 	Training 1000/1106. train loss: 0.4373,	0.6373 s / batch. (data: 1.60e-02). ETA=15:40:53, max mem: 15.9 GB 
[10/31 10:08:16 visual_prompt]: 	Training 1100/1106. train loss: 0.6529,	0.6185 s / batch. (data: 2.75e-04). ETA=15:12:09, max mem: 15.9 GB 
[10/31 10:08:20 visual_prompt]: Epoch 20 / 100: avg data time: 6.98e-03, avg batch time: 0.6398, average train loss: 0.7413
[10/31 10:09:14 visual_prompt]: 	Test 100/123. loss: 0.780, 0.2245 s / batch. (data: 2.74e-05)max mem: 15.94594 GB 
[10/31 10:09:26 visual_prompt]: Inference (val):avg data time: 4.84e-05, avg batch time: 0.2322, average loss: 0.8213
[10/31 10:09:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.23	
[10/31 10:09:26 visual_prompt]: Stopping early.
[10/31 10:09:26 visual_prompt]: Rank of current process: 0. World size: 1
[10/31 10:09:26 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/31 10:09:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/31 10:09:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/31 10:09:26 visual_prompt]: Training with config:
[10/31 10:09:26 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.05_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/31 10:09:26 visual_prompt]: Loading training data...
[10/31 10:09:26 visual_prompt]: Constructing mammo-cbis dataset train...
[10/31 10:09:26 visual_prompt]: Loading validation data...
[10/31 10:09:26 visual_prompt]: Constructing mammo-cbis dataset val...
[10/31 10:09:26 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/31 10:09:29 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/31 10:09:29 visual_prompt]: tuned percent:0.522
[10/31 10:09:29 visual_prompt]: Device used for model: 0
[10/31 10:09:29 visual_prompt]: Setting up Evaluator...
[10/31 10:09:29 visual_prompt]: Setting up Trainer...
[10/31 10:09:29 visual_prompt]: 	Setting up the optimizer...
[10/31 10:09:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/31 10:10:35 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6572 s / batch. (data: 1.13e-03). ETA=20:10:23, max mem: 15.9 GB 
[10/31 10:11:39 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6204 s / batch. (data: 3.77e-04). ETA=19:01:35, max mem: 15.9 GB 
[10/31 10:12:42 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6523 s / batch. (data: 8.67e-04). ETA=19:59:12, max mem: 15.9 GB 
[10/31 10:13:46 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6308 s / batch. (data: 8.82e-04). ETA=19:18:32, max mem: 15.9 GB 
[10/31 10:14:50 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6560 s / batch. (data: 1.61e-02). ETA=20:03:49, max mem: 15.9 GB 
[10/31 10:15:54 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6376 s / batch. (data: 6.02e-03). ETA=19:28:57, max mem: 15.9 GB 
[10/31 10:16:58 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6477 s / batch. (data: 9.18e-04). ETA=19:46:23, max mem: 15.9 GB 
[10/31 10:18:02 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6301 s / batch. (data: 1.06e-02). ETA=19:13:03, max mem: 15.9 GB 
[10/31 10:19:05 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6360 s / batch. (data: 3.48e-04). ETA=19:22:51, max mem: 15.9 GB 
[10/31 10:20:09 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6337 s / batch. (data: 8.43e-04). ETA=19:17:35, max mem: 15.9 GB 
[10/31 10:21:13 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6190 s / batch. (data: 2.12e-04). ETA=18:49:39, max mem: 15.9 GB 
[10/31 10:21:17 visual_prompt]: Epoch 1 / 100: avg data time: 6.14e-03, avg batch time: 0.6400, average train loss: 1.4028
[10/31 10:22:11 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2451 s / batch. (data: 4.29e-05)max mem: 15.94594 GB 
[10/31 10:22:23 visual_prompt]: Inference (val):avg data time: 4.67e-05, avg batch time: 0.2325, average loss: 1.3505
[10/31 10:22:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/31 10:22:23 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/31 10:23:29 visual_prompt]: 	Training 100/1106. train loss: 0.6521,	0.6360 s / batch. (data: 3.55e-04). ETA=19:19:33, max mem: 15.9 GB 
[10/31 10:24:33 visual_prompt]: 	Training 200/1106. train loss: 0.7177,	0.6296 s / batch. (data: 1.06e-02). ETA=19:06:52, max mem: 15.9 GB 
[10/31 10:25:36 visual_prompt]: 	Training 300/1106. train loss: 0.9955,	0.6352 s / batch. (data: 1.06e-02). ETA=19:16:02, max mem: 15.9 GB 
[10/31 10:26:40 visual_prompt]: 	Training 400/1106. train loss: 0.1142,	0.6316 s / batch. (data: 1.28e-02). ETA=19:08:24, max mem: 15.9 GB 
[10/31 10:27:44 visual_prompt]: 	Training 500/1106. train loss: 0.7279,	0.6321 s / batch. (data: 7.97e-03). ETA=19:08:16, max mem: 15.9 GB 
[10/31 10:28:48 visual_prompt]: 	Training 600/1106. train loss: 0.5402,	0.6422 s / batch. (data: 3.85e-04). ETA=19:25:36, max mem: 15.9 GB 
[10/31 10:29:51 visual_prompt]: 	Training 700/1106. train loss: 0.7548,	0.6353 s / batch. (data: 9.64e-04). ETA=19:12:01, max mem: 15.9 GB 
[10/31 10:30:55 visual_prompt]: 	Training 800/1106. train loss: 0.7634,	0.6280 s / batch. (data: 3.65e-04). ETA=18:57:41, max mem: 15.9 GB 
[10/31 10:31:58 visual_prompt]: 	Training 900/1106. train loss: 0.6604,	0.6288 s / batch. (data: 8.47e-04). ETA=18:57:59, max mem: 15.9 GB 
[10/31 10:33:02 visual_prompt]: 	Training 1000/1106. train loss: 0.4841,	0.6309 s / batch. (data: 3.56e-04). ETA=19:00:53, max mem: 15.9 GB 
[10/31 10:34:05 visual_prompt]: 	Training 1100/1106. train loss: 0.7329,	0.6173 s / batch. (data: 1.94e-04). ETA=18:35:14, max mem: 15.9 GB 
[10/31 10:34:09 visual_prompt]: Epoch 2 / 100: avg data time: 5.82e-03, avg batch time: 0.6386, average train loss: 0.7723
[10/31 10:35:04 visual_prompt]: 	Test 100/123. loss: 1.046, 0.2357 s / batch. (data: 3.60e-05)max mem: 15.94594 GB 
[10/31 10:35:15 visual_prompt]: Inference (val):avg data time: 4.62e-05, avg batch time: 0.2334, average loss: 0.9228
[10/31 10:35:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.84	
[10/31 10:35:15 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/31 10:36:22 visual_prompt]: 	Training 100/1106. train loss: 0.6736,	0.6177 s / batch. (data: 3.53e-04). ETA=18:34:50, max mem: 15.9 GB 
[10/31 10:37:26 visual_prompt]: 	Training 200/1106. train loss: 0.3152,	0.6549 s / batch. (data: 1.12e-02). ETA=19:40:47, max mem: 15.9 GB 
[10/31 10:38:30 visual_prompt]: 	Training 300/1106. train loss: 0.3381,	0.6400 s / batch. (data: 8.38e-04). ETA=19:12:56, max mem: 15.9 GB 
[10/31 10:39:33 visual_prompt]: 	Training 400/1106. train loss: 0.7652,	0.6334 s / batch. (data: 5.66e-03). ETA=19:00:00, max mem: 15.9 GB 
[10/31 10:40:37 visual_prompt]: 	Training 500/1106. train loss: 1.3852,	0.6384 s / batch. (data: 8.89e-04). ETA=19:07:59, max mem: 15.9 GB 
[10/31 10:41:41 visual_prompt]: 	Training 600/1106. train loss: 0.6544,	0.6340 s / batch. (data: 3.63e-04). ETA=18:58:58, max mem: 15.9 GB 
[10/31 10:42:45 visual_prompt]: 	Training 700/1106. train loss: 0.8252,	0.6461 s / batch. (data: 9.24e-04). ETA=19:19:40, max mem: 15.9 GB 
[10/31 10:43:49 visual_prompt]: 	Training 800/1106. train loss: 1.3520,	0.6200 s / batch. (data: 8.51e-04). ETA=18:31:43, max mem: 15.9 GB 
[10/31 10:44:52 visual_prompt]: 	Training 900/1106. train loss: 1.2172,	0.6466 s / batch. (data: 3.27e-04). ETA=19:18:19, max mem: 15.9 GB 
[10/31 10:45:56 visual_prompt]: 	Training 1000/1106. train loss: 0.6742,	0.6281 s / batch. (data: 7.98e-03). ETA=18:44:12, max mem: 15.9 GB 
[10/31 10:47:00 visual_prompt]: 	Training 1100/1106. train loss: 0.5525,	0.6183 s / batch. (data: 2.06e-04). ETA=18:25:38, max mem: 15.9 GB 
[10/31 10:47:04 visual_prompt]: Epoch 3 / 100: avg data time: 6.59e-03, avg batch time: 0.6402, average train loss: 0.7684
[10/31 10:47:58 visual_prompt]: 	Test 100/123. loss: 0.820, 0.2277 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/31 10:48:10 visual_prompt]: Inference (val):avg data time: 4.96e-05, avg batch time: 0.2328, average loss: 0.7891
[10/31 10:48:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.60	
[10/31 10:48:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/31 10:49:16 visual_prompt]: 	Training 100/1106. train loss: 1.2430,	0.6387 s / batch. (data: 8.74e-04). ETA=19:00:59, max mem: 15.9 GB 
[10/31 10:50:20 visual_prompt]: 	Training 200/1106. train loss: 1.0770,	0.6403 s / batch. (data: 3.89e-04). ETA=19:02:44, max mem: 15.9 GB 
[10/31 10:51:23 visual_prompt]: 	Training 300/1106. train loss: 0.6870,	0.6248 s / batch. (data: 5.54e-03). ETA=18:34:03, max mem: 15.9 GB 
[10/31 10:52:27 visual_prompt]: 	Training 400/1106. train loss: 0.6441,	0.6500 s / batch. (data: 1.59e-02). ETA=19:17:56, max mem: 15.9 GB 
[10/31 10:53:31 visual_prompt]: 	Training 500/1106. train loss: 2.2655,	0.6576 s / batch. (data: 3.74e-02). ETA=19:30:18, max mem: 15.9 GB 
[10/31 10:54:35 visual_prompt]: 	Training 600/1106. train loss: 0.1533,	0.6607 s / batch. (data: 6.05e-03). ETA=19:34:42, max mem: 15.9 GB 
[10/31 10:55:38 visual_prompt]: 	Training 700/1106. train loss: 1.2892,	0.6396 s / batch. (data: 4.09e-04). ETA=18:56:06, max mem: 15.9 GB 
[10/31 10:56:42 visual_prompt]: 	Training 800/1106. train loss: 0.1292,	0.6405 s / batch. (data: 8.71e-04). ETA=18:56:45, max mem: 15.9 GB 
[10/31 10:57:46 visual_prompt]: 	Training 900/1106. train loss: 0.9255,	0.6249 s / batch. (data: 3.66e-04). ETA=18:27:57, max mem: 15.9 GB 
[10/31 10:58:49 visual_prompt]: 	Training 1000/1106. train loss: 0.5610,	0.6345 s / batch. (data: 7.92e-03). ETA=18:43:53, max mem: 15.9 GB 
[10/31 10:59:53 visual_prompt]: 	Training 1100/1106. train loss: 0.7328,	0.6193 s / batch. (data: 1.60e-04). ETA=18:15:58, max mem: 15.9 GB 
[10/31 10:59:57 visual_prompt]: Epoch 4 / 100: avg data time: 5.75e-03, avg batch time: 0.6396, average train loss: 0.7869
[10/31 11:00:51 visual_prompt]: 	Test 100/123. loss: 0.959, 0.2287 s / batch. (data: 5.65e-05)max mem: 15.94594 GB 
[10/31 11:01:03 visual_prompt]: Inference (val):avg data time: 4.87e-05, avg batch time: 0.2325, average loss: 0.8548
[10/31 11:01:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.17	
[10/31 11:01:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/31 11:02:10 visual_prompt]: 	Training 100/1106. train loss: 1.2507,	0.6305 s / batch. (data: 9.83e-04). ETA=18:34:41, max mem: 15.9 GB 
[10/31 11:03:13 visual_prompt]: 	Training 200/1106. train loss: 1.1092,	0.6660 s / batch. (data: 3.79e-02). ETA=19:36:17, max mem: 15.9 GB 
[10/31 11:04:17 visual_prompt]: 	Training 300/1106. train loss: 0.7349,	0.6435 s / batch. (data: 9.02e-04). ETA=18:55:31, max mem: 15.9 GB 
[10/31 11:05:21 visual_prompt]: 	Training 400/1106. train loss: 0.9216,	0.6400 s / batch. (data: 3.70e-04). ETA=18:48:19, max mem: 15.9 GB 
[10/31 11:06:24 visual_prompt]: 	Training 500/1106. train loss: 0.1116,	0.6331 s / batch. (data: 8.56e-04). ETA=18:35:08, max mem: 15.9 GB 
[10/31 11:07:28 visual_prompt]: 	Training 600/1106. train loss: 1.7718,	0.6178 s / batch. (data: 3.64e-04). ETA=18:07:02, max mem: 15.9 GB 
[10/31 11:08:32 visual_prompt]: 	Training 700/1106. train loss: 0.6548,	0.6495 s / batch. (data: 6.18e-03). ETA=19:01:50, max mem: 15.9 GB 
[10/31 11:09:35 visual_prompt]: 	Training 800/1106. train loss: 1.1618,	0.6280 s / batch. (data: 3.55e-04). ETA=18:22:56, max mem: 15.9 GB 
[10/31 11:10:39 visual_prompt]: 	Training 900/1106. train loss: 0.8091,	0.6449 s / batch. (data: 6.04e-03). ETA=18:51:31, max mem: 15.9 GB 
[10/31 11:11:43 visual_prompt]: 	Training 1000/1106. train loss: 0.3136,	0.6512 s / batch. (data: 8.62e-04). ETA=19:01:30, max mem: 15.9 GB 
[10/31 11:12:47 visual_prompt]: 	Training 1100/1106. train loss: 0.9312,	0.6195 s / batch. (data: 2.32e-04). ETA=18:04:50, max mem: 15.9 GB 
[10/31 11:12:51 visual_prompt]: Epoch 5 / 100: avg data time: 6.37e-03, avg batch time: 0.6394, average train loss: 0.7965
[10/31 11:13:45 visual_prompt]: 	Test 100/123. loss: 0.954, 0.2345 s / batch. (data: 5.56e-05)max mem: 15.94594 GB 
[10/31 11:13:56 visual_prompt]: Inference (val):avg data time: 4.91e-05, avg batch time: 0.2336, average loss: 0.9521
[10/31 11:13:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.05	
[10/31 11:13:56 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/31 11:15:02 visual_prompt]: 	Training 100/1106. train loss: 0.8128,	0.6331 s / batch. (data: 3.77e-04). ETA=18:27:31, max mem: 15.9 GB 
[10/31 11:16:06 visual_prompt]: 	Training 200/1106. train loss: 0.4556,	0.6225 s / batch. (data: 3.44e-04). ETA=18:08:00, max mem: 15.9 GB 
[10/31 11:17:10 visual_prompt]: 	Training 300/1106. train loss: 0.0977,	0.6360 s / batch. (data: 1.11e-03). ETA=18:30:29, max mem: 15.9 GB 
[10/31 11:18:13 visual_prompt]: 	Training 400/1106. train loss: 0.8158,	0.6368 s / batch. (data: 3.79e-04). ETA=18:30:49, max mem: 15.9 GB 
[10/31 11:19:17 visual_prompt]: 	Training 500/1106. train loss: 0.7895,	0.6280 s / batch. (data: 3.81e-04). ETA=18:14:31, max mem: 15.9 GB 
[10/31 11:20:21 visual_prompt]: 	Training 600/1106. train loss: 0.5924,	0.6498 s / batch. (data: 1.59e-02). ETA=18:51:20, max mem: 15.9 GB 
[10/31 11:21:25 visual_prompt]: 	Training 700/1106. train loss: 0.7618,	0.6193 s / batch. (data: 3.43e-04). ETA=17:57:14, max mem: 15.9 GB 
[10/31 11:22:29 visual_prompt]: 	Training 800/1106. train loss: 0.7726,	0.6280 s / batch. (data: 3.68e-04). ETA=18:11:21, max mem: 15.9 GB 
[10/31 11:23:32 visual_prompt]: 	Training 900/1106. train loss: 0.3919,	0.6572 s / batch. (data: 3.56e-04). ETA=19:01:05, max mem: 15.9 GB 
[10/31 11:24:36 visual_prompt]: 	Training 1000/1106. train loss: 2.1374,	0.6440 s / batch. (data: 8.85e-04). ETA=18:36:59, max mem: 15.9 GB 
[10/31 11:25:40 visual_prompt]: 	Training 1100/1106. train loss: 0.6128,	0.6191 s / batch. (data: 2.44e-04). ETA=17:52:47, max mem: 15.9 GB 
[10/31 11:25:44 visual_prompt]: Epoch 6 / 100: avg data time: 5.54e-03, avg batch time: 0.6392, average train loss: 0.7776
[10/31 11:26:38 visual_prompt]: 	Test 100/123. loss: 0.860, 0.2267 s / batch. (data: 5.13e-05)max mem: 15.94594 GB 
[10/31 11:26:49 visual_prompt]: Inference (val):avg data time: 5.00e-05, avg batch time: 0.2324, average loss: 0.7722
[10/31 11:26:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.76	
[10/31 11:26:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/31 11:27:55 visual_prompt]: 	Training 100/1106. train loss: 0.6730,	0.6228 s / batch. (data: 5.58e-03). ETA=17:58:07, max mem: 15.9 GB 
[10/31 11:28:59 visual_prompt]: 	Training 200/1106. train loss: 0.9190,	0.6459 s / batch. (data: 1.11e-02). ETA=18:37:02, max mem: 15.9 GB 
[10/31 11:30:02 visual_prompt]: 	Training 300/1106. train loss: 0.7803,	0.6360 s / batch. (data: 3.53e-04). ETA=18:18:49, max mem: 15.9 GB 
[10/31 11:31:06 visual_prompt]: 	Training 400/1106. train loss: 1.0611,	0.6507 s / batch. (data: 6.03e-03). ETA=18:43:07, max mem: 15.9 GB 
[10/31 11:32:10 visual_prompt]: 	Training 500/1106. train loss: 0.6466,	0.6444 s / batch. (data: 5.52e-03). ETA=18:31:15, max mem: 15.9 GB 
[10/31 11:33:14 visual_prompt]: 	Training 600/1106. train loss: 0.6907,	0.6429 s / batch. (data: 8.64e-04). ETA=18:27:35, max mem: 15.9 GB 
[10/31 11:34:18 visual_prompt]: 	Training 700/1106. train loss: 0.3743,	0.6262 s / batch. (data: 5.53e-03). ETA=17:57:43, max mem: 15.9 GB 
[10/31 11:35:22 visual_prompt]: 	Training 800/1106. train loss: 0.2990,	0.6309 s / batch. (data: 8.83e-04). ETA=18:04:41, max mem: 15.9 GB 
[10/31 11:36:25 visual_prompt]: 	Training 900/1106. train loss: 0.7203,	0.6337 s / batch. (data: 8.98e-04). ETA=18:08:33, max mem: 15.9 GB 
[10/31 11:37:29 visual_prompt]: 	Training 1000/1106. train loss: 0.8429,	0.6472 s / batch. (data: 8.73e-04). ETA=18:30:39, max mem: 15.9 GB 
[10/31 11:38:33 visual_prompt]: 	Training 1100/1106. train loss: 0.4272,	0.6178 s / batch. (data: 2.24e-04). ETA=17:39:04, max mem: 15.9 GB 
[10/31 11:38:37 visual_prompt]: Epoch 7 / 100: avg data time: 5.22e-03, avg batch time: 0.6393, average train loss: 0.7703
[10/31 11:39:31 visual_prompt]: 	Test 100/123. loss: 0.786, 0.2348 s / batch. (data: 6.89e-05)max mem: 15.94594 GB 
[10/31 11:39:43 visual_prompt]: Inference (val):avg data time: 4.82e-05, avg batch time: 0.2331, average loss: 0.7213
[10/31 11:39:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.49	
[10/31 11:39:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/31 11:40:49 visual_prompt]: 	Training 100/1106. train loss: 0.6948,	0.6531 s / batch. (data: 1.31e-02). ETA=18:38:28, max mem: 15.9 GB 
[10/31 11:41:53 visual_prompt]: 	Training 200/1106. train loss: 0.8170,	0.6178 s / batch. (data: 5.22e-04). ETA=17:37:01, max mem: 15.9 GB 
[10/31 11:42:56 visual_prompt]: 	Training 300/1106. train loss: 0.2462,	0.6519 s / batch. (data: 2.90e-02). ETA=18:34:13, max mem: 15.9 GB 
[10/31 11:44:00 visual_prompt]: 	Training 400/1106. train loss: 1.3872,	0.6337 s / batch. (data: 5.57e-03). ETA=18:02:10, max mem: 15.9 GB 
[10/31 11:45:04 visual_prompt]: 	Training 500/1106. train loss: 0.3694,	0.6324 s / batch. (data: 8.56e-04). ETA=17:58:48, max mem: 15.9 GB 
[10/31 11:46:07 visual_prompt]: 	Training 600/1106. train loss: 1.1452,	0.6510 s / batch. (data: 6.03e-03). ETA=18:29:26, max mem: 15.9 GB 
[10/31 11:47:11 visual_prompt]: 	Training 700/1106. train loss: 0.9829,	0.6639 s / batch. (data: 4.08e-02). ETA=18:50:25, max mem: 15.9 GB 
[10/31 11:48:15 visual_prompt]: 	Training 800/1106. train loss: 0.9251,	0.6238 s / batch. (data: 3.43e-04). ETA=17:41:01, max mem: 15.9 GB 
[10/31 11:49:18 visual_prompt]: 	Training 900/1106. train loss: 0.2230,	0.6313 s / batch. (data: 3.69e-04). ETA=17:52:42, max mem: 15.9 GB 
[10/31 11:50:22 visual_prompt]: 	Training 1000/1106. train loss: 1.4853,	0.6314 s / batch. (data: 8.83e-04). ETA=17:51:57, max mem: 15.9 GB 
[10/31 11:51:26 visual_prompt]: 	Training 1100/1106. train loss: 0.6046,	0.6195 s / batch. (data: 1.91e-04). ETA=17:30:37, max mem: 15.9 GB 
[10/31 11:51:30 visual_prompt]: Epoch 8 / 100: avg data time: 6.35e-03, avg batch time: 0.6394, average train loss: 0.8200
[10/31 11:52:25 visual_prompt]: 	Test 100/123. loss: 0.879, 0.2251 s / batch. (data: 3.15e-05)max mem: 15.94594 GB 
[10/31 11:52:36 visual_prompt]: Inference (val):avg data time: 4.78e-05, avg batch time: 0.2327, average loss: 0.8663
[10/31 11:52:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.40	
[10/31 11:52:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/31 11:53:43 visual_prompt]: 	Training 100/1106. train loss: 0.2897,	0.6262 s / batch. (data: 6.07e-03). ETA=17:40:50, max mem: 15.9 GB 
[10/31 11:54:46 visual_prompt]: 	Training 200/1106. train loss: 0.2177,	0.6480 s / batch. (data: 5.50e-03). ETA=18:16:45, max mem: 15.9 GB 
[10/31 11:55:50 visual_prompt]: 	Training 300/1106. train loss: 1.0373,	0.6187 s / batch. (data: 3.40e-04). ETA=17:26:07, max mem: 15.9 GB 
[10/31 11:56:54 visual_prompt]: 	Training 400/1106. train loss: 0.7067,	0.6359 s / batch. (data: 3.51e-04). ETA=17:54:07, max mem: 15.9 GB 
[10/31 11:57:57 visual_prompt]: 	Training 500/1106. train loss: 0.7300,	0.6211 s / batch. (data: 3.49e-04). ETA=17:28:09, max mem: 15.9 GB 
[10/31 11:59:01 visual_prompt]: 	Training 600/1106. train loss: 0.6693,	0.6468 s / batch. (data: 6.06e-03). ETA=18:10:24, max mem: 15.9 GB 
[10/31 12:00:05 visual_prompt]: 	Training 700/1106. train loss: 1.4163,	0.6216 s / batch. (data: 3.66e-04). ETA=17:26:50, max mem: 15.9 GB 
[10/31 12:01:09 visual_prompt]: 	Training 800/1106. train loss: 0.8188,	0.6439 s / batch. (data: 3.38e-04). ETA=18:03:27, max mem: 15.9 GB 
[10/31 12:02:13 visual_prompt]: 	Training 900/1106. train loss: 0.8848,	0.6440 s / batch. (data: 3.88e-04). ETA=18:02:28, max mem: 15.9 GB 
[10/31 12:03:17 visual_prompt]: 	Training 1000/1106. train loss: 0.8661,	0.6368 s / batch. (data: 3.14e-04). ETA=17:49:20, max mem: 15.9 GB 
[10/31 12:04:20 visual_prompt]: 	Training 1100/1106. train loss: 0.7573,	0.6178 s / batch. (data: 1.72e-04). ETA=17:16:18, max mem: 15.9 GB 
[10/31 12:04:24 visual_prompt]: Epoch 9 / 100: avg data time: 7.49e-03, avg batch time: 0.6401, average train loss: 0.7698
[10/31 12:05:19 visual_prompt]: 	Test 100/123. loss: 0.751, 0.2290 s / batch. (data: 5.15e-05)max mem: 15.94594 GB 
[10/31 12:05:30 visual_prompt]: Inference (val):avg data time: 2.64e-04, avg batch time: 0.2333, average loss: 0.7103
[10/31 12:05:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 56.40	
[10/31 12:05:30 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/31 12:06:37 visual_prompt]: 	Training 100/1106. train loss: 0.8053,	0.6421 s / batch. (data: 1.19e-02). ETA=17:56:01, max mem: 15.9 GB 
[10/31 12:07:40 visual_prompt]: 	Training 200/1106. train loss: 1.1989,	0.6537 s / batch. (data: 1.06e-02). ETA=18:14:21, max mem: 15.9 GB 
[10/31 12:08:44 visual_prompt]: 	Training 300/1106. train loss: 1.0055,	0.6535 s / batch. (data: 2.09e-02). ETA=18:12:52, max mem: 15.9 GB 
[10/31 12:09:48 visual_prompt]: 	Training 400/1106. train loss: 0.6311,	0.6320 s / batch. (data: 1.20e-02). ETA=17:35:56, max mem: 15.9 GB 
[10/31 12:10:51 visual_prompt]: 	Training 500/1106. train loss: 0.7410,	0.6182 s / batch. (data: 3.69e-04). ETA=17:11:54, max mem: 15.9 GB 
[10/31 12:11:55 visual_prompt]: 	Training 600/1106. train loss: 0.5529,	0.6278 s / batch. (data: 4.06e-04). ETA=17:26:46, max mem: 15.9 GB 
[10/31 12:12:59 visual_prompt]: 	Training 700/1106. train loss: 1.1195,	0.6439 s / batch. (data: 8.79e-04). ETA=17:52:31, max mem: 15.9 GB 
[10/31 12:14:03 visual_prompt]: 	Training 800/1106. train loss: 0.7949,	0.6332 s / batch. (data: 8.64e-04). ETA=17:33:46, max mem: 15.9 GB 
[10/31 12:15:06 visual_prompt]: 	Training 900/1106. train loss: 0.6271,	0.6197 s / batch. (data: 3.42e-04). ETA=17:10:14, max mem: 15.9 GB 
[10/31 12:16:10 visual_prompt]: 	Training 1000/1106. train loss: 0.7992,	0.6338 s / batch. (data: 3.51e-04). ETA=17:32:40, max mem: 15.9 GB 
[10/31 12:17:14 visual_prompt]: 	Training 1100/1106. train loss: 0.2767,	0.6168 s / batch. (data: 1.70e-04). ETA=17:03:16, max mem: 15.9 GB 
[10/31 12:17:18 visual_prompt]: Epoch 10 / 100: avg data time: 6.35e-03, avg batch time: 0.6395, average train loss: 0.8650
[10/31 12:18:12 visual_prompt]: 	Test 100/123. loss: 0.719, 0.2263 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/31 12:18:24 visual_prompt]: Inference (val):avg data time: 1.36e-04, avg batch time: 0.2314, average loss: 0.6837
[10/31 12:18:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.83	
[10/31 12:18:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/31 12:19:31 visual_prompt]: 	Training 100/1106. train loss: 0.5624,	0.6497 s / batch. (data: 6.09e-03). ETA=17:56:47, max mem: 15.9 GB 
[10/31 12:20:34 visual_prompt]: 	Training 200/1106. train loss: 1.1968,	0.6360 s / batch. (data: 3.45e-04). ETA=17:32:57, max mem: 15.9 GB 
[10/31 12:21:38 visual_prompt]: 	Training 300/1106. train loss: 0.8948,	0.6488 s / batch. (data: 1.85e-02). ETA=17:53:03, max mem: 15.9 GB 
[10/31 12:22:42 visual_prompt]: 	Training 400/1106. train loss: 1.2578,	0.6240 s / batch. (data: 3.30e-04). ETA=17:11:01, max mem: 15.9 GB 
[10/31 12:23:45 visual_prompt]: 	Training 500/1106. train loss: 1.2248,	0.6312 s / batch. (data: 3.86e-04). ETA=17:21:55, max mem: 15.9 GB 
[10/31 12:24:49 visual_prompt]: 	Training 600/1106. train loss: 0.1888,	0.6341 s / batch. (data: 1.24e-03). ETA=17:25:37, max mem: 15.9 GB 
[10/31 12:25:53 visual_prompt]: 	Training 700/1106. train loss: 1.1638,	0.6388 s / batch. (data: 3.98e-04). ETA=17:32:16, max mem: 15.9 GB 
[10/31 12:26:57 visual_prompt]: 	Training 800/1106. train loss: 0.2628,	0.6360 s / batch. (data: 8.26e-04). ETA=17:26:38, max mem: 15.9 GB 
[10/31 12:28:00 visual_prompt]: 	Training 900/1106. train loss: 0.3986,	0.6400 s / batch. (data: 8.77e-04). ETA=17:32:09, max mem: 15.9 GB 
[10/31 12:29:04 visual_prompt]: 	Training 1000/1106. train loss: 0.3073,	0.6369 s / batch. (data: 1.89e-02). ETA=17:25:57, max mem: 15.9 GB 
[10/31 12:30:08 visual_prompt]: 	Training 1100/1106. train loss: 1.0317,	0.6183 s / batch. (data: 2.11e-04). ETA=16:54:25, max mem: 15.9 GB 
[10/31 12:30:12 visual_prompt]: Epoch 11 / 100: avg data time: 6.66e-03, avg batch time: 0.6403, average train loss: 0.7896
[10/31 12:31:07 visual_prompt]: 	Test 100/123. loss: 0.692, 0.2414 s / batch. (data: 5.46e-05)max mem: 15.94594 GB 
[10/31 12:31:18 visual_prompt]: Inference (val):avg data time: 5.34e-05, avg batch time: 0.2332, average loss: 0.6810
[10/31 12:31:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 57.61	
[10/31 12:31:18 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/31 12:32:25 visual_prompt]: 	Training 100/1106. train loss: 0.3649,	0.6375 s / batch. (data: 1.08e-02). ETA=17:24:49, max mem: 15.9 GB 
[10/31 12:33:29 visual_prompt]: 	Training 200/1106. train loss: 0.7479,	0.6308 s / batch. (data: 8.90e-04). ETA=17:12:49, max mem: 15.9 GB 
[10/31 12:34:33 visual_prompt]: 	Training 300/1106. train loss: 0.1063,	0.6305 s / batch. (data: 4.14e-04). ETA=17:11:15, max mem: 15.9 GB 
[10/31 12:35:36 visual_prompt]: 	Training 400/1106. train loss: 0.6878,	0.6480 s / batch. (data: 1.38e-02). ETA=17:38:42, max mem: 15.9 GB 
[10/31 12:36:40 visual_prompt]: 	Training 500/1106. train loss: 1.5399,	0.6423 s / batch. (data: 8.57e-04). ETA=17:28:24, max mem: 15.9 GB 
[10/31 12:37:44 visual_prompt]: 	Training 600/1106. train loss: 0.6292,	0.6289 s / batch. (data: 4.22e-04). ETA=17:05:23, max mem: 15.9 GB 
[10/31 12:38:48 visual_prompt]: 	Training 700/1106. train loss: 0.5914,	0.6440 s / batch. (data: 5.50e-03). ETA=17:29:02, max mem: 15.9 GB 
[10/31 12:39:51 visual_prompt]: 	Training 800/1106. train loss: 0.7755,	0.6275 s / batch. (data: 3.59e-04). ETA=17:01:05, max mem: 15.9 GB 
[10/31 12:40:55 visual_prompt]: 	Training 900/1106. train loss: 0.1518,	0.6320 s / batch. (data: 3.56e-04). ETA=17:07:20, max mem: 15.9 GB 
[10/31 12:41:59 visual_prompt]: 	Training 1000/1106. train loss: 0.9895,	0.6344 s / batch. (data: 8.66e-04). ETA=17:10:07, max mem: 15.9 GB 
[10/31 12:43:02 visual_prompt]: 	Training 1100/1106. train loss: 0.7311,	0.6188 s / batch. (data: 2.54e-04). ETA=16:43:46, max mem: 15.9 GB 
[10/31 12:43:06 visual_prompt]: Epoch 12 / 100: avg data time: 7.04e-03, avg batch time: 0.6405, average train loss: 0.8030
[10/31 12:44:01 visual_prompt]: 	Test 100/123. loss: 1.742, 0.2353 s / batch. (data: 5.15e-05)max mem: 15.94594 GB 
[10/31 12:44:12 visual_prompt]: Inference (val):avg data time: 4.89e-05, avg batch time: 0.2327, average loss: 1.8261
[10/31 12:44:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.82	
[10/31 12:44:12 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/31 12:45:19 visual_prompt]: 	Training 100/1106. train loss: 1.1701,	0.6178 s / batch. (data: 3.69e-04). ETA=16:41:03, max mem: 15.9 GB 
[10/31 12:46:22 visual_prompt]: 	Training 200/1106. train loss: 0.6845,	0.6248 s / batch. (data: 5.52e-03). ETA=16:51:27, max mem: 15.9 GB 
[10/31 12:47:26 visual_prompt]: 	Training 300/1106. train loss: 0.3759,	0.6443 s / batch. (data: 9.58e-04). ETA=17:21:57, max mem: 15.9 GB 
[10/31 12:48:30 visual_prompt]: 	Training 400/1106. train loss: 0.7352,	0.6340 s / batch. (data: 8.67e-04). ETA=17:04:15, max mem: 15.9 GB 
[10/31 12:49:33 visual_prompt]: 	Training 500/1106. train loss: 0.1710,	0.6317 s / batch. (data: 8.73e-04). ETA=16:59:23, max mem: 15.9 GB 
[10/31 12:50:37 visual_prompt]: 	Training 600/1106. train loss: 0.9917,	0.6392 s / batch. (data: 8.33e-04). ETA=17:10:30, max mem: 15.9 GB 
[10/31 12:51:41 visual_prompt]: 	Training 700/1106. train loss: 0.8888,	0.6462 s / batch. (data: 8.76e-04). ETA=17:20:43, max mem: 15.9 GB 
[10/31 12:52:45 visual_prompt]: 	Training 800/1106. train loss: 1.3509,	0.6335 s / batch. (data: 8.98e-04). ETA=16:59:14, max mem: 15.9 GB 
[10/31 12:53:48 visual_prompt]: 	Training 900/1106. train loss: 1.0239,	0.6198 s / batch. (data: 4.00e-04). ETA=16:36:05, max mem: 15.9 GB 
[10/31 12:54:52 visual_prompt]: 	Training 1000/1106. train loss: 0.3088,	0.6189 s / batch. (data: 3.49e-04). ETA=16:33:33, max mem: 15.9 GB 
[10/31 12:55:56 visual_prompt]: 	Training 1100/1106. train loss: 0.7253,	0.6334 s / batch. (data: 2.23e-04). ETA=16:55:47, max mem: 15.9 GB 
[10/31 12:55:59 visual_prompt]: Epoch 13 / 100: avg data time: 6.08e-03, avg batch time: 0.6397, average train loss: 0.8028
[10/31 12:56:54 visual_prompt]: 	Test 100/123. loss: 0.834, 0.2270 s / batch. (data: 5.10e-05)max mem: 15.94594 GB 
[10/31 12:57:06 visual_prompt]: Inference (val):avg data time: 1.36e-04, avg batch time: 0.2332, average loss: 0.7972
[10/31 12:57:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.14	
[10/31 12:57:06 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/31 12:58:12 visual_prompt]: 	Training 100/1106. train loss: 0.7349,	0.6275 s / batch. (data: 3.54e-04). ETA=16:45:14, max mem: 15.9 GB 
[10/31 12:59:16 visual_prompt]: 	Training 200/1106. train loss: 0.9165,	0.6245 s / batch. (data: 5.53e-03). ETA=16:39:28, max mem: 15.9 GB 
[10/31 13:00:20 visual_prompt]: 	Training 300/1106. train loss: 0.7024,	0.6493 s / batch. (data: 8.74e-04). ETA=17:17:58, max mem: 15.9 GB 
[10/31 13:01:24 visual_prompt]: 	Training 400/1106. train loss: 0.4414,	0.6399 s / batch. (data: 2.07e-02). ETA=17:01:54, max mem: 15.9 GB 
[10/31 13:02:27 visual_prompt]: 	Training 500/1106. train loss: 0.7071,	0.6446 s / batch. (data: 9.01e-04). ETA=17:08:18, max mem: 15.9 GB 
[10/31 13:03:31 visual_prompt]: 	Training 600/1106. train loss: 0.5861,	0.6390 s / batch. (data: 3.46e-04). ETA=16:58:26, max mem: 15.9 GB 
[10/31 13:04:35 visual_prompt]: 	Training 700/1106. train loss: 0.2033,	0.6477 s / batch. (data: 8.61e-04). ETA=17:11:08, max mem: 15.9 GB 
[10/31 13:05:39 visual_prompt]: 	Training 800/1106. train loss: 0.6309,	0.6341 s / batch. (data: 9.22e-04). ETA=16:48:30, max mem: 15.9 GB 
[10/31 13:06:42 visual_prompt]: 	Training 900/1106. train loss: 0.9055,	0.6280 s / batch. (data: 4.53e-04). ETA=16:37:43, max mem: 15.9 GB 
[10/31 13:07:46 visual_prompt]: 	Training 1000/1106. train loss: 0.0596,	0.6485 s / batch. (data: 5.53e-03). ETA=17:09:11, max mem: 15.9 GB 
[10/31 13:08:50 visual_prompt]: 	Training 1100/1106. train loss: 1.0850,	0.6186 s / batch. (data: 1.60e-04). ETA=16:20:39, max mem: 15.9 GB 
[10/31 13:08:54 visual_prompt]: Epoch 14 / 100: avg data time: 6.65e-03, avg batch time: 0.6402, average train loss: 0.7701
[10/31 13:09:48 visual_prompt]: 	Test 100/123. loss: 0.974, 0.2386 s / batch. (data: 3.79e-05)max mem: 15.94594 GB 
[10/31 13:10:00 visual_prompt]: Inference (val):avg data time: 5.16e-05, avg batch time: 0.2330, average loss: 0.9046
[10/31 13:10:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.01	
[10/31 13:10:00 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/31 13:11:06 visual_prompt]: 	Training 100/1106. train loss: 0.7595,	0.6459 s / batch. (data: 8.71e-04). ETA=17:02:53, max mem: 15.9 GB 
[10/31 13:12:09 visual_prompt]: 	Training 200/1106. train loss: 0.8524,	0.6589 s / batch. (data: 8.69e-04). ETA=17:22:16, max mem: 15.9 GB 
[10/31 13:13:13 visual_prompt]: 	Training 300/1106. train loss: 0.6416,	0.6360 s / batch. (data: 4.01e-04). ETA=16:44:58, max mem: 15.9 GB 
[10/31 13:14:17 visual_prompt]: 	Training 400/1106. train loss: 0.8867,	0.6272 s / batch. (data: 3.52e-04). ETA=16:30:02, max mem: 15.9 GB 
[10/31 13:15:21 visual_prompt]: 	Training 500/1106. train loss: 0.2716,	0.6602 s / batch. (data: 3.22e-02). ETA=17:21:05, max mem: 15.9 GB 
[10/31 13:16:24 visual_prompt]: 	Training 600/1106. train loss: 1.0923,	0.6400 s / batch. (data: 8.40e-04). ETA=16:48:10, max mem: 15.9 GB 
[10/31 13:17:28 visual_prompt]: 	Training 700/1106. train loss: 0.5568,	0.6480 s / batch. (data: 3.40e-04). ETA=16:59:43, max mem: 15.9 GB 
[10/31 13:18:32 visual_prompt]: 	Training 800/1106. train loss: 0.5644,	0.6375 s / batch. (data: 6.12e-03). ETA=16:42:06, max mem: 15.9 GB 
[10/31 13:19:36 visual_prompt]: 	Training 900/1106. train loss: 0.7652,	0.6517 s / batch. (data: 6.07e-03). ETA=17:03:20, max mem: 15.9 GB 
[10/31 13:20:39 visual_prompt]: 	Training 1000/1106. train loss: 0.5747,	0.6313 s / batch. (data: 3.72e-04). ETA=16:30:12, max mem: 15.9 GB 
[10/31 13:21:43 visual_prompt]: 	Training 1100/1106. train loss: 0.9030,	0.6196 s / batch. (data: 2.34e-04). ETA=16:10:54, max mem: 15.9 GB 
[10/31 13:21:47 visual_prompt]: Epoch 15 / 100: avg data time: 5.85e-03, avg batch time: 0.6391, average train loss: 0.8073
[10/31 13:22:41 visual_prompt]: 	Test 100/123. loss: 0.896, 0.2258 s / batch. (data: 5.58e-05)max mem: 15.94594 GB 
[10/31 13:22:53 visual_prompt]: Inference (val):avg data time: 5.08e-05, avg batch time: 0.2327, average loss: 0.8356
[10/31 13:22:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.47	
[10/31 13:22:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/31 13:23:59 visual_prompt]: 	Training 100/1106. train loss: 0.4105,	0.6474 s / batch. (data: 1.70e-02). ETA=16:53:19, max mem: 15.9 GB 
[10/31 13:25:02 visual_prompt]: 	Training 200/1106. train loss: 0.7036,	0.6174 s / batch. (data: 3.47e-04). ETA=16:05:19, max mem: 15.9 GB 
[10/31 13:26:06 visual_prompt]: 	Training 300/1106. train loss: 0.7002,	0.6488 s / batch. (data: 1.05e-03). ETA=16:53:21, max mem: 15.9 GB 
[10/31 13:27:10 visual_prompt]: 	Training 400/1106. train loss: 0.7176,	0.6380 s / batch. (data: 3.91e-04). ETA=16:35:25, max mem: 15.9 GB 
[10/31 13:28:13 visual_prompt]: 	Training 500/1106. train loss: 0.7944,	0.6600 s / batch. (data: 1.11e-03). ETA=17:08:36, max mem: 15.9 GB 
[10/31 13:29:17 visual_prompt]: 	Training 600/1106. train loss: 1.4158,	0.6294 s / batch. (data: 1.07e-02). ETA=16:19:48, max mem: 15.9 GB 
[10/31 13:30:21 visual_prompt]: 	Training 700/1106. train loss: 1.0836,	0.6400 s / batch. (data: 3.55e-04). ETA=16:35:16, max mem: 15.9 GB 
[10/31 13:31:25 visual_prompt]: 	Training 800/1106. train loss: 0.7482,	0.6280 s / batch. (data: 3.75e-04). ETA=16:15:35, max mem: 15.9 GB 
[10/31 13:32:28 visual_prompt]: 	Training 900/1106. train loss: 0.5697,	0.6353 s / batch. (data: 8.61e-04). ETA=16:25:50, max mem: 15.9 GB 
[10/31 13:33:32 visual_prompt]: 	Training 1000/1106. train loss: 1.0199,	0.6400 s / batch. (data: 3.53e-04). ETA=16:32:06, max mem: 15.9 GB 
[10/31 13:34:36 visual_prompt]: 	Training 1100/1106. train loss: 0.2683,	0.6176 s / batch. (data: 2.01e-04). ETA=15:56:22, max mem: 15.9 GB 
[10/31 13:34:40 visual_prompt]: Epoch 16 / 100: avg data time: 5.70e-03, avg batch time: 0.6392, average train loss: 0.7641
[10/31 13:35:34 visual_prompt]: 	Test 100/123. loss: 0.696, 0.2288 s / batch. (data: 5.13e-05)max mem: 15.94594 GB 
[10/31 13:35:46 visual_prompt]: Inference (val):avg data time: 7.65e-05, avg batch time: 0.2327, average loss: 0.6691
[10/31 13:35:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.18	
[10/31 13:35:46 visual_prompt]: Best epoch 16: best metric: -0.669
[10/31 13:35:46 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/31 13:36:53 visual_prompt]: 	Training 100/1106. train loss: 1.4815,	0.6605 s / batch. (data: 1.11e-02). ETA=17:01:34, max mem: 15.9 GB 
[10/31 13:37:56 visual_prompt]: 	Training 200/1106. train loss: 0.6240,	0.6321 s / batch. (data: 9.71e-04). ETA=16:16:37, max mem: 15.9 GB 
[10/31 13:39:00 visual_prompt]: 	Training 300/1106. train loss: 0.7965,	0.6717 s / batch. (data: 3.56e-02). ETA=17:16:40, max mem: 15.9 GB 
[10/31 13:40:04 visual_prompt]: 	Training 400/1106. train loss: 1.7708,	0.6784 s / batch. (data: 3.23e-02). ETA=17:25:50, max mem: 15.9 GB 
[10/31 13:41:07 visual_prompt]: 	Training 500/1106. train loss: 0.2242,	0.6614 s / batch. (data: 2.53e-02). ETA=16:58:31, max mem: 15.9 GB 
[10/31 13:42:11 visual_prompt]: 	Training 600/1106. train loss: 1.0333,	0.6413 s / batch. (data: 6.03e-03). ETA=16:26:35, max mem: 15.9 GB 
[10/31 13:43:14 visual_prompt]: 	Training 700/1106. train loss: 0.7510,	0.6603 s / batch. (data: 1.11e-02). ETA=16:54:39, max mem: 15.9 GB 
[10/31 13:44:18 visual_prompt]: 	Training 800/1106. train loss: 0.8543,	0.6321 s / batch. (data: 3.46e-04). ETA=16:10:16, max mem: 15.9 GB 
[10/31 13:45:22 visual_prompt]: 	Training 900/1106. train loss: 0.9477,	0.6317 s / batch. (data: 3.44e-04). ETA=16:08:41, max mem: 15.9 GB 
[10/31 13:46:25 visual_prompt]: 	Training 1000/1106. train loss: 0.0472,	0.6582 s / batch. (data: 3.83e-02). ETA=16:48:13, max mem: 15.9 GB 
[10/31 13:47:29 visual_prompt]: 	Training 1100/1106. train loss: 0.2036,	0.6184 s / batch. (data: 2.10e-04). ETA=15:46:09, max mem: 15.9 GB 
[10/31 13:47:33 visual_prompt]: Epoch 17 / 100: avg data time: 5.93e-03, avg batch time: 0.6391, average train loss: 0.8111
[10/31 13:48:28 visual_prompt]: 	Test 100/123. loss: 0.697, 0.2359 s / batch. (data: 5.20e-05)max mem: 15.94594 GB 
[10/31 13:48:39 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.2317, average loss: 0.7078
[10/31 13:48:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.47	
[10/31 13:48:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/31 13:49:45 visual_prompt]: 	Training 100/1106. train loss: 0.2871,	0.6380 s / batch. (data: 1.01e-03). ETA=16:15:02, max mem: 15.9 GB 
[10/31 13:50:49 visual_prompt]: 	Training 200/1106. train loss: 0.9365,	0.6491 s / batch. (data: 1.92e-02). ETA=16:30:58, max mem: 15.9 GB 
[10/31 13:51:53 visual_prompt]: 	Training 300/1106. train loss: 2.0739,	0.6490 s / batch. (data: 5.52e-03). ETA=16:29:44, max mem: 15.9 GB 
[10/31 13:52:56 visual_prompt]: 	Training 400/1106. train loss: 0.6916,	0.6552 s / batch. (data: 6.02e-03). ETA=16:38:06, max mem: 15.9 GB 
[10/31 13:54:00 visual_prompt]: 	Training 500/1106. train loss: 0.1344,	0.6370 s / batch. (data: 8.66e-04). ETA=16:09:19, max mem: 15.9 GB 
[10/31 13:55:04 visual_prompt]: 	Training 600/1106. train loss: 0.6563,	0.6280 s / batch. (data: 3.27e-04). ETA=15:54:31, max mem: 15.9 GB 
[10/31 13:56:07 visual_prompt]: 	Training 700/1106. train loss: 0.8053,	0.6572 s / batch. (data: 8.95e-04). ETA=16:37:51, max mem: 15.9 GB 
[10/31 13:57:11 visual_prompt]: 	Training 800/1106. train loss: 0.7315,	0.6222 s / batch. (data: 3.39e-04). ETA=15:43:41, max mem: 15.9 GB 
[10/31 13:58:15 visual_prompt]: 	Training 900/1106. train loss: 1.1157,	0.6614 s / batch. (data: 1.74e-02). ETA=16:42:02, max mem: 15.9 GB 
[10/31 13:59:18 visual_prompt]: 	Training 1000/1106. train loss: 0.7631,	0.6579 s / batch. (data: 1.19e-03). ETA=16:35:32, max mem: 15.9 GB 
[10/31 14:00:22 visual_prompt]: 	Training 1100/1106. train loss: 0.5626,	0.6193 s / batch. (data: 2.21e-04). ETA=15:36:06, max mem: 15.9 GB 
[10/31 14:00:26 visual_prompt]: Epoch 18 / 100: avg data time: 5.88e-03, avg batch time: 0.6390, average train loss: 0.8585
[10/31 14:01:21 visual_prompt]: 	Test 100/123. loss: 0.664, 0.2250 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[10/31 14:01:32 visual_prompt]: Inference (val):avg data time: 2.66e-04, avg batch time: 0.2331, average loss: 0.6768
[10/31 14:01:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.80	
[10/31 14:01:32 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[10/31 14:02:39 visual_prompt]: 	Training 100/1106. train loss: 0.2455,	0.6425 s / batch. (data: 1.45e-02). ETA=16:10:05, max mem: 15.9 GB 
[10/31 14:03:43 visual_prompt]: 	Training 200/1106. train loss: 0.7967,	0.6248 s / batch. (data: 3.71e-04). ETA=15:42:23, max mem: 15.9 GB 
[10/31 14:04:47 visual_prompt]: 	Training 300/1106. train loss: 0.3311,	0.6400 s / batch. (data: 5.68e-04). ETA=16:04:08, max mem: 15.9 GB 
[10/31 14:05:50 visual_prompt]: 	Training 400/1106. train loss: 0.7202,	0.6180 s / batch. (data: 3.72e-04). ETA=15:29:57, max mem: 15.9 GB 
[10/31 14:06:54 visual_prompt]: 	Training 500/1106. train loss: 0.6471,	0.6190 s / batch. (data: 3.59e-04). ETA=15:30:27, max mem: 15.9 GB 
[10/31 14:07:58 visual_prompt]: 	Training 600/1106. train loss: 1.1389,	0.6276 s / batch. (data: 4.64e-04). ETA=15:42:19, max mem: 15.9 GB 
[10/31 14:09:01 visual_prompt]: 	Training 700/1106. train loss: 0.6700,	0.6401 s / batch. (data: 8.89e-04). ETA=16:00:05, max mem: 15.9 GB 
[10/31 14:10:05 visual_prompt]: 	Training 800/1106. train loss: 0.1543,	0.6210 s / batch. (data: 3.57e-04). ETA=15:30:26, max mem: 15.9 GB 
[10/31 14:11:09 visual_prompt]: 	Training 900/1106. train loss: 0.5137,	0.6369 s / batch. (data: 1.84e-02). ETA=15:53:07, max mem: 15.9 GB 
[10/31 14:12:12 visual_prompt]: 	Training 1000/1106. train loss: 0.9625,	0.6440 s / batch. (data: 8.62e-04). ETA=16:02:38, max mem: 15.9 GB 
[10/31 14:13:16 visual_prompt]: 	Training 1100/1106. train loss: 0.8794,	0.6235 s / batch. (data: 2.13e-04). ETA=15:31:00, max mem: 15.9 GB 
[10/31 14:13:20 visual_prompt]: Epoch 19 / 100: avg data time: 6.58e-03, avg batch time: 0.6402, average train loss: 0.7508
[10/31 14:14:15 visual_prompt]: 	Test 100/123. loss: 1.195, 0.2287 s / batch. (data: 5.29e-05)max mem: 15.94594 GB 
[10/31 14:14:26 visual_prompt]: Inference (val):avg data time: 4.90e-05, avg batch time: 0.2323, average loss: 1.0782
[10/31 14:14:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.91	
[10/31 14:14:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[10/31 14:15:33 visual_prompt]: 	Training 100/1106. train loss: 0.7162,	0.6435 s / batch. (data: 8.85e-04). ETA=15:59:46, max mem: 15.9 GB 
[10/31 14:16:37 visual_prompt]: 	Training 200/1106. train loss: 0.3318,	0.6208 s / batch. (data: 3.42e-04). ETA=15:24:51, max mem: 15.9 GB 
[10/31 14:17:41 visual_prompt]: 	Training 300/1106. train loss: 0.5189,	0.6530 s / batch. (data: 9.15e-04). ETA=16:11:40, max mem: 15.9 GB 
[10/31 14:18:45 visual_prompt]: 	Training 400/1106. train loss: 0.4939,	0.6438 s / batch. (data: 5.30e-04). ETA=15:56:54, max mem: 15.9 GB 
[10/31 14:19:49 visual_prompt]: 	Training 500/1106. train loss: 0.4824,	0.6233 s / batch. (data: 5.52e-03). ETA=15:25:26, max mem: 15.9 GB 
[10/31 14:20:53 visual_prompt]: 	Training 600/1106. train loss: 0.9348,	0.6400 s / batch. (data: 9.14e-04). ETA=15:49:09, max mem: 15.9 GB 
[10/31 14:21:57 visual_prompt]: 	Training 700/1106. train loss: 0.5502,	0.6600 s / batch. (data: 1.20e-02). ETA=16:17:44, max mem: 15.9 GB 
[10/31 14:23:00 visual_prompt]: 	Training 800/1106. train loss: 0.6194,	0.6421 s / batch. (data: 8.70e-04). ETA=15:50:11, max mem: 15.9 GB 
[10/31 14:24:04 visual_prompt]: 	Training 900/1106. train loss: 0.5597,	0.6187 s / batch. (data: 3.98e-04). ETA=15:14:34, max mem: 15.9 GB 
[10/31 14:25:08 visual_prompt]: 	Training 1000/1106. train loss: 0.6247,	0.6288 s / batch. (data: 3.55e-04). ETA=15:28:25, max mem: 15.9 GB 
[10/31 14:26:11 visual_prompt]: 	Training 1100/1106. train loss: 0.3928,	0.6195 s / batch. (data: 1.89e-04). ETA=15:13:37, max mem: 15.9 GB 
[10/31 14:26:15 visual_prompt]: Epoch 20 / 100: avg data time: 7.08e-03, avg batch time: 0.6407, average train loss: 0.7649
[10/31 14:27:09 visual_prompt]: 	Test 100/123. loss: 0.911, 0.2395 s / batch. (data: 5.39e-05)max mem: 15.94594 GB 
[10/31 14:27:21 visual_prompt]: Inference (val):avg data time: 1.37e-04, avg batch time: 0.2331, average loss: 1.0182
[10/31 14:27:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.41	
[10/31 14:27:21 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[10/31 14:28:28 visual_prompt]: 	Training 100/1106. train loss: 0.5561,	0.6400 s / batch. (data: 7.97e-03). ETA=15:42:43, max mem: 15.9 GB 
[10/31 14:29:33 visual_prompt]: 	Training 200/1106. train loss: 0.5786,	0.6310 s / batch. (data: 3.72e-04). ETA=15:28:25, max mem: 15.9 GB 
[10/31 14:30:37 visual_prompt]: 	Training 300/1106. train loss: 0.4490,	0.6312 s / batch. (data: 3.54e-04). ETA=15:27:36, max mem: 15.9 GB 
[10/31 14:31:41 visual_prompt]: 	Training 400/1106. train loss: 0.9038,	0.6297 s / batch. (data: 5.55e-03). ETA=15:24:21, max mem: 15.9 GB 
[10/31 14:32:44 visual_prompt]: 	Training 500/1106. train loss: 1.2516,	0.6606 s / batch. (data: 3.91e-04). ETA=16:08:36, max mem: 15.9 GB 
[10/31 14:33:48 visual_prompt]: 	Training 600/1106. train loss: 1.7614,	0.6560 s / batch. (data: 9.27e-04). ETA=16:00:48, max mem: 15.9 GB 
[10/31 14:34:52 visual_prompt]: 	Training 700/1106. train loss: 0.7938,	0.6187 s / batch. (data: 8.70e-04). ETA=15:05:11, max mem: 15.9 GB 
[10/31 14:35:56 visual_prompt]: 	Training 800/1106. train loss: 0.7954,	0.6560 s / batch. (data: 8.42e-04). ETA=15:58:38, max mem: 15.9 GB 
[10/31 14:36:59 visual_prompt]: 	Training 900/1106. train loss: 0.0692,	0.6320 s / batch. (data: 3.55e-04). ETA=15:22:28, max mem: 15.9 GB 
[10/31 14:38:03 visual_prompt]: 	Training 1000/1106. train loss: 0.6712,	0.6205 s / batch. (data: 4.63e-04). ETA=15:04:43, max mem: 15.9 GB 
[10/31 14:39:07 visual_prompt]: 	Training 1100/1106. train loss: 0.6999,	0.6201 s / batch. (data: 1.86e-04). ETA=15:03:06, max mem: 15.9 GB 
[10/31 14:39:10 visual_prompt]: Epoch 21 / 100: avg data time: 6.70e-03, avg batch time: 0.6413, average train loss: 0.7676
[10/31 14:40:05 visual_prompt]: 	Test 100/123. loss: 0.623, 0.2325 s / batch. (data: 5.17e-05)max mem: 15.94594 GB 
[10/31 14:40:17 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.2323, average loss: 0.7015
[10/31 14:40:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 61.83	
[10/31 14:40:17 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[10/31 14:41:23 visual_prompt]: 	Training 100/1106. train loss: 1.2195,	0.6540 s / batch. (data: 8.32e-04). ETA=15:51:13, max mem: 15.9 GB 
[10/31 14:42:27 visual_prompt]: 	Training 200/1106. train loss: 0.3997,	0.6418 s / batch. (data: 3.79e-04). ETA=15:32:26, max mem: 15.9 GB 
[10/31 14:43:30 visual_prompt]: 	Training 300/1106. train loss: 0.8213,	0.6304 s / batch. (data: 5.50e-03). ETA=15:14:52, max mem: 15.9 GB 
[10/31 14:44:34 visual_prompt]: 	Training 400/1106. train loss: 0.1859,	0.6278 s / batch. (data: 6.75e-04). ETA=15:09:58, max mem: 15.9 GB 
[10/31 14:45:37 visual_prompt]: 	Training 500/1106. train loss: 0.3183,	0.6366 s / batch. (data: 8.97e-04). ETA=15:21:47, max mem: 15.9 GB 
[10/31 14:46:41 visual_prompt]: 	Training 600/1106. train loss: 0.3142,	0.6185 s / batch. (data: 3.82e-04). ETA=14:54:31, max mem: 15.9 GB 
[10/31 14:47:45 visual_prompt]: 	Training 700/1106. train loss: 1.4522,	0.6199 s / batch. (data: 3.73e-04). ETA=14:55:24, max mem: 15.9 GB 
[10/31 14:48:49 visual_prompt]: 	Training 800/1106. train loss: 0.8042,	0.6204 s / batch. (data: 3.77e-04). ETA=14:55:08, max mem: 15.9 GB 
[10/31 14:49:52 visual_prompt]: 	Training 900/1106. train loss: 0.8736,	0.6320 s / batch. (data: 1.20e-02). ETA=15:10:54, max mem: 15.9 GB 
[10/31 14:50:56 visual_prompt]: 	Training 1000/1106. train loss: 0.6518,	0.6481 s / batch. (data: 3.55e-04). ETA=15:32:54, max mem: 15.9 GB 
[10/31 14:52:00 visual_prompt]: 	Training 1100/1106. train loss: 0.4167,	0.6196 s / batch. (data: 2.01e-04). ETA=14:50:53, max mem: 15.9 GB 
[10/31 14:52:04 visual_prompt]: Epoch 22 / 100: avg data time: 6.11e-03, avg batch time: 0.6391, average train loss: 0.7399
[10/31 14:52:59 visual_prompt]: 	Test 100/123. loss: 0.610, 0.2249 s / batch. (data: 5.94e-05)max mem: 15.94594 GB 
[10/31 14:53:10 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.2334, average loss: 0.7382
[10/31 14:53:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 60.27	
[10/31 14:53:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[10/31 14:54:17 visual_prompt]: 	Training 100/1106. train loss: 0.5677,	0.6293 s / batch. (data: 3.75e-04). ETA=15:03:49, max mem: 15.9 GB 
[10/31 14:55:21 visual_prompt]: 	Training 200/1106. train loss: 0.7651,	0.6569 s / batch. (data: 1.06e-02). ETA=15:42:18, max mem: 15.9 GB 
[10/31 14:56:25 visual_prompt]: 	Training 300/1106. train loss: 0.7823,	0.6330 s / batch. (data: 3.72e-04). ETA=15:06:53, max mem: 15.9 GB 
[10/31 14:57:28 visual_prompt]: 	Training 400/1106. train loss: 0.7551,	0.6480 s / batch. (data: 9.07e-04). ETA=15:27:24, max mem: 15.9 GB 
[10/31 14:58:32 visual_prompt]: 	Training 500/1106. train loss: 0.6764,	0.6414 s / batch. (data: 8.95e-04). ETA=15:16:49, max mem: 15.9 GB 
[10/31 14:59:36 visual_prompt]: 	Training 600/1106. train loss: 0.6478,	0.6416 s / batch. (data: 8.77e-04). ETA=15:16:08, max mem: 15.9 GB 
[10/31 15:00:39 visual_prompt]: 	Training 700/1106. train loss: 0.6489,	0.6309 s / batch. (data: 3.59e-04). ETA=14:59:43, max mem: 15.9 GB 
[10/31 15:01:43 visual_prompt]: 	Training 800/1106. train loss: 0.7414,	0.6331 s / batch. (data: 3.96e-04). ETA=15:01:49, max mem: 15.9 GB 
[10/31 15:02:46 visual_prompt]: 	Training 900/1106. train loss: 0.7897,	0.6320 s / batch. (data: 3.66e-04). ETA=14:59:10, max mem: 15.9 GB 
[10/31 15:03:50 visual_prompt]: 	Training 1000/1106. train loss: 0.6041,	0.6280 s / batch. (data: 5.51e-03). ETA=14:52:27, max mem: 15.9 GB 
[10/31 15:04:54 visual_prompt]: 	Training 1100/1106. train loss: 0.6611,	0.6186 s / batch. (data: 1.83e-04). ETA=14:38:01, max mem: 15.9 GB 
[10/31 15:04:58 visual_prompt]: Epoch 23 / 100: avg data time: 6.61e-03, avg batch time: 0.6399, average train loss: 0.7506
[10/31 15:05:52 visual_prompt]: 	Test 100/123. loss: 0.693, 0.2253 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/31 15:06:04 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.2319, average loss: 0.6790
[10/31 15:06:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.20	
[10/31 15:06:04 visual_prompt]: Stopping early.
[10/31 15:06:04 visual_prompt]: Rank of current process: 0. World size: 1
[10/31 15:06:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/31 15:06:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/31 15:06:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/31 15:06:04 visual_prompt]: Training with config:
[10/31 15:06:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/val/seed0/lr0.05_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/31 15:06:04 visual_prompt]: Loading training data...
[10/31 15:06:04 visual_prompt]: Constructing mammo-cbis dataset train...
[10/31 15:06:04 visual_prompt]: Loading validation data...
[10/31 15:06:04 visual_prompt]: Constructing mammo-cbis dataset val...
[10/31 15:06:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/31 15:06:07 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/31 15:06:07 visual_prompt]: tuned percent:0.522
[10/31 15:06:07 visual_prompt]: Device used for model: 0
[10/31 15:06:07 visual_prompt]: Setting up Evaluator...
[10/31 15:06:07 visual_prompt]: Setting up Trainer...
[10/31 15:06:07 visual_prompt]: 	Setting up the optimizer...
[10/31 15:06:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/31 15:07:14 visual_prompt]: 	Training 100/1106. train loss: 2.8353,	0.6310 s / batch. (data: 3.84e-04). ETA=19:22:02, max mem: 15.9 GB 
[10/31 15:08:17 visual_prompt]: 	Training 200/1106. train loss: 1.2683,	0.6240 s / batch. (data: 3.57e-04). ETA=19:08:07, max mem: 15.9 GB 
[10/31 15:09:21 visual_prompt]: 	Training 300/1106. train loss: 0.0252,	0.6181 s / batch. (data: 3.61e-04). ETA=18:56:17, max mem: 15.9 GB 
[10/31 15:10:25 visual_prompt]: 	Training 400/1106. train loss: 2.9968,	0.6414 s / batch. (data: 9.30e-04). ETA=19:38:01, max mem: 15.9 GB 
[10/31 15:11:28 visual_prompt]: 	Training 500/1106. train loss: 1.3889,	0.6388 s / batch. (data: 3.47e-04). ETA=19:32:09, max mem: 15.9 GB 
[10/31 15:12:32 visual_prompt]: 	Training 600/1106. train loss: 1.3294,	0.6176 s / batch. (data: 3.67e-04). ETA=18:52:12, max mem: 15.9 GB 
[10/31 15:13:36 visual_prompt]: 	Training 700/1106. train loss: 2.5781,	0.6347 s / batch. (data: 3.60e-04). ETA=19:22:32, max mem: 15.9 GB 
[10/31 15:14:40 visual_prompt]: 	Training 800/1106. train loss: 0.0815,	0.6321 s / batch. (data: 1.06e-02). ETA=19:16:43, max mem: 15.9 GB 
[10/31 15:15:43 visual_prompt]: 	Training 900/1106. train loss: 1.1448,	0.6469 s / batch. (data: 8.19e-04). ETA=19:42:47, max mem: 15.9 GB 
[10/31 15:16:47 visual_prompt]: 	Training 1000/1106. train loss: 1.9846,	0.6402 s / batch. (data: 8.53e-04). ETA=19:29:30, max mem: 15.9 GB 
[10/31 15:17:51 visual_prompt]: 	Training 1100/1106. train loss: 1.4255,	0.6177 s / batch. (data: 2.46e-04). ETA=18:47:22, max mem: 15.9 GB 
[10/31 15:17:54 visual_prompt]: Epoch 1 / 100: avg data time: 6.43e-03, avg batch time: 0.6396, average train loss: 1.4028
[10/31 15:18:49 visual_prompt]: 	Test 100/123. loss: 1.529, 0.2278 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/31 15:19:00 visual_prompt]: Inference (val):avg data time: 4.70e-05, avg batch time: 0.2321, average loss: 1.3505
[10/31 15:19:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.48	
[10/31 15:19:00 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/31 15:20:06 visual_prompt]: 	Training 100/1106. train loss: 0.6521,	0.6379 s / batch. (data: 1.37e-03). ETA=19:23:00, max mem: 15.9 GB 
[10/31 15:21:10 visual_prompt]: 	Training 200/1106. train loss: 0.7177,	0.6316 s / batch. (data: 8.80e-04). ETA=19:10:26, max mem: 15.9 GB 
[10/31 15:22:14 visual_prompt]: 	Training 300/1106. train loss: 0.9951,	0.6240 s / batch. (data: 3.32e-04). ETA=18:55:36, max mem: 15.9 GB 
[10/31 15:23:17 visual_prompt]: 	Training 400/1106. train loss: 0.1135,	0.6295 s / batch. (data: 8.48e-04). ETA=19:04:37, max mem: 15.9 GB 
[10/31 15:24:21 visual_prompt]: 	Training 500/1106. train loss: 0.7271,	0.6444 s / batch. (data: 2.06e-02). ETA=19:30:38, max mem: 15.9 GB 
[10/31 15:25:25 visual_prompt]: 	Training 600/1106. train loss: 0.5391,	0.6504 s / batch. (data: 3.23e-04). ETA=19:40:22, max mem: 15.9 GB 
[10/31 15:26:29 visual_prompt]: 	Training 700/1106. train loss: 0.7569,	0.6201 s / batch. (data: 3.48e-04). ETA=18:44:20, max mem: 15.9 GB 
[10/31 15:27:33 visual_prompt]: 	Training 800/1106. train loss: 0.7605,	0.6520 s / batch. (data: 8.60e-04). ETA=19:41:10, max mem: 15.9 GB 
[10/31 15:28:36 visual_prompt]: 	Training 900/1106. train loss: 0.6603,	0.6367 s / batch. (data: 3.60e-04). ETA=19:12:25, max mem: 15.9 GB 
[10/31 15:29:40 visual_prompt]: 	Training 1000/1106. train loss: 0.4806,	0.6240 s / batch. (data: 4.54e-04). ETA=18:48:16, max mem: 15.9 GB 
[10/31 15:30:44 visual_prompt]: 	Training 1100/1106. train loss: 0.7337,	0.6184 s / batch. (data: 2.36e-04). ETA=18:37:05, max mem: 15.9 GB 
[10/31 15:30:47 visual_prompt]: Epoch 2 / 100: avg data time: 5.90e-03, avg batch time: 0.6393, average train loss: 0.7724
[10/31 15:31:42 visual_prompt]: 	Test 100/123. loss: 1.048, 0.2316 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[10/31 15:31:53 visual_prompt]: Inference (val):avg data time: 4.85e-05, avg batch time: 0.2314, average loss: 0.9226
[10/31 15:31:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.93	
[10/31 15:31:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/31 15:33:00 visual_prompt]: 	Training 100/1106. train loss: 0.6644,	0.6709 s / batch. (data: 4.00e-02). ETA=20:10:54, max mem: 15.9 GB 
[10/31 15:34:04 visual_prompt]: 	Training 200/1106. train loss: 0.3821,	0.6446 s / batch. (data: 9.56e-04). ETA=19:22:19, max mem: 15.9 GB 
[10/31 15:35:08 visual_prompt]: 	Training 300/1106. train loss: 0.3382,	0.6324 s / batch. (data: 8.79e-04). ETA=18:59:12, max mem: 15.9 GB 
[10/31 15:36:12 visual_prompt]: 	Training 400/1106. train loss: 0.7677,	0.6195 s / batch. (data: 3.56e-04). ETA=18:34:53, max mem: 15.9 GB 
[10/31 15:37:15 visual_prompt]: 	Training 500/1106. train loss: 1.3816,	0.6305 s / batch. (data: 4.22e-04). ETA=18:53:46, max mem: 15.9 GB 
[10/31 15:38:19 visual_prompt]: 	Training 600/1106. train loss: 0.6620,	0.6588 s / batch. (data: 1.62e-02). ETA=19:43:34, max mem: 15.9 GB 
[10/31 15:39:23 visual_prompt]: 	Training 700/1106. train loss: 0.8359,	0.6196 s / batch. (data: 3.60e-04). ETA=18:32:02, max mem: 15.9 GB 
[10/31 15:40:27 visual_prompt]: 	Training 800/1106. train loss: 1.3612,	0.6345 s / batch. (data: 3.33e-04). ETA=18:57:46, max mem: 15.9 GB 
[10/31 15:41:31 visual_prompt]: 	Training 900/1106. train loss: 1.2189,	0.6520 s / batch. (data: 3.40e-04). ETA=19:28:01, max mem: 15.9 GB 
[10/31 15:42:34 visual_prompt]: 	Training 1000/1106. train loss: 0.6663,	0.6686 s / batch. (data: 1.15e-02). ETA=19:56:44, max mem: 15.9 GB 
[10/31 15:43:38 visual_prompt]: 	Training 1100/1106. train loss: 0.5538,	0.6190 s / batch. (data: 2.33e-04). ETA=18:26:49, max mem: 15.9 GB 
[10/31 15:43:42 visual_prompt]: Epoch 3 / 100: avg data time: 6.91e-03, avg batch time: 0.6403, average train loss: 0.7681
[10/31 15:44:36 visual_prompt]: 	Test 100/123. loss: 0.830, 0.2367 s / batch. (data: 5.84e-05)max mem: 15.94594 GB 
[10/31 15:44:48 visual_prompt]: Inference (val):avg data time: 4.65e-05, avg batch time: 0.2328, average loss: 0.8012
[10/31 15:44:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.75	
[10/31 15:44:48 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/31 15:45:54 visual_prompt]: 	Training 100/1106. train loss: 1.2332,	0.6697 s / batch. (data: 3.86e-02). ETA=19:56:19, max mem: 15.9 GB 
[10/31 15:46:58 visual_prompt]: 	Training 200/1106. train loss: 1.0803,	0.6510 s / batch. (data: 6.06e-03). ETA=19:21:53, max mem: 15.9 GB 
[10/31 15:48:01 visual_prompt]: 	Training 300/1106. train loss: 0.6832,	0.6194 s / batch. (data: 8.44e-04). ETA=18:24:27, max mem: 15.9 GB 
[10/31 15:49:05 visual_prompt]: 	Training 400/1106. train loss: 0.6420,	0.6688 s / batch. (data: 1.57e-02). ETA=19:51:19, max mem: 15.9 GB 
[10/31 15:50:09 visual_prompt]: 	Training 500/1106. train loss: 2.2708,	0.6313 s / batch. (data: 3.70e-04). ETA=18:43:33, max mem: 15.9 GB 
[10/31 15:51:13 visual_prompt]: 	Training 600/1106. train loss: 0.1506,	0.6422 s / batch. (data: 3.60e-04). ETA=19:01:51, max mem: 15.9 GB 
[10/31 15:52:16 visual_prompt]: 	Training 700/1106. train loss: 1.2804,	0.6238 s / batch. (data: 3.69e-04). ETA=18:28:08, max mem: 15.9 GB 
[10/31 15:53:20 visual_prompt]: 	Training 800/1106. train loss: 0.1270,	0.6204 s / batch. (data: 3.54e-04). ETA=18:21:05, max mem: 15.9 GB 
[10/31 15:54:23 visual_prompt]: 	Training 900/1106. train loss: 0.9306,	0.6455 s / batch. (data: 1.11e-02). ETA=19:04:34, max mem: 15.9 GB 
[10/31 15:55:27 visual_prompt]: 	Training 1000/1106. train loss: 0.5554,	0.6193 s / batch. (data: 3.35e-04). ETA=18:16:59, max mem: 15.9 GB 
[10/31 15:56:31 visual_prompt]: 	Training 1100/1106. train loss: 0.7372,	0.6190 s / batch. (data: 2.20e-04). ETA=18:15:31, max mem: 15.9 GB 
[10/31 15:56:35 visual_prompt]: Epoch 4 / 100: avg data time: 5.96e-03, avg batch time: 0.6391, average train loss: 0.7875
[10/31 15:57:29 visual_prompt]: 	Test 100/123. loss: 0.973, 0.2274 s / batch. (data: 5.79e-05)max mem: 15.94594 GB 
[10/31 15:57:41 visual_prompt]: Inference (val):avg data time: 4.93e-05, avg batch time: 0.2340, average loss: 0.8656
[10/31 15:57:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.41	
[10/31 15:57:41 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/31 15:58:47 visual_prompt]: 	Training 100/1106. train loss: 1.2695,	0.6167 s / batch. (data: 3.91e-04). ETA=18:10:12, max mem: 15.9 GB 
[10/31 15:59:50 visual_prompt]: 	Training 200/1106. train loss: 1.1320,	0.6309 s / batch. (data: 3.32e-04). ETA=18:34:16, max mem: 15.9 GB 
[10/31 16:00:54 visual_prompt]: 	Training 300/1106. train loss: 0.7382,	0.6196 s / batch. (data: 3.80e-04). ETA=18:13:25, max mem: 15.9 GB 
[10/31 16:01:58 visual_prompt]: 	Training 400/1106. train loss: 0.9286,	0.6680 s / batch. (data: 8.78e-04). ETA=19:37:39, max mem: 15.9 GB 
[10/31 16:03:02 visual_prompt]: 	Training 500/1106. train loss: 0.1084,	0.6442 s / batch. (data: 1.35e-03). ETA=18:54:37, max mem: 15.9 GB 
[10/31 16:04:05 visual_prompt]: 	Training 600/1106. train loss: 1.7937,	0.6181 s / batch. (data: 3.47e-04). ETA=18:07:40, max mem: 15.9 GB 
[10/31 16:05:09 visual_prompt]: 	Training 700/1106. train loss: 0.6457,	0.6439 s / batch. (data: 4.64e-04). ETA=18:51:55, max mem: 15.9 GB 
[10/31 16:06:13 visual_prompt]: 	Training 800/1106. train loss: 1.1621,	0.6434 s / batch. (data: 1.09e-03). ETA=18:50:00, max mem: 15.9 GB 
[10/31 16:07:17 visual_prompt]: 	Training 900/1106. train loss: 0.7824,	0.6353 s / batch. (data: 8.41e-04). ETA=18:34:45, max mem: 15.9 GB 
[10/31 16:08:20 visual_prompt]: 	Training 1000/1106. train loss: 0.3092,	0.6325 s / batch. (data: 8.73e-04). ETA=18:28:44, max mem: 15.9 GB 
[10/31 16:09:24 visual_prompt]: 	Training 1100/1106. train loss: 0.9233,	0.6207 s / batch. (data: 1.92e-04). ETA=18:07:05, max mem: 15.9 GB 
[10/31 16:09:28 visual_prompt]: Epoch 5 / 100: avg data time: 5.79e-03, avg batch time: 0.6395, average train loss: 0.7966
[10/31 16:10:22 visual_prompt]: 	Test 100/123. loss: 0.968, 0.2255 s / batch. (data: 4.86e-05)max mem: 15.94594 GB 
[10/31 16:10:34 visual_prompt]: Inference (val):avg data time: 4.77e-05, avg batch time: 0.2322, average loss: 0.9602
[10/31 16:10:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.12	
[10/31 16:10:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/31 16:11:40 visual_prompt]: 	Training 100/1106. train loss: 0.8231,	0.6400 s / batch. (data: 3.48e-04). ETA=18:39:39, max mem: 15.9 GB 
[10/31 16:12:43 visual_prompt]: 	Training 200/1106. train loss: 0.4599,	0.6317 s / batch. (data: 8.91e-04). ETA=18:24:04, max mem: 15.9 GB 
[10/31 16:13:47 visual_prompt]: 	Training 300/1106. train loss: 0.0973,	0.6400 s / batch. (data: 9.25e-04). ETA=18:37:31, max mem: 15.9 GB 
[10/31 16:14:51 visual_prompt]: 	Training 400/1106. train loss: 0.8475,	0.6280 s / batch. (data: 3.37e-04). ETA=18:15:35, max mem: 15.9 GB 
[10/31 16:15:54 visual_prompt]: 	Training 500/1106. train loss: 0.9030,	0.6405 s / batch. (data: 3.43e-04). ETA=18:36:21, max mem: 15.9 GB 
[10/31 16:16:58 visual_prompt]: 	Training 600/1106. train loss: 0.6095,	0.6191 s / batch. (data: 3.56e-04). ETA=17:57:58, max mem: 15.9 GB 
[10/31 16:18:02 visual_prompt]: 	Training 700/1106. train loss: 0.7524,	0.6331 s / batch. (data: 1.07e-02). ETA=18:21:18, max mem: 15.9 GB 
[10/31 16:19:06 visual_prompt]: 	Training 800/1106. train loss: 0.7755,	0.6642 s / batch. (data: 4.02e-02). ETA=19:14:20, max mem: 15.9 GB 
[10/31 16:20:09 visual_prompt]: 	Training 900/1106. train loss: 0.3916,	0.6342 s / batch. (data: 3.38e-04). ETA=18:21:05, max mem: 15.9 GB 
[10/31 16:21:13 visual_prompt]: 	Training 1000/1106. train loss: 2.1640,	0.6600 s / batch. (data: 3.55e-04). ETA=19:04:48, max mem: 15.9 GB 
[10/31 16:22:16 visual_prompt]: 	Training 1100/1106. train loss: 0.5955,	0.6232 s / batch. (data: 2.02e-04). ETA=17:59:56, max mem: 15.9 GB 
[10/31 16:22:20 visual_prompt]: Epoch 6 / 100: avg data time: 5.74e-03, avg batch time: 0.6385, average train loss: 0.7808
[10/31 16:23:18 visual_prompt]: 	Test 100/123. loss: 0.861, 0.2244 s / batch. (data: 5.32e-05)max mem: 15.94594 GB 
[10/31 16:23:30 visual_prompt]: Inference (val):avg data time: 1.35e-04, avg batch time: 0.2314, average loss: 0.7705
[10/31 16:23:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.52	
[10/31 16:23:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/31 16:24:36 visual_prompt]: 	Training 100/1106. train loss: 0.6592,	0.6168 s / batch. (data: 3.49e-04). ETA=17:47:41, max mem: 15.9 GB 
[10/31 16:25:40 visual_prompt]: 	Training 200/1106. train loss: 0.9607,	0.6404 s / batch. (data: 3.51e-04). ETA=18:27:34, max mem: 15.9 GB 
[10/31 16:26:43 visual_prompt]: 	Training 300/1106. train loss: 0.7605,	0.6474 s / batch. (data: 1.66e-02). ETA=18:38:34, max mem: 15.9 GB 
[10/31 16:27:47 visual_prompt]: 	Training 400/1106. train loss: 1.0636,	0.6320 s / batch. (data: 3.48e-04). ETA=18:10:52, max mem: 15.9 GB 
[10/31 16:28:51 visual_prompt]: 	Training 500/1106. train loss: 0.6468,	0.6248 s / batch. (data: 5.52e-03). ETA=17:57:23, max mem: 15.9 GB 
[10/31 16:29:55 visual_prompt]: 	Training 600/1106. train loss: 0.6920,	0.6240 s / batch. (data: 3.28e-04). ETA=17:54:56, max mem: 15.9 GB 
[10/31 16:31:01 visual_prompt]: 	Training 700/1106. train loss: 0.3716,	0.6202 s / batch. (data: 8.65e-04). ETA=17:47:22, max mem: 15.9 GB 
[10/31 16:32:04 visual_prompt]: 	Training 800/1106. train loss: 0.2898,	0.6297 s / batch. (data: 8.50e-04). ETA=18:02:45, max mem: 15.9 GB 
[10/31 16:33:08 visual_prompt]: 	Training 900/1106. train loss: 0.7170,	0.6215 s / batch. (data: 3.45e-04). ETA=17:47:30, max mem: 15.9 GB 
[10/31 16:34:11 visual_prompt]: 	Training 1000/1106. train loss: 0.8332,	0.6421 s / batch. (data: 3.53e-04). ETA=18:21:50, max mem: 15.9 GB 
[10/31 16:35:14 visual_prompt]: 	Training 1100/1106. train loss: 0.4431,	0.6333 s / batch. (data: 2.93e-04). ETA=18:05:43, max mem: 15.9 GB 
[10/31 16:35:18 visual_prompt]: Epoch 7 / 100: avg data time: 9.57e-03, avg batch time: 0.6405, average train loss: 0.7734
[10/31 16:36:12 visual_prompt]: 	Test 100/123. loss: 0.790, 0.2397 s / batch. (data: 3.70e-05)max mem: 15.94594 GB 
[10/31 16:36:24 visual_prompt]: Inference (val):avg data time: 4.74e-05, avg batch time: 0.2330, average loss: 0.7238
[10/31 16:36:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.63	
[10/31 16:36:24 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/31 16:37:30 visual_prompt]: 	Training 100/1106. train loss: 0.7213,	0.6468 s / batch. (data: 8.80e-04). ETA=18:27:42, max mem: 15.9 GB 
[10/31 16:38:33 visual_prompt]: 	Training 200/1106. train loss: 0.8435,	0.6400 s / batch. (data: 8.12e-04). ETA=18:15:00, max mem: 15.9 GB 
[10/31 16:39:37 visual_prompt]: 	Training 300/1106. train loss: 0.2596,	0.6471 s / batch. (data: 8.29e-04). ETA=18:26:04, max mem: 15.9 GB 
[10/31 16:40:41 visual_prompt]: 	Training 400/1106. train loss: 1.3632,	0.6382 s / batch. (data: 8.60e-04). ETA=18:09:50, max mem: 15.9 GB 
[10/31 16:41:45 visual_prompt]: 	Training 500/1106. train loss: 0.3316,	0.6547 s / batch. (data: 8.44e-04). ETA=18:36:51, max mem: 15.9 GB 
[10/31 16:42:48 visual_prompt]: 	Training 600/1106. train loss: 1.1448,	0.6330 s / batch. (data: 8.18e-04). ETA=17:58:46, max mem: 15.9 GB 
[10/31 16:43:52 visual_prompt]: 	Training 700/1106. train loss: 1.0005,	0.6451 s / batch. (data: 8.71e-04). ETA=18:18:23, max mem: 15.9 GB 
[10/31 16:44:56 visual_prompt]: 	Training 800/1106. train loss: 0.9169,	0.6315 s / batch. (data: 8.31e-04). ETA=17:54:13, max mem: 15.9 GB 
[10/31 16:45:59 visual_prompt]: 	Training 900/1106. train loss: 0.2076,	0.6314 s / batch. (data: 5.51e-03). ETA=17:52:52, max mem: 15.9 GB 
[10/31 16:47:03 visual_prompt]: 	Training 1000/1106. train loss: 1.3337,	0.6194 s / batch. (data: 3.66e-04). ETA=17:31:27, max mem: 15.9 GB 
[10/31 16:48:07 visual_prompt]: 	Training 1100/1106. train loss: 0.6355,	0.6190 s / batch. (data: 1.66e-04). ETA=17:29:48, max mem: 15.9 GB 
[10/31 16:48:10 visual_prompt]: Epoch 8 / 100: avg data time: 5.77e-03, avg batch time: 0.6389, average train loss: 0.8244
[10/31 16:49:04 visual_prompt]: 	Test 100/123. loss: 0.883, 0.2258 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[10/31 16:49:16 visual_prompt]: Inference (val):avg data time: 1.38e-04, avg batch time: 0.2335, average loss: 0.8707
[10/31 16:49:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.01	
[10/31 16:49:16 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/31 16:50:23 visual_prompt]: 	Training 100/1106. train loss: 0.2924,	0.6176 s / batch. (data: 3.69e-04). ETA=17:26:15, max mem: 15.9 GB 
[10/31 16:51:27 visual_prompt]: 	Training 200/1106. train loss: 0.1983,	0.6338 s / batch. (data: 1.07e-02). ETA=17:52:42, max mem: 15.9 GB 
[10/31 16:52:30 visual_prompt]: 	Training 300/1106. train loss: 1.0524,	0.6284 s / batch. (data: 3.11e-04). ETA=17:42:29, max mem: 15.9 GB 
[10/31 16:53:34 visual_prompt]: 	Training 400/1106. train loss: 0.7107,	0.6274 s / batch. (data: 3.30e-04). ETA=17:39:51, max mem: 15.9 GB 
[10/31 16:54:37 visual_prompt]: 	Training 500/1106. train loss: 0.7834,	0.6433 s / batch. (data: 8.98e-04). ETA=18:05:36, max mem: 15.9 GB 
[10/31 16:55:41 visual_prompt]: 	Training 600/1106. train loss: 0.6669,	0.6196 s / batch. (data: 3.58e-04). ETA=17:24:29, max mem: 15.9 GB 
[10/31 16:56:45 visual_prompt]: 	Training 700/1106. train loss: 1.4230,	0.6440 s / batch. (data: 3.60e-04). ETA=18:04:40, max mem: 15.9 GB 
[10/31 16:57:49 visual_prompt]: 	Training 800/1106. train loss: 0.9025,	0.6188 s / batch. (data: 3.18e-04). ETA=17:21:11, max mem: 15.9 GB 
[10/31 16:58:52 visual_prompt]: 	Training 900/1106. train loss: 0.8741,	0.6480 s / batch. (data: 8.47e-04). ETA=18:09:11, max mem: 15.9 GB 
[10/31 16:59:56 visual_prompt]: 	Training 1000/1106. train loss: 0.8516,	0.6184 s / batch. (data: 3.12e-04). ETA=17:18:25, max mem: 15.9 GB 
[10/31 17:01:00 visual_prompt]: 	Training 1100/1106. train loss: 0.6950,	0.6180 s / batch. (data: 1.51e-04). ETA=17:16:46, max mem: 15.9 GB 
[10/31 17:01:03 visual_prompt]: Epoch 9 / 100: avg data time: 6.18e-03, avg batch time: 0.6396, average train loss: 0.7675
[10/31 17:01:57 visual_prompt]: 	Test 100/123. loss: 0.748, 0.2408 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[10/31 17:02:09 visual_prompt]: Inference (val):avg data time: 1.31e-04, avg batch time: 0.2338, average loss: 0.7126
[10/31 17:02:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 57.06	
[10/31 17:02:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/31 17:03:16 visual_prompt]: 	Training 100/1106. train loss: 0.8279,	0.6200 s / batch. (data: 3.33e-04). ETA=17:18:59, max mem: 15.9 GB 
[10/31 17:04:19 visual_prompt]: 	Training 200/1106. train loss: 1.1037,	0.6263 s / batch. (data: 3.54e-04). ETA=17:28:31, max mem: 15.9 GB 
[10/31 17:05:23 visual_prompt]: 	Training 300/1106. train loss: 1.0098,	0.6187 s / batch. (data: 3.28e-04). ETA=17:14:43, max mem: 15.9 GB 
[10/31 17:06:27 visual_prompt]: 	Training 400/1106. train loss: 0.6202,	0.6312 s / batch. (data: 8.24e-04). ETA=17:34:32, max mem: 15.9 GB 
[10/31 17:07:30 visual_prompt]: 	Training 500/1106. train loss: 0.7525,	0.6400 s / batch. (data: 3.63e-04). ETA=17:48:13, max mem: 15.9 GB 
[10/31 17:08:34 visual_prompt]: 	Training 600/1106. train loss: 0.5471,	0.6400 s / batch. (data: 3.54e-04). ETA=17:47:08, max mem: 15.9 GB 
[10/31 17:09:38 visual_prompt]: 	Training 700/1106. train loss: 1.0615,	0.6279 s / batch. (data: 3.49e-04). ETA=17:25:56, max mem: 15.9 GB 
[10/31 17:10:42 visual_prompt]: 	Training 800/1106. train loss: 0.8137,	0.6680 s / batch. (data: 8.16e-04). ETA=18:31:38, max mem: 15.9 GB 
[10/31 17:11:45 visual_prompt]: 	Training 900/1106. train loss: 0.6285,	0.6518 s / batch. (data: 8.44e-04). ETA=18:03:36, max mem: 15.9 GB 
[10/31 17:12:49 visual_prompt]: 	Training 1000/1106. train loss: 0.8394,	0.6322 s / batch. (data: 3.44e-04). ETA=17:29:59, max mem: 15.9 GB 
[10/31 17:13:53 visual_prompt]: 	Training 1100/1106. train loss: 0.2950,	0.6181 s / batch. (data: 1.79e-04). ETA=17:05:31, max mem: 15.9 GB 
[10/31 17:13:57 visual_prompt]: Epoch 10 / 100: avg data time: 6.44e-03, avg batch time: 0.6400, average train loss: 0.8495
[10/31 17:14:51 visual_prompt]: 	Test 100/123. loss: 0.713, 0.2249 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[10/31 17:15:02 visual_prompt]: Inference (val):avg data time: 7.56e-05, avg batch time: 0.2324, average loss: 0.6822
[10/31 17:15:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.89	
[10/31 17:15:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/31 17:16:09 visual_prompt]: 	Training 100/1106. train loss: 0.5799,	0.6578 s / batch. (data: 1.01e-03). ETA=18:10:12, max mem: 15.9 GB 
[10/31 17:17:12 visual_prompt]: 	Training 200/1106. train loss: 1.1923,	0.6438 s / batch. (data: 8.81e-04). ETA=17:45:57, max mem: 15.9 GB 
[10/31 17:18:16 visual_prompt]: 	Training 300/1106. train loss: 0.8601,	0.6320 s / batch. (data: 8.29e-04). ETA=17:25:21, max mem: 15.9 GB 
[10/31 17:19:20 visual_prompt]: 	Training 400/1106. train loss: 1.2379,	0.6392 s / batch. (data: 8.54e-04). ETA=17:36:14, max mem: 15.9 GB 
[10/31 17:20:24 visual_prompt]: 	Training 500/1106. train loss: 1.2205,	0.6327 s / batch. (data: 8.69e-04). ETA=17:24:23, max mem: 15.9 GB 
[10/31 17:21:27 visual_prompt]: 	Training 600/1106. train loss: 0.1713,	0.6324 s / batch. (data: 8.46e-04). ETA=17:22:47, max mem: 15.9 GB 
[10/31 17:22:31 visual_prompt]: 	Training 700/1106. train loss: 1.1361,	0.6422 s / batch. (data: 8.56e-04). ETA=17:37:56, max mem: 15.9 GB 
[10/31 17:23:34 visual_prompt]: 	Training 800/1106. train loss: 0.2608,	0.6235 s / batch. (data: 3.54e-04). ETA=17:06:05, max mem: 15.9 GB 
[10/31 17:24:38 visual_prompt]: 	Training 900/1106. train loss: 0.3603,	0.6191 s / batch. (data: 3.51e-04). ETA=16:57:51, max mem: 15.9 GB 
[10/31 17:25:42 visual_prompt]: 	Training 1000/1106. train loss: 0.2978,	0.6185 s / batch. (data: 3.24e-04). ETA=16:55:51, max mem: 15.9 GB 
[10/31 17:26:45 visual_prompt]: 	Training 1100/1106. train loss: 1.1115,	0.6190 s / batch. (data: 2.07e-04). ETA=16:55:37, max mem: 15.9 GB 
[10/31 17:26:49 visual_prompt]: Epoch 11 / 100: avg data time: 5.98e-03, avg batch time: 0.6389, average train loss: 0.7930
[10/31 17:27:43 visual_prompt]: 	Test 100/123. loss: 0.728, 0.2257 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/31 17:27:54 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.2324, average loss: 0.6862
[10/31 17:27:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.00	
[10/31 17:27:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/31 17:29:02 visual_prompt]: 	Training 100/1106. train loss: 0.2864,	0.6318 s / batch. (data: 9.38e-04). ETA=17:15:28, max mem: 15.9 GB 
[10/31 17:30:05 visual_prompt]: 	Training 200/1106. train loss: 0.7616,	0.6190 s / batch. (data: 3.45e-04). ETA=16:53:27, max mem: 15.9 GB 
[10/31 17:31:09 visual_prompt]: 	Training 300/1106. train loss: 0.0987,	0.6227 s / batch. (data: 5.52e-03). ETA=16:58:29, max mem: 15.9 GB 
[10/31 17:32:13 visual_prompt]: 	Training 400/1106. train loss: 0.6611,	0.6497 s / batch. (data: 2.20e-02). ETA=17:41:29, max mem: 15.9 GB 
[10/31 17:33:17 visual_prompt]: 	Training 500/1106. train loss: 1.7909,	0.6280 s / batch. (data: 4.03e-03). ETA=17:05:00, max mem: 15.9 GB 
[10/31 17:34:20 visual_prompt]: 	Training 600/1106. train loss: 0.6920,	0.6187 s / batch. (data: 3.11e-04). ETA=16:48:51, max mem: 15.9 GB 
[10/31 17:35:24 visual_prompt]: 	Training 700/1106. train loss: 0.5744,	0.6243 s / batch. (data: 3.86e-04). ETA=16:56:54, max mem: 15.9 GB 
[10/31 17:36:28 visual_prompt]: 	Training 800/1106. train loss: 0.8572,	0.6190 s / batch. (data: 3.72e-04). ETA=16:47:12, max mem: 15.9 GB 
[10/31 17:37:31 visual_prompt]: 	Training 900/1106. train loss: 0.1253,	0.6321 s / batch. (data: 3.59e-04). ETA=17:07:27, max mem: 15.9 GB 
[10/31 17:38:35 visual_prompt]: 	Training 1000/1106. train loss: 0.9889,	0.6339 s / batch. (data: 9.53e-04). ETA=17:09:23, max mem: 15.9 GB 
[10/31 17:39:38 visual_prompt]: 	Training 1100/1106. train loss: 0.8004,	0.6183 s / batch. (data: 1.86e-04). ETA=16:43:04, max mem: 15.9 GB 
[10/31 17:39:42 visual_prompt]: Epoch 12 / 100: avg data time: 6.79e-03, avg batch time: 0.6399, average train loss: 0.8107
[10/31 17:40:36 visual_prompt]: 	Test 100/123. loss: 1.834, 0.2277 s / batch. (data: 3.74e-05)max mem: 15.94594 GB 
[10/31 17:40:48 visual_prompt]: Inference (val):avg data time: 4.97e-05, avg batch time: 0.2333, average loss: 1.9176
[10/31 17:40:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.21	
[10/31 17:40:48 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/31 17:41:54 visual_prompt]: 	Training 100/1106. train loss: 1.0435,	0.6490 s / batch. (data: 1.11e-02). ETA=17:31:36, max mem: 15.9 GB 
[10/31 17:42:58 visual_prompt]: 	Training 200/1106. train loss: 0.7115,	0.6375 s / batch. (data: 3.58e-04). ETA=17:11:55, max mem: 15.9 GB 
[10/31 17:44:01 visual_prompt]: 	Training 300/1106. train loss: 0.3402,	0.6411 s / batch. (data: 8.63e-04). ETA=17:16:48, max mem: 15.9 GB 
[10/31 17:45:05 visual_prompt]: 	Training 400/1106. train loss: 0.7407,	0.6431 s / batch. (data: 8.02e-04). ETA=17:18:53, max mem: 15.9 GB 
[10/31 17:46:08 visual_prompt]: 	Training 500/1106. train loss: 0.1830,	0.6337 s / batch. (data: 3.24e-04). ETA=17:02:38, max mem: 15.9 GB 
[10/31 17:47:12 visual_prompt]: 	Training 600/1106. train loss: 1.0651,	0.6278 s / batch. (data: 3.77e-04). ETA=16:52:02, max mem: 15.9 GB 
[10/31 17:48:16 visual_prompt]: 	Training 700/1106. train loss: 0.8905,	0.6348 s / batch. (data: 8.76e-04). ETA=17:02:22, max mem: 15.9 GB 
[10/31 17:49:20 visual_prompt]: 	Training 800/1106. train loss: 1.3774,	0.6240 s / batch. (data: 3.24e-04). ETA=16:43:50, max mem: 15.9 GB 
[10/31 17:50:24 visual_prompt]: 	Training 900/1106. train loss: 1.0053,	0.6485 s / batch. (data: 3.63e-04). ETA=17:22:14, max mem: 15.9 GB 
[10/31 17:51:27 visual_prompt]: 	Training 1000/1106. train loss: 0.2891,	0.6198 s / batch. (data: 3.41e-04). ETA=16:35:08, max mem: 15.9 GB 
[10/31 17:52:31 visual_prompt]: 	Training 1100/1106. train loss: 0.7687,	0.6186 s / batch. (data: 2.18e-04). ETA=16:32:08, max mem: 15.9 GB 
[10/31 17:52:35 visual_prompt]: Epoch 13 / 100: avg data time: 5.79e-03, avg batch time: 0.6392, average train loss: 0.8129
[10/31 17:53:29 visual_prompt]: 	Test 100/123. loss: 0.857, 0.2367 s / batch. (data: 6.15e-05)max mem: 15.94594 GB 
[10/31 17:53:40 visual_prompt]: Inference (val):avg data time: 4.74e-05, avg batch time: 0.2311, average loss: 0.7994
[10/31 17:53:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.67	
[10/31 17:53:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/31 17:54:46 visual_prompt]: 	Training 100/1106. train loss: 0.6330,	0.6326 s / batch. (data: 9.33e-04). ETA=16:53:30, max mem: 15.9 GB 
[10/31 17:55:50 visual_prompt]: 	Training 200/1106. train loss: 0.9363,	0.6483 s / batch. (data: 2.08e-02). ETA=17:17:35, max mem: 15.9 GB 
[10/31 17:56:54 visual_prompt]: 	Training 300/1106. train loss: 0.7031,	0.6483 s / batch. (data: 8.83e-04). ETA=17:16:21, max mem: 15.9 GB 
[10/31 17:57:57 visual_prompt]: 	Training 400/1106. train loss: 0.3993,	0.6432 s / batch. (data: 1.25e-02). ETA=17:07:13, max mem: 15.9 GB 
[10/31 17:59:01 visual_prompt]: 	Training 500/1106. train loss: 0.7254,	0.6340 s / batch. (data: 8.44e-04). ETA=16:51:25, max mem: 15.9 GB 
[10/31 18:00:05 visual_prompt]: 	Training 600/1106. train loss: 0.6590,	0.6293 s / batch. (data: 3.37e-04). ETA=16:42:55, max mem: 15.9 GB 
[10/31 18:01:09 visual_prompt]: 	Training 700/1106. train loss: 0.2021,	0.6364 s / batch. (data: 8.96e-04). ETA=16:53:10, max mem: 15.9 GB 
[10/31 18:02:12 visual_prompt]: 	Training 800/1106. train loss: 0.6257,	0.6349 s / batch. (data: 9.76e-04). ETA=16:49:41, max mem: 15.9 GB 
[10/31 18:03:16 visual_prompt]: 	Training 900/1106. train loss: 0.8699,	0.6446 s / batch. (data: 8.48e-04). ETA=17:04:04, max mem: 15.9 GB 
[10/31 18:04:20 visual_prompt]: 	Training 1000/1106. train loss: 0.0523,	0.6339 s / batch. (data: 3.89e-04). ETA=16:46:01, max mem: 15.9 GB 
[10/31 18:05:23 visual_prompt]: 	Training 1100/1106. train loss: 0.8448,	0.6187 s / batch. (data: 1.61e-04). ETA=16:20:54, max mem: 15.9 GB 
[10/31 18:05:27 visual_prompt]: Epoch 14 / 100: avg data time: 5.70e-03, avg batch time: 0.6392, average train loss: 0.7792
[10/31 18:06:21 visual_prompt]: 	Test 100/123. loss: 0.688, 0.2405 s / batch. (data: 5.08e-05)max mem: 15.94594 GB 
[10/31 18:06:33 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.2341, average loss: 0.6749
[10/31 18:06:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 59.79	
[10/31 18:06:33 visual_prompt]: Best epoch 14: best metric: -0.675
[10/31 18:06:33 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/31 18:07:38 visual_prompt]: 	Training 100/1106. train loss: 0.8425,	0.6189 s / batch. (data: 3.36e-04). ETA=16:20:06, max mem: 15.9 GB 
[10/31 18:08:42 visual_prompt]: 	Training 200/1106. train loss: 0.9113,	0.6246 s / batch. (data: 3.28e-04). ETA=16:28:00, max mem: 15.9 GB 
[10/31 18:09:45 visual_prompt]: 	Training 300/1106. train loss: 0.6291,	0.6552 s / batch. (data: 8.41e-04). ETA=17:15:20, max mem: 15.9 GB 
[10/31 18:10:49 visual_prompt]: 	Training 400/1106. train loss: 0.9593,	0.6302 s / batch. (data: 8.51e-04). ETA=16:34:53, max mem: 15.9 GB 
[10/31 18:11:53 visual_prompt]: 	Training 500/1106. train loss: 0.3062,	0.6345 s / batch. (data: 5.52e-03). ETA=16:40:31, max mem: 15.9 GB 
[10/31 18:12:57 visual_prompt]: 	Training 600/1106. train loss: 1.1479,	0.6250 s / batch. (data: 3.45e-04). ETA=16:24:32, max mem: 15.9 GB 
[10/31 18:14:00 visual_prompt]: 	Training 700/1106. train loss: 0.5001,	0.6277 s / batch. (data: 5.51e-03). ETA=16:27:44, max mem: 15.9 GB 
[10/31 18:15:04 visual_prompt]: 	Training 800/1106. train loss: 0.6024,	0.6426 s / batch. (data: 3.59e-04). ETA=16:50:03, max mem: 15.9 GB 
[10/31 18:16:08 visual_prompt]: 	Training 900/1106. train loss: 0.7827,	0.6182 s / batch. (data: 3.44e-04). ETA=16:10:40, max mem: 15.9 GB 
[10/31 18:17:11 visual_prompt]: 	Training 1000/1106. train loss: 0.5914,	0.6392 s / batch. (data: 3.40e-04). ETA=16:42:39, max mem: 15.9 GB 
[10/31 18:18:15 visual_prompt]: 	Training 1100/1106. train loss: 1.0655,	0.6195 s / batch. (data: 2.18e-04). ETA=16:10:44, max mem: 15.9 GB 
[10/31 18:18:19 visual_prompt]: Epoch 15 / 100: avg data time: 5.25e-03, avg batch time: 0.6385, average train loss: 0.8231
[10/31 18:19:13 visual_prompt]: 	Test 100/123. loss: 0.907, 0.2251 s / batch. (data: 6.41e-05)max mem: 15.94594 GB 
[10/31 18:19:24 visual_prompt]: Inference (val):avg data time: 1.04e-04, avg batch time: 0.2331, average loss: 0.8330
[10/31 18:19:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.12	
[10/31 18:19:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/31 18:20:30 visual_prompt]: 	Training 100/1106. train loss: 0.3401,	0.6327 s / batch. (data: 1.70e-02). ETA=16:30:19, max mem: 15.9 GB 
[10/31 18:21:33 visual_prompt]: 	Training 200/1106. train loss: 0.7530,	0.6332 s / batch. (data: 8.43e-04). ETA=16:30:00, max mem: 15.9 GB 
[10/31 18:22:37 visual_prompt]: 	Training 300/1106. train loss: 0.6450,	0.6195 s / batch. (data: 3.25e-04). ETA=16:07:31, max mem: 15.9 GB 
[10/31 18:23:41 visual_prompt]: 	Training 400/1106. train loss: 0.6505,	0.6554 s / batch. (data: 2.26e-03). ETA=17:02:30, max mem: 15.9 GB 
[10/31 18:24:44 visual_prompt]: 	Training 500/1106. train loss: 0.8569,	0.6413 s / batch. (data: 8.29e-04). ETA=16:39:30, max mem: 15.9 GB 
[10/31 18:25:48 visual_prompt]: 	Training 600/1106. train loss: 1.5018,	0.6202 s / batch. (data: 3.08e-04). ETA=16:05:33, max mem: 15.9 GB 
[10/31 18:26:51 visual_prompt]: 	Training 700/1106. train loss: 1.1234,	0.6187 s / batch. (data: 3.36e-04). ETA=16:02:11, max mem: 15.9 GB 
[10/31 18:27:55 visual_prompt]: 	Training 800/1106. train loss: 0.7426,	0.6199 s / batch. (data: 3.56e-04). ETA=16:02:57, max mem: 15.9 GB 
[10/31 18:28:59 visual_prompt]: 	Training 900/1106. train loss: 0.5714,	0.6351 s / batch. (data: 9.01e-04). ETA=16:25:37, max mem: 15.9 GB 
[10/31 18:30:02 visual_prompt]: 	Training 1000/1106. train loss: 1.0056,	0.6189 s / batch. (data: 5.19e-04). ETA=15:59:27, max mem: 15.9 GB 
[10/31 18:31:06 visual_prompt]: 	Training 1100/1106. train loss: 0.2749,	0.6181 s / batch. (data: 1.69e-04). ETA=15:57:07, max mem: 15.9 GB 
[10/31 18:31:10 visual_prompt]: Epoch 16 / 100: avg data time: 5.04e-03, avg batch time: 0.6377, average train loss: 0.7741
[10/31 18:32:03 visual_prompt]: 	Test 100/123. loss: 0.685, 0.2477 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/31 18:32:15 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.2326, average loss: 0.6695
[10/31 18:32:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 61.89	
[10/31 18:32:15 visual_prompt]: Best epoch 16: best metric: -0.669
[10/31 18:32:15 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/31 18:33:21 visual_prompt]: 	Training 100/1106. train loss: 1.5619,	0.6299 s / batch. (data: 3.49e-04). ETA=16:14:18, max mem: 15.9 GB 
[10/31 18:34:24 visual_prompt]: 	Training 200/1106. train loss: 0.4982,	0.6412 s / batch. (data: 8.57e-04). ETA=16:30:45, max mem: 15.9 GB 
[10/31 18:35:28 visual_prompt]: 	Training 300/1106. train loss: 0.8904,	0.6414 s / batch. (data: 1.19e-02). ETA=16:29:59, max mem: 15.9 GB 
[10/31 18:36:32 visual_prompt]: 	Training 400/1106. train loss: 2.0229,	0.6400 s / batch. (data: 3.57e-04). ETA=16:26:42, max mem: 15.9 GB 
[10/31 18:37:36 visual_prompt]: 	Training 500/1106. train loss: 0.1823,	0.6494 s / batch. (data: 8.74e-04). ETA=16:40:04, max mem: 15.9 GB 
[10/31 18:38:39 visual_prompt]: 	Training 600/1106. train loss: 1.0401,	0.6567 s / batch. (data: 5.56e-03). ETA=16:50:13, max mem: 15.9 GB 
[10/31 18:39:43 visual_prompt]: 	Training 700/1106. train loss: 0.7497,	0.6191 s / batch. (data: 3.22e-04). ETA=15:51:22, max mem: 15.9 GB 
[10/31 18:40:47 visual_prompt]: 	Training 800/1106. train loss: 1.1750,	0.6334 s / batch. (data: 3.70e-04). ETA=16:12:14, max mem: 15.9 GB 
[10/31 18:41:50 visual_prompt]: 	Training 900/1106. train loss: 0.9910,	0.6397 s / batch. (data: 9.03e-04). ETA=16:20:52, max mem: 15.9 GB 
[10/31 18:42:54 visual_prompt]: 	Training 1000/1106. train loss: 0.0700,	0.6210 s / batch. (data: 3.69e-04). ETA=15:51:14, max mem: 15.9 GB 
[10/31 18:43:58 visual_prompt]: 	Training 1100/1106. train loss: 0.2372,	0.6192 s / batch. (data: 2.26e-04). ETA=15:47:29, max mem: 15.9 GB 
[10/31 18:44:01 visual_prompt]: Epoch 17 / 100: avg data time: 6.00e-03, avg batch time: 0.6389, average train loss: 0.7884
[10/31 18:44:55 visual_prompt]: 	Test 100/123. loss: 0.717, 0.2315 s / batch. (data: 5.34e-05)max mem: 15.94594 GB 
[10/31 18:45:07 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.2343, average loss: 0.6764
[10/31 18:45:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 60.55	
[10/31 18:45:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/31 18:46:13 visual_prompt]: 	Training 100/1106. train loss: 0.6777,	0.6354 s / batch. (data: 3.50e-04). ETA=16:11:02, max mem: 15.9 GB 
[10/31 18:47:17 visual_prompt]: 	Training 200/1106. train loss: 0.8872,	0.6305 s / batch. (data: 1.20e-02). ETA=16:02:31, max mem: 15.9 GB 
[10/31 18:48:21 visual_prompt]: 	Training 300/1106. train loss: 0.9437,	0.6196 s / batch. (data: 3.50e-04). ETA=15:44:51, max mem: 15.9 GB 
[10/31 18:49:24 visual_prompt]: 	Training 400/1106. train loss: 0.6674,	0.6302 s / batch. (data: 3.44e-04). ETA=15:59:59, max mem: 15.9 GB 
[10/31 18:50:28 visual_prompt]: 	Training 500/1106. train loss: 0.2099,	0.6400 s / batch. (data: 7.93e-04). ETA=16:13:50, max mem: 15.9 GB 
[10/31 18:51:32 visual_prompt]: 	Training 600/1106. train loss: 0.4353,	0.6234 s / batch. (data: 3.55e-04). ETA=15:47:31, max mem: 15.9 GB 
[10/31 18:52:36 visual_prompt]: 	Training 700/1106. train loss: 0.5814,	0.6366 s / batch. (data: 1.33e-03). ETA=16:06:33, max mem: 15.9 GB 
[10/31 18:53:39 visual_prompt]: 	Training 800/1106. train loss: 0.5079,	0.6305 s / batch. (data: 8.38e-04). ETA=15:56:18, max mem: 15.9 GB 
[10/31 18:54:43 visual_prompt]: 	Training 900/1106. train loss: 1.1211,	0.6594 s / batch. (data: 9.75e-04). ETA=16:38:59, max mem: 15.9 GB 
[10/31 18:55:47 visual_prompt]: 	Training 1000/1106. train loss: 0.8090,	0.6237 s / batch. (data: 4.17e-04). ETA=15:43:52, max mem: 15.9 GB 
[10/31 18:56:50 visual_prompt]: 	Training 1100/1106. train loss: 0.5645,	0.6190 s / batch. (data: 2.78e-04). ETA=15:35:43, max mem: 15.9 GB 
[10/31 18:56:54 visual_prompt]: Epoch 18 / 100: avg data time: 5.95e-03, avg batch time: 0.6392, average train loss: 0.8150
[10/31 18:57:48 visual_prompt]: 	Test 100/123. loss: 0.702, 0.2441 s / batch. (data: 6.34e-05)max mem: 15.94594 GB 
[10/31 18:58:00 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.2319, average loss: 0.6730
[10/31 18:58:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 60.90	
[10/31 18:58:00 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[10/31 18:59:06 visual_prompt]: 	Training 100/1106. train loss: 0.2882,	0.6283 s / batch. (data: 8.49e-04). ETA=15:48:41, max mem: 15.9 GB 
[10/31 19:00:10 visual_prompt]: 	Training 200/1106. train loss: 0.8031,	0.6464 s / batch. (data: 8.28e-04). ETA=16:14:57, max mem: 15.9 GB 
[10/31 19:01:14 visual_prompt]: 	Training 300/1106. train loss: 0.4049,	0.6442 s / batch. (data: 3.72e-04). ETA=16:10:34, max mem: 15.9 GB 
[10/31 19:02:17 visual_prompt]: 	Training 400/1106. train loss: 0.8086,	0.6433 s / batch. (data: 6.02e-03). ETA=16:08:06, max mem: 15.9 GB 
[10/31 19:03:21 visual_prompt]: 	Training 500/1106. train loss: 0.5549,	0.6409 s / batch. (data: 8.76e-04). ETA=16:03:24, max mem: 15.9 GB 
[10/31 19:04:25 visual_prompt]: 	Training 600/1106. train loss: 1.0915,	0.6316 s / batch. (data: 8.63e-04). ETA=15:48:26, max mem: 15.9 GB 
[10/31 19:05:29 visual_prompt]: 	Training 700/1106. train loss: 0.6963,	0.6431 s / batch. (data: 2.49e-02). ETA=16:04:31, max mem: 15.9 GB 
[10/31 19:06:32 visual_prompt]: 	Training 800/1106. train loss: 0.1819,	0.6326 s / batch. (data: 8.30e-04). ETA=15:47:46, max mem: 15.9 GB 
[10/31 19:07:36 visual_prompt]: 	Training 900/1106. train loss: 0.4120,	0.6320 s / batch. (data: 3.56e-04). ETA=15:45:48, max mem: 15.9 GB 
[10/31 19:08:39 visual_prompt]: 	Training 1000/1106. train loss: 0.9638,	0.6243 s / batch. (data: 5.50e-03). ETA=15:33:10, max mem: 15.9 GB 
[10/31 19:09:43 visual_prompt]: 	Training 1100/1106. train loss: 0.9728,	0.6360 s / batch. (data: 2.24e-04). ETA=15:49:40, max mem: 15.9 GB 
[10/31 19:09:47 visual_prompt]: Epoch 19 / 100: avg data time: 6.04e-03, avg batch time: 0.6395, average train loss: 0.7555
[10/31 19:10:41 visual_prompt]: 	Test 100/123. loss: 1.173, 0.2254 s / batch. (data: 4.55e-05)max mem: 15.94594 GB 
[10/31 19:10:53 visual_prompt]: Inference (val):avg data time: 5.03e-05, avg batch time: 0.2315, average loss: 1.0747
[10/31 19:10:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.90	
[10/31 19:10:53 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[10/31 19:11:59 visual_prompt]: 	Training 100/1106. train loss: 0.6613,	0.6350 s / batch. (data: 3.29e-04). ETA=15:47:01, max mem: 15.9 GB 
[10/31 19:13:03 visual_prompt]: 	Training 200/1106. train loss: 0.3720,	0.6312 s / batch. (data: 8.03e-04). ETA=15:40:16, max mem: 15.9 GB 
[10/31 19:14:07 visual_prompt]: 	Training 300/1106. train loss: 0.4757,	0.6470 s / batch. (data: 1.06e-02). ETA=16:02:43, max mem: 15.9 GB 
[10/31 19:15:11 visual_prompt]: 	Training 400/1106. train loss: 0.4105,	0.6355 s / batch. (data: 5.51e-03). ETA=15:44:37, max mem: 15.9 GB 
[10/31 19:16:14 visual_prompt]: 	Training 500/1106. train loss: 0.3979,	0.6596 s / batch. (data: 8.65e-04). ETA=16:19:18, max mem: 15.9 GB 
[10/31 19:17:18 visual_prompt]: 	Training 600/1106. train loss: 0.7665,	0.6346 s / batch. (data: 3.56e-04). ETA=15:41:12, max mem: 15.9 GB 
[10/31 19:18:22 visual_prompt]: 	Training 700/1106. train loss: 0.6039,	0.6296 s / batch. (data: 3.38e-04). ETA=15:32:41, max mem: 15.9 GB 
[10/31 19:19:25 visual_prompt]: 	Training 800/1106. train loss: 0.6741,	0.6560 s / batch. (data: 8.79e-04). ETA=16:10:39, max mem: 15.9 GB 
[10/31 19:20:29 visual_prompt]: 	Training 900/1106. train loss: 0.4665,	0.6323 s / batch. (data: 3.70e-04). ETA=15:34:39, max mem: 15.9 GB 
[10/31 19:21:33 visual_prompt]: 	Training 1000/1106. train loss: 0.6370,	0.6456 s / batch. (data: 8.78e-04). ETA=15:53:13, max mem: 15.9 GB 
[10/31 19:22:37 visual_prompt]: 	Training 1100/1106. train loss: 0.3144,	0.6301 s / batch. (data: 2.08e-04). ETA=15:29:13, max mem: 15.9 GB 
[10/31 19:22:41 visual_prompt]: Epoch 20 / 100: avg data time: 6.38e-03, avg batch time: 0.6402, average train loss: 0.7679
[10/31 19:23:35 visual_prompt]: 	Test 100/123. loss: 1.033, 0.2331 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[10/31 19:23:47 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.2322, average loss: 1.0129
[10/31 19:23:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.18	
[10/31 19:23:47 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[10/31 19:24:54 visual_prompt]: 	Training 100/1106. train loss: 0.7123,	0.6330 s / batch. (data: 4.63e-04). ETA=15:32:26, max mem: 15.9 GB 
[10/31 19:25:57 visual_prompt]: 	Training 200/1106. train loss: 0.7453,	0.6202 s / batch. (data: 3.47e-04). ETA=15:12:31, max mem: 15.9 GB 
[10/31 19:27:01 visual_prompt]: 	Training 300/1106. train loss: 0.5423,	0.6400 s / batch. (data: 3.76e-04). ETA=15:40:35, max mem: 15.9 GB 
[10/31 19:28:05 visual_prompt]: 	Training 400/1106. train loss: 0.9611,	0.6424 s / batch. (data: 5.52e-03). ETA=15:43:03, max mem: 15.9 GB 
[10/31 19:29:08 visual_prompt]: 	Training 500/1106. train loss: 1.6854,	0.6289 s / batch. (data: 3.59e-04). ETA=15:22:11, max mem: 15.9 GB 
[10/31 19:30:12 visual_prompt]: 	Training 600/1106. train loss: 1.9884,	0.6345 s / batch. (data: 3.71e-04). ETA=15:29:22, max mem: 15.9 GB 
[10/31 19:31:16 visual_prompt]: 	Training 700/1106. train loss: 0.8707,	0.6200 s / batch. (data: 3.59e-04). ETA=15:07:04, max mem: 15.9 GB 
[10/31 19:32:20 visual_prompt]: 	Training 800/1106. train loss: 0.7600,	0.6560 s / batch. (data: 8.88e-04). ETA=15:58:37, max mem: 15.9 GB 
[10/31 19:33:24 visual_prompt]: 	Training 900/1106. train loss: 0.0591,	0.6362 s / batch. (data: 3.72e-04). ETA=15:28:34, max mem: 15.9 GB 
[10/31 19:34:28 visual_prompt]: 	Training 1000/1106. train loss: 0.6139,	0.6190 s / batch. (data: 3.52e-04). ETA=15:02:26, max mem: 15.9 GB 
[10/31 19:35:31 visual_prompt]: 	Training 1100/1106. train loss: 0.7302,	0.6184 s / batch. (data: 2.31e-04). ETA=15:00:39, max mem: 15.9 GB 
[10/31 19:35:35 visual_prompt]: Epoch 21 / 100: avg data time: 6.55e-03, avg batch time: 0.6407, average train loss: 0.7663
[10/31 19:36:29 visual_prompt]: 	Test 100/123. loss: 0.703, 0.2411 s / batch. (data: 5.41e-05)max mem: 15.94594 GB 
[10/31 19:36:41 visual_prompt]: Inference (val):avg data time: 3.79e-04, avg batch time: 0.2341, average loss: 0.7111
[10/31 19:36:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 62.71	
[10/31 19:36:41 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[10/31 19:37:47 visual_prompt]: 	Training 100/1106. train loss: 1.0878,	0.6351 s / batch. (data: 1.50e-02). ETA=15:23:43, max mem: 15.9 GB 
[10/31 19:38:51 visual_prompt]: 	Training 200/1106. train loss: 0.4336,	0.6196 s / batch. (data: 3.32e-04). ETA=15:00:15, max mem: 15.9 GB 
[10/31 19:39:55 visual_prompt]: 	Training 300/1106. train loss: 0.9101,	0.6360 s / batch. (data: 3.41e-04). ETA=15:23:02, max mem: 15.9 GB 
[10/31 19:40:58 visual_prompt]: 	Training 400/1106. train loss: 0.1840,	0.6616 s / batch. (data: 8.32e-04). ETA=15:58:57, max mem: 15.9 GB 
[10/31 19:42:02 visual_prompt]: 	Training 500/1106. train loss: 0.2183,	0.6189 s / batch. (data: 3.65e-04). ETA=14:56:05, max mem: 15.9 GB 
[10/31 19:43:06 visual_prompt]: 	Training 600/1106. train loss: 0.2374,	0.6257 s / batch. (data: 3.69e-04). ETA=15:04:57, max mem: 15.9 GB 
[10/31 19:44:09 visual_prompt]: 	Training 700/1106. train loss: 1.4564,	0.6340 s / batch. (data: 4.24e-04). ETA=15:15:47, max mem: 15.9 GB 
[10/31 19:45:13 visual_prompt]: 	Training 800/1106. train loss: 0.7759,	0.6453 s / batch. (data: 8.54e-04). ETA=15:31:05, max mem: 15.9 GB 
[10/31 19:46:17 visual_prompt]: 	Training 900/1106. train loss: 0.8881,	0.6340 s / batch. (data: 8.65e-04). ETA=15:13:46, max mem: 15.9 GB 
[10/31 19:47:21 visual_prompt]: 	Training 1000/1106. train loss: 0.7686,	0.6480 s / batch. (data: 8.48e-04). ETA=15:32:49, max mem: 15.9 GB 
[10/31 19:48:25 visual_prompt]: 	Training 1100/1106. train loss: 0.4415,	0.6207 s / batch. (data: 2.16e-04). ETA=14:52:29, max mem: 15.9 GB 
[10/31 19:48:28 visual_prompt]: Epoch 22 / 100: avg data time: 5.54e-03, avg batch time: 0.6397, average train loss: 0.7474
[10/31 19:49:23 visual_prompt]: 	Test 100/123. loss: 0.764, 0.2256 s / batch. (data: 7.68e-05)max mem: 15.94594 GB 
[10/31 19:49:34 visual_prompt]: Inference (val):avg data time: 9.99e-05, avg batch time: 0.2328, average loss: 0.7852
[10/31 19:49:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 43.90	rocauc: 61.45	
[10/31 19:49:34 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[10/31 19:50:41 visual_prompt]: 	Training 100/1106. train loss: 0.4296,	0.6213 s / batch. (data: 3.46e-04). ETA=14:52:17, max mem: 15.9 GB 
[10/31 19:51:45 visual_prompt]: 	Training 200/1106. train loss: 0.8358,	0.6195 s / batch. (data: 3.53e-04). ETA=14:48:41, max mem: 15.9 GB 
[10/31 19:52:49 visual_prompt]: 	Training 300/1106. train loss: 0.6404,	0.6299 s / batch. (data: 3.99e-04). ETA=15:02:29, max mem: 15.9 GB 
[10/31 19:53:53 visual_prompt]: 	Training 400/1106. train loss: 0.8182,	0.6179 s / batch. (data: 3.10e-04). ETA=14:44:22, max mem: 15.9 GB 
[10/31 19:54:56 visual_prompt]: 	Training 500/1106. train loss: 0.8120,	0.6606 s / batch. (data: 1.62e-02). ETA=15:44:17, max mem: 15.9 GB 
[10/31 19:56:00 visual_prompt]: 	Training 600/1106. train loss: 0.6318,	0.6402 s / batch. (data: 8.57e-04). ETA=15:14:02, max mem: 15.9 GB 
[10/31 19:57:04 visual_prompt]: 	Training 700/1106. train loss: 0.4691,	0.6215 s / batch. (data: 3.63e-04). ETA=14:46:18, max mem: 15.9 GB 
[10/31 19:58:08 visual_prompt]: 	Training 800/1106. train loss: 0.7386,	0.6187 s / batch. (data: 3.91e-04). ETA=14:41:18, max mem: 15.9 GB 
[10/31 19:59:12 visual_prompt]: 	Training 900/1106. train loss: 0.7913,	0.6680 s / batch. (data: 8.70e-04). ETA=15:50:25, max mem: 15.9 GB 
[10/31 20:00:15 visual_prompt]: 	Training 1000/1106. train loss: 0.5713,	0.6280 s / batch. (data: 3.76e-04). ETA=14:52:26, max mem: 15.9 GB 
[10/31 20:01:19 visual_prompt]: 	Training 1100/1106. train loss: 0.8642,	0.6178 s / batch. (data: 1.77e-04). ETA=14:36:57, max mem: 15.9 GB 
[10/31 20:01:23 visual_prompt]: Epoch 23 / 100: avg data time: 7.01e-03, avg batch time: 0.6404, average train loss: 0.7575
[10/31 20:02:16 visual_prompt]: 	Test 100/123. loss: 0.651, 0.2277 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[10/31 20:02:28 visual_prompt]: Inference (val):avg data time: 5.03e-05, avg batch time: 0.2315, average loss: 0.6727
[10/31 20:02:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 62.27	
[10/31 20:02:28 visual_prompt]: Stopping early.
[10/31 20:02:30 visual_prompt]: Rank of current process: 0. World size: 1
[10/31 20:02:30 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/31 20:02:30 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/31 20:02:30 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/31 20:02:30 visual_prompt]: Training with config:
[10/31 20:02:30 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/test/seed9805/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 9805, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/31 20:02:30 visual_prompt]: Loading training data...
[10/31 20:02:30 visual_prompt]: Constructing mammo-cbis dataset train...
[10/31 20:02:30 visual_prompt]: Loading validation data...
[10/31 20:02:30 visual_prompt]: Constructing mammo-cbis dataset val...
[10/31 20:02:30 visual_prompt]: Loading test data...
[10/31 20:02:30 visual_prompt]: Constructing mammo-cbis dataset test...
[10/31 20:02:30 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[10/31 20:02:35 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[10/31 20:02:35 visual_prompt]: tuned percent:0.522
[10/31 20:02:35 visual_prompt]: Device used for model: 0
[10/31 20:02:35 visual_prompt]: Setting up Evaluator...
[10/31 20:02:35 visual_prompt]: Setting up Trainer...
[10/31 20:02:35 visual_prompt]: 	Setting up the optimizer...
[10/31 20:02:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/31 20:03:42 visual_prompt]: 	Training 100/1106. train loss: 0.3219,	0.6313 s / batch. (data: 9.97e-04). ETA=19:22:33, max mem: 15.9 GB 
[10/31 20:04:46 visual_prompt]: 	Training 200/1106. train loss: 1.5424,	0.6594 s / batch. (data: 3.94e-02). ETA=20:13:16, max mem: 15.9 GB 
[10/31 20:05:50 visual_prompt]: 	Training 300/1106. train loss: 0.1459,	0.6193 s / batch. (data: 3.87e-04). ETA=18:58:28, max mem: 15.9 GB 
[10/31 20:06:53 visual_prompt]: 	Training 400/1106. train loss: 0.7442,	0.6400 s / batch. (data: 7.96e-03). ETA=19:35:27, max mem: 15.9 GB 
[10/31 20:07:57 visual_prompt]: 	Training 500/1106. train loss: 0.3799,	0.6184 s / batch. (data: 4.15e-04). ETA=18:54:44, max mem: 15.9 GB 
[10/31 20:09:01 visual_prompt]: 	Training 600/1106. train loss: 1.7927,	0.6175 s / batch. (data: 4.00e-04). ETA=18:52:03, max mem: 15.9 GB 
[10/31 20:10:05 visual_prompt]: 	Training 700/1106. train loss: 0.7472,	0.6520 s / batch. (data: 1.02e-03). ETA=19:54:14, max mem: 15.9 GB 
[10/31 20:11:09 visual_prompt]: 	Training 800/1106. train loss: 1.0533,	0.6340 s / batch. (data: 3.78e-04). ETA=19:20:10, max mem: 15.9 GB 
[10/31 20:12:12 visual_prompt]: 	Training 900/1106. train loss: 0.3219,	0.6188 s / batch. (data: 3.58e-04). ETA=18:51:19, max mem: 15.9 GB 
[10/31 20:13:16 visual_prompt]: 	Training 1000/1106. train loss: 2.0743,	0.6353 s / batch. (data: 9.42e-04). ETA=19:20:23, max mem: 15.9 GB 
[10/31 20:14:20 visual_prompt]: 	Training 1100/1106. train loss: 0.7837,	0.6191 s / batch. (data: 2.30e-04). ETA=18:49:56, max mem: 15.9 GB 
[10/31 20:14:24 visual_prompt]: Epoch 1 / 100: avg data time: 6.95e-03, avg batch time: 0.6408, average train loss: 1.0102
[10/31 20:15:19 visual_prompt]: 	Test 100/123. loss: 0.597, 0.2254 s / batch. (data: 4.03e-05)max mem: 15.94594 GB 
[10/31 20:15:31 visual_prompt]: Inference (val):avg data time: 5.31e-05, avg batch time: 0.2321, average loss: 1.0778
[10/31 20:15:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.79	
[10/31 20:16:28 visual_prompt]: 	Test 100/323. loss: 1.641, 0.2360 s / batch. (data: 5.56e-05)max mem: 15.94594 GB 
[10/31 20:17:23 visual_prompt]: 	Test 200/323. loss: 1.159, 0.2318 s / batch. (data: 3.41e-05)max mem: 15.94594 GB 
[10/31 20:18:17 visual_prompt]: 	Test 300/323. loss: 0.940, 0.2242 s / batch. (data: 4.84e-05)max mem: 15.94594 GB 
[10/31 20:18:29 visual_prompt]: Inference (test):avg data time: 7.26e-05, avg batch time: 0.2317, average loss: 1.1233
[10/31 20:18:29 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.16	rocauc: 47.74	
[10/31 20:18:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/31 20:19:36 visual_prompt]: 	Training 100/1106. train loss: 0.5959,	0.6334 s / batch. (data: 3.61e-04). ETA=19:14:45, max mem: 15.9 GB 
[10/31 20:20:40 visual_prompt]: 	Training 200/1106. train loss: 0.5489,	0.6359 s / batch. (data: 7.98e-03). ETA=19:18:21, max mem: 15.9 GB 
[10/31 20:21:44 visual_prompt]: 	Training 300/1106. train loss: 1.7453,	0.6527 s / batch. (data: 9.86e-04). ETA=19:47:50, max mem: 15.9 GB 
[10/31 20:22:47 visual_prompt]: 	Training 400/1106. train loss: 0.9791,	0.6600 s / batch. (data: 1.18e-03). ETA=19:59:59, max mem: 15.9 GB 
[10/31 20:23:51 visual_prompt]: 	Training 500/1106. train loss: 0.3033,	0.6400 s / batch. (data: 4.80e-04). ETA=19:22:31, max mem: 15.9 GB 
[10/31 20:24:55 visual_prompt]: 	Training 600/1106. train loss: 2.2421,	0.6522 s / batch. (data: 2.09e-02). ETA=19:43:37, max mem: 15.9 GB 
[10/31 20:25:59 visual_prompt]: 	Training 700/1106. train loss: 0.6595,	0.6356 s / batch. (data: 4.40e-04). ETA=19:12:29, max mem: 15.9 GB 
[10/31 20:27:03 visual_prompt]: 	Training 800/1106. train loss: 0.7057,	0.6342 s / batch. (data: 1.02e-03). ETA=19:08:50, max mem: 15.9 GB 
[10/31 20:28:07 visual_prompt]: 	Training 900/1106. train loss: 1.3019,	0.6355 s / batch. (data: 9.81e-04). ETA=19:10:12, max mem: 15.9 GB 
[10/31 20:29:11 visual_prompt]: 	Training 1000/1106. train loss: 1.0653,	0.6199 s / batch. (data: 3.85e-04). ETA=18:40:58, max mem: 15.9 GB 
[10/31 20:30:15 visual_prompt]: 	Training 1100/1106. train loss: 1.2839,	0.6179 s / batch. (data: 2.46e-04). ETA=18:36:16, max mem: 15.9 GB 
[10/31 20:30:19 visual_prompt]: Epoch 2 / 100: avg data time: 7.08e-03, avg batch time: 0.6417, average train loss: 0.8584
[10/31 20:31:14 visual_prompt]: 	Test 100/123. loss: 0.721, 0.2344 s / batch. (data: 5.82e-05)max mem: 15.94594 GB 
[10/31 20:31:26 visual_prompt]: Inference (val):avg data time: 5.28e-05, avg batch time: 0.2318, average loss: 0.7552
[10/31 20:31:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.17	
[10/31 20:32:22 visual_prompt]: 	Test 100/323. loss: 1.000, 0.2367 s / batch. (data: 6.15e-05)max mem: 15.94594 GB 
[10/31 20:33:15 visual_prompt]: 	Test 200/323. loss: 0.726, 0.2249 s / batch. (data: 3.65e-05)max mem: 15.94594 GB 
[10/31 20:34:08 visual_prompt]: 	Test 300/323. loss: 0.746, 0.2250 s / batch. (data: 3.62e-05)max mem: 15.94594 GB 
[10/31 20:34:19 visual_prompt]: Inference (test):avg data time: 5.16e-05, avg batch time: 0.2319, average loss: 0.7708
[10/31 20:34:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 50.39	
[10/31 20:34:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/31 20:35:26 visual_prompt]: 	Training 100/1106. train loss: 0.5941,	0.6330 s / batch. (data: 3.39e-04). ETA=19:02:25, max mem: 15.9 GB 
[10/31 20:36:30 visual_prompt]: 	Training 200/1106. train loss: 0.9329,	0.6454 s / batch. (data: 1.02e-03). ETA=19:23:47, max mem: 15.9 GB 
[10/31 20:37:34 visual_prompt]: 	Training 300/1106. train loss: 0.3365,	0.6564 s / batch. (data: 1.01e-03). ETA=19:42:24, max mem: 15.9 GB 
[10/31 20:38:37 visual_prompt]: 	Training 400/1106. train loss: 0.0411,	0.6320 s / batch. (data: 3.85e-04). ETA=18:57:27, max mem: 15.9 GB 
[10/31 20:39:41 visual_prompt]: 	Training 500/1106. train loss: 0.9259,	0.6186 s / batch. (data: 4.48e-04). ETA=18:32:16, max mem: 15.9 GB 
[10/31 20:40:45 visual_prompt]: 	Training 600/1106. train loss: 0.7210,	0.6401 s / batch. (data: 1.07e-02). ETA=19:09:49, max mem: 15.9 GB 
[10/31 20:41:49 visual_prompt]: 	Training 700/1106. train loss: 0.1545,	0.6350 s / batch. (data: 9.54e-04). ETA=18:59:38, max mem: 15.9 GB 
[10/31 20:42:53 visual_prompt]: 	Training 800/1106. train loss: 0.8876,	0.6441 s / batch. (data: 9.71e-04). ETA=19:14:56, max mem: 15.9 GB 
[10/31 20:43:57 visual_prompt]: 	Training 900/1106. train loss: 0.9130,	0.6454 s / batch. (data: 1.12e-02). ETA=19:16:17, max mem: 15.9 GB 
[10/31 20:45:00 visual_prompt]: 	Training 1000/1106. train loss: 0.7338,	0.6401 s / batch. (data: 4.33e-04). ETA=19:05:34, max mem: 15.9 GB 
[10/31 20:46:04 visual_prompt]: 	Training 1100/1106. train loss: 0.3002,	0.6194 s / batch. (data: 2.43e-04). ETA=18:27:36, max mem: 15.9 GB 
[10/31 20:46:08 visual_prompt]: Epoch 3 / 100: avg data time: 6.69e-03, avg batch time: 0.6409, average train loss: 0.8671
[10/31 20:47:04 visual_prompt]: 	Test 100/123. loss: 0.741, 0.2394 s / batch. (data: 4.60e-05)max mem: 15.94594 GB 
[10/31 20:47:15 visual_prompt]: Inference (val):avg data time: 2.39e-04, avg batch time: 0.2332, average loss: 0.6969
[10/31 20:47:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.99	
[10/31 20:48:11 visual_prompt]: 	Test 100/323. loss: 0.466, 0.2249 s / batch. (data: 5.32e-05)max mem: 15.94594 GB 
[10/31 20:49:04 visual_prompt]: 	Test 200/323. loss: 0.648, 0.2325 s / batch. (data: 5.72e-05)max mem: 15.94594 GB 
[10/31 20:49:57 visual_prompt]: 	Test 300/323. loss: 0.761, 0.2240 s / batch. (data: 6.46e-05)max mem: 15.94594 GB 
[10/31 20:50:09 visual_prompt]: Inference (test):avg data time: 9.84e-05, avg batch time: 0.2312, average loss: 0.6734
[10/31 20:50:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.48	
[10/31 20:50:09 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/31 20:51:15 visual_prompt]: 	Training 100/1106. train loss: 0.6641,	0.6440 s / batch. (data: 1.00e-03). ETA=19:10:29, max mem: 15.9 GB 
[10/31 20:52:19 visual_prompt]: 	Training 200/1106. train loss: 0.8077,	0.6480 s / batch. (data: 1.04e-03). ETA=19:16:29, max mem: 15.9 GB 
[10/31 20:53:23 visual_prompt]: 	Training 300/1106. train loss: 1.0036,	0.6234 s / batch. (data: 3.77e-04). ETA=18:31:28, max mem: 15.9 GB 
[10/31 20:54:26 visual_prompt]: 	Training 400/1106. train loss: 2.8655,	0.6331 s / batch. (data: 3.56e-04). ETA=18:47:42, max mem: 15.9 GB 
[10/31 20:55:30 visual_prompt]: 	Training 500/1106. train loss: 0.5793,	0.6280 s / batch. (data: 4.07e-04). ETA=18:37:35, max mem: 15.9 GB 
[10/31 20:56:34 visual_prompt]: 	Training 600/1106. train loss: 0.4685,	0.6411 s / batch. (data: 9.84e-04). ETA=18:59:53, max mem: 15.9 GB 
[10/31 20:57:38 visual_prompt]: 	Training 700/1106. train loss: 0.6992,	0.6324 s / batch. (data: 3.96e-04). ETA=18:43:27, max mem: 15.9 GB 
[10/31 20:58:42 visual_prompt]: 	Training 800/1106. train loss: 0.6089,	0.6602 s / batch. (data: 3.67e-02). ETA=19:31:37, max mem: 15.9 GB 
[10/31 20:59:46 visual_prompt]: 	Training 900/1106. train loss: 0.8086,	0.6358 s / batch. (data: 3.71e-04). ETA=18:47:18, max mem: 15.9 GB 
[10/31 21:00:50 visual_prompt]: 	Training 1000/1106. train loss: 0.6809,	0.6400 s / batch. (data: 3.63e-04). ETA=18:53:39, max mem: 15.9 GB 
[10/31 21:01:54 visual_prompt]: 	Training 1100/1106. train loss: 0.6601,	0.6240 s / batch. (data: 2.50e-04). ETA=18:24:15, max mem: 15.9 GB 
[10/31 21:01:58 visual_prompt]: Epoch 4 / 100: avg data time: 6.64e-03, avg batch time: 0.6408, average train loss: 0.9565
[10/31 21:02:53 visual_prompt]: 	Test 100/123. loss: 0.780, 0.2275 s / batch. (data: 6.94e-05)max mem: 15.94594 GB 
[10/31 21:03:05 visual_prompt]: Inference (val):avg data time: 5.19e-05, avg batch time: 0.2330, average loss: 0.7212
[10/31 21:03:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.47	
[10/31 21:04:00 visual_prompt]: 	Test 100/323. loss: 0.370, 0.2247 s / batch. (data: 4.39e-05)max mem: 15.94594 GB 
[10/31 21:04:54 visual_prompt]: 	Test 200/323. loss: 0.682, 0.2294 s / batch. (data: 6.37e-05)max mem: 15.94594 GB 
[10/31 21:05:47 visual_prompt]: 	Test 300/323. loss: 0.804, 0.2458 s / batch. (data: 4.91e-05)max mem: 15.94594 GB 
[10/31 21:05:58 visual_prompt]: Inference (test):avg data time: 7.78e-05, avg batch time: 0.2324, average loss: 0.6860
[10/31 21:05:58 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.64	
[10/31 21:05:58 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/31 21:07:05 visual_prompt]: 	Training 100/1106. train loss: 0.0699,	0.6320 s / batch. (data: 3.71e-04). ETA=18:37:20, max mem: 15.9 GB 
[10/31 21:08:09 visual_prompt]: 	Training 200/1106. train loss: 1.1884,	0.6248 s / batch. (data: 5.55e-03). ETA=18:23:38, max mem: 15.9 GB 
[10/31 21:09:13 visual_prompt]: 	Training 300/1106. train loss: 0.4814,	0.6414 s / batch. (data: 6.15e-03). ETA=18:51:44, max mem: 15.9 GB 
[10/31 21:10:16 visual_prompt]: 	Training 400/1106. train loss: 2.1590,	0.6344 s / batch. (data: 9.99e-04). ETA=18:38:22, max mem: 15.9 GB 
[10/31 21:11:20 visual_prompt]: 	Training 500/1106. train loss: 0.5144,	0.6201 s / batch. (data: 3.62e-04). ETA=18:12:13, max mem: 15.9 GB 
[10/31 21:12:24 visual_prompt]: 	Training 600/1106. train loss: 1.0648,	0.6296 s / batch. (data: 3.98e-04). ETA=18:27:48, max mem: 15.9 GB 
[10/31 21:13:28 visual_prompt]: 	Training 700/1106. train loss: 1.2119,	0.6421 s / batch. (data: 4.24e-04). ETA=18:48:47, max mem: 15.9 GB 
[10/31 21:14:32 visual_prompt]: 	Training 800/1106. train loss: 1.1772,	0.6190 s / batch. (data: 4.15e-04). ETA=18:07:05, max mem: 15.9 GB 
[10/31 21:15:36 visual_prompt]: 	Training 900/1106. train loss: 1.1393,	0.6201 s / batch. (data: 3.90e-04). ETA=18:08:05, max mem: 15.9 GB 
[10/31 21:16:40 visual_prompt]: 	Training 1000/1106. train loss: 0.6178,	0.6187 s / batch. (data: 4.51e-04). ETA=18:04:29, max mem: 15.9 GB 
[10/31 21:17:44 visual_prompt]: 	Training 1100/1106. train loss: 1.2258,	0.6189 s / batch. (data: 2.37e-04). ETA=18:03:53, max mem: 15.9 GB 
[10/31 21:17:47 visual_prompt]: Epoch 5 / 100: avg data time: 6.92e-03, avg batch time: 0.6413, average train loss: 0.9628
[10/31 21:18:43 visual_prompt]: 	Test 100/123. loss: 1.522, 0.2252 s / batch. (data: 4.94e-05)max mem: 15.94594 GB 
[10/31 21:18:54 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.2335, average loss: 1.3498
[10/31 21:18:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.58	
[10/31 21:19:50 visual_prompt]: 	Test 100/323. loss: 0.049, 0.2246 s / batch. (data: 4.79e-05)max mem: 15.94594 GB 
[10/31 21:20:44 visual_prompt]: 	Test 200/323. loss: 1.368, 0.2344 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[10/31 21:21:37 visual_prompt]: 	Test 300/323. loss: 1.613, 0.2270 s / batch. (data: 3.74e-05)max mem: 15.94594 GB 
[10/31 21:21:48 visual_prompt]: Inference (test):avg data time: 2.56e-04, avg batch time: 0.2319, average loss: 1.2267
[10/31 21:21:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.81	
[10/31 21:21:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/31 21:22:55 visual_prompt]: 	Training 100/1106. train loss: 0.5868,	0.6172 s / batch. (data: 3.86e-04). ETA=17:59:50, max mem: 15.9 GB 
[10/31 21:23:58 visual_prompt]: 	Training 200/1106. train loss: 0.6297,	0.6322 s / batch. (data: 1.01e-03). ETA=18:24:56, max mem: 15.9 GB 
[10/31 21:25:02 visual_prompt]: 	Training 300/1106. train loss: 0.5601,	0.6480 s / batch. (data: 1.20e-02). ETA=18:51:32, max mem: 15.9 GB 
[10/31 21:26:06 visual_prompt]: 	Training 400/1106. train loss: 0.8129,	0.6617 s / batch. (data: 1.63e-02). ETA=19:14:21, max mem: 15.9 GB 
[10/31 21:27:10 visual_prompt]: 	Training 500/1106. train loss: 0.7956,	0.6345 s / batch. (data: 1.01e-03). ETA=18:25:53, max mem: 15.9 GB 
[10/31 21:28:14 visual_prompt]: 	Training 600/1106. train loss: 0.0710,	0.6234 s / batch. (data: 3.96e-04). ETA=18:05:22, max mem: 15.9 GB 
[10/31 21:29:17 visual_prompt]: 	Training 700/1106. train loss: 0.2355,	0.6276 s / batch. (data: 4.15e-04). ETA=18:11:38, max mem: 15.9 GB 
[10/31 21:30:21 visual_prompt]: 	Training 800/1106. train loss: 0.0672,	0.6411 s / batch. (data: 1.01e-03). ETA=18:34:11, max mem: 15.9 GB 
[10/31 21:31:25 visual_prompt]: 	Training 900/1106. train loss: 0.1181,	0.6199 s / batch. (data: 3.89e-04). ETA=17:56:19, max mem: 15.9 GB 
[10/31 21:32:28 visual_prompt]: 	Training 1000/1106. train loss: 3.3113,	0.6190 s / batch. (data: 3.61e-04). ETA=17:53:42, max mem: 15.9 GB 
[10/31 21:33:33 visual_prompt]: 	Training 1100/1106. train loss: 0.8153,	0.6193 s / batch. (data: 2.35e-04). ETA=17:53:07, max mem: 15.9 GB 
[10/31 21:33:37 visual_prompt]: Epoch 6 / 100: avg data time: 7.03e-03, avg batch time: 0.6405, average train loss: 1.0526
[10/31 21:34:31 visual_prompt]: 	Test 100/123. loss: 0.728, 0.2397 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/31 21:34:42 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.2322, average loss: 0.7100
[10/31 21:34:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 57.59	
[10/31 21:35:38 visual_prompt]: 	Test 100/323. loss: 0.831, 0.2249 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[10/31 21:36:30 visual_prompt]: 	Test 200/323. loss: 0.641, 0.2243 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/31 21:37:22 visual_prompt]: 	Test 300/323. loss: 0.765, 0.2417 s / batch. (data: 5.41e-05)max mem: 15.94594 GB 
[10/31 21:37:33 visual_prompt]: Inference (test):avg data time: 2.59e-04, avg batch time: 0.2322, average loss: 0.7260
[10/31 21:37:33 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.62	rocauc: 56.15	
[10/31 21:37:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/31 21:38:40 visual_prompt]: 	Training 100/1106. train loss: 0.8720,	0.6306 s / batch. (data: 1.20e-02). ETA=18:11:33, max mem: 15.9 GB 
[10/31 21:39:43 visual_prompt]: 	Training 200/1106. train loss: 0.8587,	0.6342 s / batch. (data: 1.06e-02). ETA=18:16:51, max mem: 15.9 GB 
[10/31 21:40:47 visual_prompt]: 	Training 300/1106. train loss: 0.2428,	0.6274 s / batch. (data: 3.59e-04). ETA=18:03:54, max mem: 15.9 GB 
[10/31 21:41:51 visual_prompt]: 	Training 400/1106. train loss: 0.9660,	0.6459 s / batch. (data: 8.92e-04). ETA=18:34:48, max mem: 15.9 GB 
[10/31 21:42:54 visual_prompt]: 	Training 500/1106. train loss: 0.7591,	0.6387 s / batch. (data: 8.78e-04). ETA=18:21:22, max mem: 15.9 GB 
[10/31 21:43:58 visual_prompt]: 	Training 600/1106. train loss: 1.5920,	0.6469 s / batch. (data: 8.48e-04). ETA=18:34:29, max mem: 15.9 GB 
[10/31 21:45:02 visual_prompt]: 	Training 700/1106. train loss: 0.5891,	0.6402 s / batch. (data: 3.79e-04). ETA=18:21:53, max mem: 15.9 GB 
[10/31 21:46:06 visual_prompt]: 	Training 800/1106. train loss: 1.2211,	0.6403 s / batch. (data: 8.69e-04). ETA=18:20:55, max mem: 15.9 GB 
[10/31 21:47:10 visual_prompt]: 	Training 900/1106. train loss: 0.8339,	0.6381 s / batch. (data: 3.46e-04). ETA=18:16:03, max mem: 15.9 GB 
[10/31 21:48:14 visual_prompt]: 	Training 1000/1106. train loss: 1.2515,	0.6514 s / batch. (data: 1.55e-02). ETA=18:37:51, max mem: 15.9 GB 
[10/31 21:49:17 visual_prompt]: 	Training 1100/1106. train loss: 0.7517,	0.6179 s / batch. (data: 2.32e-04). ETA=17:39:22, max mem: 15.9 GB 
[10/31 21:49:21 visual_prompt]: Epoch 7 / 100: avg data time: 6.37e-03, avg batch time: 0.6400, average train loss: 1.0685
[10/31 21:50:16 visual_prompt]: 	Test 100/123. loss: 0.751, 0.2356 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/31 21:50:27 visual_prompt]: Inference (val):avg data time: 1.88e-04, avg batch time: 0.2315, average loss: 0.7394
[10/31 21:50:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.77	
[10/31 21:51:23 visual_prompt]: 	Test 100/323. loss: 0.936, 0.2247 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/31 21:52:15 visual_prompt]: 	Test 200/323. loss: 0.626, 0.2240 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/31 21:53:07 visual_prompt]: 	Test 300/323. loss: 0.787, 0.2324 s / batch. (data: 6.06e-05)max mem: 15.94594 GB 
[10/31 21:53:18 visual_prompt]: Inference (test):avg data time: 8.92e-05, avg batch time: 0.2317, average loss: 0.7684
[10/31 21:53:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.46	
[10/31 21:53:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/31 21:54:24 visual_prompt]: 	Training 100/1106. train loss: 0.0753,	0.6349 s / batch. (data: 3.83e-04). ETA=18:07:26, max mem: 15.9 GB 
[10/31 21:55:28 visual_prompt]: 	Training 200/1106. train loss: 2.3903,	0.6504 s / batch. (data: 1.06e-02). ETA=18:32:45, max mem: 15.9 GB 
[10/31 21:56:32 visual_prompt]: 	Training 300/1106. train loss: 3.0551,	0.6329 s / batch. (data: 3.50e-04). ETA=18:01:47, max mem: 15.9 GB 
[10/31 21:57:36 visual_prompt]: 	Training 400/1106. train loss: 1.1818,	0.6390 s / batch. (data: 3.74e-04). ETA=18:11:13, max mem: 15.9 GB 
[10/31 21:58:40 visual_prompt]: 	Training 500/1106. train loss: 0.0677,	0.6733 s / batch. (data: 8.54e-04). ETA=19:08:38, max mem: 15.9 GB 
[10/31 21:59:44 visual_prompt]: 	Training 600/1106. train loss: 0.2003,	0.6441 s / batch. (data: 1.06e-02). ETA=18:17:48, max mem: 15.9 GB 
[10/31 22:00:47 visual_prompt]: 	Training 700/1106. train loss: 0.3057,	0.6399 s / batch. (data: 8.65e-04). ETA=18:09:30, max mem: 15.9 GB 
[10/31 22:01:51 visual_prompt]: 	Training 800/1106. train loss: 1.2753,	0.6180 s / batch. (data: 4.17e-04). ETA=17:31:10, max mem: 15.9 GB 
[10/31 22:02:55 visual_prompt]: 	Training 900/1106. train loss: 0.8212,	0.6480 s / batch. (data: 8.45e-04). ETA=18:21:12, max mem: 15.9 GB 
[10/31 22:03:58 visual_prompt]: 	Training 1000/1106. train loss: 1.5669,	0.6203 s / batch. (data: 1.03e-03). ETA=17:33:01, max mem: 15.9 GB 
[10/31 22:05:02 visual_prompt]: 	Training 1100/1106. train loss: 4.6611,	0.6188 s / batch. (data: 1.76e-04). ETA=17:29:32, max mem: 15.9 GB 
[10/31 22:05:06 visual_prompt]: Epoch 8 / 100: avg data time: 6.14e-03, avg batch time: 0.6398, average train loss: 0.9996
[10/31 22:06:00 visual_prompt]: 	Test 100/123. loss: 2.052, 0.2249 s / batch. (data: 6.03e-05)max mem: 15.94594 GB 
[10/31 22:06:12 visual_prompt]: Inference (val):avg data time: 9.89e-05, avg batch time: 0.2327, average loss: 1.7833
[10/31 22:06:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.65	
[10/31 22:07:07 visual_prompt]: 	Test 100/323. loss: 0.019, 0.2351 s / batch. (data: 5.27e-05)max mem: 15.94594 GB 
[10/31 22:07:59 visual_prompt]: 	Test 200/323. loss: 1.788, 0.2243 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[10/31 22:08:52 visual_prompt]: 	Test 300/323. loss: 2.154, 0.2242 s / batch. (data: 6.56e-05)max mem: 15.94594 GB 
[10/31 22:09:03 visual_prompt]: Inference (test):avg data time: 1.86e-04, avg batch time: 0.2325, average loss: 1.6203
[10/31 22:09:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.39	
[10/31 22:09:03 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/31 22:10:10 visual_prompt]: 	Training 100/1106. train loss: 1.4475,	0.6510 s / batch. (data: 8.77e-04). ETA=18:22:53, max mem: 15.9 GB 
[10/31 22:11:14 visual_prompt]: 	Training 200/1106. train loss: 1.6459,	0.6241 s / batch. (data: 3.12e-04). ETA=17:36:17, max mem: 15.9 GB 
[10/31 22:12:17 visual_prompt]: 	Training 300/1106. train loss: 2.3286,	0.6189 s / batch. (data: 3.57e-04). ETA=17:26:24, max mem: 15.9 GB 
[10/31 22:13:21 visual_prompt]: 	Training 400/1106. train loss: 1.6775,	0.6383 s / batch. (data: 8.75e-04). ETA=17:58:13, max mem: 15.9 GB 
[10/31 22:14:25 visual_prompt]: 	Training 500/1106. train loss: 1.6683,	0.6183 s / batch. (data: 3.55e-04). ETA=17:23:27, max mem: 15.9 GB 
[10/31 22:15:29 visual_prompt]: 	Training 600/1106. train loss: 0.5357,	0.6483 s / batch. (data: 8.89e-04). ETA=18:12:55, max mem: 15.9 GB 
[10/31 22:16:33 visual_prompt]: 	Training 700/1106. train loss: 1.2556,	0.6440 s / batch. (data: 3.37e-04). ETA=18:04:36, max mem: 15.9 GB 
[10/31 22:17:36 visual_prompt]: 	Training 800/1106. train loss: 0.6668,	0.6360 s / batch. (data: 8.39e-04). ETA=17:50:05, max mem: 15.9 GB 
[10/31 22:18:40 visual_prompt]: 	Training 900/1106. train loss: 0.8496,	0.6552 s / batch. (data: 8.69e-04). ETA=18:21:21, max mem: 15.9 GB 
[10/31 22:19:44 visual_prompt]: 	Training 1000/1106. train loss: 0.4286,	0.6315 s / batch. (data: 3.49e-04). ETA=17:40:20, max mem: 15.9 GB 
[10/31 22:20:48 visual_prompt]: 	Training 1100/1106. train loss: 0.8912,	0.6176 s / batch. (data: 3.77e-04). ETA=17:16:04, max mem: 15.9 GB 
[10/31 22:20:52 visual_prompt]: Epoch 9 / 100: avg data time: 7.09e-03, avg batch time: 0.6409, average train loss: 1.1566
[10/31 22:21:46 visual_prompt]: 	Test 100/123. loss: 1.594, 0.2436 s / batch. (data: 3.74e-05)max mem: 15.94594 GB 
[10/31 22:21:58 visual_prompt]: Inference (val):avg data time: 5.38e-05, avg batch time: 0.2317, average loss: 1.3490
[10/31 22:21:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.07	
[10/31 22:22:53 visual_prompt]: 	Test 100/323. loss: 0.040, 0.2291 s / batch. (data: 4.96e-05)max mem: 15.94594 GB 
[10/31 22:23:45 visual_prompt]: 	Test 200/323. loss: 1.206, 0.2362 s / batch. (data: 5.44e-05)max mem: 15.94594 GB 
[10/31 22:24:37 visual_prompt]: 	Test 300/323. loss: 1.719, 0.2237 s / batch. (data: 4.89e-05)max mem: 15.94594 GB 
[10/31 22:24:49 visual_prompt]: Inference (test):avg data time: 1.18e-04, avg batch time: 0.2320, average loss: 1.2301
[10/31 22:24:49 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.19	
[10/31 22:24:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/31 22:25:56 visual_prompt]: 	Training 100/1106. train loss: 1.9486,	0.6600 s / batch. (data: 8.88e-04). ETA=18:26:00, max mem: 15.9 GB 
[10/31 22:27:00 visual_prompt]: 	Training 200/1106. train loss: 0.8885,	0.6240 s / batch. (data: 3.75e-04). ETA=17:24:40, max mem: 15.9 GB 
[10/31 22:28:03 visual_prompt]: 	Training 300/1106. train loss: 1.2749,	0.6196 s / batch. (data: 4.37e-04). ETA=17:16:13, max mem: 15.9 GB 
[10/31 22:29:07 visual_prompt]: 	Training 400/1106. train loss: 3.4491,	0.6286 s / batch. (data: 3.83e-04). ETA=17:30:10, max mem: 15.9 GB 
[10/31 22:30:11 visual_prompt]: 	Training 500/1106. train loss: 0.1215,	0.6536 s / batch. (data: 8.95e-04). ETA=18:10:51, max mem: 15.9 GB 
[10/31 22:31:15 visual_prompt]: 	Training 600/1106. train loss: 2.0117,	0.6556 s / batch. (data: 1.62e-02). ETA=18:13:13, max mem: 15.9 GB 
[10/31 22:32:18 visual_prompt]: 	Training 700/1106. train loss: 1.9438,	0.6467 s / batch. (data: 8.62e-04). ETA=17:57:14, max mem: 15.9 GB 
[10/31 22:33:22 visual_prompt]: 	Training 800/1106. train loss: 1.0205,	0.6790 s / batch. (data: 6.05e-03). ETA=18:49:52, max mem: 15.9 GB 
[10/31 22:34:26 visual_prompt]: 	Training 900/1106. train loss: 1.2109,	0.6281 s / batch. (data: 3.79e-04). ETA=17:24:07, max mem: 15.9 GB 
[10/31 22:35:30 visual_prompt]: 	Training 1000/1106. train loss: 0.1838,	0.6364 s / batch. (data: 3.72e-04). ETA=17:36:58, max mem: 15.9 GB 
[10/31 22:36:33 visual_prompt]: 	Training 1100/1106. train loss: 0.7785,	0.6353 s / batch. (data: 1.98e-04). ETA=17:34:05, max mem: 15.9 GB 
[10/31 22:36:37 visual_prompt]: Epoch 10 / 100: avg data time: 6.72e-03, avg batch time: 0.6406, average train loss: 1.0923
[10/31 22:37:32 visual_prompt]: 	Test 100/123. loss: 0.662, 0.2334 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[10/31 22:37:43 visual_prompt]: Inference (val):avg data time: 1.99e-04, avg batch time: 0.2326, average loss: 0.6739
[10/31 22:37:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 58.78	
[10/31 22:38:38 visual_prompt]: 	Test 100/323. loss: 0.590, 0.2380 s / batch. (data: 6.94e-05)max mem: 15.94594 GB 
[10/31 22:39:31 visual_prompt]: 	Test 200/323. loss: 0.608, 0.2240 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[10/31 22:40:23 visual_prompt]: 	Test 300/323. loss: 0.761, 0.2448 s / batch. (data: 6.18e-05)max mem: 15.94594 GB 
[10/31 22:40:34 visual_prompt]: Inference (test):avg data time: 1.18e-04, avg batch time: 0.2319, average loss: 0.6790
[10/31 22:40:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.74	rocauc: 53.75	
[10/31 22:40:34 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/31 22:41:41 visual_prompt]: 	Training 100/1106. train loss: 0.8376,	0.6360 s / batch. (data: 9.09e-04). ETA=17:34:08, max mem: 15.9 GB 
[10/31 22:42:45 visual_prompt]: 	Training 200/1106. train loss: 0.7135,	0.6448 s / batch. (data: 8.40e-04). ETA=17:47:32, max mem: 15.9 GB 
[10/31 22:43:49 visual_prompt]: 	Training 300/1106. train loss: 4.1042,	0.6216 s / batch. (data: 3.57e-04). ETA=17:08:08, max mem: 15.9 GB 
[10/31 22:44:52 visual_prompt]: 	Training 400/1106. train loss: 1.4450,	0.6319 s / batch. (data: 1.06e-02). ETA=17:24:09, max mem: 15.9 GB 
[10/31 22:45:56 visual_prompt]: 	Training 500/1106. train loss: 1.0384,	0.6366 s / batch. (data: 8.43e-04). ETA=17:30:51, max mem: 15.9 GB 
[10/31 22:47:00 visual_prompt]: 	Training 600/1106. train loss: 1.3761,	0.6643 s / batch. (data: 1.11e-02). ETA=18:15:28, max mem: 15.9 GB 
[10/31 22:48:04 visual_prompt]: 	Training 700/1106. train loss: 0.3681,	0.6493 s / batch. (data: 6.19e-03). ETA=17:49:35, max mem: 15.9 GB 
[10/31 22:49:07 visual_prompt]: 	Training 800/1106. train loss: 0.3365,	0.6336 s / batch. (data: 3.62e-04). ETA=17:22:44, max mem: 15.9 GB 
[10/31 22:50:11 visual_prompt]: 	Training 900/1106. train loss: 0.7424,	0.6320 s / batch. (data: 5.52e-03). ETA=17:19:00, max mem: 15.9 GB 
[10/31 22:51:15 visual_prompt]: 	Training 1000/1106. train loss: 0.0229,	0.6459 s / batch. (data: 1.19e-03). ETA=17:40:51, max mem: 15.9 GB 
[10/31 22:52:18 visual_prompt]: 	Training 1100/1106. train loss: 1.2420,	0.6192 s / batch. (data: 4.39e-04). ETA=16:55:56, max mem: 15.9 GB 
[10/31 22:52:22 visual_prompt]: Epoch 11 / 100: avg data time: 6.52e-03, avg batch time: 0.6401, average train loss: 1.2228
[10/31 22:53:16 visual_prompt]: 	Test 100/123. loss: 0.542, 0.2397 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[10/31 22:53:28 visual_prompt]: Inference (val):avg data time: 5.09e-05, avg batch time: 0.2326, average loss: 0.7219
[10/31 22:53:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.04	
[10/31 22:54:23 visual_prompt]: 	Test 100/323. loss: 0.880, 0.2286 s / batch. (data: 6.22e-05)max mem: 15.94594 GB 
[10/31 22:55:16 visual_prompt]: 	Test 200/323. loss: 0.460, 0.2250 s / batch. (data: 5.25e-05)max mem: 15.94594 GB 
[10/31 22:56:08 visual_prompt]: 	Test 300/323. loss: 0.947, 0.2410 s / batch. (data: 5.44e-05)max mem: 15.94594 GB 
[10/31 22:56:19 visual_prompt]: Inference (test):avg data time: 7.76e-05, avg batch time: 0.2319, average loss: 0.7602
[10/31 22:56:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 47.13	rocauc: 53.42	
[10/31 22:56:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/31 22:57:26 visual_prompt]: 	Training 100/1106. train loss: 0.9950,	0.6283 s / batch. (data: 1.17e-02). ETA=17:09:45, max mem: 15.9 GB 
[10/31 22:58:29 visual_prompt]: 	Training 200/1106. train loss: 1.1877,	0.6240 s / batch. (data: 3.62e-04). ETA=17:01:37, max mem: 15.9 GB 
[10/31 22:59:33 visual_prompt]: 	Training 300/1106. train loss: 1.0621,	0.6270 s / batch. (data: 8.53e-04). ETA=17:05:25, max mem: 15.9 GB 
[10/31 23:00:36 visual_prompt]: 	Training 400/1106. train loss: 0.7568,	0.6541 s / batch. (data: 3.26e-02). ETA=17:48:39, max mem: 15.9 GB 
[10/31 23:01:40 visual_prompt]: 	Training 500/1106. train loss: 0.6427,	0.6320 s / batch. (data: 3.46e-04). ETA=17:11:33, max mem: 15.9 GB 
[10/31 23:02:44 visual_prompt]: 	Training 600/1106. train loss: 0.3971,	0.6187 s / batch. (data: 3.66e-04). ETA=16:48:53, max mem: 15.9 GB 
[10/31 23:03:48 visual_prompt]: 	Training 700/1106. train loss: 1.1679,	0.6325 s / batch. (data: 3.37e-04). ETA=17:10:14, max mem: 15.9 GB 
[10/31 23:04:52 visual_prompt]: 	Training 800/1106. train loss: 1.5536,	0.6440 s / batch. (data: 8.34e-04). ETA=17:27:53, max mem: 15.9 GB 
[10/31 23:05:55 visual_prompt]: 	Training 900/1106. train loss: 1.1135,	0.6280 s / batch. (data: 3.76e-04). ETA=17:00:50, max mem: 15.9 GB 
[10/31 23:06:59 visual_prompt]: 	Training 1000/1106. train loss: 0.6225,	0.6485 s / batch. (data: 1.57e-02). ETA=17:33:07, max mem: 15.9 GB 
[10/31 23:08:03 visual_prompt]: 	Training 1100/1106. train loss: 0.0082,	0.6183 s / batch. (data: 1.76e-04). ETA=16:42:58, max mem: 15.9 GB 
[10/31 23:08:06 visual_prompt]: Epoch 12 / 100: avg data time: 6.62e-03, avg batch time: 0.6397, average train loss: 0.9979
[10/31 23:09:01 visual_prompt]: 	Test 100/123. loss: 0.776, 0.2477 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/31 23:09:13 visual_prompt]: Inference (val):avg data time: 4.99e-05, avg batch time: 0.2321, average loss: 0.7502
[10/31 23:09:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.31	
[10/31 23:10:08 visual_prompt]: 	Test 100/323. loss: 0.281, 0.2432 s / batch. (data: 3.34e-05)max mem: 15.94594 GB 
[10/31 23:11:00 visual_prompt]: 	Test 200/323. loss: 0.532, 0.2249 s / batch. (data: 5.72e-05)max mem: 15.94594 GB 
[10/31 23:11:53 visual_prompt]: 	Test 300/323. loss: 1.046, 0.2242 s / batch. (data: 5.13e-05)max mem: 15.94594 GB 
[10/31 23:12:04 visual_prompt]: Inference (test):avg data time: 7.28e-05, avg batch time: 0.2311, average loss: 0.7080
[10/31 23:12:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 57.39	
[10/31 23:12:04 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/31 23:13:11 visual_prompt]: 	Training 100/1106. train loss: 5.6397,	0.6400 s / batch. (data: 3.38e-04). ETA=17:17:06, max mem: 15.9 GB 
[10/31 23:14:14 visual_prompt]: 	Training 200/1106. train loss: 0.8871,	0.6182 s / batch. (data: 3.45e-04). ETA=16:40:42, max mem: 15.9 GB 
[10/31 23:15:18 visual_prompt]: 	Training 300/1106. train loss: 0.6607,	0.6443 s / batch. (data: 3.51e-04). ETA=17:21:58, max mem: 15.9 GB 
[10/31 23:16:22 visual_prompt]: 	Training 400/1106. train loss: 1.4868,	0.6708 s / batch. (data: 8.63e-04). ETA=18:03:40, max mem: 15.9 GB 
[10/31 23:17:26 visual_prompt]: 	Training 500/1106. train loss: 1.5293,	0.6360 s / batch. (data: 3.61e-04). ETA=17:06:22, max mem: 15.9 GB 
[10/31 23:18:30 visual_prompt]: 	Training 600/1106. train loss: 0.0306,	0.6450 s / batch. (data: 8.50e-04). ETA=17:19:45, max mem: 15.9 GB 
[10/31 23:19:33 visual_prompt]: 	Training 700/1106. train loss: 0.3101,	0.6399 s / batch. (data: 8.81e-04). ETA=17:10:28, max mem: 15.9 GB 
[10/31 23:20:37 visual_prompt]: 	Training 800/1106. train loss: 0.9098,	0.6403 s / batch. (data: 9.01e-04). ETA=17:10:03, max mem: 15.9 GB 
[10/31 23:21:41 visual_prompt]: 	Training 900/1106. train loss: 0.6259,	0.6299 s / batch. (data: 1.26e-02). ETA=16:52:16, max mem: 15.9 GB 
[10/31 23:22:45 visual_prompt]: 	Training 1000/1106. train loss: 0.8764,	0.6350 s / batch. (data: 5.52e-03). ETA=16:59:29, max mem: 15.9 GB 
[10/31 23:23:48 visual_prompt]: 	Training 1100/1106. train loss: 1.2633,	0.6189 s / batch. (data: 1.98e-04). ETA=16:32:40, max mem: 15.9 GB 
[10/31 23:23:52 visual_prompt]: Epoch 13 / 100: avg data time: 6.44e-03, avg batch time: 0.6400, average train loss: 1.0779
[10/31 23:24:47 visual_prompt]: 	Test 100/123. loss: 0.652, 0.2357 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/31 23:24:58 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.2317, average loss: 0.6783
[10/31 23:24:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 60.00	
[10/31 23:25:53 visual_prompt]: 	Test 100/323. loss: 0.714, 0.2277 s / batch. (data: 3.89e-05)max mem: 15.94594 GB 
[10/31 23:26:46 visual_prompt]: 	Test 200/323. loss: 0.519, 0.2252 s / batch. (data: 5.87e-05)max mem: 15.94594 GB 
[10/31 23:27:38 visual_prompt]: 	Test 300/323. loss: 0.803, 0.2344 s / batch. (data: 5.10e-05)max mem: 15.94594 GB 
[10/31 23:27:49 visual_prompt]: Inference (test):avg data time: 8.00e-05, avg batch time: 0.2326, average loss: 0.6907
[10/31 23:27:49 visual_prompt]: Classification results with test_mammo-cbis: top1: 55.50	rocauc: 57.40	
[10/31 23:27:49 visual_prompt]: Best epoch 13: best metric: -0.678
[10/31 23:27:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/31 23:28:56 visual_prompt]: 	Training 100/1106. train loss: 1.0674,	0.6400 s / batch. (data: 3.53e-04). ETA=17:05:17, max mem: 15.9 GB 
[10/31 23:30:00 visual_prompt]: 	Training 200/1106. train loss: 0.4000,	0.6485 s / batch. (data: 9.07e-04). ETA=17:17:53, max mem: 15.9 GB 
[10/31 23:31:03 visual_prompt]: 	Training 300/1106. train loss: 0.6045,	0.6277 s / batch. (data: 3.67e-04). ETA=16:43:31, max mem: 15.9 GB 
[10/31 23:32:07 visual_prompt]: 	Training 400/1106. train loss: 0.8708,	0.6499 s / batch. (data: 8.86e-04). ETA=17:17:58, max mem: 15.9 GB 
[10/31 23:33:11 visual_prompt]: 	Training 500/1106. train loss: 1.1990,	0.6181 s / batch. (data: 3.65e-04). ETA=16:26:06, max mem: 15.9 GB 
[10/31 23:34:15 visual_prompt]: 	Training 600/1106. train loss: 1.1814,	0.6389 s / batch. (data: 5.52e-03). ETA=16:58:12, max mem: 15.9 GB 
[10/31 23:35:19 visual_prompt]: 	Training 700/1106. train loss: 1.0157,	0.6335 s / batch. (data: 1.38e-02). ETA=16:48:34, max mem: 15.9 GB 
[10/31 23:36:22 visual_prompt]: 	Training 800/1106. train loss: 0.0729,	0.6554 s / batch. (data: 3.81e-02). ETA=17:22:17, max mem: 15.9 GB 
[10/31 23:37:26 visual_prompt]: 	Training 900/1106. train loss: 0.7524,	0.6559 s / batch. (data: 6.10e-03). ETA=17:22:01, max mem: 15.9 GB 
[10/31 23:38:30 visual_prompt]: 	Training 1000/1106. train loss: 0.0170,	0.6551 s / batch. (data: 6.04e-03). ETA=17:19:40, max mem: 15.9 GB 
[10/31 23:39:34 visual_prompt]: 	Training 1100/1106. train loss: 0.6793,	0.6194 s / batch. (data: 2.07e-04). ETA=16:21:59, max mem: 15.9 GB 
[10/31 23:39:38 visual_prompt]: Epoch 14 / 100: avg data time: 6.73e-03, avg batch time: 0.6403, average train loss: 1.0660
[10/31 23:40:32 visual_prompt]: 	Test 100/123. loss: 0.628, 0.2325 s / batch. (data: 5.70e-05)max mem: 15.94594 GB 
[10/31 23:40:44 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.2335, average loss: 0.7248
[10/31 23:40:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 58.57	
[10/31 23:41:39 visual_prompt]: 	Test 100/323. loss: 0.841, 0.2264 s / batch. (data: 5.70e-05)max mem: 15.94594 GB 
[10/31 23:42:32 visual_prompt]: 	Test 200/323. loss: 0.457, 0.2396 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[10/31 23:43:24 visual_prompt]: 	Test 300/323. loss: 0.863, 0.2244 s / batch. (data: 5.39e-05)max mem: 15.94594 GB 
[10/31 23:43:35 visual_prompt]: Inference (test):avg data time: 2.23e-04, avg batch time: 0.2329, average loss: 0.7475
[10/31 23:43:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 48.22	rocauc: 58.02	
[10/31 23:43:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/31 23:44:41 visual_prompt]: 	Training 100/1106. train loss: 1.4637,	0.6410 s / batch. (data: 8.72e-04). ETA=16:55:09, max mem: 15.9 GB 
[10/31 23:45:45 visual_prompt]: 	Training 200/1106. train loss: 1.9214,	0.6692 s / batch. (data: 6.04e-03). ETA=17:38:36, max mem: 15.9 GB 
[10/31 23:46:49 visual_prompt]: 	Training 300/1106. train loss: 0.4180,	0.6332 s / batch. (data: 8.89e-04). ETA=16:40:40, max mem: 15.9 GB 
[10/31 23:47:53 visual_prompt]: 	Training 400/1106. train loss: 0.6450,	0.6480 s / batch. (data: 8.96e-04). ETA=17:02:54, max mem: 15.9 GB 
[10/31 23:48:56 visual_prompt]: 	Training 500/1106. train loss: 0.2552,	0.6440 s / batch. (data: 3.49e-04). ETA=16:55:33, max mem: 15.9 GB 
[10/31 23:50:00 visual_prompt]: 	Training 600/1106. train loss: 1.2834,	0.6288 s / batch. (data: 3.40e-04). ETA=16:30:36, max mem: 15.9 GB 
[10/31 23:51:03 visual_prompt]: 	Training 700/1106. train loss: 0.4519,	0.6485 s / batch. (data: 1.57e-02). ETA=17:00:32, max mem: 15.9 GB 
[10/31 23:52:07 visual_prompt]: 	Training 800/1106. train loss: 0.7617,	0.6445 s / batch. (data: 9.12e-04). ETA=16:53:10, max mem: 15.9 GB 
[10/31 23:53:11 visual_prompt]: 	Training 900/1106. train loss: 1.5614,	0.6303 s / batch. (data: 8.81e-04). ETA=16:29:43, max mem: 15.9 GB 
[10/31 23:54:15 visual_prompt]: 	Training 1000/1106. train loss: 2.9085,	0.6564 s / batch. (data: 1.11e-02). ETA=17:09:36, max mem: 15.9 GB 
[10/31 23:55:19 visual_prompt]: 	Training 1100/1106. train loss: 0.8081,	0.6177 s / batch. (data: 2.24e-04). ETA=16:07:53, max mem: 15.9 GB 
[10/31 23:55:22 visual_prompt]: Epoch 15 / 100: avg data time: 6.08e-03, avg batch time: 0.6396, average train loss: 1.0532
[10/31 23:56:17 visual_prompt]: 	Test 100/123. loss: 0.903, 0.2250 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[10/31 23:56:29 visual_prompt]: Inference (val):avg data time: 4.81e-05, avg batch time: 0.2323, average loss: 0.8516
[10/31 23:56:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.99	
[10/31 23:57:24 visual_prompt]: 	Test 100/323. loss: 0.211, 0.2244 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[10/31 23:58:17 visual_prompt]: 	Test 200/323. loss: 0.686, 0.2276 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[10/31 23:59:09 visual_prompt]: 	Test 300/323. loss: 1.111, 0.2430 s / batch. (data: 6.72e-05)max mem: 15.94594 GB 
[10/31 23:59:20 visual_prompt]: Inference (test):avg data time: 1.53e-04, avg batch time: 0.2316, average loss: 0.7793
[10/31 23:59:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.39	
[10/31 23:59:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/01 00:00:27 visual_prompt]: 	Training 100/1106. train loss: 0.0050,	0.6215 s / batch. (data: 3.59e-04). ETA=16:12:45, max mem: 15.9 GB 
[11/01 00:01:31 visual_prompt]: 	Training 200/1106. train loss: 0.4101,	0.6261 s / batch. (data: 3.43e-04). ETA=16:18:51, max mem: 15.9 GB 
[11/01 00:02:34 visual_prompt]: 	Training 300/1106. train loss: 1.3606,	0.6428 s / batch. (data: 8.81e-04). ETA=16:43:57, max mem: 15.9 GB 
[11/01 00:03:38 visual_prompt]: 	Training 400/1106. train loss: 0.9608,	0.6442 s / batch. (data: 8.65e-04). ETA=16:45:00, max mem: 15.9 GB 
[11/01 00:04:42 visual_prompt]: 	Training 500/1106. train loss: 0.0023,	0.6428 s / batch. (data: 1.23e-02). ETA=16:41:44, max mem: 15.9 GB 
[11/01 00:05:46 visual_prompt]: 	Training 600/1106. train loss: 2.5657,	0.6547 s / batch. (data: 1.19e-03). ETA=16:59:15, max mem: 15.9 GB 
[11/01 00:06:49 visual_prompt]: 	Training 700/1106. train loss: 2.9459,	0.6320 s / batch. (data: 3.42e-04). ETA=16:22:52, max mem: 15.9 GB 
[11/01 00:07:53 visual_prompt]: 	Training 800/1106. train loss: 0.5161,	0.6600 s / batch. (data: 3.63e-04). ETA=17:05:18, max mem: 15.9 GB 
[11/01 00:08:57 visual_prompt]: 	Training 900/1106. train loss: 0.6515,	0.6249 s / batch. (data: 3.66e-04). ETA=16:09:43, max mem: 15.9 GB 
[11/01 00:10:00 visual_prompt]: 	Training 1000/1106. train loss: 0.6496,	0.6237 s / batch. (data: 3.26e-04). ETA=16:06:52, max mem: 15.9 GB 
[11/01 00:11:04 visual_prompt]: 	Training 1100/1106. train loss: 1.0214,	0.6193 s / batch. (data: 2.51e-04). ETA=15:59:00, max mem: 15.9 GB 
[11/01 00:11:08 visual_prompt]: Epoch 16 / 100: avg data time: 7.02e-03, avg batch time: 0.6401, average train loss: 1.0984
[11/01 00:12:02 visual_prompt]: 	Test 100/123. loss: 0.604, 0.2326 s / batch. (data: 6.08e-05)max mem: 15.94594 GB 
[11/01 00:12:14 visual_prompt]: Inference (val):avg data time: 4.87e-05, avg batch time: 0.2332, average loss: 0.7446
[11/01 00:12:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 60.98	
[11/01 00:13:08 visual_prompt]: 	Test 100/323. loss: 0.930, 0.2366 s / batch. (data: 6.03e-05)max mem: 15.94594 GB 
[11/01 00:14:01 visual_prompt]: 	Test 200/323. loss: 0.571, 0.2241 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 00:14:53 visual_prompt]: 	Test 300/323. loss: 0.742, 0.2242 s / batch. (data: 4.67e-05)max mem: 15.94594 GB 
[11/01 00:15:04 visual_prompt]: Inference (test):avg data time: 1.05e-04, avg batch time: 0.2320, average loss: 0.7895
[11/01 00:15:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 42.17	rocauc: 55.89	
[11/01 00:15:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/01 00:16:10 visual_prompt]: 	Training 100/1106. train loss: 2.2174,	0.6400 s / batch. (data: 5.53e-03). ETA=16:29:54, max mem: 15.9 GB 
[11/01 00:17:13 visual_prompt]: 	Training 200/1106. train loss: 1.7627,	0.6185 s / batch. (data: 3.58e-04). ETA=15:55:33, max mem: 15.9 GB 
[11/01 00:18:17 visual_prompt]: 	Training 300/1106. train loss: 0.0353,	0.6360 s / batch. (data: 3.69e-04). ETA=16:21:37, max mem: 15.9 GB 
[11/01 00:19:21 visual_prompt]: 	Training 400/1106. train loss: 0.2475,	0.6542 s / batch. (data: 1.41e-02). ETA=16:48:33, max mem: 15.9 GB 
[11/01 00:20:24 visual_prompt]: 	Training 500/1106. train loss: 0.6839,	0.6441 s / batch. (data: 1.28e-02). ETA=16:32:01, max mem: 15.9 GB 
[11/01 00:21:28 visual_prompt]: 	Training 600/1106. train loss: 0.6526,	0.6349 s / batch. (data: 1.49e-02). ETA=16:16:46, max mem: 15.9 GB 
[11/01 00:22:31 visual_prompt]: 	Training 700/1106. train loss: 0.2450,	0.6296 s / batch. (data: 8.27e-04). ETA=16:07:28, max mem: 15.9 GB 
[11/01 00:23:35 visual_prompt]: 	Training 800/1106. train loss: 0.2180,	0.6320 s / batch. (data: 3.49e-04). ETA=16:10:09, max mem: 15.9 GB 
[11/01 00:24:39 visual_prompt]: 	Training 900/1106. train loss: 0.8981,	0.6195 s / batch. (data: 3.21e-04). ETA=15:49:58, max mem: 15.9 GB 
[11/01 00:25:43 visual_prompt]: 	Training 1000/1106. train loss: 0.2737,	0.6327 s / batch. (data: 8.86e-04). ETA=16:09:07, max mem: 15.9 GB 
[11/01 00:26:46 visual_prompt]: 	Training 1100/1106. train loss: 0.2346,	0.6183 s / batch. (data: 2.16e-04). ETA=15:46:06, max mem: 15.9 GB 
[11/01 00:26:50 visual_prompt]: Epoch 17 / 100: avg data time: 6.15e-03, avg batch time: 0.6385, average train loss: 0.9984
[11/01 00:27:45 visual_prompt]: 	Test 100/123. loss: 0.618, 0.2386 s / batch. (data: 4.05e-05)max mem: 15.94594 GB 
[11/01 00:27:56 visual_prompt]: Inference (val):avg data time: 1.37e-04, avg batch time: 0.2318, average loss: 0.6990
[11/01 00:27:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 59.73	
[11/01 00:28:51 visual_prompt]: 	Test 100/323. loss: 0.825, 0.2316 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 00:29:43 visual_prompt]: 	Test 200/323. loss: 0.446, 0.2563 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[11/01 00:30:35 visual_prompt]: 	Test 300/323. loss: 0.680, 0.2245 s / batch. (data: 5.65e-05)max mem: 15.94594 GB 
[11/01 00:30:46 visual_prompt]: Inference (test):avg data time: 1.80e-04, avg batch time: 0.2322, average loss: 0.7159
[11/01 00:30:46 visual_prompt]: Classification results with test_mammo-cbis: top1: 51.78	rocauc: 59.60	
[11/01 00:30:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/01 00:31:53 visual_prompt]: 	Training 100/1106. train loss: 1.0400,	0.6179 s / batch. (data: 3.69e-04). ETA=15:44:18, max mem: 15.9 GB 
[11/01 00:32:58 visual_prompt]: 	Training 200/1106. train loss: 1.0680,	0.6609 s / batch. (data: 1.11e-02). ETA=16:48:55, max mem: 15.9 GB 
[11/01 00:34:02 visual_prompt]: 	Training 300/1106. train loss: 0.6831,	0.6424 s / batch. (data: 2.27e-02). ETA=16:19:35, max mem: 15.9 GB 
[11/01 00:35:05 visual_prompt]: 	Training 400/1106. train loss: 1.4984,	0.6336 s / batch. (data: 1.60e-02). ETA=16:05:07, max mem: 15.9 GB 
[11/01 00:36:09 visual_prompt]: 	Training 500/1106. train loss: 1.8985,	0.6384 s / batch. (data: 8.73e-04). ETA=16:11:26, max mem: 15.9 GB 
[11/01 00:37:13 visual_prompt]: 	Training 600/1106. train loss: 1.3490,	0.6560 s / batch. (data: 8.83e-04). ETA=16:37:08, max mem: 15.9 GB 
[11/01 00:38:16 visual_prompt]: 	Training 700/1106. train loss: 0.9255,	0.6432 s / batch. (data: 8.66e-04). ETA=16:16:36, max mem: 15.9 GB 
[11/01 00:39:20 visual_prompt]: 	Training 800/1106. train loss: 0.7442,	0.6299 s / batch. (data: 8.58e-04). ETA=15:55:16, max mem: 15.9 GB 
[11/01 00:40:24 visual_prompt]: 	Training 900/1106. train loss: 2.3954,	0.6336 s / batch. (data: 8.39e-04). ETA=15:59:55, max mem: 15.9 GB 
[11/01 00:41:28 visual_prompt]: 	Training 1000/1106. train loss: 1.8764,	0.6280 s / batch. (data: 3.39e-04). ETA=15:50:20, max mem: 15.9 GB 
[11/01 00:42:32 visual_prompt]: 	Training 1100/1106. train loss: 2.1208,	0.6193 s / batch. (data: 2.17e-04). ETA=15:36:04, max mem: 15.9 GB 
[11/01 00:42:35 visual_prompt]: Epoch 18 / 100: avg data time: 7.42e-03, avg batch time: 0.6412, average train loss: 1.0508
[11/01 00:43:30 visual_prompt]: 	Test 100/123. loss: 1.054, 0.2249 s / batch. (data: 5.79e-05)max mem: 15.94594 GB 
[11/01 00:43:41 visual_prompt]: Inference (val):avg data time: 4.81e-05, avg batch time: 0.2321, average loss: 0.9659
[11/01 00:43:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.08	
[11/01 00:44:36 visual_prompt]: 	Test 100/323. loss: 0.112, 0.2394 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[11/01 00:45:29 visual_prompt]: 	Test 200/323. loss: 0.491, 0.2244 s / batch. (data: 7.84e-05)max mem: 15.94594 GB 
[11/01 00:46:21 visual_prompt]: 	Test 300/323. loss: 1.503, 0.2297 s / batch. (data: 4.63e-05)max mem: 15.94594 GB 
[11/01 00:46:31 visual_prompt]: Inference (test):avg data time: 4.93e-05, avg batch time: 0.2323, average loss: 0.8988
[11/01 00:46:31 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.91	rocauc: 56.56	
[11/01 00:46:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/01 00:47:37 visual_prompt]: 	Training 100/1106. train loss: 2.4825,	0.6326 s / batch. (data: 3.61e-04). ETA=15:55:13, max mem: 15.9 GB 
[11/01 00:48:41 visual_prompt]: 	Training 200/1106. train loss: 1.2786,	0.6280 s / batch. (data: 3.64e-04). ETA=15:47:09, max mem: 15.9 GB 
[11/01 00:49:44 visual_prompt]: 	Training 300/1106. train loss: 0.6520,	0.6448 s / batch. (data: 1.11e-02). ETA=16:11:23, max mem: 15.9 GB 
[11/01 00:50:48 visual_prompt]: 	Training 400/1106. train loss: 0.3643,	0.6583 s / batch. (data: 6.08e-03). ETA=16:30:39, max mem: 15.9 GB 
[11/01 00:51:52 visual_prompt]: 	Training 500/1106. train loss: 0.4577,	0.6200 s / batch. (data: 3.85e-04). ETA=15:31:57, max mem: 15.9 GB 
[11/01 00:52:55 visual_prompt]: 	Training 600/1106. train loss: 1.3693,	0.6327 s / batch. (data: 5.57e-03). ETA=15:50:05, max mem: 15.9 GB 
[11/01 00:53:59 visual_prompt]: 	Training 700/1106. train loss: 1.7280,	0.6400 s / batch. (data: 8.08e-04). ETA=15:59:54, max mem: 15.9 GB 
[11/01 00:55:03 visual_prompt]: 	Training 800/1106. train loss: 0.8469,	0.6480 s / batch. (data: 8.48e-04). ETA=16:10:51, max mem: 15.9 GB 
[11/01 00:56:07 visual_prompt]: 	Training 900/1106. train loss: 0.6623,	0.6435 s / batch. (data: 8.77e-04). ETA=16:02:59, max mem: 15.9 GB 
[11/01 00:57:10 visual_prompt]: 	Training 1000/1106. train loss: 0.7865,	0.6280 s / batch. (data: 3.49e-04). ETA=15:38:45, max mem: 15.9 GB 
[11/01 00:58:14 visual_prompt]: 	Training 1100/1106. train loss: 2.6994,	0.6314 s / batch. (data: 2.32e-04). ETA=15:42:50, max mem: 15.9 GB 
[11/01 00:58:18 visual_prompt]: Epoch 19 / 100: avg data time: 5.45e-03, avg batch time: 0.6388, average train loss: 1.0415
[11/01 00:59:13 visual_prompt]: 	Test 100/123. loss: 0.553, 0.2278 s / batch. (data: 5.29e-05)max mem: 15.94594 GB 
[11/01 00:59:24 visual_prompt]: Inference (val):avg data time: 5.06e-05, avg batch time: 0.2326, average loss: 0.7205
[11/01 00:59:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 58.73	
[11/01 01:00:19 visual_prompt]: 	Test 100/323. loss: 0.653, 0.2241 s / batch. (data: 5.60e-05)max mem: 15.94594 GB 
[11/01 01:01:11 visual_prompt]: 	Test 200/323. loss: 0.246, 0.2517 s / batch. (data: 3.15e-05)max mem: 15.94594 GB 
[11/01 01:02:03 visual_prompt]: 	Test 300/323. loss: 0.689, 0.2296 s / batch. (data: 4.70e-05)max mem: 15.94594 GB 
[11/01 01:02:14 visual_prompt]: Inference (test):avg data time: 9.59e-05, avg batch time: 0.2321, average loss: 0.7266
[11/01 01:02:14 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.14	rocauc: 59.69	
[11/01 01:02:14 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[11/01 01:03:20 visual_prompt]: 	Training 100/1106. train loss: 0.6220,	0.6427 s / batch. (data: 4.43e-04). ETA=15:58:35, max mem: 15.9 GB 
[11/01 01:04:24 visual_prompt]: 	Training 200/1106. train loss: 1.1063,	0.6462 s / batch. (data: 6.03e-03). ETA=16:02:43, max mem: 15.9 GB 
[11/01 01:05:28 visual_prompt]: 	Training 300/1106. train loss: 0.8220,	0.6682 s / batch. (data: 3.79e-02). ETA=16:34:22, max mem: 15.9 GB 
[11/01 01:06:31 visual_prompt]: 	Training 400/1106. train loss: 0.5753,	0.6480 s / batch. (data: 8.46e-04). ETA=16:03:13, max mem: 15.9 GB 
[11/01 01:07:35 visual_prompt]: 	Training 500/1106. train loss: 0.2128,	0.6266 s / batch. (data: 3.72e-04). ETA=15:30:22, max mem: 15.9 GB 
[11/01 01:08:39 visual_prompt]: 	Training 600/1106. train loss: 0.8239,	0.6391 s / batch. (data: 8.87e-04). ETA=15:47:49, max mem: 15.9 GB 
[11/01 01:09:43 visual_prompt]: 	Training 700/1106. train loss: 0.3344,	0.6433 s / batch. (data: 5.55e-03). ETA=15:53:00, max mem: 15.9 GB 
[11/01 01:10:47 visual_prompt]: 	Training 800/1106. train loss: 0.6722,	0.6468 s / batch. (data: 8.96e-04). ETA=15:57:06, max mem: 15.9 GB 
[11/01 01:11:50 visual_prompt]: 	Training 900/1106. train loss: 0.2495,	0.6430 s / batch. (data: 8.59e-04). ETA=15:50:26, max mem: 15.9 GB 
[11/01 01:12:54 visual_prompt]: 	Training 1000/1106. train loss: 0.6905,	0.6179 s / batch. (data: 3.58e-04). ETA=15:12:19, max mem: 15.9 GB 
[11/01 01:13:58 visual_prompt]: 	Training 1100/1106. train loss: 0.8409,	0.6191 s / batch. (data: 2.32e-04). ETA=15:12:58, max mem: 15.9 GB 
[11/01 01:14:02 visual_prompt]: Epoch 20 / 100: avg data time: 6.60e-03, avg batch time: 0.6401, average train loss: 1.0522
[11/01 01:14:56 visual_prompt]: 	Test 100/123. loss: 0.931, 0.2316 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[11/01 01:15:07 visual_prompt]: Inference (val):avg data time: 5.03e-05, avg batch time: 0.2326, average loss: 0.9056
[11/01 01:15:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.99	
[11/01 01:16:02 visual_prompt]: 	Test 100/323. loss: 0.166, 0.2268 s / batch. (data: 4.96e-05)max mem: 15.94594 GB 
[11/01 01:16:55 visual_prompt]: 	Test 200/323. loss: 0.560, 0.2357 s / batch. (data: 3.74e-05)max mem: 15.94594 GB 
[11/01 01:17:46 visual_prompt]: 	Test 300/323. loss: 1.126, 0.2247 s / batch. (data: 5.29e-05)max mem: 15.94594 GB 
[11/01 01:17:58 visual_prompt]: Inference (test):avg data time: 2.05e-04, avg batch time: 0.2324, average loss: 0.8379
[11/01 01:17:58 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.35	
[11/01 01:17:58 visual_prompt]: Stopping early.
[11/01 01:17:58 visual_prompt]: Rank of current process: 0. World size: 1
[11/01 01:17:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/01 01:17:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/01 01:17:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/01 01:17:58 visual_prompt]: Training with config:
[11/01 01:17:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/test/seed875/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 875, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/01 01:17:58 visual_prompt]: Loading training data...
[11/01 01:17:58 visual_prompt]: Constructing mammo-cbis dataset train...
[11/01 01:17:58 visual_prompt]: Loading validation data...
[11/01 01:17:58 visual_prompt]: Constructing mammo-cbis dataset val...
[11/01 01:17:58 visual_prompt]: Loading test data...
[11/01 01:17:58 visual_prompt]: Constructing mammo-cbis dataset test...
[11/01 01:17:58 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[11/01 01:18:05 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[11/01 01:18:05 visual_prompt]: tuned percent:0.522
[11/01 01:18:05 visual_prompt]: Device used for model: 0
[11/01 01:18:05 visual_prompt]: Setting up Evaluator...
[11/01 01:18:05 visual_prompt]: Setting up Trainer...
[11/01 01:18:05 visual_prompt]: 	Setting up the optimizer...
[11/01 01:18:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/01 01:19:11 visual_prompt]: 	Training 100/1106. train loss: 1.8820,	0.6409 s / batch. (data: 1.11e-02). ETA=19:40:18, max mem: 15.9 GB 
[11/01 01:20:15 visual_prompt]: 	Training 200/1106. train loss: 0.5512,	0.6319 s / batch. (data: 3.18e-04). ETA=19:22:42, max mem: 15.9 GB 
[11/01 01:21:18 visual_prompt]: 	Training 300/1106. train loss: 0.2093,	0.6399 s / batch. (data: 3.44e-04). ETA=19:36:15, max mem: 15.9 GB 
[11/01 01:22:22 visual_prompt]: 	Training 400/1106. train loss: 0.2503,	0.6305 s / batch. (data: 1.20e-02). ETA=19:18:01, max mem: 15.9 GB 
[11/01 01:23:26 visual_prompt]: 	Training 500/1106. train loss: 1.1625,	0.6406 s / batch. (data: 6.03e-03). ETA=19:35:31, max mem: 15.9 GB 
[11/01 01:24:30 visual_prompt]: 	Training 600/1106. train loss: 0.4247,	0.6360 s / batch. (data: 8.62e-04). ETA=19:26:03, max mem: 15.9 GB 
[11/01 01:25:34 visual_prompt]: 	Training 700/1106. train loss: 1.5730,	0.6482 s / batch. (data: 8.51e-04). ETA=19:47:19, max mem: 15.9 GB 
[11/01 01:26:38 visual_prompt]: 	Training 800/1106. train loss: 1.7968,	0.6331 s / batch. (data: 1.31e-02). ETA=19:18:34, max mem: 15.9 GB 
[11/01 01:27:41 visual_prompt]: 	Training 900/1106. train loss: 0.6195,	0.6519 s / batch. (data: 9.42e-04). ETA=19:51:53, max mem: 15.9 GB 
[11/01 01:28:45 visual_prompt]: 	Training 1000/1106. train loss: 0.2661,	0.6404 s / batch. (data: 1.64e-02). ETA=19:29:51, max mem: 15.9 GB 
[11/01 01:29:49 visual_prompt]: 	Training 1100/1106. train loss: 0.9523,	0.6181 s / batch. (data: 2.05e-04). ETA=18:48:01, max mem: 15.9 GB 
[11/01 01:29:52 visual_prompt]: Epoch 1 / 100: avg data time: 6.81e-03, avg batch time: 0.6399, average train loss: 0.8011
[11/01 01:30:47 visual_prompt]: 	Test 100/123. loss: 0.542, 0.2401 s / batch. (data: 7.27e-05)max mem: 15.94594 GB 
[11/01 01:30:59 visual_prompt]: Inference (val):avg data time: 5.02e-05, avg batch time: 0.2319, average loss: 0.7152
[11/01 01:30:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 53.56	
[11/01 01:31:54 visual_prompt]: 	Test 100/323. loss: 0.579, 0.2246 s / batch. (data: 5.15e-05)max mem: 15.94594 GB 
[11/01 01:32:47 visual_prompt]: 	Test 200/323. loss: 0.565, 0.2242 s / batch. (data: 5.34e-05)max mem: 15.94594 GB 
[11/01 01:33:39 visual_prompt]: 	Test 300/323. loss: 0.939, 0.2236 s / batch. (data: 6.63e-05)max mem: 15.94594 GB 
[11/01 01:33:50 visual_prompt]: Inference (test):avg data time: 8.43e-05, avg batch time: 0.2318, average loss: 0.7249
[11/01 01:33:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.67	rocauc: 51.18	
[11/01 01:33:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/01 01:34:56 visual_prompt]: 	Training 100/1106. train loss: 0.8477,	0.6283 s / batch. (data: 8.84e-04). ETA=19:05:33, max mem: 15.9 GB 
[11/01 01:36:00 visual_prompt]: 	Training 200/1106. train loss: 0.6400,	0.6183 s / batch. (data: 3.92e-04). ETA=18:46:13, max mem: 15.9 GB 
[11/01 01:37:04 visual_prompt]: 	Training 300/1106. train loss: 0.7103,	0.6588 s / batch. (data: 8.76e-04). ETA=19:58:57, max mem: 15.9 GB 
[11/01 01:38:07 visual_prompt]: 	Training 400/1106. train loss: 1.2375,	0.6563 s / batch. (data: 3.30e-02). ETA=19:53:19, max mem: 15.9 GB 
[11/01 01:39:11 visual_prompt]: 	Training 500/1106. train loss: 0.8449,	0.6251 s / batch. (data: 3.59e-04). ETA=18:55:27, max mem: 15.9 GB 
[11/01 01:40:15 visual_prompt]: 	Training 600/1106. train loss: 0.3262,	0.6454 s / batch. (data: 1.11e-02). ETA=19:31:16, max mem: 15.9 GB 
[11/01 01:41:19 visual_prompt]: 	Training 700/1106. train loss: 1.1154,	0.6177 s / batch. (data: 3.40e-04). ETA=18:40:03, max mem: 15.9 GB 
[11/01 01:42:22 visual_prompt]: 	Training 800/1106. train loss: 0.1775,	0.6440 s / batch. (data: 3.64e-04). ETA=19:26:38, max mem: 15.9 GB 
[11/01 01:43:26 visual_prompt]: 	Training 900/1106. train loss: 0.7968,	0.6496 s / batch. (data: 8.37e-04). ETA=19:35:45, max mem: 15.9 GB 
[11/01 01:44:29 visual_prompt]: 	Training 1000/1106. train loss: 0.4773,	0.6297 s / batch. (data: 1.07e-02). ETA=18:58:40, max mem: 15.9 GB 
[11/01 01:45:33 visual_prompt]: 	Training 1100/1106. train loss: 1.1141,	0.6341 s / batch. (data: 2.05e-04). ETA=19:05:32, max mem: 15.9 GB 
[11/01 01:45:37 visual_prompt]: Epoch 2 / 100: avg data time: 5.83e-03, avg batch time: 0.6389, average train loss: 0.8821
[11/01 01:46:32 visual_prompt]: 	Test 100/123. loss: 1.361, 0.2473 s / batch. (data: 6.08e-05)max mem: 15.94594 GB 
[11/01 01:46:43 visual_prompt]: Inference (val):avg data time: 4.95e-05, avg batch time: 0.2323, average loss: 1.2213
[11/01 01:46:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.46	
[11/01 01:47:38 visual_prompt]: 	Test 100/323. loss: 0.079, 0.2247 s / batch. (data: 6.25e-05)max mem: 15.94594 GB 
[11/01 01:48:31 visual_prompt]: 	Test 200/323. loss: 1.283, 0.2248 s / batch. (data: 4.03e-05)max mem: 15.94594 GB 
[11/01 01:49:23 visual_prompt]: 	Test 300/323. loss: 1.369, 0.2239 s / batch. (data: 5.08e-05)max mem: 15.94594 GB 
[11/01 01:49:34 visual_prompt]: Inference (test):avg data time: 8.41e-05, avg batch time: 0.2324, average loss: 1.1171
[11/01 01:49:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 51.77	
[11/01 01:49:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/01 01:50:41 visual_prompt]: 	Training 100/1106. train loss: 0.3793,	0.6240 s / batch. (data: 3.73e-04). ETA=18:46:14, max mem: 15.9 GB 
[11/01 01:51:45 visual_prompt]: 	Training 200/1106. train loss: 0.8012,	0.6640 s / batch. (data: 8.78e-04). ETA=19:57:17, max mem: 15.9 GB 
[11/01 01:52:48 visual_prompt]: 	Training 300/1106. train loss: 0.6219,	0.6604 s / batch. (data: 8.41e-03). ETA=19:49:45, max mem: 15.9 GB 
[11/01 01:53:52 visual_prompt]: 	Training 400/1106. train loss: 0.5432,	0.6440 s / batch. (data: 3.61e-04). ETA=19:19:04, max mem: 15.9 GB 
[11/01 01:54:56 visual_prompt]: 	Training 500/1106. train loss: 0.0461,	0.6188 s / batch. (data: 3.54e-04). ETA=18:32:43, max mem: 15.9 GB 
[11/01 01:56:00 visual_prompt]: 	Training 600/1106. train loss: 0.4561,	0.6186 s / batch. (data: 3.58e-04). ETA=18:31:21, max mem: 15.9 GB 
[11/01 01:57:04 visual_prompt]: 	Training 700/1106. train loss: 0.6903,	0.6425 s / batch. (data: 1.20e-02). ETA=19:13:10, max mem: 15.9 GB 
[11/01 01:58:07 visual_prompt]: 	Training 800/1106. train loss: 0.3028,	0.6342 s / batch. (data: 3.28e-04). ETA=18:57:15, max mem: 15.9 GB 
[11/01 01:59:11 visual_prompt]: 	Training 900/1106. train loss: 0.6271,	0.6324 s / batch. (data: 1.05e-03). ETA=18:52:51, max mem: 15.9 GB 
[11/01 02:00:15 visual_prompt]: 	Training 1000/1106. train loss: 0.4918,	0.6446 s / batch. (data: 1.06e-02). ETA=19:13:46, max mem: 15.9 GB 
[11/01 02:01:18 visual_prompt]: 	Training 1100/1106. train loss: 0.2907,	0.6174 s / batch. (data: 2.29e-04). ETA=18:24:03, max mem: 15.9 GB 
[11/01 02:01:22 visual_prompt]: Epoch 3 / 100: avg data time: 6.78e-03, avg batch time: 0.6402, average train loss: 0.8899
[11/01 02:02:17 visual_prompt]: 	Test 100/123. loss: 0.679, 0.2423 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[11/01 02:02:28 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.2336, average loss: 0.6941
[11/01 02:02:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 54.94	
[11/01 02:03:23 visual_prompt]: 	Test 100/323. loss: 0.744, 0.2244 s / batch. (data: 5.82e-05)max mem: 15.94594 GB 
[11/01 02:04:16 visual_prompt]: 	Test 200/323. loss: 0.648, 0.2385 s / batch. (data: 5.22e-05)max mem: 15.94594 GB 
[11/01 02:05:08 visual_prompt]: 	Test 300/323. loss: 0.713, 0.2242 s / batch. (data: 5.05e-05)max mem: 15.94594 GB 
[11/01 02:05:19 visual_prompt]: Inference (test):avg data time: 8.95e-05, avg batch time: 0.2328, average loss: 0.6983
[11/01 02:05:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 45.58	rocauc: 52.91	
[11/01 02:05:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/01 02:06:26 visual_prompt]: 	Training 100/1106. train loss: 1.2133,	0.6390 s / batch. (data: 5.53e-03). ETA=19:01:28, max mem: 15.9 GB 
[11/01 02:07:30 visual_prompt]: 	Training 200/1106. train loss: 0.9804,	0.6245 s / batch. (data: 3.67e-04). ETA=18:34:36, max mem: 15.9 GB 
[11/01 02:08:33 visual_prompt]: 	Training 300/1106. train loss: 2.5842,	0.6610 s / batch. (data: 1.07e-02). ETA=19:38:31, max mem: 15.9 GB 
[11/01 02:09:37 visual_prompt]: 	Training 400/1106. train loss: 1.0659,	0.6339 s / batch. (data: 1.11e-03). ETA=18:49:13, max mem: 15.9 GB 
[11/01 02:10:41 visual_prompt]: 	Training 500/1106. train loss: 0.7916,	0.6254 s / batch. (data: 3.70e-04). ETA=18:32:57, max mem: 15.9 GB 
[11/01 02:11:44 visual_prompt]: 	Training 600/1106. train loss: 0.6705,	0.6565 s / batch. (data: 1.62e-02). ETA=19:27:14, max mem: 15.9 GB 
[11/01 02:12:48 visual_prompt]: 	Training 700/1106. train loss: 0.6758,	0.6328 s / batch. (data: 3.79e-04). ETA=18:44:03, max mem: 15.9 GB 
[11/01 02:13:52 visual_prompt]: 	Training 800/1106. train loss: 3.2829,	0.6194 s / batch. (data: 3.29e-04). ETA=18:19:14, max mem: 15.9 GB 
[11/01 02:14:56 visual_prompt]: 	Training 900/1106. train loss: 1.7628,	0.6805 s / batch. (data: 1.06e-02). ETA=20:06:37, max mem: 15.9 GB 
[11/01 02:16:00 visual_prompt]: 	Training 1000/1106. train loss: 0.7816,	0.6320 s / batch. (data: 3.63e-04). ETA=18:39:31, max mem: 15.9 GB 
[11/01 02:17:04 visual_prompt]: 	Training 1100/1106. train loss: 0.8861,	0.6190 s / batch. (data: 1.70e-04). ETA=18:15:29, max mem: 15.9 GB 
[11/01 02:17:08 visual_prompt]: Epoch 4 / 100: avg data time: 6.62e-03, avg batch time: 0.6402, average train loss: 0.9820
[11/01 02:18:02 visual_prompt]: 	Test 100/123. loss: 0.987, 0.2447 s / batch. (data: 6.27e-05)max mem: 15.94594 GB 
[11/01 02:18:14 visual_prompt]: Inference (val):avg data time: 3.00e-04, avg batch time: 0.2345, average loss: 0.8937
[11/01 02:18:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.78	
[11/01 02:19:09 visual_prompt]: 	Test 100/323. loss: 0.187, 0.2359 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[11/01 02:20:02 visual_prompt]: 	Test 200/323. loss: 0.884, 0.2366 s / batch. (data: 6.29e-05)max mem: 15.94594 GB 
[11/01 02:20:54 visual_prompt]: 	Test 300/323. loss: 1.022, 0.2240 s / batch. (data: 7.15e-05)max mem: 15.94594 GB 
[11/01 02:21:05 visual_prompt]: Inference (test):avg data time: 1.27e-04, avg batch time: 0.2325, average loss: 0.8284
[11/01 02:21:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 54.97	
[11/01 02:21:05 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/01 02:22:10 visual_prompt]: 	Training 100/1106. train loss: 2.4766,	0.6321 s / batch. (data: 3.83e-04). ETA=18:37:29, max mem: 15.9 GB 
[11/01 02:23:14 visual_prompt]: 	Training 200/1106. train loss: 1.3772,	0.6320 s / batch. (data: 3.53e-04). ETA=18:36:17, max mem: 15.9 GB 
[11/01 02:24:18 visual_prompt]: 	Training 300/1106. train loss: 0.8840,	0.6526 s / batch. (data: 1.15e-03). ETA=19:11:39, max mem: 15.9 GB 
[11/01 02:25:22 visual_prompt]: 	Training 400/1106. train loss: 3.3837,	0.6480 s / batch. (data: 3.62e-04). ETA=19:02:24, max mem: 15.9 GB 
[11/01 02:26:25 visual_prompt]: 	Training 500/1106. train loss: 1.9732,	0.6462 s / batch. (data: 6.03e-03). ETA=18:58:03, max mem: 15.9 GB 
[11/01 02:27:29 visual_prompt]: 	Training 600/1106. train loss: 2.1543,	0.6187 s / batch. (data: 4.81e-04). ETA=18:08:35, max mem: 15.9 GB 
[11/01 02:28:33 visual_prompt]: 	Training 700/1106. train loss: 1.4092,	0.6338 s / batch. (data: 3.37e-04). ETA=18:34:06, max mem: 15.9 GB 
[11/01 02:29:37 visual_prompt]: 	Training 800/1106. train loss: 0.8357,	0.6449 s / batch. (data: 8.49e-04). ETA=18:52:32, max mem: 15.9 GB 
[11/01 02:30:41 visual_prompt]: 	Training 900/1106. train loss: 0.8230,	0.6298 s / batch. (data: 3.40e-04). ETA=18:25:03, max mem: 15.9 GB 
[11/01 02:31:44 visual_prompt]: 	Training 1000/1106. train loss: 0.9942,	0.6524 s / batch. (data: 2.48e-02). ETA=19:03:35, max mem: 15.9 GB 
[11/01 02:32:48 visual_prompt]: 	Training 1100/1106. train loss: 1.6784,	0.6183 s / batch. (data: 2.12e-04). ETA=18:02:50, max mem: 15.9 GB 
[11/01 02:32:52 visual_prompt]: Epoch 5 / 100: avg data time: 5.07e-03, avg batch time: 0.6390, average train loss: 0.9495
[11/01 02:33:46 visual_prompt]: 	Test 100/123. loss: 1.368, 0.2489 s / batch. (data: 5.46e-05)max mem: 15.94594 GB 
[11/01 02:33:57 visual_prompt]: Inference (val):avg data time: 4.89e-05, avg batch time: 0.2328, average loss: 1.5325
[11/01 02:33:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.27	
[11/01 02:34:52 visual_prompt]: 	Test 100/323. loss: 2.654, 0.2248 s / batch. (data: 6.03e-05)max mem: 15.94594 GB 
[11/01 02:35:44 visual_prompt]: 	Test 200/323. loss: 1.336, 0.2248 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/01 02:36:36 visual_prompt]: 	Test 300/323. loss: 1.390, 0.2328 s / batch. (data: 8.15e-05)max mem: 15.94594 GB 
[11/01 02:36:47 visual_prompt]: Inference (test):avg data time: 9.47e-05, avg batch time: 0.2315, average loss: 1.6508
[11/01 02:36:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 55.69	
[11/01 02:36:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/01 02:37:53 visual_prompt]: 	Training 100/1106. train loss: 2.1919,	0.6225 s / batch. (data: 3.42e-04). ETA=18:09:06, max mem: 15.9 GB 
[11/01 02:38:57 visual_prompt]: 	Training 200/1106. train loss: 0.2922,	0.6175 s / batch. (data: 3.38e-04). ETA=17:59:21, max mem: 15.9 GB 
[11/01 02:40:01 visual_prompt]: 	Training 300/1106. train loss: 1.2150,	0.6368 s / batch. (data: 8.94e-04). ETA=18:32:00, max mem: 15.9 GB 
[11/01 02:41:05 visual_prompt]: 	Training 400/1106. train loss: 0.7690,	0.6481 s / batch. (data: 8.46e-04). ETA=18:50:32, max mem: 15.9 GB 
[11/01 02:42:08 visual_prompt]: 	Training 500/1106. train loss: 0.7847,	0.6339 s / batch. (data: 3.67e-04). ETA=18:24:51, max mem: 15.9 GB 
[11/01 02:43:12 visual_prompt]: 	Training 600/1106. train loss: 1.0427,	0.6252 s / batch. (data: 3.79e-04). ETA=18:08:33, max mem: 15.9 GB 
[11/01 02:44:16 visual_prompt]: 	Training 700/1106. train loss: 0.0698,	0.6246 s / batch. (data: 5.56e-03). ETA=18:06:32, max mem: 15.9 GB 
[11/01 02:45:19 visual_prompt]: 	Training 800/1106. train loss: 0.2930,	0.6320 s / batch. (data: 3.78e-04). ETA=18:18:17, max mem: 15.9 GB 
[11/01 02:46:23 visual_prompt]: 	Training 900/1106. train loss: 0.4064,	0.6316 s / batch. (data: 3.80e-04). ETA=18:16:38, max mem: 15.9 GB 
[11/01 02:47:27 visual_prompt]: 	Training 1000/1106. train loss: 0.8083,	0.6374 s / batch. (data: 5.56e-03). ETA=18:25:37, max mem: 15.9 GB 
[11/01 02:48:30 visual_prompt]: 	Training 1100/1106. train loss: 0.5079,	0.6183 s / batch. (data: 2.15e-04). ETA=17:51:24, max mem: 15.9 GB 
[11/01 02:48:34 visual_prompt]: Epoch 6 / 100: avg data time: 6.71e-03, avg batch time: 0.6395, average train loss: 1.0053
[11/01 02:49:28 visual_prompt]: 	Test 100/123. loss: 1.128, 0.2495 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 02:49:40 visual_prompt]: Inference (val):avg data time: 4.87e-05, avg batch time: 0.2324, average loss: 1.2622
[11/01 02:49:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.59	
[11/01 02:50:35 visual_prompt]: 	Test 100/323. loss: 2.159, 0.2243 s / batch. (data: 4.65e-05)max mem: 15.94594 GB 
[11/01 02:51:28 visual_prompt]: 	Test 200/323. loss: 1.110, 0.2332 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[11/01 02:52:19 visual_prompt]: 	Test 300/323. loss: 1.160, 0.2370 s / batch. (data: 6.20e-05)max mem: 15.94594 GB 
[11/01 02:52:30 visual_prompt]: Inference (test):avg data time: 2.31e-04, avg batch time: 0.2312, average loss: 1.3635
[11/01 02:52:30 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.51	
[11/01 02:52:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/01 02:53:36 visual_prompt]: 	Training 100/1106. train loss: 1.4372,	0.6459 s / batch. (data: 9.01e-04). ETA=18:38:05, max mem: 15.9 GB 
[11/01 02:54:40 visual_prompt]: 	Training 200/1106. train loss: 0.8734,	0.6324 s / batch. (data: 9.08e-04). ETA=18:13:38, max mem: 15.9 GB 
[11/01 02:55:44 visual_prompt]: 	Training 300/1106. train loss: 0.8758,	0.6514 s / batch. (data: 2.33e-02). ETA=18:45:24, max mem: 15.9 GB 
[11/01 02:56:48 visual_prompt]: 	Training 400/1106. train loss: 2.6569,	0.6360 s / batch. (data: 8.91e-04). ETA=18:17:45, max mem: 15.9 GB 
[11/01 02:57:52 visual_prompt]: 	Training 500/1106. train loss: 0.0298,	0.6325 s / batch. (data: 3.67e-04). ETA=18:10:37, max mem: 15.9 GB 
[11/01 02:58:55 visual_prompt]: 	Training 600/1106. train loss: 0.6711,	0.6280 s / batch. (data: 8.27e-04). ETA=18:01:51, max mem: 15.9 GB 
[11/01 02:59:59 visual_prompt]: 	Training 700/1106. train loss: 1.7094,	0.6350 s / batch. (data: 3.55e-04). ETA=18:12:51, max mem: 15.9 GB 
[11/01 03:01:02 visual_prompt]: 	Training 800/1106. train loss: 2.4358,	0.6479 s / batch. (data: 8.66e-04). ETA=18:34:00, max mem: 15.9 GB 
[11/01 03:02:06 visual_prompt]: 	Training 900/1106. train loss: 1.9490,	0.6441 s / batch. (data: 9.46e-04). ETA=18:26:20, max mem: 15.9 GB 
[11/01 03:03:10 visual_prompt]: 	Training 1000/1106. train loss: 1.3745,	0.6437 s / batch. (data: 2.44e-02). ETA=18:24:36, max mem: 15.9 GB 
[11/01 03:04:14 visual_prompt]: 	Training 1100/1106. train loss: 0.5790,	0.6183 s / batch. (data: 2.06e-04). ETA=17:39:58, max mem: 15.9 GB 
[11/01 03:04:18 visual_prompt]: Epoch 7 / 100: avg data time: 5.80e-03, avg batch time: 0.6395, average train loss: 1.0970
[11/01 03:05:12 visual_prompt]: 	Test 100/123. loss: 2.258, 0.2317 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/01 03:05:23 visual_prompt]: Inference (val):avg data time: 1.39e-04, avg batch time: 0.2322, average loss: 2.0212
[11/01 03:05:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.74	
[11/01 03:06:19 visual_prompt]: 	Test 100/323. loss: 0.011, 0.2254 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/01 03:07:10 visual_prompt]: 	Test 200/323. loss: 2.164, 0.2317 s / batch. (data: 3.55e-05)max mem: 15.94594 GB 
[11/01 03:08:02 visual_prompt]: 	Test 300/323. loss: 2.332, 0.2248 s / batch. (data: 4.89e-05)max mem: 15.94594 GB 
[11/01 03:08:13 visual_prompt]: Inference (test):avg data time: 1.41e-04, avg batch time: 0.2325, average loss: 1.8341
[11/01 03:08:13 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 54.60	
[11/01 03:08:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/01 03:09:20 visual_prompt]: 	Training 100/1106. train loss: 0.4588,	0.6290 s / batch. (data: 9.25e-04). ETA=17:57:13, max mem: 15.9 GB 
[11/01 03:10:23 visual_prompt]: 	Training 200/1106. train loss: 0.8733,	0.6363 s / batch. (data: 1.07e-02). ETA=18:08:42, max mem: 15.9 GB 
[11/01 03:11:27 visual_prompt]: 	Training 300/1106. train loss: 0.7416,	0.6321 s / batch. (data: 8.87e-04). ETA=18:00:23, max mem: 15.9 GB 
[11/01 03:12:31 visual_prompt]: 	Training 400/1106. train loss: 0.8012,	0.6396 s / batch. (data: 1.56e-02). ETA=18:12:15, max mem: 15.9 GB 
[11/01 03:13:35 visual_prompt]: 	Training 500/1106. train loss: 0.6300,	0.6297 s / batch. (data: 1.13e-03). ETA=17:54:18, max mem: 15.9 GB 
[11/01 03:14:39 visual_prompt]: 	Training 600/1106. train loss: 0.7779,	0.6698 s / batch. (data: 2.58e-02). ETA=19:01:31, max mem: 15.9 GB 
[11/01 03:15:43 visual_prompt]: 	Training 700/1106. train loss: 0.6806,	0.6400 s / batch. (data: 8.69e-03). ETA=18:09:41, max mem: 15.9 GB 
[11/01 03:16:46 visual_prompt]: 	Training 800/1106. train loss: 1.2603,	0.6326 s / batch. (data: 8.77e-04). ETA=17:55:57, max mem: 15.9 GB 
[11/01 03:17:50 visual_prompt]: 	Training 900/1106. train loss: 0.5406,	0.6488 s / batch. (data: 8.74e-03). ETA=18:22:27, max mem: 15.9 GB 
[11/01 03:18:54 visual_prompt]: 	Training 1000/1106. train loss: 0.8977,	0.6324 s / batch. (data: 1.25e-03). ETA=17:53:34, max mem: 15.9 GB 
[11/01 03:19:58 visual_prompt]: 	Training 1100/1106. train loss: 0.6604,	0.6185 s / batch. (data: 2.10e-04). ETA=17:28:59, max mem: 15.9 GB 
[11/01 03:20:01 visual_prompt]: Epoch 8 / 100: avg data time: 6.04e-03, avg batch time: 0.6401, average train loss: 1.0461
[11/01 03:20:56 visual_prompt]: 	Test 100/123. loss: 0.793, 0.2326 s / batch. (data: 6.13e-05)max mem: 15.94594 GB 
[11/01 03:21:07 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.2336, average loss: 0.7584
[11/01 03:21:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.12	
[11/01 03:22:02 visual_prompt]: 	Test 100/323. loss: 0.252, 0.2276 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[11/01 03:22:55 visual_prompt]: 	Test 200/323. loss: 0.667, 0.2249 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/01 03:23:46 visual_prompt]: 	Test 300/323. loss: 0.944, 0.2254 s / batch. (data: 3.29e-05)max mem: 15.94594 GB 
[11/01 03:23:58 visual_prompt]: Inference (test):avg data time: 2.67e-04, avg batch time: 0.2325, average loss: 0.7203
[11/01 03:23:58 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.00	
[11/01 03:23:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/01 03:25:04 visual_prompt]: 	Training 100/1106. train loss: 0.6122,	0.6600 s / batch. (data: 3.57e-04). ETA=18:38:08, max mem: 15.9 GB 
[11/01 03:26:08 visual_prompt]: 	Training 200/1106. train loss: 0.7645,	0.6421 s / batch. (data: 3.42e-04). ETA=18:06:44, max mem: 15.9 GB 
[11/01 03:27:12 visual_prompt]: 	Training 300/1106. train loss: 1.3884,	0.6626 s / batch. (data: 1.19e-03). ETA=18:40:17, max mem: 15.9 GB 
[11/01 03:28:16 visual_prompt]: 	Training 400/1106. train loss: 2.2400,	0.6435 s / batch. (data: 8.87e-04). ETA=18:06:56, max mem: 15.9 GB 
[11/01 03:29:19 visual_prompt]: 	Training 500/1106. train loss: 1.7722,	0.6428 s / batch. (data: 1.11e-02). ETA=18:04:46, max mem: 15.9 GB 
[11/01 03:30:23 visual_prompt]: 	Training 600/1106. train loss: 0.4883,	0.6441 s / batch. (data: 8.76e-04). ETA=18:05:51, max mem: 15.9 GB 
[11/01 03:31:27 visual_prompt]: 	Training 700/1106. train loss: 2.2904,	0.6480 s / batch. (data: 3.44e-04). ETA=18:11:20, max mem: 15.9 GB 
[11/01 03:32:31 visual_prompt]: 	Training 800/1106. train loss: 0.5676,	0.6360 s / batch. (data: 8.35e-04). ETA=17:50:04, max mem: 15.9 GB 
[11/01 03:33:34 visual_prompt]: 	Training 900/1106. train loss: 0.1694,	0.6540 s / batch. (data: 1.06e-02). ETA=18:19:15, max mem: 15.9 GB 
[11/01 03:34:38 visual_prompt]: 	Training 1000/1106. train loss: 4.6115,	0.6320 s / batch. (data: 3.78e-04). ETA=17:41:14, max mem: 15.9 GB 
[11/01 03:35:42 visual_prompt]: 	Training 1100/1106. train loss: 0.8162,	0.6177 s / batch. (data: 1.65e-04). ETA=17:16:16, max mem: 15.9 GB 
[11/01 03:35:46 visual_prompt]: Epoch 9 / 100: avg data time: 6.43e-03, avg batch time: 0.6398, average train loss: 1.1689
[11/01 03:36:40 visual_prompt]: 	Test 100/123. loss: 0.676, 0.2365 s / batch. (data: 1.12e-02)max mem: 15.94594 GB 
[11/01 03:36:51 visual_prompt]: Inference (val):avg data time: 3.10e-04, avg batch time: 0.2325, average loss: 0.7093
[11/01 03:36:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 58.05	
[11/01 03:37:46 visual_prompt]: 	Test 100/323. loss: 0.819, 0.2248 s / batch. (data: 6.89e-05)max mem: 15.94594 GB 
[11/01 03:38:39 visual_prompt]: 	Test 200/323. loss: 0.658, 0.2247 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[11/01 03:39:31 visual_prompt]: 	Test 300/323. loss: 0.699, 0.2243 s / batch. (data: 7.13e-05)max mem: 15.94594 GB 
[11/01 03:39:41 visual_prompt]: Inference (test):avg data time: 1.79e-04, avg batch time: 0.2323, average loss: 0.7284
[11/01 03:39:41 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.09	rocauc: 55.01	
[11/01 03:39:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/01 03:40:47 visual_prompt]: 	Training 100/1106. train loss: 0.1300,	0.6198 s / batch. (data: 3.55e-04). ETA=17:18:42, max mem: 15.9 GB 
[11/01 03:41:51 visual_prompt]: 	Training 200/1106. train loss: 4.7240,	0.6390 s / batch. (data: 9.67e-04). ETA=17:49:45, max mem: 15.9 GB 
[11/01 03:42:55 visual_prompt]: 	Training 300/1106. train loss: 0.8072,	0.6433 s / batch. (data: 6.09e-03). ETA=17:55:56, max mem: 15.9 GB 
[11/01 03:43:59 visual_prompt]: 	Training 400/1106. train loss: 0.0181,	0.6374 s / batch. (data: 8.36e-04). ETA=17:45:00, max mem: 15.9 GB 
[11/01 03:45:02 visual_prompt]: 	Training 500/1106. train loss: 0.2103,	0.6514 s / batch. (data: 8.96e-04). ETA=18:07:13, max mem: 15.9 GB 
[11/01 03:46:06 visual_prompt]: 	Training 600/1106. train loss: 1.0765,	0.6205 s / batch. (data: 3.85e-04). ETA=17:14:40, max mem: 15.9 GB 
[11/01 03:47:10 visual_prompt]: 	Training 700/1106. train loss: 1.2016,	0.6442 s / batch. (data: 8.48e-04). ETA=17:53:07, max mem: 15.9 GB 
[11/01 03:48:13 visual_prompt]: 	Training 800/1106. train loss: 0.8502,	0.6514 s / batch. (data: 1.17e-03). ETA=18:03:58, max mem: 15.9 GB 
[11/01 03:49:17 visual_prompt]: 	Training 900/1106. train loss: 0.8624,	0.6356 s / batch. (data: 3.37e-04). ETA=17:36:35, max mem: 15.9 GB 
[11/01 03:50:21 visual_prompt]: 	Training 1000/1106. train loss: 0.1307,	0.6480 s / batch. (data: 6.20e-03). ETA=17:56:11, max mem: 15.9 GB 
[11/01 03:51:25 visual_prompt]: 	Training 1100/1106. train loss: 1.3369,	0.6185 s / batch. (data: 2.13e-04). ETA=17:06:11, max mem: 15.9 GB 
[11/01 03:51:28 visual_prompt]: Epoch 10 / 100: avg data time: 5.63e-03, avg batch time: 0.6392, average train loss: 1.2517
[11/01 03:52:23 visual_prompt]: 	Test 100/123. loss: 0.605, 0.2278 s / batch. (data: 5.77e-05)max mem: 15.94594 GB 
[11/01 03:52:34 visual_prompt]: Inference (val):avg data time: 4.00e-04, avg batch time: 0.2322, average loss: 0.7371
[11/01 03:52:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 57.36	
[11/01 03:53:29 visual_prompt]: 	Test 100/323. loss: 0.882, 0.2265 s / batch. (data: 5.13e-05)max mem: 15.94594 GB 
[11/01 03:54:21 visual_prompt]: 	Test 200/323. loss: 0.646, 0.2300 s / batch. (data: 5.96e-05)max mem: 15.94594 GB 
[11/01 03:55:13 visual_prompt]: 	Test 300/323. loss: 0.799, 0.2373 s / batch. (data: 4.46e-05)max mem: 15.94594 GB 
[11/01 03:55:24 visual_prompt]: Inference (test):avg data time: 6.94e-05, avg batch time: 0.2324, average loss: 0.7805
[11/01 03:55:24 visual_prompt]: Classification results with test_mammo-cbis: top1: 44.19	rocauc: 54.80	
[11/01 03:55:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/01 03:56:30 visual_prompt]: 	Training 100/1106. train loss: 1.1877,	0.6537 s / batch. (data: 1.11e-02). ETA=18:03:28, max mem: 15.9 GB 
[11/01 03:57:34 visual_prompt]: 	Training 200/1106. train loss: 1.0850,	0.6440 s / batch. (data: 5.51e-03). ETA=17:46:15, max mem: 15.9 GB 
[11/01 03:58:37 visual_prompt]: 	Training 300/1106. train loss: 2.6086,	0.6185 s / batch. (data: 3.63e-04). ETA=17:03:04, max mem: 15.9 GB 
[11/01 03:59:41 visual_prompt]: 	Training 400/1106. train loss: 0.9906,	0.6343 s / batch. (data: 3.64e-04). ETA=17:28:07, max mem: 15.9 GB 
[11/01 04:00:44 visual_prompt]: 	Training 500/1106. train loss: 0.7724,	0.6183 s / batch. (data: 3.35e-04). ETA=17:00:40, max mem: 15.9 GB 
[11/01 04:01:48 visual_prompt]: 	Training 600/1106. train loss: 0.6679,	0.6279 s / batch. (data: 3.28e-04). ETA=17:15:20, max mem: 15.9 GB 
[11/01 04:02:52 visual_prompt]: 	Training 700/1106. train loss: 0.5986,	0.6458 s / batch. (data: 8.54e-04). ETA=17:43:52, max mem: 15.9 GB 
[11/01 04:03:55 visual_prompt]: 	Training 800/1106. train loss: 0.5159,	0.6440 s / batch. (data: 3.38e-04). ETA=17:39:46, max mem: 15.9 GB 
[11/01 04:04:59 visual_prompt]: 	Training 900/1106. train loss: 0.6274,	0.6205 s / batch. (data: 3.37e-04). ETA=17:00:05, max mem: 15.9 GB 
[11/01 04:06:03 visual_prompt]: 	Training 1000/1106. train loss: 0.3659,	0.6319 s / batch. (data: 8.62e-04). ETA=17:17:42, max mem: 15.9 GB 
[11/01 04:07:07 visual_prompt]: 	Training 1100/1106. train loss: 0.8456,	0.6172 s / batch. (data: 3.03e-04). ETA=16:52:38, max mem: 15.9 GB 
[11/01 04:07:10 visual_prompt]: Epoch 11 / 100: avg data time: 5.13e-03, avg batch time: 0.6387, average train loss: 1.0695
[11/01 04:08:05 visual_prompt]: 	Test 100/123. loss: 0.592, 0.2244 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[11/01 04:08:16 visual_prompt]: Inference (val):avg data time: 9.99e-05, avg batch time: 0.2325, average loss: 0.6769
[11/01 04:08:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 59.86	
[11/01 04:09:11 visual_prompt]: 	Test 100/323. loss: 0.652, 0.2364 s / batch. (data: 5.34e-05)max mem: 15.94594 GB 
[11/01 04:10:04 visual_prompt]: 	Test 200/323. loss: 0.547, 0.2557 s / batch. (data: 3.70e-05)max mem: 15.94594 GB 
[11/01 04:10:56 visual_prompt]: 	Test 300/323. loss: 0.804, 0.2243 s / batch. (data: 4.55e-05)max mem: 15.94594 GB 
[11/01 04:11:07 visual_prompt]: Inference (test):avg data time: 1.56e-04, avg batch time: 0.2314, average loss: 0.6962
[11/01 04:11:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 52.87	rocauc: 55.45	
[11/01 04:11:07 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/01 04:12:13 visual_prompt]: 	Training 100/1106. train loss: 0.5302,	0.6257 s / batch. (data: 3.91e-04). ETA=17:05:24, max mem: 15.9 GB 
[11/01 04:13:17 visual_prompt]: 	Training 200/1106. train loss: 0.1812,	0.6400 s / batch. (data: 3.48e-04). ETA=17:27:50, max mem: 15.9 GB 
[11/01 04:14:21 visual_prompt]: 	Training 300/1106. train loss: 1.5607,	0.6180 s / batch. (data: 3.78e-04). ETA=16:50:45, max mem: 15.9 GB 
[11/01 04:15:24 visual_prompt]: 	Training 400/1106. train loss: 1.5483,	0.6181 s / batch. (data: 3.66e-04). ETA=16:49:50, max mem: 15.9 GB 
[11/01 04:16:28 visual_prompt]: 	Training 500/1106. train loss: 0.2639,	0.6413 s / batch. (data: 8.38e-04). ETA=17:26:43, max mem: 15.9 GB 
[11/01 04:17:32 visual_prompt]: 	Training 600/1106. train loss: 0.7598,	0.6423 s / batch. (data: 8.89e-04). ETA=17:27:23, max mem: 15.9 GB 
[11/01 04:18:35 visual_prompt]: 	Training 700/1106. train loss: 0.6748,	0.6545 s / batch. (data: 9.82e-04). ETA=17:46:09, max mem: 15.9 GB 
[11/01 04:19:39 visual_prompt]: 	Training 800/1106. train loss: 3.3182,	0.6440 s / batch. (data: 8.60e-04). ETA=17:27:54, max mem: 15.9 GB 
[11/01 04:20:43 visual_prompt]: 	Training 900/1106. train loss: 0.7047,	0.6532 s / batch. (data: 8.49e-04). ETA=17:41:45, max mem: 15.9 GB 
[11/01 04:21:47 visual_prompt]: 	Training 1000/1106. train loss: 1.4609,	0.6251 s / batch. (data: 3.59e-04). ETA=16:55:07, max mem: 15.9 GB 
[11/01 04:22:51 visual_prompt]: 	Training 1100/1106. train loss: 0.0808,	0.6190 s / batch. (data: 2.07e-04). ETA=16:44:11, max mem: 15.9 GB 
[11/01 04:22:54 visual_prompt]: Epoch 12 / 100: avg data time: 6.63e-03, avg batch time: 0.6399, average train loss: 1.2106
[11/01 04:23:49 visual_prompt]: 	Test 100/123. loss: 0.910, 0.2332 s / batch. (data: 3.48e-05)max mem: 15.94594 GB 
[11/01 04:24:00 visual_prompt]: Inference (val):avg data time: 4.69e-05, avg batch time: 0.2324, average loss: 0.8227
[11/01 04:24:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.05	
[11/01 04:24:56 visual_prompt]: 	Test 100/323. loss: 0.199, 0.2277 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[11/01 04:25:47 visual_prompt]: 	Test 200/323. loss: 0.691, 0.2355 s / batch. (data: 5.22e-05)max mem: 15.94594 GB 
[11/01 04:26:39 visual_prompt]: 	Test 300/323. loss: 1.058, 0.2374 s / batch. (data: 7.32e-05)max mem: 15.94594 GB 
[11/01 04:26:50 visual_prompt]: Inference (test):avg data time: 1.26e-04, avg batch time: 0.2319, average loss: 0.7644
[11/01 04:26:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.37	
[11/01 04:26:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/01 04:27:56 visual_prompt]: 	Training 100/1106. train loss: 0.8317,	0.6289 s / batch. (data: 8.95e-04). ETA=16:59:03, max mem: 15.9 GB 
[11/01 04:28:59 visual_prompt]: 	Training 200/1106. train loss: 2.3509,	0.6429 s / batch. (data: 5.55e-03). ETA=17:20:44, max mem: 15.9 GB 
[11/01 04:30:03 visual_prompt]: 	Training 300/1106. train loss: 3.9340,	0.6287 s / batch. (data: 3.79e-04). ETA=16:56:38, max mem: 15.9 GB 
[11/01 04:31:07 visual_prompt]: 	Training 400/1106. train loss: 0.0823,	0.6303 s / batch. (data: 8.69e-04). ETA=16:58:15, max mem: 15.9 GB 
[11/01 04:32:10 visual_prompt]: 	Training 500/1106. train loss: 0.6599,	0.6181 s / batch. (data: 3.89e-04). ETA=16:37:25, max mem: 15.9 GB 
[11/01 04:33:14 visual_prompt]: 	Training 600/1106. train loss: 0.0083,	0.6280 s / batch. (data: 3.78e-04). ETA=16:52:24, max mem: 15.9 GB 
[11/01 04:34:18 visual_prompt]: 	Training 700/1106. train loss: 0.0354,	0.6259 s / batch. (data: 1.15e-03). ETA=16:48:01, max mem: 15.9 GB 
[11/01 04:35:21 visual_prompt]: 	Training 800/1106. train loss: 0.8610,	0.6404 s / batch. (data: 8.69e-04). ETA=17:10:19, max mem: 15.9 GB 
[11/01 04:36:25 visual_prompt]: 	Training 900/1106. train loss: 0.4386,	0.6303 s / batch. (data: 8.88e-04). ETA=16:53:00, max mem: 15.9 GB 
[11/01 04:37:29 visual_prompt]: 	Training 1000/1106. train loss: 1.6888,	0.6222 s / batch. (data: 3.31e-04). ETA=16:38:58, max mem: 15.9 GB 
[11/01 04:38:33 visual_prompt]: 	Training 1100/1106. train loss: 0.8568,	0.6240 s / batch. (data: 2.08e-04). ETA=16:40:45, max mem: 15.9 GB 
[11/01 04:38:37 visual_prompt]: Epoch 13 / 100: avg data time: 4.97e-03, avg batch time: 0.6385, average train loss: 1.1027
[11/01 04:39:30 visual_prompt]: 	Test 100/123. loss: 0.711, 0.2324 s / batch. (data: 5.63e-05)max mem: 15.94594 GB 
[11/01 04:39:42 visual_prompt]: Inference (val):avg data time: 4.82e-05, avg batch time: 0.2326, average loss: 0.6842
[11/01 04:39:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.65	
[11/01 04:40:37 visual_prompt]: 	Test 100/323. loss: 0.411, 0.2277 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[11/01 04:41:30 visual_prompt]: 	Test 200/323. loss: 0.580, 0.2259 s / batch. (data: 3.41e-05)max mem: 15.94594 GB 
[11/01 04:42:21 visual_prompt]: 	Test 300/323. loss: 0.785, 0.2243 s / batch. (data: 6.96e-05)max mem: 15.94594 GB 
[11/01 04:42:32 visual_prompt]: Inference (test):avg data time: 6.85e-05, avg batch time: 0.2322, average loss: 0.6653
[11/01 04:42:32 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 58.22	
[11/01 04:42:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/01 04:43:38 visual_prompt]: 	Training 100/1106. train loss: 0.1453,	0.6301 s / batch. (data: 3.57e-04). ETA=16:49:25, max mem: 15.9 GB 
[11/01 04:44:42 visual_prompt]: 	Training 200/1106. train loss: 0.6587,	0.6374 s / batch. (data: 8.72e-04). ETA=17:00:05, max mem: 15.9 GB 
[11/01 04:45:46 visual_prompt]: 	Training 300/1106. train loss: 0.0352,	0.6320 s / batch. (data: 3.37e-04). ETA=16:50:24, max mem: 15.9 GB 
[11/01 04:46:50 visual_prompt]: 	Training 400/1106. train loss: 0.7780,	0.6200 s / batch. (data: 3.63e-04). ETA=16:30:05, max mem: 15.9 GB 
[11/01 04:47:53 visual_prompt]: 	Training 500/1106. train loss: 0.4603,	0.6320 s / batch. (data: 3.51e-04). ETA=16:48:17, max mem: 15.9 GB 
[11/01 04:48:57 visual_prompt]: 	Training 600/1106. train loss: 3.3195,	0.6436 s / batch. (data: 1.11e-02). ETA=17:05:39, max mem: 15.9 GB 
[11/01 04:50:01 visual_prompt]: 	Training 700/1106. train loss: 0.8985,	0.6473 s / batch. (data: 8.66e-04). ETA=17:10:27, max mem: 15.9 GB 
[11/01 04:51:05 visual_prompt]: 	Training 800/1106. train loss: 0.7373,	0.6191 s / batch. (data: 3.88e-04). ETA=16:24:39, max mem: 15.9 GB 
[11/01 04:52:08 visual_prompt]: 	Training 900/1106. train loss: 1.9749,	0.6297 s / batch. (data: 2.99e-04). ETA=16:40:20, max mem: 15.9 GB 
[11/01 04:53:12 visual_prompt]: 	Training 1000/1106. train loss: 0.6269,	0.6673 s / batch. (data: 6.02e-03). ETA=17:39:05, max mem: 15.9 GB 
[11/01 04:54:16 visual_prompt]: 	Training 1100/1106. train loss: 0.1959,	0.6313 s / batch. (data: 1.91e-04). ETA=16:40:51, max mem: 15.9 GB 
[11/01 04:54:19 visual_prompt]: Epoch 14 / 100: avg data time: 5.91e-03, avg batch time: 0.6390, average train loss: 1.1072
[11/01 04:55:13 visual_prompt]: 	Test 100/123. loss: 0.912, 0.2255 s / batch. (data: 6.63e-05)max mem: 15.94594 GB 
[11/01 04:55:25 visual_prompt]: Inference (val):avg data time: 2.59e-04, avg batch time: 0.2343, average loss: 0.8770
[11/01 04:55:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.75	
[11/01 04:56:20 visual_prompt]: 	Test 100/323. loss: 0.155, 0.2477 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[11/01 04:57:13 visual_prompt]: 	Test 200/323. loss: 0.683, 0.2249 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[11/01 04:58:04 visual_prompt]: 	Test 300/323. loss: 1.235, 0.2357 s / batch. (data: 3.81e-05)max mem: 15.94594 GB 
[11/01 04:58:15 visual_prompt]: Inference (test):avg data time: 1.04e-04, avg batch time: 0.2320, average loss: 0.8234
[11/01 04:58:15 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.53	rocauc: 56.34	
[11/01 04:58:15 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/01 04:59:22 visual_prompt]: 	Training 100/1106. train loss: 0.8660,	0.6452 s / batch. (data: 7.97e-03). ETA=17:01:48, max mem: 15.9 GB 
[11/01 05:00:26 visual_prompt]: 	Training 200/1106. train loss: 1.0882,	0.6440 s / batch. (data: 8.72e-04). ETA=16:58:45, max mem: 15.9 GB 
[11/01 05:01:29 visual_prompt]: 	Training 300/1106. train loss: 3.2846,	0.6440 s / batch. (data: 3.35e-04). ETA=16:57:41, max mem: 15.9 GB 
[11/01 05:02:33 visual_prompt]: 	Training 400/1106. train loss: 1.1226,	0.6345 s / batch. (data: 3.64e-04). ETA=16:41:33, max mem: 15.9 GB 
[11/01 05:03:37 visual_prompt]: 	Training 500/1106. train loss: 0.0288,	0.6605 s / batch. (data: 1.11e-02). ETA=17:21:30, max mem: 15.9 GB 
[11/01 05:04:41 visual_prompt]: 	Training 600/1106. train loss: 5.3319,	0.6358 s / batch. (data: 3.87e-04). ETA=16:41:32, max mem: 15.9 GB 
[11/01 05:05:45 visual_prompt]: 	Training 700/1106. train loss: 0.9493,	0.6480 s / batch. (data: 1.19e-02). ETA=16:59:41, max mem: 15.9 GB 
[11/01 05:06:49 visual_prompt]: 	Training 800/1106. train loss: 1.0789,	0.6314 s / batch. (data: 3.78e-04). ETA=16:32:26, max mem: 15.9 GB 
[11/01 05:07:53 visual_prompt]: 	Training 900/1106. train loss: 0.8977,	0.6280 s / batch. (data: 3.81e-04). ETA=16:26:05, max mem: 15.9 GB 
[11/01 05:08:57 visual_prompt]: 	Training 1000/1106. train loss: 0.7727,	0.6348 s / batch. (data: 3.49e-04). ETA=16:35:42, max mem: 15.9 GB 
[11/01 05:10:00 visual_prompt]: 	Training 1100/1106. train loss: 0.1282,	0.6179 s / batch. (data: 2.06e-04). ETA=16:08:15, max mem: 15.9 GB 
[11/01 05:10:04 visual_prompt]: Epoch 15 / 100: avg data time: 6.12e-03, avg batch time: 0.6406, average train loss: 0.9834
[11/01 05:10:59 visual_prompt]: 	Test 100/123. loss: 0.674, 0.2285 s / batch. (data: 5.29e-05)max mem: 15.94594 GB 
[11/01 05:11:10 visual_prompt]: Inference (val):avg data time: 1.37e-04, avg batch time: 0.2326, average loss: 0.6940
[11/01 05:11:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.69	
[11/01 05:12:05 visual_prompt]: 	Test 100/323. loss: 0.380, 0.2276 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[11/01 05:12:58 visual_prompt]: 	Test 200/323. loss: 0.441, 0.2337 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 05:13:49 visual_prompt]: 	Test 300/323. loss: 0.916, 0.2248 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[11/01 05:14:00 visual_prompt]: Inference (test):avg data time: 1.57e-04, avg batch time: 0.2322, average loss: 0.6757
[11/01 05:14:00 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.09	rocauc: 59.50	
[11/01 05:14:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/01 05:15:06 visual_prompt]: 	Training 100/1106. train loss: 0.7923,	0.6386 s / batch. (data: 8.79e-04). ETA=16:39:33, max mem: 15.9 GB 
[11/01 05:16:10 visual_prompt]: 	Training 200/1106. train loss: 0.8776,	0.6265 s / batch. (data: 9.62e-03). ETA=16:19:36, max mem: 15.9 GB 
[11/01 05:17:13 visual_prompt]: 	Training 300/1106. train loss: 1.0265,	0.6280 s / batch. (data: 8.73e-04). ETA=16:20:48, max mem: 15.9 GB 
[11/01 05:18:17 visual_prompt]: 	Training 400/1106. train loss: 0.9323,	0.6405 s / batch. (data: 3.80e-04). ETA=16:39:17, max mem: 15.9 GB 
[11/01 05:19:20 visual_prompt]: 	Training 500/1106. train loss: 1.3947,	0.6186 s / batch. (data: 3.48e-04). ETA=16:04:08, max mem: 15.9 GB 
[11/01 05:20:24 visual_prompt]: 	Training 600/1106. train loss: 0.0820,	0.6358 s / batch. (data: 1.26e-02). ETA=16:29:46, max mem: 15.9 GB 
[11/01 05:21:28 visual_prompt]: 	Training 700/1106. train loss: 1.1469,	0.6473 s / batch. (data: 5.52e-03). ETA=16:46:41, max mem: 15.9 GB 
[11/01 05:22:32 visual_prompt]: 	Training 800/1106. train loss: 0.4607,	0.6387 s / batch. (data: 8.10e-04). ETA=16:32:12, max mem: 15.9 GB 
[11/01 05:23:35 visual_prompt]: 	Training 900/1106. train loss: 0.8911,	0.6330 s / batch. (data: 1.26e-02). ETA=16:22:14, max mem: 15.9 GB 
[11/01 05:24:39 visual_prompt]: 	Training 1000/1106. train loss: 0.6890,	0.6328 s / batch. (data: 1.06e-02). ETA=16:20:58, max mem: 15.9 GB 
[11/01 05:25:43 visual_prompt]: 	Training 1100/1106. train loss: 0.0503,	0.6172 s / batch. (data: 1.64e-04). ETA=15:55:40, max mem: 15.9 GB 
[11/01 05:25:47 visual_prompt]: Epoch 16 / 100: avg data time: 5.76e-03, avg batch time: 0.6386, average train loss: 1.0179
[11/01 05:26:41 visual_prompt]: 	Test 100/123. loss: 1.386, 0.2280 s / batch. (data: 6.53e-05)max mem: 15.94594 GB 
[11/01 05:26:53 visual_prompt]: Inference (val):avg data time: 4.88e-05, avg batch time: 0.2339, average loss: 1.3066
[11/01 05:26:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.03	
[11/01 05:27:47 visual_prompt]: 	Test 100/323. loss: 0.046, 0.2449 s / batch. (data: 7.15e-05)max mem: 15.94594 GB 
[11/01 05:28:40 visual_prompt]: 	Test 200/323. loss: 1.181, 0.2397 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[11/01 05:29:31 visual_prompt]: 	Test 300/323. loss: 1.759, 0.2372 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[11/01 05:29:43 visual_prompt]: Inference (test):avg data time: 1.64e-04, avg batch time: 0.2325, average loss: 1.1965
[11/01 05:29:43 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.54	
[11/01 05:29:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/01 05:30:48 visual_prompt]: 	Training 100/1106. train loss: 0.7522,	0.6470 s / batch. (data: 5.53e-03). ETA=16:40:45, max mem: 15.9 GB 
[11/01 05:31:52 visual_prompt]: 	Training 200/1106. train loss: 0.9018,	0.6299 s / batch. (data: 3.44e-04). ETA=16:13:13, max mem: 15.9 GB 
[11/01 05:32:56 visual_prompt]: 	Training 300/1106. train loss: 1.3429,	0.6355 s / batch. (data: 8.70e-04). ETA=16:20:46, max mem: 15.9 GB 
[11/01 05:33:59 visual_prompt]: 	Training 400/1106. train loss: 0.9057,	0.6294 s / batch. (data: 3.80e-04). ETA=16:10:25, max mem: 15.9 GB 
[11/01 05:35:03 visual_prompt]: 	Training 500/1106. train loss: 1.2286,	0.6371 s / batch. (data: 9.01e-04). ETA=16:21:07, max mem: 15.9 GB 
[11/01 05:36:07 visual_prompt]: 	Training 600/1106. train loss: 0.6347,	0.6177 s / batch. (data: 3.70e-04). ETA=15:50:17, max mem: 15.9 GB 
[11/01 05:37:10 visual_prompt]: 	Training 700/1106. train loss: 0.3849,	0.6422 s / batch. (data: 6.22e-03). ETA=16:26:55, max mem: 15.9 GB 
[11/01 05:38:14 visual_prompt]: 	Training 800/1106. train loss: 0.1373,	0.6474 s / batch. (data: 8.72e-04). ETA=16:33:47, max mem: 15.9 GB 
[11/01 05:39:18 visual_prompt]: 	Training 900/1106. train loss: 1.3145,	0.6295 s / batch. (data: 3.59e-04). ETA=16:05:12, max mem: 15.9 GB 
[11/01 05:40:21 visual_prompt]: 	Training 1000/1106. train loss: 1.1329,	0.6185 s / batch. (data: 3.58e-04). ETA=15:47:25, max mem: 15.9 GB 
[11/01 05:41:25 visual_prompt]: 	Training 1100/1106. train loss: 0.9060,	0.6186 s / batch. (data: 1.86e-04). ETA=15:46:33, max mem: 15.9 GB 
[11/01 05:41:29 visual_prompt]: Epoch 17 / 100: avg data time: 5.43e-03, avg batch time: 0.6385, average train loss: 1.0701
[11/01 05:42:23 visual_prompt]: 	Test 100/123. loss: 0.841, 0.2257 s / batch. (data: 5.27e-05)max mem: 15.94594 GB 
[11/01 05:42:35 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.2327, average loss: 0.7950
[11/01 05:42:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.65	
[11/01 05:43:30 visual_prompt]: 	Test 100/323. loss: 0.261, 0.2248 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[11/01 05:44:22 visual_prompt]: 	Test 200/323. loss: 0.600, 0.2242 s / batch. (data: 3.48e-05)max mem: 15.94594 GB 
[11/01 05:45:14 visual_prompt]: 	Test 300/323. loss: 0.982, 0.2487 s / batch. (data: 6.10e-05)max mem: 15.94594 GB 
[11/01 05:45:25 visual_prompt]: Inference (test):avg data time: 1.21e-04, avg batch time: 0.2316, average loss: 0.7393
[11/01 05:45:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.90	
[11/01 05:45:25 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/01 05:46:31 visual_prompt]: 	Training 100/1106. train loss: 0.4569,	0.6386 s / batch. (data: 3.44e-04). ETA=16:16:02, max mem: 15.9 GB 
[11/01 05:47:35 visual_prompt]: 	Training 200/1106. train loss: 0.6436,	0.6747 s / batch. (data: 3.47e-02). ETA=17:10:05, max mem: 15.9 GB 
[11/01 05:48:38 visual_prompt]: 	Training 300/1106. train loss: 0.3293,	0.6316 s / batch. (data: 8.39e-04). ETA=16:03:07, max mem: 15.9 GB 
[11/01 05:49:42 visual_prompt]: 	Training 400/1106. train loss: 1.0619,	0.6600 s / batch. (data: 2.58e-02). ETA=16:45:23, max mem: 15.9 GB 
[11/01 05:50:46 visual_prompt]: 	Training 500/1106. train loss: 0.0869,	0.6549 s / batch. (data: 8.83e-04). ETA=16:36:29, max mem: 15.9 GB 
[11/01 05:51:49 visual_prompt]: 	Training 600/1106. train loss: 0.0380,	0.6535 s / batch. (data: 8.74e-04). ETA=16:33:21, max mem: 15.9 GB 
[11/01 05:52:53 visual_prompt]: 	Training 700/1106. train loss: 3.8178,	0.6183 s / batch. (data: 3.80e-04). ETA=15:38:42, max mem: 15.9 GB 
[11/01 05:53:57 visual_prompt]: 	Training 800/1106. train loss: 0.7150,	0.6404 s / batch. (data: 3.75e-04). ETA=16:11:12, max mem: 15.9 GB 
[11/01 05:55:01 visual_prompt]: 	Training 900/1106. train loss: 0.8559,	0.6292 s / batch. (data: 3.24e-04). ETA=15:53:17, max mem: 15.9 GB 
[11/01 05:56:04 visual_prompt]: 	Training 1000/1106. train loss: 0.0617,	0.6644 s / batch. (data: 1.06e-02). ETA=16:45:28, max mem: 15.9 GB 
[11/01 05:57:08 visual_prompt]: 	Training 1100/1106. train loss: 0.3049,	0.6282 s / batch. (data: 2.17e-04). ETA=15:49:34, max mem: 15.9 GB 
[11/01 05:57:12 visual_prompt]: Epoch 18 / 100: avg data time: 6.14e-03, avg batch time: 0.6395, average train loss: 0.9879
[11/01 05:58:06 visual_prompt]: 	Test 100/123. loss: 0.704, 0.2253 s / batch. (data: 5.75e-05)max mem: 15.94594 GB 
[11/01 05:58:18 visual_prompt]: Inference (val):avg data time: 2.33e-04, avg batch time: 0.2329, average loss: 0.6817
[11/01 05:58:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 65.65	
[11/01 05:59:13 visual_prompt]: 	Test 100/323. loss: 0.322, 0.2274 s / batch. (data: 5.27e-05)max mem: 15.94594 GB 
[11/01 06:00:05 visual_prompt]: 	Test 200/323. loss: 0.370, 0.2394 s / batch. (data: 3.74e-05)max mem: 15.94594 GB 
[11/01 06:00:57 visual_prompt]: 	Test 300/323. loss: 0.893, 0.2421 s / batch. (data: 4.72e-05)max mem: 15.94594 GB 
[11/01 06:01:08 visual_prompt]: Inference (test):avg data time: 1.23e-04, avg batch time: 0.2315, average loss: 0.6644
[11/01 06:01:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 62.10	
[11/01 06:01:08 visual_prompt]: Stopping early.
[11/01 06:01:08 visual_prompt]: Rank of current process: 0. World size: 1
[11/01 06:01:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/01 06:01:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/01 06:01:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/01 06:01:08 visual_prompt]: Training with config:
[11/01 06:01:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/test/seed4536/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 4536, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/01 06:01:08 visual_prompt]: Loading training data...
[11/01 06:01:08 visual_prompt]: Constructing mammo-cbis dataset train...
[11/01 06:01:08 visual_prompt]: Loading validation data...
[11/01 06:01:08 visual_prompt]: Constructing mammo-cbis dataset val...
[11/01 06:01:08 visual_prompt]: Loading test data...
[11/01 06:01:08 visual_prompt]: Constructing mammo-cbis dataset test...
[11/01 06:01:08 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[11/01 06:01:11 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[11/01 06:01:11 visual_prompt]: tuned percent:0.522
[11/01 06:01:11 visual_prompt]: Device used for model: 0
[11/01 06:01:11 visual_prompt]: Setting up Evaluator...
[11/01 06:01:11 visual_prompt]: Setting up Trainer...
[11/01 06:01:11 visual_prompt]: 	Setting up the optimizer...
[11/01 06:01:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/01 06:02:17 visual_prompt]: 	Training 100/1106. train loss: 1.3533,	0.6320 s / batch. (data: 3.35e-04). ETA=19:23:56, max mem: 15.9 GB 
[11/01 06:03:20 visual_prompt]: 	Training 200/1106. train loss: 1.8889,	0.6320 s / batch. (data: 3.80e-04). ETA=19:22:53, max mem: 15.9 GB 
[11/01 06:04:24 visual_prompt]: 	Training 300/1106. train loss: 0.0410,	0.6280 s / batch. (data: 3.34e-04). ETA=19:14:30, max mem: 15.9 GB 
[11/01 06:05:28 visual_prompt]: 	Training 400/1106. train loss: 2.2141,	0.6276 s / batch. (data: 3.88e-04). ETA=19:12:36, max mem: 15.9 GB 
[11/01 06:06:32 visual_prompt]: 	Training 500/1106. train loss: 1.5464,	0.6239 s / batch. (data: 3.55e-04). ETA=19:04:53, max mem: 15.9 GB 
[11/01 06:07:35 visual_prompt]: 	Training 600/1106. train loss: 0.0292,	0.6246 s / batch. (data: 3.53e-04). ETA=19:05:10, max mem: 15.9 GB 
[11/01 06:08:39 visual_prompt]: 	Training 700/1106. train loss: 1.5244,	0.6436 s / batch. (data: 1.58e-02). ETA=19:38:56, max mem: 15.9 GB 
[11/01 06:09:43 visual_prompt]: 	Training 800/1106. train loss: 1.2395,	0.6359 s / batch. (data: 1.06e-02). ETA=19:23:39, max mem: 15.9 GB 
[11/01 06:10:46 visual_prompt]: 	Training 900/1106. train loss: 0.0773,	0.6299 s / batch. (data: 3.64e-04). ETA=19:11:34, max mem: 15.9 GB 
[11/01 06:11:50 visual_prompt]: 	Training 1000/1106. train loss: 0.0560,	0.6415 s / batch. (data: 8.42e-04). ETA=19:31:51, max mem: 15.9 GB 
[11/01 06:12:54 visual_prompt]: 	Training 1100/1106. train loss: 0.0996,	0.6176 s / batch. (data: 1.48e-04). ETA=18:47:02, max mem: 15.9 GB 
[11/01 06:12:58 visual_prompt]: Epoch 1 / 100: avg data time: 5.47e-03, avg batch time: 0.6386, average train loss: 1.4695
[11/01 06:13:51 visual_prompt]: 	Test 100/123. loss: 1.192, 0.2311 s / batch. (data: 4.63e-05)max mem: 15.94594 GB 
[11/01 06:14:03 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.2319, average loss: 1.4142
[11/01 06:14:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.14	
[11/01 06:14:58 visual_prompt]: 	Test 100/323. loss: 0.043, 0.2245 s / batch. (data: 3.31e-05)max mem: 15.94594 GB 
[11/01 06:15:51 visual_prompt]: 	Test 200/323. loss: 1.498, 0.2317 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[11/01 06:16:43 visual_prompt]: 	Test 300/323. loss: 1.414, 0.2356 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/01 06:16:54 visual_prompt]: Inference (test):avg data time: 1.01e-04, avg batch time: 0.2325, average loss: 1.3005
[11/01 06:16:54 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 48.01	
[11/01 06:16:54 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/01 06:18:00 visual_prompt]: 	Training 100/1106. train loss: 1.5015,	0.6306 s / batch. (data: 3.77e-04). ETA=19:09:42, max mem: 15.9 GB 
[11/01 06:19:04 visual_prompt]: 	Training 200/1106. train loss: 1.4629,	0.6174 s / batch. (data: 3.80e-04). ETA=18:44:38, max mem: 15.9 GB 
[11/01 06:20:08 visual_prompt]: 	Training 300/1106. train loss: 1.1676,	0.6297 s / batch. (data: 1.02e-02). ETA=19:05:59, max mem: 15.9 GB 
[11/01 06:21:11 visual_prompt]: 	Training 400/1106. train loss: 0.6736,	0.6459 s / batch. (data: 1.62e-02). ETA=19:34:27, max mem: 15.9 GB 
[11/01 06:22:15 visual_prompt]: 	Training 500/1106. train loss: 0.7876,	0.6473 s / batch. (data: 8.82e-04). ETA=19:35:49, max mem: 15.9 GB 
[11/01 06:23:20 visual_prompt]: 	Training 600/1106. train loss: 0.3917,	0.6447 s / batch. (data: 8.62e-04). ETA=19:30:02, max mem: 15.9 GB 
[11/01 06:24:23 visual_prompt]: 	Training 700/1106. train loss: 0.7538,	0.6200 s / batch. (data: 4.81e-04). ETA=18:44:08, max mem: 15.9 GB 
[11/01 06:25:27 visual_prompt]: 	Training 800/1106. train loss: 0.8141,	0.6320 s / batch. (data: 4.52e-04). ETA=19:04:56, max mem: 15.9 GB 
[11/01 06:26:31 visual_prompt]: 	Training 900/1106. train loss: 1.2860,	0.6195 s / batch. (data: 5.44e-04). ETA=18:41:10, max mem: 15.9 GB 
[11/01 06:27:35 visual_prompt]: 	Training 1000/1106. train loss: 0.7351,	0.6455 s / batch. (data: 9.38e-04). ETA=19:27:08, max mem: 15.9 GB 
[11/01 06:28:38 visual_prompt]: 	Training 1100/1106. train loss: 0.3728,	0.6192 s / batch. (data: 1.44e-04). ETA=18:38:39, max mem: 15.9 GB 
[11/01 06:28:42 visual_prompt]: Epoch 2 / 100: avg data time: 6.80e-03, avg batch time: 0.6401, average train loss: 0.8978
[11/01 06:29:36 visual_prompt]: 	Test 100/123. loss: 0.744, 0.2326 s / batch. (data: 5.79e-05)max mem: 15.94594 GB 
[11/01 06:29:48 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.2325, average loss: 0.7265
[11/01 06:29:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.28	
[11/01 06:30:42 visual_prompt]: 	Test 100/323. loss: 0.338, 0.2293 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[11/01 06:31:35 visual_prompt]: 	Test 200/323. loss: 0.692, 0.2283 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[11/01 06:32:27 visual_prompt]: 	Test 300/323. loss: 0.774, 0.2238 s / batch. (data: 7.75e-05)max mem: 15.94594 GB 
[11/01 06:32:38 visual_prompt]: Inference (test):avg data time: 7.18e-05, avg batch time: 0.2320, average loss: 0.6980
[11/01 06:32:38 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 51.44	
[11/01 06:32:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/01 06:33:43 visual_prompt]: 	Training 100/1106. train loss: 0.8744,	0.6319 s / batch. (data: 9.08e-04). ETA=19:00:25, max mem: 15.9 GB 
[11/01 06:34:47 visual_prompt]: 	Training 200/1106. train loss: 1.3748,	0.6341 s / batch. (data: 8.76e-04). ETA=19:03:21, max mem: 15.9 GB 
[11/01 06:35:51 visual_prompt]: 	Training 300/1106. train loss: 0.7408,	0.6381 s / batch. (data: 1.57e-02). ETA=19:09:33, max mem: 15.9 GB 
[11/01 06:36:54 visual_prompt]: 	Training 400/1106. train loss: 0.0694,	0.6195 s / batch. (data: 3.64e-04). ETA=18:35:00, max mem: 15.9 GB 
[11/01 06:37:58 visual_prompt]: 	Training 500/1106. train loss: 0.7999,	0.6400 s / batch. (data: 3.80e-04). ETA=19:10:47, max mem: 15.9 GB 
[11/01 06:39:02 visual_prompt]: 	Training 600/1106. train loss: 0.9381,	0.6328 s / batch. (data: 4.25e-04). ETA=18:56:51, max mem: 15.9 GB 
[11/01 06:40:05 visual_prompt]: 	Training 700/1106. train loss: 2.9015,	0.6299 s / batch. (data: 8.46e-04). ETA=18:50:35, max mem: 15.9 GB 
[11/01 06:41:09 visual_prompt]: 	Training 800/1106. train loss: 0.9511,	0.6338 s / batch. (data: 1.06e-03). ETA=18:56:24, max mem: 15.9 GB 
[11/01 06:42:13 visual_prompt]: 	Training 900/1106. train loss: 1.5429,	0.6578 s / batch. (data: 1.35e-02). ETA=19:38:25, max mem: 15.9 GB 
[11/01 06:43:16 visual_prompt]: 	Training 1000/1106. train loss: 1.5090,	0.6260 s / batch. (data: 3.67e-04). ETA=18:40:30, max mem: 15.9 GB 
[11/01 06:44:20 visual_prompt]: 	Training 1100/1106. train loss: 2.3635,	0.6182 s / batch. (data: 2.22e-04). ETA=18:25:30, max mem: 15.9 GB 
[11/01 06:44:24 visual_prompt]: Epoch 3 / 100: avg data time: 5.87e-03, avg batch time: 0.6385, average train loss: 0.9093
[11/01 06:45:18 visual_prompt]: 	Test 100/123. loss: 0.800, 0.2373 s / batch. (data: 5.53e-05)max mem: 15.94594 GB 
[11/01 06:45:30 visual_prompt]: Inference (val):avg data time: 4.93e-05, avg batch time: 0.2334, average loss: 0.7652
[11/01 06:45:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.28	
[11/01 06:46:24 visual_prompt]: 	Test 100/323. loss: 0.256, 0.2437 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 06:47:17 visual_prompt]: 	Test 200/323. loss: 0.711, 0.2547 s / batch. (data: 5.44e-05)max mem: 15.94594 GB 
[11/01 06:48:09 visual_prompt]: 	Test 300/323. loss: 0.857, 0.2404 s / batch. (data: 5.22e-05)max mem: 15.94594 GB 
[11/01 06:48:20 visual_prompt]: Inference (test):avg data time: 7.49e-05, avg batch time: 0.2317, average loss: 0.7246
[11/01 06:48:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 54.07	
[11/01 06:48:20 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/01 06:49:27 visual_prompt]: 	Training 100/1106. train loss: 0.1952,	0.6270 s / batch. (data: 1.06e-02). ETA=18:40:00, max mem: 15.9 GB 
[11/01 06:50:30 visual_prompt]: 	Training 200/1106. train loss: 1.5260,	0.6317 s / batch. (data: 8.77e-04). ETA=18:47:27, max mem: 15.9 GB 
[11/01 06:51:34 visual_prompt]: 	Training 300/1106. train loss: 1.2308,	0.6472 s / batch. (data: 1.66e-02). ETA=19:14:02, max mem: 15.9 GB 
[11/01 06:52:38 visual_prompt]: 	Training 400/1106. train loss: 3.0167,	0.6279 s / batch. (data: 3.45e-04). ETA=18:38:29, max mem: 15.9 GB 
[11/01 06:53:41 visual_prompt]: 	Training 500/1106. train loss: 1.1129,	0.6481 s / batch. (data: 2.81e-02). ETA=19:13:26, max mem: 15.9 GB 
[11/01 06:54:45 visual_prompt]: 	Training 600/1106. train loss: 2.2696,	0.6386 s / batch. (data: 8.72e-04). ETA=18:55:30, max mem: 15.9 GB 
[11/01 06:55:49 visual_prompt]: 	Training 700/1106. train loss: 1.0155,	0.6306 s / batch. (data: 3.74e-04). ETA=18:40:15, max mem: 15.9 GB 
[11/01 06:56:53 visual_prompt]: 	Training 800/1106. train loss: 0.9689,	0.6245 s / batch. (data: 5.53e-03). ETA=18:28:17, max mem: 15.9 GB 
[11/01 06:57:56 visual_prompt]: 	Training 900/1106. train loss: 0.7878,	0.6320 s / batch. (data: 4.52e-04). ETA=18:40:30, max mem: 15.9 GB 
[11/01 06:59:00 visual_prompt]: 	Training 1000/1106. train loss: 1.7464,	0.6325 s / batch. (data: 1.27e-02). ETA=18:40:28, max mem: 15.9 GB 
[11/01 07:00:04 visual_prompt]: 	Training 1100/1106. train loss: 0.8051,	0.6190 s / batch. (data: 1.65e-04). ETA=18:15:27, max mem: 15.9 GB 
[11/01 07:00:07 visual_prompt]: Epoch 4 / 100: avg data time: 6.08e-03, avg batch time: 0.6396, average train loss: 0.9226
[11/01 07:01:02 visual_prompt]: 	Test 100/123. loss: 0.846, 0.2366 s / batch. (data: 5.70e-05)max mem: 15.94594 GB 
[11/01 07:01:13 visual_prompt]: Inference (val):avg data time: 8.34e-05, avg batch time: 0.2321, average loss: 0.7943
[11/01 07:01:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.59	
[11/01 07:02:08 visual_prompt]: 	Test 100/323. loss: 0.254, 0.2247 s / batch. (data: 7.01e-05)max mem: 15.94594 GB 
[11/01 07:03:01 visual_prompt]: 	Test 200/323. loss: 0.764, 0.2248 s / batch. (data: 5.89e-05)max mem: 15.94594 GB 
[11/01 07:03:53 visual_prompt]: 	Test 300/323. loss: 0.868, 0.2242 s / batch. (data: 7.30e-05)max mem: 15.94594 GB 
[11/01 07:04:04 visual_prompt]: Inference (test):avg data time: 9.71e-05, avg batch time: 0.2324, average loss: 0.7471
[11/01 07:04:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 53.60	
[11/01 07:04:04 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/01 07:05:10 visual_prompt]: 	Training 100/1106. train loss: 0.1414,	0.6418 s / batch. (data: 1.06e-02). ETA=18:54:41, max mem: 15.9 GB 
[11/01 07:06:14 visual_prompt]: 	Training 200/1106. train loss: 0.6849,	0.6570 s / batch. (data: 1.12e-02). ETA=19:20:23, max mem: 15.9 GB 
[11/01 07:07:17 visual_prompt]: 	Training 300/1106. train loss: 0.6343,	0.6379 s / batch. (data: 6.00e-03). ETA=18:45:33, max mem: 15.9 GB 
[11/01 07:08:21 visual_prompt]: 	Training 400/1106. train loss: 0.0091,	0.6357 s / batch. (data: 9.08e-04). ETA=18:40:38, max mem: 15.9 GB 
[11/01 07:09:25 visual_prompt]: 	Training 500/1106. train loss: 0.7159,	0.6297 s / batch. (data: 1.20e-02). ETA=18:29:02, max mem: 15.9 GB 
[11/01 07:10:29 visual_prompt]: 	Training 600/1106. train loss: 0.5822,	0.6337 s / batch. (data: 3.84e-04). ETA=18:35:05, max mem: 15.9 GB 
[11/01 07:11:32 visual_prompt]: 	Training 700/1106. train loss: 4.0012,	0.6280 s / batch. (data: 5.50e-03). ETA=18:23:58, max mem: 15.9 GB 
[11/01 07:12:36 visual_prompt]: 	Training 800/1106. train loss: 1.0067,	0.6186 s / batch. (data: 3.20e-04). ETA=18:06:22, max mem: 15.9 GB 
[11/01 07:13:39 visual_prompt]: 	Training 900/1106. train loss: 1.7102,	0.6395 s / batch. (data: 8.68e-04). ETA=18:42:03, max mem: 15.9 GB 
[11/01 07:14:43 visual_prompt]: 	Training 1000/1106. train loss: 1.0161,	0.6480 s / batch. (data: 5.90e-04). ETA=18:55:55, max mem: 15.9 GB 
[11/01 07:15:47 visual_prompt]: 	Training 1100/1106. train loss: 0.9221,	0.6176 s / batch. (data: 1.73e-04). ETA=18:01:35, max mem: 15.9 GB 
[11/01 07:15:51 visual_prompt]: Epoch 5 / 100: avg data time: 6.25e-03, avg batch time: 0.6391, average train loss: 0.9724
[11/01 07:16:45 visual_prompt]: 	Test 100/123. loss: 1.093, 0.2246 s / batch. (data: 5.94e-05)max mem: 15.94594 GB 
[11/01 07:16:56 visual_prompt]: Inference (val):avg data time: 4.94e-05, avg batch time: 0.2324, average loss: 0.9940
[11/01 07:16:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.68	
[11/01 07:17:51 visual_prompt]: 	Test 100/323. loss: 0.111, 0.2238 s / batch. (data: 6.60e-05)max mem: 15.94594 GB 
[11/01 07:18:43 visual_prompt]: 	Test 200/323. loss: 0.866, 0.2352 s / batch. (data: 5.20e-05)max mem: 15.94594 GB 
[11/01 07:19:35 visual_prompt]: 	Test 300/323. loss: 1.210, 0.2242 s / batch. (data: 4.91e-05)max mem: 15.94594 GB 
[11/01 07:19:46 visual_prompt]: Inference (test):avg data time: 8.00e-05, avg batch time: 0.2330, average loss: 0.9168
[11/01 07:19:46 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.83	
[11/01 07:19:46 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/01 07:20:53 visual_prompt]: 	Training 100/1106. train loss: 0.8124,	0.6320 s / batch. (data: 3.59e-04). ETA=18:25:40, max mem: 15.9 GB 
[11/01 07:21:57 visual_prompt]: 	Training 200/1106. train loss: 0.9987,	0.6480 s / batch. (data: 4.04e-04). ETA=18:52:33, max mem: 15.9 GB 
[11/01 07:23:00 visual_prompt]: 	Training 300/1106. train loss: 0.1869,	0.6301 s / batch. (data: 3.34e-04). ETA=18:20:13, max mem: 15.9 GB 
[11/01 07:24:04 visual_prompt]: 	Training 400/1106. train loss: 4.6666,	0.6296 s / batch. (data: 1.20e-02). ETA=18:18:20, max mem: 15.9 GB 
[11/01 07:25:08 visual_prompt]: 	Training 500/1106. train loss: 1.0418,	0.6379 s / batch. (data: 5.80e-04). ETA=18:31:40, max mem: 15.9 GB 
[11/01 07:26:11 visual_prompt]: 	Training 600/1106. train loss: 0.5993,	0.6410 s / batch. (data: 3.30e-04). ETA=18:36:02, max mem: 15.9 GB 
[11/01 07:27:15 visual_prompt]: 	Training 700/1106. train loss: 0.0745,	0.6320 s / batch. (data: 3.57e-04). ETA=18:19:22, max mem: 15.9 GB 
[11/01 07:28:19 visual_prompt]: 	Training 800/1106. train loss: 1.9173,	0.6525 s / batch. (data: 1.25e-02). ETA=18:54:01, max mem: 15.9 GB 
[11/01 07:29:22 visual_prompt]: 	Training 900/1106. train loss: 0.7656,	0.6278 s / batch. (data: 3.69e-04). ETA=18:10:03, max mem: 15.9 GB 
[11/01 07:30:26 visual_prompt]: 	Training 1000/1106. train loss: 1.9094,	0.6447 s / batch. (data: 1.61e-02). ETA=18:38:12, max mem: 15.9 GB 
[11/01 07:31:30 visual_prompt]: 	Training 1100/1106. train loss: 0.0444,	0.6185 s / batch. (data: 1.73e-04). ETA=17:51:44, max mem: 15.9 GB 
[11/01 07:31:34 visual_prompt]: Epoch 6 / 100: avg data time: 6.83e-03, avg batch time: 0.6395, average train loss: 1.0270
[11/01 07:32:28 visual_prompt]: 	Test 100/123. loss: 0.821, 0.2249 s / batch. (data: 1.12e-04)max mem: 15.94594 GB 
[11/01 07:32:40 visual_prompt]: Inference (val):avg data time: 4.86e-05, avg batch time: 0.2323, average loss: 0.7920
[11/01 07:32:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.88	
[11/01 07:33:35 visual_prompt]: 	Test 100/323. loss: 0.243, 0.2408 s / batch. (data: 6.63e-05)max mem: 15.94594 GB 
[11/01 07:34:27 visual_prompt]: 	Test 200/323. loss: 0.704, 0.2320 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[11/01 07:35:19 visual_prompt]: 	Test 300/323. loss: 0.973, 0.2386 s / batch. (data: 4.84e-05)max mem: 15.94594 GB 
[11/01 07:35:30 visual_prompt]: Inference (test):avg data time: 5.01e-05, avg batch time: 0.2329, average loss: 0.7495
[11/01 07:35:30 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.48	
[11/01 07:35:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/01 07:36:36 visual_prompt]: 	Training 100/1106. train loss: 0.5483,	0.6480 s / batch. (data: 3.44e-04). ETA=18:41:43, max mem: 15.9 GB 
[11/01 07:37:40 visual_prompt]: 	Training 200/1106. train loss: 0.0807,	0.6626 s / batch. (data: 1.66e-02). ETA=19:05:58, max mem: 15.9 GB 
[11/01 07:38:44 visual_prompt]: 	Training 300/1106. train loss: 4.8374,	0.6406 s / batch. (data: 4.01e-04). ETA=18:26:47, max mem: 15.9 GB 
[11/01 07:39:48 visual_prompt]: 	Training 400/1106. train loss: 0.2419,	0.6317 s / batch. (data: 9.36e-04). ETA=18:10:21, max mem: 15.9 GB 
[11/01 07:40:51 visual_prompt]: 	Training 500/1106. train loss: 0.1389,	0.6317 s / batch. (data: 1.29e-02). ETA=18:09:19, max mem: 15.9 GB 
[11/01 07:41:55 visual_prompt]: 	Training 600/1106. train loss: 1.0675,	0.6503 s / batch. (data: 8.34e-04). ETA=18:40:13, max mem: 15.9 GB 
[11/01 07:42:59 visual_prompt]: 	Training 700/1106. train loss: 0.7187,	0.6317 s / batch. (data: 8.87e-04). ETA=18:07:07, max mem: 15.9 GB 
[11/01 07:44:02 visual_prompt]: 	Training 800/1106. train loss: 0.9513,	0.6480 s / batch. (data: 3.90e-04). ETA=18:34:13, max mem: 15.9 GB 
[11/01 07:45:06 visual_prompt]: 	Training 900/1106. train loss: 1.4382,	0.6332 s / batch. (data: 3.61e-04). ETA=18:07:41, max mem: 15.9 GB 
[11/01 07:46:10 visual_prompt]: 	Training 1000/1106. train loss: 1.7057,	0.6680 s / batch. (data: 6.00e-03). ETA=19:06:19, max mem: 15.9 GB 
[11/01 07:47:13 visual_prompt]: 	Training 1100/1106. train loss: 0.0022,	0.6188 s / batch. (data: 2.04e-04). ETA=17:40:55, max mem: 15.9 GB 
[11/01 07:47:17 visual_prompt]: Epoch 7 / 100: avg data time: 6.67e-03, avg batch time: 0.6397, average train loss: 1.1856
[11/01 07:48:12 visual_prompt]: 	Test 100/123. loss: 3.162, 0.2359 s / batch. (data: 4.96e-05)max mem: 15.94594 GB 
[11/01 07:48:23 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.2324, average loss: 2.9095
[11/01 07:48:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.53	
[11/01 07:49:19 visual_prompt]: 	Test 100/323. loss: 0.001, 0.2466 s / batch. (data: 4.98e-05)max mem: 15.94594 GB 
[11/01 07:50:11 visual_prompt]: 	Test 200/323. loss: 2.874, 0.2782 s / batch. (data: 2.55e-02)max mem: 15.94594 GB 
[11/01 07:51:03 visual_prompt]: 	Test 300/323. loss: 3.481, 0.2354 s / batch. (data: 7.20e-05)max mem: 15.94594 GB 
[11/01 07:51:14 visual_prompt]: Inference (test):avg data time: 3.16e-04, avg batch time: 0.2320, average loss: 2.6431
[11/01 07:51:14 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.24	
[11/01 07:51:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/01 07:52:20 visual_prompt]: 	Training 100/1106. train loss: 3.2922,	0.6173 s / batch. (data: 3.66e-04). ETA=17:37:16, max mem: 15.9 GB 
[11/01 07:53:24 visual_prompt]: 	Training 200/1106. train loss: 1.8355,	0.6338 s / batch. (data: 8.88e-04). ETA=18:04:23, max mem: 15.9 GB 
[11/01 07:54:28 visual_prompt]: 	Training 300/1106. train loss: 0.2282,	0.6399 s / batch. (data: 8.55e-04). ETA=18:13:49, max mem: 15.9 GB 
[11/01 07:55:31 visual_prompt]: 	Training 400/1106. train loss: 1.2541,	0.6483 s / batch. (data: 6.02e-03). ETA=18:27:04, max mem: 15.9 GB 
[11/01 07:56:35 visual_prompt]: 	Training 500/1106. train loss: 0.6429,	0.6546 s / batch. (data: 8.51e-04). ETA=18:36:44, max mem: 15.9 GB 
[11/01 07:57:39 visual_prompt]: 	Training 600/1106. train loss: 0.7282,	0.6393 s / batch. (data: 5.50e-03). ETA=18:09:29, max mem: 15.9 GB 
[11/01 07:58:43 visual_prompt]: 	Training 700/1106. train loss: 0.9921,	0.6400 s / batch. (data: 9.17e-04). ETA=18:09:40, max mem: 15.9 GB 
[11/01 07:59:47 visual_prompt]: 	Training 800/1106. train loss: 0.8042,	0.6485 s / batch. (data: 5.51e-03). ETA=18:23:05, max mem: 15.9 GB 
[11/01 08:00:50 visual_prompt]: 	Training 900/1106. train loss: 0.6856,	0.6524 s / batch. (data: 8.58e-04). ETA=18:28:34, max mem: 15.9 GB 
[11/01 08:01:55 visual_prompt]: 	Training 1000/1106. train loss: 2.5062,	0.6496 s / batch. (data: 3.70e-04). ETA=18:22:44, max mem: 15.9 GB 
[11/01 08:02:58 visual_prompt]: 	Training 1100/1106. train loss: 1.7514,	0.6191 s / batch. (data: 2.13e-04). ETA=17:29:59, max mem: 15.9 GB 
[11/01 08:03:02 visual_prompt]: Epoch 8 / 100: avg data time: 6.62e-03, avg batch time: 0.6406, average train loss: 1.1174
[11/01 08:03:57 visual_prompt]: 	Test 100/123. loss: 0.760, 0.2256 s / batch. (data: 5.34e-05)max mem: 15.94594 GB 
[11/01 08:04:08 visual_prompt]: Inference (val):avg data time: 3.18e-04, avg batch time: 0.2325, average loss: 0.7381
[11/01 08:04:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.91	
[11/01 08:05:03 visual_prompt]: 	Test 100/323. loss: 0.306, 0.2543 s / batch. (data: 4.94e-05)max mem: 15.94594 GB 
[11/01 08:05:56 visual_prompt]: 	Test 200/323. loss: 0.545, 0.2249 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[11/01 08:06:47 visual_prompt]: 	Test 300/323. loss: 0.877, 0.2439 s / batch. (data: 3.34e-05)max mem: 15.94594 GB 
[11/01 08:06:58 visual_prompt]: Inference (test):avg data time: 8.29e-05, avg batch time: 0.2324, average loss: 0.7057
[11/01 08:06:58 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.12	
[11/01 08:06:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/01 08:08:04 visual_prompt]: 	Training 100/1106. train loss: 0.7526,	0.6462 s / batch. (data: 9.26e-04). ETA=18:14:45, max mem: 15.9 GB 
[11/01 08:09:08 visual_prompt]: 	Training 200/1106. train loss: 0.8355,	0.6360 s / batch. (data: 3.34e-04). ETA=17:56:29, max mem: 15.9 GB 
[11/01 08:10:12 visual_prompt]: 	Training 300/1106. train loss: 1.6953,	0.6330 s / batch. (data: 5.54e-03). ETA=17:50:19, max mem: 15.9 GB 
[11/01 08:11:16 visual_prompt]: 	Training 400/1106. train loss: 0.2489,	0.6481 s / batch. (data: 8.40e-04). ETA=18:14:45, max mem: 15.9 GB 
[11/01 08:12:19 visual_prompt]: 	Training 500/1106. train loss: 1.0245,	0.6317 s / batch. (data: 3.62e-04). ETA=17:45:58, max mem: 15.9 GB 
[11/01 08:13:23 visual_prompt]: 	Training 600/1106. train loss: 0.2784,	0.6473 s / batch. (data: 8.40e-04). ETA=18:11:12, max mem: 15.9 GB 
[11/01 08:14:27 visual_prompt]: 	Training 700/1106. train loss: 0.0071,	0.6393 s / batch. (data: 6.04e-03). ETA=17:56:38, max mem: 15.9 GB 
[11/01 08:15:30 visual_prompt]: 	Training 800/1106. train loss: 0.1179,	0.6233 s / batch. (data: 3.46e-04). ETA=17:28:41, max mem: 15.9 GB 
[11/01 08:16:34 visual_prompt]: 	Training 900/1106. train loss: 1.6865,	0.6199 s / batch. (data: 3.48e-04). ETA=17:22:00, max mem: 15.9 GB 
[11/01 08:17:38 visual_prompt]: 	Training 1000/1106. train loss: 0.7125,	0.6329 s / batch. (data: 3.32e-04). ETA=17:42:45, max mem: 15.9 GB 
[11/01 08:18:42 visual_prompt]: 	Training 1100/1106. train loss: 0.1153,	0.6178 s / batch. (data: 2.07e-04). ETA=17:16:24, max mem: 15.9 GB 
[11/01 08:18:45 visual_prompt]: Epoch 9 / 100: avg data time: 5.87e-03, avg batch time: 0.6390, average train loss: 1.0434
[11/01 08:19:40 visual_prompt]: 	Test 100/123. loss: 0.775, 0.2257 s / batch. (data: 3.60e-05)max mem: 15.94594 GB 
[11/01 08:19:51 visual_prompt]: Inference (val):avg data time: 1.39e-04, avg batch time: 0.2340, average loss: 0.7428
[11/01 08:19:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 58.10	
[11/01 08:20:46 visual_prompt]: 	Test 100/323. loss: 0.261, 0.2294 s / batch. (data: 5.63e-05)max mem: 15.94594 GB 
[11/01 08:21:39 visual_prompt]: 	Test 200/323. loss: 0.440, 0.2317 s / batch. (data: 3.39e-05)max mem: 15.94594 GB 
[11/01 08:22:31 visual_prompt]: 	Test 300/323. loss: 0.910, 0.2242 s / batch. (data: 4.82e-05)max mem: 15.94594 GB 
[11/01 08:22:42 visual_prompt]: Inference (test):avg data time: 4.79e-05, avg batch time: 0.2320, average loss: 0.7045
[11/01 08:22:42 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 58.52	
[11/01 08:22:42 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/01 08:23:48 visual_prompt]: 	Training 100/1106. train loss: 0.0828,	0.6420 s / batch. (data: 8.82e-04). ETA=17:55:51, max mem: 15.9 GB 
[11/01 08:24:52 visual_prompt]: 	Training 200/1106. train loss: 2.7558,	0.6311 s / batch. (data: 8.49e-04). ETA=17:36:27, max mem: 15.9 GB 
[11/01 08:25:55 visual_prompt]: 	Training 300/1106. train loss: 0.6253,	0.6287 s / batch. (data: 4.92e-04). ETA=17:31:28, max mem: 15.9 GB 
[11/01 08:26:59 visual_prompt]: 	Training 400/1106. train loss: 0.0115,	0.6362 s / batch. (data: 1.67e-02). ETA=17:42:53, max mem: 15.9 GB 
[11/01 08:28:03 visual_prompt]: 	Training 500/1106. train loss: 0.8327,	0.6480 s / batch. (data: 3.60e-04). ETA=18:01:35, max mem: 15.9 GB 
[11/01 08:29:06 visual_prompt]: 	Training 600/1106. train loss: 0.6543,	0.6487 s / batch. (data: 8.55e-04). ETA=18:01:39, max mem: 15.9 GB 
[11/01 08:30:10 visual_prompt]: 	Training 700/1106. train loss: 1.5662,	0.6592 s / batch. (data: 8.41e-04). ETA=18:18:07, max mem: 15.9 GB 
[11/01 08:31:14 visual_prompt]: 	Training 800/1106. train loss: 0.3774,	0.6399 s / batch. (data: 8.54e-04). ETA=17:44:51, max mem: 15.9 GB 
[11/01 08:32:18 visual_prompt]: 	Training 900/1106. train loss: 3.4364,	0.6272 s / batch. (data: 3.49e-04). ETA=17:22:38, max mem: 15.9 GB 
[11/01 08:33:21 visual_prompt]: 	Training 1000/1106. train loss: 0.7023,	0.6476 s / batch. (data: 9.42e-03). ETA=17:55:34, max mem: 15.9 GB 
[11/01 08:34:25 visual_prompt]: 	Training 1100/1106. train loss: 1.6302,	0.6185 s / batch. (data: 1.84e-04). ETA=17:06:04, max mem: 15.9 GB 
[11/01 08:34:29 visual_prompt]: Epoch 10 / 100: avg data time: 5.69e-03, avg batch time: 0.6392, average train loss: 1.1352
[11/01 08:35:23 visual_prompt]: 	Test 100/123. loss: 1.616, 0.2516 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[11/01 08:35:35 visual_prompt]: Inference (val):avg data time: 1.36e-04, avg batch time: 0.2328, average loss: 1.3806
[11/01 08:35:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.83	
[11/01 08:36:30 visual_prompt]: 	Test 100/323. loss: 0.050, 0.2244 s / batch. (data: 3.36e-05)max mem: 15.94594 GB 
[11/01 08:37:22 visual_prompt]: 	Test 200/323. loss: 1.197, 0.2381 s / batch. (data: 5.27e-05)max mem: 15.94594 GB 
[11/01 08:38:14 visual_prompt]: 	Test 300/323. loss: 1.644, 0.2243 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/01 08:38:25 visual_prompt]: Inference (test):avg data time: 1.62e-04, avg batch time: 0.2321, average loss: 1.2410
[11/01 08:38:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.72	
[11/01 08:38:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/01 08:39:32 visual_prompt]: 	Training 100/1106. train loss: 0.6439,	0.6316 s / batch. (data: 3.87e-04). ETA=17:26:42, max mem: 15.9 GB 
[11/01 08:40:36 visual_prompt]: 	Training 200/1106. train loss: 0.9640,	0.6395 s / batch. (data: 8.59e-04). ETA=17:38:50, max mem: 15.9 GB 
[11/01 08:41:39 visual_prompt]: 	Training 300/1106. train loss: 0.8333,	0.6443 s / batch. (data: 8.68e-04). ETA=17:45:37, max mem: 15.9 GB 
[11/01 08:42:43 visual_prompt]: 	Training 400/1106. train loss: 3.5583,	0.6532 s / batch. (data: 8.14e-04). ETA=17:59:21, max mem: 15.9 GB 
[11/01 08:43:47 visual_prompt]: 	Training 500/1106. train loss: 0.0152,	0.6400 s / batch. (data: 3.55e-04). ETA=17:36:24, max mem: 15.9 GB 
[11/01 08:44:51 visual_prompt]: 	Training 600/1106. train loss: 1.2619,	0.6358 s / batch. (data: 3.84e-04). ETA=17:28:27, max mem: 15.9 GB 
[11/01 08:45:55 visual_prompt]: 	Training 700/1106. train loss: 0.5083,	0.6618 s / batch. (data: 1.62e-02). ETA=18:10:11, max mem: 15.9 GB 
[11/01 08:46:59 visual_prompt]: 	Training 800/1106. train loss: 1.1419,	0.6280 s / batch. (data: 4.01e-04). ETA=17:13:26, max mem: 15.9 GB 
[11/01 08:48:02 visual_prompt]: 	Training 900/1106. train loss: 0.5350,	0.6250 s / batch. (data: 3.39e-04). ETA=17:07:34, max mem: 15.9 GB 
[11/01 08:49:06 visual_prompt]: 	Training 1000/1106. train loss: 2.7266,	0.6588 s / batch. (data: 2.83e-02). ETA=18:02:02, max mem: 15.9 GB 
[11/01 08:50:10 visual_prompt]: 	Training 1100/1106. train loss: 0.7256,	0.6285 s / batch. (data: 3.04e-04). ETA=17:11:07, max mem: 15.9 GB 
[11/01 08:50:14 visual_prompt]: Epoch 11 / 100: avg data time: 6.48e-03, avg batch time: 0.6406, average train loss: 1.2002
[11/01 08:51:08 visual_prompt]: 	Test 100/123. loss: 0.670, 0.2250 s / batch. (data: 5.10e-05)max mem: 15.94594 GB 
[11/01 08:51:20 visual_prompt]: Inference (val):avg data time: 2.27e-04, avg batch time: 0.2332, average loss: 0.8478
[11/01 08:51:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.85	
[11/01 08:52:15 visual_prompt]: 	Test 100/323. loss: 1.200, 0.2243 s / batch. (data: 3.15e-05)max mem: 15.94594 GB 
[11/01 08:53:08 visual_prompt]: 	Test 200/323. loss: 0.520, 0.2242 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 08:53:59 visual_prompt]: 	Test 300/323. loss: 0.820, 0.2366 s / batch. (data: 5.48e-05)max mem: 15.94594 GB 
[11/01 08:54:10 visual_prompt]: Inference (test):avg data time: 1.10e-04, avg batch time: 0.2309, average loss: 0.9015
[11/01 08:54:10 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 59.67	
[11/01 08:54:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/01 08:55:16 visual_prompt]: 	Training 100/1106. train loss: 3.5011,	0.6442 s / batch. (data: 8.93e-04). ETA=17:35:43, max mem: 15.9 GB 
[11/01 08:56:20 visual_prompt]: 	Training 200/1106. train loss: 2.0299,	0.6577 s / batch. (data: 4.07e-02). ETA=17:56:45, max mem: 15.9 GB 
[11/01 08:57:23 visual_prompt]: 	Training 300/1106. train loss: 0.6191,	0.6441 s / batch. (data: 5.52e-03). ETA=17:33:28, max mem: 15.9 GB 
[11/01 08:58:27 visual_prompt]: 	Training 400/1106. train loss: 1.0535,	0.6369 s / batch. (data: 8.73e-04). ETA=17:20:39, max mem: 15.9 GB 
[11/01 08:59:31 visual_prompt]: 	Training 500/1106. train loss: 0.8607,	0.6321 s / batch. (data: 8.45e-04). ETA=17:11:47, max mem: 15.9 GB 
[11/01 09:00:34 visual_prompt]: 	Training 600/1106. train loss: 0.8390,	0.6400 s / batch. (data: 8.72e-04). ETA=17:23:33, max mem: 15.9 GB 
[11/01 09:01:38 visual_prompt]: 	Training 700/1106. train loss: 0.0918,	0.6620 s / batch. (data: 9.05e-04). ETA=17:58:16, max mem: 15.9 GB 
[11/01 09:02:42 visual_prompt]: 	Training 800/1106. train loss: 0.7659,	0.6180 s / batch. (data: 4.27e-04). ETA=16:45:33, max mem: 15.9 GB 
[11/01 09:03:45 visual_prompt]: 	Training 900/1106. train loss: 2.2057,	0.6279 s / batch. (data: 5.55e-03). ETA=17:00:37, max mem: 15.9 GB 
[11/01 09:04:49 visual_prompt]: 	Training 1000/1106. train loss: 1.2335,	0.6336 s / batch. (data: 8.44e-04). ETA=17:08:58, max mem: 15.9 GB 
[11/01 09:05:53 visual_prompt]: 	Training 1100/1106. train loss: 0.8066,	0.6196 s / batch. (data: 2.17e-04). ETA=16:45:08, max mem: 15.9 GB 
[11/01 09:05:57 visual_prompt]: Epoch 12 / 100: avg data time: 5.38e-03, avg batch time: 0.6386, average train loss: 1.1656
[11/01 09:06:51 visual_prompt]: 	Test 100/123. loss: 0.604, 0.2366 s / batch. (data: 5.67e-05)max mem: 15.94594 GB 
[11/01 09:07:03 visual_prompt]: Inference (val):avg data time: 4.72e-05, avg batch time: 0.2321, average loss: 0.6999
[11/01 09:07:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 58.10	
[11/01 09:07:58 visual_prompt]: 	Test 100/323. loss: 0.806, 0.2289 s / batch. (data: 5.25e-05)max mem: 15.94594 GB 
[11/01 09:08:50 visual_prompt]: 	Test 200/323. loss: 0.404, 0.2242 s / batch. (data: 5.27e-05)max mem: 15.94594 GB 
[11/01 09:09:42 visual_prompt]: 	Test 300/323. loss: 0.784, 0.2442 s / batch. (data: 4.65e-05)max mem: 15.94594 GB 
[11/01 09:09:53 visual_prompt]: Inference (test):avg data time: 1.73e-04, avg batch time: 0.2313, average loss: 0.7125
[11/01 09:09:53 visual_prompt]: Classification results with test_mammo-cbis: top1: 53.64	rocauc: 57.83	
[11/01 09:09:53 visual_prompt]: Best epoch 12: best metric: -0.700
[11/01 09:09:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/01 09:11:00 visual_prompt]: 	Training 100/1106. train loss: 0.5718,	0.6780 s / batch. (data: 3.79e-02). ETA=18:18:39, max mem: 15.9 GB 
[11/01 09:12:04 visual_prompt]: 	Training 200/1106. train loss: 0.4491,	0.6332 s / batch. (data: 8.64e-04). ETA=17:05:04, max mem: 15.9 GB 
[11/01 09:13:08 visual_prompt]: 	Training 300/1106. train loss: 0.7883,	0.6397 s / batch. (data: 8.63e-04). ETA=17:14:28, max mem: 15.9 GB 
[11/01 09:14:12 visual_prompt]: 	Training 400/1106. train loss: 0.1901,	0.6307 s / batch. (data: 3.63e-04). ETA=16:58:55, max mem: 15.9 GB 
[11/01 09:15:15 visual_prompt]: 	Training 500/1106. train loss: 1.2745,	0.6440 s / batch. (data: 3.53e-04). ETA=17:19:16, max mem: 15.9 GB 
[11/01 09:16:19 visual_prompt]: 	Training 600/1106. train loss: 1.9867,	0.6302 s / batch. (data: 5.34e-04). ETA=16:55:55, max mem: 15.9 GB 
[11/01 09:17:23 visual_prompt]: 	Training 700/1106. train loss: 0.0330,	0.6375 s / batch. (data: 6.03e-03). ETA=17:06:44, max mem: 15.9 GB 
[11/01 09:18:27 visual_prompt]: 	Training 800/1106. train loss: 3.0453,	0.6491 s / batch. (data: 1.06e-02). ETA=17:24:11, max mem: 15.9 GB 
[11/01 09:19:30 visual_prompt]: 	Training 900/1106. train loss: 0.0654,	0.6314 s / batch. (data: 8.62e-04). ETA=16:54:40, max mem: 15.9 GB 
[11/01 09:20:34 visual_prompt]: 	Training 1000/1106. train loss: 2.6643,	0.6561 s / batch. (data: 2.01e-02). ETA=17:33:25, max mem: 15.9 GB 
[11/01 09:21:38 visual_prompt]: 	Training 1100/1106. train loss: 0.5240,	0.6191 s / batch. (data: 1.92e-04). ETA=16:32:51, max mem: 15.9 GB 
[11/01 09:21:42 visual_prompt]: Epoch 13 / 100: avg data time: 7.12e-03, avg batch time: 0.6403, average train loss: 1.1031
[11/01 09:22:37 visual_prompt]: 	Test 100/123. loss: 0.692, 0.2450 s / batch. (data: 5.84e-05)max mem: 15.94594 GB 
[11/01 09:22:47 visual_prompt]: Inference (val):avg data time: 1.41e-04, avg batch time: 0.2326, average loss: 0.9400
[11/01 09:22:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.43	
[11/01 09:23:42 visual_prompt]: 	Test 100/323. loss: 1.463, 0.2249 s / batch. (data: 5.32e-05)max mem: 15.94594 GB 
[11/01 09:24:35 visual_prompt]: 	Test 200/323. loss: 0.713, 0.2397 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/01 09:25:27 visual_prompt]: 	Test 300/323. loss: 0.849, 0.2522 s / batch. (data: 4.86e-05)max mem: 15.94594 GB 
[11/01 09:25:38 visual_prompt]: Inference (test):avg data time: 5.20e-05, avg batch time: 0.2327, average loss: 1.0110
[11/01 09:25:38 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.60	
[11/01 09:25:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/01 09:26:44 visual_prompt]: 	Training 100/1106. train loss: 0.5351,	0.6217 s / batch. (data: 4.56e-03). ETA=16:35:56, max mem: 15.9 GB 
[11/01 09:27:47 visual_prompt]: 	Training 200/1106. train loss: 2.8647,	0.6211 s / batch. (data: 3.42e-04). ETA=16:34:01, max mem: 15.9 GB 
[11/01 09:28:51 visual_prompt]: 	Training 300/1106. train loss: 5.5479,	0.6199 s / batch. (data: 4.59e-04). ETA=16:31:06, max mem: 15.9 GB 
[11/01 09:29:55 visual_prompt]: 	Training 400/1106. train loss: 0.3158,	0.6199 s / batch. (data: 3.84e-04). ETA=16:30:02, max mem: 15.9 GB 
[11/01 09:30:58 visual_prompt]: 	Training 500/1106. train loss: 0.9247,	0.6210 s / batch. (data: 3.46e-04). ETA=16:30:38, max mem: 15.9 GB 
[11/01 09:32:02 visual_prompt]: 	Training 600/1106. train loss: 0.5854,	0.6532 s / batch. (data: 8.92e-04). ETA=17:21:03, max mem: 15.9 GB 
[11/01 09:33:06 visual_prompt]: 	Training 700/1106. train loss: 0.8128,	0.6281 s / batch. (data: 3.54e-04). ETA=16:39:57, max mem: 15.9 GB 
[11/01 09:34:10 visual_prompt]: 	Training 800/1106. train loss: 1.8062,	0.6388 s / batch. (data: 3.67e-04). ETA=16:55:59, max mem: 15.9 GB 
[11/01 09:35:13 visual_prompt]: 	Training 900/1106. train loss: 0.1272,	0.6460 s / batch. (data: 8.73e-04). ETA=17:06:18, max mem: 15.9 GB 
[11/01 09:36:17 visual_prompt]: 	Training 1000/1106. train loss: 0.0602,	0.6328 s / batch. (data: 9.02e-04). ETA=16:44:17, max mem: 15.9 GB 
[11/01 09:37:21 visual_prompt]: 	Training 1100/1106. train loss: 6.4984,	0.6271 s / batch. (data: 2.19e-04). ETA=16:34:11, max mem: 15.9 GB 
[11/01 09:37:24 visual_prompt]: Epoch 14 / 100: avg data time: 5.57e-03, avg batch time: 0.6388, average train loss: 1.1536
[11/01 09:38:19 visual_prompt]: 	Test 100/123. loss: 0.658, 0.2320 s / batch. (data: 2.69e-05)max mem: 15.94594 GB 
[11/01 09:38:30 visual_prompt]: Inference (val):avg data time: 4.96e-05, avg batch time: 0.2313, average loss: 0.7457
[11/01 09:38:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 61.15	
[11/01 09:39:25 visual_prompt]: 	Test 100/323. loss: 0.996, 0.2242 s / batch. (data: 5.05e-05)max mem: 15.94594 GB 
[11/01 09:40:17 visual_prompt]: 	Test 200/323. loss: 0.460, 0.2242 s / batch. (data: 4.82e-05)max mem: 15.94594 GB 
[11/01 09:41:10 visual_prompt]: 	Test 300/323. loss: 0.782, 0.2333 s / batch. (data: 7.96e-05)max mem: 15.94594 GB 
[11/01 09:41:20 visual_prompt]: Inference (test):avg data time: 8.10e-05, avg batch time: 0.2321, average loss: 0.8010
[11/01 09:41:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 43.10	rocauc: 57.59	
[11/01 09:41:20 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/01 09:42:26 visual_prompt]: 	Training 100/1106. train loss: 0.8929,	0.6221 s / batch. (data: 3.55e-04). ETA=16:25:13, max mem: 15.9 GB 
[11/01 09:43:30 visual_prompt]: 	Training 200/1106. train loss: 2.2581,	0.6237 s / batch. (data: 3.54e-04). ETA=16:26:42, max mem: 15.9 GB 
[11/01 09:44:34 visual_prompt]: 	Training 300/1106. train loss: 0.6653,	0.6271 s / batch. (data: 3.95e-04). ETA=16:30:59, max mem: 15.9 GB 
[11/01 09:45:38 visual_prompt]: 	Training 400/1106. train loss: 0.0210,	0.6445 s / batch. (data: 1.57e-02). ETA=16:57:28, max mem: 15.9 GB 
[11/01 09:46:41 visual_prompt]: 	Training 500/1106. train loss: 0.4696,	0.6544 s / batch. (data: 6.03e-03). ETA=17:12:00, max mem: 15.9 GB 
[11/01 09:47:45 visual_prompt]: 	Training 600/1106. train loss: 0.0088,	0.6320 s / batch. (data: 3.53e-04). ETA=16:35:33, max mem: 15.9 GB 
[11/01 09:48:49 visual_prompt]: 	Training 700/1106. train loss: 0.1713,	0.6333 s / batch. (data: 9.04e-04). ETA=16:36:32, max mem: 15.9 GB 
[11/01 09:49:52 visual_prompt]: 	Training 800/1106. train loss: 0.8793,	0.6352 s / batch. (data: 3.69e-04). ETA=16:38:25, max mem: 15.9 GB 
[11/01 09:50:56 visual_prompt]: 	Training 900/1106. train loss: 0.8763,	0.6360 s / batch. (data: 1.03e-03). ETA=16:38:37, max mem: 15.9 GB 
[11/01 09:52:00 visual_prompt]: 	Training 1000/1106. train loss: 1.3441,	0.6491 s / batch. (data: 8.72e-04). ETA=16:58:08, max mem: 15.9 GB 
[11/01 09:53:04 visual_prompt]: 	Training 1100/1106. train loss: 0.5893,	0.6180 s / batch. (data: 1.79e-04). ETA=16:08:20, max mem: 15.9 GB 
[11/01 09:53:08 visual_prompt]: Epoch 15 / 100: avg data time: 5.64e-03, avg batch time: 0.6393, average train loss: 1.0991
[11/01 09:54:02 visual_prompt]: 	Test 100/123. loss: 0.995, 0.2252 s / batch. (data: 4.77e-05)max mem: 15.94594 GB 
[11/01 09:54:13 visual_prompt]: Inference (val):avg data time: 1.84e-04, avg batch time: 0.2334, average loss: 0.9017
[11/01 09:54:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.44	
[11/01 09:55:08 visual_prompt]: 	Test 100/323. loss: 0.134, 0.2317 s / batch. (data: 3.53e-05)max mem: 15.94594 GB 
[11/01 09:56:01 visual_prompt]: 	Test 200/323. loss: 0.248, 0.2285 s / batch. (data: 3.43e-05)max mem: 15.94594 GB 
[11/01 09:56:52 visual_prompt]: 	Test 300/323. loss: 1.277, 0.2239 s / batch. (data: 4.74e-05)max mem: 15.94594 GB 
[11/01 09:57:04 visual_prompt]: Inference (test):avg data time: 1.60e-04, avg batch time: 0.2317, average loss: 0.8262
[11/01 09:57:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 58.89	
[11/01 09:57:04 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/01 09:58:10 visual_prompt]: 	Training 100/1106. train loss: 0.0477,	0.6285 s / batch. (data: 4.09e-04). ETA=16:23:41, max mem: 15.9 GB 
[11/01 09:59:13 visual_prompt]: 	Training 200/1106. train loss: 0.5976,	0.6547 s / batch. (data: 9.19e-04). ETA=17:03:36, max mem: 15.9 GB 
[11/01 10:00:17 visual_prompt]: 	Training 300/1106. train loss: 0.1165,	0.6570 s / batch. (data: 8.71e-04). ETA=17:06:09, max mem: 15.9 GB 
[11/01 10:01:21 visual_prompt]: 	Training 400/1106. train loss: 4.4731,	0.6331 s / batch. (data: 8.92e-04). ETA=16:27:43, max mem: 15.9 GB 
[11/01 10:02:24 visual_prompt]: 	Training 500/1106. train loss: 0.8295,	0.6360 s / batch. (data: 3.30e-04). ETA=16:31:12, max mem: 15.9 GB 
[11/01 10:03:28 visual_prompt]: 	Training 600/1106. train loss: 0.5449,	0.6416 s / batch. (data: 8.92e-04). ETA=16:38:55, max mem: 15.9 GB 
[11/01 10:04:32 visual_prompt]: 	Training 700/1106. train loss: 0.1134,	0.6560 s / batch. (data: 8.22e-04). ETA=17:00:10, max mem: 15.9 GB 
[11/01 10:05:36 visual_prompt]: 	Training 800/1106. train loss: 1.5034,	0.6353 s / batch. (data: 3.53e-04). ETA=16:26:58, max mem: 15.9 GB 
[11/01 10:06:39 visual_prompt]: 	Training 900/1106. train loss: 0.1272,	0.6421 s / batch. (data: 3.80e-04). ETA=16:36:27, max mem: 15.9 GB 
[11/01 10:07:43 visual_prompt]: 	Training 1000/1106. train loss: 0.4339,	0.6367 s / batch. (data: 7.97e-03). ETA=16:26:56, max mem: 15.9 GB 
[11/01 10:08:47 visual_prompt]: 	Training 1100/1106. train loss: 2.0287,	0.6192 s / batch. (data: 2.06e-04). ETA=15:58:49, max mem: 15.9 GB 
[11/01 10:08:51 visual_prompt]: Epoch 16 / 100: avg data time: 5.68e-03, avg batch time: 0.6392, average train loss: 1.0637
[11/01 10:09:45 visual_prompt]: 	Test 100/123. loss: 0.819, 0.2397 s / batch. (data: 2.93e-05)max mem: 15.94594 GB 
[11/01 10:09:57 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.2336, average loss: 1.0218
[11/01 10:09:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.03	
[11/01 10:10:52 visual_prompt]: 	Test 100/323. loss: 1.699, 0.2398 s / batch. (data: 5.44e-05)max mem: 15.94594 GB 
[11/01 10:11:44 visual_prompt]: 	Test 200/323. loss: 0.623, 0.2334 s / batch. (data: 5.41e-05)max mem: 15.94594 GB 
[11/01 10:12:36 visual_prompt]: 	Test 300/323. loss: 0.824, 0.2379 s / batch. (data: 6.51e-05)max mem: 15.94594 GB 
[11/01 10:12:47 visual_prompt]: Inference (test):avg data time: 2.06e-04, avg batch time: 0.2327, average loss: 1.1280
[11/01 10:12:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.24	rocauc: 57.51	
[11/01 10:12:47 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/01 10:13:53 visual_prompt]: 	Training 100/1106. train loss: 0.5931,	0.6322 s / batch. (data: 9.03e-04). ETA=16:17:49, max mem: 15.9 GB 
[11/01 10:14:57 visual_prompt]: 	Training 200/1106. train loss: 0.8514,	0.6297 s / batch. (data: 3.57e-04). ETA=16:12:57, max mem: 15.9 GB 
[11/01 10:16:01 visual_prompt]: 	Training 300/1106. train loss: 1.5888,	0.6396 s / batch. (data: 7.89e-04). ETA=16:27:11, max mem: 15.9 GB 
[11/01 10:17:04 visual_prompt]: 	Training 400/1106. train loss: 0.5917,	0.6578 s / batch. (data: 8.95e-03). ETA=16:54:06, max mem: 15.9 GB 
[11/01 10:18:08 visual_prompt]: 	Training 500/1106. train loss: 0.5372,	0.6520 s / batch. (data: 7.98e-03). ETA=16:44:06, max mem: 15.9 GB 
[11/01 10:19:11 visual_prompt]: 	Training 600/1106. train loss: 3.0663,	0.6446 s / batch. (data: 5.51e-03). ETA=16:31:36, max mem: 15.9 GB 
[11/01 10:20:15 visual_prompt]: 	Training 700/1106. train loss: 0.2259,	0.6463 s / batch. (data: 6.23e-03). ETA=16:33:14, max mem: 15.9 GB 
[11/01 10:21:18 visual_prompt]: 	Training 800/1106. train loss: 0.0172,	0.6280 s / batch. (data: 3.65e-04). ETA=16:04:01, max mem: 15.9 GB 
[11/01 10:22:22 visual_prompt]: 	Training 900/1106. train loss: 0.1345,	0.6285 s / batch. (data: 3.45e-04). ETA=16:03:47, max mem: 15.9 GB 
[11/01 10:23:25 visual_prompt]: 	Training 1000/1106. train loss: 1.6227,	0.6520 s / batch. (data: 1.20e-02). ETA=16:38:41, max mem: 15.9 GB 
[11/01 10:24:28 visual_prompt]: 	Training 1100/1106. train loss: 1.8143,	0.6175 s / batch. (data: 2.00e-04). ETA=15:44:48, max mem: 15.9 GB 
[11/01 10:24:32 visual_prompt]: Epoch 17 / 100: avg data time: 6.32e-03, avg batch time: 0.6379, average train loss: 1.0430
[11/01 10:25:28 visual_prompt]: 	Test 100/123. loss: 0.635, 0.2257 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[11/01 10:25:40 visual_prompt]: Inference (val):avg data time: 4.86e-05, avg batch time: 0.2315, average loss: 0.9128
[11/01 10:25:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 60.88	
[11/01 10:26:36 visual_prompt]: 	Test 100/323. loss: 1.347, 0.2246 s / batch. (data: 4.41e-05)max mem: 15.94594 GB 
[11/01 10:27:28 visual_prompt]: 	Test 200/323. loss: 0.606, 0.2246 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[11/01 10:28:21 visual_prompt]: 	Test 300/323. loss: 0.707, 0.2238 s / batch. (data: 4.15e-05)max mem: 15.94594 GB 
[11/01 10:28:32 visual_prompt]: Inference (test):avg data time: 4.57e-05, avg batch time: 0.2299, average loss: 0.9867
[11/01 10:28:32 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.24	rocauc: 59.16	
[11/01 10:28:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/01 10:29:39 visual_prompt]: 	Training 100/1106. train loss: 1.0930,	0.6623 s / batch. (data: 6.00e-03). ETA=16:52:12, max mem: 15.9 GB 
[11/01 10:30:42 visual_prompt]: 	Training 200/1106. train loss: 0.6905,	0.6286 s / batch. (data: 3.53e-04). ETA=15:59:39, max mem: 15.9 GB 
[11/01 10:31:46 visual_prompt]: 	Training 300/1106. train loss: 1.2386,	0.6626 s / batch. (data: 1.19e-02). ETA=16:50:30, max mem: 15.9 GB 
[11/01 10:32:50 visual_prompt]: 	Training 400/1106. train loss: 0.5989,	0.6337 s / batch. (data: 3.43e-04). ETA=16:05:20, max mem: 15.9 GB 
[11/01 10:33:53 visual_prompt]: 	Training 500/1106. train loss: 0.7995,	0.6234 s / batch. (data: 5.12e-04). ETA=15:48:34, max mem: 15.9 GB 
[11/01 10:34:57 visual_prompt]: 	Training 600/1106. train loss: 1.2413,	0.6201 s / batch. (data: 3.47e-04). ETA=15:42:30, max mem: 15.9 GB 
[11/01 10:36:01 visual_prompt]: 	Training 700/1106. train loss: 1.3261,	0.6399 s / batch. (data: 1.06e-02). ETA=16:11:31, max mem: 15.9 GB 
[11/01 10:37:05 visual_prompt]: 	Training 800/1106. train loss: 0.8262,	0.6464 s / batch. (data: 1.57e-02). ETA=16:20:24, max mem: 15.9 GB 
[11/01 10:38:09 visual_prompt]: 	Training 900/1106. train loss: 1.3068,	0.6202 s / batch. (data: 3.97e-04). ETA=15:39:38, max mem: 15.9 GB 
[11/01 10:39:12 visual_prompt]: 	Training 1000/1106. train loss: 0.6356,	0.6246 s / batch. (data: 3.62e-04). ETA=15:45:14, max mem: 15.9 GB 
[11/01 10:40:16 visual_prompt]: 	Training 1100/1106. train loss: 0.4788,	0.6218 s / batch. (data: 2.85e-04). ETA=15:39:57, max mem: 15.9 GB 
[11/01 10:40:20 visual_prompt]: Epoch 18 / 100: avg data time: 6.52e-03, avg batch time: 0.6395, average train loss: 1.0227
[11/01 10:41:14 visual_prompt]: 	Test 100/123. loss: 0.498, 0.2321 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[11/01 10:41:25 visual_prompt]: Inference (val):avg data time: 1.08e-04, avg batch time: 0.2315, average loss: 0.8617
[11/01 10:41:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 58.75	
[11/01 10:42:21 visual_prompt]: 	Test 100/323. loss: 1.065, 0.2277 s / batch. (data: 3.86e-05)max mem: 15.94594 GB 
[11/01 10:43:12 visual_prompt]: 	Test 200/323. loss: 0.463, 0.2477 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[11/01 10:44:04 visual_prompt]: 	Test 300/323. loss: 0.650, 0.2243 s / batch. (data: 6.56e-05)max mem: 15.94594 GB 
[11/01 10:44:15 visual_prompt]: Inference (test):avg data time: 1.34e-04, avg batch time: 0.2322, average loss: 0.9216
[11/01 10:44:15 visual_prompt]: Classification results with test_mammo-cbis: top1: 46.36	rocauc: 58.44	
[11/01 10:44:15 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/01 10:45:21 visual_prompt]: 	Training 100/1106. train loss: 0.8793,	0.6291 s / batch. (data: 3.62e-04). ETA=15:49:48, max mem: 15.9 GB 
[11/01 10:46:25 visual_prompt]: 	Training 200/1106. train loss: 0.7141,	0.6518 s / batch. (data: 1.57e-02). ETA=16:23:00, max mem: 15.9 GB 
[11/01 10:47:28 visual_prompt]: 	Training 300/1106. train loss: 1.0573,	0.6310 s / batch. (data: 1.26e-02). ETA=15:50:41, max mem: 15.9 GB 
[11/01 10:48:32 visual_prompt]: 	Training 400/1106. train loss: 0.9667,	0.6300 s / batch. (data: 7.99e-04). ETA=15:48:05, max mem: 15.9 GB 
[11/01 10:49:36 visual_prompt]: 	Training 500/1106. train loss: 0.6920,	0.6571 s / batch. (data: 8.61e-04). ETA=16:27:43, max mem: 15.9 GB 
[11/01 10:50:40 visual_prompt]: 	Training 600/1106. train loss: 1.2204,	0.6319 s / batch. (data: 5.28e-04). ETA=15:48:49, max mem: 15.9 GB 
[11/01 10:51:43 visual_prompt]: 	Training 700/1106. train loss: 0.0773,	0.6440 s / batch. (data: 3.73e-04). ETA=16:05:53, max mem: 15.9 GB 
[11/01 10:52:47 visual_prompt]: 	Training 800/1106. train loss: 1.5966,	0.6336 s / batch. (data: 8.86e-04). ETA=15:49:11, max mem: 15.9 GB 
[11/01 10:53:51 visual_prompt]: 	Training 900/1106. train loss: 1.3860,	0.6343 s / batch. (data: 4.03e-04). ETA=15:49:19, max mem: 15.9 GB 
[11/01 10:54:55 visual_prompt]: 	Training 1000/1106. train loss: 0.0682,	0.6323 s / batch. (data: 1.47e-02). ETA=15:45:09, max mem: 15.9 GB 
[11/01 10:55:58 visual_prompt]: 	Training 1100/1106. train loss: 2.6480,	0.6191 s / batch. (data: 2.02e-04). ETA=15:24:23, max mem: 15.9 GB 
[11/01 10:56:02 visual_prompt]: Epoch 19 / 100: avg data time: 5.87e-03, avg batch time: 0.6393, average train loss: 1.0266
[11/01 10:56:57 visual_prompt]: 	Test 100/123. loss: 2.734, 0.2317 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[11/01 10:57:08 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.2320, average loss: 2.4471
[11/01 10:57:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.99	
[11/01 10:58:03 visual_prompt]: 	Test 100/323. loss: 0.003, 0.2242 s / batch. (data: 5.17e-05)max mem: 15.94594 GB 
[11/01 10:58:55 visual_prompt]: 	Test 200/323. loss: 1.952, 0.2286 s / batch. (data: 5.77e-05)max mem: 15.94594 GB 
[11/01 10:59:46 visual_prompt]: 	Test 300/323. loss: 3.051, 0.2344 s / batch. (data: 4.08e-05)max mem: 15.94594 GB 
[11/01 10:59:57 visual_prompt]: Inference (test):avg data time: 1.44e-04, avg batch time: 0.2319, average loss: 2.2105
[11/01 10:59:57 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.78	
[11/01 10:59:57 visual_prompt]: Stopping early.
[11/01 10:59:58 visual_prompt]: Rank of current process: 0. World size: 1
[11/01 10:59:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/01 10:59:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/01 10:59:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/01 10:59:58 visual_prompt]: Training with config:
[11/01 10:59:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/test/seed3172/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 3172, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/01 10:59:58 visual_prompt]: Loading training data...
[11/01 10:59:58 visual_prompt]: Constructing mammo-cbis dataset train...
[11/01 10:59:58 visual_prompt]: Loading validation data...
[11/01 10:59:58 visual_prompt]: Constructing mammo-cbis dataset val...
[11/01 10:59:58 visual_prompt]: Loading test data...
[11/01 10:59:58 visual_prompt]: Constructing mammo-cbis dataset test...
[11/01 10:59:58 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[11/01 11:00:05 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[11/01 11:00:05 visual_prompt]: tuned percent:0.522
[11/01 11:00:06 visual_prompt]: Device used for model: 0
[11/01 11:00:06 visual_prompt]: Setting up Evaluator...
[11/01 11:00:06 visual_prompt]: Setting up Trainer...
[11/01 11:00:06 visual_prompt]: 	Setting up the optimizer...
[11/01 11:00:06 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/01 11:01:11 visual_prompt]: 	Training 100/1106. train loss: 0.6118,	0.6341 s / batch. (data: 1.06e-02). ETA=19:27:47, max mem: 15.9 GB 
[11/01 11:02:15 visual_prompt]: 	Training 200/1106. train loss: 0.4465,	0.6680 s / batch. (data: 8.68e-04). ETA=20:29:07, max mem: 15.9 GB 
[11/01 11:03:19 visual_prompt]: 	Training 300/1106. train loss: 0.7021,	0.6238 s / batch. (data: 3.41e-04). ETA=19:06:43, max mem: 15.9 GB 
[11/01 11:04:22 visual_prompt]: 	Training 400/1106. train loss: 0.1227,	0.6440 s / batch. (data: 3.25e-04). ETA=19:42:53, max mem: 15.9 GB 
[11/01 11:05:26 visual_prompt]: 	Training 500/1106. train loss: 0.8152,	0.6356 s / batch. (data: 1.04e-03). ETA=19:26:18, max mem: 15.9 GB 
[11/01 11:06:30 visual_prompt]: 	Training 600/1106. train loss: 0.3434,	0.6201 s / batch. (data: 3.66e-04). ETA=18:56:55, max mem: 15.9 GB 
[11/01 11:07:33 visual_prompt]: 	Training 700/1106. train loss: 0.9582,	0.6334 s / batch. (data: 8.21e-04). ETA=19:20:12, max mem: 15.9 GB 
[11/01 11:08:37 visual_prompt]: 	Training 800/1106. train loss: 0.3589,	0.6529 s / batch. (data: 6.28e-03). ETA=19:54:46, max mem: 15.9 GB 
[11/01 11:09:41 visual_prompt]: 	Training 900/1106. train loss: 2.0940,	0.6238 s / batch. (data: 3.57e-04). ETA=19:00:28, max mem: 15.9 GB 
[11/01 11:10:45 visual_prompt]: 	Training 1000/1106. train loss: 0.2206,	0.6438 s / batch. (data: 9.41e-03). ETA=19:36:05, max mem: 15.9 GB 
[11/01 11:11:49 visual_prompt]: 	Training 1100/1106. train loss: 0.6577,	0.6186 s / batch. (data: 2.04e-04). ETA=18:48:56, max mem: 15.9 GB 
[11/01 11:11:52 visual_prompt]: Epoch 1 / 100: avg data time: 5.74e-03, avg batch time: 0.6389, average train loss: 0.9958
[11/01 11:12:47 visual_prompt]: 	Test 100/123. loss: 0.800, 0.2257 s / batch. (data: 6.22e-05)max mem: 15.94594 GB 
[11/01 11:12:58 visual_prompt]: Inference (val):avg data time: 4.69e-05, avg batch time: 0.2321, average loss: 0.9582
[11/01 11:12:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 47.26	
[11/01 11:13:52 visual_prompt]: 	Test 100/323. loss: 1.747, 0.2244 s / batch. (data: 5.91e-05)max mem: 15.94594 GB 
[11/01 11:14:45 visual_prompt]: 	Test 200/323. loss: 1.109, 0.2319 s / batch. (data: 7.46e-05)max mem: 15.94594 GB 
[11/01 11:15:36 visual_prompt]: 	Test 300/323. loss: 0.439, 0.2312 s / batch. (data: 6.45e-03)max mem: 15.94594 GB 
[11/01 11:15:47 visual_prompt]: Inference (test):avg data time: 1.12e-04, avg batch time: 0.2321, average loss: 0.9583
[11/01 11:15:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 42.17	rocauc: 52.00	
[11/01 11:15:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/01 11:16:54 visual_prompt]: 	Training 100/1106. train loss: 0.6529,	0.6320 s / batch. (data: 3.19e-04). ETA=19:12:17, max mem: 15.9 GB 
[11/01 11:17:57 visual_prompt]: 	Training 200/1106. train loss: 0.7977,	0.6187 s / batch. (data: 3.45e-04). ETA=18:47:03, max mem: 15.9 GB 
[11/01 11:19:01 visual_prompt]: 	Training 300/1106. train loss: 0.5530,	0.6468 s / batch. (data: 1.02e-02). ETA=19:37:11, max mem: 15.9 GB 
[11/01 11:20:05 visual_prompt]: 	Training 400/1106. train loss: 0.7680,	0.6311 s / batch. (data: 3.62e-04). ETA=19:07:34, max mem: 15.9 GB 
[11/01 11:21:09 visual_prompt]: 	Training 500/1106. train loss: 1.5339,	0.6252 s / batch. (data: 3.62e-04). ETA=18:55:45, max mem: 15.9 GB 
[11/01 11:22:12 visual_prompt]: 	Training 600/1106. train loss: 0.6917,	0.6191 s / batch. (data: 3.54e-04). ETA=18:43:41, max mem: 15.9 GB 
[11/01 11:23:16 visual_prompt]: 	Training 700/1106. train loss: 0.9814,	0.6242 s / batch. (data: 3.63e-04). ETA=18:51:50, max mem: 15.9 GB 
[11/01 11:24:19 visual_prompt]: 	Training 800/1106. train loss: 0.9832,	0.6370 s / batch. (data: 8.63e-04). ETA=19:14:02, max mem: 15.9 GB 
[11/01 11:25:23 visual_prompt]: 	Training 900/1106. train loss: 0.5898,	0.6195 s / batch. (data: 3.48e-04). ETA=18:41:17, max mem: 15.9 GB 
[11/01 11:26:26 visual_prompt]: 	Training 1000/1106. train loss: 0.7791,	0.6185 s / batch. (data: 5.10e-04). ETA=18:38:26, max mem: 15.9 GB 
[11/01 11:27:30 visual_prompt]: 	Training 1100/1106. train loss: 0.8392,	0.6179 s / batch. (data: 1.68e-04). ETA=18:36:17, max mem: 15.9 GB 
[11/01 11:27:34 visual_prompt]: Epoch 2 / 100: avg data time: 6.19e-03, avg batch time: 0.6387, average train loss: 0.8708
[11/01 11:28:27 visual_prompt]: 	Test 100/123. loss: 0.732, 0.2270 s / batch. (data: 3.65e-05)max mem: 15.94594 GB 
[11/01 11:28:39 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.2327, average loss: 0.7701
[11/01 11:28:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.31	
[11/01 11:29:34 visual_prompt]: 	Test 100/323. loss: 1.071, 0.2366 s / batch. (data: 5.67e-05)max mem: 15.94594 GB 
[11/01 11:30:26 visual_prompt]: 	Test 200/323. loss: 0.753, 0.2293 s / batch. (data: 4.17e-05)max mem: 15.94594 GB 
[11/01 11:31:17 visual_prompt]: 	Test 300/323. loss: 0.692, 0.2361 s / batch. (data: 3.36e-05)max mem: 15.94594 GB 
[11/01 11:31:28 visual_prompt]: Inference (test):avg data time: 7.85e-05, avg batch time: 0.2321, average loss: 0.7904
[11/01 11:31:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 52.66	
[11/01 11:31:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/01 11:32:36 visual_prompt]: 	Training 100/1106. train loss: 0.6859,	0.6440 s / batch. (data: 7.97e-03). ETA=19:22:17, max mem: 15.9 GB 
[11/01 11:33:39 visual_prompt]: 	Training 200/1106. train loss: 0.8954,	0.6337 s / batch. (data: 3.52e-04). ETA=19:02:34, max mem: 15.9 GB 
[11/01 11:34:43 visual_prompt]: 	Training 300/1106. train loss: 2.0159,	0.6289 s / batch. (data: 3.69e-04). ETA=18:52:59, max mem: 15.9 GB 
[11/01 11:35:47 visual_prompt]: 	Training 400/1106. train loss: 0.4317,	0.6314 s / batch. (data: 3.44e-04). ETA=18:56:27, max mem: 15.9 GB 
[11/01 11:36:51 visual_prompt]: 	Training 500/1106. train loss: 0.6956,	0.6227 s / batch. (data: 3.50e-04). ETA=18:39:36, max mem: 15.9 GB 
[11/01 11:37:54 visual_prompt]: 	Training 600/1106. train loss: 0.0293,	0.6346 s / batch. (data: 4.60e-04). ETA=19:00:00, max mem: 15.9 GB 
[11/01 11:38:58 visual_prompt]: 	Training 700/1106. train loss: 1.2872,	0.6280 s / batch. (data: 3.39e-04). ETA=18:47:05, max mem: 15.9 GB 
[11/01 11:40:01 visual_prompt]: 	Training 800/1106. train loss: 0.6163,	0.6250 s / batch. (data: 5.02e-03). ETA=18:40:45, max mem: 15.9 GB 
[11/01 11:41:05 visual_prompt]: 	Training 900/1106. train loss: 0.7760,	0.6340 s / batch. (data: 8.59e-04). ETA=18:55:49, max mem: 15.9 GB 
[11/01 11:42:08 visual_prompt]: 	Training 1000/1106. train loss: 0.7622,	0.6563 s / batch. (data: 3.95e-02). ETA=19:34:42, max mem: 15.9 GB 
[11/01 11:43:12 visual_prompt]: 	Training 1100/1106. train loss: 1.3492,	0.6190 s / batch. (data: 1.96e-04). ETA=18:26:50, max mem: 15.9 GB 
[11/01 11:43:16 visual_prompt]: Epoch 3 / 100: avg data time: 6.70e-03, avg batch time: 0.6395, average train loss: 0.8857
[11/01 11:44:10 visual_prompt]: 	Test 100/123. loss: 0.698, 0.2257 s / batch. (data: 3.93e-05)max mem: 15.94594 GB 
[11/01 11:44:22 visual_prompt]: Inference (val):avg data time: 1.34e-04, avg batch time: 0.2317, average loss: 0.6960
[11/01 11:44:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 52.56	
[11/01 11:45:17 visual_prompt]: 	Test 100/323. loss: 0.729, 0.2345 s / batch. (data: 4.82e-05)max mem: 15.94594 GB 
[11/01 11:46:08 visual_prompt]: 	Test 200/323. loss: 0.649, 0.2305 s / batch. (data: 4.82e-05)max mem: 15.94594 GB 
[11/01 11:47:00 visual_prompt]: 	Test 300/323. loss: 0.688, 0.2417 s / batch. (data: 7.03e-05)max mem: 15.94594 GB 
[11/01 11:47:11 visual_prompt]: Inference (test):avg data time: 1.29e-04, avg batch time: 0.2320, average loss: 0.6983
[11/01 11:47:11 visual_prompt]: Classification results with test_mammo-cbis: top1: 47.13	rocauc: 54.25	
[11/01 11:47:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/01 11:48:17 visual_prompt]: 	Training 100/1106. train loss: 0.6727,	0.6274 s / batch. (data: 1.06e-02). ETA=18:40:50, max mem: 15.9 GB 
[11/01 11:49:20 visual_prompt]: 	Training 200/1106. train loss: 0.8121,	0.6574 s / batch. (data: 1.57e-02). ETA=19:33:11, max mem: 15.9 GB 
[11/01 11:50:24 visual_prompt]: 	Training 300/1106. train loss: 0.7460,	0.6185 s / batch. (data: 9.28e-04). ETA=18:22:51, max mem: 15.9 GB 
[11/01 11:51:27 visual_prompt]: 	Training 400/1106. train loss: 0.6830,	0.6492 s / batch. (data: 1.24e-03). ETA=19:16:28, max mem: 15.9 GB 
[11/01 11:52:31 visual_prompt]: 	Training 500/1106. train loss: 0.1762,	0.6311 s / batch. (data: 5.21e-03). ETA=18:43:09, max mem: 15.9 GB 
[11/01 11:53:35 visual_prompt]: 	Training 600/1106. train loss: 0.4380,	0.6336 s / batch. (data: 1.23e-03). ETA=18:46:38, max mem: 15.9 GB 
[11/01 11:54:39 visual_prompt]: 	Training 700/1106. train loss: 0.4555,	0.6400 s / batch. (data: 3.09e-04). ETA=18:56:52, max mem: 15.9 GB 
[11/01 11:55:42 visual_prompt]: 	Training 800/1106. train loss: 0.8098,	0.6331 s / batch. (data: 8.51e-04). ETA=18:43:31, max mem: 15.9 GB 
[11/01 11:56:46 visual_prompt]: 	Training 900/1106. train loss: 1.0218,	0.6440 s / batch. (data: 3.63e-04). ETA=19:01:49, max mem: 15.9 GB 
[11/01 11:57:50 visual_prompt]: 	Training 1000/1106. train loss: 1.2243,	0.6431 s / batch. (data: 8.88e-04). ETA=18:59:05, max mem: 15.9 GB 
[11/01 11:58:53 visual_prompt]: 	Training 1100/1106. train loss: 0.7754,	0.6193 s / batch. (data: 1.98e-04). ETA=18:16:01, max mem: 15.9 GB 
[11/01 11:58:57 visual_prompt]: Epoch 4 / 100: avg data time: 5.43e-03, avg batch time: 0.6385, average train loss: 0.9424
[11/01 11:59:52 visual_prompt]: 	Test 100/123. loss: 0.831, 0.2410 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[11/01 12:00:03 visual_prompt]: Inference (val):avg data time: 4.67e-05, avg batch time: 0.2333, average loss: 0.7727
[11/01 12:00:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.01	
[11/01 12:00:59 visual_prompt]: 	Test 100/323. loss: 0.276, 0.2246 s / batch. (data: 4.39e-05)max mem: 15.94594 GB 
[11/01 12:01:56 visual_prompt]: 	Test 200/323. loss: 0.730, 0.2317 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[11/01 12:02:49 visual_prompt]: 	Test 300/323. loss: 0.889, 0.2312 s / batch. (data: 4.86e-05)max mem: 15.94594 GB 
[11/01 12:03:00 visual_prompt]: Inference (test):avg data time: 1.17e-04, avg batch time: 0.2305, average loss: 0.7260
[11/01 12:03:00 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.02	
[11/01 12:03:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/01 12:04:07 visual_prompt]: 	Training 100/1106. train loss: 0.1453,	0.6424 s / batch. (data: 5.50e-03). ETA=18:55:45, max mem: 15.9 GB 
[11/01 12:05:12 visual_prompt]: 	Training 200/1106. train loss: 0.8635,	0.6957 s / batch. (data: 3.31e-04). ETA=20:28:43, max mem: 15.9 GB 
[11/01 12:06:16 visual_prompt]: 	Training 300/1106. train loss: 0.5923,	0.6474 s / batch. (data: 3.87e-04). ETA=19:02:27, max mem: 15.9 GB 
[11/01 12:07:20 visual_prompt]: 	Training 400/1106. train loss: 1.6779,	0.6346 s / batch. (data: 3.56e-04). ETA=18:38:49, max mem: 15.9 GB 
[11/01 12:08:24 visual_prompt]: 	Training 500/1106. train loss: 0.9269,	0.6242 s / batch. (data: 8.74e-04). ETA=18:19:19, max mem: 15.9 GB 
[11/01 12:09:28 visual_prompt]: 	Training 600/1106. train loss: 0.8414,	0.6193 s / batch. (data: 4.03e-04). ETA=18:09:39, max mem: 15.9 GB 
[11/01 12:10:31 visual_prompt]: 	Training 700/1106. train loss: 0.1498,	0.6350 s / batch. (data: 8.41e-04). ETA=18:36:13, max mem: 15.9 GB 
[11/01 12:11:35 visual_prompt]: 	Training 800/1106. train loss: 0.8426,	0.6195 s / batch. (data: 3.54e-04). ETA=18:08:04, max mem: 15.9 GB 
[11/01 12:12:38 visual_prompt]: 	Training 900/1106. train loss: 0.1926,	0.6240 s / batch. (data: 3.42e-04). ETA=18:14:54, max mem: 15.9 GB 
[11/01 12:13:45 visual_prompt]: 	Training 1000/1106. train loss: 0.8936,	0.6512 s / batch. (data: 3.38e-04). ETA=19:01:35, max mem: 15.9 GB 
[11/01 12:14:49 visual_prompt]: 	Training 1100/1106. train loss: 0.6522,	0.6182 s / batch. (data: 2.11e-04). ETA=18:02:35, max mem: 15.9 GB 
[11/01 12:14:52 visual_prompt]: Epoch 5 / 100: avg data time: 1.32e-02, avg batch time: 0.6441, average train loss: 1.0018
[11/01 12:15:57 visual_prompt]: 	Test 100/123. loss: 0.684, 0.2366 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 12:16:11 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.2309, average loss: 0.7084
[11/01 12:16:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 43.90	rocauc: 56.87	
[11/01 12:17:14 visual_prompt]: 	Test 100/323. loss: 0.827, 0.2246 s / batch. (data: 5.58e-05)max mem: 15.94594 GB 
[11/01 12:18:07 visual_prompt]: 	Test 200/323. loss: 0.649, 0.2247 s / batch. (data: 5.41e-05)max mem: 15.94594 GB 
[11/01 12:19:00 visual_prompt]: 	Test 300/323. loss: 0.719, 0.2244 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[11/01 12:19:11 visual_prompt]: Inference (test):avg data time: 6.74e-05, avg batch time: 0.2302, average loss: 0.7226
[11/01 12:19:11 visual_prompt]: Classification results with test_mammo-cbis: top1: 42.33	rocauc: 56.05	
[11/01 12:19:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/01 12:20:19 visual_prompt]: 	Training 100/1106. train loss: 0.1329,	0.6388 s / batch. (data: 1.52e-02). ETA=18:37:31, max mem: 15.9 GB 
[11/01 12:21:22 visual_prompt]: 	Training 200/1106. train loss: 0.8961,	0.6560 s / batch. (data: 1.11e-02). ETA=19:06:30, max mem: 15.9 GB 
[11/01 12:22:26 visual_prompt]: 	Training 300/1106. train loss: 0.1494,	0.6352 s / batch. (data: 3.50e-04). ETA=18:29:06, max mem: 15.9 GB 
[11/01 12:23:30 visual_prompt]: 	Training 400/1106. train loss: 0.5932,	0.6623 s / batch. (data: 1.57e-02). ETA=19:15:21, max mem: 15.9 GB 
[11/01 12:24:33 visual_prompt]: 	Training 500/1106. train loss: 0.5069,	0.6303 s / batch. (data: 3.36e-04). ETA=18:18:26, max mem: 15.9 GB 
[11/01 12:25:37 visual_prompt]: 	Training 600/1106. train loss: 0.6734,	0.6410 s / batch. (data: 1.36e-02). ETA=18:36:02, max mem: 15.9 GB 
[11/01 12:26:40 visual_prompt]: 	Training 700/1106. train loss: 0.9841,	0.6172 s / batch. (data: 3.33e-04). ETA=17:53:36, max mem: 15.9 GB 
[11/01 12:27:44 visual_prompt]: 	Training 800/1106. train loss: 0.0618,	0.6200 s / batch. (data: 3.57e-04). ETA=17:57:29, max mem: 15.9 GB 
[11/01 12:28:48 visual_prompt]: 	Training 900/1106. train loss: 0.0486,	0.6491 s / batch. (data: 3.48e-04). ETA=18:46:54, max mem: 15.9 GB 
[11/01 12:29:51 visual_prompt]: 	Training 1000/1106. train loss: 0.5926,	0.6190 s / batch. (data: 3.42e-04). ETA=17:53:38, max mem: 15.9 GB 
[11/01 12:30:55 visual_prompt]: 	Training 1100/1106. train loss: 3.2431,	0.6176 s / batch. (data: 1.67e-04). ETA=17:50:10, max mem: 15.9 GB 
[11/01 12:30:59 visual_prompt]: Epoch 6 / 100: avg data time: 7.40e-03, avg batch time: 0.6397, average train loss: 1.0369
[11/01 12:31:53 visual_prompt]: 	Test 100/123. loss: 2.434, 0.2252 s / batch. (data: 5.41e-05)max mem: 15.94594 GB 
[11/01 12:32:05 visual_prompt]: Inference (val):avg data time: 6.59e-05, avg batch time: 0.2318, average loss: 2.1757
[11/01 12:32:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.69	
[11/01 12:33:00 visual_prompt]: 	Test 100/323. loss: 0.008, 0.2358 s / batch. (data: 3.93e-05)max mem: 15.94594 GB 
[11/01 12:33:52 visual_prompt]: 	Test 200/323. loss: 2.233, 0.2318 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[11/01 12:34:43 visual_prompt]: 	Test 300/323. loss: 2.541, 0.2326 s / batch. (data: 5.65e-05)max mem: 15.94594 GB 
[11/01 12:34:54 visual_prompt]: Inference (test):avg data time: 4.62e-05, avg batch time: 0.2327, average loss: 1.9688
[11/01 12:34:54 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.40	
[11/01 12:34:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/01 12:36:01 visual_prompt]: 	Training 100/1106. train loss: 0.8108,	0.6320 s / batch. (data: 3.05e-04). ETA=18:14:01, max mem: 15.9 GB 
[11/01 12:37:05 visual_prompt]: 	Training 200/1106. train loss: 0.6469,	0.6280 s / batch. (data: 3.59e-04). ETA=18:06:02, max mem: 15.9 GB 
[11/01 12:38:08 visual_prompt]: 	Training 300/1106. train loss: 0.5704,	0.6684 s / batch. (data: 4.16e-02). ETA=19:14:49, max mem: 15.9 GB 
[11/01 12:39:12 visual_prompt]: 	Training 400/1106. train loss: 0.1125,	0.6680 s / batch. (data: 7.97e-03). ETA=19:13:00, max mem: 15.9 GB 
[11/01 12:40:16 visual_prompt]: 	Training 500/1106. train loss: 3.0959,	0.6182 s / batch. (data: 3.50e-04). ETA=17:46:00, max mem: 15.9 GB 
[11/01 12:41:20 visual_prompt]: 	Training 600/1106. train loss: 0.8581,	0.6360 s / batch. (data: 3.51e-04). ETA=18:15:37, max mem: 15.9 GB 
[11/01 12:42:23 visual_prompt]: 	Training 700/1106. train loss: 0.3332,	0.6292 s / batch. (data: 1.06e-02). ETA=18:02:50, max mem: 15.9 GB 
[11/01 12:43:27 visual_prompt]: 	Training 800/1106. train loss: 0.6402,	0.6301 s / batch. (data: 5.51e-03). ETA=18:03:23, max mem: 15.9 GB 
[11/01 12:44:30 visual_prompt]: 	Training 900/1106. train loss: 0.7434,	0.6255 s / batch. (data: 3.31e-04). ETA=17:54:26, max mem: 15.9 GB 
[11/01 12:45:34 visual_prompt]: 	Training 1000/1106. train loss: 0.8963,	0.6273 s / batch. (data: 3.32e-04). ETA=17:56:29, max mem: 15.9 GB 
[11/01 12:46:38 visual_prompt]: 	Training 1100/1106. train loss: 0.5048,	0.6189 s / batch. (data: 2.09e-04). ETA=17:41:03, max mem: 15.9 GB 
[11/01 12:46:41 visual_prompt]: Epoch 7 / 100: avg data time: 6.91e-03, avg batch time: 0.6393, average train loss: 1.0358
[11/01 12:47:36 visual_prompt]: 	Test 100/123. loss: 0.780, 0.2254 s / batch. (data: 7.92e-05)max mem: 15.94594 GB 
[11/01 12:47:47 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.2337, average loss: 0.8339
[11/01 12:47:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.12	
[11/01 12:48:42 visual_prompt]: 	Test 100/323. loss: 1.229, 0.2384 s / batch. (data: 5.46e-05)max mem: 15.94594 GB 
[11/01 12:49:34 visual_prompt]: 	Test 200/323. loss: 0.722, 0.2357 s / batch. (data: 5.15e-05)max mem: 15.94594 GB 
[11/01 12:50:26 visual_prompt]: 	Test 300/323. loss: 0.798, 0.2244 s / batch. (data: 4.58e-05)max mem: 15.94594 GB 
[11/01 12:50:36 visual_prompt]: Inference (test):avg data time: 7.05e-05, avg batch time: 0.2320, average loss: 0.8821
[11/01 12:50:37 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.54	
[11/01 12:50:37 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/01 12:51:44 visual_prompt]: 	Training 100/1106. train loss: 0.7323,	0.6400 s / batch. (data: 3.50e-04). ETA=18:16:03, max mem: 15.9 GB 
[11/01 12:52:47 visual_prompt]: 	Training 200/1106. train loss: 0.9522,	0.6533 s / batch. (data: 1.56e-02). ETA=18:37:46, max mem: 15.9 GB 
[11/01 12:53:51 visual_prompt]: 	Training 300/1106. train loss: 0.7041,	0.6480 s / batch. (data: 3.25e-04). ETA=18:27:38, max mem: 15.9 GB 
[11/01 12:54:55 visual_prompt]: 	Training 400/1106. train loss: 0.9597,	0.6337 s / batch. (data: 3.52e-04). ETA=18:02:12, max mem: 15.9 GB 
[11/01 12:55:59 visual_prompt]: 	Training 500/1106. train loss: 1.7713,	0.6525 s / batch. (data: 3.64e-04). ETA=18:33:11, max mem: 15.9 GB 
[11/01 12:57:02 visual_prompt]: 	Training 600/1106. train loss: 1.0314,	0.6360 s / batch. (data: 8.31e-04). ETA=18:03:54, max mem: 15.9 GB 
[11/01 12:58:06 visual_prompt]: 	Training 700/1106. train loss: 1.2939,	0.6402 s / batch. (data: 1.34e-02). ETA=18:10:00, max mem: 15.9 GB 
[11/01 12:59:10 visual_prompt]: 	Training 800/1106. train loss: 2.2178,	0.6447 s / batch. (data: 8.81e-04). ETA=18:16:36, max mem: 15.9 GB 
[11/01 13:00:14 visual_prompt]: 	Training 900/1106. train loss: 1.0206,	0.6421 s / batch. (data: 6.97e-04). ETA=18:11:04, max mem: 15.9 GB 
[11/01 13:01:17 visual_prompt]: 	Training 1000/1106. train loss: 0.2131,	0.6190 s / batch. (data: 3.84e-04). ETA=17:30:54, max mem: 15.9 GB 
[11/01 13:02:21 visual_prompt]: 	Training 1100/1106. train loss: 4.5252,	0.6175 s / batch. (data: 2.20e-04). ETA=17:27:20, max mem: 15.9 GB 
[11/01 13:02:25 visual_prompt]: Epoch 8 / 100: avg data time: 7.30e-03, avg batch time: 0.6401, average train loss: 1.1430
[11/01 13:03:18 visual_prompt]: 	Test 100/123. loss: 2.634, 0.2453 s / batch. (data: 4.84e-05)max mem: 15.94594 GB 
[11/01 13:03:30 visual_prompt]: Inference (val):avg data time: 4.67e-05, avg batch time: 0.2323, average loss: 2.3441
[11/01 13:03:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.82	
[11/01 13:04:25 visual_prompt]: 	Test 100/323. loss: 0.005, 0.2292 s / batch. (data: 3.74e-05)max mem: 15.94594 GB 
[11/01 13:05:17 visual_prompt]: 	Test 200/323. loss: 2.324, 0.2246 s / batch. (data: 4.98e-05)max mem: 15.94594 GB 
[11/01 13:06:09 visual_prompt]: 	Test 300/323. loss: 2.776, 0.2288 s / batch. (data: 4.43e-05)max mem: 15.94594 GB 
[11/01 13:06:20 visual_prompt]: Inference (test):avg data time: 8.28e-05, avg batch time: 0.2316, average loss: 2.1206
[11/01 13:06:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 58.01	
[11/01 13:06:20 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/01 13:07:27 visual_prompt]: 	Training 100/1106. train loss: 0.9872,	0.6300 s / batch. (data: 8.41e-04). ETA=17:47:19, max mem: 15.9 GB 
[11/01 13:08:31 visual_prompt]: 	Training 200/1106. train loss: 1.4042,	0.6400 s / batch. (data: 5.43e-03). ETA=18:03:16, max mem: 15.9 GB 
[11/01 13:09:35 visual_prompt]: 	Training 300/1106. train loss: 1.5513,	0.6356 s / batch. (data: 8.63e-04). ETA=17:54:42, max mem: 15.9 GB 
[11/01 13:10:39 visual_prompt]: 	Training 400/1106. train loss: 1.4783,	0.6191 s / batch. (data: 3.51e-04). ETA=17:25:42, max mem: 15.9 GB 
[11/01 13:11:42 visual_prompt]: 	Training 500/1106. train loss: 0.2305,	0.6441 s / batch. (data: 3.63e-04). ETA=18:06:53, max mem: 15.9 GB 
[11/01 13:12:46 visual_prompt]: 	Training 600/1106. train loss: 1.2377,	0.6243 s / batch. (data: 5.54e-03). ETA=17:32:25, max mem: 15.9 GB 
[11/01 13:13:49 visual_prompt]: 	Training 700/1106. train loss: 1.6772,	0.6210 s / batch. (data: 3.40e-04). ETA=17:25:57, max mem: 15.9 GB 
[11/01 13:14:53 visual_prompt]: 	Training 800/1106. train loss: 0.8499,	0.6338 s / batch. (data: 8.40e-04). ETA=17:46:26, max mem: 15.9 GB 
[11/01 13:15:56 visual_prompt]: 	Training 900/1106. train loss: 1.1806,	0.6336 s / batch. (data: 8.74e-04). ETA=17:45:00, max mem: 15.9 GB 
[11/01 13:17:00 visual_prompt]: 	Training 1000/1106. train loss: 0.5979,	0.6678 s / batch. (data: 8.52e-04). ETA=18:41:23, max mem: 15.9 GB 
[11/01 13:18:03 visual_prompt]: 	Training 1100/1106. train loss: 0.7670,	0.6187 s / batch. (data: 1.79e-04). ETA=17:17:50, max mem: 15.9 GB 
[11/01 13:18:07 visual_prompt]: Epoch 9 / 100: avg data time: 7.52e-03, avg batch time: 0.6392, average train loss: 1.1267
[11/01 13:19:02 visual_prompt]: 	Test 100/123. loss: 1.209, 0.2387 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/01 13:19:13 visual_prompt]: Inference (val):avg data time: 1.38e-04, avg batch time: 0.2324, average loss: 1.3443
[11/01 13:19:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.21	
[11/01 13:20:09 visual_prompt]: 	Test 100/323. loss: 2.324, 0.2456 s / batch. (data: 4.43e-05)max mem: 15.94594 GB 
[11/01 13:21:01 visual_prompt]: 	Test 200/323. loss: 1.174, 0.2277 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[11/01 13:21:53 visual_prompt]: 	Test 300/323. loss: 1.211, 0.2331 s / batch. (data: 7.15e-05)max mem: 15.94594 GB 
[11/01 13:22:05 visual_prompt]: Inference (test):avg data time: 8.56e-05, avg batch time: 0.2317, average loss: 1.4607
[11/01 13:22:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.45	
[11/01 13:22:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/01 13:23:11 visual_prompt]: 	Training 100/1106. train loss: 0.9415,	0.6202 s / batch. (data: 8.29e-04). ETA=17:19:21, max mem: 15.9 GB 
[11/01 13:24:14 visual_prompt]: 	Training 200/1106. train loss: 0.7202,	0.6424 s / batch. (data: 8.70e-04). ETA=17:55:25, max mem: 15.9 GB 
[11/01 13:25:17 visual_prompt]: 	Training 300/1106. train loss: 0.2753,	0.6358 s / batch. (data: 3.57e-04). ETA=17:43:22, max mem: 15.9 GB 
[11/01 13:26:21 visual_prompt]: 	Training 400/1106. train loss: 0.1861,	0.6204 s / batch. (data: 8.42e-04). ETA=17:16:28, max mem: 15.9 GB 
[11/01 13:27:25 visual_prompt]: 	Training 500/1106. train loss: 0.2269,	0.6440 s / batch. (data: 3.11e-04). ETA=17:54:54, max mem: 15.9 GB 
[11/01 13:28:28 visual_prompt]: 	Training 600/1106. train loss: 0.7538,	0.6335 s / batch. (data: 8.28e-04). ETA=17:36:17, max mem: 15.9 GB 
[11/01 13:29:32 visual_prompt]: 	Training 700/1106. train loss: 1.2320,	0.6360 s / batch. (data: 3.19e-04). ETA=17:39:26, max mem: 15.9 GB 
[11/01 13:30:35 visual_prompt]: 	Training 800/1106. train loss: 2.8843,	0.6462 s / batch. (data: 8.40e-04). ETA=17:55:21, max mem: 15.9 GB 
[11/01 13:31:39 visual_prompt]: 	Training 900/1106. train loss: 9.5117,	0.6213 s / batch. (data: 2.38e-04). ETA=17:12:52, max mem: 15.9 GB 
[11/01 13:32:42 visual_prompt]: 	Training 1000/1106. train loss: 1.4888,	0.6210 s / batch. (data: 3.50e-04). ETA=17:11:20, max mem: 15.9 GB 
[11/01 13:33:46 visual_prompt]: 	Training 1100/1106. train loss: 1.8713,	0.6195 s / batch. (data: 2.19e-04). ETA=17:07:48, max mem: 15.9 GB 
[11/01 13:33:50 visual_prompt]: Epoch 10 / 100: avg data time: 5.87e-03, avg batch time: 0.6375, average train loss: 1.1501
[11/01 13:34:45 visual_prompt]: 	Test 100/123. loss: 1.296, 0.2257 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/01 13:34:56 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.2318, average loss: 1.5449
[11/01 13:34:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.77	
[11/01 13:35:51 visual_prompt]: 	Test 100/323. loss: 2.738, 0.2244 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[11/01 13:36:43 visual_prompt]: 	Test 200/323. loss: 1.286, 0.2366 s / batch. (data: 5.84e-05)max mem: 15.94594 GB 
[11/01 13:37:35 visual_prompt]: 	Test 300/323. loss: 1.356, 0.2244 s / batch. (data: 4.27e-05)max mem: 15.94594 GB 
[11/01 13:37:46 visual_prompt]: Inference (test):avg data time: 4.72e-05, avg batch time: 0.2319, average loss: 1.6811
[11/01 13:37:46 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.39	
[11/01 13:37:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/01 13:38:52 visual_prompt]: 	Training 100/1106. train loss: 2.0607,	0.6658 s / batch. (data: 8.37e-04). ETA=18:23:29, max mem: 15.9 GB 
[11/01 13:39:55 visual_prompt]: 	Training 200/1106. train loss: 2.6026,	0.6360 s / batch. (data: 3.48e-04). ETA=17:32:59, max mem: 15.9 GB 
[11/01 13:40:59 visual_prompt]: 	Training 300/1106. train loss: 0.0985,	0.6480 s / batch. (data: 2.85e-03). ETA=17:51:47, max mem: 15.9 GB 
[11/01 13:42:03 visual_prompt]: 	Training 400/1106. train loss: 4.6052,	0.6297 s / batch. (data: 9.94e-03). ETA=17:20:27, max mem: 15.9 GB 
[11/01 13:43:06 visual_prompt]: 	Training 500/1106. train loss: 0.3009,	0.6360 s / batch. (data: 3.50e-04). ETA=17:29:48, max mem: 15.9 GB 
[11/01 13:44:10 visual_prompt]: 	Training 600/1106. train loss: 0.9773,	0.6228 s / batch. (data: 3.67e-04). ETA=17:06:57, max mem: 15.9 GB 
[11/01 13:45:14 visual_prompt]: 	Training 700/1106. train loss: 0.8995,	0.6304 s / batch. (data: 8.43e-04). ETA=17:18:30, max mem: 15.9 GB 
[11/01 13:46:18 visual_prompt]: 	Training 800/1106. train loss: 0.0324,	0.6354 s / batch. (data: 8.67e-04). ETA=17:25:34, max mem: 15.9 GB 
[11/01 13:47:21 visual_prompt]: 	Training 900/1106. train loss: 0.0173,	0.6303 s / batch. (data: 3.59e-04). ETA=17:16:10, max mem: 15.9 GB 
[11/01 13:48:25 visual_prompt]: 	Training 1000/1106. train loss: 0.0974,	0.6340 s / batch. (data: 3.41e-04). ETA=17:21:13, max mem: 15.9 GB 
[11/01 13:49:29 visual_prompt]: 	Training 1100/1106. train loss: 0.8383,	0.6181 s / batch. (data: 2.12e-04). ETA=16:54:01, max mem: 15.9 GB 
[11/01 13:49:32 visual_prompt]: Epoch 11 / 100: avg data time: 5.68e-03, avg batch time: 0.6390, average train loss: 1.1510
[11/01 13:50:26 visual_prompt]: 	Test 100/123. loss: 1.539, 0.2296 s / batch. (data: 4.46e-05)max mem: 15.94594 GB 
[11/01 13:50:38 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.2327, average loss: 1.3557
[11/01 13:50:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.68	
[11/01 13:51:34 visual_prompt]: 	Test 100/323. loss: 0.044, 0.2397 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[11/01 13:52:26 visual_prompt]: 	Test 200/323. loss: 1.059, 0.2317 s / batch. (data: 3.15e-05)max mem: 15.94594 GB 
[11/01 13:53:17 visual_prompt]: 	Test 300/323. loss: 1.657, 0.2254 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[11/01 13:53:28 visual_prompt]: Inference (test):avg data time: 4.61e-05, avg batch time: 0.2325, average loss: 1.2303
[11/01 13:53:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.71	
[11/01 13:53:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/01 13:54:34 visual_prompt]: 	Training 100/1106. train loss: 1.0824,	0.6170 s / batch. (data: 3.35e-04). ETA=16:51:13, max mem: 15.9 GB 
[11/01 13:55:38 visual_prompt]: 	Training 200/1106. train loss: 0.7440,	0.6474 s / batch. (data: 6.06e-03). ETA=17:40:01, max mem: 15.9 GB 
[11/01 13:56:42 visual_prompt]: 	Training 300/1106. train loss: 0.6408,	0.6200 s / batch. (data: 3.64e-04). ETA=16:54:00, max mem: 15.9 GB 
[11/01 13:57:46 visual_prompt]: 	Training 400/1106. train loss: 0.6543,	0.6332 s / batch. (data: 3.77e-04). ETA=17:14:39, max mem: 15.9 GB 
[11/01 13:58:49 visual_prompt]: 	Training 500/1106. train loss: 2.3545,	0.6329 s / batch. (data: 8.47e-04). ETA=17:13:02, max mem: 15.9 GB 
[11/01 13:59:53 visual_prompt]: 	Training 600/1106. train loss: 1.0674,	0.6348 s / batch. (data: 3.59e-04). ETA=17:15:03, max mem: 15.9 GB 
[11/01 14:00:57 visual_prompt]: 	Training 700/1106. train loss: 1.8165,	0.6334 s / batch. (data: 9.00e-04). ETA=17:11:42, max mem: 15.9 GB 
[11/01 14:02:00 visual_prompt]: 	Training 800/1106. train loss: 1.9847,	0.6343 s / batch. (data: 3.43e-04). ETA=17:12:06, max mem: 15.9 GB 
[11/01 14:03:04 visual_prompt]: 	Training 900/1106. train loss: 0.8686,	0.6242 s / batch. (data: 3.71e-04). ETA=16:54:42, max mem: 15.9 GB 
[11/01 14:04:08 visual_prompt]: 	Training 1000/1106. train loss: 1.7810,	0.6362 s / batch. (data: 8.52e-04). ETA=17:13:08, max mem: 15.9 GB 
[11/01 14:05:11 visual_prompt]: 	Training 1100/1106. train loss: 1.9809,	0.6185 s / batch. (data: 2.46e-04). ETA=16:43:17, max mem: 15.9 GB 
[11/01 14:05:15 visual_prompt]: Epoch 12 / 100: avg data time: 6.26e-03, avg batch time: 0.6393, average train loss: 1.1037
[11/01 14:06:09 visual_prompt]: 	Test 100/123. loss: 2.626, 0.2326 s / batch. (data: 8.06e-05)max mem: 15.94594 GB 
[11/01 14:06:20 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.2328, average loss: 2.2619
[11/01 14:06:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.89	
[11/01 14:07:15 visual_prompt]: 	Test 100/323. loss: 0.005, 0.2250 s / batch. (data: 3.74e-05)max mem: 15.94594 GB 
[11/01 14:08:07 visual_prompt]: 	Test 200/323. loss: 1.946, 0.2366 s / batch. (data: 5.48e-05)max mem: 15.94594 GB 
[11/01 14:08:59 visual_prompt]: 	Test 300/323. loss: 2.807, 0.2244 s / batch. (data: 7.37e-05)max mem: 15.94594 GB 
[11/01 14:09:10 visual_prompt]: Inference (test):avg data time: 1.53e-04, avg batch time: 0.2317, average loss: 2.0476
[11/01 14:09:10 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.32	
[11/01 14:09:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/01 14:10:16 visual_prompt]: 	Training 100/1106. train loss: 3.9150,	0.6256 s / batch. (data: 3.42e-04). ETA=16:53:43, max mem: 15.9 GB 
[11/01 14:11:20 visual_prompt]: 	Training 200/1106. train loss: 0.2907,	0.6474 s / batch. (data: 9.17e-04). ETA=17:28:04, max mem: 15.9 GB 
[11/01 14:12:24 visual_prompt]: 	Training 300/1106. train loss: 3.1277,	0.6194 s / batch. (data: 3.86e-04). ETA=16:41:34, max mem: 15.9 GB 
[11/01 14:13:27 visual_prompt]: 	Training 400/1106. train loss: 1.2143,	0.6388 s / batch. (data: 8.61e-04). ETA=17:12:00, max mem: 15.9 GB 
[11/01 14:14:31 visual_prompt]: 	Training 500/1106. train loss: 1.0087,	0.6239 s / batch. (data: 3.58e-04). ETA=16:46:53, max mem: 15.9 GB 
[11/01 14:15:35 visual_prompt]: 	Training 600/1106. train loss: 1.6826,	0.6350 s / batch. (data: 1.52e-02). ETA=17:03:38, max mem: 15.9 GB 
[11/01 14:16:38 visual_prompt]: 	Training 700/1106. train loss: 1.7437,	0.6479 s / batch. (data: 9.50e-04). ETA=17:23:22, max mem: 15.9 GB 
[11/01 14:17:42 visual_prompt]: 	Training 800/1106. train loss: 3.3112,	0.6190 s / batch. (data: 3.55e-04). ETA=16:35:53, max mem: 15.9 GB 
[11/01 14:18:46 visual_prompt]: 	Training 900/1106. train loss: 0.7526,	0.6311 s / batch. (data: 1.20e-02). ETA=16:54:13, max mem: 15.9 GB 
[11/01 14:19:50 visual_prompt]: 	Training 1000/1106. train loss: 2.6475,	0.6560 s / batch. (data: 8.35e-04). ETA=17:33:11, max mem: 15.9 GB 
[11/01 14:20:53 visual_prompt]: 	Training 1100/1106. train loss: 2.2932,	0.6184 s / batch. (data: 1.90e-04). ETA=16:31:46, max mem: 15.9 GB 
[11/01 14:20:57 visual_prompt]: Epoch 13 / 100: avg data time: 6.18e-03, avg batch time: 0.6396, average train loss: 1.2426
[11/01 14:21:51 visual_prompt]: 	Test 100/123. loss: 0.808, 0.2368 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[11/01 14:22:03 visual_prompt]: Inference (val):avg data time: 1.06e-04, avg batch time: 0.2329, average loss: 0.7670
[11/01 14:22:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.23	
[11/01 14:22:58 visual_prompt]: 	Test 100/323. loss: 0.245, 0.2300 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/01 14:23:50 visual_prompt]: 	Test 200/323. loss: 0.400, 0.2317 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/01 14:24:41 visual_prompt]: 	Test 300/323. loss: 0.969, 0.2328 s / batch. (data: 5.20e-05)max mem: 15.94594 GB 
[11/01 14:24:52 visual_prompt]: Inference (test):avg data time: 1.10e-04, avg batch time: 0.2320, average loss: 0.7174
[11/01 14:24:52 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.80	
[11/01 14:24:52 visual_prompt]: Best epoch 13: best metric: -0.767
[11/01 14:24:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/01 14:25:58 visual_prompt]: 	Training 100/1106. train loss: 1.1317,	0.6215 s / batch. (data: 9.10e-04). ETA=16:35:39, max mem: 15.9 GB 
[11/01 14:27:02 visual_prompt]: 	Training 200/1106. train loss: 0.4431,	0.6480 s / batch. (data: 8.49e-04). ETA=17:17:01, max mem: 15.9 GB 
[11/01 14:28:05 visual_prompt]: 	Training 300/1106. train loss: 0.9498,	0.6290 s / batch. (data: 1.06e-02). ETA=16:45:35, max mem: 15.9 GB 
[11/01 14:29:09 visual_prompt]: 	Training 400/1106. train loss: 1.5668,	0.6440 s / batch. (data: 4.85e-03). ETA=17:08:30, max mem: 15.9 GB 
[11/01 14:30:13 visual_prompt]: 	Training 500/1106. train loss: 0.3564,	0.6330 s / batch. (data: 8.37e-04). ETA=16:49:55, max mem: 15.9 GB 
[11/01 14:31:17 visual_prompt]: 	Training 600/1106. train loss: 0.3773,	0.6339 s / batch. (data: 8.46e-04). ETA=16:50:12, max mem: 15.9 GB 
[11/01 14:32:20 visual_prompt]: 	Training 700/1106. train loss: 0.0034,	0.6604 s / batch. (data: 1.56e-02). ETA=17:31:21, max mem: 15.9 GB 
[11/01 14:33:24 visual_prompt]: 	Training 800/1106. train loss: 0.6805,	0.6461 s / batch. (data: 8.31e-04). ETA=17:07:35, max mem: 15.9 GB 
[11/01 14:34:28 visual_prompt]: 	Training 900/1106. train loss: 1.9697,	0.6322 s / batch. (data: 8.43e-04). ETA=16:44:24, max mem: 15.9 GB 
[11/01 14:35:32 visual_prompt]: 	Training 1000/1106. train loss: 0.6747,	0.6545 s / batch. (data: 8.44e-04). ETA=17:18:47, max mem: 15.9 GB 
[11/01 14:36:35 visual_prompt]: 	Training 1100/1106. train loss: 2.4757,	0.6323 s / batch. (data: 1.96e-04). ETA=16:42:21, max mem: 15.9 GB 
[11/01 14:36:39 visual_prompt]: Epoch 14 / 100: avg data time: 5.36e-03, avg batch time: 0.6387, average train loss: 1.1794
[11/01 14:37:33 visual_prompt]: 	Test 100/123. loss: 2.890, 0.2497 s / batch. (data: 5.84e-05)max mem: 15.94594 GB 
[11/01 14:37:45 visual_prompt]: Inference (val):avg data time: 4.58e-05, avg batch time: 0.2328, average loss: 2.6205
[11/01 14:37:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.94	
[11/01 14:38:40 visual_prompt]: 	Test 100/323. loss: 0.002, 0.2245 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[11/01 14:39:32 visual_prompt]: 	Test 200/323. loss: 2.744, 0.2487 s / batch. (data: 6.22e-05)max mem: 15.94594 GB 
[11/01 14:40:24 visual_prompt]: 	Test 300/323. loss: 3.022, 0.2243 s / batch. (data: 4.65e-05)max mem: 15.94594 GB 
[11/01 14:40:34 visual_prompt]: Inference (test):avg data time: 4.59e-05, avg batch time: 0.2322, average loss: 2.3732
[11/01 14:40:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 60.15	
[11/01 14:40:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/01 14:41:40 visual_prompt]: 	Training 100/1106. train loss: 2.2290,	0.6279 s / batch. (data: 1.06e-02). ETA=16:34:20, max mem: 15.9 GB 
[11/01 14:42:43 visual_prompt]: 	Training 200/1106. train loss: 0.0616,	0.6326 s / batch. (data: 3.61e-04). ETA=16:40:43, max mem: 15.9 GB 
[11/01 14:43:47 visual_prompt]: 	Training 300/1106. train loss: 1.9866,	0.6590 s / batch. (data: 8.44e-04). ETA=17:21:21, max mem: 15.9 GB 
[11/01 14:44:51 visual_prompt]: 	Training 400/1106. train loss: 0.5434,	0.6682 s / batch. (data: 1.12e-03). ETA=17:34:48, max mem: 15.9 GB 
[11/01 14:45:55 visual_prompt]: 	Training 500/1106. train loss: 1.7417,	0.6341 s / batch. (data: 5.51e-03). ETA=16:39:54, max mem: 15.9 GB 
[11/01 14:46:58 visual_prompt]: 	Training 600/1106. train loss: 5.4649,	0.6444 s / batch. (data: 8.95e-04). ETA=16:55:10, max mem: 15.9 GB 
[11/01 14:48:02 visual_prompt]: 	Training 700/1106. train loss: 1.6710,	0.6325 s / batch. (data: 8.43e-04). ETA=16:35:16, max mem: 15.9 GB 
[11/01 14:49:06 visual_prompt]: 	Training 800/1106. train loss: 0.8200,	0.6525 s / batch. (data: 1.11e-02). ETA=17:05:36, max mem: 15.9 GB 
[11/01 14:50:09 visual_prompt]: 	Training 900/1106. train loss: 0.9775,	0.6324 s / batch. (data: 5.51e-03). ETA=16:32:58, max mem: 15.9 GB 
[11/01 14:51:13 visual_prompt]: 	Training 1000/1106. train loss: 1.0435,	0.6191 s / batch. (data: 8.45e-04). ETA=16:11:06, max mem: 15.9 GB 
[11/01 14:52:16 visual_prompt]: 	Training 1100/1106. train loss: 1.3105,	0.6193 s / batch. (data: 2.03e-04). ETA=16:10:27, max mem: 15.9 GB 
[11/01 14:52:20 visual_prompt]: Epoch 15 / 100: avg data time: 5.08e-03, avg batch time: 0.6381, average train loss: 1.2561
[11/01 14:53:15 visual_prompt]: 	Test 100/123. loss: 1.328, 0.2345 s / batch. (data: 5.08e-05)max mem: 15.94594 GB 
[11/01 14:53:26 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.2323, average loss: 1.2553
[11/01 14:53:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.49	
[11/01 14:54:21 visual_prompt]: 	Test 100/323. loss: 0.054, 0.2247 s / batch. (data: 7.15e-05)max mem: 15.94594 GB 
[11/01 14:55:13 visual_prompt]: 	Test 200/323. loss: 0.962, 0.2278 s / batch. (data: 3.29e-05)max mem: 15.94594 GB 
[11/01 14:56:04 visual_prompt]: 	Test 300/323. loss: 1.671, 0.2302 s / batch. (data: 4.41e-05)max mem: 15.94594 GB 
[11/01 14:56:16 visual_prompt]: Inference (test):avg data time: 1.79e-04, avg batch time: 0.2322, average loss: 1.1408
[11/01 14:56:16 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.80	
[11/01 14:56:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/01 14:57:23 visual_prompt]: 	Training 100/1106. train loss: 0.4662,	0.6195 s / batch. (data: 3.60e-04). ETA=16:09:39, max mem: 15.9 GB 
[11/01 14:58:27 visual_prompt]: 	Training 200/1106. train loss: 0.6841,	0.6189 s / batch. (data: 3.13e-04). ETA=16:07:35, max mem: 15.9 GB 
[11/01 14:59:30 visual_prompt]: 	Training 300/1106. train loss: 1.2020,	0.6531 s / batch. (data: 1.30e-02). ETA=16:59:57, max mem: 15.9 GB 
[11/01 15:00:34 visual_prompt]: 	Training 400/1106. train loss: 0.1362,	0.6557 s / batch. (data: 1.58e-02). ETA=17:03:04, max mem: 15.9 GB 
[11/01 15:01:38 visual_prompt]: 	Training 500/1106. train loss: 0.7279,	0.6316 s / batch. (data: 8.77e-04). ETA=16:24:21, max mem: 15.9 GB 
[11/01 15:02:42 visual_prompt]: 	Training 600/1106. train loss: 0.6936,	0.6273 s / batch. (data: 3.87e-04). ETA=16:16:33, max mem: 15.9 GB 
[11/01 15:03:45 visual_prompt]: 	Training 700/1106. train loss: 0.6867,	0.6238 s / batch. (data: 3.19e-04). ETA=16:10:05, max mem: 15.9 GB 
[11/01 15:04:49 visual_prompt]: 	Training 800/1106. train loss: 1.7983,	0.6188 s / batch. (data: 3.71e-04). ETA=16:01:18, max mem: 15.9 GB 
[11/01 15:05:53 visual_prompt]: 	Training 900/1106. train loss: 0.5487,	0.6224 s / batch. (data: 7.80e-04). ETA=16:05:52, max mem: 15.9 GB 
[11/01 15:06:56 visual_prompt]: 	Training 1000/1106. train loss: 0.0342,	0.6520 s / batch. (data: 8.58e-04). ETA=16:50:41, max mem: 15.9 GB 
[11/01 15:08:00 visual_prompt]: 	Training 1100/1106. train loss: 0.3580,	0.6179 s / batch. (data: 2.06e-04). ETA=15:56:48, max mem: 15.9 GB 
[11/01 15:08:04 visual_prompt]: Epoch 16 / 100: avg data time: 6.91e-03, avg batch time: 0.6402, average train loss: 1.0673
[11/01 15:08:59 visual_prompt]: 	Test 100/123. loss: 1.301, 0.2254 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[11/01 15:09:09 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.2336, average loss: 1.4865
[11/01 15:09:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.36	
[11/01 15:10:04 visual_prompt]: 	Test 100/323. loss: 2.696, 0.2385 s / batch. (data: 6.06e-05)max mem: 15.94594 GB 
[11/01 15:10:56 visual_prompt]: 	Test 200/323. loss: 1.182, 0.2326 s / batch. (data: 6.53e-05)max mem: 15.94594 GB 
[11/01 15:11:48 visual_prompt]: 	Test 300/323. loss: 1.272, 0.2485 s / batch. (data: 4.48e-05)max mem: 15.94594 GB 
[11/01 15:11:59 visual_prompt]: Inference (test):avg data time: 1.44e-04, avg batch time: 0.2320, average loss: 1.6281
[11/01 15:11:59 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 60.34	
[11/01 15:11:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/01 15:13:05 visual_prompt]: 	Training 100/1106. train loss: 0.8895,	0.6345 s / batch. (data: 1.45e-02). ETA=16:21:26, max mem: 15.9 GB 
[11/01 15:14:08 visual_prompt]: 	Training 200/1106. train loss: 1.8399,	0.6233 s / batch. (data: 3.31e-04). ETA=16:03:03, max mem: 15.9 GB 
[11/01 15:15:12 visual_prompt]: 	Training 300/1106. train loss: 0.7997,	0.6320 s / batch. (data: 3.51e-04). ETA=16:15:26, max mem: 15.9 GB 
[11/01 15:16:16 visual_prompt]: 	Training 400/1106. train loss: 0.6005,	0.6468 s / batch. (data: 1.22e-03). ETA=16:37:07, max mem: 15.9 GB 
[11/01 15:17:20 visual_prompt]: 	Training 500/1106. train loss: 1.5966,	0.6335 s / batch. (data: 8.71e-04). ETA=16:15:42, max mem: 15.9 GB 
[11/01 15:18:24 visual_prompt]: 	Training 600/1106. train loss: 0.7316,	0.6218 s / batch. (data: 3.54e-04). ETA=15:56:30, max mem: 15.9 GB 
[11/01 15:19:28 visual_prompt]: 	Training 700/1106. train loss: 0.2234,	0.6200 s / batch. (data: 3.15e-04). ETA=15:52:43, max mem: 15.9 GB 
[11/01 15:20:31 visual_prompt]: 	Training 800/1106. train loss: 1.0051,	0.6537 s / batch. (data: 2.57e-02). ETA=16:43:26, max mem: 15.9 GB 
[11/01 15:21:35 visual_prompt]: 	Training 900/1106. train loss: 1.5080,	0.6351 s / batch. (data: 3.53e-04). ETA=16:13:51, max mem: 15.9 GB 
[11/01 15:22:38 visual_prompt]: 	Training 1000/1106. train loss: 1.5374,	0.6294 s / batch. (data: 6.03e-03). ETA=16:04:01, max mem: 15.9 GB 
[11/01 15:23:42 visual_prompt]: 	Training 1100/1106. train loss: 0.3033,	0.6200 s / batch. (data: 2.18e-04). ETA=15:48:35, max mem: 15.9 GB 
[11/01 15:23:46 visual_prompt]: Epoch 17 / 100: avg data time: 5.94e-03, avg batch time: 0.6394, average train loss: 1.1143
[11/01 15:24:41 visual_prompt]: 	Test 100/123. loss: 0.642, 0.2359 s / batch. (data: 7.17e-03)max mem: 15.94594 GB 
[11/01 15:24:52 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.2347, average loss: 0.7018
[11/01 15:24:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 59.26	
[11/01 15:25:47 visual_prompt]: 	Test 100/323. loss: 0.786, 0.2476 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[11/01 15:26:39 visual_prompt]: 	Test 200/323. loss: 0.493, 0.2249 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/01 15:27:30 visual_prompt]: 	Test 300/323. loss: 0.741, 0.2432 s / batch. (data: 4.67e-05)max mem: 15.94594 GB 
[11/01 15:27:41 visual_prompt]: Inference (test):avg data time: 1.65e-04, avg batch time: 0.2321, average loss: 0.7297
[11/01 15:27:41 visual_prompt]: Classification results with test_mammo-cbis: top1: 48.99	rocauc: 57.90	
[11/01 15:27:41 visual_prompt]: Best epoch 17: best metric: -0.702
[11/01 15:27:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/01 15:28:48 visual_prompt]: 	Training 100/1106. train loss: 0.2133,	0.6540 s / batch. (data: 5.96e-03). ETA=16:39:26, max mem: 15.9 GB 
[11/01 15:29:51 visual_prompt]: 	Training 200/1106. train loss: 0.2471,	0.6237 s / batch. (data: 3.87e-04). ETA=15:52:10, max mem: 15.9 GB 
[11/01 15:30:55 visual_prompt]: 	Training 300/1106. train loss: 2.2292,	0.6356 s / batch. (data: 8.64e-04). ETA=16:09:18, max mem: 15.9 GB 
[11/01 15:31:58 visual_prompt]: 	Training 400/1106. train loss: 0.8983,	0.6436 s / batch. (data: 3.58e-04). ETA=16:20:23, max mem: 15.9 GB 
[11/01 15:33:02 visual_prompt]: 	Training 500/1106. train loss: 1.3115,	0.6462 s / batch. (data: 3.78e-04). ETA=16:23:18, max mem: 15.9 GB 
[11/01 15:34:06 visual_prompt]: 	Training 600/1106. train loss: 0.1124,	0.6480 s / batch. (data: 8.45e-04). ETA=16:24:55, max mem: 15.9 GB 
[11/01 15:35:10 visual_prompt]: 	Training 700/1106. train loss: 0.0359,	0.6397 s / batch. (data: 5.51e-03). ETA=16:11:17, max mem: 15.9 GB 
[11/01 15:36:14 visual_prompt]: 	Training 800/1106. train loss: 2.3601,	0.6393 s / batch. (data: 3.91e-04). ETA=16:09:37, max mem: 15.9 GB 
[11/01 15:37:18 visual_prompt]: 	Training 900/1106. train loss: 0.5871,	0.6203 s / batch. (data: 3.29e-04). ETA=15:39:44, max mem: 15.9 GB 
[11/01 15:38:21 visual_prompt]: 	Training 1000/1106. train loss: 1.3834,	0.6728 s / batch. (data: 6.08e-03). ETA=16:58:11, max mem: 15.9 GB 
[11/01 15:39:25 visual_prompt]: 	Training 1100/1106. train loss: 0.7519,	0.6195 s / batch. (data: 3.73e-04). ETA=15:36:25, max mem: 15.9 GB 
[11/01 15:39:29 visual_prompt]: Epoch 18 / 100: avg data time: 5.92e-03, avg batch time: 0.6396, average train loss: 1.0707
[11/01 15:40:23 visual_prompt]: 	Test 100/123. loss: 0.799, 0.2477 s / batch. (data: 3.84e-05)max mem: 15.94594 GB 
[11/01 15:40:34 visual_prompt]: Inference (val):avg data time: 2.35e-04, avg batch time: 0.2338, average loss: 0.8004
[11/01 15:40:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 61.59	
[11/01 15:41:29 visual_prompt]: 	Test 100/323. loss: 0.210, 0.2245 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[11/01 15:42:21 visual_prompt]: 	Test 200/323. loss: 0.422, 0.2286 s / batch. (data: 4.89e-05)max mem: 15.94594 GB 
[11/01 15:43:13 visual_prompt]: 	Test 300/323. loss: 1.054, 0.2284 s / batch. (data: 5.89e-05)max mem: 15.94594 GB 
[11/01 15:43:24 visual_prompt]: Inference (test):avg data time: 8.94e-05, avg batch time: 0.2323, average loss: 0.7593
[11/01 15:43:24 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.84	rocauc: 58.68	
[11/01 15:43:24 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/01 15:44:30 visual_prompt]: 	Training 100/1106. train loss: 0.6172,	0.6703 s / batch. (data: 5.98e-03). ETA=16:52:08, max mem: 15.9 GB 
[11/01 15:45:34 visual_prompt]: 	Training 200/1106. train loss: 2.4302,	0.6291 s / batch. (data: 8.20e-04). ETA=15:48:50, max mem: 15.9 GB 
[11/01 15:46:38 visual_prompt]: 	Training 300/1106. train loss: 1.1257,	0.6678 s / batch. (data: 6.00e-03). ETA=16:46:07, max mem: 15.9 GB 
[11/01 15:47:41 visual_prompt]: 	Training 400/1106. train loss: 0.2224,	0.6435 s / batch. (data: 5.99e-03). ETA=16:08:25, max mem: 15.9 GB 
[11/01 15:48:45 visual_prompt]: 	Training 500/1106. train loss: 0.3543,	0.6280 s / batch. (data: 7.97e-03). ETA=15:44:01, max mem: 15.9 GB 
[11/01 15:49:49 visual_prompt]: 	Training 600/1106. train loss: 0.3579,	0.6399 s / batch. (data: 5.51e-03). ETA=16:00:51, max mem: 15.9 GB 
[11/01 15:50:52 visual_prompt]: 	Training 700/1106. train loss: 0.0079,	0.6520 s / batch. (data: 8.55e-04). ETA=16:17:52, max mem: 15.9 GB 
[11/01 15:51:56 visual_prompt]: 	Training 800/1106. train loss: 0.5902,	0.6400 s / batch. (data: 8.40e-04). ETA=15:58:52, max mem: 15.9 GB 
[11/01 15:52:59 visual_prompt]: 	Training 900/1106. train loss: 0.3461,	0.6332 s / batch. (data: 3.72e-04). ETA=15:47:38, max mem: 15.9 GB 
[11/01 15:54:03 visual_prompt]: 	Training 1000/1106. train loss: 1.8003,	0.6302 s / batch. (data: 4.21e-04). ETA=15:42:08, max mem: 15.9 GB 
[11/01 15:55:07 visual_prompt]: 	Training 1100/1106. train loss: 0.1227,	0.6195 s / batch. (data: 2.23e-04). ETA=15:24:59, max mem: 15.9 GB 
[11/01 15:55:11 visual_prompt]: Epoch 19 / 100: avg data time: 5.79e-03, avg batch time: 0.6390, average train loss: 0.9981
[11/01 15:56:05 visual_prompt]: 	Test 100/123. loss: 0.983, 0.2276 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[11/01 15:56:16 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.2337, average loss: 1.0845
[11/01 15:56:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.53	
[11/01 15:57:11 visual_prompt]: 	Test 100/323. loss: 1.813, 0.2249 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/01 15:58:03 visual_prompt]: 	Test 200/323. loss: 0.851, 0.2245 s / batch. (data: 4.36e-05)max mem: 15.94594 GB 
[11/01 15:58:54 visual_prompt]: 	Test 300/323. loss: 1.015, 0.2276 s / batch. (data: 3.91e-05)max mem: 15.94594 GB 
[11/01 15:59:05 visual_prompt]: Inference (test):avg data time: 2.12e-04, avg batch time: 0.2330, average loss: 1.1914
[11/01 15:59:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 59.41	
[11/01 15:59:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[11/01 16:00:12 visual_prompt]: 	Training 100/1106. train loss: 1.6581,	0.6374 s / batch. (data: 3.78e-04). ETA=15:50:42, max mem: 15.9 GB 
[11/01 16:01:16 visual_prompt]: 	Training 200/1106. train loss: 1.1355,	0.6352 s / batch. (data: 8.45e-04). ETA=15:46:14, max mem: 15.9 GB 
[11/01 16:02:19 visual_prompt]: 	Training 300/1106. train loss: 1.1152,	0.6202 s / batch. (data: 5.72e-04). ETA=15:22:53, max mem: 15.9 GB 
[11/01 16:03:23 visual_prompt]: 	Training 400/1106. train loss: 0.6759,	0.6318 s / batch. (data: 3.41e-04). ETA=15:39:06, max mem: 15.9 GB 
[11/01 16:04:27 visual_prompt]: 	Training 500/1106. train loss: 2.7660,	0.6536 s / batch. (data: 3.28e-03). ETA=16:10:26, max mem: 15.9 GB 
[11/01 16:05:31 visual_prompt]: 	Training 600/1106. train loss: 0.7020,	0.6280 s / batch. (data: 3.13e-04). ETA=15:31:23, max mem: 15.9 GB 
[11/01 16:06:34 visual_prompt]: 	Training 700/1106. train loss: 0.2609,	0.6305 s / batch. (data: 8.73e-04). ETA=15:33:58, max mem: 15.9 GB 
[11/01 16:07:38 visual_prompt]: 	Training 800/1106. train loss: 0.5501,	0.6490 s / batch. (data: 8.71e-04). ETA=16:00:20, max mem: 15.9 GB 
[11/01 16:08:42 visual_prompt]: 	Training 900/1106. train loss: 0.0322,	0.6482 s / batch. (data: 8.63e-04). ETA=15:58:09, max mem: 15.9 GB 
[11/01 16:09:46 visual_prompt]: 	Training 1000/1106. train loss: 0.6977,	0.6233 s / batch. (data: 3.57e-04). ETA=15:20:16, max mem: 15.9 GB 
[11/01 16:10:49 visual_prompt]: 	Training 1100/1106. train loss: 1.4902,	0.6182 s / batch. (data: 2.19e-04). ETA=15:11:44, max mem: 15.9 GB 
[11/01 16:10:53 visual_prompt]: Epoch 20 / 100: avg data time: 6.30e-03, avg batch time: 0.6399, average train loss: 1.0227
[11/01 16:11:47 visual_prompt]: 	Test 100/123. loss: 0.751, 0.2257 s / batch. (data: 3.72e-05)max mem: 15.94594 GB 
[11/01 16:11:59 visual_prompt]: Inference (val):avg data time: 2.66e-04, avg batch time: 0.2337, average loss: 0.7350
[11/01 16:11:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 60.31	
[11/01 16:12:54 visual_prompt]: 	Test 100/323. loss: 0.955, 0.2487 s / batch. (data: 6.10e-05)max mem: 15.94594 GB 
[11/01 16:13:46 visual_prompt]: 	Test 200/323. loss: 0.576, 0.2250 s / batch. (data: 3.41e-05)max mem: 15.94594 GB 
[11/01 16:14:38 visual_prompt]: 	Test 300/323. loss: 0.592, 0.2244 s / batch. (data: 4.86e-05)max mem: 15.94594 GB 
[11/01 16:14:49 visual_prompt]: Inference (test):avg data time: 4.79e-05, avg batch time: 0.2325, average loss: 0.7717
[11/01 16:14:49 visual_prompt]: Classification results with test_mammo-cbis: top1: 48.53	rocauc: 60.51	
[11/01 16:14:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[11/01 16:15:55 visual_prompt]: 	Training 100/1106. train loss: 0.6859,	0.6252 s / batch. (data: 3.44e-04). ETA=15:20:56, max mem: 15.9 GB 
[11/01 16:16:59 visual_prompt]: 	Training 200/1106. train loss: 1.0983,	0.6554 s / batch. (data: 1.59e-02). ETA=16:04:21, max mem: 15.9 GB 
[11/01 16:18:03 visual_prompt]: 	Training 300/1106. train loss: 2.3473,	0.6439 s / batch. (data: 1.36e-03). ETA=15:46:21, max mem: 15.9 GB 
[11/01 16:19:07 visual_prompt]: 	Training 400/1106. train loss: 0.1057,	0.6359 s / batch. (data: 3.24e-04). ETA=15:33:33, max mem: 15.9 GB 
[11/01 16:20:10 visual_prompt]: 	Training 500/1106. train loss: 0.9662,	0.6625 s / batch. (data: 8.35e-04). ETA=16:11:24, max mem: 15.9 GB 
[11/01 16:21:14 visual_prompt]: 	Training 600/1106. train loss: 0.4148,	0.6392 s / batch. (data: 8.33e-04). ETA=15:36:16, max mem: 15.9 GB 
[11/01 16:22:18 visual_prompt]: 	Training 700/1106. train loss: 0.6856,	0.6479 s / batch. (data: 3.61e-04). ETA=15:47:56, max mem: 15.9 GB 
[11/01 16:23:22 visual_prompt]: 	Training 800/1106. train loss: 1.7238,	0.6203 s / batch. (data: 3.53e-04). ETA=15:06:31, max mem: 15.9 GB 
[11/01 16:24:25 visual_prompt]: 	Training 900/1106. train loss: 1.4465,	0.6236 s / batch. (data: 3.61e-04). ETA=15:10:13, max mem: 15.9 GB 
[11/01 16:25:29 visual_prompt]: 	Training 1000/1106. train loss: 0.0806,	0.6490 s / batch. (data: 8.98e-04). ETA=15:46:14, max mem: 15.9 GB 
[11/01 16:26:33 visual_prompt]: 	Training 1100/1106. train loss: 0.0721,	0.6194 s / batch. (data: 2.24e-04). ETA=15:02:03, max mem: 15.9 GB 
[11/01 16:26:36 visual_prompt]: Epoch 21 / 100: avg data time: 6.12e-03, avg batch time: 0.6396, average train loss: 1.0300
[11/01 16:27:31 visual_prompt]: 	Test 100/123. loss: 0.701, 0.2258 s / batch. (data: 4.20e-05)max mem: 15.94594 GB 
[11/01 16:27:42 visual_prompt]: Inference (val):avg data time: 1.39e-04, avg batch time: 0.2337, average loss: 0.6837
[11/01 16:27:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.21	
[11/01 16:28:37 visual_prompt]: 	Test 100/323. loss: 0.704, 0.2408 s / batch. (data: 5.91e-05)max mem: 15.94594 GB 
[11/01 16:29:30 visual_prompt]: 	Test 200/323. loss: 0.383, 0.2250 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[11/01 16:30:22 visual_prompt]: 	Test 300/323. loss: 0.623, 0.2350 s / batch. (data: 8.68e-05)max mem: 15.94594 GB 
[11/01 16:30:33 visual_prompt]: Inference (test):avg data time: 8.04e-05, avg batch time: 0.2313, average loss: 0.7024
[11/01 16:30:33 visual_prompt]: Classification results with test_mammo-cbis: top1: 54.26	rocauc: 62.09	
[11/01 16:30:33 visual_prompt]: Best epoch 21: best metric: -0.684
[11/01 16:30:33 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[11/01 16:31:40 visual_prompt]: 	Training 100/1106. train loss: 0.7237,	0.6299 s / batch. (data: 8.99e-04). ETA=15:16:17, max mem: 15.9 GB 
[11/01 16:32:43 visual_prompt]: 	Training 200/1106. train loss: 1.2327,	0.6380 s / batch. (data: 9.47e-04). ETA=15:26:58, max mem: 15.9 GB 
[11/01 16:33:47 visual_prompt]: 	Training 300/1106. train loss: 2.3054,	0.6420 s / batch. (data: 1.92e-02). ETA=15:31:41, max mem: 15.9 GB 
[11/01 16:34:51 visual_prompt]: 	Training 400/1106. train loss: 1.0131,	0.6203 s / batch. (data: 3.55e-04). ETA=14:59:10, max mem: 15.9 GB 
[11/01 16:35:54 visual_prompt]: 	Training 500/1106. train loss: 0.0327,	0.6304 s / batch. (data: 3.55e-04). ETA=15:12:48, max mem: 15.9 GB 
[11/01 16:36:58 visual_prompt]: 	Training 600/1106. train loss: 1.0245,	0.6186 s / batch. (data: 3.55e-04). ETA=14:54:37, max mem: 15.9 GB 
[11/01 16:38:02 visual_prompt]: 	Training 700/1106. train loss: 0.5004,	0.6344 s / batch. (data: 3.66e-04). ETA=15:16:25, max mem: 15.9 GB 
[11/01 16:39:05 visual_prompt]: 	Training 800/1106. train loss: 0.9627,	0.6186 s / batch. (data: 3.18e-04). ETA=14:52:35, max mem: 15.9 GB 
[11/01 16:40:09 visual_prompt]: 	Training 900/1106. train loss: 1.9020,	0.6340 s / batch. (data: 8.90e-04). ETA=15:13:46, max mem: 15.9 GB 
[11/01 16:41:13 visual_prompt]: 	Training 1000/1106. train loss: 0.7731,	0.6639 s / batch. (data: 1.11e-02). ETA=15:55:43, max mem: 15.9 GB 
[11/01 16:42:17 visual_prompt]: 	Training 1100/1106. train loss: 0.5668,	0.6190 s / batch. (data: 1.79e-04). ETA=14:50:04, max mem: 15.9 GB 
[11/01 16:42:21 visual_prompt]: Epoch 22 / 100: avg data time: 6.39e-03, avg batch time: 0.6400, average train loss: 1.0058
[11/01 16:43:15 visual_prompt]: 	Test 100/123. loss: 0.799, 0.2257 s / batch. (data: 3.81e-05)max mem: 15.94594 GB 
[11/01 16:43:27 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.2322, average loss: 0.7197
[11/01 16:43:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.70	
[11/01 16:44:22 visual_prompt]: 	Test 100/323. loss: 0.250, 0.2367 s / batch. (data: 6.44e-05)max mem: 15.94594 GB 
[11/01 16:45:14 visual_prompt]: 	Test 200/323. loss: 0.364, 0.2405 s / batch. (data: 8.13e-05)max mem: 15.94594 GB 
[11/01 16:46:07 visual_prompt]: 	Test 300/323. loss: 0.948, 0.2389 s / batch. (data: 5.10e-05)max mem: 15.94594 GB 
[11/01 16:46:17 visual_prompt]: Inference (test):avg data time: 7.42e-05, avg batch time: 0.2323, average loss: 0.6744
[11/01 16:46:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.24	rocauc: 63.57	
[11/01 16:46:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[11/01 16:47:25 visual_prompt]: 	Training 100/1106. train loss: 0.9483,	0.6480 s / batch. (data: 8.96e-04). ETA=15:30:34, max mem: 15.9 GB 
[11/01 16:48:28 visual_prompt]: 	Training 200/1106. train loss: 1.4941,	0.6573 s / batch. (data: 5.51e-03). ETA=15:42:51, max mem: 15.9 GB 
[11/01 16:49:32 visual_prompt]: 	Training 300/1106. train loss: 0.5065,	0.6360 s / batch. (data: 8.56e-04). ETA=15:11:15, max mem: 15.9 GB 
[11/01 16:50:36 visual_prompt]: 	Training 400/1106. train loss: 2.0443,	0.6351 s / batch. (data: 8.43e-04). ETA=15:08:54, max mem: 15.9 GB 
[11/01 16:51:39 visual_prompt]: 	Training 500/1106. train loss: 0.1176,	0.6185 s / batch. (data: 3.68e-04). ETA=14:44:07, max mem: 15.9 GB 
[11/01 16:52:43 visual_prompt]: 	Training 600/1106. train loss: 0.6283,	0.6335 s / batch. (data: 8.94e-04). ETA=15:04:33, max mem: 15.9 GB 
[11/01 16:53:47 visual_prompt]: 	Training 700/1106. train loss: 0.8559,	0.6511 s / batch. (data: 8.44e-04). ETA=15:28:34, max mem: 15.9 GB 
[11/01 16:54:51 visual_prompt]: 	Training 800/1106. train loss: 1.5455,	0.6331 s / batch. (data: 9.43e-04). ETA=15:01:47, max mem: 15.9 GB 
[11/01 16:55:54 visual_prompt]: 	Training 900/1106. train loss: 1.1298,	0.6425 s / batch. (data: 8.76e-04). ETA=15:14:07, max mem: 15.9 GB 
[11/01 16:56:58 visual_prompt]: 	Training 1000/1106. train loss: 1.6247,	0.6239 s / batch. (data: 3.62e-04). ETA=14:46:38, max mem: 15.9 GB 
[11/01 16:58:02 visual_prompt]: 	Training 1100/1106. train loss: 0.2875,	0.6190 s / batch. (data: 2.48e-04). ETA=14:38:35, max mem: 15.9 GB 
[11/01 16:58:06 visual_prompt]: Epoch 23 / 100: avg data time: 6.67e-03, avg batch time: 0.6407, average train loss: 1.0049
[11/01 16:59:01 visual_prompt]: 	Test 100/123. loss: 0.962, 0.2437 s / batch. (data: 5.15e-05)max mem: 15.94594 GB 
[11/01 16:59:12 visual_prompt]: Inference (val):avg data time: 1.25e-04, avg batch time: 0.2327, average loss: 1.0724
[11/01 16:59:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.79	
[11/01 17:00:08 visual_prompt]: 	Test 100/323. loss: 1.859, 0.2437 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[11/01 17:01:00 visual_prompt]: 	Test 200/323. loss: 0.755, 0.2477 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/01 17:01:52 visual_prompt]: 	Test 300/323. loss: 0.830, 0.2458 s / batch. (data: 4.17e-05)max mem: 15.94594 GB 
[11/01 17:02:04 visual_prompt]: Inference (test):avg data time: 4.82e-05, avg batch time: 0.2321, average loss: 1.1721
[11/01 17:02:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 62.15	
[11/01 17:02:04 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[11/01 17:03:11 visual_prompt]: 	Training 100/1106. train loss: 0.3428,	0.6463 s / batch. (data: 8.90e-04). ETA=15:16:18, max mem: 15.9 GB 
[11/01 17:04:15 visual_prompt]: 	Training 200/1106. train loss: 0.1234,	0.6352 s / batch. (data: 3.62e-04). ETA=14:59:31, max mem: 15.9 GB 
[11/01 17:05:19 visual_prompt]: 	Training 300/1106. train loss: 0.0930,	0.6271 s / batch. (data: 3.34e-04). ETA=14:47:00, max mem: 15.9 GB 
[11/01 17:06:22 visual_prompt]: 	Training 400/1106. train loss: 0.7837,	0.6651 s / batch. (data: 1.63e-02). ETA=15:39:33, max mem: 15.9 GB 
[11/01 17:07:26 visual_prompt]: 	Training 500/1106. train loss: 1.0690,	0.6200 s / batch. (data: 3.45e-04). ETA=14:34:50, max mem: 15.9 GB 
[11/01 17:08:30 visual_prompt]: 	Training 600/1106. train loss: 0.2264,	0.6330 s / batch. (data: 8.28e-04). ETA=14:52:07, max mem: 15.9 GB 
[11/01 17:09:34 visual_prompt]: 	Training 700/1106. train loss: 0.8459,	0.6477 s / batch. (data: 4.15e-04). ETA=15:11:49, max mem: 15.9 GB 
[11/01 17:10:38 visual_prompt]: 	Training 800/1106. train loss: 0.4330,	0.6207 s / batch. (data: 3.49e-04). ETA=14:32:44, max mem: 15.9 GB 
[11/01 17:11:41 visual_prompt]: 	Training 900/1106. train loss: 0.9848,	0.6222 s / batch. (data: 3.13e-04). ETA=14:33:44, max mem: 15.9 GB 
[11/01 17:12:45 visual_prompt]: 	Training 1000/1106. train loss: 0.7335,	0.6400 s / batch. (data: 8.66e-04). ETA=14:57:41, max mem: 15.9 GB 
[11/01 17:13:49 visual_prompt]: 	Training 1100/1106. train loss: 0.0254,	0.6185 s / batch. (data: 2.23e-04). ETA=14:26:35, max mem: 15.9 GB 
[11/01 17:13:53 visual_prompt]: Epoch 24 / 100: avg data time: 6.98e-03, avg batch time: 0.6410, average train loss: 0.9471
[11/01 17:14:47 visual_prompt]: 	Test 100/123. loss: 1.107, 0.2267 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[11/01 17:14:59 visual_prompt]: Inference (val):avg data time: 4.87e-05, avg batch time: 0.2344, average loss: 1.2213
[11/01 17:14:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.21	
[11/01 17:15:54 visual_prompt]: 	Test 100/323. loss: 2.075, 0.2477 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/01 17:16:47 visual_prompt]: 	Test 200/323. loss: 0.945, 0.2317 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[11/01 17:17:39 visual_prompt]: 	Test 300/323. loss: 1.194, 0.2244 s / batch. (data: 4.74e-05)max mem: 15.94594 GB 
[11/01 17:17:50 visual_prompt]: Inference (test):avg data time: 2.14e-04, avg batch time: 0.2324, average loss: 1.3384
[11/01 17:17:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 60.20	
[11/01 17:17:50 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[11/01 17:18:56 visual_prompt]: 	Training 100/1106. train loss: 0.1755,	0.6497 s / batch. (data: 8.82e-04). ETA=15:09:04, max mem: 15.9 GB 
[11/01 17:20:00 visual_prompt]: 	Training 200/1106. train loss: 3.0936,	0.6480 s / batch. (data: 8.73e-04). ETA=15:05:37, max mem: 15.9 GB 
[11/01 17:21:04 visual_prompt]: 	Training 300/1106. train loss: 0.4292,	0.6333 s / batch. (data: 3.36e-04). ETA=14:44:00, max mem: 15.9 GB 
[11/01 17:22:08 visual_prompt]: 	Training 400/1106. train loss: 1.0090,	0.6604 s / batch. (data: 1.61e-02). ETA=15:20:44, max mem: 15.9 GB 
[11/01 17:23:11 visual_prompt]: 	Training 500/1106. train loss: 0.8523,	0.6321 s / batch. (data: 4.10e-04). ETA=14:40:13, max mem: 15.9 GB 
[11/01 17:24:15 visual_prompt]: 	Training 600/1106. train loss: 0.0869,	0.6322 s / batch. (data: 3.75e-04). ETA=14:39:19, max mem: 15.9 GB 
[11/01 17:25:19 visual_prompt]: 	Training 700/1106. train loss: 0.9306,	0.6246 s / batch. (data: 5.52e-03). ETA=14:27:44, max mem: 15.9 GB 
[11/01 17:26:22 visual_prompt]: 	Training 800/1106. train loss: 1.1893,	0.6324 s / batch. (data: 3.50e-04). ETA=14:37:26, max mem: 15.9 GB 
[11/01 17:27:27 visual_prompt]: 	Training 900/1106. train loss: 0.8308,	0.6195 s / batch. (data: 3.70e-04). ETA=14:18:36, max mem: 15.9 GB 
[11/01 17:28:30 visual_prompt]: 	Training 1000/1106. train loss: 0.7930,	0.6360 s / batch. (data: 8.72e-04). ETA=14:40:20, max mem: 15.9 GB 
[11/01 17:29:34 visual_prompt]: 	Training 1100/1106. train loss: 2.5245,	0.6214 s / batch. (data: 2.07e-04). ETA=14:19:11, max mem: 15.9 GB 
[11/01 17:29:38 visual_prompt]: Epoch 25 / 100: avg data time: 6.15e-03, avg batch time: 0.6398, average train loss: 0.9818
[11/01 17:30:33 visual_prompt]: 	Test 100/123. loss: 1.397, 0.2254 s / batch. (data: 5.91e-05)max mem: 15.94594 GB 
[11/01 17:30:44 visual_prompt]: Inference (val):avg data time: 1.95e-04, avg batch time: 0.2327, average loss: 1.2221
[11/01 17:30:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.11	
[11/01 17:31:40 visual_prompt]: 	Test 100/323. loss: 0.061, 0.2348 s / batch. (data: 3.77e-05)max mem: 15.94594 GB 
[11/01 17:32:33 visual_prompt]: 	Test 200/323. loss: 0.718, 0.2268 s / batch. (data: 5.25e-05)max mem: 15.94594 GB 
[11/01 17:33:24 visual_prompt]: 	Test 300/323. loss: 1.656, 0.2429 s / batch. (data: 5.05e-05)max mem: 15.94594 GB 
[11/01 17:33:36 visual_prompt]: Inference (test):avg data time: 7.37e-05, avg batch time: 0.2320, average loss: 1.1054
[11/01 17:33:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 61.84	
[11/01 17:33:36 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[11/01 17:34:43 visual_prompt]: 	Training 100/1106. train loss: 0.0980,	0.6303 s / batch. (data: 8.06e-04). ETA=14:30:19, max mem: 15.9 GB 
[11/01 17:35:46 visual_prompt]: 	Training 200/1106. train loss: 1.3420,	0.6350 s / batch. (data: 8.98e-04). ETA=14:35:49, max mem: 15.9 GB 
[11/01 17:36:50 visual_prompt]: 	Training 300/1106. train loss: 0.2227,	0.6197 s / batch. (data: 4.05e-04). ETA=14:13:40, max mem: 15.9 GB 
[11/01 17:37:53 visual_prompt]: 	Training 400/1106. train loss: 4.2503,	0.6480 s / batch. (data: 7.98e-03). ETA=14:51:31, max mem: 15.9 GB 
[11/01 17:38:57 visual_prompt]: 	Training 500/1106. train loss: 0.8472,	0.6318 s / batch. (data: 3.48e-04). ETA=14:28:08, max mem: 15.9 GB 
[11/01 17:40:01 visual_prompt]: 	Training 600/1106. train loss: 1.4446,	0.6185 s / batch. (data: 3.07e-04). ETA=14:08:53, max mem: 15.9 GB 
[11/01 17:41:04 visual_prompt]: 	Training 700/1106. train loss: 4.3397,	0.6321 s / batch. (data: 3.51e-04). ETA=14:26:31, max mem: 15.9 GB 
[11/01 17:42:08 visual_prompt]: 	Training 800/1106. train loss: 0.8646,	0.6487 s / batch. (data: 8.83e-04). ETA=14:48:10, max mem: 15.9 GB 
[11/01 17:43:12 visual_prompt]: 	Training 900/1106. train loss: 0.5726,	0.6530 s / batch. (data: 8.49e-04). ETA=14:53:00, max mem: 15.9 GB 
[11/01 17:44:16 visual_prompt]: 	Training 1000/1106. train loss: 0.1389,	0.6235 s / batch. (data: 3.65e-04). ETA=14:11:36, max mem: 15.9 GB 
[11/01 17:45:20 visual_prompt]: 	Training 1100/1106. train loss: 0.7203,	0.6194 s / batch. (data: 2.23e-04). ETA=14:05:00, max mem: 15.9 GB 
[11/01 17:45:24 visual_prompt]: Epoch 26 / 100: avg data time: 6.71e-03, avg batch time: 0.6400, average train loss: 1.0713
[11/01 17:46:17 visual_prompt]: 	Test 100/123. loss: 0.857, 0.2465 s / batch. (data: 4.96e-05)max mem: 15.94594 GB 
[11/01 17:46:29 visual_prompt]: Inference (val):avg data time: 5.06e-05, avg batch time: 0.2343, average loss: 0.8250
[11/01 17:46:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 62.97	
[11/01 17:47:24 visual_prompt]: 	Test 100/323. loss: 0.150, 0.2366 s / batch. (data: 5.79e-05)max mem: 15.94594 GB 
[11/01 17:48:16 visual_prompt]: 	Test 200/323. loss: 0.401, 0.2286 s / batch. (data: 5.91e-05)max mem: 15.94594 GB 
[11/01 17:49:08 visual_prompt]: 	Test 300/323. loss: 1.171, 0.2481 s / batch. (data: 5.94e-05)max mem: 15.94594 GB 
[11/01 17:49:19 visual_prompt]: Inference (test):avg data time: 1.28e-04, avg batch time: 0.2324, average loss: 0.7786
[11/01 17:49:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.00	rocauc: 60.03	
[11/01 17:49:19 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[11/01 17:50:25 visual_prompt]: 	Training 100/1106. train loss: 0.5361,	0.6220 s / batch. (data: 5.52e-03). ETA=14:07:26, max mem: 15.9 GB 
[11/01 17:51:29 visual_prompt]: 	Training 200/1106. train loss: 1.5146,	0.6317 s / batch. (data: 9.64e-04). ETA=14:19:32, max mem: 15.9 GB 
[11/01 17:52:32 visual_prompt]: 	Training 300/1106. train loss: 0.0307,	0.6313 s / batch. (data: 3.18e-04). ETA=14:17:56, max mem: 15.9 GB 
[11/01 17:53:36 visual_prompt]: 	Training 400/1106. train loss: 0.4200,	0.6331 s / batch. (data: 8.67e-04). ETA=14:19:22, max mem: 15.9 GB 
[11/01 17:54:39 visual_prompt]: 	Training 500/1106. train loss: 1.1523,	0.6418 s / batch. (data: 9.77e-04). ETA=14:30:06, max mem: 15.9 GB 
[11/01 17:55:43 visual_prompt]: 	Training 600/1106. train loss: 6.2824,	0.6539 s / batch. (data: 8.85e-04). ETA=14:45:22, max mem: 15.9 GB 
[11/01 17:56:47 visual_prompt]: 	Training 700/1106. train loss: 1.6008,	0.6495 s / batch. (data: 8.61e-04). ETA=14:38:21, max mem: 15.9 GB 
[11/01 17:57:51 visual_prompt]: 	Training 800/1106. train loss: 0.8981,	0.6480 s / batch. (data: 8.68e-04). ETA=14:35:16, max mem: 15.9 GB 
[11/01 17:58:54 visual_prompt]: 	Training 900/1106. train loss: 1.2388,	0.6514 s / batch. (data: 8.73e-04). ETA=14:38:45, max mem: 15.9 GB 
[11/01 17:59:58 visual_prompt]: 	Training 1000/1106. train loss: 0.5620,	0.6600 s / batch. (data: 8.84e-04). ETA=14:49:17, max mem: 15.9 GB 
[11/01 18:01:02 visual_prompt]: 	Training 1100/1106. train loss: 0.6113,	0.6199 s / batch. (data: 2.07e-04). ETA=13:54:12, max mem: 15.9 GB 
[11/01 18:01:06 visual_prompt]: Epoch 27 / 100: avg data time: 5.63e-03, avg batch time: 0.6393, average train loss: 1.0797
[11/01 18:02:00 visual_prompt]: 	Test 100/123. loss: 0.673, 0.2257 s / batch. (data: 5.22e-05)max mem: 15.94594 GB 
[11/01 18:02:11 visual_prompt]: Inference (val):avg data time: 5.04e-05, avg batch time: 0.2333, average loss: 0.6880
[11/01 18:02:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 63.37	
[11/01 18:03:06 visual_prompt]: 	Test 100/323. loss: 0.618, 0.2282 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/01 18:03:59 visual_prompt]: 	Test 200/323. loss: 0.332, 0.2357 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[11/01 18:04:50 visual_prompt]: 	Test 300/323. loss: 0.708, 0.2246 s / batch. (data: 5.05e-05)max mem: 15.94594 GB 
[11/01 18:05:01 visual_prompt]: Inference (test):avg data time: 1.01e-04, avg batch time: 0.2319, average loss: 0.7130
[11/01 18:05:01 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.43	rocauc: 62.39	
[11/01 18:05:01 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[11/01 18:06:07 visual_prompt]: 	Training 100/1106. train loss: 1.5352,	0.6328 s / batch. (data: 8.80e-04). ETA=14:10:27, max mem: 15.9 GB 
[11/01 18:07:10 visual_prompt]: 	Training 200/1106. train loss: 1.0627,	0.6338 s / batch. (data: 3.50e-04). ETA=14:10:46, max mem: 15.9 GB 
[11/01 18:08:14 visual_prompt]: 	Training 300/1106. train loss: 2.5740,	0.6360 s / batch. (data: 3.37e-04). ETA=14:12:36, max mem: 15.9 GB 
[11/01 18:09:18 visual_prompt]: 	Training 400/1106. train loss: 1.0719,	0.6338 s / batch. (data: 9.69e-04). ETA=14:08:36, max mem: 15.9 GB 
[11/01 18:10:22 visual_prompt]: 	Training 500/1106. train loss: 1.2048,	0.6330 s / batch. (data: 9.13e-04). ETA=14:06:26, max mem: 15.9 GB 
[11/01 18:11:26 visual_prompt]: 	Training 600/1106. train loss: 1.1463,	0.6350 s / batch. (data: 8.82e-04). ETA=14:08:10, max mem: 15.9 GB 
[11/01 18:12:30 visual_prompt]: 	Training 700/1106. train loss: 0.0919,	0.6479 s / batch. (data: 3.26e-04). ETA=14:24:15, max mem: 15.9 GB 
[11/01 18:13:34 visual_prompt]: 	Training 800/1106. train loss: 1.6046,	0.6720 s / batch. (data: 8.77e-04). ETA=14:55:18, max mem: 15.9 GB 
[11/01 18:14:38 visual_prompt]: 	Training 900/1106. train loss: 0.5713,	0.6439 s / batch. (data: 4.88e-04). ETA=14:16:49, max mem: 15.9 GB 
[11/01 18:15:41 visual_prompt]: 	Training 1000/1106. train loss: 0.6841,	0.6280 s / batch. (data: 3.58e-04). ETA=13:54:34, max mem: 15.9 GB 
[11/01 18:16:45 visual_prompt]: 	Training 1100/1106. train loss: 1.1393,	0.6192 s / batch. (data: 1.65e-04). ETA=13:41:54, max mem: 15.9 GB 
[11/01 18:16:49 visual_prompt]: Epoch 28 / 100: avg data time: 5.11e-03, avg batch time: 0.6401, average train loss: 0.9359
[11/01 18:17:44 visual_prompt]: 	Test 100/123. loss: 0.810, 0.2397 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[11/01 18:17:55 visual_prompt]: Inference (val):avg data time: 4.84e-05, avg batch time: 0.2330, average loss: 0.6792
[11/01 18:17:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 62.78	
[11/01 18:18:50 visual_prompt]: 	Test 100/323. loss: 0.397, 0.2366 s / batch. (data: 5.98e-05)max mem: 15.94594 GB 
[11/01 18:19:43 visual_prompt]: 	Test 200/323. loss: 0.270, 0.2486 s / batch. (data: 6.46e-05)max mem: 15.94594 GB 
[11/01 18:20:34 visual_prompt]: 	Test 300/323. loss: 0.612, 0.2246 s / batch. (data: 5.13e-05)max mem: 15.94594 GB 
[11/01 18:20:45 visual_prompt]: Inference (test):avg data time: 1.62e-04, avg batch time: 0.2323, average loss: 0.6555
[11/01 18:20:45 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.62	rocauc: 64.27	
[11/01 18:20:45 visual_prompt]: Best epoch 28: best metric: -0.679
[11/01 18:20:45 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[11/01 18:21:52 visual_prompt]: 	Training 100/1106. train loss: 0.3074,	0.6315 s / batch. (data: 8.81e-04). ETA=13:57:05, max mem: 15.9 GB 
[11/01 18:22:56 visual_prompt]: 	Training 200/1106. train loss: 0.4172,	0.6187 s / batch. (data: 3.79e-04). ETA=13:39:04, max mem: 15.9 GB 
[11/01 18:24:00 visual_prompt]: 	Training 300/1106. train loss: 0.0427,	0.6424 s / batch. (data: 5.54e-03). ETA=14:09:23, max mem: 15.9 GB 
[11/01 18:25:04 visual_prompt]: 	Training 400/1106. train loss: 0.8389,	0.6510 s / batch. (data: 1.11e-02). ETA=14:19:40, max mem: 15.9 GB 
[11/01 18:26:08 visual_prompt]: 	Training 500/1106. train loss: 0.9132,	0.6338 s / batch. (data: 3.50e-04). ETA=13:55:54, max mem: 15.9 GB 
[11/01 18:27:11 visual_prompt]: 	Training 600/1106. train loss: 0.8129,	0.6319 s / batch. (data: 9.07e-04). ETA=13:52:22, max mem: 15.9 GB 
[11/01 18:28:15 visual_prompt]: 	Training 700/1106. train loss: 0.1688,	0.6508 s / batch. (data: 5.53e-03). ETA=14:16:09, max mem: 15.9 GB 
[11/01 18:29:19 visual_prompt]: 	Training 800/1106. train loss: 0.0592,	0.6400 s / batch. (data: 9.26e-04). ETA=14:00:54, max mem: 15.9 GB 
[11/01 18:30:23 visual_prompt]: 	Training 900/1106. train loss: 2.2396,	0.6443 s / batch. (data: 1.62e-02). ETA=14:05:26, max mem: 15.9 GB 
[11/01 18:31:26 visual_prompt]: 	Training 1000/1106. train loss: 1.1636,	0.6360 s / batch. (data: 8.77e-04). ETA=13:53:29, max mem: 15.9 GB 
[11/01 18:32:30 visual_prompt]: 	Training 1100/1106. train loss: 0.5725,	0.6257 s / batch. (data: 2.01e-04). ETA=13:38:58, max mem: 15.9 GB 
[11/01 18:32:34 visual_prompt]: Epoch 29 / 100: avg data time: 6.86e-03, avg batch time: 0.6406, average train loss: 0.9691
[11/01 18:33:28 visual_prompt]: 	Test 100/123. loss: 0.726, 0.2381 s / batch. (data: 3.05e-05)max mem: 15.94594 GB 
[11/01 18:33:40 visual_prompt]: Inference (val):avg data time: 4.70e-05, avg batch time: 0.2326, average loss: 0.6627
[11/01 18:33:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 63.89	
[11/01 18:34:34 visual_prompt]: 	Test 100/323. loss: 0.399, 0.2247 s / batch. (data: 5.05e-05)max mem: 15.94594 GB 
[11/01 18:35:27 visual_prompt]: 	Test 200/323. loss: 0.287, 0.2248 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[11/01 18:36:19 visual_prompt]: 	Test 300/323. loss: 0.707, 0.2425 s / batch. (data: 5.70e-05)max mem: 15.94594 GB 
[11/01 18:36:30 visual_prompt]: Inference (test):avg data time: 1.13e-04, avg batch time: 0.2319, average loss: 0.6689
[11/01 18:36:30 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.62	rocauc: 62.10	
[11/01 18:36:30 visual_prompt]: Best epoch 29: best metric: -0.663
[11/01 18:36:30 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[11/01 18:37:36 visual_prompt]: 	Training 100/1106. train loss: 3.4710,	0.6335 s / batch. (data: 4.24e-04). ETA=13:47:59, max mem: 15.9 GB 
[11/01 18:38:40 visual_prompt]: 	Training 200/1106. train loss: 1.4891,	0.6537 s / batch. (data: 6.11e-03). ETA=14:13:24, max mem: 15.9 GB 
[11/01 18:39:43 visual_prompt]: 	Training 300/1106. train loss: 2.9203,	0.6320 s / batch. (data: 3.67e-04). ETA=13:43:57, max mem: 15.9 GB 
[11/01 18:40:47 visual_prompt]: 	Training 400/1106. train loss: 0.5604,	0.6335 s / batch. (data: 3.60e-04). ETA=13:44:51, max mem: 15.9 GB 
[11/01 18:41:51 visual_prompt]: 	Training 500/1106. train loss: 0.5847,	0.6459 s / batch. (data: 1.06e-02). ETA=13:59:57, max mem: 15.9 GB 
[11/01 18:42:54 visual_prompt]: 	Training 600/1106. train loss: 0.1836,	0.6196 s / batch. (data: 8.70e-04). ETA=13:24:44, max mem: 15.9 GB 
[11/01 18:43:58 visual_prompt]: 	Training 700/1106. train loss: 1.8042,	0.6320 s / batch. (data: 3.55e-04). ETA=13:39:46, max mem: 15.9 GB 
[11/01 18:45:02 visual_prompt]: 	Training 800/1106. train loss: 0.7698,	0.6311 s / batch. (data: 8.88e-04). ETA=13:37:30, max mem: 15.9 GB 
[11/01 18:46:06 visual_prompt]: 	Training 900/1106. train loss: 1.0723,	0.6282 s / batch. (data: 3.84e-04). ETA=13:32:45, max mem: 15.9 GB 
[11/01 18:47:09 visual_prompt]: 	Training 1000/1106. train loss: 1.6221,	0.6517 s / batch. (data: 8.68e-04). ETA=14:02:01, max mem: 15.9 GB 
[11/01 18:48:13 visual_prompt]: 	Training 1100/1106. train loss: 0.2499,	0.6201 s / batch. (data: 2.10e-04). ETA=13:20:10, max mem: 15.9 GB 
[11/01 18:48:16 visual_prompt]: Epoch 30 / 100: avg data time: 6.37e-03, avg batch time: 0.6390, average train loss: 0.9290
[11/01 18:49:11 visual_prompt]: 	Test 100/123. loss: 1.101, 0.2344 s / batch. (data: 6.44e-05)max mem: 15.94594 GB 
[11/01 18:49:22 visual_prompt]: Inference (val):avg data time: 4.93e-05, avg batch time: 0.2315, average loss: 0.7777
[11/01 18:49:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 67.38	
[11/01 18:50:17 visual_prompt]: 	Test 100/323. loss: 0.983, 0.2369 s / batch. (data: 4.53e-05)max mem: 15.94594 GB 
[11/01 18:51:10 visual_prompt]: 	Test 200/323. loss: 0.524, 0.2397 s / batch. (data: 3.50e-05)max mem: 15.94594 GB 
[11/01 18:52:01 visual_prompt]: 	Test 300/323. loss: 0.561, 0.2380 s / batch. (data: 4.74e-05)max mem: 15.94594 GB 
[11/01 18:52:12 visual_prompt]: Inference (test):avg data time: 4.91e-05, avg batch time: 0.2318, average loss: 0.8498
[11/01 18:52:12 visual_prompt]: Classification results with test_mammo-cbis: top1: 53.64	rocauc: 63.60	
[11/01 18:52:12 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[11/01 18:53:18 visual_prompt]: 	Training 100/1106. train loss: 1.0098,	0.6332 s / batch. (data: 3.84e-04). ETA=13:35:57, max mem: 15.9 GB 
[11/01 18:54:22 visual_prompt]: 	Training 200/1106. train loss: 0.3378,	0.6187 s / batch. (data: 5.31e-04). ETA=13:16:12, max mem: 15.9 GB 
[11/01 18:55:26 visual_prompt]: 	Training 300/1106. train loss: 0.8863,	0.6349 s / batch. (data: 1.08e-02). ETA=13:36:03, max mem: 15.9 GB 
[11/01 18:56:30 visual_prompt]: 	Training 400/1106. train loss: 2.4493,	0.6406 s / batch. (data: 5.51e-03). ETA=13:42:16, max mem: 15.9 GB 
[11/01 18:57:34 visual_prompt]: 	Training 500/1106. train loss: 0.0340,	0.6321 s / batch. (data: 8.66e-04). ETA=13:30:23, max mem: 15.9 GB 
[11/01 18:58:37 visual_prompt]: 	Training 600/1106. train loss: 0.7795,	0.6397 s / batch. (data: 5.51e-03). ETA=13:39:04, max mem: 15.9 GB 
[11/01 18:59:41 visual_prompt]: 	Training 700/1106. train loss: 0.7557,	0.6280 s / batch. (data: 3.15e-04). ETA=13:23:00, max mem: 15.9 GB 
[11/01 19:00:45 visual_prompt]: 	Training 800/1106. train loss: 0.8602,	0.6359 s / batch. (data: 9.08e-04). ETA=13:32:01, max mem: 15.9 GB 
[11/01 19:01:48 visual_prompt]: 	Training 900/1106. train loss: 1.1994,	0.6313 s / batch. (data: 1.36e-03). ETA=13:25:08, max mem: 15.9 GB 
[11/01 19:02:52 visual_prompt]: 	Training 1000/1106. train loss: 0.0000,	0.6190 s / batch. (data: 3.19e-04). ETA=13:08:23, max mem: 15.9 GB 
[11/01 19:03:56 visual_prompt]: 	Training 1100/1106. train loss: 0.7684,	0.6181 s / batch. (data: 2.15e-04). ETA=13:06:16, max mem: 15.9 GB 
[11/01 19:03:59 visual_prompt]: Epoch 31 / 100: avg data time: 6.26e-03, avg batch time: 0.6394, average train loss: 0.9911
[11/01 19:04:54 visual_prompt]: 	Test 100/123. loss: 0.835, 0.2397 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/01 19:05:05 visual_prompt]: Inference (val):avg data time: 1.06e-04, avg batch time: 0.2324, average loss: 0.9163
[11/01 19:05:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 63.88	
[11/01 19:06:01 visual_prompt]: 	Test 100/323. loss: 0.909, 0.2317 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[11/01 19:06:53 visual_prompt]: 	Test 200/323. loss: 0.422, 0.2251 s / batch. (data: 5.29e-05)max mem: 15.94594 GB 
[11/01 19:07:45 visual_prompt]: 	Test 300/323. loss: 0.568, 0.2244 s / batch. (data: 7.41e-05)max mem: 15.94594 GB 
[11/01 19:07:56 visual_prompt]: Inference (test):avg data time: 8.13e-05, avg batch time: 0.2325, average loss: 0.9911
[11/01 19:07:56 visual_prompt]: Classification results with test_mammo-cbis: top1: 46.36	rocauc: 63.07	
[11/01 19:07:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[11/01 19:09:02 visual_prompt]: 	Training 100/1106. train loss: 1.6878,	0.6320 s / batch. (data: 3.57e-04). ETA=13:22:45, max mem: 15.9 GB 
[11/01 19:10:05 visual_prompt]: 	Training 200/1106. train loss: 0.3585,	0.6210 s / batch. (data: 3.67e-04). ETA=13:07:48, max mem: 15.9 GB 
[11/01 19:11:09 visual_prompt]: 	Training 300/1106. train loss: 0.0235,	0.6520 s / batch. (data: 9.28e-04). ETA=13:46:02, max mem: 15.9 GB 
[11/01 19:12:13 visual_prompt]: 	Training 400/1106. train loss: 0.7321,	0.6680 s / batch. (data: 9.86e-04). ETA=14:05:12, max mem: 15.9 GB 
[11/01 19:13:17 visual_prompt]: 	Training 500/1106. train loss: 1.3852,	0.6589 s / batch. (data: 8.60e-04). ETA=13:52:30, max mem: 15.9 GB 
[11/01 19:14:21 visual_prompt]: 	Training 600/1106. train loss: 0.9251,	0.6480 s / batch. (data: 9.16e-04). ETA=13:37:42, max mem: 15.9 GB 
[11/01 19:15:24 visual_prompt]: 	Training 700/1106. train loss: 1.5932,	0.6303 s / batch. (data: 8.74e-04). ETA=13:14:17, max mem: 15.9 GB 
[11/01 19:16:28 visual_prompt]: 	Training 800/1106. train loss: 1.7503,	0.6320 s / batch. (data: 9.36e-04). ETA=13:15:23, max mem: 15.9 GB 
[11/01 19:17:31 visual_prompt]: 	Training 900/1106. train loss: 2.5082,	0.6366 s / batch. (data: 8.80e-04). ETA=13:20:07, max mem: 15.9 GB 
[11/01 19:18:35 visual_prompt]: 	Training 1000/1106. train loss: 2.2871,	0.6520 s / batch. (data: 8.95e-04). ETA=13:38:24, max mem: 15.9 GB 
[11/01 19:19:39 visual_prompt]: 	Training 1100/1106. train loss: 1.8390,	0.6197 s / batch. (data: 2.21e-04). ETA=12:56:46, max mem: 15.9 GB 
[11/01 19:19:42 visual_prompt]: Epoch 32 / 100: avg data time: 5.76e-03, avg batch time: 0.6390, average train loss: 0.9526
[11/01 19:20:37 visual_prompt]: 	Test 100/123. loss: 0.929, 0.2290 s / batch. (data: 3.58e-05)max mem: 15.94594 GB 
[11/01 19:20:48 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.2333, average loss: 0.7421
[11/01 19:20:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 66.52	
[11/01 19:21:43 visual_prompt]: 	Test 100/323. loss: 0.195, 0.2286 s / batch. (data: 5.91e-05)max mem: 15.94594 GB 
[11/01 19:22:35 visual_prompt]: 	Test 200/323. loss: 0.293, 0.2393 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 19:23:27 visual_prompt]: 	Test 300/323. loss: 0.655, 0.2293 s / batch. (data: 5.05e-05)max mem: 15.94594 GB 
[11/01 19:23:38 visual_prompt]: Inference (test):avg data time: 4.88e-05, avg batch time: 0.2315, average loss: 0.6842
[11/01 19:23:38 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.72	rocauc: 66.09	
[11/01 19:23:38 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[11/01 19:24:45 visual_prompt]: 	Training 100/1106. train loss: 6.1890,	0.6419 s / batch. (data: 8.70e-04). ETA=13:23:32, max mem: 15.9 GB 
[11/01 19:25:48 visual_prompt]: 	Training 200/1106. train loss: 0.5297,	0.6446 s / batch. (data: 1.25e-02). ETA=13:25:46, max mem: 15.9 GB 
[11/01 19:26:52 visual_prompt]: 	Training 300/1106. train loss: 1.0649,	0.6400 s / batch. (data: 8.69e-04). ETA=13:19:00, max mem: 15.9 GB 
[11/01 19:27:56 visual_prompt]: 	Training 400/1106. train loss: 1.4244,	0.6360 s / batch. (data: 3.50e-04). ETA=13:12:58, max mem: 15.9 GB 
[11/01 19:29:00 visual_prompt]: 	Training 500/1106. train loss: 1.0064,	0.6477 s / batch. (data: 8.72e-04). ETA=13:26:30, max mem: 15.9 GB 
[11/01 19:30:03 visual_prompt]: 	Training 600/1106. train loss: 2.2465,	0.6369 s / batch. (data: 1.06e-02). ETA=13:11:55, max mem: 15.9 GB 
[11/01 19:31:07 visual_prompt]: 	Training 700/1106. train loss: 0.8318,	0.6369 s / batch. (data: 8.81e-04). ETA=13:10:51, max mem: 15.9 GB 
[11/01 19:32:11 visual_prompt]: 	Training 800/1106. train loss: 0.7322,	0.6775 s / batch. (data: 1.06e-02). ETA=14:00:10, max mem: 15.9 GB 
[11/01 19:33:14 visual_prompt]: 	Training 900/1106. train loss: 0.9370,	0.6415 s / batch. (data: 1.12e-02). ETA=13:14:25, max mem: 15.9 GB 
[11/01 19:34:18 visual_prompt]: 	Training 1000/1106. train loss: 0.0570,	0.6182 s / batch. (data: 3.31e-04). ETA=12:44:32, max mem: 15.9 GB 
[11/01 19:35:22 visual_prompt]: 	Training 1100/1106. train loss: 0.9047,	0.6180 s / batch. (data: 1.83e-04). ETA=12:43:17, max mem: 15.9 GB 
[11/01 19:35:25 visual_prompt]: Epoch 33 / 100: avg data time: 6.57e-03, avg batch time: 0.6395, average train loss: 0.9388
[11/01 19:36:20 visual_prompt]: 	Test 100/123. loss: 0.772, 0.2356 s / batch. (data: 3.70e-05)max mem: 15.94594 GB 
[11/01 19:36:31 visual_prompt]: Inference (val):avg data time: 1.44e-04, avg batch time: 0.2315, average loss: 0.7752
[11/01 19:36:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 64.34	
[11/01 19:37:27 visual_prompt]: 	Test 100/323. loss: 1.073, 0.2241 s / batch. (data: 3.93e-05)max mem: 15.94594 GB 
[11/01 19:38:19 visual_prompt]: 	Test 200/323. loss: 0.571, 0.2380 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[11/01 19:39:10 visual_prompt]: 	Test 300/323. loss: 0.601, 0.2385 s / batch. (data: 4.55e-05)max mem: 15.94594 GB 
[11/01 19:39:21 visual_prompt]: Inference (test):avg data time: 1.11e-04, avg batch time: 0.2322, average loss: 0.8446
[11/01 19:39:21 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.40	rocauc: 61.07	
[11/01 19:39:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[11/01 19:40:28 visual_prompt]: 	Training 100/1106. train loss: 1.2021,	0.6400 s / batch. (data: 3.46e-04). ETA=13:09:20, max mem: 15.9 GB 
[11/01 19:41:32 visual_prompt]: 	Training 200/1106. train loss: 0.2099,	0.6625 s / batch. (data: 1.47e-02). ETA=13:36:01, max mem: 15.9 GB 
[11/01 19:42:35 visual_prompt]: 	Training 300/1106. train loss: 0.8533,	0.6491 s / batch. (data: 6.08e-03). ETA=13:18:24, max mem: 15.9 GB 
[11/01 19:43:39 visual_prompt]: 	Training 400/1106. train loss: 1.0546,	0.6365 s / batch. (data: 8.58e-04). ETA=13:01:50, max mem: 15.9 GB 
[11/01 19:44:42 visual_prompt]: 	Training 500/1106. train loss: 0.0126,	0.6403 s / batch. (data: 3.90e-04). ETA=13:05:26, max mem: 15.9 GB 
[11/01 19:45:46 visual_prompt]: 	Training 600/1106. train loss: 2.0236,	0.6243 s / batch. (data: 3.70e-04). ETA=12:44:43, max mem: 15.9 GB 
[11/01 19:46:50 visual_prompt]: 	Training 700/1106. train loss: 0.5229,	0.6319 s / batch. (data: 8.94e-04). ETA=12:53:03, max mem: 15.9 GB 
[11/01 19:47:53 visual_prompt]: 	Training 800/1106. train loss: 0.6052,	0.6402 s / batch. (data: 6.02e-03). ETA=13:02:07, max mem: 15.9 GB 
[11/01 19:48:57 visual_prompt]: 	Training 900/1106. train loss: 0.6605,	0.6203 s / batch. (data: 4.00e-04). ETA=12:36:49, max mem: 15.9 GB 
[11/01 19:50:01 visual_prompt]: 	Training 1000/1106. train loss: 0.2432,	0.6287 s / batch. (data: 1.06e-02). ETA=12:46:02, max mem: 15.9 GB 
[11/01 19:51:04 visual_prompt]: 	Training 1100/1106. train loss: 0.9671,	0.6193 s / batch. (data: 1.65e-04). ETA=12:33:27, max mem: 15.9 GB 
[11/01 19:51:08 visual_prompt]: Epoch 34 / 100: avg data time: 6.23e-03, avg batch time: 0.6392, average train loss: 0.8971
[11/01 19:52:02 visual_prompt]: 	Test 100/123. loss: 0.677, 0.2400 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[11/01 19:52:14 visual_prompt]: Inference (val):avg data time: 1.04e-04, avg batch time: 0.2332, average loss: 0.8371
[11/01 19:52:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 70.16	
[11/01 19:53:09 visual_prompt]: 	Test 100/323. loss: 0.076, 0.2286 s / batch. (data: 5.79e-05)max mem: 15.94594 GB 
[11/01 19:54:01 visual_prompt]: 	Test 200/323. loss: 0.209, 0.2357 s / batch. (data: 2.98e-05)max mem: 15.94594 GB 
[11/01 19:54:53 visual_prompt]: 	Test 300/323. loss: 1.743, 0.2241 s / batch. (data: 4.60e-05)max mem: 15.94594 GB 
[11/01 19:55:04 visual_prompt]: Inference (test):avg data time: 7.57e-05, avg batch time: 0.2322, average loss: 0.8159
[11/01 19:55:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.17	rocauc: 63.77	
[11/01 19:55:04 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[11/01 19:56:11 visual_prompt]: 	Training 100/1106. train loss: 0.7995,	0.6170 s / batch. (data: 4.48e-04). ETA=12:29:37, max mem: 15.9 GB 
[11/01 19:57:15 visual_prompt]: 	Training 200/1106. train loss: 0.4228,	0.6748 s / batch. (data: 1.65e-02). ETA=13:38:43, max mem: 15.9 GB 
[11/01 19:58:18 visual_prompt]: 	Training 300/1106. train loss: 0.9056,	0.6650 s / batch. (data: 4.10e-02). ETA=13:25:40, max mem: 15.9 GB 
[11/01 19:59:22 visual_prompt]: 	Training 400/1106. train loss: 2.5878,	0.6226 s / batch. (data: 3.45e-04). ETA=12:33:17, max mem: 15.9 GB 
[11/01 20:00:26 visual_prompt]: 	Training 500/1106. train loss: 1.2037,	0.6262 s / batch. (data: 8.92e-04). ETA=12:36:36, max mem: 15.9 GB 
[11/01 20:01:29 visual_prompt]: 	Training 600/1106. train loss: 0.9473,	0.6480 s / batch. (data: 3.33e-04). ETA=13:01:53, max mem: 15.9 GB 
[11/01 20:02:33 visual_prompt]: 	Training 700/1106. train loss: 1.0227,	0.6201 s / batch. (data: 3.63e-04). ETA=12:27:09, max mem: 15.9 GB 
[11/01 20:03:37 visual_prompt]: 	Training 800/1106. train loss: 0.1474,	0.6517 s / batch. (data: 6.03e-03). ETA=13:04:13, max mem: 15.9 GB 
[11/01 20:04:40 visual_prompt]: 	Training 900/1106. train loss: 0.3164,	0.6184 s / batch. (data: 3.75e-04). ETA=12:23:05, max mem: 15.9 GB 
[11/01 20:05:44 visual_prompt]: 	Training 1000/1106. train loss: 0.6205,	0.6186 s / batch. (data: 3.51e-04). ETA=12:22:15, max mem: 15.9 GB 
[11/01 20:06:47 visual_prompt]: 	Training 1100/1106. train loss: 0.2409,	0.6185 s / batch. (data: 2.14e-04). ETA=12:21:04, max mem: 15.9 GB 
[11/01 20:06:51 visual_prompt]: Epoch 35 / 100: avg data time: 6.96e-03, avg batch time: 0.6395, average train loss: 0.9242
[11/01 20:07:45 visual_prompt]: 	Test 100/123. loss: 0.568, 0.2256 s / batch. (data: 3.31e-05)max mem: 15.94594 GB 
[11/01 20:07:57 visual_prompt]: Inference (val):avg data time: 5.05e-05, avg batch time: 0.2325, average loss: 0.6398
[11/01 20:07:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.77	
[11/01 20:08:52 visual_prompt]: 	Test 100/323. loss: 0.247, 0.2316 s / batch. (data: 3.31e-05)max mem: 15.94594 GB 
[11/01 20:09:45 visual_prompt]: 	Test 200/323. loss: 0.263, 0.2244 s / batch. (data: 5.08e-05)max mem: 15.94594 GB 
[11/01 20:10:37 visual_prompt]: 	Test 300/323. loss: 0.623, 0.2239 s / batch. (data: 9.11e-05)max mem: 15.94594 GB 
[11/01 20:10:48 visual_prompt]: Inference (test):avg data time: 4.71e-05, avg batch time: 0.2318, average loss: 0.6355
[11/01 20:10:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 64.03	rocauc: 66.35	
[11/01 20:10:48 visual_prompt]: Best epoch 35: best metric: -0.640
[11/01 20:10:48 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[11/01 20:11:55 visual_prompt]: 	Training 100/1106. train loss: 0.6859,	0.6327 s / batch. (data: 3.54e-04). ETA=12:37:04, max mem: 15.9 GB 
[11/01 20:12:58 visual_prompt]: 	Training 200/1106. train loss: 0.5300,	0.6284 s / batch. (data: 3.52e-04). ETA=12:30:52, max mem: 15.9 GB 
[11/01 20:14:02 visual_prompt]: 	Training 300/1106. train loss: 4.8377,	0.6326 s / batch. (data: 3.65e-04). ETA=12:34:50, max mem: 15.9 GB 
[11/01 20:15:06 visual_prompt]: 	Training 400/1106. train loss: 1.2009,	0.6494 s / batch. (data: 6.06e-03). ETA=12:53:46, max mem: 15.9 GB 
[11/01 20:16:09 visual_prompt]: 	Training 500/1106. train loss: 0.9331,	0.6183 s / batch. (data: 3.61e-04). ETA=12:15:40, max mem: 15.9 GB 
[11/01 20:17:13 visual_prompt]: 	Training 600/1106. train loss: 0.8546,	0.6314 s / batch. (data: 8.83e-04). ETA=12:30:12, max mem: 15.9 GB 
[11/01 20:18:17 visual_prompt]: 	Training 700/1106. train loss: 0.6764,	0.6208 s / batch. (data: 4.31e-04). ETA=12:16:33, max mem: 15.9 GB 
[11/01 20:19:21 visual_prompt]: 	Training 800/1106. train loss: 1.3493,	0.6303 s / batch. (data: 1.06e-02). ETA=12:26:47, max mem: 15.9 GB 
[11/01 20:20:24 visual_prompt]: 	Training 900/1106. train loss: 0.6978,	0.6340 s / batch. (data: 8.74e-04). ETA=12:30:07, max mem: 15.9 GB 
[11/01 20:21:28 visual_prompt]: 	Training 1000/1106. train loss: 0.8106,	0.6341 s / batch. (data: 9.33e-04). ETA=12:29:11, max mem: 15.9 GB 
[11/01 20:22:32 visual_prompt]: 	Training 1100/1106. train loss: 1.8340,	0.6183 s / batch. (data: 2.25e-04). ETA=12:09:32, max mem: 15.9 GB 
[11/01 20:22:35 visual_prompt]: Epoch 36 / 100: avg data time: 6.44e-03, avg batch time: 0.6399, average train loss: 0.8938
[11/01 20:23:29 visual_prompt]: 	Test 100/123. loss: 0.864, 0.2251 s / batch. (data: 8.30e-05)max mem: 15.94594 GB 
[11/01 20:23:42 visual_prompt]: Inference (val):avg data time: 2.11e-04, avg batch time: 0.2327, average loss: 0.6292
[11/01 20:23:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 69.44	
[11/01 20:24:37 visual_prompt]: 	Test 100/323. loss: 0.316, 0.2317 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 20:25:29 visual_prompt]: 	Test 200/323. loss: 0.246, 0.2303 s / batch. (data: 5.96e-05)max mem: 15.94594 GB 
[11/01 20:26:22 visual_prompt]: 	Test 300/323. loss: 0.664, 0.2242 s / batch. (data: 4.72e-05)max mem: 15.94594 GB 
[11/01 20:26:32 visual_prompt]: Inference (test):avg data time: 1.16e-04, avg batch time: 0.2330, average loss: 0.6345
[11/01 20:26:32 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.10	rocauc: 67.58	
[11/01 20:26:32 visual_prompt]: Best epoch 36: best metric: -0.629
[11/01 20:26:32 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[11/01 20:27:39 visual_prompt]: 	Training 100/1106. train loss: 0.6872,	0.6368 s / batch. (data: 3.66e-04). ETA=12:30:10, max mem: 15.9 GB 
[11/01 20:28:42 visual_prompt]: 	Training 200/1106. train loss: 0.5917,	0.6189 s / batch. (data: 3.89e-04). ETA=12:08:04, max mem: 15.9 GB 
[11/01 20:29:46 visual_prompt]: 	Training 300/1106. train loss: 0.4359,	0.6421 s / batch. (data: 5.55e-03). ETA=12:34:14, max mem: 15.9 GB 
[11/01 20:30:50 visual_prompt]: 	Training 400/1106. train loss: 0.6079,	0.6200 s / batch. (data: 3.55e-04). ETA=12:07:17, max mem: 15.9 GB 
[11/01 20:31:54 visual_prompt]: 	Training 500/1106. train loss: 0.8120,	0.6324 s / batch. (data: 3.75e-04). ETA=12:20:46, max mem: 15.9 GB 
[11/01 20:32:58 visual_prompt]: 	Training 600/1106. train loss: 0.4013,	0.6189 s / batch. (data: 3.31e-04). ETA=12:04:00, max mem: 15.9 GB 
[11/01 20:34:02 visual_prompt]: 	Training 700/1106. train loss: 2.1078,	0.6460 s / batch. (data: 8.69e-04). ETA=12:34:36, max mem: 15.9 GB 
[11/01 20:35:05 visual_prompt]: 	Training 800/1106. train loss: 0.5102,	0.6279 s / batch. (data: 4.63e-04). ETA=12:12:20, max mem: 15.9 GB 
[11/01 20:36:09 visual_prompt]: 	Training 900/1106. train loss: 0.6067,	0.6418 s / batch. (data: 3.36e-04). ETA=12:27:30, max mem: 15.9 GB 
[11/01 20:37:13 visual_prompt]: 	Training 1000/1106. train loss: 0.0626,	0.6276 s / batch. (data: 3.64e-04). ETA=12:09:59, max mem: 15.9 GB 
[11/01 20:38:16 visual_prompt]: 	Training 1100/1106. train loss: 1.2676,	0.6211 s / batch. (data: 2.07e-04). ETA=12:01:23, max mem: 15.9 GB 
[11/01 20:38:20 visual_prompt]: Epoch 37 / 100: avg data time: 6.01e-03, avg batch time: 0.6400, average train loss: 0.8607
[11/01 20:39:14 visual_prompt]: 	Test 100/123. loss: 0.759, 0.2418 s / batch. (data: 4.32e-05)max mem: 15.94594 GB 
[11/01 20:39:26 visual_prompt]: Inference (val):avg data time: 1.80e-04, avg batch time: 0.2318, average loss: 0.6939
[11/01 20:39:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.46	
[11/01 20:40:21 visual_prompt]: 	Test 100/323. loss: 0.357, 0.2286 s / batch. (data: 5.70e-05)max mem: 15.94594 GB 
[11/01 20:41:14 visual_prompt]: 	Test 200/323. loss: 0.220, 0.2253 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[11/01 20:42:05 visual_prompt]: 	Test 300/323. loss: 0.376, 0.2261 s / batch. (data: 5.39e-05)max mem: 15.94594 GB 
[11/01 20:42:16 visual_prompt]: Inference (test):avg data time: 5.91e-05, avg batch time: 0.2325, average loss: 0.7220
[11/01 20:42:16 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.78	rocauc: 65.66	
[11/01 20:42:16 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[11/01 20:43:22 visual_prompt]: 	Training 100/1106. train loss: 1.8131,	0.6333 s / batch. (data: 8.90e-04). ETA=12:14:22, max mem: 15.9 GB 
[11/01 20:44:26 visual_prompt]: 	Training 200/1106. train loss: 0.1963,	0.6275 s / batch. (data: 3.45e-04). ETA=12:06:34, max mem: 15.9 GB 
[11/01 20:45:30 visual_prompt]: 	Training 300/1106. train loss: 0.4370,	0.6444 s / batch. (data: 2.44e-02). ETA=12:25:09, max mem: 15.9 GB 
[11/01 20:46:34 visual_prompt]: 	Training 400/1106. train loss: 0.0513,	0.6658 s / batch. (data: 1.06e-02). ETA=12:48:44, max mem: 15.9 GB 
[11/01 20:47:38 visual_prompt]: 	Training 500/1106. train loss: 1.0003,	0.6196 s / batch. (data: 3.69e-04). ETA=11:54:23, max mem: 15.9 GB 
[11/01 20:48:42 visual_prompt]: 	Training 600/1106. train loss: 0.6408,	0.6505 s / batch. (data: 1.45e-02). ETA=12:28:53, max mem: 15.9 GB 
[11/01 20:49:46 visual_prompt]: 	Training 700/1106. train loss: 1.3198,	0.6680 s / batch. (data: 1.66e-02). ETA=12:47:56, max mem: 15.9 GB 
[11/01 20:50:49 visual_prompt]: 	Training 800/1106. train loss: 0.8418,	0.6394 s / batch. (data: 1.17e-02). ETA=12:14:03, max mem: 15.9 GB 
[11/01 20:51:53 visual_prompt]: 	Training 900/1106. train loss: 0.0482,	0.6208 s / batch. (data: 3.82e-04). ETA=11:51:34, max mem: 15.9 GB 
[11/01 20:52:57 visual_prompt]: 	Training 1000/1106. train loss: 0.0053,	0.6208 s / batch. (data: 3.36e-04). ETA=11:50:36, max mem: 15.9 GB 
[11/01 20:54:00 visual_prompt]: 	Training 1100/1106. train loss: 0.9966,	0.6190 s / batch. (data: 1.52e-04). ETA=11:47:29, max mem: 15.9 GB 
[11/01 20:54:04 visual_prompt]: Epoch 38 / 100: avg data time: 6.06e-03, avg batch time: 0.6399, average train loss: 0.8348
[11/01 20:54:59 visual_prompt]: 	Test 100/123. loss: 0.786, 0.2330 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/01 20:55:10 visual_prompt]: Inference (val):avg data time: 4.88e-05, avg batch time: 0.2322, average loss: 0.6692
[11/01 20:55:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.34	
[11/01 20:56:05 visual_prompt]: 	Test 100/323. loss: 0.154, 0.2357 s / batch. (data: 6.58e-05)max mem: 15.94594 GB 
[11/01 20:56:57 visual_prompt]: 	Test 200/323. loss: 0.326, 0.2567 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[11/01 20:57:49 visual_prompt]: 	Test 300/323. loss: 0.525, 0.2249 s / batch. (data: 5.82e-05)max mem: 15.94594 GB 
[11/01 20:58:00 visual_prompt]: Inference (test):avg data time: 4.75e-05, avg batch time: 0.2317, average loss: 0.6538
[11/01 20:58:00 visual_prompt]: Classification results with test_mammo-cbis: top1: 65.43	rocauc: 67.21	
[11/01 20:58:00 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[11/01 20:59:07 visual_prompt]: 	Training 100/1106. train loss: 0.0341,	0.6361 s / batch. (data: 1.56e-02). ETA=12:05:53, max mem: 15.9 GB 
[11/01 21:00:11 visual_prompt]: 	Training 200/1106. train loss: 2.3108,	0.6256 s / batch. (data: 5.54e-03). ETA=11:52:54, max mem: 15.9 GB 
[11/01 21:01:15 visual_prompt]: 	Training 300/1106. train loss: 0.3645,	0.6315 s / batch. (data: 8.56e-04). ETA=11:58:36, max mem: 15.9 GB 
[11/01 21:02:18 visual_prompt]: 	Training 400/1106. train loss: 1.4694,	0.6450 s / batch. (data: 3.52e-04). ETA=12:12:50, max mem: 15.9 GB 
[11/01 21:03:22 visual_prompt]: 	Training 500/1106. train loss: 1.4833,	0.6290 s / batch. (data: 3.56e-04). ETA=11:53:37, max mem: 15.9 GB 
[11/01 21:04:26 visual_prompt]: 	Training 600/1106. train loss: 1.9722,	0.6301 s / batch. (data: 5.00e-04). ETA=11:53:49, max mem: 15.9 GB 
[11/01 21:05:30 visual_prompt]: 	Training 700/1106. train loss: 0.8893,	0.6459 s / batch. (data: 3.56e-04). ETA=12:10:39, max mem: 15.9 GB 
[11/01 21:06:33 visual_prompt]: 	Training 800/1106. train loss: 1.2739,	0.6502 s / batch. (data: 8.68e-04). ETA=12:14:24, max mem: 15.9 GB 
[11/01 21:07:37 visual_prompt]: 	Training 900/1106. train loss: 1.1048,	0.6320 s / batch. (data: 8.98e-04). ETA=11:52:51, max mem: 15.9 GB 
[11/01 21:08:41 visual_prompt]: 	Training 1000/1106. train loss: 0.4968,	0.6480 s / batch. (data: 8.71e-04). ETA=12:09:47, max mem: 15.9 GB 
[11/01 21:09:45 visual_prompt]: 	Training 1100/1106. train loss: 0.1647,	0.6185 s / batch. (data: 1.99e-04). ETA=11:35:34, max mem: 15.9 GB 
[11/01 21:09:49 visual_prompt]: Epoch 39 / 100: avg data time: 6.97e-03, avg batch time: 0.6407, average train loss: 0.8755
[11/01 21:10:43 visual_prompt]: 	Test 100/123. loss: 1.220, 0.2326 s / batch. (data: 5.87e-05)max mem: 15.94594 GB 
[11/01 21:10:54 visual_prompt]: Inference (val):avg data time: 2.26e-04, avg batch time: 0.2325, average loss: 1.0711
[11/01 21:10:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 66.42	
[11/01 21:11:49 visual_prompt]: 	Test 100/323. loss: 1.629, 0.2357 s / batch. (data: 5.25e-05)max mem: 15.94594 GB 
[11/01 21:12:42 visual_prompt]: 	Test 200/323. loss: 0.885, 0.2247 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[11/01 21:13:33 visual_prompt]: 	Test 300/323. loss: 0.704, 0.2241 s / batch. (data: 6.37e-05)max mem: 15.94594 GB 
[11/01 21:13:44 visual_prompt]: Inference (test):avg data time: 1.25e-04, avg batch time: 0.2314, average loss: 1.1806
[11/01 21:13:44 visual_prompt]: Classification results with test_mammo-cbis: top1: 42.48	rocauc: 63.05	
[11/01 21:13:44 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[11/01 21:14:51 visual_prompt]: 	Training 100/1106. train loss: 0.9518,	0.6335 s / batch. (data: 8.48e-04). ETA=11:51:13, max mem: 15.9 GB 
[11/01 21:15:55 visual_prompt]: 	Training 200/1106. train loss: 0.5345,	0.6319 s / batch. (data: 1.20e-02). ETA=11:48:23, max mem: 15.9 GB 
[11/01 21:16:59 visual_prompt]: 	Training 300/1106. train loss: 0.3053,	0.6246 s / batch. (data: 8.15e-04). ETA=11:39:08, max mem: 15.9 GB 
[11/01 21:18:02 visual_prompt]: 	Training 400/1106. train loss: 0.0319,	0.6269 s / batch. (data: 8.62e-04). ETA=11:40:45, max mem: 15.9 GB 
[11/01 21:19:06 visual_prompt]: 	Training 500/1106. train loss: 2.9104,	0.6360 s / batch. (data: 3.49e-04). ETA=11:49:50, max mem: 15.9 GB 
[11/01 21:20:10 visual_prompt]: 	Training 600/1106. train loss: 1.4407,	0.6241 s / batch. (data: 3.46e-04). ETA=11:35:28, max mem: 15.9 GB 
[11/01 21:21:14 visual_prompt]: 	Training 700/1106. train loss: 0.1552,	0.6432 s / batch. (data: 1.64e-02). ETA=11:55:41, max mem: 15.9 GB 
[11/01 21:22:17 visual_prompt]: 	Training 800/1106. train loss: 0.4029,	0.6400 s / batch. (data: 8.68e-04). ETA=11:51:04, max mem: 15.9 GB 
[11/01 21:23:21 visual_prompt]: 	Training 900/1106. train loss: 1.5554,	0.6291 s / batch. (data: 3.69e-04). ETA=11:37:59, max mem: 15.9 GB 
[11/01 21:24:25 visual_prompt]: 	Training 1000/1106. train loss: 1.1729,	0.6547 s / batch. (data: 3.57e-02). ETA=12:05:15, max mem: 15.9 GB 
[11/01 21:25:28 visual_prompt]: 	Training 1100/1106. train loss: 0.7056,	0.6185 s / batch. (data: 1.75e-04). ETA=11:24:07, max mem: 15.9 GB 
[11/01 21:25:32 visual_prompt]: Epoch 40 / 100: avg data time: 7.12e-03, avg batch time: 0.6400, average train loss: 0.8401
[11/01 21:26:27 visual_prompt]: 	Test 100/123. loss: 0.794, 0.2259 s / batch. (data: 5.36e-05)max mem: 15.94594 GB 
[11/01 21:26:38 visual_prompt]: Inference (val):avg data time: 3.51e-04, avg batch time: 0.2330, average loss: 0.6627
[11/01 21:26:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 67.38	
[11/01 21:27:33 visual_prompt]: 	Test 100/323. loss: 0.217, 0.2397 s / batch. (data: 2.84e-05)max mem: 15.94594 GB 
[11/01 21:28:25 visual_prompt]: 	Test 200/323. loss: 0.557, 0.2246 s / batch. (data: 5.03e-05)max mem: 15.94594 GB 
[11/01 21:29:17 visual_prompt]: 	Test 300/323. loss: 0.500, 0.2327 s / batch. (data: 6.15e-05)max mem: 15.94594 GB 
[11/01 21:29:28 visual_prompt]: Inference (test):avg data time: 1.39e-04, avg batch time: 0.2318, average loss: 0.6577
[11/01 21:29:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.41	rocauc: 64.13	
[11/01 21:29:28 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[11/01 21:30:35 visual_prompt]: 	Training 100/1106. train loss: 2.4332,	0.6477 s / batch. (data: 3.84e-04). ETA=11:55:14, max mem: 15.9 GB 
[11/01 21:31:38 visual_prompt]: 	Training 200/1106. train loss: 0.6566,	0.6189 s / batch. (data: 3.64e-04). ETA=11:22:26, max mem: 15.9 GB 
[11/01 21:32:42 visual_prompt]: 	Training 300/1106. train loss: 0.9721,	0.6360 s / batch. (data: 3.12e-04). ETA=11:40:14, max mem: 15.9 GB 
[11/01 21:33:46 visual_prompt]: 	Training 400/1106. train loss: 1.2612,	0.6446 s / batch. (data: 6.11e-03). ETA=11:48:36, max mem: 15.9 GB 
[11/01 21:34:49 visual_prompt]: 	Training 500/1106. train loss: 0.0103,	0.6482 s / batch. (data: 8.40e-04). ETA=11:51:28, max mem: 15.9 GB 
[11/01 21:35:53 visual_prompt]: 	Training 600/1106. train loss: 0.2650,	0.6400 s / batch. (data: 8.53e-04). ETA=11:41:27, max mem: 15.9 GB 
[11/01 21:36:57 visual_prompt]: 	Training 700/1106. train loss: 1.0196,	0.6280 s / batch. (data: 3.54e-04). ETA=11:27:14, max mem: 15.9 GB 
[11/01 21:38:01 visual_prompt]: 	Training 800/1106. train loss: 0.4909,	0.6179 s / batch. (data: 3.12e-04). ETA=11:15:09, max mem: 15.9 GB 
[11/01 21:39:04 visual_prompt]: 	Training 900/1106. train loss: 0.0743,	0.6319 s / batch. (data: 4.32e-04). ETA=11:29:27, max mem: 15.9 GB 
[11/01 21:40:08 visual_prompt]: 	Training 1000/1106. train loss: 0.8053,	0.6460 s / batch. (data: 8.75e-04). ETA=11:43:45, max mem: 15.9 GB 
[11/01 21:41:12 visual_prompt]: 	Training 1100/1106. train loss: 3.4633,	0.6180 s / batch. (data: 1.79e-04). ETA=11:12:10, max mem: 15.9 GB 
[11/01 21:41:16 visual_prompt]: Epoch 41 / 100: avg data time: 6.30e-03, avg batch time: 0.6399, average train loss: 0.8880
[11/01 21:42:10 visual_prompt]: 	Test 100/123. loss: 1.729, 0.2366 s / batch. (data: 5.84e-05)max mem: 15.94594 GB 
[11/01 21:42:22 visual_prompt]: Inference (val):avg data time: 1.38e-04, avg batch time: 0.2325, average loss: 1.7574
[11/01 21:42:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.33	
[11/01 21:43:17 visual_prompt]: 	Test 100/323. loss: 0.006, 0.2357 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[11/01 21:44:09 visual_prompt]: 	Test 200/323. loss: 1.950, 0.2562 s / batch. (data: 5.13e-05)max mem: 15.94594 GB 
[11/01 21:45:01 visual_prompt]: 	Test 300/323. loss: 2.029, 0.2321 s / batch. (data: 4.77e-05)max mem: 15.94594 GB 
[11/01 21:45:11 visual_prompt]: Inference (test):avg data time: 1.86e-04, avg batch time: 0.2322, average loss: 1.5942
[11/01 21:45:11 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 65.98	
[11/01 21:45:11 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[11/01 21:46:19 visual_prompt]: 	Training 100/1106. train loss: 0.2789,	0.6309 s / batch. (data: 3.59e-04). ETA=11:25:08, max mem: 15.9 GB 
[11/01 21:47:22 visual_prompt]: 	Training 200/1106. train loss: 0.3621,	0.6260 s / batch. (data: 3.60e-04). ETA=11:18:42, max mem: 15.9 GB 
[11/01 21:48:26 visual_prompt]: 	Training 300/1106. train loss: 1.9242,	0.6313 s / batch. (data: 1.20e-02). ETA=11:23:22, max mem: 15.9 GB 
[11/01 21:49:29 visual_prompt]: 	Training 400/1106. train loss: 0.8166,	0.6450 s / batch. (data: 2.07e-02). ETA=11:37:11, max mem: 15.9 GB 
[11/01 21:50:33 visual_prompt]: 	Training 500/1106. train loss: 0.4461,	0.6390 s / batch. (data: 4.12e-04). ETA=11:29:38, max mem: 15.9 GB 
[11/01 21:51:37 visual_prompt]: 	Training 600/1106. train loss: 0.0732,	0.6339 s / batch. (data: 5.55e-03). ETA=11:23:02, max mem: 15.9 GB 
[11/01 21:52:41 visual_prompt]: 	Training 700/1106. train loss: 0.9053,	0.6364 s / batch. (data: 3.68e-04). ETA=11:24:39, max mem: 15.9 GB 
[11/01 21:53:44 visual_prompt]: 	Training 800/1106. train loss: 0.3971,	0.6334 s / batch. (data: 1.34e-02). ETA=11:20:24, max mem: 15.9 GB 
[11/01 21:54:48 visual_prompt]: 	Training 900/1106. train loss: 0.1747,	0.6448 s / batch. (data: 9.27e-04). ETA=11:31:33, max mem: 15.9 GB 
[11/01 21:55:52 visual_prompt]: 	Training 1000/1106. train loss: 0.5304,	0.6320 s / batch. (data: 8.46e-04). ETA=11:16:48, max mem: 15.9 GB 
[11/01 21:56:55 visual_prompt]: 	Training 1100/1106. train loss: 0.0067,	0.6185 s / batch. (data: 2.07e-04). ETA=11:01:16, max mem: 15.9 GB 
[11/01 21:56:59 visual_prompt]: Epoch 42 / 100: avg data time: 6.44e-03, avg batch time: 0.6399, average train loss: 0.7984
[11/01 21:57:54 visual_prompt]: 	Test 100/123. loss: 2.712, 0.2476 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 21:58:05 visual_prompt]: Inference (val):avg data time: 1.97e-04, avg batch time: 0.2340, average loss: 2.1713
[11/01 21:58:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.78	
[11/01 21:59:00 visual_prompt]: 	Test 100/323. loss: 3.540, 0.2477 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[11/01 21:59:52 visual_prompt]: 	Test 200/323. loss: 1.911, 0.2357 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[11/01 22:00:44 visual_prompt]: 	Test 300/323. loss: 1.132, 0.2330 s / batch. (data: 5.63e-05)max mem: 15.94594 GB 
[11/01 22:00:55 visual_prompt]: Inference (test):avg data time: 1.20e-04, avg batch time: 0.2322, average loss: 2.3220
[11/01 22:00:55 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 67.08	
[11/01 22:00:55 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[11/01 22:02:01 visual_prompt]: 	Training 100/1106. train loss: 0.6681,	0.6509 s / batch. (data: 1.49e-02). ETA=11:34:48, max mem: 15.9 GB 
[11/01 22:03:05 visual_prompt]: 	Training 200/1106. train loss: 0.2881,	0.6287 s / batch. (data: 3.65e-04). ETA=11:10:03, max mem: 15.9 GB 
[11/01 22:04:09 visual_prompt]: 	Training 300/1106. train loss: 0.3280,	0.6405 s / batch. (data: 8.80e-04). ETA=11:21:36, max mem: 15.9 GB 
[11/01 22:05:12 visual_prompt]: 	Training 400/1106. train loss: 0.7196,	0.6520 s / batch. (data: 8.31e-04). ETA=11:32:40, max mem: 15.9 GB 
[11/01 22:06:16 visual_prompt]: 	Training 500/1106. train loss: 0.5219,	0.6193 s / batch. (data: 3.60e-04). ETA=10:56:55, max mem: 15.9 GB 
[11/01 22:07:20 visual_prompt]: 	Training 600/1106. train loss: 0.7667,	0.6840 s / batch. (data: 8.37e-04). ETA=12:04:28, max mem: 15.9 GB 
[11/01 22:08:24 visual_prompt]: 	Training 700/1106. train loss: 0.2694,	0.6183 s / batch. (data: 3.54e-04). ETA=10:53:48, max mem: 15.9 GB 
[11/01 22:09:27 visual_prompt]: 	Training 800/1106. train loss: 1.2165,	0.6523 s / batch. (data: 1.69e-02). ETA=11:28:39, max mem: 15.9 GB 
[11/01 22:10:31 visual_prompt]: 	Training 900/1106. train loss: 0.2636,	0.6383 s / batch. (data: 6.04e-03). ETA=11:12:51, max mem: 15.9 GB 
[11/01 22:11:35 visual_prompt]: 	Training 1000/1106. train loss: 0.4477,	0.6254 s / batch. (data: 3.47e-04). ETA=10:58:14, max mem: 15.9 GB 
[11/01 22:12:39 visual_prompt]: 	Training 1100/1106. train loss: 1.6497,	0.6333 s / batch. (data: 2.08e-04). ETA=11:05:29, max mem: 15.9 GB 
[11/01 22:12:43 visual_prompt]: Epoch 43 / 100: avg data time: 6.02e-03, avg batch time: 0.6396, average train loss: 0.7932
[11/01 22:13:37 visual_prompt]: 	Test 100/123. loss: 1.311, 0.2256 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[11/01 22:13:49 visual_prompt]: Inference (val):avg data time: 1.26e-04, avg batch time: 0.2338, average loss: 0.8212
[11/01 22:13:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 70.30	
[11/01 22:14:44 visual_prompt]: 	Test 100/323. loss: 0.638, 0.2408 s / batch. (data: 6.77e-05)max mem: 15.94594 GB 
[11/01 22:15:36 visual_prompt]: 	Test 200/323. loss: 0.386, 0.2248 s / batch. (data: 3.19e-05)max mem: 15.94594 GB 
[11/01 22:16:28 visual_prompt]: 	Test 300/323. loss: 0.260, 0.2364 s / batch. (data: 4.67e-05)max mem: 15.94594 GB 
[11/01 22:16:39 visual_prompt]: Inference (test):avg data time: 1.65e-04, avg batch time: 0.2320, average loss: 0.9109
[11/01 22:16:39 visual_prompt]: Classification results with test_mammo-cbis: top1: 51.32	rocauc: 67.01	
[11/01 22:16:39 visual_prompt]: Stopping early.
[11/01 22:16:39 visual_prompt]: Rank of current process: 0. World size: 1
[11/01 22:16:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/01 22:16:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '2', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '896', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/01 22:16:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/01 22:16:39 visual_prompt]: Training with config:
[11/01 22:16:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop896/test/seed8393/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 8393, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 896, 'NO_TEST': False, 'BATCH_SIZE': 2, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/01 22:16:39 visual_prompt]: Loading training data...
[11/01 22:16:39 visual_prompt]: Constructing mammo-cbis dataset train...
[11/01 22:16:39 visual_prompt]: Loading validation data...
[11/01 22:16:39 visual_prompt]: Constructing mammo-cbis dataset val...
[11/01 22:16:39 visual_prompt]: Loading test data...
[11/01 22:16:39 visual_prompt]: Constructing mammo-cbis dataset test...
[11/01 22:16:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 3137, 768])
load_pretrained: grid-size from 14 to 56
[11/01 22:16:45 visual_prompt]: Total Parameters: 88518914	 Gradient Parameters: 462338
[11/01 22:16:45 visual_prompt]: tuned percent:0.522
[11/01 22:16:45 visual_prompt]: Device used for model: 0
[11/01 22:16:45 visual_prompt]: Setting up Evaluator...
[11/01 22:16:45 visual_prompt]: Setting up Trainer...
[11/01 22:16:45 visual_prompt]: 	Setting up the optimizer...
[11/01 22:16:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/01 22:17:51 visual_prompt]: 	Training 100/1106. train loss: 0.0127,	0.6168 s / batch. (data: 3.57e-04). ETA=18:56:01, max mem: 15.9 GB 
[11/01 22:18:54 visual_prompt]: 	Training 200/1106. train loss: 1.8388,	0.6289 s / batch. (data: 3.48e-04). ETA=19:17:07, max mem: 15.9 GB 
[11/01 22:19:58 visual_prompt]: 	Training 300/1106. train loss: 1.8339,	0.6178 s / batch. (data: 3.24e-04). ETA=18:55:45, max mem: 15.9 GB 
[11/01 22:21:02 visual_prompt]: 	Training 400/1106. train loss: 1.8473,	0.6331 s / batch. (data: 3.73e-04). ETA=19:22:43, max mem: 15.9 GB 
[11/01 22:22:05 visual_prompt]: 	Training 500/1106. train loss: 0.7790,	0.6190 s / batch. (data: 4.01e-04). ETA=18:55:51, max mem: 15.9 GB 
[11/01 22:23:09 visual_prompt]: 	Training 600/1106. train loss: 0.0354,	0.6246 s / batch. (data: 3.71e-04). ETA=19:05:01, max mem: 15.9 GB 
[11/01 22:24:13 visual_prompt]: 	Training 700/1106. train loss: 0.0400,	0.6297 s / batch. (data: 8.25e-04). ETA=19:13:23, max mem: 15.9 GB 
[11/01 22:25:17 visual_prompt]: 	Training 800/1106. train loss: 2.2646,	0.6320 s / batch. (data: 3.92e-04). ETA=19:16:31, max mem: 15.9 GB 
[11/01 22:26:20 visual_prompt]: 	Training 900/1106. train loss: 1.0889,	0.6560 s / batch. (data: 8.65e-04). ETA=19:59:20, max mem: 15.9 GB 
[11/01 22:27:24 visual_prompt]: 	Training 1000/1106. train loss: 0.1075,	0.6567 s / batch. (data: 9.19e-04). ETA=19:59:32, max mem: 15.9 GB 
[11/01 22:28:28 visual_prompt]: 	Training 1100/1106. train loss: 1.3982,	0.6174 s / batch. (data: 1.95e-04). ETA=18:46:47, max mem: 15.9 GB 
[11/01 22:28:31 visual_prompt]: Epoch 1 / 100: avg data time: 5.34e-03, avg batch time: 0.6388, average train loss: 1.3024
[11/01 22:29:26 visual_prompt]: 	Test 100/123. loss: 1.259, 0.2296 s / batch. (data: 5.70e-05)max mem: 15.94594 GB 
[11/01 22:29:37 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.2327, average loss: 1.1469
[11/01 22:29:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.83	
[11/01 22:30:32 visual_prompt]: 	Test 100/323. loss: 0.105, 0.2247 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[11/01 22:31:24 visual_prompt]: 	Test 200/323. loss: 1.034, 0.2371 s / batch. (data: 6.06e-05)max mem: 15.94594 GB 
[11/01 22:32:16 visual_prompt]: 	Test 300/323. loss: 0.935, 0.2238 s / batch. (data: 4.84e-05)max mem: 15.94594 GB 
[11/01 22:32:27 visual_prompt]: Inference (test):avg data time: 1.19e-04, avg batch time: 0.2324, average loss: 1.0550
[11/01 22:32:27 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 47.93	
[11/01 22:32:27 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/01 22:33:34 visual_prompt]: 	Training 100/1106. train loss: 0.4682,	0.6173 s / batch. (data: 3.62e-04). ETA=18:45:25, max mem: 15.9 GB 
[11/01 22:34:38 visual_prompt]: 	Training 200/1106. train loss: 0.9504,	0.6437 s / batch. (data: 8.48e-04). ETA=19:32:37, max mem: 15.9 GB 
[11/01 22:35:42 visual_prompt]: 	Training 300/1106. train loss: 0.2005,	0.6320 s / batch. (data: 1.32e-02). ETA=19:10:12, max mem: 15.9 GB 
[11/01 22:36:45 visual_prompt]: 	Training 400/1106. train loss: 0.8455,	0.6329 s / batch. (data: 1.28e-03). ETA=19:10:41, max mem: 15.9 GB 
[11/01 22:37:49 visual_prompt]: 	Training 500/1106. train loss: 0.8114,	0.6323 s / batch. (data: 8.25e-04). ETA=19:08:32, max mem: 15.9 GB 
[11/01 22:38:53 visual_prompt]: 	Training 600/1106. train loss: 1.2593,	0.6425 s / batch. (data: 8.52e-04). ETA=19:26:08, max mem: 15.9 GB 
[11/01 22:39:56 visual_prompt]: 	Training 700/1106. train loss: 0.6358,	0.6353 s / batch. (data: 8.44e-04). ETA=19:11:52, max mem: 15.9 GB 
[11/01 22:41:00 visual_prompt]: 	Training 800/1106. train loss: 0.6083,	0.6191 s / batch. (data: 3.47e-04). ETA=18:41:33, max mem: 15.9 GB 
[11/01 22:42:04 visual_prompt]: 	Training 900/1106. train loss: 0.3118,	0.6358 s / batch. (data: 1.00e-03). ETA=19:10:41, max mem: 15.9 GB 
[11/01 22:43:08 visual_prompt]: 	Training 1000/1106. train loss: 0.6589,	0.6280 s / batch. (data: 3.35e-04). ETA=18:55:34, max mem: 15.9 GB 
[11/01 22:44:11 visual_prompt]: 	Training 1100/1106. train loss: 0.3931,	0.6178 s / batch. (data: 2.22e-04). ETA=18:36:08, max mem: 15.9 GB 
[11/01 22:44:15 visual_prompt]: Epoch 2 / 100: avg data time: 6.58e-03, avg batch time: 0.6398, average train loss: 0.8535
[11/01 22:45:09 visual_prompt]: 	Test 100/123. loss: 0.674, 0.2277 s / batch. (data: 2.88e-05)max mem: 15.94594 GB 
[11/01 22:45:21 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.2334, average loss: 0.7251
[11/01 22:45:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.92	
[11/01 22:46:16 visual_prompt]: 	Test 100/323. loss: 0.856, 0.2250 s / batch. (data: 3.46e-05)max mem: 15.94594 GB 
[11/01 22:47:09 visual_prompt]: 	Test 200/323. loss: 0.707, 0.2287 s / batch. (data: 5.72e-05)max mem: 15.94594 GB 
[11/01 22:48:01 visual_prompt]: 	Test 300/323. loss: 0.691, 0.2244 s / batch. (data: 3.34e-05)max mem: 15.94594 GB 
[11/01 22:48:12 visual_prompt]: Inference (test):avg data time: 1.08e-04, avg batch time: 0.2312, average loss: 0.7442
[11/01 22:48:12 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 50.57	
[11/01 22:48:12 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/01 22:49:19 visual_prompt]: 	Training 100/1106. train loss: 0.1540,	0.6281 s / batch. (data: 1.06e-02). ETA=18:53:35, max mem: 15.9 GB 
[11/01 22:50:22 visual_prompt]: 	Training 200/1106. train loss: 0.8277,	0.6465 s / batch. (data: 9.54e-04). ETA=19:25:46, max mem: 15.9 GB 
[11/01 22:51:26 visual_prompt]: 	Training 300/1106. train loss: 0.3180,	0.6209 s / batch. (data: 3.57e-04). ETA=18:38:32, max mem: 15.9 GB 
[11/01 22:52:30 visual_prompt]: 	Training 400/1106. train loss: 1.7154,	0.6407 s / batch. (data: 8.74e-04). ETA=19:13:03, max mem: 15.9 GB 
[11/01 22:53:33 visual_prompt]: 	Training 500/1106. train loss: 0.9492,	0.6189 s / batch. (data: 3.71e-04). ETA=18:32:50, max mem: 15.9 GB 
[11/01 22:54:37 visual_prompt]: 	Training 600/1106. train loss: 1.9491,	0.6577 s / batch. (data: 6.10e-03). ETA=19:41:32, max mem: 15.9 GB 
[11/01 22:55:41 visual_prompt]: 	Training 700/1106. train loss: 0.3147,	0.6350 s / batch. (data: 1.66e-02). ETA=18:59:36, max mem: 15.9 GB 
[11/01 22:56:44 visual_prompt]: 	Training 800/1106. train loss: 0.3926,	0.6399 s / batch. (data: 1.04e-03). ETA=19:07:26, max mem: 15.9 GB 
[11/01 22:57:48 visual_prompt]: 	Training 900/1106. train loss: 1.7308,	0.6393 s / batch. (data: 9.32e-04). ETA=19:05:19, max mem: 15.9 GB 
[11/01 22:58:52 visual_prompt]: 	Training 1000/1106. train loss: 0.1787,	0.6185 s / batch. (data: 3.59e-04). ETA=18:26:58, max mem: 15.9 GB 
[11/01 22:59:56 visual_prompt]: 	Training 1100/1106. train loss: 0.9538,	0.6302 s / batch. (data: 2.06e-04). ETA=18:46:51, max mem: 15.9 GB 
[11/01 22:59:59 visual_prompt]: Epoch 3 / 100: avg data time: 6.73e-03, avg batch time: 0.6398, average train loss: 0.9193
[11/01 23:00:54 visual_prompt]: 	Test 100/123. loss: 0.661, 0.2441 s / batch. (data: 5.39e-05)max mem: 15.94594 GB 
[11/01 23:01:05 visual_prompt]: Inference (val):avg data time: 1.39e-04, avg batch time: 0.2334, average loss: 0.6955
[11/01 23:01:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.41	rocauc: 54.32	
[11/01 23:02:00 visual_prompt]: 	Test 100/323. loss: 0.724, 0.2253 s / batch. (data: 4.65e-05)max mem: 15.94594 GB 
[11/01 23:02:52 visual_prompt]: 	Test 200/323. loss: 0.689, 0.2391 s / batch. (data: 5.56e-05)max mem: 15.94594 GB 
[11/01 23:03:44 visual_prompt]: 	Test 300/323. loss: 0.682, 0.2294 s / batch. (data: 6.89e-05)max mem: 15.94594 GB 
[11/01 23:03:55 visual_prompt]: Inference (test):avg data time: 1.42e-04, avg batch time: 0.2319, average loss: 0.7049
[11/01 23:03:55 visual_prompt]: Classification results with test_mammo-cbis: top1: 42.95	rocauc: 51.31	
[11/01 23:03:55 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/01 23:05:01 visual_prompt]: 	Training 100/1106. train loss: 0.4556,	0.6187 s / batch. (data: 3.44e-04). ETA=18:25:18, max mem: 15.9 GB 
[11/01 23:06:05 visual_prompt]: 	Training 200/1106. train loss: 0.5423,	0.6520 s / batch. (data: 5.55e-03). ETA=19:23:36, max mem: 15.9 GB 
[11/01 23:07:08 visual_prompt]: 	Training 300/1106. train loss: 1.0221,	0.6500 s / batch. (data: 6.08e-03). ETA=19:19:02, max mem: 15.9 GB 
[11/01 23:08:12 visual_prompt]: 	Training 400/1106. train loss: 0.7711,	0.6236 s / batch. (data: 3.99e-04). ETA=18:30:55, max mem: 15.9 GB 
[11/01 23:09:16 visual_prompt]: 	Training 500/1106. train loss: 1.0793,	0.6448 s / batch. (data: 3.67e-04). ETA=19:07:34, max mem: 15.9 GB 
[11/01 23:10:19 visual_prompt]: 	Training 600/1106. train loss: 0.7028,	0.6337 s / batch. (data: 8.88e-04). ETA=18:46:46, max mem: 15.9 GB 
[11/01 23:11:23 visual_prompt]: 	Training 700/1106. train loss: 0.8417,	0.6372 s / batch. (data: 3.67e-04). ETA=18:51:53, max mem: 15.9 GB 
[11/01 23:12:27 visual_prompt]: 	Training 800/1106. train loss: 0.9917,	0.6280 s / batch. (data: 3.77e-04). ETA=18:34:33, max mem: 15.9 GB 
[11/01 23:13:30 visual_prompt]: 	Training 900/1106. train loss: 0.8534,	0.6186 s / batch. (data: 3.22e-04). ETA=18:16:45, max mem: 15.9 GB 
[11/01 23:14:34 visual_prompt]: 	Training 1000/1106. train loss: 1.0548,	0.6362 s / batch. (data: 1.14e-03). ETA=18:46:53, max mem: 15.9 GB 
[11/01 23:15:37 visual_prompt]: 	Training 1100/1106. train loss: 0.7023,	0.6199 s / batch. (data: 1.63e-04). ETA=18:17:05, max mem: 15.9 GB 
[11/01 23:15:41 visual_prompt]: Epoch 4 / 100: avg data time: 5.80e-03, avg batch time: 0.6382, average train loss: 0.8683
[11/01 23:16:35 visual_prompt]: 	Test 100/123. loss: 0.788, 0.2406 s / batch. (data: 5.79e-05)max mem: 15.94594 GB 
[11/01 23:16:47 visual_prompt]: Inference (val):avg data time: 1.12e-04, avg batch time: 0.2343, average loss: 0.8499
[11/01 23:16:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.05	
[11/01 23:17:42 visual_prompt]: 	Test 100/323. loss: 1.246, 0.2316 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/01 23:18:34 visual_prompt]: 	Test 200/323. loss: 0.750, 0.2432 s / batch. (data: 7.50e-03)max mem: 15.94594 GB 
[11/01 23:19:25 visual_prompt]: 	Test 300/323. loss: 0.800, 0.2247 s / batch. (data: 3.31e-05)max mem: 15.94594 GB 
[11/01 23:19:37 visual_prompt]: Inference (test):avg data time: 1.34e-04, avg batch time: 0.2315, average loss: 0.9011
[11/01 23:19:37 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 54.69	
[11/01 23:19:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/01 23:20:43 visual_prompt]: 	Training 100/1106. train loss: 0.1649,	0.6175 s / batch. (data: 3.59e-04). ETA=18:11:46, max mem: 15.9 GB 
[11/01 23:21:46 visual_prompt]: 	Training 200/1106. train loss: 1.3407,	0.6440 s / batch. (data: 9.76e-04). ETA=18:57:32, max mem: 15.9 GB 
[11/01 23:22:50 visual_prompt]: 	Training 300/1106. train loss: 0.2213,	0.6404 s / batch. (data: 9.76e-03). ETA=18:50:03, max mem: 15.9 GB 
[11/01 23:23:54 visual_prompt]: 	Training 400/1106. train loss: 0.7671,	0.6416 s / batch. (data: 1.35e-02). ETA=18:51:00, max mem: 15.9 GB 
[11/01 23:24:58 visual_prompt]: 	Training 500/1106. train loss: 1.4128,	0.6350 s / batch. (data: 1.61e-02). ETA=18:38:24, max mem: 15.9 GB 
[11/01 23:26:01 visual_prompt]: 	Training 600/1106. train loss: 1.0065,	0.6560 s / batch. (data: 8.80e-04). ETA=19:14:18, max mem: 15.9 GB 
[11/01 23:27:05 visual_prompt]: 	Training 700/1106. train loss: 1.3038,	0.6329 s / batch. (data: 1.06e-02). ETA=18:32:35, max mem: 15.9 GB 
[11/01 23:28:09 visual_prompt]: 	Training 800/1106. train loss: 0.6162,	0.6195 s / batch. (data: 8.21e-04). ETA=18:07:56, max mem: 15.9 GB 
[11/01 23:29:13 visual_prompt]: 	Training 900/1106. train loss: 0.6927,	0.6393 s / batch. (data: 1.08e-02). ETA=18:41:39, max mem: 15.9 GB 
[11/01 23:30:17 visual_prompt]: 	Training 1000/1106. train loss: 0.7304,	0.6360 s / batch. (data: 8.39e-04). ETA=18:34:51, max mem: 15.9 GB 
[11/01 23:31:20 visual_prompt]: 	Training 1100/1106. train loss: 1.3840,	0.6195 s / batch. (data: 2.26e-04). ETA=18:04:55, max mem: 15.9 GB 
[11/01 23:31:24 visual_prompt]: Epoch 5 / 100: avg data time: 5.93e-03, avg batch time: 0.6397, average train loss: 0.9537
[11/01 23:32:18 visual_prompt]: 	Test 100/123. loss: 0.731, 0.2517 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/01 23:32:30 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.2341, average loss: 0.7425
[11/01 23:32:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.91	
[11/01 23:33:25 visual_prompt]: 	Test 100/323. loss: 0.979, 0.2327 s / batch. (data: 5.82e-05)max mem: 15.94594 GB 
[11/01 23:34:17 visual_prompt]: 	Test 200/323. loss: 0.730, 0.2242 s / batch. (data: 5.77e-05)max mem: 15.94594 GB 
[11/01 23:35:09 visual_prompt]: 	Test 300/323. loss: 0.727, 0.2237 s / batch. (data: 6.70e-05)max mem: 15.94594 GB 
[11/01 23:35:20 visual_prompt]: Inference (test):avg data time: 8.57e-05, avg batch time: 0.2328, average loss: 0.7617
[11/01 23:35:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 51.23	
[11/01 23:35:20 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/01 23:36:27 visual_prompt]: 	Training 100/1106. train loss: 0.6252,	0.6316 s / batch. (data: 3.50e-04). ETA=18:24:57, max mem: 15.9 GB 
[11/01 23:37:31 visual_prompt]: 	Training 200/1106. train loss: 0.7464,	0.6328 s / batch. (data: 3.53e-04). ETA=18:26:03, max mem: 15.9 GB 
[11/01 23:38:35 visual_prompt]: 	Training 300/1106. train loss: 1.1125,	0.6419 s / batch. (data: 7.82e-03). ETA=18:40:53, max mem: 15.9 GB 
[11/01 23:39:38 visual_prompt]: 	Training 400/1106. train loss: 0.3090,	0.6240 s / batch. (data: 3.45e-04). ETA=18:08:34, max mem: 15.9 GB 
[11/01 23:40:42 visual_prompt]: 	Training 500/1106. train loss: 0.8618,	0.6320 s / batch. (data: 3.54e-04). ETA=18:21:31, max mem: 15.9 GB 
[11/01 23:41:46 visual_prompt]: 	Training 600/1106. train loss: 0.9000,	0.6433 s / batch. (data: 8.71e-04). ETA=18:40:10, max mem: 15.9 GB 
[11/01 23:42:49 visual_prompt]: 	Training 700/1106. train loss: 0.9116,	0.6332 s / batch. (data: 8.43e-04). ETA=18:21:26, max mem: 15.9 GB 
[11/01 23:43:53 visual_prompt]: 	Training 800/1106. train loss: 0.7867,	0.6455 s / batch. (data: 8.81e-04). ETA=18:41:42, max mem: 15.9 GB 
[11/01 23:44:57 visual_prompt]: 	Training 900/1106. train loss: 0.7644,	0.6437 s / batch. (data: 2.37e-02). ETA=18:37:32, max mem: 15.9 GB 
[11/01 23:46:01 visual_prompt]: 	Training 1000/1106. train loss: 2.3394,	0.6379 s / batch. (data: 3.67e-04). ETA=18:26:28, max mem: 15.9 GB 
[11/01 23:47:05 visual_prompt]: 	Training 1100/1106. train loss: 0.4023,	0.6202 s / batch. (data: 2.78e-04). ETA=17:54:41, max mem: 15.9 GB 
[11/01 23:47:09 visual_prompt]: Epoch 6 / 100: avg data time: 7.18e-03, avg batch time: 0.6406, average train loss: 0.9948
[11/01 23:48:02 visual_prompt]: 	Test 100/123. loss: 0.731, 0.2253 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/01 23:48:14 visual_prompt]: Inference (val):avg data time: 4.69e-05, avg batch time: 0.2328, average loss: 0.7084
[11/01 23:48:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.73	
[11/01 23:49:09 visual_prompt]: 	Test 100/323. loss: 0.419, 0.2500 s / batch. (data: 5.01e-05)max mem: 15.94594 GB 
[11/01 23:50:01 visual_prompt]: 	Test 200/323. loss: 0.731, 0.2244 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[11/01 23:50:53 visual_prompt]: 	Test 300/323. loss: 0.767, 0.2241 s / batch. (data: 6.84e-05)max mem: 15.94594 GB 
[11/01 23:51:04 visual_prompt]: Inference (test):avg data time: 1.44e-04, avg batch time: 0.2310, average loss: 0.6823
[11/01 23:51:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 51.94	
[11/01 23:51:04 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/01 23:52:11 visual_prompt]: 	Training 100/1106. train loss: 0.1182,	0.6450 s / batch. (data: 5.56e-03). ETA=18:36:31, max mem: 15.9 GB 
[11/01 23:53:14 visual_prompt]: 	Training 200/1106. train loss: 0.7609,	0.6506 s / batch. (data: 6.06e-03). ETA=18:45:08, max mem: 15.9 GB 
[11/01 23:54:18 visual_prompt]: 	Training 300/1106. train loss: 0.5942,	0.6347 s / batch. (data: 9.35e-04). ETA=18:16:39, max mem: 15.9 GB 
[11/01 23:55:22 visual_prompt]: 	Training 400/1106. train loss: 0.9839,	0.6319 s / batch. (data: 1.06e-02). ETA=18:10:42, max mem: 15.9 GB 
[11/01 23:56:26 visual_prompt]: 	Training 500/1106. train loss: 0.6286,	0.6485 s / batch. (data: 1.10e-02). ETA=18:38:13, max mem: 15.9 GB 
[11/01 23:57:30 visual_prompt]: 	Training 600/1106. train loss: 0.8716,	0.6725 s / batch. (data: 6.03e-03). ETA=19:18:37, max mem: 15.9 GB 
[11/01 23:58:33 visual_prompt]: 	Training 700/1106. train loss: 0.5228,	0.6240 s / batch. (data: 3.69e-04). ETA=17:53:56, max mem: 15.9 GB 
[11/01 23:59:37 visual_prompt]: 	Training 800/1106. train loss: 0.7699,	0.6194 s / batch. (data: 3.19e-04). ETA=17:45:03, max mem: 15.9 GB 
[11/02 00:00:41 visual_prompt]: 	Training 900/1106. train loss: 1.1938,	0.6466 s / batch. (data: 1.11e-03). ETA=18:30:44, max mem: 15.9 GB 
[11/02 00:01:44 visual_prompt]: 	Training 1000/1106. train loss: 0.1576,	0.6240 s / batch. (data: 3.51e-04). ETA=17:50:48, max mem: 15.9 GB 
[11/02 00:02:48 visual_prompt]: 	Training 1100/1106. train loss: 0.8588,	0.6314 s / batch. (data: 1.98e-04). ETA=18:02:31, max mem: 15.9 GB 
[11/02 00:02:52 visual_prompt]: Epoch 7 / 100: avg data time: 6.41e-03, avg batch time: 0.6400, average train loss: 1.0718
[11/02 00:03:46 visual_prompt]: 	Test 100/123. loss: 0.895, 0.2249 s / batch. (data: 5.94e-05)max mem: 15.94594 GB 
[11/02 00:03:58 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.2330, average loss: 1.0019
[11/02 00:03:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.65	
[11/02 00:04:52 visual_prompt]: 	Test 100/323. loss: 1.645, 0.2246 s / batch. (data: 6.82e-05)max mem: 15.94594 GB 
[11/02 00:05:45 visual_prompt]: 	Test 200/323. loss: 0.894, 0.2244 s / batch. (data: 3.12e-05)max mem: 15.94594 GB 
[11/02 00:06:36 visual_prompt]: 	Test 300/323. loss: 0.963, 0.2248 s / batch. (data: 5.58e-05)max mem: 15.94594 GB 
[11/02 00:06:47 visual_prompt]: Inference (test):avg data time: 9.65e-05, avg batch time: 0.2322, average loss: 1.0596
[11/02 00:06:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 55.37	
[11/02 00:06:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/02 00:07:54 visual_prompt]: 	Training 100/1106. train loss: 0.6478,	0.6560 s / batch. (data: 8.75e-04). ETA=18:43:31, max mem: 15.9 GB 
[11/02 00:08:58 visual_prompt]: 	Training 200/1106. train loss: 0.1747,	0.6237 s / batch. (data: 3.25e-04). ETA=17:47:03, max mem: 15.9 GB 
[11/02 00:10:01 visual_prompt]: 	Training 300/1106. train loss: 0.6796,	0.6238 s / batch. (data: 3.69e-04). ETA=17:46:12, max mem: 15.9 GB 
[11/02 00:11:05 visual_prompt]: 	Training 400/1106. train loss: 0.9209,	0.6262 s / batch. (data: 6.33e-03). ETA=17:49:21, max mem: 15.9 GB 
[11/02 00:12:09 visual_prompt]: 	Training 500/1106. train loss: 0.8782,	0.6358 s / batch. (data: 1.20e-02). ETA=18:04:35, max mem: 15.9 GB 
[11/02 00:13:12 visual_prompt]: 	Training 600/1106. train loss: 1.1569,	0.6242 s / batch. (data: 3.84e-04). ETA=17:43:53, max mem: 15.9 GB 
[11/02 00:14:16 visual_prompt]: 	Training 700/1106. train loss: 1.7263,	0.6360 s / batch. (data: 3.57e-04). ETA=18:02:52, max mem: 15.9 GB 
[11/02 00:15:20 visual_prompt]: 	Training 800/1106. train loss: 2.1854,	0.6203 s / batch. (data: 3.40e-04). ETA=17:35:09, max mem: 15.9 GB 
[11/02 00:16:23 visual_prompt]: 	Training 900/1106. train loss: 1.5140,	0.6302 s / batch. (data: 5.45e-04). ETA=17:50:49, max mem: 15.9 GB 
[11/02 00:17:27 visual_prompt]: 	Training 1000/1106. train loss: 0.6783,	0.6261 s / batch. (data: 8.68e-04). ETA=17:42:53, max mem: 15.9 GB 
[11/02 00:18:32 visual_prompt]: 	Training 1100/1106. train loss: 0.0365,	0.6212 s / batch. (data: 2.21e-04). ETA=17:33:28, max mem: 15.9 GB 
[11/02 00:18:35 visual_prompt]: Epoch 8 / 100: avg data time: 6.69e-03, avg batch time: 0.6401, average train loss: 1.1806
[11/02 00:19:30 visual_prompt]: 	Test 100/123. loss: 0.733, 0.2357 s / batch. (data: 4.08e-05)max mem: 15.94594 GB 
[11/02 00:19:41 visual_prompt]: Inference (val):avg data time: 5.16e-05, avg batch time: 0.2330, average loss: 0.7022
[11/02 00:19:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 51.22	
[11/02 00:20:36 visual_prompt]: 	Test 100/323. loss: 0.512, 0.2348 s / batch. (data: 9.76e-03)max mem: 15.94594 GB 
[11/02 00:21:28 visual_prompt]: 	Test 200/323. loss: 0.593, 0.2252 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[11/02 00:22:20 visual_prompt]: 	Test 300/323. loss: 0.828, 0.2312 s / batch. (data: 4.29e-05)max mem: 15.94594 GB 
[11/02 00:22:31 visual_prompt]: Inference (test):avg data time: 1.11e-04, avg batch time: 0.2321, average loss: 0.6746
[11/02 00:22:31 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.91	rocauc: 54.81	
[11/02 00:22:31 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/02 00:23:37 visual_prompt]: 	Training 100/1106. train loss: 0.6656,	0.6320 s / batch. (data: 1.19e-02). ETA=17:50:44, max mem: 15.9 GB 
[11/02 00:24:41 visual_prompt]: 	Training 200/1106. train loss: 0.3791,	0.6444 s / batch. (data: 9.35e-04). ETA=18:10:44, max mem: 15.9 GB 
[11/02 00:25:45 visual_prompt]: 	Training 300/1106. train loss: 2.9879,	0.6314 s / batch. (data: 3.70e-04). ETA=17:47:32, max mem: 15.9 GB 
[11/02 00:26:49 visual_prompt]: 	Training 400/1106. train loss: 2.2879,	0.6349 s / batch. (data: 8.82e-04). ETA=17:52:26, max mem: 15.9 GB 
[11/02 00:27:53 visual_prompt]: 	Training 500/1106. train loss: 0.8459,	0.6455 s / batch. (data: 3.19e-04). ETA=18:09:14, max mem: 15.9 GB 
[11/02 00:28:57 visual_prompt]: 	Training 600/1106. train loss: 0.8350,	0.6483 s / batch. (data: 5.65e-03). ETA=18:12:55, max mem: 15.9 GB 
[11/02 00:30:00 visual_prompt]: 	Training 700/1106. train loss: 1.1091,	0.6529 s / batch. (data: 8.87e-04). ETA=18:19:35, max mem: 15.9 GB 
[11/02 00:31:04 visual_prompt]: 	Training 800/1106. train loss: 0.0225,	0.6468 s / batch. (data: 3.61e-04). ETA=18:08:14, max mem: 15.9 GB 
[11/02 00:32:08 visual_prompt]: 	Training 900/1106. train loss: 1.2579,	0.6195 s / batch. (data: 3.63e-04). ETA=17:21:19, max mem: 15.9 GB 
[11/02 00:33:11 visual_prompt]: 	Training 1000/1106. train loss: 1.0565,	0.6527 s / batch. (data: 9.14e-04). ETA=18:16:02, max mem: 15.9 GB 
[11/02 00:34:15 visual_prompt]: 	Training 1100/1106. train loss: 4.1073,	0.6205 s / batch. (data: 2.75e-04). ETA=17:20:53, max mem: 15.9 GB 
[11/02 00:34:19 visual_prompt]: Epoch 9 / 100: avg data time: 6.67e-03, avg batch time: 0.6403, average train loss: 1.2566
[11/02 00:35:13 visual_prompt]: 	Test 100/123. loss: 1.460, 0.2314 s / batch. (data: 5.27e-05)max mem: 15.94594 GB 
[11/02 00:35:25 visual_prompt]: Inference (val):avg data time: 4.62e-04, avg batch time: 0.2325, average loss: 1.6629
[11/02 00:35:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.68	
[11/02 00:36:19 visual_prompt]: 	Test 100/323. loss: 2.932, 0.2454 s / batch. (data: 6.87e-05)max mem: 15.94594 GB 
[11/02 00:37:11 visual_prompt]: 	Test 200/323. loss: 1.511, 0.2403 s / batch. (data: 3.17e-05)max mem: 15.94594 GB 
[11/02 00:38:03 visual_prompt]: 	Test 300/323. loss: 1.516, 0.2241 s / batch. (data: 6.68e-05)max mem: 15.94594 GB 
[11/02 00:38:14 visual_prompt]: Inference (test):avg data time: 1.19e-04, avg batch time: 0.2313, average loss: 1.7954
[11/02 00:38:14 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 53.54	
[11/02 00:38:14 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/02 00:39:20 visual_prompt]: 	Training 100/1106. train loss: 1.4494,	0.6224 s / batch. (data: 3.81e-04). ETA=17:23:00, max mem: 15.9 GB 
[11/02 00:40:24 visual_prompt]: 	Training 200/1106. train loss: 0.8149,	0.6440 s / batch. (data: 8.66e-04). ETA=17:58:06, max mem: 15.9 GB 
[11/02 00:41:28 visual_prompt]: 	Training 300/1106. train loss: 0.0166,	0.6288 s / batch. (data: 5.58e-03). ETA=17:31:37, max mem: 15.9 GB 
[11/02 00:42:32 visual_prompt]: 	Training 400/1106. train loss: 5.6897,	0.6460 s / batch. (data: 1.06e-02). ETA=17:59:19, max mem: 15.9 GB 
[11/02 00:43:36 visual_prompt]: 	Training 500/1106. train loss: 0.8140,	0.6341 s / batch. (data: 3.29e-04). ETA=17:38:21, max mem: 15.9 GB 
[11/02 00:44:39 visual_prompt]: 	Training 600/1106. train loss: 0.8471,	0.6334 s / batch. (data: 8.67e-04). ETA=17:36:04, max mem: 15.9 GB 
[11/02 00:45:43 visual_prompt]: 	Training 700/1106. train loss: 1.9077,	0.6563 s / batch. (data: 1.11e-02). ETA=18:13:16, max mem: 15.9 GB 
[11/02 00:46:47 visual_prompt]: 	Training 800/1106. train loss: 0.5320,	0.6609 s / batch. (data: 4.18e-02). ETA=18:19:44, max mem: 15.9 GB 
[11/02 00:47:51 visual_prompt]: 	Training 900/1106. train loss: 0.2175,	0.6500 s / batch. (data: 6.05e-03). ETA=18:00:32, max mem: 15.9 GB 
[11/02 00:48:54 visual_prompt]: 	Training 1000/1106. train loss: 0.0290,	0.6447 s / batch. (data: 2.69e-02). ETA=17:50:41, max mem: 15.9 GB 
[11/02 00:49:58 visual_prompt]: 	Training 1100/1106. train loss: 0.8703,	0.6180 s / batch. (data: 2.15e-04). ETA=17:05:23, max mem: 15.9 GB 
[11/02 00:50:02 visual_prompt]: Epoch 10 / 100: avg data time: 6.41e-03, avg batch time: 0.6399, average train loss: 1.1753
[11/02 00:50:57 visual_prompt]: 	Test 100/123. loss: 0.848, 0.2382 s / batch. (data: 5.72e-05)max mem: 15.94594 GB 
[11/02 00:51:08 visual_prompt]: Inference (val):avg data time: 4.98e-05, avg batch time: 0.2332, average loss: 0.7994
[11/02 00:51:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.97	
[11/02 00:52:03 visual_prompt]: 	Test 100/323. loss: 0.261, 0.2378 s / batch. (data: 5.34e-05)max mem: 15.94594 GB 
[11/02 00:52:55 visual_prompt]: 	Test 200/323. loss: 0.791, 0.2422 s / batch. (data: 4.15e-05)max mem: 15.94594 GB 
[11/02 00:53:47 visual_prompt]: 	Test 300/323. loss: 0.907, 0.2344 s / batch. (data: 7.53e-05)max mem: 15.94594 GB 
[11/02 00:53:58 visual_prompt]: Inference (test):avg data time: 1.16e-04, avg batch time: 0.2320, average loss: 0.7461
[11/02 00:53:58 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 54.47	
[11/02 00:53:58 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/02 00:55:04 visual_prompt]: 	Training 100/1106. train loss: 0.7380,	0.6302 s / batch. (data: 8.89e-04). ETA=17:24:25, max mem: 15.9 GB 
[11/02 00:56:08 visual_prompt]: 	Training 200/1106. train loss: 0.7824,	0.6470 s / batch. (data: 8.50e-04). ETA=17:51:08, max mem: 15.9 GB 
[11/02 00:57:11 visual_prompt]: 	Training 300/1106. train loss: 1.0326,	0.6174 s / batch. (data: 3.39e-04). ETA=17:01:13, max mem: 15.9 GB 
[11/02 00:58:15 visual_prompt]: 	Training 400/1106. train loss: 2.4223,	0.6341 s / batch. (data: 8.73e-04). ETA=17:27:48, max mem: 15.9 GB 
[11/02 00:59:19 visual_prompt]: 	Training 500/1106. train loss: 0.7165,	0.6520 s / batch. (data: 3.32e-04). ETA=17:56:13, max mem: 15.9 GB 
[11/02 01:00:23 visual_prompt]: 	Training 600/1106. train loss: 0.7699,	0.6332 s / batch. (data: 3.89e-03). ETA=17:24:09, max mem: 15.9 GB 
[11/02 01:01:26 visual_prompt]: 	Training 700/1106. train loss: 0.0997,	0.6396 s / batch. (data: 1.06e-02). ETA=17:33:40, max mem: 15.9 GB 
[11/02 01:02:30 visual_prompt]: 	Training 800/1106. train loss: 2.2141,	0.6286 s / batch. (data: 8.62e-04). ETA=17:14:25, max mem: 15.9 GB 
[11/02 01:03:33 visual_prompt]: 	Training 900/1106. train loss: 0.6892,	0.6421 s / batch. (data: 8.74e-04). ETA=17:35:39, max mem: 15.9 GB 
[11/02 01:04:37 visual_prompt]: 	Training 1000/1106. train loss: 0.7268,	0.6369 s / batch. (data: 3.80e-04). ETA=17:25:55, max mem: 15.9 GB 
[11/02 01:05:41 visual_prompt]: 	Training 1100/1106. train loss: 6.9272,	0.6182 s / batch. (data: 2.06e-04). ETA=16:54:17, max mem: 15.9 GB 
[11/02 01:05:45 visual_prompt]: Epoch 11 / 100: avg data time: 5.86e-03, avg batch time: 0.6392, average train loss: 1.1040
[11/02 01:06:39 visual_prompt]: 	Test 100/123. loss: 0.966, 0.2251 s / batch. (data: 1.20e-04)max mem: 15.94594 GB 
[11/02 01:06:51 visual_prompt]: Inference (val):avg data time: 5.05e-05, avg batch time: 0.2318, average loss: 1.1704
[11/02 01:06:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.68	
[11/02 01:07:45 visual_prompt]: 	Test 100/323. loss: 2.012, 0.2479 s / batch. (data: 5.17e-05)max mem: 15.94594 GB 
[11/02 01:08:38 visual_prompt]: 	Test 200/323. loss: 1.066, 0.2247 s / batch. (data: 3.34e-05)max mem: 15.94594 GB 
[11/02 01:09:29 visual_prompt]: 	Test 300/323. loss: 1.095, 0.2254 s / batch. (data: 7.72e-05)max mem: 15.94594 GB 
[11/02 01:09:40 visual_prompt]: Inference (test):avg data time: 8.37e-05, avg batch time: 0.2323, average loss: 1.2567
[11/02 01:09:40 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 55.79	
[11/02 01:09:40 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/02 01:10:47 visual_prompt]: 	Training 100/1106. train loss: 1.6057,	0.6627 s / batch. (data: 2.27e-02). ETA=18:06:07, max mem: 15.9 GB 
[11/02 01:11:51 visual_prompt]: 	Training 200/1106. train loss: 0.7800,	0.6337 s / batch. (data: 3.61e-04). ETA=17:17:34, max mem: 15.9 GB 
[11/02 01:12:55 visual_prompt]: 	Training 300/1106. train loss: 0.9643,	0.6640 s / batch. (data: 7.97e-03). ETA=18:05:59, max mem: 15.9 GB 
[11/02 01:13:58 visual_prompt]: 	Training 400/1106. train loss: 0.8439,	0.6400 s / batch. (data: 3.53e-04). ETA=17:25:42, max mem: 15.9 GB 
[11/02 01:15:02 visual_prompt]: 	Training 500/1106. train loss: 0.0769,	0.6511 s / batch. (data: 8.11e-04). ETA=17:42:48, max mem: 15.9 GB 
[11/02 01:16:06 visual_prompt]: 	Training 600/1106. train loss: 2.8942,	0.6440 s / batch. (data: 3.76e-04). ETA=17:30:06, max mem: 15.9 GB 
[11/02 01:17:10 visual_prompt]: 	Training 700/1106. train loss: 1.4194,	0.6480 s / batch. (data: 8.91e-04). ETA=17:35:31, max mem: 15.9 GB 
[11/02 01:18:13 visual_prompt]: 	Training 800/1106. train loss: 0.6726,	0.6290 s / batch. (data: 9.79e-03). ETA=17:03:31, max mem: 15.9 GB 
[11/02 01:19:17 visual_prompt]: 	Training 900/1106. train loss: 1.2961,	0.6341 s / batch. (data: 3.92e-04). ETA=17:10:50, max mem: 15.9 GB 
[11/02 01:20:21 visual_prompt]: 	Training 1000/1106. train loss: 0.0816,	0.6448 s / batch. (data: 8.90e-04). ETA=17:27:02, max mem: 15.9 GB 
[11/02 01:21:25 visual_prompt]: 	Training 1100/1106. train loss: 0.7573,	0.6189 s / batch. (data: 2.55e-04). ETA=16:43:59, max mem: 15.9 GB 
[11/02 01:21:28 visual_prompt]: Epoch 12 / 100: avg data time: 6.72e-03, avg batch time: 0.6400, average train loss: 1.1224
[11/02 01:22:23 visual_prompt]: 	Test 100/123. loss: 0.677, 0.2477 s / batch. (data: 3.67e-05)max mem: 15.94594 GB 
[11/02 01:22:35 visual_prompt]: Inference (val):avg data time: 4.69e-05, avg batch time: 0.2310, average loss: 0.7048
[11/02 01:22:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.65	
[11/02 01:23:30 visual_prompt]: 	Test 100/323. loss: 0.805, 0.2566 s / batch. (data: 5.79e-05)max mem: 15.94594 GB 
[11/02 01:24:22 visual_prompt]: 	Test 200/323. loss: 0.651, 0.2317 s / batch. (data: 3.58e-05)max mem: 15.94594 GB 
[11/02 01:25:14 visual_prompt]: 	Test 300/323. loss: 0.707, 0.2449 s / batch. (data: 5.32e-05)max mem: 15.94594 GB 
[11/02 01:25:25 visual_prompt]: Inference (test):avg data time: 1.60e-04, avg batch time: 0.2324, average loss: 0.7199
[11/02 01:25:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 53.61	
[11/02 01:25:25 visual_prompt]: Best epoch 12: best metric: -0.705
[11/02 01:25:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/02 01:26:31 visual_prompt]: 	Training 100/1106. train loss: 0.6785,	0.6395 s / batch. (data: 9.11e-04). ETA=17:16:18, max mem: 15.9 GB 
[11/02 01:27:35 visual_prompt]: 	Training 200/1106. train loss: 0.6257,	0.6199 s / batch. (data: 3.62e-04). ETA=16:43:31, max mem: 15.9 GB 
[11/02 01:28:39 visual_prompt]: 	Training 300/1106. train loss: 0.2286,	0.6440 s / batch. (data: 5.51e-03). ETA=17:21:24, max mem: 15.9 GB 
[11/02 01:29:42 visual_prompt]: 	Training 400/1106. train loss: 0.8715,	0.6315 s / batch. (data: 8.27e-04). ETA=17:00:08, max mem: 15.9 GB 
[11/02 01:30:46 visual_prompt]: 	Training 500/1106. train loss: 2.0537,	0.6356 s / batch. (data: 8.88e-04). ETA=17:05:42, max mem: 15.9 GB 
[11/02 01:31:50 visual_prompt]: 	Training 600/1106. train loss: 0.8448,	0.6480 s / batch. (data: 3.12e-04). ETA=17:24:39, max mem: 15.9 GB 
[11/02 01:32:53 visual_prompt]: 	Training 700/1106. train loss: 0.9841,	0.6351 s / batch. (data: 9.37e-04). ETA=17:02:51, max mem: 15.9 GB 
[11/02 01:33:57 visual_prompt]: 	Training 800/1106. train loss: 0.9337,	0.6440 s / batch. (data: 2.07e-02). ETA=17:16:07, max mem: 15.9 GB 
[11/02 01:35:01 visual_prompt]: 	Training 900/1106. train loss: 1.3060,	0.6474 s / batch. (data: 8.85e-04). ETA=17:20:31, max mem: 15.9 GB 
[11/02 01:36:05 visual_prompt]: 	Training 1000/1106. train loss: 0.9218,	0.6319 s / batch. (data: 1.26e-03). ETA=16:54:29, max mem: 15.9 GB 
[11/02 01:37:09 visual_prompt]: 	Training 1100/1106. train loss: 1.8764,	0.6181 s / batch. (data: 1.81e-04). ETA=16:31:20, max mem: 15.9 GB 
[11/02 01:37:13 visual_prompt]: Epoch 13 / 100: avg data time: 5.93e-03, avg batch time: 0.6396, average train loss: 1.0951
[11/02 01:38:07 visual_prompt]: 	Test 100/123. loss: 0.735, 0.2396 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[11/02 01:38:18 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.2328, average loss: 0.7140
[11/02 01:38:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.99	
[11/02 01:39:13 visual_prompt]: 	Test 100/323. loss: 0.363, 0.2357 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[11/02 01:40:05 visual_prompt]: 	Test 200/323. loss: 0.639, 0.2253 s / batch. (data: 3.79e-05)max mem: 15.94594 GB 
[11/02 01:40:57 visual_prompt]: 	Test 300/323. loss: 0.806, 0.2237 s / batch. (data: 5.13e-05)max mem: 15.94594 GB 
[11/02 01:41:08 visual_prompt]: Inference (test):avg data time: 4.77e-05, avg batch time: 0.2319, average loss: 0.6844
[11/02 01:41:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.80	
[11/02 01:41:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/02 01:42:15 visual_prompt]: 	Training 100/1106. train loss: 0.8530,	0.6240 s / batch. (data: 3.24e-04). ETA=16:39:39, max mem: 15.9 GB 
[11/02 01:43:18 visual_prompt]: 	Training 200/1106. train loss: 0.6101,	0.6369 s / batch. (data: 8.85e-04). ETA=16:59:15, max mem: 15.9 GB 
[11/02 01:44:22 visual_prompt]: 	Training 300/1106. train loss: 0.8161,	0.6625 s / batch. (data: 1.11e-02). ETA=17:39:04, max mem: 15.9 GB 
[11/02 01:45:26 visual_prompt]: 	Training 400/1106. train loss: 0.6692,	0.6200 s / batch. (data: 3.57e-04). ETA=16:30:08, max mem: 15.9 GB 
[11/02 01:46:30 visual_prompt]: 	Training 500/1106. train loss: 0.7237,	0.6458 s / batch. (data: 8.67e-04). ETA=17:10:19, max mem: 15.9 GB 
[11/02 01:47:33 visual_prompt]: 	Training 600/1106. train loss: 0.7486,	0.6811 s / batch. (data: 2.11e-02). ETA=18:05:31, max mem: 15.9 GB 
[11/02 01:48:37 visual_prompt]: 	Training 700/1106. train loss: 3.3107,	0.6196 s / batch. (data: 3.77e-04). ETA=16:26:27, max mem: 15.9 GB 
[11/02 01:49:41 visual_prompt]: 	Training 800/1106. train loss: 1.3854,	0.6499 s / batch. (data: 5.53e-03). ETA=17:13:36, max mem: 15.9 GB 
[11/02 01:50:44 visual_prompt]: 	Training 900/1106. train loss: 2.3116,	0.6510 s / batch. (data: 1.20e-02). ETA=17:14:17, max mem: 15.9 GB 
[11/02 01:51:48 visual_prompt]: 	Training 1000/1106. train loss: 2.2752,	0.6278 s / batch. (data: 5.52e-03). ETA=16:36:20, max mem: 15.9 GB 
[11/02 01:52:52 visual_prompt]: 	Training 1100/1106. train loss: 0.6096,	0.6340 s / batch. (data: 2.15e-04). ETA=16:45:11, max mem: 15.9 GB 
[11/02 01:52:56 visual_prompt]: Epoch 14 / 100: avg data time: 6.21e-03, avg batch time: 0.6397, average train loss: 1.0700
[11/02 01:53:50 visual_prompt]: 	Test 100/123. loss: 1.226, 0.2431 s / batch. (data: 4.98e-05)max mem: 15.94594 GB 
[11/02 01:54:01 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.2330, average loss: 1.0478
[11/02 01:54:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.07	
[11/02 01:54:56 visual_prompt]: 	Test 100/323. loss: 0.118, 0.2299 s / batch. (data: 5.46e-05)max mem: 15.94594 GB 
[11/02 01:55:48 visual_prompt]: 	Test 200/323. loss: 0.838, 0.2385 s / batch. (data: 5.27e-05)max mem: 15.94594 GB 
[11/02 01:56:41 visual_prompt]: 	Test 300/323. loss: 1.344, 0.2267 s / batch. (data: 4.82e-05)max mem: 15.94594 GB 
[11/02 01:56:51 visual_prompt]: Inference (test):avg data time: 2.20e-04, avg batch time: 0.2326, average loss: 0.9426
[11/02 01:56:51 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.58	
[11/02 01:56:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/02 01:57:58 visual_prompt]: 	Training 100/1106. train loss: 0.6853,	0.6406 s / batch. (data: 9.18e-04). ETA=16:54:26, max mem: 15.9 GB 
[11/02 01:59:01 visual_prompt]: 	Training 200/1106. train loss: 0.8376,	0.6357 s / batch. (data: 3.80e-04). ETA=16:45:38, max mem: 15.9 GB 
[11/02 02:00:05 visual_prompt]: 	Training 300/1106. train loss: 1.2487,	0.6561 s / batch. (data: 1.01e-03). ETA=17:16:47, max mem: 15.9 GB 
[11/02 02:01:09 visual_prompt]: 	Training 400/1106. train loss: 2.0800,	0.6484 s / batch. (data: 9.10e-04). ETA=17:03:34, max mem: 15.9 GB 
[11/02 02:02:13 visual_prompt]: 	Training 500/1106. train loss: 3.1928,	0.6453 s / batch. (data: 8.65e-04). ETA=16:57:31, max mem: 15.9 GB 
[11/02 02:03:17 visual_prompt]: 	Training 600/1106. train loss: 1.2515,	0.6573 s / batch. (data: 6.05e-03). ETA=17:15:22, max mem: 15.9 GB 
[11/02 02:04:20 visual_prompt]: 	Training 700/1106. train loss: 0.3257,	0.6240 s / batch. (data: 3.33e-04). ETA=16:21:55, max mem: 15.9 GB 
[11/02 02:05:24 visual_prompt]: 	Training 800/1106. train loss: 0.8608,	0.6426 s / batch. (data: 1.19e-02). ETA=16:50:06, max mem: 15.9 GB 
[11/02 02:06:28 visual_prompt]: 	Training 900/1106. train loss: 2.2379,	0.6280 s / batch. (data: 3.58e-04). ETA=16:26:07, max mem: 15.9 GB 
[11/02 02:07:32 visual_prompt]: 	Training 1000/1106. train loss: 0.6750,	0.6384 s / batch. (data: 8.82e-04). ETA=16:41:26, max mem: 15.9 GB 
[11/02 02:08:35 visual_prompt]: 	Training 1100/1106. train loss: 0.5728,	0.6238 s / batch. (data: 2.35e-04). ETA=16:17:24, max mem: 15.9 GB 
[11/02 02:08:39 visual_prompt]: Epoch 15 / 100: avg data time: 5.93e-03, avg batch time: 0.6400, average train loss: 1.0566
[11/02 02:09:34 visual_prompt]: 	Test 100/123. loss: 0.628, 0.2393 s / batch. (data: 4.29e-05)max mem: 15.94594 GB 
[11/02 02:09:45 visual_prompt]: Inference (val):avg data time: 1.27e-04, avg batch time: 0.2336, average loss: 0.6775
[11/02 02:09:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 57.99	
[11/02 02:10:40 visual_prompt]: 	Test 100/323. loss: 0.508, 0.2242 s / batch. (data: 5.63e-05)max mem: 15.94594 GB 
[11/02 02:11:32 visual_prompt]: 	Test 200/323. loss: 0.557, 0.2248 s / batch. (data: 5.51e-05)max mem: 15.94594 GB 
[11/02 02:12:23 visual_prompt]: 	Test 300/323. loss: 0.731, 0.2274 s / batch. (data: 3.00e-05)max mem: 15.94594 GB 
[11/02 02:12:34 visual_prompt]: Inference (test):avg data time: 7.20e-05, avg batch time: 0.2319, average loss: 0.6714
[11/02 02:12:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.29	rocauc: 56.26	
[11/02 02:12:34 visual_prompt]: Best epoch 15: best metric: -0.678
[11/02 02:12:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/02 02:13:40 visual_prompt]: 	Training 100/1106. train loss: 1.1502,	0.6296 s / batch. (data: 3.75e-04). ETA=16:25:24, max mem: 15.9 GB 
[11/02 02:14:44 visual_prompt]: 	Training 200/1106. train loss: 0.9002,	0.6527 s / batch. (data: 8.93e-04). ETA=17:00:32, max mem: 15.9 GB 
[11/02 02:15:47 visual_prompt]: 	Training 300/1106. train loss: 0.5294,	0.6343 s / batch. (data: 8.23e-04). ETA=16:30:41, max mem: 15.9 GB 
[11/02 02:16:51 visual_prompt]: 	Training 400/1106. train loss: 0.1708,	0.6520 s / batch. (data: 1.28e-03). ETA=16:57:09, max mem: 15.9 GB 
[11/02 02:17:55 visual_prompt]: 	Training 500/1106. train loss: 0.7733,	0.6213 s / batch. (data: 3.79e-04). ETA=16:08:16, max mem: 15.9 GB 
[11/02 02:18:59 visual_prompt]: 	Training 600/1106. train loss: 1.9840,	0.6529 s / batch. (data: 8.75e-04). ETA=16:56:29, max mem: 15.9 GB 
[11/02 02:20:02 visual_prompt]: 	Training 700/1106. train loss: 0.0609,	0.6300 s / batch. (data: 1.06e-02). ETA=16:19:49, max mem: 15.9 GB 
[11/02 02:21:06 visual_prompt]: 	Training 800/1106. train loss: 1.1618,	0.6406 s / batch. (data: 1.20e-02). ETA=16:35:11, max mem: 15.9 GB 
[11/02 02:22:10 visual_prompt]: 	Training 900/1106. train loss: 0.6967,	0.6708 s / batch. (data: 8.32e-04). ETA=17:20:55, max mem: 15.9 GB 
[11/02 02:23:14 visual_prompt]: 	Training 1000/1106. train loss: 1.6200,	0.6345 s / batch. (data: 9.12e-04). ETA=16:23:34, max mem: 15.9 GB 
[11/02 02:24:18 visual_prompt]: 	Training 1100/1106. train loss: 0.6525,	0.6189 s / batch. (data: 3.94e-04). ETA=15:58:17, max mem: 15.9 GB 
[11/02 02:24:21 visual_prompt]: Epoch 16 / 100: avg data time: 5.40e-03, avg batch time: 0.6392, average train loss: 1.0117
[11/02 02:25:15 visual_prompt]: 	Test 100/123. loss: 0.670, 0.2420 s / batch. (data: 5.79e-05)max mem: 15.94594 GB 
[11/02 02:25:27 visual_prompt]: Inference (val):avg data time: 4.67e-05, avg batch time: 0.2332, average loss: 0.6792
[11/02 02:25:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.22	
[11/02 02:26:22 visual_prompt]: 	Test 100/323. loss: 0.561, 0.2317 s / batch. (data: 3.22e-05)max mem: 15.94594 GB 
[11/02 02:27:14 visual_prompt]: 	Test 200/323. loss: 0.616, 0.2249 s / batch. (data: 3.27e-05)max mem: 15.94594 GB 
[11/02 02:28:06 visual_prompt]: 	Test 300/323. loss: 0.722, 0.2241 s / batch. (data: 5.13e-05)max mem: 15.94594 GB 
[11/02 02:28:17 visual_prompt]: Inference (test):avg data time: 7.17e-05, avg batch time: 0.2316, average loss: 0.6697
[11/02 02:28:17 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.45	rocauc: 56.74	
[11/02 02:28:17 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/02 02:29:23 visual_prompt]: 	Training 100/1106. train loss: 0.8472,	0.6461 s / batch. (data: 8.54e-04). ETA=16:39:19, max mem: 15.9 GB 
[11/02 02:30:26 visual_prompt]: 	Training 200/1106. train loss: 1.4346,	0.6191 s / batch. (data: 3.69e-04). ETA=15:56:30, max mem: 15.9 GB 
[11/02 02:31:30 visual_prompt]: 	Training 300/1106. train loss: 2.7218,	0.6198 s / batch. (data: 8.06e-04). ETA=15:56:37, max mem: 15.9 GB 
[11/02 02:32:34 visual_prompt]: 	Training 400/1106. train loss: 0.0756,	0.6196 s / batch. (data: 5.25e-04). ETA=15:55:19, max mem: 15.9 GB 
[11/02 02:33:37 visual_prompt]: 	Training 500/1106. train loss: 0.0859,	0.6353 s / batch. (data: 3.45e-04). ETA=16:18:24, max mem: 15.9 GB 
[11/02 02:34:41 visual_prompt]: 	Training 600/1106. train loss: 0.6508,	0.6465 s / batch. (data: 8.57e-04). ETA=16:34:31, max mem: 15.9 GB 
[11/02 02:35:45 visual_prompt]: 	Training 700/1106. train loss: 0.4042,	0.6187 s / batch. (data: 3.29e-04). ETA=15:50:44, max mem: 15.9 GB 
[11/02 02:36:49 visual_prompt]: 	Training 800/1106. train loss: 0.7618,	0.6444 s / batch. (data: 8.86e-04). ETA=16:29:07, max mem: 15.9 GB 
[11/02 02:37:53 visual_prompt]: 	Training 900/1106. train loss: 1.7873,	0.6600 s / batch. (data: 8.51e-04). ETA=16:52:02, max mem: 15.9 GB 
[11/02 02:38:56 visual_prompt]: 	Training 1000/1106. train loss: 0.5305,	0.6243 s / batch. (data: 4.12e-04). ETA=15:56:12, max mem: 15.9 GB 
[11/02 02:40:00 visual_prompt]: 	Training 1100/1106. train loss: 0.6436,	0.6339 s / batch. (data: 2.12e-04). ETA=16:09:56, max mem: 15.9 GB 
[11/02 02:40:04 visual_prompt]: Epoch 17 / 100: avg data time: 5.44e-03, avg batch time: 0.6393, average train loss: 1.0189
[11/02 02:40:58 visual_prompt]: 	Test 100/123. loss: 0.902, 0.2404 s / batch. (data: 3.03e-05)max mem: 15.94594 GB 
[11/02 02:41:10 visual_prompt]: Inference (val):avg data time: 4.58e-05, avg batch time: 0.2328, average loss: 0.8367
[11/02 02:41:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.20	
[11/02 02:42:04 visual_prompt]: 	Test 100/323. loss: 0.213, 0.2252 s / batch. (data: 6.32e-05)max mem: 15.94594 GB 
[11/02 02:42:57 visual_prompt]: 	Test 200/323. loss: 0.673, 0.2326 s / batch. (data: 6.27e-05)max mem: 15.94594 GB 
[11/02 02:43:48 visual_prompt]: 	Test 300/323. loss: 1.060, 0.2316 s / batch. (data: 3.24e-05)max mem: 15.94594 GB 
[11/02 02:43:59 visual_prompt]: Inference (test):avg data time: 1.03e-04, avg batch time: 0.2320, average loss: 0.7785
[11/02 02:43:59 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.85	
[11/02 02:43:59 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/02 02:45:06 visual_prompt]: 	Training 100/1106. train loss: 1.7021,	0.6827 s / batch. (data: 1.11e-02). ETA=17:23:25, max mem: 15.9 GB 
[11/02 02:46:10 visual_prompt]: 	Training 200/1106. train loss: 1.1857,	0.6362 s / batch. (data: 8.92e-04). ETA=16:11:15, max mem: 15.9 GB 
[11/02 02:47:14 visual_prompt]: 	Training 300/1106. train loss: 0.0452,	0.6447 s / batch. (data: 8.59e-04). ETA=16:23:10, max mem: 15.9 GB 
[11/02 02:48:17 visual_prompt]: 	Training 400/1106. train loss: 0.7178,	0.6546 s / batch. (data: 4.17e-04). ETA=16:37:09, max mem: 15.9 GB 
[11/02 02:49:21 visual_prompt]: 	Training 500/1106. train loss: 0.2466,	0.6187 s / batch. (data: 3.14e-04). ETA=15:41:24, max mem: 15.9 GB 
[11/02 02:50:25 visual_prompt]: 	Training 600/1106. train loss: 0.8111,	0.6344 s / batch. (data: 9.05e-04). ETA=16:04:19, max mem: 15.9 GB 
[11/02 02:51:29 visual_prompt]: 	Training 700/1106. train loss: 0.4488,	0.6404 s / batch. (data: 5.53e-03). ETA=16:12:20, max mem: 15.9 GB 
[11/02 02:52:32 visual_prompt]: 	Training 800/1106. train loss: 0.6230,	0.6364 s / batch. (data: 3.21e-04). ETA=16:05:12, max mem: 15.9 GB 
[11/02 02:53:36 visual_prompt]: 	Training 900/1106. train loss: 0.6183,	0.6180 s / batch. (data: 3.52e-04). ETA=15:36:17, max mem: 15.9 GB 
[11/02 02:54:40 visual_prompt]: 	Training 1000/1106. train loss: 0.2359,	0.6474 s / batch. (data: 9.34e-04). ETA=16:19:40, max mem: 15.9 GB 
[11/02 02:55:43 visual_prompt]: 	Training 1100/1106. train loss: 2.8789,	0.6179 s / batch. (data: 2.12e-04). ETA=15:34:06, max mem: 15.9 GB 
[11/02 02:55:47 visual_prompt]: Epoch 18 / 100: avg data time: 6.56e-03, avg batch time: 0.6400, average train loss: 1.0593
[11/02 02:56:41 visual_prompt]: 	Test 100/123. loss: 0.579, 0.2253 s / batch. (data: 5.87e-05)max mem: 15.94594 GB 
[11/02 02:56:53 visual_prompt]: Inference (val):avg data time: 2.27e-04, avg batch time: 0.2314, average loss: 0.7252
[11/02 02:56:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 56.85	
[11/02 02:57:48 visual_prompt]: 	Test 100/323. loss: 0.785, 0.2350 s / batch. (data: 5.25e-05)max mem: 15.94594 GB 
[11/02 02:58:40 visual_prompt]: 	Test 200/323. loss: 0.529, 0.2385 s / batch. (data: 5.51e-05)max mem: 15.94594 GB 
[11/02 02:59:31 visual_prompt]: 	Test 300/323. loss: 0.756, 0.2305 s / batch. (data: 4.55e-05)max mem: 15.94594 GB 
[11/02 02:59:43 visual_prompt]: Inference (test):avg data time: 1.69e-04, avg batch time: 0.2325, average loss: 0.7391
[11/02 02:59:43 visual_prompt]: Classification results with test_mammo-cbis: top1: 50.23	rocauc: 57.19	
[11/02 02:59:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/02 03:00:49 visual_prompt]: 	Training 100/1106. train loss: 0.9460,	0.6430 s / batch. (data: 9.35e-04). ETA=16:10:51, max mem: 15.9 GB 
[11/02 03:01:52 visual_prompt]: 	Training 200/1106. train loss: 3.2668,	0.6440 s / batch. (data: 3.29e-04). ETA=16:11:17, max mem: 15.9 GB 
[11/02 03:02:56 visual_prompt]: 	Training 300/1106. train loss: 1.1307,	0.6329 s / batch. (data: 1.18e-03). ETA=15:53:29, max mem: 15.9 GB 
[11/02 03:04:00 visual_prompt]: 	Training 400/1106. train loss: 1.5647,	0.6285 s / batch. (data: 3.68e-04). ETA=15:45:48, max mem: 15.9 GB 
[11/02 03:05:04 visual_prompt]: 	Training 500/1106. train loss: 0.1442,	0.6314 s / batch. (data: 3.59e-04). ETA=15:49:10, max mem: 15.9 GB 
[11/02 03:06:08 visual_prompt]: 	Training 600/1106. train loss: 1.2558,	0.6434 s / batch. (data: 1.30e-02). ETA=16:06:01, max mem: 15.9 GB 
[11/02 03:07:12 visual_prompt]: 	Training 700/1106. train loss: 0.6044,	0.6600 s / batch. (data: 2.01e-02). ETA=16:29:52, max mem: 15.9 GB 
[11/02 03:08:15 visual_prompt]: 	Training 800/1106. train loss: 0.5208,	0.6183 s / batch. (data: 3.54e-04). ETA=15:26:23, max mem: 15.9 GB 
[11/02 03:09:19 visual_prompt]: 	Training 900/1106. train loss: 0.0585,	0.6520 s / batch. (data: 8.63e-04). ETA=16:15:46, max mem: 15.9 GB 
[11/02 03:10:23 visual_prompt]: 	Training 1000/1106. train loss: 1.4362,	0.6518 s / batch. (data: 9.27e-04). ETA=16:14:20, max mem: 15.9 GB 
[11/02 03:11:26 visual_prompt]: 	Training 1100/1106. train loss: 0.0496,	0.6178 s / batch. (data: 2.95e-04). ETA=15:22:26, max mem: 15.9 GB 
[11/02 03:11:30 visual_prompt]: Epoch 19 / 100: avg data time: 5.97e-03, avg batch time: 0.6395, average train loss: 0.9870
[11/02 03:12:25 visual_prompt]: 	Test 100/123. loss: 1.397, 0.2447 s / batch. (data: 6.08e-05)max mem: 15.94594 GB 
[11/02 03:12:36 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.2336, average loss: 1.2416
[11/02 03:12:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.02	
[11/02 03:13:31 visual_prompt]: 	Test 100/323. loss: 0.068, 0.2246 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[11/02 03:14:23 visual_prompt]: 	Test 200/323. loss: 1.048, 0.2305 s / batch. (data: 5.15e-05)max mem: 15.94594 GB 
[11/02 03:15:15 visual_prompt]: 	Test 300/323. loss: 1.617, 0.2244 s / batch. (data: 4.65e-05)max mem: 15.94594 GB 
[11/02 03:15:26 visual_prompt]: Inference (test):avg data time: 8.21e-05, avg batch time: 0.2320, average loss: 1.1242
[11/02 03:15:26 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.07	
[11/02 03:15:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[11/02 03:16:31 visual_prompt]: 	Training 100/1106. train loss: 1.1778,	0.6377 s / batch. (data: 9.70e-03). ETA=15:51:07, max mem: 15.9 GB 
[11/02 03:17:35 visual_prompt]: 	Training 200/1106. train loss: 0.7915,	0.6570 s / batch. (data: 1.29e-02). ETA=16:18:44, max mem: 15.9 GB 
[11/02 03:18:39 visual_prompt]: 	Training 300/1106. train loss: 1.7197,	0.6376 s / batch. (data: 5.56e-03). ETA=15:48:50, max mem: 15.9 GB 
[11/02 03:19:43 visual_prompt]: 	Training 400/1106. train loss: 0.5547,	0.6243 s / batch. (data: 3.28e-04). ETA=15:27:58, max mem: 15.9 GB 
[11/02 03:20:46 visual_prompt]: 	Training 500/1106. train loss: 2.4195,	0.6447 s / batch. (data: 1.13e-03). ETA=15:57:13, max mem: 15.9 GB 
[11/02 03:21:50 visual_prompt]: 	Training 600/1106. train loss: 1.3290,	0.6720 s / batch. (data: 3.41e-04). ETA=16:36:37, max mem: 15.9 GB 
[11/02 03:22:54 visual_prompt]: 	Training 700/1106. train loss: 0.7261,	0.6233 s / batch. (data: 3.97e-04). ETA=15:23:18, max mem: 15.9 GB 
[11/02 03:23:58 visual_prompt]: 	Training 800/1106. train loss: 0.0374,	0.6242 s / batch. (data: 3.38e-04). ETA=15:23:37, max mem: 15.9 GB 
[11/02 03:25:01 visual_prompt]: 	Training 900/1106. train loss: 0.5885,	0.6353 s / batch. (data: 9.00e-04). ETA=15:39:04, max mem: 15.9 GB 
[11/02 03:26:05 visual_prompt]: 	Training 1000/1106. train loss: 0.0439,	0.6288 s / batch. (data: 3.47e-04). ETA=15:28:22, max mem: 15.9 GB 
[11/02 03:27:09 visual_prompt]: 	Training 1100/1106. train loss: 0.0688,	0.6183 s / batch. (data: 2.18e-04). ETA=15:11:55, max mem: 15.9 GB 
[11/02 03:27:13 visual_prompt]: Epoch 20 / 100: avg data time: 5.47e-03, avg batch time: 0.6393, average train loss: 0.9581
[11/02 03:28:07 visual_prompt]: 	Test 100/123. loss: 0.659, 0.2258 s / batch. (data: 5.10e-05)max mem: 15.94594 GB 
[11/02 03:28:18 visual_prompt]: Inference (val):avg data time: 1.41e-04, avg batch time: 0.2329, average loss: 0.8369
[11/02 03:28:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.17	
[11/02 03:29:13 visual_prompt]: 	Test 100/323. loss: 1.307, 0.2437 s / batch. (data: 2.96e-05)max mem: 15.94594 GB 
[11/02 03:30:06 visual_prompt]: 	Test 200/323. loss: 0.657, 0.2242 s / batch. (data: 3.10e-05)max mem: 15.94594 GB 
[11/02 03:30:58 visual_prompt]: 	Test 300/323. loss: 0.820, 0.2240 s / batch. (data: 4.29e-05)max mem: 15.94594 GB 
[11/02 03:31:08 visual_prompt]: Inference (test):avg data time: 4.88e-05, avg batch time: 0.2321, average loss: 0.8911
[11/02 03:31:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.09	rocauc: 57.23	
[11/02 03:31:08 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[11/02 03:32:14 visual_prompt]: 	Training 100/1106. train loss: 1.8665,	0.6410 s / batch. (data: 1.06e-02). ETA=15:44:09, max mem: 15.9 GB 
[11/02 03:33:18 visual_prompt]: 	Training 200/1106. train loss: 1.4906,	0.6193 s / batch. (data: 3.93e-04). ETA=15:11:09, max mem: 15.9 GB 
[11/02 03:34:21 visual_prompt]: 	Training 300/1106. train loss: 1.6533,	0.6341 s / batch. (data: 8.97e-04). ETA=15:31:53, max mem: 15.9 GB 
[11/02 03:35:25 visual_prompt]: 	Training 400/1106. train loss: 0.8108,	0.6467 s / batch. (data: 8.67e-04). ETA=15:49:25, max mem: 15.9 GB 
[11/02 03:36:29 visual_prompt]: 	Training 500/1106. train loss: 0.4408,	0.6360 s / batch. (data: 8.36e-04). ETA=15:32:35, max mem: 15.9 GB 
[11/02 03:37:33 visual_prompt]: 	Training 600/1106. train loss: 3.9670,	0.6369 s / batch. (data: 8.92e-04). ETA=15:32:47, max mem: 15.9 GB 
[11/02 03:38:37 visual_prompt]: 	Training 700/1106. train loss: 1.7950,	0.6508 s / batch. (data: 9.02e-04). ETA=15:52:04, max mem: 15.9 GB 
[11/02 03:39:40 visual_prompt]: 	Training 800/1106. train loss: 0.7256,	0.6480 s / batch. (data: 5.49e-03). ETA=15:46:55, max mem: 15.9 GB 
[11/02 03:40:44 visual_prompt]: 	Training 900/1106. train loss: 4.0045,	0.6205 s / batch. (data: 3.33e-04). ETA=15:05:41, max mem: 15.9 GB 
[11/02 03:41:48 visual_prompt]: 	Training 1000/1106. train loss: 0.0458,	0.6354 s / batch. (data: 3.48e-04). ETA=15:26:20, max mem: 15.9 GB 
[11/02 03:42:51 visual_prompt]: 	Training 1100/1106. train loss: 0.6504,	0.6198 s / batch. (data: 2.16e-04). ETA=15:02:34, max mem: 15.9 GB 
[11/02 03:42:55 visual_prompt]: Epoch 21 / 100: avg data time: 5.52e-03, avg batch time: 0.6388, average train loss: 1.0420
[11/02 03:43:49 visual_prompt]: 	Test 100/123. loss: 0.677, 0.2316 s / batch. (data: 3.41e-05)max mem: 15.94594 GB 
[11/02 03:44:01 visual_prompt]: Inference (val):avg data time: 4.95e-05, avg batch time: 0.2322, average loss: 0.7162
[11/02 03:44:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 57.65	
[11/02 03:44:56 visual_prompt]: 	Test 100/323. loss: 0.390, 0.2405 s / batch. (data: 5.44e-05)max mem: 15.94594 GB 
[11/02 03:45:47 visual_prompt]: 	Test 200/323. loss: 0.440, 0.2282 s / batch. (data: 3.08e-05)max mem: 15.94594 GB 
[11/02 03:46:39 visual_prompt]: 	Test 300/323. loss: 0.916, 0.2251 s / batch. (data: 2.91e-05)max mem: 15.94594 GB 
[11/02 03:46:50 visual_prompt]: Inference (test):avg data time: 4.74e-05, avg batch time: 0.2308, average loss: 0.6851
[11/02 03:46:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.76	rocauc: 57.56	
[11/02 03:46:50 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[11/02 03:47:56 visual_prompt]: 	Training 100/1106. train loss: 0.5870,	0.6372 s / batch. (data: 8.30e-04). ETA=15:26:50, max mem: 15.9 GB 
[11/02 03:49:00 visual_prompt]: 	Training 200/1106. train loss: 0.0580,	0.6219 s / batch. (data: 5.91e-04). ETA=15:03:29, max mem: 15.9 GB 
[11/02 03:50:04 visual_prompt]: 	Training 300/1106. train loss: 0.4418,	0.6235 s / batch. (data: 5.60e-03). ETA=15:04:53, max mem: 15.9 GB 
[11/02 03:51:07 visual_prompt]: 	Training 400/1106. train loss: 2.2115,	0.6372 s / batch. (data: 4.08e-04). ETA=15:23:42, max mem: 15.9 GB 
[11/02 03:52:11 visual_prompt]: 	Training 500/1106. train loss: 1.3425,	0.6330 s / batch. (data: 3.54e-04). ETA=15:16:29, max mem: 15.9 GB 
[11/02 03:53:15 visual_prompt]: 	Training 600/1106. train loss: 1.1516,	0.6308 s / batch. (data: 8.88e-04). ETA=15:12:20, max mem: 15.9 GB 
[11/02 03:54:18 visual_prompt]: 	Training 700/1106. train loss: 1.1406,	0.6313 s / batch. (data: 8.77e-04). ETA=15:11:58, max mem: 15.9 GB 
[11/02 03:55:22 visual_prompt]: 	Training 800/1106. train loss: 1.4315,	0.6600 s / batch. (data: 9.06e-04). ETA=15:52:22, max mem: 15.9 GB 
[11/02 03:56:26 visual_prompt]: 	Training 900/1106. train loss: 0.4623,	0.6322 s / batch. (data: 3.35e-04). ETA=15:11:09, max mem: 15.9 GB 
[11/02 03:57:30 visual_prompt]: 	Training 1000/1106. train loss: 0.0678,	0.6560 s / batch. (data: 8.72e-04). ETA=15:44:18, max mem: 15.9 GB 
[11/02 03:58:34 visual_prompt]: 	Training 1100/1106. train loss: 1.2812,	0.6203 s / batch. (data: 2.32e-04). ETA=14:51:54, max mem: 15.9 GB 
[11/02 03:58:37 visual_prompt]: Epoch 22 / 100: avg data time: 6.19e-03, avg batch time: 0.6393, average train loss: 0.9520
[11/02 03:59:31 visual_prompt]: 	Test 100/123. loss: 1.160, 0.2406 s / batch. (data: 5.60e-05)max mem: 15.94594 GB 
[11/02 03:59:43 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.2327, average loss: 1.0982
[11/02 03:59:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.31	
[11/02 04:00:38 visual_prompt]: 	Test 100/323. loss: 0.099, 0.2467 s / batch. (data: 4.65e-05)max mem: 15.94594 GB 
[11/02 04:01:31 visual_prompt]: 	Test 200/323. loss: 0.903, 0.2391 s / batch. (data: 5.34e-05)max mem: 15.94594 GB 
[11/02 04:02:22 visual_prompt]: 	Test 300/323. loss: 1.376, 0.2287 s / batch. (data: 5.70e-05)max mem: 15.94594 GB 
[11/02 04:02:33 visual_prompt]: Inference (test):avg data time: 1.06e-04, avg batch time: 0.2322, average loss: 1.0004
[11/02 04:02:33 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.97	
[11/02 04:02:33 visual_prompt]: Stopping early.
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

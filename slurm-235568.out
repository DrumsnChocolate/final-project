[09/17 00:32:24 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 00:32:24 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 00:32:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)', 'DATA.NUMBER_CLASSES', '16', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed42'], train_type='')
[09/17 00:32:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 00:32:24 visual_prompt]: Training with config:
[09/17 00:32:24 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)',
          'NO_TEST': False,
          'NUMBER_CLASSES': 16,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed42/vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 00:32:24 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 00:32:24.681232: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 00:32:24.881847: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 00:32:30.138332: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 00:32:30.138424: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 00:32:30.138433: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 00:32:40.764721: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 00:32:40.764985: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 00:32:40.765016: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 00:32:40 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
2023-09-17 00:32:40.884987: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[:800]+train[589824:590024], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 00:32:42 visual_prompt]: Number of images: 1000
[09/17 00:32:42 visual_prompt]: Number of classes: 16 / 16
[09/17 00:32:42 visual_prompt]: Loading validation data...
[09/17 00:32:42 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[589824:590024], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 00:32:43 visual_prompt]: Number of images: 200
[09/17 00:32:43 visual_prompt]: Number of classes: 16 / 16
[09/17 00:32:43 visual_prompt]: Loading test data...
[09/17 00:32:43 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[663552:], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 00:34:19 visual_prompt]: Number of images: 73728
[09/17 00:34:19 visual_prompt]: Number of classes: 16 / 16
[09/17 00:34:20 visual_prompt]: Constructing models...
[09/17 00:34:23 visual_prompt]: Total Parameters: 86732560	 Gradient Parameters: 933904
[09/17 00:34:23 visual_prompt]: tuned percent:1.077
[09/17 00:34:26 visual_prompt]: Device used for model: 0
[09/17 00:34:26 visual_prompt]: Setting up Evalutator...
[09/17 00:34:26 visual_prompt]: Setting up Trainer...
[09/17 00:34:26 visual_prompt]: 	Setting up the optimizer...
[09/17 00:34:26 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 00:34:44 visual_prompt]: Epoch 1 / 100: avg data time: 3.45e-01, avg batch time: 0.8357, average train loss: 3.0572
[09/17 00:34:53 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1424, average loss: 3.0850
[09/17 00:34:53 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.50	top5: 31.00	
[09/17 00:35:17 visual_prompt]: 	Test 100/1152. loss: 2.947, 0.1826 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 00:35:37 visual_prompt]: 	Test 200/1152. loss: 3.193, 0.2093 s / batch. (data: 1.27e-02)max mem: 17.22454 GB 
[09/17 00:35:57 visual_prompt]: 	Test 300/1152. loss: 3.128, 0.1896 s / batch. (data: 3.34e-05)max mem: 17.22454 GB 
[09/17 00:36:17 visual_prompt]: 	Test 400/1152. loss: 2.950, 0.1847 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 00:36:36 visual_prompt]: 	Test 500/1152. loss: 3.032, 0.1958 s / batch. (data: 3.84e-05)max mem: 17.22454 GB 
[09/17 00:36:56 visual_prompt]: 	Test 600/1152. loss: 3.029, 0.1878 s / batch. (data: 1.69e-04)max mem: 17.22454 GB 
[09/17 00:37:15 visual_prompt]: 	Test 700/1152. loss: 2.963, 0.2095 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 00:37:35 visual_prompt]: 	Test 800/1152. loss: 3.024, 0.1850 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 00:37:55 visual_prompt]: 	Test 900/1152. loss: 2.842, 0.2147 s / batch. (data: 3.10e-02)max mem: 17.22454 GB 
[09/17 00:38:15 visual_prompt]: 	Test 1000/1152. loss: 3.043, 0.1837 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 00:38:35 visual_prompt]: 	Test 1100/1152. loss: 2.932, 0.1836 s / batch. (data: 2.17e-04)max mem: 17.22454 GB 
[09/17 00:38:50 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1964, average loss: 3.0520
[09/17 00:38:51 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.28	top5: 31.29	
[09/17 00:38:51 visual_prompt]: Best epoch 1: best metric: 0.045
[09/17 00:38:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/17 00:39:06 visual_prompt]: Epoch 2 / 100: avg data time: 3.14e-01, avg batch time: 0.7161, average train loss: 3.5637
[09/17 00:39:15 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1485, average loss: 3.1207
[09/17 00:39:15 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 5.50	top5: 30.00	
[09/17 00:39:40 visual_prompt]: 	Test 100/1152. loss: 2.910, 0.1900 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 00:39:59 visual_prompt]: 	Test 200/1152. loss: 3.265, 0.2326 s / batch. (data: 3.65e-05)max mem: 17.22454 GB 
[09/17 00:40:19 visual_prompt]: 	Test 300/1152. loss: 2.977, 0.2126 s / batch. (data: 1.21e-04)max mem: 17.22454 GB 
[09/17 00:40:38 visual_prompt]: 	Test 400/1152. loss: 3.344, 0.1841 s / batch. (data: 9.80e-05)max mem: 17.22454 GB 
[09/17 00:40:58 visual_prompt]: 	Test 500/1152. loss: 3.495, 0.1852 s / batch. (data: 3.00e-05)max mem: 17.22454 GB 
[09/17 00:41:18 visual_prompt]: 	Test 600/1152. loss: 3.278, 0.1981 s / batch. (data: 1.44e-02)max mem: 17.22454 GB 
[09/17 00:41:38 visual_prompt]: 	Test 700/1152. loss: 3.061, 0.1981 s / batch. (data: 1.46e-02)max mem: 17.22454 GB 
[09/17 00:41:57 visual_prompt]: 	Test 800/1152. loss: 3.220, 0.1908 s / batch. (data: 1.09e-04)max mem: 17.22454 GB 
[09/17 00:42:17 visual_prompt]: 	Test 900/1152. loss: 3.562, 0.1844 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 00:42:37 visual_prompt]: 	Test 1000/1152. loss: 3.376, 0.1918 s / batch. (data: 1.50e-04)max mem: 17.22454 GB 
[09/17 00:42:57 visual_prompt]: 	Test 1100/1152. loss: 3.409, 0.1842 s / batch. (data: 2.96e-05)max mem: 17.22454 GB 
[09/17 00:43:13 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1963, average loss: 3.2303
[09/17 00:43:13 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 31.18	
[09/17 00:43:13 visual_prompt]: Best epoch 2: best metric: 0.055
[09/17 00:43:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/17 00:43:28 visual_prompt]: Epoch 3 / 100: avg data time: 3.03e-01, avg batch time: 0.7062, average train loss: 2.9630
[09/17 00:43:37 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1495, average loss: 2.8363
[09/17 00:43:37 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 5.50	top5: 31.00	
[09/17 00:44:01 visual_prompt]: 	Test 100/1152. loss: 2.794, 0.1976 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 00:44:21 visual_prompt]: 	Test 200/1152. loss: 2.918, 0.1921 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 00:44:41 visual_prompt]: 	Test 300/1152. loss: 2.824, 0.1958 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 00:45:01 visual_prompt]: 	Test 400/1152. loss: 2.797, 0.2181 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 00:45:20 visual_prompt]: 	Test 500/1152. loss: 2.796, 0.1849 s / batch. (data: 2.06e-04)max mem: 17.22454 GB 
[09/17 00:45:41 visual_prompt]: 	Test 600/1152. loss: 2.841, 0.2154 s / batch. (data: 1.57e-02)max mem: 17.22454 GB 
[09/17 00:46:00 visual_prompt]: 	Test 700/1152. loss: 2.842, 0.1848 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 00:46:20 visual_prompt]: 	Test 800/1152. loss: 2.835, 0.1960 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 00:46:40 visual_prompt]: 	Test 900/1152. loss: 2.856, 0.1878 s / batch. (data: 1.68e-04)max mem: 17.22454 GB 
[09/17 00:47:00 visual_prompt]: 	Test 1000/1152. loss: 2.888, 0.1843 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 00:47:20 visual_prompt]: 	Test 1100/1152. loss: 2.847, 0.2046 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 00:47:35 visual_prompt]: Inference (test):avg data time: 8.05e-03, avg batch time: 0.1970, average loss: 2.8399
[09/17 00:47:36 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 31.24	
[09/17 00:47:36 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/17 00:47:51 visual_prompt]: Epoch 4 / 100: avg data time: 2.98e-01, avg batch time: 0.7046, average train loss: 3.0429
[09/17 00:48:01 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1430, average loss: 3.0265
[09/17 00:48:01 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.00	top5: 35.50	
[09/17 00:48:25 visual_prompt]: 	Test 100/1152. loss: 2.839, 0.1829 s / batch. (data: 8.96e-05)max mem: 17.22454 GB 
[09/17 00:48:45 visual_prompt]: 	Test 200/1152. loss: 3.010, 0.1836 s / batch. (data: 1.69e-04)max mem: 17.22454 GB 
[09/17 00:49:04 visual_prompt]: 	Test 300/1152. loss: 3.061, 0.1892 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 00:49:24 visual_prompt]: 	Test 400/1152. loss: 3.144, 0.1961 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 00:49:44 visual_prompt]: 	Test 500/1152. loss: 2.887, 0.1848 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 00:50:04 visual_prompt]: 	Test 600/1152. loss: 3.027, 0.1889 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 00:50:23 visual_prompt]: 	Test 700/1152. loss: 2.835, 0.2053 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 00:50:43 visual_prompt]: 	Test 800/1152. loss: 3.020, 0.2069 s / batch. (data: 1.83e-02)max mem: 17.22454 GB 
[09/17 00:51:03 visual_prompt]: 	Test 900/1152. loss: 3.161, 0.2052 s / batch. (data: 2.21e-02)max mem: 17.22454 GB 
[09/17 00:51:23 visual_prompt]: 	Test 1000/1152. loss: 3.010, 0.1844 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 00:51:42 visual_prompt]: 	Test 1100/1152. loss: 3.013, 0.1844 s / batch. (data: 1.56e-04)max mem: 17.22454 GB 
[09/17 00:51:58 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1965, average loss: 3.0411
[09/17 00:51:58 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 31.21	
[09/17 00:51:58 visual_prompt]: Best epoch 4: best metric: 0.060
[09/17 00:51:58 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/17 00:52:14 visual_prompt]: Epoch 5 / 100: avg data time: 3.14e-01, avg batch time: 0.7175, average train loss: 3.0332
[09/17 00:52:23 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.1450, average loss: 3.0204
[09/17 00:52:23 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.50	top5: 28.00	
[09/17 00:52:48 visual_prompt]: 	Test 100/1152. loss: 2.960, 0.1948 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 00:53:08 visual_prompt]: 	Test 200/1152. loss: 3.106, 0.1846 s / batch. (data: 1.92e-04)max mem: 17.22454 GB 
[09/17 00:53:27 visual_prompt]: 	Test 300/1152. loss: 2.878, 0.1844 s / batch. (data: 1.58e-04)max mem: 17.22454 GB 
[09/17 00:53:47 visual_prompt]: 	Test 400/1152. loss: 2.948, 0.1985 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 00:54:07 visual_prompt]: 	Test 500/1152. loss: 3.097, 0.1932 s / batch. (data: 1.67e-04)max mem: 17.22454 GB 
[09/17 00:54:27 visual_prompt]: 	Test 600/1152. loss: 3.162, 0.1956 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 00:54:47 visual_prompt]: 	Test 700/1152. loss: 2.868, 0.1851 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 00:55:06 visual_prompt]: 	Test 800/1152. loss: 2.931, 0.2091 s / batch. (data: 2.52e-02)max mem: 17.22454 GB 
[09/17 00:55:26 visual_prompt]: 	Test 900/1152. loss: 3.027, 0.2003 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 00:55:46 visual_prompt]: 	Test 1000/1152. loss: 3.019, 0.1955 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 00:56:06 visual_prompt]: 	Test 1100/1152. loss: 2.967, 0.1994 s / batch. (data: 1.70e-04)max mem: 17.22454 GB 
[09/17 00:56:22 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1966, average loss: 3.0011
[09/17 00:56:22 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.22	top5: 31.49	
[09/17 00:56:22 visual_prompt]: Best epoch 5: best metric: 0.075
[09/17 00:56:22 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/17 00:56:37 visual_prompt]: Epoch 6 / 100: avg data time: 3.07e-01, avg batch time: 0.7124, average train loss: 3.1255
[09/17 00:56:47 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1436, average loss: 3.1822
[09/17 00:56:47 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 9.50	top5: 37.50	
[09/17 00:57:11 visual_prompt]: 	Test 100/1152. loss: 3.373, 0.1834 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 00:57:31 visual_prompt]: 	Test 200/1152. loss: 3.266, 0.1975 s / batch. (data: 1.46e-02)max mem: 17.22454 GB 
[09/17 00:57:50 visual_prompt]: 	Test 300/1152. loss: 3.346, 0.2076 s / batch. (data: 2.45e-02)max mem: 17.22454 GB 
[09/17 00:58:10 visual_prompt]: 	Test 400/1152. loss: 3.325, 0.1847 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 00:58:30 visual_prompt]: 	Test 500/1152. loss: 3.295, 0.2359 s / batch. (data: 3.93e-02)max mem: 17.22454 GB 
[09/17 00:58:50 visual_prompt]: 	Test 600/1152. loss: 3.313, 0.2074 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 00:59:09 visual_prompt]: 	Test 700/1152. loss: 3.310, 0.1845 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 00:59:29 visual_prompt]: 	Test 800/1152. loss: 3.270, 0.1835 s / batch. (data: 3.36e-05)max mem: 17.22454 GB 
[09/17 00:59:49 visual_prompt]: 	Test 900/1152. loss: 3.718, 0.1998 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 01:00:09 visual_prompt]: 	Test 1000/1152. loss: 3.523, 0.1998 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 01:00:29 visual_prompt]: 	Test 1100/1152. loss: 3.291, 0.1921 s / batch. (data: 3.36e-05)max mem: 17.22454 GB 
[09/17 01:00:45 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1967, average loss: 3.3391
[09/17 01:00:45 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.13	top5: 31.07	
[09/17 01:00:45 visual_prompt]: Best epoch 6: best metric: 0.095
[09/17 01:00:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/17 01:01:00 visual_prompt]: Epoch 7 / 100: avg data time: 3.01e-01, avg batch time: 0.7068, average train loss: 3.3825
[09/17 01:01:10 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1441, average loss: 3.0838
[09/17 01:01:10 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 35.00	
[09/17 01:01:34 visual_prompt]: 	Test 100/1152. loss: 3.133, 0.2259 s / batch. (data: 4.40e-02)max mem: 17.22454 GB 
[09/17 01:01:54 visual_prompt]: 	Test 200/1152. loss: 3.073, 0.2283 s / batch. (data: 4.56e-02)max mem: 17.22454 GB 
[09/17 01:02:13 visual_prompt]: 	Test 300/1152. loss: 3.128, 0.2120 s / batch. (data: 1.69e-02)max mem: 17.22454 GB 
[09/17 01:02:33 visual_prompt]: 	Test 400/1152. loss: 3.192, 0.1837 s / batch. (data: 3.70e-05)max mem: 17.22454 GB 
[09/17 01:02:53 visual_prompt]: 	Test 500/1152. loss: 3.062, 0.1960 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 01:03:13 visual_prompt]: 	Test 600/1152. loss: 3.053, 0.1851 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 01:03:32 visual_prompt]: 	Test 700/1152. loss: 3.136, 0.1847 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 01:03:52 visual_prompt]: 	Test 800/1152. loss: 3.245, 0.1846 s / batch. (data: 1.68e-04)max mem: 17.22454 GB 
[09/17 01:04:12 visual_prompt]: 	Test 900/1152. loss: 3.374, 0.1966 s / batch. (data: 1.31e-02)max mem: 17.22454 GB 
[09/17 01:04:32 visual_prompt]: 	Test 1000/1152. loss: 3.207, 0.2150 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 01:04:52 visual_prompt]: 	Test 1100/1152. loss: 3.311, 0.1919 s / batch. (data: 9.42e-05)max mem: 17.22454 GB 
[09/17 01:05:08 visual_prompt]: Inference (test):avg data time: 8.39e-03, avg batch time: 0.1968, average loss: 3.1598
[09/17 01:05:08 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 31.25	
[09/17 01:05:08 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/17 01:05:23 visual_prompt]: Epoch 8 / 100: avg data time: 3.12e-01, avg batch time: 0.7141, average train loss: 3.5547
[09/17 01:05:33 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1429, average loss: 3.5129
[09/17 01:05:33 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 34.00	
[09/17 01:05:58 visual_prompt]: 	Test 100/1152. loss: 3.360, 0.1830 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 01:06:17 visual_prompt]: 	Test 200/1152. loss: 3.685, 0.2044 s / batch. (data: 2.14e-02)max mem: 17.22454 GB 
[09/17 01:06:37 visual_prompt]: 	Test 300/1152. loss: 3.568, 0.2081 s / batch. (data: 1.35e-02)max mem: 17.22454 GB 
[09/17 01:06:57 visual_prompt]: 	Test 400/1152. loss: 3.560, 0.1946 s / batch. (data: 1.13e-02)max mem: 17.22454 GB 
[09/17 01:07:16 visual_prompt]: 	Test 500/1152. loss: 3.949, 0.2247 s / batch. (data: 4.16e-02)max mem: 17.22454 GB 
[09/17 01:07:36 visual_prompt]: 	Test 600/1152. loss: 3.832, 0.1978 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 01:07:56 visual_prompt]: 	Test 700/1152. loss: 3.509, 0.2135 s / batch. (data: 1.31e-02)max mem: 17.22454 GB 
[09/17 01:08:16 visual_prompt]: 	Test 800/1152. loss: 3.355, 0.1923 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 01:08:36 visual_prompt]: 	Test 900/1152. loss: 3.914, 0.1918 s / batch. (data: 8.41e-03)max mem: 17.22454 GB 
[09/17 01:08:57 visual_prompt]: 	Test 1000/1152. loss: 3.972, 0.1988 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 01:09:17 visual_prompt]: 	Test 1100/1152. loss: 3.348, 0.2047 s / batch. (data: 2.16e-04)max mem: 17.22454 GB 
[09/17 01:09:32 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1976, average loss: 3.5865
[09/17 01:09:32 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.17	top5: 30.83	
[09/17 01:09:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/17 01:09:48 visual_prompt]: Epoch 9 / 100: avg data time: 3.10e-01, avg batch time: 0.7121, average train loss: 3.2092
[09/17 01:09:57 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1428, average loss: 3.1564
[09/17 01:09:57 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 37.00	
[09/17 01:10:21 visual_prompt]: 	Test 100/1152. loss: 3.337, 0.2004 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 01:10:41 visual_prompt]: 	Test 200/1152. loss: 3.216, 0.1962 s / batch. (data: 1.33e-02)max mem: 17.22454 GB 
[09/17 01:11:00 visual_prompt]: 	Test 300/1152. loss: 3.168, 0.2161 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 01:11:20 visual_prompt]: 	Test 400/1152. loss: 3.086, 0.2158 s / batch. (data: 1.92e-02)max mem: 17.22454 GB 
[09/17 01:11:40 visual_prompt]: 	Test 500/1152. loss: 3.465, 0.2164 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 01:12:00 visual_prompt]: 	Test 600/1152. loss: 3.368, 0.1843 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 01:12:19 visual_prompt]: 	Test 700/1152. loss: 3.130, 0.2130 s / batch. (data: 1.44e-02)max mem: 17.22454 GB 
[09/17 01:12:40 visual_prompt]: 	Test 800/1152. loss: 3.004, 0.1967 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 01:13:00 visual_prompt]: 	Test 900/1152. loss: 3.270, 0.1835 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 01:13:20 visual_prompt]: 	Test 1000/1152. loss: 3.323, 0.1953 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 01:13:40 visual_prompt]: 	Test 1100/1152. loss: 3.108, 0.1950 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 01:13:55 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1976, average loss: 3.2056
[09/17 01:13:56 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.21	top5: 31.09	
[09/17 01:13:56 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/17 01:14:11 visual_prompt]: Epoch 10 / 100: avg data time: 3.11e-01, avg batch time: 0.7147, average train loss: 3.1905
[09/17 01:14:20 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1448, average loss: 3.6138
[09/17 01:14:20 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 3.50	top5: 27.00	
[09/17 01:14:46 visual_prompt]: 	Test 100/1152. loss: 4.002, 0.1840 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 01:15:05 visual_prompt]: 	Test 200/1152. loss: 3.506, 0.1958 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 01:15:25 visual_prompt]: 	Test 300/1152. loss: 3.378, 0.1918 s / batch. (data: 1.16e-04)max mem: 17.22454 GB 
[09/17 01:15:45 visual_prompt]: 	Test 400/1152. loss: 3.746, 0.2276 s / batch. (data: 3.62e-05)max mem: 17.22454 GB 
[09/17 01:16:04 visual_prompt]: 	Test 500/1152. loss: 3.908, 0.1959 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 01:16:24 visual_prompt]: 	Test 600/1152. loss: 3.441, 0.1930 s / batch. (data: 1.90e-04)max mem: 17.22454 GB 
[09/17 01:16:44 visual_prompt]: 	Test 700/1152. loss: 3.763, 0.2084 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 01:17:04 visual_prompt]: 	Test 800/1152. loss: 3.387, 0.1845 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 01:17:24 visual_prompt]: 	Test 900/1152. loss: 3.431, 0.1857 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 01:17:43 visual_prompt]: 	Test 1000/1152. loss: 3.260, 0.1996 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 01:18:03 visual_prompt]: 	Test 1100/1152. loss: 3.691, 0.2075 s / batch. (data: 4.41e-05)max mem: 17.22454 GB 
[09/17 01:18:19 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1973, average loss: 3.6018
[09/17 01:18:19 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 31.14	
[09/17 01:18:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/17 01:18:35 visual_prompt]: Epoch 11 / 100: avg data time: 3.08e-01, avg batch time: 0.7131, average train loss: 3.4970
[09/17 01:18:44 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1430, average loss: 3.4543
[09/17 01:18:44 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 13.00	top5: 39.00	
[09/17 01:19:09 visual_prompt]: 	Test 100/1152. loss: 3.467, 0.2049 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 01:19:29 visual_prompt]: 	Test 200/1152. loss: 3.189, 0.1840 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 01:19:48 visual_prompt]: 	Test 300/1152. loss: 2.990, 0.1921 s / batch. (data: 1.67e-04)max mem: 17.22454 GB 
[09/17 01:20:08 visual_prompt]: 	Test 400/1152. loss: 3.614, 0.1838 s / batch. (data: 8.77e-05)max mem: 17.22454 GB 
[09/17 01:20:28 visual_prompt]: 	Test 500/1152. loss: 3.720, 0.1840 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 01:20:48 visual_prompt]: 	Test 600/1152. loss: 3.449, 0.1909 s / batch. (data: 4.08e-05)max mem: 17.22454 GB 
[09/17 01:21:08 visual_prompt]: 	Test 700/1152. loss: 3.719, 0.1959 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 01:21:28 visual_prompt]: 	Test 800/1152. loss: 3.285, 0.1977 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 01:21:47 visual_prompt]: 	Test 900/1152. loss: 3.211, 0.1931 s / batch. (data: 1.50e-04)max mem: 17.22454 GB 
[09/17 01:22:07 visual_prompt]: 	Test 1000/1152. loss: 3.291, 0.2110 s / batch. (data: 2.74e-02)max mem: 17.22454 GB 
[09/17 01:22:27 visual_prompt]: 	Test 1100/1152. loss: 3.717, 0.1953 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 01:22:43 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1971, average loss: 3.4012
[09/17 01:22:44 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 13.18	top5: 38.99	
[09/17 01:22:44 visual_prompt]: Best epoch 11: best metric: 0.130
[09/17 01:22:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/17 01:22:59 visual_prompt]: Epoch 12 / 100: avg data time: 3.13e-01, avg batch time: 0.7173, average train loss: 4.5764
[09/17 01:23:08 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1431, average loss: 7.0476
[09/17 01:23:08 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 9.50	top5: 36.00	
[09/17 01:23:33 visual_prompt]: 	Test 100/1152. loss: 8.811, 0.1959 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 01:23:53 visual_prompt]: 	Test 200/1152. loss: 8.460, 0.1842 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 01:24:13 visual_prompt]: 	Test 300/1152. loss: 8.662, 0.1840 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 01:24:33 visual_prompt]: 	Test 400/1152. loss: 7.402, 0.1915 s / batch. (data: 6.51e-05)max mem: 17.22454 GB 
[09/17 01:24:52 visual_prompt]: 	Test 500/1152. loss: 9.694, 0.1999 s / batch. (data: 1.64e-02)max mem: 17.22454 GB 
[09/17 01:25:12 visual_prompt]: 	Test 600/1152. loss: 8.083, 0.2046 s / batch. (data: 1.20e-02)max mem: 17.22454 GB 
[09/17 01:25:32 visual_prompt]: 	Test 700/1152. loss: 8.459, 0.1842 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 01:25:52 visual_prompt]: 	Test 800/1152. loss: 7.876, 0.1846 s / batch. (data: 1.64e-04)max mem: 17.22454 GB 
[09/17 01:26:12 visual_prompt]: 	Test 900/1152. loss: 7.642, 0.1842 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 01:26:33 visual_prompt]: 	Test 1000/1152. loss: 7.458, 0.1846 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 01:26:53 visual_prompt]: 	Test 1100/1152. loss: 7.274, 0.1838 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 01:27:09 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1989, average loss: 7.9388
[09/17 01:27:09 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.13	top5: 31.10	
[09/17 01:27:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/17 01:27:25 visual_prompt]: Epoch 13 / 100: avg data time: 2.93e-01, avg batch time: 0.6985, average train loss: 7.6402
[09/17 01:27:34 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1452, average loss: 8.9294
[09/17 01:27:34 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 33.00	
[09/17 01:27:59 visual_prompt]: 	Test 100/1152. loss: 8.174, 0.2101 s / batch. (data: 2.72e-02)max mem: 17.22454 GB 
[09/17 01:28:19 visual_prompt]: 	Test 200/1152. loss: 10.324, 0.1840 s / batch. (data: 1.06e-04)max mem: 17.22454 GB 
[09/17 01:28:39 visual_prompt]: 	Test 300/1152. loss: 7.785, 0.2078 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 01:28:59 visual_prompt]: 	Test 400/1152. loss: 10.726, 0.1838 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 01:29:19 visual_prompt]: 	Test 500/1152. loss: 10.847, 0.2150 s / batch. (data: 3.19e-02)max mem: 17.22454 GB 
[09/17 01:29:38 visual_prompt]: 	Test 600/1152. loss: 10.409, 0.1834 s / batch. (data: 3.96e-05)max mem: 17.22454 GB 
[09/17 01:29:58 visual_prompt]: 	Test 700/1152. loss: 7.174, 0.1946 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 01:30:18 visual_prompt]: 	Test 800/1152. loss: 8.485, 0.2475 s / batch. (data: 2.57e-04)max mem: 17.22454 GB 
[09/17 01:30:38 visual_prompt]: 	Test 900/1152. loss: 11.918, 0.1845 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 01:30:58 visual_prompt]: 	Test 1000/1152. loss: 8.814, 0.1840 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 01:31:18 visual_prompt]: 	Test 1100/1152. loss: 8.966, 0.2182 s / batch. (data: 5.10e-05)max mem: 17.22454 GB 
[09/17 01:31:33 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1977, average loss: 9.3074
[09/17 01:31:34 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 31.13	
[09/17 01:31:34 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/17 01:31:49 visual_prompt]: Epoch 14 / 100: avg data time: 3.04e-01, avg batch time: 0.7083, average train loss: 10.6637
[09/17 01:31:58 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1429, average loss: 10.4308
[09/17 01:31:58 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.50	top5: 28.50	
[09/17 01:32:22 visual_prompt]: 	Test 100/1152. loss: 10.426, 0.2007 s / batch. (data: 9.56e-05)max mem: 17.22454 GB 
[09/17 01:32:42 visual_prompt]: 	Test 200/1152. loss: 10.732, 0.1839 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 01:33:02 visual_prompt]: 	Test 300/1152. loss: 10.642, 0.1956 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 01:33:21 visual_prompt]: 	Test 400/1152. loss: 10.463, 0.2011 s / batch. (data: 1.70e-02)max mem: 17.22454 GB 
[09/17 01:33:42 visual_prompt]: 	Test 500/1152. loss: 10.332, 0.1839 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 01:34:01 visual_prompt]: 	Test 600/1152. loss: 10.357, 0.1861 s / batch. (data: 4.03e-05)max mem: 17.22454 GB 
[09/17 01:34:21 visual_prompt]: 	Test 700/1152. loss: 9.083, 0.1981 s / batch. (data: 1.41e-02)max mem: 17.22454 GB 
[09/17 01:34:41 visual_prompt]: 	Test 800/1152. loss: 9.423, 0.1851 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 01:35:01 visual_prompt]: 	Test 900/1152. loss: 9.655, 0.1986 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 01:35:20 visual_prompt]: 	Test 1000/1152. loss: 8.908, 0.1999 s / batch. (data: 1.57e-02)max mem: 17.22454 GB 
[09/17 01:35:40 visual_prompt]: 	Test 1100/1152. loss: 8.981, 0.2066 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 01:35:56 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1968, average loss: 10.2385
[09/17 01:35:56 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.22	top5: 31.32	
[09/17 01:35:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/17 01:36:11 visual_prompt]: Epoch 15 / 100: avg data time: 2.97e-01, avg batch time: 0.7016, average train loss: 13.2096
[09/17 01:36:20 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1491, average loss: 14.9434
[09/17 01:36:20 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.50	top5: 26.00	
[09/17 01:36:44 visual_prompt]: 	Test 100/1152. loss: 15.486, 0.1928 s / batch. (data: 1.72e-04)max mem: 17.22454 GB 
[09/17 01:37:04 visual_prompt]: 	Test 200/1152. loss: 17.036, 0.1834 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 01:37:24 visual_prompt]: 	Test 300/1152. loss: 16.466, 0.2110 s / batch. (data: 1.31e-02)max mem: 17.22454 GB 
[09/17 01:37:44 visual_prompt]: 	Test 400/1152. loss: 14.267, 0.1845 s / batch. (data: 1.03e-04)max mem: 17.22454 GB 
[09/17 01:38:03 visual_prompt]: 	Test 500/1152. loss: 15.420, 0.2027 s / batch. (data: 1.50e-04)max mem: 17.22454 GB 
[09/17 01:38:23 visual_prompt]: 	Test 600/1152. loss: 15.701, 0.1973 s / batch. (data: 1.36e-02)max mem: 17.22454 GB 
[09/17 01:38:43 visual_prompt]: 	Test 700/1152. loss: 13.459, 0.1975 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 01:39:04 visual_prompt]: 	Test 800/1152. loss: 14.104, 0.1842 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 01:39:23 visual_prompt]: 	Test 900/1152. loss: 13.695, 0.1843 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 01:39:43 visual_prompt]: 	Test 1000/1152. loss: 12.167, 0.1847 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 01:40:03 visual_prompt]: 	Test 1100/1152. loss: 12.993, 0.1840 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 01:40:19 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1971, average loss: 15.0060
[09/17 01:40:19 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.22	top5: 31.15	
[09/17 01:40:19 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/17 01:40:35 visual_prompt]: Epoch 16 / 100: avg data time: 3.11e-01, avg batch time: 0.7130, average train loss: 12.5282
[09/17 01:40:44 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1429, average loss: 13.9145
[09/17 01:40:44 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.50	top5: 29.50	
[09/17 01:41:09 visual_prompt]: 	Test 100/1152. loss: 9.503, 0.1958 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 01:41:28 visual_prompt]: 	Test 200/1152. loss: 14.892, 0.1841 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 01:41:48 visual_prompt]: 	Test 300/1152. loss: 13.541, 0.1976 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 01:42:08 visual_prompt]: 	Test 400/1152. loss: 12.097, 0.2128 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 01:42:28 visual_prompt]: 	Test 500/1152. loss: 13.293, 0.1842 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 01:42:48 visual_prompt]: 	Test 600/1152. loss: 14.580, 0.1888 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 01:43:08 visual_prompt]: 	Test 700/1152. loss: 11.163, 0.1985 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 01:43:28 visual_prompt]: 	Test 800/1152. loss: 13.949, 0.2140 s / batch. (data: 1.37e-02)max mem: 17.22454 GB 
[09/17 01:43:48 visual_prompt]: 	Test 900/1152. loss: 12.864, 0.1842 s / batch. (data: 1.64e-04)max mem: 17.22454 GB 
[09/17 01:44:08 visual_prompt]: 	Test 1000/1152. loss: 14.091, 0.1995 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 01:44:28 visual_prompt]: 	Test 1100/1152. loss: 13.787, 0.1848 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 01:44:43 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1979, average loss: 13.4093
[09/17 01:44:44 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.22	top5: 31.26	
[09/17 01:44:44 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/17 01:44:59 visual_prompt]: Epoch 17 / 100: avg data time: 3.04e-01, avg batch time: 0.7083, average train loss: 10.2596
[09/17 01:45:08 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1428, average loss: 12.4038
[09/17 01:45:08 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.00	top5: 31.00	
[09/17 01:45:33 visual_prompt]: 	Test 100/1152. loss: 10.697, 0.1992 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 01:45:52 visual_prompt]: 	Test 200/1152. loss: 11.900, 0.1964 s / batch. (data: 1.34e-02)max mem: 17.22454 GB 
[09/17 01:46:12 visual_prompt]: 	Test 300/1152. loss: 11.708, 0.2044 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 01:46:32 visual_prompt]: 	Test 400/1152. loss: 11.185, 0.1849 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 01:46:52 visual_prompt]: 	Test 500/1152. loss: 11.196, 0.1841 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 01:47:12 visual_prompt]: 	Test 600/1152. loss: 12.981, 0.1967 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 01:47:32 visual_prompt]: 	Test 700/1152. loss: 10.709, 0.2315 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 01:47:51 visual_prompt]: 	Test 800/1152. loss: 12.092, 0.1962 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 01:48:11 visual_prompt]: 	Test 900/1152. loss: 11.978, 0.1847 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 01:48:31 visual_prompt]: 	Test 1000/1152. loss: 12.630, 0.1842 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 01:48:51 visual_prompt]: 	Test 1100/1152. loss: 13.012, 0.1841 s / batch. (data: 3.08e-05)max mem: 17.22454 GB 
[09/17 01:49:06 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1969, average loss: 12.0294
[09/17 01:49:07 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.34	top5: 31.39	
[09/17 01:49:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/17 01:49:22 visual_prompt]: Epoch 18 / 100: avg data time: 3.17e-01, avg batch time: 0.7211, average train loss: 10.5186
[09/17 01:49:31 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1432, average loss: 11.2965
[09/17 01:49:31 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.00	top5: 34.50	
[09/17 01:49:56 visual_prompt]: 	Test 100/1152. loss: 11.342, 0.2106 s / batch. (data: 2.82e-02)max mem: 17.22454 GB 
[09/17 01:50:15 visual_prompt]: 	Test 200/1152. loss: 12.139, 0.1958 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 01:50:35 visual_prompt]: 	Test 300/1152. loss: 12.480, 0.1966 s / batch. (data: 1.70e-04)max mem: 17.22454 GB 
[09/17 01:50:55 visual_prompt]: 	Test 400/1152. loss: 11.165, 0.1930 s / batch. (data: 9.39e-03)max mem: 17.22454 GB 
[09/17 01:51:15 visual_prompt]: 	Test 500/1152. loss: 11.553, 0.1988 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 01:51:34 visual_prompt]: 	Test 600/1152. loss: 11.621, 0.1969 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 01:51:54 visual_prompt]: 	Test 700/1152. loss: 11.858, 0.1964 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 01:52:14 visual_prompt]: 	Test 800/1152. loss: 10.357, 0.2237 s / batch. (data: 3.99e-02)max mem: 17.22454 GB 
[09/17 01:52:34 visual_prompt]: 	Test 900/1152. loss: 10.669, 0.1999 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 01:52:54 visual_prompt]: 	Test 1000/1152. loss: 11.924, 0.1917 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 01:53:14 visual_prompt]: 	Test 1100/1152. loss: 10.201, 0.1840 s / batch. (data: 1.69e-04)max mem: 17.22454 GB 
[09/17 01:53:30 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1968, average loss: 11.5297
[09/17 01:53:30 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.32	top5: 31.28	
[09/17 01:53:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/17 01:53:45 visual_prompt]: Epoch 19 / 100: avg data time: 2.94e-01, avg batch time: 0.7266, average train loss: 8.0277
[09/17 01:53:55 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1434, average loss: 7.5701
[09/17 01:53:55 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 28.00	
[09/17 01:54:19 visual_prompt]: 	Test 100/1152. loss: 6.888, 0.1950 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 01:54:38 visual_prompt]: 	Test 200/1152. loss: 8.747, 0.1950 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 01:54:58 visual_prompt]: 	Test 300/1152. loss: 7.655, 0.1833 s / batch. (data: 1.05e-04)max mem: 17.22454 GB 
[09/17 01:55:18 visual_prompt]: 	Test 400/1152. loss: 7.695, 0.2219 s / batch. (data: 3.85e-02)max mem: 17.22454 GB 
[09/17 01:55:37 visual_prompt]: 	Test 500/1152. loss: 8.011, 0.1989 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 01:55:57 visual_prompt]: 	Test 600/1152. loss: 7.918, 0.1899 s / batch. (data: 1.56e-04)max mem: 17.22454 GB 
[09/17 01:56:17 visual_prompt]: 	Test 700/1152. loss: 6.667, 0.1850 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 01:56:37 visual_prompt]: 	Test 800/1152. loss: 6.578, 0.1842 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 01:56:56 visual_prompt]: 	Test 900/1152. loss: 6.367, 0.1849 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 01:57:16 visual_prompt]: 	Test 1000/1152. loss: 8.132, 0.2013 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 01:57:36 visual_prompt]: 	Test 1100/1152. loss: 6.202, 0.1848 s / batch. (data: 1.09e-03)max mem: 17.22454 GB 
[09/17 01:57:52 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1959, average loss: 7.5023
[09/17 01:57:52 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.21	top5: 31.31	
[09/17 01:57:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/17 01:58:08 visual_prompt]: Epoch 20 / 100: avg data time: 3.09e-01, avg batch time: 0.7124, average train loss: 6.5406
[09/17 01:58:18 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1430, average loss: 5.6264
[09/17 01:58:18 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.00	top5: 30.00	
[09/17 01:58:42 visual_prompt]: 	Test 100/1152. loss: 6.156, 0.1838 s / batch. (data: 1.09e-04)max mem: 17.22454 GB 
[09/17 01:59:02 visual_prompt]: 	Test 200/1152. loss: 6.287, 0.1842 s / batch. (data: 1.80e-04)max mem: 17.22454 GB 
[09/17 01:59:21 visual_prompt]: 	Test 300/1152. loss: 6.302, 0.2121 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 01:59:41 visual_prompt]: 	Test 400/1152. loss: 5.178, 0.2035 s / batch. (data: 1.65e-04)max mem: 17.22454 GB 
[09/17 02:00:01 visual_prompt]: 	Test 500/1152. loss: 5.567, 0.1844 s / batch. (data: 1.63e-04)max mem: 17.22454 GB 
[09/17 02:00:21 visual_prompt]: 	Test 600/1152. loss: 5.460, 0.1956 s / batch. (data: 1.20e-02)max mem: 17.22454 GB 
[09/17 02:00:41 visual_prompt]: 	Test 700/1152. loss: 4.971, 0.1999 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 02:01:01 visual_prompt]: 	Test 800/1152. loss: 4.985, 0.1937 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 02:01:20 visual_prompt]: 	Test 900/1152. loss: 5.248, 0.1997 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 02:01:40 visual_prompt]: 	Test 1000/1152. loss: 6.194, 0.1850 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 02:02:00 visual_prompt]: 	Test 1100/1152. loss: 3.916, 0.2057 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 02:02:15 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1967, average loss: 5.5223
[09/17 02:02:16 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.33	top5: 31.32	
[09/17 02:02:16 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/17 02:02:31 visual_prompt]: Epoch 21 / 100: avg data time: 3.09e-01, avg batch time: 0.7128, average train loss: 5.2743
[09/17 02:02:40 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1430, average loss: 4.2549
[09/17 02:02:40 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 29.00	
[09/17 02:03:04 visual_prompt]: 	Test 100/1152. loss: 3.657, 0.2000 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 02:03:24 visual_prompt]: 	Test 200/1152. loss: 4.231, 0.2070 s / batch. (data: 2.39e-02)max mem: 17.22454 GB 
[09/17 02:03:44 visual_prompt]: 	Test 300/1152. loss: 4.235, 0.1923 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 02:04:04 visual_prompt]: 	Test 400/1152. loss: 4.301, 0.2052 s / batch. (data: 3.43e-05)max mem: 17.22454 GB 
[09/17 02:04:23 visual_prompt]: 	Test 500/1152. loss: 3.599, 0.2220 s / batch. (data: 1.03e-02)max mem: 17.22454 GB 
[09/17 02:04:44 visual_prompt]: 	Test 600/1152. loss: 4.155, 0.1843 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 02:05:03 visual_prompt]: 	Test 700/1152. loss: 3.889, 0.2164 s / batch. (data: 3.26e-02)max mem: 17.22454 GB 
[09/17 02:05:23 visual_prompt]: 	Test 800/1152. loss: 4.199, 0.1944 s / batch. (data: 1.06e-02)max mem: 17.22454 GB 
[09/17 02:05:43 visual_prompt]: 	Test 900/1152. loss: 3.843, 0.2042 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 02:06:03 visual_prompt]: 	Test 1000/1152. loss: 4.111, 0.1851 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 02:06:23 visual_prompt]: 	Test 1100/1152. loss: 4.273, 0.2004 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 02:06:39 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1971, average loss: 4.1400
[09/17 02:06:39 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.24	top5: 31.23	
[09/17 02:06:39 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/17 02:06:54 visual_prompt]: Epoch 22 / 100: avg data time: 3.11e-01, avg batch time: 0.7117, average train loss: 3.9361
[09/17 02:07:04 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1428, average loss: 4.4519
[09/17 02:07:04 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.00	top5: 34.50	
[09/17 02:07:28 visual_prompt]: 	Test 100/1152. loss: 4.250, 0.1835 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 02:07:48 visual_prompt]: 	Test 200/1152. loss: 4.378, 0.1981 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 02:08:07 visual_prompt]: 	Test 300/1152. loss: 4.650, 0.1948 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 02:08:27 visual_prompt]: 	Test 400/1152. loss: 5.269, 0.1840 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 02:08:47 visual_prompt]: 	Test 500/1152. loss: 5.389, 0.1960 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 02:09:07 visual_prompt]: 	Test 600/1152. loss: 4.903, 0.1943 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 02:09:26 visual_prompt]: 	Test 700/1152. loss: 5.237, 0.2051 s / batch. (data: 2.15e-02)max mem: 17.22454 GB 
[09/17 02:09:46 visual_prompt]: 	Test 800/1152. loss: 4.952, 0.2475 s / batch. (data: 1.59e-02)max mem: 17.22454 GB 
[09/17 02:10:06 visual_prompt]: 	Test 900/1152. loss: 5.796, 0.2073 s / batch. (data: 2.27e-02)max mem: 17.22454 GB 
[09/17 02:10:26 visual_prompt]: 	Test 1000/1152. loss: 5.491, 0.1850 s / batch. (data: 1.68e-04)max mem: 17.22454 GB 
[09/17 02:10:45 visual_prompt]: 	Test 1100/1152. loss: 5.381, 0.2108 s / batch. (data: 2.71e-02)max mem: 17.22454 GB 
[09/17 02:11:01 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1962, average loss: 4.9012
[09/17 02:11:01 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 31.11	
[09/17 02:11:01 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/17 02:11:17 visual_prompt]: Epoch 23 / 100: avg data time: 3.00e-01, avg batch time: 0.7099, average train loss: 4.0076
[09/17 02:11:26 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1429, average loss: 3.5835
[09/17 02:11:26 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 3.50	top5: 31.50	
[09/17 02:11:51 visual_prompt]: 	Test 100/1152. loss: 3.687, 0.2081 s / batch. (data: 2.20e-02)max mem: 17.22454 GB 
[09/17 02:12:10 visual_prompt]: 	Test 200/1152. loss: 3.809, 0.1955 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 02:12:30 visual_prompt]: 	Test 300/1152. loss: 3.437, 0.1970 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 02:12:50 visual_prompt]: 	Test 400/1152. loss: 3.830, 0.2149 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 02:13:10 visual_prompt]: 	Test 500/1152. loss: 3.863, 0.1848 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 02:13:30 visual_prompt]: 	Test 600/1152. loss: 3.569, 0.2106 s / batch. (data: 1.38e-02)max mem: 17.22454 GB 
[09/17 02:13:50 visual_prompt]: 	Test 700/1152. loss: 3.637, 0.2011 s / batch. (data: 1.77e-04)max mem: 17.22454 GB 
[09/17 02:14:10 visual_prompt]: 	Test 800/1152. loss: 3.545, 0.1852 s / batch. (data: 9.27e-05)max mem: 17.22454 GB 
[09/17 02:14:29 visual_prompt]: 	Test 900/1152. loss: 4.104, 0.1837 s / batch. (data: 3.24e-05)max mem: 17.22454 GB 
[09/17 02:14:49 visual_prompt]: 	Test 1000/1152. loss: 3.959, 0.2172 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 02:15:09 visual_prompt]: 	Test 1100/1152. loss: 3.729, 0.1953 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 02:15:25 visual_prompt]: Inference (test):avg data time: 8.31e-03, avg batch time: 0.1974, average loss: 3.6848
[09/17 02:15:26 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 31.00	
[09/17 02:15:26 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/17 02:15:41 visual_prompt]: Epoch 24 / 100: avg data time: 3.06e-01, avg batch time: 0.7090, average train loss: 3.1487
[09/17 02:15:50 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1429, average loss: 2.9591
[09/17 02:15:50 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 14.00	top5: 43.00	
[09/17 02:16:15 visual_prompt]: 	Test 100/1152. loss: 2.772, 0.1832 s / batch. (data: 1.16e-04)max mem: 17.22454 GB 
[09/17 02:16:34 visual_prompt]: 	Test 200/1152. loss: 2.939, 0.2214 s / batch. (data: 3.50e-02)max mem: 17.22454 GB 
[09/17 02:16:54 visual_prompt]: 	Test 300/1152. loss: 2.944, 0.1935 s / batch. (data: 1.65e-04)max mem: 17.22454 GB 
[09/17 02:17:14 visual_prompt]: 	Test 400/1152. loss: 3.005, 0.2313 s / batch. (data: 8.06e-05)max mem: 17.22454 GB 
[09/17 02:17:34 visual_prompt]: 	Test 500/1152. loss: 3.050, 0.1866 s / batch. (data: 4.89e-05)max mem: 17.22454 GB 
[09/17 02:17:53 visual_prompt]: 	Test 600/1152. loss: 3.020, 0.1859 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 02:18:13 visual_prompt]: 	Test 700/1152. loss: 2.853, 0.1849 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 02:18:33 visual_prompt]: 	Test 800/1152. loss: 3.027, 0.1997 s / batch. (data: 1.52e-04)max mem: 17.22454 GB 
[09/17 02:18:54 visual_prompt]: 	Test 900/1152. loss: 2.908, 0.1960 s / batch. (data: 3.05e-05)max mem: 17.22454 GB 
[09/17 02:19:15 visual_prompt]: 	Test 1000/1152. loss: 2.881, 0.1953 s / batch. (data: 1.19e-02)max mem: 17.22454 GB 
[09/17 02:19:35 visual_prompt]: 	Test 1100/1152. loss: 2.947, 0.2181 s / batch. (data: 1.39e-02)max mem: 17.22454 GB 
[09/17 02:19:51 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1991, average loss: 2.9585
[09/17 02:19:51 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 12.62	top5: 41.90	
[09/17 02:19:51 visual_prompt]: Best epoch 24: best metric: 0.140
[09/17 02:19:51 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/17 02:20:06 visual_prompt]: Epoch 25 / 100: avg data time: 3.10e-01, avg batch time: 0.7140, average train loss: 2.9178
[09/17 02:20:16 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1429, average loss: 4.1427
[09/17 02:20:16 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 35.00	
[09/17 02:20:41 visual_prompt]: 	Test 100/1152. loss: 4.367, 0.1884 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 02:21:00 visual_prompt]: 	Test 200/1152. loss: 4.382, 0.1836 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 02:21:20 visual_prompt]: 	Test 300/1152. loss: 4.148, 0.1989 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 02:21:39 visual_prompt]: 	Test 400/1152. loss: 4.531, 0.2091 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 02:21:59 visual_prompt]: 	Test 500/1152. loss: 4.846, 0.1883 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 02:22:19 visual_prompt]: 	Test 600/1152. loss: 4.269, 0.1849 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 02:22:39 visual_prompt]: 	Test 700/1152. loss: 4.633, 0.2256 s / batch. (data: 1.05e-02)max mem: 17.22454 GB 
[09/17 02:22:58 visual_prompt]: 	Test 800/1152. loss: 4.300, 0.1953 s / batch. (data: 1.18e-02)max mem: 17.22454 GB 
[09/17 02:23:18 visual_prompt]: 	Test 900/1152. loss: 4.622, 0.1840 s / batch. (data: 1.77e-04)max mem: 17.22454 GB 
[09/17 02:23:38 visual_prompt]: 	Test 1000/1152. loss: 4.378, 0.2170 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 02:23:58 visual_prompt]: 	Test 1100/1152. loss: 4.420, 0.1960 s / batch. (data: 9.15e-03)max mem: 17.22454 GB 
[09/17 02:24:13 visual_prompt]: Inference (test):avg data time: 7.71e-03, avg batch time: 0.1962, average loss: 4.3798
[09/17 02:24:14 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 31.09	
[09/17 02:24:14 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/17 02:24:29 visual_prompt]: Epoch 26 / 100: avg data time: 3.02e-01, avg batch time: 0.7064, average train loss: 3.4109
[09/17 02:24:39 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1430, average loss: 3.2629
[09/17 02:24:39 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.00	top5: 32.00	
[09/17 02:25:03 visual_prompt]: 	Test 100/1152. loss: 3.642, 0.1829 s / batch. (data: 4.94e-05)max mem: 17.22454 GB 
[09/17 02:25:23 visual_prompt]: 	Test 200/1152. loss: 3.308, 0.2027 s / batch. (data: 1.96e-02)max mem: 17.22454 GB 
[09/17 02:25:42 visual_prompt]: 	Test 300/1152. loss: 3.332, 0.1840 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 02:26:02 visual_prompt]: 	Test 400/1152. loss: 3.163, 0.1846 s / batch. (data: 1.71e-04)max mem: 17.22454 GB 
[09/17 02:26:22 visual_prompt]: 	Test 500/1152. loss: 3.410, 0.1841 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 02:26:42 visual_prompt]: 	Test 600/1152. loss: 3.097, 0.1838 s / batch. (data: 9.92e-05)max mem: 17.22454 GB 
[09/17 02:27:02 visual_prompt]: 	Test 700/1152. loss: 3.284, 0.1975 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 02:27:21 visual_prompt]: 	Test 800/1152. loss: 3.268, 0.1843 s / batch. (data: 1.70e-04)max mem: 17.22454 GB 
[09/17 02:27:42 visual_prompt]: 	Test 900/1152. loss: 3.117, 0.1843 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 02:28:02 visual_prompt]: 	Test 1000/1152. loss: 3.196, 0.1998 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 02:28:21 visual_prompt]: 	Test 1100/1152. loss: 3.140, 0.1844 s / batch. (data: 3.19e-05)max mem: 17.22454 GB 
[09/17 02:28:37 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1972, average loss: 3.2529
[09/17 02:28:37 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.32	top5: 31.35	
[09/17 02:28:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/17 02:28:52 visual_prompt]: Epoch 27 / 100: avg data time: 3.03e-01, avg batch time: 0.7082, average train loss: 3.2608
[09/17 02:29:02 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1429, average loss: 3.2614
[09/17 02:29:02 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 30.00	
[09/17 02:29:27 visual_prompt]: 	Test 100/1152. loss: 3.666, 0.1836 s / batch. (data: 1.12e-04)max mem: 17.22454 GB 
[09/17 02:29:46 visual_prompt]: 	Test 200/1152. loss: 3.226, 0.1834 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 02:30:06 visual_prompt]: 	Test 300/1152. loss: 3.236, 0.1831 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 02:30:25 visual_prompt]: 	Test 400/1152. loss: 3.214, 0.1834 s / batch. (data: 3.29e-05)max mem: 17.22454 GB 
[09/17 02:30:45 visual_prompt]: 	Test 500/1152. loss: 3.191, 0.2477 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 02:31:05 visual_prompt]: 	Test 600/1152. loss: 3.016, 0.1963 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 02:31:25 visual_prompt]: 	Test 700/1152. loss: 3.237, 0.2080 s / batch. (data: 2.42e-02)max mem: 17.22454 GB 
[09/17 02:31:45 visual_prompt]: 	Test 800/1152. loss: 3.255, 0.2039 s / batch. (data: 2.03e-02)max mem: 17.22454 GB 
[09/17 02:32:05 visual_prompt]: 	Test 900/1152. loss: 3.296, 0.2091 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 02:32:24 visual_prompt]: 	Test 1000/1152. loss: 3.052, 0.1927 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 02:32:44 visual_prompt]: 	Test 1100/1152. loss: 3.163, 0.2037 s / batch. (data: 1.94e-02)max mem: 17.22454 GB 
[09/17 02:33:00 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1965, average loss: 3.2216
[09/17 02:33:00 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.17	top5: 31.06	
[09/17 02:33:00 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/17 02:33:15 visual_prompt]: Epoch 28 / 100: avg data time: 3.04e-01, avg batch time: 0.7089, average train loss: 3.2686
[09/17 02:33:24 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1430, average loss: 3.0726
[09/17 02:33:24 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 31.00	
[09/17 02:33:49 visual_prompt]: 	Test 100/1152. loss: 3.124, 0.1829 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 02:34:09 visual_prompt]: 	Test 200/1152. loss: 3.280, 0.1830 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 02:34:28 visual_prompt]: 	Test 300/1152. loss: 3.189, 0.1839 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 02:34:48 visual_prompt]: 	Test 400/1152. loss: 3.077, 0.1908 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 02:35:08 visual_prompt]: 	Test 500/1152. loss: 3.025, 0.1841 s / batch. (data: 1.79e-04)max mem: 17.22454 GB 
[09/17 02:35:28 visual_prompt]: 	Test 600/1152. loss: 3.116, 0.1982 s / batch. (data: 1.43e-02)max mem: 17.22454 GB 
[09/17 02:35:48 visual_prompt]: 	Test 700/1152. loss: 2.989, 0.1983 s / batch. (data: 1.46e-02)max mem: 17.22454 GB 
[09/17 02:36:08 visual_prompt]: 	Test 800/1152. loss: 3.043, 0.2038 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 02:36:28 visual_prompt]: 	Test 900/1152. loss: 3.139, 0.1916 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 02:36:47 visual_prompt]: 	Test 1000/1152. loss: 3.126, 0.1959 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 02:37:07 visual_prompt]: 	Test 1100/1152. loss: 2.940, 0.1923 s / batch. (data: 4.67e-05)max mem: 17.22454 GB 
[09/17 02:37:23 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1972, average loss: 3.1145
[09/17 02:37:23 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.24	top5: 31.18	
[09/17 02:37:23 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/17 02:37:39 visual_prompt]: Epoch 29 / 100: avg data time: 2.89e-01, avg batch time: 0.6972, average train loss: 3.1778
[09/17 02:37:48 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1429, average loss: 3.0846
[09/17 02:37:48 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 30.50	
[09/17 02:38:13 visual_prompt]: 	Test 100/1152. loss: 3.280, 0.1842 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 02:38:32 visual_prompt]: 	Test 200/1152. loss: 3.249, 0.1958 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 02:38:52 visual_prompt]: 	Test 300/1152. loss: 3.132, 0.1840 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 02:39:12 visual_prompt]: 	Test 400/1152. loss: 3.175, 0.1839 s / batch. (data: 9.85e-05)max mem: 17.22454 GB 
[09/17 02:39:32 visual_prompt]: 	Test 500/1152. loss: 3.083, 0.1977 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 02:39:51 visual_prompt]: 	Test 600/1152. loss: 2.854, 0.1983 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 02:40:11 visual_prompt]: 	Test 700/1152. loss: 3.118, 0.2014 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 02:40:31 visual_prompt]: 	Test 800/1152. loss: 3.196, 0.2007 s / batch. (data: 1.38e-02)max mem: 17.22454 GB 
[09/17 02:40:51 visual_prompt]: 	Test 900/1152. loss: 3.009, 0.1920 s / batch. (data: 2.96e-05)max mem: 17.22454 GB 
[09/17 02:41:11 visual_prompt]: 	Test 1000/1152. loss: 3.023, 0.2215 s / batch. (data: 3.78e-02)max mem: 17.22454 GB 
[09/17 02:41:31 visual_prompt]: 	Test 1100/1152. loss: 3.013, 0.2071 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 02:41:47 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1975, average loss: 3.1062
[09/17 02:41:47 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.24	top5: 31.49	
[09/17 02:41:47 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/17 02:42:02 visual_prompt]: Epoch 30 / 100: avg data time: 2.90e-01, avg batch time: 0.6981, average train loss: 3.0506
[09/17 02:42:12 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1429, average loss: 3.1613
[09/17 02:42:12 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 26.50	
[09/17 02:42:36 visual_prompt]: 	Test 100/1152. loss: 2.896, 0.1961 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 02:42:56 visual_prompt]: 	Test 200/1152. loss: 3.399, 0.1851 s / batch. (data: 1.78e-04)max mem: 17.22454 GB 
[09/17 02:43:15 visual_prompt]: 	Test 300/1152. loss: 3.170, 0.2088 s / batch. (data: 2.41e-02)max mem: 17.22454 GB 
[09/17 02:43:35 visual_prompt]: 	Test 400/1152. loss: 3.224, 0.1841 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 02:43:55 visual_prompt]: 	Test 500/1152. loss: 3.173, 0.1841 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 02:44:15 visual_prompt]: 	Test 600/1152. loss: 3.354, 0.1948 s / batch. (data: 1.08e-02)max mem: 17.22454 GB 
[09/17 02:44:35 visual_prompt]: 	Test 700/1152. loss: 2.998, 0.1836 s / batch. (data: 2.07e-04)max mem: 17.22454 GB 
[09/17 02:44:54 visual_prompt]: 	Test 800/1152. loss: 3.056, 0.2103 s / batch. (data: 2.34e-02)max mem: 17.22454 GB 
[09/17 02:45:14 visual_prompt]: 	Test 900/1152. loss: 3.531, 0.1995 s / batch. (data: 1.43e-02)max mem: 17.22454 GB 
[09/17 02:45:35 visual_prompt]: 	Test 1000/1152. loss: 3.430, 0.1962 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 02:45:54 visual_prompt]: 	Test 1100/1152. loss: 3.035, 0.1967 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 02:46:10 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1972, average loss: 3.1943
[09/17 02:46:10 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 31.32	
[09/17 02:46:10 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/17 02:46:25 visual_prompt]: Epoch 31 / 100: avg data time: 2.98e-01, avg batch time: 0.7017, average train loss: 2.9860
[09/17 02:46:35 visual_prompt]: Inference (val):avg data time: 1.15e-04, avg batch time: 0.2180, average loss: 2.9767
[09/17 02:46:35 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 10.50	top5: 43.00	
[09/17 02:46:59 visual_prompt]: 	Test 100/1152. loss: 2.955, 0.1843 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 02:47:19 visual_prompt]: 	Test 200/1152. loss: 2.853, 0.1966 s / batch. (data: 1.35e-02)max mem: 17.22454 GB 
[09/17 02:47:39 visual_prompt]: 	Test 300/1152. loss: 3.066, 0.2120 s / batch. (data: 2.32e-02)max mem: 17.22454 GB 
[09/17 02:47:58 visual_prompt]: 	Test 400/1152. loss: 2.925, 0.1920 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 02:48:18 visual_prompt]: 	Test 500/1152. loss: 2.785, 0.1853 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 02:48:38 visual_prompt]: 	Test 600/1152. loss: 2.990, 0.1976 s / batch. (data: 1.36e-02)max mem: 17.22454 GB 
[09/17 02:48:57 visual_prompt]: 	Test 700/1152. loss: 3.137, 0.1988 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 02:49:17 visual_prompt]: 	Test 800/1152. loss: 3.030, 0.1992 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 02:49:37 visual_prompt]: 	Test 900/1152. loss: 3.003, 0.1922 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 02:49:58 visual_prompt]: 	Test 1000/1152. loss: 3.173, 0.1969 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 02:50:17 visual_prompt]: 	Test 1100/1152. loss: 2.897, 0.1985 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 02:50:33 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1968, average loss: 2.9457
[09/17 02:50:34 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 11.71	top5: 43.66	
[09/17 02:50:34 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/17 02:50:49 visual_prompt]: Epoch 32 / 100: avg data time: 3.16e-01, avg batch time: 0.7187, average train loss: 2.8503
[09/17 02:50:59 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1428, average loss: 2.6501
[09/17 02:50:59 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 13.00	top5: 57.50	
[09/17 02:51:23 visual_prompt]: 	Test 100/1152. loss: 2.730, 0.2057 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 02:51:42 visual_prompt]: 	Test 200/1152. loss: 2.560, 0.2042 s / batch. (data: 2.14e-02)max mem: 17.22454 GB 
[09/17 02:52:02 visual_prompt]: 	Test 300/1152. loss: 2.782, 0.1837 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 02:52:22 visual_prompt]: 	Test 400/1152. loss: 2.619, 0.1905 s / batch. (data: 1.67e-04)max mem: 17.22454 GB 
[09/17 02:52:42 visual_prompt]: 	Test 500/1152. loss: 2.546, 0.1959 s / batch. (data: 1.04e-04)max mem: 17.22454 GB 
[09/17 02:53:02 visual_prompt]: 	Test 600/1152. loss: 2.521, 0.1840 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 02:53:22 visual_prompt]: 	Test 700/1152. loss: 2.571, 0.2247 s / batch. (data: 7.94e-03)max mem: 17.22454 GB 
[09/17 02:53:41 visual_prompt]: 	Test 800/1152. loss: 2.506, 0.1961 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 02:54:01 visual_prompt]: 	Test 900/1152. loss: 2.504, 0.1845 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 02:54:21 visual_prompt]: 	Test 1000/1152. loss: 2.544, 0.1976 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 02:54:41 visual_prompt]: 	Test 1100/1152. loss: 2.475, 0.2045 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 02:54:56 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1966, average loss: 2.5961
[09/17 02:54:56 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 13.25	top5: 60.36	
[09/17 02:54:56 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/17 02:55:12 visual_prompt]: Epoch 33 / 100: avg data time: 3.04e-01, avg batch time: 0.7048, average train loss: 2.7286
[09/17 02:55:21 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1476, average loss: 2.5094
[09/17 02:55:21 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 13.00	top5: 57.50	
[09/17 02:55:45 visual_prompt]: 	Test 100/1152. loss: 2.524, 0.1978 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 02:56:04 visual_prompt]: 	Test 200/1152. loss: 2.450, 0.2159 s / batch. (data: 2.48e-02)max mem: 17.22454 GB 
[09/17 02:56:24 visual_prompt]: 	Test 300/1152. loss: 2.586, 0.1830 s / batch. (data: 3.29e-05)max mem: 17.22454 GB 
[09/17 02:56:44 visual_prompt]: 	Test 400/1152. loss: 2.567, 0.1984 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 02:57:04 visual_prompt]: 	Test 500/1152. loss: 2.687, 0.2069 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 02:57:25 visual_prompt]: 	Test 600/1152. loss: 2.723, 0.2285 s / batch. (data: 1.44e-02)max mem: 17.22454 GB 
[09/17 02:57:45 visual_prompt]: 	Test 700/1152. loss: 2.625, 0.1968 s / batch. (data: 1.34e-02)max mem: 17.22454 GB 
[09/17 02:58:04 visual_prompt]: 	Test 800/1152. loss: 2.470, 0.1848 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 02:58:24 visual_prompt]: 	Test 900/1152. loss: 2.504, 0.1962 s / batch. (data: 1.27e-02)max mem: 17.22454 GB 
[09/17 02:58:44 visual_prompt]: 	Test 1000/1152. loss: 2.400, 0.1843 s / batch. (data: 1.79e-04)max mem: 17.22454 GB 
[09/17 02:59:05 visual_prompt]: 	Test 1100/1152. loss: 2.461, 0.2146 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 02:59:20 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1983, average loss: 2.5121
[09/17 02:59:20 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 14.15	top5: 57.98	
[09/17 02:59:20 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/17 02:59:36 visual_prompt]: Epoch 34 / 100: avg data time: 3.13e-01, avg batch time: 0.7152, average train loss: 2.3637
[09/17 02:59:45 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1447, average loss: 2.2556
[09/17 02:59:45 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 19.00	top5: 79.00	
[09/17 03:00:09 visual_prompt]: 	Test 100/1152. loss: 2.259, 0.1915 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 03:00:29 visual_prompt]: 	Test 200/1152. loss: 2.395, 0.1832 s / batch. (data: 1.15e-04)max mem: 17.22454 GB 
[09/17 03:00:48 visual_prompt]: 	Test 300/1152. loss: 2.300, 0.1887 s / batch. (data: 1.71e-04)max mem: 17.22454 GB 
[09/17 03:01:08 visual_prompt]: 	Test 400/1152. loss: 2.098, 0.2050 s / batch. (data: 2.16e-02)max mem: 17.22454 GB 
[09/17 03:01:28 visual_prompt]: 	Test 500/1152. loss: 2.132, 0.1969 s / batch. (data: 3.39e-03)max mem: 17.22454 GB 
[09/17 03:01:47 visual_prompt]: 	Test 600/1152. loss: 2.244, 0.1850 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 03:02:07 visual_prompt]: 	Test 700/1152. loss: 2.112, 0.1840 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 03:02:27 visual_prompt]: 	Test 800/1152. loss: 2.340, 0.1837 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 03:02:47 visual_prompt]: 	Test 900/1152. loss: 2.402, 0.1977 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 03:03:07 visual_prompt]: 	Test 1000/1152. loss: 2.290, 0.2083 s / batch. (data: 2.47e-02)max mem: 17.22454 GB 
[09/17 03:03:27 visual_prompt]: 	Test 1100/1152. loss: 2.399, 0.1841 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 03:03:42 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1966, average loss: 2.2922
[09/17 03:03:43 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 17.63	top5: 78.22	
[09/17 03:03:43 visual_prompt]: Best epoch 34: best metric: 0.190
[09/17 03:03:43 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/17 03:03:58 visual_prompt]: Epoch 35 / 100: avg data time: 3.16e-01, avg batch time: 0.7181, average train loss: 2.3200
[09/17 03:04:07 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1427, average loss: 2.2953
[09/17 03:04:07 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 19.50	top5: 71.00	
[09/17 03:04:32 visual_prompt]: 	Test 100/1152. loss: 2.053, 0.1922 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 03:04:51 visual_prompt]: 	Test 200/1152. loss: 2.395, 0.1978 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 03:05:11 visual_prompt]: 	Test 300/1152. loss: 2.214, 0.1990 s / batch. (data: 1.56e-02)max mem: 17.22454 GB 
[09/17 03:05:31 visual_prompt]: 	Test 400/1152. loss: 2.470, 0.2109 s / batch. (data: 2.80e-02)max mem: 17.22454 GB 
[09/17 03:05:51 visual_prompt]: 	Test 500/1152. loss: 2.253, 0.1841 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 03:06:11 visual_prompt]: 	Test 600/1152. loss: 2.221, 0.1842 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 03:06:31 visual_prompt]: 	Test 700/1152. loss: 2.196, 0.2051 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 03:06:51 visual_prompt]: 	Test 800/1152. loss: 2.247, 0.2024 s / batch. (data: 9.43e-03)max mem: 17.22454 GB 
[09/17 03:07:11 visual_prompt]: 	Test 900/1152. loss: 2.219, 0.2224 s / batch. (data: 1.41e-02)max mem: 17.22454 GB 
[09/17 03:07:30 visual_prompt]: 	Test 1000/1152. loss: 2.484, 0.1840 s / batch. (data: 5.32e-05)max mem: 17.22454 GB 
[09/17 03:07:50 visual_prompt]: 	Test 1100/1152. loss: 2.110, 0.1944 s / batch. (data: 1.06e-02)max mem: 17.22454 GB 
[09/17 03:08:06 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1972, average loss: 2.2631
[09/17 03:08:06 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 21.02	top5: 73.04	
[09/17 03:08:06 visual_prompt]: Best epoch 35: best metric: 0.195
[09/17 03:08:06 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/17 03:08:21 visual_prompt]: Epoch 36 / 100: avg data time: 3.07e-01, avg batch time: 0.7134, average train loss: 2.1310
[09/17 03:08:30 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1466, average loss: 2.6229
[09/17 03:08:30 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 17.50	top5: 73.50	
[09/17 03:08:55 visual_prompt]: 	Test 100/1152. loss: 2.836, 0.1827 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 03:09:15 visual_prompt]: 	Test 200/1152. loss: 2.536, 0.1844 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 03:09:35 visual_prompt]: 	Test 300/1152. loss: 2.767, 0.2081 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 03:09:55 visual_prompt]: 	Test 400/1152. loss: 2.400, 0.1872 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 03:10:14 visual_prompt]: 	Test 500/1152. loss: 2.415, 0.1845 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 03:10:34 visual_prompt]: 	Test 600/1152. loss: 2.510, 0.1852 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 03:10:54 visual_prompt]: 	Test 700/1152. loss: 2.601, 0.1985 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 03:11:14 visual_prompt]: 	Test 800/1152. loss: 2.485, 0.1847 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 03:11:34 visual_prompt]: 	Test 900/1152. loss: 2.677, 0.1987 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 03:11:54 visual_prompt]: 	Test 1000/1152. loss: 2.548, 0.2037 s / batch. (data: 2.03e-02)max mem: 17.22454 GB 
[09/17 03:12:14 visual_prompt]: 	Test 1100/1152. loss: 2.365, 0.1912 s / batch. (data: 1.95e-04)max mem: 17.22454 GB 
[09/17 03:12:30 visual_prompt]: Inference (test):avg data time: 8.05e-03, avg batch time: 0.1980, average loss: 2.5773
[09/17 03:12:30 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 15.26	top5: 76.94	
[09/17 03:12:30 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/17 03:12:46 visual_prompt]: Epoch 37 / 100: avg data time: 3.06e-01, avg batch time: 0.7111, average train loss: 2.3685
[09/17 03:12:55 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1428, average loss: 2.5483
[09/17 03:12:55 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 17.00	top5: 67.50	
[09/17 03:13:19 visual_prompt]: 	Test 100/1152. loss: 2.709, 0.1834 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 03:13:39 visual_prompt]: 	Test 200/1152. loss: 2.702, 0.1842 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 03:13:59 visual_prompt]: 	Test 300/1152. loss: 2.808, 0.2173 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 03:14:18 visual_prompt]: 	Test 400/1152. loss: 2.716, 0.1847 s / batch. (data: 5.46e-05)max mem: 17.22454 GB 
[09/17 03:14:38 visual_prompt]: 	Test 500/1152. loss: 2.749, 0.2078 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 03:14:58 visual_prompt]: 	Test 600/1152. loss: 2.709, 0.1962 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 03:15:18 visual_prompt]: 	Test 700/1152. loss: 2.783, 0.1879 s / batch. (data: 3.55e-05)max mem: 17.22454 GB 
[09/17 03:15:38 visual_prompt]: 	Test 800/1152. loss: 2.581, 0.1867 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 03:15:57 visual_prompt]: 	Test 900/1152. loss: 2.736, 0.2068 s / batch. (data: 1.56e-02)max mem: 17.22454 GB 
[09/17 03:16:17 visual_prompt]: 	Test 1000/1152. loss: 2.520, 0.1987 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 03:16:37 visual_prompt]: 	Test 1100/1152. loss: 2.699, 0.1838 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 03:16:52 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1964, average loss: 2.6790
[09/17 03:16:53 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 12.84	top5: 61.60	
[09/17 03:16:53 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/17 03:17:08 visual_prompt]: Epoch 38 / 100: avg data time: 3.18e-01, avg batch time: 0.7208, average train loss: 2.0297
[09/17 03:17:17 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1452, average loss: 2.1435
[09/17 03:17:17 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 12.00	top5: 91.00	
[09/17 03:17:42 visual_prompt]: 	Test 100/1152. loss: 1.963, 0.1945 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 03:18:02 visual_prompt]: 	Test 200/1152. loss: 2.052, 0.1959 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 03:18:21 visual_prompt]: 	Test 300/1152. loss: 2.091, 0.2235 s / batch. (data: 3.66e-02)max mem: 17.22454 GB 
[09/17 03:18:41 visual_prompt]: 	Test 400/1152. loss: 2.059, 0.1839 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 03:19:01 visual_prompt]: 	Test 500/1152. loss: 1.864, 0.1841 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 03:19:21 visual_prompt]: 	Test 600/1152. loss: 2.056, 0.1970 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 03:19:41 visual_prompt]: 	Test 700/1152. loss: 2.128, 0.2195 s / batch. (data: 3.58e-02)max mem: 17.22454 GB 
[09/17 03:20:00 visual_prompt]: 	Test 800/1152. loss: 2.229, 0.1840 s / batch. (data: 1.65e-04)max mem: 17.22454 GB 
[09/17 03:20:20 visual_prompt]: 	Test 900/1152. loss: 2.199, 0.1844 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 03:20:40 visual_prompt]: 	Test 1000/1152. loss: 2.184, 0.1991 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 03:21:00 visual_prompt]: 	Test 1100/1152. loss: 2.016, 0.2087 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 03:21:16 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1972, average loss: 2.0735
[09/17 03:21:16 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 17.01	top5: 92.78	
[09/17 03:21:16 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/17 03:21:31 visual_prompt]: Epoch 39 / 100: avg data time: 3.14e-01, avg batch time: 0.7158, average train loss: 1.9719
[09/17 03:21:41 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1429, average loss: 1.6508
[09/17 03:21:41 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 31.50	top5: 97.50	
[09/17 03:22:05 visual_prompt]: 	Test 100/1152. loss: 1.591, 0.2318 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 03:22:25 visual_prompt]: 	Test 200/1152. loss: 1.653, 0.1965 s / batch. (data: 1.37e-02)max mem: 17.22454 GB 
[09/17 03:22:45 visual_prompt]: 	Test 300/1152. loss: 1.668, 0.1845 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 03:23:05 visual_prompt]: 	Test 400/1152. loss: 1.521, 0.2084 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 03:23:24 visual_prompt]: 	Test 500/1152. loss: 1.639, 0.2143 s / batch. (data: 3.05e-02)max mem: 17.22454 GB 
[09/17 03:23:44 visual_prompt]: 	Test 600/1152. loss: 1.725, 0.2068 s / batch. (data: 2.34e-02)max mem: 17.22454 GB 
[09/17 03:24:04 visual_prompt]: 	Test 700/1152. loss: 1.715, 0.2073 s / batch. (data: 1.46e-02)max mem: 17.22454 GB 
[09/17 03:24:24 visual_prompt]: 	Test 800/1152. loss: 1.616, 0.2278 s / batch. (data: 3.84e-02)max mem: 17.22454 GB 
[09/17 03:24:44 visual_prompt]: 	Test 900/1152. loss: 1.670, 0.1999 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 03:25:04 visual_prompt]: 	Test 1000/1152. loss: 1.737, 0.2096 s / batch. (data: 2.63e-02)max mem: 17.22454 GB 
[09/17 03:25:24 visual_prompt]: 	Test 1100/1152. loss: 1.589, 0.1842 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 03:25:39 visual_prompt]: Inference (test):avg data time: 8.05e-03, avg batch time: 0.1969, average loss: 1.6451
[09/17 03:25:39 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 30.90	top5: 98.15	
[09/17 03:25:39 visual_prompt]: Best epoch 39: best metric: 0.315
[09/17 03:25:39 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/17 03:25:55 visual_prompt]: Epoch 40 / 100: avg data time: 3.09e-01, avg batch time: 0.7135, average train loss: 1.7501
[09/17 03:26:04 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1432, average loss: 1.5577
[09/17 03:26:04 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 28.50	top5: 98.50	
[09/17 03:26:29 visual_prompt]: 	Test 100/1152. loss: 1.465, 0.2023 s / batch. (data: 1.85e-04)max mem: 17.22454 GB 
[09/17 03:26:48 visual_prompt]: 	Test 200/1152. loss: 1.549, 0.2185 s / batch. (data: 1.57e-02)max mem: 17.22454 GB 
[09/17 03:27:08 visual_prompt]: 	Test 300/1152. loss: 1.677, 0.1960 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 03:27:28 visual_prompt]: 	Test 400/1152. loss: 1.493, 0.1850 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 03:27:47 visual_prompt]: 	Test 500/1152. loss: 1.449, 0.1975 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 03:28:07 visual_prompt]: 	Test 600/1152. loss: 1.621, 0.1941 s / batch. (data: 1.83e-04)max mem: 17.22454 GB 
[09/17 03:28:27 visual_prompt]: 	Test 700/1152. loss: 1.620, 0.1921 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 03:28:46 visual_prompt]: 	Test 800/1152. loss: 1.638, 0.1942 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 03:29:06 visual_prompt]: 	Test 900/1152. loss: 1.578, 0.1848 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 03:29:26 visual_prompt]: 	Test 1000/1152. loss: 1.529, 0.2069 s / batch. (data: 2.10e-02)max mem: 17.22454 GB 
[09/17 03:29:46 visual_prompt]: 	Test 1100/1152. loss: 1.553, 0.1844 s / batch. (data: 1.52e-04)max mem: 17.22454 GB 
[09/17 03:30:01 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1960, average loss: 1.5726
[09/17 03:30:02 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 31.10	top5: 98.58	
[09/17 03:30:02 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/17 03:30:17 visual_prompt]: Epoch 41 / 100: avg data time: 3.08e-01, avg batch time: 0.7116, average train loss: 1.6881
[09/17 03:30:26 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1430, average loss: 1.7671
[09/17 03:30:26 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 28.50	top5: 89.50	
[09/17 03:30:51 visual_prompt]: 	Test 100/1152. loss: 1.537, 0.2073 s / batch. (data: 2.48e-02)max mem: 17.22454 GB 
[09/17 03:31:10 visual_prompt]: 	Test 200/1152. loss: 1.791, 0.1838 s / batch. (data: 8.99e-05)max mem: 17.22454 GB 
[09/17 03:31:30 visual_prompt]: 	Test 300/1152. loss: 1.798, 0.1996 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 03:31:49 visual_prompt]: 	Test 400/1152. loss: 1.838, 0.1847 s / batch. (data: 9.66e-05)max mem: 17.22454 GB 
[09/17 03:32:09 visual_prompt]: 	Test 500/1152. loss: 1.807, 0.1978 s / batch. (data: 1.43e-02)max mem: 17.22454 GB 
[09/17 03:32:29 visual_prompt]: 	Test 600/1152. loss: 1.974, 0.2101 s / batch. (data: 1.87e-02)max mem: 17.22454 GB 
[09/17 03:32:49 visual_prompt]: 	Test 700/1152. loss: 1.848, 0.1844 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 03:33:09 visual_prompt]: 	Test 800/1152. loss: 1.838, 0.1966 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 03:33:29 visual_prompt]: 	Test 900/1152. loss: 2.052, 0.1974 s / batch. (data: 1.94e-04)max mem: 17.22454 GB 
[09/17 03:33:49 visual_prompt]: 	Test 1000/1152. loss: 2.072, 0.1845 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 03:34:09 visual_prompt]: 	Test 1100/1152. loss: 1.931, 0.1994 s / batch. (data: 4.01e-05)max mem: 17.22454 GB 
[09/17 03:34:24 visual_prompt]: Inference (test):avg data time: 8.37e-03, avg batch time: 0.1966, average loss: 1.8414
[09/17 03:34:24 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 27.62	top5: 87.23	
[09/17 03:34:24 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/17 03:34:40 visual_prompt]: Epoch 42 / 100: avg data time: 3.15e-01, avg batch time: 0.7164, average train loss: 1.7210
[09/17 03:34:49 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1432, average loss: 1.5470
[09/17 03:34:49 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 27.50	top5: 93.00	
[09/17 03:35:13 visual_prompt]: 	Test 100/1152. loss: 1.615, 0.1856 s / batch. (data: 9.39e-05)max mem: 17.22454 GB 
[09/17 03:35:33 visual_prompt]: 	Test 200/1152. loss: 1.514, 0.1844 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 03:35:53 visual_prompt]: 	Test 300/1152. loss: 1.650, 0.1836 s / batch. (data: 1.01e-04)max mem: 17.22454 GB 
[09/17 03:36:13 visual_prompt]: 	Test 400/1152. loss: 1.592, 0.1985 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 03:36:33 visual_prompt]: 	Test 500/1152. loss: 1.536, 0.1986 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 03:36:52 visual_prompt]: 	Test 600/1152. loss: 1.470, 0.1840 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 03:37:12 visual_prompt]: 	Test 700/1152. loss: 1.497, 0.1846 s / batch. (data: 1.01e-04)max mem: 17.22454 GB 
[09/17 03:37:32 visual_prompt]: 	Test 800/1152. loss: 1.443, 0.1978 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 03:37:52 visual_prompt]: 	Test 900/1152. loss: 1.662, 0.2125 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 03:38:12 visual_prompt]: 	Test 1000/1152. loss: 1.487, 0.2004 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 03:38:32 visual_prompt]: 	Test 1100/1152. loss: 1.561, 0.2120 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 03:38:48 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1971, average loss: 1.6096
[09/17 03:38:48 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 32.19	top5: 93.06	
[09/17 03:38:48 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/17 03:39:03 visual_prompt]: Epoch 43 / 100: avg data time: 2.99e-01, avg batch time: 0.7071, average train loss: 1.7203
[09/17 03:39:13 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1433, average loss: 1.6037
[09/17 03:39:13 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 33.00	top5: 98.50	
[09/17 03:39:37 visual_prompt]: 	Test 100/1152. loss: 1.759, 0.1848 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 03:39:57 visual_prompt]: 	Test 200/1152. loss: 1.695, 0.1839 s / batch. (data: 1.09e-04)max mem: 17.22454 GB 
[09/17 03:40:17 visual_prompt]: 	Test 300/1152. loss: 1.781, 0.2079 s / batch. (data: 2.47e-02)max mem: 17.22454 GB 
[09/17 03:40:37 visual_prompt]: 	Test 400/1152. loss: 1.632, 0.2049 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 03:40:57 visual_prompt]: 	Test 500/1152. loss: 1.705, 0.1996 s / batch. (data: 1.56e-02)max mem: 17.22454 GB 
[09/17 03:41:17 visual_prompt]: 	Test 600/1152. loss: 1.750, 0.1913 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 03:41:36 visual_prompt]: 	Test 700/1152. loss: 1.574, 0.1846 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 03:41:56 visual_prompt]: 	Test 800/1152. loss: 1.655, 0.2028 s / batch. (data: 1.90e-02)max mem: 17.22454 GB 
[09/17 03:42:16 visual_prompt]: 	Test 900/1152. loss: 1.686, 0.1849 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 03:42:35 visual_prompt]: 	Test 1000/1152. loss: 1.542, 0.1847 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 03:42:55 visual_prompt]: 	Test 1100/1152. loss: 1.605, 0.1928 s / batch. (data: 9.04e-05)max mem: 17.22454 GB 
[09/17 03:43:11 visual_prompt]: Inference (test):avg data time: 7.71e-03, avg batch time: 0.1968, average loss: 1.6593
[09/17 03:43:11 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 32.44	top5: 96.87	
[09/17 03:43:11 visual_prompt]: Best epoch 43: best metric: 0.330
[09/17 03:43:11 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/17 03:43:26 visual_prompt]: Epoch 44 / 100: avg data time: 3.12e-01, avg batch time: 0.7147, average train loss: 1.8222
[09/17 03:43:36 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.1429, average loss: 1.5968
[09/17 03:43:36 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 35.50	top5: 96.00	
[09/17 03:44:00 visual_prompt]: 	Test 100/1152. loss: 1.758, 0.1842 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 03:44:20 visual_prompt]: 	Test 200/1152. loss: 1.438, 0.1985 s / batch. (data: 1.56e-02)max mem: 17.22454 GB 
[09/17 03:44:39 visual_prompt]: 	Test 300/1152. loss: 1.901, 0.1841 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 03:44:59 visual_prompt]: 	Test 400/1152. loss: 1.532, 0.1958 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 03:45:19 visual_prompt]: 	Test 500/1152. loss: 1.584, 0.2012 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 03:45:39 visual_prompt]: 	Test 600/1152. loss: 1.624, 0.1840 s / batch. (data: 1.85e-04)max mem: 17.22454 GB 
[09/17 03:45:59 visual_prompt]: 	Test 700/1152. loss: 1.660, 0.1848 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 03:46:18 visual_prompt]: 	Test 800/1152. loss: 1.583, 0.2092 s / batch. (data: 1.06e-02)max mem: 17.22454 GB 
[09/17 03:46:38 visual_prompt]: 	Test 900/1152. loss: 1.732, 0.2042 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 03:46:58 visual_prompt]: 	Test 1000/1152. loss: 1.691, 0.1912 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 03:47:18 visual_prompt]: 	Test 1100/1152. loss: 1.513, 0.2164 s / batch. (data: 1.57e-02)max mem: 17.22454 GB 
[09/17 03:47:34 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1969, average loss: 1.6697
[09/17 03:47:34 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 34.09	top5: 93.80	
[09/17 03:47:34 visual_prompt]: Best epoch 44: best metric: 0.355
[09/17 03:47:34 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/17 03:47:49 visual_prompt]: Epoch 45 / 100: avg data time: 3.16e-01, avg batch time: 0.7178, average train loss: 1.5004
[09/17 03:47:59 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1430, average loss: 1.6813
[09/17 03:47:59 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 32.00	top5: 99.50	
[09/17 03:48:24 visual_prompt]: 	Test 100/1152. loss: 1.775, 0.1997 s / batch. (data: 1.67e-02)max mem: 17.22454 GB 
[09/17 03:48:43 visual_prompt]: 	Test 200/1152. loss: 1.768, 0.1986 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 03:49:03 visual_prompt]: 	Test 300/1152. loss: 1.813, 0.2201 s / batch. (data: 1.44e-02)max mem: 17.22454 GB 
[09/17 03:49:23 visual_prompt]: 	Test 400/1152. loss: 1.491, 0.1842 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 03:49:42 visual_prompt]: 	Test 500/1152. loss: 1.440, 0.1979 s / batch. (data: 1.33e-02)max mem: 17.22454 GB 
[09/17 03:50:02 visual_prompt]: 	Test 600/1152. loss: 1.597, 0.2152 s / batch. (data: 2.82e-02)max mem: 17.22454 GB 
[09/17 03:50:22 visual_prompt]: 	Test 700/1152. loss: 1.465, 0.2038 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 03:50:42 visual_prompt]: 	Test 800/1152. loss: 1.745, 0.1850 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 03:51:02 visual_prompt]: 	Test 900/1152. loss: 1.862, 0.2008 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 03:51:21 visual_prompt]: 	Test 1000/1152. loss: 1.674, 0.1845 s / batch. (data: 1.09e-04)max mem: 17.22454 GB 
[09/17 03:51:41 visual_prompt]: 	Test 1100/1152. loss: 1.400, 0.1984 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 03:51:57 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1969, average loss: 1.6797
[09/17 03:51:57 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 34.56	top5: 98.98	
[09/17 03:51:57 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/17 03:52:13 visual_prompt]: Epoch 46 / 100: avg data time: 3.09e-01, avg batch time: 0.7126, average train loss: 1.4069
[09/17 03:52:22 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1429, average loss: 1.3792
[09/17 03:52:22 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 38.00	top5: 100.00	
[09/17 03:52:47 visual_prompt]: 	Test 100/1152. loss: 1.652, 0.1843 s / batch. (data: 1.58e-04)max mem: 17.22454 GB 
[09/17 03:53:07 visual_prompt]: 	Test 200/1152. loss: 1.561, 0.1829 s / batch. (data: 3.72e-05)max mem: 17.22454 GB 
[09/17 03:53:27 visual_prompt]: 	Test 300/1152. loss: 1.574, 0.1971 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 03:53:47 visual_prompt]: 	Test 400/1152. loss: 1.366, 0.1847 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 03:54:06 visual_prompt]: 	Test 500/1152. loss: 1.427, 0.2165 s / batch. (data: 3.30e-02)max mem: 17.22454 GB 
[09/17 03:54:26 visual_prompt]: 	Test 600/1152. loss: 1.580, 0.1887 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 03:54:46 visual_prompt]: 	Test 700/1152. loss: 1.609, 0.1837 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 03:55:06 visual_prompt]: 	Test 800/1152. loss: 1.519, 0.1888 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 03:55:26 visual_prompt]: 	Test 900/1152. loss: 1.583, 0.2056 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 03:55:45 visual_prompt]: 	Test 1000/1152. loss: 1.664, 0.2033 s / batch. (data: 1.91e-02)max mem: 17.22454 GB 
[09/17 03:56:05 visual_prompt]: 	Test 1100/1152. loss: 1.589, 0.1843 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 03:56:21 visual_prompt]: Inference (test):avg data time: 8.44e-03, avg batch time: 0.1971, average loss: 1.5204
[09/17 03:56:22 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 35.18	top5: 99.10	
[09/17 03:56:22 visual_prompt]: Best epoch 46: best metric: 0.380
[09/17 03:56:22 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/17 03:56:37 visual_prompt]: Epoch 47 / 100: avg data time: 3.05e-01, avg batch time: 0.7099, average train loss: 1.4805
[09/17 03:56:46 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1430, average loss: 1.4624
[09/17 03:56:46 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 38.50	top5: 100.00	
[09/17 03:57:11 visual_prompt]: 	Test 100/1152. loss: 1.329, 0.1960 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 03:57:30 visual_prompt]: 	Test 200/1152. loss: 1.572, 0.1973 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 03:57:50 visual_prompt]: 	Test 300/1152. loss: 1.580, 0.2446 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 03:58:10 visual_prompt]: 	Test 400/1152. loss: 1.622, 0.2103 s / batch. (data: 2.70e-02)max mem: 17.22454 GB 
[09/17 03:58:30 visual_prompt]: 	Test 500/1152. loss: 1.383, 0.2076 s / batch. (data: 2.44e-02)max mem: 17.22454 GB 
[09/17 03:58:50 visual_prompt]: 	Test 600/1152. loss: 1.445, 0.7506 s / batch. (data: 2.01e-02)max mem: 17.22454 GB 
[09/17 03:59:10 visual_prompt]: 	Test 700/1152. loss: 1.562, 0.1838 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 03:59:31 visual_prompt]: 	Test 800/1152. loss: 1.807, 0.1846 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 03:59:51 visual_prompt]: 	Test 900/1152. loss: 1.688, 0.2030 s / batch. (data: 1.94e-02)max mem: 17.22454 GB 
[09/17 04:00:10 visual_prompt]: 	Test 1000/1152. loss: 1.615, 0.1834 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 04:00:30 visual_prompt]: 	Test 1100/1152. loss: 1.834, 0.1842 s / batch. (data: 1.78e-04)max mem: 17.22454 GB 
[09/17 04:00:46 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1982, average loss: 1.5678
[09/17 04:00:46 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 36.84	top5: 99.68	
[09/17 04:00:46 visual_prompt]: Best epoch 47: best metric: 0.385
[09/17 04:00:46 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/17 04:01:02 visual_prompt]: Epoch 48 / 100: avg data time: 3.04e-01, avg batch time: 0.7109, average train loss: 1.3046
[09/17 04:01:11 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1464, average loss: 1.3815
[09/17 04:01:11 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 35.00	top5: 100.00	
[09/17 04:01:35 visual_prompt]: 	Test 100/1152. loss: 1.355, 0.1960 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 04:01:55 visual_prompt]: 	Test 200/1152. loss: 1.513, 0.1837 s / batch. (data: 1.63e-04)max mem: 17.22454 GB 
[09/17 04:02:15 visual_prompt]: 	Test 300/1152. loss: 1.771, 0.1979 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 04:02:34 visual_prompt]: 	Test 400/1152. loss: 1.371, 0.2127 s / batch. (data: 1.62e-02)max mem: 17.22454 GB 
[09/17 04:02:54 visual_prompt]: 	Test 500/1152. loss: 1.751, 0.1998 s / batch. (data: 1.64e-02)max mem: 17.22454 GB 
[09/17 04:03:14 visual_prompt]: 	Test 600/1152. loss: 1.776, 0.1961 s / batch. (data: 1.27e-02)max mem: 17.22454 GB 
[09/17 04:03:34 visual_prompt]: 	Test 700/1152. loss: 1.554, 0.1907 s / batch. (data: 1.15e-04)max mem: 17.22454 GB 
[09/17 04:03:53 visual_prompt]: 	Test 800/1152. loss: 1.511, 0.1962 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 04:04:13 visual_prompt]: 	Test 900/1152. loss: 1.448, 0.2106 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 04:04:33 visual_prompt]: 	Test 1000/1152. loss: 1.379, 0.1967 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 04:04:52 visual_prompt]: 	Test 1100/1152. loss: 1.351, 0.1938 s / batch. (data: 2.86e-05)max mem: 17.22454 GB 
[09/17 04:05:08 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1961, average loss: 1.5267
[09/17 04:05:08 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 37.21	top5: 99.98	
[09/17 04:05:08 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/17 04:05:23 visual_prompt]: Epoch 49 / 100: avg data time: 3.02e-01, avg batch time: 0.7049, average train loss: 1.3640
[09/17 04:05:33 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1432, average loss: 1.3354
[09/17 04:05:33 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 45.50	top5: 98.50	
[09/17 04:05:58 visual_prompt]: 	Test 100/1152. loss: 1.452, 0.2178 s / batch. (data: 2.79e-02)max mem: 17.22454 GB 
[09/17 04:06:17 visual_prompt]: 	Test 200/1152. loss: 1.416, 0.2110 s / batch. (data: 2.45e-02)max mem: 17.22454 GB 
[09/17 04:06:37 visual_prompt]: 	Test 300/1152. loss: 1.311, 0.1977 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 04:06:57 visual_prompt]: 	Test 400/1152. loss: 1.566, 0.2108 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 04:07:17 visual_prompt]: 	Test 500/1152. loss: 1.215, 0.2014 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 04:07:37 visual_prompt]: 	Test 600/1152. loss: 1.217, 0.1876 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 04:07:57 visual_prompt]: 	Test 700/1152. loss: 1.632, 0.1836 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 04:08:16 visual_prompt]: 	Test 800/1152. loss: 1.389, 0.1970 s / batch. (data: 1.07e-04)max mem: 17.22454 GB 
[09/17 04:08:36 visual_prompt]: 	Test 900/1152. loss: 1.389, 0.1933 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 04:08:56 visual_prompt]: 	Test 1000/1152. loss: 1.542, 0.2231 s / batch. (data: 2.25e-02)max mem: 17.22454 GB 
[09/17 04:09:16 visual_prompt]: 	Test 1100/1152. loss: 1.155, 0.1990 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 04:09:31 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1967, average loss: 1.3543
[09/17 04:09:31 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 43.27	top5: 98.78	
[09/17 04:09:31 visual_prompt]: Best epoch 49: best metric: 0.455
[09/17 04:09:31 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/17 04:09:47 visual_prompt]: Epoch 50 / 100: avg data time: 3.01e-01, avg batch time: 0.7092, average train loss: 1.1443
[09/17 04:09:56 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1431, average loss: 1.0701
[09/17 04:09:56 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 55.00	top5: 100.00	
[09/17 04:10:21 visual_prompt]: 	Test 100/1152. loss: 1.189, 0.1957 s / batch. (data: 1.31e-02)max mem: 17.22454 GB 
[09/17 04:10:40 visual_prompt]: 	Test 200/1152. loss: 1.308, 0.1985 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 04:11:01 visual_prompt]: 	Test 300/1152. loss: 1.423, 0.1960 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 04:11:20 visual_prompt]: 	Test 400/1152. loss: 1.243, 0.2111 s / batch. (data: 3.31e-05)max mem: 17.22454 GB 
[09/17 04:11:40 visual_prompt]: 	Test 500/1152. loss: 1.088, 0.1969 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 04:12:00 visual_prompt]: 	Test 600/1152. loss: 1.347, 0.1843 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 04:12:20 visual_prompt]: 	Test 700/1152. loss: 1.204, 0.1998 s / batch. (data: 3.53e-05)max mem: 17.22454 GB 
[09/17 04:12:40 visual_prompt]: 	Test 800/1152. loss: 1.387, 0.1837 s / batch. (data: 9.82e-05)max mem: 17.22454 GB 
[09/17 04:12:59 visual_prompt]: 	Test 900/1152. loss: 1.401, 0.2023 s / batch. (data: 1.89e-02)max mem: 17.22454 GB 
[09/17 04:13:19 visual_prompt]: 	Test 1000/1152. loss: 1.516, 0.1978 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 04:13:39 visual_prompt]: 	Test 1100/1152. loss: 1.467, 0.1847 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 04:13:55 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1972, average loss: 1.3262
[09/17 04:13:55 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 44.53	top5: 100.00	
[09/17 04:13:55 visual_prompt]: Best epoch 50: best metric: 0.550
[09/17 04:13:55 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/17 04:14:10 visual_prompt]: Epoch 51 / 100: avg data time: 3.12e-01, avg batch time: 0.7157, average train loss: 1.1898
[09/17 04:14:20 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1471, average loss: 0.9472
[09/17 04:14:20 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 67.50	top5: 100.00	
[09/17 04:14:45 visual_prompt]: 	Test 100/1152. loss: 0.891, 0.2090 s / batch. (data: 9.21e-03)max mem: 17.22454 GB 
[09/17 04:15:04 visual_prompt]: 	Test 200/1152. loss: 0.947, 0.1982 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 04:15:24 visual_prompt]: 	Test 300/1152. loss: 1.077, 0.2054 s / batch. (data: 6.52e-03)max mem: 17.22454 GB 
[09/17 04:15:44 visual_prompt]: 	Test 400/1152. loss: 1.076, 0.1836 s / batch. (data: 8.92e-05)max mem: 17.22454 GB 
[09/17 04:16:04 visual_prompt]: 	Test 500/1152. loss: 0.854, 0.2039 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 04:16:24 visual_prompt]: 	Test 600/1152. loss: 1.107, 0.2035 s / batch. (data: 1.09e-02)max mem: 17.22454 GB 
[09/17 04:16:44 visual_prompt]: 	Test 700/1152. loss: 1.118, 0.1948 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 04:17:04 visual_prompt]: 	Test 800/1152. loss: 1.129, 0.2081 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 04:17:24 visual_prompt]: 	Test 900/1152. loss: 1.060, 0.1843 s / batch. (data: 1.68e-04)max mem: 17.22454 GB 
[09/17 04:17:44 visual_prompt]: 	Test 1000/1152. loss: 1.146, 0.1842 s / batch. (data: 1.08e-04)max mem: 17.22454 GB 
[09/17 04:18:03 visual_prompt]: 	Test 1100/1152. loss: 1.137, 0.1875 s / batch. (data: 3.93e-05)max mem: 17.22454 GB 
[09/17 04:18:19 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1974, average loss: 1.0391
[09/17 04:18:19 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 57.89	top5: 100.00	
[09/17 04:18:19 visual_prompt]: Best epoch 51: best metric: 0.675
[09/17 04:18:19 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/17 04:18:34 visual_prompt]: Epoch 52 / 100: avg data time: 3.19e-01, avg batch time: 0.7210, average train loss: 1.0970
[09/17 04:18:44 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1428, average loss: 1.1492
[09/17 04:18:44 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 44.00	top5: 100.00	
[09/17 04:19:08 visual_prompt]: 	Test 100/1152. loss: 1.172, 0.2202 s / batch. (data: 2.88e-02)max mem: 17.22454 GB 
[09/17 04:19:28 visual_prompt]: 	Test 200/1152. loss: 1.089, 0.1869 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 04:19:47 visual_prompt]: 	Test 300/1152. loss: 1.210, 0.1983 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 04:20:07 visual_prompt]: 	Test 400/1152. loss: 1.029, 0.2171 s / batch. (data: 4.12e-05)max mem: 17.22454 GB 
[09/17 04:20:27 visual_prompt]: 	Test 500/1152. loss: 1.081, 0.1837 s / batch. (data: 1.52e-04)max mem: 17.22454 GB 
[09/17 04:20:47 visual_prompt]: 	Test 600/1152. loss: 1.364, 0.2103 s / batch. (data: 1.37e-02)max mem: 17.22454 GB 
[09/17 04:21:07 visual_prompt]: 	Test 700/1152. loss: 1.040, 0.2006 s / batch. (data: 1.56e-04)max mem: 17.22454 GB 
[09/17 04:21:27 visual_prompt]: 	Test 800/1152. loss: 1.135, 0.1961 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 04:21:47 visual_prompt]: 	Test 900/1152. loss: 1.434, 0.1987 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 04:22:07 visual_prompt]: 	Test 1000/1152. loss: 1.295, 0.1957 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 04:22:27 visual_prompt]: 	Test 1100/1152. loss: 1.032, 0.1991 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 04:22:43 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1975, average loss: 1.1914
[09/17 04:22:43 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 47.55	top5: 100.00	
[09/17 04:22:43 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/17 04:22:59 visual_prompt]: Epoch 53 / 100: avg data time: 3.02e-01, avg batch time: 0.7075, average train loss: 1.2727
[09/17 04:23:08 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1451, average loss: 1.0210
[09/17 04:23:08 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 50.50	top5: 100.00	
[09/17 04:23:32 visual_prompt]: 	Test 100/1152. loss: 0.996, 0.1986 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 04:23:52 visual_prompt]: 	Test 200/1152. loss: 1.123, 0.1965 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 04:24:12 visual_prompt]: 	Test 300/1152. loss: 1.042, 0.2086 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 04:24:31 visual_prompt]: 	Test 400/1152. loss: 1.152, 0.1998 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 04:24:52 visual_prompt]: 	Test 500/1152. loss: 0.949, 0.1840 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 04:25:11 visual_prompt]: 	Test 600/1152. loss: 0.972, 0.1952 s / batch. (data: 1.19e-02)max mem: 17.22454 GB 
[09/17 04:25:31 visual_prompt]: 	Test 700/1152. loss: 1.145, 0.1976 s / batch. (data: 5.48e-05)max mem: 17.22454 GB 
[09/17 04:25:51 visual_prompt]: 	Test 800/1152. loss: 1.121, 0.1963 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 04:26:11 visual_prompt]: 	Test 900/1152. loss: 1.036, 0.1847 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 04:26:31 visual_prompt]: 	Test 1000/1152. loss: 1.094, 0.2117 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 04:26:51 visual_prompt]: 	Test 1100/1152. loss: 1.134, 0.1943 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 04:27:06 visual_prompt]: Inference (test):avg data time: 8.11e-03, avg batch time: 0.1974, average loss: 1.0925
[09/17 04:27:06 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 47.91	top5: 100.00	
[09/17 04:27:06 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/17 04:27:22 visual_prompt]: Epoch 54 / 100: avg data time: 3.18e-01, avg batch time: 0.7223, average train loss: 0.9730
[09/17 04:27:32 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1430, average loss: 1.0613
[09/17 04:27:32 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 61.00	top5: 100.00	
[09/17 04:27:56 visual_prompt]: 	Test 100/1152. loss: 0.774, 0.2149 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 04:28:16 visual_prompt]: 	Test 200/1152. loss: 1.064, 0.2137 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 04:28:36 visual_prompt]: 	Test 300/1152. loss: 0.899, 0.1835 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 04:28:56 visual_prompt]: 	Test 400/1152. loss: 1.202, 0.1844 s / batch. (data: 1.71e-04)max mem: 17.22454 GB 
[09/17 04:29:16 visual_prompt]: 	Test 500/1152. loss: 1.022, 0.1922 s / batch. (data: 4.29e-05)max mem: 17.22454 GB 
[09/17 04:29:35 visual_prompt]: 	Test 600/1152. loss: 1.022, 0.2072 s / batch. (data: 1.72e-04)max mem: 17.22454 GB 
[09/17 04:29:55 visual_prompt]: 	Test 700/1152. loss: 1.022, 0.1845 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 04:30:15 visual_prompt]: 	Test 800/1152. loss: 1.107, 0.2277 s / batch. (data: 2.71e-02)max mem: 17.22454 GB 
[09/17 04:30:35 visual_prompt]: 	Test 900/1152. loss: 1.055, 0.1848 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 04:30:55 visual_prompt]: 	Test 1000/1152. loss: 1.224, 0.2502 s / batch. (data: 2.48e-04)max mem: 17.22454 GB 
[09/17 04:31:14 visual_prompt]: 	Test 1100/1152. loss: 1.088, 0.1990 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 04:31:30 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1969, average loss: 1.0748
[09/17 04:31:30 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 57.26	top5: 100.00	
[09/17 04:31:30 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/17 04:31:45 visual_prompt]: Epoch 55 / 100: avg data time: 3.21e-01, avg batch time: 0.7239, average train loss: 1.1521
[09/17 04:31:55 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1429, average loss: 1.1325
[09/17 04:31:55 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 47.00	top5: 100.00	
[09/17 04:32:19 visual_prompt]: 	Test 100/1152. loss: 1.153, 0.1834 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 04:32:39 visual_prompt]: 	Test 200/1152. loss: 1.255, 0.1848 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 04:32:59 visual_prompt]: 	Test 300/1152. loss: 1.210, 0.2052 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 04:33:18 visual_prompt]: 	Test 400/1152. loss: 1.211, 0.1986 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 04:33:38 visual_prompt]: 	Test 500/1152. loss: 1.079, 0.1993 s / batch. (data: 1.52e-04)max mem: 17.22454 GB 
[09/17 04:33:58 visual_prompt]: 	Test 600/1152. loss: 1.215, 0.1841 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 04:34:17 visual_prompt]: 	Test 700/1152. loss: 1.140, 0.1863 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 04:34:37 visual_prompt]: 	Test 800/1152. loss: 1.110, 0.2037 s / batch. (data: 1.43e-02)max mem: 17.22454 GB 
[09/17 04:34:58 visual_prompt]: 	Test 900/1152. loss: 1.405, 0.1855 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 04:35:17 visual_prompt]: 	Test 1000/1152. loss: 1.444, 0.1917 s / batch. (data: 9.54e-05)max mem: 17.22454 GB 
[09/17 04:35:37 visual_prompt]: 	Test 1100/1152. loss: 0.911, 0.1847 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 04:35:52 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1964, average loss: 1.1984
[09/17 04:35:53 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 48.32	top5: 100.00	
[09/17 04:35:53 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 04:36:09 visual_prompt]: Epoch 56 / 100: avg data time: 3.17e-01, avg batch time: 0.7760, average train loss: 0.9941
[09/17 04:36:18 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1428, average loss: 0.8970
[09/17 04:36:18 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 62.50	top5: 100.00	
[09/17 04:36:43 visual_prompt]: 	Test 100/1152. loss: 1.147, 0.2116 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 04:37:02 visual_prompt]: 	Test 200/1152. loss: 1.110, 0.2036 s / batch. (data: 2.07e-02)max mem: 17.22454 GB 
[09/17 04:37:22 visual_prompt]: 	Test 300/1152. loss: 0.999, 0.2016 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 04:37:42 visual_prompt]: 	Test 400/1152. loss: 0.922, 0.1886 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 04:38:02 visual_prompt]: 	Test 500/1152. loss: 1.017, 0.1996 s / batch. (data: 1.55e-02)max mem: 17.22454 GB 
[09/17 04:38:22 visual_prompt]: 	Test 600/1152. loss: 1.191, 0.1897 s / batch. (data: 1.03e-04)max mem: 17.22454 GB 
[09/17 04:38:42 visual_prompt]: 	Test 700/1152. loss: 1.037, 0.2025 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 04:39:01 visual_prompt]: 	Test 800/1152. loss: 1.044, 0.1845 s / batch. (data: 3.10e-05)max mem: 17.22454 GB 
[09/17 04:39:21 visual_prompt]: 	Test 900/1152. loss: 1.117, 0.1855 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 04:39:41 visual_prompt]: 	Test 1000/1152. loss: 0.872, 0.1877 s / batch. (data: 1.66e-04)max mem: 17.22454 GB 
[09/17 04:40:01 visual_prompt]: 	Test 1100/1152. loss: 1.055, 0.1917 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 04:40:17 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1972, average loss: 1.0626
[09/17 04:40:17 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 54.80	top5: 100.00	
[09/17 04:40:17 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 04:40:32 visual_prompt]: Epoch 57 / 100: avg data time: 3.03e-01, avg batch time: 0.7137, average train loss: 0.9647
[09/17 04:40:41 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1428, average loss: 1.1206
[09/17 04:40:41 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 53.50	top5: 100.00	
[09/17 04:41:06 visual_prompt]: 	Test 100/1152. loss: 1.263, 0.1927 s / batch. (data: 9.98e-03)max mem: 17.22454 GB 
[09/17 04:41:26 visual_prompt]: 	Test 200/1152. loss: 0.986, 0.2012 s / batch. (data: 1.82e-02)max mem: 17.22454 GB 
[09/17 04:41:45 visual_prompt]: 	Test 300/1152. loss: 1.015, 0.1957 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 04:42:05 visual_prompt]: 	Test 400/1152. loss: 1.161, 0.1970 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 04:42:25 visual_prompt]: 	Test 500/1152. loss: 1.369, 0.1844 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 04:42:45 visual_prompt]: 	Test 600/1152. loss: 1.104, 0.2004 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 04:43:05 visual_prompt]: 	Test 700/1152. loss: 1.287, 0.2120 s / batch. (data: 9.39e-05)max mem: 17.22454 GB 
[09/17 04:43:25 visual_prompt]: 	Test 800/1152. loss: 1.073, 0.1878 s / batch. (data: 1.07e-04)max mem: 17.22454 GB 
[09/17 04:43:44 visual_prompt]: 	Test 900/1152. loss: 1.322, 0.2058 s / batch. (data: 3.22e-05)max mem: 17.22454 GB 
[09/17 04:44:04 visual_prompt]: 	Test 1000/1152. loss: 1.388, 0.1976 s / batch. (data: 9.73e-05)max mem: 17.22454 GB 
[09/17 04:44:24 visual_prompt]: 	Test 1100/1152. loss: 0.968, 0.2279 s / batch. (data: 4.43e-02)max mem: 17.22454 GB 
[09/17 04:44:39 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1968, average loss: 1.2396
[09/17 04:44:40 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 50.00	top5: 99.89	
[09/17 04:44:40 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 04:44:55 visual_prompt]: Epoch 58 / 100: avg data time: 3.18e-01, avg batch time: 0.7214, average train loss: 1.0776
[09/17 04:45:04 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1434, average loss: 1.2095
[09/17 04:45:04 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 46.50	top5: 100.00	
[09/17 04:45:29 visual_prompt]: 	Test 100/1152. loss: 1.174, 0.1953 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 04:45:48 visual_prompt]: 	Test 200/1152. loss: 1.177, 0.2126 s / batch. (data: 3.01e-02)max mem: 17.22454 GB 
[09/17 04:46:08 visual_prompt]: 	Test 300/1152. loss: 1.174, 0.1880 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 04:46:28 visual_prompt]: 	Test 400/1152. loss: 1.066, 0.1934 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 04:46:48 visual_prompt]: 	Test 500/1152. loss: 1.290, 0.1879 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 04:47:08 visual_prompt]: 	Test 600/1152. loss: 1.217, 0.1966 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 04:47:28 visual_prompt]: 	Test 700/1152. loss: 1.255, 0.2159 s / batch. (data: 1.57e-02)max mem: 17.22454 GB 
[09/17 04:47:48 visual_prompt]: 	Test 800/1152. loss: 1.249, 0.1908 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 04:48:08 visual_prompt]: 	Test 900/1152. loss: 1.140, 0.1849 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 04:48:27 visual_prompt]: 	Test 1000/1152. loss: 1.140, 0.1853 s / batch. (data: 1.15e-04)max mem: 17.22454 GB 
[09/17 04:48:47 visual_prompt]: 	Test 1100/1152. loss: 1.266, 0.1846 s / batch. (data: 9.87e-05)max mem: 17.22454 GB 
[09/17 04:49:03 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1968, average loss: 1.2174
[09/17 04:49:03 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 47.36	top5: 100.00	
[09/17 04:49:03 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 04:49:18 visual_prompt]: Epoch 59 / 100: avg data time: 3.09e-01, avg batch time: 0.7108, average train loss: 1.0922
[09/17 04:49:28 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.2343, average loss: 0.8759
[09/17 04:49:28 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 58.00	top5: 100.00	
[09/17 04:49:53 visual_prompt]: 	Test 100/1152. loss: 0.820, 0.1833 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 04:50:12 visual_prompt]: 	Test 200/1152. loss: 1.075, 0.2076 s / batch. (data: 2.43e-02)max mem: 17.22454 GB 
[09/17 04:50:32 visual_prompt]: 	Test 300/1152. loss: 1.123, 0.1948 s / batch. (data: 1.13e-02)max mem: 17.22454 GB 
[09/17 04:50:51 visual_prompt]: 	Test 400/1152. loss: 0.968, 0.2078 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 04:51:11 visual_prompt]: 	Test 500/1152. loss: 0.864, 0.2069 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 04:51:31 visual_prompt]: 	Test 600/1152. loss: 1.023, 0.1956 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 04:51:51 visual_prompt]: 	Test 700/1152. loss: 0.950, 0.1864 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 04:52:11 visual_prompt]: 	Test 800/1152. loss: 1.024, 0.1845 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 04:52:31 visual_prompt]: 	Test 900/1152. loss: 1.137, 0.1848 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 04:52:50 visual_prompt]: 	Test 1000/1152. loss: 1.166, 0.1971 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 04:53:10 visual_prompt]: 	Test 1100/1152. loss: 0.968, 0.1846 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 04:53:26 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1967, average loss: 1.0134
[09/17 04:53:26 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 54.98	top5: 99.99	
[09/17 04:53:26 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 04:53:42 visual_prompt]: Epoch 60 / 100: avg data time: 3.14e-01, avg batch time: 0.7167, average train loss: 0.8450
[09/17 04:53:51 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1428, average loss: 0.7808
[09/17 04:53:51 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 70.50	top5: 100.00	
[09/17 04:54:15 visual_prompt]: 	Test 100/1152. loss: 0.999, 0.1960 s / batch. (data: 1.34e-02)max mem: 17.22454 GB 
[09/17 04:54:35 visual_prompt]: 	Test 200/1152. loss: 0.863, 0.1841 s / batch. (data: 1.87e-04)max mem: 17.22454 GB 
[09/17 04:54:55 visual_prompt]: 	Test 300/1152. loss: 0.852, 0.2081 s / batch. (data: 1.56e-02)max mem: 17.22454 GB 
[09/17 04:55:15 visual_prompt]: 	Test 400/1152. loss: 0.946, 0.1986 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 04:55:34 visual_prompt]: 	Test 500/1152. loss: 1.200, 0.1853 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 04:55:54 visual_prompt]: 	Test 600/1152. loss: 0.978, 0.1846 s / batch. (data: 1.01e-04)max mem: 17.22454 GB 
[09/17 04:56:14 visual_prompt]: 	Test 700/1152. loss: 1.080, 0.1837 s / batch. (data: 1.58e-04)max mem: 17.22454 GB 
[09/17 04:56:34 visual_prompt]: 	Test 800/1152. loss: 0.988, 0.2076 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 04:56:53 visual_prompt]: 	Test 900/1152. loss: 1.048, 0.1967 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 04:57:13 visual_prompt]: 	Test 1000/1152. loss: 1.041, 0.2039 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 04:57:33 visual_prompt]: 	Test 1100/1152. loss: 1.007, 0.2099 s / batch. (data: 1.02e-04)max mem: 17.22454 GB 
[09/17 04:57:49 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1966, average loss: 1.0150
[09/17 04:57:49 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 59.91	top5: 100.00	
[09/17 04:57:49 visual_prompt]: Best epoch 60: best metric: 0.705
[09/17 04:57:49 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 04:58:05 visual_prompt]: Epoch 61 / 100: avg data time: 3.20e-01, avg batch time: 0.7241, average train loss: 0.8661
[09/17 04:58:14 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1430, average loss: 0.5792
[09/17 04:58:14 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 79.00	top5: 100.00	
[09/17 04:58:39 visual_prompt]: 	Test 100/1152. loss: 0.757, 0.1837 s / batch. (data: 1.50e-04)max mem: 17.22454 GB 
[09/17 04:58:59 visual_prompt]: 	Test 200/1152. loss: 0.797, 0.1841 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 04:59:18 visual_prompt]: 	Test 300/1152. loss: 0.706, 0.1983 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 04:59:38 visual_prompt]: 	Test 400/1152. loss: 0.771, 0.1987 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 04:59:58 visual_prompt]: 	Test 500/1152. loss: 0.797, 0.1906 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 05:00:18 visual_prompt]: 	Test 600/1152. loss: 0.863, 0.2113 s / batch. (data: 2.79e-02)max mem: 17.22454 GB 
[09/17 05:00:38 visual_prompt]: 	Test 700/1152. loss: 0.988, 0.1837 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 05:00:57 visual_prompt]: 	Test 800/1152. loss: 0.817, 0.2007 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 05:01:17 visual_prompt]: 	Test 900/1152. loss: 0.960, 0.2117 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 05:01:37 visual_prompt]: 	Test 1000/1152. loss: 1.121, 0.2157 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 05:01:57 visual_prompt]: 	Test 1100/1152. loss: 0.877, 0.1967 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 05:02:13 visual_prompt]: Inference (test):avg data time: 8.59e-03, avg batch time: 0.1973, average loss: 0.8453
[09/17 05:02:13 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 67.62	top5: 100.00	
[09/17 05:02:13 visual_prompt]: Best epoch 61: best metric: 0.790
[09/17 05:02:13 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 05:02:29 visual_prompt]: Epoch 62 / 100: avg data time: 3.02e-01, avg batch time: 0.7444, average train loss: 0.7628
[09/17 05:02:38 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1431, average loss: 0.7334
[09/17 05:02:38 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 68.50	top5: 100.00	
[09/17 05:03:03 visual_prompt]: 	Test 100/1152. loss: 1.001, 0.1843 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 05:03:23 visual_prompt]: 	Test 200/1152. loss: 0.777, 0.1998 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 05:03:42 visual_prompt]: 	Test 300/1152. loss: 0.813, 0.1922 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 05:04:02 visual_prompt]: 	Test 400/1152. loss: 0.732, 0.1846 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 05:04:22 visual_prompt]: 	Test 500/1152. loss: 0.799, 0.1893 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 05:04:42 visual_prompt]: 	Test 600/1152. loss: 0.825, 0.1926 s / batch. (data: 1.65e-04)max mem: 17.22454 GB 
[09/17 05:05:02 visual_prompt]: 	Test 700/1152. loss: 0.827, 0.1838 s / batch. (data: 1.02e-04)max mem: 17.22454 GB 
[09/17 05:05:22 visual_prompt]: 	Test 800/1152. loss: 0.633, 0.1992 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 05:05:41 visual_prompt]: 	Test 900/1152. loss: 0.770, 0.1873 s / batch. (data: 3.15e-05)max mem: 17.22454 GB 
[09/17 05:06:01 visual_prompt]: 	Test 1000/1152. loss: 0.786, 0.1839 s / batch. (data: 1.05e-04)max mem: 17.22454 GB 
[09/17 05:06:21 visual_prompt]: 	Test 1100/1152. loss: 0.576, 0.1838 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 05:06:37 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1970, average loss: 0.8602
[09/17 05:06:37 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 64.56	top5: 100.00	
[09/17 05:06:37 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 05:06:53 visual_prompt]: Epoch 63 / 100: avg data time: 3.15e-01, avg batch time: 0.7200, average train loss: 0.7940
[09/17 05:07:02 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1429, average loss: 0.8124
[09/17 05:07:02 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 63.00	top5: 100.00	
[09/17 05:07:27 visual_prompt]: 	Test 100/1152. loss: 1.040, 0.1979 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 05:07:46 visual_prompt]: 	Test 200/1152. loss: 0.988, 0.1981 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 05:08:06 visual_prompt]: 	Test 300/1152. loss: 0.842, 0.1866 s / batch. (data: 2.70e-03)max mem: 17.22454 GB 
[09/17 05:08:26 visual_prompt]: 	Test 400/1152. loss: 0.780, 0.2339 s / batch. (data: 3.55e-02)max mem: 17.22454 GB 
[09/17 05:08:46 visual_prompt]: 	Test 500/1152. loss: 0.922, 0.2043 s / batch. (data: 2.08e-02)max mem: 17.22454 GB 
[09/17 05:09:06 visual_prompt]: 	Test 600/1152. loss: 0.914, 0.2010 s / batch. (data: 6.34e-05)max mem: 17.22454 GB 
[09/17 05:09:25 visual_prompt]: 	Test 700/1152. loss: 0.934, 0.1952 s / batch. (data: 1.15e-02)max mem: 17.22454 GB 
[09/17 05:09:45 visual_prompt]: 	Test 800/1152. loss: 0.649, 0.1906 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 05:10:05 visual_prompt]: 	Test 900/1152. loss: 0.885, 0.2205 s / batch. (data: 3.72e-02)max mem: 17.22454 GB 
[09/17 05:10:25 visual_prompt]: 	Test 1000/1152. loss: 0.895, 0.2470 s / batch. (data: 2.29e-02)max mem: 17.22454 GB 
[09/17 05:10:45 visual_prompt]: 	Test 1100/1152. loss: 0.575, 0.1990 s / batch. (data: 1.55e-02)max mem: 17.22454 GB 
[09/17 05:11:00 visual_prompt]: Inference (test):avg data time: 8.23e-03, avg batch time: 0.1968, average loss: 0.9211
[09/17 05:11:01 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 61.72	top5: 99.99	
[09/17 05:11:01 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 05:11:16 visual_prompt]: Epoch 64 / 100: avg data time: 3.00e-01, avg batch time: 0.7062, average train loss: 0.9253
[09/17 05:11:25 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1431, average loss: 0.6203
[09/17 05:11:25 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.50	top5: 100.00	
[09/17 05:11:50 visual_prompt]: 	Test 100/1152. loss: 0.659, 0.2074 s / batch. (data: 2.48e-02)max mem: 17.22454 GB 
[09/17 05:12:10 visual_prompt]: 	Test 200/1152. loss: 0.703, 0.2068 s / batch. (data: 2.31e-02)max mem: 17.22454 GB 
[09/17 05:12:29 visual_prompt]: 	Test 300/1152. loss: 0.769, 0.1959 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 05:12:49 visual_prompt]: 	Test 400/1152. loss: 0.701, 0.2027 s / batch. (data: 1.09e-02)max mem: 17.22454 GB 
[09/17 05:13:09 visual_prompt]: 	Test 500/1152. loss: 0.822, 0.1837 s / batch. (data: 2.93e-05)max mem: 17.22454 GB 
[09/17 05:13:28 visual_prompt]: 	Test 600/1152. loss: 0.868, 0.1995 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 05:13:48 visual_prompt]: 	Test 700/1152. loss: 0.776, 0.1845 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 05:14:08 visual_prompt]: 	Test 800/1152. loss: 0.766, 0.1838 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 05:14:28 visual_prompt]: 	Test 900/1152. loss: 0.903, 0.1969 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 05:14:49 visual_prompt]: 	Test 1000/1152. loss: 0.821, 0.1958 s / batch. (data: 1.04e-04)max mem: 17.22454 GB 
[09/17 05:15:08 visual_prompt]: 	Test 1100/1152. loss: 0.669, 0.2199 s / batch. (data: 2.72e-02)max mem: 17.22454 GB 
[09/17 05:15:24 visual_prompt]: Inference (test):avg data time: 8.26e-03, avg batch time: 0.1972, average loss: 0.7751
[09/17 05:15:24 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 66.48	top5: 100.00	
[09/17 05:15:24 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 05:15:39 visual_prompt]: Epoch 65 / 100: avg data time: 3.13e-01, avg batch time: 0.7137, average train loss: 0.7184
[09/17 05:15:49 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1430, average loss: 0.6488
[09/17 05:15:49 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 70.00	top5: 100.00	
[09/17 05:16:14 visual_prompt]: 	Test 100/1152. loss: 0.893, 0.1974 s / batch. (data: 1.45e-02)max mem: 17.22454 GB 
[09/17 05:16:33 visual_prompt]: 	Test 200/1152. loss: 0.638, 0.1981 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 05:16:53 visual_prompt]: 	Test 300/1152. loss: 0.865, 0.1977 s / batch. (data: 2.84e-05)max mem: 17.22454 GB 
[09/17 05:17:13 visual_prompt]: 	Test 400/1152. loss: 0.788, 0.1847 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 05:17:33 visual_prompt]: 	Test 500/1152. loss: 0.803, 0.1919 s / batch. (data: 1.72e-04)max mem: 17.22454 GB 
[09/17 05:17:53 visual_prompt]: 	Test 600/1152. loss: 0.953, 0.1932 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 05:18:13 visual_prompt]: 	Test 700/1152. loss: 0.971, 0.2132 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 05:18:32 visual_prompt]: 	Test 800/1152. loss: 0.735, 0.1962 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 05:18:52 visual_prompt]: 	Test 900/1152. loss: 0.895, 0.1844 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 05:19:12 visual_prompt]: 	Test 1000/1152. loss: 0.928, 0.1889 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 05:19:32 visual_prompt]: 	Test 1100/1152. loss: 0.714, 0.1988 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 05:19:47 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1970, average loss: 0.8991
[09/17 05:19:48 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 62.97	top5: 100.00	
[09/17 05:19:48 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 05:20:03 visual_prompt]: Epoch 66 / 100: avg data time: 3.20e-01, avg batch time: 0.7234, average train loss: 0.6644
[09/17 05:20:13 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1437, average loss: 0.5925
[09/17 05:20:13 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 75.50	top5: 100.00	
[09/17 05:20:37 visual_prompt]: 	Test 100/1152. loss: 1.010, 0.1837 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 05:20:57 visual_prompt]: 	Test 200/1152. loss: 1.163, 0.1964 s / batch. (data: 1.27e-02)max mem: 17.22454 GB 
[09/17 05:21:16 visual_prompt]: 	Test 300/1152. loss: 1.140, 0.1840 s / batch. (data: 1.99e-04)max mem: 17.22454 GB 
[09/17 05:21:36 visual_prompt]: 	Test 400/1152. loss: 0.959, 0.1970 s / batch. (data: 1.75e-04)max mem: 17.22454 GB 
[09/17 05:21:56 visual_prompt]: 	Test 500/1152. loss: 0.831, 0.2024 s / batch. (data: 1.92e-02)max mem: 17.22454 GB 
[09/17 05:22:15 visual_prompt]: 	Test 600/1152. loss: 1.101, 0.1984 s / batch. (data: 1.45e-02)max mem: 17.22454 GB 
[09/17 05:22:35 visual_prompt]: 	Test 700/1152. loss: 0.735, 0.1840 s / batch. (data: 9.68e-05)max mem: 17.22454 GB 
[09/17 05:22:55 visual_prompt]: 	Test 800/1152. loss: 0.780, 0.1909 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 05:23:15 visual_prompt]: 	Test 900/1152. loss: 1.272, 0.1850 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 05:23:35 visual_prompt]: 	Test 1000/1152. loss: 1.232, 0.2081 s / batch. (data: 2.43e-02)max mem: 17.22454 GB 
[09/17 05:23:55 visual_prompt]: 	Test 1100/1152. loss: 0.589, 0.1906 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 05:24:10 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1962, average loss: 0.9633
[09/17 05:24:11 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 64.17	top5: 100.00	
[09/17 05:24:11 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 05:24:27 visual_prompt]: Epoch 67 / 100: avg data time: 3.15e-01, avg batch time: 0.7570, average train loss: 0.7023
[09/17 05:24:36 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1430, average loss: 0.8537
[09/17 05:24:36 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 61.00	top5: 100.00	
[09/17 05:25:01 visual_prompt]: 	Test 100/1152. loss: 1.275, 0.1968 s / batch. (data: 1.72e-04)max mem: 17.22454 GB 
[09/17 05:25:20 visual_prompt]: 	Test 200/1152. loss: 0.884, 0.2073 s / batch. (data: 2.43e-02)max mem: 17.22454 GB 
[09/17 05:25:40 visual_prompt]: 	Test 300/1152. loss: 0.745, 0.1960 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 05:26:00 visual_prompt]: 	Test 400/1152. loss: 0.997, 0.1837 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 05:26:20 visual_prompt]: 	Test 500/1152. loss: 1.079, 0.2326 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 05:26:40 visual_prompt]: 	Test 600/1152. loss: 1.006, 0.1843 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 05:27:00 visual_prompt]: 	Test 700/1152. loss: 1.184, 0.1839 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 05:27:20 visual_prompt]: 	Test 800/1152. loss: 0.768, 0.2039 s / batch. (data: 3.79e-05)max mem: 17.22454 GB 
[09/17 05:27:39 visual_prompt]: 	Test 900/1152. loss: 0.843, 0.2160 s / batch. (data: 1.57e-02)max mem: 17.22454 GB 
[09/17 05:27:59 visual_prompt]: 	Test 1000/1152. loss: 0.927, 0.2561 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 05:28:19 visual_prompt]: 	Test 1100/1152. loss: 0.697, 0.1847 s / batch. (data: 1.06e-04)max mem: 17.22454 GB 
[09/17 05:28:34 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1977, average loss: 0.9754
[09/17 05:28:35 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 58.75	top5: 100.00	
[09/17 05:28:35 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 05:28:51 visual_prompt]: Epoch 68 / 100: avg data time: 2.97e-01, avg batch time: 0.7023, average train loss: 0.6790
[09/17 05:29:00 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1435, average loss: 0.5121
[09/17 05:29:00 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 79.00	top5: 100.00	
[09/17 05:29:24 visual_prompt]: 	Test 100/1152. loss: 0.704, 0.1830 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 05:29:44 visual_prompt]: 	Test 200/1152. loss: 0.581, 0.1836 s / batch. (data: 1.21e-04)max mem: 17.22454 GB 
[09/17 05:30:04 visual_prompt]: 	Test 300/1152. loss: 0.680, 0.2001 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 05:30:24 visual_prompt]: 	Test 400/1152. loss: 0.636, 0.2115 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 05:30:43 visual_prompt]: 	Test 500/1152. loss: 0.675, 0.1842 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 05:31:03 visual_prompt]: 	Test 600/1152. loss: 0.657, 0.1845 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 05:31:23 visual_prompt]: 	Test 700/1152. loss: 0.688, 0.1844 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 05:31:42 visual_prompt]: 	Test 800/1152. loss: 0.731, 0.1879 s / batch. (data: 1.21e-04)max mem: 17.22454 GB 
[09/17 05:32:02 visual_prompt]: 	Test 900/1152. loss: 0.646, 0.1934 s / batch. (data: 2.16e-04)max mem: 17.22454 GB 
[09/17 05:32:22 visual_prompt]: 	Test 1000/1152. loss: 0.696, 0.1904 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 05:32:42 visual_prompt]: 	Test 1100/1152. loss: 0.665, 0.1988 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 05:32:57 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1966, average loss: 0.7626
[09/17 05:32:58 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 67.43	top5: 100.00	
[09/17 05:32:58 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 05:33:13 visual_prompt]: Epoch 69 / 100: avg data time: 3.02e-01, avg batch time: 0.7083, average train loss: 0.5204
[09/17 05:33:22 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1427, average loss: 0.3648
[09/17 05:33:22 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 84.00	top5: 100.00	
[09/17 05:33:47 visual_prompt]: 	Test 100/1152. loss: 0.579, 0.1987 s / batch. (data: 1.41e-02)max mem: 17.22454 GB 
[09/17 05:34:06 visual_prompt]: 	Test 200/1152. loss: 0.639, 0.1838 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 05:34:26 visual_prompt]: 	Test 300/1152. loss: 0.701, 0.1855 s / batch. (data: 3.02e-04)max mem: 17.22454 GB 
[09/17 05:34:46 visual_prompt]: 	Test 400/1152. loss: 0.454, 0.1838 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 05:35:06 visual_prompt]: 	Test 500/1152. loss: 0.652, 0.1850 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 05:35:26 visual_prompt]: 	Test 600/1152. loss: 0.745, 0.1837 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 05:35:46 visual_prompt]: 	Test 700/1152. loss: 0.800, 0.2042 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 05:36:05 visual_prompt]: 	Test 800/1152. loss: 0.550, 0.1888 s / batch. (data: 4.24e-03)max mem: 17.22454 GB 
[09/17 05:36:25 visual_prompt]: 	Test 900/1152. loss: 0.752, 0.2120 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 05:36:45 visual_prompt]: 	Test 1000/1152. loss: 0.770, 0.2026 s / batch. (data: 1.92e-02)max mem: 17.22454 GB 
[09/17 05:37:05 visual_prompt]: 	Test 1100/1152. loss: 0.551, 0.1912 s / batch. (data: 1.67e-04)max mem: 17.22454 GB 
[09/17 05:37:20 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1971, average loss: 0.6841
[09/17 05:37:21 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 72.85	top5: 100.00	
[09/17 05:37:21 visual_prompt]: Best epoch 69: best metric: 0.840
[09/17 05:37:21 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 05:37:36 visual_prompt]: Epoch 70 / 100: avg data time: 3.10e-01, avg batch time: 0.7153, average train loss: 0.4469
[09/17 05:37:46 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.2567, average loss: 0.3687
[09/17 05:37:46 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 85.50	top5: 100.00	
[09/17 05:38:10 visual_prompt]: 	Test 100/1152. loss: 0.576, 0.1957 s / batch. (data: 1.31e-02)max mem: 17.22454 GB 
[09/17 05:38:30 visual_prompt]: 	Test 200/1152. loss: 0.836, 0.1839 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 05:38:50 visual_prompt]: 	Test 300/1152. loss: 0.752, 0.1838 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 05:39:10 visual_prompt]: 	Test 400/1152. loss: 0.570, 0.1842 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 05:39:29 visual_prompt]: 	Test 500/1152. loss: 0.679, 0.1984 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 05:39:49 visual_prompt]: 	Test 600/1152. loss: 0.715, 0.2113 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 05:40:09 visual_prompt]: 	Test 700/1152. loss: 0.809, 0.2855 s / batch. (data: 5.70e-02)max mem: 17.22454 GB 
[09/17 05:40:30 visual_prompt]: 	Test 800/1152. loss: 0.748, 0.2036 s / batch. (data: 2.03e-02)max mem: 17.22454 GB 
[09/17 05:40:49 visual_prompt]: 	Test 900/1152. loss: 0.914, 0.1919 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 05:41:09 visual_prompt]: 	Test 1000/1152. loss: 0.879, 0.1997 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 05:41:29 visual_prompt]: 	Test 1100/1152. loss: 0.792, 0.2120 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 05:41:45 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1974, average loss: 0.7766
[09/17 05:41:45 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 71.17	top5: 100.00	
[09/17 05:41:45 visual_prompt]: Best epoch 70: best metric: 0.855
[09/17 05:41:45 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 05:42:00 visual_prompt]: Epoch 71 / 100: avg data time: 2.97e-01, avg batch time: 0.7005, average train loss: 0.4891
[09/17 05:42:10 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1430, average loss: 0.4810
[09/17 05:42:10 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 77.50	top5: 100.00	
[09/17 05:42:35 visual_prompt]: 	Test 100/1152. loss: 0.700, 0.1863 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 05:42:54 visual_prompt]: 	Test 200/1152. loss: 0.989, 0.1840 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 05:43:14 visual_prompt]: 	Test 300/1152. loss: 0.724, 0.1836 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 05:43:34 visual_prompt]: 	Test 400/1152. loss: 0.720, 0.1840 s / batch. (data: 1.70e-04)max mem: 17.22454 GB 
[09/17 05:43:53 visual_prompt]: 	Test 500/1152. loss: 0.746, 0.2082 s / batch. (data: 2.42e-02)max mem: 17.22454 GB 
[09/17 05:44:13 visual_prompt]: 	Test 600/1152. loss: 0.788, 0.1986 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 05:44:33 visual_prompt]: 	Test 700/1152. loss: 0.923, 0.1922 s / batch. (data: 7.81e-03)max mem: 17.22454 GB 
[09/17 05:44:53 visual_prompt]: 	Test 800/1152. loss: 0.806, 0.1963 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 05:45:13 visual_prompt]: 	Test 900/1152. loss: 0.948, 0.2007 s / batch. (data: 1.20e-02)max mem: 17.22454 GB 
[09/17 05:45:33 visual_prompt]: 	Test 1000/1152. loss: 0.940, 0.2104 s / batch. (data: 1.34e-02)max mem: 17.22454 GB 
[09/17 05:45:53 visual_prompt]: 	Test 1100/1152. loss: 0.781, 0.1856 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 05:46:09 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1974, average loss: 0.8433
[09/17 05:46:10 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 66.12	top5: 100.00	
[09/17 05:46:10 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 05:46:25 visual_prompt]: Epoch 72 / 100: avg data time: 3.05e-01, avg batch time: 0.7110, average train loss: 0.6603
[09/17 05:46:34 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1464, average loss: 0.6009
[09/17 05:46:34 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 75.00	top5: 100.00	
[09/17 05:46:59 visual_prompt]: 	Test 100/1152. loss: 1.085, 0.1904 s / batch. (data: 1.16e-04)max mem: 17.22454 GB 
[09/17 05:47:19 visual_prompt]: 	Test 200/1152. loss: 1.065, 0.1963 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 05:47:38 visual_prompt]: 	Test 300/1152. loss: 1.017, 0.1951 s / batch. (data: 1.18e-02)max mem: 17.22454 GB 
[09/17 05:47:58 visual_prompt]: 	Test 400/1152. loss: 0.762, 0.1847 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 05:48:18 visual_prompt]: 	Test 500/1152. loss: 0.898, 0.1979 s / batch. (data: 1.45e-02)max mem: 17.22454 GB 
[09/17 05:48:38 visual_prompt]: 	Test 600/1152. loss: 0.893, 0.2034 s / batch. (data: 1.08e-02)max mem: 17.22454 GB 
[09/17 05:48:57 visual_prompt]: 	Test 700/1152. loss: 0.775, 0.1977 s / batch. (data: 1.39e-02)max mem: 17.22454 GB 
[09/17 05:49:17 visual_prompt]: 	Test 800/1152. loss: 0.683, 0.1961 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 05:49:37 visual_prompt]: 	Test 900/1152. loss: 1.034, 0.1845 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 05:49:57 visual_prompt]: 	Test 1000/1152. loss: 1.087, 0.1891 s / batch. (data: 1.02e-04)max mem: 17.22454 GB 
[09/17 05:50:17 visual_prompt]: 	Test 1100/1152. loss: 0.565, 0.1853 s / batch. (data: 1.12e-04)max mem: 17.22454 GB 
[09/17 05:50:32 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1968, average loss: 0.9204
[09/17 05:50:32 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 63.11	top5: 100.00	
[09/17 05:50:32 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 05:50:48 visual_prompt]: Epoch 73 / 100: avg data time: 3.17e-01, avg batch time: 0.7215, average train loss: 0.5808
[09/17 05:50:57 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1429, average loss: 0.4334
[09/17 05:50:57 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 83.50	top5: 100.00	
[09/17 05:51:22 visual_prompt]: 	Test 100/1152. loss: 0.712, 0.2145 s / batch. (data: 2.47e-02)max mem: 17.22454 GB 
[09/17 05:51:41 visual_prompt]: 	Test 200/1152. loss: 0.822, 0.1838 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 05:52:01 visual_prompt]: 	Test 300/1152. loss: 0.745, 0.1844 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 05:52:21 visual_prompt]: 	Test 400/1152. loss: 0.667, 0.2027 s / batch. (data: 1.92e-02)max mem: 17.22454 GB 
[09/17 05:52:40 visual_prompt]: 	Test 500/1152. loss: 0.616, 0.1842 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 05:53:00 visual_prompt]: 	Test 600/1152. loss: 0.664, 0.1835 s / batch. (data: 3.65e-05)max mem: 17.22454 GB 
[09/17 05:53:20 visual_prompt]: 	Test 700/1152. loss: 0.752, 0.1988 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 05:53:40 visual_prompt]: 	Test 800/1152. loss: 0.501, 0.1845 s / batch. (data: 1.84e-04)max mem: 17.22454 GB 
[09/17 05:54:00 visual_prompt]: 	Test 900/1152. loss: 0.817, 0.2123 s / batch. (data: 1.27e-02)max mem: 17.22454 GB 
[09/17 05:54:20 visual_prompt]: 	Test 1000/1152. loss: 0.828, 0.1892 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 05:54:40 visual_prompt]: 	Test 1100/1152. loss: 0.486, 0.1839 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 05:54:56 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1970, average loss: 0.7167
[09/17 05:54:56 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 70.86	top5: 100.00	
[09/17 05:54:56 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 05:55:11 visual_prompt]: Epoch 74 / 100: avg data time: 3.05e-01, avg batch time: 0.7321, average train loss: 0.4772
[09/17 05:55:21 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1430, average loss: 0.4452
[09/17 05:55:21 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 82.00	top5: 100.00	
[09/17 05:55:46 visual_prompt]: 	Test 100/1152. loss: 0.839, 0.1961 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 05:56:05 visual_prompt]: 	Test 200/1152. loss: 0.737, 0.1877 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 05:56:25 visual_prompt]: 	Test 300/1152. loss: 0.766, 0.2090 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 05:56:45 visual_prompt]: 	Test 400/1152. loss: 0.689, 0.2039 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 05:57:04 visual_prompt]: 	Test 500/1152. loss: 0.685, 0.1986 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 05:57:24 visual_prompt]: 	Test 600/1152. loss: 0.769, 0.2199 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 05:57:44 visual_prompt]: 	Test 700/1152. loss: 0.859, 0.1847 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 05:58:04 visual_prompt]: 	Test 800/1152. loss: 0.639, 0.1847 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 05:58:24 visual_prompt]: 	Test 900/1152. loss: 0.954, 0.1961 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 05:58:44 visual_prompt]: 	Test 1000/1152. loss: 0.811, 0.1840 s / batch. (data: 9.30e-05)max mem: 17.22454 GB 
[09/17 05:59:03 visual_prompt]: 	Test 1100/1152. loss: 0.398, 0.1976 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 05:59:19 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1966, average loss: 0.8291
[09/17 05:59:19 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 70.49	top5: 99.98	
[09/17 05:59:19 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 05:59:34 visual_prompt]: Epoch 75 / 100: avg data time: 3.08e-01, avg batch time: 0.7156, average train loss: 0.4325
[09/17 05:59:44 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1429, average loss: 0.2860
[09/17 05:59:44 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 92.50	top5: 100.00	
[09/17 06:00:08 visual_prompt]: 	Test 100/1152. loss: 0.621, 0.1843 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 06:00:28 visual_prompt]: 	Test 200/1152. loss: 0.704, 0.1877 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 06:00:48 visual_prompt]: 	Test 300/1152. loss: 0.676, 0.1983 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 06:01:08 visual_prompt]: 	Test 400/1152. loss: 0.646, 0.1836 s / batch. (data: 3.43e-05)max mem: 17.22454 GB 
[09/17 06:01:28 visual_prompt]: 	Test 500/1152. loss: 0.580, 0.1851 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 06:01:48 visual_prompt]: 	Test 600/1152. loss: 0.612, 0.1842 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 06:02:08 visual_prompt]: 	Test 700/1152. loss: 0.782, 0.1917 s / batch. (data: 1.72e-04)max mem: 17.22454 GB 
[09/17 06:02:28 visual_prompt]: 	Test 800/1152. loss: 0.467, 0.1843 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 06:02:48 visual_prompt]: 	Test 900/1152. loss: 0.775, 0.1856 s / batch. (data: 1.32e-03)max mem: 17.22454 GB 
[09/17 06:03:08 visual_prompt]: 	Test 1000/1152. loss: 0.880, 0.1997 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 06:03:28 visual_prompt]: 	Test 1100/1152. loss: 0.447, 0.1858 s / batch. (data: 1.78e-04)max mem: 17.22454 GB 
[09/17 06:03:43 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1980, average loss: 0.6562
[09/17 06:03:43 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 75.41	top5: 100.00	
[09/17 06:03:43 visual_prompt]: Best epoch 75: best metric: 0.925
[09/17 06:03:43 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 06:03:59 visual_prompt]: Epoch 76 / 100: avg data time: 3.12e-01, avg batch time: 0.7157, average train loss: 0.4342
[09/17 06:04:08 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1429, average loss: 0.2220
[09/17 06:04:08 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 93.50	top5: 100.00	
[09/17 06:04:32 visual_prompt]: 	Test 100/1152. loss: 0.516, 0.1911 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 06:04:52 visual_prompt]: 	Test 200/1152. loss: 0.700, 0.1872 s / batch. (data: 1.08e-04)max mem: 17.22454 GB 
[09/17 06:05:12 visual_prompt]: 	Test 300/1152. loss: 0.688, 0.1855 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 06:05:32 visual_prompt]: 	Test 400/1152. loss: 0.609, 0.1948 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 06:05:52 visual_prompt]: 	Test 500/1152. loss: 0.464, 0.2042 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 06:06:12 visual_prompt]: 	Test 600/1152. loss: 0.612, 0.1843 s / batch. (data: 1.01e-04)max mem: 17.22454 GB 
[09/17 06:06:31 visual_prompt]: 	Test 700/1152. loss: 0.437, 0.1859 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 06:06:51 visual_prompt]: 	Test 800/1152. loss: 0.438, 0.1875 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 06:07:11 visual_prompt]: 	Test 900/1152. loss: 0.727, 0.1983 s / batch. (data: 1.46e-02)max mem: 17.22454 GB 
[09/17 06:07:31 visual_prompt]: 	Test 1000/1152. loss: 0.698, 0.2416 s / batch. (data: 2.65e-02)max mem: 17.22454 GB 
[09/17 06:07:50 visual_prompt]: 	Test 1100/1152. loss: 0.389, 0.2000 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 06:08:06 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1968, average loss: 0.6031
[09/17 06:08:06 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 77.31	top5: 100.00	
[09/17 06:08:06 visual_prompt]: Best epoch 76: best metric: 0.935
[09/17 06:08:06 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 06:08:22 visual_prompt]: Epoch 77 / 100: avg data time: 3.06e-01, avg batch time: 0.7105, average train loss: 0.4136
[09/17 06:08:32 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1428, average loss: 0.4107
[09/17 06:08:32 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 84.50	top5: 100.00	
[09/17 06:08:56 visual_prompt]: 	Test 100/1152. loss: 0.896, 0.1853 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 06:09:16 visual_prompt]: 	Test 200/1152. loss: 0.794, 0.1835 s / batch. (data: 1.07e-04)max mem: 17.22454 GB 
[09/17 06:09:36 visual_prompt]: 	Test 300/1152. loss: 0.838, 0.1832 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 06:09:56 visual_prompt]: 	Test 400/1152. loss: 0.726, 0.1986 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 06:10:16 visual_prompt]: 	Test 500/1152. loss: 0.756, 0.1999 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 06:10:36 visual_prompt]: 	Test 600/1152. loss: 0.795, 0.1849 s / batch. (data: 1.21e-04)max mem: 17.22454 GB 
[09/17 06:10:56 visual_prompt]: 	Test 700/1152. loss: 0.892, 0.2042 s / batch. (data: 2.06e-02)max mem: 17.22454 GB 
[09/17 06:11:16 visual_prompt]: 	Test 800/1152. loss: 0.621, 0.1982 s / batch. (data: 1.44e-02)max mem: 17.22454 GB 
[09/17 06:11:35 visual_prompt]: 	Test 900/1152. loss: 0.709, 0.2079 s / batch. (data: 2.44e-02)max mem: 17.22454 GB 
[09/17 06:11:56 visual_prompt]: 	Test 1000/1152. loss: 0.935, 0.1913 s / batch. (data: 7.30e-03)max mem: 17.22454 GB 
[09/17 06:12:15 visual_prompt]: 	Test 1100/1152. loss: 0.570, 0.1847 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 06:12:31 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1978, average loss: 0.8299
[09/17 06:12:31 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 69.09	top5: 100.00	
[09/17 06:12:31 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 06:12:47 visual_prompt]: Epoch 78 / 100: avg data time: 3.13e-01, avg batch time: 0.7197, average train loss: 0.4673
[09/17 06:12:56 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1430, average loss: 0.3493
[09/17 06:12:56 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 85.50	top5: 100.00	
[09/17 06:13:21 visual_prompt]: 	Test 100/1152. loss: 0.685, 0.1834 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 06:13:40 visual_prompt]: 	Test 200/1152. loss: 0.831, 0.1981 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 06:14:01 visual_prompt]: 	Test 300/1152. loss: 0.706, 0.7711 s / batch. (data: 2.04e-02)max mem: 17.22454 GB 
[09/17 06:14:21 visual_prompt]: 	Test 400/1152. loss: 0.617, 0.1842 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 06:14:40 visual_prompt]: 	Test 500/1152. loss: 0.562, 0.1967 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 06:15:00 visual_prompt]: 	Test 600/1152. loss: 0.630, 0.2112 s / batch. (data: 2.79e-02)max mem: 17.22454 GB 
[09/17 06:15:20 visual_prompt]: 	Test 700/1152. loss: 0.663, 0.1845 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 06:15:40 visual_prompt]: 	Test 800/1152. loss: 0.488, 0.1970 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 06:16:00 visual_prompt]: 	Test 900/1152. loss: 0.707, 0.2039 s / batch. (data: 2.02e-02)max mem: 17.22454 GB 
[09/17 06:16:20 visual_prompt]: 	Test 1000/1152. loss: 0.776, 0.1997 s / batch. (data: 1.57e-02)max mem: 17.22454 GB 
[09/17 06:16:39 visual_prompt]: 	Test 1100/1152. loss: 0.486, 0.1905 s / batch. (data: 1.64e-04)max mem: 17.22454 GB 
[09/17 06:16:55 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1974, average loss: 0.6839
[09/17 06:16:55 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 72.69	top5: 100.00	
[09/17 06:16:55 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 06:17:10 visual_prompt]: Epoch 79 / 100: avg data time: 3.13e-01, avg batch time: 0.7166, average train loss: 0.3602
[09/17 06:17:20 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1429, average loss: 0.2983
[09/17 06:17:20 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 87.50	top5: 100.00	
[09/17 06:17:44 visual_prompt]: 	Test 100/1152. loss: 0.785, 0.1978 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 06:18:04 visual_prompt]: 	Test 200/1152. loss: 0.723, 0.1896 s / batch. (data: 2.79e-05)max mem: 17.22454 GB 
[09/17 06:18:24 visual_prompt]: 	Test 300/1152. loss: 0.668, 0.2287 s / batch. (data: 4.55e-02)max mem: 17.22454 GB 
[09/17 06:18:44 visual_prompt]: 	Test 400/1152. loss: 0.814, 0.2029 s / batch. (data: 4.98e-05)max mem: 17.22454 GB 
[09/17 06:19:04 visual_prompt]: 	Test 500/1152. loss: 0.647, 0.2184 s / batch. (data: 3.49e-02)max mem: 17.22454 GB 
[09/17 06:19:24 visual_prompt]: 	Test 600/1152. loss: 0.732, 0.1836 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 06:19:44 visual_prompt]: 	Test 700/1152. loss: 0.850, 0.1952 s / batch. (data: 5.34e-05)max mem: 17.22454 GB 
[09/17 06:20:04 visual_prompt]: 	Test 800/1152. loss: 0.570, 0.1961 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 06:20:24 visual_prompt]: 	Test 900/1152. loss: 0.739, 0.1988 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 06:20:43 visual_prompt]: 	Test 1000/1152. loss: 0.777, 0.1921 s / batch. (data: 8.24e-03)max mem: 17.22454 GB 
[09/17 06:21:03 visual_prompt]: 	Test 1100/1152. loss: 0.558, 0.1837 s / batch. (data: 1.64e-04)max mem: 17.22454 GB 
[09/17 06:21:19 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1975, average loss: 0.8097
[09/17 06:21:19 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 69.96	top5: 100.00	
[09/17 06:21:19 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 06:21:34 visual_prompt]: Epoch 80 / 100: avg data time: 3.07e-01, avg batch time: 0.7093, average train loss: 0.3852
[09/17 06:21:44 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1431, average loss: 0.1992
[09/17 06:21:44 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 93.00	top5: 100.00	
[09/17 06:22:08 visual_prompt]: 	Test 100/1152. loss: 0.515, 0.1828 s / batch. (data: 1.12e-04)max mem: 17.22454 GB 
[09/17 06:22:28 visual_prompt]: 	Test 200/1152. loss: 0.565, 0.2014 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 06:22:47 visual_prompt]: 	Test 300/1152. loss: 0.596, 0.2212 s / batch. (data: 3.84e-02)max mem: 17.22454 GB 
[09/17 06:23:07 visual_prompt]: 	Test 400/1152. loss: 0.575, 0.2215 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 06:23:27 visual_prompt]: 	Test 500/1152. loss: 0.562, 0.1975 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 06:23:47 visual_prompt]: 	Test 600/1152. loss: 0.540, 0.1999 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 06:24:07 visual_prompt]: 	Test 700/1152. loss: 0.579, 0.2075 s / batch. (data: 2.37e-02)max mem: 17.22454 GB 
[09/17 06:24:27 visual_prompt]: 	Test 800/1152. loss: 0.508, 0.2021 s / batch. (data: 1.34e-02)max mem: 17.22454 GB 
[09/17 06:24:47 visual_prompt]: 	Test 900/1152. loss: 0.556, 0.1839 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 06:25:06 visual_prompt]: 	Test 1000/1152. loss: 0.623, 0.1847 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 06:25:26 visual_prompt]: 	Test 1100/1152. loss: 0.401, 0.2022 s / batch. (data: 3.36e-05)max mem: 17.22454 GB 
[09/17 06:25:42 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1969, average loss: 0.6390
[09/17 06:25:42 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 75.61	top5: 100.00	
[09/17 06:25:42 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 06:25:58 visual_prompt]: Epoch 81 / 100: avg data time: 3.08e-01, avg batch time: 0.7125, average train loss: 0.2552
[09/17 06:26:07 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1429, average loss: 0.1534
[09/17 06:26:07 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 96.00	top5: 100.00	
[09/17 06:26:32 visual_prompt]: 	Test 100/1152. loss: 0.767, 0.2233 s / batch. (data: 3.41e-02)max mem: 17.22454 GB 
[09/17 06:26:52 visual_prompt]: 	Test 200/1152. loss: 0.596, 0.1979 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 06:27:11 visual_prompt]: 	Test 300/1152. loss: 0.771, 0.2517 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 06:27:31 visual_prompt]: 	Test 400/1152. loss: 0.579, 0.2351 s / batch. (data: 1.44e-02)max mem: 17.22454 GB 
[09/17 06:27:51 visual_prompt]: 	Test 500/1152. loss: 0.564, 0.1987 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 06:28:11 visual_prompt]: 	Test 600/1152. loss: 0.665, 0.1850 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 06:28:31 visual_prompt]: 	Test 700/1152. loss: 0.663, 0.1970 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 06:28:51 visual_prompt]: 	Test 800/1152. loss: 0.431, 0.1841 s / batch. (data: 2.77e-05)max mem: 17.22454 GB 
[09/17 06:29:11 visual_prompt]: 	Test 900/1152. loss: 0.777, 0.2014 s / batch. (data: 1.66e-02)max mem: 17.22454 GB 
[09/17 06:29:31 visual_prompt]: 	Test 1000/1152. loss: 0.720, 0.1951 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 06:29:51 visual_prompt]: 	Test 1100/1152. loss: 0.311, 0.2055 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 06:30:07 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1975, average loss: 0.6782
[09/17 06:30:07 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.12	top5: 100.00	
[09/17 06:30:07 visual_prompt]: Best epoch 81: best metric: 0.960
[09/17 06:30:07 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 06:30:24 visual_prompt]: Epoch 82 / 100: avg data time: 3.13e-01, avg batch time: 0.7692, average train loss: 0.2489
[09/17 06:30:33 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1428, average loss: 0.1645
[09/17 06:30:33 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 93.00	top5: 100.00	
[09/17 06:30:58 visual_prompt]: 	Test 100/1152. loss: 0.758, 0.2037 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 06:31:18 visual_prompt]: 	Test 200/1152. loss: 0.674, 0.1986 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 06:31:37 visual_prompt]: 	Test 300/1152. loss: 0.794, 0.1881 s / batch. (data: 1.12e-04)max mem: 17.22454 GB 
[09/17 06:31:57 visual_prompt]: 	Test 400/1152. loss: 0.588, 0.1986 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 06:32:17 visual_prompt]: 	Test 500/1152. loss: 0.631, 0.1850 s / batch. (data: 3.12e-05)max mem: 17.22454 GB 
[09/17 06:32:37 visual_prompt]: 	Test 600/1152. loss: 0.706, 0.2086 s / batch. (data: 1.59e-02)max mem: 17.22454 GB 
[09/17 06:32:57 visual_prompt]: 	Test 700/1152. loss: 0.634, 0.1988 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 06:33:17 visual_prompt]: 	Test 800/1152. loss: 0.494, 0.1935 s / batch. (data: 3.41e-05)max mem: 17.22454 GB 
[09/17 06:33:37 visual_prompt]: 	Test 900/1152. loss: 0.922, 0.1850 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 06:33:57 visual_prompt]: 	Test 1000/1152. loss: 0.995, 0.1845 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 06:34:17 visual_prompt]: 	Test 1100/1152. loss: 0.397, 0.2213 s / batch. (data: 2.77e-02)max mem: 17.22454 GB 
[09/17 06:34:33 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1977, average loss: 0.7161
[09/17 06:34:33 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.77	top5: 100.00	
[09/17 06:34:33 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 06:34:48 visual_prompt]: Epoch 83 / 100: avg data time: 3.13e-01, avg batch time: 0.7145, average train loss: 0.2866
[09/17 06:34:58 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1432, average loss: 0.4937
[09/17 06:34:58 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 80.50	top5: 100.00	
[09/17 06:35:23 visual_prompt]: 	Test 100/1152. loss: 1.129, 0.1837 s / batch. (data: 9.27e-05)max mem: 17.22454 GB 
[09/17 06:35:42 visual_prompt]: 	Test 200/1152. loss: 1.135, 0.1923 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 06:36:02 visual_prompt]: 	Test 300/1152. loss: 1.191, 0.1950 s / batch. (data: 1.04e-02)max mem: 17.22454 GB 
[09/17 06:36:22 visual_prompt]: 	Test 400/1152. loss: 1.206, 0.1839 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 06:36:42 visual_prompt]: 	Test 500/1152. loss: 0.833, 0.2026 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 06:37:02 visual_prompt]: 	Test 600/1152. loss: 0.871, 0.1842 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 06:37:21 visual_prompt]: 	Test 700/1152. loss: 1.097, 0.1844 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 06:37:41 visual_prompt]: 	Test 800/1152. loss: 1.010, 0.1845 s / batch. (data: 3.74e-05)max mem: 17.22454 GB 
[09/17 06:38:01 visual_prompt]: 	Test 900/1152. loss: 1.280, 0.1943 s / batch. (data: 1.02e-02)max mem: 17.22454 GB 
[09/17 06:38:21 visual_prompt]: 	Test 1000/1152. loss: 1.129, 0.1943 s / batch. (data: 1.05e-02)max mem: 17.22454 GB 
[09/17 06:38:41 visual_prompt]: 	Test 1100/1152. loss: 0.801, 0.1992 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 06:38:57 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1970, average loss: 1.1781
[09/17 06:38:57 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 67.26	top5: 99.99	
[09/17 06:38:57 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 06:39:12 visual_prompt]: Epoch 84 / 100: avg data time: 3.04e-01, avg batch time: 0.7097, average train loss: 0.2617
[09/17 06:39:22 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1431, average loss: 0.1936
[09/17 06:39:22 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 94.00	top5: 100.00	
[09/17 06:39:46 visual_prompt]: 	Test 100/1152. loss: 0.801, 0.1976 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 06:40:06 visual_prompt]: 	Test 200/1152. loss: 0.633, 0.1835 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 06:40:25 visual_prompt]: 	Test 300/1152. loss: 0.674, 0.1841 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 06:40:45 visual_prompt]: 	Test 400/1152. loss: 0.846, 0.2074 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 06:41:05 visual_prompt]: 	Test 500/1152. loss: 0.539, 0.1950 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 06:41:27 visual_prompt]: 	Test 600/1152. loss: 0.615, 0.2111 s / batch. (data: 2.79e-02)max mem: 17.22454 GB 
[09/17 06:41:47 visual_prompt]: 	Test 700/1152. loss: 0.796, 0.1899 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 06:42:07 visual_prompt]: 	Test 800/1152. loss: 0.537, 0.1996 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 06:42:34 visual_prompt]: 	Test 900/1152. loss: 0.878, 0.1982 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 06:42:55 visual_prompt]: 	Test 1000/1152. loss: 0.772, 0.1828 s / batch. (data: 6.22e-05)max mem: 17.22454 GB 
[09/17 06:43:19 visual_prompt]: 	Test 1100/1152. loss: 0.431, 0.1827 s / batch. (data: 5.44e-05)max mem: 17.22454 GB 
[09/17 06:43:37 visual_prompt]: Inference (test):avg data time: 6.57e-03, avg batch time: 0.2031, average loss: 0.7964
[09/17 06:43:38 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 73.51	top5: 100.00	
[09/17 06:43:38 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 06:43:53 visual_prompt]: Epoch 85 / 100: avg data time: 2.89e-01, avg batch time: 0.6921, average train loss: 0.1953
[09/17 06:44:02 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1427, average loss: 0.0734
[09/17 06:44:02 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 98.50	top5: 100.00	
[09/17 06:44:26 visual_prompt]: 	Test 100/1152. loss: 0.721, 0.1916 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 06:44:46 visual_prompt]: 	Test 200/1152. loss: 0.503, 0.1965 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 06:45:05 visual_prompt]: 	Test 300/1152. loss: 0.871, 0.1839 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 06:45:25 visual_prompt]: 	Test 400/1152. loss: 0.597, 0.2236 s / batch. (data: 1.90e-02)max mem: 17.22454 GB 
[09/17 06:45:44 visual_prompt]: 	Test 500/1152. loss: 0.621, 0.2059 s / batch. (data: 2.05e-02)max mem: 17.22454 GB 
[09/17 06:46:04 visual_prompt]: 	Test 600/1152. loss: 0.605, 0.1992 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 06:46:24 visual_prompt]: 	Test 700/1152. loss: 0.711, 0.1893 s / batch. (data: 5.24e-03)max mem: 17.22454 GB 
visual_prompt_tuning/experiments/vit_vtab.sh: line 9: 234580 Killed                  python visual_prompt_tuning/train.py --config-file visual_prompt_tuning/configs/prompt/cub.yaml MODEL.TYPE "vit" DATA.BATCH_SIZE "64" MODEL.PROMPT.NUM_TOKENS "100" MODEL.PROMPT.DEEP "True" MODEL.PROMPT.DROPOUT "0.1" DATA.FEATURE "sup_vitb16_imagenet21k" DATA.NAME "vtab-${dataset}" DATA.NUMBER_CLASSES "${num_classes}" SOLVER.BASE_LR "5.0" SOLVER.WEIGHT_DECAY "0.0001" MODEL.MODEL_ROOT "${model_root}" DATA.DATAPATH "${data_path}" OUTPUT_DIR "${output_dir}/seed${seed}"
[09/17 06:47:06 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 06:47:06 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 06:47:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)', 'DATA.NUMBER_CLASSES', '16', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed44'], train_type='')
[09/17 06:47:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 06:47:06 visual_prompt]: Training with config:
[09/17 06:47:06 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)',
          'NO_TEST': False,
          'NUMBER_CLASSES': 16,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed44/vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 06:47:06 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 06:47:06.764835: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 06:47:06.950558: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 06:47:07.942236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:47:07.942316: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:47:07.942325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 06:47:10.092361: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:47:10.092467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:47:10.092480: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 06:47:10 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
2023-09-17 06:47:10.109210: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[:800]+train[589824:590024], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 06:47:11 visual_prompt]: Number of images: 1000
[09/17 06:47:11 visual_prompt]: Number of classes: 16 / 16
[09/17 06:47:11 visual_prompt]: Loading validation data...
[09/17 06:47:11 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[589824:590024], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 06:47:12 visual_prompt]: Number of images: 200
[09/17 06:47:12 visual_prompt]: Number of classes: 16 / 16
[09/17 06:47:12 visual_prompt]: Loading test data...
[09/17 06:47:12 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[663552:], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 06:49:10 visual_prompt]: Number of images: 73728
[09/17 06:49:10 visual_prompt]: Number of classes: 16 / 16
[09/17 06:49:10 visual_prompt]: Constructing models...
[09/17 06:49:14 visual_prompt]: Total Parameters: 86732560	 Gradient Parameters: 933904
[09/17 06:49:14 visual_prompt]: tuned percent:1.077
[09/17 06:49:17 visual_prompt]: Device used for model: 0
[09/17 06:49:17 visual_prompt]: Setting up Evalutator...
[09/17 06:49:17 visual_prompt]: Setting up Trainer...
[09/17 06:49:17 visual_prompt]: 	Setting up the optimizer...
[09/17 06:49:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 06:49:34 visual_prompt]: Epoch 1 / 100: avg data time: 3.79e-01, avg batch time: 0.8519, average train loss: 2.8731
[09/17 06:49:45 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1421, average loss: 2.8551
[09/17 06:49:45 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 31.50	
[09/17 06:50:10 visual_prompt]: 	Test 100/1152. loss: 2.910, 0.1881 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 06:50:29 visual_prompt]: 	Test 200/1152. loss: 2.927, 0.1962 s / batch. (data: 1.34e-02)max mem: 17.22454 GB 
[09/17 06:50:49 visual_prompt]: 	Test 300/1152. loss: 2.765, 0.1832 s / batch. (data: 6.01e-05)max mem: 17.22454 GB 
[09/17 06:51:09 visual_prompt]: 	Test 400/1152. loss: 2.818, 0.2052 s / batch. (data: 2.25e-02)max mem: 17.22454 GB 
[09/17 06:51:50 visual_prompt]: 	Test 500/1152. loss: 2.917, 0.2411 s / batch. (data: 3.54e-02)max mem: 17.22454 GB 
[09/17 06:52:10 visual_prompt]: 	Test 600/1152. loss: 2.834, 0.1973 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 06:52:29 visual_prompt]: 	Test 700/1152. loss: 2.826, 0.1833 s / batch. (data: 1.68e-04)max mem: 17.22454 GB 
[09/17 06:52:49 visual_prompt]: 	Test 800/1152. loss: 2.842, 0.1953 s / batch. (data: 1.17e-02)max mem: 17.22454 GB 
[09/17 06:53:08 visual_prompt]: 	Test 900/1152. loss: 2.925, 0.1837 s / batch. (data: 3.84e-05)max mem: 17.22454 GB 
[09/17 06:53:28 visual_prompt]: 	Test 1000/1152. loss: 2.859, 0.1833 s / batch. (data: 1.12e-04)max mem: 17.22454 GB 
visual_prompt_tuning/experiments/vit_vtab.sh: line 9: 326625 Killed                  python visual_prompt_tuning/train.py --config-file visual_prompt_tuning/configs/prompt/cub.yaml MODEL.TYPE "vit" DATA.BATCH_SIZE "64" MODEL.PROMPT.NUM_TOKENS "100" MODEL.PROMPT.DEEP "True" MODEL.PROMPT.DROPOUT "0.1" DATA.FEATURE "sup_vitb16_imagenet21k" DATA.NAME "vtab-${dataset}" DATA.NUMBER_CLASSES "${num_classes}" SOLVER.BASE_LR "5.0" SOLVER.WEIGHT_DECAY "0.0001" MODEL.MODEL_ROOT "${model_root}" DATA.DATAPATH "${data_path}" OUTPUT_DIR "${output_dir}/seed${seed}"
[09/17 06:54:04 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 06:54:05 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 06:54:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)', 'DATA.NUMBER_CLASSES', '16', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed82'], train_type='')
[09/17 06:54:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 06:54:05 visual_prompt]: Training with config:
[09/17 06:54:05 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)',
          'NO_TEST': False,
          'NUMBER_CLASSES': 16,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed82/vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 06:54:05 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 06:54:05.087576: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 06:54:05.478088: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 06:54:07.040547: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:54:07.040852: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:54:07.040890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 06:54:10.399898: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:54:10.400130: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:54:10.400146: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 06:54:10 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
2023-09-17 06:54:10.423487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[:800]+train[589824:590024], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 06:54:12 visual_prompt]: Number of images: 1000
[09/17 06:54:12 visual_prompt]: Number of classes: 16 / 16
[09/17 06:54:12 visual_prompt]: Loading validation data...
[09/17 06:54:12 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[589824:590024], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 06:54:12 visual_prompt]: Number of images: 200
[09/17 06:54:12 visual_prompt]: Number of classes: 16 / 16
[09/17 06:54:12 visual_prompt]: Loading test data...
[09/17 06:54:12 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[663552:], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 06:55:46 visual_prompt]: Number of images: 73728
[09/17 06:55:46 visual_prompt]: Number of classes: 16 / 16
[09/17 06:55:46 visual_prompt]: Constructing models...
[09/17 06:55:49 visual_prompt]: Total Parameters: 86732560	 Gradient Parameters: 933904
[09/17 06:55:49 visual_prompt]: tuned percent:1.077
[09/17 06:55:52 visual_prompt]: Device used for model: 0
[09/17 06:55:52 visual_prompt]: Setting up Evalutator...
[09/17 06:55:52 visual_prompt]: Setting up Trainer...
[09/17 06:55:52 visual_prompt]: 	Setting up the optimizer...
[09/17 06:55:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 06:56:07 visual_prompt]: Epoch 1 / 100: avg data time: 2.65e-01, avg batch time: 0.7471, average train loss: 3.2735
[09/17 06:56:14 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1421, average loss: 3.2070
[09/17 06:56:14 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 5.50	top5: 33.50	
[09/17 06:56:38 visual_prompt]: 	Test 100/1152. loss: 3.326, 0.2306 s / batch. (data: 4.87e-02)max mem: 17.22454 GB 
[09/17 06:57:07 visual_prompt]: 	Test 200/1152. loss: 3.289, 0.1904 s / batch. (data: 7.52e-03)max mem: 17.22454 GB 
[09/17 06:57:26 visual_prompt]: 	Test 300/1152. loss: 3.079, 0.1974 s / batch. (data: 1.36e-02)max mem: 17.22454 GB 
[09/17 06:57:46 visual_prompt]: 	Test 400/1152. loss: 3.191, 0.1851 s / batch. (data: 8.30e-05)max mem: 17.22454 GB 
[09/17 06:58:05 visual_prompt]: 	Test 500/1152. loss: 3.410, 0.1992 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 06:58:25 visual_prompt]: 	Test 600/1152. loss: 3.159, 0.1984 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 06:58:44 visual_prompt]: 	Test 700/1152. loss: 3.312, 0.1992 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 06:59:04 visual_prompt]: 	Test 800/1152. loss: 3.233, 0.1962 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 06:59:23 visual_prompt]: 	Test 900/1152. loss: 3.366, 0.1979 s / batch. (data: 1.37e-02)max mem: 17.22454 GB 
visual_prompt_tuning/experiments/vit_vtab.sh: line 9: 328320 Killed                  python visual_prompt_tuning/train.py --config-file visual_prompt_tuning/configs/prompt/cub.yaml MODEL.TYPE "vit" DATA.BATCH_SIZE "64" MODEL.PROMPT.NUM_TOKENS "100" MODEL.PROMPT.DEEP "True" MODEL.PROMPT.DROPOUT "0.1" DATA.FEATURE "sup_vitb16_imagenet21k" DATA.NAME "vtab-${dataset}" DATA.NUMBER_CLASSES "${num_classes}" SOLVER.BASE_LR "5.0" SOLVER.WEIGHT_DECAY "0.0001" MODEL.MODEL_ROOT "${model_root}" DATA.DATAPATH "${data_path}" OUTPUT_DIR "${output_dir}/seed${seed}"
[09/17 06:59:53 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 06:59:53 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 06:59:53 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)', 'DATA.NUMBER_CLASSES', '16', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed100'], train_type='')
[09/17 06:59:53 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 06:59:53 visual_prompt]: Training with config:
[09/17 06:59:53 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)',
          'NO_TEST': False,
          'NUMBER_CLASSES': 16,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed100/vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 06:59:53 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 06:59:53.941423: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 06:59:54.357162: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 06:59:55.957040: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:59:55.957240: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:59:55.957253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 06:59:59.191074: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:59:59.191294: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:59:59.191310: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 06:59:59 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
2023-09-17 06:59:59.212864: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[:800]+train[589824:590024], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 07:00:01 visual_prompt]: Number of images: 1000
[09/17 07:00:01 visual_prompt]: Number of classes: 16 / 16
[09/17 07:00:01 visual_prompt]: Loading validation data...
[09/17 07:00:01 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[589824:590024], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 07:00:01 visual_prompt]: Number of images: 200
[09/17 07:00:01 visual_prompt]: Number of classes: 16 / 16
[09/17 07:00:01 visual_prompt]: Loading test data...
[09/17 07:00:01 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[663552:], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 07:01:35 visual_prompt]: Number of images: 73728
[09/17 07:01:35 visual_prompt]: Number of classes: 16 / 16
[09/17 07:01:36 visual_prompt]: Constructing models...
[09/17 07:01:39 visual_prompt]: Total Parameters: 86732560	 Gradient Parameters: 933904
[09/17 07:01:39 visual_prompt]: tuned percent:1.077
[09/17 07:01:43 visual_prompt]: Device used for model: 0
[09/17 07:01:43 visual_prompt]: Setting up Evalutator...
[09/17 07:01:43 visual_prompt]: Setting up Trainer...
[09/17 07:01:43 visual_prompt]: 	Setting up the optimizer...
[09/17 07:01:43 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 07:01:58 visual_prompt]: Epoch 1 / 100: avg data time: 2.47e-01, avg batch time: 0.7444, average train loss: 3.0045
[09/17 07:02:05 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1432, average loss: 3.0049
[09/17 07:02:05 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.00	top5: 30.50	
visual_prompt_tuning/experiments/vit_vtab.sh: line 9: 329768 Killed                  python visual_prompt_tuning/train.py --config-file visual_prompt_tuning/configs/prompt/cub.yaml MODEL.TYPE "vit" DATA.BATCH_SIZE "64" MODEL.PROMPT.NUM_TOKENS "100" MODEL.PROMPT.DEEP "True" MODEL.PROMPT.DROPOUT "0.1" DATA.FEATURE "sup_vitb16_imagenet21k" DATA.NAME "vtab-${dataset}" DATA.NUMBER_CLASSES "${num_classes}" SOLVER.BASE_LR "5.0" SOLVER.WEIGHT_DECAY "0.0001" MODEL.MODEL_ROOT "${model_root}" DATA.DATAPATH "${data_path}" OUTPUT_DIR "${output_dir}/seed${seed}"
[09/17 07:02:36 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 07:02:37 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 07:02:37 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)', 'DATA.NUMBER_CLASSES', '16', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed800'], train_type='')
[09/17 07:02:37 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 07:02:37 visual_prompt]: Training with config:
[09/17 07:02:37 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)',
          'NO_TEST': False,
          'NUMBER_CLASSES': 16,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed800/vtab-dsprites(predicted_attribute="label_x_position",num_classes=16)/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 07:02:37 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 07:02:37.063832: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 07:02:37.461612: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 07:02:39.117792: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 07:02:39.118059: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 07:02:39.118078: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 07:02:42.639880: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 07:02:42.640108: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 07:02:42.640124: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 07:02:42 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
2023-09-17 07:02:42.663795: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[:800]+train[589824:590024], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 07:02:44 visual_prompt]: Number of images: 1000
[09/17 07:02:44 visual_prompt]: Number of classes: 16 / 16
[09/17 07:02:44 visual_prompt]: Loading validation data...
[09/17 07:02:44 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[589824:590024], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 07:02:44 visual_prompt]: Number of images: 200
[09/17 07:02:44 visual_prompt]: Number of classes: 16 / 16
[09/17 07:02:44 visual_prompt]: Loading test data...
[09/17 07:02:44 visual_prompt]: Constructing vtab-dsprites(predicted_attribute="label_x_position",num_classes=16) dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dsprites/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset dsprites (visual_prompt_tuning/data_path/dsprites/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dsprites for split train[663552:], from visual_prompt_tuning/data_path/dsprites/2.0.0
[WARNING: feature.py:   71]: `FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.
[09/17 07:04:19 visual_prompt]: Number of images: 73728
[09/17 07:04:19 visual_prompt]: Number of classes: 16 / 16
[09/17 07:04:20 visual_prompt]: Constructing models...
[09/17 07:04:23 visual_prompt]: Total Parameters: 86732560	 Gradient Parameters: 933904
[09/17 07:04:23 visual_prompt]: tuned percent:1.077
[09/17 07:04:27 visual_prompt]: Device used for model: 0
[09/17 07:04:27 visual_prompt]: Setting up Evalutator...
[09/17 07:04:27 visual_prompt]: Setting up Trainer...
[09/17 07:04:27 visual_prompt]: 	Setting up the optimizer...
[09/17 07:04:27 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 07:04:43 visual_prompt]: Epoch 1 / 100: avg data time: 2.85e-01, avg batch time: 0.7804, average train loss: 2.9999
[09/17 07:04:52 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1424, average loss: 3.0368
[09/17 07:04:52 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.50	top5: 28.50	
[09/17 07:05:16 visual_prompt]: 	Test 100/1152. loss: 2.909, 0.1833 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 07:05:35 visual_prompt]: 	Test 200/1152. loss: 3.107, 0.1968 s / batch. (data: 1.43e-02)max mem: 17.22454 GB 
[09/17 07:06:03 visual_prompt]: 	Test 300/1152. loss: 2.877, 0.2247 s / batch. (data: 2.90e-02)max mem: 17.22454 GB 
[09/17 07:06:23 visual_prompt]: 	Test 400/1152. loss: 3.010, 0.1838 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 07:06:42 visual_prompt]: 	Test 500/1152. loss: 3.186, 0.1834 s / batch. (data: 3.91e-05)max mem: 17.22454 GB 
[09/17 07:07:02 visual_prompt]: 	Test 600/1152. loss: 3.097, 0.1847 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 07:07:21 visual_prompt]: 	Test 700/1152. loss: 2.959, 0.1836 s / batch. (data: 9.99e-05)max mem: 17.22454 GB 
[09/17 07:07:41 visual_prompt]: 	Test 800/1152. loss: 2.936, 0.1846 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 07:08:00 visual_prompt]: 	Test 900/1152. loss: 2.878, 0.1883 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 07:08:20 visual_prompt]: 	Test 1000/1152. loss: 2.991, 0.1973 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 07:08:40 visual_prompt]: 	Test 1100/1152. loss: 2.913, 0.1991 s / batch. (data: 1.73e-04)max mem: 17.22454 GB 
[09/17 07:08:55 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.2026, average loss: 2.9978
[09/17 07:08:55 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.22	top5: 31.60	
[09/17 07:08:55 visual_prompt]: Best epoch 1: best metric: 0.075
[09/17 07:08:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/17 07:09:09 visual_prompt]: Epoch 2 / 100: avg data time: 2.55e-01, avg batch time: 0.6596, average train loss: 3.3729
[09/17 07:09:17 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1432, average loss: 2.9470
[09/17 07:09:17 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.50	top5: 37.50	
[09/17 07:09:41 visual_prompt]: 	Test 100/1152. loss: 3.011, 0.1964 s / batch. (data: 1.36e-02)max mem: 17.22454 GB 
[09/17 07:10:00 visual_prompt]: 	Test 200/1152. loss: 3.118, 0.2122 s / batch. (data: 3.65e-05)max mem: 17.22454 GB 
[09/17 07:10:20 visual_prompt]: 	Test 300/1152. loss: 3.031, 0.1841 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 07:10:39 visual_prompt]: 	Test 400/1152. loss: 3.026, 0.1843 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 07:10:59 visual_prompt]: 	Test 500/1152. loss: 3.107, 0.1854 s / batch. (data: 1.08e-04)max mem: 17.22454 GB 
[09/17 07:11:19 visual_prompt]: 	Test 600/1152. loss: 2.987, 0.1844 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 07:11:39 visual_prompt]: 	Test 700/1152. loss: 2.990, 0.1960 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 07:11:59 visual_prompt]: 	Test 800/1152. loss: 2.938, 0.1987 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 07:12:19 visual_prompt]: 	Test 900/1152. loss: 2.990, 0.1843 s / batch. (data: 1.69e-04)max mem: 17.22454 GB 
[09/17 07:12:39 visual_prompt]: 	Test 1000/1152. loss: 2.959, 0.1833 s / batch. (data: 1.12e-04)max mem: 17.22454 GB 
[09/17 07:12:58 visual_prompt]: 	Test 1100/1152. loss: 3.060, 0.1917 s / batch. (data: 1.64e-04)max mem: 17.22454 GB 
[09/17 07:13:13 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1966, average loss: 3.0340
[09/17 07:13:13 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.22	top5: 31.16	
[09/17 07:13:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/17 07:13:27 visual_prompt]: Epoch 3 / 100: avg data time: 2.62e-01, avg batch time: 0.6651, average train loss: 2.9912
[09/17 07:13:35 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1440, average loss: 2.9628
[09/17 07:13:35 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 34.00	
[09/17 07:13:59 visual_prompt]: 	Test 100/1152. loss: 2.794, 0.2131 s / batch. (data: 3.04e-02)max mem: 17.22454 GB 
[09/17 07:14:19 visual_prompt]: 	Test 200/1152. loss: 3.090, 0.1979 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 07:14:38 visual_prompt]: 	Test 300/1152. loss: 2.964, 0.1966 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 07:14:58 visual_prompt]: 	Test 400/1152. loss: 3.044, 0.1843 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 07:15:18 visual_prompt]: 	Test 500/1152. loss: 3.136, 0.1844 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 07:15:37 visual_prompt]: 	Test 600/1152. loss: 3.134, 0.2099 s / batch. (data: 2.31e-02)max mem: 17.22454 GB 
[09/17 07:15:57 visual_prompt]: 	Test 700/1152. loss: 2.954, 0.1845 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 07:16:17 visual_prompt]: 	Test 800/1152. loss: 2.958, 0.1893 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 07:16:37 visual_prompt]: 	Test 900/1152. loss: 3.096, 0.1995 s / batch. (data: 1.08e-04)max mem: 17.22454 GB 
[09/17 07:16:56 visual_prompt]: 	Test 1000/1152. loss: 3.123, 0.1925 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 07:17:16 visual_prompt]: 	Test 1100/1152. loss: 2.969, 0.1847 s / batch. (data: 1.81e-04)max mem: 17.22454 GB 
[09/17 07:17:31 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1959, average loss: 3.0221
[09/17 07:17:31 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 31.32	
[09/17 07:17:31 visual_prompt]: Best epoch 3: best metric: 0.080
[09/17 07:17:31 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/17 07:17:46 visual_prompt]: Epoch 4 / 100: avg data time: 2.61e-01, avg batch time: 0.6637, average train loss: 3.1887
[09/17 07:17:54 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.1470, average loss: 2.9463
[09/17 07:17:54 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 28.50	
[09/17 07:18:18 visual_prompt]: 	Test 100/1152. loss: 2.880, 0.1903 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 07:18:37 visual_prompt]: 	Test 200/1152. loss: 2.933, 0.1896 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 07:18:57 visual_prompt]: 	Test 300/1152. loss: 3.004, 0.1841 s / batch. (data: 1.69e-04)max mem: 17.22454 GB 
[09/17 07:19:17 visual_prompt]: 	Test 400/1152. loss: 2.984, 0.1971 s / batch. (data: 1.36e-02)max mem: 17.22454 GB 
[09/17 07:19:36 visual_prompt]: 	Test 500/1152. loss: 3.112, 0.1841 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 07:19:56 visual_prompt]: 	Test 600/1152. loss: 3.065, 0.1879 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 07:20:16 visual_prompt]: 	Test 700/1152. loss: 3.020, 0.1842 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 07:20:36 visual_prompt]: 	Test 800/1152. loss: 3.017, 0.1848 s / batch. (data: 1.15e-04)max mem: 17.22454 GB 
[09/17 07:20:56 visual_prompt]: 	Test 900/1152. loss: 2.869, 0.1838 s / batch. (data: 1.07e-04)max mem: 17.22454 GB 
[09/17 07:21:15 visual_prompt]: 	Test 1000/1152. loss: 2.893, 0.2121 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 07:21:35 visual_prompt]: 	Test 1100/1152. loss: 2.980, 0.2009 s / batch. (data: 1.39e-02)max mem: 17.22454 GB 
[09/17 07:21:50 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1964, average loss: 2.9715
[09/17 07:21:50 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 31.05	
[09/17 07:21:50 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/17 07:22:05 visual_prompt]: Epoch 5 / 100: avg data time: 2.69e-01, avg batch time: 0.6716, average train loss: 3.1789
[09/17 07:22:13 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1430, average loss: 3.2273
[09/17 07:22:13 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 33.00	
[09/17 07:22:36 visual_prompt]: 	Test 100/1152. loss: 3.155, 0.1959 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 07:22:56 visual_prompt]: 	Test 200/1152. loss: 3.281, 0.2033 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 07:23:15 visual_prompt]: 	Test 300/1152. loss: 3.117, 0.1844 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 07:23:35 visual_prompt]: 	Test 400/1152. loss: 3.165, 0.1929 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 07:23:55 visual_prompt]: 	Test 500/1152. loss: 3.372, 0.1993 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 07:24:14 visual_prompt]: 	Test 600/1152. loss: 3.373, 0.1981 s / batch. (data: 3.12e-05)max mem: 17.22454 GB 
[09/17 07:24:34 visual_prompt]: 	Test 700/1152. loss: 3.268, 0.1969 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 07:24:54 visual_prompt]: 	Test 800/1152. loss: 3.222, 0.1913 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 07:25:13 visual_prompt]: 	Test 900/1152. loss: 3.467, 0.2529 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 07:25:33 visual_prompt]: 	Test 1000/1152. loss: 3.480, 0.1958 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 07:25:53 visual_prompt]: 	Test 1100/1152. loss: 3.374, 0.1975 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 07:26:08 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1955, average loss: 3.2964
[09/17 07:26:08 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 31.24	
[09/17 07:26:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/17 07:26:23 visual_prompt]: Epoch 6 / 100: avg data time: 2.79e-01, avg batch time: 0.6847, average train loss: 3.3169
[09/17 07:26:31 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1431, average loss: 2.9446
[09/17 07:26:31 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 37.50	
[09/17 07:26:54 visual_prompt]: 	Test 100/1152. loss: 3.119, 0.1829 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 07:27:14 visual_prompt]: 	Test 200/1152. loss: 3.149, 0.2233 s / batch. (data: 4.01e-02)max mem: 17.22454 GB 
[09/17 07:27:33 visual_prompt]: 	Test 300/1152. loss: 3.108, 0.1840 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 07:27:53 visual_prompt]: 	Test 400/1152. loss: 3.144, 0.1999 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 07:28:13 visual_prompt]: 	Test 500/1152. loss: 3.207, 0.2083 s / batch. (data: 1.92e-02)max mem: 17.22454 GB 
[09/17 07:28:33 visual_prompt]: 	Test 600/1152. loss: 3.082, 0.2240 s / batch. (data: 1.43e-02)max mem: 17.22454 GB 
[09/17 07:28:52 visual_prompt]: 	Test 700/1152. loss: 3.044, 0.2096 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 07:29:12 visual_prompt]: 	Test 800/1152. loss: 3.148, 0.2149 s / batch. (data: 4.17e-05)max mem: 17.22454 GB 
[09/17 07:29:32 visual_prompt]: 	Test 900/1152. loss: 3.102, 0.2288 s / batch. (data: 1.95e-02)max mem: 17.22454 GB 
[09/17 07:29:52 visual_prompt]: 	Test 1000/1152. loss: 2.967, 0.1851 s / batch. (data: 1.71e-04)max mem: 17.22454 GB 
[09/17 07:30:12 visual_prompt]: 	Test 1100/1152. loss: 3.078, 0.2036 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 07:30:27 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1966, average loss: 3.0795
[09/17 07:30:27 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 31.15	
[09/17 07:30:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/17 07:30:42 visual_prompt]: Epoch 7 / 100: avg data time: 2.73e-01, avg batch time: 0.6756, average train loss: 3.2355
[09/17 07:30:50 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1430, average loss: 3.2099
[09/17 07:30:50 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.00	top5: 24.00	
[09/17 07:31:13 visual_prompt]: 	Test 100/1152. loss: 3.190, 0.1899 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 07:31:33 visual_prompt]: 	Test 200/1152. loss: 3.195, 0.1838 s / batch. (data: 1.09e-04)max mem: 17.22454 GB 
[09/17 07:31:52 visual_prompt]: 	Test 300/1152. loss: 3.079, 0.1966 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 07:32:12 visual_prompt]: 	Test 400/1152. loss: 3.057, 0.2020 s / batch. (data: 1.66e-02)max mem: 17.22454 GB 
[09/17 07:32:32 visual_prompt]: 	Test 500/1152. loss: 3.198, 0.1845 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 07:32:52 visual_prompt]: 	Test 600/1152. loss: 3.251, 0.1849 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 07:33:12 visual_prompt]: 	Test 700/1152. loss: 2.990, 0.2074 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 07:33:32 visual_prompt]: 	Test 800/1152. loss: 2.956, 0.1863 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 07:33:51 visual_prompt]: 	Test 900/1152. loss: 3.086, 0.1991 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 07:34:11 visual_prompt]: 	Test 1000/1152. loss: 3.002, 0.2081 s / batch. (data: 1.57e-02)max mem: 17.22454 GB 
[09/17 07:34:31 visual_prompt]: 	Test 1100/1152. loss: 2.987, 0.2000 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 07:34:46 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1964, average loss: 3.1158
[09/17 07:34:46 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.34	top5: 31.21	
[09/17 07:34:46 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/17 07:35:00 visual_prompt]: Epoch 8 / 100: avg data time: 2.63e-01, avg batch time: 0.6676, average train loss: 3.1348
[09/17 07:35:08 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1431, average loss: 3.0116
[09/17 07:35:08 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.00	top5: 29.50	
[09/17 07:35:32 visual_prompt]: 	Test 100/1152. loss: 2.843, 0.1836 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 07:35:52 visual_prompt]: 	Test 200/1152. loss: 3.118, 0.1960 s / batch. (data: 1.31e-02)max mem: 17.22454 GB 
[09/17 07:36:11 visual_prompt]: 	Test 300/1152. loss: 3.127, 0.1994 s / batch. (data: 1.59e-02)max mem: 17.22454 GB 
[09/17 07:36:31 visual_prompt]: 	Test 400/1152. loss: 2.979, 0.2203 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 07:36:51 visual_prompt]: 	Test 500/1152. loss: 2.838, 0.1975 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 07:37:10 visual_prompt]: 	Test 600/1152. loss: 3.085, 0.2105 s / batch. (data: 2.67e-02)max mem: 17.22454 GB 
[09/17 07:37:30 visual_prompt]: 	Test 700/1152. loss: 2.885, 0.1903 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 07:37:50 visual_prompt]: 	Test 800/1152. loss: 2.982, 0.1858 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 07:38:10 visual_prompt]: 	Test 900/1152. loss: 3.003, 0.1916 s / batch. (data: 3.27e-05)max mem: 17.22454 GB 
[09/17 07:38:30 visual_prompt]: 	Test 1000/1152. loss: 2.911, 0.2055 s / batch. (data: 2.18e-02)max mem: 17.22454 GB 
[09/17 07:38:50 visual_prompt]: 	Test 1100/1152. loss: 2.962, 0.2268 s / batch. (data: 4.36e-02)max mem: 17.22454 GB 
[09/17 07:39:05 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1965, average loss: 3.0105
[09/17 07:39:05 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 31.16	
[09/17 07:39:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/17 07:39:19 visual_prompt]: Epoch 9 / 100: avg data time: 2.71e-01, avg batch time: 0.6741, average train loss: 3.3593
[09/17 07:39:27 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1432, average loss: 3.6372
[09/17 07:39:27 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 36.00	
[09/17 07:39:52 visual_prompt]: 	Test 100/1152. loss: 3.460, 0.2882 s / batch. (data: 7.09e-04)max mem: 17.22454 GB 
[09/17 07:40:11 visual_prompt]: 	Test 200/1152. loss: 3.946, 0.1850 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 07:40:31 visual_prompt]: 	Test 300/1152. loss: 3.915, 0.2041 s / batch. (data: 9.18e-05)max mem: 17.22454 GB 
[09/17 07:40:50 visual_prompt]: 	Test 400/1152. loss: 3.686, 0.1979 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 07:41:10 visual_prompt]: 	Test 500/1152. loss: 3.141, 0.1992 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 07:41:30 visual_prompt]: 	Test 600/1152. loss: 3.693, 0.2027 s / batch. (data: 1.92e-02)max mem: 17.22454 GB 
[09/17 07:41:49 visual_prompt]: 	Test 700/1152. loss: 3.613, 0.1878 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 07:42:09 visual_prompt]: 	Test 800/1152. loss: 3.765, 0.1992 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 07:42:29 visual_prompt]: 	Test 900/1152. loss: 3.814, 0.2000 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 07:42:49 visual_prompt]: 	Test 1000/1152. loss: 3.665, 0.2099 s / batch. (data: 2.60e-02)max mem: 17.22454 GB 
[09/17 07:43:09 visual_prompt]: 	Test 1100/1152. loss: 3.653, 0.1958 s / batch. (data: 1.21e-02)max mem: 17.22454 GB 
[09/17 07:43:24 visual_prompt]: Inference (test):avg data time: 8.06e-03, avg batch time: 0.1965, average loss: 3.6840
[09/17 07:43:24 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.24	top5: 31.03	
[09/17 07:43:24 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/17 07:43:38 visual_prompt]: Epoch 10 / 100: avg data time: 2.65e-01, avg batch time: 0.6687, average train loss: 3.5187
[09/17 07:43:47 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1432, average loss: 3.0476
[09/17 07:43:47 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 5.50	top5: 37.00	
[09/17 07:44:11 visual_prompt]: 	Test 100/1152. loss: 3.026, 0.1839 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 07:44:30 visual_prompt]: 	Test 200/1152. loss: 3.166, 0.2038 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 07:44:50 visual_prompt]: 	Test 300/1152. loss: 3.108, 0.1856 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 07:45:10 visual_prompt]: 	Test 400/1152. loss: 3.215, 0.2052 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 07:45:30 visual_prompt]: 	Test 500/1152. loss: 3.300, 0.1993 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 07:45:50 visual_prompt]: 	Test 600/1152. loss: 3.252, 0.2031 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 07:46:10 visual_prompt]: 	Test 700/1152. loss: 3.186, 0.1999 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 07:46:30 visual_prompt]: 	Test 800/1152. loss: 3.193, 0.1993 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 07:46:49 visual_prompt]: 	Test 900/1152. loss: 3.149, 0.1966 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 07:47:10 visual_prompt]: 	Test 1000/1152. loss: 3.152, 0.2061 s / batch. (data: 2.24e-02)max mem: 17.22454 GB 
[09/17 07:47:29 visual_prompt]: 	Test 1100/1152. loss: 3.142, 0.1902 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 07:47:44 visual_prompt]: Inference (test):avg data time: 8.31e-03, avg batch time: 0.1975, average loss: 3.1245
[09/17 07:47:45 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 36.51	
[09/17 07:47:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/17 07:47:59 visual_prompt]: Epoch 11 / 100: avg data time: 2.63e-01, avg batch time: 0.6665, average train loss: 3.3207
[09/17 07:48:07 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1432, average loss: 4.6471
[09/17 07:48:07 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.00	top5: 35.00	
[09/17 07:48:31 visual_prompt]: 	Test 100/1152. loss: 5.212, 0.1829 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 07:48:50 visual_prompt]: 	Test 200/1152. loss: 4.872, 0.1842 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 07:49:10 visual_prompt]: 	Test 300/1152. loss: 4.810, 0.2154 s / batch. (data: 3.20e-02)max mem: 17.22454 GB 
[09/17 07:49:29 visual_prompt]: 	Test 400/1152. loss: 4.807, 0.2309 s / batch. (data: 8.58e-05)max mem: 17.22454 GB 
[09/17 07:49:49 visual_prompt]: 	Test 500/1152. loss: 5.057, 0.1997 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 07:50:10 visual_prompt]: 	Test 600/1152. loss: 4.425, 0.1846 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 07:50:29 visual_prompt]: 	Test 700/1152. loss: 5.039, 0.1945 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 07:50:49 visual_prompt]: 	Test 800/1152. loss: 4.779, 0.1836 s / batch. (data: 3.46e-05)max mem: 17.22454 GB 
[09/17 07:51:09 visual_prompt]: 	Test 900/1152. loss: 4.533, 0.1952 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 07:51:28 visual_prompt]: 	Test 1000/1152. loss: 4.341, 0.2030 s / batch. (data: 1.13e-02)max mem: 17.22454 GB 
[09/17 07:51:48 visual_prompt]: 	Test 1100/1152. loss: 4.581, 0.1848 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 07:52:03 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1963, average loss: 4.7383
[09/17 07:52:03 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.32	top5: 31.21	
[09/17 07:52:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/17 07:52:18 visual_prompt]: Epoch 12 / 100: avg data time: 2.62e-01, avg batch time: 0.6682, average train loss: 5.6569
[09/17 07:52:26 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1432, average loss: 10.2176
[09/17 07:52:26 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.00	top5: 26.00	
[09/17 07:52:50 visual_prompt]: 	Test 100/1152. loss: 10.105, 0.2008 s / batch. (data: 1.88e-04)max mem: 17.22454 GB 
[09/17 07:53:10 visual_prompt]: 	Test 200/1152. loss: 11.194, 0.2071 s / batch. (data: 1.04e-02)max mem: 17.22454 GB 
[09/17 07:53:30 visual_prompt]: 	Test 300/1152. loss: 9.468, 0.1850 s / batch. (data: 1.70e-04)max mem: 17.22454 GB 
[09/17 07:53:49 visual_prompt]: 	Test 400/1152. loss: 7.911, 0.2301 s / batch. (data: 4.71e-02)max mem: 17.22454 GB 
[09/17 07:54:09 visual_prompt]: 	Test 500/1152. loss: 8.363, 0.1947 s / batch. (data: 1.16e-02)max mem: 17.22454 GB 
[09/17 07:54:29 visual_prompt]: 	Test 600/1152. loss: 8.773, 0.1955 s / batch. (data: 1.08e-02)max mem: 17.22454 GB 
[09/17 07:54:49 visual_prompt]: 	Test 700/1152. loss: 9.596, 0.1913 s / batch. (data: 7.28e-03)max mem: 17.22454 GB 
[09/17 07:55:08 visual_prompt]: 	Test 800/1152. loss: 10.537, 0.1902 s / batch. (data: 9.30e-05)max mem: 17.22454 GB 
[09/17 07:55:28 visual_prompt]: 	Test 900/1152. loss: 9.702, 0.2167 s / batch. (data: 2.40e-02)max mem: 17.22454 GB 
[09/17 07:55:48 visual_prompt]: 	Test 1000/1152. loss: 9.482, 0.2319 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 07:56:08 visual_prompt]: 	Test 1100/1152. loss: 11.047, 0.2049 s / batch. (data: 1.94e-02)max mem: 17.22454 GB 
[09/17 07:56:23 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1970, average loss: 9.7992
[09/17 07:56:23 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.34	top5: 31.14	
[09/17 07:56:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/17 07:56:38 visual_prompt]: Epoch 13 / 100: avg data time: 2.57e-01, avg batch time: 0.6647, average train loss: 9.3362
[09/17 07:56:46 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1429, average loss: 10.4509
[09/17 07:56:46 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.00	top5: 32.00	
[09/17 07:57:10 visual_prompt]: 	Test 100/1152. loss: 8.918, 0.1830 s / batch. (data: 3.43e-05)max mem: 17.22454 GB 
[09/17 07:57:29 visual_prompt]: 	Test 200/1152. loss: 10.232, 0.2203 s / batch. (data: 3.75e-02)max mem: 17.22454 GB 
[09/17 07:57:49 visual_prompt]: 	Test 300/1152. loss: 9.337, 0.2070 s / batch. (data: 9.20e-05)max mem: 17.22454 GB 
[09/17 07:58:08 visual_prompt]: 	Test 400/1152. loss: 8.944, 0.2253 s / batch. (data: 3.66e-02)max mem: 17.22454 GB 
[09/17 07:58:28 visual_prompt]: 	Test 500/1152. loss: 9.107, 0.1842 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 07:58:48 visual_prompt]: 	Test 600/1152. loss: 9.700, 0.1913 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 07:59:08 visual_prompt]: 	Test 700/1152. loss: 10.263, 0.1841 s / batch. (data: 1.09e-04)max mem: 17.22454 GB 
[09/17 07:59:28 visual_prompt]: 	Test 800/1152. loss: 11.237, 0.2209 s / batch. (data: 3.20e-02)max mem: 17.22454 GB 
[09/17 07:59:47 visual_prompt]: 	Test 900/1152. loss: 10.765, 0.2048 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 08:00:07 visual_prompt]: 	Test 1000/1152. loss: 10.604, 0.2074 s / batch. (data: 1.88e-02)max mem: 17.22454 GB 
[09/17 08:00:27 visual_prompt]: 	Test 1100/1152. loss: 11.652, 0.2038 s / batch. (data: 1.96e-02)max mem: 17.22454 GB 
[09/17 08:00:42 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1960, average loss: 10.1133
[09/17 08:00:42 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.32	top5: 31.40	
[09/17 08:00:42 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/17 08:00:56 visual_prompt]: Epoch 14 / 100: avg data time: 2.63e-01, avg batch time: 0.6683, average train loss: 17.2224
[09/17 08:01:05 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1435, average loss: 17.8433
[09/17 08:01:05 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 5.50	top5: 30.00	
[09/17 08:01:28 visual_prompt]: 	Test 100/1152. loss: 16.017, 0.1841 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 08:01:48 visual_prompt]: 	Test 200/1152. loss: 21.097, 0.1892 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 08:02:07 visual_prompt]: 	Test 300/1152. loss: 19.246, 0.1987 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 08:02:27 visual_prompt]: 	Test 400/1152. loss: 19.207, 0.1876 s / batch. (data: 1.97e-04)max mem: 17.22454 GB 
[09/17 08:02:47 visual_prompt]: 	Test 500/1152. loss: 20.187, 0.1947 s / batch. (data: 3.36e-05)max mem: 17.22454 GB 
[09/17 08:03:06 visual_prompt]: 	Test 600/1152. loss: 19.063, 0.1985 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 08:03:26 visual_prompt]: 	Test 700/1152. loss: 18.250, 0.1876 s / batch. (data: 1.68e-04)max mem: 17.22454 GB 
[09/17 08:03:46 visual_prompt]: 	Test 800/1152. loss: 16.831, 0.2058 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 08:04:05 visual_prompt]: 	Test 900/1152. loss: 18.101, 0.2190 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 08:04:25 visual_prompt]: 	Test 1000/1152. loss: 20.576, 0.1870 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 08:04:45 visual_prompt]: 	Test 1100/1152. loss: 16.529, 0.2042 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 08:05:00 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1958, average loss: 18.6321
[09/17 08:05:00 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 31.22	
[09/17 08:05:00 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/17 08:05:15 visual_prompt]: Epoch 15 / 100: avg data time: 2.71e-01, avg batch time: 0.6758, average train loss: 26.4865
[09/17 08:05:23 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1432, average loss: 26.6114
[09/17 08:05:23 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 37.50	
[09/17 08:05:47 visual_prompt]: 	Test 100/1152. loss: 28.887, 0.1958 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 08:06:06 visual_prompt]: 	Test 200/1152. loss: 29.861, 0.2053 s / batch. (data: 2.22e-02)max mem: 17.22454 GB 
[09/17 08:06:26 visual_prompt]: 	Test 300/1152. loss: 29.082, 0.2042 s / batch. (data: 2.06e-02)max mem: 17.22454 GB 
[09/17 08:06:46 visual_prompt]: 	Test 400/1152. loss: 29.114, 0.2117 s / batch. (data: 2.81e-02)max mem: 17.22454 GB 
[09/17 08:07:05 visual_prompt]: 	Test 500/1152. loss: 26.930, 0.1997 s / batch. (data: 7.06e-03)max mem: 17.22454 GB 
[09/17 08:07:25 visual_prompt]: 	Test 600/1152. loss: 28.702, 0.2313 s / batch. (data: 2.78e-02)max mem: 17.22454 GB 
[09/17 08:07:45 visual_prompt]: 	Test 700/1152. loss: 26.031, 0.1847 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 08:08:06 visual_prompt]: 	Test 800/1152. loss: 26.756, 0.2139 s / batch. (data: 2.43e-02)max mem: 17.22454 GB 
[09/17 08:08:25 visual_prompt]: 	Test 900/1152. loss: 32.800, 0.2133 s / batch. (data: 3.29e-05)max mem: 17.22454 GB 
[09/17 08:08:45 visual_prompt]: 	Test 1000/1152. loss: 30.239, 0.2156 s / batch. (data: 2.29e-02)max mem: 17.22454 GB 
[09/17 08:09:05 visual_prompt]: 	Test 1100/1152. loss: 26.316, 0.1953 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 08:09:20 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1968, average loss: 28.1339
[09/17 08:09:20 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.17	top5: 31.10	
[09/17 08:09:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/17 08:09:34 visual_prompt]: Epoch 16 / 100: avg data time: 2.62e-01, avg batch time: 0.6682, average train loss: 38.2811
[09/17 08:09:42 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1433, average loss: 43.0886
[09/17 08:09:42 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 5.00	top5: 30.00	
[09/17 08:10:06 visual_prompt]: 	Test 100/1152. loss: 41.592, 0.1832 s / batch. (data: 1.12e-04)max mem: 17.22454 GB 
[09/17 08:10:26 visual_prompt]: 	Test 200/1152. loss: 51.448, 0.2092 s / batch. (data: 2.46e-02)max mem: 17.22454 GB 
[09/17 08:10:45 visual_prompt]: 	Test 300/1152. loss: 42.846, 0.1955 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 08:11:05 visual_prompt]: 	Test 400/1152. loss: 43.576, 0.1993 s / batch. (data: 1.59e-02)max mem: 17.22454 GB 
[09/17 08:11:24 visual_prompt]: 	Test 500/1152. loss: 40.738, 0.1992 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 08:11:45 visual_prompt]: 	Test 600/1152. loss: 44.800, 0.2002 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 08:12:05 visual_prompt]: 	Test 700/1152. loss: 35.493, 0.1980 s / batch. (data: 3.17e-05)max mem: 17.22454 GB 
[09/17 08:12:25 visual_prompt]: 	Test 800/1152. loss: 42.342, 0.1962 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 08:12:45 visual_prompt]: 	Test 900/1152. loss: 45.222, 0.1843 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 08:13:05 visual_prompt]: 	Test 1000/1152. loss: 46.732, 0.2164 s / batch. (data: 1.96e-02)max mem: 17.22454 GB 
[09/17 08:13:25 visual_prompt]: 	Test 1100/1152. loss: 39.433, 0.1935 s / batch. (data: 3.67e-05)max mem: 17.22454 GB 
[09/17 08:13:40 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1978, average loss: 43.5360
[09/17 08:13:40 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 3.99	top5: 31.31	
[09/17 08:13:40 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/17 08:13:54 visual_prompt]: Epoch 17 / 100: avg data time: 2.64e-01, avg batch time: 0.6721, average train loss: 46.3627
[09/17 08:14:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1431, average loss: 43.7473
[09/17 08:14:03 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.00	top5: 31.00	
[09/17 08:14:27 visual_prompt]: 	Test 100/1152. loss: 41.818, 0.1939 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 08:14:46 visual_prompt]: 	Test 200/1152. loss: 43.304, 0.1881 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 08:15:05 visual_prompt]: 	Test 300/1152. loss: 38.033, 0.1840 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 08:15:25 visual_prompt]: 	Test 400/1152. loss: 40.536, 0.2234 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 08:15:45 visual_prompt]: 	Test 500/1152. loss: 45.869, 0.1978 s / batch. (data: 1.45e-02)max mem: 17.22454 GB 
[09/17 08:16:04 visual_prompt]: 	Test 600/1152. loss: 41.115, 0.1991 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 08:16:24 visual_prompt]: 	Test 700/1152. loss: 45.348, 0.1984 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 08:16:44 visual_prompt]: 	Test 800/1152. loss: 39.238, 0.2118 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 08:17:04 visual_prompt]: 	Test 900/1152. loss: 34.515, 0.1904 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 08:17:23 visual_prompt]: 	Test 1000/1152. loss: 39.648, 0.1844 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 08:17:43 visual_prompt]: 	Test 1100/1152. loss: 38.659, 0.1906 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 08:17:58 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1958, average loss: 40.9408
[09/17 08:17:58 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.32	top5: 31.29	
[09/17 08:17:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/17 08:18:12 visual_prompt]: Epoch 18 / 100: avg data time: 2.56e-01, avg batch time: 0.6607, average train loss: 38.5290
[09/17 08:18:21 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1448, average loss: 40.3242
[09/17 08:18:21 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 9.50	top5: 32.50	
[09/17 08:18:44 visual_prompt]: 	Test 100/1152. loss: 48.339, 0.2055 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 08:19:04 visual_prompt]: 	Test 200/1152. loss: 41.456, 0.1847 s / batch. (data: 9.42e-05)max mem: 17.22454 GB 
[09/17 08:19:24 visual_prompt]: 	Test 300/1152. loss: 40.573, 0.1920 s / batch. (data: 1.12e-04)max mem: 17.22454 GB 
[09/17 08:19:43 visual_prompt]: 	Test 400/1152. loss: 40.308, 0.1979 s / batch. (data: 1.39e-02)max mem: 17.22454 GB 
[09/17 08:20:03 visual_prompt]: 	Test 500/1152. loss: 42.264, 0.1850 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 08:20:23 visual_prompt]: 	Test 600/1152. loss: 39.702, 0.2019 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 08:20:42 visual_prompt]: 	Test 700/1152. loss: 38.981, 0.1970 s / batch. (data: 1.87e-04)max mem: 17.22454 GB 
[09/17 08:21:02 visual_prompt]: 	Test 800/1152. loss: 42.123, 0.2015 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 08:21:22 visual_prompt]: 	Test 900/1152. loss: 47.286, 0.1843 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 08:21:42 visual_prompt]: 	Test 1000/1152. loss: 41.298, 0.2107 s / batch. (data: 1.41e-02)max mem: 17.22454 GB 
[09/17 08:22:02 visual_prompt]: 	Test 1100/1152. loss: 40.504, 0.2427 s / batch. (data: 1.33e-02)max mem: 17.22454 GB 
[09/17 08:22:17 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1966, average loss: 41.2408
[09/17 08:22:17 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.13	top5: 31.21	
[09/17 08:22:17 visual_prompt]: Best epoch 18: best metric: 0.095
[09/17 08:22:17 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/17 08:22:31 visual_prompt]: Epoch 19 / 100: avg data time: 2.66e-01, avg batch time: 0.6708, average train loss: 38.6904
[09/17 08:22:39 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1432, average loss: 34.8423
[09/17 08:22:39 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.00	top5: 30.50	
[09/17 08:23:03 visual_prompt]: 	Test 100/1152. loss: 31.708, 0.1833 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 08:23:23 visual_prompt]: 	Test 200/1152. loss: 37.934, 0.2015 s / batch. (data: 1.78e-02)max mem: 17.22454 GB 
[09/17 08:23:43 visual_prompt]: 	Test 300/1152. loss: 42.906, 0.1843 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 08:24:02 visual_prompt]: 	Test 400/1152. loss: 32.780, 0.1977 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 08:24:22 visual_prompt]: 	Test 500/1152. loss: 36.252, 0.2011 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 08:24:42 visual_prompt]: 	Test 600/1152. loss: 37.229, 0.2115 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 08:25:02 visual_prompt]: 	Test 700/1152. loss: 31.333, 0.2205 s / batch. (data: 3.69e-02)max mem: 17.22454 GB 
[09/17 08:25:22 visual_prompt]: 	Test 800/1152. loss: 29.433, 0.2052 s / batch. (data: 1.65e-02)max mem: 17.22454 GB 
[09/17 08:25:42 visual_prompt]: 	Test 900/1152. loss: 30.113, 0.2196 s / batch. (data: 2.04e-02)max mem: 17.22454 GB 
[09/17 08:26:02 visual_prompt]: 	Test 1000/1152. loss: 38.641, 0.2092 s / batch. (data: 2.56e-02)max mem: 17.22454 GB 
[09/17 08:26:22 visual_prompt]: 	Test 1100/1152. loss: 24.422, 0.2005 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 08:26:37 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1978, average loss: 34.7287
[09/17 08:26:37 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.33	top5: 31.20	
[09/17 08:26:37 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/17 08:26:52 visual_prompt]: Epoch 20 / 100: avg data time: 2.66e-01, avg batch time: 0.6720, average train loss: 33.2145
[09/17 08:27:00 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1441, average loss: 29.5150
[09/17 08:27:00 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.00	top5: 26.00	
[09/17 08:27:24 visual_prompt]: 	Test 100/1152. loss: 27.855, 0.1963 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 08:27:43 visual_prompt]: 	Test 200/1152. loss: 25.847, 0.1988 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 08:28:03 visual_prompt]: 	Test 300/1152. loss: 24.324, 0.1843 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 08:28:22 visual_prompt]: 	Test 400/1152. loss: 34.954, 0.2033 s / batch. (data: 1.05e-02)max mem: 17.22454 GB 
[09/17 08:28:42 visual_prompt]: 	Test 500/1152. loss: 31.364, 0.2027 s / batch. (data: 1.93e-02)max mem: 17.22454 GB 
[09/17 08:29:02 visual_prompt]: 	Test 600/1152. loss: 24.097, 0.1867 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 08:29:22 visual_prompt]: 	Test 700/1152. loss: 31.361, 0.1998 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 08:29:42 visual_prompt]: 	Test 800/1152. loss: 24.915, 0.1999 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 08:30:01 visual_prompt]: 	Test 900/1152. loss: 23.736, 0.1923 s / batch. (data: 3.31e-05)max mem: 17.22454 GB 
[09/17 08:30:21 visual_prompt]: 	Test 1000/1152. loss: 23.048, 0.1851 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 08:30:41 visual_prompt]: 	Test 1100/1152. loss: 28.055, 0.1957 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 08:30:56 visual_prompt]: Inference (test):avg data time: 8.41e-03, avg batch time: 0.1965, average loss: 27.8718
[09/17 08:30:57 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.34	top5: 31.40	
[09/17 08:30:57 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/17 08:31:11 visual_prompt]: Epoch 21 / 100: avg data time: 2.58e-01, avg batch time: 0.6609, average train loss: 34.2213
[09/17 08:31:19 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1432, average loss: 24.1762
[09/17 08:31:19 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 3.50	top5: 31.50	
[09/17 08:31:43 visual_prompt]: 	Test 100/1152. loss: 24.891, 0.1835 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 08:32:02 visual_prompt]: 	Test 200/1152. loss: 24.404, 0.2080 s / batch. (data: 2.49e-02)max mem: 17.22454 GB 
[09/17 08:32:21 visual_prompt]: 	Test 300/1152. loss: 24.477, 0.1948 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 08:32:41 visual_prompt]: 	Test 400/1152. loss: 25.579, 0.1969 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 08:33:01 visual_prompt]: 	Test 500/1152. loss: 22.145, 0.2167 s / batch. (data: 2.11e-02)max mem: 17.22454 GB 
[09/17 08:33:21 visual_prompt]: 	Test 600/1152. loss: 23.952, 0.1866 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 08:33:40 visual_prompt]: 	Test 700/1152. loss: 23.627, 0.1845 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 08:34:00 visual_prompt]: 	Test 800/1152. loss: 25.009, 0.1849 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 08:34:20 visual_prompt]: 	Test 900/1152. loss: 27.923, 0.1984 s / batch. (data: 1.38e-02)max mem: 17.22454 GB 
[09/17 08:34:40 visual_prompt]: 	Test 1000/1152. loss: 23.429, 0.1886 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 08:35:00 visual_prompt]: 	Test 1100/1152. loss: 23.962, 0.1893 s / batch. (data: 4.32e-05)max mem: 17.22454 GB 
[09/17 08:35:15 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1963, average loss: 24.0628
[09/17 08:35:15 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 31.01	
[09/17 08:35:15 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/17 08:35:30 visual_prompt]: Epoch 22 / 100: avg data time: 2.56e-01, avg batch time: 0.6625, average train loss: 31.7527
[09/17 08:35:38 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1431, average loss: 25.8616
[09/17 08:35:38 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.00	top5: 30.50	
[09/17 08:36:01 visual_prompt]: 	Test 100/1152. loss: 23.576, 0.1942 s / batch. (data: 1.15e-02)max mem: 17.22454 GB 
[09/17 08:36:21 visual_prompt]: 	Test 200/1152. loss: 23.364, 0.2145 s / batch. (data: 1.38e-02)max mem: 17.22454 GB 
[09/17 08:36:40 visual_prompt]: 	Test 300/1152. loss: 26.159, 0.1853 s / batch. (data: 1.38e-03)max mem: 17.22454 GB 
[09/17 08:37:00 visual_prompt]: 	Test 400/1152. loss: 26.104, 0.1983 s / batch. (data: 1.44e-02)max mem: 17.22454 GB 
[09/17 08:37:20 visual_prompt]: 	Test 500/1152. loss: 20.534, 0.1844 s / batch. (data: 9.63e-05)max mem: 17.22454 GB 
[09/17 08:37:39 visual_prompt]: 	Test 600/1152. loss: 25.473, 0.1841 s / batch. (data: 1.65e-04)max mem: 17.22454 GB 
[09/17 08:37:59 visual_prompt]: 	Test 700/1152. loss: 27.670, 0.2020 s / batch. (data: 1.12e-04)max mem: 17.22454 GB 
[09/17 08:38:19 visual_prompt]: 	Test 800/1152. loss: 28.335, 0.1840 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 08:38:39 visual_prompt]: 	Test 900/1152. loss: 27.303, 0.1838 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 08:38:59 visual_prompt]: 	Test 1000/1152. loss: 28.815, 0.1886 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 08:39:18 visual_prompt]: 	Test 1100/1152. loss: 27.142, 0.2275 s / batch. (data: 2.67e-02)max mem: 17.22454 GB 
[09/17 08:39:33 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1958, average loss: 25.3648
[09/17 08:39:33 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.35	top5: 31.37	
[09/17 08:39:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/17 08:39:48 visual_prompt]: Epoch 23 / 100: avg data time: 2.55e-01, avg batch time: 0.6622, average train loss: 21.4566
[09/17 08:39:56 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1433, average loss: 23.5495
[09/17 08:39:56 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.00	top5: 31.50	
[09/17 08:40:19 visual_prompt]: 	Test 100/1152. loss: 21.417, 0.2079 s / batch. (data: 2.54e-02)max mem: 17.22454 GB 
[09/17 08:40:39 visual_prompt]: 	Test 200/1152. loss: 23.237, 0.1840 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 08:40:59 visual_prompt]: 	Test 300/1152. loss: 23.831, 0.1839 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 08:41:18 visual_prompt]: 	Test 400/1152. loss: 24.213, 0.1842 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 08:41:38 visual_prompt]: 	Test 500/1152. loss: 19.251, 0.1957 s / batch. (data: 1.52e-04)max mem: 17.22454 GB 
[09/17 08:41:58 visual_prompt]: 	Test 600/1152. loss: 24.211, 0.1976 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 08:42:17 visual_prompt]: 	Test 700/1152. loss: 22.029, 0.1842 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 08:42:37 visual_prompt]: 	Test 800/1152. loss: 24.363, 0.1897 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 08:42:57 visual_prompt]: 	Test 900/1152. loss: 26.746, 0.1897 s / batch. (data: 1.56e-04)max mem: 17.22454 GB 
[09/17 08:43:17 visual_prompt]: 	Test 1000/1152. loss: 26.041, 0.1928 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 08:43:37 visual_prompt]: 	Test 1100/1152. loss: 25.257, 0.2310 s / batch. (data: 4.77e-02)max mem: 17.22454 GB 
[09/17 08:43:52 visual_prompt]: Inference (test):avg data time: 8.03e-03, avg batch time: 0.1964, average loss: 23.9025
[09/17 08:43:52 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.35	top5: 31.37	
[09/17 08:43:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/17 08:44:07 visual_prompt]: Epoch 24 / 100: avg data time: 2.73e-01, avg batch time: 0.6761, average train loss: 27.0247
[09/17 08:44:15 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1431, average loss: 20.5854
[09/17 08:44:15 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 3.50	top5: 26.50	
[09/17 08:44:38 visual_prompt]: 	Test 100/1152. loss: 20.581, 0.1962 s / batch. (data: 1.31e-02)max mem: 17.22454 GB 
[09/17 08:44:58 visual_prompt]: 	Test 200/1152. loss: 22.151, 0.2015 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 08:45:18 visual_prompt]: 	Test 300/1152. loss: 17.987, 0.2010 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 08:45:37 visual_prompt]: 	Test 400/1152. loss: 20.919, 0.2040 s / batch. (data: 1.13e-02)max mem: 17.22454 GB 
[09/17 08:45:58 visual_prompt]: 	Test 500/1152. loss: 22.698, 0.1845 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 08:46:18 visual_prompt]: 	Test 600/1152. loss: 24.742, 0.1923 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 08:46:37 visual_prompt]: 	Test 700/1152. loss: 16.513, 0.1891 s / batch. (data: 1.21e-04)max mem: 17.22454 GB 
[09/17 08:46:57 visual_prompt]: 	Test 800/1152. loss: 17.983, 0.1868 s / batch. (data: 4.60e-05)max mem: 17.22454 GB 
[09/17 08:47:18 visual_prompt]: 	Test 900/1152. loss: 22.158, 0.1843 s / batch. (data: 9.99e-05)max mem: 17.22454 GB 
[09/17 08:47:37 visual_prompt]: 	Test 1000/1152. loss: 19.623, 0.1848 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 08:47:58 visual_prompt]: 	Test 1100/1152. loss: 18.467, 0.2086 s / batch. (data: 1.59e-02)max mem: 17.22454 GB 
[09/17 08:48:13 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1981, average loss: 19.9361
[09/17 08:48:13 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 31.37	
[09/17 08:48:13 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/17 08:48:27 visual_prompt]: Epoch 25 / 100: avg data time: 2.71e-01, avg batch time: 0.6747, average train loss: 24.4854
[09/17 08:48:35 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1431, average loss: 21.8454
[09/17 08:48:35 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 34.00	
[09/17 08:49:00 visual_prompt]: 	Test 100/1152. loss: 21.571, 0.2251 s / batch. (data: 3.67e-02)max mem: 17.22454 GB 
[09/17 08:49:19 visual_prompt]: 	Test 200/1152. loss: 21.253, 0.1838 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 08:49:39 visual_prompt]: 	Test 300/1152. loss: 23.333, 0.2114 s / batch. (data: 2.81e-02)max mem: 17.22454 GB 
[09/17 08:49:58 visual_prompt]: 	Test 400/1152. loss: 24.563, 0.1977 s / batch. (data: 1.44e-02)max mem: 17.22454 GB 
[09/17 08:50:18 visual_prompt]: 	Test 500/1152. loss: 23.514, 0.1846 s / batch. (data: 1.89e-04)max mem: 17.22454 GB 
[09/17 08:50:38 visual_prompt]: 	Test 600/1152. loss: 24.060, 0.2133 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 08:50:58 visual_prompt]: 	Test 700/1152. loss: 26.534, 0.1844 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 08:51:17 visual_prompt]: 	Test 800/1152. loss: 23.467, 0.1926 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 08:51:37 visual_prompt]: 	Test 900/1152. loss: 25.733, 0.2405 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 08:51:57 visual_prompt]: 	Test 1000/1152. loss: 25.301, 0.1850 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 08:52:16 visual_prompt]: 	Test 1100/1152. loss: 22.601, 0.1844 s / batch. (data: 9.68e-05)max mem: 17.22454 GB 
[09/17 08:52:31 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1963, average loss: 22.9323
[09/17 08:52:31 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 31.08	
[09/17 08:52:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/17 08:52:46 visual_prompt]: Epoch 26 / 100: avg data time: 2.77e-01, avg batch time: 0.6816, average train loss: 19.4004
[09/17 08:52:54 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1431, average loss: 20.0384
[09/17 08:52:54 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 32.50	
[09/17 08:53:18 visual_prompt]: 	Test 100/1152. loss: 23.543, 0.1982 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 08:53:37 visual_prompt]: 	Test 200/1152. loss: 20.332, 0.1852 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 08:53:57 visual_prompt]: 	Test 300/1152. loss: 20.514, 0.1984 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 08:54:16 visual_prompt]: 	Test 400/1152. loss: 21.059, 0.1847 s / batch. (data: 1.06e-04)max mem: 17.22454 GB 
[09/17 08:54:36 visual_prompt]: 	Test 500/1152. loss: 23.244, 0.1948 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 08:54:56 visual_prompt]: 	Test 600/1152. loss: 20.667, 0.1847 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 08:55:16 visual_prompt]: 	Test 700/1152. loss: 20.384, 0.1850 s / batch. (data: 1.16e-04)max mem: 17.22454 GB 
[09/17 08:55:36 visual_prompt]: 	Test 800/1152. loss: 18.302, 0.2117 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 08:55:55 visual_prompt]: 	Test 900/1152. loss: 21.629, 0.1847 s / batch. (data: 1.67e-04)max mem: 17.22454 GB 
[09/17 08:56:15 visual_prompt]: 	Test 1000/1152. loss: 19.888, 0.1848 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 08:56:35 visual_prompt]: 	Test 1100/1152. loss: 17.960, 0.2100 s / batch. (data: 2.66e-02)max mem: 17.22454 GB 
[09/17 08:56:50 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1962, average loss: 20.7197
[09/17 08:56:50 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 30.99	
[09/17 08:56:50 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/17 08:57:04 visual_prompt]: Epoch 27 / 100: avg data time: 2.69e-01, avg batch time: 0.6720, average train loss: 23.0715
[09/17 08:57:13 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1431, average loss: 20.0611
[09/17 08:57:13 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.00	top5: 32.50	
[09/17 08:57:37 visual_prompt]: 	Test 100/1152. loss: 21.951, 0.1990 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 08:57:57 visual_prompt]: 	Test 200/1152. loss: 19.085, 0.1852 s / batch. (data: 1.15e-04)max mem: 17.22454 GB 
[09/17 08:58:16 visual_prompt]: 	Test 300/1152. loss: 21.961, 0.2049 s / batch. (data: 1.62e-02)max mem: 17.22454 GB 
[09/17 08:58:36 visual_prompt]: 	Test 400/1152. loss: 19.303, 0.2126 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 08:58:56 visual_prompt]: 	Test 500/1152. loss: 17.117, 0.1889 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 08:59:16 visual_prompt]: 	Test 600/1152. loss: 19.060, 0.2180 s / batch. (data: 2.81e-02)max mem: 17.22454 GB 
[09/17 08:59:36 visual_prompt]: 	Test 700/1152. loss: 18.945, 0.1977 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 08:59:56 visual_prompt]: 	Test 800/1152. loss: 20.684, 0.2194 s / batch. (data: 2.82e-02)max mem: 17.22454 GB 
[09/17 09:00:15 visual_prompt]: 	Test 900/1152. loss: 22.776, 0.1953 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 09:00:35 visual_prompt]: 	Test 1000/1152. loss: 21.710, 0.2112 s / batch. (data: 1.46e-02)max mem: 17.22454 GB 
[09/17 09:00:55 visual_prompt]: 	Test 1100/1152. loss: 18.859, 0.2185 s / batch. (data: 2.98e-02)max mem: 17.22454 GB 
[09/17 09:01:11 visual_prompt]: Inference (test):avg data time: 8.54e-03, avg batch time: 0.1974, average loss: 19.7583
[09/17 09:01:11 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.35	top5: 31.23	
[09/17 09:01:11 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/17 09:01:26 visual_prompt]: Epoch 28 / 100: avg data time: 2.64e-01, avg batch time: 0.6702, average train loss: 20.4285
[09/17 09:01:34 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.1431, average loss: 18.1014
[09/17 09:01:34 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 26.50	
[09/17 09:01:58 visual_prompt]: 	Test 100/1152. loss: 17.632, 0.1869 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 09:02:17 visual_prompt]: 	Test 200/1152. loss: 17.711, 0.1977 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 09:02:37 visual_prompt]: 	Test 300/1152. loss: 14.707, 0.1918 s / batch. (data: 1.00e-04)max mem: 17.22454 GB 
[09/17 09:02:56 visual_prompt]: 	Test 400/1152. loss: 19.341, 0.1896 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 09:03:16 visual_prompt]: 	Test 500/1152. loss: 18.551, 0.1911 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 09:03:36 visual_prompt]: 	Test 600/1152. loss: 16.279, 0.1988 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 09:03:56 visual_prompt]: 	Test 700/1152. loss: 17.630, 0.2289 s / batch. (data: 3.62e-02)max mem: 17.22454 GB 
[09/17 09:04:15 visual_prompt]: 	Test 800/1152. loss: 18.125, 0.1923 s / batch. (data: 4.63e-05)max mem: 17.22454 GB 
[09/17 09:04:35 visual_prompt]: 	Test 900/1152. loss: 15.161, 0.1894 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 09:04:55 visual_prompt]: 	Test 1000/1152. loss: 14.718, 0.1840 s / batch. (data: 9.97e-05)max mem: 17.22454 GB 
[09/17 09:05:15 visual_prompt]: 	Test 1100/1152. loss: 16.808, 0.1985 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 09:05:30 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1960, average loss: 16.8546
[09/17 09:05:30 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.26	top5: 31.25	
[09/17 09:05:30 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/17 09:05:45 visual_prompt]: Epoch 29 / 100: avg data time: 2.83e-01, avg batch time: 0.7020, average train loss: 17.4294
[09/17 09:05:53 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1430, average loss: 21.8407
[09/17 09:05:53 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.00	top5: 28.50	
[09/17 09:06:17 visual_prompt]: 	Test 100/1152. loss: 25.185, 0.1843 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 09:06:36 visual_prompt]: 	Test 200/1152. loss: 21.841, 0.1990 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 09:06:56 visual_prompt]: 	Test 300/1152. loss: 21.693, 0.1867 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 09:07:15 visual_prompt]: 	Test 400/1152. loss: 21.193, 0.1966 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 09:07:36 visual_prompt]: 	Test 500/1152. loss: 24.518, 0.1842 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 09:07:56 visual_prompt]: 	Test 600/1152. loss: 22.901, 0.1838 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 09:08:15 visual_prompt]: 	Test 700/1152. loss: 21.163, 0.1997 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 09:08:35 visual_prompt]: 	Test 800/1152. loss: 22.107, 0.1970 s / batch. (data: 1.37e-02)max mem: 17.22454 GB 
[09/17 09:08:55 visual_prompt]: 	Test 900/1152. loss: 20.466, 0.1997 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 09:09:15 visual_prompt]: 	Test 1000/1152. loss: 19.672, 0.2177 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 09:09:34 visual_prompt]: 	Test 1100/1152. loss: 20.348, 0.2121 s / batch. (data: 2.85e-02)max mem: 17.22454 GB 
[09/17 09:09:49 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1966, average loss: 21.8129
[09/17 09:09:50 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.32	top5: 31.41	
[09/17 09:09:50 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/17 09:10:04 visual_prompt]: Epoch 30 / 100: avg data time: 2.74e-01, avg batch time: 0.6773, average train loss: 19.2279
[09/17 09:10:12 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1431, average loss: 20.6847
[09/17 09:10:12 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.00	top5: 31.00	
[09/17 09:10:36 visual_prompt]: 	Test 100/1152. loss: 17.713, 0.2164 s / batch. (data: 3.31e-02)max mem: 17.22454 GB 
[09/17 09:10:56 visual_prompt]: 	Test 200/1152. loss: 22.538, 0.1862 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 09:11:15 visual_prompt]: 	Test 300/1152. loss: 23.938, 0.1835 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 09:11:35 visual_prompt]: 	Test 400/1152. loss: 20.952, 0.1850 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 09:11:54 visual_prompt]: 	Test 500/1152. loss: 22.900, 0.1917 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 09:12:14 visual_prompt]: 	Test 600/1152. loss: 22.144, 0.1983 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 09:12:34 visual_prompt]: 	Test 700/1152. loss: 22.139, 0.1990 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 09:12:54 visual_prompt]: 	Test 800/1152. loss: 21.761, 0.1988 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 09:13:13 visual_prompt]: 	Test 900/1152. loss: 17.111, 0.1914 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 09:13:34 visual_prompt]: 	Test 1000/1152. loss: 22.012, 0.1944 s / batch. (data: 1.03e-02)max mem: 17.22454 GB 
[09/17 09:13:53 visual_prompt]: 	Test 1100/1152. loss: 18.547, 0.1961 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 09:14:08 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1962, average loss: 20.9978
[09/17 09:14:08 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.32	top5: 31.33	
[09/17 09:14:08 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/17 09:14:23 visual_prompt]: Epoch 31 / 100: avg data time: 2.66e-01, avg batch time: 0.6698, average train loss: 17.4904
[09/17 09:14:31 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1469, average loss: 15.7258
[09/17 09:14:31 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.00	top5: 30.00	
[09/17 09:14:55 visual_prompt]: 	Test 100/1152. loss: 11.539, 0.2214 s / batch. (data: 2.44e-02)max mem: 17.22454 GB 
[09/17 09:15:15 visual_prompt]: 	Test 200/1152. loss: 15.887, 0.3703 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 09:15:35 visual_prompt]: 	Test 300/1152. loss: 14.382, 0.1848 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 09:15:54 visual_prompt]: 	Test 400/1152. loss: 17.668, 0.1985 s / batch. (data: 1.46e-02)max mem: 17.22454 GB 
[09/17 09:16:14 visual_prompt]: 	Test 500/1152. loss: 14.708, 0.2144 s / batch. (data: 2.30e-02)max mem: 17.22454 GB 
[09/17 09:16:34 visual_prompt]: 	Test 600/1152. loss: 14.850, 0.1849 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 09:16:53 visual_prompt]: 	Test 700/1152. loss: 12.073, 0.1991 s / batch. (data: 1.95e-04)max mem: 17.22454 GB 
[09/17 09:17:13 visual_prompt]: 	Test 800/1152. loss: 16.251, 0.2218 s / batch. (data: 2.65e-02)max mem: 17.22454 GB 
[09/17 09:17:33 visual_prompt]: 	Test 900/1152. loss: 19.339, 0.2072 s / batch. (data: 1.86e-02)max mem: 17.22454 GB 
[09/17 09:17:53 visual_prompt]: 	Test 1000/1152. loss: 17.858, 0.1979 s / batch. (data: 1.37e-02)max mem: 17.22454 GB 
[09/17 09:18:13 visual_prompt]: 	Test 1100/1152. loss: 17.521, 0.1997 s / batch. (data: 1.71e-04)max mem: 17.22454 GB 
[09/17 09:18:27 visual_prompt]: Inference (test):avg data time: 8.26e-03, avg batch time: 0.1968, average loss: 16.1736
[09/17 09:18:28 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.34	top5: 31.06	
[09/17 09:18:28 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/17 09:18:42 visual_prompt]: Epoch 32 / 100: avg data time: 2.70e-01, avg batch time: 0.6741, average train loss: 11.2498
[09/17 09:18:51 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1431, average loss: 9.7275
[09/17 09:18:51 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 4.50	top5: 29.50	
[09/17 09:19:14 visual_prompt]: 	Test 100/1152. loss: 10.605, 0.2073 s / batch. (data: 2.50e-02)max mem: 17.22454 GB 
[09/17 09:19:34 visual_prompt]: 	Test 200/1152. loss: 10.461, 0.2095 s / batch. (data: 2.63e-02)max mem: 17.22454 GB 
[09/17 09:19:54 visual_prompt]: 	Test 300/1152. loss: 10.371, 0.1922 s / batch. (data: 1.73e-04)max mem: 17.22454 GB 
[09/17 09:20:14 visual_prompt]: 	Test 400/1152. loss: 8.664, 0.2085 s / batch. (data: 2.54e-02)max mem: 17.22454 GB 
[09/17 09:20:34 visual_prompt]: 	Test 500/1152. loss: 7.564, 0.2026 s / batch. (data: 1.91e-02)max mem: 17.22454 GB 
[09/17 09:20:53 visual_prompt]: 	Test 600/1152. loss: 9.398, 0.2068 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 09:21:13 visual_prompt]: 	Test 700/1152. loss: 7.582, 0.2000 s / batch. (data: 1.59e-02)max mem: 17.22454 GB 
[09/17 09:21:33 visual_prompt]: 	Test 800/1152. loss: 9.305, 0.1854 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 09:21:53 visual_prompt]: 	Test 900/1152. loss: 9.010, 0.1846 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 09:22:13 visual_prompt]: 	Test 1000/1152. loss: 7.533, 0.2080 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 09:22:32 visual_prompt]: 	Test 1100/1152. loss: 9.189, 0.2087 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 09:22:47 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1964, average loss: 9.4220
[09/17 09:22:48 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.28	top5: 31.15	
[09/17 09:22:48 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/17 09:23:02 visual_prompt]: Epoch 33 / 100: avg data time: 2.58e-01, avg batch time: 0.6653, average train loss: 7.9608
[09/17 09:23:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1433, average loss: 6.7588
[09/17 09:23:10 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.00	top5: 35.50	
[09/17 09:23:34 visual_prompt]: 	Test 100/1152. loss: 7.429, 0.1957 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 09:23:53 visual_prompt]: 	Test 200/1152. loss: 7.839, 0.1969 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 09:24:13 visual_prompt]: 	Test 300/1152. loss: 7.038, 0.1844 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 09:24:33 visual_prompt]: 	Test 400/1152. loss: 5.293, 0.2107 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 09:24:53 visual_prompt]: 	Test 500/1152. loss: 4.925, 0.1959 s / batch. (data: 1.20e-02)max mem: 17.22454 GB 
[09/17 09:25:13 visual_prompt]: 	Test 600/1152. loss: 5.893, 0.1908 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 09:25:33 visual_prompt]: 	Test 700/1152. loss: 5.239, 0.1851 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 09:25:53 visual_prompt]: 	Test 800/1152. loss: 7.084, 0.1988 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 09:26:13 visual_prompt]: 	Test 900/1152. loss: 7.060, 0.1850 s / batch. (data: 1.84e-04)max mem: 17.22454 GB 
[09/17 09:26:33 visual_prompt]: 	Test 1000/1152. loss: 5.620, 0.2239 s / batch. (data: 3.72e-05)max mem: 17.22454 GB 
[09/17 09:26:52 visual_prompt]: 	Test 1100/1152. loss: 7.589, 0.2132 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 09:27:08 visual_prompt]: Inference (test):avg data time: 8.56e-03, avg batch time: 0.1976, average loss: 6.7731
[09/17 09:27:08 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.35	top5: 32.41	
[09/17 09:27:08 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/17 09:27:22 visual_prompt]: Epoch 34 / 100: avg data time: 2.54e-01, avg batch time: 0.6600, average train loss: 6.2093
[09/17 09:27:30 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1442, average loss: 5.2492
[09/17 09:27:30 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.00	top5: 30.00	
[09/17 09:27:54 visual_prompt]: 	Test 100/1152. loss: 4.863, 0.2026 s / batch. (data: 1.31e-02)max mem: 17.22454 GB 
[09/17 09:28:14 visual_prompt]: 	Test 200/1152. loss: 5.964, 0.2000 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 09:28:33 visual_prompt]: 	Test 300/1152. loss: 5.200, 0.1964 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 09:28:53 visual_prompt]: 	Test 400/1152. loss: 5.186, 0.1848 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 09:29:13 visual_prompt]: 	Test 500/1152. loss: 4.940, 0.1937 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 09:29:33 visual_prompt]: 	Test 600/1152. loss: 4.914, 0.1997 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 09:29:52 visual_prompt]: 	Test 700/1152. loss: 4.207, 0.1899 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 09:30:12 visual_prompt]: 	Test 800/1152. loss: 4.892, 0.1977 s / batch. (data: 1.36e-02)max mem: 17.22454 GB 
[09/17 09:30:32 visual_prompt]: 	Test 900/1152. loss: 5.489, 0.1847 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 09:30:52 visual_prompt]: 	Test 1000/1152. loss: 5.431, 0.1954 s / batch. (data: 1.15e-02)max mem: 17.22454 GB 
[09/17 09:31:12 visual_prompt]: 	Test 1100/1152. loss: 4.202, 0.1977 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 09:31:27 visual_prompt]: Inference (test):avg data time: 8.29e-03, avg batch time: 0.1968, average loss: 5.1552
[09/17 09:31:27 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.33	top5: 33.06	
[09/17 09:31:27 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/17 09:31:42 visual_prompt]: Epoch 35 / 100: avg data time: 2.69e-01, avg batch time: 0.6722, average train loss: 5.0407
[09/17 09:31:49 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1474, average loss: 4.1408
[09/17 09:31:49 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 32.00	
[09/17 09:32:13 visual_prompt]: 	Test 100/1152. loss: 3.804, 0.1985 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 09:32:33 visual_prompt]: 	Test 200/1152. loss: 4.327, 0.1926 s / batch. (data: 9.20e-03)max mem: 17.22454 GB 
[09/17 09:32:52 visual_prompt]: 	Test 300/1152. loss: 4.564, 0.1876 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 09:33:12 visual_prompt]: 	Test 400/1152. loss: 4.149, 0.1835 s / batch. (data: 3.86e-05)max mem: 17.22454 GB 
[09/17 09:33:31 visual_prompt]: 	Test 500/1152. loss: 3.261, 0.1899 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 09:33:51 visual_prompt]: 	Test 600/1152. loss: 3.904, 0.1841 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 09:34:11 visual_prompt]: 	Test 700/1152. loss: 3.394, 0.1998 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 09:34:31 visual_prompt]: 	Test 800/1152. loss: 4.165, 0.2060 s / batch. (data: 1.34e-02)max mem: 17.22454 GB 
[09/17 09:34:51 visual_prompt]: 	Test 900/1152. loss: 4.465, 0.1841 s / batch. (data: 1.69e-04)max mem: 17.22454 GB 
[09/17 09:35:10 visual_prompt]: 	Test 1000/1152. loss: 4.124, 0.1852 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 09:35:30 visual_prompt]: 	Test 1100/1152. loss: 4.001, 0.2049 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 09:35:45 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1960, average loss: 4.1543
[09/17 09:35:45 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.24	top5: 31.51	
[09/17 09:35:45 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/17 09:36:01 visual_prompt]: Epoch 36 / 100: avg data time: 2.72e-01, avg batch time: 0.7194, average train loss: 4.3367
[09/17 09:36:09 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1431, average loss: 3.7059
[09/17 09:36:09 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.00	top5: 41.00	
[09/17 09:36:33 visual_prompt]: 	Test 100/1152. loss: 3.690, 0.1992 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 09:36:52 visual_prompt]: 	Test 200/1152. loss: 4.029, 0.1963 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 09:37:12 visual_prompt]: 	Test 300/1152. loss: 3.777, 0.2207 s / batch. (data: 3.81e-02)max mem: 17.22454 GB 
[09/17 09:37:31 visual_prompt]: 	Test 400/1152. loss: 3.942, 0.1841 s / batch. (data: 1.87e-04)max mem: 17.22454 GB 
[09/17 09:37:51 visual_prompt]: 	Test 500/1152. loss: 3.760, 0.1850 s / batch. (data: 1.58e-04)max mem: 17.22454 GB 
[09/17 09:38:11 visual_prompt]: 	Test 600/1152. loss: 3.638, 0.2195 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 09:38:31 visual_prompt]: 	Test 700/1152. loss: 3.512, 0.1850 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 09:38:50 visual_prompt]: 	Test 800/1152. loss: 3.559, 0.1850 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 09:39:10 visual_prompt]: 	Test 900/1152. loss: 4.025, 0.1842 s / batch. (data: 1.16e-04)max mem: 17.22454 GB 
[09/17 09:39:30 visual_prompt]: 	Test 1000/1152. loss: 3.970, 0.1927 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 09:39:50 visual_prompt]: 	Test 1100/1152. loss: 3.448, 0.2084 s / batch. (data: 1.56e-02)max mem: 17.22454 GB 
[09/17 09:40:05 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1960, average loss: 3.7911
[09/17 09:40:05 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.16	top5: 39.05	
[09/17 09:40:05 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/17 09:40:19 visual_prompt]: Epoch 37 / 100: avg data time: 2.71e-01, avg batch time: 0.6746, average train loss: 3.5293
[09/17 09:40:28 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1429, average loss: 3.6048
[09/17 09:40:28 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 7.00	top5: 33.50	
[09/17 09:40:51 visual_prompt]: 	Test 100/1152. loss: 3.576, 0.1980 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 09:41:11 visual_prompt]: 	Test 200/1152. loss: 3.338, 0.1836 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 09:41:30 visual_prompt]: 	Test 300/1152. loss: 3.433, 0.1847 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 09:41:50 visual_prompt]: 	Test 400/1152. loss: 3.702, 0.2130 s / batch. (data: 1.62e-02)max mem: 17.22454 GB 
[09/17 09:42:10 visual_prompt]: 	Test 500/1152. loss: 3.108, 0.1846 s / batch. (data: 1.50e-04)max mem: 17.22454 GB 
[09/17 09:42:30 visual_prompt]: 	Test 600/1152. loss: 3.075, 0.1848 s / batch. (data: 9.78e-05)max mem: 17.22454 GB 
[09/17 09:42:50 visual_prompt]: 	Test 700/1152. loss: 3.833, 0.1873 s / batch. (data: 9.94e-05)max mem: 17.22454 GB 
[09/17 09:43:10 visual_prompt]: 	Test 800/1152. loss: 3.620, 0.2124 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 09:43:30 visual_prompt]: 	Test 900/1152. loss: 3.472, 0.1879 s / batch. (data: 3.48e-05)max mem: 17.22454 GB 
[09/17 09:43:49 visual_prompt]: 	Test 1000/1152. loss: 3.383, 0.1992 s / batch. (data: 1.56e-02)max mem: 17.22454 GB 
[09/17 09:44:09 visual_prompt]: 	Test 1100/1152. loss: 3.567, 0.2067 s / batch. (data: 9.32e-03)max mem: 17.22454 GB 
[09/17 09:44:24 visual_prompt]: Inference (test):avg data time: 8.03e-03, avg batch time: 0.1967, average loss: 3.4492
[09/17 09:44:24 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.32	top5: 37.58	
[09/17 09:44:24 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/17 09:44:39 visual_prompt]: Epoch 38 / 100: avg data time: 2.75e-01, avg batch time: 0.6783, average train loss: 3.4582
[09/17 09:44:47 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1438, average loss: 3.4901
[09/17 09:44:47 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 6.50	top5: 54.50	
[09/17 09:45:11 visual_prompt]: 	Test 100/1152. loss: 3.017, 0.1901 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 09:45:30 visual_prompt]: 	Test 200/1152. loss: 3.226, 0.1844 s / batch. (data: 1.50e-04)max mem: 17.22454 GB 
[09/17 09:45:50 visual_prompt]: 	Test 300/1152. loss: 3.013, 0.1836 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 09:46:10 visual_prompt]: 	Test 400/1152. loss: 3.466, 0.1843 s / batch. (data: 1.21e-04)max mem: 17.22454 GB 
[09/17 09:46:30 visual_prompt]: 	Test 500/1152. loss: 3.319, 0.1848 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 09:46:50 visual_prompt]: 	Test 600/1152. loss: 3.285, 0.1961 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 09:47:10 visual_prompt]: 	Test 700/1152. loss: 3.381, 0.1845 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 09:47:30 visual_prompt]: 	Test 800/1152. loss: 3.598, 0.1895 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 09:47:49 visual_prompt]: 	Test 900/1152. loss: 3.239, 0.1845 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 09:48:09 visual_prompt]: 	Test 1000/1152. loss: 3.209, 0.1954 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 09:48:29 visual_prompt]: 	Test 1100/1152. loss: 3.683, 0.1986 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 09:48:44 visual_prompt]: Inference (test):avg data time: 8.51e-03, avg batch time: 0.1972, average loss: 3.3190
[09/17 09:48:44 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 8.01	top5: 51.35	
[09/17 09:48:44 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/17 09:48:58 visual_prompt]: Epoch 39 / 100: avg data time: 2.68e-01, avg batch time: 0.6703, average train loss: 2.8739
[09/17 09:49:06 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1431, average loss: 2.6992
[09/17 09:49:06 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 11.50	top5: 64.50	
[09/17 09:49:30 visual_prompt]: 	Test 100/1152. loss: 2.827, 0.2082 s / batch. (data: 2.52e-02)max mem: 17.22454 GB 
[09/17 09:49:50 visual_prompt]: 	Test 200/1152. loss: 2.869, 0.1942 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 09:50:09 visual_prompt]: 	Test 300/1152. loss: 2.705, 0.1878 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 09:50:29 visual_prompt]: 	Test 400/1152. loss: 2.650, 0.2392 s / batch. (data: 3.96e-02)max mem: 17.22454 GB 
[09/17 09:50:49 visual_prompt]: 	Test 500/1152. loss: 3.023, 0.2121 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 09:51:09 visual_prompt]: 	Test 600/1152. loss: 2.587, 0.2303 s / batch. (data: 4.69e-02)max mem: 17.22454 GB 
[09/17 09:51:28 visual_prompt]: 	Test 700/1152. loss: 2.571, 0.1844 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 09:51:48 visual_prompt]: 	Test 800/1152. loss: 2.652, 0.1961 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 09:52:08 visual_prompt]: 	Test 900/1152. loss: 2.390, 0.2086 s / batch. (data: 1.10e-02)max mem: 17.22454 GB 
[09/17 09:52:28 visual_prompt]: 	Test 1000/1152. loss: 2.690, 0.2276 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 09:52:48 visual_prompt]: 	Test 1100/1152. loss: 2.243, 0.2007 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 09:53:03 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1970, average loss: 2.6442
[09/17 09:53:03 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 16.55	top5: 66.77	
[09/17 09:53:03 visual_prompt]: Best epoch 39: best metric: 0.115
[09/17 09:53:03 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/17 09:53:17 visual_prompt]: Epoch 40 / 100: avg data time: 2.69e-01, avg batch time: 0.6728, average train loss: 2.2931
[09/17 09:53:26 visual_prompt]: Inference (val):avg data time: 4.66e-05, avg batch time: 0.1434, average loss: 2.2320
[09/17 09:53:26 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 19.00	top5: 77.00	
[09/17 09:53:49 visual_prompt]: 	Test 100/1152. loss: 2.040, 0.1968 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 09:54:10 visual_prompt]: 	Test 200/1152. loss: 2.048, 0.2340 s / batch. (data: 5.12e-02)max mem: 17.22454 GB 
[09/17 09:54:29 visual_prompt]: 	Test 300/1152. loss: 2.223, 0.1957 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 09:54:49 visual_prompt]: 	Test 400/1152. loss: 2.254, 0.2037 s / batch. (data: 2.04e-02)max mem: 17.22454 GB 
[09/17 09:55:09 visual_prompt]: 	Test 500/1152. loss: 2.106, 0.2280 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 09:55:29 visual_prompt]: 	Test 600/1152. loss: 2.220, 0.1847 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 09:55:49 visual_prompt]: 	Test 700/1152. loss: 2.159, 0.2040 s / batch. (data: 2.00e-02)max mem: 17.22454 GB 
[09/17 09:56:10 visual_prompt]: 	Test 800/1152. loss: 2.273, 0.1850 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 09:56:29 visual_prompt]: 	Test 900/1152. loss: 2.333, 0.1844 s / batch. (data: 1.56e-04)max mem: 17.22454 GB 
[09/17 09:56:49 visual_prompt]: 	Test 1000/1152. loss: 2.337, 0.1962 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 09:57:10 visual_prompt]: 	Test 1100/1152. loss: 2.386, 0.1886 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 09:57:25 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1989, average loss: 2.2392
[09/17 09:57:25 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 19.61	top5: 74.66	
[09/17 09:57:25 visual_prompt]: Best epoch 40: best metric: 0.190
[09/17 09:57:25 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/17 09:57:40 visual_prompt]: Epoch 41 / 100: avg data time: 2.69e-01, avg batch time: 0.6722, average train loss: 2.0260
[09/17 09:57:48 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1434, average loss: 1.9752
[09/17 09:57:48 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 27.00	top5: 86.00	
[09/17 09:58:11 visual_prompt]: 	Test 100/1152. loss: 2.217, 0.1987 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 09:58:31 visual_prompt]: 	Test 200/1152. loss: 2.125, 0.1838 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 09:58:50 visual_prompt]: 	Test 300/1152. loss: 1.991, 0.1843 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 09:59:10 visual_prompt]: 	Test 400/1152. loss: 1.783, 0.1955 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 09:59:30 visual_prompt]: 	Test 500/1152. loss: 1.866, 0.2025 s / batch. (data: 3.10e-05)max mem: 17.22454 GB 
[09/17 09:59:50 visual_prompt]: 	Test 600/1152. loss: 1.896, 0.1999 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 10:00:10 visual_prompt]: 	Test 700/1152. loss: 1.879, 0.1973 s / batch. (data: 1.33e-02)max mem: 17.22454 GB 
[09/17 10:00:29 visual_prompt]: 	Test 800/1152. loss: 2.187, 0.1992 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 10:00:49 visual_prompt]: 	Test 900/1152. loss: 1.949, 0.1858 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 10:01:09 visual_prompt]: 	Test 1000/1152. loss: 1.677, 0.1846 s / batch. (data: 9.39e-05)max mem: 17.22454 GB 
[09/17 10:01:28 visual_prompt]: 	Test 1100/1152. loss: 2.198, 0.1851 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 10:01:44 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1963, average loss: 1.9845
[09/17 10:01:44 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 29.03	top5: 86.96	
[09/17 10:01:44 visual_prompt]: Best epoch 41: best metric: 0.270
[09/17 10:01:44 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/17 10:01:58 visual_prompt]: Epoch 42 / 100: avg data time: 2.70e-01, avg batch time: 0.6738, average train loss: 1.8141
[09/17 10:02:06 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1571, average loss: 2.1330
[09/17 10:02:06 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 27.50	top5: 94.00	
[09/17 10:02:30 visual_prompt]: 	Test 100/1152. loss: 2.667, 0.1870 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 10:02:50 visual_prompt]: 	Test 200/1152. loss: 2.025, 0.1978 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 10:03:09 visual_prompt]: 	Test 300/1152. loss: 2.347, 0.2040 s / batch. (data: 2.04e-02)max mem: 17.22454 GB 
[09/17 10:03:29 visual_prompt]: 	Test 400/1152. loss: 2.115, 0.1961 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 10:03:49 visual_prompt]: 	Test 500/1152. loss: 2.435, 0.1846 s / batch. (data: 1.02e-04)max mem: 17.22454 GB 
[09/17 10:04:09 visual_prompt]: 	Test 600/1152. loss: 2.189, 0.1926 s / batch. (data: 4.46e-05)max mem: 17.22454 GB 
[09/17 10:04:28 visual_prompt]: 	Test 700/1152. loss: 2.387, 0.1893 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 10:04:48 visual_prompt]: 	Test 800/1152. loss: 2.316, 0.1849 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 10:05:08 visual_prompt]: 	Test 900/1152. loss: 2.518, 0.1840 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 10:05:28 visual_prompt]: 	Test 1000/1152. loss: 2.161, 0.1849 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 10:05:48 visual_prompt]: 	Test 1100/1152. loss: 2.432, 0.2143 s / batch. (data: 2.82e-02)max mem: 17.22454 GB 
[09/17 10:06:03 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1963, average loss: 2.2873
[09/17 10:06:03 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 24.00	top5: 92.45	
[09/17 10:06:03 visual_prompt]: Best epoch 42: best metric: 0.275
[09/17 10:06:03 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/17 10:06:17 visual_prompt]: Epoch 43 / 100: avg data time: 2.62e-01, avg batch time: 0.6717, average train loss: 2.0048
[09/17 10:06:25 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1430, average loss: 1.9053
[09/17 10:06:25 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 34.00	top5: 92.50	
[09/17 10:06:49 visual_prompt]: 	Test 100/1152. loss: 1.767, 0.1952 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 10:07:08 visual_prompt]: 	Test 200/1152. loss: 1.726, 0.1847 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 10:07:28 visual_prompt]: 	Test 300/1152. loss: 1.749, 0.1849 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 10:07:47 visual_prompt]: 	Test 400/1152. loss: 2.101, 0.1960 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 10:08:07 visual_prompt]: 	Test 500/1152. loss: 1.976, 0.1845 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 10:08:27 visual_prompt]: 	Test 600/1152. loss: 1.694, 0.1918 s / batch. (data: 1.64e-04)max mem: 17.22454 GB 
[09/17 10:08:47 visual_prompt]: 	Test 700/1152. loss: 1.991, 0.1918 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 10:09:07 visual_prompt]: 	Test 800/1152. loss: 2.219, 0.1839 s / batch. (data: 1.58e-04)max mem: 17.22454 GB 
[09/17 10:09:26 visual_prompt]: 	Test 900/1152. loss: 1.979, 0.1976 s / batch. (data: 1.31e-02)max mem: 17.22454 GB 
[09/17 10:09:46 visual_prompt]: 	Test 1000/1152. loss: 2.042, 0.1847 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 10:10:06 visual_prompt]: 	Test 1100/1152. loss: 2.302, 0.1834 s / batch. (data: 8.96e-05)max mem: 17.22454 GB 
[09/17 10:10:21 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1962, average loss: 1.9408
[09/17 10:10:22 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 32.50	top5: 88.64	
[09/17 10:10:22 visual_prompt]: Best epoch 43: best metric: 0.340
[09/17 10:10:22 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/17 10:10:36 visual_prompt]: Epoch 44 / 100: avg data time: 2.68e-01, avg batch time: 0.6713, average train loss: 1.8766
[09/17 10:10:44 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1431, average loss: 1.6360
[09/17 10:10:44 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 36.50	top5: 98.50	
[09/17 10:11:08 visual_prompt]: 	Test 100/1152. loss: 1.805, 0.1836 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 10:11:27 visual_prompt]: 	Test 200/1152. loss: 1.587, 0.1843 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 10:11:47 visual_prompt]: 	Test 300/1152. loss: 1.656, 0.1842 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 10:12:07 visual_prompt]: 	Test 400/1152. loss: 1.941, 0.1877 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 10:12:27 visual_prompt]: 	Test 500/1152. loss: 1.944, 0.1958 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 10:12:47 visual_prompt]: 	Test 600/1152. loss: 1.468, 0.1994 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 10:13:07 visual_prompt]: 	Test 700/1152. loss: 2.008, 0.1846 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 10:13:26 visual_prompt]: 	Test 800/1152. loss: 1.688, 0.1844 s / batch. (data: 8.85e-05)max mem: 17.22454 GB 
[09/17 10:13:46 visual_prompt]: 	Test 900/1152. loss: 1.649, 0.1985 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 10:14:06 visual_prompt]: 	Test 1000/1152. loss: 1.731, 0.1990 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 10:14:26 visual_prompt]: 	Test 1100/1152. loss: 1.791, 0.1845 s / batch. (data: 1.50e-04)max mem: 17.22454 GB 
[09/17 10:14:41 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1966, average loss: 1.7450
[09/17 10:14:41 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 33.32	top5: 98.07	
[09/17 10:14:41 visual_prompt]: Best epoch 44: best metric: 0.365
[09/17 10:14:41 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/17 10:14:55 visual_prompt]: Epoch 45 / 100: avg data time: 2.67e-01, avg batch time: 0.6692, average train loss: 1.5741
[09/17 10:15:03 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1431, average loss: 1.4460
[09/17 10:15:03 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 41.00	top5: 100.00	
[09/17 10:15:27 visual_prompt]: 	Test 100/1152. loss: 1.489, 0.2313 s / batch. (data: 4.87e-02)max mem: 17.22454 GB 
[09/17 10:15:47 visual_prompt]: 	Test 200/1152. loss: 1.435, 0.1841 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 10:16:06 visual_prompt]: 	Test 300/1152. loss: 1.654, 0.2081 s / batch. (data: 2.46e-02)max mem: 17.22454 GB 
[09/17 10:16:26 visual_prompt]: 	Test 400/1152. loss: 1.575, 0.2005 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 10:16:46 visual_prompt]: 	Test 500/1152. loss: 1.487, 0.2018 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 10:17:06 visual_prompt]: 	Test 600/1152. loss: 1.423, 0.2037 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 10:17:25 visual_prompt]: 	Test 700/1152. loss: 1.622, 0.1853 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 10:17:45 visual_prompt]: 	Test 800/1152. loss: 1.628, 0.1909 s / batch. (data: 1.52e-04)max mem: 17.22454 GB 
[09/17 10:18:05 visual_prompt]: 	Test 900/1152. loss: 1.343, 0.2092 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 10:18:25 visual_prompt]: 	Test 1000/1152. loss: 1.507, 0.1952 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 10:18:45 visual_prompt]: 	Test 1100/1152. loss: 1.578, 0.1988 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 10:18:59 visual_prompt]: Inference (test):avg data time: 8.66e-03, avg batch time: 0.1964, average loss: 1.5367
[09/17 10:19:00 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 37.29	top5: 99.31	
[09/17 10:19:00 visual_prompt]: Best epoch 45: best metric: 0.410
[09/17 10:19:00 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/17 10:19:14 visual_prompt]: Epoch 46 / 100: avg data time: 2.76e-01, avg batch time: 0.6786, average train loss: 1.5225
[09/17 10:19:22 visual_prompt]: Inference (val):avg data time: 5.42e-05, avg batch time: 0.1604, average loss: 1.3764
[09/17 10:19:22 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 44.50	top5: 98.50	
[09/17 10:19:46 visual_prompt]: 	Test 100/1152. loss: 1.337, 0.1969 s / batch. (data: 1.95e-04)max mem: 17.22454 GB 
[09/17 10:20:06 visual_prompt]: 	Test 200/1152. loss: 1.538, 0.1830 s / batch. (data: 1.08e-04)max mem: 17.22454 GB 
[09/17 10:20:26 visual_prompt]: 	Test 300/1152. loss: 1.342, 0.2015 s / batch. (data: 1.78e-02)max mem: 17.22454 GB 
[09/17 10:20:45 visual_prompt]: 	Test 400/1152. loss: 1.453, 0.1996 s / batch. (data: 1.75e-04)max mem: 17.22454 GB 
[09/17 10:21:05 visual_prompt]: 	Test 500/1152. loss: 1.494, 0.1942 s / batch. (data: 2.22e-03)max mem: 17.22454 GB 
[09/17 10:21:25 visual_prompt]: 	Test 600/1152. loss: 1.241, 0.1850 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 10:21:45 visual_prompt]: 	Test 700/1152. loss: 1.388, 0.2038 s / batch. (data: 1.99e-02)max mem: 17.22454 GB 
[09/17 10:22:05 visual_prompt]: 	Test 800/1152. loss: 1.465, 0.1848 s / batch. (data: 1.15e-04)max mem: 17.22454 GB 
[09/17 10:22:25 visual_prompt]: 	Test 900/1152. loss: 1.433, 0.1954 s / batch. (data: 1.09e-02)max mem: 17.22454 GB 
[09/17 10:22:44 visual_prompt]: 	Test 1000/1152. loss: 1.556, 0.2230 s / batch. (data: 3.92e-02)max mem: 17.22454 GB 
[09/17 10:23:04 visual_prompt]: 	Test 1100/1152. loss: 1.524, 0.1843 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 10:23:19 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1967, average loss: 1.4371
[09/17 10:23:20 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 39.48	top5: 98.06	
[09/17 10:23:20 visual_prompt]: Best epoch 46: best metric: 0.445
[09/17 10:23:20 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/17 10:23:34 visual_prompt]: Epoch 47 / 100: avg data time: 2.69e-01, avg batch time: 0.6713, average train loss: 1.4986
[09/17 10:23:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1432, average loss: 1.6639
[09/17 10:23:42 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 49.00	top5: 97.50	
[09/17 10:24:06 visual_prompt]: 	Test 100/1152. loss: 1.676, 0.1946 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 10:24:25 visual_prompt]: 	Test 200/1152. loss: 2.067, 0.1854 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 10:24:45 visual_prompt]: 	Test 300/1152. loss: 1.675, 0.1983 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 10:25:05 visual_prompt]: 	Test 400/1152. loss: 1.677, 0.1890 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 10:25:25 visual_prompt]: 	Test 500/1152. loss: 1.891, 0.1955 s / batch. (data: 1.13e-02)max mem: 17.22454 GB 
[09/17 10:25:44 visual_prompt]: 	Test 600/1152. loss: 1.779, 0.1846 s / batch. (data: 3.98e-05)max mem: 17.22454 GB 
[09/17 10:26:04 visual_prompt]: 	Test 700/1152. loss: 1.716, 0.1847 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 10:26:24 visual_prompt]: 	Test 800/1152. loss: 1.501, 0.2297 s / batch. (data: 2.72e-02)max mem: 17.22454 GB 
[09/17 10:26:44 visual_prompt]: 	Test 900/1152. loss: 1.821, 0.2073 s / batch. (data: 2.33e-02)max mem: 17.22454 GB 
[09/17 10:27:04 visual_prompt]: 	Test 1000/1152. loss: 2.107, 0.1957 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 10:27:24 visual_prompt]: 	Test 1100/1152. loss: 1.412, 0.1930 s / batch. (data: 1.56e-04)max mem: 17.22454 GB 
[09/17 10:27:39 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1968, average loss: 1.7308
[09/17 10:27:39 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 42.03	top5: 96.45	
[09/17 10:27:39 visual_prompt]: Best epoch 47: best metric: 0.490
[09/17 10:27:39 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/17 10:27:53 visual_prompt]: Epoch 48 / 100: avg data time: 2.75e-01, avg batch time: 0.6787, average train loss: 1.4755
[09/17 10:28:01 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1438, average loss: 1.5481
[09/17 10:28:01 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 41.00	top5: 97.50	
[09/17 10:28:25 visual_prompt]: 	Test 100/1152. loss: 1.584, 0.1841 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 10:28:45 visual_prompt]: 	Test 200/1152. loss: 1.803, 0.1916 s / batch. (data: 8.17e-03)max mem: 17.22454 GB 
[09/17 10:29:05 visual_prompt]: 	Test 300/1152. loss: 1.820, 0.1998 s / batch. (data: 1.62e-02)max mem: 17.22454 GB 
[09/17 10:29:24 visual_prompt]: 	Test 400/1152. loss: 1.562, 0.1976 s / batch. (data: 1.36e-02)max mem: 17.22454 GB 
[09/17 10:29:44 visual_prompt]: 	Test 500/1152. loss: 1.612, 0.2022 s / batch. (data: 1.79e-02)max mem: 17.22454 GB 
[09/17 10:30:04 visual_prompt]: 	Test 600/1152. loss: 1.656, 0.1991 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 10:30:24 visual_prompt]: 	Test 700/1152. loss: 1.630, 0.1991 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 10:30:43 visual_prompt]: 	Test 800/1152. loss: 1.641, 0.1845 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 10:31:03 visual_prompt]: 	Test 900/1152. loss: 1.581, 0.1850 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 10:31:23 visual_prompt]: 	Test 1000/1152. loss: 1.430, 0.1962 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 10:31:43 visual_prompt]: 	Test 1100/1152. loss: 1.636, 0.1984 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 10:31:58 visual_prompt]: Inference (test):avg data time: 8.31e-03, avg batch time: 0.1965, average loss: 1.6760
[09/17 10:31:58 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 38.84	top5: 95.39	
[09/17 10:31:58 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/17 10:32:12 visual_prompt]: Epoch 49 / 100: avg data time: 2.65e-01, avg batch time: 0.6681, average train loss: 1.4752
[09/17 10:32:21 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1432, average loss: 1.8553
[09/17 10:32:21 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 33.50	top5: 97.00	
[09/17 10:32:44 visual_prompt]: 	Test 100/1152. loss: 1.679, 0.2027 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 10:33:04 visual_prompt]: 	Test 200/1152. loss: 1.771, 0.1835 s / batch. (data: 3.03e-04)max mem: 17.22454 GB 
[09/17 10:33:23 visual_prompt]: 	Test 300/1152. loss: 1.691, 0.1965 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 10:33:43 visual_prompt]: 	Test 400/1152. loss: 1.923, 0.1840 s / batch. (data: 9.13e-05)max mem: 17.22454 GB 
[09/17 10:34:02 visual_prompt]: 	Test 500/1152. loss: 2.001, 0.1851 s / batch. (data: 1.15e-04)max mem: 17.22454 GB 
[09/17 10:34:22 visual_prompt]: 	Test 600/1152. loss: 1.726, 0.2008 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 10:34:42 visual_prompt]: 	Test 700/1152. loss: 1.921, 0.1879 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 10:35:02 visual_prompt]: 	Test 800/1152. loss: 2.016, 0.2000 s / batch. (data: 9.39e-05)max mem: 17.22454 GB 
[09/17 10:35:22 visual_prompt]: 	Test 900/1152. loss: 2.157, 0.1847 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 10:35:42 visual_prompt]: 	Test 1000/1152. loss: 2.110, 0.1885 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 10:36:02 visual_prompt]: 	Test 1100/1152. loss: 1.931, 0.2127 s / batch. (data: 1.59e-02)max mem: 17.22454 GB 
[09/17 10:36:17 visual_prompt]: Inference (test):avg data time: 8.40e-03, avg batch time: 0.1962, average loss: 1.9474
[09/17 10:36:17 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 31.23	top5: 95.94	
[09/17 10:36:17 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/17 10:36:31 visual_prompt]: Epoch 50 / 100: avg data time: 2.70e-01, avg batch time: 0.6756, average train loss: 1.5537
[09/17 10:36:39 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1432, average loss: 1.3509
[09/17 10:36:39 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 43.50	top5: 100.00	
[09/17 10:37:03 visual_prompt]: 	Test 100/1152. loss: 1.139, 0.1911 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 10:37:23 visual_prompt]: 	Test 200/1152. loss: 1.505, 0.1917 s / batch. (data: 1.72e-04)max mem: 17.22454 GB 
[09/17 10:37:42 visual_prompt]: 	Test 300/1152. loss: 1.627, 0.1850 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 10:38:02 visual_prompt]: 	Test 400/1152. loss: 1.432, 0.2025 s / batch. (data: 1.93e-02)max mem: 17.22454 GB 
[09/17 10:38:22 visual_prompt]: 	Test 500/1152. loss: 1.334, 0.1969 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 10:38:42 visual_prompt]: 	Test 600/1152. loss: 1.408, 0.1960 s / batch. (data: 1.21e-02)max mem: 17.22454 GB 
[09/17 10:39:01 visual_prompt]: 	Test 700/1152. loss: 1.362, 0.1853 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 10:39:21 visual_prompt]: 	Test 800/1152. loss: 1.464, 0.1958 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 10:39:42 visual_prompt]: 	Test 900/1152. loss: 1.311, 0.2182 s / batch. (data: 2.80e-02)max mem: 17.22454 GB 
[09/17 10:40:01 visual_prompt]: 	Test 1000/1152. loss: 1.599, 0.1895 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 10:40:22 visual_prompt]: 	Test 1100/1152. loss: 1.368, 0.1847 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 10:40:37 visual_prompt]: Inference (test):avg data time: 8.60e-03, avg batch time: 0.1977, average loss: 1.4542
[09/17 10:40:37 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 39.61	top5: 99.63	
[09/17 10:40:37 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/17 10:40:52 visual_prompt]: Epoch 51 / 100: avg data time: 2.76e-01, avg batch time: 0.6790, average train loss: 1.1944
[09/17 10:41:00 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1431, average loss: 1.1599
[09/17 10:41:00 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 46.00	top5: 100.00	
[09/17 10:41:24 visual_prompt]: 	Test 100/1152. loss: 1.357, 0.2443 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 10:41:44 visual_prompt]: 	Test 200/1152. loss: 1.331, 0.1939 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 10:42:04 visual_prompt]: 	Test 300/1152. loss: 1.400, 0.2105 s / batch. (data: 2.71e-02)max mem: 17.22454 GB 
[09/17 10:42:24 visual_prompt]: 	Test 400/1152. loss: 1.433, 0.1967 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 10:42:43 visual_prompt]: 	Test 500/1152. loss: 1.334, 0.2042 s / batch. (data: 2.08e-02)max mem: 17.22454 GB 
[09/17 10:43:03 visual_prompt]: 	Test 600/1152. loss: 1.303, 0.1993 s / batch. (data: 6.69e-03)max mem: 17.22454 GB 
[09/17 10:43:23 visual_prompt]: 	Test 700/1152. loss: 1.396, 0.1973 s / batch. (data: 1.03e-02)max mem: 17.22454 GB 
[09/17 10:43:43 visual_prompt]: 	Test 800/1152. loss: 1.468, 0.1847 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 10:44:03 visual_prompt]: 	Test 900/1152. loss: 1.477, 0.1846 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 10:44:23 visual_prompt]: 	Test 1000/1152. loss: 1.597, 0.1845 s / batch. (data: 9.44e-05)max mem: 17.22454 GB 
[09/17 10:44:43 visual_prompt]: 	Test 1100/1152. loss: 1.571, 0.2258 s / batch. (data: 9.93e-03)max mem: 17.22454 GB 
[09/17 10:44:58 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1977, average loss: 1.4257
[09/17 10:44:58 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 40.13	top5: 100.00	
[09/17 10:44:58 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/17 10:45:13 visual_prompt]: Epoch 52 / 100: avg data time: 2.78e-01, avg batch time: 0.6804, average train loss: 1.1072
[09/17 10:45:21 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1431, average loss: 1.2121
[09/17 10:45:21 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 48.00	top5: 100.00	
[09/17 10:45:45 visual_prompt]: 	Test 100/1152. loss: 1.252, 0.1960 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 10:46:04 visual_prompt]: 	Test 200/1152. loss: 1.541, 0.1887 s / batch. (data: 1.83e-04)max mem: 17.22454 GB 
[09/17 10:46:24 visual_prompt]: 	Test 300/1152. loss: 1.414, 0.1933 s / batch. (data: 9.66e-03)max mem: 17.22454 GB 
[09/17 10:46:43 visual_prompt]: 	Test 400/1152. loss: 1.248, 0.1847 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 10:47:03 visual_prompt]: 	Test 500/1152. loss: 1.323, 0.1945 s / batch. (data: 1.08e-02)max mem: 17.22454 GB 
[09/17 10:47:22 visual_prompt]: 	Test 600/1152. loss: 1.360, 0.1852 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 10:47:42 visual_prompt]: 	Test 700/1152. loss: 1.159, 0.2153 s / batch. (data: 2.38e-02)max mem: 17.22454 GB 
[09/17 10:48:02 visual_prompt]: 	Test 800/1152. loss: 1.106, 0.1952 s / batch. (data: 1.13e-02)max mem: 17.22454 GB 
[09/17 10:48:22 visual_prompt]: 	Test 900/1152. loss: 1.391, 0.1877 s / batch. (data: 6.41e-05)max mem: 17.22454 GB 
[09/17 10:48:41 visual_prompt]: 	Test 1000/1152. loss: 1.509, 0.1963 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 10:49:01 visual_prompt]: 	Test 1100/1152. loss: 0.913, 0.1849 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 10:49:16 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1958, average loss: 1.3064
[09/17 10:49:16 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 44.36	top5: 100.00	
[09/17 10:49:16 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/17 10:49:31 visual_prompt]: Epoch 53 / 100: avg data time: 2.71e-01, avg batch time: 0.6764, average train loss: 1.1314
[09/17 10:49:39 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1432, average loss: 1.2526
[09/17 10:49:39 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 50.00	top5: 99.50	
[09/17 10:50:03 visual_prompt]: 	Test 100/1152. loss: 1.298, 0.2252 s / batch. (data: 4.21e-02)max mem: 17.22454 GB 
[09/17 10:50:23 visual_prompt]: 	Test 200/1152. loss: 1.091, 0.1960 s / batch. (data: 1.31e-02)max mem: 17.22454 GB 
[09/17 10:50:43 visual_prompt]: 	Test 300/1152. loss: 1.426, 0.1961 s / batch. (data: 1.28e-02)max mem: 17.22454 GB 
[09/17 10:51:02 visual_prompt]: 	Test 400/1152. loss: 1.285, 0.1852 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 10:51:22 visual_prompt]: 	Test 500/1152. loss: 1.233, 0.1889 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 10:51:42 visual_prompt]: 	Test 600/1152. loss: 1.300, 0.2063 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 10:52:02 visual_prompt]: 	Test 700/1152. loss: 1.451, 0.2110 s / batch. (data: 1.03e-02)max mem: 17.22454 GB 
[09/17 10:52:22 visual_prompt]: 	Test 800/1152. loss: 1.345, 0.1941 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 10:52:41 visual_prompt]: 	Test 900/1152. loss: 1.636, 0.2370 s / batch. (data: 5.14e-02)max mem: 17.22454 GB 
[09/17 10:53:01 visual_prompt]: 	Test 1000/1152. loss: 1.531, 0.1992 s / batch. (data: 1.57e-02)max mem: 17.22454 GB 
[09/17 10:53:21 visual_prompt]: 	Test 1100/1152. loss: 1.293, 0.1952 s / batch. (data: 1.06e-02)max mem: 17.22454 GB 
[09/17 10:53:36 visual_prompt]: Inference (test):avg data time: 8.39e-03, avg batch time: 0.1970, average loss: 1.2940
[09/17 10:53:37 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 48.61	top5: 99.67	
[09/17 10:53:37 visual_prompt]: Best epoch 53: best metric: 0.500
[09/17 10:53:37 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/17 10:53:51 visual_prompt]: Epoch 54 / 100: avg data time: 2.53e-01, avg batch time: 0.6589, average train loss: 1.1848
[09/17 10:53:59 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1429, average loss: 0.9532
[09/17 10:53:59 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 59.00	top5: 100.00	
[09/17 10:54:23 visual_prompt]: 	Test 100/1152. loss: 1.246, 0.1891 s / batch. (data: 5.21e-03)max mem: 17.22454 GB 
[09/17 10:54:42 visual_prompt]: 	Test 200/1152. loss: 1.150, 0.1970 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 10:55:02 visual_prompt]: 	Test 300/1152. loss: 1.143, 0.1927 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 10:55:22 visual_prompt]: 	Test 400/1152. loss: 0.836, 0.1839 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 10:55:41 visual_prompt]: 	Test 500/1152. loss: 1.115, 0.1848 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 10:56:02 visual_prompt]: 	Test 600/1152. loss: 1.070, 0.1851 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 10:56:22 visual_prompt]: 	Test 700/1152. loss: 1.139, 0.1974 s / batch. (data: 1.36e-02)max mem: 17.22454 GB 
[09/17 10:56:42 visual_prompt]: 	Test 800/1152. loss: 0.992, 0.1877 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 10:57:01 visual_prompt]: 	Test 900/1152. loss: 1.084, 0.1839 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 10:57:21 visual_prompt]: 	Test 1000/1152. loss: 1.116, 0.2009 s / batch. (data: 1.03e-02)max mem: 17.22454 GB 
[09/17 10:57:41 visual_prompt]: 	Test 1100/1152. loss: 1.103, 0.2009 s / batch. (data: 1.62e-02)max mem: 17.22454 GB 
[09/17 10:57:56 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1969, average loss: 1.0980
[09/17 10:57:56 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 51.59	top5: 100.00	
[09/17 10:57:56 visual_prompt]: Best epoch 54: best metric: 0.590
[09/17 10:57:56 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/17 10:58:10 visual_prompt]: Epoch 55 / 100: avg data time: 2.61e-01, avg batch time: 0.6646, average train loss: 1.1738
[09/17 10:58:19 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1431, average loss: 0.9057
[09/17 10:58:19 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 62.00	top5: 100.00	
[09/17 10:58:42 visual_prompt]: 	Test 100/1152. loss: 1.231, 0.1875 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 10:59:02 visual_prompt]: 	Test 200/1152. loss: 1.150, 0.1833 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 10:59:22 visual_prompt]: 	Test 300/1152. loss: 0.976, 0.1893 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 10:59:41 visual_prompt]: 	Test 400/1152. loss: 0.903, 0.1888 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 11:00:01 visual_prompt]: 	Test 500/1152. loss: 1.055, 0.1845 s / batch. (data: 3.24e-05)max mem: 17.22454 GB 
[09/17 11:00:21 visual_prompt]: 	Test 600/1152. loss: 0.933, 0.2040 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 11:00:41 visual_prompt]: 	Test 700/1152. loss: 1.030, 0.1929 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 11:01:01 visual_prompt]: 	Test 800/1152. loss: 0.859, 0.1996 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 11:01:21 visual_prompt]: 	Test 900/1152. loss: 1.010, 0.1878 s / batch. (data: 1.65e-04)max mem: 17.22454 GB 
[09/17 11:01:41 visual_prompt]: 	Test 1000/1152. loss: 1.045, 0.1848 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 11:02:01 visual_prompt]: 	Test 1100/1152. loss: 0.867, 0.1992 s / batch. (data: 1.46e-02)max mem: 17.22454 GB 
[09/17 11:02:16 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1978, average loss: 1.0052
[09/17 11:02:16 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 57.07	top5: 100.00	
[09/17 11:02:16 visual_prompt]: Best epoch 55: best metric: 0.620
[09/17 11:02:16 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 11:02:31 visual_prompt]: Epoch 56 / 100: avg data time: 2.60e-01, avg batch time: 0.6657, average train loss: 1.1050
[09/17 11:02:39 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1431, average loss: 0.9754
[09/17 11:02:39 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 54.50	top5: 100.00	
[09/17 11:03:02 visual_prompt]: 	Test 100/1152. loss: 0.878, 0.1973 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 11:03:22 visual_prompt]: 	Test 200/1152. loss: 0.916, 0.2077 s / batch. (data: 2.45e-02)max mem: 17.22454 GB 
[09/17 11:03:42 visual_prompt]: 	Test 300/1152. loss: 1.021, 0.1842 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 11:04:01 visual_prompt]: 	Test 400/1152. loss: 1.204, 0.1958 s / batch. (data: 4.24e-05)max mem: 17.22454 GB 
[09/17 11:04:21 visual_prompt]: 	Test 500/1152. loss: 0.994, 0.2315 s / batch. (data: 2.28e-02)max mem: 17.22454 GB 
[09/17 11:04:41 visual_prompt]: 	Test 600/1152. loss: 0.966, 0.1853 s / batch. (data: 1.58e-04)max mem: 17.22454 GB 
[09/17 11:05:01 visual_prompt]: 	Test 700/1152. loss: 1.117, 0.1989 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 11:05:20 visual_prompt]: 	Test 800/1152. loss: 1.209, 0.2231 s / batch. (data: 2.33e-02)max mem: 17.22454 GB 
[09/17 11:05:40 visual_prompt]: 	Test 900/1152. loss: 1.153, 0.1841 s / batch. (data: 1.61e-04)max mem: 17.22454 GB 
[09/17 11:06:00 visual_prompt]: 	Test 1000/1152. loss: 1.230, 0.1961 s / batch. (data: 4.58e-05)max mem: 17.22454 GB 
[09/17 11:06:20 visual_prompt]: 	Test 1100/1152. loss: 1.158, 0.1852 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 11:06:35 visual_prompt]: Inference (test):avg data time: 8.31e-03, avg batch time: 0.1967, average loss: 1.1067
[09/17 11:06:36 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 51.10	top5: 100.00	
[09/17 11:06:36 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 11:06:50 visual_prompt]: Epoch 57 / 100: avg data time: 2.67e-01, avg batch time: 0.6724, average train loss: 1.0499
[09/17 11:06:58 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1430, average loss: 0.8537
[09/17 11:06:58 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 66.00	top5: 100.00	
[09/17 11:07:22 visual_prompt]: 	Test 100/1152. loss: 1.027, 0.1845 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 11:07:42 visual_prompt]: 	Test 200/1152. loss: 0.920, 0.1847 s / batch. (data: 1.04e-04)max mem: 17.22454 GB 
[09/17 11:08:01 visual_prompt]: 	Test 300/1152. loss: 0.936, 0.1966 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 11:08:21 visual_prompt]: 	Test 400/1152. loss: 1.070, 0.1988 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 11:08:41 visual_prompt]: 	Test 500/1152. loss: 0.950, 0.2280 s / batch. (data: 3.81e-02)max mem: 17.22454 GB 
[09/17 11:09:01 visual_prompt]: 	Test 600/1152. loss: 0.836, 0.2606 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 11:09:20 visual_prompt]: 	Test 700/1152. loss: 1.150, 0.2116 s / batch. (data: 2.78e-02)max mem: 17.22454 GB 
[09/17 11:09:40 visual_prompt]: 	Test 800/1152. loss: 0.974, 0.1926 s / batch. (data: 4.17e-05)max mem: 17.22454 GB 
[09/17 11:10:00 visual_prompt]: 	Test 900/1152. loss: 1.051, 0.1985 s / batch. (data: 1.37e-02)max mem: 17.22454 GB 
[09/17 11:10:20 visual_prompt]: 	Test 1000/1152. loss: 1.101, 0.1850 s / batch. (data: 1.52e-04)max mem: 17.22454 GB 
[09/17 11:10:40 visual_prompt]: 	Test 1100/1152. loss: 0.837, 0.2086 s / batch. (data: 3.98e-05)max mem: 17.22454 GB 
[09/17 11:10:55 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1970, average loss: 0.9700
[09/17 11:10:55 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 57.16	top5: 99.98	
[09/17 11:10:55 visual_prompt]: Best epoch 57: best metric: 0.660
[09/17 11:10:55 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 11:11:09 visual_prompt]: Epoch 58 / 100: avg data time: 2.65e-01, avg batch time: 0.6684, average train loss: 0.8961
[09/17 11:11:18 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1430, average loss: 0.6621
[09/17 11:11:18 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 78.00	top5: 100.00	
[09/17 11:11:41 visual_prompt]: 	Test 100/1152. loss: 0.813, 0.2113 s / batch. (data: 6.08e-05)max mem: 17.22454 GB 
[09/17 11:12:01 visual_prompt]: 	Test 200/1152. loss: 0.794, 0.2014 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 11:12:21 visual_prompt]: 	Test 300/1152. loss: 0.852, 0.1844 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 11:12:40 visual_prompt]: 	Test 400/1152. loss: 0.832, 0.1839 s / batch. (data: 1.06e-04)max mem: 17.22454 GB 
[09/17 11:13:00 visual_prompt]: 	Test 500/1152. loss: 0.807, 0.2237 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 11:13:20 visual_prompt]: 	Test 600/1152. loss: 0.932, 0.1967 s / batch. (data: 1.75e-04)max mem: 17.22454 GB 
[09/17 11:13:40 visual_prompt]: 	Test 700/1152. loss: 0.933, 0.2322 s / batch. (data: 3.96e-02)max mem: 17.22454 GB 
[09/17 11:14:00 visual_prompt]: 	Test 800/1152. loss: 0.853, 0.2219 s / batch. (data: 3.85e-02)max mem: 17.22454 GB 
[09/17 11:14:19 visual_prompt]: 	Test 900/1152. loss: 1.021, 0.2054 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 11:14:39 visual_prompt]: 	Test 1000/1152. loss: 1.114, 0.1985 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 11:14:59 visual_prompt]: 	Test 1100/1152. loss: 0.883, 0.1959 s / batch. (data: 1.20e-02)max mem: 17.22454 GB 
[09/17 11:15:14 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1970, average loss: 0.8386
[09/17 11:15:14 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 64.16	top5: 100.00	
[09/17 11:15:14 visual_prompt]: Best epoch 58: best metric: 0.780
[09/17 11:15:14 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 11:15:29 visual_prompt]: Epoch 59 / 100: avg data time: 2.64e-01, avg batch time: 0.6675, average train loss: 0.9092
[09/17 11:15:37 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1431, average loss: 0.6924
[09/17 11:15:37 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 69.50	top5: 100.00	
[09/17 11:16:01 visual_prompt]: 	Test 100/1152. loss: 0.767, 0.2078 s / batch. (data: 2.48e-02)max mem: 17.22454 GB 
[09/17 11:16:21 visual_prompt]: 	Test 200/1152. loss: 0.732, 0.1840 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 11:16:40 visual_prompt]: 	Test 300/1152. loss: 0.709, 0.1958 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 11:17:00 visual_prompt]: 	Test 400/1152. loss: 0.830, 0.1998 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 11:17:20 visual_prompt]: 	Test 500/1152. loss: 1.037, 0.1998 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 11:17:40 visual_prompt]: 	Test 600/1152. loss: 0.934, 0.2263 s / batch. (data: 4.29e-02)max mem: 17.22454 GB 
[09/17 11:18:00 visual_prompt]: 	Test 700/1152. loss: 0.915, 0.1846 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 11:18:20 visual_prompt]: 	Test 800/1152. loss: 0.651, 0.1984 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 11:18:39 visual_prompt]: 	Test 900/1152. loss: 0.866, 0.1998 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 11:18:59 visual_prompt]: 	Test 1000/1152. loss: 0.858, 0.1887 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 11:19:19 visual_prompt]: 	Test 1100/1152. loss: 0.763, 0.2199 s / batch. (data: 3.63e-02)max mem: 17.22454 GB 
[09/17 11:19:34 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1967, average loss: 0.8261
[09/17 11:19:34 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 63.22	top5: 100.00	
[09/17 11:19:34 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 11:19:49 visual_prompt]: Epoch 60 / 100: avg data time: 2.64e-01, avg batch time: 0.6683, average train loss: 0.8130
[09/17 11:19:57 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1430, average loss: 1.0245
[09/17 11:19:57 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 61.50	top5: 100.00	
[09/17 11:20:21 visual_prompt]: 	Test 100/1152. loss: 0.860, 0.1961 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 11:20:41 visual_prompt]: 	Test 200/1152. loss: 1.116, 0.1922 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 11:21:00 visual_prompt]: 	Test 300/1152. loss: 0.931, 0.1904 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 11:21:20 visual_prompt]: 	Test 400/1152. loss: 1.191, 0.1852 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 11:21:40 visual_prompt]: 	Test 500/1152. loss: 1.150, 0.1883 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 11:21:59 visual_prompt]: 	Test 600/1152. loss: 1.187, 0.2030 s / batch. (data: 1.90e-02)max mem: 17.22454 GB 
[09/17 11:22:19 visual_prompt]: 	Test 700/1152. loss: 1.119, 0.2064 s / batch. (data: 9.64e-03)max mem: 17.22454 GB 
[09/17 11:22:39 visual_prompt]: 	Test 800/1152. loss: 1.228, 0.2127 s / batch. (data: 2.00e-02)max mem: 17.22454 GB 
[09/17 11:22:58 visual_prompt]: 	Test 900/1152. loss: 1.118, 0.2037 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 11:23:18 visual_prompt]: 	Test 1000/1152. loss: 1.099, 0.1983 s / batch. (data: 1.46e-02)max mem: 17.22454 GB 
[09/17 11:23:38 visual_prompt]: 	Test 1100/1152. loss: 1.413, 0.2001 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 11:23:53 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1964, average loss: 1.1674
[09/17 11:23:54 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 53.20	top5: 99.99	
[09/17 11:23:54 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 11:24:08 visual_prompt]: Epoch 61 / 100: avg data time: 2.58e-01, avg batch time: 0.6622, average train loss: 0.8811
[09/17 11:24:16 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1448, average loss: 0.7243
[09/17 11:24:16 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 63.00	top5: 100.00	
[09/17 11:24:40 visual_prompt]: 	Test 100/1152. loss: 0.869, 0.1994 s / batch. (data: 1.65e-04)max mem: 17.22454 GB 
[09/17 11:24:59 visual_prompt]: 	Test 200/1152. loss: 0.798, 0.1944 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 11:25:19 visual_prompt]: 	Test 300/1152. loss: 0.914, 0.1884 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 11:25:39 visual_prompt]: 	Test 400/1152. loss: 0.859, 0.1840 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 11:25:58 visual_prompt]: 	Test 500/1152. loss: 0.906, 0.1994 s / batch. (data: 1.57e-02)max mem: 17.22454 GB 
[09/17 11:26:18 visual_prompt]: 	Test 600/1152. loss: 0.958, 0.1995 s / batch. (data: 1.56e-04)max mem: 17.22454 GB 
[09/17 11:26:38 visual_prompt]: 	Test 700/1152. loss: 0.922, 0.1917 s / batch. (data: 1.80e-04)max mem: 17.22454 GB 
[09/17 11:26:57 visual_prompt]: 	Test 800/1152. loss: 0.856, 0.1855 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 11:27:17 visual_prompt]: 	Test 900/1152. loss: 0.968, 0.1836 s / batch. (data: 1.16e-04)max mem: 17.22454 GB 
[09/17 11:27:37 visual_prompt]: 	Test 1000/1152. loss: 1.079, 0.1851 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 11:27:57 visual_prompt]: 	Test 1100/1152. loss: 0.745, 0.1844 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 11:28:12 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1960, average loss: 0.8981
[09/17 11:28:12 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 59.37	top5: 100.00	
[09/17 11:28:12 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 11:28:27 visual_prompt]: Epoch 62 / 100: avg data time: 2.72e-01, avg batch time: 0.6972, average train loss: 0.8201
[09/17 11:28:36 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1431, average loss: 0.6661
[09/17 11:28:36 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 69.50	top5: 100.00	
[09/17 11:29:00 visual_prompt]: 	Test 100/1152. loss: 0.880, 0.1852 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 11:29:19 visual_prompt]: 	Test 200/1152. loss: 0.827, 0.1836 s / batch. (data: 9.75e-05)max mem: 17.22454 GB 
[09/17 11:29:39 visual_prompt]: 	Test 300/1152. loss: 0.759, 0.2080 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 11:29:58 visual_prompt]: 	Test 400/1152. loss: 1.034, 0.1926 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 11:30:18 visual_prompt]: 	Test 500/1152. loss: 0.941, 0.1918 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 11:30:38 visual_prompt]: 	Test 600/1152. loss: 0.821, 0.2127 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 11:30:58 visual_prompt]: 	Test 700/1152. loss: 1.055, 0.2008 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 11:31:18 visual_prompt]: 	Test 800/1152. loss: 0.743, 0.1841 s / batch. (data: 1.16e-04)max mem: 17.22454 GB 
[09/17 11:31:38 visual_prompt]: 	Test 900/1152. loss: 0.982, 0.1850 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 11:31:58 visual_prompt]: 	Test 1000/1152. loss: 0.970, 0.1842 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 11:32:17 visual_prompt]: 	Test 1100/1152. loss: 0.845, 0.1958 s / batch. (data: 1.54e-04)max mem: 17.22454 GB 
[09/17 11:32:32 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1967, average loss: 0.9135
[09/17 11:32:32 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 60.43	top5: 100.00	
[09/17 11:32:32 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 11:32:47 visual_prompt]: Epoch 63 / 100: avg data time: 2.67e-01, avg batch time: 0.6700, average train loss: 0.8462
[09/17 11:32:55 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1432, average loss: 0.6670
[09/17 11:32:55 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 72.00	top5: 100.00	
[09/17 11:33:19 visual_prompt]: 	Test 100/1152. loss: 0.881, 0.1838 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 11:33:38 visual_prompt]: 	Test 200/1152. loss: 0.813, 0.1877 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 11:33:58 visual_prompt]: 	Test 300/1152. loss: 1.077, 0.1835 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 11:34:17 visual_prompt]: 	Test 400/1152. loss: 0.857, 0.1888 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 11:34:38 visual_prompt]: 	Test 500/1152. loss: 0.987, 0.1984 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 11:34:58 visual_prompt]: 	Test 600/1152. loss: 0.942, 0.2065 s / batch. (data: 1.09e-02)max mem: 17.22454 GB 
[09/17 11:35:18 visual_prompt]: 	Test 700/1152. loss: 0.858, 0.2119 s / batch. (data: 2.82e-02)max mem: 17.22454 GB 
[09/17 11:35:38 visual_prompt]: 	Test 800/1152. loss: 0.760, 0.2007 s / batch. (data: 7.97e-03)max mem: 17.22454 GB 
[09/17 11:35:57 visual_prompt]: 	Test 900/1152. loss: 0.927, 0.1982 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 11:36:17 visual_prompt]: 	Test 1000/1152. loss: 0.936, 0.2102 s / batch. (data: 2.64e-02)max mem: 17.22454 GB 
[09/17 11:36:37 visual_prompt]: 	Test 1100/1152. loss: 0.760, 0.1997 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 11:36:52 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1973, average loss: 0.9147
[09/17 11:36:52 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 57.98	top5: 99.96	
[09/17 11:36:52 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 11:37:07 visual_prompt]: Epoch 64 / 100: avg data time: 2.77e-01, avg batch time: 0.6807, average train loss: 0.7069
[09/17 11:37:15 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1431, average loss: 0.5461
[09/17 11:37:15 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 80.50	top5: 100.00	
[09/17 11:37:39 visual_prompt]: 	Test 100/1152. loss: 0.762, 0.1841 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 11:37:58 visual_prompt]: 	Test 200/1152. loss: 0.725, 0.1883 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 11:38:18 visual_prompt]: 	Test 300/1152. loss: 0.652, 0.1841 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 11:38:38 visual_prompt]: 	Test 400/1152. loss: 0.751, 0.1839 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 11:38:58 visual_prompt]: 	Test 500/1152. loss: 1.004, 0.2012 s / batch. (data: 3.50e-05)max mem: 17.22454 GB 
[09/17 11:39:18 visual_prompt]: 	Test 600/1152. loss: 0.829, 0.1890 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 11:39:37 visual_prompt]: 	Test 700/1152. loss: 0.748, 0.2260 s / batch. (data: 4.01e-02)max mem: 17.22454 GB 
[09/17 11:39:57 visual_prompt]: 	Test 800/1152. loss: 0.636, 0.1965 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 11:40:17 visual_prompt]: 	Test 900/1152. loss: 0.998, 0.1845 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 11:40:37 visual_prompt]: 	Test 1000/1152. loss: 0.930, 0.1843 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 11:40:56 visual_prompt]: 	Test 1100/1152. loss: 0.740, 0.1879 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 11:41:11 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1964, average loss: 0.7872
[09/17 11:41:12 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 64.76	top5: 100.00	
[09/17 11:41:12 visual_prompt]: Best epoch 64: best metric: 0.805
[09/17 11:41:12 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 11:41:26 visual_prompt]: Epoch 65 / 100: avg data time: 2.72e-01, avg batch time: 0.6758, average train loss: 0.6713
[09/17 11:41:34 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1432, average loss: 0.8009
[09/17 11:41:34 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 66.00	top5: 100.00	
[09/17 11:41:59 visual_prompt]: 	Test 100/1152. loss: 1.019, 0.1842 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 11:42:18 visual_prompt]: 	Test 200/1152. loss: 1.006, 0.1835 s / batch. (data: 5.05e-05)max mem: 17.22454 GB 
[09/17 11:42:38 visual_prompt]: 	Test 300/1152. loss: 1.084, 0.1837 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 11:42:57 visual_prompt]: 	Test 400/1152. loss: 1.013, 0.1941 s / batch. (data: 1.05e-02)max mem: 17.22454 GB 
[09/17 11:43:17 visual_prompt]: 	Test 500/1152. loss: 1.019, 0.1850 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 11:43:37 visual_prompt]: 	Test 600/1152. loss: 0.991, 0.1936 s / batch. (data: 1.03e-02)max mem: 17.22454 GB 
[09/17 11:43:57 visual_prompt]: 	Test 700/1152. loss: 0.967, 0.1969 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 11:44:16 visual_prompt]: 	Test 800/1152. loss: 0.930, 0.1995 s / batch. (data: 1.55e-02)max mem: 17.22454 GB 
[09/17 11:44:36 visual_prompt]: 	Test 900/1152. loss: 1.003, 0.2052 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 11:44:56 visual_prompt]: 	Test 1000/1152. loss: 1.128, 0.1851 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 11:45:15 visual_prompt]: 	Test 1100/1152. loss: 0.807, 0.1904 s / batch. (data: 1.03e-04)max mem: 17.22454 GB 
[09/17 11:45:31 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1967, average loss: 0.9794
[09/17 11:45:32 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 60.67	top5: 100.00	
[09/17 11:45:32 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 11:45:46 visual_prompt]: Epoch 66 / 100: avg data time: 2.59e-01, avg batch time: 0.6673, average train loss: 0.7008
[09/17 11:45:54 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1431, average loss: 0.7165
[09/17 11:45:54 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 68.50	top5: 100.00	
[09/17 11:46:18 visual_prompt]: 	Test 100/1152. loss: 0.982, 0.1831 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 11:46:37 visual_prompt]: 	Test 200/1152. loss: 0.779, 0.1979 s / batch. (data: 9.85e-05)max mem: 17.22454 GB 
[09/17 11:46:57 visual_prompt]: 	Test 300/1152. loss: 0.837, 0.1879 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 11:47:17 visual_prompt]: 	Test 400/1152. loss: 1.051, 0.1918 s / batch. (data: 1.38e-04)max mem: 17.22454 GB 
[09/17 11:47:36 visual_prompt]: 	Test 500/1152. loss: 1.078, 0.1967 s / batch. (data: 4.65e-05)max mem: 17.22454 GB 
[09/17 11:47:57 visual_prompt]: 	Test 600/1152. loss: 0.925, 0.1958 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 11:48:17 visual_prompt]: 	Test 700/1152. loss: 0.885, 0.1843 s / batch. (data: 9.66e-05)max mem: 17.22454 GB 
[09/17 11:48:36 visual_prompt]: 	Test 800/1152. loss: 0.776, 0.1921 s / batch. (data: 8.00e-03)max mem: 17.22454 GB 
[09/17 11:48:56 visual_prompt]: 	Test 900/1152. loss: 0.942, 0.1834 s / batch. (data: 3.55e-05)max mem: 17.22454 GB 
[09/17 11:49:16 visual_prompt]: 	Test 1000/1152. loss: 0.797, 0.1850 s / batch. (data: 1.80e-04)max mem: 17.22454 GB 
[09/17 11:49:36 visual_prompt]: 	Test 1100/1152. loss: 0.786, 0.1858 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 11:49:51 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1976, average loss: 0.8987
[09/17 11:49:52 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 61.28	top5: 100.00	
[09/17 11:49:52 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 11:50:06 visual_prompt]: Epoch 67 / 100: avg data time: 2.70e-01, avg batch time: 0.6741, average train loss: 0.6864
[09/17 11:50:15 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1430, average loss: 0.6281
[09/17 11:50:15 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 70.00	top5: 100.00	
[09/17 11:50:39 visual_prompt]: 	Test 100/1152. loss: 0.728, 0.2259 s / batch. (data: 3.72e-02)max mem: 17.22454 GB 
[09/17 11:50:58 visual_prompt]: 	Test 200/1152. loss: 0.681, 0.1841 s / batch. (data: 1.16e-04)max mem: 17.22454 GB 
[09/17 11:51:18 visual_prompt]: 	Test 300/1152. loss: 0.753, 0.1842 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 11:51:37 visual_prompt]: 	Test 400/1152. loss: 0.737, 0.1840 s / batch. (data: 1.07e-04)max mem: 17.22454 GB 
[09/17 11:51:57 visual_prompt]: 	Test 500/1152. loss: 0.945, 0.1977 s / batch. (data: 1.39e-02)max mem: 17.22454 GB 
[09/17 11:52:17 visual_prompt]: 	Test 600/1152. loss: 0.825, 0.1882 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 11:52:36 visual_prompt]: 	Test 700/1152. loss: 0.808, 0.2164 s / batch. (data: 9.51e-03)max mem: 17.22454 GB 
[09/17 11:52:56 visual_prompt]: 	Test 800/1152. loss: 0.733, 0.1960 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 11:53:16 visual_prompt]: 	Test 900/1152. loss: 0.916, 0.1997 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 11:53:36 visual_prompt]: 	Test 1000/1152. loss: 1.047, 0.1847 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 11:53:56 visual_prompt]: 	Test 1100/1152. loss: 0.766, 0.2027 s / batch. (data: 1.41e-02)max mem: 17.22454 GB 
[09/17 11:54:11 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1967, average loss: 0.8515
[09/17 11:54:12 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 61.59	top5: 100.00	
[09/17 11:54:12 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 11:54:26 visual_prompt]: Epoch 68 / 100: avg data time: 2.71e-01, avg batch time: 0.6729, average train loss: 0.7480
[09/17 11:54:34 visual_prompt]: Inference (val):avg data time: 5.07e-05, avg batch time: 0.1432, average loss: 0.7767
[09/17 11:54:34 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 66.00	top5: 100.00	
[09/17 11:54:58 visual_prompt]: 	Test 100/1152. loss: 0.997, 0.1888 s / batch. (data: 1.65e-04)max mem: 17.22454 GB 
[09/17 11:55:18 visual_prompt]: 	Test 200/1152. loss: 1.017, 0.2320 s / batch. (data: 4.89e-02)max mem: 17.22454 GB 
[09/17 11:55:37 visual_prompt]: 	Test 300/1152. loss: 1.087, 0.1978 s / batch. (data: 1.45e-02)max mem: 17.22454 GB 
[09/17 11:55:57 visual_prompt]: 	Test 400/1152. loss: 0.880, 0.1844 s / batch. (data: 9.99e-05)max mem: 17.22454 GB 
[09/17 11:56:17 visual_prompt]: 	Test 500/1152. loss: 0.978, 0.1833 s / batch. (data: 2.98e-05)max mem: 17.22454 GB 
[09/17 11:56:36 visual_prompt]: 	Test 600/1152. loss: 1.109, 0.1847 s / batch. (data: 1.17e-04)max mem: 17.22454 GB 
[09/17 11:56:56 visual_prompt]: 	Test 700/1152. loss: 1.001, 0.2308 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 11:57:16 visual_prompt]: 	Test 800/1152. loss: 0.852, 0.2038 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 11:57:36 visual_prompt]: 	Test 900/1152. loss: 0.959, 0.1894 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 11:57:56 visual_prompt]: 	Test 1000/1152. loss: 0.919, 0.1839 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 11:58:15 visual_prompt]: 	Test 1100/1152. loss: 0.919, 0.1878 s / batch. (data: 1.64e-04)max mem: 17.22454 GB 
[09/17 11:58:30 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1962, average loss: 0.9717
[09/17 11:58:30 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 59.65	top5: 99.99	
[09/17 11:58:30 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 11:58:44 visual_prompt]: Epoch 69 / 100: avg data time: 2.68e-01, avg batch time: 0.6719, average train loss: 0.7834
[09/17 11:58:53 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1432, average loss: 0.5479
[09/17 11:58:53 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 79.00	top5: 100.00	
[09/17 11:59:16 visual_prompt]: 	Test 100/1152. loss: 0.709, 0.1846 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 11:59:36 visual_prompt]: 	Test 200/1152. loss: 0.762, 0.2333 s / batch. (data: 2.72e-02)max mem: 17.22454 GB 
[09/17 11:59:55 visual_prompt]: 	Test 300/1152. loss: 0.870, 0.1847 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 12:00:15 visual_prompt]: 	Test 400/1152. loss: 0.768, 0.2268 s / batch. (data: 2.26e-02)max mem: 17.22454 GB 
[09/17 12:00:35 visual_prompt]: 	Test 500/1152. loss: 0.822, 0.1951 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 12:00:54 visual_prompt]: 	Test 600/1152. loss: 0.876, 0.1851 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 12:01:14 visual_prompt]: 	Test 700/1152. loss: 0.773, 0.1848 s / batch. (data: 9.39e-05)max mem: 17.22454 GB 
[09/17 12:01:34 visual_prompt]: 	Test 800/1152. loss: 0.698, 0.2073 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 12:01:53 visual_prompt]: 	Test 900/1152. loss: 1.036, 0.1847 s / batch. (data: 1.89e-04)max mem: 17.22454 GB 
[09/17 12:02:13 visual_prompt]: 	Test 1000/1152. loss: 0.790, 0.1850 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 12:02:33 visual_prompt]: 	Test 1100/1152. loss: 0.735, 0.2035 s / batch. (data: 1.06e-04)max mem: 17.22454 GB 
[09/17 12:02:48 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1960, average loss: 0.8163
[09/17 12:02:48 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 62.92	top5: 100.00	
[09/17 12:02:48 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 12:03:03 visual_prompt]: Epoch 70 / 100: avg data time: 2.56e-01, avg batch time: 0.6673, average train loss: 0.7254
[09/17 12:03:11 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1439, average loss: 0.5677
[09/17 12:03:11 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 78.50	top5: 100.00	
[09/17 12:03:35 visual_prompt]: 	Test 100/1152. loss: 0.769, 0.1879 s / batch. (data: 1.21e-04)max mem: 17.22454 GB 
[09/17 12:03:55 visual_prompt]: 	Test 200/1152. loss: 0.791, 0.1918 s / batch. (data: 4.10e-05)max mem: 17.22454 GB 
[09/17 12:04:14 visual_prompt]: 	Test 300/1152. loss: 0.943, 0.2092 s / batch. (data: 2.06e-02)max mem: 17.22454 GB 
[09/17 12:04:34 visual_prompt]: 	Test 400/1152. loss: 0.963, 0.2163 s / batch. (data: 1.17e-02)max mem: 17.22454 GB 
[09/17 12:04:54 visual_prompt]: 	Test 500/1152. loss: 0.706, 0.1966 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 12:05:13 visual_prompt]: 	Test 600/1152. loss: 0.812, 0.1989 s / batch. (data: 1.56e-02)max mem: 17.22454 GB 
[09/17 12:05:33 visual_prompt]: 	Test 700/1152. loss: 0.747, 0.2126 s / batch. (data: 2.24e-02)max mem: 17.22454 GB 
[09/17 12:05:53 visual_prompt]: 	Test 800/1152. loss: 0.799, 0.1953 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 12:06:13 visual_prompt]: 	Test 900/1152. loss: 1.019, 0.1841 s / batch. (data: 1.43e-04)max mem: 17.22454 GB 
[09/17 12:06:32 visual_prompt]: 	Test 1000/1152. loss: 0.944, 0.1840 s / batch. (data: 3.24e-05)max mem: 17.22454 GB 
[09/17 12:06:52 visual_prompt]: 	Test 1100/1152. loss: 0.715, 0.1987 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 12:07:07 visual_prompt]: Inference (test):avg data time: 7.50e-03, avg batch time: 0.1961, average loss: 0.8391
[09/17 12:07:07 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 64.59	top5: 100.00	
[09/17 12:07:07 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 12:07:22 visual_prompt]: Epoch 71 / 100: avg data time: 2.72e-01, avg batch time: 0.7016, average train loss: 0.5921
[09/17 12:07:30 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1432, average loss: 0.4886
[09/17 12:07:30 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 83.50	top5: 100.00	
[09/17 12:07:55 visual_prompt]: 	Test 100/1152. loss: 0.633, 0.2268 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 12:08:14 visual_prompt]: 	Test 200/1152. loss: 0.707, 0.1841 s / batch. (data: 1.21e-04)max mem: 17.22454 GB 
[09/17 12:08:34 visual_prompt]: 	Test 300/1152. loss: 0.792, 0.2156 s / batch. (data: 2.75e-02)max mem: 17.22454 GB 
[09/17 12:08:54 visual_prompt]: 	Test 400/1152. loss: 0.741, 0.1997 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 12:09:13 visual_prompt]: 	Test 500/1152. loss: 0.653, 0.1836 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 12:09:33 visual_prompt]: 	Test 600/1152. loss: 0.804, 0.2127 s / batch. (data: 2.34e-02)max mem: 17.22454 GB 
[09/17 12:09:53 visual_prompt]: 	Test 700/1152. loss: 0.677, 0.1841 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 12:10:13 visual_prompt]: 	Test 800/1152. loss: 0.643, 0.1837 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 12:10:33 visual_prompt]: 	Test 900/1152. loss: 0.812, 0.1837 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 12:10:53 visual_prompt]: 	Test 1000/1152. loss: 0.808, 0.1845 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 12:11:12 visual_prompt]: 	Test 1100/1152. loss: 0.630, 0.1848 s / batch. (data: 1.52e-04)max mem: 17.22454 GB 
[09/17 12:11:27 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1970, average loss: 0.7347
[09/17 12:11:27 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 72.00	top5: 100.00	
[09/17 12:11:27 visual_prompt]: Best epoch 71: best metric: 0.835
[09/17 12:11:27 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 12:11:42 visual_prompt]: Epoch 72 / 100: avg data time: 2.73e-01, avg batch time: 0.6767, average train loss: 0.4916
[09/17 12:11:51 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1432, average loss: 0.4992
[09/17 12:11:51 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 83.00	top5: 100.00	
[09/17 12:12:15 visual_prompt]: 	Test 100/1152. loss: 1.051, 0.1992 s / batch. (data: 8.00e-03)max mem: 17.22454 GB 
[09/17 12:12:34 visual_prompt]: 	Test 200/1152. loss: 0.748, 0.1843 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 12:12:54 visual_prompt]: 	Test 300/1152. loss: 0.890, 0.2073 s / batch. (data: 1.56e-02)max mem: 17.22454 GB 
[09/17 12:13:13 visual_prompt]: 	Test 400/1152. loss: 0.833, 0.2027 s / batch. (data: 1.73e-04)max mem: 17.22454 GB 
[09/17 12:13:33 visual_prompt]: 	Test 500/1152. loss: 0.902, 0.2037 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 12:13:53 visual_prompt]: 	Test 600/1152. loss: 0.893, 0.1988 s / batch. (data: 1.41e-02)max mem: 17.22454 GB 
[09/17 12:14:13 visual_prompt]: 	Test 700/1152. loss: 0.880, 0.1846 s / batch. (data: 1.56e-04)max mem: 17.22454 GB 
[09/17 12:14:33 visual_prompt]: 	Test 800/1152. loss: 0.886, 0.1964 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 12:14:53 visual_prompt]: 	Test 900/1152. loss: 0.815, 0.1863 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 12:15:12 visual_prompt]: 	Test 1000/1152. loss: 0.748, 0.1849 s / batch. (data: 2.06e-04)max mem: 17.22454 GB 
[09/17 12:15:32 visual_prompt]: 	Test 1100/1152. loss: 0.766, 0.1999 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 12:15:47 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1967, average loss: 0.8856
[09/17 12:15:47 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 65.41	top5: 100.00	
[09/17 12:15:47 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 12:16:02 visual_prompt]: Epoch 73 / 100: avg data time: 2.65e-01, avg batch time: 0.6688, average train loss: 0.6067
[09/17 12:16:10 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1431, average loss: 0.6373
[09/17 12:16:10 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 73.50	top5: 100.00	
[09/17 12:16:34 visual_prompt]: 	Test 100/1152. loss: 0.970, 0.1839 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 12:16:54 visual_prompt]: 	Test 200/1152. loss: 0.880, 0.1840 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 12:17:13 visual_prompt]: 	Test 300/1152. loss: 0.990, 0.1862 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 12:17:33 visual_prompt]: 	Test 400/1152. loss: 0.905, 0.1986 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 12:17:53 visual_prompt]: 	Test 500/1152. loss: 0.979, 0.2022 s / batch. (data: 1.87e-02)max mem: 17.22454 GB 
[09/17 12:18:13 visual_prompt]: 	Test 600/1152. loss: 1.025, 0.1842 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 12:18:32 visual_prompt]: 	Test 700/1152. loss: 0.843, 0.2259 s / batch. (data: 4.26e-02)max mem: 17.22454 GB 
[09/17 12:18:52 visual_prompt]: 	Test 800/1152. loss: 0.936, 0.1966 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 12:19:12 visual_prompt]: 	Test 900/1152. loss: 0.996, 0.2077 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 12:19:32 visual_prompt]: 	Test 1000/1152. loss: 0.941, 0.1844 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 12:19:52 visual_prompt]: 	Test 1100/1152. loss: 0.770, 0.1857 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 12:20:07 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1964, average loss: 0.9465
[09/17 12:20:07 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 62.25	top5: 99.99	
[09/17 12:20:07 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 12:20:21 visual_prompt]: Epoch 74 / 100: avg data time: 2.73e-01, avg batch time: 0.6770, average train loss: 0.5992
[09/17 12:20:30 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1432, average loss: 0.4744
[09/17 12:20:30 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 81.50	top5: 100.00	
[09/17 12:20:53 visual_prompt]: 	Test 100/1152. loss: 0.823, 0.1837 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 12:21:13 visual_prompt]: 	Test 200/1152. loss: 0.737, 0.1841 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 12:21:32 visual_prompt]: 	Test 300/1152. loss: 0.846, 0.1857 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 12:21:52 visual_prompt]: 	Test 400/1152. loss: 0.741, 0.2113 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 12:22:12 visual_prompt]: 	Test 500/1152. loss: 0.894, 0.1839 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 12:22:31 visual_prompt]: 	Test 600/1152. loss: 0.784, 0.2128 s / batch. (data: 1.62e-02)max mem: 17.22454 GB 
[09/17 12:22:51 visual_prompt]: 	Test 700/1152. loss: 0.642, 0.2071 s / batch. (data: 2.23e-02)max mem: 17.22454 GB 
[09/17 12:23:11 visual_prompt]: 	Test 800/1152. loss: 0.649, 0.1837 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 12:23:31 visual_prompt]: 	Test 900/1152. loss: 0.812, 0.1962 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 12:23:50 visual_prompt]: 	Test 1000/1152. loss: 0.698, 0.2192 s / batch. (data: 1.45e-02)max mem: 17.22454 GB 
[09/17 12:24:10 visual_prompt]: 	Test 1100/1152. loss: 0.740, 0.2241 s / batch. (data: 4.09e-02)max mem: 17.22454 GB 
[09/17 12:24:25 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1958, average loss: 0.8087
[09/17 12:24:25 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 64.76	top5: 99.99	
[09/17 12:24:25 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 12:24:39 visual_prompt]: Epoch 75 / 100: avg data time: 2.66e-01, avg batch time: 0.6707, average train loss: 0.5227
[09/17 12:24:48 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1442, average loss: 0.5945
[09/17 12:24:48 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 78.50	top5: 100.00	
[09/17 12:25:11 visual_prompt]: 	Test 100/1152. loss: 1.122, 0.1839 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 12:25:31 visual_prompt]: 	Test 200/1152. loss: 1.102, 0.2070 s / batch. (data: 2.42e-02)max mem: 17.22454 GB 
[09/17 12:25:50 visual_prompt]: 	Test 300/1152. loss: 0.765, 0.1838 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 12:26:10 visual_prompt]: 	Test 400/1152. loss: 0.912, 0.2036 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 12:26:30 visual_prompt]: 	Test 500/1152. loss: 0.979, 0.1939 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 12:26:50 visual_prompt]: 	Test 600/1152. loss: 0.909, 0.2006 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 12:27:10 visual_prompt]: 	Test 700/1152. loss: 0.837, 0.2039 s / batch. (data: 1.13e-02)max mem: 17.22454 GB 
[09/17 12:27:30 visual_prompt]: 	Test 800/1152. loss: 0.824, 0.1998 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 12:27:50 visual_prompt]: 	Test 900/1152. loss: 0.923, 0.2043 s / batch. (data: 1.07e-04)max mem: 17.22454 GB 
[09/17 12:28:09 visual_prompt]: 	Test 1000/1152. loss: 0.902, 0.1868 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 12:28:29 visual_prompt]: 	Test 1100/1152. loss: 0.785, 0.1840 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 12:28:44 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1965, average loss: 0.9504
[09/17 12:28:44 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 63.12	top5: 99.93	
[09/17 12:28:44 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 12:28:59 visual_prompt]: Epoch 76 / 100: avg data time: 2.66e-01, avg batch time: 0.6699, average train loss: 0.4450
[09/17 12:29:07 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1431, average loss: 0.3107
[09/17 12:29:07 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 89.50	top5: 100.00	
[09/17 12:29:31 visual_prompt]: 	Test 100/1152. loss: 0.799, 0.1837 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 12:29:50 visual_prompt]: 	Test 200/1152. loss: 0.538, 0.1835 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 12:30:10 visual_prompt]: 	Test 300/1152. loss: 0.489, 0.1835 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 12:30:29 visual_prompt]: 	Test 400/1152. loss: 0.630, 0.2038 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 12:30:49 visual_prompt]: 	Test 500/1152. loss: 0.685, 0.1948 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 12:31:09 visual_prompt]: 	Test 600/1152. loss: 0.692, 0.1970 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 12:31:29 visual_prompt]: 	Test 700/1152. loss: 0.681, 0.1997 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 12:31:48 visual_prompt]: 	Test 800/1152. loss: 0.561, 0.1843 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 12:32:08 visual_prompt]: 	Test 900/1152. loss: 0.738, 0.1847 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 12:32:28 visual_prompt]: 	Test 1000/1152. loss: 0.753, 0.1994 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 12:32:48 visual_prompt]: 	Test 1100/1152. loss: 0.606, 0.1993 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 12:33:03 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1964, average loss: 0.6956
[09/17 12:33:03 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 72.43	top5: 100.00	
[09/17 12:33:03 visual_prompt]: Best epoch 76: best metric: 0.895
[09/17 12:33:03 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 12:33:18 visual_prompt]: Epoch 77 / 100: avg data time: 2.75e-01, avg batch time: 0.6770, average train loss: 0.4410
[09/17 12:33:26 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1432, average loss: 0.3503
[09/17 12:33:26 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 88.50	top5: 100.00	
[09/17 12:33:50 visual_prompt]: 	Test 100/1152. loss: 0.688, 0.1831 s / batch. (data: 3.81e-05)max mem: 17.22454 GB 
[09/17 12:34:10 visual_prompt]: 	Test 200/1152. loss: 0.511, 0.1982 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 12:34:29 visual_prompt]: 	Test 300/1152. loss: 0.365, 0.1851 s / batch. (data: 1.01e-04)max mem: 17.22454 GB 
[09/17 12:34:49 visual_prompt]: 	Test 400/1152. loss: 0.791, 0.1846 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 12:35:09 visual_prompt]: 	Test 500/1152. loss: 0.735, 0.2245 s / batch. (data: 3.11e-02)max mem: 17.22454 GB 
[09/17 12:35:28 visual_prompt]: 	Test 600/1152. loss: 0.652, 0.1851 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 12:35:48 visual_prompt]: 	Test 700/1152. loss: 0.690, 0.1975 s / batch. (data: 1.38e-02)max mem: 17.22454 GB 
[09/17 12:36:08 visual_prompt]: 	Test 800/1152. loss: 0.713, 0.2295 s / batch. (data: 4.59e-02)max mem: 17.22454 GB 
[09/17 12:36:28 visual_prompt]: 	Test 900/1152. loss: 0.802, 0.2074 s / batch. (data: 1.88e-02)max mem: 17.22454 GB 
[09/17 12:36:47 visual_prompt]: 	Test 1000/1152. loss: 0.743, 0.1993 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 12:37:08 visual_prompt]: 	Test 1100/1152. loss: 0.686, 0.1909 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 12:37:23 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1963, average loss: 0.7374
[09/17 12:37:23 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 72.58	top5: 100.00	
[09/17 12:37:23 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 12:37:37 visual_prompt]: Epoch 78 / 100: avg data time: 2.70e-01, avg batch time: 0.6774, average train loss: 0.4611
[09/17 12:37:45 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1431, average loss: 0.6674
[09/17 12:37:45 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 71.00	top5: 100.00	
[09/17 12:38:09 visual_prompt]: 	Test 100/1152. loss: 0.996, 0.1960 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 12:38:29 visual_prompt]: 	Test 200/1152. loss: 0.761, 0.2033 s / batch. (data: 2.00e-02)max mem: 17.22454 GB 
[09/17 12:38:48 visual_prompt]: 	Test 300/1152. loss: 0.725, 0.1841 s / batch. (data: 1.47e-04)max mem: 17.22454 GB 
[09/17 12:39:08 visual_prompt]: 	Test 400/1152. loss: 0.813, 0.2075 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 12:39:28 visual_prompt]: 	Test 500/1152. loss: 1.119, 0.2219 s / batch. (data: 1.50e-02)max mem: 17.22454 GB 
[09/17 12:39:48 visual_prompt]: 	Test 600/1152. loss: 1.260, 0.1852 s / batch. (data: 1.27e-04)max mem: 17.22454 GB 
[09/17 12:40:07 visual_prompt]: 	Test 700/1152. loss: 0.989, 0.1850 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 12:40:27 visual_prompt]: 	Test 800/1152. loss: 0.695, 0.1960 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 12:40:47 visual_prompt]: 	Test 900/1152. loss: 1.109, 0.2014 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 12:41:07 visual_prompt]: 	Test 1000/1152. loss: 0.995, 0.2070 s / batch. (data: 1.41e-02)max mem: 17.22454 GB 
[09/17 12:41:26 visual_prompt]: 	Test 1100/1152. loss: 0.981, 0.1844 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 12:41:41 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1960, average loss: 0.9618
[09/17 12:41:41 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 63.73	top5: 100.00	
[09/17 12:41:41 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 12:41:56 visual_prompt]: Epoch 79 / 100: avg data time: 2.75e-01, avg batch time: 0.6784, average train loss: 0.5326
[09/17 12:42:04 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1431, average loss: 0.3363
[09/17 12:42:04 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 88.00	top5: 100.00	
[09/17 12:42:28 visual_prompt]: 	Test 100/1152. loss: 0.751, 0.1831 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 12:42:47 visual_prompt]: 	Test 200/1152. loss: 0.710, 0.1920 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 12:43:07 visual_prompt]: 	Test 300/1152. loss: 0.750, 0.1840 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 12:43:26 visual_prompt]: 	Test 400/1152. loss: 0.696, 0.1845 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 12:43:46 visual_prompt]: 	Test 500/1152. loss: 0.706, 0.2239 s / batch. (data: 1.15e-04)max mem: 17.22454 GB 
[09/17 12:44:06 visual_prompt]: 	Test 600/1152. loss: 0.907, 0.1998 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 12:44:26 visual_prompt]: 	Test 700/1152. loss: 0.669, 0.1843 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 12:44:46 visual_prompt]: 	Test 800/1152. loss: 0.736, 0.2038 s / batch. (data: 1.98e-02)max mem: 17.22454 GB 
[09/17 12:45:06 visual_prompt]: 	Test 900/1152. loss: 1.012, 0.1949 s / batch. (data: 1.33e-04)max mem: 17.22454 GB 
[09/17 12:45:25 visual_prompt]: 	Test 1000/1152. loss: 0.720, 0.1920 s / batch. (data: 1.30e-04)max mem: 17.22454 GB 
[09/17 12:45:45 visual_prompt]: 	Test 1100/1152. loss: 0.690, 0.1918 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 12:46:00 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1962, average loss: 0.7895
[09/17 12:46:01 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 69.16	top5: 100.00	
[09/17 12:46:01 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 12:46:15 visual_prompt]: Epoch 80 / 100: avg data time: 2.65e-01, avg batch time: 0.6698, average train loss: 0.3598
[09/17 12:46:23 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1448, average loss: 0.3196
[09/17 12:46:23 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 88.00	top5: 100.00	
[09/17 12:46:47 visual_prompt]: 	Test 100/1152. loss: 0.829, 0.2096 s / batch. (data: 2.45e-02)max mem: 17.22454 GB 
[09/17 12:47:07 visual_prompt]: 	Test 200/1152. loss: 0.568, 0.2038 s / batch. (data: 2.08e-02)max mem: 17.22454 GB 
[09/17 12:47:26 visual_prompt]: 	Test 300/1152. loss: 0.705, 0.2085 s / batch. (data: 2.49e-02)max mem: 17.22454 GB 
[09/17 12:47:46 visual_prompt]: 	Test 400/1152. loss: 0.868, 0.2038 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 12:48:06 visual_prompt]: 	Test 500/1152. loss: 0.602, 0.1841 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 12:48:26 visual_prompt]: 	Test 600/1152. loss: 0.759, 0.1913 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 12:48:45 visual_prompt]: 	Test 700/1152. loss: 0.829, 0.2071 s / batch. (data: 1.45e-02)max mem: 17.22454 GB 
[09/17 12:49:05 visual_prompt]: 	Test 800/1152. loss: 0.729, 0.1913 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 12:49:25 visual_prompt]: 	Test 900/1152. loss: 1.089, 0.1938 s / batch. (data: 1.02e-02)max mem: 17.22454 GB 
[09/17 12:49:45 visual_prompt]: 	Test 1000/1152. loss: 0.767, 0.1910 s / batch. (data: 9.63e-05)max mem: 17.22454 GB 
[09/17 12:50:05 visual_prompt]: 	Test 1100/1152. loss: 0.524, 0.1993 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 12:50:20 visual_prompt]: Inference (test):avg data time: 8.03e-03, avg batch time: 0.1970, average loss: 0.7877
[09/17 12:50:20 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 71.31	top5: 100.00	
[09/17 12:50:20 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 12:50:35 visual_prompt]: Epoch 81 / 100: avg data time: 2.73e-01, avg batch time: 0.6754, average train loss: 0.3719
[09/17 12:50:43 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1430, average loss: 0.3117
[09/17 12:50:43 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 90.00	top5: 100.00	
[09/17 12:51:07 visual_prompt]: 	Test 100/1152. loss: 0.813, 0.1983 s / batch. (data: 1.55e-02)max mem: 17.22454 GB 
[09/17 12:51:27 visual_prompt]: 	Test 200/1152. loss: 0.623, 0.1903 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 12:51:46 visual_prompt]: 	Test 300/1152. loss: 0.788, 0.1962 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 12:52:06 visual_prompt]: 	Test 400/1152. loss: 0.793, 0.1842 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 12:52:25 visual_prompt]: 	Test 500/1152. loss: 0.746, 0.1850 s / batch. (data: 1.01e-04)max mem: 17.22454 GB 
[09/17 12:52:45 visual_prompt]: 	Test 600/1152. loss: 0.622, 0.1979 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 12:53:05 visual_prompt]: 	Test 700/1152. loss: 0.761, 0.1872 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 12:53:25 visual_prompt]: 	Test 800/1152. loss: 0.828, 0.1998 s / batch. (data: 3.93e-05)max mem: 17.22454 GB 
[09/17 12:53:44 visual_prompt]: 	Test 900/1152. loss: 0.996, 0.1846 s / batch. (data: 1.15e-04)max mem: 17.22454 GB 
[09/17 12:54:04 visual_prompt]: 	Test 1000/1152. loss: 0.930, 0.2060 s / batch. (data: 1.33e-02)max mem: 17.22454 GB 
[09/17 12:54:24 visual_prompt]: 	Test 1100/1152. loss: 0.624, 0.1840 s / batch. (data: 1.37e-04)max mem: 17.22454 GB 
[09/17 12:54:39 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1964, average loss: 0.8209
[09/17 12:54:39 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 69.64	top5: 100.00	
[09/17 12:54:39 visual_prompt]: Best epoch 81: best metric: 0.900
[09/17 12:54:39 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 12:54:53 visual_prompt]: Epoch 82 / 100: avg data time: 2.67e-01, avg batch time: 0.6705, average train loss: 0.3227
[09/17 12:55:02 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1433, average loss: 0.2603
[09/17 12:55:02 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 92.00	top5: 100.00	
[09/17 12:55:25 visual_prompt]: 	Test 100/1152. loss: 0.735, 0.1973 s / batch. (data: 1.46e-02)max mem: 17.22454 GB 
[09/17 12:55:45 visual_prompt]: 	Test 200/1152. loss: 0.611, 0.1839 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 12:56:05 visual_prompt]: 	Test 300/1152. loss: 0.759, 0.1960 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 12:56:24 visual_prompt]: 	Test 400/1152. loss: 0.688, 0.1914 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 12:56:44 visual_prompt]: 	Test 500/1152. loss: 0.651, 0.1840 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 12:57:04 visual_prompt]: 	Test 600/1152. loss: 0.823, 0.1850 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 12:57:24 visual_prompt]: 	Test 700/1152. loss: 0.748, 0.2028 s / batch. (data: 1.90e-02)max mem: 17.22454 GB 
[09/17 12:57:44 visual_prompt]: 	Test 800/1152. loss: 0.807, 0.1847 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 12:58:03 visual_prompt]: 	Test 900/1152. loss: 0.909, 0.1841 s / batch. (data: 1.04e-04)max mem: 17.22454 GB 
[09/17 12:58:23 visual_prompt]: 	Test 1000/1152. loss: 0.794, 0.1843 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 12:58:43 visual_prompt]: 	Test 1100/1152. loss: 0.569, 0.1847 s / batch. (data: 4.51e-05)max mem: 17.22454 GB 
[09/17 12:58:58 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1969, average loss: 0.7817
[09/17 12:58:58 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 71.11	top5: 100.00	
[09/17 12:58:58 visual_prompt]: Best epoch 82: best metric: 0.920
[09/17 12:58:58 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 12:59:13 visual_prompt]: Epoch 83 / 100: avg data time: 2.69e-01, avg batch time: 0.6711, average train loss: 0.2819
[09/17 12:59:21 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1433, average loss: 0.2800
[09/17 12:59:21 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 89.00	top5: 100.00	
[09/17 12:59:45 visual_prompt]: 	Test 100/1152. loss: 0.733, 0.1960 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 13:00:04 visual_prompt]: 	Test 200/1152. loss: 0.826, 0.1836 s / batch. (data: 3.31e-05)max mem: 17.22454 GB 
[09/17 13:00:24 visual_prompt]: 	Test 300/1152. loss: 0.773, 0.1837 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 13:00:43 visual_prompt]: 	Test 400/1152. loss: 0.669, 0.2070 s / batch. (data: 1.42e-02)max mem: 17.22454 GB 
[09/17 13:01:03 visual_prompt]: 	Test 500/1152. loss: 0.763, 0.2087 s / batch. (data: 2.39e-02)max mem: 17.22454 GB 
[09/17 13:01:23 visual_prompt]: 	Test 600/1152. loss: 1.231, 0.2041 s / batch. (data: 1.54e-02)max mem: 17.22454 GB 
[09/17 13:01:43 visual_prompt]: 	Test 700/1152. loss: 0.834, 0.1991 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 13:02:03 visual_prompt]: 	Test 800/1152. loss: 0.843, 0.1984 s / batch. (data: 1.73e-04)max mem: 17.22454 GB 
[09/17 13:02:22 visual_prompt]: 	Test 900/1152. loss: 1.100, 0.1998 s / batch. (data: 1.45e-04)max mem: 17.22454 GB 
[09/17 13:02:42 visual_prompt]: 	Test 1000/1152. loss: 0.756, 0.2200 s / batch. (data: 1.13e-02)max mem: 17.22454 GB 
[09/17 13:03:02 visual_prompt]: 	Test 1100/1152. loss: 0.754, 0.1860 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 13:03:17 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1963, average loss: 0.8898
[09/17 13:03:17 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 68.94	top5: 99.99	
[09/17 13:03:17 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 13:03:32 visual_prompt]: Epoch 84 / 100: avg data time: 2.65e-01, avg batch time: 0.6706, average train loss: 0.3114
[09/17 13:03:40 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1431, average loss: 0.1888
[09/17 13:03:40 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 94.50	top5: 100.00	
[09/17 13:04:03 visual_prompt]: 	Test 100/1152. loss: 0.603, 0.2078 s / batch. (data: 2.52e-02)max mem: 17.22454 GB 
[09/17 13:04:23 visual_prompt]: 	Test 200/1152. loss: 0.532, 0.1844 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 13:04:42 visual_prompt]: 	Test 300/1152. loss: 0.548, 0.1852 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 13:05:02 visual_prompt]: 	Test 400/1152. loss: 0.771, 0.2095 s / batch. (data: 1.68e-02)max mem: 17.22454 GB 
[09/17 13:05:22 visual_prompt]: 	Test 500/1152. loss: 0.535, 0.1864 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 13:05:42 visual_prompt]: 	Test 600/1152. loss: 0.654, 0.2242 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 13:06:01 visual_prompt]: 	Test 700/1152. loss: 0.647, 0.1985 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 13:06:21 visual_prompt]: 	Test 800/1152. loss: 0.812, 0.1961 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 13:06:41 visual_prompt]: 	Test 900/1152. loss: 0.911, 0.2032 s / batch. (data: 1.33e-02)max mem: 17.22454 GB 
[09/17 13:07:01 visual_prompt]: 	Test 1000/1152. loss: 0.784, 0.1859 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 13:07:20 visual_prompt]: 	Test 1100/1152. loss: 0.687, 0.2079 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 13:07:35 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1963, average loss: 0.7393
[09/17 13:07:36 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 74.11	top5: 99.99	
[09/17 13:07:36 visual_prompt]: Best epoch 84: best metric: 0.945
[09/17 13:07:36 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 13:07:50 visual_prompt]: Epoch 85 / 100: avg data time: 2.67e-01, avg batch time: 0.6701, average train loss: 0.2225
[09/17 13:07:59 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1431, average loss: 0.1564
[09/17 13:07:59 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 96.50	top5: 100.00	
[09/17 13:08:22 visual_prompt]: 	Test 100/1152. loss: 0.795, 0.1962 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 13:08:42 visual_prompt]: 	Test 200/1152. loss: 0.485, 0.1890 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 13:09:01 visual_prompt]: 	Test 300/1152. loss: 0.587, 0.1958 s / batch. (data: 1.23e-02)max mem: 17.22454 GB 
[09/17 13:09:21 visual_prompt]: 	Test 400/1152. loss: 0.721, 0.2023 s / batch. (data: 1.89e-02)max mem: 17.22454 GB 
[09/17 13:09:41 visual_prompt]: 	Test 500/1152. loss: 0.637, 0.2038 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 13:10:01 visual_prompt]: 	Test 600/1152. loss: 0.821, 0.2095 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 13:10:21 visual_prompt]: 	Test 700/1152. loss: 0.552, 0.1971 s / batch. (data: 1.11e-04)max mem: 17.22454 GB 
[09/17 13:10:40 visual_prompt]: 	Test 800/1152. loss: 0.716, 0.2135 s / batch. (data: 2.68e-02)max mem: 17.22454 GB 
[09/17 13:11:00 visual_prompt]: 	Test 900/1152. loss: 0.901, 0.1853 s / batch. (data: 1.42e-04)max mem: 17.22454 GB 
[09/17 13:11:20 visual_prompt]: 	Test 1000/1152. loss: 0.577, 0.1987 s / batch. (data: 1.37e-02)max mem: 17.22454 GB 
[09/17 13:11:39 visual_prompt]: 	Test 1100/1152. loss: 0.563, 0.1918 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 13:11:54 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1963, average loss: 0.7536
[09/17 13:11:54 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 73.85	top5: 99.99	
[09/17 13:11:54 visual_prompt]: Best epoch 85: best metric: 0.965
[09/17 13:11:54 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/17 13:12:09 visual_prompt]: Epoch 86 / 100: avg data time: 2.69e-01, avg batch time: 0.6729, average train loss: 0.1909
[09/17 13:12:17 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1432, average loss: 0.1106
[09/17 13:12:17 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 97.00	top5: 100.00	
[09/17 13:12:41 visual_prompt]: 	Test 100/1152. loss: 0.749, 0.1999 s / batch. (data: 1.41e-02)max mem: 17.22454 GB 
[09/17 13:13:01 visual_prompt]: 	Test 200/1152. loss: 0.418, 0.1841 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 13:13:21 visual_prompt]: 	Test 300/1152. loss: 0.462, 0.2119 s / batch. (data: 2.78e-02)max mem: 17.22454 GB 
[09/17 13:13:41 visual_prompt]: 	Test 400/1152. loss: 0.693, 0.1844 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 13:14:00 visual_prompt]: 	Test 500/1152. loss: 0.553, 0.1846 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 13:14:21 visual_prompt]: 	Test 600/1152. loss: 0.758, 0.1970 s / batch. (data: 1.34e-02)max mem: 17.22454 GB 
[09/17 13:14:40 visual_prompt]: 	Test 700/1152. loss: 0.535, 0.1839 s / batch. (data: 1.52e-04)max mem: 17.22454 GB 
[09/17 13:15:00 visual_prompt]: 	Test 800/1152. loss: 0.521, 0.1989 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 13:15:20 visual_prompt]: 	Test 900/1152. loss: 0.847, 0.1955 s / batch. (data: 1.08e-02)max mem: 17.22454 GB 
[09/17 13:15:41 visual_prompt]: 	Test 1000/1152. loss: 0.674, 0.1835 s / batch. (data: 4.53e-05)max mem: 17.22454 GB 
[09/17 13:16:01 visual_prompt]: 	Test 1100/1152. loss: 0.541, 0.3331 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 13:16:16 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1984, average loss: 0.6801
[09/17 13:16:16 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.95	top5: 100.00	
[09/17 13:16:16 visual_prompt]: Best epoch 86: best metric: 0.970
[09/17 13:16:16 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/17 13:16:30 visual_prompt]: Epoch 87 / 100: avg data time: 2.71e-01, avg batch time: 0.6753, average train loss: 0.1392
[09/17 13:16:39 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1434, average loss: 0.1231
[09/17 13:16:39 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 95.50	top5: 100.00	
[09/17 13:17:02 visual_prompt]: 	Test 100/1152. loss: 0.830, 0.1842 s / batch. (data: 1.55e-04)max mem: 17.22454 GB 
[09/17 13:17:22 visual_prompt]: 	Test 200/1152. loss: 0.721, 0.2023 s / batch. (data: 1.92e-02)max mem: 17.22454 GB 
[09/17 13:17:42 visual_prompt]: 	Test 300/1152. loss: 0.571, 0.1838 s / batch. (data: 1.56e-04)max mem: 17.22454 GB 
[09/17 13:18:02 visual_prompt]: 	Test 400/1152. loss: 0.760, 0.2224 s / batch. (data: 3.39e-05)max mem: 17.22454 GB 
[09/17 13:18:22 visual_prompt]: 	Test 500/1152. loss: 0.526, 0.2077 s / batch. (data: 2.40e-02)max mem: 17.22454 GB 
[09/17 13:18:42 visual_prompt]: 	Test 600/1152. loss: 0.894, 0.1988 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 13:19:01 visual_prompt]: 	Test 700/1152. loss: 0.585, 0.2000 s / batch. (data: 3.79e-05)max mem: 17.22454 GB 
[09/17 13:19:21 visual_prompt]: 	Test 800/1152. loss: 0.694, 0.2020 s / batch. (data: 1.33e-02)max mem: 17.22454 GB 
[09/17 13:19:41 visual_prompt]: 	Test 900/1152. loss: 1.134, 0.1835 s / batch. (data: 1.13e-04)max mem: 17.22454 GB 
[09/17 13:20:01 visual_prompt]: 	Test 1000/1152. loss: 0.779, 0.1940 s / batch. (data: 9.56e-05)max mem: 17.22454 GB 
[09/17 13:20:21 visual_prompt]: 	Test 1100/1152. loss: 0.622, 0.1957 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 13:20:36 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1969, average loss: 0.7895
[09/17 13:20:36 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 73.66	top5: 99.99	
[09/17 13:20:36 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/17 13:20:51 visual_prompt]: Epoch 88 / 100: avg data time: 2.69e-01, avg batch time: 0.6753, average train loss: 0.1412
[09/17 13:20:59 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1467, average loss: 0.0671
[09/17 13:20:59 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.00	top5: 100.00	
[09/17 13:21:23 visual_prompt]: 	Test 100/1152. loss: 0.788, 0.1905 s / batch. (data: 7.51e-03)max mem: 17.22454 GB 
[09/17 13:21:42 visual_prompt]: 	Test 200/1152. loss: 0.576, 0.1980 s / batch. (data: 1.37e-02)max mem: 17.22454 GB 
[09/17 13:22:02 visual_prompt]: 	Test 300/1152. loss: 0.670, 0.1841 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 13:22:22 visual_prompt]: 	Test 400/1152. loss: 0.691, 0.1859 s / batch. (data: 1.08e-04)max mem: 17.22454 GB 
[09/17 13:22:41 visual_prompt]: 	Test 500/1152. loss: 0.469, 0.2088 s / batch. (data: 1.22e-02)max mem: 17.22454 GB 
[09/17 13:23:01 visual_prompt]: 	Test 600/1152. loss: 0.838, 0.1962 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 13:23:21 visual_prompt]: 	Test 700/1152. loss: 0.534, 0.1839 s / batch. (data: 1.72e-04)max mem: 17.22454 GB 
[09/17 13:23:41 visual_prompt]: 	Test 800/1152. loss: 0.755, 0.1892 s / batch. (data: 1.51e-04)max mem: 17.22454 GB 
[09/17 13:24:01 visual_prompt]: 	Test 900/1152. loss: 1.081, 0.1893 s / batch. (data: 1.66e-04)max mem: 17.22454 GB 
[09/17 13:24:20 visual_prompt]: 	Test 1000/1152. loss: 0.723, 0.1972 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 13:24:40 visual_prompt]: 	Test 1100/1152. loss: 0.578, 0.1947 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 13:24:55 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1965, average loss: 0.7405
[09/17 13:24:56 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.20	top5: 99.99	
[09/17 13:24:56 visual_prompt]: Best epoch 88: best metric: 0.990
[09/17 13:24:56 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/17 13:25:10 visual_prompt]: Epoch 89 / 100: avg data time: 2.71e-01, avg batch time: 0.6775, average train loss: 0.1229
[09/17 13:25:18 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.1438, average loss: 0.0593
[09/17 13:25:18 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 13:25:42 visual_prompt]: 	Test 100/1152. loss: 0.770, 0.1880 s / batch. (data: 5.21e-03)max mem: 17.22454 GB 
[09/17 13:26:02 visual_prompt]: 	Test 200/1152. loss: 0.543, 0.1966 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 13:26:21 visual_prompt]: 	Test 300/1152. loss: 0.493, 0.1843 s / batch. (data: 1.25e-04)max mem: 17.22454 GB 
[09/17 13:26:41 visual_prompt]: 	Test 400/1152. loss: 0.724, 0.1959 s / batch. (data: 1.32e-04)max mem: 17.22454 GB 
[09/17 13:27:01 visual_prompt]: 	Test 500/1152. loss: 0.459, 0.1842 s / batch. (data: 1.01e-04)max mem: 17.22454 GB 
[09/17 13:27:21 visual_prompt]: 	Test 600/1152. loss: 0.691, 0.1854 s / batch. (data: 1.58e-04)max mem: 17.22454 GB 
[09/17 13:27:41 visual_prompt]: 	Test 700/1152. loss: 0.636, 0.2164 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 13:28:01 visual_prompt]: 	Test 800/1152. loss: 0.659, 0.1955 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 13:28:21 visual_prompt]: 	Test 900/1152. loss: 1.065, 0.1999 s / batch. (data: 1.53e-02)max mem: 17.22454 GB 
[09/17 13:28:41 visual_prompt]: 	Test 1000/1152. loss: 0.801, 0.1859 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 13:29:00 visual_prompt]: 	Test 1100/1152. loss: 0.534, 0.1841 s / batch. (data: 1.35e-04)max mem: 17.22454 GB 
[09/17 13:29:16 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1972, average loss: 0.7129
[09/17 13:29:16 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 77.24	top5: 100.00	
[09/17 13:29:16 visual_prompt]: Best epoch 89: best metric: 0.995
[09/17 13:29:16 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/17 13:29:30 visual_prompt]: Epoch 90 / 100: avg data time: 2.63e-01, avg batch time: 0.6660, average train loss: 0.1115
[09/17 13:29:38 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1432, average loss: 0.0603
[09/17 13:29:38 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 13:30:02 visual_prompt]: 	Test 100/1152. loss: 0.801, 0.1947 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 13:30:21 visual_prompt]: 	Test 200/1152. loss: 0.608, 0.1838 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 13:30:41 visual_prompt]: 	Test 300/1152. loss: 0.455, 0.1964 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 13:31:01 visual_prompt]: 	Test 400/1152. loss: 0.689, 0.1846 s / batch. (data: 1.66e-04)max mem: 17.22454 GB 
[09/17 13:31:21 visual_prompt]: 	Test 500/1152. loss: 0.455, 0.1978 s / batch. (data: 1.41e-02)max mem: 17.22454 GB 
[09/17 13:31:41 visual_prompt]: 	Test 600/1152. loss: 0.882, 0.1968 s / batch. (data: 1.73e-04)max mem: 17.22454 GB 
[09/17 13:32:01 visual_prompt]: 	Test 700/1152. loss: 0.645, 0.1848 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 13:32:21 visual_prompt]: 	Test 800/1152. loss: 0.715, 0.2002 s / batch. (data: 1.60e-02)max mem: 17.22454 GB 
[09/17 13:32:40 visual_prompt]: 	Test 900/1152. loss: 1.177, 0.1996 s / batch. (data: 1.74e-04)max mem: 17.22454 GB 
[09/17 13:33:00 visual_prompt]: 	Test 1000/1152. loss: 0.712, 0.2079 s / batch. (data: 3.24e-05)max mem: 17.22454 GB 
[09/17 13:33:20 visual_prompt]: 	Test 1100/1152. loss: 0.562, 0.2052 s / batch. (data: 2.08e-02)max mem: 17.22454 GB 
[09/17 13:33:35 visual_prompt]: Inference (test):avg data time: 8.29e-03, avg batch time: 0.1975, average loss: 0.7228
[09/17 13:33:36 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 77.05	top5: 100.00	
[09/17 13:33:36 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/17 13:33:50 visual_prompt]: Epoch 91 / 100: avg data time: 2.54e-01, avg batch time: 0.6605, average train loss: 0.0871
[09/17 13:33:58 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1433, average loss: 0.0596
[09/17 13:33:58 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 13:34:22 visual_prompt]: 	Test 100/1152. loss: 0.731, 0.2115 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 13:34:41 visual_prompt]: 	Test 200/1152. loss: 0.591, 0.1987 s / batch. (data: 1.55e-02)max mem: 17.22454 GB 
[09/17 13:35:01 visual_prompt]: 	Test 300/1152. loss: 0.507, 0.2083 s / batch. (data: 1.91e-02)max mem: 17.22454 GB 
[09/17 13:35:21 visual_prompt]: 	Test 400/1152. loss: 0.694, 0.1857 s / batch. (data: 1.15e-04)max mem: 17.22454 GB 
[09/17 13:35:40 visual_prompt]: 	Test 500/1152. loss: 0.477, 0.1844 s / batch. (data: 1.07e-04)max mem: 17.22454 GB 
[09/17 13:36:00 visual_prompt]: 	Test 600/1152. loss: 0.895, 0.1998 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 13:36:20 visual_prompt]: 	Test 700/1152. loss: 0.661, 0.2078 s / batch. (data: 2.36e-02)max mem: 17.22454 GB 
[09/17 13:36:40 visual_prompt]: 	Test 800/1152. loss: 0.730, 0.2074 s / batch. (data: 1.47e-02)max mem: 17.22454 GB 
[09/17 13:37:00 visual_prompt]: 	Test 900/1152. loss: 1.228, 0.1957 s / batch. (data: 1.18e-02)max mem: 17.22454 GB 
[09/17 13:37:21 visual_prompt]: 	Test 1000/1152. loss: 0.726, 0.1954 s / batch. (data: 1.14e-02)max mem: 17.22454 GB 
[09/17 13:37:40 visual_prompt]: 	Test 1100/1152. loss: 0.593, 0.2312 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 13:37:55 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1972, average loss: 0.7554
[09/17 13:37:55 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.38	top5: 99.99	
[09/17 13:37:55 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/17 13:38:09 visual_prompt]: Epoch 92 / 100: avg data time: 2.71e-01, avg batch time: 0.6735, average train loss: 0.0840
[09/17 13:38:17 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1430, average loss: 0.0524
[09/17 13:38:17 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 13:38:41 visual_prompt]: 	Test 100/1152. loss: 0.741, 0.1961 s / batch. (data: 1.33e-02)max mem: 17.22454 GB 
[09/17 13:39:01 visual_prompt]: 	Test 200/1152. loss: 0.604, 0.1842 s / batch. (data: 3.62e-05)max mem: 17.22454 GB 
[09/17 13:39:21 visual_prompt]: 	Test 300/1152. loss: 0.502, 0.1844 s / batch. (data: 1.50e-04)max mem: 17.22454 GB 
[09/17 13:39:40 visual_prompt]: 	Test 400/1152. loss: 0.670, 0.2158 s / batch. (data: 1.48e-02)max mem: 17.22454 GB 
[09/17 13:40:00 visual_prompt]: 	Test 500/1152. loss: 0.484, 0.1843 s / batch. (data: 1.34e-04)max mem: 17.22454 GB 
[09/17 13:40:20 visual_prompt]: 	Test 600/1152. loss: 0.865, 0.1997 s / batch. (data: 1.58e-02)max mem: 17.22454 GB 
[09/17 13:40:40 visual_prompt]: 	Test 700/1152. loss: 0.561, 0.1918 s / batch. (data: 1.46e-04)max mem: 17.22454 GB 
[09/17 13:41:00 visual_prompt]: 	Test 800/1152. loss: 0.703, 0.1842 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 13:41:20 visual_prompt]: 	Test 900/1152. loss: 1.236, 0.2001 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 13:41:39 visual_prompt]: 	Test 1000/1152. loss: 0.728, 0.1941 s / batch. (data: 9.43e-03)max mem: 17.22454 GB 
[09/17 13:41:59 visual_prompt]: 	Test 1100/1152. loss: 0.638, 0.1987 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 13:42:14 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1962, average loss: 0.7609
[09/17 13:42:14 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.54	top5: 99.99	
[09/17 13:42:14 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/17 13:42:29 visual_prompt]: Epoch 93 / 100: avg data time: 2.74e-01, avg batch time: 0.6829, average train loss: 0.0750
[09/17 13:42:37 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1430, average loss: 0.0534
[09/17 13:42:37 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 13:43:01 visual_prompt]: 	Test 100/1152. loss: 0.830, 0.1840 s / batch. (data: 9.23e-05)max mem: 17.22454 GB 
[09/17 13:43:20 visual_prompt]: 	Test 200/1152. loss: 0.604, 0.1847 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 13:43:40 visual_prompt]: 	Test 300/1152. loss: 0.559, 0.1845 s / batch. (data: 1.36e-04)max mem: 17.22454 GB 
[09/17 13:43:59 visual_prompt]: 	Test 400/1152. loss: 0.653, 0.1982 s / batch. (data: 1.40e-02)max mem: 17.22454 GB 
[09/17 13:44:19 visual_prompt]: 	Test 500/1152. loss: 0.518, 0.1999 s / batch. (data: 1.59e-02)max mem: 17.22454 GB 
[09/17 13:44:39 visual_prompt]: 	Test 600/1152. loss: 0.937, 0.1853 s / batch. (data: 1.40e-04)max mem: 17.22454 GB 
[09/17 13:44:58 visual_prompt]: 	Test 700/1152. loss: 0.659, 0.2153 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 13:45:18 visual_prompt]: 	Test 800/1152. loss: 0.701, 0.1845 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 13:45:38 visual_prompt]: 	Test 900/1152. loss: 1.111, 0.2372 s / batch. (data: 4.34e-02)max mem: 17.22454 GB 
[09/17 13:45:58 visual_prompt]: 	Test 1000/1152. loss: 0.620, 0.2106 s / batch. (data: 1.38e-02)max mem: 17.22454 GB 
[09/17 13:46:18 visual_prompt]: 	Test 1100/1152. loss: 0.569, 0.2040 s / batch. (data: 1.09e-04)max mem: 17.22454 GB 
[09/17 13:46:33 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1958, average loss: 0.7704
[09/17 13:46:33 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.69	top5: 99.99	
[09/17 13:46:33 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/17 13:46:47 visual_prompt]: Epoch 94 / 100: avg data time: 2.76e-01, avg batch time: 0.6801, average train loss: 0.0641
[09/17 13:46:56 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1431, average loss: 0.0473
[09/17 13:46:56 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 13:47:19 visual_prompt]: 	Test 100/1152. loss: 0.766, 0.1958 s / batch. (data: 1.29e-02)max mem: 17.22454 GB 
[09/17 13:47:39 visual_prompt]: 	Test 200/1152. loss: 0.592, 0.1986 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 13:47:58 visual_prompt]: 	Test 300/1152. loss: 0.504, 0.1843 s / batch. (data: 1.23e-04)max mem: 17.22454 GB 
[09/17 13:48:18 visual_prompt]: 	Test 400/1152. loss: 0.715, 0.1992 s / batch. (data: 1.56e-02)max mem: 17.22454 GB 
[09/17 13:48:38 visual_prompt]: 	Test 500/1152. loss: 0.491, 0.1955 s / batch. (data: 1.13e-02)max mem: 17.22454 GB 
[09/17 13:48:58 visual_prompt]: 	Test 600/1152. loss: 0.802, 0.1841 s / batch. (data: 4.22e-05)max mem: 17.22454 GB 
[09/17 13:49:18 visual_prompt]: 	Test 700/1152. loss: 0.580, 0.1966 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 13:49:38 visual_prompt]: 	Test 800/1152. loss: 0.720, 0.1843 s / batch. (data: 1.80e-04)max mem: 17.22454 GB 
[09/17 13:49:57 visual_prompt]: 	Test 900/1152. loss: 1.198, 0.2095 s / batch. (data: 1.21e-02)max mem: 17.22454 GB 
[09/17 13:50:17 visual_prompt]: 	Test 1000/1152. loss: 0.768, 0.2038 s / batch. (data: 1.51e-02)max mem: 17.22454 GB 
[09/17 13:50:37 visual_prompt]: 	Test 1100/1152. loss: 0.631, 0.1845 s / batch. (data: 1.31e-04)max mem: 17.22454 GB 
[09/17 13:50:52 visual_prompt]: Inference (test):avg data time: 8.48e-03, avg batch time: 0.1968, average loss: 0.7638
[09/17 13:50:53 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 77.01	top5: 99.99	
[09/17 13:50:53 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/17 13:51:07 visual_prompt]: Epoch 95 / 100: avg data time: 2.73e-01, avg batch time: 0.6745, average train loss: 0.0669
[09/17 13:51:15 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1432, average loss: 0.0565
[09/17 13:51:15 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 13:51:39 visual_prompt]: 	Test 100/1152. loss: 0.816, 0.1846 s / batch. (data: 1.28e-04)max mem: 17.22454 GB 
[09/17 13:51:58 visual_prompt]: 	Test 200/1152. loss: 0.647, 0.1964 s / batch. (data: 1.26e-02)max mem: 17.22454 GB 
[09/17 13:52:18 visual_prompt]: 	Test 300/1152. loss: 0.567, 0.1927 s / batch. (data: 1.04e-04)max mem: 17.22454 GB 
[09/17 13:52:37 visual_prompt]: 	Test 400/1152. loss: 0.726, 0.1845 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 13:52:57 visual_prompt]: 	Test 500/1152. loss: 0.518, 0.1843 s / batch. (data: 1.48e-04)max mem: 17.22454 GB 
[09/17 13:53:17 visual_prompt]: 	Test 600/1152. loss: 0.866, 0.2075 s / batch. (data: 1.89e-02)max mem: 17.22454 GB 
[09/17 13:53:37 visual_prompt]: 	Test 700/1152. loss: 0.638, 0.1961 s / batch. (data: 1.24e-02)max mem: 17.22454 GB 
[09/17 13:53:56 visual_prompt]: 	Test 800/1152. loss: 0.764, 0.1985 s / batch. (data: 1.49e-02)max mem: 17.22454 GB 
[09/17 13:54:16 visual_prompt]: 	Test 900/1152. loss: 1.273, 0.1844 s / batch. (data: 1.39e-04)max mem: 17.22454 GB 
[09/17 13:54:36 visual_prompt]: 	Test 1000/1152. loss: 0.710, 0.1845 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 13:54:56 visual_prompt]: 	Test 1100/1152. loss: 0.625, 0.1958 s / batch. (data: 1.50e-04)max mem: 17.22454 GB 
[09/17 13:55:11 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1963, average loss: 0.8020
[09/17 13:55:11 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 75.77	top5: 99.99	
[09/17 13:55:11 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/17 13:55:25 visual_prompt]: Epoch 96 / 100: avg data time: 2.50e-01, avg batch time: 0.6586, average train loss: 0.0616
[09/17 13:55:34 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1433, average loss: 0.0417
[09/17 13:55:34 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 13:55:58 visual_prompt]: 	Test 100/1152. loss: 0.796, 0.1834 s / batch. (data: 1.14e-04)max mem: 17.22454 GB 
[09/17 13:56:18 visual_prompt]: 	Test 200/1152. loss: 0.592, 0.1838 s / batch. (data: 1.22e-04)max mem: 17.22454 GB 
[09/17 13:56:37 visual_prompt]: 	Test 300/1152. loss: 0.513, 0.1928 s / batch. (data: 1.65e-04)max mem: 17.22454 GB 
[09/17 13:56:57 visual_prompt]: 	Test 400/1152. loss: 0.695, 0.2102 s / batch. (data: 2.70e-02)max mem: 17.22454 GB 
[09/17 13:57:17 visual_prompt]: 	Test 500/1152. loss: 0.489, 0.2109 s / batch. (data: 1.43e-02)max mem: 17.22454 GB 
[09/17 13:57:37 visual_prompt]: 	Test 600/1152. loss: 0.893, 0.1843 s / batch. (data: 1.66e-04)max mem: 17.22454 GB 
[09/17 13:57:56 visual_prompt]: 	Test 700/1152. loss: 0.581, 0.2272 s / batch. (data: 4.15e-05)max mem: 17.22454 GB 
[09/17 13:58:16 visual_prompt]: 	Test 800/1152. loss: 0.689, 0.1931 s / batch. (data: 1.20e-04)max mem: 17.22454 GB 
[09/17 13:58:37 visual_prompt]: 	Test 900/1152. loss: 1.219, 0.1850 s / batch. (data: 1.19e-04)max mem: 17.22454 GB 
[09/17 13:58:57 visual_prompt]: 	Test 1000/1152. loss: 0.701, 0.2311 s / batch. (data: 2.38e-02)max mem: 17.22454 GB 
[09/17 13:59:16 visual_prompt]: 	Test 1100/1152. loss: 0.582, 0.1850 s / batch. (data: 1.18e-04)max mem: 17.22454 GB 
[09/17 13:59:31 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1971, average loss: 0.7604
[09/17 13:59:31 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 77.06	top5: 100.00	
[09/17 13:59:31 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/17 13:59:46 visual_prompt]: Epoch 97 / 100: avg data time: 2.73e-01, avg batch time: 0.6763, average train loss: 0.0553
[09/17 13:59:54 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1432, average loss: 0.0423
[09/17 13:59:54 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 14:00:18 visual_prompt]: 	Test 100/1152. loss: 0.777, 0.1929 s / batch. (data: 1.41e-04)max mem: 17.22454 GB 
[09/17 14:00:37 visual_prompt]: 	Test 200/1152. loss: 0.616, 0.2124 s / batch. (data: 2.76e-02)max mem: 17.22454 GB 
[09/17 14:00:57 visual_prompt]: 	Test 300/1152. loss: 0.524, 0.1945 s / batch. (data: 1.11e-02)max mem: 17.22454 GB 
[09/17 14:01:17 visual_prompt]: 	Test 400/1152. loss: 0.704, 0.1896 s / batch. (data: 9.47e-05)max mem: 17.22454 GB 
[09/17 14:01:36 visual_prompt]: 	Test 500/1152. loss: 0.498, 0.1833 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 14:01:56 visual_prompt]: 	Test 600/1152. loss: 0.904, 0.1947 s / batch. (data: 1.10e-02)max mem: 17.22454 GB 
[09/17 14:02:16 visual_prompt]: 	Test 700/1152. loss: 0.578, 0.2273 s / batch. (data: 2.81e-02)max mem: 17.22454 GB 
[09/17 14:02:36 visual_prompt]: 	Test 800/1152. loss: 0.715, 0.2006 s / batch. (data: 1.45e-02)max mem: 17.22454 GB 
[09/17 14:02:56 visual_prompt]: 	Test 900/1152. loss: 1.251, 0.1863 s / batch. (data: 3.36e-05)max mem: 17.22454 GB 
[09/17 14:03:15 visual_prompt]: 	Test 1000/1152. loss: 0.717, 0.1954 s / batch. (data: 1.13e-02)max mem: 17.22454 GB 
[09/17 14:03:35 visual_prompt]: 	Test 1100/1152. loss: 0.610, 0.1848 s / batch. (data: 4.24e-05)max mem: 17.22454 GB 
[09/17 14:03:50 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1959, average loss: 0.7730
[09/17 14:03:50 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.80	top5: 99.99	
[09/17 14:03:50 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/17 14:04:04 visual_prompt]: Epoch 98 / 100: avg data time: 2.70e-01, avg batch time: 0.6724, average train loss: 0.0530
[09/17 14:04:13 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1432, average loss: 0.0381
[09/17 14:04:13 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 14:04:37 visual_prompt]: 	Test 100/1152. loss: 0.786, 0.2038 s / batch. (data: 1.30e-02)max mem: 17.22454 GB 
[09/17 14:04:56 visual_prompt]: 	Test 200/1152. loss: 0.612, 0.1977 s / batch. (data: 1.02e-04)max mem: 17.22454 GB 
[09/17 14:05:16 visual_prompt]: 	Test 300/1152. loss: 0.517, 0.2106 s / batch. (data: 2.72e-02)max mem: 17.22454 GB 
[09/17 14:05:35 visual_prompt]: 	Test 400/1152. loss: 0.705, 0.1843 s / batch. (data: 1.10e-04)max mem: 17.22454 GB 
[09/17 14:05:55 visual_prompt]: 	Test 500/1152. loss: 0.489, 0.2018 s / batch. (data: 1.52e-02)max mem: 17.22454 GB 
[09/17 14:06:15 visual_prompt]: 	Test 600/1152. loss: 0.895, 0.1839 s / batch. (data: 3.41e-05)max mem: 17.22454 GB 
[09/17 14:06:35 visual_prompt]: 	Test 700/1152. loss: 0.586, 0.1946 s / batch. (data: 1.12e-02)max mem: 17.22454 GB 
[09/17 14:06:54 visual_prompt]: 	Test 800/1152. loss: 0.711, 0.1857 s / batch. (data: 1.56e-04)max mem: 17.22454 GB 
[09/17 14:07:14 visual_prompt]: 	Test 900/1152. loss: 1.251, 0.2048 s / batch. (data: 1.49e-04)max mem: 17.22454 GB 
[09/17 14:07:34 visual_prompt]: 	Test 1000/1152. loss: 0.712, 0.1969 s / batch. (data: 1.93e-04)max mem: 17.22454 GB 
[09/17 14:07:54 visual_prompt]: 	Test 1100/1152. loss: 0.600, 0.2142 s / batch. (data: 1.64e-02)max mem: 17.22454 GB 
[09/17 14:08:10 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1966, average loss: 0.7716
[09/17 14:08:10 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.95	top5: 99.99	
[09/17 14:08:10 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/17 14:08:25 visual_prompt]: Epoch 99 / 100: avg data time: 2.78e-01, avg batch time: 0.6814, average train loss: 0.0550
[09/17 14:08:33 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1437, average loss: 0.0391
[09/17 14:08:33 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 14:08:57 visual_prompt]: 	Test 100/1152. loss: 0.794, 0.1877 s / batch. (data: 1.53e-04)max mem: 17.22454 GB 
[09/17 14:09:17 visual_prompt]: 	Test 200/1152. loss: 0.619, 0.1845 s / batch. (data: 1.08e-04)max mem: 17.22454 GB 
[09/17 14:09:36 visual_prompt]: 	Test 300/1152. loss: 0.527, 0.1925 s / batch. (data: 1.59e-04)max mem: 17.22454 GB 
[09/17 14:09:56 visual_prompt]: 	Test 400/1152. loss: 0.704, 0.1844 s / batch. (data: 1.66e-04)max mem: 17.22454 GB 
[09/17 14:10:16 visual_prompt]: 	Test 500/1152. loss: 0.493, 0.1889 s / batch. (data: 1.75e-04)max mem: 17.22454 GB 
[09/17 14:10:35 visual_prompt]: 	Test 600/1152. loss: 0.901, 0.1846 s / batch. (data: 1.26e-04)max mem: 17.22454 GB 
[09/17 14:10:55 visual_prompt]: 	Test 700/1152. loss: 0.591, 0.1912 s / batch. (data: 1.44e-04)max mem: 17.22454 GB 
[09/17 14:11:15 visual_prompt]: 	Test 800/1152. loss: 0.723, 0.2076 s / batch. (data: 1.10e-02)max mem: 17.22454 GB 
[09/17 14:11:35 visual_prompt]: 	Test 900/1152. loss: 1.254, 0.1965 s / batch. (data: 1.25e-02)max mem: 17.22454 GB 
[09/17 14:11:55 visual_prompt]: 	Test 1000/1152. loss: 0.712, 0.1912 s / batch. (data: 1.29e-04)max mem: 17.22454 GB 
[09/17 14:12:14 visual_prompt]: 	Test 1100/1152. loss: 0.608, 0.1839 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 14:12:29 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1966, average loss: 0.7774
[09/17 14:12:30 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.87	top5: 99.99	
[09/17 14:12:30 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/17 14:12:44 visual_prompt]: Epoch 100 / 100: avg data time: 2.76e-01, avg batch time: 0.6792, average train loss: 0.0572
[09/17 14:12:52 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1430, average loss: 0.0391
[09/17 14:12:52 visual_prompt]: Classification results with val_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 99.50	top5: 100.00	
[09/17 14:13:16 visual_prompt]: 	Test 100/1152. loss: 0.795, 0.1838 s / batch. (data: 1.24e-04)max mem: 17.22454 GB 
[09/17 14:13:35 visual_prompt]: 	Test 200/1152. loss: 0.620, 0.1849 s / batch. (data: 1.60e-04)max mem: 17.22454 GB 
[09/17 14:13:55 visual_prompt]: 	Test 300/1152. loss: 0.530, 0.1844 s / batch. (data: 1.57e-04)max mem: 17.22454 GB 
[09/17 14:14:15 visual_prompt]: 	Test 400/1152. loss: 0.704, 0.1997 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 14:14:35 visual_prompt]: 	Test 500/1152. loss: 0.493, 0.1835 s / batch. (data: 4.84e-05)max mem: 17.22454 GB 
[09/17 14:14:54 visual_prompt]: 	Test 600/1152. loss: 0.900, 0.2209 s / batch. (data: 2.54e-02)max mem: 17.22454 GB 
[09/17 14:15:14 visual_prompt]: 	Test 700/1152. loss: 0.591, 0.1991 s / batch. (data: 1.03e-04)max mem: 17.22454 GB 
[09/17 14:15:34 visual_prompt]: 	Test 800/1152. loss: 0.724, 0.1838 s / batch. (data: 1.12e-04)max mem: 17.22454 GB 
[09/17 14:15:54 visual_prompt]: 	Test 900/1152. loss: 1.255, 0.2232 s / batch. (data: 1.61e-02)max mem: 17.22454 GB 
[09/17 14:16:14 visual_prompt]: 	Test 1000/1152. loss: 0.712, 0.1861 s / batch. (data: 1.62e-04)max mem: 17.22454 GB 
[09/17 14:16:34 visual_prompt]: 	Test 1100/1152. loss: 0.609, 0.2016 s / batch. (data: 1.32e-02)max mem: 17.22454 GB 
[09/17 14:16:49 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1965, average loss: 0.7776
[09/17 14:16:49 visual_prompt]: Classification results with test_vtab-dsprites(predicted_attribute="label_x_position",num_classes=16): top1: 76.87	top5: 99.99	

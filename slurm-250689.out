/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/28 13:02:35 visual_prompt]: Rank of current process: 0. World size: 1
[11/28 13:02:37 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 13:02:37 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/28 13:02:37 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/28 13:02:37 visual_prompt]: Training with config:
[11/28 13:02:37 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr50.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/28 13:02:37 visual_prompt]: Loading training data...
[11/28 13:02:37 visual_prompt]: Constructing mammo-cbis dataset train...
[11/28 13:02:37 visual_prompt]: Loading validation data...
[11/28 13:02:37 visual_prompt]: Constructing mammo-cbis dataset val...
[11/28 13:02:37 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/28 13:02:42 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/28 13:02:42 visual_prompt]: tuned percent:0.536
[11/28 13:02:42 visual_prompt]: Device used for model: 0
[11/28 13:02:42 visual_prompt]: Setting up Evaluator...
[11/28 13:02:42 visual_prompt]: Setting up Trainer...
[11/28 13:02:42 visual_prompt]: 	Setting up the optimizer...
[11/28 13:02:42 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/28 13:10:28 visual_prompt]: Epoch 1 / 100: avg data time: 1.23e+01, avg batch time: 13.3108, average train loss: 1.4006
[11/28 13:11:21 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.5242, average loss: 1.2969
[11/28 13:11:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/28 13:11:21 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/28 13:18:58 visual_prompt]: Epoch 2 / 100: avg data time: 1.22e+01, avg batch time: 13.0800, average train loss: 47.2207
[11/28 13:19:52 visual_prompt]: Inference (val):avg data time: 6.48e-05, avg batch time: 0.5251, average loss: 21.1442
[11/28 13:19:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.51	
[11/28 13:19:52 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/28 13:27:29 visual_prompt]: Epoch 3 / 100: avg data time: 1.22e+01, avg batch time: 13.0468, average train loss: 21.4057
[11/28 13:28:21 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.5291, average loss: 7.6746
[11/28 13:28:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/28 13:28:21 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/28 13:35:57 visual_prompt]: Epoch 4 / 100: avg data time: 1.22e+01, avg batch time: 13.0292, average train loss: 25.2795
[11/28 13:36:48 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.5243, average loss: 29.5045
[11/28 13:36:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.18	
[11/28 13:36:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/28 13:44:09 visual_prompt]: Epoch 5 / 100: avg data time: 1.17e+01, avg batch time: 12.5918, average train loss: 35.3392
[11/28 13:45:00 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.5223, average loss: 40.5801
[11/28 13:45:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/28 13:45:00 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/28 13:52:32 visual_prompt]: Epoch 6 / 100: avg data time: 1.20e+01, avg batch time: 12.9069, average train loss: 75.6072
[11/28 13:53:25 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.5254, average loss: 46.1451
[11/28 13:53:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.12	
[11/28 13:53:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/28 14:01:05 visual_prompt]: Epoch 7 / 100: avg data time: 1.23e+01, avg batch time: 13.1256, average train loss: 64.3204
[11/28 14:01:57 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.5261, average loss: 126.3663
[11/28 14:01:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.34	
[11/28 14:01:57 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/28 14:09:33 visual_prompt]: Epoch 8 / 100: avg data time: 1.22e+01, avg batch time: 13.0153, average train loss: 84.2415
[11/28 14:10:26 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.5233, average loss: 21.1667
[11/28 14:10:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 48.56	
[11/28 14:10:26 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/28 14:18:04 visual_prompt]: Epoch 9 / 100: avg data time: 1.22e+01, avg batch time: 13.0813, average train loss: 122.1390
[11/28 14:18:57 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.5257, average loss: 82.8475
[11/28 14:18:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.13	
[11/28 14:18:57 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/28 14:26:36 visual_prompt]: Epoch 10 / 100: avg data time: 1.22e+01, avg batch time: 13.0973, average train loss: 89.4662
[11/28 14:27:28 visual_prompt]: Inference (val):avg data time: 4.85e-05, avg batch time: 0.5237, average loss: 129.9683
[11/28 14:27:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.94	
[11/28 14:27:28 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/28 14:35:05 visual_prompt]: Epoch 11 / 100: avg data time: 1.22e+01, avg batch time: 13.0445, average train loss: 121.1973
[11/28 14:35:58 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.5317, average loss: 28.3087
[11/28 14:35:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.33	
[11/28 14:35:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/28 14:43:36 visual_prompt]: Epoch 12 / 100: avg data time: 1.22e+01, avg batch time: 13.0813, average train loss: 118.6507
[11/28 14:44:29 visual_prompt]: Inference (val):avg data time: 5.65e-05, avg batch time: 0.5272, average loss: 4.7298
[11/28 14:44:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.62	
[11/28 14:44:29 visual_prompt]: Best epoch 12: best metric: -4.730
[11/28 14:44:29 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/28 14:51:55 visual_prompt]: Epoch 13 / 100: avg data time: 1.19e+01, avg batch time: 12.7233, average train loss: 99.8405
[11/28 14:52:46 visual_prompt]: Inference (val):avg data time: 5.61e-05, avg batch time: 0.5276, average loss: 268.0182
[11/28 14:52:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.27	
[11/28 14:52:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/28 15:00:08 visual_prompt]: Epoch 14 / 100: avg data time: 1.18e+01, avg batch time: 12.6203, average train loss: 128.8177
[11/28 15:00:59 visual_prompt]: Inference (val):avg data time: 4.91e-05, avg batch time: 0.5279, average loss: 32.0419
[11/28 15:00:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.65	
[11/28 15:00:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/28 15:08:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.18e+01, avg batch time: 12.6469, average train loss: 143.5749
[11/28 15:09:13 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.5283, average loss: 5.4027
[11/28 15:09:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.66	
[11/28 15:09:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/28 15:16:35 visual_prompt]: Epoch 16 / 100: avg data time: 1.18e+01, avg batch time: 12.6235, average train loss: 153.1298
[11/28 15:17:29 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.5252, average loss: 64.2137
[11/28 15:17:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.48	
[11/28 15:17:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/28 15:25:20 visual_prompt]: Epoch 17 / 100: avg data time: 1.26e+01, avg batch time: 13.4737, average train loss: 122.3338
[11/28 15:26:14 visual_prompt]: Inference (val):avg data time: 5.26e-05, avg batch time: 0.5271, average loss: 93.1703
[11/28 15:26:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.70	
[11/28 15:26:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/28 15:33:54 visual_prompt]: Epoch 18 / 100: avg data time: 1.23e+01, avg batch time: 13.1486, average train loss: 141.9983
[11/28 15:34:47 visual_prompt]: Inference (val):avg data time: 5.05e-05, avg batch time: 0.5298, average loss: 13.3476
[11/28 15:34:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.66	
[11/28 15:34:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/28 15:42:23 visual_prompt]: Epoch 19 / 100: avg data time: 1.22e+01, avg batch time: 13.0358, average train loss: 107.0472
[11/28 15:43:16 visual_prompt]: Inference (val):avg data time: 4.91e-05, avg batch time: 0.5259, average loss: 40.1287
[11/28 15:43:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/28 15:43:16 visual_prompt]: Stopping early.
[11/28 15:43:17 visual_prompt]: Rank of current process: 0. World size: 1
[11/28 15:43:17 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 15:43:17 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/28 15:43:17 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/28 15:43:17 visual_prompt]: Training with config:
[11/28 15:43:17 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr50.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/28 15:43:17 visual_prompt]: Loading training data...
[11/28 15:43:17 visual_prompt]: Constructing mammo-cbis dataset train...
[11/28 15:43:17 visual_prompt]: Loading validation data...
[11/28 15:43:17 visual_prompt]: Constructing mammo-cbis dataset val...
[11/28 15:43:17 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/28 15:43:24 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/28 15:43:24 visual_prompt]: tuned percent:0.536
[11/28 15:43:24 visual_prompt]: Device used for model: 0
[11/28 15:43:24 visual_prompt]: Setting up Evaluator...
[11/28 15:43:24 visual_prompt]: Setting up Trainer...
[11/28 15:43:24 visual_prompt]: 	Setting up the optimizer...
[11/28 15:43:24 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/28 15:51:03 visual_prompt]: Epoch 1 / 100: avg data time: 1.22e+01, avg batch time: 13.0873, average train loss: 1.4006
[11/28 15:51:56 visual_prompt]: Inference (val):avg data time: 5.67e-05, avg batch time: 0.5190, average loss: 1.2969
[11/28 15:51:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/28 15:51:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/28 15:59:33 visual_prompt]: Epoch 2 / 100: avg data time: 1.22e+01, avg batch time: 13.0687, average train loss: 27.0155
[11/28 16:00:26 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.5263, average loss: 10.3730
[11/28 16:00:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.98	
[11/28 16:00:26 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/28 16:08:04 visual_prompt]: Epoch 3 / 100: avg data time: 1.22e+01, avg batch time: 13.0786, average train loss: 24.1599
[11/28 16:08:57 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.5213, average loss: 43.2925
[11/28 16:08:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.26	
[11/28 16:08:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/28 16:16:35 visual_prompt]: Epoch 4 / 100: avg data time: 1.22e+01, avg batch time: 13.0830, average train loss: 28.1329
[11/28 16:17:29 visual_prompt]: Inference (val):avg data time: 5.29e-05, avg batch time: 0.5211, average loss: 34.0368
[11/28 16:17:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.94	
[11/28 16:17:29 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/28 16:24:52 visual_prompt]: Epoch 5 / 100: avg data time: 1.18e+01, avg batch time: 12.6551, average train loss: 38.8075
[11/28 16:25:43 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.5248, average loss: 24.0847
[11/28 16:25:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.62	
[11/28 16:25:43 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/28 16:33:06 visual_prompt]: Epoch 6 / 100: avg data time: 1.18e+01, avg batch time: 12.6612, average train loss: 68.2230
[11/28 16:33:58 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.5216, average loss: 4.7247
[11/28 16:33:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.24	
[11/28 16:33:58 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/28 16:41:21 visual_prompt]: Epoch 7 / 100: avg data time: 1.18e+01, avg batch time: 12.6686, average train loss: 68.6292
[11/28 16:42:13 visual_prompt]: Inference (val):avg data time: 5.39e-05, avg batch time: 0.5296, average loss: 267.1232
[11/28 16:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.87	
[11/28 16:42:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/28 16:49:36 visual_prompt]: Epoch 8 / 100: avg data time: 1.18e+01, avg batch time: 12.6435, average train loss: 117.2350
[11/28 16:50:27 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.5250, average loss: 28.0913
[11/28 16:50:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[11/28 16:50:27 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/28 16:57:51 visual_prompt]: Epoch 9 / 100: avg data time: 1.18e+01, avg batch time: 12.6781, average train loss: 78.8698
[11/28 16:58:42 visual_prompt]: Inference (val):avg data time: 4.56e-05, avg batch time: 0.5278, average loss: 38.8818
[11/28 16:58:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.94	
[11/28 16:58:42 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/28 17:06:05 visual_prompt]: Epoch 10 / 100: avg data time: 1.18e+01, avg batch time: 12.6438, average train loss: 74.8236
[11/28 17:06:56 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.5228, average loss: 67.9963
[11/28 17:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.31	
[11/28 17:06:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/28 17:14:35 visual_prompt]: Epoch 11 / 100: avg data time: 1.23e+01, avg batch time: 13.1247, average train loss: 79.9933
[11/28 17:15:34 visual_prompt]: Inference (val):avg data time: 4.92e-05, avg batch time: 0.5217, average loss: 158.3058
[11/28 17:15:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.29	
[11/28 17:15:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/28 17:23:30 visual_prompt]: Epoch 12 / 100: avg data time: 1.27e+01, avg batch time: 13.5888, average train loss: 107.5670
[11/28 17:24:23 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.5200, average loss: 46.9732
[11/28 17:24:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.28	
[11/28 17:24:23 visual_prompt]: Best epoch 12: best metric: -46.973
[11/28 17:24:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/28 17:32:05 visual_prompt]: Epoch 13 / 100: avg data time: 1.23e+01, avg batch time: 13.1909, average train loss: 183.5818
[11/28 17:33:02 visual_prompt]: Inference (val):avg data time: 5.29e-05, avg batch time: 0.5189, average loss: 55.5075
[11/28 17:33:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.34	
[11/28 17:33:02 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/28 17:40:44 visual_prompt]: Epoch 14 / 100: avg data time: 1.23e+01, avg batch time: 13.2149, average train loss: 126.3127
[11/28 17:41:37 visual_prompt]: Inference (val):avg data time: 5.20e-05, avg batch time: 0.5253, average loss: 63.5713
[11/28 17:41:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.71	
[11/28 17:41:37 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/28 17:49:28 visual_prompt]: Epoch 15 / 100: avg data time: 1.26e+01, avg batch time: 13.4370, average train loss: 131.3694
[11/28 17:50:22 visual_prompt]: Inference (val):avg data time: 5.95e-05, avg batch time: 0.5237, average loss: 136.1798
[11/28 17:50:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.79	
[11/28 17:50:22 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/28 17:58:12 visual_prompt]: Epoch 16 / 100: avg data time: 1.26e+01, avg batch time: 13.4231, average train loss: 105.8246
[11/28 17:59:06 visual_prompt]: Inference (val):avg data time: 6.04e-05, avg batch time: 0.5205, average loss: 209.0344
[11/28 17:59:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.94	
[11/28 17:59:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/28 18:06:46 visual_prompt]: Epoch 17 / 100: avg data time: 1.23e+01, avg batch time: 13.1359, average train loss: 186.2917
[11/28 18:07:39 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.5223, average loss: 115.1705
[11/28 18:07:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.51	
[11/28 18:07:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/28 18:15:19 visual_prompt]: Epoch 18 / 100: avg data time: 1.23e+01, avg batch time: 13.1232, average train loss: 120.0132
[11/28 18:16:12 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.5297, average loss: 433.3973
[11/28 18:16:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.83	
[11/28 18:16:12 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/28 18:23:50 visual_prompt]: Epoch 19 / 100: avg data time: 1.22e+01, avg batch time: 13.0752, average train loss: 168.5670
[11/28 18:24:43 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.5180, average loss: 122.0169
[11/28 18:24:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.69	
[11/28 18:24:43 visual_prompt]: Stopping early.
[11/28 18:24:44 visual_prompt]: Rank of current process: 0. World size: 1
[11/28 18:24:44 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 18:24:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/28 18:24:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/28 18:24:44 visual_prompt]: Training with config:
[11/28 18:24:44 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr50.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/28 18:24:44 visual_prompt]: Loading training data...
[11/28 18:24:44 visual_prompt]: Constructing mammo-cbis dataset train...
[11/28 18:24:44 visual_prompt]: Loading validation data...
[11/28 18:24:44 visual_prompt]: Constructing mammo-cbis dataset val...
[11/28 18:24:44 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/28 18:24:52 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/28 18:24:52 visual_prompt]: tuned percent:0.536
[11/28 18:24:52 visual_prompt]: Device used for model: 0
[11/28 18:24:52 visual_prompt]: Setting up Evaluator...
[11/28 18:24:52 visual_prompt]: Setting up Trainer...
[11/28 18:24:52 visual_prompt]: 	Setting up the optimizer...
[11/28 18:24:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/28 18:32:32 visual_prompt]: Epoch 1 / 100: avg data time: 1.23e+01, avg batch time: 13.1347, average train loss: 1.4006
[11/28 18:33:25 visual_prompt]: Inference (val):avg data time: 5.54e-05, avg batch time: 0.5266, average loss: 1.2969
[11/28 18:33:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/28 18:33:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/28 18:41:01 visual_prompt]: Epoch 2 / 100: avg data time: 1.21e+01, avg batch time: 13.0241, average train loss: 21.6884
[11/28 18:41:53 visual_prompt]: Inference (val):avg data time: 4.69e-05, avg batch time: 0.5245, average loss: 1.0083
[11/28 18:41:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 57.03	
[11/28 18:41:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/28 18:49:17 visual_prompt]: Epoch 3 / 100: avg data time: 1.18e+01, avg batch time: 12.6746, average train loss: 38.6201
[11/28 18:50:08 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.5262, average loss: 0.7869
[11/28 18:50:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 57.16	
[11/28 18:50:08 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/28 18:57:31 visual_prompt]: Epoch 4 / 100: avg data time: 1.18e+01, avg batch time: 12.6495, average train loss: 16.9552
[11/28 18:58:22 visual_prompt]: Inference (val):avg data time: 5.10e-05, avg batch time: 0.5294, average loss: 5.0919
[11/28 18:58:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.75	
[11/28 18:58:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/28 19:05:45 visual_prompt]: Epoch 5 / 100: avg data time: 1.18e+01, avg batch time: 12.6359, average train loss: 19.9032
[11/28 19:06:37 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.5256, average loss: 45.1114
[11/28 19:06:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.01	
[11/28 19:06:37 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/28 19:14:00 visual_prompt]: Epoch 6 / 100: avg data time: 1.18e+01, avg batch time: 12.6673, average train loss: 35.5889
[11/28 19:14:52 visual_prompt]: Inference (val):avg data time: 4.69e-05, avg batch time: 0.5245, average loss: 26.3993
[11/28 19:14:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.96	
[11/28 19:14:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/28 19:22:15 visual_prompt]: Epoch 7 / 100: avg data time: 1.18e+01, avg batch time: 12.6575, average train loss: 31.7600
[11/28 19:23:06 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.5281, average loss: 42.0422
[11/28 19:23:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.99	
[11/28 19:23:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/28 19:30:29 visual_prompt]: Epoch 8 / 100: avg data time: 1.18e+01, avg batch time: 12.6394, average train loss: 91.7362
[11/28 19:31:20 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.5254, average loss: 71.1311
[11/28 19:31:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.15	
[11/28 19:31:20 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/28 19:38:44 visual_prompt]: Epoch 9 / 100: avg data time: 1.18e+01, avg batch time: 12.6765, average train loss: 88.7644
[11/28 19:39:36 visual_prompt]: Inference (val):avg data time: 4.53e-05, avg batch time: 0.5286, average loss: 27.8329
[11/28 19:39:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.10	
[11/28 19:39:36 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/28 19:46:59 visual_prompt]: Epoch 10 / 100: avg data time: 1.18e+01, avg batch time: 12.6570, average train loss: 91.6330
[11/28 19:47:51 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.5357, average loss: 92.8379
[11/28 19:47:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.37	
[11/28 19:47:51 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/28 19:55:13 visual_prompt]: Epoch 11 / 100: avg data time: 1.18e+01, avg batch time: 12.6410, average train loss: 123.4984
[11/28 19:56:05 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.5290, average loss: 14.5397
[11/28 19:56:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.09	
[11/28 19:56:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/28 20:03:28 visual_prompt]: Epoch 12 / 100: avg data time: 1.18e+01, avg batch time: 12.6427, average train loss: 93.7713
[11/28 20:04:19 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.5266, average loss: 33.3844
[11/28 20:04:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.21	
[11/28 20:04:19 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/28 20:11:42 visual_prompt]: Epoch 13 / 100: avg data time: 1.18e+01, avg batch time: 12.6512, average train loss: 204.6375
[11/28 20:12:33 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.5221, average loss: 87.7408
[11/28 20:12:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.99	
[11/28 20:12:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/28 20:19:56 visual_prompt]: Epoch 14 / 100: avg data time: 1.18e+01, avg batch time: 12.6374, average train loss: 86.7965
[11/28 20:20:47 visual_prompt]: Inference (val):avg data time: 5.54e-05, avg batch time: 0.5251, average loss: 96.9401
[11/28 20:20:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 39.79	
[11/28 20:20:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/28 20:28:12 visual_prompt]: Epoch 15 / 100: avg data time: 1.18e+01, avg batch time: 12.6895, average train loss: 101.9606
[11/28 20:29:03 visual_prompt]: Inference (val):avg data time: 4.65e-05, avg batch time: 0.5222, average loss: 15.5796
[11/28 20:29:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.35	
[11/28 20:29:04 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/28 20:36:27 visual_prompt]: Epoch 16 / 100: avg data time: 1.18e+01, avg batch time: 12.6629, average train loss: 108.8948
[11/28 20:37:18 visual_prompt]: Inference (val):avg data time: 4.99e-05, avg batch time: 0.5302, average loss: 10.1484
[11/28 20:37:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.41	
[11/28 20:37:18 visual_prompt]: Best epoch 16: best metric: -10.148
[11/28 20:37:18 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/28 20:44:41 visual_prompt]: Epoch 17 / 100: avg data time: 1.18e+01, avg batch time: 12.6468, average train loss: 140.4017
[11/28 20:45:33 visual_prompt]: Inference (val):avg data time: 4.47e-05, avg batch time: 0.5317, average loss: 28.2596
[11/28 20:45:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.51	
[11/28 20:45:33 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/28 20:52:56 visual_prompt]: Epoch 18 / 100: avg data time: 1.18e+01, avg batch time: 12.6516, average train loss: 76.1672
[11/28 20:53:47 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.5318, average loss: 94.7159
[11/28 20:53:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.16	
[11/28 20:53:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/28 21:01:10 visual_prompt]: Epoch 19 / 100: avg data time: 1.18e+01, avg batch time: 12.6417, average train loss: 77.4839
[11/28 21:02:01 visual_prompt]: Inference (val):avg data time: 6.13e-05, avg batch time: 0.5278, average loss: 48.5832
[11/28 21:02:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.16	
[11/28 21:02:01 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/28 21:09:24 visual_prompt]: Epoch 20 / 100: avg data time: 1.18e+01, avg batch time: 12.6449, average train loss: 74.0089
[11/28 21:10:16 visual_prompt]: Inference (val):avg data time: 5.84e-05, avg batch time: 0.5299, average loss: 73.6098
[11/28 21:10:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.86	
[11/28 21:10:16 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/28 21:17:39 visual_prompt]: Epoch 21 / 100: avg data time: 1.18e+01, avg batch time: 12.6562, average train loss: 46.9716
[11/28 21:18:30 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.5234, average loss: 68.4389
[11/28 21:18:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.13	
[11/28 21:18:30 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/28 21:25:52 visual_prompt]: Epoch 22 / 100: avg data time: 1.17e+01, avg batch time: 12.6188, average train loss: 53.8320
[11/28 21:26:44 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.5307, average loss: 103.9562
[11/28 21:26:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.67	
[11/28 21:26:44 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/28 21:34:06 visual_prompt]: Epoch 23 / 100: avg data time: 1.18e+01, avg batch time: 12.6425, average train loss: 64.1752
[11/28 21:34:58 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.5319, average loss: 8.6518
[11/28 21:34:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.81	
[11/28 21:34:58 visual_prompt]: Best epoch 23: best metric: -8.652
[11/28 21:34:58 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/28 21:42:22 visual_prompt]: Epoch 24 / 100: avg data time: 1.18e+01, avg batch time: 12.6975, average train loss: 53.3917
[11/28 21:43:15 visual_prompt]: Inference (val):avg data time: 5.03e-05, avg batch time: 0.5282, average loss: 79.1550
[11/28 21:43:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.53	
[11/28 21:43:15 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/28 21:50:45 visual_prompt]: Epoch 25 / 100: avg data time: 1.20e+01, avg batch time: 12.8454, average train loss: 83.9509
[11/28 21:51:36 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.5291, average loss: 121.5777
[11/28 21:51:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.93	
[11/28 21:51:36 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/28 21:58:59 visual_prompt]: Epoch 26 / 100: avg data time: 1.18e+01, avg batch time: 12.6600, average train loss: 100.4154
[11/28 21:59:51 visual_prompt]: Inference (val):avg data time: 5.59e-05, avg batch time: 0.5236, average loss: 50.2287
[11/28 21:59:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.91	
[11/28 21:59:51 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/28 22:07:14 visual_prompt]: Epoch 27 / 100: avg data time: 1.18e+01, avg batch time: 12.6487, average train loss: 56.2868
[11/28 22:08:05 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.5284, average loss: 45.5013
[11/28 22:08:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.50	
[11/28 22:08:05 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/28 22:15:29 visual_prompt]: Epoch 28 / 100: avg data time: 1.18e+01, avg batch time: 12.6819, average train loss: 67.3416
[11/28 22:16:21 visual_prompt]: Inference (val):avg data time: 5.13e-05, avg batch time: 0.5257, average loss: 4.4116
[11/28 22:16:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.03	
[11/28 22:16:21 visual_prompt]: Best epoch 28: best metric: -4.412
[11/28 22:16:21 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/28 22:23:49 visual_prompt]: Epoch 29 / 100: avg data time: 1.19e+01, avg batch time: 12.8135, average train loss: 67.4864
[11/28 22:24:41 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.5193, average loss: 11.7534
[11/28 22:24:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.43	
[11/28 22:24:41 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/28 22:32:03 visual_prompt]: Epoch 30 / 100: avg data time: 1.18e+01, avg batch time: 12.6260, average train loss: 118.5525
[11/28 22:32:54 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.5307, average loss: 26.2126
[11/28 22:32:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.01	
[11/28 22:32:54 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/28 22:40:16 visual_prompt]: Epoch 31 / 100: avg data time: 1.18e+01, avg batch time: 12.6351, average train loss: 127.2978
[11/28 22:41:08 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.5264, average loss: 29.5896
[11/28 22:41:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.84	
[11/28 22:41:08 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/28 22:48:31 visual_prompt]: Epoch 32 / 100: avg data time: 1.18e+01, avg batch time: 12.6624, average train loss: 82.8848
[11/28 22:49:23 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.5272, average loss: 126.7155
[11/28 22:49:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.09	
[11/28 22:49:23 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/28 22:56:45 visual_prompt]: Epoch 33 / 100: avg data time: 1.18e+01, avg batch time: 12.6334, average train loss: 78.9427
[11/28 22:57:36 visual_prompt]: Inference (val):avg data time: 4.69e-05, avg batch time: 0.5315, average loss: 105.0000
[11/28 22:57:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.73	
[11/28 22:57:36 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/28 23:04:58 visual_prompt]: Epoch 34 / 100: avg data time: 1.17e+01, avg batch time: 12.6061, average train loss: 55.3709
[11/28 23:05:50 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.5259, average loss: 73.8822
[11/28 23:05:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.97	
[11/28 23:05:50 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/28 23:13:12 visual_prompt]: Epoch 35 / 100: avg data time: 1.18e+01, avg batch time: 12.6483, average train loss: 65.2232
[11/28 23:14:04 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.5229, average loss: 634.3558
[11/28 23:14:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.99	
[11/28 23:14:04 visual_prompt]: Stopping early.
[11/28 23:14:04 visual_prompt]: Rank of current process: 0. World size: 1
[11/28 23:14:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 23:14:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/28 23:14:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/28 23:14:04 visual_prompt]: Training with config:
[11/28 23:14:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr50.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/28 23:14:04 visual_prompt]: Loading training data...
[11/28 23:14:04 visual_prompt]: Constructing mammo-cbis dataset train...
[11/28 23:14:04 visual_prompt]: Loading validation data...
[11/28 23:14:04 visual_prompt]: Constructing mammo-cbis dataset val...
[11/28 23:14:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/28 23:14:13 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/28 23:14:13 visual_prompt]: tuned percent:0.536
[11/28 23:14:13 visual_prompt]: Device used for model: 0
[11/28 23:14:13 visual_prompt]: Setting up Evaluator...
[11/28 23:14:13 visual_prompt]: Setting up Trainer...
[11/28 23:14:13 visual_prompt]: 	Setting up the optimizer...
[11/28 23:14:13 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/28 23:21:36 visual_prompt]: Epoch 1 / 100: avg data time: 1.18e+01, avg batch time: 12.6418, average train loss: 1.4006
[11/28 23:22:28 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.5247, average loss: 1.2969
[11/28 23:22:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/28 23:22:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/28 23:29:49 visual_prompt]: Epoch 2 / 100: avg data time: 1.17e+01, avg batch time: 12.6107, average train loss: 22.2948
[11/28 23:30:40 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.5210, average loss: 17.1110
[11/28 23:30:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.22	
[11/28 23:30:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/28 23:38:03 visual_prompt]: Epoch 3 / 100: avg data time: 1.18e+01, avg batch time: 12.6407, average train loss: 22.4783
[11/28 23:38:54 visual_prompt]: Inference (val):avg data time: 5.19e-05, avg batch time: 0.5273, average loss: 22.4050
[11/28 23:38:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.81	
[11/28 23:38:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/28 23:46:15 visual_prompt]: Epoch 4 / 100: avg data time: 1.17e+01, avg batch time: 12.6135, average train loss: 41.8588
[11/28 23:47:07 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.5291, average loss: 27.7860
[11/28 23:47:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.88	
[11/28 23:47:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/28 23:54:29 visual_prompt]: Epoch 5 / 100: avg data time: 1.17e+01, avg batch time: 12.6151, average train loss: 54.8232
[11/28 23:55:20 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.5252, average loss: 24.0043
[11/28 23:55:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.88	
[11/28 23:55:20 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/29 00:02:43 visual_prompt]: Epoch 6 / 100: avg data time: 1.18e+01, avg batch time: 12.6451, average train loss: 54.2931
[11/29 00:03:34 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.5301, average loss: 55.2725
[11/29 00:03:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.53	
[11/29 00:03:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/29 00:10:57 visual_prompt]: Epoch 7 / 100: avg data time: 1.18e+01, avg batch time: 12.6530, average train loss: 27.7218
[11/29 00:11:49 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.5260, average loss: 7.0102
[11/29 00:11:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.49	
[11/29 00:11:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/29 00:19:10 visual_prompt]: Epoch 8 / 100: avg data time: 1.17e+01, avg batch time: 12.6081, average train loss: 81.4585
[11/29 00:20:02 visual_prompt]: Inference (val):avg data time: 4.88e-05, avg batch time: 0.5250, average loss: 160.3830
[11/29 00:20:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.04	
[11/29 00:20:02 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/29 00:27:25 visual_prompt]: Epoch 9 / 100: avg data time: 1.18e+01, avg batch time: 12.6555, average train loss: 39.7723
[11/29 00:28:16 visual_prompt]: Inference (val):avg data time: 6.26e-05, avg batch time: 0.5227, average loss: 20.0599
[11/29 00:28:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.98	
[11/29 00:28:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/29 00:35:38 visual_prompt]: Epoch 10 / 100: avg data time: 1.18e+01, avg batch time: 12.6314, average train loss: 53.7560
[11/29 00:36:29 visual_prompt]: Inference (val):avg data time: 4.99e-05, avg batch time: 0.5259, average loss: 55.4346
[11/29 00:36:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.93	
[11/29 00:36:29 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/29 00:43:52 visual_prompt]: Epoch 11 / 100: avg data time: 1.18e+01, avg batch time: 12.6273, average train loss: 38.6414
[11/29 00:44:43 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.5239, average loss: 56.3611
[11/29 00:44:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.28	
[11/29 00:44:43 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/29 00:52:05 visual_prompt]: Epoch 12 / 100: avg data time: 1.18e+01, avg batch time: 12.6309, average train loss: 45.0364
[11/29 00:52:57 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.5242, average loss: 43.5440
[11/29 00:52:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.31	
[11/29 00:52:57 visual_prompt]: Best epoch 12: best metric: -43.544
[11/29 00:52:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/29 01:00:20 visual_prompt]: Epoch 13 / 100: avg data time: 1.18e+01, avg batch time: 12.6571, average train loss: 30.3548
[11/29 01:01:11 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.5239, average loss: 18.9613
[11/29 01:01:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.15	
[11/29 01:01:11 visual_prompt]: Best epoch 13: best metric: -18.961
[11/29 01:01:11 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/29 01:08:33 visual_prompt]: Epoch 14 / 100: avg data time: 1.18e+01, avg batch time: 12.6293, average train loss: 53.3074
[11/29 01:09:24 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.5285, average loss: 0.7582
[11/29 01:09:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.52	
[11/29 01:09:24 visual_prompt]: Best epoch 14: best metric: -0.758
[11/29 01:09:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/29 01:16:47 visual_prompt]: Epoch 15 / 100: avg data time: 1.18e+01, avg batch time: 12.6508, average train loss: 32.6008
[11/29 01:17:39 visual_prompt]: Inference (val):avg data time: 5.20e-05, avg batch time: 0.5300, average loss: 55.0331
[11/29 01:17:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.51	
[11/29 01:17:39 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/29 01:25:01 visual_prompt]: Epoch 16 / 100: avg data time: 1.17e+01, avg batch time: 12.6176, average train loss: 23.8085
[11/29 01:25:52 visual_prompt]: Inference (val):avg data time: 5.33e-05, avg batch time: 0.5282, average loss: 25.0575
[11/29 01:25:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.27	
[11/29 01:25:52 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/29 01:33:14 visual_prompt]: Epoch 17 / 100: avg data time: 1.17e+01, avg batch time: 12.6211, average train loss: 21.4958
[11/29 01:34:06 visual_prompt]: Inference (val):avg data time: 4.82e-05, avg batch time: 0.5307, average loss: 54.8485
[11/29 01:34:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.94	
[11/29 01:34:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/29 01:41:28 visual_prompt]: Epoch 18 / 100: avg data time: 1.18e+01, avg batch time: 12.6414, average train loss: 76.7608
[11/29 01:42:20 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.5222, average loss: 93.1804
[11/29 01:42:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.08	
[11/29 01:42:20 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/29 01:49:41 visual_prompt]: Epoch 19 / 100: avg data time: 1.17e+01, avg batch time: 12.6115, average train loss: 28.2852
[11/29 01:50:32 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.5224, average loss: 19.4200
[11/29 01:50:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.33	
[11/29 01:50:32 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/29 01:57:54 visual_prompt]: Epoch 20 / 100: avg data time: 1.18e+01, avg batch time: 12.6205, average train loss: 39.3531
[11/29 01:58:45 visual_prompt]: Inference (val):avg data time: 5.61e-05, avg batch time: 0.5188, average loss: 93.7001
[11/29 01:58:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.31	
[11/29 01:58:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/29 02:06:09 visual_prompt]: Epoch 21 / 100: avg data time: 1.18e+01, avg batch time: 12.6738, average train loss: 24.9626
[11/29 02:07:00 visual_prompt]: Inference (val):avg data time: 5.13e-05, avg batch time: 0.5228, average loss: 30.7846
[11/29 02:07:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.94	
[11/29 02:07:00 visual_prompt]: Stopping early.
[11/29 02:07:01 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 02:07:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 02:07:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 02:07:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 02:07:01 visual_prompt]: Training with config:
[11/29 02:07:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr25.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 02:07:01 visual_prompt]: Loading training data...
[11/29 02:07:01 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 02:07:01 visual_prompt]: Loading validation data...
[11/29 02:07:01 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 02:07:01 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/29 02:07:04 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/29 02:07:04 visual_prompt]: tuned percent:0.536
[11/29 02:07:05 visual_prompt]: Device used for model: 0
[11/29 02:07:05 visual_prompt]: Setting up Evaluator...
[11/29 02:07:05 visual_prompt]: Setting up Trainer...
[11/29 02:07:05 visual_prompt]: 	Setting up the optimizer...
[11/29 02:07:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 02:14:28 visual_prompt]: Epoch 1 / 100: avg data time: 1.18e+01, avg batch time: 12.6676, average train loss: 1.4006
[11/29 02:15:20 visual_prompt]: Inference (val):avg data time: 4.76e-05, avg batch time: 0.5354, average loss: 1.2969
[11/29 02:15:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/29 02:15:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/29 02:22:43 visual_prompt]: Epoch 2 / 100: avg data time: 1.18e+01, avg batch time: 12.6595, average train loss: 15.7918
[11/29 02:23:35 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.5202, average loss: 6.8894
[11/29 02:23:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.72	
[11/29 02:23:35 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/29 02:30:59 visual_prompt]: Epoch 3 / 100: avg data time: 1.18e+01, avg batch time: 12.6717, average train loss: 10.7627
[11/29 02:31:50 visual_prompt]: Inference (val):avg data time: 5.12e-05, avg batch time: 0.5276, average loss: 7.5916
[11/29 02:31:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.39	
[11/29 02:31:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/29 02:39:12 visual_prompt]: Epoch 4 / 100: avg data time: 1.17e+01, avg batch time: 12.6128, average train loss: 16.2732
[11/29 02:40:03 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.5238, average loss: 4.7213
[11/29 02:40:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.61	
[11/29 02:40:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/29 02:47:26 visual_prompt]: Epoch 5 / 100: avg data time: 1.18e+01, avg batch time: 12.6385, average train loss: 37.4550
[11/29 02:48:17 visual_prompt]: Inference (val):avg data time: 4.85e-05, avg batch time: 0.5270, average loss: 19.0919
[11/29 02:48:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.24	
[11/29 02:48:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/29 02:55:41 visual_prompt]: Epoch 6 / 100: avg data time: 1.18e+01, avg batch time: 12.6629, average train loss: 39.3881
[11/29 02:56:31 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.5191, average loss: 102.4557
[11/29 02:56:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.86	
[11/29 02:56:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/29 03:03:51 visual_prompt]: Epoch 7 / 100: avg data time: 1.17e+01, avg batch time: 12.5690, average train loss: 39.6249
[11/29 03:04:42 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.5279, average loss: 64.6401
[11/29 03:04:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.81	
[11/29 03:04:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/29 03:12:05 visual_prompt]: Epoch 8 / 100: avg data time: 1.18e+01, avg batch time: 12.6335, average train loss: 49.9475
[11/29 03:12:56 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.5241, average loss: 160.6079
[11/29 03:12:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.92	
[11/29 03:12:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/29 03:20:18 visual_prompt]: Epoch 9 / 100: avg data time: 1.17e+01, avg batch time: 12.6159, average train loss: 45.3639
[11/29 03:21:09 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.5340, average loss: 67.4572
[11/29 03:21:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.20	
[11/29 03:21:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/29 03:28:29 visual_prompt]: Epoch 10 / 100: avg data time: 1.17e+01, avg batch time: 12.5704, average train loss: 65.7773
[11/29 03:29:20 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.5241, average loss: 56.5179
[11/29 03:29:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.09	
[11/29 03:29:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/29 03:36:41 visual_prompt]: Epoch 11 / 100: avg data time: 1.17e+01, avg batch time: 12.5800, average train loss: 45.6347
[11/29 03:37:31 visual_prompt]: Inference (val):avg data time: 4.54e-05, avg batch time: 0.5296, average loss: 54.4974
[11/29 03:37:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.88	
[11/29 03:37:31 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/29 03:44:51 visual_prompt]: Epoch 12 / 100: avg data time: 1.17e+01, avg batch time: 12.5632, average train loss: 49.6809
[11/29 03:45:43 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.5232, average loss: 17.6987
[11/29 03:45:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.14	
[11/29 03:45:43 visual_prompt]: Best epoch 12: best metric: -17.699
[11/29 03:45:43 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/29 03:53:01 visual_prompt]: Epoch 13 / 100: avg data time: 1.17e+01, avg batch time: 12.5253, average train loss: 66.5660
[11/29 03:53:52 visual_prompt]: Inference (val):avg data time: 4.58e-05, avg batch time: 0.5206, average loss: 68.4455
[11/29 03:53:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.13	
[11/29 03:53:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/29 04:01:08 visual_prompt]: Epoch 14 / 100: avg data time: 1.16e+01, avg batch time: 12.4577, average train loss: 83.4128
[11/29 04:01:59 visual_prompt]: Inference (val):avg data time: 4.85e-05, avg batch time: 0.5264, average loss: 299.3634
[11/29 04:01:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.00	
[11/29 04:01:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/29 04:09:17 visual_prompt]: Epoch 15 / 100: avg data time: 1.16e+01, avg batch time: 12.4961, average train loss: 130.0345
[11/29 04:10:08 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.5326, average loss: 2.0601
[11/29 04:10:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.65	
[11/29 04:10:08 visual_prompt]: Best epoch 15: best metric: -2.060
[11/29 04:10:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/29 04:17:26 visual_prompt]: Epoch 16 / 100: avg data time: 1.16e+01, avg batch time: 12.5047, average train loss: 74.8319
[11/29 04:18:17 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.5250, average loss: 105.7599
[11/29 04:18:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.21	
[11/29 04:18:17 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/29 04:25:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.16e+01, avg batch time: 12.5055, average train loss: 63.3187
[11/29 04:26:26 visual_prompt]: Inference (val):avg data time: 4.67e-05, avg batch time: 0.5227, average loss: 30.9655
[11/29 04:26:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.98	
[11/29 04:26:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/29 04:33:44 visual_prompt]: Epoch 18 / 100: avg data time: 1.17e+01, avg batch time: 12.5262, average train loss: 82.0289
[11/29 04:34:35 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.5350, average loss: 174.5963
[11/29 04:34:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.96	
[11/29 04:34:35 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/29 04:41:54 visual_prompt]: Epoch 19 / 100: avg data time: 1.16e+01, avg batch time: 12.5134, average train loss: 71.7941
[11/29 04:42:45 visual_prompt]: Inference (val):avg data time: 5.32e-05, avg batch time: 0.5229, average loss: 134.8451
[11/29 04:42:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.59	
[11/29 04:42:45 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/29 04:50:04 visual_prompt]: Epoch 20 / 100: avg data time: 1.17e+01, avg batch time: 12.5494, average train loss: 62.4867
[11/29 04:50:55 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.5299, average loss: 30.6840
[11/29 04:50:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.36	
[11/29 04:50:55 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/29 04:58:15 visual_prompt]: Epoch 21 / 100: avg data time: 1.17e+01, avg batch time: 12.5609, average train loss: 95.0501
[11/29 04:59:06 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.5276, average loss: 190.0296
[11/29 04:59:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.43	
[11/29 04:59:06 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/29 05:06:25 visual_prompt]: Epoch 22 / 100: avg data time: 1.16e+01, avg batch time: 12.5195, average train loss: 80.5103
[11/29 05:07:15 visual_prompt]: Inference (val):avg data time: 4.65e-05, avg batch time: 0.5226, average loss: 122.3092
[11/29 05:07:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.20	
[11/29 05:07:15 visual_prompt]: Stopping early.
[11/29 05:07:16 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 05:07:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 05:07:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 05:07:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 05:07:16 visual_prompt]: Training with config:
[11/29 05:07:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr25.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 05:07:16 visual_prompt]: Loading training data...
[11/29 05:07:16 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 05:07:16 visual_prompt]: Loading validation data...
[11/29 05:07:16 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 05:07:16 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/29 05:07:19 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/29 05:07:19 visual_prompt]: tuned percent:0.536
[11/29 05:07:19 visual_prompt]: Device used for model: 0
[11/29 05:07:19 visual_prompt]: Setting up Evaluator...
[11/29 05:07:19 visual_prompt]: Setting up Trainer...
[11/29 05:07:19 visual_prompt]: 	Setting up the optimizer...
[11/29 05:07:19 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 05:14:39 visual_prompt]: Epoch 1 / 100: avg data time: 1.17e+01, avg batch time: 12.5570, average train loss: 1.4006
[11/29 05:15:30 visual_prompt]: Inference (val):avg data time: 4.81e-05, avg batch time: 0.5305, average loss: 1.2969
[11/29 05:15:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/29 05:15:30 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/29 05:22:49 visual_prompt]: Epoch 2 / 100: avg data time: 1.17e+01, avg batch time: 12.5434, average train loss: 23.9563
[11/29 05:23:40 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.5306, average loss: 2.2611
[11/29 05:23:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.94	
[11/29 05:23:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/29 05:30:58 visual_prompt]: Epoch 3 / 100: avg data time: 1.16e+01, avg batch time: 12.5208, average train loss: 12.5572
[11/29 05:31:49 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.5254, average loss: 5.1162
[11/29 05:31:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.45	
[11/29 05:31:49 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/29 05:39:09 visual_prompt]: Epoch 4 / 100: avg data time: 1.17e+01, avg batch time: 12.5702, average train loss: 11.6014
[11/29 05:40:00 visual_prompt]: Inference (val):avg data time: 5.29e-05, avg batch time: 0.5273, average loss: 5.2183
[11/29 05:40:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.73	
[11/29 05:40:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/29 05:47:17 visual_prompt]: Epoch 5 / 100: avg data time: 1.16e+01, avg batch time: 12.4840, average train loss: 28.7568
[11/29 05:48:08 visual_prompt]: Inference (val):avg data time: 4.64e-05, avg batch time: 0.5182, average loss: 15.6132
[11/29 05:48:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.37	
[11/29 05:48:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/29 05:55:26 visual_prompt]: Epoch 6 / 100: avg data time: 1.17e+01, avg batch time: 12.5275, average train loss: 17.9456
[11/29 05:56:18 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.5312, average loss: 11.7070
[11/29 05:56:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.61	
[11/29 05:56:18 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/29 06:03:36 visual_prompt]: Epoch 7 / 100: avg data time: 1.17e+01, avg batch time: 12.5343, average train loss: 25.2146
[11/29 06:04:28 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.5256, average loss: 8.6318
[11/29 06:04:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.34	
[11/29 06:04:28 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/29 06:11:45 visual_prompt]: Epoch 8 / 100: avg data time: 1.16e+01, avg batch time: 12.4902, average train loss: 33.6537
[11/29 06:12:36 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.5240, average loss: 3.5247
[11/29 06:12:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.36	
[11/29 06:12:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/29 06:19:54 visual_prompt]: Epoch 9 / 100: avg data time: 1.16e+01, avg batch time: 12.5091, average train loss: 46.5278
[11/29 06:20:46 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.5220, average loss: 21.3520
[11/29 06:20:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.28	
[11/29 06:20:46 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/29 06:28:04 visual_prompt]: Epoch 10 / 100: avg data time: 1.16e+01, avg batch time: 12.5131, average train loss: 53.4978
[11/29 06:28:54 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.5176, average loss: 34.7211
[11/29 06:28:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.19	
[11/29 06:28:54 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/29 06:36:12 visual_prompt]: Epoch 11 / 100: avg data time: 1.16e+01, avg batch time: 12.4886, average train loss: 39.1158
[11/29 06:37:02 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.5268, average loss: 85.8978
[11/29 06:37:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.06	
[11/29 06:37:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/29 06:44:18 visual_prompt]: Epoch 12 / 100: avg data time: 1.16e+01, avg batch time: 12.4667, average train loss: 89.5430
[11/29 06:45:08 visual_prompt]: Inference (val):avg data time: 4.58e-05, avg batch time: 0.5280, average loss: 4.1313
[11/29 06:45:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.58	
[11/29 06:45:08 visual_prompt]: Best epoch 12: best metric: -4.131
[11/29 06:45:08 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/29 06:52:26 visual_prompt]: Epoch 13 / 100: avg data time: 1.16e+01, avg batch time: 12.4956, average train loss: 71.3667
[11/29 06:53:16 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.5168, average loss: 4.0069
[11/29 06:53:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.74	
[11/29 06:53:16 visual_prompt]: Best epoch 13: best metric: -4.007
[11/29 06:53:16 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/29 07:00:31 visual_prompt]: Epoch 14 / 100: avg data time: 1.16e+01, avg batch time: 12.4426, average train loss: 54.6791
[11/29 07:01:22 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.5186, average loss: 50.7133
[11/29 07:01:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.42	
[11/29 07:01:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/29 07:08:39 visual_prompt]: Epoch 15 / 100: avg data time: 1.16e+01, avg batch time: 12.4738, average train loss: 46.9962
[11/29 07:09:30 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5244, average loss: 33.4857
[11/29 07:09:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.11	
[11/29 07:09:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/29 07:16:47 visual_prompt]: Epoch 16 / 100: avg data time: 1.16e+01, avg batch time: 12.4965, average train loss: 53.9086
[11/29 07:17:38 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.5220, average loss: 64.7179
[11/29 07:17:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[11/29 07:17:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/29 07:24:56 visual_prompt]: Epoch 17 / 100: avg data time: 1.16e+01, avg batch time: 12.4918, average train loss: 38.0387
[11/29 07:25:47 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.5240, average loss: 39.6790
[11/29 07:25:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.57	
[11/29 07:25:47 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/29 07:33:04 visual_prompt]: Epoch 18 / 100: avg data time: 1.16e+01, avg batch time: 12.4962, average train loss: 53.2982
[11/29 07:33:55 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.5232, average loss: 64.1893
[11/29 07:33:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.51	
[11/29 07:33:55 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/29 07:41:13 visual_prompt]: Epoch 19 / 100: avg data time: 1.16e+01, avg batch time: 12.4901, average train loss: 67.6427
[11/29 07:42:04 visual_prompt]: Inference (val):avg data time: 5.82e-05, avg batch time: 0.5217, average loss: 32.7374
[11/29 07:42:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.62	
[11/29 07:42:04 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/29 07:49:21 visual_prompt]: Epoch 20 / 100: avg data time: 1.16e+01, avg batch time: 12.4981, average train loss: 65.8348
[11/29 07:50:12 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.5229, average loss: 170.4924
[11/29 07:50:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.49	
[11/29 07:50:12 visual_prompt]: Stopping early.
[11/29 07:50:13 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 07:50:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 07:50:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 07:50:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 07:50:13 visual_prompt]: Training with config:
[11/29 07:50:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr25.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 07:50:13 visual_prompt]: Loading training data...
[11/29 07:50:13 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 07:50:13 visual_prompt]: Loading validation data...
[11/29 07:50:13 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 07:50:13 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/29 07:50:16 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/29 07:50:16 visual_prompt]: tuned percent:0.536
[11/29 07:50:16 visual_prompt]: Device used for model: 0
[11/29 07:50:16 visual_prompt]: Setting up Evaluator...
[11/29 07:50:16 visual_prompt]: Setting up Trainer...
[11/29 07:50:16 visual_prompt]: 	Setting up the optimizer...
[11/29 07:50:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 07:57:34 visual_prompt]: Epoch 1 / 100: avg data time: 1.16e+01, avg batch time: 12.5150, average train loss: 1.4006
[11/29 07:58:25 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.5196, average loss: 1.2969
[11/29 07:58:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/29 07:58:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/29 08:05:42 visual_prompt]: Epoch 2 / 100: avg data time: 1.16e+01, avg batch time: 12.5013, average train loss: 23.3420
[11/29 08:06:33 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.5159, average loss: 1.6945
[11/29 08:06:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.77	
[11/29 08:06:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/29 08:13:49 visual_prompt]: Epoch 3 / 100: avg data time: 1.16e+01, avg batch time: 12.4536, average train loss: 6.8467
[11/29 08:14:40 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.5222, average loss: 3.2745
[11/29 08:14:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.98	
[11/29 08:14:40 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/29 08:21:57 visual_prompt]: Epoch 4 / 100: avg data time: 1.16e+01, avg batch time: 12.4859, average train loss: 13.5289
[11/29 08:22:48 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.5183, average loss: 31.5207
[11/29 08:22:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.00	
[11/29 08:22:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/29 08:30:05 visual_prompt]: Epoch 5 / 100: avg data time: 1.16e+01, avg batch time: 12.4936, average train loss: 17.9638
[11/29 08:30:56 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.5256, average loss: 4.0814
[11/29 08:30:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.27	
[11/29 08:30:56 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/29 08:38:14 visual_prompt]: Epoch 6 / 100: avg data time: 1.16e+01, avg batch time: 12.5038, average train loss: 23.6279
[11/29 08:39:05 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.5238, average loss: 16.5253
[11/29 08:39:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.50	
[11/29 08:39:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/29 08:46:22 visual_prompt]: Epoch 7 / 100: avg data time: 1.16e+01, avg batch time: 12.4753, average train loss: 42.5847
[11/29 08:47:13 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.5223, average loss: 28.9656
[11/29 08:47:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.21	
[11/29 08:47:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/29 08:54:28 visual_prompt]: Epoch 8 / 100: avg data time: 1.16e+01, avg batch time: 12.4401, average train loss: 34.4184
[11/29 08:55:19 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.5193, average loss: 25.3895
[11/29 08:55:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.06	
[11/29 08:55:19 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/29 09:02:36 visual_prompt]: Epoch 9 / 100: avg data time: 1.16e+01, avg batch time: 12.4858, average train loss: 10.9663
[11/29 09:03:27 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.5220, average loss: 6.2567
[11/29 09:03:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.19	
[11/29 09:03:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/29 09:10:45 visual_prompt]: Epoch 10 / 100: avg data time: 1.16e+01, avg batch time: 12.5097, average train loss: 29.6243
[11/29 09:11:36 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.5161, average loss: 50.6515
[11/29 09:11:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.30	
[11/29 09:11:36 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/29 09:18:56 visual_prompt]: Epoch 11 / 100: avg data time: 1.17e+01, avg batch time: 12.5815, average train loss: 36.6971
[11/29 09:19:48 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.5234, average loss: 67.0725
[11/29 09:19:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.37	
[11/29 09:19:48 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/29 09:27:11 visual_prompt]: Epoch 12 / 100: avg data time: 1.18e+01, avg batch time: 12.6471, average train loss: 43.5188
[11/29 09:28:02 visual_prompt]: Inference (val):avg data time: 5.15e-05, avg batch time: 0.5182, average loss: 47.8986
[11/29 09:28:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.15	
[11/29 09:28:02 visual_prompt]: Best epoch 12: best metric: -47.899
[11/29 09:28:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/29 09:35:23 visual_prompt]: Epoch 13 / 100: avg data time: 1.17e+01, avg batch time: 12.5759, average train loss: 27.3156
[11/29 09:36:14 visual_prompt]: Inference (val):avg data time: 5.16e-05, avg batch time: 0.5172, average loss: 10.5741
[11/29 09:36:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.03	
[11/29 09:36:14 visual_prompt]: Best epoch 13: best metric: -10.574
[11/29 09:36:14 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/29 09:43:32 visual_prompt]: Epoch 14 / 100: avg data time: 1.16e+01, avg batch time: 12.5195, average train loss: 31.3186
[11/29 09:44:23 visual_prompt]: Inference (val):avg data time: 5.01e-05, avg batch time: 0.5149, average loss: 1.0008
[11/29 09:44:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 57.16	
[11/29 09:44:23 visual_prompt]: Best epoch 14: best metric: -1.001
[11/29 09:44:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/29 09:51:43 visual_prompt]: Epoch 15 / 100: avg data time: 1.17e+01, avg batch time: 12.5605, average train loss: 32.5071
[11/29 09:52:34 visual_prompt]: Inference (val):avg data time: 4.85e-05, avg batch time: 0.5211, average loss: 13.2455
[11/29 09:52:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.11	
[11/29 09:52:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/29 09:59:56 visual_prompt]: Epoch 16 / 100: avg data time: 1.18e+01, avg batch time: 12.6214, average train loss: 80.8210
[11/29 10:00:47 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.5227, average loss: 134.5552
[11/29 10:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.44	
[11/29 10:00:47 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/29 10:08:05 visual_prompt]: Epoch 17 / 100: avg data time: 1.17e+01, avg batch time: 12.5202, average train loss: 55.7091
[11/29 10:08:56 visual_prompt]: Inference (val):avg data time: 5.48e-05, avg batch time: 0.5312, average loss: 29.5391
[11/29 10:08:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.83	
[11/29 10:08:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/29 10:16:17 visual_prompt]: Epoch 18 / 100: avg data time: 1.17e+01, avg batch time: 12.5853, average train loss: 27.6651
[11/29 10:17:08 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.5185, average loss: 17.4893
[11/29 10:17:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.62	
[11/29 10:17:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/29 10:24:29 visual_prompt]: Epoch 19 / 100: avg data time: 1.17e+01, avg batch time: 12.5705, average train loss: 30.7046
[11/29 10:25:19 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.5139, average loss: 41.1395
[11/29 10:25:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.63	
[11/29 10:25:19 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/29 10:32:38 visual_prompt]: Epoch 20 / 100: avg data time: 1.16e+01, avg batch time: 12.5254, average train loss: 19.0451
[11/29 10:33:29 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.5155, average loss: 8.2098
[11/29 10:33:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.27	
[11/29 10:33:29 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/29 10:40:47 visual_prompt]: Epoch 21 / 100: avg data time: 1.16e+01, avg batch time: 12.5033, average train loss: 38.2065
[11/29 10:41:38 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.5223, average loss: 87.9089
[11/29 10:41:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.20	
[11/29 10:41:38 visual_prompt]: Stopping early.
[11/29 10:41:38 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 10:41:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 10:41:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 10:41:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 10:41:38 visual_prompt]: Training with config:
[11/29 10:41:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr25.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 10:41:38 visual_prompt]: Loading training data...
[11/29 10:41:38 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 10:41:38 visual_prompt]: Loading validation data...
[11/29 10:41:38 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 10:41:38 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/29 10:41:41 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/29 10:41:41 visual_prompt]: tuned percent:0.536
[11/29 10:41:42 visual_prompt]: Device used for model: 0
[11/29 10:41:42 visual_prompt]: Setting up Evaluator...
[11/29 10:41:42 visual_prompt]: Setting up Trainer...
[11/29 10:41:42 visual_prompt]: 	Setting up the optimizer...
[11/29 10:41:42 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 10:49:02 visual_prompt]: Epoch 1 / 100: avg data time: 1.17e+01, avg batch time: 12.5703, average train loss: 1.4006
[11/29 10:49:52 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.5276, average loss: 1.2969
[11/29 10:49:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/29 10:49:52 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/29 10:57:19 visual_prompt]: Epoch 2 / 100: avg data time: 1.19e+01, avg batch time: 12.7563, average train loss: 23.2444
[11/29 10:58:11 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.5190, average loss: 0.7595
[11/29 10:58:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 56.32	
[11/29 10:58:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/29 11:05:29 visual_prompt]: Epoch 3 / 100: avg data time: 1.17e+01, avg batch time: 12.5360, average train loss: 12.6302
[11/29 11:06:20 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.5161, average loss: 5.1407
[11/29 11:06:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.78	
[11/29 11:06:20 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/29 11:13:36 visual_prompt]: Epoch 4 / 100: avg data time: 1.16e+01, avg batch time: 12.4632, average train loss: 12.7690
[11/29 11:14:27 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.5216, average loss: 22.4596
[11/29 11:14:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.25	
[11/29 11:14:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/29 11:21:46 visual_prompt]: Epoch 5 / 100: avg data time: 1.17e+01, avg batch time: 12.5404, average train loss: 14.5014
[11/29 11:22:37 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.5178, average loss: 8.8947
[11/29 11:22:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.46	
[11/29 11:22:37 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/29 11:29:56 visual_prompt]: Epoch 6 / 100: avg data time: 1.17e+01, avg batch time: 12.5443, average train loss: 19.9883
[11/29 11:30:47 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.5167, average loss: 49.3884
[11/29 11:30:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.02	
[11/29 11:30:47 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/29 11:38:07 visual_prompt]: Epoch 7 / 100: avg data time: 1.17e+01, avg batch time: 12.5721, average train loss: 37.1695
[11/29 11:38:59 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.5262, average loss: 18.4642
[11/29 11:38:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.75	
[11/29 11:38:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/29 11:46:16 visual_prompt]: Epoch 8 / 100: avg data time: 1.16e+01, avg batch time: 12.4806, average train loss: 10.4049
[11/29 11:47:06 visual_prompt]: Inference (val):avg data time: 4.59e-05, avg batch time: 0.5180, average loss: 2.4897
[11/29 11:47:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[11/29 11:47:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/29 11:54:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.19e+01, avg batch time: 12.7703, average train loss: 23.1159
[11/29 11:55:24 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.5237, average loss: 1.9623
[11/29 11:55:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.83	
[11/29 11:55:24 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/29 12:02:42 visual_prompt]: Epoch 10 / 100: avg data time: 1.16e+01, avg batch time: 12.4969, average train loss: 40.3583
[11/29 12:03:32 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.5181, average loss: 94.7424
[11/29 12:03:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.32	
[11/29 12:03:32 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/29 12:10:52 visual_prompt]: Epoch 11 / 100: avg data time: 1.17e+01, avg batch time: 12.5619, average train loss: 55.4712
[11/29 12:11:43 visual_prompt]: Inference (val):avg data time: 4.98e-05, avg batch time: 0.5215, average loss: 162.2737
[11/29 12:11:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.03	
[11/29 12:11:43 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/29 12:19:03 visual_prompt]: Epoch 12 / 100: avg data time: 1.17e+01, avg batch time: 12.5791, average train loss: 35.7930
[11/29 12:19:54 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.5179, average loss: 1.2686
[11/29 12:19:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.94	
[11/29 12:19:54 visual_prompt]: Best epoch 12: best metric: -1.269
[11/29 12:19:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/29 12:27:14 visual_prompt]: Epoch 13 / 100: avg data time: 1.17e+01, avg batch time: 12.5502, average train loss: 44.0898
[11/29 12:28:04 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.5192, average loss: 38.4840
[11/29 12:28:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.53	
[11/29 12:28:04 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/29 12:35:21 visual_prompt]: Epoch 14 / 100: avg data time: 1.16e+01, avg batch time: 12.4626, average train loss: 59.5598
[11/29 12:36:12 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.5221, average loss: 82.5873
[11/29 12:36:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.52	
[11/29 12:36:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/29 12:43:30 visual_prompt]: Epoch 15 / 100: avg data time: 1.16e+01, avg batch time: 12.5220, average train loss: 38.4200
[11/29 12:44:21 visual_prompt]: Inference (val):avg data time: 4.86e-05, avg batch time: 0.5275, average loss: 45.0434
[11/29 12:44:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.27	
[11/29 12:44:21 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/29 12:51:41 visual_prompt]: Epoch 16 / 100: avg data time: 1.17e+01, avg batch time: 12.5527, average train loss: 32.8641
[11/29 12:52:32 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.5269, average loss: 23.7690
[11/29 12:52:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.69	
[11/29 12:52:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/29 12:59:49 visual_prompt]: Epoch 17 / 100: avg data time: 1.16e+01, avg batch time: 12.5034, average train loss: 15.7211
[11/29 13:00:40 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.5197, average loss: 30.0516
[11/29 13:00:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.51	
[11/29 13:00:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/29 13:07:58 visual_prompt]: Epoch 18 / 100: avg data time: 1.16e+01, avg batch time: 12.4984, average train loss: 16.7668
[11/29 13:08:49 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.5233, average loss: 34.4558
[11/29 13:08:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.33	
[11/29 13:08:49 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/29 13:16:32 visual_prompt]: Epoch 19 / 100: avg data time: 1.24e+01, avg batch time: 13.2390, average train loss: 12.1271
[11/29 13:17:29 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.5149, average loss: 19.3706
[11/29 13:17:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.33	
[11/29 13:17:29 visual_prompt]: Stopping early.
[11/29 13:17:30 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 13:17:30 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 13:17:30 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 13:17:30 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 13:17:30 visual_prompt]: Training with config:
[11/29 13:17:30 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr10.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 13:17:30 visual_prompt]: Loading training data...
[11/29 13:17:30 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 13:17:30 visual_prompt]: Loading validation data...
[11/29 13:17:30 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 13:17:30 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/29 13:17:45 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/29 13:17:45 visual_prompt]: tuned percent:0.536
[11/29 13:17:45 visual_prompt]: Device used for model: 0
[11/29 13:17:45 visual_prompt]: Setting up Evaluator...
[11/29 13:17:45 visual_prompt]: Setting up Trainer...
[11/29 13:17:45 visual_prompt]: 	Setting up the optimizer...
[11/29 13:17:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 13:25:21 visual_prompt]: Epoch 1 / 100: avg data time: 1.22e+01, avg batch time: 13.0347, average train loss: 1.4006
[11/29 13:26:12 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.5263, average loss: 1.2969
[11/29 13:26:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/29 13:26:12 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/29 13:33:31 visual_prompt]: Epoch 2 / 100: avg data time: 1.17e+01, avg batch time: 12.5328, average train loss: 9.6878
[11/29 13:34:21 visual_prompt]: Inference (val):avg data time: 5.17e-05, avg batch time: 0.5268, average loss: 1.7040
[11/29 13:34:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.49	
[11/29 13:34:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/29 13:41:38 visual_prompt]: Epoch 3 / 100: avg data time: 1.16e+01, avg batch time: 12.4858, average train loss: 2.8475
[11/29 13:42:29 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.5251, average loss: 0.7634
[11/29 13:42:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.50	
[11/29 13:42:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/29 13:49:47 visual_prompt]: Epoch 4 / 100: avg data time: 1.16e+01, avg batch time: 12.5091, average train loss: 6.9648
[11/29 13:50:38 visual_prompt]: Inference (val):avg data time: 4.66e-05, avg batch time: 0.5208, average loss: 0.7041
[11/29 13:50:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.91	
[11/29 13:50:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/29 13:57:55 visual_prompt]: Epoch 5 / 100: avg data time: 1.16e+01, avg batch time: 12.4852, average train loss: 10.4579
[11/29 13:58:49 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.5258, average loss: 13.9793
[11/29 13:58:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.79	
[11/29 13:58:49 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/29 14:06:14 visual_prompt]: Epoch 6 / 100: avg data time: 1.19e+01, avg batch time: 12.7289, average train loss: 7.5407
[11/29 14:07:05 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.5291, average loss: 8.0931
[11/29 14:07:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.43	
[11/29 14:07:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/29 14:14:23 visual_prompt]: Epoch 7 / 100: avg data time: 1.16e+01, avg batch time: 12.5123, average train loss: 12.2111
[11/29 14:15:14 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.5239, average loss: 3.7391
[11/29 14:15:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.29	
[11/29 14:15:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/29 14:22:40 visual_prompt]: Epoch 8 / 100: avg data time: 1.19e+01, avg batch time: 12.7600, average train loss: 15.8164
[11/29 14:23:31 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.5225, average loss: 13.4892
[11/29 14:23:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.16	
[11/29 14:23:31 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/29 14:30:52 visual_prompt]: Epoch 9 / 100: avg data time: 1.17e+01, avg batch time: 12.5708, average train loss: 14.4870
[11/29 14:31:47 visual_prompt]: Inference (val):avg data time: 5.05e-05, avg batch time: 0.5278, average loss: 52.3034
[11/29 14:31:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.99	
[11/29 14:31:47 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/29 14:38:56 visual_prompt]: Epoch 10 / 100: avg data time: 1.14e+01, avg batch time: 12.2613, average train loss: 24.2669
[11/29 14:39:45 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.5214, average loss: 5.5893
[11/29 14:39:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.14	
[11/29 14:39:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/29 14:46:51 visual_prompt]: Epoch 11 / 100: avg data time: 1.13e+01, avg batch time: 12.1658, average train loss: 27.1817
[11/29 14:47:40 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.5239, average loss: 27.1629
[11/29 14:47:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.60	
[11/29 14:47:40 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/29 14:55:12 visual_prompt]: Epoch 12 / 100: avg data time: 1.20e+01, avg batch time: 12.8984, average train loss: 30.5393
[11/29 14:56:07 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.5204, average loss: 59.9750
[11/29 14:56:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 39.84	
[11/29 14:56:07 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/29 15:03:38 visual_prompt]: Epoch 13 / 100: avg data time: 1.20e+01, avg batch time: 12.8983, average train loss: 23.8207
[11/29 15:04:32 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5216, average loss: 14.0262
[11/29 15:04:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.08	
[11/29 15:04:32 visual_prompt]: Best epoch 13: best metric: -14.026
[11/29 15:04:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/29 15:12:47 visual_prompt]: Epoch 14 / 100: avg data time: 1.33e+01, avg batch time: 14.1346, average train loss: 18.6387
[11/29 15:13:39 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.5259, average loss: 7.5840
[11/29 15:13:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[11/29 15:13:39 visual_prompt]: Best epoch 14: best metric: -7.584
[11/29 15:13:39 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/29 15:21:39 visual_prompt]: Epoch 15 / 100: avg data time: 1.28e+01, avg batch time: 13.7174, average train loss: 22.0395
[11/29 15:22:31 visual_prompt]: Inference (val):avg data time: 4.95e-05, avg batch time: 0.5266, average loss: 5.9340
[11/29 15:22:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.48	
[11/29 15:22:31 visual_prompt]: Best epoch 15: best metric: -5.934
[11/29 15:22:31 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/29 15:30:04 visual_prompt]: Epoch 16 / 100: avg data time: 1.21e+01, avg batch time: 12.9438, average train loss: 30.3283
[11/29 15:30:55 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.5273, average loss: 30.8174
[11/29 15:30:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.80	
[11/29 15:30:55 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/29 15:38:20 visual_prompt]: Epoch 17 / 100: avg data time: 1.18e+01, avg batch time: 12.7210, average train loss: 20.2773
[11/29 15:39:12 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.5240, average loss: 3.7752
[11/29 15:39:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.46	
[11/29 15:39:12 visual_prompt]: Best epoch 17: best metric: -3.775
[11/29 15:39:12 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/29 15:46:31 visual_prompt]: Epoch 18 / 100: avg data time: 1.17e+01, avg batch time: 12.5365, average train loss: 19.0705
[11/29 15:47:22 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.5237, average loss: 3.9265
[11/29 15:47:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.95	
[11/29 15:47:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/29 15:54:51 visual_prompt]: Epoch 19 / 100: avg data time: 1.20e+01, avg batch time: 12.8198, average train loss: 25.8139
[11/29 15:55:43 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.5261, average loss: 2.0072
[11/29 15:55:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.23	
[11/29 15:55:43 visual_prompt]: Best epoch 19: best metric: -2.007
[11/29 15:55:43 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/29 16:03:07 visual_prompt]: Epoch 20 / 100: avg data time: 1.18e+01, avg batch time: 12.6877, average train loss: 17.3089
[11/29 16:03:58 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.5218, average loss: 6.7882
[11/29 16:03:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.71	
[11/29 16:03:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/29 16:11:20 visual_prompt]: Epoch 21 / 100: avg data time: 1.17e+01, avg batch time: 12.6219, average train loss: 19.3203
[11/29 16:12:11 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.5233, average loss: 11.3943
[11/29 16:12:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.03	
[11/29 16:12:11 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/29 16:19:17 visual_prompt]: Epoch 22 / 100: avg data time: 1.13e+01, avg batch time: 12.1621, average train loss: 14.7027
[11/29 16:20:06 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.5253, average loss: 15.7471
[11/29 16:20:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.99	
[11/29 16:20:06 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/29 16:27:10 visual_prompt]: Epoch 23 / 100: avg data time: 1.12e+01, avg batch time: 12.0969, average train loss: 27.0608
[11/29 16:27:59 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.5240, average loss: 41.6329
[11/29 16:27:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.62	
[11/29 16:27:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/29 16:35:02 visual_prompt]: Epoch 24 / 100: avg data time: 1.12e+01, avg batch time: 12.0918, average train loss: 21.6319
[11/29 16:35:51 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.5304, average loss: 3.8854
[11/29 16:35:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.14	
[11/29 16:35:51 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/29 16:42:54 visual_prompt]: Epoch 25 / 100: avg data time: 1.12e+01, avg batch time: 12.0634, average train loss: 20.8695
[11/29 16:43:43 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.5246, average loss: 40.8797
[11/29 16:43:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.46	
[11/29 16:43:43 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/29 16:50:53 visual_prompt]: Epoch 26 / 100: avg data time: 1.14e+01, avg batch time: 12.2971, average train loss: 21.9512
[11/29 16:51:45 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.5172, average loss: 26.2833
[11/29 16:51:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.09	
[11/29 16:51:45 visual_prompt]: Stopping early.
[11/29 16:51:45 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 16:51:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 16:51:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 16:51:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 16:51:45 visual_prompt]: Training with config:
[11/29 16:51:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr10.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 16:51:45 visual_prompt]: Loading training data...
[11/29 16:51:45 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 16:51:45 visual_prompt]: Loading validation data...
[11/29 16:51:45 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 16:51:45 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/29 16:51:55 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/29 16:51:55 visual_prompt]: tuned percent:0.536
[11/29 16:51:55 visual_prompt]: Device used for model: 0
[11/29 16:51:55 visual_prompt]: Setting up Evaluator...
[11/29 16:51:55 visual_prompt]: Setting up Trainer...
[11/29 16:51:55 visual_prompt]: 	Setting up the optimizer...
[11/29 16:51:55 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 16:59:13 visual_prompt]: Epoch 1 / 100: avg data time: 1.16e+01, avg batch time: 12.5018, average train loss: 1.4006
[11/29 17:00:04 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.5255, average loss: 1.2969
[11/29 17:00:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/29 17:00:04 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/29 17:07:06 visual_prompt]: Epoch 2 / 100: avg data time: 1.12e+01, avg batch time: 12.0578, average train loss: 10.1201
[11/29 17:07:55 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.5279, average loss: 2.1930
[11/29 17:07:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.01	
[11/29 17:07:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/29 17:15:13 visual_prompt]: Epoch 3 / 100: avg data time: 1.16e+01, avg batch time: 12.4972, average train loss: 3.4053
[11/29 17:16:04 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.5237, average loss: 3.8260
[11/29 17:16:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.26	
[11/29 17:16:04 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/29 17:23:17 visual_prompt]: Epoch 4 / 100: avg data time: 1.15e+01, avg batch time: 12.3850, average train loss: 2.3014
[11/29 17:24:08 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.5195, average loss: 1.4221
[11/29 17:24:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.75	
[11/29 17:24:08 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/29 17:31:19 visual_prompt]: Epoch 5 / 100: avg data time: 1.14e+01, avg batch time: 12.3213, average train loss: 4.2847
[11/29 17:32:09 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.5225, average loss: 2.4232
[11/29 17:32:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.93	
[11/29 17:32:09 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/29 17:39:15 visual_prompt]: Epoch 6 / 100: avg data time: 1.13e+01, avg batch time: 12.1630, average train loss: 4.8928
[11/29 17:40:05 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.5241, average loss: 6.3698
[11/29 17:40:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.97	
[11/29 17:40:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/29 17:47:22 visual_prompt]: Epoch 7 / 100: avg data time: 1.16e+01, avg batch time: 12.4692, average train loss: 6.9463
[11/29 17:48:11 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.5280, average loss: 0.7822
[11/29 17:48:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.04	
[11/29 17:48:11 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/29 17:55:24 visual_prompt]: Epoch 8 / 100: avg data time: 1.15e+01, avg batch time: 12.3628, average train loss: 8.9275
[11/29 17:56:14 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.5257, average loss: 14.4215
[11/29 17:56:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.16	
[11/29 17:56:14 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/29 18:03:27 visual_prompt]: Epoch 9 / 100: avg data time: 1.15e+01, avg batch time: 12.3582, average train loss: 12.5080
[11/29 18:04:17 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.5277, average loss: 2.9173
[11/29 18:04:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.68	
[11/29 18:04:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/29 18:11:28 visual_prompt]: Epoch 10 / 100: avg data time: 1.14e+01, avg batch time: 12.3116, average train loss: 15.9372
[11/29 18:12:17 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.5224, average loss: 9.9117
[11/29 18:12:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.83	
[11/29 18:12:17 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/29 18:19:16 visual_prompt]: Epoch 11 / 100: avg data time: 1.11e+01, avg batch time: 11.9741, average train loss: 15.8442
[11/29 18:20:04 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.5224, average loss: 23.2342
[11/29 18:20:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.81	
[11/29 18:20:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/29 18:27:03 visual_prompt]: Epoch 12 / 100: avg data time: 1.11e+01, avg batch time: 11.9464, average train loss: 24.9834
[11/29 18:27:51 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.5260, average loss: 2.3868
[11/29 18:27:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[11/29 18:27:51 visual_prompt]: Best epoch 12: best metric: -2.387
[11/29 18:27:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/29 18:34:49 visual_prompt]: Epoch 13 / 100: avg data time: 1.11e+01, avg batch time: 11.9520, average train loss: 13.8200
[11/29 18:35:38 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5251, average loss: 5.9859
[11/29 18:35:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.45	
[11/29 18:35:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/29 18:42:37 visual_prompt]: Epoch 14 / 100: avg data time: 1.11e+01, avg batch time: 11.9803, average train loss: 11.9746
[11/29 18:43:26 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.5257, average loss: 1.8129
[11/29 18:43:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.24	
[11/29 18:43:26 visual_prompt]: Best epoch 14: best metric: -1.813
[11/29 18:43:26 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/29 18:50:26 visual_prompt]: Epoch 15 / 100: avg data time: 1.11e+01, avg batch time: 12.0018, average train loss: 20.0068
[11/29 18:51:15 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.5259, average loss: 27.1529
[11/29 18:51:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.63	
[11/29 18:51:15 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/29 18:58:15 visual_prompt]: Epoch 16 / 100: avg data time: 1.11e+01, avg batch time: 11.9898, average train loss: 20.6050
[11/29 18:59:03 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5208, average loss: 15.1098
[11/29 18:59:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.14	
[11/29 18:59:03 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/29 19:06:00 visual_prompt]: Epoch 17 / 100: avg data time: 1.10e+01, avg batch time: 11.9072, average train loss: 25.0266
[11/29 19:06:48 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.5201, average loss: 14.8906
[11/29 19:06:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.17	
[11/29 19:06:48 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/29 19:13:45 visual_prompt]: Epoch 18 / 100: avg data time: 1.10e+01, avg batch time: 11.9177, average train loss: 18.4176
[11/29 19:14:33 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.5247, average loss: 6.2258
[11/29 19:14:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.20	
[11/29 19:14:33 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/29 19:21:29 visual_prompt]: Epoch 19 / 100: avg data time: 1.10e+01, avg batch time: 11.8930, average train loss: 23.5572
[11/29 19:22:18 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5226, average loss: 55.8363
[11/29 19:22:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.49	
[11/29 19:22:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/29 19:29:14 visual_prompt]: Epoch 20 / 100: avg data time: 1.10e+01, avg batch time: 11.8841, average train loss: 26.3414
[11/29 19:30:02 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.5232, average loss: 39.1729
[11/29 19:30:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.58	
[11/29 19:30:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/29 19:36:59 visual_prompt]: Epoch 21 / 100: avg data time: 1.10e+01, avg batch time: 11.8903, average train loss: 21.2023
[11/29 19:37:47 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.5252, average loss: 24.5147
[11/29 19:37:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.00	
[11/29 19:37:47 visual_prompt]: Stopping early.
[11/29 19:37:47 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 19:37:47 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 19:37:47 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 19:37:47 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 19:37:47 visual_prompt]: Training with config:
[11/29 19:37:47 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr10.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 19:37:47 visual_prompt]: Loading training data...
[11/29 19:37:47 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 19:37:48 visual_prompt]: Loading validation data...
[11/29 19:37:48 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 19:37:48 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/29 19:37:57 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/29 19:37:57 visual_prompt]: tuned percent:0.536
[11/29 19:37:57 visual_prompt]: Device used for model: 0
[11/29 19:37:57 visual_prompt]: Setting up Evaluator...
[11/29 19:37:57 visual_prompt]: Setting up Trainer...
[11/29 19:37:57 visual_prompt]: 	Setting up the optimizer...
[11/29 19:37:57 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 19:44:55 visual_prompt]: Epoch 1 / 100: avg data time: 1.11e+01, avg batch time: 11.9480, average train loss: 1.4006
[11/29 19:45:43 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5261, average loss: 1.2969
[11/29 19:45:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/29 19:45:43 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/29 19:52:42 visual_prompt]: Epoch 2 / 100: avg data time: 1.11e+01, avg batch time: 11.9603, average train loss: 10.1987
[11/29 19:53:30 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5251, average loss: 2.5202
[11/29 19:53:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.96	
[11/29 19:53:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/29 20:00:27 visual_prompt]: Epoch 3 / 100: avg data time: 1.10e+01, avg batch time: 11.9131, average train loss: 2.5644
[11/29 20:01:15 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.5270, average loss: 3.9701
[11/29 20:01:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.83	
[11/29 20:01:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/29 20:08:14 visual_prompt]: Epoch 4 / 100: avg data time: 1.11e+01, avg batch time: 11.9699, average train loss: 2.8702
[11/29 20:09:03 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.5193, average loss: 1.1752
[11/29 20:09:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.86	
[11/29 20:09:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/29 20:15:59 visual_prompt]: Epoch 5 / 100: avg data time: 1.10e+01, avg batch time: 11.8838, average train loss: 6.8052
[11/29 20:16:47 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.5233, average loss: 8.3414
[11/29 20:16:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.14	
[11/29 20:16:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/29 20:23:44 visual_prompt]: Epoch 6 / 100: avg data time: 1.10e+01, avg batch time: 11.9107, average train loss: 8.7556
[11/29 20:24:33 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.5249, average loss: 2.1012
[11/29 20:24:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.26	
[11/29 20:24:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/29 20:31:30 visual_prompt]: Epoch 7 / 100: avg data time: 1.10e+01, avg batch time: 11.9147, average train loss: 4.3254
[11/29 20:32:18 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.5290, average loss: 3.5681
[11/29 20:32:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.86	
[11/29 20:32:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/29 20:39:13 visual_prompt]: Epoch 8 / 100: avg data time: 1.10e+01, avg batch time: 11.8719, average train loss: 3.1552
[11/29 20:40:02 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.5224, average loss: 2.1651
[11/29 20:40:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.32	
[11/29 20:40:02 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/29 20:46:59 visual_prompt]: Epoch 9 / 100: avg data time: 1.10e+01, avg batch time: 11.9212, average train loss: 8.6852
[11/29 20:47:47 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.5301, average loss: 2.7322
[11/29 20:47:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.15	
[11/29 20:47:47 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/29 20:54:43 visual_prompt]: Epoch 10 / 100: avg data time: 1.10e+01, avg batch time: 11.8871, average train loss: 12.1530
[11/29 20:55:32 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.5210, average loss: 7.9331
[11/29 20:55:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.03	
[11/29 20:55:32 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/29 21:02:29 visual_prompt]: Epoch 11 / 100: avg data time: 1.11e+01, avg batch time: 11.9232, average train loss: 23.0792
[11/29 21:03:17 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.5287, average loss: 16.2686
[11/29 21:03:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.97	
[11/29 21:03:17 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/29 21:10:13 visual_prompt]: Epoch 12 / 100: avg data time: 1.10e+01, avg batch time: 11.8820, average train loss: 24.5487
[11/29 21:11:01 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.5262, average loss: 49.6954
[11/29 21:11:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.95	
[11/29 21:11:01 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/29 21:17:58 visual_prompt]: Epoch 13 / 100: avg data time: 1.10e+01, avg batch time: 11.9100, average train loss: 21.6232
[11/29 21:18:47 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.5218, average loss: 38.3648
[11/29 21:18:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.55	
[11/29 21:18:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/29 21:25:43 visual_prompt]: Epoch 14 / 100: avg data time: 1.10e+01, avg batch time: 11.8784, average train loss: 28.2021
[11/29 21:26:31 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.5230, average loss: 11.8693
[11/29 21:26:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.33	
[11/29 21:26:31 visual_prompt]: Best epoch 14: best metric: -11.869
[11/29 21:26:31 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/29 21:33:29 visual_prompt]: Epoch 15 / 100: avg data time: 1.11e+01, avg batch time: 11.9302, average train loss: 7.1888
[11/29 21:34:17 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.5232, average loss: 11.5257
[11/29 21:34:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.11	
[11/29 21:34:17 visual_prompt]: Best epoch 15: best metric: -11.526
[11/29 21:34:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/29 21:41:13 visual_prompt]: Epoch 16 / 100: avg data time: 1.10e+01, avg batch time: 11.8836, average train loss: 5.9526
[11/29 21:42:01 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.5239, average loss: 10.4423
[11/29 21:42:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.45	
[11/29 21:42:01 visual_prompt]: Best epoch 16: best metric: -10.442
[11/29 21:42:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/29 21:48:58 visual_prompt]: Epoch 17 / 100: avg data time: 1.10e+01, avg batch time: 11.8978, average train loss: 8.1148
[11/29 21:49:46 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.5220, average loss: 6.6077
[11/29 21:49:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.45	
[11/29 21:49:46 visual_prompt]: Best epoch 17: best metric: -6.608
[11/29 21:49:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/29 21:56:42 visual_prompt]: Epoch 18 / 100: avg data time: 1.10e+01, avg batch time: 11.8786, average train loss: 11.7990
[11/29 21:57:30 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.5260, average loss: 3.0264
[11/29 21:57:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.08	
[11/29 21:57:30 visual_prompt]: Best epoch 18: best metric: -3.026
[11/29 21:57:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/29 22:04:30 visual_prompt]: Epoch 19 / 100: avg data time: 1.11e+01, avg batch time: 12.0041, average train loss: 8.5704
[11/29 22:05:21 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.5253, average loss: 0.7341
[11/29 22:05:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.34	
[11/29 22:05:21 visual_prompt]: Best epoch 19: best metric: -0.734
[11/29 22:05:21 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/29 22:12:34 visual_prompt]: Epoch 20 / 100: avg data time: 1.15e+01, avg batch time: 12.3757, average train loss: 3.7360
[11/29 22:13:22 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5239, average loss: 4.5141
[11/29 22:13:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.24	
[11/29 22:13:22 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/29 22:20:33 visual_prompt]: Epoch 21 / 100: avg data time: 1.14e+01, avg batch time: 12.3063, average train loss: 5.4714
[11/29 22:21:23 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.5287, average loss: 0.8325
[11/29 22:21:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.81	
[11/29 22:21:23 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/29 22:28:29 visual_prompt]: Epoch 22 / 100: avg data time: 1.13e+01, avg batch time: 12.1564, average train loss: 8.2995
[11/29 22:29:19 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.5251, average loss: 0.7415
[11/29 22:29:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.99	
[11/29 22:29:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/29 22:36:30 visual_prompt]: Epoch 23 / 100: avg data time: 1.14e+01, avg batch time: 12.2874, average train loss: 3.8361
[11/29 22:37:17 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.5203, average loss: 17.2913
[11/29 22:37:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.17	
[11/29 22:37:17 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/29 22:44:14 visual_prompt]: Epoch 24 / 100: avg data time: 1.10e+01, avg batch time: 11.8973, average train loss: 5.9416
[11/29 22:45:02 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.5240, average loss: 1.0192
[11/29 22:45:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.13	
[11/29 22:45:02 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/29 22:51:58 visual_prompt]: Epoch 25 / 100: avg data time: 1.10e+01, avg batch time: 11.8819, average train loss: 14.1372
[11/29 22:52:46 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.5280, average loss: 9.3590
[11/29 22:52:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.62	
[11/29 22:52:46 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/29 22:59:44 visual_prompt]: Epoch 26 / 100: avg data time: 1.11e+01, avg batch time: 11.9246, average train loss: 10.5590
[11/29 23:00:32 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5257, average loss: 6.5284
[11/29 23:00:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.73	
[11/29 23:00:32 visual_prompt]: Stopping early.
[11/29 23:00:32 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 23:00:32 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 23:00:32 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 23:00:32 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 23:00:32 visual_prompt]: Training with config:
[11/29 23:00:32 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr10.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 23:00:32 visual_prompt]: Loading training data...
[11/29 23:00:32 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 23:00:32 visual_prompt]: Loading validation data...
[11/29 23:00:32 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 23:00:32 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/29 23:00:40 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/29 23:00:40 visual_prompt]: tuned percent:0.536
[11/29 23:00:40 visual_prompt]: Device used for model: 0
[11/29 23:00:40 visual_prompt]: Setting up Evaluator...
[11/29 23:00:40 visual_prompt]: Setting up Trainer...
[11/29 23:00:40 visual_prompt]: 	Setting up the optimizer...
[11/29 23:00:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 23:07:36 visual_prompt]: Epoch 1 / 100: avg data time: 1.10e+01, avg batch time: 11.8965, average train loss: 1.4006
[11/29 23:08:25 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.5288, average loss: 1.2969
[11/29 23:08:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/29 23:08:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/29 23:15:21 visual_prompt]: Epoch 2 / 100: avg data time: 1.10e+01, avg batch time: 11.8895, average train loss: 10.2088
[11/29 23:16:09 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5331, average loss: 2.5505
[11/29 23:16:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[11/29 23:16:09 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/29 23:23:48 visual_prompt]: Epoch 3 / 100: avg data time: 1.22e+01, avg batch time: 13.1025, average train loss: 1.4470
[11/29 23:24:39 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.5277, average loss: 0.9457
[11/29 23:24:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.98	
[11/29 23:24:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/29 23:32:02 visual_prompt]: Epoch 4 / 100: avg data time: 1.18e+01, avg batch time: 12.6468, average train loss: 3.2261
[11/29 23:32:53 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.5314, average loss: 4.6026
[11/29 23:32:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.94	
[11/29 23:32:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/29 23:40:14 visual_prompt]: Epoch 5 / 100: avg data time: 1.17e+01, avg batch time: 12.6038, average train loss: 8.2075
[11/29 23:41:05 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.5268, average loss: 0.9075
[11/29 23:41:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.71	
[11/29 23:41:05 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/29 23:48:28 visual_prompt]: Epoch 6 / 100: avg data time: 1.18e+01, avg batch time: 12.6364, average train loss: 8.0717
[11/29 23:49:19 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5261, average loss: 6.1090
[11/29 23:49:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.48	
[11/29 23:49:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/29 23:56:41 visual_prompt]: Epoch 7 / 100: avg data time: 1.17e+01, avg batch time: 12.6275, average train loss: 6.7583
[11/29 23:57:32 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5315, average loss: 10.7021
[11/29 23:57:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.98	
[11/29 23:57:32 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/30 00:04:53 visual_prompt]: Epoch 8 / 100: avg data time: 1.17e+01, avg batch time: 12.6098, average train loss: 9.9355
[11/30 00:05:44 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.5233, average loss: 0.7084
[11/30 00:05:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.04	
[11/30 00:05:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/30 00:13:07 visual_prompt]: Epoch 9 / 100: avg data time: 1.18e+01, avg batch time: 12.6404, average train loss: 3.5084
[11/30 00:13:57 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.5288, average loss: 7.9393
[11/30 00:13:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.22	
[11/30 00:13:57 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/30 00:21:19 visual_prompt]: Epoch 10 / 100: avg data time: 1.17e+01, avg batch time: 12.6157, average train loss: 8.4031
[11/30 00:22:10 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.5304, average loss: 14.8827
[11/30 00:22:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.81	
[11/30 00:22:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/30 00:29:32 visual_prompt]: Epoch 11 / 100: avg data time: 1.17e+01, avg batch time: 12.6135, average train loss: 8.3237
[11/30 00:30:23 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.5300, average loss: 3.5747
[11/30 00:30:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.38	
[11/30 00:30:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/30 00:37:44 visual_prompt]: Epoch 12 / 100: avg data time: 1.17e+01, avg batch time: 12.6115, average train loss: 6.5167
[11/30 00:38:35 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.5253, average loss: 15.7908
[11/30 00:38:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.81	
[11/30 00:38:35 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/30 00:45:57 visual_prompt]: Epoch 13 / 100: avg data time: 1.18e+01, avg batch time: 12.6288, average train loss: 6.7646
[11/30 00:46:48 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.5339, average loss: 3.6533
[11/30 00:46:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.14	
[11/30 00:46:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/30 00:54:10 visual_prompt]: Epoch 14 / 100: avg data time: 1.17e+01, avg batch time: 12.6021, average train loss: 2.7122
[11/30 00:55:00 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.5285, average loss: 1.0168
[11/30 00:55:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.00	
[11/30 00:55:01 visual_prompt]: Best epoch 14: best metric: -1.017
[11/30 00:55:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/30 01:02:36 visual_prompt]: Epoch 15 / 100: avg data time: 1.21e+01, avg batch time: 13.0097, average train loss: 6.1370
[11/30 01:03:28 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.5313, average loss: 3.7023
[11/30 01:03:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.86	
[11/30 01:03:28 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/30 01:10:55 visual_prompt]: Epoch 16 / 100: avg data time: 1.19e+01, avg batch time: 12.7546, average train loss: 10.8778
[11/30 01:11:46 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.5269, average loss: 4.6222
[11/30 01:11:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.79	
[11/30 01:11:46 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/30 01:19:07 visual_prompt]: Epoch 17 / 100: avg data time: 1.17e+01, avg batch time: 12.5887, average train loss: 4.8772
[11/30 01:19:58 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5318, average loss: 10.4433
[11/30 01:19:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.85	
[11/30 01:19:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/30 01:26:59 visual_prompt]: Epoch 18 / 100: avg data time: 1.12e+01, avg batch time: 12.0321, average train loss: 12.3658
[11/30 01:27:47 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5326, average loss: 4.3448
[11/30 01:27:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.19	
[11/30 01:27:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/30 01:34:43 visual_prompt]: Epoch 19 / 100: avg data time: 1.10e+01, avg batch time: 11.8779, average train loss: 15.7922
[11/30 01:35:31 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.5286, average loss: 8.2085
[11/30 01:35:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.86	
[11/30 01:35:31 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/30 01:42:26 visual_prompt]: Epoch 20 / 100: avg data time: 1.10e+01, avg batch time: 11.8530, average train loss: 5.2009
[11/30 01:43:14 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.5254, average loss: 1.2667
[11/30 01:43:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.51	
[11/30 01:43:14 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/30 01:50:09 visual_prompt]: Epoch 21 / 100: avg data time: 1.10e+01, avg batch time: 11.8608, average train loss: 7.9571
[11/30 01:50:57 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.5290, average loss: 13.6850
[11/30 01:50:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.65	
[11/30 01:50:57 visual_prompt]: Stopping early.
[11/30 01:50:57 visual_prompt]: Rank of current process: 0. World size: 1
[11/30 01:50:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/30 01:50:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/30 01:50:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/30 01:50:57 visual_prompt]: Training with config:
[11/30 01:50:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr5.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/30 01:50:57 visual_prompt]: Loading training data...
[11/30 01:50:57 visual_prompt]: Constructing mammo-cbis dataset train...
[11/30 01:50:58 visual_prompt]: Loading validation data...
[11/30 01:50:58 visual_prompt]: Constructing mammo-cbis dataset val...
[11/30 01:50:58 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/30 01:51:05 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/30 01:51:05 visual_prompt]: tuned percent:0.536
[11/30 01:51:05 visual_prompt]: Device used for model: 0
[11/30 01:51:05 visual_prompt]: Setting up Evaluator...
[11/30 01:51:05 visual_prompt]: Setting up Trainer...
[11/30 01:51:05 visual_prompt]: 	Setting up the optimizer...
[11/30 01:51:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/30 01:58:04 visual_prompt]: Epoch 1 / 100: avg data time: 1.11e+01, avg batch time: 11.9497, average train loss: 1.4006
[11/30 01:58:52 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.5261, average loss: 1.2969
[11/30 01:58:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/30 01:58:52 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/30 02:05:49 visual_prompt]: Epoch 2 / 100: avg data time: 1.10e+01, avg batch time: 11.9112, average train loss: 5.3540
[11/30 02:06:37 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.5165, average loss: 0.9691
[11/30 02:06:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.01	
[11/30 02:06:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/30 02:13:33 visual_prompt]: Epoch 3 / 100: avg data time: 1.10e+01, avg batch time: 11.8794, average train loss: 1.2822
[11/30 02:14:21 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.5247, average loss: 0.6994
[11/30 02:14:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.19	
[11/30 02:14:21 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/30 02:21:18 visual_prompt]: Epoch 4 / 100: avg data time: 1.10e+01, avg batch time: 11.8978, average train loss: 1.1404
[11/30 02:22:06 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.5336, average loss: 1.8387
[11/30 02:22:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.47	
[11/30 02:22:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/30 02:29:01 visual_prompt]: Epoch 5 / 100: avg data time: 1.10e+01, avg batch time: 11.8742, average train loss: 2.1522
[11/30 02:29:50 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.5152, average loss: 0.6887
[11/30 02:29:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.17	
[11/30 02:29:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/30 02:36:46 visual_prompt]: Epoch 6 / 100: avg data time: 1.10e+01, avg batch time: 11.9001, average train loss: 6.3042
[11/30 02:37:34 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.5252, average loss: 5.0757
[11/30 02:37:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.47	
[11/30 02:37:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/30 02:44:31 visual_prompt]: Epoch 7 / 100: avg data time: 1.10e+01, avg batch time: 11.9143, average train loss: 5.9115
[11/30 02:45:20 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5144, average loss: 1.5645
[11/30 02:45:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.22	
[11/30 02:45:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/30 02:52:16 visual_prompt]: Epoch 8 / 100: avg data time: 1.10e+01, avg batch time: 11.9102, average train loss: 8.5357
[11/30 02:53:05 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.5266, average loss: 5.0684
[11/30 02:53:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.48	
[11/30 02:53:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/30 03:00:01 visual_prompt]: Epoch 9 / 100: avg data time: 1.10e+01, avg batch time: 11.8976, average train loss: 5.7838
[11/30 03:00:50 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5181, average loss: 24.0742
[11/30 03:00:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.85	
[11/30 03:00:50 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/30 03:07:45 visual_prompt]: Epoch 10 / 100: avg data time: 1.10e+01, avg batch time: 11.8683, average train loss: 7.1797
[11/30 03:08:33 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.5208, average loss: 7.2378
[11/30 03:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.00	
[11/30 03:08:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/30 03:15:32 visual_prompt]: Epoch 11 / 100: avg data time: 1.11e+01, avg batch time: 11.9528, average train loss: 10.4551
[11/30 03:16:20 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5199, average loss: 34.7399
[11/30 03:16:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.90	
[11/30 03:16:20 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/30 03:23:18 visual_prompt]: Epoch 12 / 100: avg data time: 1.10e+01, avg batch time: 11.9199, average train loss: 15.2720
[11/30 03:24:06 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.5186, average loss: 3.7902
[11/30 03:24:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.45	
[11/30 03:24:06 visual_prompt]: Best epoch 12: best metric: -3.790
[11/30 03:24:06 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/30 03:31:04 visual_prompt]: Epoch 13 / 100: avg data time: 1.11e+01, avg batch time: 11.9290, average train loss: 10.2133
[11/30 03:31:52 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.5214, average loss: 4.6997
[11/30 03:31:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 47.95	
[11/30 03:31:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/30 03:38:48 visual_prompt]: Epoch 14 / 100: avg data time: 1.10e+01, avg batch time: 11.8826, average train loss: 11.8191
[11/30 03:39:36 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.5188, average loss: 1.1306
[11/30 03:39:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.56	
[11/30 03:39:36 visual_prompt]: Best epoch 14: best metric: -1.131
[11/30 03:39:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/30 03:46:33 visual_prompt]: Epoch 15 / 100: avg data time: 1.10e+01, avg batch time: 11.9179, average train loss: 9.8719
[11/30 03:47:22 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5242, average loss: 7.8313
[11/30 03:47:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.07	
[11/30 03:47:22 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/30 03:54:18 visual_prompt]: Epoch 16 / 100: avg data time: 1.10e+01, avg batch time: 11.9005, average train loss: 11.0199
[11/30 03:55:02 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.5235, average loss: 0.7000
[11/30 03:55:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 42.68	rocauc: 43.42	
[11/30 03:55:02 visual_prompt]: Best epoch 16: best metric: -0.700
[11/30 03:55:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/30 04:01:16 visual_prompt]: Epoch 17 / 100: avg data time: 9.82e+00, avg batch time: 10.6901, average train loss: 13.2595
[11/30 04:01:59 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.5380, average loss: 2.2908
[11/30 04:01:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.87	
[11/30 04:01:59 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/30 04:08:14 visual_prompt]: Epoch 18 / 100: avg data time: 9.82e+00, avg batch time: 10.6943, average train loss: 13.2253
[11/30 04:08:57 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5194, average loss: 6.6935
[11/30 04:08:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.36	
[11/30 04:08:57 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/30 04:15:11 visual_prompt]: Epoch 19 / 100: avg data time: 9.81e+00, avg batch time: 10.6815, average train loss: 8.1761
[11/30 04:15:55 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5100, average loss: 20.1681
[11/30 04:15:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.01	
[11/30 04:15:55 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/30 04:22:09 visual_prompt]: Epoch 20 / 100: avg data time: 9.82e+00, avg batch time: 10.6930, average train loss: 11.8562
[11/30 04:22:53 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.5159, average loss: 10.9958
[11/30 04:22:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.62	
[11/30 04:22:53 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/30 04:29:07 visual_prompt]: Epoch 21 / 100: avg data time: 9.82e+00, avg batch time: 10.6884, average train loss: 7.4542
[11/30 04:29:50 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.5258, average loss: 12.9450
[11/30 04:29:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 37.12	
[11/30 04:29:50 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/30 04:36:04 visual_prompt]: Epoch 22 / 100: avg data time: 9.81e+00, avg batch time: 10.6860, average train loss: 10.5049
[11/30 04:36:48 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5167, average loss: 12.7140
[11/30 04:36:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.13	
[11/30 04:36:48 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/30 04:43:02 visual_prompt]: Epoch 23 / 100: avg data time: 9.83e+00, avg batch time: 10.6955, average train loss: 7.5065
[11/30 04:43:46 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5134, average loss: 17.8515
[11/30 04:43:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.70	
[11/30 04:43:46 visual_prompt]: Stopping early.
[11/30 04:43:46 visual_prompt]: Rank of current process: 0. World size: 1
[11/30 04:43:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/30 04:43:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/30 04:43:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/30 04:43:46 visual_prompt]: Training with config:
[11/30 04:43:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr5.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/30 04:43:46 visual_prompt]: Loading training data...
[11/30 04:43:46 visual_prompt]: Constructing mammo-cbis dataset train...
[11/30 04:43:46 visual_prompt]: Loading validation data...
[11/30 04:43:46 visual_prompt]: Constructing mammo-cbis dataset val...
[11/30 04:43:46 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/30 04:43:49 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/30 04:43:49 visual_prompt]: tuned percent:0.536
[11/30 04:43:49 visual_prompt]: Device used for model: 0
[11/30 04:43:49 visual_prompt]: Setting up Evaluator...
[11/30 04:43:49 visual_prompt]: Setting up Trainer...
[11/30 04:43:49 visual_prompt]: 	Setting up the optimizer...
[11/30 04:43:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/30 04:50:04 visual_prompt]: Epoch 1 / 100: avg data time: 9.84e+00, avg batch time: 10.7204, average train loss: 1.4006
[11/30 04:50:48 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.5269, average loss: 1.2969
[11/30 04:50:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/30 04:50:48 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/30 04:57:02 visual_prompt]: Epoch 2 / 100: avg data time: 9.81e+00, avg batch time: 10.6918, average train loss: 5.6515
[11/30 04:57:45 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.5203, average loss: 1.1846
[11/30 04:57:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/30 04:57:45 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/30 05:04:00 visual_prompt]: Epoch 3 / 100: avg data time: 9.83e+00, avg batch time: 10.7025, average train loss: 0.9254
[11/30 05:04:44 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5159, average loss: 0.7127
[11/30 05:04:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.36	
[11/30 05:04:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/30 05:10:58 visual_prompt]: Epoch 4 / 100: avg data time: 9.83e+00, avg batch time: 10.7010, average train loss: 1.5246
[11/30 05:11:42 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5215, average loss: 0.8991
[11/30 05:11:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.06	
[11/30 05:11:42 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/30 05:17:56 visual_prompt]: Epoch 5 / 100: avg data time: 9.81e+00, avg batch time: 10.6851, average train loss: 1.1852
[11/30 05:18:39 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.5209, average loss: 0.8050
[11/30 05:18:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.98	
[11/30 05:18:39 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/30 05:24:54 visual_prompt]: Epoch 6 / 100: avg data time: 9.84e+00, avg batch time: 10.7087, average train loss: 3.0204
[11/30 05:25:38 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.5143, average loss: 4.9522
[11/30 05:25:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.74	
[11/30 05:25:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/30 05:31:52 visual_prompt]: Epoch 7 / 100: avg data time: 9.83e+00, avg batch time: 10.7047, average train loss: 4.1733
[11/30 05:32:36 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.5263, average loss: 4.9571
[11/30 05:32:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.44	
[11/30 05:32:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/30 05:38:50 visual_prompt]: Epoch 8 / 100: avg data time: 9.81e+00, avg batch time: 10.6894, average train loss: 3.5050
[11/30 05:39:33 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.5256, average loss: 13.0725
[11/30 05:39:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.72	
[11/30 05:39:33 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/30 05:45:49 visual_prompt]: Epoch 9 / 100: avg data time: 9.84e+00, avg batch time: 10.7146, average train loss: 5.9956
[11/30 05:46:32 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.5224, average loss: 1.8695
[11/30 05:46:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.56	
[11/30 05:46:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/30 05:52:47 visual_prompt]: Epoch 10 / 100: avg data time: 9.83e+00, avg batch time: 10.7041, average train loss: 7.1528
[11/30 05:53:30 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5165, average loss: 18.8593
[11/30 05:53:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.79	
[11/30 05:53:30 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/30 05:59:45 visual_prompt]: Epoch 11 / 100: avg data time: 9.83e+00, avg batch time: 10.7127, average train loss: 12.3416
[11/30 06:00:29 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5274, average loss: 12.8336
[11/30 06:00:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.99	
[11/30 06:00:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/30 06:06:43 visual_prompt]: Epoch 12 / 100: avg data time: 9.82e+00, avg batch time: 10.6995, average train loss: 7.6709
[11/30 06:07:27 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5161, average loss: 6.9060
[11/30 06:07:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.25	
[11/30 06:07:27 visual_prompt]: Best epoch 12: best metric: -6.906
[11/30 06:07:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/30 06:13:41 visual_prompt]: Epoch 13 / 100: avg data time: 9.83e+00, avg batch time: 10.7030, average train loss: 10.3000
[11/30 06:14:25 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.5206, average loss: 5.5765
[11/30 06:14:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.12	
[11/30 06:14:25 visual_prompt]: Best epoch 13: best metric: -5.576
[11/30 06:14:25 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/30 06:20:39 visual_prompt]: Epoch 14 / 100: avg data time: 9.81e+00, avg batch time: 10.6827, average train loss: 5.7824
[11/30 06:21:22 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.5107, average loss: 8.5292
[11/30 06:21:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.53	
[11/30 06:21:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/30 06:27:37 visual_prompt]: Epoch 15 / 100: avg data time: 9.83e+00, avg batch time: 10.7024, average train loss: 6.0883
[11/30 06:28:20 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.5278, average loss: 6.2720
[11/30 06:28:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.47	
[11/30 06:28:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/30 06:34:35 visual_prompt]: Epoch 16 / 100: avg data time: 9.81e+00, avg batch time: 10.6876, average train loss: 5.6700
[11/30 06:35:18 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5236, average loss: 1.3561
[11/30 06:35:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.82	
[11/30 06:35:18 visual_prompt]: Best epoch 16: best metric: -1.356
[11/30 06:35:18 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/30 06:41:32 visual_prompt]: Epoch 17 / 100: avg data time: 9.80e+00, avg batch time: 10.6755, average train loss: 7.1390
[11/30 06:42:15 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.5209, average loss: 0.7291
[11/30 06:42:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.13	
[11/30 06:42:15 visual_prompt]: Best epoch 17: best metric: -0.729
[11/30 06:42:15 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/30 06:48:29 visual_prompt]: Epoch 18 / 100: avg data time: 9.81e+00, avg batch time: 10.6915, average train loss: 7.4325
[11/30 06:49:13 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.5229, average loss: 3.8594
[11/30 06:49:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.68	
[11/30 06:49:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/30 06:55:28 visual_prompt]: Epoch 19 / 100: avg data time: 9.83e+00, avg batch time: 10.7034, average train loss: 5.6259
[11/30 06:56:11 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.5153, average loss: 5.7350
[11/30 06:56:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.33	
[11/30 06:56:11 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/30 07:02:26 visual_prompt]: Epoch 20 / 100: avg data time: 9.83e+00, avg batch time: 10.7017, average train loss: 4.2629
[11/30 07:03:09 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5255, average loss: 0.6868
[11/30 07:03:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.13	
[11/30 07:03:09 visual_prompt]: Best epoch 20: best metric: -0.687
[11/30 07:03:09 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/30 07:09:23 visual_prompt]: Epoch 21 / 100: avg data time: 9.81e+00, avg batch time: 10.6886, average train loss: 4.0492
[11/30 07:10:07 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5243, average loss: 4.1133
[11/30 07:10:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.81	
[11/30 07:10:07 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/30 07:16:21 visual_prompt]: Epoch 22 / 100: avg data time: 9.81e+00, avg batch time: 10.6843, average train loss: 4.6679
[11/30 07:17:04 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.5196, average loss: 5.4969
[11/30 07:17:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.48	
[11/30 07:17:04 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/30 07:23:19 visual_prompt]: Epoch 23 / 100: avg data time: 9.83e+00, avg batch time: 10.7064, average train loss: 10.6579
[11/30 07:24:03 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5271, average loss: 1.3095
[11/30 07:24:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.06	
[11/30 07:24:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/30 07:30:18 visual_prompt]: Epoch 24 / 100: avg data time: 9.85e+00, avg batch time: 10.7202, average train loss: 4.0030
[11/30 07:31:03 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5283, average loss: 0.7067
[11/30 07:31:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.04	
[11/30 07:31:03 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/30 07:37:17 visual_prompt]: Epoch 25 / 100: avg data time: 9.81e+00, avg batch time: 10.6895, average train loss: 5.7001
[11/30 07:38:00 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.5288, average loss: 7.7067
[11/30 07:38:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.19	
[11/30 07:38:00 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/30 07:44:14 visual_prompt]: Epoch 26 / 100: avg data time: 9.81e+00, avg batch time: 10.6886, average train loss: 3.7169
[11/30 07:44:58 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.5259, average loss: 5.6729
[11/30 07:44:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.40	
[11/30 07:44:58 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/30 07:51:12 visual_prompt]: Epoch 27 / 100: avg data time: 9.81e+00, avg batch time: 10.6871, average train loss: 8.9983
[11/30 07:51:55 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5259, average loss: 0.9146
[11/30 07:51:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.02	
[11/30 07:51:55 visual_prompt]: Stopping early.
[11/30 07:51:56 visual_prompt]: Rank of current process: 0. World size: 1
[11/30 07:51:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/30 07:51:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/30 07:51:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/30 07:51:56 visual_prompt]: Training with config:
[11/30 07:51:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr5.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/30 07:51:56 visual_prompt]: Loading training data...
[11/30 07:51:56 visual_prompt]: Constructing mammo-cbis dataset train...
[11/30 07:51:56 visual_prompt]: Loading validation data...
[11/30 07:51:56 visual_prompt]: Constructing mammo-cbis dataset val...
[11/30 07:51:56 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/30 07:51:58 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/30 07:51:58 visual_prompt]: tuned percent:0.536
[11/30 07:51:58 visual_prompt]: Device used for model: 0
[11/30 07:51:58 visual_prompt]: Setting up Evaluator...
[11/30 07:51:58 visual_prompt]: Setting up Trainer...
[11/30 07:51:58 visual_prompt]: 	Setting up the optimizer...
[11/30 07:51:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/30 07:58:19 visual_prompt]: Epoch 1 / 100: avg data time: 1.00e+01, avg batch time: 10.8803, average train loss: 1.4006
[11/30 07:59:03 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5187, average loss: 1.2969
[11/30 07:59:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/30 07:59:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/30 08:05:18 visual_prompt]: Epoch 2 / 100: avg data time: 9.86e+00, avg batch time: 10.7302, average train loss: 5.6991
[11/30 08:06:02 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5097, average loss: 1.3484
[11/30 08:06:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.43	
[11/30 08:06:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/30 08:12:17 visual_prompt]: Epoch 3 / 100: avg data time: 9.84e+00, avg batch time: 10.7115, average train loss: 0.9334
[11/30 08:13:00 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5225, average loss: 0.7732
[11/30 08:13:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.68	
[11/30 08:13:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/30 08:19:15 visual_prompt]: Epoch 4 / 100: avg data time: 9.84e+00, avg batch time: 10.7051, average train loss: 1.5592
[11/30 08:19:59 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5206, average loss: 0.7553
[11/30 08:19:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.69	
[11/30 08:19:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/30 08:26:13 visual_prompt]: Epoch 5 / 100: avg data time: 9.82e+00, avg batch time: 10.6933, average train loss: 3.2565
[11/30 08:26:57 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5183, average loss: 6.8707
[11/30 08:26:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.82	
[11/30 08:26:57 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/30 08:33:12 visual_prompt]: Epoch 6 / 100: avg data time: 9.85e+00, avg batch time: 10.7266, average train loss: 3.6536
[11/30 08:33:56 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5172, average loss: 5.5577
[11/30 08:33:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.85	
[11/30 08:33:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/30 08:40:11 visual_prompt]: Epoch 7 / 100: avg data time: 9.85e+00, avg batch time: 10.7178, average train loss: 4.7739
[11/30 08:40:54 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.5219, average loss: 6.8771
[11/30 08:40:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.32	
[11/30 08:40:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/30 08:47:09 visual_prompt]: Epoch 8 / 100: avg data time: 9.83e+00, avg batch time: 10.7002, average train loss: 4.3686
[11/30 08:47:52 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5231, average loss: 0.6893
[11/30 08:47:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.69	
[11/30 08:47:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/30 08:54:16 visual_prompt]: Epoch 9 / 100: avg data time: 1.01e+01, avg batch time: 10.9582, average train loss: 3.1863
[11/30 08:55:02 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.5222, average loss: 1.6099
[11/30 08:55:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.93	
[11/30 08:55:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/30 09:01:36 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 11.2657, average train loss: 1.8370
[11/30 09:02:21 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5121, average loss: 2.4886
[11/30 09:02:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.29	
[11/30 09:02:21 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/30 09:08:38 visual_prompt]: Epoch 11 / 100: avg data time: 9.88e+00, avg batch time: 10.7438, average train loss: 2.9984
[11/30 09:09:21 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5187, average loss: 2.5507
[11/30 09:09:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.14	
[11/30 09:09:21 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/30 09:15:35 visual_prompt]: Epoch 12 / 100: avg data time: 9.82e+00, avg batch time: 10.6945, average train loss: 9.4320
[11/30 09:16:19 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.5181, average loss: 10.1587
[11/30 09:16:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.34	
[11/30 09:16:19 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/30 09:22:34 visual_prompt]: Epoch 13 / 100: avg data time: 9.84e+00, avg batch time: 10.7071, average train loss: 11.7931
[11/30 09:23:17 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.5228, average loss: 1.5056
[11/30 09:23:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.05	
[11/30 09:23:17 visual_prompt]: Best epoch 13: best metric: -1.506
[11/30 09:23:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/30 09:29:31 visual_prompt]: Epoch 14 / 100: avg data time: 9.82e+00, avg batch time: 10.6933, average train loss: 8.6446
[11/30 09:30:15 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5176, average loss: 3.5846
[11/30 09:30:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.64	
[11/30 09:30:15 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/30 09:36:30 visual_prompt]: Epoch 15 / 100: avg data time: 9.84e+00, avg batch time: 10.7137, average train loss: 6.4145
[11/30 09:37:13 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5221, average loss: 5.1652
[11/30 09:37:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.25	
[11/30 09:37:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/30 09:43:30 visual_prompt]: Epoch 16 / 100: avg data time: 9.87e+00, avg batch time: 10.7441, average train loss: 11.6323
[11/30 09:44:13 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.5209, average loss: 5.3893
[11/30 09:44:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.74	
[11/30 09:44:13 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/30 09:50:27 visual_prompt]: Epoch 17 / 100: avg data time: 9.81e+00, avg batch time: 10.6843, average train loss: 10.2096
[11/30 09:51:10 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.5108, average loss: 14.3606
[11/30 09:51:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.17	
[11/30 09:51:10 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/30 09:57:25 visual_prompt]: Epoch 18 / 100: avg data time: 9.83e+00, avg batch time: 10.6937, average train loss: 12.3201
[11/30 09:58:08 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5186, average loss: 30.1740
[11/30 09:58:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.58	
[11/30 09:58:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/30 10:04:22 visual_prompt]: Epoch 19 / 100: avg data time: 9.81e+00, avg batch time: 10.6861, average train loss: 11.5244
[11/30 10:05:06 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5267, average loss: 5.7095
[11/30 10:05:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.61	
[11/30 10:05:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/30 10:11:21 visual_prompt]: Epoch 20 / 100: avg data time: 9.84e+00, avg batch time: 10.7086, average train loss: 7.2507
[11/30 10:12:04 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5208, average loss: 4.1722
[11/30 10:12:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[11/30 10:12:04 visual_prompt]: Stopping early.
[11/30 10:12:04 visual_prompt]: Rank of current process: 0. World size: 1
[11/30 10:12:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/30 10:12:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/30 10:12:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/30 10:12:04 visual_prompt]: Training with config:
[11/30 10:12:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr5.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/30 10:12:04 visual_prompt]: Loading training data...
[11/30 10:12:04 visual_prompt]: Constructing mammo-cbis dataset train...
[11/30 10:12:04 visual_prompt]: Loading validation data...
[11/30 10:12:04 visual_prompt]: Constructing mammo-cbis dataset val...
[11/30 10:12:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/30 10:12:13 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/30 10:12:13 visual_prompt]: tuned percent:0.536
[11/30 10:12:13 visual_prompt]: Device used for model: 0
[11/30 10:12:13 visual_prompt]: Setting up Evaluator...
[11/30 10:12:13 visual_prompt]: Setting up Trainer...
[11/30 10:12:13 visual_prompt]: 	Setting up the optimizer...
[11/30 10:12:13 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/30 10:18:28 visual_prompt]: Epoch 1 / 100: avg data time: 9.83e+00, avg batch time: 10.7021, average train loss: 1.4006
[11/30 10:19:11 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.5142, average loss: 1.2969
[11/30 10:19:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/30 10:19:11 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/30 10:25:25 visual_prompt]: Epoch 2 / 100: avg data time: 9.81e+00, avg batch time: 10.6857, average train loss: 5.6945
[11/30 10:26:08 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5237, average loss: 1.3670
[11/30 10:26:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.55	
[11/30 10:26:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/30 10:32:22 visual_prompt]: Epoch 3 / 100: avg data time: 9.81e+00, avg batch time: 10.6847, average train loss: 0.9387
[11/30 10:33:06 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.5223, average loss: 0.7804
[11/30 10:33:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.10	
[11/30 10:33:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/30 10:39:20 visual_prompt]: Epoch 4 / 100: avg data time: 9.82e+00, avg batch time: 10.6901, average train loss: 1.6253
[11/30 10:40:04 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5233, average loss: 5.5548
[11/30 10:40:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.64	
[11/30 10:40:04 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/30 10:46:17 visual_prompt]: Epoch 5 / 100: avg data time: 9.79e+00, avg batch time: 10.6654, average train loss: 2.0992
[11/30 10:47:00 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5271, average loss: 3.6663
[11/30 10:47:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.35	
[11/30 10:47:00 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/30 10:53:17 visual_prompt]: Epoch 6 / 100: avg data time: 9.87e+00, avg batch time: 10.7468, average train loss: 4.8855
[11/30 10:54:00 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5163, average loss: 6.1723
[11/30 10:54:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.58	
[11/30 10:54:00 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/30 11:00:15 visual_prompt]: Epoch 7 / 100: avg data time: 9.85e+00, avg batch time: 10.7166, average train loss: 4.1054
[11/30 11:00:59 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.5264, average loss: 6.9784
[11/30 11:00:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.44	
[11/30 11:00:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/30 11:07:13 visual_prompt]: Epoch 8 / 100: avg data time: 9.81e+00, avg batch time: 10.6892, average train loss: 7.3526
[11/30 11:07:56 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5242, average loss: 1.3838
[11/30 11:07:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.31	
[11/30 11:07:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/30 11:14:43 visual_prompt]: Epoch 9 / 100: avg data time: 1.08e+01, avg batch time: 11.6323, average train loss: 5.3312
[11/30 11:15:32 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5230, average loss: 7.9830
[11/30 11:15:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.21	
[11/30 11:15:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/30 11:22:01 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 11.1340, average train loss: 2.3202
[11/30 11:22:45 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.5268, average loss: 0.7197
[11/30 11:22:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.66	
[11/30 11:22:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/30 11:28:59 visual_prompt]: Epoch 11 / 100: avg data time: 9.83e+00, avg batch time: 10.7058, average train loss: 2.4559
[11/30 11:29:43 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.5182, average loss: 0.7193
[11/30 11:29:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.18	
[11/30 11:29:43 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/30 11:35:57 visual_prompt]: Epoch 12 / 100: avg data time: 9.82e+00, avg batch time: 10.6958, average train loss: 3.2892
[11/30 11:36:41 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5157, average loss: 8.5075
[11/30 11:36:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.61	
[11/30 11:36:41 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/30 11:42:55 visual_prompt]: Epoch 13 / 100: avg data time: 9.83e+00, avg batch time: 10.6979, average train loss: 2.4713
[11/30 11:43:38 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.5244, average loss: 1.1359
[11/30 11:43:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/30 11:43:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/30 11:49:53 visual_prompt]: Epoch 14 / 100: avg data time: 9.81e+00, avg batch time: 10.6857, average train loss: 2.4583
[11/30 11:50:36 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5217, average loss: 2.7678
[11/30 11:50:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.16	
[11/30 11:50:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/30 11:56:51 visual_prompt]: Epoch 15 / 100: avg data time: 9.83e+00, avg batch time: 10.7069, average train loss: 4.0242
[11/30 11:57:34 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5238, average loss: 1.1153
[11/30 11:57:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.12	
[11/30 11:57:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/30 12:03:48 visual_prompt]: Epoch 16 / 100: avg data time: 9.82e+00, avg batch time: 10.6934, average train loss: 1.0410
[11/30 12:04:32 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5219, average loss: 1.8635
[11/30 12:04:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.34	
[11/30 12:04:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/30 12:10:45 visual_prompt]: Epoch 17 / 100: avg data time: 9.79e+00, avg batch time: 10.6632, average train loss: 4.6222
[11/30 12:11:28 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.5218, average loss: 9.2348
[11/30 12:11:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.28	
[11/30 12:11:28 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/30 12:17:42 visual_prompt]: Epoch 18 / 100: avg data time: 9.81e+00, avg batch time: 10.6815, average train loss: 6.6472
[11/30 12:18:26 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.5274, average loss: 1.7886
[11/30 12:18:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.75	
[11/30 12:18:26 visual_prompt]: Stopping early.
[11/30 12:18:26 visual_prompt]: Rank of current process: 0. World size: 1
[11/30 12:18:26 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/30 12:18:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/30 12:18:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/30 12:18:26 visual_prompt]: Training with config:
[11/30 12:18:26 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr2.5_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/30 12:18:26 visual_prompt]: Loading training data...
[11/30 12:18:26 visual_prompt]: Constructing mammo-cbis dataset train...
[11/30 12:18:26 visual_prompt]: Loading validation data...
[11/30 12:18:26 visual_prompt]: Constructing mammo-cbis dataset val...
[11/30 12:18:26 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/30 12:18:32 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/30 12:18:32 visual_prompt]: tuned percent:0.536
[11/30 12:18:32 visual_prompt]: Device used for model: 0
[11/30 12:18:32 visual_prompt]: Setting up Evaluator...
[11/30 12:18:32 visual_prompt]: Setting up Trainer...
[11/30 12:18:32 visual_prompt]: 	Setting up the optimizer...
[11/30 12:18:32 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/30 12:24:46 visual_prompt]: Epoch 1 / 100: avg data time: 9.82e+00, avg batch time: 10.6907, average train loss: 1.4006
[11/30 12:25:29 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5248, average loss: 1.2969
[11/30 12:25:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/30 12:25:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/30 12:31:43 visual_prompt]: Epoch 2 / 100: avg data time: 9.81e+00, avg batch time: 10.6893, average train loss: 3.3699
[11/30 12:32:27 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5242, average loss: 0.6860
[11/30 12:32:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.27	
[11/30 12:32:27 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/30 12:38:41 visual_prompt]: Epoch 3 / 100: avg data time: 9.82e+00, avg batch time: 10.6953, average train loss: 0.7258
[11/30 12:39:24 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5185, average loss: 0.7401
[11/30 12:39:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.34	
[11/30 12:39:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/30 12:45:38 visual_prompt]: Epoch 4 / 100: avg data time: 9.81e+00, avg batch time: 10.6826, average train loss: 0.7616
[11/30 12:46:22 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5250, average loss: 0.8856
[11/30 12:46:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.57	
[11/30 12:46:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/30 12:52:35 visual_prompt]: Epoch 5 / 100: avg data time: 9.80e+00, avg batch time: 10.6751, average train loss: 1.0729
[11/30 12:53:19 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.5236, average loss: 0.8617
[11/30 12:53:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.85	
[11/30 12:53:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/30 12:59:34 visual_prompt]: Epoch 6 / 100: avg data time: 9.84e+00, avg batch time: 10.7119, average train loss: 1.3040
[11/30 13:00:17 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.5201, average loss: 1.4463
[11/30 13:00:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.39	
[11/30 13:00:17 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/30 13:06:35 visual_prompt]: Epoch 7 / 100: avg data time: 9.94e+00, avg batch time: 10.8062, average train loss: 2.3479
[11/30 13:07:20 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5203, average loss: 1.8607
[11/30 13:07:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.28	
[11/30 13:07:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/30 13:13:47 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 11.0274, average train loss: 1.7663
[11/30 13:14:30 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.5236, average loss: 0.7228
[11/30 13:14:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.31	
[11/30 13:14:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/30 13:20:45 visual_prompt]: Epoch 9 / 100: avg data time: 9.84e+00, avg batch time: 10.7127, average train loss: 2.3725
[11/30 13:21:28 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5248, average loss: 5.0524
[11/30 13:21:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.44	
[11/30 13:21:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/30 13:27:43 visual_prompt]: Epoch 10 / 100: avg data time: 9.83e+00, avg batch time: 10.6973, average train loss: 4.4172
[11/30 13:28:26 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5230, average loss: 4.2301
[11/30 13:28:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.55	
[11/30 13:28:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/30 13:34:41 visual_prompt]: Epoch 11 / 100: avg data time: 9.83e+00, avg batch time: 10.6983, average train loss: 5.0814
[11/30 13:35:24 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.5172, average loss: 4.2661
[11/30 13:35:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.99	
[11/30 13:35:24 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/30 13:41:38 visual_prompt]: Epoch 12 / 100: avg data time: 9.82e+00, avg batch time: 10.6914, average train loss: 5.1510
[11/30 13:42:22 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5136, average loss: 9.4924
[11/30 13:42:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.58	
[11/30 13:42:22 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/30 13:48:37 visual_prompt]: Epoch 13 / 100: avg data time: 9.85e+00, avg batch time: 10.7233, average train loss: 5.3912
[11/30 13:49:20 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5170, average loss: 7.8631
[11/30 13:49:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.61	
[11/30 13:49:20 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/30 13:55:35 visual_prompt]: Epoch 14 / 100: avg data time: 9.82e+00, avg batch time: 10.6883, average train loss: 3.8460
[11/30 13:56:18 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5112, average loss: 5.5936
[11/30 13:56:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.52	
[11/30 13:56:18 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/30 14:02:33 visual_prompt]: Epoch 15 / 100: avg data time: 9.83e+00, avg batch time: 10.7037, average train loss: 4.7465
[11/30 14:03:16 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5210, average loss: 4.3303
[11/30 14:03:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.00	
[11/30 14:03:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/30 14:09:30 visual_prompt]: Epoch 16 / 100: avg data time: 9.81e+00, avg batch time: 10.6820, average train loss: 3.8909
[11/30 14:10:13 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.5241, average loss: 13.7328
[11/30 14:10:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.53	
[11/30 14:10:13 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/30 14:16:27 visual_prompt]: Epoch 17 / 100: avg data time: 9.80e+00, avg batch time: 10.6744, average train loss: 5.9041
[11/30 14:17:11 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5186, average loss: 2.4741
[11/30 14:17:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.76	
[11/30 14:17:11 visual_prompt]: Best epoch 17: best metric: -2.474
[11/30 14:17:11 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/30 14:23:25 visual_prompt]: Epoch 18 / 100: avg data time: 9.81e+00, avg batch time: 10.6850, average train loss: 3.4335
[11/30 14:24:08 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.5213, average loss: 6.1525
[11/30 14:24:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.15	
[11/30 14:24:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/30 14:30:21 visual_prompt]: Epoch 19 / 100: avg data time: 9.80e+00, avg batch time: 10.6738, average train loss: 5.2571
[11/30 14:31:05 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5164, average loss: 5.7841
[11/30 14:31:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.00	
[11/30 14:31:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/30 14:37:19 visual_prompt]: Epoch 20 / 100: avg data time: 9.81e+00, avg batch time: 10.6798, average train loss: 4.8364
[11/30 14:38:02 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5248, average loss: 3.3507
[11/30 14:38:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.19	
[11/30 14:38:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/30 14:44:46 visual_prompt]: Epoch 21 / 100: avg data time: 1.07e+01, avg batch time: 11.5386, average train loss: 2.7007
[11/30 14:45:34 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.5176, average loss: 0.7965
[11/30 14:45:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 47.35	
[11/30 14:45:34 visual_prompt]: Best epoch 21: best metric: -0.797
[11/30 14:45:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/30 14:52:20 visual_prompt]: Epoch 22 / 100: avg data time: 1.07e+01, avg batch time: 11.5931, average train loss: 3.1721
[11/30 14:53:06 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5159, average loss: 1.9093
[11/30 14:53:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.59	
[11/30 14:53:06 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/30 14:59:36 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 11.1576, average train loss: 3.6573
[11/30 15:00:23 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5199, average loss: 4.7770
[11/30 15:00:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.47	
[11/30 15:00:23 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/30 15:07:03 visual_prompt]: Epoch 24 / 100: avg data time: 1.06e+01, avg batch time: 11.4435, average train loss: 4.7145
[11/30 15:07:48 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.5195, average loss: 34.9005
[11/30 15:07:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.20	
[11/30 15:07:48 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/30 15:14:31 visual_prompt]: Epoch 25 / 100: avg data time: 1.06e+01, avg batch time: 11.5095, average train loss: 5.5821
[11/30 15:15:24 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5212, average loss: 0.8313
[11/30 15:15:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.06	
[11/30 15:15:24 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/30 15:22:13 visual_prompt]: Epoch 26 / 100: avg data time: 1.08e+01, avg batch time: 11.6722, average train loss: 2.7703
[11/30 15:23:01 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.5156, average loss: 1.6427
[11/30 15:23:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.00	
[11/30 15:23:01 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/30 15:29:43 visual_prompt]: Epoch 27 / 100: avg data time: 1.06e+01, avg batch time: 11.4762, average train loss: 3.6755
[11/30 15:30:28 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5198, average loss: 6.1555
[11/30 15:30:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.32	
[11/30 15:30:28 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/30 15:37:10 visual_prompt]: Epoch 28 / 100: avg data time: 1.06e+01, avg batch time: 11.4647, average train loss: 6.8746
[11/30 15:37:58 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.5213, average loss: 9.4362
[11/30 15:37:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.49	
[11/30 15:37:58 visual_prompt]: Stopping early.
[11/30 15:37:58 visual_prompt]: Rank of current process: 0. World size: 1
[11/30 15:37:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/30 15:37:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/30 15:37:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/30 15:37:58 visual_prompt]: Training with config:
[11/30 15:37:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr2.5_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/30 15:37:58 visual_prompt]: Loading training data...
[11/30 15:37:58 visual_prompt]: Constructing mammo-cbis dataset train...
[11/30 15:37:58 visual_prompt]: Loading validation data...
[11/30 15:37:58 visual_prompt]: Constructing mammo-cbis dataset val...
[11/30 15:37:58 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/30 15:38:01 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/30 15:38:01 visual_prompt]: tuned percent:0.536
[11/30 15:38:02 visual_prompt]: Device used for model: 0
[11/30 15:38:02 visual_prompt]: Setting up Evaluator...
[11/30 15:38:02 visual_prompt]: Setting up Trainer...
[11/30 15:38:02 visual_prompt]: 	Setting up the optimizer...
[11/30 15:38:02 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/30 15:44:56 visual_prompt]: Epoch 1 / 100: avg data time: 1.10e+01, avg batch time: 11.8373, average train loss: 1.4006
[11/30 15:45:44 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.5252, average loss: 1.2969
[11/30 15:45:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/30 15:45:44 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/30 15:52:39 visual_prompt]: Epoch 2 / 100: avg data time: 1.10e+01, avg batch time: 11.8570, average train loss: 3.4720
[11/30 15:53:27 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.5252, average loss: 0.7154
[11/30 15:53:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.18	
[11/30 15:53:27 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/30 16:00:20 visual_prompt]: Epoch 3 / 100: avg data time: 1.09e+01, avg batch time: 11.8096, average train loss: 0.7399
[11/30 16:01:08 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.5296, average loss: 0.6848
[11/30 16:01:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.57	
[11/30 16:01:08 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/30 16:07:59 visual_prompt]: Epoch 4 / 100: avg data time: 1.09e+01, avg batch time: 11.7620, average train loss: 0.8627
[11/30 16:08:48 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5272, average loss: 0.7197
[11/30 16:08:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.39	
[11/30 16:08:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/30 16:15:41 visual_prompt]: Epoch 5 / 100: avg data time: 1.09e+01, avg batch time: 11.8064, average train loss: 1.0651
[11/30 16:16:29 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.5243, average loss: 0.8666
[11/30 16:16:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.11	
[11/30 16:16:29 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/30 16:23:21 visual_prompt]: Epoch 6 / 100: avg data time: 1.09e+01, avg batch time: 11.7790, average train loss: 1.6559
[11/30 16:24:09 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.5303, average loss: 1.0535
[11/30 16:24:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.80	
[11/30 16:24:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/30 16:31:01 visual_prompt]: Epoch 7 / 100: avg data time: 1.09e+01, avg batch time: 11.7701, average train loss: 0.9782
[11/30 16:31:49 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.5345, average loss: 1.9568
[11/30 16:31:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.12	
[11/30 16:31:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/30 16:38:48 visual_prompt]: Epoch 8 / 100: avg data time: 1.11e+01, avg batch time: 11.9551, average train loss: 1.3010
[11/30 16:39:36 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.5300, average loss: 1.1560
[11/30 16:39:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.75	
[11/30 16:39:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/30 16:46:34 visual_prompt]: Epoch 9 / 100: avg data time: 1.11e+01, avg batch time: 11.9434, average train loss: 1.5081
[11/30 16:47:23 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.5271, average loss: 0.7169
[11/30 16:47:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.15	
[11/30 16:47:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/30 16:54:21 visual_prompt]: Epoch 10 / 100: avg data time: 1.11e+01, avg batch time: 11.9505, average train loss: 1.0039
[11/30 16:55:10 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.5290, average loss: 0.9307
[11/30 16:55:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.61	
[11/30 16:55:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/30 17:02:03 visual_prompt]: Epoch 11 / 100: avg data time: 1.09e+01, avg batch time: 11.7989, average train loss: 0.9469
[11/30 17:02:51 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5257, average loss: 0.6996
[11/30 17:02:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.62	
[11/30 17:02:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/30 17:09:42 visual_prompt]: Epoch 12 / 100: avg data time: 1.09e+01, avg batch time: 11.7369, average train loss: 3.0795
[11/30 17:10:30 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.5322, average loss: 0.9356
[11/30 17:10:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.46	
[11/30 17:10:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/30 17:17:21 visual_prompt]: Epoch 13 / 100: avg data time: 1.09e+01, avg batch time: 11.7391, average train loss: 3.7045
[11/30 17:18:07 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5277, average loss: 0.6992
[11/30 17:18:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.26	
[11/30 17:18:07 visual_prompt]: Best epoch 13: best metric: -0.699
[11/30 17:18:07 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/30 17:24:57 visual_prompt]: Epoch 14 / 100: avg data time: 1.08e+01, avg batch time: 11.7119, average train loss: 1.3648
[11/30 17:25:46 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.5265, average loss: 0.8836
[11/30 17:25:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.13	
[11/30 17:25:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/30 17:32:42 visual_prompt]: Epoch 15 / 100: avg data time: 1.10e+01, avg batch time: 11.8877, average train loss: 5.0260
[11/30 17:33:30 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.5230, average loss: 9.0985
[11/30 17:33:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.82	
[11/30 17:33:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/30 17:40:24 visual_prompt]: Epoch 16 / 100: avg data time: 1.09e+01, avg batch time: 11.8193, average train loss: 4.1740
[11/30 17:41:12 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.5286, average loss: 0.6884
[11/30 17:41:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.10	
[11/30 17:41:12 visual_prompt]: Best epoch 16: best metric: -0.688
[11/30 17:41:12 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/30 17:48:00 visual_prompt]: Epoch 17 / 100: avg data time: 1.08e+01, avg batch time: 11.6545, average train loss: 1.7869
[11/30 17:48:47 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.5228, average loss: 1.2948
[11/30 17:48:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.79	
[11/30 17:48:47 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/30 17:55:35 visual_prompt]: Epoch 18 / 100: avg data time: 1.08e+01, avg batch time: 11.6411, average train loss: 2.3638
[11/30 17:56:22 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.5233, average loss: 0.7903
[11/30 17:56:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.62	
[11/30 17:56:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/30 18:02:55 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 11.2275, average train loss: 5.3296
[11/30 18:03:40 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.5307, average loss: 2.8957
[11/30 18:03:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.48	
[11/30 18:03:40 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/30 18:10:08 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 11.0877, average train loss: 3.3733
[11/30 18:10:52 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5249, average loss: 2.3840
[11/30 18:10:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.43	
[11/30 18:10:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/30 18:17:11 visual_prompt]: Epoch 21 / 100: avg data time: 9.95e+00, avg batch time: 10.8156, average train loss: 3.2587
[11/30 18:17:55 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5294, average loss: 8.8746
[11/30 18:17:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 39.95	
[11/30 18:17:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/30 18:24:19 visual_prompt]: Epoch 22 / 100: avg data time: 1.01e+01, avg batch time: 10.9612, average train loss: 2.9532
[11/30 18:25:05 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.5266, average loss: 1.5694
[11/30 18:25:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.39	
[11/30 18:25:05 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/30 18:31:36 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 11.1867, average train loss: 3.0449
[11/30 18:32:20 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5211, average loss: 2.6377
[11/30 18:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.57	
[11/30 18:32:20 visual_prompt]: Stopping early.
[11/30 18:32:21 visual_prompt]: Rank of current process: 0. World size: 1
[11/30 18:32:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/30 18:32:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/30 18:32:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/30 18:32:21 visual_prompt]: Training with config:
[11/30 18:32:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr2.5_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/30 18:32:21 visual_prompt]: Loading training data...
[11/30 18:32:21 visual_prompt]: Constructing mammo-cbis dataset train...
[11/30 18:32:21 visual_prompt]: Loading validation data...
[11/30 18:32:21 visual_prompt]: Constructing mammo-cbis dataset val...
[11/30 18:32:21 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/30 18:32:26 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/30 18:32:26 visual_prompt]: tuned percent:0.536
[11/30 18:32:27 visual_prompt]: Device used for model: 0
[11/30 18:32:27 visual_prompt]: Setting up Evaluator...
[11/30 18:32:27 visual_prompt]: Setting up Trainer...
[11/30 18:32:27 visual_prompt]: 	Setting up the optimizer...
[11/30 18:32:27 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/30 18:38:47 visual_prompt]: Epoch 1 / 100: avg data time: 9.98e+00, avg batch time: 10.8541, average train loss: 1.4006
[11/30 18:39:30 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5283, average loss: 1.2969
[11/30 18:39:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/30 18:39:30 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/30 18:45:50 visual_prompt]: Epoch 2 / 100: avg data time: 9.95e+00, avg batch time: 10.8276, average train loss: 3.4462
[11/30 18:46:33 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5300, average loss: 0.6881
[11/30 18:46:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.01	
[11/30 18:46:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/30 18:52:53 visual_prompt]: Epoch 3 / 100: avg data time: 9.96e+00, avg batch time: 10.8369, average train loss: 0.7411
[11/30 18:53:37 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5289, average loss: 0.6804
[11/30 18:53:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.18	
[11/30 18:53:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/30 18:59:59 visual_prompt]: Epoch 4 / 100: avg data time: 1.01e+01, avg batch time: 10.9278, average train loss: 0.8913
[11/30 19:00:47 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.5241, average loss: 0.7379
[11/30 19:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.76	
[11/30 19:00:47 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/30 19:07:32 visual_prompt]: Epoch 5 / 100: avg data time: 1.07e+01, avg batch time: 11.5578, average train loss: 1.1565
[11/30 19:08:17 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.5301, average loss: 0.7966
[11/30 19:08:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.15	
[11/30 19:08:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/30 19:14:46 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 11.1317, average train loss: 0.8104
[11/30 19:15:31 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5234, average loss: 0.7666
[11/30 19:15:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.06	
[11/30 19:15:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/30 19:22:01 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 11.1179, average train loss: 1.9810
[11/30 19:22:46 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5247, average loss: 5.3661
[11/30 19:22:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.66	
[11/30 19:22:46 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/30 19:29:14 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 11.0902, average train loss: 5.3285
[11/30 19:29:59 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.5288, average loss: 0.7372
[11/30 19:29:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.64	
[11/30 19:29:59 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/30 19:36:28 visual_prompt]: Epoch 9 / 100: avg data time: 1.02e+01, avg batch time: 11.1208, average train loss: 4.2134
[11/30 19:37:13 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5307, average loss: 1.2099
[11/30 19:37:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.02	
[11/30 19:37:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/30 19:43:42 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 11.1029, average train loss: 0.9462
[11/30 19:44:27 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.5259, average loss: 1.1169
[11/30 19:44:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.73	
[11/30 19:44:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/30 19:50:56 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 11.1028, average train loss: 1.5373
[11/30 19:51:41 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5278, average loss: 1.5131
[11/30 19:51:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.07	
[11/30 19:51:41 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/30 19:58:09 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 11.0891, average train loss: 1.3870
[11/30 19:58:54 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.5281, average loss: 0.9609
[11/30 19:58:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.79	
[11/30 19:58:54 visual_prompt]: Best epoch 12: best metric: -0.961
[11/30 19:58:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/30 20:05:23 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 11.1083, average train loss: 1.0989
[11/30 20:06:08 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.5228, average loss: 0.7132
[11/30 20:06:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.75	
[11/30 20:06:08 visual_prompt]: Best epoch 13: best metric: -0.713
[11/30 20:06:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/30 20:12:36 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 11.0814, average train loss: 1.0539
[11/30 20:13:21 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.5276, average loss: 0.7854
[11/30 20:13:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.54	
[11/30 20:13:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/30 20:19:50 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 11.1092, average train loss: 0.8671
[11/30 20:20:35 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.5260, average loss: 0.8919
[11/30 20:20:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.59	
[11/30 20:20:35 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/30 20:27:03 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 11.0907, average train loss: 0.9253
[11/30 20:27:48 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.5269, average loss: 0.7166
[11/30 20:27:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.82	
[11/30 20:27:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/30 20:34:16 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 11.0822, average train loss: 0.9034
[11/30 20:35:01 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.5238, average loss: 1.2001
[11/30 20:35:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.22	
[11/30 20:35:01 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/30 20:41:29 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 11.0949, average train loss: 0.8904
[11/30 20:42:14 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5281, average loss: 0.7554
[11/30 20:42:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[11/30 20:42:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/30 20:48:42 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 11.0778, average train loss: 0.8317
[11/30 20:49:27 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5199, average loss: 0.6986
[11/30 20:49:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.01	
[11/30 20:49:27 visual_prompt]: Best epoch 19: best metric: -0.699
[11/30 20:49:27 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/30 20:55:54 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 11.0648, average train loss: 0.7180
[11/30 20:56:39 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.5283, average loss: 0.6836
[11/30 20:56:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.85	
[11/30 20:56:39 visual_prompt]: Best epoch 20: best metric: -0.684
[11/30 20:56:39 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/30 21:02:58 visual_prompt]: Epoch 21 / 100: avg data time: 9.96e+00, avg batch time: 10.8365, average train loss: 0.7931
[11/30 21:03:42 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5219, average loss: 0.6893
[11/30 21:03:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.64	
[11/30 21:03:42 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/30 21:10:01 visual_prompt]: Epoch 22 / 100: avg data time: 9.95e+00, avg batch time: 10.8245, average train loss: 1.0000
[11/30 21:10:45 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5225, average loss: 0.6869
[11/30 21:10:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.37	
[11/30 21:10:45 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/30 21:17:06 visual_prompt]: Epoch 23 / 100: avg data time: 1.00e+01, avg batch time: 10.8751, average train loss: 0.7470
[11/30 21:17:50 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5231, average loss: 0.7234
[11/30 21:17:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.80	
[11/30 21:17:50 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/30 21:24:11 visual_prompt]: Epoch 24 / 100: avg data time: 1.00e+01, avg batch time: 10.8746, average train loss: 0.7880
[11/30 21:24:55 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5263, average loss: 0.7472
[11/30 21:24:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.34	
[11/30 21:24:55 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/30 21:31:14 visual_prompt]: Epoch 25 / 100: avg data time: 9.95e+00, avg batch time: 10.8260, average train loss: 0.7322
[11/30 21:31:58 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5278, average loss: 0.7025
[11/30 21:31:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 60.30	
[11/30 21:31:58 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/30 21:38:18 visual_prompt]: Epoch 26 / 100: avg data time: 9.98e+00, avg batch time: 10.8539, average train loss: 0.8326
[11/30 21:39:02 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5321, average loss: 0.8469
[11/30 21:39:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.88	
[11/30 21:39:02 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/30 21:45:23 visual_prompt]: Epoch 27 / 100: avg data time: 1.00e+01, avg batch time: 10.8787, average train loss: 0.7651
[11/30 21:46:07 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5243, average loss: 1.0787
[11/30 21:46:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.83	
[11/30 21:46:07 visual_prompt]: Stopping early.
[11/30 21:46:07 visual_prompt]: Rank of current process: 0. World size: 1
[11/30 21:46:07 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/30 21:46:07 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/30 21:46:07 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/30 21:46:07 visual_prompt]: Training with config:
[11/30 21:46:07 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr2.5_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/30 21:46:07 visual_prompt]: Loading training data...
[11/30 21:46:07 visual_prompt]: Constructing mammo-cbis dataset train...
[11/30 21:46:07 visual_prompt]: Loading validation data...
[11/30 21:46:07 visual_prompt]: Constructing mammo-cbis dataset val...
[11/30 21:46:07 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/30 21:46:13 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/30 21:46:13 visual_prompt]: tuned percent:0.536
[11/30 21:46:13 visual_prompt]: Device used for model: 0
[11/30 21:46:13 visual_prompt]: Setting up Evaluator...
[11/30 21:46:13 visual_prompt]: Setting up Trainer...
[11/30 21:46:13 visual_prompt]: 	Setting up the optimizer...
[11/30 21:46:13 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/30 21:52:33 visual_prompt]: Epoch 1 / 100: avg data time: 9.98e+00, avg batch time: 10.8542, average train loss: 1.4006
[11/30 21:53:17 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5262, average loss: 1.2969
[11/30 21:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/30 21:53:17 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/30 21:59:38 visual_prompt]: Epoch 2 / 100: avg data time: 1.00e+01, avg batch time: 10.8774, average train loss: 3.4464
[11/30 22:00:22 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.5233, average loss: 0.6890
[11/30 22:00:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.14	
[11/30 22:00:22 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/30 22:06:42 visual_prompt]: Epoch 3 / 100: avg data time: 9.98e+00, avg batch time: 10.8527, average train loss: 0.7411
[11/30 22:07:26 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5293, average loss: 0.6804
[11/30 22:07:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.62	
[11/30 22:07:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/30 22:13:46 visual_prompt]: Epoch 4 / 100: avg data time: 9.99e+00, avg batch time: 10.8613, average train loss: 0.8931
[11/30 22:14:30 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5270, average loss: 0.7471
[11/30 22:14:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.48	
[11/30 22:14:30 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/30 22:20:50 visual_prompt]: Epoch 5 / 100: avg data time: 9.98e+00, avg batch time: 10.8502, average train loss: 1.1473
[11/30 22:21:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.5241, average loss: 0.8585
[11/30 22:21:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.56	
[11/30 22:21:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/30 22:27:55 visual_prompt]: Epoch 6 / 100: avg data time: 1.00e+01, avg batch time: 10.8695, average train loss: 0.8060
[11/30 22:28:39 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5198, average loss: 0.7609
[11/30 22:28:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.26	
[11/30 22:28:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/30 22:34:59 visual_prompt]: Epoch 7 / 100: avg data time: 1.00e+01, avg batch time: 10.8648, average train loss: 2.0366
[11/30 22:35:43 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5304, average loss: 2.9040
[11/30 22:35:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.12	
[11/30 22:35:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/30 22:42:03 visual_prompt]: Epoch 8 / 100: avg data time: 9.96e+00, avg batch time: 10.8293, average train loss: 1.6617
[11/30 22:42:47 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5222, average loss: 1.0023
[11/30 22:42:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.83	
[11/30 22:42:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/30 22:49:08 visual_prompt]: Epoch 9 / 100: avg data time: 1.00e+01, avg batch time: 10.8867, average train loss: 3.0392
[11/30 22:49:52 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5265, average loss: 2.4669
[11/30 22:49:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.87	
[11/30 22:49:52 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/30 22:56:12 visual_prompt]: Epoch 10 / 100: avg data time: 9.98e+00, avg batch time: 10.8453, average train loss: 4.4061
[11/30 22:56:55 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.5216, average loss: 0.9629
[11/30 22:56:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.74	
[11/30 22:56:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/30 23:03:16 visual_prompt]: Epoch 11 / 100: avg data time: 9.99e+00, avg batch time: 10.8581, average train loss: 2.4165
[11/30 23:04:00 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.5237, average loss: 1.9366
[11/30 23:04:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.12	
[11/30 23:04:00 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/30 23:10:20 visual_prompt]: Epoch 12 / 100: avg data time: 9.99e+00, avg batch time: 10.8609, average train loss: 1.8818
[11/30 23:11:04 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.5226, average loss: 0.6832
[11/30 23:11:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 58.13	
[11/30 23:11:04 visual_prompt]: Best epoch 12: best metric: -0.683
[11/30 23:11:04 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/30 23:17:40 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 11.3110, average train loss: 1.1836
[11/30 23:18:26 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.5272, average loss: 0.7320
[11/30 23:18:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.62	
[11/30 23:18:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/30 23:24:56 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 11.1341, average train loss: 1.5084
[11/30 23:25:42 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5179, average loss: 2.4211
[11/30 23:25:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.69	
[11/30 23:25:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/30 23:32:21 visual_prompt]: Epoch 15 / 100: avg data time: 1.05e+01, avg batch time: 11.3900, average train loss: 1.1882
[11/30 23:33:08 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5231, average loss: 0.9154
[11/30 23:33:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.34	
[11/30 23:33:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/30 23:39:42 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 11.2718, average train loss: 0.7712
[11/30 23:40:28 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.5273, average loss: 0.6913
[11/30 23:40:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 60.56	
[11/30 23:40:28 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/30 23:47:02 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 11.2662, average train loss: 0.9954
[11/30 23:47:46 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5219, average loss: 0.8424
[11/30 23:47:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.74	
[11/30 23:47:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/30 23:54:07 visual_prompt]: Epoch 18 / 100: avg data time: 1.00e+01, avg batch time: 10.8778, average train loss: 0.8370
[11/30 23:54:51 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.5253, average loss: 0.6988
[11/30 23:54:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 62.54	
[11/30 23:54:51 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[12/01 00:01:27 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 11.3006, average train loss: 0.8658
[12/01 00:02:13 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5218, average loss: 0.6901
[12/01 00:02:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.98	
[12/01 00:02:13 visual_prompt]: Stopping early.
[12/01 00:02:13 visual_prompt]: Rank of current process: 0. World size: 1
[12/01 00:02:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/01 00:02:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/01 00:02:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/01 00:02:13 visual_prompt]: Training with config:
[12/01 00:02:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr1.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/01 00:02:13 visual_prompt]: Loading training data...
[12/01 00:02:13 visual_prompt]: Constructing mammo-cbis dataset train...
[12/01 00:02:13 visual_prompt]: Loading validation data...
[12/01 00:02:13 visual_prompt]: Constructing mammo-cbis dataset val...
[12/01 00:02:13 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[12/01 00:02:20 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[12/01 00:02:20 visual_prompt]: tuned percent:0.536
[12/01 00:02:20 visual_prompt]: Device used for model: 0
[12/01 00:02:20 visual_prompt]: Setting up Evaluator...
[12/01 00:02:20 visual_prompt]: Setting up Trainer...
[12/01 00:02:20 visual_prompt]: 	Setting up the optimizer...
[12/01 00:02:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/01 00:08:53 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 11.2417, average train loss: 1.4006
[12/01 00:09:39 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.5114, average loss: 1.2969
[12/01 00:09:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[12/01 00:09:39 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[12/01 00:16:11 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 11.2046, average train loss: 2.1592
[12/01 00:16:57 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5176, average loss: 0.6879
[12/01 00:16:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.30	
[12/01 00:16:57 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[12/01 00:23:29 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 11.2127, average train loss: 0.7533
[12/01 00:24:13 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5219, average loss: 0.6938
[12/01 00:24:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[12/01 00:24:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[12/01 00:30:38 visual_prompt]: Epoch 4 / 100: avg data time: 1.01e+01, avg batch time: 10.9863, average train loss: 0.7219
[12/01 00:31:22 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.5228, average loss: 0.7721
[12/01 00:31:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.03	
[12/01 00:31:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[12/01 00:37:45 visual_prompt]: Epoch 5 / 100: avg data time: 1.01e+01, avg batch time: 10.9353, average train loss: 0.7179
[12/01 00:38:31 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5196, average loss: 0.6924
[12/01 00:38:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.88	
[12/01 00:38:31 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[12/01 00:45:11 visual_prompt]: Epoch 6 / 100: avg data time: 1.05e+01, avg batch time: 11.4221, average train loss: 0.7286
[12/01 00:45:58 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.5196, average loss: 0.6924
[12/01 00:45:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.36	
[12/01 00:45:58 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[12/01 00:52:39 visual_prompt]: Epoch 7 / 100: avg data time: 1.06e+01, avg batch time: 11.4533, average train loss: 0.7507
[12/01 00:53:25 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5210, average loss: 1.0640
[12/01 00:53:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.95	
[12/01 00:53:25 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[12/01 01:00:07 visual_prompt]: Epoch 8 / 100: avg data time: 1.06e+01, avg batch time: 11.4556, average train loss: 0.7506
[12/01 01:00:53 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5203, average loss: 0.6907
[12/01 01:00:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 39.41	
[12/01 01:00:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[12/01 01:07:36 visual_prompt]: Epoch 9 / 100: avg data time: 1.06e+01, avg batch time: 11.5132, average train loss: 0.7703
[12/01 01:08:23 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.5205, average loss: 0.6890
[12/01 01:08:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.47	
[12/01 01:08:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[12/01 01:15:00 visual_prompt]: Epoch 10 / 100: avg data time: 1.05e+01, avg batch time: 11.3460, average train loss: 0.7368
[12/01 01:15:46 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.5175, average loss: 0.7129
[12/01 01:15:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.22	
[12/01 01:15:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[12/01 01:22:28 visual_prompt]: Epoch 11 / 100: avg data time: 1.06e+01, avg batch time: 11.4844, average train loss: 1.1397
[12/01 01:23:15 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5277, average loss: 1.7558
[12/01 01:23:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.55	
[12/01 01:23:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[12/01 01:29:44 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 11.1092, average train loss: 1.1263
[12/01 01:30:28 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.5215, average loss: 1.1164
[12/01 01:30:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.13	
[12/01 01:30:28 visual_prompt]: Best epoch 12: best metric: -1.116
[12/01 01:30:28 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[12/01 01:36:55 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 11.0521, average train loss: 1.2693
[12/01 01:37:40 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5269, average loss: 0.9655
[12/01 01:37:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.01	
[12/01 01:37:40 visual_prompt]: Best epoch 13: best metric: -0.966
[12/01 01:37:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[12/01 01:44:04 visual_prompt]: Epoch 14 / 100: avg data time: 1.01e+01, avg batch time: 10.9764, average train loss: 2.3697
[12/01 01:44:49 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5173, average loss: 1.1207
[12/01 01:44:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.71	
[12/01 01:44:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[12/01 01:51:15 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 11.0472, average train loss: 1.9807
[12/01 01:52:00 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.5225, average loss: 0.7091
[12/01 01:52:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.69	
[12/01 01:52:00 visual_prompt]: Best epoch 15: best metric: -0.709
[12/01 01:52:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[12/01 01:58:25 visual_prompt]: Epoch 16 / 100: avg data time: 1.01e+01, avg batch time: 11.0082, average train loss: 0.9670
[12/01 01:59:10 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5268, average loss: 0.9257
[12/01 01:59:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.42	
[12/01 01:59:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[12/01 02:05:34 visual_prompt]: Epoch 17 / 100: avg data time: 1.01e+01, avg batch time: 10.9748, average train loss: 0.8435
[12/01 02:06:19 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.5190, average loss: 0.8919
[12/01 02:06:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.93	
[12/01 02:06:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[12/01 02:12:47 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 11.0761, average train loss: 0.7797
[12/01 02:13:31 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.5169, average loss: 0.6905
[12/01 02:13:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.45	
[12/01 02:13:31 visual_prompt]: Best epoch 18: best metric: -0.691
[12/01 02:13:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[12/01 02:19:57 visual_prompt]: Epoch 19 / 100: avg data time: 1.01e+01, avg batch time: 11.0193, average train loss: 0.8639
[12/01 02:20:42 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5258, average loss: 1.5774
[12/01 02:20:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.51	
[12/01 02:20:42 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[12/01 02:27:08 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 11.0381, average train loss: 0.8325
[12/01 02:27:53 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.5268, average loss: 0.6975
[12/01 02:27:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.40	
[12/01 02:27:53 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[12/01 02:34:18 visual_prompt]: Epoch 21 / 100: avg data time: 1.01e+01, avg batch time: 10.9904, average train loss: 0.8661
[12/01 02:35:02 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5242, average loss: 1.3138
[12/01 02:35:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.75	
[12/01 02:35:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[12/01 02:41:25 visual_prompt]: Epoch 22 / 100: avg data time: 1.01e+01, avg batch time: 10.9409, average train loss: 0.8668
[12/01 02:42:10 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.5121, average loss: 0.8454
[12/01 02:42:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.63	
[12/01 02:42:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[12/01 02:48:34 visual_prompt]: Epoch 23 / 100: avg data time: 1.01e+01, avg batch time: 10.9641, average train loss: 0.7891
[12/01 02:49:18 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5252, average loss: 0.6956
[12/01 02:49:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.95	
[12/01 02:49:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[12/01 02:56:01 visual_prompt]: Epoch 24 / 100: avg data time: 1.06e+01, avg batch time: 11.4999, average train loss: 0.8349
[12/01 02:56:47 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5131, average loss: 0.7273
[12/01 02:56:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.58	
[12/01 02:56:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[12/01 03:03:09 visual_prompt]: Epoch 25 / 100: avg data time: 1.00e+01, avg batch time: 10.9089, average train loss: 1.2129
[12/01 03:03:52 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.5154, average loss: 0.8310
[12/01 03:03:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.29	
[12/01 03:03:52 visual_prompt]: Stopping early.
[12/01 03:03:53 visual_prompt]: Rank of current process: 0. World size: 1
[12/01 03:03:53 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/01 03:03:53 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/01 03:03:53 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/01 03:03:53 visual_prompt]: Training with config:
[12/01 03:03:53 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr1.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/01 03:03:53 visual_prompt]: Loading training data...
[12/01 03:03:53 visual_prompt]: Constructing mammo-cbis dataset train...
[12/01 03:03:53 visual_prompt]: Loading validation data...
[12/01 03:03:53 visual_prompt]: Constructing mammo-cbis dataset val...
[12/01 03:03:53 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[12/01 03:03:59 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[12/01 03:03:59 visual_prompt]: tuned percent:0.536
[12/01 03:03:59 visual_prompt]: Device used for model: 0
[12/01 03:03:59 visual_prompt]: Setting up Evaluator...
[12/01 03:03:59 visual_prompt]: Setting up Trainer...
[12/01 03:03:59 visual_prompt]: 	Setting up the optimizer...
[12/01 03:03:59 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/01 03:10:17 visual_prompt]: Epoch 1 / 100: avg data time: 9.91e+00, avg batch time: 10.7876, average train loss: 1.4006
[12/01 03:11:00 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5193, average loss: 1.2969
[12/01 03:11:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[12/01 03:11:00 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[12/01 03:17:17 visual_prompt]: Epoch 2 / 100: avg data time: 9.90e+00, avg batch time: 10.7714, average train loss: 2.2246
[12/01 03:18:01 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.5227, average loss: 0.6883
[12/01 03:18:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 51.57	
[12/01 03:18:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[12/01 03:24:19 visual_prompt]: Epoch 3 / 100: avg data time: 9.91e+00, avg batch time: 10.7842, average train loss: 0.7838
[12/01 03:25:02 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5235, average loss: 0.6890
[12/01 03:25:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.56	
[12/01 03:25:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[12/01 03:31:23 visual_prompt]: Epoch 4 / 100: avg data time: 1.00e+01, avg batch time: 10.8685, average train loss: 0.7249
[12/01 03:32:07 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5166, average loss: 0.7030
[12/01 03:32:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.23	
[12/01 03:32:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[12/01 03:38:26 visual_prompt]: Epoch 5 / 100: avg data time: 9.95e+00, avg batch time: 10.8262, average train loss: 0.8061
[12/01 03:39:10 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5181, average loss: 0.7331
[12/01 03:39:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.78	
[12/01 03:39:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[12/01 03:45:31 visual_prompt]: Epoch 6 / 100: avg data time: 1.00e+01, avg batch time: 10.8828, average train loss: 0.8022
[12/01 03:46:15 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.5111, average loss: 0.6887
[12/01 03:46:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.37	
[12/01 03:46:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[12/01 03:52:36 visual_prompt]: Epoch 7 / 100: avg data time: 1.00e+01, avg batch time: 10.8837, average train loss: 0.7032
[12/01 03:53:20 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5244, average loss: 1.3003
[12/01 03:53:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.69	
[12/01 03:53:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[12/01 03:59:39 visual_prompt]: Epoch 8 / 100: avg data time: 9.95e+00, avg batch time: 10.8226, average train loss: 0.8210
[12/01 04:00:23 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.5136, average loss: 0.6832
[12/01 04:00:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 60.55	
[12/01 04:00:23 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[12/01 04:06:43 visual_prompt]: Epoch 9 / 100: avg data time: 1.00e+01, avg batch time: 10.8718, average train loss: 0.7957
[12/01 04:07:27 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5194, average loss: 0.7088
[12/01 04:07:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.75	
[12/01 04:07:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[12/01 04:13:46 visual_prompt]: Epoch 10 / 100: avg data time: 9.95e+00, avg batch time: 10.8218, average train loss: 0.7533
[12/01 04:14:30 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5202, average loss: 0.7465
[12/01 04:14:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.65	
[12/01 04:14:30 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[12/01 04:20:50 visual_prompt]: Epoch 11 / 100: avg data time: 9.99e+00, avg batch time: 10.8609, average train loss: 0.7584
[12/01 04:21:34 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5199, average loss: 0.7771
[12/01 04:21:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.46	
[12/01 04:21:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[12/01 04:27:54 visual_prompt]: Epoch 12 / 100: avg data time: 9.98e+00, avg batch time: 10.8510, average train loss: 0.7227
[12/01 04:28:38 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5179, average loss: 0.6905
[12/01 04:28:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 59.32	
[12/01 04:28:38 visual_prompt]: Best epoch 12: best metric: -0.690
[12/01 04:28:38 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[12/01 04:34:58 visual_prompt]: Epoch 13 / 100: avg data time: 9.97e+00, avg batch time: 10.8436, average train loss: 0.7525
[12/01 04:35:42 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5242, average loss: 0.6899
[12/01 04:35:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.98	
[12/01 04:35:42 visual_prompt]: Best epoch 13: best metric: -0.690
[12/01 04:35:42 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[12/01 04:42:02 visual_prompt]: Epoch 14 / 100: avg data time: 9.98e+00, avg batch time: 10.8584, average train loss: 0.7603
[12/01 04:42:46 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5141, average loss: 0.7067
[12/01 04:42:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.87	
[12/01 04:42:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[12/01 04:49:06 visual_prompt]: Epoch 15 / 100: avg data time: 9.98e+00, avg batch time: 10.8524, average train loss: 0.7629
[12/01 04:49:50 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5191, average loss: 0.7203
[12/01 04:49:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.67	
[12/01 04:49:50 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[12/01 04:56:10 visual_prompt]: Epoch 16 / 100: avg data time: 9.99e+00, avg batch time: 10.8637, average train loss: 0.7333
[12/01 04:56:54 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5148, average loss: 0.9394
[12/01 04:56:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.81	
[12/01 04:56:54 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[12/01 05:03:14 visual_prompt]: Epoch 17 / 100: avg data time: 9.96e+00, avg batch time: 10.8351, average train loss: 0.7859
[12/01 05:03:57 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5167, average loss: 0.8035
[12/01 05:03:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.30	
[12/01 05:03:57 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[12/01 05:10:17 visual_prompt]: Epoch 18 / 100: avg data time: 9.96e+00, avg batch time: 10.8300, average train loss: 0.7496
[12/01 05:11:00 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.5222, average loss: 0.9038
[12/01 05:11:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.23	
[12/01 05:11:00 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[12/01 05:17:21 visual_prompt]: Epoch 19 / 100: avg data time: 9.98e+00, avg batch time: 10.8595, average train loss: 0.7214
[12/01 05:18:05 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5211, average loss: 0.7870
[12/01 05:18:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.91	
[12/01 05:18:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[12/01 05:24:25 visual_prompt]: Epoch 20 / 100: avg data time: 9.99e+00, avg batch time: 10.8632, average train loss: 0.7102
[12/01 05:25:09 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5223, average loss: 0.7399
[12/01 05:25:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.18	
[12/01 05:25:09 visual_prompt]: Stopping early.
[12/01 05:25:09 visual_prompt]: Rank of current process: 0. World size: 1
[12/01 05:25:09 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/01 05:25:09 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/01 05:25:09 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/01 05:25:09 visual_prompt]: Training with config:
[12/01 05:25:09 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr1.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/01 05:25:09 visual_prompt]: Loading training data...
[12/01 05:25:09 visual_prompt]: Constructing mammo-cbis dataset train...
[12/01 05:25:09 visual_prompt]: Loading validation data...
[12/01 05:25:09 visual_prompt]: Constructing mammo-cbis dataset val...
[12/01 05:25:09 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[12/01 05:25:12 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[12/01 05:25:12 visual_prompt]: tuned percent:0.536
[12/01 05:25:12 visual_prompt]: Device used for model: 0
[12/01 05:25:12 visual_prompt]: Setting up Evaluator...
[12/01 05:25:12 visual_prompt]: Setting up Trainer...
[12/01 05:25:12 visual_prompt]: 	Setting up the optimizer...
[12/01 05:25:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/01 05:31:31 visual_prompt]: Epoch 1 / 100: avg data time: 9.95e+00, avg batch time: 10.8241, average train loss: 1.4006
[12/01 05:32:14 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.5235, average loss: 1.2969
[12/01 05:32:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[12/01 05:32:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[12/01 05:38:32 visual_prompt]: Epoch 2 / 100: avg data time: 9.89e+00, avg batch time: 10.7738, average train loss: 2.2323
[12/01 05:39:15 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5143, average loss: 0.6883
[12/01 05:39:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 51.95	
[12/01 05:39:15 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[12/01 05:45:33 visual_prompt]: Epoch 3 / 100: avg data time: 9.90e+00, avg batch time: 10.7808, average train loss: 0.7876
[12/01 05:46:17 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5265, average loss: 0.6862
[12/01 05:46:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.19	
[12/01 05:46:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[12/01 05:52:34 visual_prompt]: Epoch 4 / 100: avg data time: 9.92e+00, avg batch time: 10.7905, average train loss: 0.7290
[12/01 05:53:18 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5199, average loss: 0.7010
[12/01 05:53:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.06	
[12/01 05:53:18 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[12/01 05:59:38 visual_prompt]: Epoch 5 / 100: avg data time: 9.96e+00, avg batch time: 10.8411, average train loss: 0.8125
[12/01 06:00:22 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5201, average loss: 0.7241
[12/01 06:00:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.94	
[12/01 06:00:22 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[12/01 06:06:43 visual_prompt]: Epoch 6 / 100: avg data time: 1.00e+01, avg batch time: 10.8850, average train loss: 0.8284
[12/01 06:07:27 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5210, average loss: 0.6762
[12/01 06:07:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 60.99	
[12/01 06:07:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[12/01 06:13:46 visual_prompt]: Epoch 7 / 100: avg data time: 9.97e+00, avg batch time: 10.8457, average train loss: 0.7057
[12/01 06:14:30 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.5197, average loss: 1.4666
[12/01 06:14:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.31	
[12/01 06:14:30 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[12/01 06:20:50 visual_prompt]: Epoch 8 / 100: avg data time: 9.96e+00, avg batch time: 10.8388, average train loss: 0.9079
[12/01 06:21:34 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5202, average loss: 0.6768
[12/01 06:21:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 61.13	
[12/01 06:21:34 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[12/01 06:27:56 visual_prompt]: Epoch 9 / 100: avg data time: 1.00e+01, avg batch time: 10.9035, average train loss: 0.8885
[12/01 06:28:40 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5249, average loss: 0.6916
[12/01 06:28:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 60.59	
[12/01 06:28:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[12/01 06:35:00 visual_prompt]: Epoch 10 / 100: avg data time: 9.98e+00, avg batch time: 10.8558, average train loss: 0.7908
[12/01 06:35:44 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5189, average loss: 0.7353
[12/01 06:35:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 60.24	
[12/01 06:35:44 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[12/01 06:42:02 visual_prompt]: Epoch 11 / 100: avg data time: 9.93e+00, avg batch time: 10.8087, average train loss: 0.8163
[12/01 06:42:46 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5307, average loss: 0.9303
[12/01 06:42:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.95	
[12/01 06:42:46 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[12/01 06:49:07 visual_prompt]: Epoch 12 / 100: avg data time: 1.00e+01, avg batch time: 10.8806, average train loss: 0.7214
[12/01 06:49:51 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.5302, average loss: 0.9696
[12/01 06:49:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.13	
[12/01 06:49:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[12/01 06:56:10 visual_prompt]: Epoch 13 / 100: avg data time: 9.94e+00, avg batch time: 10.8204, average train loss: 0.8989
[12/01 06:56:54 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5183, average loss: 0.8084
[12/01 06:56:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.80	
[12/01 06:56:54 visual_prompt]: Best epoch 13: best metric: -0.808
[12/01 06:56:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[12/01 07:03:14 visual_prompt]: Epoch 14 / 100: avg data time: 9.99e+00, avg batch time: 10.8644, average train loss: 0.9532
[12/01 07:03:58 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5317, average loss: 1.9877
[12/01 07:03:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.26	
[12/01 07:03:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[12/01 07:10:18 visual_prompt]: Epoch 15 / 100: avg data time: 9.98e+00, avg batch time: 10.8539, average train loss: 1.4786
[12/01 07:11:02 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5218, average loss: 1.0999
[12/01 07:11:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.60	
[12/01 07:11:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[12/01 07:17:23 visual_prompt]: Epoch 16 / 100: avg data time: 9.99e+00, avg batch time: 10.8703, average train loss: 0.9565
[12/01 07:18:07 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5214, average loss: 0.8753
[12/01 07:18:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.50	
[12/01 07:18:07 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[12/01 07:24:27 visual_prompt]: Epoch 17 / 100: avg data time: 9.96e+00, avg batch time: 10.8442, average train loss: 0.8613
[12/01 07:25:11 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.5192, average loss: 1.2170
[12/01 07:25:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.48	
[12/01 07:25:11 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[12/01 07:31:34 visual_prompt]: Epoch 18 / 100: avg data time: 1.01e+01, avg batch time: 10.9431, average train loss: 0.8424
[12/01 07:32:18 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5268, average loss: 0.6535
[12/01 07:32:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 65.88	
[12/01 07:32:18 visual_prompt]: Best epoch 18: best metric: -0.653
[12/01 07:32:18 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[12/01 07:38:39 visual_prompt]: Epoch 19 / 100: avg data time: 1.00e+01, avg batch time: 10.8751, average train loss: 0.7048
[12/01 07:39:23 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.5296, average loss: 0.6751
[12/01 07:39:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 65.64	
[12/01 07:39:23 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[12/01 07:45:43 visual_prompt]: Epoch 20 / 100: avg data time: 9.96e+00, avg batch time: 10.8460, average train loss: 0.6753
[12/01 07:46:27 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5199, average loss: 0.6346
[12/01 07:46:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.92	rocauc: 73.54	
[12/01 07:46:27 visual_prompt]: Best epoch 20: best metric: -0.635
[12/01 07:46:27 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[12/01 07:52:47 visual_prompt]: Epoch 21 / 100: avg data time: 9.98e+00, avg batch time: 10.8621, average train loss: 0.7380
[12/01 07:53:31 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5287, average loss: 0.6622
[12/01 07:53:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 69.19	
[12/01 07:53:31 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[12/01 07:59:51 visual_prompt]: Epoch 22 / 100: avg data time: 9.99e+00, avg batch time: 10.8670, average train loss: 0.9098
[12/01 08:00:35 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.5165, average loss: 0.6395
[12/01 08:00:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 71.22	
[12/01 08:00:35 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[12/01 08:06:54 visual_prompt]: Epoch 23 / 100: avg data time: 9.93e+00, avg batch time: 10.8088, average train loss: 0.7086
[12/01 08:07:38 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.5225, average loss: 0.6266
[12/01 08:07:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 70.67	
[12/01 08:07:38 visual_prompt]: Best epoch 23: best metric: -0.627
[12/01 08:07:38 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[12/01 08:13:58 visual_prompt]: Epoch 24 / 100: avg data time: 1.00e+01, avg batch time: 10.8785, average train loss: 0.8221
[12/01 08:14:43 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5201, average loss: 0.6440
[12/01 08:14:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 72.82	
[12/01 08:14:43 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[12/01 08:21:03 visual_prompt]: Epoch 25 / 100: avg data time: 9.99e+00, avg batch time: 10.8700, average train loss: 0.6570
[12/01 08:21:49 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.5269, average loss: 0.6079
[12/01 08:21:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 73.53	
[12/01 08:21:49 visual_prompt]: Best epoch 25: best metric: -0.608
[12/01 08:21:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[12/01 08:28:34 visual_prompt]: Epoch 26 / 100: avg data time: 1.07e+01, avg batch time: 11.5711, average train loss: 0.6918
[12/01 08:29:19 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5179, average loss: 0.7877
[12/01 08:29:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 72.77	
[12/01 08:29:19 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[12/01 08:35:38 visual_prompt]: Epoch 27 / 100: avg data time: 9.94e+00, avg batch time: 10.8250, average train loss: 0.6656
[12/01 08:36:22 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5202, average loss: 0.6794
[12/01 08:36:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 72.43	
[12/01 08:36:22 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[12/01 08:42:39 visual_prompt]: Epoch 28 / 100: avg data time: 9.88e+00, avg batch time: 10.7621, average train loss: 0.7083
[12/01 08:43:23 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5196, average loss: 0.7694
[12/01 08:43:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 70.57	
[12/01 08:43:23 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[12/01 08:49:42 visual_prompt]: Epoch 29 / 100: avg data time: 9.95e+00, avg batch time: 10.8257, average train loss: 0.6656
[12/01 08:50:25 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5167, average loss: 0.5942
[12/01 08:50:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.51	rocauc: 75.36	
[12/01 08:50:25 visual_prompt]: Best epoch 29: best metric: -0.594
[12/01 08:50:25 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[12/01 08:56:43 visual_prompt]: Epoch 30 / 100: avg data time: 9.92e+00, avg batch time: 10.7951, average train loss: 0.6202
[12/01 08:57:27 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5177, average loss: 0.6069
[12/01 08:57:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 73.55	
[12/01 08:57:27 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[12/01 09:03:47 visual_prompt]: Epoch 31 / 100: avg data time: 9.98e+00, avg batch time: 10.8592, average train loss: 0.6091
[12/01 09:04:32 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.5229, average loss: 0.6375
[12/01 09:04:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 74.70	
[12/01 09:04:32 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[12/01 09:10:53 visual_prompt]: Epoch 32 / 100: avg data time: 1.00e+01, avg batch time: 10.8927, average train loss: 0.6301
[12/01 09:11:37 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5177, average loss: 0.6098
[12/01 09:11:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 75.68	
[12/01 09:11:37 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[12/01 09:17:58 visual_prompt]: Epoch 33 / 100: avg data time: 1.00e+01, avg batch time: 10.8731, average train loss: 0.6265
[12/01 09:18:42 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.5152, average loss: 0.6121
[12/01 09:18:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 73.58	
[12/01 09:18:42 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[12/01 09:25:02 visual_prompt]: Epoch 34 / 100: avg data time: 9.98e+00, avg batch time: 10.8522, average train loss: 0.6056
[12/01 09:25:46 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5208, average loss: 0.6046
[12/01 09:25:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.73	rocauc: 73.73	
[12/01 09:25:46 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[12/01 09:32:06 visual_prompt]: Epoch 35 / 100: avg data time: 9.99e+00, avg batch time: 10.8686, average train loss: 0.6071
[12/01 09:32:50 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5192, average loss: 0.6745
[12/01 09:32:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 73.35	
[12/01 09:32:50 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[12/01 09:39:08 visual_prompt]: Epoch 36 / 100: avg data time: 9.91e+00, avg batch time: 10.7830, average train loss: 0.6337
[12/01 09:39:51 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5221, average loss: 0.6777
[12/01 09:39:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 73.10	
[12/01 09:39:51 visual_prompt]: Stopping early.
[12/01 09:39:51 visual_prompt]: Rank of current process: 0. World size: 1
[12/01 09:39:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/01 09:39:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/01 09:39:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/01 09:39:51 visual_prompt]: Training with config:
[12/01 09:39:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr1.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/01 09:39:51 visual_prompt]: Loading training data...
[12/01 09:39:51 visual_prompt]: Constructing mammo-cbis dataset train...
[12/01 09:39:51 visual_prompt]: Loading validation data...
[12/01 09:39:51 visual_prompt]: Constructing mammo-cbis dataset val...
[12/01 09:39:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[12/01 09:39:58 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[12/01 09:39:58 visual_prompt]: tuned percent:0.536
[12/01 09:39:58 visual_prompt]: Device used for model: 0
[12/01 09:39:58 visual_prompt]: Setting up Evaluator...
[12/01 09:39:58 visual_prompt]: Setting up Trainer...
[12/01 09:39:58 visual_prompt]: 	Setting up the optimizer...
[12/01 09:39:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/01 09:46:19 visual_prompt]: Epoch 1 / 100: avg data time: 1.00e+01, avg batch time: 10.8949, average train loss: 1.4006
[12/01 09:47:02 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5234, average loss: 1.2969
[12/01 09:47:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[12/01 09:47:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[12/01 09:53:32 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 11.1378, average train loss: 2.2326
[12/01 09:54:18 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5201, average loss: 0.6879
[12/01 09:54:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 52.27	
[12/01 09:54:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[12/01 10:00:53 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 11.2613, average train loss: 0.7887
[12/01 10:01:39 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5191, average loss: 0.6878
[12/01 10:01:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.67	
[12/01 10:01:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[12/01 10:08:08 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 11.1093, average train loss: 0.7299
[12/01 10:08:53 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5226, average loss: 0.7012
[12/01 10:08:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.51	
[12/01 10:08:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[12/01 10:15:27 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 11.2561, average train loss: 0.8018
[12/01 10:16:13 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5266, average loss: 0.7035
[12/01 10:16:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 59.06	
[12/01 10:16:13 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[12/01 10:22:45 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 11.2130, average train loss: 0.8506
[12/01 10:23:31 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5172, average loss: 0.6777
[12/01 10:23:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.51	
[12/01 10:23:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[12/01 10:30:06 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 11.2758, average train loss: 0.7054
[12/01 10:30:52 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5201, average loss: 1.5080
[12/01 10:30:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.55	
[12/01 10:30:52 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[12/01 10:37:29 visual_prompt]: Epoch 8 / 100: avg data time: 1.05e+01, avg batch time: 11.3447, average train loss: 0.9132
[12/01 10:38:15 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5171, average loss: 0.6750
[12/01 10:38:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.91	
[12/01 10:38:15 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[12/01 10:44:41 visual_prompt]: Epoch 9 / 100: avg data time: 1.02e+01, avg batch time: 11.0221, average train loss: 0.8940
[12/01 10:45:25 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5201, average loss: 0.7035
[12/01 10:45:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 60.51	
[12/01 10:45:25 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[12/01 10:51:44 visual_prompt]: Epoch 10 / 100: avg data time: 9.95e+00, avg batch time: 10.8221, average train loss: 0.8085
[12/01 10:52:27 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.5280, average loss: 0.7392
[12/01 10:52:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 60.27	
[12/01 10:52:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[12/01 10:58:45 visual_prompt]: Epoch 11 / 100: avg data time: 9.92e+00, avg batch time: 10.7941, average train loss: 0.8412
[12/01 10:59:29 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5197, average loss: 0.9663
[12/01 10:59:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.34	
[12/01 10:59:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[12/01 11:05:47 visual_prompt]: Epoch 12 / 100: avg data time: 9.93e+00, avg batch time: 10.8051, average train loss: 0.7254
[12/01 11:06:31 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5134, average loss: 1.0173
[12/01 11:06:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.52	
[12/01 11:06:31 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[12/01 11:12:48 visual_prompt]: Epoch 13 / 100: avg data time: 9.89e+00, avg batch time: 10.7582, average train loss: 0.9835
[12/01 11:13:31 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.5142, average loss: 0.6822
[12/01 11:13:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 63.78	
[12/01 11:13:31 visual_prompt]: Best epoch 13: best metric: -0.682
[12/01 11:13:31 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[12/01 11:19:47 visual_prompt]: Epoch 14 / 100: avg data time: 9.85e+00, avg batch time: 10.7213, average train loss: 0.8652
[12/01 11:20:30 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5150, average loss: 1.1285
[12/01 11:20:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.57	
[12/01 11:20:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[12/01 11:26:46 visual_prompt]: Epoch 15 / 100: avg data time: 9.87e+00, avg batch time: 10.7394, average train loss: 0.8781
[12/01 11:27:30 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5261, average loss: 0.8684
[12/01 11:27:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.73	
[12/01 11:27:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[12/01 11:33:46 visual_prompt]: Epoch 16 / 100: avg data time: 9.89e+00, avg batch time: 10.7612, average train loss: 0.8644
[12/01 11:34:30 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.5177, average loss: 0.8879
[12/01 11:34:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.32	
[12/01 11:34:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[12/01 11:40:45 visual_prompt]: Epoch 17 / 100: avg data time: 9.85e+00, avg batch time: 10.7253, average train loss: 0.8003
[12/01 11:41:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5224, average loss: 1.4857
[12/01 11:41:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.15	
[12/01 11:41:29 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[12/01 11:47:47 visual_prompt]: Epoch 18 / 100: avg data time: 9.93e+00, avg batch time: 10.7977, average train loss: 0.8189
[12/01 11:48:31 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.5217, average loss: 0.6704
[12/01 11:48:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 67.79	
[12/01 11:48:31 visual_prompt]: Best epoch 18: best metric: -0.670
[12/01 11:48:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[12/01 11:54:49 visual_prompt]: Epoch 19 / 100: avg data time: 9.93e+00, avg batch time: 10.8013, average train loss: 0.7070
[12/01 11:55:32 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5249, average loss: 0.6454
[12/01 11:55:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 68.80	
[12/01 11:55:32 visual_prompt]: Best epoch 19: best metric: -0.645
[12/01 11:55:32 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[12/01 12:01:56 visual_prompt]: Epoch 20 / 100: avg data time: 1.01e+01, avg batch time: 10.9450, average train loss: 0.6882
[12/01 12:02:42 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5220, average loss: 0.6081
[12/01 12:02:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 74.72	
[12/01 12:02:42 visual_prompt]: Best epoch 20: best metric: -0.608
[12/01 12:02:42 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[12/01 12:09:25 visual_prompt]: Epoch 21 / 100: avg data time: 1.06e+01, avg batch time: 11.5130, average train loss: 0.7393
[12/01 12:10:09 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5193, average loss: 0.6143
[12/01 12:10:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.92	rocauc: 72.58	
[12/01 12:10:09 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[12/01 12:16:29 visual_prompt]: Epoch 22 / 100: avg data time: 1.00e+01, avg batch time: 10.8728, average train loss: 0.9749
[12/01 12:17:17 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.5203, average loss: 0.6402
[12/01 12:17:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 71.45	
[12/01 12:17:17 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[12/01 12:23:54 visual_prompt]: Epoch 23 / 100: avg data time: 1.05e+01, avg batch time: 11.3552, average train loss: 0.7033
[12/01 12:24:41 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5228, average loss: 0.6250
[12/01 12:24:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.63	
[12/01 12:24:41 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[12/01 12:31:17 visual_prompt]: Epoch 24 / 100: avg data time: 1.04e+01, avg batch time: 11.3105, average train loss: 0.8331
[12/01 12:32:00 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.5216, average loss: 0.7592
[12/01 12:32:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 73.34	
[12/01 12:32:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[12/01 12:38:35 visual_prompt]: Epoch 25 / 100: avg data time: 1.04e+01, avg batch time: 11.2634, average train loss: 0.6576
[12/01 12:39:21 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.5205, average loss: 0.6961
[12/01 12:39:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 73.69	
[12/01 12:39:21 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[12/01 12:45:57 visual_prompt]: Epoch 26 / 100: avg data time: 1.04e+01, avg batch time: 11.3131, average train loss: 0.8149
[12/01 12:46:44 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5194, average loss: 0.6360
[12/01 12:46:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 74.93	
[12/01 12:46:44 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[12/01 12:53:19 visual_prompt]: Epoch 27 / 100: avg data time: 1.04e+01, avg batch time: 11.2960, average train loss: 0.6914
[12/01 12:54:05 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5220, average loss: 0.6106
[12/01 12:54:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 73.21	
[12/01 12:54:05 visual_prompt]: Stopping early.
[12/01 12:54:06 visual_prompt]: Rank of current process: 0. World size: 1
[12/01 12:54:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/01 12:54:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/01 12:54:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/01 12:54:06 visual_prompt]: Training with config:
[12/01 12:54:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.5_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/01 12:54:06 visual_prompt]: Loading training data...
[12/01 12:54:06 visual_prompt]: Constructing mammo-cbis dataset train...
[12/01 12:54:06 visual_prompt]: Loading validation data...
[12/01 12:54:06 visual_prompt]: Constructing mammo-cbis dataset val...
[12/01 12:54:06 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[12/01 12:54:09 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[12/01 12:54:09 visual_prompt]: tuned percent:0.536
[12/01 12:54:09 visual_prompt]: Device used for model: 0
[12/01 12:54:09 visual_prompt]: Setting up Evaluator...
[12/01 12:54:09 visual_prompt]: Setting up Trainer...
[12/01 12:54:09 visual_prompt]: 	Setting up the optimizer...
[12/01 12:54:09 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/01 13:00:41 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 11.2074, average train loss: 1.4006
[12/01 13:01:25 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5286, average loss: 1.2969
[12/01 13:01:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[12/01 13:01:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[12/01 13:07:41 visual_prompt]: Epoch 2 / 100: avg data time: 9.88e+00, avg batch time: 10.7533, average train loss: 1.8132
[12/01 13:08:25 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5250, average loss: 0.6886
[12/01 13:08:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.48	
[12/01 13:08:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[12/01 13:14:42 visual_prompt]: Epoch 3 / 100: avg data time: 9.89e+00, avg batch time: 10.7670, average train loss: 0.7056
[12/01 13:15:25 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5282, average loss: 0.7022
[12/01 13:15:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.76	
[12/01 13:15:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[12/01 13:21:44 visual_prompt]: Epoch 4 / 100: avg data time: 9.92e+00, avg batch time: 10.8004, average train loss: 0.6988
[12/01 13:22:27 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.5293, average loss: 0.6887
[12/01 13:22:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.71	
[12/01 13:22:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[12/01 13:28:43 visual_prompt]: Epoch 5 / 100: avg data time: 9.87e+00, avg batch time: 10.7471, average train loss: 0.7294
[12/01 13:29:30 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5266, average loss: 0.7277
[12/01 13:29:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.17	
[12/01 13:29:30 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[12/01 13:36:12 visual_prompt]: Epoch 6 / 100: avg data time: 1.06e+01, avg batch time: 11.4780, average train loss: 0.7441
[12/01 13:36:59 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5312, average loss: 0.6887
[12/01 13:36:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.35	
[12/01 13:36:59 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[12/01 13:43:40 visual_prompt]: Epoch 7 / 100: avg data time: 1.06e+01, avg batch time: 11.4794, average train loss: 0.7370
[12/01 13:44:27 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.5265, average loss: 0.7550
[12/01 13:44:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.29	
[12/01 13:44:27 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[12/01 13:51:07 visual_prompt]: Epoch 8 / 100: avg data time: 1.05e+01, avg batch time: 11.4237, average train loss: 0.7114
[12/01 13:51:53 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.5299, average loss: 0.6908
[12/01 13:51:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.25	
[12/01 13:51:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[12/01 13:58:30 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 11.3228, average train loss: 0.7088
[12/01 13:59:13 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5303, average loss: 0.7090
[12/01 13:59:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.90	
[12/01 13:59:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[12/01 14:05:32 visual_prompt]: Epoch 10 / 100: avg data time: 9.94e+00, avg batch time: 10.8209, average train loss: 0.8441
[12/01 14:06:16 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.5274, average loss: 0.7827
[12/01 14:06:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.82	
[12/01 14:06:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[12/01 14:12:35 visual_prompt]: Epoch 11 / 100: avg data time: 9.95e+00, avg batch time: 10.8329, average train loss: 0.7514
[12/01 14:13:19 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.5307, average loss: 0.6911
[12/01 14:13:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.24	
[12/01 14:13:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[12/01 14:19:38 visual_prompt]: Epoch 12 / 100: avg data time: 9.93e+00, avg batch time: 10.8097, average train loss: 0.7088
[12/01 14:20:21 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5286, average loss: 0.7275
[12/01 14:20:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.84	
[12/01 14:20:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[12/01 14:26:41 visual_prompt]: Epoch 13 / 100: avg data time: 9.97e+00, avg batch time: 10.8515, average train loss: 0.7274
[12/01 14:27:29 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5284, average loss: 0.7023
[12/01 14:27:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.74	
[12/01 14:27:29 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[12/01 14:34:11 visual_prompt]: Epoch 14 / 100: avg data time: 1.06e+01, avg batch time: 11.4679, average train loss: 0.7393
[12/01 14:34:55 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5367, average loss: 0.7596
[12/01 14:34:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.88	
[12/01 14:34:55 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[12/01 14:41:16 visual_prompt]: Epoch 15 / 100: avg data time: 1.00e+01, avg batch time: 10.8883, average train loss: 0.7481
[12/01 14:42:00 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.5331, average loss: 0.7060
[12/01 14:42:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.96	
[12/01 14:42:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[12/01 14:48:20 visual_prompt]: Epoch 16 / 100: avg data time: 9.96e+00, avg batch time: 10.8422, average train loss: 0.7025
[12/01 14:49:04 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.5261, average loss: 0.8708
[12/01 14:49:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.88	
[12/01 14:49:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[12/01 14:55:21 visual_prompt]: Epoch 17 / 100: avg data time: 9.90e+00, avg batch time: 10.7811, average train loss: 0.7429
[12/01 14:56:05 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5303, average loss: 0.6952
[12/01 14:56:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.85	
[12/01 14:56:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[12/01 15:02:22 visual_prompt]: Epoch 18 / 100: avg data time: 9.89e+00, avg batch time: 10.7661, average train loss: 0.7264
[12/01 15:03:05 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5307, average loss: 0.6891
[12/01 15:03:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.88	
[12/01 15:03:05 visual_prompt]: Best epoch 18: best metric: -0.689
[12/01 15:03:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[12/01 15:09:21 visual_prompt]: Epoch 19 / 100: avg data time: 9.86e+00, avg batch time: 10.7409, average train loss: 0.7082
[12/01 15:10:05 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5288, average loss: 0.6977
[12/01 15:10:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.20	
[12/01 15:10:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[12/01 15:16:24 visual_prompt]: Epoch 20 / 100: avg data time: 9.96e+00, avg batch time: 10.8398, average train loss: 0.6969
[12/01 15:17:08 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5340, average loss: 0.8146
[12/01 15:17:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.44	
[12/01 15:17:08 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[12/01 15:23:25 visual_prompt]: Epoch 21 / 100: avg data time: 9.89e+00, avg batch time: 10.7704, average train loss: 0.7062
[12/01 15:24:09 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5285, average loss: 0.7045
[12/01 15:24:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[12/01 15:24:09 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[12/01 15:30:26 visual_prompt]: Epoch 22 / 100: avg data time: 9.88e+00, avg batch time: 10.7583, average train loss: 0.7440
[12/01 15:31:09 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5353, average loss: 0.7014
[12/01 15:31:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.18	
[12/01 15:31:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[12/01 15:37:26 visual_prompt]: Epoch 23 / 100: avg data time: 9.88e+00, avg batch time: 10.7596, average train loss: 0.7189
[12/01 15:38:10 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5374, average loss: 0.7331
[12/01 15:38:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.25	
[12/01 15:38:10 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[12/01 15:44:27 visual_prompt]: Epoch 24 / 100: avg data time: 9.89e+00, avg batch time: 10.7714, average train loss: 0.7421
[12/01 15:45:10 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.5296, average loss: 0.6885
[12/01 15:45:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.05	
[12/01 15:45:10 visual_prompt]: Best epoch 24: best metric: -0.688
[12/01 15:45:10 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[12/01 15:51:27 visual_prompt]: Epoch 25 / 100: avg data time: 9.88e+00, avg batch time: 10.7511, average train loss: 0.7102
[12/01 15:52:10 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5393, average loss: 0.6929
[12/01 15:52:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 38.42	
[12/01 15:52:10 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[12/01 15:58:27 visual_prompt]: Epoch 26 / 100: avg data time: 9.88e+00, avg batch time: 10.7633, average train loss: 0.7204
[12/01 15:59:11 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.5227, average loss: 0.7149
[12/01 15:59:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.00	
[12/01 15:59:11 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[12/01 16:05:29 visual_prompt]: Epoch 27 / 100: avg data time: 9.92e+00, avg batch time: 10.7952, average train loss: 0.7196
[12/01 16:06:13 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.5281, average loss: 0.8172
[12/01 16:06:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.17	
[12/01 16:06:13 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[12/01 16:12:31 visual_prompt]: Epoch 28 / 100: avg data time: 9.93e+00, avg batch time: 10.8103, average train loss: 0.7321
[12/01 16:13:15 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5306, average loss: 0.6883
[12/01 16:13:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.79	
[12/01 16:13:15 visual_prompt]: Best epoch 28: best metric: -0.688
[12/01 16:13:15 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[12/01 16:19:34 visual_prompt]: Epoch 29 / 100: avg data time: 9.95e+00, avg batch time: 10.8295, average train loss: 0.7004
[12/01 16:20:18 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5305, average loss: 0.6886
[12/01 16:20:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.15	
[12/01 16:20:18 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[12/01 16:26:39 visual_prompt]: Epoch 30 / 100: avg data time: 1.00e+01, avg batch time: 10.8767, average train loss: 0.7130
[12/01 16:27:23 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5287, average loss: 0.6936
[12/01 16:27:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.43	
[12/01 16:27:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[12/01 16:33:40 visual_prompt]: Epoch 31 / 100: avg data time: 9.87e+00, avg batch time: 10.7577, average train loss: 0.7045
[12/01 16:34:24 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.5341, average loss: 0.7386
[12/01 16:34:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.54	
[12/01 16:34:24 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[12/01 16:41:27 visual_prompt]: Epoch 32 / 100: avg data time: 1.12e+01, avg batch time: 12.0934, average train loss: 0.7272
[12/01 16:42:37 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.5226, average loss: 0.6996
[12/01 16:42:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.32	
[12/01 16:42:37 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[12/01 16:51:40 visual_prompt]: Epoch 33 / 100: avg data time: 1.46e+01, avg batch time: 15.5138, average train loss: 0.6971
[12/01 16:52:25 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5326, average loss: 0.6938
[12/01 16:52:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.43	
[12/01 16:52:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[12/01 16:59:42 visual_prompt]: Epoch 34 / 100: avg data time: 1.16e+01, avg batch time: 12.4736, average train loss: 0.7066
[12/01 17:00:50 visual_prompt]: Inference (val):avg data time: 1.88e-05, avg batch time: 0.5257, average loss: 0.6884
[12/01 17:00:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.80	
[12/01 17:00:50 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[12/01 17:08:17 visual_prompt]: Epoch 35 / 100: avg data time: 1.19e+01, avg batch time: 12.7711, average train loss: 0.7117
[12/01 17:09:01 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.5276, average loss: 0.7209
[12/01 17:09:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.43	
[12/01 17:09:01 visual_prompt]: Stopping early.
[12/01 17:09:01 visual_prompt]: Rank of current process: 0. World size: 1
[12/01 17:09:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/01 17:09:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/01 17:09:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/01 17:09:01 visual_prompt]: Training with config:
[12/01 17:09:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.5_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/01 17:09:01 visual_prompt]: Loading training data...
[12/01 17:09:01 visual_prompt]: Constructing mammo-cbis dataset train...
[12/01 17:09:01 visual_prompt]: Loading validation data...
[12/01 17:09:01 visual_prompt]: Constructing mammo-cbis dataset val...
[12/01 17:09:01 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[12/01 17:09:16 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[12/01 17:09:16 visual_prompt]: tuned percent:0.536
[12/01 17:09:16 visual_prompt]: Device used for model: 0
[12/01 17:09:16 visual_prompt]: Setting up Evaluator...
[12/01 17:09:16 visual_prompt]: Setting up Trainer...
[12/01 17:09:16 visual_prompt]: 	Setting up the optimizer...
[12/01 17:09:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/01 17:15:36 visual_prompt]: Epoch 1 / 100: avg data time: 9.97e+00, avg batch time: 10.8543, average train loss: 1.4006
[12/01 17:16:20 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.5339, average loss: 1.2969
[12/01 17:16:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[12/01 17:16:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[12/01 17:22:40 visual_prompt]: Epoch 2 / 100: avg data time: 9.97e+00, avg batch time: 10.8558, average train loss: 1.8299
[12/01 17:23:24 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5367, average loss: 0.6888
[12/01 17:23:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.56	
[12/01 17:23:24 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[12/01 17:29:42 visual_prompt]: Epoch 3 / 100: avg data time: 9.92e+00, avg batch time: 10.8060, average train loss: 0.7137
[12/01 17:30:26 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.5326, average loss: 0.7050
[12/01 17:30:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.78	
[12/01 17:30:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[12/01 17:36:45 visual_prompt]: Epoch 4 / 100: avg data time: 9.93e+00, avg batch time: 10.8127, average train loss: 0.7117
[12/01 17:37:28 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.5340, average loss: 0.6869
[12/01 17:37:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.53	
[12/01 17:37:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[12/01 17:43:46 visual_prompt]: Epoch 5 / 100: avg data time: 9.90e+00, avg batch time: 10.7761, average train loss: 0.7440
[12/01 17:44:30 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.5358, average loss: 0.8135
[12/01 17:44:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.53	
[12/01 17:44:30 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[12/01 17:50:49 visual_prompt]: Epoch 6 / 100: avg data time: 9.95e+00, avg batch time: 10.8264, average train loss: 0.7520
[12/01 17:51:32 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5340, average loss: 0.7138
[12/01 17:51:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.63	
[12/01 17:51:32 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[12/01 17:57:50 visual_prompt]: Epoch 7 / 100: avg data time: 9.90e+00, avg batch time: 10.7759, average train loss: 0.7714
[12/01 17:58:33 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.5323, average loss: 0.7137
[12/01 17:58:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 57.41	
[12/01 17:58:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[12/01 18:04:50 visual_prompt]: Epoch 8 / 100: avg data time: 9.87e+00, avg batch time: 10.7525, average train loss: 0.7598
[12/01 18:05:33 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5289, average loss: 0.6865
[12/01 18:05:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 57.62	
[12/01 18:05:33 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[12/01 18:11:51 visual_prompt]: Epoch 9 / 100: avg data time: 9.90e+00, avg batch time: 10.7781, average train loss: 0.7428
[12/01 18:12:35 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5356, average loss: 0.7583
[12/01 18:12:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.99	
[12/01 18:12:35 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[12/01 18:18:52 visual_prompt]: Epoch 10 / 100: avg data time: 9.89e+00, avg batch time: 10.7686, average train loss: 0.6970
[12/01 18:19:35 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.5298, average loss: 0.7928
[12/01 18:19:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.90	
[12/01 18:19:35 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[12/01 18:25:52 visual_prompt]: Epoch 11 / 100: avg data time: 9.86e+00, avg batch time: 10.7485, average train loss: 0.7586
[12/01 18:26:35 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5250, average loss: 0.7083
[12/01 18:26:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 61.30	
[12/01 18:26:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[12/01 18:32:52 visual_prompt]: Epoch 12 / 100: avg data time: 9.89e+00, avg batch time: 10.7718, average train loss: 0.7263
[12/01 18:33:36 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5293, average loss: 0.7739
[12/01 18:33:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.86	
[12/01 18:33:36 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[12/01 18:39:53 visual_prompt]: Epoch 13 / 100: avg data time: 9.89e+00, avg batch time: 10.7706, average train loss: 0.7413
[12/01 18:40:37 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5300, average loss: 0.6853
[12/01 18:40:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 64.12	
[12/01 18:40:37 visual_prompt]: Best epoch 13: best metric: -0.685
[12/01 18:40:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[12/01 18:46:53 visual_prompt]: Epoch 14 / 100: avg data time: 9.86e+00, avg batch time: 10.7452, average train loss: 0.7319
[12/01 18:47:37 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5313, average loss: 0.7726
[12/01 18:47:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.92	
[12/01 18:47:37 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[12/01 18:53:54 visual_prompt]: Epoch 15 / 100: avg data time: 9.90e+00, avg batch time: 10.7902, average train loss: 0.7212
[12/01 18:54:38 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5358, average loss: 0.7313
[12/01 18:54:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.10	
[12/01 18:54:38 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[12/01 19:00:55 visual_prompt]: Epoch 16 / 100: avg data time: 9.88e+00, avg batch time: 10.7633, average train loss: 0.6909
[12/01 19:01:39 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5259, average loss: 0.8936
[12/01 19:01:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.56	
[12/01 19:01:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[12/01 19:07:54 visual_prompt]: Epoch 17 / 100: avg data time: 9.85e+00, avg batch time: 10.7321, average train loss: 0.7803
[12/01 19:08:38 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5363, average loss: 0.6839
[12/01 19:08:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 67.10	
[12/01 19:08:38 visual_prompt]: Best epoch 17: best metric: -0.684
[12/01 19:08:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[12/01 19:14:56 visual_prompt]: Epoch 18 / 100: avg data time: 9.92e+00, avg batch time: 10.7989, average train loss: 0.7145
[12/01 19:15:40 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5332, average loss: 0.7380
[12/01 19:15:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.76	
[12/01 19:15:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[12/01 19:22:00 visual_prompt]: Epoch 19 / 100: avg data time: 9.97e+00, avg batch time: 10.8520, average train loss: 0.6734
[12/01 19:22:44 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5313, average loss: 0.6884
[12/01 19:22:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.54	
[12/01 19:22:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[12/01 19:29:04 visual_prompt]: Epoch 20 / 100: avg data time: 9.98e+00, avg batch time: 10.8606, average train loss: 0.7104
[12/01 19:29:49 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5294, average loss: 0.7874
[12/01 19:29:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.95	
[12/01 19:29:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[12/01 19:36:09 visual_prompt]: Epoch 21 / 100: avg data time: 9.98e+00, avg batch time: 10.8609, average train loss: 0.7008
[12/01 19:36:52 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5334, average loss: 0.6816
[12/01 19:36:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.23	
[12/01 19:36:53 visual_prompt]: Best epoch 21: best metric: -0.682
[12/01 19:36:53 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[12/01 19:43:09 visual_prompt]: Epoch 22 / 100: avg data time: 9.87e+00, avg batch time: 10.7512, average train loss: 0.7028
[12/01 19:43:53 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5271, average loss: 0.6707
[12/01 19:43:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 62.88	
[12/01 19:43:53 visual_prompt]: Best epoch 22: best metric: -0.671
[12/01 19:43:53 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[12/01 19:50:09 visual_prompt]: Epoch 23 / 100: avg data time: 9.88e+00, avg batch time: 10.7589, average train loss: 0.7001
[12/01 19:50:53 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5271, average loss: 0.6852
[12/01 19:50:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.49	
[12/01 19:50:53 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[12/01 19:57:10 visual_prompt]: Epoch 24 / 100: avg data time: 9.88e+00, avg batch time: 10.7666, average train loss: 0.7294
[12/01 19:57:54 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.5297, average loss: 0.6821
[12/01 19:57:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.46	
[12/01 19:57:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[12/01 20:04:11 visual_prompt]: Epoch 25 / 100: avg data time: 9.90e+00, avg batch time: 10.7815, average train loss: 0.6934
[12/01 20:04:55 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.5351, average loss: 0.6851
[12/01 20:04:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 64.00	
[12/01 20:04:55 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[12/01 20:11:11 visual_prompt]: Epoch 26 / 100: avg data time: 9.87e+00, avg batch time: 10.7508, average train loss: 0.7215
[12/01 20:11:55 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.5351, average loss: 0.7359
[12/01 20:11:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.72	
[12/01 20:11:55 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[12/01 20:18:12 visual_prompt]: Epoch 27 / 100: avg data time: 9.88e+00, avg batch time: 10.7604, average train loss: 0.7167
[12/01 20:18:55 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5318, average loss: 0.7708
[12/01 20:18:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.77	
[12/01 20:18:55 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[12/01 20:25:12 visual_prompt]: Epoch 28 / 100: avg data time: 9.86e+00, avg batch time: 10.7481, average train loss: 0.7255
[12/01 20:25:55 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5276, average loss: 0.7188
[12/01 20:25:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.05	
[12/01 20:25:55 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[12/01 20:32:13 visual_prompt]: Epoch 29 / 100: avg data time: 9.90e+00, avg batch time: 10.7791, average train loss: 0.7040
[12/01 20:32:57 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.5278, average loss: 0.6921
[12/01 20:32:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.63	
[12/01 20:32:57 visual_prompt]: Stopping early.
[12/01 20:32:57 visual_prompt]: Rank of current process: 0. World size: 1
[12/01 20:32:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/01 20:32:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/01 20:32:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/01 20:32:57 visual_prompt]: Training with config:
[12/01 20:32:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.5_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/01 20:32:57 visual_prompt]: Loading training data...
[12/01 20:32:57 visual_prompt]: Constructing mammo-cbis dataset train...
[12/01 20:32:58 visual_prompt]: Loading validation data...
[12/01 20:32:58 visual_prompt]: Constructing mammo-cbis dataset val...
[12/01 20:32:58 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[12/01 20:33:00 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[12/01 20:33:00 visual_prompt]: tuned percent:0.536
[12/01 20:33:00 visual_prompt]: Device used for model: 0
[12/01 20:33:00 visual_prompt]: Setting up Evaluator...
[12/01 20:33:00 visual_prompt]: Setting up Trainer...
[12/01 20:33:00 visual_prompt]: 	Setting up the optimizer...
[12/01 20:33:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/01 20:39:18 visual_prompt]: Epoch 1 / 100: avg data time: 9.90e+00, avg batch time: 10.7881, average train loss: 1.4006
[12/01 20:40:02 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5381, average loss: 1.2969
[12/01 20:40:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[12/01 20:40:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[12/01 20:46:18 visual_prompt]: Epoch 2 / 100: avg data time: 9.88e+00, avg batch time: 10.7606, average train loss: 1.8312
[12/01 20:47:02 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.5224, average loss: 0.6890
[12/01 20:47:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.24	
[12/01 20:47:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[12/01 20:53:19 visual_prompt]: Epoch 3 / 100: avg data time: 9.87e+00, avg batch time: 10.7569, average train loss: 0.7147
[12/01 20:54:02 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5246, average loss: 0.7051
[12/01 20:54:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.28	
[12/01 20:54:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[12/01 21:00:19 visual_prompt]: Epoch 4 / 100: avg data time: 9.88e+00, avg batch time: 10.7610, average train loss: 0.7134
[12/01 21:01:03 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5335, average loss: 0.6865
[12/01 21:01:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[12/01 21:01:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[12/01 21:07:19 visual_prompt]: Epoch 5 / 100: avg data time: 9.86e+00, avg batch time: 10.7421, average train loss: 0.7466
[12/01 21:08:02 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5239, average loss: 0.8192
[12/01 21:08:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.44	
[12/01 21:08:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[12/01 21:14:20 visual_prompt]: Epoch 6 / 100: avg data time: 9.90e+00, avg batch time: 10.7863, average train loss: 0.7552
[12/01 21:15:04 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.5430, average loss: 0.7114
[12/01 21:15:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.65	
[12/01 21:15:04 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[12/01 21:21:21 visual_prompt]: Epoch 7 / 100: avg data time: 9.89e+00, avg batch time: 10.7726, average train loss: 0.7767
[12/01 21:22:05 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5414, average loss: 0.7206
[12/01 21:22:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.38	
[12/01 21:22:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[12/01 21:28:23 visual_prompt]: Epoch 8 / 100: avg data time: 9.91e+00, avg batch time: 10.7942, average train loss: 0.7780
[12/01 21:29:07 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5337, average loss: 0.6876
[12/01 21:29:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.65	
[12/01 21:29:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[12/01 21:35:25 visual_prompt]: Epoch 9 / 100: avg data time: 9.90e+00, avg batch time: 10.7839, average train loss: 0.7477
[12/01 21:36:09 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5384, average loss: 0.7322
[12/01 21:36:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.70	
[12/01 21:36:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[12/01 21:42:25 visual_prompt]: Epoch 10 / 100: avg data time: 9.88e+00, avg batch time: 10.7629, average train loss: 0.7048
[12/01 21:43:09 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5276, average loss: 0.8743
[12/01 21:43:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.72	
[12/01 21:43:09 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[12/01 21:49:26 visual_prompt]: Epoch 11 / 100: avg data time: 9.89e+00, avg batch time: 10.7772, average train loss: 0.8294
[12/01 21:50:10 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5401, average loss: 0.6773
[12/01 21:50:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 60.51	
[12/01 21:50:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[12/01 21:56:27 visual_prompt]: Epoch 12 / 100: avg data time: 9.87e+00, avg batch time: 10.7543, average train loss: 0.7236
[12/01 21:57:10 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.5303, average loss: 0.6633
[12/01 21:57:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 63.70	
[12/01 21:57:10 visual_prompt]: Best epoch 12: best metric: -0.663
[12/01 21:57:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[12/01 22:03:28 visual_prompt]: Epoch 13 / 100: avg data time: 9.91e+00, avg batch time: 10.7958, average train loss: 0.7057
[12/01 22:04:12 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5353, average loss: 0.6547
[12/01 22:04:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 66.29	
[12/01 22:04:12 visual_prompt]: Best epoch 13: best metric: -0.655
[12/01 22:04:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[12/01 22:10:28 visual_prompt]: Epoch 14 / 100: avg data time: 9.87e+00, avg batch time: 10.7500, average train loss: 0.7519
[12/01 22:11:12 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5372, average loss: 0.7839
[12/01 22:11:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.72	
[12/01 22:11:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[12/01 22:17:30 visual_prompt]: Epoch 15 / 100: avg data time: 9.90e+00, avg batch time: 10.7795, average train loss: 0.7829
[12/01 22:18:13 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.5397, average loss: 0.8116
[12/01 22:18:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.56	
[12/01 22:18:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[12/01 22:24:30 visual_prompt]: Epoch 16 / 100: avg data time: 9.87e+00, avg batch time: 10.7542, average train loss: 0.6917
[12/01 22:25:13 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5296, average loss: 1.0649
[12/01 22:25:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.55	
[12/01 22:25:13 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[12/01 22:31:30 visual_prompt]: Epoch 17 / 100: avg data time: 9.87e+00, avg batch time: 10.7558, average train loss: 0.7422
[12/01 22:32:14 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5378, average loss: 0.7263
[12/01 22:32:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.17	
[12/01 22:32:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[12/01 22:38:30 visual_prompt]: Epoch 18 / 100: avg data time: 9.87e+00, avg batch time: 10.7493, average train loss: 0.6633
[12/01 22:39:14 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5282, average loss: 0.7538
[12/01 22:39:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 73.10	
[12/01 22:39:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[12/01 22:45:30 visual_prompt]: Epoch 19 / 100: avg data time: 9.86e+00, avg batch time: 10.7439, average train loss: 0.6446
[12/01 22:46:14 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5352, average loss: 0.6219
[12/01 22:46:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 71.92	
[12/01 22:46:14 visual_prompt]: Best epoch 19: best metric: -0.622
[12/01 22:46:14 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[12/01 22:52:33 visual_prompt]: Epoch 20 / 100: avg data time: 9.96e+00, avg batch time: 10.8432, average train loss: 0.6458
[12/01 22:53:18 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5330, average loss: 0.6163
[12/01 22:53:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.51	rocauc: 74.16	
[12/01 22:53:18 visual_prompt]: Best epoch 20: best metric: -0.616
[12/01 22:53:18 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[12/01 22:59:34 visual_prompt]: Epoch 21 / 100: avg data time: 9.88e+00, avg batch time: 10.7545, average train loss: 0.6661
[12/01 23:00:18 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.5335, average loss: 0.6131
[12/01 23:00:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 73.20	
[12/01 23:00:18 visual_prompt]: Best epoch 21: best metric: -0.613
[12/01 23:00:18 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[12/01 23:06:34 visual_prompt]: Epoch 22 / 100: avg data time: 9.87e+00, avg batch time: 10.7469, average train loss: 0.7037
[12/01 23:07:18 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.5322, average loss: 0.6401
[12/01 23:07:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 73.18	
[12/01 23:07:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[12/01 23:13:34 visual_prompt]: Epoch 23 / 100: avg data time: 9.88e+00, avg batch time: 10.7638, average train loss: 0.6143
[12/01 23:14:18 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.5370, average loss: 0.6865
[12/01 23:14:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 71.75	
[12/01 23:14:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[12/01 23:20:35 visual_prompt]: Epoch 24 / 100: avg data time: 9.89e+00, avg batch time: 10.7678, average train loss: 0.6276
[12/01 23:21:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5293, average loss: 0.6169
[12/01 23:21:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 73.87	
[12/01 23:21:19 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[12/01 23:27:35 visual_prompt]: Epoch 25 / 100: avg data time: 9.87e+00, avg batch time: 10.7511, average train loss: 0.5928
[12/01 23:28:19 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5319, average loss: 0.6271
[12/01 23:28:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 73.84	
[12/01 23:28:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[12/01 23:34:35 visual_prompt]: Epoch 26 / 100: avg data time: 9.87e+00, avg batch time: 10.7506, average train loss: 0.6679
[12/01 23:35:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5350, average loss: 0.6236
[12/01 23:35:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 73.87	
[12/01 23:35:19 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[12/01 23:41:36 visual_prompt]: Epoch 27 / 100: avg data time: 9.88e+00, avg batch time: 10.7634, average train loss: 0.5936
[12/01 23:42:20 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.5314, average loss: 0.6161
[12/01 23:42:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 71.83	
[12/01 23:42:20 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[12/01 23:48:36 visual_prompt]: Epoch 28 / 100: avg data time: 9.87e+00, avg batch time: 10.7538, average train loss: 0.6253
[12/01 23:49:20 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5240, average loss: 0.6926
[12/01 23:49:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 70.85	
[12/01 23:49:20 visual_prompt]: Stopping early.
[12/01 23:49:20 visual_prompt]: Rank of current process: 0. World size: 1
[12/01 23:49:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/01 23:49:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/01 23:49:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/01 23:49:20 visual_prompt]: Training with config:
[12/01 23:49:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.5_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/01 23:49:20 visual_prompt]: Loading training data...
[12/01 23:49:20 visual_prompt]: Constructing mammo-cbis dataset train...
[12/01 23:49:20 visual_prompt]: Loading validation data...
[12/01 23:49:20 visual_prompt]: Constructing mammo-cbis dataset val...
[12/01 23:49:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[12/01 23:49:23 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[12/01 23:49:23 visual_prompt]: tuned percent:0.536
[12/01 23:49:23 visual_prompt]: Device used for model: 0
[12/01 23:49:23 visual_prompt]: Setting up Evaluator...
[12/01 23:49:23 visual_prompt]: Setting up Trainer...
[12/01 23:49:23 visual_prompt]: 	Setting up the optimizer...
[12/01 23:49:23 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/01 23:55:40 visual_prompt]: Epoch 1 / 100: avg data time: 9.89e+00, avg batch time: 10.7749, average train loss: 1.4006
[12/01 23:56:24 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5307, average loss: 1.2969
[12/01 23:56:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[12/01 23:56:24 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[12/02 00:02:41 visual_prompt]: Epoch 2 / 100: avg data time: 9.90e+00, avg batch time: 10.7830, average train loss: 1.8314
[12/02 00:03:25 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.5308, average loss: 0.6891
[12/02 00:03:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.31	
[12/02 00:03:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[12/02 00:09:42 visual_prompt]: Epoch 3 / 100: avg data time: 9.88e+00, avg batch time: 10.7606, average train loss: 0.7148
[12/02 00:10:25 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5316, average loss: 0.7044
[12/02 00:10:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.55	
[12/02 00:10:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[12/02 00:16:44 visual_prompt]: Epoch 4 / 100: avg data time: 9.95e+00, avg batch time: 10.8259, average train loss: 0.7143
[12/02 00:17:28 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5358, average loss: 0.6849
[12/02 00:17:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.77	
[12/02 00:17:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[12/02 00:23:49 visual_prompt]: Epoch 5 / 100: avg data time: 9.98e+00, avg batch time: 10.8634, average train loss: 0.7509
[12/02 00:24:33 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5386, average loss: 0.8104
[12/02 00:24:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.95	
[12/02 00:24:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[12/02 00:30:52 visual_prompt]: Epoch 6 / 100: avg data time: 9.95e+00, avg batch time: 10.8354, average train loss: 0.7592
[12/02 00:31:36 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.5315, average loss: 0.7212
[12/02 00:31:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[12/02 00:31:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[12/02 00:37:53 visual_prompt]: Epoch 7 / 100: avg data time: 9.88e+00, avg batch time: 10.7603, average train loss: 0.7817
[12/02 00:38:36 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5315, average loss: 0.7184
[12/02 00:38:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.75	
[12/02 00:38:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[12/02 00:44:52 visual_prompt]: Epoch 8 / 100: avg data time: 9.86e+00, avg batch time: 10.7393, average train loss: 0.7735
[12/02 00:45:36 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5296, average loss: 0.6866
[12/02 00:45:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.42	
[12/02 00:45:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[12/02 00:51:54 visual_prompt]: Epoch 9 / 100: avg data time: 9.90e+00, avg batch time: 10.7900, average train loss: 0.7449
[12/02 00:52:38 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5285, average loss: 0.7455
[12/02 00:52:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.86	
[12/02 00:52:38 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[12/02 00:58:55 visual_prompt]: Epoch 10 / 100: avg data time: 9.90e+00, avg batch time: 10.7736, average train loss: 0.7030
[12/02 00:59:39 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5365, average loss: 0.7469
[12/02 00:59:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 59.97	
[12/02 00:59:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[12/02 01:05:56 visual_prompt]: Epoch 11 / 100: avg data time: 9.89e+00, avg batch time: 10.7748, average train loss: 0.7902
[12/02 01:06:39 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5273, average loss: 0.7339
[12/02 01:06:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 61.61	
[12/02 01:06:39 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[12/02 01:12:56 visual_prompt]: Epoch 12 / 100: avg data time: 9.88e+00, avg batch time: 10.7583, average train loss: 0.7502
[12/02 01:13:40 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.5378, average loss: 0.6653
[12/02 01:13:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 64.02	
[12/02 01:13:40 visual_prompt]: Best epoch 12: best metric: -0.665
[12/02 01:13:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[12/02 01:19:57 visual_prompt]: Epoch 13 / 100: avg data time: 9.89e+00, avg batch time: 10.7705, average train loss: 0.7030
[12/02 01:20:41 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5420, average loss: 0.6529
[12/02 01:20:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 66.53	
[12/02 01:20:41 visual_prompt]: Best epoch 13: best metric: -0.653
[12/02 01:20:41 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[12/02 01:26:57 visual_prompt]: Epoch 14 / 100: avg data time: 9.86e+00, avg batch time: 10.7408, average train loss: 0.7449
[12/02 01:27:40 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5310, average loss: 0.7182
[12/02 01:27:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 70.23	
[12/02 01:27:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[12/02 01:33:58 visual_prompt]: Epoch 15 / 100: avg data time: 9.90e+00, avg batch time: 10.7751, average train loss: 0.7914
[12/02 01:34:41 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5285, average loss: 0.7617
[12/02 01:34:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.81	
[12/02 01:34:41 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[12/02 01:40:57 visual_prompt]: Epoch 16 / 100: avg data time: 9.87e+00, avg batch time: 10.7465, average train loss: 0.7061
[12/02 01:41:41 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5291, average loss: 1.0598
[12/02 01:41:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.25	
[12/02 01:41:41 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[12/02 01:47:57 visual_prompt]: Epoch 17 / 100: avg data time: 9.87e+00, avg batch time: 10.7493, average train loss: 0.7755
[12/02 01:48:41 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.5238, average loss: 0.7725
[12/02 01:48:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 70.65	
[12/02 01:48:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[12/02 01:54:58 visual_prompt]: Epoch 18 / 100: avg data time: 9.88e+00, avg batch time: 10.7592, average train loss: 0.6831
[12/02 01:55:41 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5301, average loss: 0.8806
[12/02 01:55:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 72.71	
[12/02 01:55:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[12/02 02:01:58 visual_prompt]: Epoch 19 / 100: avg data time: 9.87e+00, avg batch time: 10.7559, average train loss: 0.6573
[12/02 02:02:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.5189, average loss: 0.6150
[12/02 02:02:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 72.32	
[12/02 02:02:42 visual_prompt]: Best epoch 19: best metric: -0.615
[12/02 02:02:42 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[12/02 02:08:58 visual_prompt]: Epoch 20 / 100: avg data time: 9.86e+00, avg batch time: 10.7425, average train loss: 0.6488
[12/02 02:09:41 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.5302, average loss: 0.6179
[12/02 02:09:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 72.69	
[12/02 02:09:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[12/02 02:15:58 visual_prompt]: Epoch 21 / 100: avg data time: 9.88e+00, avg batch time: 10.7546, average train loss: 0.6757
[12/02 02:16:42 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5326, average loss: 0.6259
[12/02 02:16:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 73.93	
[12/02 02:16:42 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[12/02 02:22:58 visual_prompt]: Epoch 22 / 100: avg data time: 9.86e+00, avg batch time: 10.7426, average train loss: 0.6916
[12/02 02:23:41 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.5228, average loss: 0.6390
[12/02 02:23:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 72.39	
[12/02 02:23:41 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[12/02 02:29:58 visual_prompt]: Epoch 23 / 100: avg data time: 9.87e+00, avg batch time: 10.7562, average train loss: 0.6134
[12/02 02:30:41 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.5311, average loss: 0.7332
[12/02 02:30:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 71.88	
[12/02 02:30:41 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[12/02 02:36:58 visual_prompt]: Epoch 24 / 100: avg data time: 9.87e+00, avg batch time: 10.7506, average train loss: 0.6315
[12/02 02:37:41 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5342, average loss: 0.6292
[12/02 02:37:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 71.07	
[12/02 02:37:42 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[12/02 02:43:58 visual_prompt]: Epoch 25 / 100: avg data time: 9.87e+00, avg batch time: 10.7526, average train loss: 0.5940
[12/02 02:44:42 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5383, average loss: 0.6308
[12/02 02:44:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 72.97	
[12/02 02:44:42 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[12/02 02:50:58 visual_prompt]: Epoch 26 / 100: avg data time: 9.87e+00, avg batch time: 10.7538, average train loss: 0.6377
[12/02 02:51:42 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.5322, average loss: 0.6442
[12/02 02:51:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 71.42	
[12/02 02:51:42 visual_prompt]: Stopping early.
[12/02 02:51:42 visual_prompt]: Rank of current process: 0. World size: 1
[12/02 02:51:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/02 02:51:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/02 02:51:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/02 02:51:42 visual_prompt]: Training with config:
[12/02 02:51:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.25_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/02 02:51:42 visual_prompt]: Loading training data...
[12/02 02:51:42 visual_prompt]: Constructing mammo-cbis dataset train...
[12/02 02:51:42 visual_prompt]: Loading validation data...
[12/02 02:51:42 visual_prompt]: Constructing mammo-cbis dataset val...
[12/02 02:51:42 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[12/02 02:51:45 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[12/02 02:51:45 visual_prompt]: tuned percent:0.536
[12/02 02:51:45 visual_prompt]: Device used for model: 0
[12/02 02:51:45 visual_prompt]: Setting up Evaluator...
[12/02 02:51:45 visual_prompt]: Setting up Trainer...
[12/02 02:51:45 visual_prompt]: 	Setting up the optimizer...
[12/02 02:51:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/02 02:58:03 visual_prompt]: Epoch 1 / 100: avg data time: 9.92e+00, avg batch time: 10.7934, average train loss: 1.4006
[12/02 02:58:47 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5288, average loss: 1.2969
[12/02 02:58:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[12/02 02:58:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[12/02 03:05:03 visual_prompt]: Epoch 2 / 100: avg data time: 9.88e+00, avg batch time: 10.7562, average train loss: 1.3950
[12/02 03:05:47 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5305, average loss: 0.6916
[12/02 03:05:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 52.82	
[12/02 03:05:47 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[12/02 03:12:03 visual_prompt]: Epoch 3 / 100: avg data time: 9.87e+00, avg batch time: 10.7505, average train loss: 0.7023
[12/02 03:12:47 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5348, average loss: 0.6888
[12/02 03:12:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.36	
[12/02 03:12:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[12/02 03:19:04 visual_prompt]: Epoch 4 / 100: avg data time: 9.88e+00, avg batch time: 10.7571, average train loss: 0.6965
[12/02 03:19:47 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5309, average loss: 0.6799
[12/02 03:19:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 61.05	
[12/02 03:19:47 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[12/02 03:26:03 visual_prompt]: Epoch 5 / 100: avg data time: 9.86e+00, avg batch time: 10.7386, average train loss: 0.7122
[12/02 03:26:47 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.5320, average loss: 0.6839
[12/02 03:26:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.14	
[12/02 03:26:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[12/02 03:33:04 visual_prompt]: Epoch 6 / 100: avg data time: 9.89e+00, avg batch time: 10.7734, average train loss: 0.7140
[12/02 03:33:48 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5317, average loss: 0.7077
[12/02 03:33:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.78	
[12/02 03:33:48 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[12/02 03:40:05 visual_prompt]: Epoch 7 / 100: avg data time: 9.90e+00, avg batch time: 10.7728, average train loss: 0.7093
[12/02 03:40:49 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.5285, average loss: 0.6906
[12/02 03:40:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.33	
[12/02 03:40:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[12/02 03:47:05 visual_prompt]: Epoch 8 / 100: avg data time: 9.86e+00, avg batch time: 10.7405, average train loss: 0.7054
[12/02 03:47:48 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.5328, average loss: 0.6858
[12/02 03:47:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.27	
[12/02 03:47:48 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[12/02 03:54:06 visual_prompt]: Epoch 9 / 100: avg data time: 9.90e+00, avg batch time: 10.7781, average train loss: 0.7233
[12/02 03:54:50 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5327, average loss: 0.6898
[12/02 03:54:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.69	
[12/02 03:54:50 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[12/02 04:01:06 visual_prompt]: Epoch 10 / 100: avg data time: 9.88e+00, avg batch time: 10.7614, average train loss: 0.7047
[12/02 04:01:50 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5396, average loss: 0.7100
[12/02 04:01:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.17	
[12/02 04:01:50 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[12/02 04:08:07 visual_prompt]: Epoch 11 / 100: avg data time: 9.89e+00, avg batch time: 10.7693, average train loss: 0.6964
[12/02 04:08:51 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.5322, average loss: 0.6951
[12/02 04:08:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.74	
[12/02 04:08:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[12/02 04:15:07 visual_prompt]: Epoch 12 / 100: avg data time: 9.88e+00, avg batch time: 10.7568, average train loss: 0.7274
[12/02 04:15:51 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.5314, average loss: 0.7142
[12/02 04:15:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.60	
[12/02 04:15:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[12/02 04:22:08 visual_prompt]: Epoch 13 / 100: avg data time: 9.90e+00, avg batch time: 10.7761, average train loss: 0.7346
[12/02 04:22:52 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5302, average loss: 0.6932
[12/02 04:22:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 42.68	rocauc: 50.18	
[12/02 04:22:52 visual_prompt]: Best epoch 13: best metric: -0.693
[12/02 04:22:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[12/02 04:29:08 visual_prompt]: Epoch 14 / 100: avg data time: 9.87e+00, avg batch time: 10.7409, average train loss: 0.7097
[12/02 04:29:51 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5248, average loss: 0.7968
[12/02 04:29:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.53	
[12/02 04:29:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[12/02 04:36:09 visual_prompt]: Epoch 15 / 100: avg data time: 9.89e+00, avg batch time: 10.7715, average train loss: 0.7103
[12/02 04:36:52 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5348, average loss: 0.6905
[12/02 04:36:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.84	
[12/02 04:36:52 visual_prompt]: Best epoch 15: best metric: -0.691
[12/02 04:36:52 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[12/02 04:43:09 visual_prompt]: Epoch 16 / 100: avg data time: 9.87e+00, avg batch time: 10.7515, average train loss: 0.7040
[12/02 04:43:53 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.5324, average loss: 0.7238
[12/02 04:43:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.89	
[12/02 04:43:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[12/02 04:50:09 visual_prompt]: Epoch 17 / 100: avg data time: 9.87e+00, avg batch time: 10.7445, average train loss: 0.7080
[12/02 04:50:52 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.5200, average loss: 0.6985
[12/02 04:50:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.46	
[12/02 04:50:52 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[12/02 04:57:09 visual_prompt]: Epoch 18 / 100: avg data time: 9.88e+00, avg batch time: 10.7563, average train loss: 0.7095
[12/02 04:57:53 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5304, average loss: 0.7898
[12/02 04:57:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.35	
[12/02 04:57:53 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[12/02 05:04:09 visual_prompt]: Epoch 19 / 100: avg data time: 9.87e+00, avg batch time: 10.7474, average train loss: 0.7068
[12/02 05:04:53 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5339, average loss: 0.7250
[12/02 05:04:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.12	
[12/02 05:04:53 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[12/02 05:11:09 visual_prompt]: Epoch 20 / 100: avg data time: 9.88e+00, avg batch time: 10.7584, average train loss: 0.7124
[12/02 05:11:53 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5303, average loss: 0.7099
[12/02 05:11:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.30	
[12/02 05:11:53 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[12/02 05:18:12 visual_prompt]: Epoch 21 / 100: avg data time: 9.96e+00, avg batch time: 10.8382, average train loss: 0.7057
[12/02 05:18:57 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5276, average loss: 0.6903
[12/02 05:18:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.54	
[12/02 05:18:57 visual_prompt]: Best epoch 21: best metric: -0.690
[12/02 05:18:57 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[12/02 05:25:15 visual_prompt]: Epoch 22 / 100: avg data time: 9.93e+00, avg batch time: 10.8114, average train loss: 0.7080
[12/02 05:25:59 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.5353, average loss: 0.7377
[12/02 05:25:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[12/02 05:25:59 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[12/02 05:32:15 visual_prompt]: Epoch 23 / 100: avg data time: 9.88e+00, avg batch time: 10.7561, average train loss: 0.7015
[12/02 05:32:59 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.5192, average loss: 0.6919
[12/02 05:32:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.61	
[12/02 05:32:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[12/02 05:39:15 visual_prompt]: Epoch 24 / 100: avg data time: 9.87e+00, avg batch time: 10.7512, average train loss: 0.6964
[12/02 05:39:59 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.5327, average loss: 0.6891
[12/02 05:39:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.02	
[12/02 05:39:59 visual_prompt]: Best epoch 24: best metric: -0.689
[12/02 05:39:59 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[12/02 05:46:15 visual_prompt]: Epoch 25 / 100: avg data time: 9.86e+00, avg batch time: 10.7433, average train loss: 0.7028
[12/02 05:46:59 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5293, average loss: 0.6888
[12/02 05:46:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.10	
[12/02 05:46:59 visual_prompt]: Best epoch 25: best metric: -0.689
[12/02 05:46:59 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[12/02 05:53:15 visual_prompt]: Epoch 26 / 100: avg data time: 9.86e+00, avg batch time: 10.7435, average train loss: 0.7140
[12/02 05:53:59 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5323, average loss: 0.7139
[12/02 05:53:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.80	
[12/02 05:53:59 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[12/02 06:00:15 visual_prompt]: Epoch 27 / 100: avg data time: 9.86e+00, avg batch time: 10.7356, average train loss: 0.7088
[12/02 06:00:58 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5253, average loss: 0.6899
[12/02 06:00:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.07	
[12/02 06:00:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[12/02 06:07:14 visual_prompt]: Epoch 28 / 100: avg data time: 9.85e+00, avg batch time: 10.7317, average train loss: 0.7065
[12/02 06:07:58 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.5309, average loss: 0.7032
[12/02 06:07:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.33	
[12/02 06:07:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[12/02 06:14:15 visual_prompt]: Epoch 29 / 100: avg data time: 9.90e+00, avg batch time: 10.7793, average train loss: 0.6968
[12/02 06:14:59 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5269, average loss: 0.6999
[12/02 06:14:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.46	
[12/02 06:14:59 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[12/02 06:21:17 visual_prompt]: Epoch 30 / 100: avg data time: 9.93e+00, avg batch time: 10.8056, average train loss: 0.6972
[12/02 06:22:01 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5330, average loss: 0.7123
[12/02 06:22:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.78	
[12/02 06:22:01 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[12/02 06:28:17 visual_prompt]: Epoch 31 / 100: avg data time: 9.87e+00, avg batch time: 10.7468, average train loss: 0.6998
[12/02 06:29:01 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.5313, average loss: 0.7209
[12/02 06:29:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.52	
[12/02 06:29:01 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[12/02 06:35:18 visual_prompt]: Epoch 32 / 100: avg data time: 9.90e+00, avg batch time: 10.7705, average train loss: 0.7374
[12/02 06:36:02 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5345, average loss: 0.7400
[12/02 06:36:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.09	
[12/02 06:36:02 visual_prompt]: Stopping early.
[12/02 06:36:02 visual_prompt]: Rank of current process: 0. World size: 1
[12/02 06:36:02 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/02 06:36:02 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/02 06:36:02 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/02 06:36:02 visual_prompt]: Training with config:
[12/02 06:36:02 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.25_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/02 06:36:02 visual_prompt]: Loading training data...
[12/02 06:36:02 visual_prompt]: Constructing mammo-cbis dataset train...
[12/02 06:36:02 visual_prompt]: Loading validation data...
[12/02 06:36:02 visual_prompt]: Constructing mammo-cbis dataset val...
[12/02 06:36:02 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[12/02 06:36:05 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[12/02 06:36:05 visual_prompt]: tuned percent:0.536
[12/02 06:36:05 visual_prompt]: Device used for model: 0
[12/02 06:36:05 visual_prompt]: Setting up Evaluator...
[12/02 06:36:05 visual_prompt]: Setting up Trainer...
[12/02 06:36:05 visual_prompt]: 	Setting up the optimizer...
[12/02 06:36:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/02 06:42:22 visual_prompt]: Epoch 1 / 100: avg data time: 9.89e+00, avg batch time: 10.7646, average train loss: 1.4006
[12/02 06:43:05 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.5336, average loss: 1.2969
[12/02 06:43:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[12/02 06:43:05 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[12/02 06:49:22 visual_prompt]: Epoch 2 / 100: avg data time: 9.89e+00, avg batch time: 10.7677, average train loss: 1.3993
[12/02 06:50:06 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5311, average loss: 0.6925
[12/02 06:50:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 52.70	
[12/02 06:50:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[12/02 06:56:23 visual_prompt]: Epoch 3 / 100: avg data time: 9.89e+00, avg batch time: 10.7692, average train loss: 0.7063
[12/02 06:57:07 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5334, average loss: 0.6898
[12/02 06:57:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.48	
[12/02 06:57:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[12/02 07:03:24 visual_prompt]: Epoch 4 / 100: avg data time: 9.88e+00, avg batch time: 10.7648, average train loss: 0.6949
[12/02 07:04:07 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.5307, average loss: 0.6819
[12/02 07:04:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 61.33	
[12/02 07:04:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[12/02 07:10:23 visual_prompt]: Epoch 5 / 100: avg data time: 9.85e+00, avg batch time: 10.7294, average train loss: 0.7281
[12/02 07:11:07 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.5284, average loss: 0.6772
[12/02 07:11:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 61.54	
[12/02 07:11:07 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[12/02 07:17:23 visual_prompt]: Epoch 6 / 100: avg data time: 9.88e+00, avg batch time: 10.7594, average train loss: 0.7184
[12/02 07:18:07 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.5269, average loss: 0.6790
[12/02 07:18:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.11	
[12/02 07:18:07 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[12/02 07:24:25 visual_prompt]: Epoch 7 / 100: avg data time: 9.91e+00, avg batch time: 10.7881, average train loss: 0.7048
[12/02 07:25:09 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5313, average loss: 0.6735
[12/02 07:25:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.18	
[12/02 07:25:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[12/02 07:31:27 visual_prompt]: Epoch 8 / 100: avg data time: 9.93e+00, avg batch time: 10.8056, average train loss: 0.7026
[12/02 07:32:11 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.5370, average loss: 0.6677
[12/02 07:32:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 61.55	
[12/02 07:32:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[12/02 07:38:28 visual_prompt]: Epoch 9 / 100: avg data time: 9.89e+00, avg batch time: 10.7673, average train loss: 0.6946
[12/02 07:39:11 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5357, average loss: 0.7027
[12/02 07:39:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 65.23	
[12/02 07:39:11 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[12/02 07:45:28 visual_prompt]: Epoch 10 / 100: avg data time: 9.87e+00, avg batch time: 10.7557, average train loss: 0.7348
[12/02 07:46:12 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.5316, average loss: 0.6761
[12/02 07:46:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 63.45	
[12/02 07:46:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[12/02 07:52:28 visual_prompt]: Epoch 11 / 100: avg data time: 9.87e+00, avg batch time: 10.7528, average train loss: 0.6997
[12/02 07:53:12 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5308, average loss: 0.6715
[12/02 07:53:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 65.97	
[12/02 07:53:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[12/02 07:59:28 visual_prompt]: Epoch 12 / 100: avg data time: 9.87e+00, avg batch time: 10.7444, average train loss: 0.6965
[12/02 08:00:12 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5227, average loss: 0.7356
[12/02 08:00:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.96	
[12/02 08:00:12 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[12/02 08:06:29 visual_prompt]: Epoch 13 / 100: avg data time: 9.90e+00, avg batch time: 10.7746, average train loss: 0.7295
[12/02 08:07:13 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5312, average loss: 0.6857
[12/02 08:07:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 63.97	
[12/02 08:07:13 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[12/02 08:13:29 visual_prompt]: Epoch 14 / 100: avg data time: 9.87e+00, avg batch time: 10.7460, average train loss: 0.6884
[12/02 08:14:13 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5307, average loss: 0.7068
[12/02 08:14:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.88	
[12/02 08:14:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[12/02 08:20:30 visual_prompt]: Epoch 15 / 100: avg data time: 9.90e+00, avg batch time: 10.7765, average train loss: 0.7107
[12/02 08:21:14 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.5401, average loss: 0.8164
[12/02 08:21:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.35	
[12/02 08:21:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[12/02 08:27:29 visual_prompt]: Epoch 16 / 100: avg data time: 9.86e+00, avg batch time: 10.7396, average train loss: 0.7402
[12/02 08:28:13 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5290, average loss: 0.6585
[12/02 08:28:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.20	
[12/02 08:28:13 visual_prompt]: Best epoch 16: best metric: -0.658
[12/02 08:28:13 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[12/02 08:34:29 visual_prompt]: Epoch 17 / 100: avg data time: 9.85e+00, avg batch time: 10.7303, average train loss: 0.6993
[12/02 08:35:13 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.5250, average loss: 0.6697
[12/02 08:35:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 66.49	
[12/02 08:35:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[12/02 08:41:29 visual_prompt]: Epoch 18 / 100: avg data time: 9.87e+00, avg batch time: 10.7529, average train loss: 0.6679
[12/02 08:42:13 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5316, average loss: 0.6516
[12/02 08:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 69.20	
[12/02 08:42:13 visual_prompt]: Best epoch 18: best metric: -0.652
[12/02 08:42:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[12/02 08:48:29 visual_prompt]: Epoch 19 / 100: avg data time: 9.86e+00, avg batch time: 10.7410, average train loss: 0.6398
[12/02 08:49:13 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.5296, average loss: 0.6772
[12/02 08:49:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.28	
[12/02 08:49:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[12/02 08:55:29 visual_prompt]: Epoch 20 / 100: avg data time: 9.88e+00, avg batch time: 10.7610, average train loss: 0.6531
[12/02 08:56:13 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.5307, average loss: 0.6964
[12/02 08:56:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.41	rocauc: 68.11	
[12/02 08:56:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[12/02 09:02:30 visual_prompt]: Epoch 21 / 100: avg data time: 9.88e+00, avg batch time: 10.7625, average train loss: 0.6666
[12/02 09:03:13 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5305, average loss: 0.6906
[12/02 09:03:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.55	
[12/02 09:03:13 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[12/02 09:09:30 visual_prompt]: Epoch 22 / 100: avg data time: 9.88e+00, avg batch time: 10.7586, average train loss: 0.6735
[12/02 09:10:14 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.5345, average loss: 0.6341
[12/02 09:10:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.16	
[12/02 09:10:14 visual_prompt]: Best epoch 22: best metric: -0.634
[12/02 09:10:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[12/02 09:16:30 visual_prompt]: Epoch 23 / 100: avg data time: 9.88e+00, avg batch time: 10.7572, average train loss: 0.6506
[12/02 09:17:14 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.5266, average loss: 0.7318
[12/02 09:17:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.72	
[12/02 09:17:14 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[12/02 09:23:31 visual_prompt]: Epoch 24 / 100: avg data time: 9.88e+00, avg batch time: 10.7635, average train loss: 0.6983
[12/02 09:24:15 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5314, average loss: 0.6478
[12/02 09:24:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 67.60	
[12/02 09:24:15 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[12/02 09:30:31 visual_prompt]: Epoch 25 / 100: avg data time: 9.87e+00, avg batch time: 10.7486, average train loss: 0.6335
[12/02 09:31:15 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5356, average loss: 0.6325
[12/02 09:31:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.91	
[12/02 09:31:15 visual_prompt]: Best epoch 25: best metric: -0.632
[12/02 09:31:15 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[12/02 09:37:31 visual_prompt]: Epoch 26 / 100: avg data time: 9.86e+00, avg batch time: 10.7351, average train loss: 0.7118
[12/02 09:38:14 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5327, average loss: 0.6665
[12/02 09:38:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.38	
[12/02 09:38:14 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[12/02 09:44:32 visual_prompt]: Epoch 27 / 100: avg data time: 9.90e+00, avg batch time: 10.7811, average train loss: 0.6563
[12/02 09:45:16 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5347, average loss: 0.6319
[12/02 09:45:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 70.76	
[12/02 09:45:16 visual_prompt]: Best epoch 27: best metric: -0.632
[12/02 09:45:16 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[12/02 09:51:32 visual_prompt]: Epoch 28 / 100: avg data time: 9.87e+00, avg batch time: 10.7498, average train loss: 0.6762
[12/02 09:52:15 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5270, average loss: 0.7221
[12/02 09:52:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.28	
[12/02 09:52:15 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[12/02 09:58:32 visual_prompt]: Epoch 29 / 100: avg data time: 9.89e+00, avg batch time: 10.7751, average train loss: 0.6358
[12/02 09:59:16 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5324, average loss: 0.6440
[12/02 09:59:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 71.84	
[12/02 09:59:16 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[12/02 10:05:34 visual_prompt]: Epoch 30 / 100: avg data time: 9.90e+00, avg batch time: 10.7760, average train loss: 0.6330
[12/02 10:06:17 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.5246, average loss: 0.6835
[12/02 10:06:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 70.80	
[12/02 10:06:17 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[12/02 10:12:34 visual_prompt]: Epoch 31 / 100: avg data time: 9.89e+00, avg batch time: 10.7697, average train loss: 0.6208
[12/02 10:13:18 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5341, average loss: 0.6708
[12/02 10:13:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 73.38	
[12/02 10:13:18 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[12/02 10:19:35 visual_prompt]: Epoch 32 / 100: avg data time: 9.89e+00, avg batch time: 10.7693, average train loss: 0.6322
[12/02 10:20:19 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5335, average loss: 0.7142
[12/02 10:20:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 70.80	
[12/02 10:20:19 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[12/02 10:26:36 visual_prompt]: Epoch 33 / 100: avg data time: 9.89e+00, avg batch time: 10.7637, average train loss: 0.6137
[12/02 10:27:19 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.5325, average loss: 0.6594
[12/02 10:27:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 71.26	
[12/02 10:27:19 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[12/02 10:33:36 visual_prompt]: Epoch 34 / 100: avg data time: 9.87e+00, avg batch time: 10.7456, average train loss: 0.6065
[12/02 10:34:19 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5296, average loss: 0.6241
[12/02 10:34:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 72.13	
[12/02 10:34:19 visual_prompt]: Best epoch 34: best metric: -0.624
[12/02 10:34:19 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[12/02 10:40:36 visual_prompt]: Epoch 35 / 100: avg data time: 9.87e+00, avg batch time: 10.7487, average train loss: 0.6196
[12/02 10:41:19 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5333, average loss: 0.6649
[12/02 10:41:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 69.85	
[12/02 10:41:19 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[12/02 10:47:35 visual_prompt]: Epoch 36 / 100: avg data time: 9.86e+00, avg batch time: 10.7424, average train loss: 0.6399
[12/02 10:48:19 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5350, average loss: 0.6727
[12/02 10:48:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.81	
[12/02 10:48:19 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[12/02 10:54:34 visual_prompt]: Epoch 37 / 100: avg data time: 9.85e+00, avg batch time: 10.7277, average train loss: 0.6208
[12/02 10:55:18 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5314, average loss: 0.6427
[12/02 10:55:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 72.60	
[12/02 10:55:18 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[12/02 11:01:35 visual_prompt]: Epoch 38 / 100: avg data time: 9.88e+00, avg batch time: 10.7613, average train loss: 0.6195
[12/02 11:02:19 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5340, average loss: 0.7439
[12/02 11:02:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 71.40	
[12/02 11:02:19 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[12/02 11:08:36 visual_prompt]: Epoch 39 / 100: avg data time: 9.90e+00, avg batch time: 10.7724, average train loss: 0.6485
[12/02 11:09:20 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.5297, average loss: 0.6567
[12/02 11:09:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 72.01	
[12/02 11:09:20 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506

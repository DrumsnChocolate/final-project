/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/07 14:57:04 visual_prompt]: Rank of current process: 0. World size: 1
[11/07 14:57:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/07 14:57:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/07 14:57:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/07 14:57:04 visual_prompt]: Training with config:
[11/07 14:57:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr50.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/07 14:57:04 visual_prompt]: Loading training data...
[11/07 14:57:04 visual_prompt]: Constructing mammo-cbis dataset train...
[11/07 14:57:04 visual_prompt]: Loading validation data...
[11/07 14:57:04 visual_prompt]: Constructing mammo-cbis dataset val...
[11/07 14:57:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/07 14:57:07 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/07 14:57:07 visual_prompt]: tuned percent:0.536
[11/07 14:57:07 visual_prompt]: Device used for model: 0
[11/07 14:57:07 visual_prompt]: Setting up Evaluator...
[11/07 14:57:07 visual_prompt]: Setting up Trainer...
[11/07 14:57:07 visual_prompt]: 	Setting up the optimizer...
[11/07 14:57:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/07 15:03:22 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6955, average train loss: 1.4017
[11/07 15:04:04 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1567, average loss: 1.2969
[11/07 15:04:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/07 15:04:04 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/07 15:10:13 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5510, average train loss: 41.7326
[11/07 15:10:56 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1556, average loss: 26.4996
[11/07 15:10:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.38	
[11/07 15:10:56 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/07 15:17:18 visual_prompt]: Epoch 3 / 100: avg data time: 1.06e+01, avg batch time: 10.9154, average train loss: 21.3312
[11/07 15:18:00 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1561, average loss: 28.1031
[11/07 15:18:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.55	
[11/07 15:18:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/07 15:24:21 visual_prompt]: Epoch 4 / 100: avg data time: 1.05e+01, avg batch time: 10.8869, average train loss: 30.9754
[11/07 15:25:03 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1561, average loss: 28.4330
[11/07 15:25:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.87	
[11/07 15:25:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/07 15:31:12 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5531, average train loss: 61.5555
[11/07 15:31:54 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1578, average loss: 5.5913
[11/07 15:31:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.85	
[11/07 15:31:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/07 15:38:07 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6539, average train loss: 54.7343
[11/07 15:38:49 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1566, average loss: 60.5731
[11/07 15:38:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.52	
[11/07 15:38:49 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/07 15:45:01 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6303, average train loss: 93.2979
[11/07 15:45:43 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1559, average loss: 155.7742
[11/07 15:45:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.47	
[11/07 15:45:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/07 15:51:53 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.5518, average train loss: 107.0309
[11/07 15:52:35 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1559, average loss: 78.7135
[11/07 15:52:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.10	
[11/07 15:52:35 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/07 15:59:06 visual_prompt]: Epoch 9 / 100: avg data time: 1.08e+01, avg batch time: 11.1653, average train loss: 150.3894
[11/07 15:59:49 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1573, average loss: 61.5476
[11/07 15:59:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.88	
[11/07 15:59:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/07 16:06:12 visual_prompt]: Epoch 10 / 100: avg data time: 1.06e+01, avg batch time: 10.9346, average train loss: 113.5575
[11/07 16:06:53 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1558, average loss: 241.0500
[11/07 16:06:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.41	
[11/07 16:06:53 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/07 16:13:04 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5962, average train loss: 150.2141
[11/07 16:13:46 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1561, average loss: 83.8923
[11/07 16:13:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.13	
[11/07 16:13:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/07 16:19:57 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5725, average train loss: 125.9721
[11/07 16:20:39 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1558, average loss: 27.2413
[11/07 16:20:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.39	
[11/07 16:20:39 visual_prompt]: Best epoch 12: best metric: -27.241
[11/07 16:20:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/07 16:26:50 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.5824, average train loss: 193.1136
[11/07 16:27:31 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1563, average loss: 61.8789
[11/07 16:27:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.29	
[11/07 16:27:31 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/07 16:33:41 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5538, average train loss: 118.6384
[11/07 16:34:23 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1560, average loss: 345.6418
[11/07 16:34:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.38	
[11/07 16:34:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/07 16:40:41 visual_prompt]: Epoch 15 / 100: avg data time: 1.05e+01, avg batch time: 10.8144, average train loss: 161.0755
[11/07 16:41:23 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1560, average loss: 414.8229
[11/07 16:41:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.76	
[11/07 16:41:23 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/07 16:47:33 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.5667, average train loss: 172.0898
[11/07 16:48:15 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1561, average loss: 4.8787
[11/07 16:48:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.59	
[11/07 16:48:15 visual_prompt]: Best epoch 16: best metric: -4.879
[11/07 16:48:15 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/07 16:54:25 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.5534, average train loss: 114.2237
[11/07 16:55:07 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1560, average loss: 185.6744
[11/07 16:55:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 46.68	
[11/07 16:55:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/07 17:01:17 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.5658, average train loss: 133.5301
[11/07 17:01:59 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1560, average loss: 40.2502
[11/07 17:01:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.67	
[11/07 17:01:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/07 17:08:08 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.5415, average train loss: 105.4043
[11/07 17:08:50 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1563, average loss: 47.7231
[11/07 17:08:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.09	
[11/07 17:08:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/07 17:15:00 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.5738, average train loss: 140.3453
[11/07 17:15:42 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1557, average loss: 145.7717
[11/07 17:15:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.03	
[11/07 17:15:42 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/07 17:21:51 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.5522, average train loss: 162.4553
[11/07 17:22:33 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1559, average loss: 141.7085
[11/07 17:22:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.28	
[11/07 17:22:33 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/07 17:28:43 visual_prompt]: Epoch 22 / 100: avg data time: 1.02e+01, avg batch time: 10.5584, average train loss: 164.5457
[11/07 17:29:25 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1561, average loss: 196.1657
[11/07 17:29:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.26	
[11/07 17:29:25 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/07 17:35:34 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e+01, avg batch time: 10.5571, average train loss: 143.4300
[11/07 17:36:16 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1560, average loss: 122.1477
[11/07 17:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.24	
[11/07 17:36:16 visual_prompt]: Stopping early.
[11/07 17:36:16 visual_prompt]: Rank of current process: 0. World size: 1
[11/07 17:36:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/07 17:36:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/07 17:36:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/07 17:36:16 visual_prompt]: Training with config:
[11/07 17:36:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr50.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/07 17:36:16 visual_prompt]: Loading training data...
[11/07 17:36:16 visual_prompt]: Constructing mammo-cbis dataset train...
[11/07 17:36:17 visual_prompt]: Loading validation data...
[11/07 17:36:17 visual_prompt]: Constructing mammo-cbis dataset val...
[11/07 17:36:17 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/07 17:36:19 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/07 17:36:19 visual_prompt]: tuned percent:0.536
[11/07 17:36:19 visual_prompt]: Device used for model: 0
[11/07 17:36:19 visual_prompt]: Setting up Evaluator...
[11/07 17:36:19 visual_prompt]: Setting up Trainer...
[11/07 17:36:19 visual_prompt]: 	Setting up the optimizer...
[11/07 17:36:19 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/07 17:42:30 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.5886, average train loss: 1.4017
[11/07 17:43:12 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1559, average loss: 1.2969
[11/07 17:43:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/07 17:43:12 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/07 17:49:22 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5708, average train loss: 23.6110
[11/07 17:50:05 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1560, average loss: 5.8490
[11/07 17:50:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.10	
[11/07 17:50:05 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/07 17:56:15 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.5701, average train loss: 19.9750
[11/07 17:56:57 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1562, average loss: 15.5227
[11/07 17:56:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.19	
[11/07 17:56:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/07 18:03:07 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.5741, average train loss: 29.2622
[11/07 18:03:49 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1557, average loss: 42.5903
[11/07 18:03:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.03	
[11/07 18:03:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/07 18:09:59 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5595, average train loss: 42.5479
[11/07 18:10:41 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1559, average loss: 91.3031
[11/07 18:10:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.93	
[11/07 18:10:41 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/07 18:16:53 visual_prompt]: Epoch 6 / 100: avg data time: 1.02e+01, avg batch time: 10.6038, average train loss: 82.3112
[11/07 18:17:35 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1597, average loss: 5.4077
[11/07 18:17:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.87	
[11/07 18:17:35 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/07 18:23:45 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.5915, average train loss: 83.1905
[11/07 18:24:28 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1559, average loss: 17.8734
[11/07 18:24:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.35	
[11/07 18:24:28 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/07 18:30:39 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.6018, average train loss: 120.9400
[11/07 18:31:21 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1558, average loss: 56.7837
[11/07 18:31:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.62	
[11/07 18:31:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/07 18:37:32 visual_prompt]: Epoch 9 / 100: avg data time: 1.02e+01, avg batch time: 10.5949, average train loss: 93.1148
[11/07 18:38:14 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1568, average loss: 27.9779
[11/07 18:38:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.46	
[11/07 18:38:14 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/07 18:44:24 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5670, average train loss: 94.5493
[11/07 18:45:06 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1556, average loss: 78.7065
[11/07 18:45:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.33	
[11/07 18:45:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/07 18:51:17 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5920, average train loss: 100.2712
[11/07 18:51:59 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1589, average loss: 213.1924
[11/07 18:51:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.50	
[11/07 18:51:59 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/07 18:58:09 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5652, average train loss: 140.1140
[11/07 18:58:52 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1561, average loss: 62.5851
[11/07 18:58:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.80	
[11/07 18:58:52 visual_prompt]: Best epoch 12: best metric: -62.585
[11/07 18:58:52 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/07 19:05:02 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.5866, average train loss: 161.3989
[11/07 19:05:45 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1557, average loss: 604.5018
[11/07 19:05:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.07	
[11/07 19:05:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/07 19:11:54 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5548, average train loss: 97.6883
[11/07 19:12:36 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1566, average loss: 360.5183
[11/07 19:12:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.48	
[11/07 19:12:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/07 19:18:47 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.5967, average train loss: 115.8086
[11/07 19:19:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1561, average loss: 43.2895
[11/07 19:19:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.34	
[11/07 19:19:29 visual_prompt]: Best epoch 15: best metric: -43.289
[11/07 19:19:29 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/07 19:25:39 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.5611, average train loss: 93.2741
[11/07 19:26:21 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1555, average loss: 125.7308
[11/07 19:26:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.25	
[11/07 19:26:21 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/07 19:32:31 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.5516, average train loss: 150.8188
[11/07 19:33:13 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1558, average loss: 86.5378
[11/07 19:33:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.85	
[11/07 19:33:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/07 19:39:23 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.5725, average train loss: 207.3823
[11/07 19:40:05 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1561, average loss: 249.7586
[11/07 19:40:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.08	
[11/07 19:40:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/07 19:46:15 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.5561, average train loss: 100.9299
[11/07 19:46:57 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1564, average loss: 71.4048
[11/07 19:46:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.79	
[11/07 19:46:57 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/07 19:53:07 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.5662, average train loss: 129.8789
[11/07 19:53:49 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1560, average loss: 109.9322
[11/07 19:53:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.03	
[11/07 19:53:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/07 19:59:59 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.5584, average train loss: 126.5888
[11/07 20:00:41 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1559, average loss: 43.1655
[11/07 20:00:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.89	
[11/07 20:00:41 visual_prompt]: Best epoch 21: best metric: -43.166
[11/07 20:00:41 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/07 20:06:51 visual_prompt]: Epoch 22 / 100: avg data time: 1.02e+01, avg batch time: 10.5639, average train loss: 106.7672
[11/07 20:07:33 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 93.1690
[11/07 20:07:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.11	
[11/07 20:07:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/07 20:13:43 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e+01, avg batch time: 10.5605, average train loss: 129.5828
[11/07 20:14:25 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1563, average loss: 89.3717
[11/07 20:14:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.39	
[11/07 20:14:25 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/07 20:20:38 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e+01, avg batch time: 10.6498, average train loss: 108.7643
[11/07 20:21:20 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1556, average loss: 226.1878
[11/07 20:21:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.34	
[11/07 20:21:20 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/07 20:27:30 visual_prompt]: Epoch 25 / 100: avg data time: 1.02e+01, avg batch time: 10.5618, average train loss: 109.0276
[11/07 20:28:12 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1559, average loss: 47.7153
[11/07 20:28:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.19	
[11/07 20:28:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/07 20:34:22 visual_prompt]: Epoch 26 / 100: avg data time: 1.02e+01, avg batch time: 10.5609, average train loss: 92.6415
[11/07 20:35:04 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1566, average loss: 166.3768
[11/07 20:35:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.11	
[11/07 20:35:04 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/07 20:41:14 visual_prompt]: Epoch 27 / 100: avg data time: 1.02e+01, avg batch time: 10.5694, average train loss: 142.8833
[11/07 20:41:56 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1559, average loss: 270.1199
[11/07 20:41:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.77	
[11/07 20:41:56 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/07 20:48:06 visual_prompt]: Epoch 28 / 100: avg data time: 1.02e+01, avg batch time: 10.5709, average train loss: 110.8392
[11/07 20:48:48 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1558, average loss: 62.8878
[11/07 20:48:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.45	
[11/07 20:48:48 visual_prompt]: Stopping early.
[11/07 20:48:48 visual_prompt]: Rank of current process: 0. World size: 1
[11/07 20:48:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/07 20:48:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/07 20:48:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/07 20:48:48 visual_prompt]: Training with config:
[11/07 20:48:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr50.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/07 20:48:48 visual_prompt]: Loading training data...
[11/07 20:48:48 visual_prompt]: Constructing mammo-cbis dataset train...
[11/07 20:48:48 visual_prompt]: Loading validation data...
[11/07 20:48:48 visual_prompt]: Constructing mammo-cbis dataset val...
[11/07 20:48:48 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/07 20:48:51 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/07 20:48:51 visual_prompt]: tuned percent:0.536
[11/07 20:48:51 visual_prompt]: Device used for model: 0
[11/07 20:48:51 visual_prompt]: Setting up Evaluator...
[11/07 20:48:51 visual_prompt]: Setting up Trainer...
[11/07 20:48:51 visual_prompt]: 	Setting up the optimizer...
[11/07 20:48:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/07 20:55:02 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.5958, average train loss: 1.4017
[11/07 20:55:44 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1560, average loss: 1.2969
[11/07 20:55:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/07 20:55:44 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/07 21:01:54 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5671, average train loss: 22.1121
[11/07 21:02:36 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1562, average loss: 12.3772
[11/07 21:02:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.20	
[11/07 21:02:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/07 21:08:46 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.5687, average train loss: 23.4895
[11/07 21:09:28 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1562, average loss: 32.5041
[11/07 21:09:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.99	
[11/07 21:09:28 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/07 21:15:39 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.5915, average train loss: 39.9400
[11/07 21:16:21 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1558, average loss: 73.8900
[11/07 21:16:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.01	
[11/07 21:16:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/07 21:22:32 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5949, average train loss: 68.0024
[11/07 21:23:15 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1559, average loss: 63.6253
[11/07 21:23:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.42	
[11/07 21:23:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/07 21:29:28 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6677, average train loss: 85.1397
[11/07 21:30:11 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1560, average loss: 82.4955
[11/07 21:30:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.13	
[11/07 21:30:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/07 21:36:22 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.6031, average train loss: 45.2378
[11/07 21:37:04 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1588, average loss: 87.7445
[11/07 21:37:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.75	
[11/07 21:37:04 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/07 21:43:15 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.5849, average train loss: 58.8196
[11/07 21:43:57 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1560, average loss: 76.0677
[11/07 21:43:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.23	
[11/07 21:43:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/07 21:50:08 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6096, average train loss: 84.5682
[11/07 21:50:51 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1563, average loss: 126.4814
[11/07 21:50:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.42	
[11/07 21:50:51 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/07 21:57:01 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5774, average train loss: 54.1289
[11/07 21:57:43 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1559, average loss: 132.2860
[11/07 21:57:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.23	
[11/07 21:57:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/07 22:03:54 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5773, average train loss: 89.0246
[11/07 22:04:36 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1576, average loss: 88.2778
[11/07 22:04:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.90	
[11/07 22:04:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/07 22:10:46 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5647, average train loss: 60.0773
[11/07 22:11:28 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1558, average loss: 88.3181
[11/07 22:11:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.17	
[11/07 22:11:28 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/07 22:17:39 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.5857, average train loss: 102.6829
[11/07 22:18:21 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1582, average loss: 125.7947
[11/07 22:18:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.11	
[11/07 22:18:21 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/07 22:24:31 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5690, average train loss: 98.7248
[11/07 22:25:13 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1588, average loss: 249.8005
[11/07 22:25:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.81	
[11/07 22:25:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/07 22:31:24 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.5954, average train loss: 135.1511
[11/07 22:32:06 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1559, average loss: 251.1749
[11/07 22:32:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.82	
[11/07 22:32:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/07 22:38:17 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.6030, average train loss: 86.3924
[11/07 22:39:00 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1558, average loss: 141.1784
[11/07 22:39:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.99	
[11/07 22:39:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/07 22:45:14 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6772, average train loss: 102.0093
[11/07 22:45:56 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1556, average loss: 48.0747
[11/07 22:45:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.21	
[11/07 22:45:57 visual_prompt]: Best epoch 17: best metric: -48.075
[11/07 22:45:57 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/07 22:52:10 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6679, average train loss: 86.9874
[11/07 22:52:53 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1559, average loss: 57.4515
[11/07 22:52:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.56	
[11/07 22:52:53 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/07 22:59:05 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6296, average train loss: 80.2692
[11/07 22:59:47 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1590, average loss: 135.2312
[11/07 22:59:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.16	
[11/07 22:59:47 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/07 23:05:57 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.5732, average train loss: 98.2957
[11/07 23:06:40 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1558, average loss: 79.6041
[11/07 23:06:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.22	
[11/07 23:06:40 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/07 23:12:50 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.5690, average train loss: 95.6013
[11/07 23:13:32 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1580, average loss: 154.4986
[11/07 23:13:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.79	
[11/07 23:13:32 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/07 23:19:43 visual_prompt]: Epoch 22 / 100: avg data time: 1.02e+01, avg batch time: 10.5894, average train loss: 61.4619
[11/07 23:20:25 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1558, average loss: 159.0264
[11/07 23:20:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.86	
[11/07 23:20:25 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/07 23:26:35 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e+01, avg batch time: 10.5828, average train loss: 92.0237
[11/07 23:27:18 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1563, average loss: 29.3649
[11/07 23:27:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.03	
[11/07 23:27:18 visual_prompt]: Best epoch 23: best metric: -29.365
[11/07 23:27:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/07 23:33:28 visual_prompt]: Epoch 24 / 100: avg data time: 1.02e+01, avg batch time: 10.5866, average train loss: 56.4870
[11/07 23:34:11 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1558, average loss: 109.6253
[11/07 23:34:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 39.10	
[11/07 23:34:11 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/07 23:40:21 visual_prompt]: Epoch 25 / 100: avg data time: 1.02e+01, avg batch time: 10.5744, average train loss: 63.3988
[11/07 23:41:03 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1558, average loss: 80.8156
[11/07 23:41:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.00	
[11/07 23:41:03 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/07 23:47:14 visual_prompt]: Epoch 26 / 100: avg data time: 1.02e+01, avg batch time: 10.5794, average train loss: 53.7435
[11/07 23:47:56 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1556, average loss: 4.8900
[11/07 23:47:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.38	
[11/07 23:47:56 visual_prompt]: Best epoch 26: best metric: -4.890
[11/07 23:47:56 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/07 23:54:06 visual_prompt]: Epoch 27 / 100: avg data time: 1.02e+01, avg batch time: 10.5787, average train loss: 84.0545
[11/07 23:54:48 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1561, average loss: 231.9026
[11/07 23:54:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.74	
[11/07 23:54:48 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/08 00:00:59 visual_prompt]: Epoch 28 / 100: avg data time: 1.02e+01, avg batch time: 10.5782, average train loss: 88.4511
[11/08 00:01:41 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1560, average loss: 113.8948
[11/08 00:01:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.78	
[11/08 00:01:41 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/08 00:07:53 visual_prompt]: Epoch 29 / 100: avg data time: 1.03e+01, avg batch time: 10.6149, average train loss: 89.1379
[11/08 00:08:35 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1558, average loss: 187.0372
[11/08 00:08:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.06	
[11/08 00:08:35 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/08 00:14:46 visual_prompt]: Epoch 30 / 100: avg data time: 1.02e+01, avg batch time: 10.5908, average train loss: 85.3108
[11/08 00:15:28 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1594, average loss: 103.6908
[11/08 00:15:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.96	
[11/08 00:15:28 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/08 00:21:39 visual_prompt]: Epoch 31 / 100: avg data time: 1.02e+01, avg batch time: 10.5848, average train loss: 72.5413
[11/08 00:22:21 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1558, average loss: 14.8531
[11/08 00:22:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.09	
[11/08 00:22:21 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/08 00:28:32 visual_prompt]: Epoch 32 / 100: avg data time: 1.02e+01, avg batch time: 10.5850, average train loss: 88.2951
[11/08 00:29:14 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1559, average loss: 67.6624
[11/08 00:29:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.32	
[11/08 00:29:14 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/08 00:35:24 visual_prompt]: Epoch 33 / 100: avg data time: 1.02e+01, avg batch time: 10.5733, average train loss: 74.5266
[11/08 00:36:06 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1561, average loss: 167.3409
[11/08 00:36:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.54	
[11/08 00:36:06 visual_prompt]: Stopping early.
[11/08 00:36:07 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 00:36:07 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 00:36:07 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 00:36:07 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 00:36:07 visual_prompt]: Training with config:
[11/08 00:36:07 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr50.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 00:36:07 visual_prompt]: Loading training data...
[11/08 00:36:07 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 00:36:07 visual_prompt]: Loading validation data...
[11/08 00:36:07 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 00:36:07 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 00:36:09 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 00:36:09 visual_prompt]: tuned percent:0.536
[11/08 00:36:09 visual_prompt]: Device used for model: 0
[11/08 00:36:09 visual_prompt]: Setting up Evaluator...
[11/08 00:36:09 visual_prompt]: Setting up Trainer...
[11/08 00:36:09 visual_prompt]: 	Setting up the optimizer...
[11/08 00:36:09 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 00:42:20 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.5967, average train loss: 1.4017
[11/08 00:43:02 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1559, average loss: 1.2969
[11/08 00:43:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/08 00:43:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/08 00:49:13 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5750, average train loss: 21.7661
[11/08 00:49:55 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1561, average loss: 25.3822
[11/08 00:49:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.86	
[11/08 00:49:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/08 00:56:05 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.5704, average train loss: 34.1849
[11/08 00:56:47 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1559, average loss: 16.2270
[11/08 00:56:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.02	
[11/08 00:56:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/08 01:02:58 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.5944, average train loss: 47.3847
[11/08 01:03:40 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1561, average loss: 92.7004
[11/08 01:03:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.74	
[11/08 01:03:40 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/08 01:09:51 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5761, average train loss: 46.3534
[11/08 01:10:33 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1561, average loss: 17.4367
[11/08 01:10:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.76	
[11/08 01:10:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/08 01:16:44 visual_prompt]: Epoch 6 / 100: avg data time: 1.02e+01, avg batch time: 10.5938, average train loss: 50.4854
[11/08 01:17:26 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1563, average loss: 52.2753
[11/08 01:17:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.56	
[11/08 01:17:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/08 01:23:37 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.5941, average train loss: 60.1930
[11/08 01:24:19 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1598, average loss: 48.0877
[11/08 01:24:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.22	
[11/08 01:24:19 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/08 01:30:29 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.5649, average train loss: 54.7363
[11/08 01:31:11 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1586, average loss: 58.8555
[11/08 01:31:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.03	
[11/08 01:31:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/08 01:37:22 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6035, average train loss: 42.7252
[11/08 01:38:05 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1563, average loss: 2.2491
[11/08 01:38:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.39	
[11/08 01:38:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/08 01:44:15 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5793, average train loss: 40.9211
[11/08 01:44:58 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1563, average loss: 58.8204
[11/08 01:44:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.17	
[11/08 01:44:58 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/08 01:51:08 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5793, average train loss: 26.3818
[11/08 01:51:51 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1560, average loss: 26.9018
[11/08 01:51:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.10	
[11/08 01:51:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/08 01:58:01 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5938, average train loss: 31.1934
[11/08 01:58:44 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1588, average loss: 33.0704
[11/08 01:58:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.81	
[11/08 01:58:44 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/08 02:04:58 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.6752, average train loss: 31.9190
[11/08 02:05:40 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1559, average loss: 0.7135
[11/08 02:05:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.05	
[11/08 02:05:40 visual_prompt]: Best epoch 13: best metric: -0.713
[11/08 02:05:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/08 02:11:54 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.6594, average train loss: 43.5940
[11/08 02:12:36 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1558, average loss: 171.8142
[11/08 02:12:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.90	
[11/08 02:12:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/08 02:18:50 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6938, average train loss: 46.4447
[11/08 02:19:33 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1558, average loss: 59.5566
[11/08 02:19:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.29	
[11/08 02:19:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/08 02:25:46 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6543, average train loss: 43.0909
[11/08 02:26:29 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1562, average loss: 30.7253
[11/08 02:26:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.77	
[11/08 02:26:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/08 02:32:41 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6278, average train loss: 34.5476
[11/08 02:33:23 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1560, average loss: 21.3194
[11/08 02:33:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.23	
[11/08 02:33:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/08 02:39:35 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6167, average train loss: 36.2032
[11/08 02:40:17 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1560, average loss: 36.6952
[11/08 02:40:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.02	
[11/08 02:40:18 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/08 02:46:31 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6632, average train loss: 46.2990
[11/08 02:47:13 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1559, average loss: 86.4099
[11/08 02:47:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.17	
[11/08 02:47:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/08 02:53:27 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.6818, average train loss: 27.7558
[11/08 02:54:10 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1564, average loss: 31.5779
[11/08 02:54:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.16	
[11/08 02:54:10 visual_prompt]: Stopping early.
[11/08 02:54:10 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 02:54:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 02:54:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 02:54:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 02:54:10 visual_prompt]: Training with config:
[11/08 02:54:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr25.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 02:54:10 visual_prompt]: Loading training data...
[11/08 02:54:10 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 02:54:10 visual_prompt]: Loading validation data...
[11/08 02:54:10 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 02:54:10 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 02:54:13 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 02:54:13 visual_prompt]: tuned percent:0.536
[11/08 02:54:13 visual_prompt]: Device used for model: 0
[11/08 02:54:13 visual_prompt]: Setting up Evaluator...
[11/08 02:54:13 visual_prompt]: Setting up Trainer...
[11/08 02:54:13 visual_prompt]: 	Setting up the optimizer...
[11/08 02:54:13 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 03:00:26 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6735, average train loss: 1.4017
[11/08 03:01:09 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1560, average loss: 1.2969
[11/08 03:01:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/08 03:01:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/08 03:07:22 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6573, average train loss: 17.7458
[11/08 03:08:05 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1569, average loss: 2.5264
[11/08 03:08:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.93	
[11/08 03:08:05 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/08 03:14:18 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.6653, average train loss: 6.0762
[11/08 03:15:00 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1582, average loss: 10.0471
[11/08 03:15:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.86	
[11/08 03:15:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/08 03:21:11 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.5705, average train loss: 13.3022
[11/08 03:21:53 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1558, average loss: 0.7905
[11/08 03:21:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.14	
[11/08 03:21:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/08 03:28:03 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5646, average train loss: 20.1636
[11/08 03:28:45 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1561, average loss: 11.0347
[11/08 03:28:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.68	
[11/08 03:28:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/08 03:34:56 visual_prompt]: Epoch 6 / 100: avg data time: 1.02e+01, avg batch time: 10.5994, average train loss: 28.9459
[11/08 03:35:38 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1562, average loss: 9.0186
[11/08 03:35:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.18	
[11/08 03:35:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/08 03:41:49 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.5875, average train loss: 33.6986
[11/08 03:42:31 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1560, average loss: 16.0607
[11/08 03:42:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.59	
[11/08 03:42:31 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/08 03:48:42 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.5842, average train loss: 52.7007
[11/08 03:49:24 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1560, average loss: 47.4837
[11/08 03:49:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.61	
[11/08 03:49:24 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/08 03:55:36 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6220, average train loss: 43.3817
[11/08 03:56:18 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1561, average loss: 47.4411
[11/08 03:56:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.65	
[11/08 03:56:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/08 04:02:28 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5676, average train loss: 60.4023
[11/08 04:03:10 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1596, average loss: 99.3796
[11/08 04:03:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.62	
[11/08 04:03:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/08 04:09:20 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5672, average train loss: 67.8739
[11/08 04:10:02 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1599, average loss: 28.6182
[11/08 04:10:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.49	
[11/08 04:10:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/08 04:16:12 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5606, average train loss: 57.0712
[11/08 04:16:54 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1585, average loss: 62.8103
[11/08 04:16:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.88	
[11/08 04:16:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/08 04:23:05 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.5864, average train loss: 64.5640
[11/08 04:23:47 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1561, average loss: 52.9235
[11/08 04:23:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.52	
[11/08 04:23:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/08 04:29:58 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5777, average train loss: 71.7291
[11/08 04:30:40 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1590, average loss: 26.0238
[11/08 04:30:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.23	
[11/08 04:30:40 visual_prompt]: Best epoch 14: best metric: -26.024
[11/08 04:30:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/08 04:36:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6073, average train loss: 62.6632
[11/08 04:37:33 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1563, average loss: 131.6016
[11/08 04:37:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.86	
[11/08 04:37:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/08 04:43:43 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.5689, average train loss: 63.4820
[11/08 04:44:26 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1562, average loss: 138.1876
[11/08 04:44:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.39	
[11/08 04:44:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/08 04:50:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.5568, average train loss: 82.9967
[11/08 04:51:17 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1559, average loss: 22.7312
[11/08 04:51:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.14	
[11/08 04:51:17 visual_prompt]: Best epoch 17: best metric: -22.731
[11/08 04:51:17 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/08 04:57:27 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.5639, average train loss: 59.9483
[11/08 04:58:09 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1560, average loss: 54.1222
[11/08 04:58:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.00	
[11/08 04:58:09 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/08 05:04:19 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.5583, average train loss: 50.7026
[11/08 05:05:01 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1560, average loss: 39.4407
[11/08 05:05:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.38	
[11/08 05:05:01 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/08 05:11:12 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.5851, average train loss: 43.7648
[11/08 05:11:54 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1591, average loss: 79.8305
[11/08 05:11:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.47	
[11/08 05:11:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/08 05:18:05 visual_prompt]: Epoch 21 / 100: avg data time: 1.03e+01, avg batch time: 10.6061, average train loss: 84.6545
[11/08 05:18:48 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1558, average loss: 201.8625
[11/08 05:18:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.33	
[11/08 05:18:48 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/08 05:25:01 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e+01, avg batch time: 10.6528, average train loss: 95.4676
[11/08 05:25:43 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1561, average loss: 59.5939
[11/08 05:25:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/08 05:25:43 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/08 05:31:57 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 10.6688, average train loss: 62.8857
[11/08 05:32:39 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1563, average loss: 308.6205
[11/08 05:32:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.81	
[11/08 05:32:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/08 05:38:53 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e+01, avg batch time: 10.6722, average train loss: 84.8989
[11/08 05:39:36 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1559, average loss: 29.9426
[11/08 05:39:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.02	
[11/08 05:39:36 visual_prompt]: Stopping early.
[11/08 05:39:36 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 05:39:36 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 05:39:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 05:39:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 05:39:36 visual_prompt]: Training with config:
[11/08 05:39:36 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr25.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 05:39:36 visual_prompt]: Loading training data...
[11/08 05:39:36 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 05:39:36 visual_prompt]: Loading validation data...
[11/08 05:39:36 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 05:39:36 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 05:39:38 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 05:39:38 visual_prompt]: tuned percent:0.536
[11/08 05:39:39 visual_prompt]: Device used for model: 0
[11/08 05:39:39 visual_prompt]: Setting up Evaluator...
[11/08 05:39:39 visual_prompt]: Setting up Trainer...
[11/08 05:39:39 visual_prompt]: 	Setting up the optimizer...
[11/08 05:39:39 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 05:45:54 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.7258, average train loss: 1.4017
[11/08 05:46:37 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1560, average loss: 1.2969
[11/08 05:46:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/08 05:46:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/08 05:52:51 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6798, average train loss: 24.8288
[11/08 05:53:33 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1560, average loss: 2.4321
[11/08 05:53:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.44	
[11/08 05:53:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/08 05:59:47 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.6783, average train loss: 7.2929
[11/08 06:00:30 visual_prompt]: Inference (val):avg data time: 1.81e-05, avg batch time: 0.1581, average loss: 6.2044
[11/08 06:00:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/08 06:00:30 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/08 06:06:44 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.6862, average train loss: 8.7177
[11/08 06:07:27 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1558, average loss: 17.2451
[11/08 06:07:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.74	
[11/08 06:07:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/08 06:13:40 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6663, average train loss: 11.6588
[11/08 06:14:23 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 14.9135
[11/08 06:14:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.94	
[11/08 06:14:23 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/08 06:20:38 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.7031, average train loss: 27.8789
[11/08 06:21:20 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1560, average loss: 7.9302
[11/08 06:21:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.43	
[11/08 06:21:20 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/08 06:27:34 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6853, average train loss: 35.2016
[11/08 06:28:17 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1560, average loss: 73.8037
[11/08 06:28:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.85	
[11/08 06:28:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/08 06:34:30 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.6550, average train loss: 33.3791
[11/08 06:35:13 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1560, average loss: 12.5573
[11/08 06:35:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.67	
[11/08 06:35:13 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/08 06:41:27 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6883, average train loss: 25.4118
[11/08 06:42:10 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1568, average loss: 41.1000
[11/08 06:42:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.89	
[11/08 06:42:10 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/08 06:48:23 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.6639, average train loss: 46.6766
[11/08 06:49:05 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1560, average loss: 23.1010
[11/08 06:49:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.65	
[11/08 06:49:05 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/08 06:55:15 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5668, average train loss: 47.1053
[11/08 06:55:57 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1587, average loss: 35.3989
[11/08 06:55:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.17	
[11/08 06:55:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/08 07:02:08 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5729, average train loss: 63.8425
[11/08 07:02:50 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1560, average loss: 126.7082
[11/08 07:02:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.32	
[11/08 07:02:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/08 07:09:00 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.5781, average train loss: 53.6279
[11/08 07:09:42 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1564, average loss: 56.6610
[11/08 07:09:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.42	
[11/08 07:09:42 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/08 07:15:52 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5711, average train loss: 56.2397
[11/08 07:16:35 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1592, average loss: 1.8293
[11/08 07:16:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.28	
[11/08 07:16:35 visual_prompt]: Best epoch 14: best metric: -1.829
[11/08 07:16:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/08 07:22:45 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.5878, average train loss: 61.9125
[11/08 07:23:28 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1598, average loss: 41.9314
[11/08 07:23:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.67	
[11/08 07:23:28 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/08 07:29:38 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.5724, average train loss: 58.6238
[11/08 07:30:20 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1558, average loss: 20.3802
[11/08 07:30:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.60	
[11/08 07:30:20 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/08 07:36:30 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.5647, average train loss: 53.6322
[11/08 07:37:12 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1564, average loss: 58.1741
[11/08 07:37:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.32	
[11/08 07:37:12 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/08 07:43:22 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.5697, average train loss: 54.9381
[11/08 07:44:04 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1568, average loss: 29.1895
[11/08 07:44:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.14	
[11/08 07:44:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/08 07:50:14 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.5673, average train loss: 40.6116
[11/08 07:50:57 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1600, average loss: 31.7277
[11/08 07:50:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.72	
[11/08 07:50:57 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/08 07:57:07 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.5757, average train loss: 41.6645
[11/08 07:57:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1558, average loss: 13.8676
[11/08 07:57:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.51	
[11/08 07:57:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/08 08:03:59 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.5725, average train loss: 64.4864
[11/08 08:04:42 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1561, average loss: 28.7377
[11/08 08:04:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.94	
[11/08 08:04:42 visual_prompt]: Stopping early.
[11/08 08:04:42 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 08:04:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 08:04:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 08:04:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 08:04:42 visual_prompt]: Training with config:
[11/08 08:04:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr25.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 08:04:42 visual_prompt]: Loading training data...
[11/08 08:04:42 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 08:04:42 visual_prompt]: Loading validation data...
[11/08 08:04:42 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 08:04:42 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 08:04:44 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 08:04:44 visual_prompt]: tuned percent:0.536
[11/08 08:04:44 visual_prompt]: Device used for model: 0
[11/08 08:04:44 visual_prompt]: Setting up Evaluator...
[11/08 08:04:44 visual_prompt]: Setting up Trainer...
[11/08 08:04:44 visual_prompt]: 	Setting up the optimizer...
[11/08 08:04:44 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 08:10:55 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.5748, average train loss: 1.4017
[11/08 08:11:37 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1560, average loss: 1.2969
[11/08 08:11:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/08 08:11:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/08 08:17:47 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5762, average train loss: 24.5553
[11/08 08:18:29 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1558, average loss: 8.8651
[11/08 08:18:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.91	
[11/08 08:18:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/08 08:24:41 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.6185, average train loss: 8.0350
[11/08 08:25:23 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1555, average loss: 0.7456
[11/08 08:25:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.71	
[11/08 08:25:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/08 08:31:38 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.6913, average train loss: 9.3909
[11/08 08:32:20 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1558, average loss: 0.7597
[11/08 08:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.77	
[11/08 08:32:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/08 08:38:34 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6652, average train loss: 27.1847
[11/08 08:39:16 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1560, average loss: 3.3412
[11/08 08:39:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.56	
[11/08 08:39:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/08 08:45:30 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6862, average train loss: 20.2154
[11/08 08:46:13 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1592, average loss: 1.4100
[11/08 08:46:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.93	
[11/08 08:46:13 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/08 08:52:27 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6803, average train loss: 8.7733
[11/08 08:53:09 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1561, average loss: 3.1323
[11/08 08:53:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.06	
[11/08 08:53:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/08 08:59:22 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.6563, average train loss: 20.1753
[11/08 09:00:05 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1556, average loss: 7.5689
[11/08 09:00:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.59	
[11/08 09:00:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/08 09:06:19 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6974, average train loss: 48.7753
[11/08 09:07:02 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1596, average loss: 16.9321
[11/08 09:07:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.95	
[11/08 09:07:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/08 09:13:15 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.6589, average train loss: 37.2875
[11/08 09:13:58 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1560, average loss: 78.6905
[11/08 09:13:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.75	
[11/08 09:13:58 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/08 09:20:12 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.6846, average train loss: 41.0837
[11/08 09:20:55 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1563, average loss: 105.0052
[11/08 09:20:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.96	
[11/08 09:20:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/08 09:27:08 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.6710, average train loss: 42.5592
[11/08 09:27:51 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1557, average loss: 10.8496
[11/08 09:27:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.70	
[11/08 09:27:51 visual_prompt]: Best epoch 12: best metric: -10.850
[11/08 09:27:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/08 09:34:04 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.6762, average train loss: 37.2229
[11/08 09:34:47 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1559, average loss: 166.4183
[11/08 09:34:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.52	
[11/08 09:34:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/08 09:41:01 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.6731, average train loss: 102.9233
[11/08 09:41:43 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1565, average loss: 246.5512
[11/08 09:41:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.40	
[11/08 09:41:43 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/08 09:47:58 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.7066, average train loss: 65.3282
[11/08 09:48:40 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1568, average loss: 39.3691
[11/08 09:48:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.02	
[11/08 09:48:40 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/08 09:54:53 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6521, average train loss: 56.2067
[11/08 09:55:36 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1560, average loss: 12.3369
[11/08 09:55:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.25	
[11/08 09:55:36 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/08 10:01:50 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6773, average train loss: 67.2136
[11/08 10:02:32 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1587, average loss: 129.8758
[11/08 10:02:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.06	
[11/08 10:02:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/08 10:08:46 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6780, average train loss: 59.4263
[11/08 10:09:29 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1558, average loss: 64.5834
[11/08 10:09:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.64	
[11/08 10:09:29 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/08 10:15:43 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6804, average train loss: 65.2182
[11/08 10:16:25 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1560, average loss: 64.0788
[11/08 10:16:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.83	
[11/08 10:16:25 visual_prompt]: Stopping early.
[11/08 10:16:25 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 10:16:25 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 10:16:25 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 10:16:25 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 10:16:25 visual_prompt]: Training with config:
[11/08 10:16:25 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr25.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 10:16:25 visual_prompt]: Loading training data...
[11/08 10:16:25 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 10:16:25 visual_prompt]: Loading validation data...
[11/08 10:16:25 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 10:16:25 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 10:16:28 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 10:16:28 visual_prompt]: tuned percent:0.536
[11/08 10:16:28 visual_prompt]: Device used for model: 0
[11/08 10:16:28 visual_prompt]: Setting up Evaluator...
[11/08 10:16:28 visual_prompt]: Setting up Trainer...
[11/08 10:16:28 visual_prompt]: 	Setting up the optimizer...
[11/08 10:16:28 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 10:22:43 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6963, average train loss: 1.4017
[11/08 10:23:25 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1558, average loss: 1.2969
[11/08 10:23:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/08 10:23:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/08 10:29:38 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6567, average train loss: 25.1135
[11/08 10:30:21 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1561, average loss: 2.0770
[11/08 10:30:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.43	
[11/08 10:30:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/08 10:36:35 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.6842, average train loss: 5.8461
[11/08 10:37:17 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1581, average loss: 0.8126
[11/08 10:37:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.03	
[11/08 10:37:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/08 10:43:32 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.6904, average train loss: 8.5846
[11/08 10:44:14 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1556, average loss: 6.9338
[11/08 10:44:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.46	
[11/08 10:44:14 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/08 10:50:28 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6788, average train loss: 33.7006
[11/08 10:51:11 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1556, average loss: 17.0240
[11/08 10:51:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.01	
[11/08 10:51:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/08 10:57:25 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6952, average train loss: 20.2576
[11/08 10:58:08 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1558, average loss: 26.2030
[11/08 10:58:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.29	
[11/08 10:58:08 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/08 11:04:22 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6951, average train loss: 39.1594
[11/08 11:05:05 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1556, average loss: 39.5488
[11/08 11:05:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.50	
[11/08 11:05:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/08 11:11:18 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.6605, average train loss: 16.1101
[11/08 11:12:01 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1558, average loss: 7.4987
[11/08 11:12:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.72	
[11/08 11:12:01 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/08 11:18:16 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.7184, average train loss: 14.4558
[11/08 11:18:59 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1559, average loss: 22.4585
[11/08 11:18:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.93	
[11/08 11:18:59 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/08 11:25:13 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.6912, average train loss: 12.4430
[11/08 11:25:56 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1560, average loss: 26.0708
[11/08 11:25:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.19	
[11/08 11:25:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/08 11:32:10 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.6842, average train loss: 64.4758
[11/08 11:32:52 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1557, average loss: 82.2012
[11/08 11:32:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/08 11:32:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/08 11:39:06 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.6708, average train loss: 47.2654
[11/08 11:39:49 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1595, average loss: 8.8583
[11/08 11:39:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.29	
[11/08 11:39:49 visual_prompt]: Best epoch 12: best metric: -8.858
[11/08 11:39:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/08 11:46:03 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.6878, average train loss: 20.9574
[11/08 11:46:45 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1573, average loss: 9.9548
[11/08 11:46:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.13	
[11/08 11:46:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/08 11:52:58 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.6549, average train loss: 21.7312
[11/08 11:53:41 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1588, average loss: 31.0348
[11/08 11:53:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.95	
[11/08 11:53:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/08 12:00:05 visual_prompt]: Epoch 15 / 100: avg data time: 1.06e+01, avg batch time: 10.9802, average train loss: 20.2959
[11/08 12:00:48 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1579, average loss: 9.5821
[11/08 12:00:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.06	
[11/08 12:00:48 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/08 12:07:01 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6532, average train loss: 26.0469
[11/08 12:07:44 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1560, average loss: 2.4154
[11/08 12:07:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.84	
[11/08 12:07:44 visual_prompt]: Best epoch 16: best metric: -2.415
[11/08 12:07:44 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/08 12:13:57 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6536, average train loss: 14.7281
[11/08 12:14:40 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1561, average loss: 16.4192
[11/08 12:14:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.75	
[11/08 12:14:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/08 12:20:54 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6849, average train loss: 28.4965
[11/08 12:21:36 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1560, average loss: 3.2744
[11/08 12:21:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.05	
[11/08 12:21:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/08 12:27:50 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6646, average train loss: 11.5865
[11/08 12:28:32 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1561, average loss: 19.1156
[11/08 12:28:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.21	
[11/08 12:28:32 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/08 12:34:43 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.5958, average train loss: 19.0805
[11/08 12:35:25 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1593, average loss: 62.0658
[11/08 12:35:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.25	
[11/08 12:35:25 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/08 12:41:36 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.5847, average train loss: 55.4854
[11/08 12:42:18 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1560, average loss: 36.8634
[11/08 12:42:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.60	
[11/08 12:42:18 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/08 12:49:00 visual_prompt]: Epoch 22 / 100: avg data time: 1.11e+01, avg batch time: 11.4655, average train loss: 13.7307
[11/08 12:49:44 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1570, average loss: 22.3492
[11/08 12:49:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.46	
[11/08 12:49:44 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/08 12:55:56 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 10.6170, average train loss: 18.4936
[11/08 12:56:40 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1577, average loss: 4.2544
[11/08 12:56:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.81	
[11/08 12:56:40 visual_prompt]: Stopping early.
[11/08 12:56:40 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 12:56:40 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 12:56:40 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 12:56:40 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 12:56:40 visual_prompt]: Training with config:
[11/08 12:56:40 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr10.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 12:56:40 visual_prompt]: Loading training data...
[11/08 12:56:40 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 12:56:40 visual_prompt]: Loading validation data...
[11/08 12:56:40 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 12:56:40 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 12:56:43 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 12:56:43 visual_prompt]: tuned percent:0.536
[11/08 12:56:43 visual_prompt]: Device used for model: 0
[11/08 12:56:43 visual_prompt]: Setting up Evaluator...
[11/08 12:56:43 visual_prompt]: Setting up Trainer...
[11/08 12:56:43 visual_prompt]: 	Setting up the optimizer...
[11/08 12:56:43 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 13:02:55 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6259, average train loss: 1.4017
[11/08 13:03:37 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1577, average loss: 1.2969
[11/08 13:03:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/08 13:03:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/08 13:09:49 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6090, average train loss: 8.9951
[11/08 13:10:31 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1558, average loss: 2.1304
[11/08 13:10:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.63	
[11/08 13:10:31 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/08 13:16:43 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.6251, average train loss: 2.8160
[11/08 13:17:25 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1561, average loss: 2.6874
[11/08 13:17:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.06	
[11/08 13:17:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/08 13:23:37 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.6054, average train loss: 4.5749
[11/08 13:24:19 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1567, average loss: 8.3327
[11/08 13:24:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.33	
[11/08 13:24:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/08 13:30:38 visual_prompt]: Epoch 5 / 100: avg data time: 1.05e+01, avg batch time: 10.8116, average train loss: 8.0197
[11/08 13:31:21 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1566, average loss: 1.9192
[11/08 13:31:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.87	
[11/08 13:31:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/08 13:38:09 visual_prompt]: Epoch 6 / 100: avg data time: 1.13e+01, avg batch time: 11.6786, average train loss: 10.5793
[11/08 13:38:52 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1576, average loss: 7.4902
[11/08 13:38:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.26	
[11/08 13:38:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/08 13:45:17 visual_prompt]: Epoch 7 / 100: avg data time: 1.06e+01, avg batch time: 10.9705, average train loss: 15.7977
[11/08 13:45:59 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1573, average loss: 5.8549
[11/08 13:45:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.70	
[11/08 13:45:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/08 13:52:14 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.7138, average train loss: 10.1090
[11/08 13:52:57 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1587, average loss: 0.8975
[11/08 13:52:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.06	
[11/08 13:52:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/08 13:59:14 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.7496, average train loss: 15.7293
[11/08 13:59:56 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1558, average loss: 13.2975
[11/08 13:59:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.44	
[11/08 13:59:56 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/08 14:06:14 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.7924, average train loss: 21.5171
[11/08 14:06:57 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1560, average loss: 1.9822
[11/08 14:06:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.57	
[11/08 14:06:57 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/08 14:13:12 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.7020, average train loss: 19.0509
[11/08 14:13:54 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1557, average loss: 4.3224
[11/08 14:13:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.27	
[11/08 14:13:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/08 14:20:07 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.6503, average train loss: 28.0083
[11/08 14:20:50 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1565, average loss: 22.5628
[11/08 14:20:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.17	
[11/08 14:20:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/08 14:27:05 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.7036, average train loss: 24.5884
[11/08 14:27:47 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1594, average loss: 18.4375
[11/08 14:27:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.49	
[11/08 14:27:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/08 14:33:57 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5738, average train loss: 14.8870
[11/08 14:34:40 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1561, average loss: 5.2757
[11/08 14:34:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.51	
[11/08 14:34:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/08 14:40:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.6029, average train loss: 21.5277
[11/08 14:41:33 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1557, average loss: 21.8561
[11/08 14:41:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.66	
[11/08 14:41:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/08 14:47:53 visual_prompt]: Epoch 16 / 100: avg data time: 1.05e+01, avg batch time: 10.8385, average train loss: 21.7384
[11/08 14:48:35 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1591, average loss: 24.6512
[11/08 14:48:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.45	
[11/08 14:48:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/08 14:54:46 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.5889, average train loss: 23.5457
[11/08 14:55:28 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1561, average loss: 52.6756
[11/08 14:55:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.22	
[11/08 14:55:28 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/08 15:01:39 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.6006, average train loss: 29.2653
[11/08 15:02:21 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1599, average loss: 34.3844
[11/08 15:02:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.41	
[11/08 15:02:22 visual_prompt]: Stopping early.
[11/08 15:02:22 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 15:02:22 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 15:02:22 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 15:02:22 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 15:02:22 visual_prompt]: Training with config:
[11/08 15:02:22 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr10.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 15:02:22 visual_prompt]: Loading training data...
[11/08 15:02:22 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 15:02:22 visual_prompt]: Loading validation data...
[11/08 15:02:22 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 15:02:22 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 15:02:24 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 15:02:24 visual_prompt]: tuned percent:0.536
[11/08 15:02:24 visual_prompt]: Device used for model: 0
[11/08 15:02:24 visual_prompt]: Setting up Evaluator...
[11/08 15:02:24 visual_prompt]: Setting up Trainer...
[11/08 15:02:24 visual_prompt]: 	Setting up the optimizer...
[11/08 15:02:24 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 15:08:35 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.5961, average train loss: 1.4017
[11/08 15:09:17 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1558, average loss: 1.2969
[11/08 15:09:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/08 15:09:17 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/08 15:15:28 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5767, average train loss: 10.4317
[11/08 15:16:10 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1553, average loss: 2.4653
[11/08 15:16:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[11/08 15:16:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/08 15:22:20 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.5699, average train loss: 2.4359
[11/08 15:23:02 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1592, average loss: 2.0612
[11/08 15:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.71	
[11/08 15:23:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/08 15:29:13 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.5878, average train loss: 3.2391
[11/08 15:29:55 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1571, average loss: 4.3621
[11/08 15:29:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.53	
[11/08 15:29:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/08 15:36:05 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5651, average train loss: 5.8642
[11/08 15:36:47 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1594, average loss: 8.3780
[11/08 15:36:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.63	
[11/08 15:36:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/08 15:42:58 visual_prompt]: Epoch 6 / 100: avg data time: 1.02e+01, avg batch time: 10.5906, average train loss: 13.1138
[11/08 15:43:40 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1561, average loss: 16.5069
[11/08 15:43:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.03	
[11/08 15:43:40 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/08 15:49:51 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6074, average train loss: 12.8504
[11/08 15:50:34 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1562, average loss: 11.6823
[11/08 15:50:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.76	
[11/08 15:50:34 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/08 15:56:44 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.5714, average train loss: 12.0912
[11/08 15:57:26 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1593, average loss: 16.9862
[11/08 15:57:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.79	
[11/08 15:57:26 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/08 16:03:38 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6159, average train loss: 8.4848
[11/08 16:04:20 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1559, average loss: 5.3961
[11/08 16:04:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.19	
[11/08 16:04:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/08 16:10:31 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5885, average train loss: 11.0905
[11/08 16:11:13 visual_prompt]: Inference (val):avg data time: 1.86e-05, avg batch time: 0.1599, average loss: 17.7035
[11/08 16:11:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.49	
[11/08 16:11:13 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/08 16:17:25 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.6115, average train loss: 14.7811
[11/08 16:18:07 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1559, average loss: 6.9861
[11/08 16:18:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.78	
[11/08 16:18:07 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/08 16:24:17 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5685, average train loss: 21.3058
[11/08 16:24:59 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1591, average loss: 36.9276
[11/08 16:24:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.34	
[11/08 16:24:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/08 16:31:11 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.6075, average train loss: 15.8511
[11/08 16:31:53 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1562, average loss: 16.0947
[11/08 16:31:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.72	
[11/08 16:31:53 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/08 16:38:05 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.6135, average train loss: 15.7302
[11/08 16:38:47 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1557, average loss: 7.0918
[11/08 16:38:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.03	
[11/08 16:38:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/08 16:44:59 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6204, average train loss: 21.8307
[11/08 16:45:41 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1563, average loss: 8.7284
[11/08 16:45:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 38.50	
[11/08 16:45:41 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/08 16:51:54 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6596, average train loss: 11.5582
[11/08 16:52:37 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1559, average loss: 12.9695
[11/08 16:52:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.56	
[11/08 16:52:37 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/08 16:58:51 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6751, average train loss: 13.2553
[11/08 16:59:33 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1591, average loss: 22.1504
[11/08 16:59:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.76	
[11/08 16:59:33 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/08 17:05:44 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.5860, average train loss: 20.7829
[11/08 17:06:26 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1560, average loss: 30.7949
[11/08 17:06:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.03	
[11/08 17:06:26 visual_prompt]: Stopping early.
[11/08 17:06:26 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 17:06:26 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 17:06:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 17:06:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 17:06:26 visual_prompt]: Training with config:
[11/08 17:06:26 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr10.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 17:06:26 visual_prompt]: Loading training data...
[11/08 17:06:26 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 17:06:26 visual_prompt]: Loading validation data...
[11/08 17:06:26 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 17:06:26 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 17:06:29 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 17:06:29 visual_prompt]: tuned percent:0.536
[11/08 17:06:29 visual_prompt]: Device used for model: 0
[11/08 17:06:29 visual_prompt]: Setting up Evaluator...
[11/08 17:06:29 visual_prompt]: Setting up Trainer...
[11/08 17:06:29 visual_prompt]: 	Setting up the optimizer...
[11/08 17:06:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 17:12:40 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.5998, average train loss: 1.4017
[11/08 17:13:22 visual_prompt]: Inference (val):avg data time: 1.82e-05, avg batch time: 0.1563, average loss: 1.2969
[11/08 17:13:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/08 17:13:22 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/08 17:19:37 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6979, average train loss: 10.4425
[11/08 17:20:20 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1560, average loss: 0.6938
[11/08 17:20:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.53	
[11/08 17:20:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/08 17:26:34 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.6892, average train loss: 1.1895
[11/08 17:27:17 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1561, average loss: 1.3877
[11/08 17:27:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.12	
[11/08 17:27:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/08 17:33:31 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.7081, average train loss: 3.4619
[11/08 17:34:14 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1596, average loss: 12.7618
[11/08 17:34:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.38	
[11/08 17:34:14 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/08 17:40:28 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6814, average train loss: 9.8489
[11/08 17:41:11 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1557, average loss: 0.8044
[11/08 17:41:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.56	
[11/08 17:41:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/08 17:47:26 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.7139, average train loss: 3.7464
[11/08 17:48:09 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1589, average loss: 7.9896
[11/08 17:48:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.63	
[11/08 17:48:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/08 17:54:24 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.7146, average train loss: 4.7458
[11/08 17:55:06 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1598, average loss: 7.3967
[11/08 17:55:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.93	
[11/08 17:55:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/08 18:01:21 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.6923, average train loss: 4.4850
[11/08 18:02:03 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1591, average loss: 4.1082
[11/08 18:02:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.95	
[11/08 18:02:03 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/08 18:08:19 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.7189, average train loss: 7.8319
[11/08 18:09:01 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1584, average loss: 3.7704
[11/08 18:09:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.18	
[11/08 18:09:01 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/08 18:15:14 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.6499, average train loss: 3.6409
[11/08 18:15:56 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1600, average loss: 5.6437
[11/08 18:15:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.87	
[11/08 18:15:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/08 18:22:07 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5843, average train loss: 11.5482
[11/08 18:22:49 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1560, average loss: 1.0355
[11/08 18:22:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.05	
[11/08 18:22:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/08 18:29:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5814, average train loss: 12.6833
[11/08 18:29:42 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1572, average loss: 6.1286
[11/08 18:29:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.46	
[11/08 18:29:42 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/08 18:35:52 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.5853, average train loss: 5.0546
[11/08 18:36:35 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1559, average loss: 0.6965
[11/08 18:36:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.41	
[11/08 18:36:35 visual_prompt]: Best epoch 13: best metric: -0.697
[11/08 18:36:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/08 18:42:45 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5689, average train loss: 9.0732
[11/08 18:43:27 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1581, average loss: 3.7761
[11/08 18:43:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.37	
[11/08 18:43:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/08 18:49:39 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6111, average train loss: 8.7014
[11/08 18:50:21 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1559, average loss: 8.1259
[11/08 18:50:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.58	
[11/08 18:50:21 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/08 18:56:31 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.5696, average train loss: 5.2885
[11/08 18:57:13 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1560, average loss: 3.1583
[11/08 18:57:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.88	
[11/08 18:57:13 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/08 19:03:23 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.5655, average train loss: 11.5686
[11/08 19:04:05 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1573, average loss: 12.0836
[11/08 19:04:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.03	
[11/08 19:04:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/08 19:10:16 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.5800, average train loss: 7.0676
[11/08 19:10:58 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1562, average loss: 12.7832
[11/08 19:10:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.02	
[11/08 19:10:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/08 19:17:08 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.5785, average train loss: 6.4098
[11/08 19:17:51 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1561, average loss: 3.2318
[11/08 19:17:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.06	
[11/08 19:17:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/08 19:24:02 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.6033, average train loss: 5.8050
[11/08 19:24:45 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1613, average loss: 21.2096
[11/08 19:24:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.08	
[11/08 19:24:45 visual_prompt]: Stopping early.
[11/08 19:24:45 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 19:24:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 19:24:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 19:24:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 19:24:45 visual_prompt]: Training with config:
[11/08 19:24:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr10.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 19:24:45 visual_prompt]: Loading training data...
[11/08 19:24:45 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 19:24:45 visual_prompt]: Loading validation data...
[11/08 19:24:45 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 19:24:45 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 19:24:47 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 19:24:47 visual_prompt]: tuned percent:0.536
[11/08 19:24:47 visual_prompt]: Device used for model: 0
[11/08 19:24:47 visual_prompt]: Setting up Evaluator...
[11/08 19:24:47 visual_prompt]: Setting up Trainer...
[11/08 19:24:47 visual_prompt]: 	Setting up the optimizer...
[11/08 19:24:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 19:31:02 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6908, average train loss: 1.4017
[11/08 19:31:44 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1561, average loss: 1.2969
[11/08 19:31:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/08 19:31:44 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/08 19:37:58 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6762, average train loss: 10.4418
[11/08 19:38:40 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1559, average loss: 0.6945
[11/08 19:38:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.45	
[11/08 19:38:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/08 19:44:55 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.7083, average train loss: 1.1239
[11/08 19:45:38 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1559, average loss: 0.7940
[11/08 19:45:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.82	
[11/08 19:45:38 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/08 19:51:52 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.6932, average train loss: 3.3517
[11/08 19:52:35 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1592, average loss: 14.6353
[11/08 19:52:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.42	
[11/08 19:52:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/08 19:58:50 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.7046, average train loss: 7.4177
[11/08 19:59:32 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1587, average loss: 5.1087
[11/08 19:59:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.35	
[11/08 19:59:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/08 20:05:47 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.7026, average train loss: 7.1617
[11/08 20:06:30 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1561, average loss: 8.6704
[11/08 20:06:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.53	
[11/08 20:06:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/08 20:12:45 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.7155, average train loss: 9.3843
[11/08 20:13:28 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1559, average loss: 1.6088
[11/08 20:13:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.15	
[11/08 20:13:28 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/08 20:19:42 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.6704, average train loss: 3.9312
[11/08 20:20:24 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1563, average loss: 10.4945
[11/08 20:20:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.85	
[11/08 20:20:24 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/08 20:26:35 visual_prompt]: Epoch 9 / 100: avg data time: 1.02e+01, avg batch time: 10.6063, average train loss: 4.8908
[11/08 20:27:17 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1571, average loss: 2.2516
[11/08 20:27:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.33	
[11/08 20:27:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/08 20:33:28 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5827, average train loss: 9.2791
[11/08 20:34:10 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1558, average loss: 11.8856
[11/08 20:34:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.17	
[11/08 20:34:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/08 20:40:21 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5865, average train loss: 8.3222
[11/08 20:41:03 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1560, average loss: 3.1799
[11/08 20:41:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.81	
[11/08 20:41:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/08 20:47:14 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5893, average train loss: 18.3171
[11/08 20:47:56 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1563, average loss: 1.1984
[11/08 20:47:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.48	
[11/08 20:47:56 visual_prompt]: Best epoch 12: best metric: -1.198
[11/08 20:47:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/08 20:54:07 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.6021, average train loss: 13.1095
[11/08 20:54:50 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1559, average loss: 24.5212
[11/08 20:54:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.34	
[11/08 20:54:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/08 21:01:02 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.6317, average train loss: 13.2785
[11/08 21:01:45 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1555, average loss: 8.4924
[11/08 21:01:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.66	
[11/08 21:01:45 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/08 21:08:00 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.7254, average train loss: 12.6995
[11/08 21:08:43 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1560, average loss: 21.4061
[11/08 21:08:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/08 21:08:43 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/08 21:14:58 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6996, average train loss: 6.7786
[11/08 21:15:41 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1560, average loss: 3.9845
[11/08 21:15:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.05	
[11/08 21:15:41 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/08 21:22:00 visual_prompt]: Epoch 17 / 100: avg data time: 1.05e+01, avg batch time: 10.8397, average train loss: 4.6889
[11/08 21:22:43 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1556, average loss: 8.7248
[11/08 21:22:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.55	
[11/08 21:22:43 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/08 21:28:57 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6933, average train loss: 4.7508
[11/08 21:29:40 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1556, average loss: 8.7396
[11/08 21:29:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[11/08 21:29:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/08 21:35:54 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6852, average train loss: 4.9378
[11/08 21:36:36 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1560, average loss: 10.4313
[11/08 21:36:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.37	
[11/08 21:36:36 visual_prompt]: Stopping early.
[11/08 21:36:36 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 21:36:36 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 21:36:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 21:36:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 21:36:36 visual_prompt]: Training with config:
[11/08 21:36:36 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr5.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 21:36:36 visual_prompt]: Loading training data...
[11/08 21:36:36 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 21:36:37 visual_prompt]: Loading validation data...
[11/08 21:36:37 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 21:36:37 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 21:36:39 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 21:36:39 visual_prompt]: tuned percent:0.536
[11/08 21:36:39 visual_prompt]: Device used for model: 0
[11/08 21:36:39 visual_prompt]: Setting up Evaluator...
[11/08 21:36:39 visual_prompt]: Setting up Trainer...
[11/08 21:36:39 visual_prompt]: 	Setting up the optimizer...
[11/08 21:36:39 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 21:42:51 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6209, average train loss: 1.4017
[11/08 21:43:33 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1565, average loss: 1.2969
[11/08 21:43:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/08 21:43:34 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/08 21:49:44 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5952, average train loss: 5.6248
[11/08 21:50:27 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1562, average loss: 0.8103
[11/08 21:50:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.58	
[11/08 21:50:27 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/08 21:56:37 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.5925, average train loss: 1.2066
[11/08 21:57:20 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1559, average loss: 0.8393
[11/08 21:57:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.35	
[11/08 21:57:20 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/08 22:03:31 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.6053, average train loss: 1.0557
[11/08 22:04:13 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1570, average loss: 1.1743
[11/08 22:04:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.49	
[11/08 22:04:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/08 22:10:24 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5794, average train loss: 2.6304
[11/08 22:11:06 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1577, average loss: 1.6485
[11/08 22:11:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.36	
[11/08 22:11:06 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/08 22:17:17 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6133, average train loss: 2.6649
[11/08 22:18:00 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1559, average loss: 4.4023
[11/08 22:18:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.11	
[11/08 22:18:00 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/08 22:24:11 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.6015, average train loss: 4.6162
[11/08 22:24:53 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1559, average loss: 1.1700
[11/08 22:24:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.43	
[11/08 22:24:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/08 22:31:04 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.5984, average train loss: 5.3331
[11/08 22:31:47 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1561, average loss: 1.4131
[11/08 22:31:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.27	
[11/08 22:31:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/08 22:37:59 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6366, average train loss: 8.9154
[11/08 22:38:41 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1561, average loss: 0.9131
[11/08 22:38:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.11	
[11/08 22:38:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/08 22:44:53 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5993, average train loss: 9.3795
[11/08 22:45:35 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1573, average loss: 13.2028
[11/08 22:45:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.27	
[11/08 22:45:35 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/08 22:51:46 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5962, average train loss: 7.1777
[11/08 22:52:28 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1558, average loss: 3.5527
[11/08 22:52:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.16	
[11/08 22:52:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/08 22:58:39 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5932, average train loss: 10.5042
[11/08 22:59:21 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1576, average loss: 14.4873
[11/08 22:59:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.39	
[11/08 22:59:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/08 23:05:32 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.6005, average train loss: 10.3349
[11/08 23:06:15 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1559, average loss: 1.7624
[11/08 23:06:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.23	
[11/08 23:06:15 visual_prompt]: Best epoch 13: best metric: -1.762
[11/08 23:06:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/08 23:12:25 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5792, average train loss: 11.2342
[11/08 23:13:07 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1595, average loss: 21.3650
[11/08 23:13:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.39	
[11/08 23:13:07 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/08 23:19:20 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6484, average train loss: 9.6737
[11/08 23:20:02 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1579, average loss: 12.0789
[11/08 23:20:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.67	
[11/08 23:20:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/08 23:26:14 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6233, average train loss: 13.6611
[11/08 23:26:57 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1598, average loss: 16.7691
[11/08 23:26:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.37	
[11/08 23:26:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/08 23:33:11 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6950, average train loss: 8.3092
[11/08 23:33:54 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1580, average loss: 11.7267
[11/08 23:33:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.58	
[11/08 23:33:54 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/08 23:40:08 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6833, average train loss: 9.4240
[11/08 23:40:51 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1583, average loss: 7.8038
[11/08 23:40:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.61	
[11/08 23:40:51 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/08 23:47:05 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6832, average train loss: 14.3100
[11/08 23:47:48 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1566, average loss: 14.5410
[11/08 23:47:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.89	
[11/08 23:47:48 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/08 23:54:03 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.7097, average train loss: 8.6450
[11/08 23:54:46 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1560, average loss: 9.2779
[11/08 23:54:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.16	
[11/08 23:54:46 visual_prompt]: Stopping early.
[11/08 23:54:46 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 23:54:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 23:54:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/08 23:54:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/08 23:54:46 visual_prompt]: Training with config:
[11/08 23:54:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr5.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/08 23:54:46 visual_prompt]: Loading training data...
[11/08 23:54:46 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 23:54:46 visual_prompt]: Loading validation data...
[11/08 23:54:46 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 23:54:46 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 23:54:48 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/08 23:54:48 visual_prompt]: tuned percent:0.536
[11/08 23:54:49 visual_prompt]: Device used for model: 0
[11/08 23:54:49 visual_prompt]: Setting up Evaluator...
[11/08 23:54:49 visual_prompt]: Setting up Trainer...
[11/08 23:54:49 visual_prompt]: 	Setting up the optimizer...
[11/08 23:54:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/09 00:01:03 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6980, average train loss: 1.4017
[11/09 00:01:46 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1562, average loss: 1.2969
[11/09 00:01:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/09 00:01:46 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/09 00:08:00 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6864, average train loss: 5.7731
[11/09 00:08:43 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1562, average loss: 1.2430
[11/09 00:08:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.04	
[11/09 00:08:43 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/09 00:14:58 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.7066, average train loss: 0.8244
[11/09 00:15:41 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1559, average loss: 0.7191
[11/09 00:15:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.04	
[11/09 00:15:41 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/09 00:21:55 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.7054, average train loss: 1.5215
[11/09 00:22:38 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1558, average loss: 1.0503
[11/09 00:22:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.47	
[11/09 00:22:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/09 00:28:49 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5868, average train loss: 2.0434
[11/09 00:29:31 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1583, average loss: 0.7513
[11/09 00:29:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.70	
[11/09 00:29:31 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/09 00:35:42 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6062, average train loss: 1.1479
[11/09 00:36:24 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1558, average loss: 0.7207
[11/09 00:36:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.36	
[11/09 00:36:24 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/09 00:42:36 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.6022, average train loss: 1.1843
[11/09 00:43:18 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1561, average loss: 6.4927
[11/09 00:43:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.44	
[11/09 00:43:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/09 00:49:29 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.5917, average train loss: 6.4970
[11/09 00:50:11 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1561, average loss: 10.6573
[11/09 00:50:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.70	
[11/09 00:50:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/09 00:56:22 visual_prompt]: Epoch 9 / 100: avg data time: 1.02e+01, avg batch time: 10.6041, average train loss: 6.4295
[11/09 00:57:05 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1562, average loss: 2.9461
[11/09 00:57:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/09 00:57:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/09 01:03:15 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5728, average train loss: 6.7578
[11/09 01:03:57 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1558, average loss: 23.0779
[11/09 01:03:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.16	
[11/09 01:03:57 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/09 01:10:07 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5802, average train loss: 13.0989
[11/09 01:10:50 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1587, average loss: 3.6098
[11/09 01:10:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.79	
[11/09 01:10:50 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/09 01:17:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5778, average train loss: 9.3778
[11/09 01:17:42 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1560, average loss: 6.1048
[11/09 01:17:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.64	
[11/09 01:17:42 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/09 01:23:53 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.5841, average train loss: 11.8091
[11/09 01:24:35 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1556, average loss: 3.0228
[11/09 01:24:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.86	
[11/09 01:24:35 visual_prompt]: Best epoch 13: best metric: -3.023
[11/09 01:24:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/09 01:30:45 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5810, average train loss: 5.2985
[11/09 01:31:28 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1558, average loss: 0.8025
[11/09 01:31:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.36	
[11/09 01:31:28 visual_prompt]: Best epoch 14: best metric: -0.802
[11/09 01:31:28 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/09 01:37:39 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6062, average train loss: 5.5812
[11/09 01:38:21 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1557, average loss: 2.5390
[11/09 01:38:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.81	
[11/09 01:38:21 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/09 01:44:32 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.5732, average train loss: 5.5829
[11/09 01:45:14 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1562, average loss: 14.6342
[11/09 01:45:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.24	
[11/09 01:45:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/09 01:51:24 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.5642, average train loss: 6.0420
[11/09 01:52:06 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1588, average loss: 2.2742
[11/09 01:52:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.22	
[11/09 01:52:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/09 01:58:16 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.5753, average train loss: 14.2533
[11/09 01:58:59 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1555, average loss: 8.9785
[11/09 01:58:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.56	
[11/09 01:58:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/09 02:05:11 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6370, average train loss: 10.0716
[11/09 02:05:53 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1580, average loss: 1.7788
[11/09 02:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.62	
[11/09 02:05:53 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/09 02:12:04 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.5842, average train loss: 13.9437
[11/09 02:12:46 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1559, average loss: 16.7936
[11/09 02:12:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.20	
[11/09 02:12:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/09 02:18:57 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.5866, average train loss: 6.9809
[11/09 02:19:39 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1556, average loss: 7.3219
[11/09 02:19:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.45	
[11/09 02:19:39 visual_prompt]: Stopping early.
[11/09 02:19:39 visual_prompt]: Rank of current process: 0. World size: 1
[11/09 02:19:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/09 02:19:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/09 02:19:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/09 02:19:39 visual_prompt]: Training with config:
[11/09 02:19:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr5.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/09 02:19:39 visual_prompt]: Loading training data...
[11/09 02:19:39 visual_prompt]: Constructing mammo-cbis dataset train...
[11/09 02:19:39 visual_prompt]: Loading validation data...
[11/09 02:19:39 visual_prompt]: Constructing mammo-cbis dataset val...
[11/09 02:19:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/09 02:19:42 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/09 02:19:42 visual_prompt]: tuned percent:0.536
[11/09 02:19:42 visual_prompt]: Device used for model: 0
[11/09 02:19:42 visual_prompt]: Setting up Evaluator...
[11/09 02:19:42 visual_prompt]: Setting up Trainer...
[11/09 02:19:42 visual_prompt]: 	Setting up the optimizer...
[11/09 02:19:42 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/09 02:25:53 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.5966, average train loss: 1.4017
[11/09 02:26:35 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1559, average loss: 1.2969
[11/09 02:26:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/09 02:26:35 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/09 02:32:46 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5819, average train loss: 5.9467
[11/09 02:33:28 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1567, average loss: 1.1333
[11/09 02:33:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.80	
[11/09 02:33:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/09 02:39:39 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.5980, average train loss: 0.9754
[11/09 02:40:21 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1560, average loss: 0.6982
[11/09 02:40:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 58.32	
[11/09 02:40:21 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/09 02:46:33 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.6097, average train loss: 2.6569
[11/09 02:47:15 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1590, average loss: 0.9267
[11/09 02:47:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.34	
[11/09 02:47:15 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/09 02:53:26 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6064, average train loss: 6.1755
[11/09 02:54:09 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1560, average loss: 5.0799
[11/09 02:54:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.84	
[11/09 02:54:09 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/09 03:00:21 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6286, average train loss: 4.4714
[11/09 03:01:04 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1558, average loss: 4.5129
[11/09 03:01:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.92	
[11/09 03:01:04 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/09 03:07:16 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6364, average train loss: 3.5116
[11/09 03:07:58 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1559, average loss: 3.4072
[11/09 03:07:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.30	
[11/09 03:07:58 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/09 03:14:10 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.6135, average train loss: 2.0414
[11/09 03:14:53 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1557, average loss: 0.8509
[11/09 03:14:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.58	
[11/09 03:14:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/09 03:21:06 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6566, average train loss: 1.5073
[11/09 03:21:48 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1561, average loss: 0.7142
[11/09 03:21:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.32	
[11/09 03:21:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/09 03:27:59 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.6039, average train loss: 1.4093
[11/09 03:28:42 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1567, average loss: 0.7343
[11/09 03:28:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.23	
[11/09 03:28:42 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/09 03:34:54 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.6195, average train loss: 0.8321
[11/09 03:35:36 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1558, average loss: 1.1717
[11/09 03:35:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.85	
[11/09 03:35:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/09 03:41:49 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.6454, average train loss: 1.9024
[11/09 03:42:31 visual_prompt]: Inference (val):avg data time: 1.86e-05, avg batch time: 0.1594, average loss: 3.1157
[11/09 03:42:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.43	
[11/09 03:42:31 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/09 03:48:47 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.7227, average train loss: 5.6455
[11/09 03:49:30 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1556, average loss: 5.9497
[11/09 03:49:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.85	
[11/09 03:49:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/09 03:55:44 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.6851, average train loss: 4.6794
[11/09 03:56:27 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1575, average loss: 2.4449
[11/09 03:56:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.18	
[11/09 03:56:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/09 04:02:42 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.7130, average train loss: 3.0954
[11/09 04:03:24 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1564, average loss: 6.1072
[11/09 04:03:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.36	
[11/09 04:03:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/09 04:09:39 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.7108, average train loss: 2.9854
[11/09 04:10:22 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1565, average loss: 1.7035
[11/09 04:10:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.93	
[11/09 04:10:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/09 04:16:37 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6948, average train loss: 6.3599
[11/09 04:17:19 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1560, average loss: 2.1592
[11/09 04:17:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.12	
[11/09 04:17:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/09 04:23:34 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.7110, average train loss: 4.1144
[11/09 04:24:17 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1557, average loss: 1.3735
[11/09 04:24:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.52	
[11/09 04:24:17 visual_prompt]: Stopping early.
[11/09 04:24:17 visual_prompt]: Rank of current process: 0. World size: 1
[11/09 04:24:17 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/09 04:24:17 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/09 04:24:17 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/09 04:24:17 visual_prompt]: Training with config:
[11/09 04:24:17 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr5.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/09 04:24:17 visual_prompt]: Loading training data...
[11/09 04:24:17 visual_prompt]: Constructing mammo-cbis dataset train...
[11/09 04:24:17 visual_prompt]: Loading validation data...
[11/09 04:24:17 visual_prompt]: Constructing mammo-cbis dataset val...
[11/09 04:24:17 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/09 04:24:20 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/09 04:24:20 visual_prompt]: tuned percent:0.536
[11/09 04:24:20 visual_prompt]: Device used for model: 0
[11/09 04:24:20 visual_prompt]: Setting up Evaluator...
[11/09 04:24:20 visual_prompt]: Setting up Trainer...
[11/09 04:24:20 visual_prompt]: 	Setting up the optimizer...
[11/09 04:24:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/09 04:30:34 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6964, average train loss: 1.4017
[11/09 04:31:17 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1560, average loss: 1.2969
[11/09 04:31:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/09 04:31:17 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/09 04:37:27 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5693, average train loss: 5.9509
[11/09 04:38:09 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1564, average loss: 1.1372
[11/09 04:38:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.85	
[11/09 04:38:09 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/09 04:44:20 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.5881, average train loss: 0.9748
[11/09 04:45:02 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1559, average loss: 0.6893
[11/09 04:45:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 58.63	
[11/09 04:45:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/09 04:51:17 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.7116, average train loss: 2.6736
[11/09 04:52:00 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1559, average loss: 0.9165
[11/09 04:52:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.29	
[11/09 04:52:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/09 04:58:12 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6209, average train loss: 3.0461
[11/09 04:58:54 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1588, average loss: 1.7254
[11/09 04:58:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.43	
[11/09 04:58:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/09 05:05:06 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6209, average train loss: 3.2436
[11/09 05:05:48 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1559, average loss: 5.7840
[11/09 05:05:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.44	
[11/09 05:05:48 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/09 05:12:01 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6348, average train loss: 4.8327
[11/09 05:12:43 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1563, average loss: 7.7874
[11/09 05:12:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.46	
[11/09 05:12:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/09 05:18:54 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.6033, average train loss: 8.2628
[11/09 05:19:37 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1563, average loss: 5.8034
[11/09 05:19:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.36	
[11/09 05:19:37 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/09 05:25:49 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6208, average train loss: 3.3418
[11/09 05:26:31 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1562, average loss: 1.6935
[11/09 05:26:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.55	
[11/09 05:26:31 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/09 05:32:43 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.6078, average train loss: 2.9051
[11/09 05:33:25 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1573, average loss: 1.1303
[11/09 05:33:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/09 05:33:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/09 05:39:37 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.6202, average train loss: 1.4977
[11/09 05:40:19 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1554, average loss: 1.1440
[11/09 05:40:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.34	
[11/09 05:40:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/09 05:46:30 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5931, average train loss: 3.5161
[11/09 05:47:13 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1586, average loss: 1.1288
[11/09 05:47:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.57	
[11/09 05:47:13 visual_prompt]: Best epoch 12: best metric: -1.129
[11/09 05:47:13 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/09 05:53:23 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.5858, average train loss: 3.3973
[11/09 05:54:06 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1559, average loss: 0.7094
[11/09 05:54:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/09 05:54:06 visual_prompt]: Best epoch 13: best metric: -0.709
[11/09 05:54:06 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/09 06:00:16 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5690, average train loss: 2.4030
[11/09 06:00:58 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1560, average loss: 2.3921
[11/09 06:00:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.71	
[11/09 06:00:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/09 06:07:09 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.5944, average train loss: 1.4844
[11/09 06:07:51 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1560, average loss: 1.2943
[11/09 06:07:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.07	
[11/09 06:07:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/09 06:14:05 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6713, average train loss: 1.0320
[11/09 06:14:48 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1566, average loss: 0.7432
[11/09 06:14:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.61	
[11/09 06:14:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/09 06:21:01 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6745, average train loss: 2.2063
[11/09 06:21:44 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1559, average loss: 0.8792
[11/09 06:21:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.50	
[11/09 06:21:44 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/09 06:27:59 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6923, average train loss: 0.9722
[11/09 06:28:41 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1558, average loss: 0.9630
[11/09 06:28:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.64	
[11/09 06:28:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/09 06:34:55 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6846, average train loss: 1.2871
[11/09 06:35:38 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1597, average loss: 1.6609
[11/09 06:35:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.69	
[11/09 06:35:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/09 06:41:53 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.6962, average train loss: 2.2097
[11/09 06:42:35 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1562, average loss: 0.8099
[11/09 06:42:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.47	
[11/09 06:42:35 visual_prompt]: Stopping early.
[11/09 06:42:35 visual_prompt]: Rank of current process: 0. World size: 1
[11/09 06:42:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/09 06:42:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/09 06:42:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/09 06:42:35 visual_prompt]: Training with config:
[11/09 06:42:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr2.5_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/09 06:42:35 visual_prompt]: Loading training data...
[11/09 06:42:35 visual_prompt]: Constructing mammo-cbis dataset train...
[11/09 06:42:35 visual_prompt]: Loading validation data...
[11/09 06:42:35 visual_prompt]: Constructing mammo-cbis dataset val...
[11/09 06:42:35 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/09 06:42:38 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/09 06:42:38 visual_prompt]: tuned percent:0.536
[11/09 06:42:38 visual_prompt]: Device used for model: 0
[11/09 06:42:38 visual_prompt]: Setting up Evaluator...
[11/09 06:42:38 visual_prompt]: Setting up Trainer...
[11/09 06:42:38 visual_prompt]: 	Setting up the optimizer...
[11/09 06:42:38 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/09 06:48:53 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.7196, average train loss: 1.4017
[11/09 06:49:36 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1560, average loss: 1.2969
[11/09 06:49:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/09 06:49:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/09 06:55:50 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6781, average train loss: 3.6919
[11/09 06:56:32 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1561, average loss: 0.8725
[11/09 06:56:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.65	
[11/09 06:56:32 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/09 07:02:43 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.5923, average train loss: 0.7178
[11/09 07:03:25 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1561, average loss: 0.7616
[11/09 07:03:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.37	
[11/09 07:03:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/09 07:09:37 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.6102, average train loss: 0.7537
[11/09 07:10:19 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1559, average loss: 0.7970
[11/09 07:10:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.56	
[11/09 07:10:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/09 07:16:30 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5938, average train loss: 0.8644
[11/09 07:17:12 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1558, average loss: 0.7015
[11/09 07:17:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.84	
[11/09 07:17:12 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/09 07:23:24 visual_prompt]: Epoch 6 / 100: avg data time: 1.02e+01, avg batch time: 10.6028, average train loss: 0.9545
[11/09 07:24:06 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1559, average loss: 0.7445
[11/09 07:24:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.83	
[11/09 07:24:06 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/09 07:30:17 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6149, average train loss: 1.0405
[11/09 07:31:00 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1596, average loss: 4.0140
[11/09 07:31:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.13	
[11/09 07:31:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/09 07:37:10 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.5821, average train loss: 2.3116
[11/09 07:37:53 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1564, average loss: 6.1037
[11/09 07:37:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.65	
[11/09 07:37:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/09 07:44:04 visual_prompt]: Epoch 9 / 100: avg data time: 1.02e+01, avg batch time: 10.6107, average train loss: 3.1626
[11/09 07:44:47 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1598, average loss: 1.9523
[11/09 07:44:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.88	
[11/09 07:44:47 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/09 07:50:58 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5849, average train loss: 2.9009
[11/09 07:51:40 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1561, average loss: 2.3492
[11/09 07:51:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.85	
[11/09 07:51:40 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/09 07:57:51 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5878, average train loss: 4.5700
[11/09 07:58:33 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1564, average loss: 0.7204
[11/09 07:58:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.40	
[11/09 07:58:33 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/09 08:04:44 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.6035, average train loss: 5.3959
[11/09 08:05:27 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1587, average loss: 11.3228
[11/09 08:05:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.87	
[11/09 08:05:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/09 08:11:41 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.6985, average train loss: 5.1083
[11/09 08:12:24 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1560, average loss: 3.4097
[11/09 08:12:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.59	
[11/09 08:12:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/09 08:18:39 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.6971, average train loss: 4.9301
[11/09 08:19:22 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1561, average loss: 3.6980
[11/09 08:19:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.01	
[11/09 08:19:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/09 08:25:33 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6123, average train loss: 3.3812
[11/09 08:26:16 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1567, average loss: 4.4571
[11/09 08:26:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.01	
[11/09 08:26:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/09 08:32:29 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6469, average train loss: 4.7471
[11/09 08:33:11 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1559, average loss: 4.3415
[11/09 08:33:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.03	
[11/09 08:33:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/09 08:39:22 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.6072, average train loss: 4.8584
[11/09 08:40:05 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1583, average loss: 1.6898
[11/09 08:40:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.91	
[11/09 08:40:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/09 08:46:18 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6480, average train loss: 4.7254
[11/09 08:47:00 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1558, average loss: 5.6253
[11/09 08:47:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.76	
[11/09 08:47:00 visual_prompt]: Stopping early.
[11/09 08:47:00 visual_prompt]: Rank of current process: 0. World size: 1
[11/09 08:47:00 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/09 08:47:00 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/09 08:47:00 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/09 08:47:00 visual_prompt]: Training with config:
[11/09 08:47:00 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr2.5_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/09 08:47:00 visual_prompt]: Loading training data...
[11/09 08:47:00 visual_prompt]: Constructing mammo-cbis dataset train...
[11/09 08:47:00 visual_prompt]: Loading validation data...
[11/09 08:47:00 visual_prompt]: Constructing mammo-cbis dataset val...
[11/09 08:47:00 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/09 08:47:03 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/09 08:47:03 visual_prompt]: tuned percent:0.536
[11/09 08:47:03 visual_prompt]: Device used for model: 0
[11/09 08:47:03 visual_prompt]: Setting up Evaluator...
[11/09 08:47:03 visual_prompt]: Setting up Trainer...
[11/09 08:47:03 visual_prompt]: 	Setting up the optimizer...
[11/09 08:47:03 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/09 08:53:18 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.7202, average train loss: 1.4017
[11/09 08:54:01 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1560, average loss: 1.2969
[11/09 08:54:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/09 08:54:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/09 09:00:16 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.7099, average train loss: 3.6304
[11/09 09:00:59 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1562, average loss: 0.6825
[11/09 09:00:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 55.38	
[11/09 09:00:59 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/09 09:07:14 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.7064, average train loss: 0.7231
[11/09 09:07:56 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1599, average loss: 0.6997
[11/09 09:07:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 59.25	
[11/09 09:07:56 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/09 09:14:12 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.7296, average train loss: 0.9047
[11/09 09:14:55 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1561, average loss: 0.6883
[11/09 09:14:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.20	
[11/09 09:14:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/09 09:21:10 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6948, average train loss: 1.0396
[11/09 09:21:52 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1561, average loss: 0.6825
[11/09 09:21:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.41	
[11/09 09:21:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/09 09:28:07 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.7111, average train loss: 1.0942
[11/09 09:28:50 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1556, average loss: 1.2490
[11/09 09:28:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.27	
[11/09 09:28:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/09 09:35:06 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.7366, average train loss: 0.9117
[11/09 09:35:49 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1559, average loss: 1.1022
[11/09 09:35:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.14	
[11/09 09:35:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/09 09:42:03 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.6990, average train loss: 0.8611
[11/09 09:42:46 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1560, average loss: 1.0243
[11/09 09:42:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.29	
[11/09 09:42:46 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/09 09:49:02 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.7372, average train loss: 0.8088
[11/09 09:49:45 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1562, average loss: 0.6899
[11/09 09:49:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.63	
[11/09 09:49:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/09 09:55:59 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.6956, average train loss: 0.9031
[11/09 09:56:42 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1560, average loss: 3.8567
[11/09 09:56:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.05	
[11/09 09:56:42 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/09 10:02:58 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.7318, average train loss: 3.8047
[11/09 10:03:41 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1558, average loss: 1.5610
[11/09 10:03:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.07	
[11/09 10:03:41 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/09 10:09:56 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.7165, average train loss: 1.0283
[11/09 10:10:39 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1573, average loss: 0.8314
[11/09 10:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.94	
[11/09 10:10:39 visual_prompt]: Best epoch 12: best metric: -0.831
[11/09 10:10:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/09 10:16:54 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.7205, average train loss: 1.4775
[11/09 10:17:37 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1560, average loss: 4.7126
[11/09 10:17:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.21	
[11/09 10:17:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/09 10:23:52 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.7098, average train loss: 2.7144
[11/09 10:24:35 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1578, average loss: 2.6430
[11/09 10:24:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.01	
[11/09 10:24:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/09 10:30:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.7407, average train loss: 1.2353
[11/09 10:31:34 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1557, average loss: 1.1076
[11/09 10:31:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.77	
[11/09 10:31:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/09 10:37:49 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.7206, average train loss: 1.0892
[11/09 10:38:32 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1592, average loss: 0.7286
[11/09 10:38:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.76	
[11/09 10:38:32 visual_prompt]: Best epoch 16: best metric: -0.729
[11/09 10:38:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/09 10:44:44 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6380, average train loss: 0.8599
[11/09 10:45:27 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1561, average loss: 0.7720
[11/09 10:45:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.85	
[11/09 10:45:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/09 10:51:50 visual_prompt]: Epoch 18 / 100: avg data time: 1.06e+01, avg batch time: 10.9475, average train loss: 0.9803
[11/09 10:52:33 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1563, average loss: 0.6886
[11/09 10:52:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.48	
[11/09 10:52:33 visual_prompt]: Best epoch 18: best metric: -0.689
[11/09 10:52:33 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/09 10:58:47 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.7003, average train loss: 1.1256
[11/09 10:59:30 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1579, average loss: 1.1808
[11/09 10:59:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.70	
[11/09 10:59:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/09 11:05:46 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.7156, average train loss: 1.3883
[11/09 11:06:28 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1596, average loss: 1.0114
[11/09 11:06:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.66	
[11/09 11:06:28 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/09 11:12:43 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.7118, average train loss: 0.8843
[11/09 11:13:26 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1562, average loss: 0.9811
[11/09 11:13:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.20	
[11/09 11:13:26 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/09 11:19:41 visual_prompt]: Epoch 22 / 100: avg data time: 1.04e+01, avg batch time: 10.7152, average train loss: 1.0432
[11/09 11:20:24 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1561, average loss: 0.6965
[11/09 11:20:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.83	
[11/09 11:20:24 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/09 11:26:39 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 10.7003, average train loss: 1.1710
[11/09 11:27:22 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1558, average loss: 0.6930
[11/09 11:27:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.37	
[11/09 11:27:22 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/09 11:33:34 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e+01, avg batch time: 10.6308, average train loss: 0.8601
[11/09 11:34:16 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1561, average loss: 0.6942
[11/09 11:34:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.00	
[11/09 11:34:16 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/09 11:40:27 visual_prompt]: Epoch 25 / 100: avg data time: 1.02e+01, avg batch time: 10.5975, average train loss: 0.7406
[11/09 11:41:10 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1575, average loss: 0.7404
[11/09 11:41:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.30	
[11/09 11:41:10 visual_prompt]: Stopping early.
[11/09 11:41:10 visual_prompt]: Rank of current process: 0. World size: 1
[11/09 11:41:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/09 11:41:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/09 11:41:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/09 11:41:10 visual_prompt]: Training with config:
[11/09 11:41:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr2.5_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/09 11:41:10 visual_prompt]: Loading training data...
[11/09 11:41:10 visual_prompt]: Constructing mammo-cbis dataset train...
[11/09 11:41:10 visual_prompt]: Loading validation data...
[11/09 11:41:10 visual_prompt]: Constructing mammo-cbis dataset val...
[11/09 11:41:10 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/09 11:41:12 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/09 11:41:12 visual_prompt]: tuned percent:0.536
[11/09 11:41:12 visual_prompt]: Device used for model: 0
[11/09 11:41:12 visual_prompt]: Setting up Evaluator...
[11/09 11:41:12 visual_prompt]: Setting up Trainer...
[11/09 11:41:12 visual_prompt]: 	Setting up the optimizer...
[11/09 11:41:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/09 11:47:24 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6098, average train loss: 1.4017
[11/09 11:48:06 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1559, average loss: 1.2969
[11/09 11:48:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/09 11:48:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/09 11:54:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6715, average train loss: 3.6454
[11/09 11:55:02 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1571, average loss: 0.6816
[11/09 11:55:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 56.35	
[11/09 11:55:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/09 12:01:28 visual_prompt]: Epoch 3 / 100: avg data time: 1.07e+01, avg batch time: 11.0112, average train loss: 0.7275
[11/09 12:02:11 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1601, average loss: 0.6955
[11/09 12:02:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.41	rocauc: 57.91	
[11/09 12:02:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/09 12:08:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.7206, average train loss: 0.9362
[11/09 12:09:09 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1557, average loss: 0.6933
[11/09 12:09:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.59	
[11/09 12:09:09 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/09 12:15:24 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.7169, average train loss: 1.1451
[11/09 12:16:07 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1559, average loss: 0.7387
[11/09 12:16:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.34	
[11/09 12:16:07 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/09 12:22:22 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.7240, average train loss: 2.1868
[11/09 12:23:05 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1559, average loss: 2.5805
[11/09 12:23:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.00	
[11/09 12:23:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/09 12:29:21 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.7288, average train loss: 2.1259
[11/09 12:30:03 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1559, average loss: 3.0898
[11/09 12:30:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.52	
[11/09 12:30:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/09 12:36:18 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.7012, average train loss: 3.2019
[11/09 12:37:01 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1577, average loss: 1.8020
[11/09 12:37:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.57	
[11/09 12:37:01 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/09 12:43:16 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.7290, average train loss: 1.3700
[11/09 12:43:59 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1598, average loss: 0.6804
[11/09 12:43:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.34	
[11/09 12:43:59 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/09 12:50:14 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.6980, average train loss: 0.8586
[11/09 12:50:57 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1561, average loss: 1.1703
[11/09 12:50:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.34	
[11/09 12:50:57 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/09 12:57:11 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.7052, average train loss: 3.0920
[11/09 12:57:54 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1571, average loss: 0.9227
[11/09 12:57:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.77	
[11/09 12:57:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/09 13:04:09 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.6998, average train loss: 1.8829
[11/09 13:04:51 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1560, average loss: 2.9093
[11/09 13:04:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.71	
[11/09 13:04:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/09 13:11:06 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.7072, average train loss: 1.2886
[11/09 13:11:51 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1558, average loss: 0.7878
[11/09 13:11:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.66	
[11/09 13:11:51 visual_prompt]: Best epoch 13: best metric: -0.788
[11/09 13:11:51 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/09 13:18:06 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.7035, average train loss: 1.0460
[11/09 13:18:49 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1582, average loss: 1.5405
[11/09 13:18:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.96	
[11/09 13:18:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/09 13:25:04 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.7182, average train loss: 1.0369
[11/09 13:25:47 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1562, average loss: 0.8985
[11/09 13:25:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.55	
[11/09 13:25:47 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/09 13:32:09 visual_prompt]: Epoch 16 / 100: avg data time: 1.06e+01, avg batch time: 10.9318, average train loss: 0.8825
[11/09 13:32:52 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1560, average loss: 0.7928
[11/09 13:32:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.30	
[11/09 13:32:52 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/09 13:39:02 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.5701, average train loss: 0.8888
[11/09 13:39:44 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1559, average loss: 1.2473
[11/09 13:39:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.67	
[11/09 13:39:44 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/09 13:45:56 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.6011, average train loss: 0.9067
[11/09 13:46:38 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1560, average loss: 0.8143
[11/09 13:46:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.55	
[11/09 13:46:38 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/09 13:52:48 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.5726, average train loss: 0.8457
[11/09 13:53:30 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1561, average loss: 0.7060
[11/09 13:53:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.85	
[11/09 13:53:30 visual_prompt]: Best epoch 19: best metric: -0.706
[11/09 13:53:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/09 13:59:42 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.6107, average train loss: 0.7155
[11/09 14:00:24 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1562, average loss: 0.7022
[11/09 14:00:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.95	
[11/09 14:00:24 visual_prompt]: Best epoch 20: best metric: -0.702
[11/09 14:00:24 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/09 14:06:35 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.5791, average train loss: 0.7812
[11/09 14:07:17 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1558, average loss: 0.7009
[11/09 14:07:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.77	
[11/09 14:07:17 visual_prompt]: Best epoch 21: best metric: -0.701
[11/09 14:07:17 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/09 14:13:27 visual_prompt]: Epoch 22 / 100: avg data time: 1.02e+01, avg batch time: 10.5810, average train loss: 0.9935
[11/09 14:14:10 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1561, average loss: 0.7742
[11/09 14:14:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.47	
[11/09 14:14:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/09 14:20:20 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e+01, avg batch time: 10.5902, average train loss: 0.8299
[11/09 14:21:03 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1563, average loss: 1.3047
[11/09 14:21:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.37	
[11/09 14:21:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/09 14:27:14 visual_prompt]: Epoch 24 / 100: avg data time: 1.02e+01, avg batch time: 10.6028, average train loss: 0.8788
[11/09 14:27:56 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1589, average loss: 0.7853
[11/09 14:27:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.79	
[11/09 14:27:56 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/09 14:34:07 visual_prompt]: Epoch 25 / 100: avg data time: 1.02e+01, avg batch time: 10.5920, average train loss: 0.7833
[11/09 14:34:49 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1559, average loss: 0.9416
[11/09 14:34:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.30	
[11/09 14:34:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/09 14:41:00 visual_prompt]: Epoch 26 / 100: avg data time: 1.02e+01, avg batch time: 10.5973, average train loss: 0.8801
[11/09 14:41:43 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1560, average loss: 0.8171
[11/09 14:41:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.38	
[11/09 14:41:43 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/09 14:47:58 visual_prompt]: Epoch 27 / 100: avg data time: 1.03e+01, avg batch time: 10.7062, average train loss: 0.7572
[11/09 14:48:41 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1588, average loss: 0.8759
[11/09 14:48:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.49	
[11/09 14:48:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/09 14:54:55 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.6948, average train loss: 0.8196
[11/09 14:55:38 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1560, average loss: 0.6887
[11/09 14:55:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.07	
[11/09 14:55:38 visual_prompt]: Best epoch 28: best metric: -0.689
[11/09 14:55:38 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[11/09 15:01:53 visual_prompt]: Epoch 29 / 100: avg data time: 1.04e+01, avg batch time: 10.7105, average train loss: 0.7494
[11/09 15:02:35 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1562, average loss: 0.7312
[11/09 15:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.85	
[11/09 15:02:35 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[11/09 15:08:50 visual_prompt]: Epoch 30 / 100: avg data time: 1.03e+01, avg batch time: 10.6902, average train loss: 0.7541
[11/09 15:09:33 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1563, average loss: 0.6784
[11/09 15:09:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 61.10	
[11/09 15:09:33 visual_prompt]: Best epoch 30: best metric: -0.678
[11/09 15:09:33 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[11/09 15:15:47 visual_prompt]: Epoch 31 / 100: avg data time: 1.03e+01, avg batch time: 10.7006, average train loss: 0.7567
[11/09 15:16:30 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1558, average loss: 0.6956
[11/09 15:16:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.21	
[11/09 15:16:30 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[11/09 15:22:45 visual_prompt]: Epoch 32 / 100: avg data time: 1.04e+01, avg batch time: 10.7223, average train loss: 0.7742
[11/09 15:23:28 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1562, average loss: 0.6922
[11/09 15:23:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 60.18	
[11/09 15:23:28 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[11/09 15:29:43 visual_prompt]: Epoch 33 / 100: avg data time: 1.04e+01, avg batch time: 10.7115, average train loss: 0.7725
[11/09 15:30:26 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1563, average loss: 0.6895
[11/09 15:30:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.12	
[11/09 15:30:26 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[11/09 15:36:41 visual_prompt]: Epoch 34 / 100: avg data time: 1.03e+01, avg batch time: 10.7033, average train loss: 0.7480
[11/09 15:37:23 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1585, average loss: 0.7044
[11/09 15:37:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.48	
[11/09 15:37:23 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[11/09 15:43:36 visual_prompt]: Epoch 35 / 100: avg data time: 1.03e+01, avg batch time: 10.6324, average train loss: 0.7402
[11/09 15:44:18 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1564, average loss: 0.7032
[11/09 15:44:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.59	
[11/09 15:44:18 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[11/09 15:50:29 visual_prompt]: Epoch 36 / 100: avg data time: 1.02e+01, avg batch time: 10.6016, average train loss: 0.7698
[11/09 15:51:12 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1583, average loss: 0.7362
[11/09 15:51:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.68	
[11/09 15:51:12 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[11/09 15:57:26 visual_prompt]: Epoch 37 / 100: avg data time: 1.03e+01, avg batch time: 10.6849, average train loss: 0.7190
[11/09 15:58:09 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1597, average loss: 0.6828
[11/09 15:58:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.80	
[11/09 15:58:09 visual_prompt]: Stopping early.
[11/09 15:58:09 visual_prompt]: Rank of current process: 0. World size: 1
[11/09 15:58:09 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/09 15:58:09 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/09 15:58:09 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/09 15:58:09 visual_prompt]: Training with config:
[11/09 15:58:09 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr2.5_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/09 15:58:09 visual_prompt]: Loading training data...
[11/09 15:58:09 visual_prompt]: Constructing mammo-cbis dataset train...
[11/09 15:58:09 visual_prompt]: Loading validation data...
[11/09 15:58:09 visual_prompt]: Constructing mammo-cbis dataset val...
[11/09 15:58:09 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/09 15:58:11 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/09 15:58:11 visual_prompt]: tuned percent:0.536
[11/09 15:58:12 visual_prompt]: Device used for model: 0
[11/09 15:58:12 visual_prompt]: Setting up Evaluator...
[11/09 15:58:12 visual_prompt]: Setting up Trainer...
[11/09 15:58:12 visual_prompt]: 	Setting up the optimizer...
[11/09 15:58:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/09 16:04:25 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6746, average train loss: 1.4017
[11/09 16:05:08 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1560, average loss: 1.2969
[11/09 16:05:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/09 16:05:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/09 16:11:19 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5957, average train loss: 3.6468
[11/09 16:12:01 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1559, average loss: 0.6812
[11/09 16:12:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 56.78	
[11/09 16:12:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/09 16:18:12 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.6089, average train loss: 0.7277
[11/09 16:18:54 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1563, average loss: 0.6949
[11/09 16:18:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 59.53	
[11/09 16:18:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/09 16:25:05 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.5912, average train loss: 0.9364
[11/09 16:25:48 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1561, average loss: 0.6894
[11/09 16:25:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.26	
[11/09 16:25:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/09 16:31:58 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5816, average train loss: 1.1632
[11/09 16:32:40 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1563, average loss: 0.7970
[11/09 16:32:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.89	
[11/09 16:32:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/09 16:38:52 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6165, average train loss: 1.6444
[11/09 16:39:34 visual_prompt]: Inference (val):avg data time: 1.81e-05, avg batch time: 0.1587, average loss: 1.4777
[11/09 16:39:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.03	
[11/09 16:39:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/09 16:45:46 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6289, average train loss: 2.5634
[11/09 16:46:29 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1560, average loss: 1.7295
[11/09 16:46:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.24	
[11/09 16:46:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/09 16:52:40 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.5965, average train loss: 1.3294
[11/09 16:53:22 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1561, average loss: 1.5858
[11/09 16:53:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.43	
[11/09 16:53:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/09 16:59:35 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6335, average train loss: 1.0449
[11/09 17:00:17 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1557, average loss: 1.5837
[11/09 17:00:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.59	
[11/09 17:00:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/09 17:06:28 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5956, average train loss: 0.9955
[11/09 17:07:11 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1561, average loss: 1.4498
[11/09 17:07:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.75	
[11/09 17:07:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/09 17:13:21 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5930, average train loss: 2.7400
[11/09 17:14:04 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1559, average loss: 0.6725
[11/09 17:14:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 60.86	
[11/09 17:14:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/09 17:20:15 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5994, average train loss: 1.7798
[11/09 17:20:57 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1562, average loss: 1.5732
[11/09 17:20:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.55	
[11/09 17:20:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/09 17:27:09 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.6165, average train loss: 3.1057
[11/09 17:27:51 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1567, average loss: 2.8106
[11/09 17:27:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.71	
[11/09 17:27:51 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/09 17:34:02 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5789, average train loss: 1.8764
[11/09 17:34:44 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1560, average loss: 1.8100
[11/09 17:34:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.51	
[11/09 17:34:44 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/09 17:40:56 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6172, average train loss: 1.4916
[11/09 17:41:38 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1557, average loss: 0.6773
[11/09 17:41:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.61	
[11/09 17:41:38 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/09 17:47:49 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.6038, average train loss: 0.9782
[11/09 17:48:31 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1560, average loss: 0.7296
[11/09 17:48:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.93	
[11/09 17:48:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/09 17:54:42 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.5924, average train loss: 2.6419
[11/09 17:55:25 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1561, average loss: 3.1785
[11/09 17:55:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.29	
[11/09 17:55:25 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/09 18:01:37 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6132, average train loss: 2.1624
[11/09 18:02:19 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1560, average loss: 2.9060
[11/09 18:02:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.04	
[11/09 18:02:19 visual_prompt]: Stopping early.
[11/09 18:02:19 visual_prompt]: Rank of current process: 0. World size: 1
[11/09 18:02:19 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/09 18:02:19 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/09 18:02:19 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/09 18:02:19 visual_prompt]: Training with config:
[11/09 18:02:19 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr1.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/09 18:02:19 visual_prompt]: Loading training data...
[11/09 18:02:19 visual_prompt]: Constructing mammo-cbis dataset train...
[11/09 18:02:19 visual_prompt]: Loading validation data...
[11/09 18:02:19 visual_prompt]: Constructing mammo-cbis dataset val...
[11/09 18:02:19 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/09 18:02:22 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/09 18:02:22 visual_prompt]: tuned percent:0.536
[11/09 18:02:22 visual_prompt]: Device used for model: 0
[11/09 18:02:22 visual_prompt]: Setting up Evaluator...
[11/09 18:02:22 visual_prompt]: Setting up Trainer...
[11/09 18:02:22 visual_prompt]: 	Setting up the optimizer...
[11/09 18:02:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/09 18:08:34 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6243, average train loss: 1.4017
[11/09 18:09:16 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1561, average loss: 1.2969
[11/09 18:09:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/09 18:09:16 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/09 18:15:28 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.6110, average train loss: 2.3252
[11/09 18:16:10 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1576, average loss: 0.6886
[11/09 18:16:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 51.43	
[11/09 18:16:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/09 18:22:21 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.6196, average train loss: 0.7411
[11/09 18:23:04 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1559, average loss: 0.6876
[11/09 18:23:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.54	
[11/09 18:23:04 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/09 18:29:19 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.7036, average train loss: 0.7144
[11/09 18:30:01 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1559, average loss: 0.7505
[11/09 18:30:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.94	
[11/09 18:30:01 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/09 18:36:16 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6996, average train loss: 0.7138
[11/09 18:36:59 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1558, average loss: 0.6898
[11/09 18:36:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.08	
[11/09 18:36:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/09 18:43:14 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.7044, average train loss: 0.7073
[11/09 18:43:56 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1590, average loss: 0.7206
[11/09 18:43:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.18	
[11/09 18:43:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/09 18:50:12 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.7231, average train loss: 0.7291
[11/09 18:50:54 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1559, average loss: 1.0410
[11/09 18:50:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.60	
[11/09 18:50:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/09 18:57:09 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.7050, average train loss: 0.7992
[11/09 18:57:52 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1565, average loss: 0.7395
[11/09 18:57:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.76	
[11/09 18:57:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/09 19:04:08 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.7277, average train loss: 0.8031
[11/09 19:04:50 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1555, average loss: 0.6885
[11/09 19:04:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.07	
[11/09 19:04:50 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/09 19:11:05 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.7093, average train loss: 0.7939
[11/09 19:11:48 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1582, average loss: 1.0083
[11/09 19:11:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.16	
[11/09 19:11:48 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/09 19:18:01 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.6619, average train loss: 0.8696
[11/09 19:18:44 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1559, average loss: 0.7229
[11/09 19:18:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.56	
[11/09 19:18:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/09 19:24:54 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5862, average train loss: 0.7553
[11/09 19:25:37 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1557, average loss: 1.1101
[11/09 19:25:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.71	
[11/09 19:25:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/09 19:31:48 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.6043, average train loss: 1.3041
[11/09 19:32:30 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1568, average loss: 0.8014
[11/09 19:32:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.66	
[11/09 19:32:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/09 19:38:41 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5922, average train loss: 0.8819
[11/09 19:39:23 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1560, average loss: 3.4008
[11/09 19:39:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.93	
[11/09 19:39:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/09 19:45:35 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6146, average train loss: 1.0585
[11/09 19:46:17 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1559, average loss: 0.7373
[11/09 19:46:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.04	
[11/09 19:46:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/09 19:52:27 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.5717, average train loss: 1.1577
[11/09 19:53:10 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1560, average loss: 0.9042
[11/09 19:53:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.19	
[11/09 19:53:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/09 19:59:20 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.5778, average train loss: 1.8154
[11/09 20:00:02 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1564, average loss: 0.9966
[11/09 20:00:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.62	
[11/09 20:00:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/09 20:06:13 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.5920, average train loss: 1.0424
[11/09 20:06:56 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1558, average loss: 0.8716
[11/09 20:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/09 20:06:56 visual_prompt]: Stopping early.
[11/09 20:06:56 visual_prompt]: Rank of current process: 0. World size: 1
[11/09 20:06:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/09 20:06:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/09 20:06:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/09 20:06:56 visual_prompt]: Training with config:
[11/09 20:06:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr1.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/09 20:06:56 visual_prompt]: Loading training data...
[11/09 20:06:56 visual_prompt]: Constructing mammo-cbis dataset train...
[11/09 20:06:56 visual_prompt]: Loading validation data...
[11/09 20:06:56 visual_prompt]: Constructing mammo-cbis dataset val...
[11/09 20:06:56 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/09 20:06:58 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/09 20:06:58 visual_prompt]: tuned percent:0.536
[11/09 20:06:58 visual_prompt]: Device used for model: 0
[11/09 20:06:58 visual_prompt]: Setting up Evaluator...
[11/09 20:06:58 visual_prompt]: Setting up Trainer...
[11/09 20:06:58 visual_prompt]: 	Setting up the optimizer...
[11/09 20:06:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/09 20:13:09 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.5987, average train loss: 1.4017
[11/09 20:13:51 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1561, average loss: 1.2969
[11/09 20:13:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/09 20:13:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/09 20:20:02 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5822, average train loss: 2.3839
[11/09 20:20:44 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1563, average loss: 0.6869
[11/09 20:20:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.61	
[11/09 20:20:44 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/09 20:26:55 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.5792, average train loss: 0.7623
[11/09 20:27:37 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1604, average loss: 0.6867
[11/09 20:27:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 57.20	
[11/09 20:27:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/09 20:33:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.6224, average train loss: 0.7195
[11/09 20:34:31 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1560, average loss: 0.6913
[11/09 20:34:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.58	
[11/09 20:34:31 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/09 20:40:42 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5896, average train loss: 0.7861
[11/09 20:41:24 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1561, average loss: 0.6954
[11/09 20:41:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 59.88	
[11/09 20:41:24 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/09 20:47:36 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6293, average train loss: 0.7758
[11/09 20:48:19 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1560, average loss: 0.6790
[11/09 20:48:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 58.89	
[11/09 20:48:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/09 20:54:31 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6335, average train loss: 0.7122
[11/09 20:55:14 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1568, average loss: 1.4528
[11/09 20:55:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.64	
[11/09 20:55:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/09 21:01:24 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.5844, average train loss: 0.8635
[11/09 21:02:06 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1582, average loss: 0.6842
[11/09 21:02:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 58.75	
[11/09 21:02:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/09 21:08:19 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6362, average train loss: 0.8505
[11/09 21:09:01 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1559, average loss: 0.7907
[11/09 21:09:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.55	
[11/09 21:09:01 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/09 21:15:12 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5874, average train loss: 0.7273
[11/09 21:15:54 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1555, average loss: 0.7180
[11/09 21:15:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.24	
[11/09 21:15:54 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/09 21:22:05 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5873, average train loss: 0.7782
[11/09 21:22:47 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1592, average loss: 0.8833
[11/09 21:22:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.68	
[11/09 21:22:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/09 21:28:57 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5869, average train loss: 0.7381
[11/09 21:29:40 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1570, average loss: 0.6908
[11/09 21:29:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.60	
[11/09 21:29:40 visual_prompt]: Best epoch 12: best metric: -0.691
[11/09 21:29:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/09 21:35:54 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.6976, average train loss: 0.7580
[11/09 21:36:37 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1560, average loss: 0.6869
[11/09 21:36:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.67	
[11/09 21:36:37 visual_prompt]: Best epoch 13: best metric: -0.687
[11/09 21:36:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/09 21:42:51 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.6889, average train loss: 0.7634
[11/09 21:43:34 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1559, average loss: 0.7071
[11/09 21:43:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.69	
[11/09 21:43:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/09 21:49:49 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.7233, average train loss: 0.7538
[11/09 21:50:32 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1563, average loss: 0.6997
[11/09 21:50:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.43	
[11/09 21:50:32 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/09 21:56:47 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.7156, average train loss: 0.7532
[11/09 21:57:30 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1559, average loss: 1.0249
[11/09 21:57:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.33	
[11/09 21:57:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/09 22:03:46 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.7270, average train loss: 0.7676
[11/09 22:04:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1567, average loss: 0.9649
[11/09 22:04:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.19	
[11/09 22:04:29 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/09 22:10:44 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.7302, average train loss: 0.7477
[11/09 22:11:27 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1558, average loss: 0.8711
[11/09 22:11:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.90	
[11/09 22:11:27 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/09 22:17:41 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6816, average train loss: 0.7134
[11/09 22:18:23 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1557, average loss: 0.7062
[11/09 22:18:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.07	
[11/09 22:18:23 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/09 22:24:35 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.6069, average train loss: 0.6941
[11/09 22:25:17 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1557, average loss: 0.8264
[11/09 22:25:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.28	
[11/09 22:25:17 visual_prompt]: Stopping early.
[11/09 22:25:17 visual_prompt]: Rank of current process: 0. World size: 1
[11/09 22:25:17 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/09 22:25:17 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/09 22:25:17 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/09 22:25:17 visual_prompt]: Training with config:
[11/09 22:25:17 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr1.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/09 22:25:17 visual_prompt]: Loading training data...
[11/09 22:25:17 visual_prompt]: Constructing mammo-cbis dataset train...
[11/09 22:25:17 visual_prompt]: Loading validation data...
[11/09 22:25:17 visual_prompt]: Constructing mammo-cbis dataset val...
[11/09 22:25:17 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/09 22:25:20 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/09 22:25:20 visual_prompt]: tuned percent:0.536
[11/09 22:25:20 visual_prompt]: Device used for model: 0
[11/09 22:25:20 visual_prompt]: Setting up Evaluator...
[11/09 22:25:20 visual_prompt]: Setting up Trainer...
[11/09 22:25:20 visual_prompt]: 	Setting up the optimizer...
[11/09 22:25:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/09 22:31:32 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6416, average train loss: 1.4017
[11/09 22:32:15 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1585, average loss: 1.2969
[11/09 22:32:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/09 22:32:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/09 22:38:28 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6632, average train loss: 2.3838
[11/09 22:39:11 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1560, average loss: 0.6868
[11/09 22:39:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.68	
[11/09 22:39:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/09 22:45:25 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.6818, average train loss: 0.7630
[11/09 22:46:07 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1562, average loss: 0.6921
[11/09 22:46:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 56.57	
[11/09 22:46:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/09 22:52:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.05e+01, avg batch time: 10.8336, average train loss: 0.7209
[11/09 22:53:09 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1569, average loss: 0.6886
[11/09 22:53:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.51	
[11/09 22:53:09 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/09 22:59:22 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6564, average train loss: 0.7916
[11/09 23:00:04 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1559, average loss: 0.7003
[11/09 23:00:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 57.28	
[11/09 23:00:04 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/09 23:06:18 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6742, average train loss: 0.7844
[11/09 23:07:00 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1563, average loss: 0.6790
[11/09 23:07:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.11	
[11/09 23:07:00 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/09 23:13:13 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6489, average train loss: 0.7213
[11/09 23:13:56 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1560, average loss: 1.5615
[11/09 23:13:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.46	
[11/09 23:13:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/09 23:20:08 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.6216, average train loss: 0.9059
[11/09 23:20:51 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1560, average loss: 0.6771
[11/09 23:20:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 59.84	
[11/09 23:20:51 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/09 23:27:03 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6512, average train loss: 0.9008
[11/09 23:27:46 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1595, average loss: 0.7714
[11/09 23:27:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.90	
[11/09 23:27:46 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/09 23:33:57 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.6165, average train loss: 0.8158
[11/09 23:34:40 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1560, average loss: 0.8030
[11/09 23:34:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.84	
[11/09 23:34:40 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/09 23:40:52 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.6194, average train loss: 0.7922
[11/09 23:41:34 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1562, average loss: 0.7035
[11/09 23:41:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 63.11	
[11/09 23:41:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/09 23:47:47 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.6455, average train loss: 0.7841
[11/09 23:48:29 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1563, average loss: 1.2673
[11/09 23:48:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.03	
[11/09 23:48:29 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/09 23:54:45 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.7239, average train loss: 0.9815
[11/09 23:55:28 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1567, average loss: 0.6713
[11/09 23:55:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.95	
[11/09 23:55:28 visual_prompt]: Best epoch 13: best metric: -0.671
[11/09 23:55:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/10 00:01:43 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.7107, average train loss: 0.8481
[11/10 00:02:26 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1562, average loss: 1.1224
[11/10 00:02:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.18	
[11/10 00:02:26 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/10 00:08:41 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.7309, average train loss: 0.8763
[11/10 00:09:24 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1587, average loss: 0.7881
[11/10 00:09:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.86	
[11/10 00:09:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/10 00:15:39 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6986, average train loss: 0.8560
[11/10 00:16:21 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1571, average loss: 0.8235
[11/10 00:16:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.43	
[11/10 00:16:21 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/10 00:22:36 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6894, average train loss: 0.7877
[11/10 00:23:18 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1558, average loss: 1.2729
[11/10 00:23:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.00	
[11/10 00:23:18 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/10 00:29:33 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6996, average train loss: 0.8015
[11/10 00:30:16 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1570, average loss: 0.6836
[11/10 00:30:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 68.99	
[11/10 00:30:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/10 00:36:30 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6990, average train loss: 0.7269
[11/10 00:37:13 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1561, average loss: 0.6563
[11/10 00:37:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 67.62	
[11/10 00:37:13 visual_prompt]: Best epoch 19: best metric: -0.656
[11/10 00:37:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/10 00:43:28 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.7059, average train loss: 0.6895
[11/10 00:44:11 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1562, average loss: 0.6313
[11/10 00:44:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 74.00	
[11/10 00:44:11 visual_prompt]: Best epoch 20: best metric: -0.631
[11/10 00:44:11 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[11/10 00:50:26 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.7115, average train loss: 0.7534
[11/10 00:51:09 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1563, average loss: 0.6307
[11/10 00:51:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 71.39	
[11/10 00:51:09 visual_prompt]: Best epoch 21: best metric: -0.631
[11/10 00:51:09 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[11/10 00:57:21 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e+01, avg batch time: 10.6286, average train loss: 1.0107
[11/10 00:58:04 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1557, average loss: 0.8245
[11/10 00:58:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[11/10 00:58:04 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[11/10 01:04:18 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 10.7003, average train loss: 0.7548
[11/10 01:05:01 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1563, average loss: 0.6719
[11/10 01:05:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 68.89	
[11/10 01:05:01 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[11/10 01:11:13 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e+01, avg batch time: 10.6308, average train loss: 0.7281
[11/10 01:11:56 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1562, average loss: 0.6827
[11/10 01:11:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 67.11	
[11/10 01:11:56 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[11/10 01:18:07 visual_prompt]: Epoch 25 / 100: avg data time: 1.03e+01, avg batch time: 10.6179, average train loss: 0.6765
[11/10 01:18:50 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1597, average loss: 0.7280
[11/10 01:18:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.41	rocauc: 70.97	
[11/10 01:18:50 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[11/10 01:25:01 visual_prompt]: Epoch 26 / 100: avg data time: 1.03e+01, avg batch time: 10.6070, average train loss: 0.7592
[11/10 01:25:44 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1597, average loss: 0.8253
[11/10 01:25:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.08	
[11/10 01:25:44 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[11/10 01:31:55 visual_prompt]: Epoch 27 / 100: avg data time: 1.02e+01, avg batch time: 10.6089, average train loss: 0.7015
[11/10 01:32:37 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1561, average loss: 0.8911
[11/10 01:32:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 66.46	
[11/10 01:32:37 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[11/10 01:38:49 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.6090, average train loss: 0.7425
[11/10 01:39:31 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1562, average loss: 0.7833
[11/10 01:39:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.45	
[11/10 01:39:31 visual_prompt]: Stopping early.
[11/10 01:39:31 visual_prompt]: Rank of current process: 0. World size: 1
[11/10 01:39:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/10 01:39:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/10 01:39:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/10 01:39:31 visual_prompt]: Training with config:
[11/10 01:39:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr1.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/10 01:39:31 visual_prompt]: Loading training data...
[11/10 01:39:31 visual_prompt]: Constructing mammo-cbis dataset train...
[11/10 01:39:31 visual_prompt]: Loading validation data...
[11/10 01:39:31 visual_prompt]: Constructing mammo-cbis dataset val...
[11/10 01:39:31 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/10 01:39:34 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/10 01:39:34 visual_prompt]: tuned percent:0.536
[11/10 01:39:34 visual_prompt]: Device used for model: 0
[11/10 01:39:34 visual_prompt]: Setting up Evaluator...
[11/10 01:39:34 visual_prompt]: Setting up Trainer...
[11/10 01:39:34 visual_prompt]: 	Setting up the optimizer...
[11/10 01:39:34 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/10 01:45:46 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6420, average train loss: 1.4017
[11/10 01:46:29 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1562, average loss: 1.2969
[11/10 01:46:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/10 01:46:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/10 01:52:41 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6144, average train loss: 2.3837
[11/10 01:53:23 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1560, average loss: 0.6870
[11/10 01:53:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.31	
[11/10 01:53:23 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/10 01:59:35 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.6158, average train loss: 0.7638
[11/10 02:00:17 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1561, average loss: 0.6927
[11/10 02:00:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 56.68	
[11/10 02:00:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/10 02:06:33 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.7285, average train loss: 0.7212
[11/10 02:07:16 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1562, average loss: 0.6890
[11/10 02:07:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.52	
[11/10 02:07:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/10 02:13:31 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.7205, average train loss: 0.7934
[11/10 02:14:14 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1587, average loss: 0.7004
[11/10 02:14:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 57.28	
[11/10 02:14:14 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/10 02:20:30 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.7353, average train loss: 0.7839
[11/10 02:21:13 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1563, average loss: 0.6782
[11/10 02:21:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 57.76	
[11/10 02:21:13 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/10 02:27:28 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.7220, average train loss: 0.7145
[11/10 02:28:11 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1560, average loss: 1.5308
[11/10 02:28:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.15	
[11/10 02:28:11 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/10 02:34:25 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.7024, average train loss: 0.9186
[11/10 02:35:08 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1562, average loss: 0.6984
[11/10 02:35:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 58.64	
[11/10 02:35:08 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/10 02:41:24 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.7320, average train loss: 0.9092
[11/10 02:42:06 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1562, average loss: 0.7654
[11/10 02:42:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.11	
[11/10 02:42:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/10 02:48:18 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.6107, average train loss: 0.8134
[11/10 02:49:00 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1567, average loss: 0.7783
[11/10 02:49:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.21	
[11/10 02:49:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/10 02:55:11 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.6018, average train loss: 0.8128
[11/10 02:55:54 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1561, average loss: 0.6920
[11/10 02:55:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 59.77	
[11/10 02:55:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/10 03:02:05 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5951, average train loss: 0.8227
[11/10 03:02:47 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1603, average loss: 1.2181
[11/10 03:02:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.21	
[11/10 03:02:47 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/10 03:08:58 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.6013, average train loss: 0.9879
[11/10 03:09:41 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1558, average loss: 0.6647
[11/10 03:09:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 63.97	
[11/10 03:09:41 visual_prompt]: Best epoch 13: best metric: -0.665
[11/10 03:09:41 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/10 03:15:52 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5972, average train loss: 0.9055
[11/10 03:16:34 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1574, average loss: 0.7470
[11/10 03:16:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.10	
[11/10 03:16:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/10 03:22:46 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6290, average train loss: 0.9364
[11/10 03:23:29 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1561, average loss: 0.7376
[11/10 03:23:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 64.47	
[11/10 03:23:29 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/10 03:29:42 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6652, average train loss: 0.8586
[11/10 03:30:25 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1561, average loss: 0.8393
[11/10 03:30:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.10	
[11/10 03:30:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/10 03:36:37 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6234, average train loss: 0.9024
[11/10 03:37:19 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1578, average loss: 1.4949
[11/10 03:37:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.81	
[11/10 03:37:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/10 03:43:30 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.6067, average train loss: 0.9396
[11/10 03:44:13 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1558, average loss: 0.6471
[11/10 03:44:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 67.72	
[11/10 03:44:13 visual_prompt]: Best epoch 18: best metric: -0.647
[11/10 03:44:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/10 03:50:26 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6649, average train loss: 0.7226
[11/10 03:51:09 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1575, average loss: 0.6440
[11/10 03:51:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 68.51	
[11/10 03:51:09 visual_prompt]: Best epoch 19: best metric: -0.644
[11/10 03:51:09 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/10 03:57:25 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.7202, average train loss: 0.6846
[11/10 03:58:08 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1575, average loss: 0.6398
[11/10 03:58:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 69.21	
[11/10 03:58:08 visual_prompt]: Best epoch 20: best metric: -0.640
[11/10 03:58:08 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[11/10 04:04:21 visual_prompt]: Epoch 21 / 100: avg data time: 1.03e+01, avg batch time: 10.6538, average train loss: 0.7334
[11/10 04:05:03 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1559, average loss: 0.6257
[11/10 04:05:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 72.01	
[11/10 04:05:03 visual_prompt]: Best epoch 21: best metric: -0.626
[11/10 04:05:03 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[11/10 04:11:14 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e+01, avg batch time: 10.6106, average train loss: 0.8954
[11/10 04:11:57 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1561, average loss: 0.8054
[11/10 04:11:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 71.37	
[11/10 04:11:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[11/10 04:18:08 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e+01, avg batch time: 10.5994, average train loss: 0.7275
[11/10 04:18:50 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1562, average loss: 0.6562
[11/10 04:18:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 70.01	
[11/10 04:18:50 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[11/10 04:25:01 visual_prompt]: Epoch 24 / 100: avg data time: 1.02e+01, avg batch time: 10.6008, average train loss: 0.7949
[11/10 04:25:44 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1561, average loss: 0.8498
[11/10 04:25:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 70.71	
[11/10 04:25:44 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[11/10 04:31:55 visual_prompt]: Epoch 25 / 100: avg data time: 1.02e+01, avg batch time: 10.6044, average train loss: 0.6597
[11/10 04:32:37 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1591, average loss: 0.6262
[11/10 04:32:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 72.23	
[11/10 04:32:37 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[11/10 04:38:48 visual_prompt]: Epoch 26 / 100: avg data time: 1.02e+01, avg batch time: 10.5922, average train loss: 0.6893
[11/10 04:39:31 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1561, average loss: 0.7597
[11/10 04:39:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 73.77	
[11/10 04:39:31 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[11/10 04:45:42 visual_prompt]: Epoch 27 / 100: avg data time: 1.03e+01, avg batch time: 10.6104, average train loss: 0.6735
[11/10 04:46:25 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1571, average loss: 0.6484
[11/10 04:46:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.90	
[11/10 04:46:25 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[11/10 04:52:39 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.7005, average train loss: 0.7117
[11/10 04:53:22 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1561, average loss: 0.8069
[11/10 04:53:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 71.71	
[11/10 04:53:22 visual_prompt]: Stopping early.
[11/10 04:53:22 visual_prompt]: Rank of current process: 0. World size: 1
[11/10 04:53:22 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/10 04:53:22 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/10 04:53:22 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/10 04:53:22 visual_prompt]: Training with config:
[11/10 04:53:22 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.5_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/10 04:53:22 visual_prompt]: Loading training data...
[11/10 04:53:22 visual_prompt]: Constructing mammo-cbis dataset train...
[11/10 04:53:22 visual_prompt]: Loading validation data...
[11/10 04:53:22 visual_prompt]: Constructing mammo-cbis dataset val...
[11/10 04:53:22 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/10 04:53:25 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/10 04:53:25 visual_prompt]: tuned percent:0.536
[11/10 04:53:25 visual_prompt]: Device used for model: 0
[11/10 04:53:25 visual_prompt]: Setting up Evaluator...
[11/10 04:53:25 visual_prompt]: Setting up Trainer...
[11/10 04:53:25 visual_prompt]: 	Setting up the optimizer...
[11/10 04:53:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/10 04:59:40 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.7094, average train loss: 1.4017
[11/10 05:00:22 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1582, average loss: 1.2969
[11/10 05:00:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/10 05:00:22 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/10 05:06:38 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.7187, average train loss: 1.8262
[11/10 05:07:20 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1601, average loss: 0.6901
[11/10 05:07:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.31	
[11/10 05:07:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/10 05:13:35 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.7026, average train loss: 0.7022
[11/10 05:14:18 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1562, average loss: 0.6911
[11/10 05:14:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.45	
[11/10 05:14:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/10 05:20:33 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.7098, average train loss: 0.6931
[11/10 05:21:16 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1560, average loss: 0.7120
[11/10 05:21:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.97	
[11/10 05:21:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/10 05:27:30 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6843, average train loss: 0.7299
[11/10 05:28:12 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1562, average loss: 0.7271
[11/10 05:28:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.46	
[11/10 05:28:12 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/10 05:34:27 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.7081, average train loss: 0.7423
[11/10 05:35:10 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1557, average loss: 0.7055
[11/10 05:35:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.62	
[11/10 05:35:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/10 05:41:26 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.7370, average train loss: 0.7393
[11/10 05:42:09 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1557, average loss: 0.7034
[11/10 05:42:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.71	
[11/10 05:42:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/10 05:48:24 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.7097, average train loss: 0.7093
[11/10 05:49:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1562, average loss: 0.6898
[11/10 05:49:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.58	
[11/10 05:49:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/10 05:55:22 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.7381, average train loss: 0.7092
[11/10 05:56:05 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1556, average loss: 0.7333
[11/10 05:56:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.82	
[11/10 05:56:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/10 06:02:20 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.7175, average train loss: 0.7067
[11/10 06:03:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1556, average loss: 0.7526
[11/10 06:03:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.47	
[11/10 06:03:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/10 06:09:17 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.6709, average train loss: 0.7159
[11/10 06:09:59 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1558, average loss: 0.6911
[11/10 06:09:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.18	
[11/10 06:09:59 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/10 06:16:10 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.6064, average train loss: 0.7329
[11/10 06:16:53 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1559, average loss: 0.7096
[11/10 06:16:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.71	
[11/10 06:16:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/10 06:23:05 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.6155, average train loss: 0.7421
[11/10 06:23:47 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1558, average loss: 0.7032
[11/10 06:23:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.23	
[11/10 06:23:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/10 06:29:58 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5888, average train loss: 0.7345
[11/10 06:30:40 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1556, average loss: 0.7341
[11/10 06:30:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.53	
[11/10 06:30:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/10 06:36:52 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6369, average train loss: 0.7460
[11/10 06:37:35 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1577, average loss: 0.7016
[11/10 06:37:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.98	
[11/10 06:37:35 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/10 06:43:48 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6597, average train loss: 0.7029
[11/10 06:44:31 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1582, average loss: 0.8806
[11/10 06:44:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.29	
[11/10 06:44:31 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/10 06:50:46 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.7097, average train loss: 0.7797
[11/10 06:51:29 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1569, average loss: 0.7726
[11/10 06:51:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.10	
[11/10 06:51:29 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/10 06:57:40 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.5899, average train loss: 0.7318
[11/10 06:58:22 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1562, average loss: 0.7669
[11/10 06:58:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.56	
[11/10 06:58:22 visual_prompt]: Stopping early.
[11/10 06:58:22 visual_prompt]: Rank of current process: 0. World size: 1
[11/10 06:58:22 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/10 06:58:22 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/10 06:58:22 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/10 06:58:22 visual_prompt]: Training with config:
[11/10 06:58:22 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.5_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/10 06:58:22 visual_prompt]: Loading training data...
[11/10 06:58:22 visual_prompt]: Constructing mammo-cbis dataset train...
[11/10 06:58:22 visual_prompt]: Loading validation data...
[11/10 06:58:22 visual_prompt]: Constructing mammo-cbis dataset val...
[11/10 06:58:22 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/10 06:58:25 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/10 06:58:25 visual_prompt]: tuned percent:0.536
[11/10 06:58:25 visual_prompt]: Device used for model: 0
[11/10 06:58:25 visual_prompt]: Setting up Evaluator...
[11/10 06:58:25 visual_prompt]: Setting up Trainer...
[11/10 06:58:25 visual_prompt]: 	Setting up the optimizer...
[11/10 06:58:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/10 07:04:35 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.5883, average train loss: 1.4017
[11/10 07:05:18 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1560, average loss: 1.2969
[11/10 07:05:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/10 07:05:18 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/10 07:11:28 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5890, average train loss: 1.8435
[11/10 07:12:11 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1562, average loss: 0.6906
[11/10 07:12:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[11/10 07:12:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/10 07:18:22 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.5989, average train loss: 0.7094
[11/10 07:19:04 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1573, average loss: 0.6943
[11/10 07:19:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.27	
[11/10 07:19:04 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/10 07:25:15 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.6108, average train loss: 0.7023
[11/10 07:25:58 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1561, average loss: 0.6986
[11/10 07:25:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.85	
[11/10 07:25:58 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/10 07:32:09 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.6082, average train loss: 0.7469
[11/10 07:32:52 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1560, average loss: 0.8048
[11/10 07:32:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.50	
[11/10 07:32:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/10 07:39:03 visual_prompt]: Epoch 6 / 100: avg data time: 1.02e+01, avg batch time: 10.6046, average train loss: 0.7441
[11/10 07:39:45 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1559, average loss: 0.7027
[11/10 07:39:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.57	
[11/10 07:39:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/10 07:45:57 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.6318, average train loss: 0.7830
[11/10 07:46:40 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1561, average loss: 0.7193
[11/10 07:46:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.11	
[11/10 07:46:40 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/10 07:52:51 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.6097, average train loss: 0.7786
[11/10 07:53:34 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1595, average loss: 0.6903
[11/10 07:53:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 58.96	
[11/10 07:53:34 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/10 07:59:46 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6437, average train loss: 0.7408
[11/10 08:00:29 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1593, average loss: 0.7273
[11/10 08:00:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.79	
[11/10 08:00:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/10 08:06:41 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.6244, average train loss: 0.6953
[11/10 08:07:23 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1562, average loss: 0.7790
[11/10 08:07:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.19	
[11/10 08:07:23 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/10 08:13:35 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.6147, average train loss: 0.7582
[11/10 08:14:17 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1560, average loss: 0.7374
[11/10 08:14:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.75	
[11/10 08:14:17 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/10 08:20:28 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5983, average train loss: 0.7390
[11/10 08:21:11 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1562, average loss: 0.6723
[11/10 08:21:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 64.70	
[11/10 08:21:11 visual_prompt]: Best epoch 12: best metric: -0.672
[11/10 08:21:11 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/10 08:27:21 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.5947, average train loss: 0.7260
[11/10 08:28:04 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1598, average loss: 0.7644
[11/10 08:28:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.44	
[11/10 08:28:04 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/10 08:34:15 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5898, average train loss: 0.7721
[11/10 08:34:57 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1600, average loss: 0.7254
[11/10 08:34:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.20	
[11/10 08:34:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/10 08:41:09 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.6165, average train loss: 0.7794
[11/10 08:41:51 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1559, average loss: 0.7724
[11/10 08:41:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.70	
[11/10 08:41:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/10 08:48:02 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.6071, average train loss: 0.7152
[11/10 08:48:45 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1581, average loss: 0.9232
[11/10 08:48:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.07	
[11/10 08:48:45 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/10 08:54:59 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6868, average train loss: 0.8351
[11/10 08:55:42 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1557, average loss: 0.7152
[11/10 08:55:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.95	
[11/10 08:55:42 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/10 09:01:54 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6248, average train loss: 0.7349
[11/10 09:02:36 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1599, average loss: 0.6877
[11/10 09:02:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.39	
[11/10 09:02:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[11/10 09:08:47 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.5887, average train loss: 0.6876
[11/10 09:09:29 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1574, average loss: 0.7146
[11/10 09:09:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.99	
[11/10 09:09:29 visual_prompt]: Stopping early.
[11/10 09:09:29 visual_prompt]: Rank of current process: 0. World size: 1
[11/10 09:09:29 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/10 09:09:29 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/10 09:09:29 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/10 09:09:29 visual_prompt]: Training with config:
[11/10 09:09:29 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.5_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/10 09:09:29 visual_prompt]: Loading training data...
[11/10 09:09:29 visual_prompt]: Constructing mammo-cbis dataset train...
[11/10 09:09:29 visual_prompt]: Loading validation data...
[11/10 09:09:29 visual_prompt]: Constructing mammo-cbis dataset val...
[11/10 09:09:29 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/10 09:09:32 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/10 09:09:32 visual_prompt]: tuned percent:0.536
[11/10 09:09:32 visual_prompt]: Device used for model: 0
[11/10 09:09:32 visual_prompt]: Setting up Evaluator...
[11/10 09:09:32 visual_prompt]: Setting up Trainer...
[11/10 09:09:32 visual_prompt]: 	Setting up the optimizer...
[11/10 09:09:32 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/10 09:15:44 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.6186, average train loss: 1.4017
[11/10 09:16:26 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1565, average loss: 1.2969
[11/10 09:16:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/10 09:16:26 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/10 09:22:38 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.6282, average train loss: 1.8453
[11/10 09:23:21 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1590, average loss: 0.6906
[11/10 09:23:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.38	
[11/10 09:23:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/10 09:29:36 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.7076, average train loss: 0.7104
[11/10 09:30:18 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1562, average loss: 0.6945
[11/10 09:30:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.29	
[11/10 09:30:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/10 09:36:33 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.7036, average train loss: 0.7037
[11/10 09:37:16 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1595, average loss: 0.6968
[11/10 09:37:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.67	
[11/10 09:37:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/10 09:43:30 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.6871, average train loss: 0.7500
[11/10 09:44:13 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1596, average loss: 0.8080
[11/10 09:44:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.01	
[11/10 09:44:13 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/10 09:50:29 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.7359, average train loss: 0.7462
[11/10 09:51:11 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1561, average loss: 0.7135
[11/10 09:51:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.93	
[11/10 09:51:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/10 09:57:27 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.7225, average train loss: 0.7995
[11/10 09:58:09 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1582, average loss: 0.7164
[11/10 09:58:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 59.40	
[11/10 09:58:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/10 10:04:24 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.6962, average train loss: 0.7933
[11/10 10:05:07 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1561, average loss: 0.7115
[11/10 10:05:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 59.75	
[11/10 10:05:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/10 10:11:22 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.7212, average train loss: 0.7332
[11/10 10:12:05 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1576, average loss: 0.6741
[11/10 10:12:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.36	
[11/10 10:12:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/10 10:18:20 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.7028, average train loss: 0.7151
[11/10 10:19:02 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1563, average loss: 0.6899
[11/10 10:19:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.15	
[11/10 10:19:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/10 10:25:17 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.6885, average train loss: 0.7315
[11/10 10:25:59 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1560, average loss: 0.7185
[11/10 10:25:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 62.13	
[11/10 10:25:59 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/10 10:32:13 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.6829, average train loss: 0.7214
[11/10 10:32:56 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1565, average loss: 0.6694
[11/10 10:32:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 64.88	
[11/10 10:32:56 visual_prompt]: Best epoch 12: best metric: -0.669
[11/10 10:32:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/10 10:39:10 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.6767, average train loss: 0.7061
[11/10 10:39:52 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1559, average loss: 0.6514
[11/10 10:39:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.07	
[11/10 10:39:52 visual_prompt]: Best epoch 13: best metric: -0.651
[11/10 10:39:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/10 10:46:04 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.6290, average train loss: 0.7379
[11/10 10:46:47 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1578, average loss: 0.6531
[11/10 10:46:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.36	
[11/10 10:46:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/10 10:53:03 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.7207, average train loss: 0.7558
[11/10 10:53:45 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1560, average loss: 0.8857
[11/10 10:53:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.32	
[11/10 10:53:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/10 10:59:59 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.6863, average train loss: 0.6961
[11/10 11:00:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1578, average loss: 1.1060
[11/10 11:00:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.64	
[11/10 11:00:42 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/10 11:06:56 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.6898, average train loss: 0.7790
[11/10 11:07:39 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1565, average loss: 0.8006
[11/10 11:07:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.61	
[11/10 11:07:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/10 11:13:54 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.6955, average train loss: 0.6780
[11/10 11:14:36 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1563, average loss: 0.7981
[11/10 11:14:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 71.70	
[11/10 11:14:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[11/10 11:20:51 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.6949, average train loss: 0.6478
[11/10 11:21:34 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1598, average loss: 0.6321
[11/10 11:21:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 70.75	
[11/10 11:21:34 visual_prompt]: Best epoch 19: best metric: -0.632
[11/10 11:21:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[11/10 11:27:48 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.7077, average train loss: 0.6462
[11/10 11:28:31 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1563, average loss: 0.6470
[11/10 11:28:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 71.01	
[11/10 11:28:31 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[11/10 11:34:46 visual_prompt]: Epoch 21 / 100: avg data time: 1.03e+01, avg batch time: 10.7034, average train loss: 0.6687
[11/10 11:35:29 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1561, average loss: 0.6271
[11/10 11:35:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 71.64	
[11/10 11:35:29 visual_prompt]: Best epoch 21: best metric: -0.627
[11/10 11:35:29 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[11/10 11:41:43 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e+01, avg batch time: 10.6936, average train loss: 0.7108
[11/10 11:42:26 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1565, average loss: 0.6187
[11/10 11:42:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 73.52	
[11/10 11:42:26 visual_prompt]: Best epoch 22: best metric: -0.619
[11/10 11:42:26 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[11/10 11:48:41 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 10.6935, average train loss: 0.6203
[11/10 11:49:23 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1563, average loss: 0.6928
[11/10 11:49:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 71.00	
[11/10 11:49:23 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[11/10 11:55:38 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e+01, avg batch time: 10.6999, average train loss: 0.6369
[11/10 11:56:21 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1583, average loss: 0.6413
[11/10 11:56:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 71.59	
[11/10 11:56:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[11/10 12:02:35 visual_prompt]: Epoch 25 / 100: avg data time: 1.03e+01, avg batch time: 10.6901, average train loss: 0.6043
[11/10 12:03:18 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1570, average loss: 0.6300
[11/10 12:03:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 72.91	
[11/10 12:03:18 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[11/10 12:09:32 visual_prompt]: Epoch 26 / 100: avg data time: 1.03e+01, avg batch time: 10.6871, average train loss: 0.6790
[11/10 12:10:15 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1562, average loss: 0.6311
[11/10 12:10:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 73.61	
[11/10 12:10:15 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[11/10 12:16:30 visual_prompt]: Epoch 27 / 100: avg data time: 1.04e+01, avg batch time: 10.7189, average train loss: 0.6128
[11/10 12:17:13 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1587, average loss: 0.6411
[11/10 12:17:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 69.82	
[11/10 12:17:13 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[11/10 12:23:27 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.6974, average train loss: 0.6404
[11/10 12:24:10 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1596, average loss: 0.6662
[11/10 12:24:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 69.54	
[11/10 12:24:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[11/10 12:30:26 visual_prompt]: Epoch 29 / 100: avg data time: 1.04e+01, avg batch time: 10.7358, average train loss: 0.6202
[11/10 12:31:09 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1561, average loss: 0.6265
[11/10 12:31:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 73.33	
[11/10 12:31:09 visual_prompt]: Stopping early.
[11/10 12:31:09 visual_prompt]: Rank of current process: 0. World size: 1
[11/10 12:31:09 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/10 12:31:09 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/10 12:31:09 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/10 12:31:09 visual_prompt]: Training with config:
[11/10 12:31:09 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.5_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/10 12:31:09 visual_prompt]: Loading training data...
[11/10 12:31:09 visual_prompt]: Constructing mammo-cbis dataset train...
[11/10 12:31:09 visual_prompt]: Loading validation data...
[11/10 12:31:09 visual_prompt]: Constructing mammo-cbis dataset val...
[11/10 12:31:09 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/10 12:31:12 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/10 12:31:12 visual_prompt]: tuned percent:0.536
[11/10 12:31:12 visual_prompt]: Device used for model: 0
[11/10 12:31:12 visual_prompt]: Setting up Evaluator...
[11/10 12:31:12 visual_prompt]: Setting up Trainer...
[11/10 12:31:12 visual_prompt]: 	Setting up the optimizer...
[11/10 12:31:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/10 12:37:27 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.7102, average train loss: 1.4017
[11/10 12:38:09 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1560, average loss: 1.2969
[11/10 12:38:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/10 12:38:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/10 12:44:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.5897, average train loss: 1.8455
[11/10 12:45:03 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1563, average loss: 0.6907
[11/10 12:45:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.20	
[11/10 12:45:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/10 12:51:14 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.6005, average train loss: 0.7105
[11/10 12:51:56 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1561, average loss: 0.6949
[11/10 12:51:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[11/10 12:51:56 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/10 12:58:07 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.5989, average train loss: 0.7042
[11/10 12:58:49 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1562, average loss: 0.6947
[11/10 12:58:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.88	
[11/10 12:58:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/10 13:05:00 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.5798, average train loss: 0.7514
[11/10 13:05:42 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1560, average loss: 0.8103
[11/10 13:05:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.07	
[11/10 13:05:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/10 13:11:55 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.6562, average train loss: 0.7477
[11/10 13:12:38 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1562, average loss: 0.7125
[11/10 13:12:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.89	
[11/10 13:12:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/10 13:18:49 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.6048, average train loss: 0.8003
[11/10 13:19:31 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1566, average loss: 0.7195
[11/10 13:19:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 59.23	
[11/10 13:19:31 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/10 13:25:47 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.7202, average train loss: 0.7903
[11/10 13:26:29 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1606, average loss: 0.7154
[11/10 13:26:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 59.36	
[11/10 13:26:29 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/10 13:32:41 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.6191, average train loss: 0.7305
[11/10 13:33:23 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1561, average loss: 0.6735
[11/10 13:33:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 60.91	
[11/10 13:33:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/10 13:39:34 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.5863, average train loss: 0.7154
[11/10 13:40:16 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1559, average loss: 0.6920
[11/10 13:40:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 61.12	
[11/10 13:40:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/10 13:46:27 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.5964, average train loss: 0.7353
[11/10 13:47:10 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1562, average loss: 0.7143
[11/10 13:47:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 61.93	
[11/10 13:47:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/10 13:53:20 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.5853, average train loss: 0.7209
[11/10 13:54:03 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1604, average loss: 0.6675
[11/10 13:54:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 64.67	
[11/10 13:54:03 visual_prompt]: Best epoch 12: best metric: -0.668
[11/10 13:54:03 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/10 14:00:14 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.6084, average train loss: 0.7065
[11/10 14:00:56 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1563, average loss: 0.6548
[11/10 14:00:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 66.39	
[11/10 14:00:56 visual_prompt]: Best epoch 13: best metric: -0.655
[11/10 14:00:56 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/10 14:07:07 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.5877, average train loss: 0.7370
[11/10 14:07:49 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1559, average loss: 0.6551
[11/10 14:07:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 67.85	
[11/10 14:07:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/10 14:14:01 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.6061, average train loss: 0.7552
[11/10 14:14:43 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1559, average loss: 0.8812
[11/10 14:14:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.71	
[11/10 14:14:43 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/10 14:20:54 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.5974, average train loss: 0.6956
[11/10 14:21:37 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1557, average loss: 1.0518
[11/10 14:21:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.12	
[11/10 14:21:37 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/10 14:27:53 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.7520, average train loss: 0.7537
[11/10 14:28:35 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1578, average loss: 0.7634
[11/10 14:28:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.23	
[11/10 14:28:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/10 14:34:46 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.5894, average train loss: 0.6820
[11/10 14:35:29 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1563, average loss: 0.8015
[11/10 14:35:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 71.46	
[11/10 14:35:29 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[11/10 14:41:39 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.5739, average train loss: 0.6552
[11/10 14:42:21 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1561, average loss: 0.6303
[11/10 14:42:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 71.41	
[11/10 14:42:21 visual_prompt]: Best epoch 19: best metric: -0.630
[11/10 14:42:21 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[11/10 14:48:41 visual_prompt]: Epoch 20 / 100: avg data time: 1.05e+01, avg batch time: 10.8391, average train loss: 0.6614
[11/10 14:49:43 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1564, average loss: 0.6415
[11/10 14:49:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 72.59	
[11/10 14:49:43 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[11/10 14:57:23 visual_prompt]: Epoch 21 / 100: avg data time: 1.28e+01, avg batch time: 13.1386, average train loss: 0.6946
[11/10 14:58:14 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1563, average loss: 0.6676
[11/10 14:58:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 72.62	
[11/10 14:58:14 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[11/10 15:05:40 visual_prompt]: Epoch 22 / 100: avg data time: 1.24e+01, avg batch time: 12.7451, average train loss: 0.7177
[11/10 15:06:31 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1597, average loss: 0.6442
[11/10 15:06:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 72.62	
[11/10 15:06:31 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[11/10 15:13:57 visual_prompt]: Epoch 23 / 100: avg data time: 1.24e+01, avg batch time: 12.7288, average train loss: 0.6219
[11/10 15:14:48 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1560, average loss: 0.7250
[11/10 15:14:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 71.95	
[11/10 15:14:48 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[11/10 15:22:15 visual_prompt]: Epoch 24 / 100: avg data time: 1.24e+01, avg batch time: 12.7771, average train loss: 0.6376
[11/10 15:23:06 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1591, average loss: 0.6360
[11/10 15:23:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 71.78	
[11/10 15:23:06 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[11/10 15:30:32 visual_prompt]: Epoch 25 / 100: avg data time: 1.24e+01, avg batch time: 12.7460, average train loss: 0.6107
[11/10 15:31:23 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1559, average loss: 0.6201
[11/10 15:31:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 73.01	
[11/10 15:31:23 visual_prompt]: Best epoch 25: best metric: -0.620
[11/10 15:31:23 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[11/10 15:38:13 visual_prompt]: Epoch 26 / 100: avg data time: 1.13e+01, avg batch time: 11.6923, average train loss: 0.6503
[11/10 15:38:55 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1592, average loss: 0.6205
[11/10 15:38:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 72.64	
[11/10 15:38:55 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[11/10 15:46:04 visual_prompt]: Epoch 27 / 100: avg data time: 1.19e+01, avg batch time: 12.2501, average train loss: 0.6140
[11/10 15:46:55 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1565, average loss: 0.6396
[11/10 15:46:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 70.72	
[11/10 15:46:55 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[11/10 15:54:23 visual_prompt]: Epoch 28 / 100: avg data time: 1.25e+01, avg batch time: 12.8155, average train loss: 0.6413
[11/10 15:55:16 visual_prompt]: Inference (val):avg data time: 5.72e-05, avg batch time: 0.1578, average loss: 0.6288
[11/10 15:55:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 71.39	
[11/10 15:55:16 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[11/10 16:02:47 visual_prompt]: Epoch 29 / 100: avg data time: 1.25e+01, avg batch time: 12.8552, average train loss: 0.5975
[11/10 16:03:42 visual_prompt]: Inference (val):avg data time: 5.22e-05, avg batch time: 0.1571, average loss: 0.6219
[11/10 16:03:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 72.24	
[11/10 16:03:42 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[11/10 16:11:43 visual_prompt]: Epoch 30 / 100: avg data time: 1.34e+01, avg batch time: 13.7477, average train loss: 0.6137
[11/10 16:12:38 visual_prompt]: Inference (val):avg data time: 9.03e-05, avg batch time: 0.1594, average loss: 0.6492
[11/10 16:12:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 71.88	
[11/10 16:12:38 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[11/10 16:20:18 visual_prompt]: Epoch 31 / 100: avg data time: 1.28e+01, avg batch time: 13.1362, average train loss: 0.5935
[11/10 16:21:09 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1562, average loss: 0.6954
[11/10 16:21:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 70.43	
[11/10 16:21:09 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[11/10 16:29:05 visual_prompt]: Epoch 32 / 100: avg data time: 1.32e+01, avg batch time: 13.6026, average train loss: 0.6072
[11/10 16:30:03 visual_prompt]: Inference (val):avg data time: 1.10e-04, avg batch time: 0.1570, average loss: 0.6059
[11/10 16:30:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.73	rocauc: 73.58	
[11/10 16:30:03 visual_prompt]: Best epoch 32: best metric: -0.606
[11/10 16:30:03 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[11/10 16:38:00 visual_prompt]: Epoch 33 / 100: avg data time: 1.33e+01, avg batch time: 13.6334, average train loss: 0.5505
[11/10 16:38:52 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1595, average loss: 0.6391
[11/10 16:38:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.53	
[11/10 16:38:52 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[11/10 16:45:26 visual_prompt]: Epoch 34 / 100: avg data time: 1.09e+01, avg batch time: 11.2362, average train loss: 0.5644
[11/10 16:46:09 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1561, average loss: 0.6228
[11/10 16:46:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 72.14	
[11/10 16:46:09 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[11/10 16:52:35 visual_prompt]: Epoch 35 / 100: avg data time: 1.07e+01, avg batch time: 11.0249, average train loss: 0.6069
[11/10 16:53:19 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1563, average loss: 0.9533
[11/10 16:53:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 71.73	
[11/10 16:53:19 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[11/10 16:59:51 visual_prompt]: Epoch 36 / 100: avg data time: 1.08e+01, avg batch time: 11.1922, average train loss: 0.6015
[11/10 17:00:36 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1578, average loss: 0.7839
[11/10 17:00:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 70.70	
[11/10 17:00:36 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[11/10 17:07:25 visual_prompt]: Epoch 37 / 100: avg data time: 1.13e+01, avg batch time: 11.6799, average train loss: 0.5496
[11/10 17:08:12 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1562, average loss: 0.6397
[11/10 17:08:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 72.72	
[11/10 17:08:12 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[11/10 17:15:10 visual_prompt]: Epoch 38 / 100: avg data time: 1.16e+01, avg batch time: 11.9366, average train loss: 0.5286
[11/10 17:15:57 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1563, average loss: 0.6540
[11/10 17:15:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 69.63	
[11/10 17:15:57 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[11/10 17:22:55 visual_prompt]: Epoch 39 / 100: avg data time: 1.16e+01, avg batch time: 11.9411, average train loss: 0.5423
[11/10 17:23:42 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1562, average loss: 0.8090
[11/10 17:23:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 70.83	
[11/10 17:23:42 visual_prompt]: Stopping early.
[11/10 17:23:43 visual_prompt]: Rank of current process: 0. World size: 1
[11/10 17:23:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/10 17:23:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/10 17:23:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/10 17:23:43 visual_prompt]: Training with config:
[11/10 17:23:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.25_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/10 17:23:43 visual_prompt]: Loading training data...
[11/10 17:23:43 visual_prompt]: Constructing mammo-cbis dataset train...
[11/10 17:23:43 visual_prompt]: Loading validation data...
[11/10 17:23:43 visual_prompt]: Constructing mammo-cbis dataset val...
[11/10 17:23:43 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/10 17:23:52 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/10 17:23:52 visual_prompt]: tuned percent:0.536
[11/10 17:23:52 visual_prompt]: Device used for model: 0
[11/10 17:23:52 visual_prompt]: Setting up Evaluator...
[11/10 17:23:52 visual_prompt]: Setting up Trainer...
[11/10 17:23:52 visual_prompt]: 	Setting up the optimizer...
[11/10 17:23:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/10 17:30:51 visual_prompt]: Epoch 1 / 100: avg data time: 1.16e+01, avg batch time: 11.9564, average train loss: 1.4017
[11/10 17:31:39 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1561, average loss: 1.2969
[11/10 17:31:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/10 17:31:39 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/10 17:38:40 visual_prompt]: Epoch 2 / 100: avg data time: 1.17e+01, avg batch time: 12.0342, average train loss: 1.4183
[11/10 17:39:28 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1564, average loss: 0.6912
[11/10 17:39:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 52.08	
[11/10 17:39:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/10 17:46:26 visual_prompt]: Epoch 3 / 100: avg data time: 1.16e+01, avg batch time: 11.9312, average train loss: 0.7009
[11/10 17:47:13 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1567, average loss: 0.6877
[11/10 17:47:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.22	
[11/10 17:47:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/10 17:54:06 visual_prompt]: Epoch 4 / 100: avg data time: 1.14e+01, avg batch time: 11.7897, average train loss: 0.6893
[11/10 17:54:51 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1571, average loss: 0.6925
[11/10 17:54:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.51	
[11/10 17:54:51 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/10 18:01:34 visual_prompt]: Epoch 5 / 100: avg data time: 1.11e+01, avg batch time: 11.5054, average train loss: 0.7201
[11/10 18:02:19 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1560, average loss: 0.6930
[11/10 18:02:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.40	
[11/10 18:02:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/10 18:09:02 visual_prompt]: Epoch 6 / 100: avg data time: 1.11e+01, avg batch time: 11.4964, average train loss: 0.7055
[11/10 18:09:48 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1573, average loss: 0.6884
[11/10 18:09:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.22	
[11/10 18:09:48 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/10 18:16:30 visual_prompt]: Epoch 7 / 100: avg data time: 1.11e+01, avg batch time: 11.4788, average train loss: 0.7063
[11/10 18:17:16 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1561, average loss: 0.6932
[11/10 18:17:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 58.12	
[11/10 18:17:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/10 18:24:01 visual_prompt]: Epoch 8 / 100: avg data time: 1.12e+01, avg batch time: 11.5849, average train loss: 0.7053
[11/10 18:24:47 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1563, average loss: 0.6867
[11/10 18:24:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.77	
[11/10 18:24:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/10 18:31:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.12e+01, avg batch time: 11.5924, average train loss: 0.7017
[11/10 18:32:23 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1561, average loss: 0.6942
[11/10 18:32:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.14	
[11/10 18:32:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/10 18:39:07 visual_prompt]: Epoch 10 / 100: avg data time: 1.12e+01, avg batch time: 11.5518, average train loss: 0.7012
[11/10 18:39:54 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1560, average loss: 0.7062
[11/10 18:39:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.14	
[11/10 18:39:54 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/10 18:46:36 visual_prompt]: Epoch 11 / 100: avg data time: 1.11e+01, avg batch time: 11.4812, average train loss: 0.6958
[11/10 18:47:21 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1560, average loss: 0.6900
[11/10 18:47:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.25	
[11/10 18:47:21 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/10 18:54:02 visual_prompt]: Epoch 12 / 100: avg data time: 1.11e+01, avg batch time: 11.4575, average train loss: 0.7014
[11/10 18:54:48 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1579, average loss: 0.6881
[11/10 18:54:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.57	
[11/10 18:54:48 visual_prompt]: Best epoch 12: best metric: -0.688
[11/10 18:54:48 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/10 19:01:30 visual_prompt]: Epoch 13 / 100: avg data time: 1.11e+01, avg batch time: 11.4784, average train loss: 0.7115
[11/10 19:02:16 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1559, average loss: 0.6885
[11/10 19:02:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.89	
[11/10 19:02:16 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/10 19:08:58 visual_prompt]: Epoch 14 / 100: avg data time: 1.11e+01, avg batch time: 11.4833, average train loss: 0.7062
[11/10 19:09:44 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1561, average loss: 0.8048
[11/10 19:09:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.71	
[11/10 19:09:44 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/10 19:16:27 visual_prompt]: Epoch 15 / 100: avg data time: 1.11e+01, avg batch time: 11.4959, average train loss: 0.7092
[11/10 19:17:13 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1557, average loss: 0.6886
[11/10 19:17:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.49	
[11/10 19:17:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/10 19:23:52 visual_prompt]: Epoch 16 / 100: avg data time: 1.10e+01, avg batch time: 11.3984, average train loss: 0.7068
[11/10 19:24:39 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1579, average loss: 0.6885
[11/10 19:24:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.77	
[11/10 19:24:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/10 19:31:38 visual_prompt]: Epoch 17 / 100: avg data time: 1.16e+01, avg batch time: 11.9647, average train loss: 0.7087
[11/10 19:32:26 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1558, average loss: 0.6925
[11/10 19:32:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.84	
[11/10 19:32:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/10 19:39:25 visual_prompt]: Epoch 18 / 100: avg data time: 1.16e+01, avg batch time: 11.9696, average train loss: 0.7181
[11/10 19:40:13 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1558, average loss: 0.7962
[11/10 19:40:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.20	
[11/10 19:40:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/10 19:47:10 visual_prompt]: Epoch 19 / 100: avg data time: 1.16e+01, avg batch time: 11.9211, average train loss: 0.7125
[11/10 19:47:58 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1564, average loss: 0.7443
[11/10 19:47:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.07	
[11/10 19:47:58 visual_prompt]: Stopping early.
[11/10 19:47:58 visual_prompt]: Rank of current process: 0. World size: 1
[11/10 19:47:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/10 19:47:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/10 19:47:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/10 19:47:58 visual_prompt]: Training with config:
[11/10 19:47:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.25_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/10 19:47:58 visual_prompt]: Loading training data...
[11/10 19:47:58 visual_prompt]: Constructing mammo-cbis dataset train...
[11/10 19:47:58 visual_prompt]: Loading validation data...
[11/10 19:47:58 visual_prompt]: Constructing mammo-cbis dataset val...
[11/10 19:47:58 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/10 19:48:01 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/10 19:48:01 visual_prompt]: tuned percent:0.536
[11/10 19:48:01 visual_prompt]: Device used for model: 0
[11/10 19:48:01 visual_prompt]: Setting up Evaluator...
[11/10 19:48:01 visual_prompt]: Setting up Trainer...
[11/10 19:48:01 visual_prompt]: 	Setting up the optimizer...
[11/10 19:48:01 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/10 19:54:48 visual_prompt]: Epoch 1 / 100: avg data time: 1.13e+01, avg batch time: 11.6293, average train loss: 1.4017
[11/10 19:55:35 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1560, average loss: 1.2969
[11/10 19:55:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/10 19:55:35 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/10 20:02:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.12e+01, avg batch time: 11.5890, average train loss: 1.4235
[11/10 20:03:06 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1558, average loss: 0.6898
[11/10 20:03:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 52.09	
[11/10 20:03:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/10 20:09:51 visual_prompt]: Epoch 3 / 100: avg data time: 1.12e+01, avg batch time: 11.5536, average train loss: 0.7048
[11/10 20:10:37 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1555, average loss: 0.6906
[11/10 20:10:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.99	
[11/10 20:10:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/10 20:17:22 visual_prompt]: Epoch 4 / 100: avg data time: 1.12e+01, avg batch time: 11.5521, average train loss: 0.6935
[11/10 20:18:08 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1593, average loss: 0.6935
[11/10 20:18:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.71	
[11/10 20:18:08 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/10 20:24:51 visual_prompt]: Epoch 5 / 100: avg data time: 1.12e+01, avg batch time: 11.5314, average train loss: 0.7290
[11/10 20:25:38 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1558, average loss: 0.7004
[11/10 20:25:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 60.60	
[11/10 20:25:38 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/10 20:32:22 visual_prompt]: Epoch 6 / 100: avg data time: 1.12e+01, avg batch time: 11.5612, average train loss: 0.7546
[11/10 20:33:08 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1559, average loss: 0.6821
[11/10 20:33:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.84	
[11/10 20:33:08 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/10 20:39:50 visual_prompt]: Epoch 7 / 100: avg data time: 1.11e+01, avg batch time: 11.4643, average train loss: 0.7174
[11/10 20:40:36 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1558, average loss: 0.6708
[11/10 20:40:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 61.88	
[11/10 20:40:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/10 20:47:49 visual_prompt]: Epoch 8 / 100: avg data time: 1.20e+01, avg batch time: 12.3802, average train loss: 0.7078
[11/10 20:48:39 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1580, average loss: 0.6659
[11/10 20:48:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 62.19	
[11/10 20:48:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/10 20:55:57 visual_prompt]: Epoch 9 / 100: avg data time: 1.22e+01, avg batch time: 12.5251, average train loss: 0.6920
[11/10 20:56:47 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1561, average loss: 0.7073
[11/10 20:56:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 64.51	
[11/10 20:56:47 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/10 21:04:03 visual_prompt]: Epoch 10 / 100: avg data time: 1.21e+01, avg batch time: 12.4622, average train loss: 0.6650
[11/10 21:04:53 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1588, average loss: 0.6646
[11/10 21:04:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.38	
[11/10 21:04:53 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/10 21:12:10 visual_prompt]: Epoch 11 / 100: avg data time: 1.21e+01, avg batch time: 12.4753, average train loss: 0.6847
[11/10 21:13:00 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1573, average loss: 0.6636
[11/10 21:13:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 65.98	
[11/10 21:13:00 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/10 21:20:15 visual_prompt]: Epoch 12 / 100: avg data time: 1.21e+01, avg batch time: 12.4467, average train loss: 0.6734
[11/10 21:21:05 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1560, average loss: 0.6851
[11/10 21:21:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 70.45	
[11/10 21:21:05 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/10 21:28:09 visual_prompt]: Epoch 13 / 100: avg data time: 1.17e+01, avg batch time: 12.1040, average train loss: 0.7218
[11/10 21:28:56 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1563, average loss: 0.6889
[11/10 21:28:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 68.50	
[11/10 21:28:56 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/10 21:35:44 visual_prompt]: Epoch 14 / 100: avg data time: 1.13e+01, avg batch time: 11.6541, average train loss: 0.6693
[11/10 21:36:30 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1557, average loss: 0.6953
[11/10 21:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 70.00	
[11/10 21:36:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/10 21:43:19 visual_prompt]: Epoch 15 / 100: avg data time: 1.13e+01, avg batch time: 11.6875, average train loss: 0.6743
[11/10 21:44:06 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1560, average loss: 0.7851
[11/10 21:44:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.01	
[11/10 21:44:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/10 21:50:57 visual_prompt]: Epoch 16 / 100: avg data time: 1.14e+01, avg batch time: 11.7407, average train loss: 0.6836
[11/10 21:51:44 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1563, average loss: 0.7908
[11/10 21:51:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 72.90	
[11/10 21:51:44 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/10 21:58:36 visual_prompt]: Epoch 17 / 100: avg data time: 1.14e+01, avg batch time: 11.7597, average train loss: 0.7032
[11/10 21:59:23 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1562, average loss: 0.6382
[11/10 21:59:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 70.62	
[11/10 21:59:23 visual_prompt]: Best epoch 17: best metric: -0.638
[11/10 21:59:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/10 22:06:13 visual_prompt]: Epoch 18 / 100: avg data time: 1.13e+01, avg batch time: 11.7057, average train loss: 0.6304
[11/10 22:07:00 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1563, average loss: 0.7878
[11/10 22:07:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 72.66	
[11/10 22:07:00 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/10 22:13:48 visual_prompt]: Epoch 19 / 100: avg data time: 1.13e+01, avg batch time: 11.6773, average train loss: 0.6323
[11/10 22:14:35 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1561, average loss: 0.6691
[11/10 22:14:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 71.86	
[11/10 22:14:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[11/10 22:21:26 visual_prompt]: Epoch 20 / 100: avg data time: 1.14e+01, avg batch time: 11.7257, average train loss: 0.6399
[11/10 22:22:13 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1571, average loss: 0.6589
[11/10 22:22:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 70.17	
[11/10 22:22:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[11/10 22:29:03 visual_prompt]: Epoch 21 / 100: avg data time: 1.14e+01, avg batch time: 11.7317, average train loss: 0.6507
[11/10 22:29:51 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1580, average loss: 0.6301
[11/10 22:29:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 71.47	
[11/10 22:29:51 visual_prompt]: Best epoch 21: best metric: -0.630
[11/10 22:29:51 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[11/10 22:36:41 visual_prompt]: Epoch 22 / 100: avg data time: 1.14e+01, avg batch time: 11.7362, average train loss: 0.6614
[11/10 22:37:29 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1560, average loss: 0.6326
[11/10 22:37:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 72.04	
[11/10 22:37:29 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[11/10 22:44:19 visual_prompt]: Epoch 23 / 100: avg data time: 1.14e+01, avg batch time: 11.7129, average train loss: 0.6358
[11/10 22:45:06 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1565, average loss: 0.6755
[11/10 22:45:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 68.23	
[11/10 22:45:06 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[11/10 22:51:57 visual_prompt]: Epoch 24 / 100: avg data time: 1.14e+01, avg batch time: 11.7472, average train loss: 0.6579
[11/10 22:52:44 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1568, average loss: 0.6712
[11/10 22:52:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 70.73	
[11/10 22:52:44 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[11/10 22:59:30 visual_prompt]: Epoch 25 / 100: avg data time: 1.12e+01, avg batch time: 11.5829, average train loss: 0.6274
[11/10 23:00:13 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1568, average loss: 0.6264
[11/10 23:00:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 71.60	
[11/10 23:00:13 visual_prompt]: Best epoch 25: best metric: -0.626
[11/10 23:00:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[11/10 23:06:28 visual_prompt]: Epoch 26 / 100: avg data time: 1.03e+01, avg batch time: 10.6919, average train loss: 0.6534
[11/10 23:07:10 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1580, average loss: 0.6683
[11/10 23:07:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 69.08	
[11/10 23:07:10 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[11/10 23:13:25 visual_prompt]: Epoch 27 / 100: avg data time: 1.03e+01, avg batch time: 10.6945, average train loss: 0.6346
[11/10 23:14:08 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1555, average loss: 0.6668
[11/10 23:14:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 70.03	
[11/10 23:14:08 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[11/10 23:20:23 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.7036, average train loss: 0.6432
[11/10 23:21:05 visual_prompt]: Inference (val):avg data time: 1.89e-05, avg batch time: 0.1559, average loss: 0.6234
[11/10 23:21:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 70.75	
[11/10 23:21:05 visual_prompt]: Best epoch 28: best metric: -0.623
[11/10 23:21:05 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[11/10 23:27:20 visual_prompt]: Epoch 29 / 100: avg data time: 1.04e+01, avg batch time: 10.7075, average train loss: 0.6084
[11/10 23:28:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1559, average loss: 0.6089
[11/10 23:28:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 73.06	
[11/10 23:28:03 visual_prompt]: Best epoch 29: best metric: -0.609
[11/10 23:28:03 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[11/10 23:34:30 visual_prompt]: Epoch 30 / 100: avg data time: 1.07e+01, avg batch time: 11.0744, average train loss: 0.6443
[11/10 23:35:18 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1565, average loss: 0.7532
[11/10 23:35:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 72.17	
[11/10 23:35:18 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[11/10 23:42:15 visual_prompt]: Epoch 31 / 100: avg data time: 1.16e+01, avg batch time: 11.9106, average train loss: 0.6261
[11/10 23:43:02 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1558, average loss: 0.6395
[11/10 23:43:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 74.16	
[11/10 23:43:02 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[11/10 23:49:58 visual_prompt]: Epoch 32 / 100: avg data time: 1.15e+01, avg batch time: 11.8740, average train loss: 0.6122
[11/10 23:50:46 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1562, average loss: 0.6150
[11/10 23:50:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 73.71	
[11/10 23:50:46 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[11/10 23:57:43 visual_prompt]: Epoch 33 / 100: avg data time: 1.16e+01, avg batch time: 11.9290, average train loss: 0.6088
[11/10 23:58:31 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1562, average loss: 0.6055
[11/10 23:58:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 73.65	
[11/10 23:58:31 visual_prompt]: Best epoch 33: best metric: -0.605
[11/10 23:58:31 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[11/11 00:05:25 visual_prompt]: Epoch 34 / 100: avg data time: 1.15e+01, avg batch time: 11.8419, average train loss: 0.5969
[11/11 00:06:13 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1564, average loss: 0.6675
[11/11 00:06:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 71.05	
[11/11 00:06:13 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[11/11 00:13:08 visual_prompt]: Epoch 35 / 100: avg data time: 1.15e+01, avg batch time: 11.8468, average train loss: 0.6063
[11/11 00:13:55 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1558, average loss: 0.6949
[11/11 00:13:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 72.10	
[11/11 00:13:55 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[11/11 00:20:47 visual_prompt]: Epoch 36 / 100: avg data time: 1.14e+01, avg batch time: 11.7842, average train loss: 0.6131
[11/11 00:21:34 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1569, average loss: 0.6433
[11/11 00:21:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 71.85	
[11/11 00:21:34 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[11/11 00:28:25 visual_prompt]: Epoch 37 / 100: avg data time: 1.14e+01, avg batch time: 11.7358, average train loss: 0.5746
[11/11 00:29:12 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1562, average loss: 0.6824
[11/11 00:29:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 70.59	
[11/11 00:29:12 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[11/11 00:36:03 visual_prompt]: Epoch 38 / 100: avg data time: 1.14e+01, avg batch time: 11.7434, average train loss: 0.5860
[11/11 00:36:50 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1561, average loss: 0.6254
[11/11 00:36:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 70.98	
[11/11 00:36:50 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[11/11 00:43:43 visual_prompt]: Epoch 39 / 100: avg data time: 1.14e+01, avg batch time: 11.7883, average train loss: 0.6084
[11/11 00:44:30 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1561, average loss: 0.6735
[11/11 00:44:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 72.70	
[11/11 00:44:30 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[11/11 00:51:21 visual_prompt]: Epoch 40 / 100: avg data time: 1.14e+01, avg batch time: 11.7391, average train loss: 0.5740
[11/11 00:52:07 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1575, average loss: 0.6229
[11/11 00:52:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 71.57	
[11/11 00:52:07 visual_prompt]: Stopping early.
[11/11 00:52:07 visual_prompt]: Rank of current process: 0. World size: 1
[11/11 00:52:07 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/11 00:52:07 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/11 00:52:07 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/11 00:52:07 visual_prompt]: Training with config:
[11/11 00:52:07 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.25_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/11 00:52:07 visual_prompt]: Loading training data...
[11/11 00:52:07 visual_prompt]: Constructing mammo-cbis dataset train...
[11/11 00:52:07 visual_prompt]: Loading validation data...
[11/11 00:52:07 visual_prompt]: Constructing mammo-cbis dataset val...
[11/11 00:52:08 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/11 00:52:10 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/11 00:52:10 visual_prompt]: tuned percent:0.536
[11/11 00:52:10 visual_prompt]: Device used for model: 0
[11/11 00:52:10 visual_prompt]: Setting up Evaluator...
[11/11 00:52:10 visual_prompt]: Setting up Trainer...
[11/11 00:52:10 visual_prompt]: 	Setting up the optimizer...
[11/11 00:52:10 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/11 00:59:00 visual_prompt]: Epoch 1 / 100: avg data time: 1.14e+01, avg batch time: 11.7114, average train loss: 1.4017
[11/11 00:59:47 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1561, average loss: 1.2969
[11/11 00:59:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/11 00:59:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/11 01:06:37 visual_prompt]: Epoch 2 / 100: avg data time: 1.14e+01, avg batch time: 11.7149, average train loss: 1.4240
[11/11 01:07:24 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1567, average loss: 0.6896
[11/11 01:07:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 52.12	
[11/11 01:07:24 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/11 01:14:13 visual_prompt]: Epoch 3 / 100: avg data time: 1.13e+01, avg batch time: 11.6943, average train loss: 0.7053
[11/11 01:15:00 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1559, average loss: 0.6911
[11/11 01:15:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.93	
[11/11 01:15:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/11 01:21:48 visual_prompt]: Epoch 4 / 100: avg data time: 1.13e+01, avg batch time: 11.6536, average train loss: 0.6941
[11/11 01:22:34 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1560, average loss: 0.6937
[11/11 01:22:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.80	
[11/11 01:22:34 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/11 01:29:37 visual_prompt]: Epoch 5 / 100: avg data time: 1.17e+01, avg batch time: 12.0698, average train loss: 0.7293
[11/11 01:30:26 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.1562, average loss: 0.7034
[11/11 01:30:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 58.65	
[11/11 01:30:26 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/11 01:37:28 visual_prompt]: Epoch 6 / 100: avg data time: 1.17e+01, avg batch time: 12.0601, average train loss: 0.7478
[11/11 01:38:16 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1560, average loss: 0.6728
[11/11 01:38:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.69	
[11/11 01:38:16 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/11 01:45:16 visual_prompt]: Epoch 7 / 100: avg data time: 1.16e+01, avg batch time: 11.9955, average train loss: 0.7139
[11/11 01:46:03 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1584, average loss: 0.6679
[11/11 01:46:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 62.05	
[11/11 01:46:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/11 01:52:59 visual_prompt]: Epoch 8 / 100: avg data time: 1.15e+01, avg batch time: 11.8656, average train loss: 0.7062
[11/11 01:53:46 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1562, average loss: 0.6654
[11/11 01:53:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 61.99	
[11/11 01:53:46 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/11 02:00:41 visual_prompt]: Epoch 9 / 100: avg data time: 1.15e+01, avg batch time: 11.8470, average train loss: 0.6886
[11/11 02:01:28 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1559, average loss: 0.6923
[11/11 02:01:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 64.92	
[11/11 02:01:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/11 02:08:22 visual_prompt]: Epoch 10 / 100: avg data time: 1.15e+01, avg batch time: 11.8104, average train loss: 0.6645
[11/11 02:09:09 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1560, average loss: 0.6487
[11/11 02:09:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.76	
[11/11 02:09:09 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/11 02:16:01 visual_prompt]: Epoch 11 / 100: avg data time: 1.14e+01, avg batch time: 11.7833, average train loss: 0.6771
[11/11 02:16:48 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1601, average loss: 0.6599
[11/11 02:16:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 69.56	
[11/11 02:16:48 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/11 02:23:41 visual_prompt]: Epoch 12 / 100: avg data time: 1.14e+01, avg batch time: 11.7937, average train loss: 0.6733
[11/11 02:24:28 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1602, average loss: 0.7517
[11/11 02:24:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 70.07	
[11/11 02:24:28 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/11 02:31:22 visual_prompt]: Epoch 13 / 100: avg data time: 1.15e+01, avg batch time: 11.8114, average train loss: 0.7598
[11/11 02:32:09 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1560, average loss: 0.6845
[11/11 02:32:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 67.20	
[11/11 02:32:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/11 02:39:01 visual_prompt]: Epoch 14 / 100: avg data time: 1.14e+01, avg batch time: 11.7802, average train loss: 0.6848
[11/11 02:39:48 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1576, average loss: 0.7344
[11/11 02:39:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 70.05	
[11/11 02:39:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/11 02:46:42 visual_prompt]: Epoch 15 / 100: avg data time: 1.15e+01, avg batch time: 11.8242, average train loss: 0.6973
[11/11 02:47:29 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1559, average loss: 0.8251
[11/11 02:47:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.00	
[11/11 02:47:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/11 02:54:22 visual_prompt]: Epoch 16 / 100: avg data time: 1.14e+01, avg batch time: 11.7876, average train loss: 0.7038
[11/11 02:55:09 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1560, average loss: 0.7210
[11/11 02:55:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 70.83	
[11/11 02:55:09 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/11 03:02:01 visual_prompt]: Epoch 17 / 100: avg data time: 1.14e+01, avg batch time: 11.7543, average train loss: 0.6889
[11/11 03:02:49 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1561, average loss: 0.6296
[11/11 03:02:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.48	
[11/11 03:02:49 visual_prompt]: Best epoch 17: best metric: -0.630
[11/11 03:02:49 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/11 03:09:55 visual_prompt]: Epoch 18 / 100: avg data time: 1.18e+01, avg batch time: 12.1593, average train loss: 0.6597
[11/11 03:10:44 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1563, average loss: 0.6428
[11/11 03:10:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 69.73	
[11/11 03:10:44 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/11 03:17:46 visual_prompt]: Epoch 19 / 100: avg data time: 1.17e+01, avg batch time: 12.0535, average train loss: 0.6638
[11/11 03:18:34 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1561, average loss: 0.6310
[11/11 03:18:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 70.41	
[11/11 03:18:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[11/11 03:25:33 visual_prompt]: Epoch 20 / 100: avg data time: 1.16e+01, avg batch time: 11.9720, average train loss: 0.6083
[11/11 03:26:21 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1592, average loss: 0.6869
[11/11 03:26:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 70.59	
[11/11 03:26:21 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[11/11 03:33:18 visual_prompt]: Epoch 21 / 100: avg data time: 1.16e+01, avg batch time: 11.9094, average train loss: 0.6039
[11/11 03:34:05 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1561, average loss: 0.6265
[11/11 03:34:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.92	rocauc: 71.36	
[11/11 03:34:05 visual_prompt]: Best epoch 21: best metric: -0.627
[11/11 03:34:05 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[11/11 03:40:59 visual_prompt]: Epoch 22 / 100: avg data time: 1.15e+01, avg batch time: 11.8229, average train loss: 0.5869
[11/11 03:41:46 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1559, average loss: 0.6334
[11/11 03:41:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 69.79	
[11/11 03:41:46 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[11/11 03:48:40 visual_prompt]: Epoch 23 / 100: avg data time: 1.15e+01, avg batch time: 11.8130, average train loss: 0.5979
[11/11 03:49:27 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1561, average loss: 0.6341
[11/11 03:49:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 70.16	
[11/11 03:49:27 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[11/11 03:56:20 visual_prompt]: Epoch 24 / 100: avg data time: 1.14e+01, avg batch time: 11.8044, average train loss: 0.6274
[11/11 03:57:07 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1576, average loss: 0.7203
[11/11 03:57:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 70.25	
[11/11 03:57:07 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[11/11 04:04:00 visual_prompt]: Epoch 25 / 100: avg data time: 1.14e+01, avg batch time: 11.7921, average train loss: 0.5901
[11/11 04:04:47 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1563, average loss: 0.6145
[11/11 04:04:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 72.58	
[11/11 04:04:47 visual_prompt]: Best epoch 25: best metric: -0.615
[11/11 04:04:47 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[11/11 04:11:40 visual_prompt]: Epoch 26 / 100: avg data time: 1.14e+01, avg batch time: 11.7931, average train loss: 0.6197
[11/11 04:12:27 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1562, average loss: 0.6446
[11/11 04:12:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 72.05	
[11/11 04:12:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[11/11 04:19:20 visual_prompt]: Epoch 27 / 100: avg data time: 1.14e+01, avg batch time: 11.7926, average train loss: 0.5683
[11/11 04:20:07 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1561, average loss: 0.6487
[11/11 04:20:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.55	
[11/11 04:20:07 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[11/11 04:27:00 visual_prompt]: Epoch 28 / 100: avg data time: 1.14e+01, avg batch time: 11.7821, average train loss: 0.5906
[11/11 04:27:47 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1562, average loss: 0.6432
[11/11 04:27:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 70.02	
[11/11 04:27:47 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[11/11 04:34:41 visual_prompt]: Epoch 29 / 100: avg data time: 1.15e+01, avg batch time: 11.8131, average train loss: 0.5563
[11/11 04:35:28 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1600, average loss: 0.6407
[11/11 04:35:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 72.64	
[11/11 04:35:28 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[11/11 04:42:32 visual_prompt]: Epoch 30 / 100: avg data time: 1.18e+01, avg batch time: 12.1076, average train loss: 0.5335
[11/11 04:43:20 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1566, average loss: 0.6586
[11/11 04:43:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 73.16	
[11/11 04:43:20 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[11/11 04:50:24 visual_prompt]: Epoch 31 / 100: avg data time: 1.18e+01, avg batch time: 12.1126, average train loss: 0.5178
[11/11 04:51:12 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1566, average loss: 0.6539
[11/11 04:51:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.83	
[11/11 04:51:12 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[11/11 04:58:14 visual_prompt]: Epoch 32 / 100: avg data time: 1.17e+01, avg batch time: 12.0389, average train loss: 0.5555
[11/11 04:59:02 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.1561, average loss: 0.6706
[11/11 04:59:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 70.54	
[11/11 04:59:02 visual_prompt]: Stopping early.
[11/11 04:59:03 visual_prompt]: Rank of current process: 0. World size: 1
[11/11 04:59:03 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/11 04:59:03 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/11 04:59:03 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/11 04:59:03 visual_prompt]: Training with config:
[11/11 04:59:03 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.25_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/11 04:59:03 visual_prompt]: Loading training data...
[11/11 04:59:03 visual_prompt]: Constructing mammo-cbis dataset train...
[11/11 04:59:04 visual_prompt]: Loading validation data...
[11/11 04:59:04 visual_prompt]: Constructing mammo-cbis dataset val...
[11/11 04:59:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/11 04:59:11 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/11 04:59:11 visual_prompt]: tuned percent:0.536
[11/11 04:59:11 visual_prompt]: Device used for model: 0
[11/11 04:59:11 visual_prompt]: Setting up Evaluator...
[11/11 04:59:11 visual_prompt]: Setting up Trainer...
[11/11 04:59:11 visual_prompt]: 	Setting up the optimizer...
[11/11 04:59:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/11 05:06:09 visual_prompt]: Epoch 1 / 100: avg data time: 1.16e+01, avg batch time: 11.9236, average train loss: 1.4017
[11/11 05:06:56 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1563, average loss: 1.2969
[11/11 05:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/11 05:06:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/11 05:13:51 visual_prompt]: Epoch 2 / 100: avg data time: 1.15e+01, avg batch time: 11.8391, average train loss: 1.4240
[11/11 05:14:38 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1561, average loss: 0.6896
[11/11 05:14:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 52.10	
[11/11 05:14:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/11 05:21:32 visual_prompt]: Epoch 3 / 100: avg data time: 1.15e+01, avg batch time: 11.8196, average train loss: 0.7053
[11/11 05:22:19 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1561, average loss: 0.6910
[11/11 05:22:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.86	
[11/11 05:22:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/11 05:29:13 visual_prompt]: Epoch 4 / 100: avg data time: 1.15e+01, avg batch time: 11.8123, average train loss: 0.6941
[11/11 05:30:00 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.1566, average loss: 0.6943
[11/11 05:30:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.92	
[11/11 05:30:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/11 05:36:52 visual_prompt]: Epoch 5 / 100: avg data time: 1.14e+01, avg batch time: 11.7751, average train loss: 0.7331
[11/11 05:37:39 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1558, average loss: 0.6873
[11/11 05:37:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.26	
[11/11 05:37:39 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/11 05:44:32 visual_prompt]: Epoch 6 / 100: avg data time: 1.14e+01, avg batch time: 11.7951, average train loss: 0.7440
[11/11 05:45:19 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1559, average loss: 0.6740
[11/11 05:45:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.84	
[11/11 05:45:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/11 05:52:13 visual_prompt]: Epoch 7 / 100: avg data time: 1.15e+01, avg batch time: 11.8156, average train loss: 0.7171
[11/11 05:53:00 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1563, average loss: 0.6689
[11/11 05:53:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.83	
[11/11 05:53:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/11 05:59:52 visual_prompt]: Epoch 8 / 100: avg data time: 1.14e+01, avg batch time: 11.7733, average train loss: 0.7066
[11/11 06:00:40 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1566, average loss: 0.6648
[11/11 06:00:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 62.47	
[11/11 06:00:40 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/11 06:07:34 visual_prompt]: Epoch 9 / 100: avg data time: 1.15e+01, avg batch time: 11.8254, average train loss: 0.6864
[11/11 06:08:21 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1588, average loss: 0.6958
[11/11 06:08:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 66.03	
[11/11 06:08:21 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/11 06:15:15 visual_prompt]: Epoch 10 / 100: avg data time: 1.15e+01, avg batch time: 11.8205, average train loss: 0.6628
[11/11 06:16:03 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1561, average loss: 0.6445
[11/11 06:16:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 67.86	
[11/11 06:16:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/11 06:23:07 visual_prompt]: Epoch 11 / 100: avg data time: 1.17e+01, avg batch time: 12.1006, average train loss: 0.6792
[11/11 06:23:55 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1560, average loss: 0.6552
[11/11 06:23:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.42	
[11/11 06:23:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/11 06:30:56 visual_prompt]: Epoch 12 / 100: avg data time: 1.17e+01, avg batch time: 12.0239, average train loss: 0.6736
[11/11 06:31:44 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1561, average loss: 0.7371
[11/11 06:31:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 69.09	
[11/11 06:31:44 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/11 06:38:42 visual_prompt]: Epoch 13 / 100: avg data time: 1.16e+01, avg batch time: 11.9407, average train loss: 0.7443
[11/11 06:39:30 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1560, average loss: 0.7039
[11/11 06:39:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 69.02	
[11/11 06:39:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/11 06:46:25 visual_prompt]: Epoch 14 / 100: avg data time: 1.15e+01, avg batch time: 11.8570, average train loss: 0.6801
[11/11 06:47:12 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1560, average loss: 0.7292
[11/11 06:47:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 71.78	
[11/11 06:47:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/11 06:54:08 visual_prompt]: Epoch 15 / 100: avg data time: 1.15e+01, avg batch time: 11.8590, average train loss: 0.6873
[11/11 06:54:55 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1563, average loss: 0.8067
[11/11 06:54:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 70.88	
[11/11 06:54:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/11 07:01:48 visual_prompt]: Epoch 16 / 100: avg data time: 1.15e+01, avg batch time: 11.8068, average train loss: 0.6775
[11/11 07:02:35 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1558, average loss: 0.8387
[11/11 07:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 70.67	
[11/11 07:02:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/11 07:09:28 visual_prompt]: Epoch 17 / 100: avg data time: 1.14e+01, avg batch time: 11.7740, average train loss: 0.7107
[11/11 07:10:15 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1562, average loss: 0.6252
[11/11 07:10:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 71.39	
[11/11 07:10:15 visual_prompt]: Best epoch 17: best metric: -0.625
[11/11 07:10:15 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/11 07:17:07 visual_prompt]: Epoch 18 / 100: avg data time: 1.14e+01, avg batch time: 11.7743, average train loss: 0.6610
[11/11 07:17:54 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1562, average loss: 0.6284
[11/11 07:17:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.66	
[11/11 07:17:54 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/11 07:24:46 visual_prompt]: Epoch 19 / 100: avg data time: 1.14e+01, avg batch time: 11.7595, average train loss: 0.6644
[11/11 07:25:33 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1560, average loss: 0.6291
[11/11 07:25:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 69.97	
[11/11 07:25:33 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[11/11 07:32:25 visual_prompt]: Epoch 20 / 100: avg data time: 1.14e+01, avg batch time: 11.7866, average train loss: 0.6069
[11/11 07:33:13 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1561, average loss: 0.6020
[11/11 07:33:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 72.82	
[11/11 07:33:13 visual_prompt]: Best epoch 20: best metric: -0.602
[11/11 07:33:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[11/11 07:40:05 visual_prompt]: Epoch 21 / 100: avg data time: 1.14e+01, avg batch time: 11.7723, average train loss: 0.6149
[11/11 07:40:52 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1559, average loss: 0.6033
[11/11 07:40:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 73.15	
[11/11 07:40:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[11/11 07:47:44 visual_prompt]: Epoch 22 / 100: avg data time: 1.14e+01, avg batch time: 11.7839, average train loss: 0.6226
[11/11 07:48:31 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1560, average loss: 0.6335
[11/11 07:48:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 72.48	
[11/11 07:48:31 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[11/11 07:55:31 visual_prompt]: Epoch 23 / 100: avg data time: 1.17e+01, avg batch time: 12.0175, average train loss: 0.6043
[11/11 07:56:20 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1562, average loss: 0.6338
[11/11 07:56:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 70.27	
[11/11 07:56:20 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[11/11 08:03:24 visual_prompt]: Epoch 24 / 100: avg data time: 1.18e+01, avg batch time: 12.1134, average train loss: 0.6458
[11/11 08:04:12 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1559, average loss: 0.6417
[11/11 08:04:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 72.02	
[11/11 08:04:12 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[11/11 08:11:13 visual_prompt]: Epoch 25 / 100: avg data time: 1.17e+01, avg batch time: 12.0084, average train loss: 0.5915
[11/11 08:12:00 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1572, average loss: 0.6203
[11/11 08:12:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 72.26	
[11/11 08:12:00 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[11/11 08:18:56 visual_prompt]: Epoch 26 / 100: avg data time: 1.15e+01, avg batch time: 11.8861, average train loss: 0.6273
[11/11 08:19:44 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1563, average loss: 0.5987
[11/11 08:19:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 71.14	rocauc: 74.00	
[11/11 08:19:44 visual_prompt]: Best epoch 26: best metric: -0.599
[11/11 08:19:44 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[11/11 08:26:39 visual_prompt]: Epoch 27 / 100: avg data time: 1.15e+01, avg batch time: 11.8525, average train loss: 0.5716
[11/11 08:27:26 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1561, average loss: 0.6553
[11/11 08:27:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 68.53	
[11/11 08:27:26 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[11/11 08:34:20 visual_prompt]: Epoch 28 / 100: avg data time: 1.15e+01, avg batch time: 11.8180, average train loss: 0.6112
[11/11 08:35:07 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1566, average loss: 0.6402
[11/11 08:35:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 71.70	
[11/11 08:35:07 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[11/11 08:42:01 visual_prompt]: Epoch 29 / 100: avg data time: 1.15e+01, avg batch time: 11.8312, average train loss: 0.5532
[11/11 08:42:48 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1561, average loss: 0.6537
[11/11 08:42:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.34	
[11/11 08:42:48 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[11/11 08:49:41 visual_prompt]: Epoch 30 / 100: avg data time: 1.14e+01, avg batch time: 11.7891, average train loss: 0.5478
[11/11 08:50:28 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1568, average loss: 0.6180
[11/11 08:50:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 72.00	
[11/11 08:50:28 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[11/11 08:57:21 visual_prompt]: Epoch 31 / 100: avg data time: 1.14e+01, avg batch time: 11.7790, average train loss: 0.5215
[11/11 08:58:08 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1562, average loss: 0.6603
[11/11 08:58:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 71.18	
[11/11 08:58:08 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[11/11 09:05:01 visual_prompt]: Epoch 32 / 100: avg data time: 1.14e+01, avg batch time: 11.7993, average train loss: 0.5315
[11/11 09:05:48 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1560, average loss: 0.6632
[11/11 09:05:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.74	
[11/11 09:05:48 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[11/11 09:12:40 visual_prompt]: Epoch 33 / 100: avg data time: 1.14e+01, avg batch time: 11.7717, average train loss: 0.5278
[11/11 09:13:27 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1615, average loss: 0.6460
[11/11 09:13:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 72.37	
[11/11 09:13:27 visual_prompt]: Stopping early.
[11/11 09:13:28 visual_prompt]: Rank of current process: 0. World size: 1
[11/11 09:13:28 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/11 09:13:28 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/11 09:13:28 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/11 09:13:28 visual_prompt]: Training with config:
[11/11 09:13:28 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.1_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/11 09:13:28 visual_prompt]: Loading training data...
[11/11 09:13:28 visual_prompt]: Constructing mammo-cbis dataset train...
[11/11 09:13:29 visual_prompt]: Loading validation data...
[11/11 09:13:29 visual_prompt]: Constructing mammo-cbis dataset val...
[11/11 09:13:29 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/11 09:13:34 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/11 09:13:34 visual_prompt]: tuned percent:0.536
[11/11 09:13:35 visual_prompt]: Device used for model: 0
[11/11 09:13:35 visual_prompt]: Setting up Evaluator...
[11/11 09:13:35 visual_prompt]: Setting up Trainer...
[11/11 09:13:35 visual_prompt]: 	Setting up the optimizer...
[11/11 09:13:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/11 09:20:27 visual_prompt]: Epoch 1 / 100: avg data time: 1.14e+01, avg batch time: 11.7903, average train loss: 1.4017
[11/11 09:21:15 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1586, average loss: 1.2969
[11/11 09:21:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/11 09:21:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[11/11 09:28:06 visual_prompt]: Epoch 2 / 100: avg data time: 1.14e+01, avg batch time: 11.7582, average train loss: 1.0705
[11/11 09:28:55 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1562, average loss: 0.6907
[11/11 09:28:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 47.90	
[11/11 09:28:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[11/11 09:36:00 visual_prompt]: Epoch 3 / 100: avg data time: 1.18e+01, avg batch time: 12.1489, average train loss: 0.7019
[11/11 09:36:49 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1558, average loss: 0.6936
[11/11 09:36:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.38	
[11/11 09:36:49 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[11/11 09:43:52 visual_prompt]: Epoch 4 / 100: avg data time: 1.17e+01, avg batch time: 12.0762, average train loss: 0.6916
[11/11 09:44:40 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1577, average loss: 0.6810
[11/11 09:44:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.06	
[11/11 09:44:40 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[11/11 09:51:38 visual_prompt]: Epoch 5 / 100: avg data time: 1.16e+01, avg batch time: 11.9554, average train loss: 0.7094
[11/11 09:52:26 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1601, average loss: 0.6902
[11/11 09:52:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.81	
[11/11 09:52:26 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[11/11 09:59:24 visual_prompt]: Epoch 6 / 100: avg data time: 1.16e+01, avg batch time: 11.9347, average train loss: 0.7276
[11/11 10:00:12 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1575, average loss: 0.6840
[11/11 10:00:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 57.69	
[11/11 10:00:12 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[11/11 10:07:08 visual_prompt]: Epoch 7 / 100: avg data time: 1.15e+01, avg batch time: 11.8778, average train loss: 0.6935
[11/11 10:07:55 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1559, average loss: 0.7299
[11/11 10:07:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.42	
[11/11 10:07:55 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[11/11 10:14:49 visual_prompt]: Epoch 8 / 100: avg data time: 1.15e+01, avg batch time: 11.8182, average train loss: 0.7299
[11/11 10:15:36 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1563, average loss: 0.6855
[11/11 10:15:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.34	
[11/11 10:15:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[11/11 10:22:30 visual_prompt]: Epoch 9 / 100: avg data time: 1.15e+01, avg batch time: 11.8332, average train loss: 0.6886
[11/11 10:23:17 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1562, average loss: 0.7100
[11/11 10:23:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.02	
[11/11 10:23:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[11/11 10:30:12 visual_prompt]: Epoch 10 / 100: avg data time: 1.15e+01, avg batch time: 11.8590, average train loss: 0.6977
[11/11 10:30:59 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1561, average loss: 0.6916
[11/11 10:30:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.20	
[11/11 10:30:59 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[11/11 10:37:53 visual_prompt]: Epoch 11 / 100: avg data time: 1.14e+01, avg batch time: 11.7962, average train loss: 0.6889
[11/11 10:38:40 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1564, average loss: 0.6854
[11/11 10:38:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.79	
[11/11 10:38:40 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[11/11 10:45:32 visual_prompt]: Epoch 12 / 100: avg data time: 1.14e+01, avg batch time: 11.7902, average train loss: 0.7003
[11/11 10:46:20 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1564, average loss: 0.6877
[11/11 10:46:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.50	
[11/11 10:46:20 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[11/11 10:53:12 visual_prompt]: Epoch 13 / 100: avg data time: 1.14e+01, avg batch time: 11.7806, average train loss: 0.7056
[11/11 10:53:59 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1599, average loss: 0.7145
[11/11 10:53:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.20	
[11/11 10:53:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[11/11 11:00:51 visual_prompt]: Epoch 14 / 100: avg data time: 1.14e+01, avg batch time: 11.7702, average train loss: 0.7051
[11/11 11:01:38 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1562, average loss: 0.6982
[11/11 11:01:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.38	
[11/11 11:01:38 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[11/11 11:08:37 visual_prompt]: Epoch 15 / 100: avg data time: 1.16e+01, avg batch time: 11.9503, average train loss: 0.7057
[11/11 11:09:26 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1585, average loss: 0.6937
[11/11 11:09:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.71	
[11/11 11:09:26 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[11/11 11:16:31 visual_prompt]: Epoch 16 / 100: avg data time: 1.18e+01, avg batch time: 12.1380, average train loss: 0.6993
[11/11 11:17:19 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1562, average loss: 0.6912
[11/11 11:17:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.71	
[11/11 11:17:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[11/11 11:24:18 visual_prompt]: Epoch 17 / 100: avg data time: 1.16e+01, avg batch time: 11.9626, average train loss: 0.6922
[11/11 11:25:06 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1562, average loss: 0.6905
[11/11 11:25:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.27	
[11/11 11:25:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[11/11 11:32:03 visual_prompt]: Epoch 18 / 100: avg data time: 1.16e+01, avg batch time: 11.9224, average train loss: 0.6950
[11/11 11:32:51 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1561, average loss: 0.6953
[11/11 11:32:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.82	
[11/11 11:32:51 visual_prompt]: Stopping early.
[11/11 11:32:51 visual_prompt]: Rank of current process: 0. World size: 1
[11/11 11:32:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/11 11:32:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/11 11:32:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/11 11:32:51 visual_prompt]: Training with config:
[11/11 11:32:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.1_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/11 11:32:51 visual_prompt]: Loading training data...
[11/11 11:32:51 visual_prompt]: Constructing mammo-cbis dataset train...
[11/11 11:32:51 visual_prompt]: Loading validation data...
[11/11 11:32:51 visual_prompt]: Constructing mammo-cbis dataset val...
[11/11 11:32:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/11 11:32:54 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/11 11:32:54 visual_prompt]: tuned percent:0.536
[11/11 11:32:54 visual_prompt]: Device used for model: 0
[11/11 11:32:54 visual_prompt]: Setting up Evaluator...
[11/11 11:32:54 visual_prompt]: Setting up Trainer...
[11/11 11:32:54 visual_prompt]: 	Setting up the optimizer...
[11/11 11:32:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/11 11:39:51 visual_prompt]: Epoch 1 / 100: avg data time: 1.15e+01, avg batch time: 11.8946, average train loss: 1.4017
[11/11 11:40:38 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1561, average loss: 1.2969
[11/11 11:40:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[11/11 11:40:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[11/11 11:47:33 visual_prompt]: Epoch 2 / 100: avg data time: 1.15e+01, avg batch time: 11.8490, average train loss: 1.0723
[11/11 11:48:20 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1565, average loss: 0.6909
[11/11 11:48:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 47.75	
[11/11 11:48:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[11/11 11:55:14 visual_prompt]: Epoch 3 / 100: avg data time: 1.15e+01, avg batch time: 11.8181, average train loss: 0.7033
[11/11 11:56:01 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1561, average loss: 0.6938
[11/11 11:56:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.75	
[11/11 11:56:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[11/11 12:02:55 visual_prompt]: Epoch 4 / 100: avg data time: 1.15e+01, avg batch time: 11.8090, average train loss: 0.6929
[11/11 12:03:42 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1599, average loss: 0.6804
[11/11 12:03:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.78	
[11/11 12:03:42 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[11/11 12:10:34 visual_prompt]: Epoch 5 / 100: avg data time: 1.14e+01, avg batch time: 11.7736, average train loss: 0.7148
[11/11 12:11:21 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1565, average loss: 0.6962
[11/11 12:11:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.88	
[11/11 12:11:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[11/11 12:18:14 visual_prompt]: Epoch 6 / 100: avg data time: 1.14e+01, avg batch time: 11.7942, average train loss: 0.7385
[11/11 12:19:01 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1565, average loss: 0.6915
[11/11 12:19:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 55.36	
[11/11 12:19:01 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[11/11 12:25:54 visual_prompt]: Epoch 7 / 100: avg data time: 1.14e+01, avg batch time: 11.7980, average train loss: 0.6963
[11/11 12:26:41 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1561, average loss: 0.6782
[11/11 12:26:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 59.73	
[11/11 12:26:41 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[11/11 12:33:33 visual_prompt]: Epoch 8 / 100: avg data time: 1.14e+01, avg batch time: 11.7632, average train loss: 0.6892
[11/11 12:34:20 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1562, average loss: 0.6818
[11/11 12:34:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.62	
[11/11 12:34:20 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[11/11 12:41:13 visual_prompt]: Epoch 9 / 100: avg data time: 1.14e+01, avg batch time: 11.7894, average train loss: 0.6901
[11/11 12:42:00 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1563, average loss: 0.7419
[11/11 12:42:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.10	
[11/11 12:42:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[11/11 12:49:04 visual_prompt]: Epoch 10 / 100: avg data time: 1.18e+01, avg batch time: 12.1242, average train loss: 0.6824
[11/11 12:49:53 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1563, average loss: 0.6651
[11/11 12:49:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 64.44	
[11/11 12:49:53 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[11/11 12:56:56 visual_prompt]: Epoch 11 / 100: avg data time: 1.17e+01, avg batch time: 12.0822, average train loss: 0.6695
[11/11 12:57:44 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1565, average loss: 0.6550
[11/11 12:57:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.46	
[11/11 12:57:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[11/11 13:04:44 visual_prompt]: Epoch 12 / 100: avg data time: 1.16e+01, avg batch time: 11.9719, average train loss: 0.6575
[11/11 13:05:31 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1573, average loss: 0.6604
[11/11 13:05:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.08	
[11/11 13:05:31 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[11/11 13:12:27 visual_prompt]: Epoch 13 / 100: avg data time: 1.15e+01, avg batch time: 11.8753, average train loss: 0.6852
[11/11 13:13:15 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1559, average loss: 0.6496
[11/11 13:13:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.13	
[11/11 13:13:15 visual_prompt]: Best epoch 13: best metric: -0.650
[11/11 13:13:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[11/11 13:20:09 visual_prompt]: Epoch 14 / 100: avg data time: 1.15e+01, avg batch time: 11.8335, average train loss: 0.6885
[11/11 13:20:56 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1572, average loss: 0.7683
[11/11 13:20:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.52	
[11/11 13:20:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[11/11 13:27:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.15e+01, avg batch time: 11.8443, average train loss: 0.6791
[11/11 13:28:38 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1597, average loss: 0.6587
[11/11 13:28:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 68.92	
[11/11 13:28:38 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[11/11 13:35:31 visual_prompt]: Epoch 16 / 100: avg data time: 1.14e+01, avg batch time: 11.7846, average train loss: 0.6772
[11/11 13:36:18 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1589, average loss: 0.7687
[11/11 13:36:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 68.35	
[11/11 13:36:18 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[11/11 13:43:10 visual_prompt]: Epoch 17 / 100: avg data time: 1.14e+01, avg batch time: 11.7762, average train loss: 0.6523
[11/11 13:43:57 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1565, average loss: 0.6259
[11/11 13:43:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 70.15	
[11/11 13:43:57 visual_prompt]: Best epoch 17: best metric: -0.626
[11/11 13:43:57 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[11/11 13:50:50 visual_prompt]: Epoch 18 / 100: avg data time: 1.14e+01, avg batch time: 11.7938, average train loss: 0.6554
[11/11 13:51:37 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1562, average loss: 0.7966
[11/11 13:51:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 70.19	
[11/11 13:51:37 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[11/11 13:58:30 visual_prompt]: Epoch 19 / 100: avg data time: 1.14e+01, avg batch time: 11.7749, average train loss: 0.6649
[11/11 13:59:17 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1565, average loss: 0.8159
[11/11 13:59:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 68.88	
[11/11 13:59:17 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[11/11 14:06:09 visual_prompt]: Epoch 20 / 100: avg data time: 1.14e+01, avg batch time: 11.7867, average train loss: 0.6487
[11/11 14:06:56 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1561, average loss: 0.7228
[11/11 14:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 68.19	
[11/11 14:06:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[11/11 14:13:49 visual_prompt]: Epoch 21 / 100: avg data time: 1.14e+01, avg batch time: 11.7765, average train loss: 0.6133
[11/11 14:14:36 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1562, average loss: 0.6511
[11/11 14:14:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 70.17	
[11/11 14:14:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[11/11 14:21:31 visual_prompt]: Epoch 22 / 100: avg data time: 1.15e+01, avg batch time: 11.8564, average train loss: 0.6008
[11/11 14:22:20 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1564, average loss: 0.6419
[11/11 14:22:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.47	
[11/11 14:22:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[11/11 14:29:24 visual_prompt]: Epoch 23 / 100: avg data time: 1.18e+01, avg batch time: 12.1127, average train loss: 0.6125
[11/11 14:30:12 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1563, average loss: 0.7167
[11/11 14:30:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 66.31	
[11/11 14:30:12 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[11/11 14:37:15 visual_prompt]: Epoch 24 / 100: avg data time: 1.17e+01, avg batch time: 12.0737, average train loss: 0.6196
[11/11 14:38:03 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1595, average loss: 0.6156
[11/11 14:38:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 72.17	
[11/11 14:38:03 visual_prompt]: Best epoch 24: best metric: -0.616
[11/11 14:38:03 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[11/11 14:45:02 visual_prompt]: Epoch 25 / 100: avg data time: 1.16e+01, avg batch time: 11.9521, average train loss: 0.5970
[11/11 14:45:49 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1578, average loss: 0.6255
[11/11 14:45:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 70.11	
[11/11 14:45:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[11/11 14:52:46 visual_prompt]: Epoch 26 / 100: avg data time: 1.15e+01, avg batch time: 11.8931, average train loss: 0.6580
[11/11 14:53:33 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1562, average loss: 0.6410
[11/11 14:53:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 68.98	
[11/11 14:53:33 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[11/11 15:00:28 visual_prompt]: Epoch 27 / 100: avg data time: 1.15e+01, avg batch time: 11.8503, average train loss: 0.6072
[11/11 15:01:15 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1564, average loss: 0.6497
[11/11 15:01:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 67.42	
[11/11 15:01:15 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[11/11 15:08:09 visual_prompt]: Epoch 28 / 100: avg data time: 1.15e+01, avg batch time: 11.8269, average train loss: 0.6254
[11/11 15:08:56 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1582, average loss: 0.6641
[11/11 15:08:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 66.57	
[11/11 15:08:56 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[11/11 15:15:52 visual_prompt]: Epoch 29 / 100: avg data time: 1.15e+01, avg batch time: 11.8647, average train loss: 0.5999
[11/11 15:16:39 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1576, average loss: 0.6243
[11/11 15:16:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 73.07	
[11/11 15:16:39 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[11/11 15:23:32 visual_prompt]: Epoch 30 / 100: avg data time: 1.14e+01, avg batch time: 11.7888, average train loss: 0.5994
[11/11 15:24:19 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1565, average loss: 0.7013
[11/11 15:24:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 71.31	
[11/11 15:24:19 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[11/11 15:31:12 visual_prompt]: Epoch 31 / 100: avg data time: 1.14e+01, avg batch time: 11.7864, average train loss: 0.5889
[11/11 15:31:59 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1564, average loss: 0.6347
[11/11 15:31:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 71.46	
[11/11 15:31:59 visual_prompt]: Stopping early.
[11/11 15:31:59 visual_prompt]: Rank of current process: 0. World size: 1
[11/11 15:31:59 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/11 15:31:59 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/11 15:31:59 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/11 15:31:59 visual_prompt]: Training with config:
[11/11 15:31:59 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size200/val/seed0/lr0.1_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/11 15:31:59 visual_prompt]: Loading training data...
[11/11 15:31:59 visual_prompt]: Constructing mammo-cbis dataset train...
[11/11 15:32:00 visual_prompt]: Loading validation data...
[11/11 15:32:00 visual_prompt]: Constructing mammo-cbis dataset val...
[11/11 15:32:00 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/11 15:32:06 visual_prompt]: Total Parameters: 86221058	 Gradient Parameters: 462338
[11/11 15:32:06 visual_prompt]: tuned percent:0.536
[11/11 15:32:06 visual_prompt]: Device used for model: 0
[11/11 15:32:06 visual_prompt]: Setting up Evaluator...
[11/11 15:32:06 visual_prompt]: Setting up Trainer...
[11/11 15:32:06 visual_prompt]: 	Setting up the optimizer...
[11/11 15:32:06 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0

/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 21:47:50 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 21:47:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 21:47:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 21:47:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/20 21:47:51 visual_prompt]: Training with config:
[11/20 21:47:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/20 21:47:51 visual_prompt]: Loading training data...
[11/20 21:47:51 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 21:47:51 visual_prompt]: Loading validation data...
[11/20 21:47:51 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 21:47:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 21:47:58 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/20 21:47:58 visual_prompt]: tuned percent:0.525
[11/20 21:47:58 visual_prompt]: Device used for model: 0
[11/20 21:47:58 visual_prompt]: Setting up Evaluator...
[11/20 21:47:58 visual_prompt]: Setting up Trainer...
[11/20 21:47:58 visual_prompt]: 	Setting up the optimizer...
[11/20 21:47:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 21:49:49 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8195 s / batch. (data: 1.06e-02). ETA=12:33:58, max mem: 20.9 GB 
[11/20 21:51:33 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8093 s / batch. (data: 3.02e-04). ETA=12:23:10, max mem: 20.9 GB 
[11/20 21:53:20 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.1480 s / batch. (data: 1.31e+00). ETA=1 day, 8:49:00, max mem: 20.9 GB 
[11/20 21:55:02 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8226 s / batch. (data: 3.20e-04). ETA=12:32:42, max mem: 20.9 GB 
[11/20 21:56:46 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8211 s / batch. (data: 9.11e-03). ETA=12:29:57, max mem: 20.9 GB 
[11/20 21:57:38 visual_prompt]: Epoch 1 / 100: avg data time: 2.23e-01, avg batch time: 1.0482, average train loss: 1.5403
[11/20 21:58:33 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.3085, average loss: 1.5201
[11/20 21:58:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/20 21:58:33 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/20 22:00:14 visual_prompt]: 	Training 100/553. train loss: 11.0660,	0.8610 s / batch. (data: 2.10e-02). ETA=13:04:09, max mem: 20.9 GB 
[11/20 22:01:50 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8440 s / batch. (data: 7.98e-03). ETA=12:47:19, max mem: 20.9 GB 
[11/20 22:03:29 visual_prompt]: 	Training 300/553. train loss: 16.4932,	0.9320 s / batch. (data: 1.03e-01). ETA=14:05:44, max mem: 20.9 GB 
[11/20 22:05:05 visual_prompt]: 	Training 400/553. train loss: 6.3848,	0.8530 s / batch. (data: 1.56e-02). ETA=12:52:39, max mem: 20.9 GB 
[11/20 22:06:43 visual_prompt]: 	Training 500/553. train loss: 1.8677,	0.8163 s / batch. (data: 3.05e-04). ETA=12:18:01, max mem: 20.9 GB 
[11/20 22:07:32 visual_prompt]: Epoch 2 / 100: avg data time: 1.47e-01, avg batch time: 0.9733, average train loss: 14.2649
[11/20 22:08:27 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3084, average loss: 67.7520
[11/20 22:08:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.35	
[11/20 22:08:27 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/20 22:10:06 visual_prompt]: 	Training 100/553. train loss: 36.6844,	0.8395 s / batch. (data: 1.56e-02). ETA=12:36:53, max mem: 20.9 GB 
[11/20 22:11:43 visual_prompt]: 	Training 200/553. train loss: 39.5236,	1.0854 s / batch. (data: 2.75e-01). ETA=16:16:43, max mem: 20.9 GB 
[11/20 22:13:19 visual_prompt]: 	Training 300/553. train loss: 87.4285,	0.8134 s / batch. (data: 3.11e-04). ETA=12:10:38, max mem: 20.9 GB 
[11/20 22:14:56 visual_prompt]: 	Training 400/553. train loss: 14.2160,	0.8504 s / batch. (data: 1.05e-02). ETA=12:42:23, max mem: 20.9 GB 
[11/20 22:16:35 visual_prompt]: 	Training 500/553. train loss: 37.0757,	1.2195 s / batch. (data: 3.91e-01). ETA=18:11:19, max mem: 20.9 GB 
[11/20 22:17:24 visual_prompt]: Epoch 3 / 100: avg data time: 1.48e-01, avg batch time: 0.9720, average train loss: 39.4319
[11/20 22:18:19 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3062, average loss: 28.8792
[11/20 22:18:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.40	
[11/20 22:18:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/20 22:20:00 visual_prompt]: 	Training 100/553. train loss: 151.6634,	0.8200 s / batch. (data: 3.47e-04). ETA=12:11:43, max mem: 20.9 GB 
[11/20 22:21:37 visual_prompt]: 	Training 200/553. train loss: 0.4510,	0.8547 s / batch. (data: 5.44e-03). ETA=12:41:17, max mem: 20.9 GB 
[11/20 22:23:13 visual_prompt]: 	Training 300/553. train loss: 7.5754,	1.2826 s / batch. (data: 4.60e-01). ETA=19:00:15, max mem: 20.9 GB 
[11/20 22:24:46 visual_prompt]: 	Training 400/553. train loss: 67.7845,	1.3840 s / batch. (data: 5.56e-01). ETA=20:28:05, max mem: 20.9 GB 
[11/20 22:26:24 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.2040 s / batch. (data: 2.40e+00). ETA=1 day, 23:17:41, max mem: 20.9 GB 
[11/20 22:27:16 visual_prompt]: Epoch 4 / 100: avg data time: 1.47e-01, avg batch time: 0.9701, average train loss: 53.2946
[11/20 22:28:13 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3100, average loss: 34.9306
[11/20 22:28:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.06	
[11/20 22:28:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/20 22:29:54 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 1.60e-02). ETA=12:00:38, max mem: 20.9 GB 
[11/20 22:31:35 visual_prompt]: 	Training 200/553. train loss: 35.7910,	1.1240 s / batch. (data: 2.97e-01). ETA=16:30:45, max mem: 20.9 GB 
[11/20 22:33:16 visual_prompt]: 	Training 300/553. train loss: 13.4560,	0.8286 s / batch. (data: 3.65e-04). ETA=12:09:02, max mem: 20.9 GB 
[11/20 22:34:55 visual_prompt]: 	Training 400/553. train loss: 74.0575,	0.8133 s / batch. (data: 7.94e-03). ETA=11:54:10, max mem: 20.9 GB 
[11/20 22:36:35 visual_prompt]: 	Training 500/553. train loss: 51.1401,	0.8531 s / batch. (data: 9.02e-03). ETA=12:27:40, max mem: 20.9 GB 
[11/20 22:37:28 visual_prompt]: Epoch 5 / 100: avg data time: 1.82e-01, avg batch time: 1.0031, average train loss: 70.3916
[11/20 22:38:25 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3075, average loss: 43.8182
[11/20 22:38:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.23	
[11/20 22:38:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/20 22:40:10 visual_prompt]: 	Training 100/553. train loss: 363.2985,	0.8240 s / batch. (data: 8.34e-04). ETA=12:00:05, max mem: 20.9 GB 
[11/20 22:41:50 visual_prompt]: 	Training 200/553. train loss: 621.9333,	0.8229 s / batch. (data: 3.49e-04). ETA=11:57:47, max mem: 20.9 GB 
[11/20 22:43:28 visual_prompt]: 	Training 300/553. train loss: 82.2972,	0.8169 s / batch. (data: 7.95e-03). ETA=11:51:12, max mem: 20.9 GB 
[11/20 22:45:11 visual_prompt]: 	Training 400/553. train loss: 33.9150,	0.8128 s / batch. (data: 3.65e-04). ETA=11:46:17, max mem: 20.9 GB 
[11/20 22:46:50 visual_prompt]: 	Training 500/553. train loss: 96.2137,	0.8239 s / batch. (data: 4.09e-04). ETA=11:54:29, max mem: 20.9 GB 
[11/20 22:47:42 visual_prompt]: Epoch 6 / 100: avg data time: 1.86e-01, avg batch time: 1.0068, average train loss: 103.8233
[11/20 22:48:39 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3076, average loss: 91.2159
[11/20 22:48:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.52	
[11/20 22:48:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/20 22:50:22 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8228 s / batch. (data: 1.07e-02). ETA=11:51:27, max mem: 20.9 GB 
[11/20 22:52:02 visual_prompt]: 	Training 200/553. train loss: 64.2342,	0.8200 s / batch. (data: 3.03e-04). ETA=11:47:42, max mem: 20.9 GB 
[11/20 22:53:44 visual_prompt]: 	Training 300/553. train loss: 5.0139,	1.4880 s / batch. (data: 6.61e-01). ETA=21:21:42, max mem: 20.9 GB 
[11/20 22:55:24 visual_prompt]: 	Training 400/553. train loss: 52.8119,	1.7231 s / batch. (data: 9.11e-01). ETA=1 day, 0:41:20, max mem: 20.9 GB 
[11/20 22:57:03 visual_prompt]: 	Training 500/553. train loss: 68.9438,	0.8363 s / batch. (data: 2.07e-02). ETA=11:57:34, max mem: 20.9 GB 
[11/20 22:57:54 visual_prompt]: Epoch 7 / 100: avg data time: 1.81e-01, avg batch time: 1.0020, average train loss: 96.4033
[11/20 22:58:50 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.3083, average loss: 174.5468
[11/20 22:58:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.70	
[11/20 22:58:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/20 23:00:31 visual_prompt]: 	Training 100/553. train loss: 53.3281,	0.8280 s / batch. (data: 4.35e-04). ETA=11:48:19, max mem: 20.9 GB 
[11/20 23:02:11 visual_prompt]: 	Training 200/553. train loss: 583.0183,	0.8280 s / batch. (data: 3.90e-04). ETA=11:46:57, max mem: 20.9 GB 
[11/20 23:03:50 visual_prompt]: 	Training 300/553. train loss: 62.8790,	0.8105 s / batch. (data: 3.05e-04). ETA=11:30:42, max mem: 20.9 GB 
[11/20 23:05:29 visual_prompt]: 	Training 400/553. train loss: 97.0186,	0.9053 s / batch. (data: 8.15e-02). ETA=12:49:54, max mem: 20.9 GB 
[11/20 23:07:08 visual_prompt]: 	Training 500/553. train loss: 524.1109,	1.3520 s / batch. (data: 5.10e-01). ETA=19:07:35, max mem: 20.9 GB 
[11/20 23:08:00 visual_prompt]: Epoch 8 / 100: avg data time: 1.73e-01, avg batch time: 0.9940, average train loss: 112.1865
[11/20 23:08:56 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3081, average loss: 16.1529
[11/20 23:08:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.73	
[11/20 23:08:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/20 23:10:38 visual_prompt]: 	Training 100/553. train loss: 0.3753,	0.8090 s / batch. (data: 3.15e-04). ETA=11:24:38, max mem: 20.9 GB 
[11/20 23:12:15 visual_prompt]: 	Training 200/553. train loss: 171.0539,	0.8416 s / batch. (data: 1.35e-02). ETA=11:50:48, max mem: 20.9 GB 
[11/20 23:13:53 visual_prompt]: 	Training 300/553. train loss: 55.7412,	1.6239 s / batch. (data: 8.13e-01). ETA=22:48:48, max mem: 20.9 GB 
[11/20 23:15:33 visual_prompt]: 	Training 400/553. train loss: 35.6600,	0.8026 s / batch. (data: 3.04e-04). ETA=11:15:13, max mem: 20.9 GB 
[11/20 23:17:12 visual_prompt]: 	Training 500/553. train loss: 203.6946,	0.9248 s / batch. (data: 1.04e-01). ETA=12:56:25, max mem: 20.9 GB 
[11/20 23:18:02 visual_prompt]: Epoch 9 / 100: avg data time: 1.68e-01, avg batch time: 0.9880, average train loss: 156.6385
[11/20 23:18:58 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3075, average loss: 34.7765
[11/20 23:18:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.50	
[11/20 23:18:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/20 23:20:43 visual_prompt]: 	Training 100/553. train loss: 35.8805,	0.8320 s / batch. (data: 3.47e-04). ETA=11:36:23, max mem: 20.9 GB 
[11/20 23:22:20 visual_prompt]: 	Training 200/553. train loss: 283.7655,	0.8097 s / batch. (data: 4.50e-04). ETA=11:16:22, max mem: 20.9 GB 
[11/20 23:23:57 visual_prompt]: 	Training 300/553. train loss: 90.8150,	0.9674 s / batch. (data: 1.51e-01). ETA=13:26:32, max mem: 20.9 GB 
[11/20 23:25:32 visual_prompt]: 	Training 400/553. train loss: 170.0476,	0.8160 s / batch. (data: 7.95e-03). ETA=11:18:56, max mem: 20.9 GB 
[11/20 23:27:13 visual_prompt]: 	Training 500/553. train loss: 16.0459,	0.8320 s / batch. (data: 3.79e-04). ETA=11:30:52, max mem: 20.9 GB 
[11/20 23:28:04 visual_prompt]: Epoch 10 / 100: avg data time: 1.65e-01, avg batch time: 0.9858, average train loss: 156.2801
[11/20 23:29:00 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3093, average loss: 7.4814
[11/20 23:29:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 57.60	
[11/20 23:29:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/20 23:30:44 visual_prompt]: 	Training 100/553. train loss: 436.6214,	0.8229 s / batch. (data: 3.33e-04). ETA=11:21:10, max mem: 20.9 GB 
[11/20 23:32:25 visual_prompt]: 	Training 200/553. train loss: 283.7959,	0.8190 s / batch. (data: 3.75e-04). ETA=11:16:39, max mem: 20.9 GB 
[11/20 23:34:03 visual_prompt]: 	Training 300/553. train loss: 7.6936,	2.1787 s / batch. (data: 1.36e+00). ETA=1 day, 5:56:22, max mem: 20.9 GB 
[11/20 23:35:39 visual_prompt]: 	Training 400/553. train loss: 61.5182,	0.8068 s / batch. (data: 3.02e-04). ETA=11:03:51, max mem: 20.9 GB 
[11/20 23:37:16 visual_prompt]: 	Training 500/553. train loss: 224.7789,	0.8240 s / batch. (data: 2.82e-04). ETA=11:16:40, max mem: 20.9 GB 
[11/20 23:38:06 visual_prompt]: Epoch 11 / 100: avg data time: 1.67e-01, avg batch time: 0.9873, average train loss: 171.8130
[11/20 23:39:03 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3080, average loss: 178.9753
[11/20 23:39:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.97	
[11/20 23:39:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/20 23:40:47 visual_prompt]: 	Training 100/553. train loss: 185.6118,	0.9920 s / batch. (data: 1.42e-01). ETA=13:32:04, max mem: 20.9 GB 
[11/20 23:42:26 visual_prompt]: 	Training 200/553. train loss: 41.3509,	0.8174 s / batch. (data: 5.47e-03). ETA=11:07:47, max mem: 20.9 GB 
[11/20 23:44:03 visual_prompt]: 	Training 300/553. train loss: 61.1303,	0.8614 s / batch. (data: 2.87e-02). ETA=11:42:15, max mem: 20.9 GB 
[11/20 23:45:43 visual_prompt]: 	Training 400/553. train loss: 41.7388,	0.8470 s / batch. (data: 3.13e-04). ETA=11:29:08, max mem: 20.9 GB 
[11/20 23:47:21 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8481 s / batch. (data: 8.67e-04). ETA=11:28:36, max mem: 20.9 GB 
[11/20 23:48:12 visual_prompt]: Epoch 12 / 100: avg data time: 1.72e-01, avg batch time: 0.9924, average train loss: 194.8883
[11/20 23:49:08 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3066, average loss: 327.9139
[11/20 23:49:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.36	
[11/20 23:49:08 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/20 23:50:51 visual_prompt]: 	Training 100/553. train loss: 219.3569,	0.8132 s / batch. (data: 3.47e-04). ETA=10:58:12, max mem: 20.9 GB 
[11/20 23:52:26 visual_prompt]: 	Training 200/553. train loss: 397.0056,	0.8309 s / batch. (data: 7.96e-03). ETA=11:11:10, max mem: 20.9 GB 
[11/20 23:54:06 visual_prompt]: 	Training 300/553. train loss: 206.8777,	1.7785 s / batch. (data: 9.45e-01). ETA=23:53:35, max mem: 20.9 GB 
[11/20 23:55:43 visual_prompt]: 	Training 400/553. train loss: 107.5091,	0.8204 s / batch. (data: 3.49e-04). ETA=10:59:55, max mem: 20.9 GB 
[11/20 23:57:24 visual_prompt]: 	Training 500/553. train loss: 72.6317,	0.8336 s / batch. (data: 3.50e-04). ETA=11:09:08, max mem: 20.9 GB 
[11/20 23:58:16 visual_prompt]: Epoch 13 / 100: avg data time: 1.71e-01, avg batch time: 0.9904, average train loss: 154.9497
[11/20 23:59:13 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3071, average loss: 150.0563
[11/20 23:59:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.86	
[11/20 23:59:13 visual_prompt]: Best epoch 13: best metric: -150.056
[11/20 23:59:13 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/21 00:00:57 visual_prompt]: 	Training 100/553. train loss: 67.0006,	0.8211 s / batch. (data: 5.46e-03). ETA=10:57:03, max mem: 20.9 GB 
[11/21 00:02:36 visual_prompt]: 	Training 200/553. train loss: 235.3682,	1.3400 s / batch. (data: 5.01e-01). ETA=17:49:59, max mem: 20.9 GB 
[11/21 00:04:16 visual_prompt]: 	Training 300/553. train loss: 105.3834,	0.8282 s / batch. (data: 4.80e-03). ETA=10:59:58, max mem: 20.9 GB 
[11/21 00:05:55 visual_prompt]: 	Training 400/553. train loss: 222.6599,	0.8200 s / batch. (data: 5.45e-03). ETA=10:52:03, max mem: 20.9 GB 
[11/21 00:07:35 visual_prompt]: 	Training 500/553. train loss: 442.3081,	0.8250 s / batch. (data: 1.20e-02). ETA=10:54:39, max mem: 20.9 GB 
[11/21 00:08:25 visual_prompt]: Epoch 14 / 100: avg data time: 1.79e-01, avg batch time: 0.9992, average train loss: 190.7561
[11/21 00:09:22 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3089, average loss: 204.7610
[11/21 00:09:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.46	
[11/21 00:09:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/21 00:11:06 visual_prompt]: 	Training 100/553. train loss: 86.5963,	0.8160 s / batch. (data: 5.48e-03). ETA=10:45:25, max mem: 20.9 GB 
[11/21 00:12:44 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8321 s / batch. (data: 2.22e-02). ETA=10:56:46, max mem: 20.9 GB 
[11/21 00:14:26 visual_prompt]: 	Training 300/553. train loss: 107.6311,	0.8197 s / batch. (data: 9.04e-04). ETA=10:45:35, max mem: 20.9 GB 
[11/21 00:16:03 visual_prompt]: 	Training 400/553. train loss: 29.8066,	1.3181 s / batch. (data: 5.09e-01). ETA=17:15:56, max mem: 20.9 GB 
[11/21 00:17:43 visual_prompt]: 	Training 500/553. train loss: 232.2905,	0.8203 s / batch. (data: 3.29e-04). ETA=10:43:19, max mem: 20.9 GB 
[11/21 00:18:36 visual_prompt]: Epoch 15 / 100: avg data time: 1.81e-01, avg batch time: 1.0011, average train loss: 186.5540
[11/21 00:19:33 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3077, average loss: 120.4946
[11/21 00:19:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.99	
[11/21 00:19:33 visual_prompt]: Best epoch 15: best metric: -120.495
[11/21 00:19:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/21 00:21:15 visual_prompt]: 	Training 100/553. train loss: 43.8713,	0.8523 s / batch. (data: 8.28e-03). ETA=11:06:18, max mem: 20.9 GB 
[11/21 00:22:57 visual_prompt]: 	Training 200/553. train loss: 317.8667,	0.8277 s / batch. (data: 3.19e-04). ETA=10:45:39, max mem: 20.9 GB 
[11/21 00:24:37 visual_prompt]: 	Training 300/553. train loss: 10.5811,	0.8249 s / batch. (data: 1.05e-02). ETA=10:42:04, max mem: 20.9 GB 
[11/21 00:26:18 visual_prompt]: 	Training 400/553. train loss: 195.0448,	0.8324 s / batch. (data: 8.43e-04). ETA=10:46:32, max mem: 20.9 GB 
[11/21 00:27:57 visual_prompt]: 	Training 500/553. train loss: 188.4037,	0.8873 s / batch. (data: 5.15e-02). ETA=11:27:44, max mem: 20.9 GB 
[11/21 00:28:51 visual_prompt]: Epoch 16 / 100: avg data time: 1.89e-01, avg batch time: 1.0089, average train loss: 190.2302
[11/21 00:29:48 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.3090, average loss: 2.1358
[11/21 00:29:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 52.24	
[11/21 00:29:48 visual_prompt]: Best epoch 16: best metric: -2.136
[11/21 00:29:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/21 00:31:32 visual_prompt]: 	Training 100/553. train loss: 243.7455,	0.8353 s / batch. (data: 2.46e-02). ETA=10:45:15, max mem: 20.9 GB 
[11/21 00:33:14 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8018 s / batch. (data: 3.37e-04). ETA=10:18:04, max mem: 20.9 GB 
[11/21 00:34:54 visual_prompt]: 	Training 300/553. train loss: 380.4587,	0.8302 s / batch. (data: 3.32e-04). ETA=10:38:33, max mem: 20.9 GB 
[11/21 00:36:35 visual_prompt]: 	Training 400/553. train loss: 251.5679,	1.2639 s / batch. (data: 4.57e-01). ETA=16:10:06, max mem: 20.9 GB 
[11/21 00:38:15 visual_prompt]: 	Training 500/553. train loss: 26.9485,	1.7639 s / batch. (data: 9.28e-01). ETA=22:30:56, max mem: 20.9 GB 
[11/21 00:39:08 visual_prompt]: Epoch 17 / 100: avg data time: 1.92e-01, avg batch time: 1.0123, average train loss: 160.8307
[11/21 00:40:06 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.3066, average loss: 67.9313
[11/21 00:40:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.46	
[11/21 00:40:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/21 00:41:51 visual_prompt]: 	Training 100/553. train loss: 337.4742,	0.8397 s / batch. (data: 5.44e-03). ETA=10:40:56, max mem: 20.9 GB 
[11/21 00:43:34 visual_prompt]: 	Training 200/553. train loss: 31.8491,	0.8480 s / batch. (data: 8.57e-04). ETA=10:45:52, max mem: 20.9 GB 
[11/21 00:45:14 visual_prompt]: 	Training 300/553. train loss: 227.8670,	0.8179 s / batch. (data: 3.32e-04). ETA=10:21:33, max mem: 20.9 GB 
[11/21 00:46:55 visual_prompt]: 	Training 400/553. train loss: 127.6485,	0.8218 s / batch. (data: 9.76e-03). ETA=10:23:11, max mem: 20.9 GB 
[11/21 00:48:34 visual_prompt]: 	Training 500/553. train loss: 38.8039,	0.8200 s / batch. (data: 4.71e-04). ETA=10:20:25, max mem: 20.9 GB 
[11/21 00:49:26 visual_prompt]: Epoch 18 / 100: avg data time: 1.93e-01, avg batch time: 1.0122, average train loss: 172.3706
[11/21 00:50:23 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3083, average loss: 23.0419
[11/21 00:50:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.30	
[11/21 00:50:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/21 00:52:07 visual_prompt]: 	Training 100/553. train loss: 30.3725,	0.8153 s / batch. (data: 5.56e-03). ETA=10:14:49, max mem: 20.9 GB 
[11/21 00:53:48 visual_prompt]: 	Training 200/553. train loss: 76.1996,	0.8030 s / batch. (data: 5.46e-03). ETA=10:04:11, max mem: 20.9 GB 
[11/21 00:55:28 visual_prompt]: 	Training 300/553. train loss: 526.4801,	0.8115 s / batch. (data: 3.47e-04). ETA=10:09:16, max mem: 20.9 GB 
[11/21 00:57:11 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8539 s / batch. (data: 8.21e-04). ETA=10:39:39, max mem: 20.9 GB 
[11/21 00:58:47 visual_prompt]: 	Training 500/553. train loss: 9.3277,	0.8125 s / batch. (data: 3.19e-04). ETA=10:07:19, max mem: 20.9 GB 
[11/21 00:59:40 visual_prompt]: Epoch 19 / 100: avg data time: 1.86e-01, avg batch time: 1.0069, average train loss: 195.7430
[11/21 01:00:38 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.3093, average loss: 76.6227
[11/21 01:00:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.08	
[11/21 01:00:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/21 01:02:21 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 3.13e-04). ETA=10:07:49, max mem: 20.9 GB 
[11/21 01:04:02 visual_prompt]: 	Training 200/553. train loss: 80.9129,	0.8260 s / batch. (data: 4.15e-04). ETA=10:13:54, max mem: 20.9 GB 
[11/21 01:05:43 visual_prompt]: 	Training 300/553. train loss: 117.1381,	0.8234 s / batch. (data: 5.44e-03). ETA=10:10:35, max mem: 20.9 GB 
[11/21 01:07:23 visual_prompt]: 	Training 400/553. train loss: 17.3723,	0.8280 s / batch. (data: 3.25e-04). ETA=10:12:38, max mem: 20.9 GB 
[11/21 01:09:03 visual_prompt]: 	Training 500/553. train loss: 100.9234,	0.8184 s / batch. (data: 3.19e-04). ETA=10:04:07, max mem: 20.9 GB 
[11/21 01:09:56 visual_prompt]: Epoch 20 / 100: avg data time: 1.91e-01, avg batch time: 1.0105, average train loss: 160.8579
[11/21 01:10:54 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3071, average loss: 58.2357
[11/21 01:10:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.21	
[11/21 01:10:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/21 01:12:41 visual_prompt]: 	Training 100/553. train loss: 113.3547,	0.8141 s / batch. (data: 1.05e-02). ETA=9:58:52, max mem: 20.9 GB 
[11/21 01:14:21 visual_prompt]: 	Training 200/553. train loss: 94.7154,	0.8196 s / batch. (data: 3.86e-04). ETA=10:01:33, max mem: 20.9 GB 
[11/21 01:16:02 visual_prompt]: 	Training 300/553. train loss: 420.3132,	1.1844 s / batch. (data: 3.67e-01). ETA=14:27:23, max mem: 20.9 GB 
[11/21 01:17:41 visual_prompt]: 	Training 400/553. train loss: 258.5422,	0.8440 s / batch. (data: 3.23e-04). ETA=10:16:40, max mem: 20.9 GB 
[11/21 01:19:23 visual_prompt]: 	Training 500/553. train loss: 24.6469,	0.8520 s / batch. (data: 1.20e-02). ETA=10:21:05, max mem: 20.9 GB 
[11/21 01:20:14 visual_prompt]: Epoch 21 / 100: avg data time: 1.93e-01, avg batch time: 1.0127, average train loss: 152.4561
[11/21 01:21:12 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3072, average loss: 8.9101
[11/21 01:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.52	
[11/21 01:21:12 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/21 01:22:55 visual_prompt]: 	Training 100/553. train loss: 76.3283,	0.8294 s / batch. (data: 9.34e-03). ETA=10:02:30, max mem: 20.9 GB 
[11/21 01:24:36 visual_prompt]: 	Training 200/553. train loss: 141.7162,	0.8000 s / batch. (data: 3.12e-04). ETA=9:39:49, max mem: 20.9 GB 
[11/21 01:26:14 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8182 s / batch. (data: 3.26e-04). ETA=9:51:39, max mem: 20.9 GB 
[11/21 01:27:56 visual_prompt]: 	Training 400/553. train loss: 18.4423,	0.8050 s / batch. (data: 3.32e-04). ETA=9:40:47, max mem: 20.9 GB 
[11/21 01:29:37 visual_prompt]: 	Training 500/553. train loss: 34.7550,	0.8206 s / batch. (data: 3.32e-04). ETA=9:50:40, max mem: 20.9 GB 
[11/21 01:30:30 visual_prompt]: Epoch 22 / 100: avg data time: 1.89e-01, avg batch time: 1.0093, average train loss: 170.8602
[11/21 01:31:28 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3072, average loss: 246.9585
[11/21 01:31:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.10	
[11/21 01:31:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/21 01:33:14 visual_prompt]: 	Training 100/553. train loss: 186.1240,	0.8440 s / batch. (data: 3.18e-04). ETA=10:05:20, max mem: 20.9 GB 
[11/21 01:34:55 visual_prompt]: 	Training 200/553. train loss: 272.6903,	0.8723 s / batch. (data: 3.40e-02). ETA=10:24:11, max mem: 20.9 GB 
[11/21 01:36:38 visual_prompt]: 	Training 300/553. train loss: 47.7774,	0.8240 s / batch. (data: 3.96e-04). ETA=9:48:15, max mem: 20.9 GB 
[11/21 01:38:16 visual_prompt]: 	Training 400/553. train loss: 123.1810,	0.8307 s / batch. (data: 3.28e-04). ETA=9:51:38, max mem: 20.9 GB 
[11/21 01:39:55 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8360 s / batch. (data: 7.95e-03). ETA=9:54:01, max mem: 20.9 GB 
[11/21 01:40:47 visual_prompt]: Epoch 23 / 100: avg data time: 1.91e-01, avg batch time: 1.0115, average train loss: 160.6898
[11/21 01:41:45 visual_prompt]: Inference (val):avg data time: 2.23e-04, avg batch time: 0.3086, average loss: 47.8060
[11/21 01:41:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.12	
[11/21 01:41:45 visual_prompt]: Stopping early.
[11/21 01:41:45 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 01:41:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 01:41:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 01:41:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/21 01:41:45 visual_prompt]: Training with config:
[11/21 01:41:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/21 01:41:45 visual_prompt]: Loading training data...
[11/21 01:41:45 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 01:41:45 visual_prompt]: Loading validation data...
[11/21 01:41:45 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 01:41:45 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 01:41:48 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/21 01:41:48 visual_prompt]: tuned percent:0.525
[11/21 01:41:48 visual_prompt]: Device used for model: 0
[11/21 01:41:48 visual_prompt]: Setting up Evaluator...
[11/21 01:41:48 visual_prompt]: Setting up Trainer...
[11/21 01:41:48 visual_prompt]: 	Setting up the optimizer...
[11/21 01:41:48 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 01:43:32 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8607 s / batch. (data: 8.63e-03). ETA=13:11:48, max mem: 20.9 GB 
[11/21 01:45:11 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8122 s / batch. (data: 3.09e-04). ETA=12:25:54, max mem: 20.9 GB 
[11/21 01:46:54 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.5719 s / batch. (data: 7.36e-01). ETA=1 day, 0:00:55, max mem: 20.9 GB 
[11/21 01:48:33 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8123 s / batch. (data: 3.20e-04). ETA=12:23:17, max mem: 20.9 GB 
[11/21 01:50:16 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8379 s / batch. (data: 1.28e-03). ETA=12:45:15, max mem: 20.9 GB 
[11/21 01:51:09 visual_prompt]: Epoch 1 / 100: avg data time: 1.86e-01, avg batch time: 1.0133, average train loss: 1.5403
[11/21 01:52:06 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3081, average loss: 1.5201
[11/21 01:52:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/21 01:52:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/21 01:53:51 visual_prompt]: 	Training 100/553. train loss: 10.6743,	0.8442 s / batch. (data: 1.21e-02). ETA=12:48:53, max mem: 20.9 GB 
[11/21 01:55:31 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.8655 s / batch. (data: 1.06e+00). ETA=1 day, 4:15:55, max mem: 20.9 GB 
[11/21 01:57:14 visual_prompt]: 	Training 300/553. train loss: 0.8119,	1.1320 s / batch. (data: 3.01e-01). ETA=17:07:13, max mem: 20.9 GB 
[11/21 01:58:53 visual_prompt]: 	Training 400/553. train loss: 11.9164,	0.8201 s / batch. (data: 3.12e-04). ETA=12:22:47, max mem: 20.9 GB 
[11/21 02:00:36 visual_prompt]: 	Training 500/553. train loss: 8.2818,	0.8280 s / batch. (data: 1.63e-02). ETA=12:28:35, max mem: 20.9 GB 
[11/21 02:01:27 visual_prompt]: Epoch 2 / 100: avg data time: 1.88e-01, avg batch time: 1.0142, average train loss: 15.2291
[11/21 02:02:25 visual_prompt]: Inference (val):avg data time: 2.71e-04, avg batch time: 0.3086, average loss: 15.8872
[11/21 02:02:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.27	
[11/21 02:02:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/21 02:04:08 visual_prompt]: 	Training 100/553. train loss: 37.7481,	0.9280 s / batch. (data: 9.25e-02). ETA=13:56:38, max mem: 20.9 GB 
[11/21 02:05:50 visual_prompt]: 	Training 200/553. train loss: 27.9482,	1.1360 s / batch. (data: 2.87e-01). ETA=17:02:18, max mem: 20.9 GB 
[11/21 02:07:30 visual_prompt]: 	Training 300/553. train loss: 10.3654,	0.8352 s / batch. (data: 7.16e-03). ETA=12:30:12, max mem: 20.9 GB 
[11/21 02:09:11 visual_prompt]: 	Training 400/553. train loss: 96.2913,	0.8320 s / batch. (data: 3.10e-04). ETA=12:25:57, max mem: 20.9 GB 
[11/21 02:10:52 visual_prompt]: 	Training 500/553. train loss: 67.0479,	1.3360 s / batch. (data: 5.16e-01). ETA=19:55:35, max mem: 20.9 GB 
[11/21 02:11:44 visual_prompt]: Epoch 3 / 100: avg data time: 1.86e-01, avg batch time: 1.0103, average train loss: 40.2478
[11/21 02:12:42 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3082, average loss: 14.2773
[11/21 02:12:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[11/21 02:12:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/21 02:14:27 visual_prompt]: 	Training 100/553. train loss: 16.8942,	0.8322 s / batch. (data: 3.43e-04). ETA=12:22:34, max mem: 20.9 GB 
[11/21 02:16:10 visual_prompt]: 	Training 200/553. train loss: 4.3071,	0.8446 s / batch. (data: 1.05e-02). ETA=12:32:15, max mem: 20.9 GB 
[11/21 02:17:50 visual_prompt]: 	Training 300/553. train loss: 86.3309,	1.4640 s / batch. (data: 6.43e-01). ETA=21:41:33, max mem: 20.9 GB 
[11/21 02:19:27 visual_prompt]: 	Training 400/553. train loss: 79.9603,	1.5764 s / batch. (data: 7.77e-01). ETA=23:18:47, max mem: 20.9 GB 
[11/21 02:21:10 visual_prompt]: 	Training 500/553. train loss: 0.0007,	3.5331 s / batch. (data: 2.73e+00). ETA=2 days, 4:09:12, max mem: 20.9 GB 
[11/21 02:22:04 visual_prompt]: Epoch 4 / 100: avg data time: 1.94e-01, avg batch time: 1.0156, average train loss: 63.0865
[11/21 02:23:01 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3086, average loss: 14.3394
[11/21 02:23:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.73	
[11/21 02:23:01 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/21 02:24:44 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8148 s / batch. (data: 7.96e-03). ETA=11:59:35, max mem: 20.9 GB 
[11/21 02:26:25 visual_prompt]: 	Training 200/553. train loss: 38.8275,	1.1760 s / batch. (data: 3.60e-01). ETA=17:16:37, max mem: 20.9 GB 
[11/21 02:28:07 visual_prompt]: 	Training 300/553. train loss: 151.9556,	0.8515 s / batch. (data: 8.03e-04). ETA=12:29:10, max mem: 20.9 GB 
[11/21 02:29:46 visual_prompt]: 	Training 400/553. train loss: 228.9831,	0.8360 s / batch. (data: 3.29e-04). ETA=12:14:06, max mem: 20.9 GB 
[11/21 02:31:28 visual_prompt]: 	Training 500/553. train loss: 162.6530,	0.8067 s / batch. (data: 2.98e-04). ETA=11:47:02, max mem: 20.9 GB 
[11/21 02:32:21 visual_prompt]: Epoch 5 / 100: avg data time: 1.90e-01, avg batch time: 1.0121, average train loss: 59.6810
[11/21 02:33:19 visual_prompt]: Inference (val):avg data time: 2.72e-04, avg batch time: 0.3095, average loss: 53.9044
[11/21 02:33:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.30	
[11/21 02:33:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/21 02:35:04 visual_prompt]: 	Training 100/553. train loss: 39.3588,	0.8169 s / batch. (data: 8.91e-03). ETA=11:53:55, max mem: 20.9 GB 
[11/21 02:36:45 visual_prompt]: 	Training 200/553. train loss: 64.8807,	0.8320 s / batch. (data: 5.44e-03). ETA=12:05:42, max mem: 20.9 GB 
[11/21 02:38:23 visual_prompt]: 	Training 300/553. train loss: 19.3553,	0.8266 s / batch. (data: 5.47e-03). ETA=11:59:38, max mem: 20.9 GB 
[11/21 02:40:09 visual_prompt]: 	Training 400/553. train loss: 41.5402,	0.8320 s / batch. (data: 5.51e-03). ETA=12:02:58, max mem: 20.9 GB 
[11/21 02:41:47 visual_prompt]: 	Training 500/553. train loss: 67.3078,	0.8235 s / batch. (data: 3.96e-04). ETA=11:54:12, max mem: 20.9 GB 
[11/21 02:42:39 visual_prompt]: Epoch 6 / 100: avg data time: 1.90e-01, avg batch time: 1.0132, average train loss: 76.6691
[11/21 02:43:37 visual_prompt]: Inference (val):avg data time: 3.37e-04, avg batch time: 0.3082, average loss: 44.6269
[11/21 02:43:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.56	
[11/21 02:43:37 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/21 02:45:21 visual_prompt]: 	Training 100/553. train loss: 801.0915,	0.8625 s / batch. (data: 1.11e-02). ETA=12:25:47, max mem: 20.9 GB 
[11/21 02:47:01 visual_prompt]: 	Training 200/553. train loss: 8.7732,	0.8320 s / batch. (data: 3.12e-04). ETA=11:58:02, max mem: 20.9 GB 
[11/21 02:48:45 visual_prompt]: 	Training 300/553. train loss: 77.1621,	1.9350 s / batch. (data: 1.13e+00). ETA=1 day, 3:46:45, max mem: 20.9 GB 
[11/21 02:50:26 visual_prompt]: 	Training 400/553. train loss: 87.9645,	1.8200 s / batch. (data: 1.00e+00). ETA=1 day, 2:04:39, max mem: 20.9 GB 
[11/21 02:52:05 visual_prompt]: 	Training 500/553. train loss: 22.3223,	0.8299 s / batch. (data: 3.02e-04). ETA=11:52:06, max mem: 20.9 GB 
[11/21 02:52:56 visual_prompt]: Epoch 7 / 100: avg data time: 1.89e-01, avg batch time: 1.0104, average train loss: 118.3119
[11/21 02:53:54 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3073, average loss: 200.2819
[11/21 02:53:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.85	
[11/21 02:53:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/21 02:55:36 visual_prompt]: 	Training 100/553. train loss: 38.9741,	0.8400 s / batch. (data: 3.49e-04). ETA=11:58:36, max mem: 20.9 GB 
[11/21 02:57:19 visual_prompt]: 	Training 200/553. train loss: 10.0451,	0.8445 s / batch. (data: 3.89e-04). ETA=12:01:02, max mem: 20.9 GB 
[11/21 02:59:00 visual_prompt]: 	Training 300/553. train loss: 94.3741,	0.8050 s / batch. (data: 4.50e-03). ETA=11:26:00, max mem: 20.9 GB 
[11/21 03:00:41 visual_prompt]: 	Training 400/553. train loss: 1.2774,	0.8960 s / batch. (data: 6.66e-02). ETA=12:42:01, max mem: 20.9 GB 
[11/21 03:02:23 visual_prompt]: 	Training 500/553. train loss: 130.4558,	1.6154 s / batch. (data: 7.85e-01). ETA=22:51:12, max mem: 20.9 GB 
[11/21 03:03:16 visual_prompt]: Epoch 8 / 100: avg data time: 1.95e-01, avg batch time: 1.0158, average train loss: 105.7661
[11/21 03:04:13 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3080, average loss: 36.9941
[11/21 03:04:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.93	
[11/21 03:04:13 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/21 03:05:58 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8127 s / batch. (data: 3.04e-04). ETA=11:27:44, max mem: 20.9 GB 
[11/21 03:07:37 visual_prompt]: 	Training 200/553. train loss: 0.3531,	0.8392 s / batch. (data: 2.33e-02). ETA=11:48:49, max mem: 20.9 GB 
[11/21 03:09:18 visual_prompt]: 	Training 300/553. train loss: 52.0310,	1.3560 s / batch. (data: 5.22e-01). ETA=19:03:00, max mem: 20.9 GB 
[11/21 03:11:00 visual_prompt]: 	Training 400/553. train loss: 45.0630,	0.8536 s / batch. (data: 8.34e-04). ETA=11:58:06, max mem: 20.9 GB 
[11/21 03:12:41 visual_prompt]: 	Training 500/553. train loss: 277.6619,	0.8068 s / batch. (data: 2.43e-04). ETA=11:17:24, max mem: 20.9 GB 
[11/21 03:13:32 visual_prompt]: Epoch 9 / 100: avg data time: 1.89e-01, avg batch time: 1.0107, average train loss: 126.2693
[11/21 03:14:30 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3077, average loss: 152.5892
[11/21 03:14:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.18	
[11/21 03:14:30 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/21 03:16:17 visual_prompt]: 	Training 100/553. train loss: 187.3826,	0.8560 s / batch. (data: 7.94e-03). ETA=11:56:30, max mem: 20.9 GB 
[11/21 03:17:57 visual_prompt]: 	Training 200/553. train loss: 37.7419,	0.8280 s / batch. (data: 3.20e-04). ETA=11:31:41, max mem: 20.9 GB 
[11/21 03:19:36 visual_prompt]: 	Training 300/553. train loss: 90.5083,	1.1339 s / batch. (data: 3.08e-01). ETA=15:45:22, max mem: 20.9 GB 
[11/21 03:21:14 visual_prompt]: 	Training 400/553. train loss: 265.5869,	0.8051 s / batch. (data: 3.33e-04). ETA=11:09:54, max mem: 20.9 GB 
[11/21 03:22:57 visual_prompt]: 	Training 500/553. train loss: 52.3765,	0.8463 s / batch. (data: 1.50e-02). ETA=11:42:42, max mem: 20.9 GB 
[11/21 03:23:50 visual_prompt]: Epoch 10 / 100: avg data time: 1.92e-01, avg batch time: 1.0117, average train loss: 146.8459
[11/21 03:24:47 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3079, average loss: 202.4706
[11/21 03:24:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.96	
[11/21 03:24:47 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/21 03:26:34 visual_prompt]: 	Training 100/553. train loss: 170.1281,	0.8375 s / batch. (data: 9.43e-03). ETA=11:33:16, max mem: 20.9 GB 
[11/21 03:28:16 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.7991 s / batch. (data: 4.30e-04). ETA=11:00:09, max mem: 20.9 GB 
[11/21 03:29:56 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1788 s / batch. (data: 1.34e+00). ETA=1 day, 5:56:24, max mem: 20.9 GB 
[11/21 03:31:35 visual_prompt]: 	Training 400/553. train loss: 144.2417,	0.8091 s / batch. (data: 3.94e-04). ETA=11:05:44, max mem: 20.9 GB 
[11/21 03:33:15 visual_prompt]: 	Training 500/553. train loss: 227.5852,	0.8156 s / batch. (data: 3.28e-04). ETA=11:09:44, max mem: 20.9 GB 
[11/21 03:34:07 visual_prompt]: Epoch 11 / 100: avg data time: 1.91e-01, avg batch time: 1.0111, average train loss: 167.5750
[11/21 03:35:04 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3076, average loss: 24.0446
[11/21 03:35:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.40	
[11/21 03:35:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/21 03:36:51 visual_prompt]: 	Training 100/553. train loss: 120.9846,	0.8104 s / batch. (data: 7.94e-03). ETA=11:03:24, max mem: 20.9 GB 
[11/21 03:38:33 visual_prompt]: 	Training 200/553. train loss: 51.3779,	1.6360 s / batch. (data: 8.30e-01). ETA=22:16:32, max mem: 20.9 GB 
[11/21 03:40:12 visual_prompt]: 	Training 300/553. train loss: 18.5274,	0.8320 s / batch. (data: 3.49e-04). ETA=11:18:18, max mem: 20.9 GB 
[11/21 03:41:53 visual_prompt]: 	Training 400/553. train loss: 151.8916,	0.8347 s / batch. (data: 1.06e-02). ETA=11:19:07, max mem: 20.9 GB 
[11/21 03:43:33 visual_prompt]: 	Training 500/553. train loss: 212.2830,	0.8210 s / batch. (data: 7.67e-04). ETA=11:06:36, max mem: 20.9 GB 
[11/21 03:44:25 visual_prompt]: Epoch 12 / 100: avg data time: 1.92e-01, avg batch time: 1.0136, average train loss: 174.7716
[11/21 03:45:23 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3095, average loss: 324.7250
[11/21 03:45:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.69	
[11/21 03:45:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/21 03:47:09 visual_prompt]: 	Training 100/553. train loss: 46.6780,	0.8327 s / batch. (data: 1.56e-02). ETA=11:13:59, max mem: 20.9 GB 
[11/21 03:48:47 visual_prompt]: 	Training 200/553. train loss: 108.3145,	0.8181 s / batch. (data: 6.24e-03). ETA=11:00:48, max mem: 20.9 GB 
[11/21 03:50:28 visual_prompt]: 	Training 300/553. train loss: 56.2402,	1.8840 s / batch. (data: 1.05e+00). ETA=1 day, 1:18:37, max mem: 20.9 GB 
[11/21 03:52:07 visual_prompt]: 	Training 400/553. train loss: 401.1780,	0.8280 s / batch. (data: 3.16e-04). ETA=11:06:02, max mem: 20.9 GB 
[11/21 03:53:50 visual_prompt]: 	Training 500/553. train loss: 278.3896,	0.8240 s / batch. (data: 3.15e-04). ETA=11:01:27, max mem: 20.9 GB 
[11/21 03:54:42 visual_prompt]: Epoch 13 / 100: avg data time: 1.91e-01, avg batch time: 1.0113, average train loss: 174.0920
[11/21 03:55:40 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.3078, average loss: 44.2031
[11/21 03:55:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 50.94	
[11/21 03:55:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/21 03:57:26 visual_prompt]: 	Training 100/553. train loss: 1101.5603,	0.8341 s / batch. (data: 1.01e-02). ETA=11:07:26, max mem: 20.9 GB 
[11/21 03:59:07 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.4075 s / batch. (data: 6.06e-01). ETA=18:43:54, max mem: 20.9 GB 
[11/21 04:00:47 visual_prompt]: 	Training 300/553. train loss: 54.3812,	0.8369 s / batch. (data: 1.06e-02). ETA=11:06:53, max mem: 20.9 GB 
[11/21 04:02:27 visual_prompt]: 	Training 400/553. train loss: 266.3464,	0.8013 s / batch. (data: 3.08e-04). ETA=10:37:09, max mem: 20.9 GB 
[11/21 04:04:08 visual_prompt]: 	Training 500/553. train loss: 163.6314,	0.8075 s / batch. (data: 3.02e-04). ETA=10:40:46, max mem: 20.9 GB 
[11/21 04:05:00 visual_prompt]: Epoch 14 / 100: avg data time: 1.92e-01, avg batch time: 1.0124, average train loss: 158.8360
[11/21 04:05:57 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3078, average loss: 98.8413
[11/21 04:05:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.20	
[11/21 04:05:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/21 04:07:42 visual_prompt]: 	Training 100/553. train loss: 106.0826,	0.8190 s / batch. (data: 6.92e-04). ETA=10:47:46, max mem: 20.9 GB 
[11/21 04:09:22 visual_prompt]: 	Training 200/553. train loss: 797.1801,	0.8197 s / batch. (data: 3.14e-04). ETA=10:47:00, max mem: 20.9 GB 
[11/21 04:11:04 visual_prompt]: 	Training 300/553. train loss: 390.2479,	0.8398 s / batch. (data: 6.01e-03). ETA=11:01:26, max mem: 20.9 GB 
[11/21 04:12:42 visual_prompt]: 	Training 400/553. train loss: 66.4679,	0.8531 s / batch. (data: 4.17e-03). ETA=11:10:30, max mem: 20.9 GB 
[11/21 04:14:24 visual_prompt]: 	Training 500/553. train loss: 56.4750,	0.8080 s / batch. (data: 3.50e-04). ETA=10:33:42, max mem: 20.9 GB 
[11/21 04:15:17 visual_prompt]: Epoch 15 / 100: avg data time: 1.92e-01, avg batch time: 1.0122, average train loss: 165.2661
[11/21 04:16:15 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3070, average loss: 518.4259
[11/21 04:16:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/21 04:16:15 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/21 04:17:59 visual_prompt]: 	Training 100/553. train loss: 103.7124,	0.8249 s / batch. (data: 2.97e-04). ETA=10:44:52, max mem: 20.9 GB 
[11/21 04:19:40 visual_prompt]: 	Training 200/553. train loss: 263.4484,	0.8360 s / batch. (data: 3.66e-04). ETA=10:52:08, max mem: 20.9 GB 
[11/21 04:21:21 visual_prompt]: 	Training 300/553. train loss: 192.0815,	0.8130 s / batch. (data: 3.07e-04). ETA=10:32:52, max mem: 20.9 GB 
[11/21 04:23:01 visual_prompt]: 	Training 400/553. train loss: 154.5278,	0.8384 s / batch. (data: 3.16e-04). ETA=10:51:12, max mem: 20.9 GB 
[11/21 04:24:41 visual_prompt]: 	Training 500/553. train loss: 77.0727,	1.6886 s / batch. (data: 8.92e-01). ETA=21:48:48, max mem: 20.9 GB 
[11/21 04:25:35 visual_prompt]: Epoch 16 / 100: avg data time: 1.91e-01, avg batch time: 1.0113, average train loss: 160.8979
[11/21 04:26:32 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.3096, average loss: 42.0164
[11/21 04:26:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 51.17	
[11/21 04:26:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/21 04:28:16 visual_prompt]: 	Training 100/553. train loss: 53.9767,	0.8344 s / batch. (data: 1.19e-02). ETA=10:44:36, max mem: 20.9 GB 
[11/21 04:29:58 visual_prompt]: 	Training 200/553. train loss: 253.7905,	0.8239 s / batch. (data: 3.23e-04). ETA=10:35:08, max mem: 20.9 GB 
[11/21 04:31:37 visual_prompt]: 	Training 300/553. train loss: 176.7908,	0.8394 s / batch. (data: 1.14e-02). ETA=10:45:41, max mem: 20.9 GB 
[11/21 04:33:17 visual_prompt]: 	Training 400/553. train loss: 53.5881,	1.3600 s / batch. (data: 5.42e-01). ETA=17:23:50, max mem: 20.9 GB 
[11/21 04:34:57 visual_prompt]: 	Training 500/553. train loss: 101.5773,	1.6005 s / batch. (data: 7.94e-01). ETA=20:25:45, max mem: 20.9 GB 
[11/21 04:35:51 visual_prompt]: Epoch 17 / 100: avg data time: 1.91e-01, avg batch time: 1.0109, average train loss: 177.7189
[11/21 04:36:49 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.3077, average loss: 180.0793
[11/21 04:36:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.05	
[11/21 04:36:49 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/21 04:38:35 visual_prompt]: 	Training 100/553. train loss: 42.3787,	0.8560 s / batch. (data: 3.05e-04). ETA=10:53:22, max mem: 20.9 GB 
[11/21 04:40:18 visual_prompt]: 	Training 200/553. train loss: 41.7151,	0.8407 s / batch. (data: 1.19e-03). ETA=10:40:18, max mem: 20.9 GB 
[11/21 04:41:59 visual_prompt]: 	Training 300/553. train loss: 76.5457,	0.8080 s / batch. (data: 3.13e-04). ETA=10:14:04, max mem: 20.9 GB 
[11/21 04:43:39 visual_prompt]: 	Training 400/553. train loss: 110.9185,	0.8207 s / batch. (data: 5.41e-03). ETA=10:22:21, max mem: 20.9 GB 
[11/21 04:45:19 visual_prompt]: 	Training 500/553. train loss: 114.4731,	0.8120 s / batch. (data: 3.21e-04). ETA=10:14:23, max mem: 20.9 GB 
[11/21 04:46:10 visual_prompt]: Epoch 18 / 100: avg data time: 1.94e-01, avg batch time: 1.0140, average train loss: 154.6931
[11/21 04:47:08 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3109, average loss: 218.2068
[11/21 04:47:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.01	
[11/21 04:47:08 visual_prompt]: Stopping early.
[11/21 04:47:08 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 04:47:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 04:47:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 04:47:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/21 04:47:08 visual_prompt]: Training with config:
[11/21 04:47:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/21 04:47:08 visual_prompt]: Loading training data...
[11/21 04:47:08 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 04:47:08 visual_prompt]: Loading validation data...
[11/21 04:47:08 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 04:47:08 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 04:47:11 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/21 04:47:11 visual_prompt]: tuned percent:0.525
[11/21 04:47:11 visual_prompt]: Device used for model: 0
[11/21 04:47:11 visual_prompt]: Setting up Evaluator...
[11/21 04:47:11 visual_prompt]: Setting up Trainer...
[11/21 04:47:11 visual_prompt]: 	Setting up the optimizer...
[11/21 04:47:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 04:48:55 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8160 s / batch. (data: 3.04e-04). ETA=12:30:43, max mem: 20.9 GB 
[11/21 04:50:34 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8400 s / batch. (data: 1.19e-02). ETA=12:51:24, max mem: 20.9 GB 
[11/21 04:52:18 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.2840 s / batch. (data: 4.59e-01). ETA=19:37:00, max mem: 20.9 GB 
[11/21 04:53:57 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8400 s / batch. (data: 7.97e-03). ETA=12:48:37, max mem: 20.9 GB 
[11/21 04:55:40 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8440 s / batch. (data: 8.18e-04). ETA=12:50:50, max mem: 20.9 GB 
[11/21 04:56:33 visual_prompt]: Epoch 1 / 100: avg data time: 1.89e-01, avg batch time: 1.0160, average train loss: 1.5403
[11/21 04:57:31 visual_prompt]: Inference (val):avg data time: 5.19e-05, avg batch time: 0.3075, average loss: 1.5201
[11/21 04:57:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/21 04:57:31 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/21 04:59:15 visual_prompt]: 	Training 100/553. train loss: 25.0325,	1.6385 s / batch. (data: 8.14e-01). ETA=1 day, 0:52:18, max mem: 20.9 GB 
[11/21 05:00:55 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8240 s / batch. (data: 7.95e-03). ETA=12:29:07, max mem: 20.9 GB 
[11/21 05:02:38 visual_prompt]: 	Training 300/553. train loss: 7.4490,	1.0600 s / batch. (data: 2.37e-01). ETA=16:01:53, max mem: 20.9 GB 
[11/21 05:04:17 visual_prompt]: 	Training 400/553. train loss: 29.1996,	0.8233 s / batch. (data: 5.47e-03). ETA=12:25:44, max mem: 20.9 GB 
[11/21 05:06:00 visual_prompt]: 	Training 500/553. train loss: 80.6536,	0.8099 s / batch. (data: 3.23e-04). ETA=12:12:17, max mem: 20.9 GB 
[11/21 05:06:52 visual_prompt]: Epoch 2 / 100: avg data time: 1.90e-01, avg batch time: 1.0137, average train loss: 26.5332
[11/21 05:07:49 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3077, average loss: 67.9892
[11/21 05:07:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.61	
[11/21 05:07:49 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/21 05:09:32 visual_prompt]: 	Training 100/553. train loss: 60.1347,	0.8080 s / batch. (data: 3.09e-04). ETA=12:08:27, max mem: 20.9 GB 
[11/21 05:11:14 visual_prompt]: 	Training 200/553. train loss: 29.4592,	0.8320 s / batch. (data: 1.20e-02). ETA=12:28:43, max mem: 20.9 GB 
[11/21 05:12:54 visual_prompt]: 	Training 300/553. train loss: 40.9004,	0.8238 s / batch. (data: 3.07e-04). ETA=12:19:57, max mem: 20.9 GB 
[11/21 05:14:35 visual_prompt]: 	Training 400/553. train loss: 14.7090,	0.8367 s / batch. (data: 2.57e-02). ETA=12:30:10, max mem: 20.9 GB 
[11/21 05:16:17 visual_prompt]: 	Training 500/553. train loss: 23.5847,	1.4676 s / batch. (data: 6.46e-01). ETA=21:53:22, max mem: 20.9 GB 
[11/21 05:17:08 visual_prompt]: Epoch 3 / 100: avg data time: 1.89e-01, avg batch time: 1.0107, average train loss: 43.5661
[11/21 05:18:06 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3084, average loss: 40.6659
[11/21 05:18:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.00	
[11/21 05:18:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/21 05:19:53 visual_prompt]: 	Training 100/553. train loss: 39.0314,	0.8328 s / batch. (data: 1.05e-02). ETA=12:23:08, max mem: 20.9 GB 
[11/21 05:21:34 visual_prompt]: 	Training 200/553. train loss: 61.0410,	0.8329 s / batch. (data: 8.54e-03). ETA=12:21:49, max mem: 20.9 GB 
[11/21 05:23:14 visual_prompt]: 	Training 300/553. train loss: 14.5468,	0.8147 s / batch. (data: 3.28e-04). ETA=12:04:18, max mem: 20.9 GB 
[11/21 05:24:51 visual_prompt]: 	Training 400/553. train loss: 25.0609,	0.8400 s / batch. (data: 1.60e-02). ETA=12:25:22, max mem: 20.9 GB 
[11/21 05:26:34 visual_prompt]: 	Training 500/553. train loss: 145.8390,	3.7294 s / batch. (data: 2.92e+00). ETA=2 days, 7:03:04, max mem: 20.9 GB 
[11/21 05:27:28 visual_prompt]: Epoch 4 / 100: avg data time: 1.91e-01, avg batch time: 1.0145, average train loss: 41.9266
[11/21 05:28:25 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3101, average loss: 32.3341
[11/21 05:28:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.60	
[11/21 05:28:25 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/21 05:30:09 visual_prompt]: 	Training 100/553. train loss: 386.8663,	0.8362 s / batch. (data: 3.40e-04). ETA=12:18:28, max mem: 20.9 GB 
[11/21 05:31:50 visual_prompt]: 	Training 200/553. train loss: 53.7453,	1.4591 s / batch. (data: 6.41e-01). ETA=21:26:10, max mem: 20.9 GB 
[11/21 05:33:32 visual_prompt]: 	Training 300/553. train loss: 214.9673,	0.8149 s / batch. (data: 3.12e-04). ETA=11:56:57, max mem: 20.9 GB 
[11/21 05:35:12 visual_prompt]: 	Training 400/553. train loss: 27.8766,	0.8469 s / batch. (data: 1.05e-02). ETA=12:23:42, max mem: 20.9 GB 
[11/21 05:36:53 visual_prompt]: 	Training 500/553. train loss: 19.4011,	0.8461 s / batch. (data: 1.01e-02). ETA=12:21:36, max mem: 20.9 GB 
[11/21 05:37:47 visual_prompt]: Epoch 5 / 100: avg data time: 1.94e-01, avg batch time: 1.0148, average train loss: 85.3709
[11/21 05:38:45 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.3062, average loss: 19.8969
[11/21 05:38:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.41	
[11/21 05:38:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/21 05:40:31 visual_prompt]: 	Training 100/553. train loss: 88.2537,	0.8289 s / batch. (data: 1.06e-02). ETA=12:04:22, max mem: 20.9 GB 
[11/21 05:42:11 visual_prompt]: 	Training 200/553. train loss: 21.5776,	0.8174 s / batch. (data: 5.47e-03). ETA=11:52:56, max mem: 20.9 GB 
[11/21 05:43:51 visual_prompt]: 	Training 300/553. train loss: 217.9171,	0.8240 s / batch. (data: 3.11e-04). ETA=11:57:22, max mem: 20.9 GB 
[11/21 05:45:35 visual_prompt]: 	Training 400/553. train loss: 6.9724,	0.8280 s / batch. (data: 6.05e-03). ETA=11:59:28, max mem: 20.9 GB 
[11/21 05:47:14 visual_prompt]: 	Training 500/553. train loss: 1.2620,	0.8509 s / batch. (data: 1.06e-02). ETA=12:17:58, max mem: 20.9 GB 
[11/21 05:48:05 visual_prompt]: Epoch 6 / 100: avg data time: 1.93e-01, avg batch time: 1.0139, average train loss: 87.7367
[11/21 05:49:03 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3076, average loss: 26.4101
[11/21 05:49:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.30	
[11/21 05:49:03 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/21 05:50:47 visual_prompt]: 	Training 100/553. train loss: 50.2073,	0.8280 s / batch. (data: 7.94e-03). ETA=11:55:57, max mem: 20.9 GB 
[11/21 05:52:27 visual_prompt]: 	Training 200/553. train loss: 18.5600,	0.8200 s / batch. (data: 3.18e-04). ETA=11:47:39, max mem: 20.9 GB 
[11/21 05:54:12 visual_prompt]: 	Training 300/553. train loss: 4.7670,	1.9080 s / batch. (data: 1.08e+00). ETA=1 day, 3:23:31, max mem: 20.9 GB 
[11/21 05:55:52 visual_prompt]: 	Training 400/553. train loss: 29.0680,	2.0126 s / batch. (data: 1.19e+00). ETA=1 day, 4:50:11, max mem: 20.9 GB 
[11/21 05:57:31 visual_prompt]: 	Training 500/553. train loss: 176.5945,	0.8320 s / batch. (data: 1.20e-02). ETA=11:53:52, max mem: 20.9 GB 
[11/21 05:58:22 visual_prompt]: Epoch 7 / 100: avg data time: 1.89e-01, avg batch time: 1.0095, average train loss: 99.9768
[11/21 05:59:20 visual_prompt]: Inference (val):avg data time: 1.66e-04, avg batch time: 0.3071, average loss: 111.2492
[11/21 05:59:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.62	
[11/21 05:59:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/21 06:01:02 visual_prompt]: 	Training 100/553. train loss: 282.0644,	0.8417 s / batch. (data: 9.66e-03). ETA=12:00:05, max mem: 20.9 GB 
[11/21 06:02:45 visual_prompt]: 	Training 200/553. train loss: 32.4692,	0.8480 s / batch. (data: 7.96e-03). ETA=12:04:02, max mem: 20.9 GB 
[11/21 06:04:26 visual_prompt]: 	Training 300/553. train loss: 50.4068,	0.8175 s / batch. (data: 7.93e-03). ETA=11:36:39, max mem: 20.9 GB 
[11/21 06:06:07 visual_prompt]: 	Training 400/553. train loss: 234.5956,	0.8760 s / batch. (data: 5.84e-02). ETA=12:25:01, max mem: 20.9 GB 
[11/21 06:07:48 visual_prompt]: 	Training 500/553. train loss: 329.2100,	1.5490 s / batch. (data: 7.16e-01). ETA=21:54:49, max mem: 20.9 GB 
[11/21 06:08:41 visual_prompt]: Epoch 8 / 100: avg data time: 1.95e-01, avg batch time: 1.0151, average train loss: 113.6568
[11/21 06:09:39 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3085, average loss: 4.6736
[11/21 06:09:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.02	
[11/21 06:09:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/21 06:11:23 visual_prompt]: 	Training 100/553. train loss: 38.1540,	0.8525 s / batch. (data: 2.06e-02). ETA=12:01:25, max mem: 20.9 GB 
[11/21 06:13:03 visual_prompt]: 	Training 200/553. train loss: 2.7190,	0.8602 s / batch. (data: 5.44e-03). ETA=12:06:30, max mem: 20.9 GB 
[11/21 06:14:44 visual_prompt]: 	Training 300/553. train loss: 53.2733,	1.7832 s / batch. (data: 9.77e-01). ETA=1 day, 1:03:05, max mem: 20.9 GB 
[11/21 06:16:26 visual_prompt]: 	Training 400/553. train loss: 240.5974,	0.8120 s / batch. (data: 4.22e-04). ETA=11:23:04, max mem: 20.9 GB 
[11/21 06:18:08 visual_prompt]: 	Training 500/553. train loss: 59.7817,	0.9602 s / batch. (data: 1.36e-01). ETA=13:26:13, max mem: 20.9 GB 
[11/21 06:18:59 visual_prompt]: Epoch 9 / 100: avg data time: 1.93e-01, avg batch time: 1.0135, average train loss: 133.1418
[11/21 06:19:57 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3074, average loss: 184.4700
[11/21 06:19:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.80	
[11/21 06:19:57 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/21 06:21:44 visual_prompt]: 	Training 100/553. train loss: 277.8878,	0.8223 s / batch. (data: 1.03e-02). ETA=11:28:16, max mem: 20.9 GB 
[11/21 06:23:23 visual_prompt]: 	Training 200/553. train loss: 8.4880,	0.8322 s / batch. (data: 3.52e-04). ETA=11:35:12, max mem: 20.9 GB 
[11/21 06:25:03 visual_prompt]: 	Training 300/553. train loss: 49.5425,	0.8313 s / batch. (data: 1.80e-02). ETA=11:33:05, max mem: 20.9 GB 
[11/21 06:26:42 visual_prompt]: 	Training 400/553. train loss: 7.6892,	0.8237 s / batch. (data: 3.30e-04). ETA=11:25:22, max mem: 20.9 GB 
[11/21 06:28:24 visual_prompt]: 	Training 500/553. train loss: 26.2307,	0.8080 s / batch. (data: 3.74e-04). ETA=11:10:56, max mem: 20.9 GB 
[11/21 06:29:16 visual_prompt]: Epoch 10 / 100: avg data time: 1.92e-01, avg batch time: 1.0103, average train loss: 164.3878
[11/21 06:30:14 visual_prompt]: Inference (val):avg data time: 3.92e-04, avg batch time: 0.3086, average loss: 31.7203
[11/21 06:30:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.98	
[11/21 06:30:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/21 06:32:01 visual_prompt]: 	Training 100/553. train loss: 9.9833,	0.8263 s / batch. (data: 3.09e-04). ETA=11:24:01, max mem: 20.9 GB 
[11/21 06:33:43 visual_prompt]: 	Training 200/553. train loss: 469.3429,	0.8320 s / batch. (data: 8.55e-04). ETA=11:27:22, max mem: 20.9 GB 
[11/21 06:35:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2027 s / batch. (data: 1.40e+00). ETA=1 day, 6:16:06, max mem: 20.9 GB 
[11/21 06:37:02 visual_prompt]: 	Training 400/553. train loss: 100.8531,	0.8560 s / batch. (data: 8.52e-04). ETA=11:44:20, max mem: 20.9 GB 
[11/21 06:38:41 visual_prompt]: 	Training 500/553. train loss: 263.9169,	0.8080 s / batch. (data: 3.40e-04). ETA=11:03:30, max mem: 20.9 GB 
[11/21 06:39:33 visual_prompt]: Epoch 11 / 100: avg data time: 1.90e-01, avg batch time: 1.0111, average train loss: 146.0352
[11/21 06:40:31 visual_prompt]: Inference (val):avg data time: 1.95e-04, avg batch time: 0.3075, average loss: 132.6761
[11/21 06:40:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.21	
[11/21 06:40:31 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/21 06:42:17 visual_prompt]: 	Training 100/553. train loss: 60.8919,	0.8320 s / batch. (data: 3.58e-04). ETA=11:21:05, max mem: 20.9 GB 
[11/21 06:43:59 visual_prompt]: 	Training 200/553. train loss: 56.2458,	0.8155 s / batch. (data: 5.46e-03). ETA=11:06:15, max mem: 20.9 GB 
[11/21 06:45:38 visual_prompt]: 	Training 300/553. train loss: 37.1902,	0.8280 s / batch. (data: 3.36e-04). ETA=11:15:02, max mem: 20.9 GB 
[11/21 06:47:19 visual_prompt]: 	Training 400/553. train loss: 9.3180,	0.8400 s / batch. (data: 3.39e-04). ETA=11:23:26, max mem: 20.9 GB 
[11/21 06:49:00 visual_prompt]: 	Training 500/553. train loss: 13.2477,	0.8280 s / batch. (data: 8.20e-04). ETA=11:12:17, max mem: 20.9 GB 
[11/21 06:49:52 visual_prompt]: Epoch 12 / 100: avg data time: 1.94e-01, avg batch time: 1.0140, average train loss: 172.4881
[11/21 06:50:50 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3070, average loss: 32.2117
[11/21 06:50:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.13	
[11/21 06:50:50 visual_prompt]: Best epoch 12: best metric: -32.212
[11/21 06:50:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/21 06:52:36 visual_prompt]: 	Training 100/553. train loss: 126.3914,	0.8440 s / batch. (data: 2.40e-02). ETA=11:23:08, max mem: 20.9 GB 
[11/21 06:54:13 visual_prompt]: 	Training 200/553. train loss: 6.3517,	0.8191 s / batch. (data: 3.43e-04). ETA=11:01:35, max mem: 20.9 GB 
[11/21 06:55:55 visual_prompt]: 	Training 300/553. train loss: 238.2179,	1.6982 s / batch. (data: 8.89e-01). ETA=22:48:50, max mem: 20.9 GB 
[11/21 06:57:34 visual_prompt]: 	Training 400/553. train loss: 48.1541,	0.8125 s / batch. (data: 4.15e-04). ETA=10:53:35, max mem: 20.9 GB 
[11/21 06:59:16 visual_prompt]: 	Training 500/553. train loss: 359.6395,	0.8125 s / batch. (data: 3.13e-04). ETA=10:52:12, max mem: 20.9 GB 
[11/21 07:00:09 visual_prompt]: Epoch 13 / 100: avg data time: 1.91e-01, avg batch time: 1.0111, average train loss: 158.3108
[11/21 07:01:07 visual_prompt]: Inference (val):avg data time: 5.17e-05, avg batch time: 0.3074, average loss: 64.0487
[11/21 07:01:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.74	
[11/21 07:01:07 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/21 07:02:53 visual_prompt]: 	Training 100/553. train loss: 354.7159,	0.8125 s / batch. (data: 3.11e-04). ETA=10:50:07, max mem: 20.9 GB 
[11/21 07:04:33 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1820 s / batch. (data: 3.64e-01). ETA=15:43:48, max mem: 20.9 GB 
[11/21 07:06:13 visual_prompt]: 	Training 300/553. train loss: 281.6381,	0.8133 s / batch. (data: 3.27e-04). ETA=10:48:05, max mem: 20.9 GB 
[11/21 07:07:54 visual_prompt]: 	Training 400/553. train loss: 143.7888,	0.8160 s / batch. (data: 3.46e-04). ETA=10:48:51, max mem: 20.9 GB 
[11/21 07:09:35 visual_prompt]: 	Training 500/553. train loss: 306.5684,	0.8029 s / batch. (data: 3.30e-04). ETA=10:37:08, max mem: 20.9 GB 
[11/21 07:10:27 visual_prompt]: Epoch 14 / 100: avg data time: 1.94e-01, avg batch time: 1.0130, average train loss: 151.1336
[11/21 07:11:25 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3077, average loss: 183.7384
[11/21 07:11:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.66	
[11/21 07:11:25 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/21 07:13:10 visual_prompt]: 	Training 100/553. train loss: 105.0922,	0.8085 s / batch. (data: 4.23e-04). ETA=10:39:30, max mem: 20.9 GB 
[11/21 07:14:49 visual_prompt]: 	Training 200/553. train loss: 1187.5264,	0.8304 s / batch. (data: 3.06e-04). ETA=10:55:25, max mem: 20.9 GB 
[11/21 07:16:32 visual_prompt]: 	Training 300/553. train loss: 32.1880,	0.8202 s / batch. (data: 1.05e-02). ETA=10:45:59, max mem: 20.9 GB 
[11/21 07:18:09 visual_prompt]: 	Training 400/553. train loss: 28.2467,	0.8243 s / batch. (data: 1.05e-02). ETA=10:47:54, max mem: 20.9 GB 
[11/21 07:19:50 visual_prompt]: 	Training 500/553. train loss: 68.7989,	0.8520 s / batch. (data: 7.96e-03). ETA=11:08:13, max mem: 20.9 GB 
[11/21 07:20:44 visual_prompt]: Epoch 15 / 100: avg data time: 1.91e-01, avg batch time: 1.0104, average train loss: 206.1229
[11/21 07:21:41 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3087, average loss: 118.2958
[11/21 07:21:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.74	
[11/21 07:21:41 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/21 07:23:25 visual_prompt]: 	Training 100/553. train loss: 202.4007,	0.8282 s / batch. (data: 5.45e-03). ETA=10:47:25, max mem: 20.9 GB 
[11/21 07:25:06 visual_prompt]: 	Training 200/553. train loss: 48.7037,	0.8120 s / batch. (data: 3.18e-04). ETA=10:33:27, max mem: 20.9 GB 
[11/21 07:26:47 visual_prompt]: 	Training 300/553. train loss: 37.0103,	0.8219 s / batch. (data: 3.07e-04). ETA=10:39:44, max mem: 20.9 GB 
[11/21 07:28:28 visual_prompt]: 	Training 400/553. train loss: 27.4519,	0.8287 s / batch. (data: 1.14e-03). ETA=10:43:41, max mem: 20.9 GB 
[11/21 07:30:07 visual_prompt]: 	Training 500/553. train loss: 121.7329,	1.5685 s / batch. (data: 7.63e-01). ETA=20:15:41, max mem: 20.9 GB 
[11/21 07:31:00 visual_prompt]: Epoch 16 / 100: avg data time: 1.90e-01, avg batch time: 1.0102, average train loss: 164.7299
[11/21 07:31:58 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3084, average loss: 117.7863
[11/21 07:31:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.32	
[11/21 07:31:58 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/21 07:33:42 visual_prompt]: 	Training 100/553. train loss: 58.3650,	0.8200 s / batch. (data: 3.32e-04). ETA=10:33:28, max mem: 20.9 GB 
[11/21 07:35:24 visual_prompt]: 	Training 200/553. train loss: 227.1207,	0.8145 s / batch. (data: 3.25e-04). ETA=10:27:54, max mem: 20.9 GB 
[11/21 07:37:04 visual_prompt]: 	Training 300/553. train loss: 46.0636,	0.8440 s / batch. (data: 3.43e-04). ETA=10:49:12, max mem: 20.9 GB 
[11/21 07:38:44 visual_prompt]: 	Training 400/553. train loss: 114.2639,	1.1157 s / batch. (data: 3.05e-01). ETA=14:16:17, max mem: 20.9 GB 
[11/21 07:40:24 visual_prompt]: 	Training 500/553. train loss: 87.6181,	1.7320 s / batch. (data: 9.07e-01). ETA=22:06:28, max mem: 20.9 GB 
[11/21 07:41:18 visual_prompt]: Epoch 17 / 100: avg data time: 1.93e-01, avg batch time: 1.0128, average train loss: 168.7049
[11/21 07:42:16 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3081, average loss: 22.6526
[11/21 07:42:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.86	
[11/21 07:42:16 visual_prompt]: Best epoch 17: best metric: -22.653
[11/21 07:42:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/21 07:44:01 visual_prompt]: 	Training 100/553. train loss: 131.4120,	0.8257 s / batch. (data: 3.45e-04). ETA=10:30:14, max mem: 20.9 GB 
[11/21 07:45:44 visual_prompt]: 	Training 200/553. train loss: 82.1683,	0.8231 s / batch. (data: 5.97e-03). ETA=10:26:52, max mem: 20.9 GB 
[11/21 07:47:26 visual_prompt]: 	Training 300/553. train loss: 208.3451,	0.8239 s / batch. (data: 3.26e-04). ETA=10:26:07, max mem: 20.9 GB 
[11/21 07:49:06 visual_prompt]: 	Training 400/553. train loss: 118.2526,	0.8161 s / batch. (data: 3.09e-04). ETA=10:18:50, max mem: 20.9 GB 
[11/21 07:50:46 visual_prompt]: 	Training 500/553. train loss: 105.4762,	0.8271 s / batch. (data: 1.56e-02). ETA=10:25:47, max mem: 20.9 GB 
[11/21 07:51:37 visual_prompt]: Epoch 18 / 100: avg data time: 1.95e-01, avg batch time: 1.0149, average train loss: 173.0700
[11/21 07:52:35 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3067, average loss: 234.8130
[11/21 07:52:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.62	
[11/21 07:52:35 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/21 07:54:20 visual_prompt]: 	Training 100/553. train loss: 102.9204,	0.8769 s / batch. (data: 6.60e-02). ETA=11:01:14, max mem: 20.9 GB 
[11/21 07:56:02 visual_prompt]: 	Training 200/553. train loss: 134.7831,	0.8388 s / batch. (data: 8.64e-04). ETA=10:31:10, max mem: 20.9 GB 
[11/21 07:57:42 visual_prompt]: 	Training 300/553. train loss: 185.0444,	0.9021 s / batch. (data: 8.21e-02). ETA=11:17:15, max mem: 20.9 GB 
[11/21 07:59:25 visual_prompt]: 	Training 400/553. train loss: 27.8588,	0.8351 s / batch. (data: 8.40e-04). ETA=10:25:36, max mem: 20.9 GB 
[11/21 08:01:01 visual_prompt]: 	Training 500/553. train loss: 32.7093,	0.8437 s / batch. (data: 1.19e-02). ETA=10:30:37, max mem: 20.9 GB 
[11/21 08:01:53 visual_prompt]: Epoch 19 / 100: avg data time: 1.89e-01, avg batch time: 1.0091, average train loss: 148.0561
[11/21 08:02:51 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3085, average loss: 392.9309
[11/21 08:02:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.46	
[11/21 08:02:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/21 08:04:35 visual_prompt]: 	Training 100/553. train loss: 4.1941,	0.8432 s / batch. (data: 5.45e-03). ETA=10:28:05, max mem: 20.9 GB 
[11/21 08:06:16 visual_prompt]: 	Training 200/553. train loss: 0.8545,	0.8226 s / batch. (data: 1.03e-02). ETA=10:11:20, max mem: 20.9 GB 
[11/21 08:07:57 visual_prompt]: 	Training 300/553. train loss: 68.5057,	0.8201 s / batch. (data: 3.76e-04). ETA=10:08:06, max mem: 20.9 GB 
[11/21 08:09:37 visual_prompt]: 	Training 400/553. train loss: 805.9830,	0.8200 s / batch. (data: 3.30e-04). ETA=10:06:43, max mem: 20.9 GB 
[11/21 08:11:17 visual_prompt]: 	Training 500/553. train loss: 203.7889,	0.8175 s / batch. (data: 5.46e-03). ETA=10:03:29, max mem: 20.9 GB 
[11/21 08:12:11 visual_prompt]: Epoch 20 / 100: avg data time: 1.92e-01, avg batch time: 1.0116, average train loss: 131.3989
[11/21 08:13:09 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3071, average loss: 165.6595
[11/21 08:13:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.91	
[11/21 08:13:09 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/21 08:14:56 visual_prompt]: 	Training 100/553. train loss: 19.3996,	0.8200 s / batch. (data: 3.05e-04). ETA=10:03:13, max mem: 20.9 GB 
[11/21 08:16:36 visual_prompt]: 	Training 200/553. train loss: 379.2604,	0.8212 s / batch. (data: 3.20e-04). ETA=10:02:44, max mem: 20.9 GB 
[11/21 08:18:16 visual_prompt]: 	Training 300/553. train loss: 641.3419,	1.0481 s / batch. (data: 2.17e-01). ETA=12:47:31, max mem: 20.9 GB 
[11/21 08:19:55 visual_prompt]: 	Training 400/553. train loss: 61.0983,	0.8415 s / batch. (data: 1.56e-02). ETA=10:14:53, max mem: 20.9 GB 
[11/21 08:21:37 visual_prompt]: 	Training 500/553. train loss: 171.5970,	0.8184 s / batch. (data: 8.53e-03). ETA=9:56:37, max mem: 20.9 GB 
[11/21 08:22:29 visual_prompt]: Epoch 21 / 100: avg data time: 1.93e-01, avg batch time: 1.0127, average train loss: 151.6709
[11/21 08:23:27 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.3087, average loss: 111.4839
[11/21 08:23:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.89	
[11/21 08:23:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/21 08:25:10 visual_prompt]: 	Training 100/553. train loss: 122.2994,	0.8319 s / batch. (data: 2.98e-04). ETA=10:04:22, max mem: 20.9 GB 
[11/21 08:26:51 visual_prompt]: 	Training 200/553. train loss: 80.7615,	0.8092 s / batch. (data: 2.93e-04). ETA=9:46:28, max mem: 20.9 GB 
[11/21 08:28:30 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8074 s / batch. (data: 3.09e-04). ETA=9:43:49, max mem: 20.9 GB 
[11/21 08:30:12 visual_prompt]: 	Training 400/553. train loss: 179.3458,	0.8164 s / batch. (data: 3.36e-04). ETA=9:49:00, max mem: 20.9 GB 
[11/21 08:31:53 visual_prompt]: 	Training 500/553. train loss: 114.1174,	0.8080 s / batch. (data: 3.22e-04). ETA=9:41:33, max mem: 20.9 GB 
[11/21 08:32:46 visual_prompt]: Epoch 22 / 100: avg data time: 1.92e-01, avg batch time: 1.0119, average train loss: 152.8411
[11/21 08:33:44 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3057, average loss: 75.6044
[11/21 08:33:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.51	
[11/21 08:33:44 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/21 08:35:30 visual_prompt]: 	Training 100/553. train loss: 238.0629,	0.8300 s / batch. (data: 5.45e-03). ETA=9:55:18, max mem: 20.9 GB 
[11/21 08:37:12 visual_prompt]: 	Training 200/553. train loss: 108.5072,	0.8619 s / batch. (data: 4.48e-02). ETA=10:16:46, max mem: 20.9 GB 
[11/21 08:38:54 visual_prompt]: 	Training 300/553. train loss: 15.3152,	0.8201 s / batch. (data: 3.00e-04). ETA=9:45:26, max mem: 20.9 GB 
[11/21 08:40:33 visual_prompt]: 	Training 400/553. train loss: 370.6310,	0.8320 s / batch. (data: 3.10e-04). ETA=9:52:35, max mem: 20.9 GB 
[11/21 08:42:11 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8032 s / batch. (data: 7.91e-03). ETA=9:30:44, max mem: 20.9 GB 
[11/21 08:43:03 visual_prompt]: Epoch 23 / 100: avg data time: 1.90e-01, avg batch time: 1.0110, average train loss: 164.8058
[11/21 08:44:01 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.3092, average loss: 19.6481
[11/21 08:44:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.39	
[11/21 08:44:01 visual_prompt]: Best epoch 23: best metric: -19.648
[11/21 08:44:01 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/21 08:45:43 visual_prompt]: 	Training 100/553. train loss: 220.7708,	0.8243 s / batch. (data: 1.52e-02). ETA=9:43:39, max mem: 20.9 GB 
[11/21 08:47:24 visual_prompt]: 	Training 200/553. train loss: 57.7030,	0.8280 s / batch. (data: 8.98e-04). ETA=9:44:50, max mem: 20.9 GB 
[11/21 08:49:05 visual_prompt]: 	Training 300/553. train loss: 165.2409,	0.8358 s / batch. (data: 7.82e-03). ETA=9:48:59, max mem: 20.9 GB 
[11/21 08:50:46 visual_prompt]: 	Training 400/553. train loss: 54.8669,	0.8320 s / batch. (data: 3.04e-04). ETA=9:44:53, max mem: 20.9 GB 
[11/21 08:52:29 visual_prompt]: 	Training 500/553. train loss: 233.0795,	0.8160 s / batch. (data: 3.22e-04). ETA=9:32:18, max mem: 20.9 GB 
[11/21 08:53:21 visual_prompt]: Epoch 24 / 100: avg data time: 1.92e-01, avg batch time: 1.0123, average train loss: 146.3013
[11/21 08:54:19 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3085, average loss: 171.0182
[11/21 08:54:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.56	
[11/21 08:54:19 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/21 08:56:07 visual_prompt]: 	Training 100/553. train loss: 15.2874,	0.8240 s / batch. (data: 3.01e-04). ETA=9:35:49, max mem: 20.9 GB 
[11/21 08:57:45 visual_prompt]: 	Training 200/553. train loss: 186.4530,	0.8320 s / batch. (data: 5.47e-03). ETA=9:40:02, max mem: 20.9 GB 
[11/21 08:59:25 visual_prompt]: 	Training 300/553. train loss: 25.8697,	0.8296 s / batch. (data: 5.50e-03). ETA=9:36:56, max mem: 20.9 GB 
[11/21 09:01:06 visual_prompt]: 	Training 400/553. train loss: 33.5844,	1.1649 s / batch. (data: 3.49e-01). ETA=13:28:11, max mem: 20.9 GB 
[11/21 09:02:47 visual_prompt]: 	Training 500/553. train loss: 120.7112,	1.5970 s / batch. (data: 7.90e-01). ETA=18:25:21, max mem: 20.9 GB 
[11/21 09:03:39 visual_prompt]: Epoch 25 / 100: avg data time: 1.93e-01, avg batch time: 1.0126, average train loss: 152.1037
[11/21 09:04:37 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3068, average loss: 209.1437
[11/21 09:04:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.07	
[11/21 09:04:37 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/21 09:06:22 visual_prompt]: 	Training 100/553. train loss: 83.1610,	0.8059 s / batch. (data: 3.43e-04). ETA=9:15:44, max mem: 20.9 GB 
[11/21 09:08:04 visual_prompt]: 	Training 200/553. train loss: 598.5208,	1.9055 s / batch. (data: 1.07e+00). ETA=21:50:49, max mem: 20.9 GB 
[11/21 09:09:45 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8254 s / batch. (data: 7.94e-03). ETA=9:26:24, max mem: 20.9 GB 
[11/21 09:11:25 visual_prompt]: 	Training 400/553. train loss: 457.4002,	0.8719 s / batch. (data: 1.20e-02). ETA=9:56:54, max mem: 20.9 GB 
[11/21 09:13:04 visual_prompt]: 	Training 500/553. train loss: 34.4658,	0.8125 s / batch. (data: 3.16e-04). ETA=9:14:51, max mem: 20.9 GB 
[11/21 09:13:56 visual_prompt]: Epoch 26 / 100: avg data time: 1.91e-01, avg batch time: 1.0110, average train loss: 149.0037
[11/21 09:14:54 visual_prompt]: Inference (val):avg data time: 4.47e-05, avg batch time: 0.3069, average loss: 44.7650
[11/21 09:14:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.99	
[11/21 09:14:54 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/21 09:16:40 visual_prompt]: 	Training 100/553. train loss: 14.7403,	0.8360 s / batch. (data: 3.01e-04). ETA=9:28:47, max mem: 20.9 GB 
[11/21 09:18:20 visual_prompt]: 	Training 200/553. train loss: 319.8436,	1.3619 s / batch. (data: 5.19e-01). ETA=15:24:18, max mem: 20.9 GB 
[11/21 09:20:01 visual_prompt]: 	Training 300/553. train loss: 199.6077,	0.8349 s / batch. (data: 1.05e-02). ETA=9:25:15, max mem: 20.9 GB 
[11/21 09:21:43 visual_prompt]: 	Training 400/553. train loss: 0.2961,	0.8275 s / batch. (data: 3.19e-04). ETA=9:18:52, max mem: 20.9 GB 
[11/21 09:23:24 visual_prompt]: 	Training 500/553. train loss: 63.2854,	0.8360 s / batch. (data: 8.18e-04). ETA=9:23:13, max mem: 20.9 GB 
[11/21 09:24:15 visual_prompt]: Epoch 27 / 100: avg data time: 1.92e-01, avg batch time: 1.0130, average train loss: 175.4549
[11/21 09:25:13 visual_prompt]: Inference (val):avg data time: 4.56e-05, avg batch time: 0.3081, average loss: 296.2717
[11/21 09:25:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 34.15	
[11/21 09:25:13 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/21 09:26:56 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8240 s / batch. (data: 3.38e-04). ETA=9:13:01, max mem: 20.9 GB 
[11/21 09:28:39 visual_prompt]: 	Training 200/553. train loss: 147.5573,	0.8198 s / batch. (data: 3.10e-04). ETA=9:08:49, max mem: 20.9 GB 
[11/21 09:30:24 visual_prompt]: 	Training 300/553. train loss: 39.2931,	1.4143 s / batch. (data: 5.95e-01). ETA=15:44:28, max mem: 20.9 GB 
[11/21 09:32:04 visual_prompt]: 	Training 400/553. train loss: 13.6634,	0.8280 s / batch. (data: 3.05e-04). ETA=9:11:34, max mem: 20.9 GB 
[11/21 09:33:44 visual_prompt]: 	Training 500/553. train loss: 440.2202,	0.8331 s / batch. (data: 3.21e-04). ETA=9:13:34, max mem: 20.9 GB 
[11/21 09:34:37 visual_prompt]: Epoch 28 / 100: avg data time: 2.01e-01, avg batch time: 1.0210, average train loss: 158.8234
[11/21 09:35:37 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3080, average loss: 254.6606
[11/21 09:35:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/21 09:35:37 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/21 09:37:27 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8036 s / batch. (data: 2.97e-04). ETA=8:51:56, max mem: 20.9 GB 
[11/21 09:39:03 visual_prompt]: 	Training 200/553. train loss: 346.1683,	1.6480 s / batch. (data: 8.43e-01). ETA=18:08:06, max mem: 20.9 GB 
[11/21 09:40:39 visual_prompt]: 	Training 300/553. train loss: 34.8636,	0.8372 s / batch. (data: 8.55e-04). ETA=9:11:22, max mem: 20.9 GB 
[11/21 09:42:13 visual_prompt]: 	Training 400/553. train loss: 364.2102,	1.1066 s / batch. (data: 3.08e-01). ETA=12:06:59, max mem: 20.9 GB 
[11/21 09:43:51 visual_prompt]: 	Training 500/553. train loss: 46.9617,	0.8278 s / batch. (data: 3.31e-04). ETA=9:02:24, max mem: 20.9 GB 
[11/21 09:44:42 visual_prompt]: Epoch 29 / 100: avg data time: 1.66e-01, avg batch time: 0.9851, average train loss: 139.2024
[11/21 09:45:38 visual_prompt]: Inference (val):avg data time: 1.75e-04, avg batch time: 0.3075, average loss: 19.5667
[11/21 09:45:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.65	
[11/21 09:45:38 visual_prompt]: Best epoch 29: best metric: -19.567
[11/21 09:45:38 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/21 09:47:19 visual_prompt]: 	Training 100/553. train loss: 154.6503,	0.8187 s / batch. (data: 5.46e-03). ETA=8:54:21, max mem: 20.9 GB 
[11/21 09:48:58 visual_prompt]: 	Training 200/553. train loss: 16.3775,	0.8440 s / batch. (data: 7.97e-03). ETA=9:09:30, max mem: 20.9 GB 
[11/21 09:50:35 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8339 s / batch. (data: 1.48e-02). ETA=9:01:29, max mem: 20.9 GB 
[11/21 09:52:15 visual_prompt]: 	Training 400/553. train loss: 98.6266,	1.1844 s / batch. (data: 3.83e-01). ETA=12:47:08, max mem: 20.9 GB 
[11/21 09:53:52 visual_prompt]: 	Training 500/553. train loss: 48.5852,	1.5070 s / batch. (data: 6.85e-01). ETA=16:13:35, max mem: 20.9 GB 
[11/21 09:54:44 visual_prompt]: Epoch 30 / 100: avg data time: 1.66e-01, avg batch time: 0.9869, average train loss: 140.6919
[11/21 09:55:40 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.3096, average loss: 85.6259
[11/21 09:55:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.40	
[11/21 09:55:40 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/21 09:57:23 visual_prompt]: 	Training 100/553. train loss: 122.3913,	0.8042 s / batch. (data: 2.52e-04). ETA=8:37:28, max mem: 20.9 GB 
[11/21 09:59:00 visual_prompt]: 	Training 200/553. train loss: 31.0906,	0.8433 s / batch. (data: 1.19e-02). ETA=9:01:13, max mem: 20.9 GB 
[11/21 10:00:35 visual_prompt]: 	Training 300/553. train loss: 229.8587,	0.8218 s / batch. (data: 3.41e-04). ETA=8:46:04, max mem: 20.9 GB 
[11/21 10:02:11 visual_prompt]: 	Training 400/553. train loss: 157.4476,	1.2866 s / batch. (data: 4.53e-01). ETA=13:41:27, max mem: 20.9 GB 
[11/21 10:03:48 visual_prompt]: 	Training 500/553. train loss: 97.8932,	0.8062 s / batch. (data: 3.13e-04). ETA=8:33:24, max mem: 20.9 GB 
[11/21 10:04:38 visual_prompt]: Epoch 31 / 100: avg data time: 1.53e-01, avg batch time: 0.9724, average train loss: 142.1276
[11/21 10:05:34 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.3075, average loss: 81.8708
[11/21 10:05:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.45	
[11/21 10:05:34 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/21 10:07:15 visual_prompt]: 	Training 100/553. train loss: 165.4758,	0.8267 s / batch. (data: 1.14e-02). ETA=8:44:19, max mem: 20.9 GB 
[11/21 10:08:51 visual_prompt]: 	Training 200/553. train loss: 15.5328,	0.8318 s / batch. (data: 1.56e-02). ETA=8:46:11, max mem: 20.9 GB 
[11/21 10:10:31 visual_prompt]: 	Training 300/553. train loss: 174.6999,	0.8027 s / batch. (data: 2.80e-04). ETA=8:26:29, max mem: 20.9 GB 
[11/21 10:12:07 visual_prompt]: 	Training 400/553. train loss: 54.7532,	0.8341 s / batch. (data: 1.05e-02). ETA=8:44:54, max mem: 20.9 GB 
[11/21 10:13:40 visual_prompt]: 	Training 500/553. train loss: 37.7764,	0.8278 s / batch. (data: 1.05e-02). ETA=8:39:33, max mem: 20.9 GB 
[11/21 10:14:28 visual_prompt]: Epoch 32 / 100: avg data time: 1.46e-01, avg batch time: 0.9669, average train loss: 139.3340
[11/21 10:15:24 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3080, average loss: 19.7126
[11/21 10:15:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.32	
[11/21 10:15:24 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/21 10:17:03 visual_prompt]: 	Training 100/553. train loss: 4.8962,	0.8586 s / batch. (data: 2.66e-02). ETA=8:56:42, max mem: 20.9 GB 
[11/21 10:18:41 visual_prompt]: 	Training 200/553. train loss: 18.0701,	1.2627 s / batch. (data: 4.52e-01). ETA=13:07:11, max mem: 20.9 GB 
[11/21 10:20:16 visual_prompt]: 	Training 300/553. train loss: 210.4590,	0.8092 s / batch. (data: 3.21e-04). ETA=8:23:05, max mem: 20.9 GB 
[11/21 10:21:54 visual_prompt]: 	Training 400/553. train loss: 330.1152,	0.8121 s / batch. (data: 2.99e-04). ETA=8:23:34, max mem: 20.9 GB 
[11/21 10:23:30 visual_prompt]: 	Training 500/553. train loss: 507.6531,	0.8391 s / batch. (data: 1.99e-02). ETA=8:38:53, max mem: 20.9 GB 
[11/21 10:24:19 visual_prompt]: Epoch 33 / 100: avg data time: 1.47e-01, avg batch time: 0.9671, average train loss: 143.6312
[11/21 10:25:15 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3081, average loss: 127.1067
[11/21 10:25:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.03	
[11/21 10:25:15 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/21 10:26:57 visual_prompt]: 	Training 100/553. train loss: 19.8462,	0.8240 s / batch. (data: 3.15e-04). ETA=8:27:27, max mem: 20.9 GB 
[11/21 10:28:32 visual_prompt]: 	Training 200/553. train loss: 18.9643,	0.8346 s / batch. (data: 1.22e-02). ETA=8:32:35, max mem: 20.9 GB 
[11/21 10:30:07 visual_prompt]: 	Training 300/553. train loss: 309.5136,	0.8200 s / batch. (data: 3.18e-04). ETA=8:22:15, max mem: 20.9 GB 
[11/21 10:31:45 visual_prompt]: 	Training 400/553. train loss: 8.1546,	0.8442 s / batch. (data: 3.05e-04). ETA=8:35:40, max mem: 20.9 GB 
[11/21 10:33:22 visual_prompt]: 	Training 500/553. train loss: 112.1696,	1.5251 s / batch. (data: 7.22e-01). ETA=15:29:02, max mem: 20.9 GB 
[11/21 10:34:12 visual_prompt]: Epoch 34 / 100: avg data time: 1.50e-01, avg batch time: 0.9705, average train loss: 149.7802
[11/21 10:35:08 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3059, average loss: 111.8793
[11/21 10:35:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.49	
[11/21 10:35:08 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/21 10:36:53 visual_prompt]: 	Training 100/553. train loss: 103.1113,	0.8359 s / batch. (data: 3.20e-04). ETA=8:27:06, max mem: 20.9 GB 
[11/21 10:38:34 visual_prompt]: 	Training 200/553. train loss: 27.2771,	0.8187 s / batch. (data: 3.23e-04). ETA=8:15:16, max mem: 20.9 GB 
[11/21 10:40:12 visual_prompt]: 	Training 300/553. train loss: 37.5424,	0.8215 s / batch. (data: 1.20e-02). ETA=8:15:35, max mem: 20.9 GB 
[11/21 10:41:50 visual_prompt]: 	Training 400/553. train loss: 14.9664,	0.8121 s / batch. (data: 3.13e-04). ETA=8:08:33, max mem: 20.9 GB 
[11/21 10:43:29 visual_prompt]: 	Training 500/553. train loss: 55.5980,	1.0960 s / batch. (data: 2.55e-01). ETA=10:57:34, max mem: 20.9 GB 
[11/21 10:44:22 visual_prompt]: Epoch 35 / 100: avg data time: 1.80e-01, avg batch time: 1.0015, average train loss: 149.4510
[11/21 10:45:20 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.3075, average loss: 265.0159
[11/21 10:45:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.16	
[11/21 10:45:20 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/21 10:47:03 visual_prompt]: 	Training 100/553. train loss: 115.7175,	0.8166 s / batch. (data: 3.35e-04). ETA=8:07:52, max mem: 20.9 GB 
[11/21 10:48:44 visual_prompt]: 	Training 200/553. train loss: 506.0617,	0.8286 s / batch. (data: 3.61e-04). ETA=8:13:38, max mem: 20.9 GB 
[11/21 10:50:27 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8095 s / batch. (data: 7.94e-03). ETA=8:00:55, max mem: 20.9 GB 
[11/21 10:52:07 visual_prompt]: 	Training 400/553. train loss: 23.3570,	1.6447 s / batch. (data: 8.23e-01). ETA=16:14:19, max mem: 20.9 GB 
[11/21 10:53:48 visual_prompt]: 	Training 500/553. train loss: 290.1146,	1.0846 s / batch. (data: 2.56e-01). ETA=10:40:43, max mem: 20.9 GB 
[11/21 10:54:38 visual_prompt]: Epoch 36 / 100: avg data time: 1.89e-01, avg batch time: 1.0083, average train loss: 140.2568
[11/21 10:55:35 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3088, average loss: 111.5739
[11/21 10:55:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[11/21 10:55:35 visual_prompt]: Stopping early.
[11/21 10:55:35 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 10:55:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 10:55:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 10:55:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/21 10:55:35 visual_prompt]: Training with config:
[11/21 10:55:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/21 10:55:35 visual_prompt]: Loading training data...
[11/21 10:55:35 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 10:55:35 visual_prompt]: Loading validation data...
[11/21 10:55:35 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 10:55:36 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 10:55:42 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/21 10:55:42 visual_prompt]: tuned percent:0.525
[11/21 10:55:42 visual_prompt]: Device used for model: 0
[11/21 10:55:42 visual_prompt]: Setting up Evaluator...
[11/21 10:55:42 visual_prompt]: Setting up Trainer...
[11/21 10:55:42 visual_prompt]: 	Setting up the optimizer...
[11/21 10:55:42 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 10:57:26 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8237 s / batch. (data: 3.08e-04). ETA=12:37:49, max mem: 20.9 GB 
[11/21 10:59:04 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8200 s / batch. (data: 3.16e-04). ETA=12:33:01, max mem: 20.9 GB 
[11/21 11:00:47 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.1520 s / batch. (data: 3.25e-01). ETA=17:36:00, max mem: 20.9 GB 
[11/21 11:02:26 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8095 s / batch. (data: 6.35e-04). ETA=12:20:41, max mem: 20.9 GB 
[11/21 11:04:08 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8200 s / batch. (data: 7.88e-04). ETA=12:28:56, max mem: 20.9 GB 
[11/21 11:05:01 visual_prompt]: Epoch 1 / 100: avg data time: 1.85e-01, avg batch time: 1.0106, average train loss: 1.5403
[11/21 11:05:59 visual_prompt]: Inference (val):avg data time: 4.70e-05, avg batch time: 0.3090, average loss: 1.5201
[11/21 11:05:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/21 11:05:59 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/21 11:07:43 visual_prompt]: 	Training 100/553. train loss: 32.7968,	0.8466 s / batch. (data: 3.46e-04). ETA=12:51:06, max mem: 20.9 GB 
[11/21 11:09:22 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8499 s / batch. (data: 3.29e-04). ETA=12:52:37, max mem: 20.9 GB 
[11/21 11:11:04 visual_prompt]: 	Training 300/553. train loss: 2.9277,	1.1000 s / batch. (data: 2.76e-01). ETA=16:38:09, max mem: 20.9 GB 
[11/21 11:12:43 visual_prompt]: 	Training 400/553. train loss: 36.1745,	0.8221 s / batch. (data: 3.48e-04). ETA=12:24:37, max mem: 20.9 GB 
[11/21 11:14:26 visual_prompt]: 	Training 500/553. train loss: 13.4565,	0.8481 s / batch. (data: 3.16e-04). ETA=12:46:44, max mem: 20.9 GB 
[11/21 11:15:17 visual_prompt]: Epoch 2 / 100: avg data time: 1.83e-01, avg batch time: 1.0092, average train loss: 22.7402
[11/21 11:16:15 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3082, average loss: 21.8746
[11/21 11:16:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.07	
[11/21 11:16:15 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/21 11:17:57 visual_prompt]: 	Training 100/553. train loss: 16.2583,	0.8423 s / batch. (data: 1.02e-02). ETA=12:39:21, max mem: 20.9 GB 
[11/21 11:19:39 visual_prompt]: 	Training 200/553. train loss: 23.2656,	1.3480 s / batch. (data: 5.41e-01). ETA=20:13:01, max mem: 20.9 GB 
[11/21 11:21:18 visual_prompt]: 	Training 300/553. train loss: 11.3496,	0.8240 s / batch. (data: 3.26e-04). ETA=12:20:08, max mem: 20.9 GB 
[11/21 11:22:59 visual_prompt]: 	Training 400/553. train loss: 189.8524,	0.8197 s / batch. (data: 3.08e-04). ETA=12:14:56, max mem: 20.9 GB 
[11/21 11:24:41 visual_prompt]: 	Training 500/553. train loss: 22.7126,	1.3762 s / batch. (data: 5.34e-01). ETA=20:31:34, max mem: 20.9 GB 
[11/21 11:25:32 visual_prompt]: Epoch 3 / 100: avg data time: 1.84e-01, avg batch time: 1.0085, average train loss: 34.7822
[11/21 11:26:30 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3090, average loss: 19.8792
[11/21 11:26:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.07	
[11/21 11:26:30 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/21 11:28:16 visual_prompt]: 	Training 100/553. train loss: 6.3957,	0.8361 s / batch. (data: 3.03e-04). ETA=12:26:03, max mem: 20.9 GB 
[11/21 11:29:56 visual_prompt]: 	Training 200/553. train loss: 52.6492,	0.8246 s / batch. (data: 3.23e-04). ETA=12:14:26, max mem: 20.9 GB 
[11/21 11:31:37 visual_prompt]: 	Training 300/553. train loss: 10.5361,	1.5329 s / batch. (data: 6.73e-01). ETA=22:42:46, max mem: 20.9 GB 
[11/21 11:33:14 visual_prompt]: 	Training 400/553. train loss: 52.3675,	1.5685 s / batch. (data: 7.39e-01). ETA=23:11:47, max mem: 20.9 GB 
[11/21 11:34:56 visual_prompt]: 	Training 500/553. train loss: 40.5577,	3.4637 s / batch. (data: 2.66e+00). ETA=2 days, 3:07:44, max mem: 20.9 GB 
[11/21 11:35:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.90e-01, avg batch time: 1.0112, average train loss: 63.2746
[11/21 11:36:47 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.3086, average loss: 170.5102
[11/21 11:36:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.57	
[11/21 11:36:47 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/21 11:38:30 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 2.97e-04). ETA=12:00:39, max mem: 20.9 GB 
[11/21 11:40:10 visual_prompt]: 	Training 200/553. train loss: 3.0704,	1.1640 s / batch. (data: 3.34e-01). ETA=17:06:01, max mem: 20.9 GB 
[11/21 11:41:51 visual_prompt]: 	Training 300/553. train loss: 18.1324,	0.8373 s / batch. (data: 3.54e-04). ETA=12:16:41, max mem: 20.9 GB 
[11/21 11:43:31 visual_prompt]: 	Training 400/553. train loss: 79.8581,	0.8200 s / batch. (data: 7.59e-04). ETA=12:00:05, max mem: 20.9 GB 
[11/21 11:45:12 visual_prompt]: 	Training 500/553. train loss: 23.4526,	0.8240 s / batch. (data: 3.09e-04). ETA=12:02:11, max mem: 20.9 GB 
[11/21 11:46:05 visual_prompt]: Epoch 5 / 100: avg data time: 1.89e-01, avg batch time: 1.0091, average train loss: 79.2135
[11/21 11:47:02 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3077, average loss: 143.7809
[11/21 11:47:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[11/21 11:47:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/21 11:48:50 visual_prompt]: 	Training 100/553. train loss: 6.2837,	0.8082 s / batch. (data: 3.11e-04). ETA=11:46:19, max mem: 20.9 GB 
[11/21 11:50:31 visual_prompt]: 	Training 200/553. train loss: 118.3204,	0.8089 s / batch. (data: 3.29e-04). ETA=11:45:35, max mem: 20.9 GB 
[11/21 11:52:11 visual_prompt]: 	Training 300/553. train loss: 54.3363,	0.8232 s / batch. (data: 3.30e-04). ETA=11:56:40, max mem: 20.9 GB 
[11/21 11:53:56 visual_prompt]: 	Training 400/553. train loss: 99.6455,	0.8200 s / batch. (data: 3.40e-04). ETA=11:52:30, max mem: 20.9 GB 
[11/21 11:55:35 visual_prompt]: 	Training 500/553. train loss: 30.0783,	0.8237 s / batch. (data: 6.88e-03). ETA=11:54:19, max mem: 20.9 GB 
[11/21 11:56:27 visual_prompt]: Epoch 6 / 100: avg data time: 1.99e-01, avg batch time: 1.0201, average train loss: 66.1335
[11/21 11:57:24 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3074, average loss: 99.1012
[11/21 11:57:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.41	
[11/21 11:57:24 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/21 11:59:07 visual_prompt]: 	Training 100/553. train loss: 0.2706,	0.8243 s / batch. (data: 1.26e-02). ETA=11:52:45, max mem: 20.9 GB 
[11/21 12:00:48 visual_prompt]: 	Training 200/553. train loss: 25.0724,	0.8124 s / batch. (data: 3.71e-04). ETA=11:41:08, max mem: 20.9 GB 
[11/21 12:02:31 visual_prompt]: 	Training 300/553. train loss: 10.4053,	0.8153 s / batch. (data: 3.36e-04). ETA=11:42:17, max mem: 20.9 GB 
[11/21 12:04:11 visual_prompt]: 	Training 400/553. train loss: 14.4586,	0.8402 s / batch. (data: 3.46e-04). ETA=12:02:21, max mem: 20.9 GB 
[11/21 12:05:50 visual_prompt]: 	Training 500/553. train loss: 24.6965,	0.8096 s / batch. (data: 3.30e-04). ETA=11:34:40, max mem: 20.9 GB 
[11/21 12:06:41 visual_prompt]: Epoch 7 / 100: avg data time: 1.87e-01, avg batch time: 1.0074, average train loss: 73.2909
[11/21 12:07:39 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3070, average loss: 76.7275
[11/21 12:07:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.65	
[11/21 12:07:39 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/21 12:09:21 visual_prompt]: 	Training 100/553. train loss: 15.1555,	0.8320 s / batch. (data: 3.08e-04). ETA=11:51:46, max mem: 20.9 GB 
[11/21 12:11:02 visual_prompt]: 	Training 200/553. train loss: 95.7314,	0.8151 s / batch. (data: 3.57e-04). ETA=11:35:56, max mem: 20.9 GB 
[11/21 12:12:44 visual_prompt]: 	Training 300/553. train loss: 156.5140,	0.8290 s / batch. (data: 1.30e-02). ETA=11:46:26, max mem: 20.9 GB 
[11/21 12:14:24 visual_prompt]: 	Training 400/553. train loss: 166.6465,	0.8320 s / batch. (data: 7.96e-03). ETA=11:47:37, max mem: 20.9 GB 
[11/21 12:16:06 visual_prompt]: 	Training 500/553. train loss: 285.2204,	1.6040 s / batch. (data: 7.66e-01). ETA=22:41:30, max mem: 20.9 GB 
[11/21 12:16:59 visual_prompt]: Epoch 8 / 100: avg data time: 1.92e-01, avg batch time: 1.0121, average train loss: 99.5197
[11/21 12:17:56 visual_prompt]: Inference (val):avg data time: 7.20e-04, avg batch time: 0.3078, average loss: 35.6332
[11/21 12:17:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.89	
[11/21 12:17:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/21 12:19:41 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 3.55e-04). ETA=11:40:41, max mem: 20.9 GB 
[11/21 12:21:21 visual_prompt]: 	Training 200/553. train loss: 43.8888,	0.8306 s / batch. (data: 3.09e-04). ETA=11:41:32, max mem: 20.9 GB 
[11/21 12:23:03 visual_prompt]: 	Training 300/553. train loss: 137.8746,	1.8708 s / batch. (data: 1.05e+00). ETA=1 day, 2:16:59, max mem: 20.9 GB 
[11/21 12:24:44 visual_prompt]: 	Training 400/553. train loss: 35.5964,	0.8152 s / batch. (data: 3.31e-04). ETA=11:25:47, max mem: 20.9 GB 
[11/21 12:26:25 visual_prompt]: 	Training 500/553. train loss: 182.5312,	0.8360 s / batch. (data: 3.27e-04). ETA=11:41:54, max mem: 20.9 GB 
[11/21 12:27:17 visual_prompt]: Epoch 9 / 100: avg data time: 1.93e-01, avg batch time: 1.0135, average train loss: 78.0807
[11/21 12:28:15 visual_prompt]: Inference (val):avg data time: 2.33e-04, avg batch time: 0.3074, average loss: 328.8175
[11/21 12:28:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.86	
[11/21 12:28:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/21 12:30:01 visual_prompt]: 	Training 100/553. train loss: 26.8022,	0.8410 s / batch. (data: 8.95e-03). ETA=11:43:57, max mem: 20.9 GB 
[11/21 12:31:41 visual_prompt]: 	Training 200/553. train loss: 121.5240,	0.8281 s / batch. (data: 3.33e-04). ETA=11:31:46, max mem: 20.9 GB 
[11/21 12:33:21 visual_prompt]: 	Training 300/553. train loss: 160.5820,	1.1749 s / batch. (data: 3.72e-01). ETA=16:19:31, max mem: 20.9 GB 
[11/21 12:35:00 visual_prompt]: 	Training 400/553. train loss: 168.6008,	0.8280 s / batch. (data: 5.47e-03). ETA=11:28:54, max mem: 20.9 GB 
[11/21 12:36:42 visual_prompt]: 	Training 500/553. train loss: 579.5065,	1.0475 s / batch. (data: 2.11e-01). ETA=14:29:49, max mem: 20.9 GB 
[11/21 12:37:37 visual_prompt]: Epoch 10 / 100: avg data time: 1.97e-01, avg batch time: 1.0169, average train loss: 138.4034
[11/21 12:38:35 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3060, average loss: 7.5994
[11/21 12:38:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.49	
[11/21 12:38:35 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/21 12:40:23 visual_prompt]: 	Training 100/553. train loss: 88.7464,	0.8025 s / batch. (data: 3.13e-04). ETA=11:04:20, max mem: 20.9 GB 
[11/21 12:42:06 visual_prompt]: 	Training 200/553. train loss: 85.2204,	0.8520 s / batch. (data: 3.22e-04). ETA=11:43:53, max mem: 20.9 GB 
[11/21 12:43:47 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1640 s / batch. (data: 1.35e+00). ETA=1 day, 5:44:14, max mem: 20.9 GB 
[11/21 12:45:26 visual_prompt]: 	Training 400/553. train loss: 99.9953,	0.8240 s / batch. (data: 3.12e-04). ETA=11:18:01, max mem: 20.9 GB 
[11/21 12:47:05 visual_prompt]: 	Training 500/553. train loss: 108.4333,	0.8287 s / batch. (data: 3.51e-04). ETA=11:20:29, max mem: 20.9 GB 
[11/21 12:47:57 visual_prompt]: Epoch 11 / 100: avg data time: 1.97e-01, avg batch time: 1.0158, average train loss: 111.4261
[11/21 12:48:55 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3084, average loss: 323.4079
[11/21 12:48:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.19	
[11/21 12:48:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/21 12:50:41 visual_prompt]: 	Training 100/553. train loss: 40.6332,	1.0544 s / batch. (data: 2.20e-01). ETA=14:23:09, max mem: 20.9 GB 
[11/21 12:52:22 visual_prompt]: 	Training 200/553. train loss: 21.3151,	0.8108 s / batch. (data: 3.35e-04). ETA=11:02:23, max mem: 20.9 GB 
[11/21 12:54:01 visual_prompt]: 	Training 300/553. train loss: 58.6921,	0.8290 s / batch. (data: 3.02e-04). ETA=11:15:53, max mem: 20.9 GB 
[11/21 12:55:43 visual_prompt]: 	Training 400/553. train loss: 200.1610,	0.8280 s / batch. (data: 7.94e-03). ETA=11:13:41, max mem: 20.9 GB 
[11/21 12:57:24 visual_prompt]: 	Training 500/553. train loss: 466.1448,	0.8349 s / batch. (data: 3.03e-04). ETA=11:17:55, max mem: 20.9 GB 
[11/21 12:58:15 visual_prompt]: Epoch 12 / 100: avg data time: 1.93e-01, avg batch time: 1.0132, average train loss: 115.6275
[11/21 12:59:13 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3073, average loss: 121.8249
[11/21 12:59:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.01	
[11/21 12:59:13 visual_prompt]: Best epoch 12: best metric: -121.825
[11/21 12:59:13 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/21 13:00:59 visual_prompt]: 	Training 100/553. train loss: 104.7513,	0.8272 s / batch. (data: 2.07e-02). ETA=11:09:32, max mem: 20.9 GB 
[11/21 13:02:37 visual_prompt]: 	Training 200/553. train loss: 6.9765,	0.8237 s / batch. (data: 5.44e-03). ETA=11:05:21, max mem: 20.9 GB 
[11/21 13:04:19 visual_prompt]: 	Training 300/553. train loss: 98.9173,	1.8240 s / batch. (data: 9.89e-01). ETA=1 day, 0:30:16, max mem: 20.9 GB 
[11/21 13:05:58 visual_prompt]: 	Training 400/553. train loss: 208.4926,	0.8247 s / batch. (data: 4.13e-04). ETA=11:03:21, max mem: 20.9 GB 
[11/21 13:07:40 visual_prompt]: 	Training 500/553. train loss: 157.6126,	0.8177 s / batch. (data: 1.19e-02). ETA=10:56:22, max mem: 20.9 GB 
[11/21 13:08:32 visual_prompt]: Epoch 13 / 100: avg data time: 1.91e-01, avg batch time: 1.0117, average train loss: 87.1781
[11/21 13:09:30 visual_prompt]: Inference (val):avg data time: 5.23e-05, avg batch time: 0.3056, average loss: 192.0014
[11/21 13:09:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.44	
[11/21 13:09:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/21 13:11:16 visual_prompt]: 	Training 100/553. train loss: 31.8856,	0.8112 s / batch. (data: 2.88e-04). ETA=10:49:08, max mem: 20.9 GB 
[11/21 13:12:57 visual_prompt]: 	Training 200/553. train loss: 20.6185,	1.0350 s / batch. (data: 2.09e-01). ETA=13:46:29, max mem: 20.9 GB 
[11/21 13:14:37 visual_prompt]: 	Training 300/553. train loss: 50.9220,	0.8096 s / batch. (data: 3.35e-04). ETA=10:45:06, max mem: 20.9 GB 
[11/21 13:16:18 visual_prompt]: 	Training 400/553. train loss: 113.5434,	0.8080 s / batch. (data: 3.03e-04). ETA=10:42:30, max mem: 20.9 GB 
[11/21 13:17:59 visual_prompt]: 	Training 500/553. train loss: 16.4016,	0.8077 s / batch. (data: 3.11e-04). ETA=10:40:57, max mem: 20.9 GB 
[11/21 13:18:50 visual_prompt]: Epoch 14 / 100: avg data time: 1.92e-01, avg batch time: 1.0118, average train loss: 95.4231
[11/21 13:19:48 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3063, average loss: 35.9889
[11/21 13:19:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.24	
[11/21 13:19:48 visual_prompt]: Best epoch 14: best metric: -35.989
[11/21 13:19:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/21 13:21:33 visual_prompt]: 	Training 100/553. train loss: 77.4675,	0.8080 s / batch. (data: 3.39e-04). ETA=10:39:05, max mem: 20.9 GB 
[11/21 13:23:12 visual_prompt]: 	Training 200/553. train loss: 24.2004,	0.8336 s / batch. (data: 9.58e-03). ETA=10:57:56, max mem: 20.9 GB 
[11/21 13:24:54 visual_prompt]: 	Training 300/553. train loss: 144.2614,	0.8049 s / batch. (data: 3.04e-04). ETA=10:34:00, max mem: 20.9 GB 
[11/21 13:26:32 visual_prompt]: 	Training 400/553. train loss: 97.0569,	1.3554 s / batch. (data: 5.21e-01). ETA=17:45:18, max mem: 20.9 GB 
[11/21 13:28:15 visual_prompt]: 	Training 500/553. train loss: 26.6353,	0.8255 s / batch. (data: 5.52e-03). ETA=10:47:26, max mem: 20.9 GB 
[11/21 13:29:07 visual_prompt]: Epoch 15 / 100: avg data time: 1.92e-01, avg batch time: 1.0117, average train loss: 137.0193
[11/21 13:30:05 visual_prompt]: Inference (val):avg data time: 1.60e-04, avg batch time: 0.3060, average loss: 318.4160
[11/21 13:30:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[11/21 13:30:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/21 13:31:49 visual_prompt]: 	Training 100/553. train loss: 188.9208,	0.8080 s / batch. (data: 3.02e-04). ETA=10:31:41, max mem: 20.9 GB 
[11/21 13:33:30 visual_prompt]: 	Training 200/553. train loss: 44.1214,	0.8400 s / batch. (data: 7.98e-03). ETA=10:55:16, max mem: 20.9 GB 
[11/21 13:35:10 visual_prompt]: 	Training 300/553. train loss: 83.9578,	0.8046 s / batch. (data: 3.32e-04). ETA=10:26:20, max mem: 20.9 GB 
[11/21 13:36:51 visual_prompt]: 	Training 400/553. train loss: 95.3845,	0.8240 s / batch. (data: 8.55e-04). ETA=10:40:01, max mem: 20.9 GB 
[11/21 13:38:31 visual_prompt]: 	Training 500/553. train loss: 26.6261,	1.5498 s / batch. (data: 6.99e-01). ETA=20:01:12, max mem: 20.9 GB 
[11/21 13:39:24 visual_prompt]: Epoch 16 / 100: avg data time: 1.90e-01, avg batch time: 1.0101, average train loss: 90.8547
[11/21 13:40:22 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3089, average loss: 12.5311
[11/21 13:40:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/21 13:40:22 visual_prompt]: Best epoch 16: best metric: -12.531
[11/21 13:40:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/21 13:42:06 visual_prompt]: 	Training 100/553. train loss: 123.2160,	0.8564 s / batch. (data: 3.09e-02). ETA=11:01:34, max mem: 20.9 GB 
[11/21 13:43:48 visual_prompt]: 	Training 200/553. train loss: 49.1753,	0.8385 s / batch. (data: 5.50e-03). ETA=10:46:23, max mem: 20.9 GB 
[11/21 13:45:28 visual_prompt]: 	Training 300/553. train loss: 124.8512,	0.8441 s / batch. (data: 5.45e-03). ETA=10:49:14, max mem: 20.9 GB 
[11/21 13:47:08 visual_prompt]: 	Training 400/553. train loss: 30.3349,	0.8160 s / batch. (data: 5.46e-03). ETA=10:26:16, max mem: 20.9 GB 
[11/21 13:48:48 visual_prompt]: 	Training 500/553. train loss: 288.6173,	0.8160 s / batch. (data: 3.42e-04). ETA=10:24:55, max mem: 20.9 GB 
[11/21 13:49:44 visual_prompt]: Epoch 17 / 100: avg data time: 1.96e-01, avg batch time: 1.0165, average train loss: 94.9273
[11/21 13:50:51 visual_prompt]: Inference (val):avg data time: 2.09e-04, avg batch time: 0.3078, average loss: 44.3459
[11/21 13:50:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.74	
[11/21 13:50:51 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/21 13:52:38 visual_prompt]: 	Training 100/553. train loss: 103.0587,	0.8004 s / batch. (data: 3.14e-04). ETA=10:10:57, max mem: 20.9 GB 
[11/21 13:54:29 visual_prompt]: 	Training 200/553. train loss: 12.4207,	0.8376 s / batch. (data: 1.22e-02). ETA=10:37:58, max mem: 20.9 GB 
[11/21 13:56:10 visual_prompt]: 	Training 300/553. train loss: 114.4238,	0.8280 s / batch. (data: 3.21e-04). ETA=10:29:15, max mem: 20.9 GB 
[11/21 13:57:50 visual_prompt]: 	Training 400/553. train loss: 77.0176,	0.8214 s / batch. (data: 5.44e-03). ETA=10:22:55, max mem: 20.9 GB 
[11/21 13:59:40 visual_prompt]: 	Training 500/553. train loss: 48.2409,	4.0724 s / batch. (data: 3.24e+00). ETA=2 days, 3:21:23, max mem: 20.9 GB 
[11/21 14:00:34 visual_prompt]: Epoch 18 / 100: avg data time: 2.35e-01, avg batch time: 1.0545, average train loss: 95.3656
[11/21 14:01:41 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.3073, average loss: 165.1938
[11/21 14:01:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.67	
[11/21 14:01:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/21 14:03:26 visual_prompt]: 	Training 100/553. train loss: 37.5920,	0.8061 s / batch. (data: 7.70e-03). ETA=10:07:53, max mem: 20.9 GB 
[11/21 14:05:09 visual_prompt]: 	Training 200/553. train loss: 78.9635,	0.8059 s / batch. (data: 4.12e-04). ETA=10:06:21, max mem: 20.9 GB 
[11/21 14:06:51 visual_prompt]: 	Training 300/553. train loss: 444.9015,	0.8160 s / batch. (data: 3.37e-04). ETA=10:12:37, max mem: 20.9 GB 
[11/21 14:08:34 visual_prompt]: 	Training 400/553. train loss: 80.1770,	0.8282 s / batch. (data: 5.48e-03). ETA=10:20:22, max mem: 20.9 GB 
[11/21 14:10:12 visual_prompt]: 	Training 500/553. train loss: 267.2652,	0.8397 s / batch. (data: 1.17e-02). ETA=10:27:37, max mem: 20.9 GB 
[11/21 14:11:04 visual_prompt]: Epoch 19 / 100: avg data time: 1.99e-01, avg batch time: 1.0187, average train loss: 111.7685
[11/21 14:12:03 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3067, average loss: 261.0260
[11/21 14:12:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.96	
[11/21 14:12:03 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/21 14:13:46 visual_prompt]: 	Training 100/553. train loss: 41.4846,	0.8280 s / batch. (data: 5.50e-03). ETA=10:16:44, max mem: 20.9 GB 
[11/21 14:15:28 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8330 s / batch. (data: 5.46e-03). ETA=10:19:07, max mem: 20.9 GB 
[11/21 14:17:09 visual_prompt]: 	Training 300/553. train loss: 32.8508,	0.8079 s / batch. (data: 3.03e-04). ETA=9:59:06, max mem: 20.9 GB 
[11/21 14:18:51 visual_prompt]: 	Training 400/553. train loss: 91.8494,	0.8063 s / batch. (data: 3.32e-04). ETA=9:56:35, max mem: 20.9 GB 
[11/21 14:20:40 visual_prompt]: 	Training 500/553. train loss: 37.3914,	0.8320 s / batch. (data: 1.21e-03). ETA=10:14:10, max mem: 20.9 GB 
[11/21 14:21:34 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e-01, avg batch time: 1.0318, average train loss: 83.9298
[11/21 14:22:32 visual_prompt]: Inference (val):avg data time: 4.55e-05, avg batch time: 0.3086, average loss: 56.4203
[11/21 14:22:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.07	
[11/21 14:22:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/21 14:24:21 visual_prompt]: 	Training 100/553. train loss: 44.5488,	0.8050 s / batch. (data: 3.39e-04). ETA=9:52:10, max mem: 20.9 GB 
[11/21 14:26:00 visual_prompt]: 	Training 200/553. train loss: 0.0001,	0.8232 s / batch. (data: 3.86e-04). ETA=10:04:12, max mem: 20.9 GB 
[11/21 14:27:44 visual_prompt]: 	Training 300/553. train loss: 287.9181,	1.0171 s / batch. (data: 1.90e-01). ETA=12:24:51, max mem: 20.9 GB 
[11/21 14:29:26 visual_prompt]: 	Training 400/553. train loss: 398.9373,	0.8400 s / batch. (data: 3.26e-04). ETA=10:13:44, max mem: 20.9 GB 
[11/21 14:31:08 visual_prompt]: 	Training 500/553. train loss: 99.6390,	0.8029 s / batch. (data: 3.36e-04). ETA=9:45:20, max mem: 20.9 GB 
[11/21 14:32:00 visual_prompt]: Epoch 21 / 100: avg data time: 2.07e-01, avg batch time: 1.0267, average train loss: 98.6770
[11/21 14:32:58 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3075, average loss: 27.0083
[11/21 14:32:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.06	
[11/21 14:32:58 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/21 14:34:41 visual_prompt]: 	Training 100/553. train loss: 215.8323,	0.8280 s / batch. (data: 3.04e-04). ETA=10:01:30, max mem: 20.9 GB 
[11/21 14:36:23 visual_prompt]: 	Training 200/553. train loss: 27.9844,	0.8141 s / batch. (data: 3.28e-04). ETA=9:50:04, max mem: 20.9 GB 
[11/21 14:38:01 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8400 s / batch. (data: 2.99e-04). ETA=10:07:25, max mem: 20.9 GB 
[11/21 14:39:43 visual_prompt]: 	Training 400/553. train loss: 2.6714,	0.8130 s / batch. (data: 3.18e-04). ETA=9:46:34, max mem: 20.9 GB 
[11/21 14:41:24 visual_prompt]: 	Training 500/553. train loss: 100.9906,	0.8200 s / batch. (data: 3.12e-04). ETA=9:50:13, max mem: 20.9 GB 
[11/21 14:42:18 visual_prompt]: Epoch 22 / 100: avg data time: 1.92e-01, avg batch time: 1.0124, average train loss: 81.1880
[11/21 14:43:15 visual_prompt]: Inference (val):avg data time: 1.25e-04, avg batch time: 0.3085, average loss: 61.1178
[11/21 14:43:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.34	
[11/21 14:43:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/21 14:45:02 visual_prompt]: 	Training 100/553. train loss: 140.7388,	0.8223 s / batch. (data: 1.02e-02). ETA=9:49:45, max mem: 20.9 GB 
[11/21 14:46:43 visual_prompt]: 	Training 200/553. train loss: 43.6858,	0.8834 s / batch. (data: 5.99e-02). ETA=10:32:05, max mem: 20.9 GB 
[11/21 14:48:25 visual_prompt]: 	Training 300/553. train loss: 9.4103,	0.8280 s / batch. (data: 3.39e-04). ETA=9:51:06, max mem: 20.9 GB 
[11/21 14:50:04 visual_prompt]: 	Training 400/553. train loss: 2.1598,	0.8391 s / batch. (data: 8.44e-04). ETA=9:57:36, max mem: 20.9 GB 
[11/21 14:51:43 visual_prompt]: 	Training 500/553. train loss: 317.9267,	0.8111 s / batch. (data: 6.65e-04). ETA=9:36:20, max mem: 20.9 GB 
[11/21 14:52:36 visual_prompt]: Epoch 23 / 100: avg data time: 1.93e-01, avg batch time: 1.0135, average train loss: 82.0441
[11/21 14:53:34 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3066, average loss: 39.6244
[11/21 14:53:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.19	
[11/21 14:53:34 visual_prompt]: Stopping early.
[11/21 14:53:34 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 14:53:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 14:53:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 14:53:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/21 14:53:34 visual_prompt]: Training with config:
[11/21 14:53:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/21 14:53:34 visual_prompt]: Loading training data...
[11/21 14:53:34 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 14:53:34 visual_prompt]: Loading validation data...
[11/21 14:53:34 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 14:53:34 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 14:53:37 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/21 14:53:37 visual_prompt]: tuned percent:0.525
[11/21 14:53:37 visual_prompt]: Device used for model: 0
[11/21 14:53:37 visual_prompt]: Setting up Evaluator...
[11/21 14:53:37 visual_prompt]: Setting up Trainer...
[11/21 14:53:37 visual_prompt]: 	Setting up the optimizer...
[11/21 14:53:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 14:55:22 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8400 s / batch. (data: 7.94e-03). ETA=12:52:47, max mem: 20.9 GB 
[11/21 14:57:01 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8452 s / batch. (data: 2.19e-02). ETA=12:56:12, max mem: 20.9 GB 
[11/21 14:58:43 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.4536 s / batch. (data: 6.44e-01). ETA=22:12:25, max mem: 20.9 GB 
[11/21 15:00:23 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8280 s / batch. (data: 3.19e-04). ETA=12:37:36, max mem: 20.9 GB 
[11/21 15:02:05 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8374 s / batch. (data: 8.37e-04). ETA=12:44:47, max mem: 20.9 GB 
[11/21 15:02:58 visual_prompt]: Epoch 1 / 100: avg data time: 1.89e-01, avg batch time: 1.0149, average train loss: 1.5403
[11/21 15:03:56 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3089, average loss: 1.5201
[11/21 15:03:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/21 15:03:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/21 15:05:41 visual_prompt]: 	Training 100/553. train loss: 12.5197,	0.8315 s / batch. (data: 7.34e-03). ETA=12:37:18, max mem: 20.9 GB 
[11/21 15:07:22 visual_prompt]: 	Training 200/553. train loss: 0.0000,	2.5800 s / batch. (data: 1.75e+00). ETA=1 day, 15:05:31, max mem: 20.9 GB 
[11/21 15:09:04 visual_prompt]: 	Training 300/553. train loss: 11.5898,	1.0251 s / batch. (data: 2.01e-01). ETA=15:30:15, max mem: 20.9 GB 
[11/21 15:10:44 visual_prompt]: 	Training 400/553. train loss: 19.1761,	0.8323 s / batch. (data: 7.94e-03). ETA=12:33:53, max mem: 20.9 GB 
[11/21 15:12:27 visual_prompt]: 	Training 500/553. train loss: 3.7540,	0.8320 s / batch. (data: 3.70e-04). ETA=12:32:12, max mem: 20.9 GB 
[11/21 15:13:18 visual_prompt]: Epoch 2 / 100: avg data time: 1.90e-01, avg batch time: 1.0156, average train loss: 8.2145
[11/21 15:14:16 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.3069, average loss: 5.2982
[11/21 15:14:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.54	
[11/21 15:14:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/21 15:15:59 visual_prompt]: 	Training 100/553. train loss: 43.9622,	0.8301 s / batch. (data: 3.44e-04). ETA=12:28:21, max mem: 20.9 GB 
[11/21 15:17:42 visual_prompt]: 	Training 200/553. train loss: 15.4211,	1.3871 s / batch. (data: 5.70e-01). ETA=20:48:15, max mem: 20.9 GB 
[11/21 15:19:22 visual_prompt]: 	Training 300/553. train loss: 16.5867,	0.8265 s / batch. (data: 5.43e-03). ETA=12:22:21, max mem: 20.9 GB 
[11/21 15:21:03 visual_prompt]: 	Training 400/553. train loss: 14.2569,	0.8318 s / batch. (data: 1.05e-02). ETA=12:25:48, max mem: 20.9 GB 
[11/21 15:22:48 visual_prompt]: 	Training 500/553. train loss: 12.0256,	1.3422 s / batch. (data: 4.89e-01). ETA=20:01:05, max mem: 20.9 GB 
[11/21 15:23:39 visual_prompt]: Epoch 3 / 100: avg data time: 1.93e-01, avg batch time: 1.0182, average train loss: 19.3897
[11/21 15:24:42 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3079, average loss: 39.4502
[11/21 15:24:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.94	
[11/21 15:24:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/21 15:26:30 visual_prompt]: 	Training 100/553. train loss: 53.0640,	0.8280 s / batch. (data: 4.15e-04). ETA=12:18:52, max mem: 20.9 GB 
[11/21 15:28:12 visual_prompt]: 	Training 200/553. train loss: 50.8620,	0.8218 s / batch. (data: 5.48e-03). ETA=12:11:58, max mem: 20.9 GB 
[11/21 15:29:52 visual_prompt]: 	Training 300/553. train loss: 58.1711,	1.0593 s / batch. (data: 2.48e-01). ETA=15:41:44, max mem: 20.9 GB 
[11/21 15:31:31 visual_prompt]: 	Training 400/553. train loss: 24.9885,	0.8209 s / batch. (data: 3.79e-04). ETA=12:08:24, max mem: 20.9 GB 
[11/21 15:33:20 visual_prompt]: 	Training 500/553. train loss: 0.0108,	3.5208 s / batch. (data: 2.69e+00). ETA=2 days, 3:58:17, max mem: 20.9 GB 
[11/21 15:34:17 visual_prompt]: Epoch 4 / 100: avg data time: 2.17e-01, avg batch time: 1.0400, average train loss: 24.8907
[11/21 15:35:15 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3082, average loss: 20.4354
[11/21 15:35:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.85	
[11/21 15:35:15 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/21 15:36:59 visual_prompt]: 	Training 100/553. train loss: 32.3181,	0.8360 s / batch. (data: 7.95e-03). ETA=12:18:17, max mem: 20.9 GB 
[11/21 15:38:41 visual_prompt]: 	Training 200/553. train loss: 23.3870,	1.2033 s / batch. (data: 3.78e-01). ETA=17:40:42, max mem: 20.9 GB 
[11/21 15:40:23 visual_prompt]: 	Training 300/553. train loss: 34.9429,	0.8359 s / batch. (data: 3.61e-04). ETA=12:15:27, max mem: 20.9 GB 
[11/21 15:42:03 visual_prompt]: 	Training 400/553. train loss: 5.3576,	0.8238 s / batch. (data: 3.34e-04). ETA=12:03:24, max mem: 20.9 GB 
[11/21 15:43:44 visual_prompt]: 	Training 500/553. train loss: 38.4655,	0.8401 s / batch. (data: 3.21e-04). ETA=12:16:17, max mem: 20.9 GB 
[11/21 15:44:38 visual_prompt]: Epoch 5 / 100: avg data time: 1.93e-01, avg batch time: 1.0166, average train loss: 35.4545
[11/21 15:45:36 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3092, average loss: 52.2161
[11/21 15:45:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.09	
[11/21 15:45:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/21 15:47:22 visual_prompt]: 	Training 100/553. train loss: 22.3957,	0.8370 s / batch. (data: 3.25e-04). ETA=12:11:27, max mem: 20.9 GB 
[11/21 15:49:03 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8109 s / batch. (data: 3.13e-04). ETA=11:47:15, max mem: 20.9 GB 
[11/21 15:50:42 visual_prompt]: 	Training 300/553. train loss: 100.9606,	0.8120 s / batch. (data: 3.34e-04). ETA=11:46:57, max mem: 20.9 GB 
[11/21 15:52:27 visual_prompt]: 	Training 400/553. train loss: 36.3876,	0.8123 s / batch. (data: 3.26e-04). ETA=11:45:48, max mem: 20.9 GB 
[11/21 15:54:06 visual_prompt]: 	Training 500/553. train loss: 7.5757,	0.8560 s / batch. (data: 1.19e-02). ETA=12:22:21, max mem: 20.9 GB 
[11/21 15:54:58 visual_prompt]: Epoch 6 / 100: avg data time: 1.94e-01, avg batch time: 1.0176, average train loss: 46.4969
[11/21 15:55:56 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3078, average loss: 2.9002
[11/21 15:55:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 42.29	
[11/21 15:55:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/21 15:57:40 visual_prompt]: 	Training 100/553. train loss: 79.7656,	0.8280 s / batch. (data: 7.93e-03). ETA=11:55:57, max mem: 20.9 GB 
[11/21 15:59:21 visual_prompt]: 	Training 200/553. train loss: 23.5082,	0.8164 s / batch. (data: 4.05e-04). ETA=11:44:33, max mem: 20.9 GB 
[11/21 16:01:04 visual_prompt]: 	Training 300/553. train loss: 89.3683,	1.2399 s / batch. (data: 4.14e-01). ETA=17:48:02, max mem: 20.9 GB 
[11/21 16:02:46 visual_prompt]: 	Training 400/553. train loss: 9.0612,	1.8800 s / batch. (data: 1.02e+00). ETA=1 day, 2:56:14, max mem: 20.9 GB 
[11/21 16:04:26 visual_prompt]: 	Training 500/553. train loss: 2.2699,	0.8600 s / batch. (data: 3.26e-04). ETA=12:17:54, max mem: 20.9 GB 
[11/21 16:05:17 visual_prompt]: Epoch 7 / 100: avg data time: 1.91e-01, avg batch time: 1.0140, average train loss: 53.5550
[11/21 16:06:15 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3071, average loss: 85.8453
[11/21 16:06:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.41	
[11/21 16:06:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/21 16:07:59 visual_prompt]: 	Training 100/553. train loss: 54.0753,	0.8360 s / batch. (data: 1.70e-03). ETA=11:55:11, max mem: 20.9 GB 
[11/21 16:09:42 visual_prompt]: 	Training 200/553. train loss: 8.1587,	0.8151 s / batch. (data: 3.49e-04). ETA=11:35:58, max mem: 20.9 GB 
[11/21 16:11:23 visual_prompt]: 	Training 300/553. train loss: 10.8678,	0.8240 s / batch. (data: 3.18e-04). ETA=11:42:10, max mem: 20.9 GB 
[11/21 16:13:05 visual_prompt]: 	Training 400/553. train loss: 60.3140,	0.8080 s / batch. (data: 3.32e-04). ETA=11:27:10, max mem: 20.9 GB 
[11/21 16:14:45 visual_prompt]: 	Training 500/553. train loss: 200.6376,	1.4630 s / batch. (data: 6.40e-01). ETA=20:41:50, max mem: 20.9 GB 
[11/21 16:15:39 visual_prompt]: Epoch 8 / 100: avg data time: 1.97e-01, avg batch time: 1.0189, average train loss: 58.2043
[11/21 16:16:37 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3080, average loss: 165.8387
[11/21 16:16:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.33	
[11/21 16:16:37 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/21 16:18:23 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8113 s / batch. (data: 3.12e-04). ETA=11:26:34, max mem: 20.9 GB 
[11/21 16:20:04 visual_prompt]: 	Training 200/553. train loss: 10.1231,	0.8116 s / batch. (data: 3.24e-04). ETA=11:25:28, max mem: 20.9 GB 
[11/21 16:21:44 visual_prompt]: 	Training 300/553. train loss: 65.8969,	1.7680 s / batch. (data: 9.52e-01). ETA=1 day, 0:50:15, max mem: 20.9 GB 
[11/21 16:23:27 visual_prompt]: 	Training 400/553. train loss: 66.5996,	0.8221 s / batch. (data: 3.36e-04). ETA=11:31:36, max mem: 20.9 GB 
[11/21 16:25:07 visual_prompt]: 	Training 500/553. train loss: 21.8308,	0.8832 s / batch. (data: 6.75e-02). ETA=12:21:32, max mem: 20.9 GB 
[11/21 16:25:59 visual_prompt]: Epoch 9 / 100: avg data time: 1.95e-01, avg batch time: 1.0157, average train loss: 71.2444
[11/21 16:26:57 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3070, average loss: 110.4002
[11/21 16:26:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.95	
[11/21 16:26:57 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/21 16:28:44 visual_prompt]: 	Training 100/553. train loss: 133.3066,	0.8244 s / batch. (data: 1.05e-02). ETA=11:30:03, max mem: 20.9 GB 
[11/21 16:30:24 visual_prompt]: 	Training 200/553. train loss: 84.7284,	0.8444 s / batch. (data: 3.31e-04). ETA=11:45:23, max mem: 20.9 GB 
[11/21 16:32:04 visual_prompt]: 	Training 300/553. train loss: 576.9364,	0.8280 s / batch. (data: 3.23e-04). ETA=11:30:18, max mem: 20.9 GB 
[11/21 16:33:43 visual_prompt]: 	Training 400/553. train loss: 51.9027,	0.8240 s / batch. (data: 7.95e-03). ETA=11:25:38, max mem: 20.9 GB 
[11/21 16:35:25 visual_prompt]: 	Training 500/553. train loss: 23.9710,	0.8324 s / batch. (data: 2.55e-03). ETA=11:31:12, max mem: 20.9 GB 
[11/21 16:36:18 visual_prompt]: Epoch 10 / 100: avg data time: 1.94e-01, avg batch time: 1.0153, average train loss: 87.3272
[11/21 16:37:16 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3067, average loss: 112.9292
[11/21 16:37:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.07	
[11/21 16:37:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/21 16:39:04 visual_prompt]: 	Training 100/553. train loss: 114.9638,	0.8280 s / batch. (data: 2.99e-04). ETA=11:25:26, max mem: 20.9 GB 
[11/21 16:40:46 visual_prompt]: 	Training 200/553. train loss: 88.0898,	0.8355 s / batch. (data: 5.43e-03). ETA=11:30:13, max mem: 20.9 GB 
[11/21 16:42:26 visual_prompt]: 	Training 300/553. train loss: 14.5853,	1.7336 s / batch. (data: 9.21e-01). ETA=23:49:20, max mem: 20.9 GB 
[11/21 16:44:08 visual_prompt]: 	Training 400/553. train loss: 141.8358,	0.8257 s / batch. (data: 3.16e-04). ETA=11:19:26, max mem: 20.9 GB 
[11/21 16:45:48 visual_prompt]: 	Training 500/553. train loss: 142.3403,	0.8383 s / batch. (data: 2.19e-02). ETA=11:28:22, max mem: 20.9 GB 
[11/21 16:46:40 visual_prompt]: Epoch 11 / 100: avg data time: 1.97e-01, avg batch time: 1.0185, average train loss: 88.3990
[11/21 16:47:38 visual_prompt]: Inference (val):avg data time: 2.96e-04, avg batch time: 0.3100, average loss: 112.8124
[11/21 16:47:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.28	
[11/21 16:47:38 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/21 16:49:24 visual_prompt]: 	Training 100/553. train loss: 19.0796,	0.8185 s / batch. (data: 3.34e-04). ETA=11:10:03, max mem: 20.9 GB 
[11/21 16:51:09 visual_prompt]: 	Training 200/553. train loss: 83.0412,	0.8299 s / batch. (data: 8.20e-03). ETA=11:17:57, max mem: 20.9 GB 
[11/21 16:52:52 visual_prompt]: 	Training 300/553. train loss: 119.7655,	0.8360 s / batch. (data: 3.89e-04). ETA=11:21:33, max mem: 20.9 GB 
[11/21 16:54:36 visual_prompt]: 	Training 400/553. train loss: 22.7943,	0.8194 s / batch. (data: 3.29e-04). ETA=11:06:38, max mem: 20.9 GB 
[11/21 16:56:15 visual_prompt]: 	Training 500/553. train loss: 38.2310,	0.8006 s / batch. (data: 2.95e-04). ETA=10:50:01, max mem: 20.9 GB 
[11/21 16:57:06 visual_prompt]: Epoch 12 / 100: avg data time: 2.07e-01, avg batch time: 1.0273, average train loss: 85.2541
[11/21 16:58:05 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3073, average loss: 152.8936
[11/21 16:58:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.11	
[11/21 16:58:05 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/21 16:59:54 visual_prompt]: 	Training 100/553. train loss: 23.9103,	0.8680 s / batch. (data: 7.92e-03). ETA=11:42:32, max mem: 20.9 GB 
[11/21 17:01:35 visual_prompt]: 	Training 200/553. train loss: 49.2319,	0.8211 s / batch. (data: 1.05e-03). ETA=11:03:12, max mem: 20.9 GB 
[11/21 17:03:19 visual_prompt]: 	Training 300/553. train loss: 28.9799,	1.9960 s / batch. (data: 1.16e+00). ETA=1 day, 2:48:55, max mem: 20.9 GB 
[11/21 17:05:01 visual_prompt]: 	Training 400/553. train loss: 96.5915,	0.8374 s / batch. (data: 9.39e-03). ETA=11:13:37, max mem: 20.9 GB 
[11/21 17:06:45 visual_prompt]: 	Training 500/553. train loss: 92.0966,	0.8050 s / batch. (data: 3.29e-04). ETA=10:46:09, max mem: 20.9 GB 
[11/21 17:07:38 visual_prompt]: Epoch 13 / 100: avg data time: 2.16e-01, avg batch time: 1.0365, average train loss: 92.7903
[11/21 17:08:37 visual_prompt]: Inference (val):avg data time: 1.98e-04, avg batch time: 0.3087, average loss: 21.4387
[11/21 17:08:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[11/21 17:08:37 visual_prompt]: Best epoch 13: best metric: -21.439
[11/21 17:08:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/21 17:10:24 visual_prompt]: 	Training 100/553. train loss: 13.0888,	0.8295 s / batch. (data: 3.39e-04). ETA=11:03:44, max mem: 20.9 GB 
[11/21 17:12:07 visual_prompt]: 	Training 200/553. train loss: 48.8425,	1.0951 s / batch. (data: 2.73e-01). ETA=14:34:26, max mem: 20.9 GB 
[11/21 17:13:50 visual_prompt]: 	Training 300/553. train loss: 38.0073,	0.8320 s / batch. (data: 1.20e-02). ETA=11:02:58, max mem: 20.9 GB 
[11/21 17:15:35 visual_prompt]: 	Training 400/553. train loss: 27.0583,	0.8523 s / batch. (data: 1.63e-02). ETA=11:17:45, max mem: 20.9 GB 
[11/21 17:17:27 visual_prompt]: 	Training 500/553. train loss: 136.6361,	0.8154 s / batch. (data: 3.26e-04). ETA=10:47:00, max mem: 20.9 GB 
[11/21 17:18:22 visual_prompt]: Epoch 14 / 100: avg data time: 2.37e-01, avg batch time: 1.0575, average train loss: 85.7605
[11/21 17:19:23 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3070, average loss: 75.6512
[11/21 17:19:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.40	
[11/21 17:19:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/21 17:21:13 visual_prompt]: 	Training 100/553. train loss: 33.4414,	0.8120 s / batch. (data: 3.32e-04). ETA=10:42:15, max mem: 20.9 GB 
[11/21 17:22:56 visual_prompt]: 	Training 200/553. train loss: 190.2530,	0.8179 s / batch. (data: 3.09e-04). ETA=10:45:33, max mem: 20.9 GB 
[11/21 17:24:44 visual_prompt]: 	Training 300/553. train loss: 39.7126,	0.8458 s / batch. (data: 1.72e-02). ETA=11:06:08, max mem: 20.9 GB 
[11/21 17:26:30 visual_prompt]: 	Training 400/553. train loss: 36.3495,	1.4532 s / batch. (data: 6.52e-01). ETA=19:02:11, max mem: 20.9 GB 
[11/21 17:28:15 visual_prompt]: 	Training 500/553. train loss: 174.3483,	0.9199 s / batch. (data: 9.37e-02). ETA=12:01:29, max mem: 20.9 GB 
[11/21 17:29:11 visual_prompt]: Epoch 15 / 100: avg data time: 2.43e-01, avg batch time: 1.0627, average train loss: 81.3469
[11/21 17:30:11 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3070, average loss: 42.3722
[11/21 17:30:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.24	
[11/21 17:30:11 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/21 17:31:57 visual_prompt]: 	Training 100/553. train loss: 59.0136,	0.8146 s / batch. (data: 3.17e-04). ETA=10:36:50, max mem: 20.9 GB 
[11/21 17:33:43 visual_prompt]: 	Training 200/553. train loss: 124.4367,	0.8169 s / batch. (data: 5.52e-03). ETA=10:37:15, max mem: 20.9 GB 
[11/21 17:35:28 visual_prompt]: 	Training 300/553. train loss: 10.7524,	0.8280 s / batch. (data: 3.12e-04). ETA=10:44:32, max mem: 20.9 GB 
[11/21 17:37:15 visual_prompt]: 	Training 400/553. train loss: 123.1823,	0.8318 s / batch. (data: 1.06e-02). ETA=10:46:07, max mem: 20.9 GB 
[11/21 17:38:59 visual_prompt]: 	Training 500/553. train loss: 230.5610,	1.3880 s / batch. (data: 5.78e-01). ETA=17:55:51, max mem: 20.9 GB 
[11/21 17:39:55 visual_prompt]: Epoch 16 / 100: avg data time: 2.35e-01, avg batch time: 1.0560, average train loss: 92.4808
[11/21 17:40:56 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3074, average loss: 6.6274
[11/21 17:40:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.14	
[11/21 17:40:56 visual_prompt]: Best epoch 16: best metric: -6.627
[11/21 17:40:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/21 17:42:46 visual_prompt]: 	Training 100/553. train loss: 2.6529,	0.8240 s / batch. (data: 3.10e-04). ETA=10:36:34, max mem: 20.9 GB 
[11/21 17:44:34 visual_prompt]: 	Training 200/553. train loss: 152.5670,	0.8084 s / batch. (data: 3.28e-04). ETA=10:23:11, max mem: 20.9 GB 
[11/21 17:46:21 visual_prompt]: 	Training 300/553. train loss: 253.1240,	0.8042 s / batch. (data: 3.30e-04). ETA=10:18:36, max mem: 20.9 GB 
[11/21 17:48:09 visual_prompt]: 	Training 400/553. train loss: 36.0315,	1.5081 s / batch. (data: 6.92e-01). ETA=19:17:29, max mem: 20.9 GB 
[11/21 17:49:56 visual_prompt]: 	Training 500/553. train loss: 14.2437,	1.2934 s / batch. (data: 4.82e-01). ETA=16:30:33, max mem: 20.9 GB 
[11/21 17:50:53 visual_prompt]: Epoch 17 / 100: avg data time: 2.59e-01, avg batch time: 1.0785, average train loss: 97.3880
[11/21 17:51:52 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3071, average loss: 34.5270
[11/21 17:51:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 37.29	
[11/21 17:51:52 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/21 17:53:42 visual_prompt]: 	Training 100/553. train loss: 21.5394,	0.8213 s / batch. (data: 1.06e-02). ETA=10:26:56, max mem: 20.9 GB 
[11/21 17:55:31 visual_prompt]: 	Training 200/553. train loss: 102.2191,	0.8317 s / batch. (data: 3.25e-04). ETA=10:33:27, max mem: 20.9 GB 
[11/21 17:57:17 visual_prompt]: 	Training 300/553. train loss: 108.1322,	0.8120 s / batch. (data: 3.52e-04). ETA=10:17:07, max mem: 20.9 GB 
[11/21 17:59:03 visual_prompt]: 	Training 400/553. train loss: 52.3275,	0.8308 s / batch. (data: 1.05e-02). ETA=10:30:01, max mem: 20.9 GB 
[11/21 18:00:48 visual_prompt]: 	Training 500/553. train loss: 47.7289,	0.8344 s / batch. (data: 2.44e-02). ETA=10:31:20, max mem: 20.9 GB 
[11/21 18:01:42 visual_prompt]: Epoch 18 / 100: avg data time: 2.47e-01, avg batch time: 1.0668, average train loss: 86.3528
[11/21 18:02:43 visual_prompt]: Inference (val):avg data time: 2.80e-04, avg batch time: 0.3093, average loss: 2.3279
[11/21 18:02:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 40.65	rocauc: 37.02	
[11/21 18:02:43 visual_prompt]: Best epoch 18: best metric: -2.328
[11/21 18:02:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/21 18:04:35 visual_prompt]: 	Training 100/553. train loss: 69.7756,	1.5867 s / batch. (data: 7.45e-01). ETA=19:56:31, max mem: 20.9 GB 
[11/21 18:06:20 visual_prompt]: 	Training 200/553. train loss: 22.1144,	0.7998 s / batch. (data: 3.00e-04). ETA=10:01:45, max mem: 20.9 GB 
[11/21 18:08:09 visual_prompt]: 	Training 300/553. train loss: 408.8005,	1.0519 s / batch. (data: 2.28e-01). ETA=13:09:45, max mem: 20.9 GB 
[11/21 18:09:57 visual_prompt]: 	Training 400/553. train loss: 72.7405,	0.7981 s / batch. (data: 3.10e-04). ETA=9:57:53, max mem: 20.9 GB 
[11/21 18:11:40 visual_prompt]: 	Training 500/553. train loss: 109.6554,	0.8351 s / batch. (data: 1.05e-02). ETA=10:24:09, max mem: 20.9 GB 
[11/21 18:12:35 visual_prompt]: Epoch 19 / 100: avg data time: 2.51e-01, avg batch time: 1.0709, average train loss: 89.7050
[11/21 18:13:36 visual_prompt]: Inference (val):avg data time: 2.79e-04, avg batch time: 0.3080, average loss: 100.0859
[11/21 18:13:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.88	
[11/21 18:13:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/21 18:15:24 visual_prompt]: 	Training 100/553. train loss: 79.4471,	0.8160 s / batch. (data: 3.34e-04). ETA=10:07:50, max mem: 20.9 GB 
[11/21 18:17:12 visual_prompt]: 	Training 200/553. train loss: 157.0300,	0.8320 s / batch. (data: 3.48e-04). ETA=10:18:21, max mem: 20.9 GB 
[11/21 18:18:59 visual_prompt]: 	Training 300/553. train loss: 8.1106,	0.8286 s / batch. (data: 5.49e-03). ETA=10:14:27, max mem: 20.9 GB 
[11/21 18:20:46 visual_prompt]: 	Training 400/553. train loss: 2.5973,	0.8150 s / batch. (data: 5.17e-03). ETA=10:03:00, max mem: 20.9 GB 
[11/21 18:22:26 visual_prompt]: 	Training 500/553. train loss: 169.9998,	0.8399 s / batch. (data: 1.19e-02). ETA=10:20:02, max mem: 20.9 GB 
[11/21 18:23:18 visual_prompt]: Epoch 20 / 100: avg data time: 2.33e-01, avg batch time: 1.0530, average train loss: 91.2562
[11/21 18:24:14 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3075, average loss: 90.9191
[11/21 18:24:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.28	
[11/21 18:24:14 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/21 18:25:58 visual_prompt]: 	Training 100/553. train loss: 4.2993,	0.8238 s / batch. (data: 1.05e-02). ETA=10:06:03, max mem: 20.9 GB 
[11/21 18:27:33 visual_prompt]: 	Training 200/553. train loss: 36.2410,	0.8245 s / batch. (data: 2.61e-04). ETA=10:05:12, max mem: 20.9 GB 
[11/21 18:29:09 visual_prompt]: 	Training 300/553. train loss: 135.9170,	0.9359 s / batch. (data: 1.14e-01). ETA=11:25:24, max mem: 20.9 GB 
[11/21 18:30:44 visual_prompt]: 	Training 400/553. train loss: 9.0030,	0.8523 s / batch. (data: 3.10e-04). ETA=10:22:45, max mem: 20.9 GB 
[11/21 18:32:21 visual_prompt]: 	Training 500/553. train loss: 113.5523,	0.8323 s / batch. (data: 2.02e-02). ETA=10:06:45, max mem: 20.9 GB 
[11/21 18:33:11 visual_prompt]: Epoch 21 / 100: avg data time: 1.48e-01, avg batch time: 0.9698, average train loss: 95.3262
[11/21 18:34:06 visual_prompt]: Inference (val):avg data time: 2.07e-04, avg batch time: 0.3062, average loss: 4.7616
[11/21 18:34:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 52.42	
[11/21 18:34:06 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/21 18:35:48 visual_prompt]: 	Training 100/553. train loss: 50.1048,	0.8142 s / batch. (data: 3.01e-04). ETA=9:51:26, max mem: 20.9 GB 
[11/21 18:37:29 visual_prompt]: 	Training 200/553. train loss: 62.6233,	0.8200 s / batch. (data: 2.95e-04). ETA=9:54:19, max mem: 20.9 GB 
[11/21 18:39:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8135 s / batch. (data: 5.47e-03). ETA=9:48:15, max mem: 20.9 GB 
[11/21 18:40:49 visual_prompt]: 	Training 400/553. train loss: 54.8575,	0.8205 s / batch. (data: 3.07e-04). ETA=9:51:55, max mem: 20.9 GB 
[11/21 18:42:30 visual_prompt]: 	Training 500/553. train loss: 48.8533,	0.8479 s / batch. (data: 1.56e-02). ETA=10:10:18, max mem: 20.9 GB 
[11/21 18:43:24 visual_prompt]: Epoch 22 / 100: avg data time: 1.88e-01, avg batch time: 1.0096, average train loss: 75.0675
[11/21 18:44:22 visual_prompt]: Inference (val):avg data time: 3.52e-04, avg batch time: 0.3079, average loss: 47.6330
[11/21 18:44:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/21 18:44:22 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/21 18:46:08 visual_prompt]: 	Training 100/553. train loss: 15.8041,	0.8400 s / batch. (data: 3.20e-04). ETA=10:02:29, max mem: 20.9 GB 
[11/21 18:47:50 visual_prompt]: 	Training 200/553. train loss: 60.4298,	1.0075 s / batch. (data: 1.88e-01). ETA=12:00:56, max mem: 20.9 GB 
[11/21 18:49:32 visual_prompt]: 	Training 300/553. train loss: 9.5525,	0.8320 s / batch. (data: 2.28e-02). ETA=9:53:58, max mem: 20.9 GB 
[11/21 18:51:13 visual_prompt]: 	Training 400/553. train loss: 76.4443,	0.8253 s / batch. (data: 3.00e-04). ETA=9:47:50, max mem: 20.9 GB 
[11/21 18:52:52 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8065 s / batch. (data: 2.99e-04). ETA=9:33:05, max mem: 20.9 GB 
[11/21 18:53:45 visual_prompt]: Epoch 23 / 100: avg data time: 1.96e-01, avg batch time: 1.0176, average train loss: 81.9495
[11/21 18:54:43 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3079, average loss: 119.5339
[11/21 18:54:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.64	
[11/21 18:54:43 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/21 18:56:24 visual_prompt]: 	Training 100/553. train loss: 16.3050,	0.8145 s / batch. (data: 3.64e-04). ETA=9:36:38, max mem: 20.9 GB 
[11/21 18:58:05 visual_prompt]: 	Training 200/553. train loss: 61.7671,	0.8047 s / batch. (data: 5.44e-03). ETA=9:28:23, max mem: 20.9 GB 
[11/21 18:59:46 visual_prompt]: 	Training 300/553. train loss: 61.0474,	1.0967 s / batch. (data: 2.71e-01). ETA=12:52:51, max mem: 20.9 GB 
[11/21 19:01:27 visual_prompt]: 	Training 400/553. train loss: 97.8063,	0.8320 s / batch. (data: 5.43e-03). ETA=9:44:54, max mem: 20.9 GB 
[11/21 19:03:09 visual_prompt]: 	Training 500/553. train loss: 311.2027,	0.8156 s / batch. (data: 3.32e-04). ETA=9:32:01, max mem: 20.9 GB 
[11/21 19:04:02 visual_prompt]: Epoch 24 / 100: avg data time: 1.91e-01, avg batch time: 1.0114, average train loss: 82.1351
[11/21 19:05:00 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3081, average loss: 112.9347
[11/21 19:05:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.77	
[11/21 19:05:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/21 19:06:48 visual_prompt]: 	Training 100/553. train loss: 107.3279,	0.8241 s / batch. (data: 8.04e-03). ETA=9:35:52, max mem: 20.9 GB 
[11/21 19:08:26 visual_prompt]: 	Training 200/553. train loss: 86.5670,	0.9097 s / batch. (data: 8.39e-02). ETA=10:34:11, max mem: 20.9 GB 
[11/21 19:10:07 visual_prompt]: 	Training 300/553. train loss: 123.3767,	0.8320 s / batch. (data: 3.26e-04). ETA=9:38:37, max mem: 20.9 GB 
[11/21 19:11:47 visual_prompt]: 	Training 400/553. train loss: 207.6805,	1.2838 s / batch. (data: 4.59e-01). ETA=14:50:42, max mem: 20.9 GB 
[11/21 19:13:28 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.6120 s / batch. (data: 7.83e-01). ETA=18:35:42, max mem: 20.9 GB 
[11/21 19:14:20 visual_prompt]: Epoch 25 / 100: avg data time: 1.93e-01, avg batch time: 1.0129, average train loss: 88.6766
[11/21 19:15:18 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3098, average loss: 65.0240
[11/21 19:15:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.04	
[11/21 19:15:18 visual_prompt]: Stopping early.
[11/21 19:15:18 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 19:15:18 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 19:15:18 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 19:15:18 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/21 19:15:18 visual_prompt]: Training with config:
[11/21 19:15:18 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/21 19:15:18 visual_prompt]: Loading training data...
[11/21 19:15:18 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 19:15:18 visual_prompt]: Loading validation data...
[11/21 19:15:18 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 19:15:18 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 19:15:24 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/21 19:15:24 visual_prompt]: tuned percent:0.525
[11/21 19:15:24 visual_prompt]: Device used for model: 0
[11/21 19:15:24 visual_prompt]: Setting up Evaluator...
[11/21 19:15:24 visual_prompt]: Setting up Trainer...
[11/21 19:15:24 visual_prompt]: 	Setting up the optimizer...
[11/21 19:15:24 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 19:17:08 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8497 s / batch. (data: 3.00e-04). ETA=13:01:45, max mem: 20.9 GB 
[11/21 19:18:46 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8480 s / batch. (data: 3.18e-04). ETA=12:58:46, max mem: 20.9 GB 
[11/21 19:20:30 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.4362 s / batch. (data: 6.19e-01). ETA=21:56:29, max mem: 20.9 GB 
[11/21 19:22:09 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8328 s / batch. (data: 7.95e-03). ETA=12:42:02, max mem: 20.9 GB 
[11/21 19:23:52 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8233 s / batch. (data: 3.10e-04). ETA=12:31:58, max mem: 20.9 GB 
[11/21 19:24:45 visual_prompt]: Epoch 1 / 100: avg data time: 1.89e-01, avg batch time: 1.0148, average train loss: 1.5403
[11/21 19:25:43 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3071, average loss: 1.5201
[11/21 19:25:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/21 19:25:43 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/21 19:27:27 visual_prompt]: 	Training 100/553. train loss: 11.4707,	0.8092 s / batch. (data: 3.58e-04). ETA=12:16:59, max mem: 20.9 GB 
[11/21 19:29:07 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8368 s / batch. (data: 2.07e-02). ETA=12:40:43, max mem: 20.9 GB 
[11/21 19:30:50 visual_prompt]: 	Training 300/553. train loss: 5.9929,	1.0611 s / batch. (data: 2.30e-01). ETA=16:02:53, max mem: 20.9 GB 
[11/21 19:32:29 visual_prompt]: 	Training 400/553. train loss: 7.5676,	0.8319 s / batch. (data: 3.25e-04). ETA=12:33:31, max mem: 20.9 GB 
[11/21 19:34:11 visual_prompt]: 	Training 500/553. train loss: 1.9307,	0.8407 s / batch. (data: 5.45e-03). ETA=12:40:04, max mem: 20.9 GB 
[11/21 19:35:03 visual_prompt]: Epoch 2 / 100: avg data time: 1.86e-01, avg batch time: 1.0119, average train loss: 9.8064
[11/21 19:36:00 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3081, average loss: 21.4495
[11/21 19:36:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.40	
[11/21 19:36:00 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/21 19:37:43 visual_prompt]: 	Training 100/553. train loss: 15.6780,	0.8281 s / batch. (data: 1.20e-02). ETA=12:26:33, max mem: 20.9 GB 
[11/21 19:39:25 visual_prompt]: 	Training 200/553. train loss: 6.2633,	0.8553 s / batch. (data: 1.56e-02). ETA=12:49:40, max mem: 20.9 GB 
[11/21 19:41:05 visual_prompt]: 	Training 300/553. train loss: 8.7254,	0.8400 s / batch. (data: 3.10e-04). ETA=12:34:31, max mem: 20.9 GB 
[11/21 19:42:47 visual_prompt]: 	Training 400/553. train loss: 15.1111,	0.8160 s / batch. (data: 3.78e-04). ETA=12:11:35, max mem: 20.9 GB 
[11/21 19:44:29 visual_prompt]: 	Training 500/553. train loss: 71.0609,	1.4317 s / batch. (data: 6.19e-01). ETA=21:21:12, max mem: 20.9 GB 
[11/21 19:45:20 visual_prompt]: Epoch 3 / 100: avg data time: 1.86e-01, avg batch time: 1.0120, average train loss: 14.2301
[11/21 19:46:18 visual_prompt]: Inference (val):avg data time: 4.62e-05, avg batch time: 0.3066, average loss: 21.8537
[11/21 19:46:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.46	
[11/21 19:46:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/21 19:48:03 visual_prompt]: 	Training 100/553. train loss: 9.9626,	0.8280 s / batch. (data: 3.23e-04). ETA=12:18:51, max mem: 20.9 GB 
[11/21 19:49:44 visual_prompt]: 	Training 200/553. train loss: 59.3786,	0.8368 s / batch. (data: 3.42e-04). ETA=12:25:19, max mem: 20.9 GB 
[11/21 19:51:26 visual_prompt]: 	Training 300/553. train loss: 4.1254,	1.3325 s / batch. (data: 5.06e-01). ETA=19:44:39, max mem: 20.9 GB 
[11/21 19:53:02 visual_prompt]: 	Training 400/553. train loss: 6.7726,	1.2534 s / batch. (data: 4.40e-01). ETA=18:32:11, max mem: 20.9 GB 
[11/21 19:54:44 visual_prompt]: 	Training 500/553. train loss: 49.7587,	3.4023 s / batch. (data: 2.58e+00). ETA=2 days, 2:13:21, max mem: 20.9 GB 
[11/21 19:55:38 visual_prompt]: Epoch 4 / 100: avg data time: 1.89e-01, avg batch time: 1.0136, average train loss: 26.8838
[11/21 19:56:36 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3084, average loss: 25.4921
[11/21 19:56:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/21 19:56:36 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/21 19:58:19 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8360 s / batch. (data: 5.45e-03). ETA=12:18:19, max mem: 20.9 GB 
[11/21 20:00:00 visual_prompt]: 	Training 200/553. train loss: 14.0414,	1.1800 s / batch. (data: 3.43e-01). ETA=17:20:08, max mem: 20.9 GB 
[11/21 20:01:41 visual_prompt]: 	Training 300/553. train loss: 54.3054,	0.8471 s / batch. (data: 1.57e-02). ETA=12:25:18, max mem: 20.9 GB 
[11/21 20:03:21 visual_prompt]: 	Training 400/553. train loss: 31.3902,	0.8273 s / batch. (data: 3.27e-04). ETA=12:06:28, max mem: 20.9 GB 
[11/21 20:05:02 visual_prompt]: 	Training 500/553. train loss: 138.4451,	0.8480 s / batch. (data: 1.20e-02). ETA=12:23:15, max mem: 20.9 GB 
[11/21 20:05:56 visual_prompt]: Epoch 5 / 100: avg data time: 1.87e-01, avg batch time: 1.0117, average train loss: 29.4678
[11/21 20:06:52 visual_prompt]: Inference (val):avg data time: 4.88e-05, avg batch time: 0.3075, average loss: 44.2434
[11/21 20:06:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.93	
[11/21 20:06:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/21 20:08:39 visual_prompt]: 	Training 100/553. train loss: 152.0571,	0.8400 s / batch. (data: 3.39e-04). ETA=12:14:06, max mem: 20.9 GB 
[11/21 20:10:20 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8506 s / batch. (data: 1.46e-02). ETA=12:21:56, max mem: 20.9 GB 
[11/21 20:12:00 visual_prompt]: 	Training 300/553. train loss: 257.1109,	0.8320 s / batch. (data: 3.15e-04). ETA=12:04:19, max mem: 20.9 GB 
[11/21 20:13:44 visual_prompt]: 	Training 400/553. train loss: 63.9711,	0.8015 s / batch. (data: 3.26e-04). ETA=11:36:25, max mem: 20.9 GB 
[11/21 20:15:24 visual_prompt]: 	Training 500/553. train loss: 22.8794,	0.8305 s / batch. (data: 3.57e-04). ETA=12:00:16, max mem: 20.9 GB 
[11/21 20:16:16 visual_prompt]: Epoch 6 / 100: avg data time: 1.95e-01, avg batch time: 1.0191, average train loss: 43.9060
[11/21 20:17:14 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3070, average loss: 43.3593
[11/21 20:17:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.47	
[11/21 20:17:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/21 20:18:57 visual_prompt]: 	Training 100/553. train loss: 202.3694,	0.8430 s / batch. (data: 3.29e-04). ETA=12:08:56, max mem: 20.9 GB 
[11/21 20:20:38 visual_prompt]: 	Training 200/553. train loss: 34.1242,	0.8123 s / batch. (data: 5.22e-04). ETA=11:41:04, max mem: 20.9 GB 
[11/21 20:22:23 visual_prompt]: 	Training 300/553. train loss: 39.2158,	2.0557 s / batch. (data: 1.22e+00). ETA=1 day, 5:30:41, max mem: 20.9 GB 
[11/21 20:24:03 visual_prompt]: 	Training 400/553. train loss: 74.3282,	1.9620 s / batch. (data: 1.15e+00). ETA=1 day, 4:06:41, max mem: 20.9 GB 
[11/21 20:25:42 visual_prompt]: 	Training 500/553. train loss: 79.8908,	0.8435 s / batch. (data: 2.34e-02). ETA=12:03:43, max mem: 20.9 GB 
[11/21 20:26:33 visual_prompt]: Epoch 7 / 100: avg data time: 1.88e-01, avg batch time: 1.0118, average train loss: 55.4997
[11/21 20:27:31 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.3079, average loss: 23.6999
[11/21 20:27:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.52	
[11/21 20:27:31 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/21 20:29:14 visual_prompt]: 	Training 100/553. train loss: 2.0181,	0.8360 s / batch. (data: 3.25e-04). ETA=11:55:12, max mem: 20.9 GB 
[11/21 20:30:57 visual_prompt]: 	Training 200/553. train loss: 61.1637,	0.8287 s / batch. (data: 1.27e-02). ETA=11:47:33, max mem: 20.9 GB 
[11/21 20:32:38 visual_prompt]: 	Training 300/553. train loss: 145.5445,	0.8400 s / batch. (data: 3.18e-04). ETA=11:55:48, max mem: 20.9 GB 
[11/21 20:34:19 visual_prompt]: 	Training 400/553. train loss: 41.8939,	0.9943 s / batch. (data: 1.76e-01). ETA=14:05:40, max mem: 20.9 GB 
[11/21 20:36:01 visual_prompt]: 	Training 500/553. train loss: 21.5642,	1.5585 s / batch. (data: 7.27e-01). ETA=22:02:55, max mem: 20.9 GB 
[11/21 20:36:53 visual_prompt]: Epoch 8 / 100: avg data time: 1.93e-01, avg batch time: 1.0163, average train loss: 56.5643
[11/21 20:37:51 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.3075, average loss: 57.6910
[11/21 20:37:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.04	
[11/21 20:37:51 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/21 20:39:36 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8315 s / batch. (data: 1.19e-02). ETA=11:43:41, max mem: 20.9 GB 
[11/21 20:41:16 visual_prompt]: 	Training 200/553. train loss: 785.9756,	0.8440 s / batch. (data: 3.09e-04). ETA=11:52:51, max mem: 20.9 GB 
[11/21 20:42:58 visual_prompt]: 	Training 300/553. train loss: 56.0445,	1.5561 s / batch. (data: 7.19e-01). ETA=21:51:41, max mem: 20.9 GB 
[11/21 20:44:39 visual_prompt]: 	Training 400/553. train loss: 135.3336,	0.8300 s / batch. (data: 8.18e-04). ETA=11:38:13, max mem: 20.9 GB 
[11/21 20:46:21 visual_prompt]: 	Training 500/553. train loss: 26.5560,	1.0067 s / batch. (data: 1.69e-01). ETA=14:05:11, max mem: 20.9 GB 
[11/21 20:47:12 visual_prompt]: Epoch 9 / 100: avg data time: 1.91e-01, avg batch time: 1.0143, average train loss: 68.2584
[11/21 20:48:10 visual_prompt]: Inference (val):avg data time: 2.67e-04, avg batch time: 0.3087, average loss: 19.6656
[11/21 20:48:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.07	
[11/21 20:48:10 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/21 20:49:58 visual_prompt]: 	Training 100/553. train loss: 115.1190,	0.8293 s / batch. (data: 1.03e-03). ETA=11:34:08, max mem: 20.9 GB 
[11/21 20:51:38 visual_prompt]: 	Training 200/553. train loss: 64.3213,	0.8090 s / batch. (data: 3.18e-04). ETA=11:15:48, max mem: 20.9 GB 
[11/21 20:53:18 visual_prompt]: 	Training 300/553. train loss: 6.0597,	1.6720 s / batch. (data: 8.41e-01). ETA=23:13:58, max mem: 20.9 GB 
[11/21 20:54:57 visual_prompt]: 	Training 400/553. train loss: 90.0477,	0.8281 s / batch. (data: 3.45e-04). ETA=11:28:59, max mem: 20.9 GB 
[11/21 20:56:40 visual_prompt]: 	Training 500/553. train loss: 8.8133,	1.1247 s / batch. (data: 3.00e-01). ETA=15:33:55, max mem: 20.9 GB 
[11/21 20:57:33 visual_prompt]: Epoch 10 / 100: avg data time: 1.94e-01, avg batch time: 1.0164, average train loss: 76.3979
[11/21 20:58:30 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3073, average loss: 44.8438
[11/21 20:58:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/21 20:58:30 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/21 21:00:17 visual_prompt]: 	Training 100/553. train loss: 196.5261,	0.8364 s / batch. (data: 2.07e-02). ETA=11:32:21, max mem: 20.9 GB 
[11/21 21:02:00 visual_prompt]: 	Training 200/553. train loss: 278.2780,	0.8120 s / batch. (data: 2.90e-04). ETA=11:10:52, max mem: 20.9 GB 
[11/21 21:03:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2760 s / batch. (data: 1.45e+00). ETA=1 day, 7:16:34, max mem: 20.9 GB 
[11/21 21:05:19 visual_prompt]: 	Training 400/553. train loss: 36.4821,	0.8627 s / batch. (data: 1.47e-02). ETA=11:49:52, max mem: 20.9 GB 
[11/21 21:07:00 visual_prompt]: 	Training 500/553. train loss: 162.8521,	0.8521 s / batch. (data: 3.33e-04). ETA=11:39:41, max mem: 20.9 GB 
[11/21 21:07:51 visual_prompt]: Epoch 11 / 100: avg data time: 1.92e-01, avg batch time: 1.0144, average train loss: 83.0005
[11/21 21:08:49 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3084, average loss: 18.7106
[11/21 21:08:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.66	
[11/21 21:08:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/21 21:10:36 visual_prompt]: 	Training 100/553. train loss: 126.5586,	0.8247 s / batch. (data: 3.30e-04). ETA=11:15:08, max mem: 20.9 GB 
[11/21 21:12:18 visual_prompt]: 	Training 200/553. train loss: 152.5565,	0.8359 s / batch. (data: 3.40e-04). ETA=11:22:54, max mem: 20.9 GB 
[11/21 21:13:57 visual_prompt]: 	Training 300/553. train loss: 21.7700,	0.8200 s / batch. (data: 3.10e-04). ETA=11:08:31, max mem: 20.9 GB 
[11/21 21:15:39 visual_prompt]: 	Training 400/553. train loss: 93.6778,	0.8280 s / batch. (data: 3.29e-04). ETA=11:13:41, max mem: 20.9 GB 
[11/21 21:17:20 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8360 s / batch. (data: 2.90e-04). ETA=11:18:47, max mem: 20.9 GB 
[11/21 21:18:11 visual_prompt]: Epoch 12 / 100: avg data time: 1.94e-01, avg batch time: 1.0157, average train loss: 86.6268
[11/21 21:19:09 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3092, average loss: 228.3232
[11/21 21:19:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.58	
[11/21 21:19:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/21 21:20:56 visual_prompt]: 	Training 100/553. train loss: 23.4021,	0.8065 s / batch. (data: 3.27e-04). ETA=10:52:48, max mem: 20.9 GB 
[11/21 21:22:36 visual_prompt]: 	Training 200/553. train loss: 55.1706,	0.8288 s / batch. (data: 1.56e-02). ETA=11:09:29, max mem: 20.9 GB 
[11/21 21:24:18 visual_prompt]: 	Training 300/553. train loss: 16.4816,	1.8200 s / batch. (data: 9.84e-01). ETA=1 day, 0:27:02, max mem: 20.9 GB 
[11/21 21:25:57 visual_prompt]: 	Training 400/553. train loss: 7.3962,	0.8468 s / batch. (data: 1.89e-02). ETA=11:21:10, max mem: 20.9 GB 
[11/21 21:27:39 visual_prompt]: 	Training 500/553. train loss: 32.3706,	0.8320 s / batch. (data: 3.35e-04). ETA=11:07:53, max mem: 20.9 GB 
[11/21 21:28:31 visual_prompt]: Epoch 13 / 100: avg data time: 1.95e-01, avg batch time: 1.0163, average train loss: 89.5001
[11/21 21:29:29 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3071, average loss: 41.0821
[11/21 21:29:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.43	
[11/21 21:29:29 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/21 21:31:14 visual_prompt]: 	Training 100/553. train loss: 79.9684,	0.8120 s / batch. (data: 3.37e-04). ETA=10:49:46, max mem: 20.9 GB 
[11/21 21:32:55 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.3040 s / batch. (data: 4.83e-01). ETA=17:21:15, max mem: 20.9 GB 
[11/21 21:34:35 visual_prompt]: 	Training 300/553. train loss: 81.4320,	0.8320 s / batch. (data: 3.50e-04). ETA=11:02:58, max mem: 20.9 GB 
[11/21 21:36:16 visual_prompt]: 	Training 400/553. train loss: 23.2426,	0.8086 s / batch. (data: 5.46e-03). ETA=10:42:58, max mem: 20.9 GB 
[11/21 21:37:57 visual_prompt]: 	Training 500/553. train loss: 128.8445,	0.8258 s / batch. (data: 3.22e-04). ETA=10:55:18, max mem: 20.9 GB 
[11/21 21:38:48 visual_prompt]: Epoch 14 / 100: avg data time: 1.88e-01, avg batch time: 1.0102, average train loss: 94.1733
[11/21 21:39:45 visual_prompt]: Inference (val):avg data time: 4.20e-04, avg batch time: 0.3091, average loss: 72.9262
[11/21 21:39:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.32	
[11/21 21:39:45 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/21 21:41:30 visual_prompt]: 	Training 100/553. train loss: 107.7245,	0.8040 s / batch. (data: 3.35e-04). ETA=10:35:55, max mem: 20.9 GB 
[11/21 21:43:20 visual_prompt]: 	Training 200/553. train loss: 338.1583,	0.8220 s / batch. (data: 5.44e-03). ETA=10:48:49, max mem: 20.9 GB 
[11/21 21:45:02 visual_prompt]: 	Training 300/553. train loss: 10.7235,	0.8353 s / batch. (data: 3.43e-04). ETA=10:57:54, max mem: 20.9 GB 
[11/21 21:46:40 visual_prompt]: 	Training 400/553. train loss: 204.6113,	0.8400 s / batch. (data: 3.33e-04). ETA=11:00:14, max mem: 20.9 GB 
[11/21 21:48:22 visual_prompt]: 	Training 500/553. train loss: 120.6582,	0.8203 s / batch. (data: 4.05e-04). ETA=10:43:19, max mem: 20.9 GB 
[11/21 21:49:15 visual_prompt]: Epoch 15 / 100: avg data time: 2.08e-01, avg batch time: 1.0297, average train loss: 93.8931
[11/21 21:50:13 visual_prompt]: Inference (val):avg data time: 4.53e-05, avg batch time: 0.3090, average loss: 5.1922
[11/21 21:50:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.96	
[11/21 21:50:13 visual_prompt]: Best epoch 15: best metric: -5.192
[11/21 21:50:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/21 21:51:56 visual_prompt]: 	Training 100/553. train loss: 40.4202,	0.8445 s / batch. (data: 5.45e-03). ETA=11:00:10, max mem: 20.9 GB 
[11/21 21:53:37 visual_prompt]: 	Training 200/553. train loss: 140.7945,	0.8361 s / batch. (data: 3.27e-04). ETA=10:52:14, max mem: 20.9 GB 
[11/21 21:55:18 visual_prompt]: 	Training 300/553. train loss: 13.6437,	0.8296 s / batch. (data: 2.91e-04). ETA=10:45:44, max mem: 20.9 GB 
[11/21 21:56:59 visual_prompt]: 	Training 400/553. train loss: 64.7440,	0.8320 s / batch. (data: 3.26e-04). ETA=10:46:14, max mem: 20.9 GB 
[11/21 21:58:39 visual_prompt]: 	Training 500/553. train loss: 203.5595,	1.2015 s / batch. (data: 3.88e-01). ETA=15:31:15, max mem: 20.9 GB 
[11/21 21:59:32 visual_prompt]: Epoch 16 / 100: avg data time: 1.89e-01, avg batch time: 1.0118, average train loss: 78.6311
[11/21 22:00:30 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3077, average loss: 4.1551
[11/21 22:00:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.29	
[11/21 22:00:30 visual_prompt]: Best epoch 16: best metric: -4.155
[11/21 22:00:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/21 22:02:15 visual_prompt]: 	Training 100/553. train loss: 3.5588,	0.8313 s / batch. (data: 1.05e-02). ETA=10:42:12, max mem: 20.9 GB 
[11/21 22:03:57 visual_prompt]: 	Training 200/553. train loss: 182.7799,	0.8325 s / batch. (data: 1.60e-02). ETA=10:41:46, max mem: 20.9 GB 
[11/21 22:05:37 visual_prompt]: 	Training 300/553. train loss: 111.7145,	0.8120 s / batch. (data: 3.09e-04). ETA=10:24:34, max mem: 20.9 GB 
[11/21 22:07:17 visual_prompt]: 	Training 400/553. train loss: 320.7078,	1.5148 s / batch. (data: 6.97e-01). ETA=19:22:38, max mem: 20.9 GB 
[11/21 22:08:56 visual_prompt]: 	Training 500/553. train loss: 124.6979,	1.0640 s / batch. (data: 2.29e-01). ETA=13:34:52, max mem: 20.9 GB 
[11/21 22:09:51 visual_prompt]: Epoch 17 / 100: avg data time: 1.92e-01, avg batch time: 1.0135, average train loss: 80.3591
[11/21 22:10:49 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3081, average loss: 219.9071
[11/21 22:10:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.28	
[11/21 22:10:49 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/21 22:12:34 visual_prompt]: 	Training 100/553. train loss: 65.2004,	0.8174 s / batch. (data: 1.56e-02). ETA=10:23:57, max mem: 20.9 GB 
[11/21 22:14:17 visual_prompt]: 	Training 200/553. train loss: 30.3554,	0.8137 s / batch. (data: 5.46e-03). ETA=10:19:45, max mem: 20.9 GB 
[11/21 22:15:58 visual_prompt]: 	Training 300/553. train loss: 118.4125,	0.8200 s / batch. (data: 3.13e-04). ETA=10:23:10, max mem: 20.9 GB 
[11/21 22:17:39 visual_prompt]: 	Training 400/553. train loss: 23.7052,	0.8406 s / batch. (data: 1.05e-02). ETA=10:37:24, max mem: 20.9 GB 
[11/21 22:19:19 visual_prompt]: 	Training 500/553. train loss: 84.4155,	0.8302 s / batch. (data: 7.93e-03). ETA=10:28:11, max mem: 20.9 GB 
[11/21 22:20:10 visual_prompt]: Epoch 18 / 100: avg data time: 1.92e-01, avg batch time: 1.0137, average train loss: 81.1541
[11/21 22:21:08 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.3089, average loss: 55.5910
[11/21 22:21:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.31	
[11/21 22:21:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/21 22:22:53 visual_prompt]: 	Training 100/553. train loss: 33.5354,	1.3840 s / batch. (data: 5.79e-01). ETA=17:23:41, max mem: 20.9 GB 
[11/21 22:24:34 visual_prompt]: 	Training 200/553. train loss: 48.6493,	0.8142 s / batch. (data: 3.11e-04). ETA=10:12:36, max mem: 20.9 GB 
[11/21 22:26:15 visual_prompt]: 	Training 300/553. train loss: 153.9044,	0.8156 s / batch. (data: 5.43e-03). ETA=10:12:18, max mem: 20.9 GB 
[11/21 22:27:57 visual_prompt]: 	Training 400/553. train loss: 50.0509,	0.8200 s / batch. (data: 8.73e-04). ETA=10:14:15, max mem: 20.9 GB 
[11/21 22:29:33 visual_prompt]: 	Training 500/553. train loss: 2.3170,	0.8248 s / batch. (data: 2.96e-04). ETA=10:16:29, max mem: 20.9 GB 
[11/21 22:30:26 visual_prompt]: Epoch 19 / 100: avg data time: 1.86e-01, avg batch time: 1.0087, average train loss: 72.0063
[11/21 22:31:24 visual_prompt]: Inference (val):avg data time: 3.82e-04, avg batch time: 0.3097, average loss: 17.9899
[11/21 22:31:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.09	
[11/21 22:31:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/21 22:33:07 visual_prompt]: 	Training 100/553. train loss: 5.2882,	0.8200 s / batch. (data: 3.40e-04). ETA=10:10:48, max mem: 20.9 GB 
[11/21 22:34:48 visual_prompt]: 	Training 200/553. train loss: 18.6954,	0.8465 s / batch. (data: 1.05e-02). ETA=10:29:07, max mem: 20.9 GB 
[11/21 22:36:29 visual_prompt]: 	Training 300/553. train loss: 22.6528,	0.8189 s / batch. (data: 4.27e-04). ETA=10:07:14, max mem: 20.9 GB 
[11/21 22:38:10 visual_prompt]: 	Training 400/553. train loss: 22.2894,	0.8044 s / batch. (data: 3.32e-04). ETA=9:55:10, max mem: 20.9 GB 
[11/21 22:39:50 visual_prompt]: 	Training 500/553. train loss: 41.9790,	0.8100 s / batch. (data: 3.29e-04). ETA=9:57:56, max mem: 20.9 GB 
[11/21 22:40:43 visual_prompt]: Epoch 20 / 100: avg data time: 1.90e-01, avg batch time: 1.0122, average train loss: 79.3548
[11/21 22:41:41 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.3089, average loss: 51.9980
[11/21 22:41:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.62	
[11/21 22:41:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/21 22:43:29 visual_prompt]: 	Training 100/553. train loss: 77.1954,	0.8280 s / batch. (data: 3.27e-04). ETA=10:09:08, max mem: 20.9 GB 
[11/21 22:45:09 visual_prompt]: 	Training 200/553. train loss: 90.5409,	0.8075 s / batch. (data: 5.44e-03). ETA=9:52:44, max mem: 20.9 GB 
[11/21 22:46:49 visual_prompt]: 	Training 300/553. train loss: 120.1923,	1.0377 s / batch. (data: 2.25e-01). ETA=12:39:56, max mem: 20.9 GB 
[11/21 22:48:29 visual_prompt]: 	Training 400/553. train loss: 23.0740,	0.8126 s / batch. (data: 3.08e-04). ETA=9:53:42, max mem: 20.9 GB 
[11/21 22:50:11 visual_prompt]: 	Training 500/553. train loss: 62.0444,	0.8199 s / batch. (data: 7.92e-03). ETA=9:57:44, max mem: 20.9 GB 
[11/21 22:51:03 visual_prompt]: Epoch 21 / 100: avg data time: 1.92e-01, avg batch time: 1.0145, average train loss: 80.6375
[11/21 22:52:01 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.3085, average loss: 5.7173
[11/21 22:52:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.95	
[11/21 22:52:01 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/21 22:53:44 visual_prompt]: 	Training 100/553. train loss: 45.5380,	0.8295 s / batch. (data: 7.92e-03). ETA=10:02:35, max mem: 20.9 GB 
[11/21 22:55:25 visual_prompt]: 	Training 200/553. train loss: 14.3998,	0.8640 s / batch. (data: 3.07e-04). ETA=10:26:10, max mem: 20.9 GB 
[11/21 22:57:03 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8407 s / batch. (data: 5.48e-03). ETA=10:07:56, max mem: 20.9 GB 
[11/21 22:58:45 visual_prompt]: 	Training 400/553. train loss: 41.4337,	0.8120 s / batch. (data: 3.04e-04). ETA=9:45:49, max mem: 20.9 GB 
[11/21 23:00:26 visual_prompt]: 	Training 500/553. train loss: 41.4893,	0.8145 s / batch. (data: 3.22e-04). ETA=9:46:15, max mem: 20.9 GB 
[11/21 23:01:20 visual_prompt]: Epoch 22 / 100: avg data time: 1.91e-01, avg batch time: 1.0119, average train loss: 82.7818
[11/21 23:02:18 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3071, average loss: 30.7563
[11/21 23:02:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[11/21 23:02:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/21 23:04:04 visual_prompt]: 	Training 100/553. train loss: 123.4828,	0.8290 s / batch. (data: 3.27e-04). ETA=9:54:35, max mem: 20.9 GB 
[11/21 23:05:47 visual_prompt]: 	Training 200/553. train loss: 15.2011,	0.9361 s / batch. (data: 1.11e-01). ETA=11:09:51, max mem: 20.9 GB 
[11/21 23:07:30 visual_prompt]: 	Training 300/553. train loss: 234.7354,	0.8641 s / batch. (data: 7.52e-04). ETA=10:16:53, max mem: 20.9 GB 
[11/21 23:09:09 visual_prompt]: 	Training 400/553. train loss: 102.2313,	0.8287 s / batch. (data: 1.05e-02). ETA=9:50:14, max mem: 20.9 GB 
[11/21 23:10:48 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8280 s / batch. (data: 3.14e-04). ETA=9:48:20, max mem: 20.9 GB 
[11/21 23:11:41 visual_prompt]: Epoch 23 / 100: avg data time: 1.96e-01, avg batch time: 1.0174, average train loss: 75.8306
[11/21 23:12:39 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.3086, average loss: 146.7328
[11/21 23:12:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.00	
[11/21 23:12:39 visual_prompt]: Stopping early.
[11/21 23:12:39 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 23:12:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 23:12:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 23:12:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/21 23:12:39 visual_prompt]: Training with config:
[11/21 23:12:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/21 23:12:39 visual_prompt]: Loading training data...
[11/21 23:12:39 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 23:12:39 visual_prompt]: Loading validation data...
[11/21 23:12:39 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 23:12:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 23:12:42 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/21 23:12:42 visual_prompt]: tuned percent:0.525
[11/21 23:12:42 visual_prompt]: Device used for model: 0
[11/21 23:12:42 visual_prompt]: Setting up Evaluator...
[11/21 23:12:42 visual_prompt]: Setting up Trainer...
[11/21 23:12:42 visual_prompt]: 	Setting up the optimizer...
[11/21 23:12:42 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 23:14:26 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8200 s / batch. (data: 6.78e-03). ETA=12:34:23, max mem: 20.9 GB 
[11/21 23:16:05 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8258 s / batch. (data: 2.88e-04). ETA=12:38:22, max mem: 20.9 GB 
[11/21 23:17:49 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3000 s / batch. (data: 4.69e-01). ETA=19:51:40, max mem: 20.9 GB 
[11/21 23:19:28 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 1.19e-02). ETA=12:52:17, max mem: 20.9 GB 
[11/21 23:21:11 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8356 s / batch. (data: 8.30e-04). ETA=12:43:08, max mem: 20.9 GB 
[11/21 23:22:04 visual_prompt]: Epoch 1 / 100: avg data time: 1.91e-01, avg batch time: 1.0171, average train loss: 1.5403
[11/21 23:23:02 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3083, average loss: 1.5201
[11/21 23:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/21 23:23:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/21 23:24:46 visual_prompt]: 	Training 100/553. train loss: 9.3182,	1.1080 s / batch. (data: 2.85e-01). ETA=16:49:09, max mem: 20.9 GB 
[11/21 23:26:27 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.5685 s / batch. (data: 7.44e-01). ETA=23:45:56, max mem: 20.9 GB 
[11/21 23:28:09 visual_prompt]: 	Training 300/553. train loss: 5.1931,	1.1800 s / batch. (data: 3.65e-01). ETA=17:50:47, max mem: 20.9 GB 
[11/21 23:29:49 visual_prompt]: 	Training 400/553. train loss: 0.6944,	0.8440 s / batch. (data: 7.95e-03). ETA=12:44:31, max mem: 20.9 GB 
[11/21 23:31:31 visual_prompt]: 	Training 500/553. train loss: 0.5637,	0.8440 s / batch. (data: 3.15e-04). ETA=12:43:05, max mem: 20.9 GB 
[11/21 23:32:23 visual_prompt]: Epoch 2 / 100: avg data time: 1.87e-01, avg batch time: 1.0136, average train loss: 9.8902
[11/21 23:33:21 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3085, average loss: 11.6990
[11/21 23:33:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.64	
[11/21 23:33:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/21 23:35:04 visual_prompt]: 	Training 100/553. train loss: 20.6916,	0.8199 s / batch. (data: 3.17e-04). ETA=12:19:13, max mem: 20.9 GB 
[11/21 23:36:47 visual_prompt]: 	Training 200/553. train loss: 7.1333,	0.8302 s / batch. (data: 2.12e-02). ETA=12:27:04, max mem: 20.9 GB 
[11/21 23:38:27 visual_prompt]: 	Training 300/553. train loss: 9.6720,	0.8360 s / batch. (data: 7.95e-03). ETA=12:30:55, max mem: 20.9 GB 
[11/21 23:40:07 visual_prompt]: 	Training 400/553. train loss: 15.5484,	0.8098 s / batch. (data: 3.02e-04). ETA=12:06:01, max mem: 20.9 GB 
[11/21 23:41:50 visual_prompt]: 	Training 500/553. train loss: 4.5820,	1.2721 s / batch. (data: 4.55e-01). ETA=18:58:22, max mem: 20.9 GB 
[11/21 23:42:41 visual_prompt]: Epoch 3 / 100: avg data time: 1.88e-01, avg batch time: 1.0136, average train loss: 12.6822
[11/21 23:43:39 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.3081, average loss: 9.8754
[11/21 23:43:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.28	
[11/21 23:43:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/21 23:45:25 visual_prompt]: 	Training 100/553. train loss: 14.6318,	0.8562 s / batch. (data: 2.41e-02). ETA=12:43:59, max mem: 20.9 GB 
[11/21 23:47:07 visual_prompt]: 	Training 200/553. train loss: 14.6047,	0.8157 s / batch. (data: 3.41e-04). ETA=12:06:32, max mem: 20.9 GB 
[11/21 23:48:49 visual_prompt]: 	Training 300/553. train loss: 12.7162,	1.3949 s / batch. (data: 5.69e-01). ETA=20:40:07, max mem: 20.9 GB 
[11/21 23:50:25 visual_prompt]: 	Training 400/553. train loss: 4.5224,	1.5840 s / batch. (data: 7.33e-01). ETA=23:25:34, max mem: 20.9 GB 
[11/21 23:52:08 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.3742 s / batch. (data: 2.58e+00). ETA=2 days, 1:48:29, max mem: 20.9 GB 
[11/21 23:53:02 visual_prompt]: Epoch 4 / 100: avg data time: 1.91e-01, avg batch time: 1.0172, average train loss: 17.3637
[11/21 23:53:59 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3081, average loss: 93.1347
[11/21 23:53:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.46	
[11/21 23:53:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/21 23:55:43 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 3.13e-04). ETA=12:00:39, max mem: 20.9 GB 
[11/21 23:57:24 visual_prompt]: 	Training 200/553. train loss: 6.3354,	1.3595 s / batch. (data: 5.23e-01). ETA=19:58:23, max mem: 20.9 GB 
[11/21 23:59:06 visual_prompt]: 	Training 300/553. train loss: 36.2018,	0.8483 s / batch. (data: 2.48e-02). ETA=12:26:20, max mem: 20.9 GB 
[11/22 00:00:46 visual_prompt]: 	Training 400/553. train loss: 41.4358,	0.8423 s / batch. (data: 8.52e-04). ETA=12:19:37, max mem: 20.9 GB 
[11/22 00:02:27 visual_prompt]: 	Training 500/553. train loss: 32.2609,	0.8150 s / batch. (data: 5.46e-03). ETA=11:54:21, max mem: 20.9 GB 
[11/22 00:03:21 visual_prompt]: Epoch 5 / 100: avg data time: 1.91e-01, avg batch time: 1.0146, average train loss: 27.6257
[11/22 00:04:18 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.3100, average loss: 89.6345
[11/22 00:04:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.05	
[11/22 00:04:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/22 00:06:05 visual_prompt]: 	Training 100/553. train loss: 9.4687,	0.8520 s / batch. (data: 1.12e-02). ETA=12:24:34, max mem: 20.9 GB 
[11/22 00:07:46 visual_prompt]: 	Training 200/553. train loss: 8.8481,	0.8360 s / batch. (data: 3.22e-04). ETA=12:09:12, max mem: 20.9 GB 
[11/22 00:09:25 visual_prompt]: 	Training 300/553. train loss: 5.5799,	0.8320 s / batch. (data: 3.00e-04). ETA=12:04:19, max mem: 20.9 GB 
[11/22 00:11:09 visual_prompt]: 	Training 400/553. train loss: 35.0367,	0.8200 s / batch. (data: 3.60e-04). ETA=11:52:28, max mem: 20.9 GB 
[11/22 00:12:48 visual_prompt]: 	Training 500/553. train loss: 27.2777,	0.8420 s / batch. (data: 7.94e-03). ETA=12:10:14, max mem: 20.9 GB 
[11/22 00:13:41 visual_prompt]: Epoch 6 / 100: avg data time: 1.91e-01, avg batch time: 1.0163, average train loss: 34.5054
[11/22 00:14:38 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.3081, average loss: 11.0577
[11/22 00:14:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.58	
[11/22 00:14:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/22 00:16:22 visual_prompt]: 	Training 100/553. train loss: 0.0337,	0.8416 s / batch. (data: 8.46e-04). ETA=12:07:43, max mem: 20.9 GB 
[11/22 00:18:04 visual_prompt]: 	Training 200/553. train loss: 22.7581,	0.8108 s / batch. (data: 3.29e-04). ETA=11:39:43, max mem: 20.9 GB 
[11/22 00:19:48 visual_prompt]: 	Training 300/553. train loss: 3.9200,	2.1920 s / batch. (data: 1.35e+00). ETA=1 day, 7:28:08, max mem: 20.9 GB 
[11/22 00:21:29 visual_prompt]: 	Training 400/553. train loss: 17.9251,	2.0409 s / batch. (data: 1.22e+00). ETA=1 day, 5:14:33, max mem: 20.9 GB 
[11/22 00:23:09 visual_prompt]: 	Training 500/553. train loss: 135.0038,	0.8289 s / batch. (data: 5.46e-03). ETA=11:51:14, max mem: 20.9 GB 
[11/22 00:23:59 visual_prompt]: Epoch 7 / 100: avg data time: 1.92e-01, avg batch time: 1.0141, average train loss: 43.3462
[11/22 00:24:57 visual_prompt]: Inference (val):avg data time: 2.25e-04, avg batch time: 0.3080, average loss: 30.7992
[11/22 00:24:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.91	
[11/22 00:24:57 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/22 00:26:40 visual_prompt]: 	Training 100/553. train loss: 92.5026,	0.8240 s / batch. (data: 3.51e-04). ETA=11:44:54, max mem: 20.9 GB 
[11/22 00:28:23 visual_prompt]: 	Training 200/553. train loss: 133.4921,	0.8361 s / batch. (data: 1.19e-02). ETA=11:53:50, max mem: 20.9 GB 
[11/22 00:30:04 visual_prompt]: 	Training 300/553. train loss: 234.0344,	0.8280 s / batch. (data: 8.37e-04). ETA=11:45:35, max mem: 20.9 GB 
[11/22 00:31:45 visual_prompt]: 	Training 400/553. train loss: 94.3837,	0.8381 s / batch. (data: 2.20e-02). ETA=11:52:45, max mem: 20.9 GB 
[11/22 00:33:26 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4360 s / batch. (data: 6.17e-01). ETA=20:18:53, max mem: 20.9 GB 
[11/22 00:34:19 visual_prompt]: Epoch 8 / 100: avg data time: 1.94e-01, avg batch time: 1.0164, average train loss: 73.7924
[11/22 00:35:17 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3077, average loss: 203.1180
[11/22 00:35:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.60	
[11/22 00:35:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/22 00:37:02 visual_prompt]: 	Training 100/553. train loss: 89.9425,	0.8281 s / batch. (data: 1.60e-02). ETA=11:40:46, max mem: 20.9 GB 
[11/22 00:38:42 visual_prompt]: 	Training 200/553. train loss: 24.2411,	0.8426 s / batch. (data: 1.05e-02). ETA=11:51:37, max mem: 20.9 GB 
[11/22 00:40:23 visual_prompt]: 	Training 300/553. train loss: 93.5788,	1.8978 s / batch. (data: 1.08e+00). ETA=1 day, 2:39:42, max mem: 20.9 GB 
[11/22 00:42:06 visual_prompt]: 	Training 400/553. train loss: 12.0334,	0.8463 s / batch. (data: 5.98e-03). ETA=11:51:58, max mem: 20.9 GB 
[11/22 00:43:47 visual_prompt]: 	Training 500/553. train loss: 3.0766,	1.0040 s / batch. (data: 1.64e-01). ETA=14:02:57, max mem: 20.9 GB 
[11/22 00:44:39 visual_prompt]: Epoch 9 / 100: avg data time: 1.92e-01, avg batch time: 1.0152, average train loss: 48.2814
[11/22 00:45:36 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.3080, average loss: 42.4104
[11/22 00:45:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/22 00:45:36 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/22 00:47:24 visual_prompt]: 	Training 100/553. train loss: 184.7925,	0.8410 s / batch. (data: 5.46e-03). ETA=11:43:57, max mem: 20.9 GB 
[11/22 00:49:03 visual_prompt]: 	Training 200/553. train loss: 3.7531,	0.8280 s / batch. (data: 3.37e-04). ETA=11:31:41, max mem: 20.9 GB 
[11/22 00:50:43 visual_prompt]: 	Training 300/553. train loss: 19.7834,	0.8280 s / batch. (data: 3.31e-04). ETA=11:30:18, max mem: 20.9 GB 
[11/22 00:52:22 visual_prompt]: 	Training 400/553. train loss: 71.5181,	0.8160 s / batch. (data: 3.25e-04). ETA=11:18:58, max mem: 20.9 GB 
[11/22 00:54:04 visual_prompt]: 	Training 500/553. train loss: 93.8140,	0.8010 s / batch. (data: 3.17e-04). ETA=11:05:08, max mem: 20.9 GB 
[11/22 00:54:57 visual_prompt]: Epoch 10 / 100: avg data time: 1.93e-01, avg batch time: 1.0142, average train loss: 82.9872
[11/22 00:55:55 visual_prompt]: Inference (val):avg data time: 3.78e-04, avg batch time: 0.3107, average loss: 48.1733
[11/22 00:55:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.14	
[11/22 00:55:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/22 00:57:42 visual_prompt]: 	Training 100/553. train loss: 151.3405,	0.8252 s / batch. (data: 2.94e-04). ETA=11:23:06, max mem: 20.9 GB 
[11/22 00:59:25 visual_prompt]: 	Training 200/553. train loss: 99.0040,	0.8480 s / batch. (data: 1.20e-02). ETA=11:40:36, max mem: 20.9 GB 
[11/22 01:01:05 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2284 s / batch. (data: 1.42e+00). ETA=1 day, 6:37:17, max mem: 20.9 GB 
[11/22 01:02:44 visual_prompt]: 	Training 400/553. train loss: 181.2127,	0.8176 s / batch. (data: 3.49e-04). ETA=11:12:42, max mem: 20.9 GB 
[11/22 01:04:24 visual_prompt]: 	Training 500/553. train loss: 98.6965,	0.8373 s / batch. (data: 1.05e-02). ETA=11:27:33, max mem: 20.9 GB 
[11/22 01:05:15 visual_prompt]: Epoch 11 / 100: avg data time: 1.91e-01, avg batch time: 1.0127, average train loss: 72.5064
[11/22 01:06:13 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3074, average loss: 107.5733
[11/22 01:06:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.46	
[11/22 01:06:13 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/22 01:07:59 visual_prompt]: 	Training 100/553. train loss: 54.0880,	0.9200 s / batch. (data: 8.54e-02). ETA=12:33:08, max mem: 20.9 GB 
[11/22 01:09:41 visual_prompt]: 	Training 200/553. train loss: 48.0289,	0.8440 s / batch. (data: 7.94e-03). ETA=11:29:30, max mem: 20.9 GB 
[11/22 01:11:20 visual_prompt]: 	Training 300/553. train loss: 37.7816,	0.8120 s / batch. (data: 3.03e-04). ETA=11:02:00, max mem: 20.9 GB 
[11/22 01:13:01 visual_prompt]: 	Training 400/553. train loss: 92.9019,	0.8146 s / batch. (data: 4.97e-04). ETA=11:02:46, max mem: 20.9 GB 
[11/22 01:14:42 visual_prompt]: 	Training 500/553. train loss: 41.5009,	0.8240 s / batch. (data: 7.95e-03). ETA=11:09:02, max mem: 20.9 GB 
[11/22 01:15:34 visual_prompt]: Epoch 12 / 100: avg data time: 1.91e-01, avg batch time: 1.0137, average train loss: 69.8166
[11/22 01:16:32 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.3070, average loss: 98.0493
[11/22 01:16:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.43	
[11/22 01:16:32 visual_prompt]: Best epoch 12: best metric: -98.049
[11/22 01:16:32 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/22 01:18:19 visual_prompt]: 	Training 100/553. train loss: 73.2958,	0.7978 s / batch. (data: 3.30e-04). ETA=10:45:45, max mem: 20.9 GB 
[11/22 01:19:56 visual_prompt]: 	Training 200/553. train loss: 69.2323,	0.8288 s / batch. (data: 3.08e-04). ETA=11:09:28, max mem: 20.9 GB 
[11/22 01:21:38 visual_prompt]: 	Training 300/553. train loss: 30.0433,	1.8416 s / batch. (data: 1.03e+00). ETA=1 day, 0:44:27, max mem: 20.9 GB 
[11/22 01:23:18 visual_prompt]: 	Training 400/553. train loss: 241.2708,	0.8240 s / batch. (data: 3.24e-04). ETA=11:02:49, max mem: 20.9 GB 
[11/22 01:25:00 visual_prompt]: 	Training 500/553. train loss: 34.8105,	0.8287 s / batch. (data: 3.25e-04). ETA=11:05:14, max mem: 20.9 GB 
[11/22 01:25:52 visual_prompt]: Epoch 13 / 100: avg data time: 1.92e-01, avg batch time: 1.0137, average train loss: 97.3977
[11/22 01:26:50 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3088, average loss: 54.5825
[11/22 01:26:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.08	
[11/22 01:26:50 visual_prompt]: Best epoch 13: best metric: -54.582
[11/22 01:26:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/22 01:28:36 visual_prompt]: 	Training 100/553. train loss: 58.1163,	0.8152 s / batch. (data: 3.16e-04). ETA=10:52:16, max mem: 20.9 GB 
[11/22 01:30:16 visual_prompt]: 	Training 200/553. train loss: 0.0389,	1.2929 s / batch. (data: 4.67e-01). ETA=17:12:23, max mem: 20.9 GB 
[11/22 01:31:57 visual_prompt]: 	Training 300/553. train loss: 34.0453,	0.8721 s / batch. (data: 2.41e-02). ETA=11:34:57, max mem: 20.9 GB 
[11/22 01:33:38 visual_prompt]: 	Training 400/553. train loss: 20.2472,	0.8319 s / batch. (data: 5.25e-04). ETA=11:01:32, max mem: 20.9 GB 
[11/22 01:35:19 visual_prompt]: 	Training 500/553. train loss: 164.4967,	0.8134 s / batch. (data: 3.06e-04). ETA=10:45:27, max mem: 20.9 GB 
[11/22 01:36:11 visual_prompt]: Epoch 14 / 100: avg data time: 1.91e-01, avg batch time: 1.0136, average train loss: 61.9254
[11/22 01:37:09 visual_prompt]: Inference (val):avg data time: 4.53e-05, avg batch time: 0.3077, average loss: 27.4170
[11/22 01:37:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.04	
[11/22 01:37:09 visual_prompt]: Best epoch 14: best metric: -27.417
[11/22 01:37:09 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/22 01:38:54 visual_prompt]: 	Training 100/553. train loss: 110.0935,	0.8136 s / batch. (data: 3.17e-04). ETA=10:43:31, max mem: 20.9 GB 
[11/22 01:40:33 visual_prompt]: 	Training 200/553. train loss: 372.1039,	0.8239 s / batch. (data: 3.30e-04). ETA=10:50:20, max mem: 20.9 GB 
[11/22 01:42:15 visual_prompt]: 	Training 300/553. train loss: 5.5879,	0.8431 s / batch. (data: 5.47e-03). ETA=11:04:03, max mem: 20.9 GB 
[11/22 01:43:53 visual_prompt]: 	Training 400/553. train loss: 0.3316,	0.8227 s / batch. (data: 3.37e-04). ETA=10:46:38, max mem: 20.9 GB 
[11/22 01:45:35 visual_prompt]: 	Training 500/553. train loss: 112.7657,	0.8480 s / batch. (data: 7.96e-03). ETA=11:05:06, max mem: 20.9 GB 
[11/22 01:46:28 visual_prompt]: Epoch 15 / 100: avg data time: 1.88e-01, avg batch time: 1.0114, average train loss: 87.3216
[11/22 01:47:26 visual_prompt]: Inference (val):avg data time: 4.60e-05, avg batch time: 0.3073, average loss: 210.5192
[11/22 01:47:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.00	
[11/22 01:47:26 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/22 01:49:10 visual_prompt]: 	Training 100/553. train loss: 16.6388,	0.8397 s / batch. (data: 1.02e-02). ETA=10:56:26, max mem: 20.9 GB 
[11/22 01:50:51 visual_prompt]: 	Training 200/553. train loss: 101.8530,	0.8256 s / batch. (data: 3.40e-04). ETA=10:44:01, max mem: 20.9 GB 
[11/22 01:52:31 visual_prompt]: 	Training 300/553. train loss: 149.5668,	0.8320 s / batch. (data: 3.09e-04). ETA=10:47:39, max mem: 20.9 GB 
[11/22 01:54:12 visual_prompt]: 	Training 400/553. train loss: 25.5012,	0.8520 s / batch. (data: 8.39e-04). ETA=11:01:46, max mem: 20.9 GB 
[11/22 01:55:52 visual_prompt]: 	Training 500/553. train loss: 156.1897,	1.4640 s / batch. (data: 6.26e-01). ETA=18:54:44, max mem: 20.9 GB 
[11/22 01:56:45 visual_prompt]: Epoch 16 / 100: avg data time: 1.88e-01, avg batch time: 1.0097, average train loss: 82.4612
[11/22 01:57:43 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.3078, average loss: 25.8207
[11/22 01:57:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.73	
[11/22 01:57:43 visual_prompt]: Best epoch 16: best metric: -25.821
[11/22 01:57:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/22 01:59:26 visual_prompt]: 	Training 100/553. train loss: 79.3474,	0.8337 s / batch. (data: 2.17e-02). ETA=10:44:03, max mem: 20.9 GB 
[11/22 02:01:09 visual_prompt]: 	Training 200/553. train loss: 313.7220,	0.8228 s / batch. (data: 1.05e-02). ETA=10:34:13, max mem: 20.9 GB 
[11/22 02:02:49 visual_prompt]: 	Training 300/553. train loss: 178.4385,	0.8281 s / batch. (data: 3.02e-04). ETA=10:36:56, max mem: 20.9 GB 
[11/22 02:04:30 visual_prompt]: 	Training 400/553. train loss: 55.0876,	1.2360 s / batch. (data: 4.10e-01). ETA=15:48:40, max mem: 20.9 GB 
[11/22 02:06:10 visual_prompt]: 	Training 500/553. train loss: 61.1381,	1.7645 s / batch. (data: 9.31e-01). ETA=22:31:21, max mem: 20.9 GB 
[11/22 02:07:03 visual_prompt]: Epoch 17 / 100: avg data time: 1.91e-01, avg batch time: 1.0139, average train loss: 96.4472
[11/22 02:08:02 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3095, average loss: 52.4195
[11/22 02:08:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.80	
[11/22 02:08:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/22 02:09:46 visual_prompt]: 	Training 100/553. train loss: 179.4604,	0.8240 s / batch. (data: 3.47e-04). ETA=10:28:58, max mem: 20.9 GB 
[11/22 02:11:30 visual_prompt]: 	Training 200/553. train loss: 6.3386,	0.8307 s / batch. (data: 9.05e-04). ETA=10:32:40, max mem: 20.9 GB 
[11/22 02:13:11 visual_prompt]: 	Training 300/553. train loss: 5.8222,	0.8320 s / batch. (data: 3.01e-04). ETA=10:32:18, max mem: 20.9 GB 
[11/22 02:14:51 visual_prompt]: 	Training 400/553. train loss: 27.2760,	0.8246 s / batch. (data: 3.20e-04). ETA=10:25:17, max mem: 20.9 GB 
[11/22 02:16:31 visual_prompt]: 	Training 500/553. train loss: 103.9776,	0.8559 s / batch. (data: 2.39e-02). ETA=10:47:38, max mem: 20.9 GB 
[11/22 02:17:22 visual_prompt]: Epoch 18 / 100: avg data time: 1.92e-01, avg batch time: 1.0137, average train loss: 108.9397
[11/22 02:18:20 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3088, average loss: 174.6840
[11/22 02:18:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.30	
[11/22 02:18:20 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/22 02:20:05 visual_prompt]: 	Training 100/553. train loss: 62.0611,	1.4490 s / batch. (data: 6.44e-01). ETA=18:12:39, max mem: 20.9 GB 
[11/22 02:21:46 visual_prompt]: 	Training 200/553. train loss: 12.6486,	0.8265 s / batch. (data: 2.81e-04). ETA=10:21:52, max mem: 20.9 GB 
[11/22 02:23:29 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8200 s / batch. (data: 5.52e-03). ETA=10:15:37, max mem: 20.9 GB 
[11/22 02:25:11 visual_prompt]: 	Training 400/553. train loss: 51.9433,	0.8074 s / batch. (data: 3.43e-04). ETA=10:04:47, max mem: 20.9 GB 
[11/22 02:26:47 visual_prompt]: 	Training 500/553. train loss: 25.7980,	0.8200 s / batch. (data: 3.33e-04). ETA=10:12:53, max mem: 20.9 GB 
[11/22 02:27:40 visual_prompt]: Epoch 19 / 100: avg data time: 1.89e-01, avg batch time: 1.0116, average train loss: 68.3429
[11/22 02:28:38 visual_prompt]: Inference (val):avg data time: 2.25e-04, avg batch time: 0.3080, average loss: 5.8235
[11/22 02:28:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/22 02:28:38 visual_prompt]: Best epoch 19: best metric: -5.824
[11/22 02:28:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/22 02:30:21 visual_prompt]: 	Training 100/553. train loss: 9.4218,	0.8221 s / batch. (data: 5.49e-03). ETA=10:12:22, max mem: 20.9 GB 
[11/22 02:32:03 visual_prompt]: 	Training 200/553. train loss: 10.9273,	0.8394 s / batch. (data: 1.54e-02). ETA=10:23:51, max mem: 20.9 GB 
[11/22 02:33:44 visual_prompt]: 	Training 300/553. train loss: 76.0858,	0.8289 s / batch. (data: 7.96e-03). ETA=10:14:38, max mem: 20.9 GB 
[11/22 02:35:25 visual_prompt]: 	Training 400/553. train loss: 187.2953,	0.8562 s / batch. (data: 2.44e-02). ETA=10:33:28, max mem: 20.9 GB 
[11/22 02:37:05 visual_prompt]: 	Training 500/553. train loss: 45.7235,	0.8507 s / batch. (data: 2.26e-02). ETA=10:27:58, max mem: 20.9 GB 
[11/22 02:37:59 visual_prompt]: Epoch 20 / 100: avg data time: 1.93e-01, avg batch time: 1.0138, average train loss: 91.9706
[11/22 02:38:56 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.3071, average loss: 98.4013
[11/22 02:38:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.25	
[11/22 02:38:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/22 02:40:43 visual_prompt]: 	Training 100/553. train loss: 251.7324,	0.8167 s / batch. (data: 3.30e-04). ETA=10:00:51, max mem: 20.9 GB 
[11/22 02:42:23 visual_prompt]: 	Training 200/553. train loss: 142.9680,	0.8360 s / batch. (data: 3.06e-04). ETA=10:13:37, max mem: 20.9 GB 
[11/22 02:44:05 visual_prompt]: 	Training 300/553. train loss: 511.1374,	1.1480 s / batch. (data: 3.22e-01). ETA=14:00:43, max mem: 20.9 GB 
[11/22 02:45:44 visual_prompt]: 	Training 400/553. train loss: 197.3434,	0.8248 s / batch. (data: 1.20e-02). ETA=10:02:38, max mem: 20.9 GB 
[11/22 02:47:26 visual_prompt]: 	Training 500/553. train loss: 40.3862,	0.8400 s / batch. (data: 3.24e-04). ETA=10:12:21, max mem: 20.9 GB 
[11/22 02:48:18 visual_prompt]: Epoch 21 / 100: avg data time: 1.94e-01, avg batch time: 1.0157, average train loss: 107.1739
[11/22 02:49:16 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3071, average loss: 11.7291
[11/22 02:49:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.31	
[11/22 02:49:16 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/22 02:51:00 visual_prompt]: 	Training 100/553. train loss: 129.5088,	0.8184 s / batch. (data: 3.16e-04). ETA=9:54:33, max mem: 20.9 GB 
[11/22 02:52:41 visual_prompt]: 	Training 200/553. train loss: 8.2347,	0.8103 s / batch. (data: 3.03e-04). ETA=9:47:16, max mem: 20.9 GB 
[11/22 02:54:20 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8244 s / batch. (data: 1.11e-02). ETA=9:56:08, max mem: 20.9 GB 
[11/22 02:56:01 visual_prompt]: 	Training 400/553. train loss: 119.1904,	0.8185 s / batch. (data: 1.06e-02). ETA=9:50:28, max mem: 20.9 GB 
[11/22 02:57:43 visual_prompt]: 	Training 500/553. train loss: 41.2609,	0.8061 s / batch. (data: 3.03e-04). ETA=9:40:15, max mem: 20.9 GB 
[11/22 02:58:37 visual_prompt]: Epoch 22 / 100: avg data time: 1.91e-01, avg batch time: 1.0139, average train loss: 77.9296
[11/22 02:59:35 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3086, average loss: 18.1174
[11/22 02:59:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.81	
[11/22 02:59:35 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/22 03:01:21 visual_prompt]: 	Training 100/553. train loss: 18.7215,	0.8400 s / batch. (data: 7.95e-03). ETA=10:02:28, max mem: 20.9 GB 
[11/22 03:03:02 visual_prompt]: 	Training 200/553. train loss: 2.1599,	0.8125 s / batch. (data: 3.49e-04). ETA=9:41:23, max mem: 20.9 GB 
[11/22 03:04:44 visual_prompt]: 	Training 300/553. train loss: 92.5511,	0.8090 s / batch. (data: 3.13e-04). ETA=9:37:33, max mem: 20.9 GB 
[11/22 03:06:23 visual_prompt]: 	Training 400/553. train loss: 17.3159,	0.8102 s / batch. (data: 3.32e-04). ETA=9:37:03, max mem: 20.9 GB 
[11/22 03:08:02 visual_prompt]: 	Training 500/553. train loss: 85.7274,	0.8277 s / batch. (data: 2.99e-04). ETA=9:48:09, max mem: 20.9 GB 
[11/22 03:08:55 visual_prompt]: Epoch 23 / 100: avg data time: 1.89e-01, avg batch time: 1.0119, average train loss: 68.7983
[11/22 03:09:52 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3083, average loss: 93.9192
[11/22 03:09:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.86	
[11/22 03:09:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/22 03:11:35 visual_prompt]: 	Training 100/553. train loss: 121.3363,	1.0400 s / batch. (data: 2.19e-01). ETA=12:16:20, max mem: 20.9 GB 
[11/22 03:13:15 visual_prompt]: 	Training 200/553. train loss: 30.4765,	0.8208 s / batch. (data: 1.06e-02). ETA=9:39:45, max mem: 20.9 GB 
[11/22 03:14:56 visual_prompt]: 	Training 300/553. train loss: 30.0802,	1.1480 s / batch. (data: 3.11e-01). ETA=13:28:58, max mem: 20.9 GB 
[11/22 03:16:37 visual_prompt]: 	Training 400/553. train loss: 24.1946,	0.8324 s / batch. (data: 3.02e-04). ETA=9:45:09, max mem: 20.9 GB 
[11/22 03:18:20 visual_prompt]: 	Training 500/553. train loss: 155.4979,	0.8140 s / batch. (data: 3.52e-04). ETA=9:30:55, max mem: 20.9 GB 
[11/22 03:19:13 visual_prompt]: Epoch 24 / 100: avg data time: 1.92e-01, avg batch time: 1.0133, average train loss: 77.4350
[11/22 03:20:11 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3073, average loss: 59.1853
[11/22 03:20:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.16	
[11/22 03:20:11 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/22 03:21:59 visual_prompt]: 	Training 100/553. train loss: 197.0322,	0.8280 s / batch. (data: 3.23e-04). ETA=9:38:36, max mem: 20.9 GB 
[11/22 03:23:36 visual_prompt]: 	Training 200/553. train loss: 35.0548,	0.8200 s / batch. (data: 2.06e-02). ETA=9:31:37, max mem: 20.9 GB 
[11/22 03:25:17 visual_prompt]: 	Training 300/553. train loss: 140.3271,	0.8234 s / batch. (data: 1.06e-02). ETA=9:32:40, max mem: 20.9 GB 
[11/22 03:26:58 visual_prompt]: 	Training 400/553. train loss: 81.6939,	1.0487 s / batch. (data: 2.13e-01). ETA=12:07:34, max mem: 20.9 GB 
[11/22 03:28:39 visual_prompt]: 	Training 500/553. train loss: 61.1020,	1.6960 s / batch. (data: 8.77e-01). ETA=19:33:50, max mem: 20.9 GB 
[11/22 03:29:32 visual_prompt]: Epoch 25 / 100: avg data time: 1.93e-01, avg batch time: 1.0141, average train loss: 73.0670
[11/22 03:30:30 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3064, average loss: 142.0155
[11/22 03:30:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/22 03:30:30 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/22 03:32:14 visual_prompt]: 	Training 100/553. train loss: 29.4657,	0.8080 s / batch. (data: 3.39e-04). ETA=9:17:10, max mem: 20.9 GB 
[11/22 03:33:56 visual_prompt]: 	Training 200/553. train loss: 385.2049,	1.7246 s / batch. (data: 8.83e-01). ETA=19:46:21, max mem: 20.9 GB 
[11/22 03:35:39 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8353 s / batch. (data: 3.18e-04). ETA=9:33:15, max mem: 20.9 GB 
[11/22 03:37:18 visual_prompt]: 	Training 400/553. train loss: 54.7317,	0.8101 s / batch. (data: 3.04e-04). ETA=9:14:33, max mem: 20.9 GB 
[11/22 03:38:58 visual_prompt]: 	Training 500/553. train loss: 57.9050,	0.8511 s / batch. (data: 1.11e-02). ETA=9:41:13, max mem: 20.9 GB 
[11/22 03:39:50 visual_prompt]: Epoch 26 / 100: avg data time: 1.91e-01, avg batch time: 1.0127, average train loss: 73.4555
[11/22 03:40:48 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3069, average loss: 80.7753
[11/22 03:40:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.97	
[11/22 03:40:48 visual_prompt]: Stopping early.
[11/22 03:40:48 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 03:40:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 03:40:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/22 03:40:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/22 03:40:48 visual_prompt]: Training with config:
[11/22 03:40:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/22 03:40:48 visual_prompt]: Loading training data...
[11/22 03:40:48 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 03:40:48 visual_prompt]: Loading validation data...
[11/22 03:40:48 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 03:40:48 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 03:40:50 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/22 03:40:50 visual_prompt]: tuned percent:0.525
[11/22 03:40:50 visual_prompt]: Device used for model: 0
[11/22 03:40:50 visual_prompt]: Setting up Evaluator...
[11/22 03:40:50 visual_prompt]: Setting up Trainer...
[11/22 03:40:50 visual_prompt]: 	Setting up the optimizer...
[11/22 03:40:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 03:42:35 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8804 s / batch. (data: 2.43e-02). ETA=13:29:55, max mem: 20.9 GB 
[11/22 03:44:14 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8346 s / batch. (data: 3.11e-04). ETA=12:46:26, max mem: 20.9 GB 
[11/22 03:45:57 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3895 s / batch. (data: 5.81e-01). ETA=21:13:42, max mem: 20.9 GB 
[11/22 03:47:37 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8106 s / batch. (data: 3.13e-04). ETA=12:21:40, max mem: 20.9 GB 
[11/22 03:49:20 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8281 s / batch. (data: 3.21e-04). ETA=12:36:17, max mem: 20.9 GB 
[11/22 03:50:13 visual_prompt]: Epoch 1 / 100: avg data time: 1.90e-01, avg batch time: 1.0161, average train loss: 1.5403
[11/22 03:51:11 visual_prompt]: Inference (val):avg data time: 4.60e-05, avg batch time: 0.3073, average loss: 1.5201
[11/22 03:51:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/22 03:51:11 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/22 03:52:55 visual_prompt]: 	Training 100/553. train loss: 7.2424,	0.8713 s / batch. (data: 3.61e-02). ETA=13:13:36, max mem: 20.9 GB 
[11/22 03:54:35 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8278 s / batch. (data: 1.20e-02). ETA=12:32:34, max mem: 20.9 GB 
[11/22 03:56:18 visual_prompt]: 	Training 300/553. train loss: 4.3327,	1.2142 s / batch. (data: 3.65e-01). ETA=18:21:48, max mem: 20.9 GB 
[11/22 03:57:57 visual_prompt]: 	Training 400/553. train loss: 1.6245,	0.8209 s / batch. (data: 3.23e-04). ETA=12:23:33, max mem: 20.9 GB 
[11/22 03:59:40 visual_prompt]: 	Training 500/553. train loss: 1.6753,	0.8208 s / batch. (data: 3.04e-04). ETA=12:22:04, max mem: 20.9 GB 
[11/22 04:00:32 visual_prompt]: Epoch 2 / 100: avg data time: 1.89e-01, avg batch time: 1.0141, average train loss: 12.4687
[11/22 04:01:30 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.3091, average loss: 17.9150
[11/22 04:01:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.10	
[11/22 04:01:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/22 04:03:13 visual_prompt]: 	Training 100/553. train loss: 28.1278,	0.8122 s / batch. (data: 2.98e-04). ETA=12:12:12, max mem: 20.9 GB 
[11/22 04:04:55 visual_prompt]: 	Training 200/553. train loss: 10.4513,	0.8319 s / batch. (data: 3.55e-04). ETA=12:28:40, max mem: 20.9 GB 
[11/22 04:06:35 visual_prompt]: 	Training 300/553. train loss: 2.1308,	0.8680 s / batch. (data: 1.07e-02). ETA=12:59:39, max mem: 20.9 GB 
[11/22 04:08:17 visual_prompt]: 	Training 400/553. train loss: 64.6666,	0.8237 s / batch. (data: 1.20e-02). ETA=12:18:29, max mem: 20.9 GB 
[11/22 04:09:59 visual_prompt]: 	Training 500/553. train loss: 6.3802,	1.3339 s / batch. (data: 4.96e-01). ETA=19:53:44, max mem: 20.9 GB 
[11/22 04:10:51 visual_prompt]: Epoch 3 / 100: avg data time: 1.88e-01, avg batch time: 1.0143, average train loss: 13.6450
[11/22 04:11:49 visual_prompt]: Inference (val):avg data time: 2.74e-04, avg batch time: 0.3092, average loss: 12.2957
[11/22 04:11:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.88	
[11/22 04:11:49 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/22 04:13:36 visual_prompt]: 	Training 100/553. train loss: 14.1667,	0.8464 s / batch. (data: 3.29e-04). ETA=12:35:14, max mem: 20.9 GB 
[11/22 04:15:16 visual_prompt]: 	Training 200/553. train loss: 13.1459,	0.8306 s / batch. (data: 1.20e-02). ETA=12:19:47, max mem: 20.9 GB 
[11/22 04:16:58 visual_prompt]: 	Training 300/553. train loss: 5.1815,	1.5360 s / batch. (data: 6.96e-01). ETA=22:45:33, max mem: 20.9 GB 
[11/22 04:18:34 visual_prompt]: 	Training 400/553. train loss: 0.4137,	1.4451 s / batch. (data: 6.36e-01). ETA=21:22:17, max mem: 20.9 GB 
[11/22 04:20:17 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.4916 s / batch. (data: 2.68e+00). ETA=2 days, 3:32:25, max mem: 20.9 GB 
[11/22 04:21:11 visual_prompt]: Epoch 4 / 100: avg data time: 1.92e-01, avg batch time: 1.0166, average train loss: 18.2450
[11/22 04:22:09 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.3081, average loss: 5.3036
[11/22 04:22:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/22 04:22:09 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/22 04:23:52 visual_prompt]: 	Training 100/553. train loss: 83.4474,	0.8378 s / batch. (data: 5.46e-03). ETA=12:19:52, max mem: 20.9 GB 
[11/22 04:25:34 visual_prompt]: 	Training 200/553. train loss: 0.9112,	1.0840 s / batch. (data: 2.46e-01). ETA=15:55:29, max mem: 20.9 GB 
[11/22 04:27:15 visual_prompt]: 	Training 300/553. train loss: 73.5406,	0.8567 s / batch. (data: 1.06e-02). ETA=12:33:43, max mem: 20.9 GB 
[11/22 04:28:55 visual_prompt]: 	Training 400/553. train loss: 2.6459,	0.8280 s / batch. (data: 3.08e-04). ETA=12:07:06, max mem: 20.9 GB 
[11/22 04:30:36 visual_prompt]: 	Training 500/553. train loss: 22.0819,	0.8282 s / batch. (data: 2.45e-02). ETA=12:05:52, max mem: 20.9 GB 
[11/22 04:31:29 visual_prompt]: Epoch 5 / 100: avg data time: 1.89e-01, avg batch time: 1.0136, average train loss: 25.7894
[11/22 04:32:27 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3054, average loss: 3.6946
[11/22 04:32:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.25	
[11/22 04:32:27 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/22 04:34:14 visual_prompt]: 	Training 100/553. train loss: 24.2818,	0.8695 s / batch. (data: 8.23e-04). ETA=12:39:52, max mem: 20.9 GB 
[11/22 04:35:54 visual_prompt]: 	Training 200/553. train loss: 99.6738,	0.8323 s / batch. (data: 3.06e-04). ETA=12:05:58, max mem: 20.9 GB 
[11/22 04:37:34 visual_prompt]: 	Training 300/553. train loss: 8.5172,	0.8219 s / batch. (data: 4.43e-04). ETA=11:55:31, max mem: 20.9 GB 
[11/22 04:39:18 visual_prompt]: 	Training 400/553. train loss: 18.0969,	0.8409 s / batch. (data: 1.06e-02). ETA=12:10:41, max mem: 20.9 GB 
[11/22 04:40:57 visual_prompt]: 	Training 500/553. train loss: 11.3561,	0.8321 s / batch. (data: 3.29e-04). ETA=12:01:36, max mem: 20.9 GB 
[11/22 04:41:49 visual_prompt]: Epoch 6 / 100: avg data time: 1.91e-01, avg batch time: 1.0159, average train loss: 23.5509
[11/22 04:42:47 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.3086, average loss: 6.1940
[11/22 04:42:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.42	
[11/22 04:42:47 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/22 04:44:30 visual_prompt]: 	Training 100/553. train loss: 71.1854,	0.8323 s / batch. (data: 3.18e-04). ETA=11:59:41, max mem: 20.9 GB 
[11/22 04:46:11 visual_prompt]: 	Training 200/553. train loss: 5.1444,	0.8317 s / batch. (data: 3.15e-04). ETA=11:57:44, max mem: 20.9 GB 
[11/22 04:47:56 visual_prompt]: 	Training 300/553. train loss: 18.4046,	2.0197 s / batch. (data: 1.21e+00). ETA=1 day, 4:59:41, max mem: 20.9 GB 
[11/22 04:49:37 visual_prompt]: 	Training 400/553. train loss: 13.8397,	1.8971 s / batch. (data: 1.07e+00). ETA=1 day, 3:10:58, max mem: 20.9 GB 
[11/22 04:51:16 visual_prompt]: 	Training 500/553. train loss: 68.1425,	0.8120 s / batch. (data: 3.09e-04). ETA=11:36:43, max mem: 20.9 GB 
[11/22 04:52:07 visual_prompt]: Epoch 7 / 100: avg data time: 1.89e-01, avg batch time: 1.0132, average train loss: 24.9745
[11/22 04:53:05 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3090, average loss: 6.3741
[11/22 04:53:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.37	
[11/22 04:53:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/22 04:54:48 visual_prompt]: 	Training 100/553. train loss: 69.9379,	0.8356 s / batch. (data: 8.55e-03). ETA=11:54:52, max mem: 20.9 GB 
[11/22 04:56:31 visual_prompt]: 	Training 200/553. train loss: 120.0211,	0.8066 s / batch. (data: 3.52e-04). ETA=11:28:43, max mem: 20.9 GB 
[11/22 04:58:12 visual_prompt]: 	Training 300/553. train loss: 14.7993,	0.8289 s / batch. (data: 2.11e-02). ETA=11:46:21, max mem: 20.9 GB 
[11/22 04:59:53 visual_prompt]: 	Training 400/553. train loss: 60.3261,	0.8150 s / batch. (data: 7.96e-03). ETA=11:33:07, max mem: 20.9 GB 
[11/22 05:01:34 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.3677 s / batch. (data: 5.50e-01). ETA=19:20:54, max mem: 20.9 GB 
[11/22 05:02:28 visual_prompt]: Epoch 8 / 100: avg data time: 1.93e-01, avg batch time: 1.0166, average train loss: 35.4590
[11/22 05:03:26 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.3088, average loss: 29.6084
[11/22 05:03:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.89	
[11/22 05:03:26 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/22 05:05:11 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 3.12e-04). ETA=11:30:33, max mem: 20.9 GB 
[11/22 05:06:50 visual_prompt]: 	Training 200/553. train loss: 17.5484,	0.8240 s / batch. (data: 3.22e-04). ETA=11:35:56, max mem: 20.9 GB 
[11/22 05:08:31 visual_prompt]: 	Training 300/553. train loss: 11.5687,	1.7200 s / batch. (data: 9.02e-01). ETA=1 day, 0:09:51, max mem: 20.9 GB 
[11/22 05:10:13 visual_prompt]: 	Training 400/553. train loss: 20.6134,	0.8440 s / batch. (data: 3.27e-04). ETA=11:50:02, max mem: 20.9 GB 
[11/22 05:11:56 visual_prompt]: 	Training 500/553. train loss: 41.5786,	1.2663 s / batch. (data: 4.48e-01). ETA=17:43:12, max mem: 20.9 GB 
[11/22 05:12:47 visual_prompt]: Epoch 9 / 100: avg data time: 1.90e-01, avg batch time: 1.0142, average train loss: 28.5791
[11/22 05:13:44 visual_prompt]: Inference (val):avg data time: 3.16e-04, avg batch time: 0.3075, average loss: 23.8281
[11/22 05:13:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.98	
[11/22 05:13:44 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/22 05:15:32 visual_prompt]: 	Training 100/553. train loss: 82.2706,	0.8088 s / batch. (data: 3.25e-04). ETA=11:16:58, max mem: 20.9 GB 
[11/22 05:17:11 visual_prompt]: 	Training 200/553. train loss: 1.1150,	0.8529 s / batch. (data: 1.29e-02). ETA=11:52:32, max mem: 20.9 GB 
[11/22 05:18:51 visual_prompt]: 	Training 300/553. train loss: 30.2138,	1.7080 s / batch. (data: 8.77e-01). ETA=23:43:59, max mem: 20.9 GB 
[11/22 05:20:31 visual_prompt]: 	Training 400/553. train loss: 40.2294,	0.8217 s / batch. (data: 5.48e-03). ETA=11:23:39, max mem: 20.9 GB 
[11/22 05:22:13 visual_prompt]: 	Training 500/553. train loss: 7.8690,	1.0640 s / batch. (data: 2.46e-01). ETA=14:43:29, max mem: 20.9 GB 
[11/22 05:23:06 visual_prompt]: Epoch 10 / 100: avg data time: 1.90e-01, avg batch time: 1.0145, average train loss: 33.4559
[11/22 05:24:04 visual_prompt]: Inference (val):avg data time: 5.08e-05, avg batch time: 0.3063, average loss: 1.7496
[11/22 05:24:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.90	
[11/22 05:24:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/22 05:25:51 visual_prompt]: 	Training 100/553. train loss: 52.0340,	0.8253 s / batch. (data: 5.46e-03). ETA=11:23:14, max mem: 20.9 GB 
[11/22 05:27:33 visual_prompt]: 	Training 200/553. train loss: 52.9471,	0.8236 s / batch. (data: 7.80e-04). ETA=11:20:25, max mem: 20.9 GB 
[11/22 05:29:14 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.3680 s / batch. (data: 1.54e+00). ETA=1 day, 8:32:25, max mem: 20.9 GB 
[11/22 05:30:53 visual_prompt]: 	Training 400/553. train loss: 18.9793,	0.8385 s / batch. (data: 7.95e-03). ETA=11:29:54, max mem: 20.9 GB 
[11/22 05:32:33 visual_prompt]: 	Training 500/553. train loss: 18.9895,	0.8160 s / batch. (data: 3.05e-04). ETA=11:10:04, max mem: 20.9 GB 
[11/22 05:33:24 visual_prompt]: Epoch 11 / 100: avg data time: 1.90e-01, avg batch time: 1.0136, average train loss: 33.8645
[11/22 05:34:22 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3071, average loss: 56.5795
[11/22 05:34:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.93	
[11/22 05:34:22 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/22 05:36:09 visual_prompt]: 	Training 100/553. train loss: 4.4777,	0.8400 s / batch. (data: 3.28e-04). ETA=11:27:38, max mem: 20.9 GB 
[11/22 05:37:51 visual_prompt]: 	Training 200/553. train loss: 12.2439,	2.0507 s / batch. (data: 1.23e+00). ETA=1 day, 3:55:20, max mem: 20.9 GB 
[11/22 05:39:30 visual_prompt]: 	Training 300/553. train loss: 6.2091,	0.8266 s / batch. (data: 3.34e-04). ETA=11:13:54, max mem: 20.9 GB 
[11/22 05:41:12 visual_prompt]: 	Training 400/553. train loss: 36.6586,	0.8173 s / batch. (data: 3.24e-04). ETA=11:04:58, max mem: 20.9 GB 
[11/22 05:42:53 visual_prompt]: 	Training 500/553. train loss: 194.9420,	0.8391 s / batch. (data: 5.99e-03). ETA=11:21:17, max mem: 20.9 GB 
[11/22 05:43:44 visual_prompt]: Epoch 12 / 100: avg data time: 1.94e-01, avg batch time: 1.0163, average train loss: 38.2417
[11/22 05:44:42 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3066, average loss: 78.2413
[11/22 05:44:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.06	
[11/22 05:44:43 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/22 05:46:29 visual_prompt]: 	Training 100/553. train loss: 25.1437,	0.8094 s / batch. (data: 9.88e-03). ETA=10:55:06, max mem: 20.9 GB 
[11/22 05:48:06 visual_prompt]: 	Training 200/553. train loss: 12.9302,	0.8282 s / batch. (data: 3.10e-04). ETA=11:08:58, max mem: 20.9 GB 
[11/22 05:49:48 visual_prompt]: 	Training 300/553. train loss: 32.4434,	1.8336 s / batch. (data: 1.03e+00). ETA=1 day, 0:38:02, max mem: 20.9 GB 
[11/22 05:51:28 visual_prompt]: 	Training 400/553. train loss: 112.1674,	0.8293 s / batch. (data: 5.75e-03). ETA=11:07:06, max mem: 20.9 GB 
[11/22 05:53:09 visual_prompt]: 	Training 500/553. train loss: 73.3396,	0.8360 s / batch. (data: 3.11e-04). ETA=11:11:04, max mem: 20.9 GB 
[11/22 05:54:02 visual_prompt]: Epoch 13 / 100: avg data time: 1.89e-01, avg batch time: 1.0115, average train loss: 42.1476
[11/22 05:55:00 visual_prompt]: Inference (val):avg data time: 2.24e-04, avg batch time: 0.3075, average loss: 14.5400
[11/22 05:55:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.97	
[11/22 05:55:00 visual_prompt]: Best epoch 13: best metric: -14.540
[11/22 05:55:00 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/22 05:56:46 visual_prompt]: 	Training 100/553. train loss: 32.7932,	0.8338 s / batch. (data: 1.01e-02). ETA=11:07:10, max mem: 20.9 GB 
[11/22 05:58:27 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.4318 s / batch. (data: 6.01e-01). ETA=19:03:18, max mem: 20.9 GB 
[11/22 06:00:08 visual_prompt]: 	Training 300/553. train loss: 16.0893,	0.8412 s / batch. (data: 5.48e-03). ETA=11:10:16, max mem: 20.9 GB 
[11/22 06:01:48 visual_prompt]: 	Training 400/553. train loss: 9.4391,	0.8447 s / batch. (data: 1.28e-02). ETA=11:11:40, max mem: 20.9 GB 
[11/22 06:03:29 visual_prompt]: 	Training 500/553. train loss: 11.7443,	0.8320 s / batch. (data: 3.13e-04). ETA=11:00:13, max mem: 20.9 GB 
[11/22 06:04:21 visual_prompt]: Epoch 14 / 100: avg data time: 1.89e-01, avg batch time: 1.0145, average train loss: 26.6456
[11/22 06:05:19 visual_prompt]: Inference (val):avg data time: 2.68e-04, avg batch time: 0.3098, average loss: 7.4371
[11/22 06:05:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.78	
[11/22 06:05:19 visual_prompt]: Best epoch 14: best metric: -7.437
[11/22 06:05:19 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/22 06:07:04 visual_prompt]: 	Training 100/553. train loss: 49.4517,	1.2408 s / batch. (data: 4.15e-01). ETA=16:21:23, max mem: 20.9 GB 
[11/22 06:08:44 visual_prompt]: 	Training 200/553. train loss: 73.2785,	0.8338 s / batch. (data: 1.11e-02). ETA=10:58:07, max mem: 20.9 GB 
[11/22 06:10:27 visual_prompt]: 	Training 300/553. train loss: 45.8299,	0.8103 s / batch. (data: 2.98e-04). ETA=10:38:14, max mem: 20.9 GB 
[11/22 06:12:05 visual_prompt]: 	Training 400/553. train loss: 3.9994,	1.1413 s / batch. (data: 3.27e-01). ETA=14:56:59, max mem: 20.9 GB 
[11/22 06:13:46 visual_prompt]: 	Training 500/553. train loss: 12.0135,	0.8453 s / batch. (data: 1.18e-02). ETA=11:02:57, max mem: 20.9 GB 
[11/22 06:14:40 visual_prompt]: Epoch 15 / 100: avg data time: 1.91e-01, avg batch time: 1.0135, average train loss: 42.6542
[11/22 06:15:38 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3067, average loss: 119.1599
[11/22 06:15:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.93	
[11/22 06:15:38 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/22 06:17:22 visual_prompt]: 	Training 100/553. train loss: 28.5121,	0.8323 s / batch. (data: 1.05e-02). ETA=10:50:41, max mem: 20.9 GB 
[11/22 06:19:03 visual_prompt]: 	Training 200/553. train loss: 4.4193,	0.8320 s / batch. (data: 1.56e-02). ETA=10:49:00, max mem: 20.9 GB 
[11/22 06:20:44 visual_prompt]: 	Training 300/553. train loss: 18.1549,	0.8401 s / batch. (data: 3.28e-04). ETA=10:53:58, max mem: 20.9 GB 
[11/22 06:22:25 visual_prompt]: 	Training 400/553. train loss: 8.0040,	0.8109 s / batch. (data: 3.08e-04). ETA=10:29:52, max mem: 20.9 GB 
[11/22 06:24:05 visual_prompt]: 	Training 500/553. train loss: 2.9585,	1.1691 s / batch. (data: 3.37e-01). ETA=15:06:07, max mem: 20.9 GB 
[11/22 06:24:58 visual_prompt]: Epoch 16 / 100: avg data time: 1.90e-01, avg batch time: 1.0130, average train loss: 32.4045
[11/22 06:25:56 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.3083, average loss: 11.1458
[11/22 06:25:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.12	
[11/22 06:25:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/22 06:27:40 visual_prompt]: 	Training 100/553. train loss: 0.0228,	0.8237 s / batch. (data: 3.46e-04). ETA=10:36:21, max mem: 20.9 GB 
[11/22 06:29:23 visual_prompt]: 	Training 200/553. train loss: 133.8049,	0.8056 s / batch. (data: 4.61e-04). ETA=10:21:02, max mem: 20.9 GB 
[11/22 06:31:04 visual_prompt]: 	Training 300/553. train loss: 80.5348,	0.8137 s / batch. (data: 7.96e-03). ETA=10:25:52, max mem: 20.9 GB 
[11/22 06:32:43 visual_prompt]: 	Training 400/553. train loss: 0.2826,	1.2175 s / batch. (data: 3.87e-01). ETA=15:34:30, max mem: 20.9 GB 
[11/22 06:34:23 visual_prompt]: 	Training 500/553. train loss: 58.0921,	0.8000 s / batch. (data: 3.35e-04). ETA=10:12:39, max mem: 20.9 GB 
[11/22 06:35:18 visual_prompt]: Epoch 17 / 100: avg data time: 1.91e-01, avg batch time: 1.0151, average train loss: 34.7385
[11/22 06:36:16 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3073, average loss: 13.7672
[11/22 06:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 65.09	
[11/22 06:36:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/22 06:38:01 visual_prompt]: 	Training 100/553. train loss: 1.1119,	0.8180 s / batch. (data: 2.96e-04). ETA=10:24:21, max mem: 20.9 GB 
[11/22 06:39:44 visual_prompt]: 	Training 200/553. train loss: 7.7370,	0.8383 s / batch. (data: 8.59e-04). ETA=10:38:29, max mem: 20.9 GB 
[11/22 06:41:24 visual_prompt]: 	Training 300/553. train loss: 15.1899,	0.8322 s / batch. (data: 3.70e-04). ETA=10:32:25, max mem: 20.9 GB 
[11/22 06:43:05 visual_prompt]: 	Training 400/553. train loss: 19.8836,	0.8360 s / batch. (data: 3.27e-04). ETA=10:33:58, max mem: 20.9 GB 
[11/22 06:44:45 visual_prompt]: 	Training 500/553. train loss: 7.3998,	0.8399 s / batch. (data: 3.26e-04). ETA=10:35:32, max mem: 20.9 GB 
[11/22 06:45:36 visual_prompt]: Epoch 18 / 100: avg data time: 1.91e-01, avg batch time: 1.0139, average train loss: 38.3788
[11/22 06:46:34 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3075, average loss: 55.3526
[11/22 06:46:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.85	
[11/22 06:46:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/22 06:48:19 visual_prompt]: 	Training 100/553. train loss: 26.4222,	0.8768 s / batch. (data: 1.56e-02). ETA=11:01:11, max mem: 20.9 GB 
[11/22 06:50:01 visual_prompt]: 	Training 200/553. train loss: 3.7860,	0.8423 s / batch. (data: 3.19e-04). ETA=10:33:47, max mem: 20.9 GB 
[11/22 06:51:42 visual_prompt]: 	Training 300/553. train loss: 22.6057,	0.8361 s / batch. (data: 1.89e-02). ETA=10:27:41, max mem: 20.9 GB 
[11/22 06:53:24 visual_prompt]: 	Training 400/553. train loss: 12.2948,	0.8463 s / batch. (data: 1.56e-02). ETA=10:33:58, max mem: 20.9 GB 
[11/22 06:55:01 visual_prompt]: 	Training 500/553. train loss: 7.6475,	0.8400 s / batch. (data: 7.94e-03). ETA=10:27:50, max mem: 20.9 GB 
[11/22 06:55:54 visual_prompt]: Epoch 19 / 100: avg data time: 1.87e-01, avg batch time: 1.0115, average train loss: 27.2900
[11/22 06:56:52 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3078, average loss: 93.1748
[11/22 06:56:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.80	
[11/22 06:56:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/22 06:58:34 visual_prompt]: 	Training 100/553. train loss: 39.9212,	0.8160 s / batch. (data: 5.47e-03). ETA=10:07:47, max mem: 20.9 GB 
[11/22 07:00:17 visual_prompt]: 	Training 200/553. train loss: 2.5359,	0.8440 s / batch. (data: 5.49e-03). ETA=10:27:17, max mem: 20.9 GB 
[11/22 07:01:58 visual_prompt]: 	Training 300/553. train loss: 85.2379,	0.8364 s / batch. (data: 5.48e-03). ETA=10:20:11, max mem: 20.9 GB 
[11/22 07:03:39 visual_prompt]: 	Training 400/553. train loss: 90.9195,	0.8288 s / batch. (data: 3.55e-04). ETA=10:13:12, max mem: 20.9 GB 
[11/22 07:05:19 visual_prompt]: 	Training 500/553. train loss: 29.1014,	0.8280 s / batch. (data: 3.21e-04). ETA=10:11:13, max mem: 20.9 GB 
[11/22 07:06:13 visual_prompt]: Epoch 20 / 100: avg data time: 1.91e-01, avg batch time: 1.0147, average train loss: 36.6560
[11/22 07:07:11 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3075, average loss: 14.7902
[11/22 07:07:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.40	
[11/22 07:07:11 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/22 07:08:58 visual_prompt]: 	Training 100/553. train loss: 19.7591,	0.8566 s / batch. (data: 3.07e-04). ETA=10:30:11, max mem: 20.9 GB 
[11/22 07:10:38 visual_prompt]: 	Training 200/553. train loss: 0.0096,	0.8128 s / batch. (data: 3.18e-04). ETA=9:56:33, max mem: 20.9 GB 
[11/22 07:12:18 visual_prompt]: 	Training 300/553. train loss: 136.8840,	0.8240 s / batch. (data: 2.98e-04). ETA=10:03:26, max mem: 20.9 GB 
[11/22 07:13:58 visual_prompt]: 	Training 400/553. train loss: 25.3831,	0.8200 s / batch. (data: 3.06e-04). ETA=9:59:09, max mem: 20.9 GB 
[11/22 07:15:40 visual_prompt]: 	Training 500/553. train loss: 31.8285,	0.8377 s / batch. (data: 1.43e-02). ETA=10:10:38, max mem: 20.9 GB 
[11/22 07:16:31 visual_prompt]: Epoch 21 / 100: avg data time: 1.91e-01, avg batch time: 1.0140, average train loss: 28.5649
[11/22 07:17:29 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3087, average loss: 14.4644
[11/22 07:17:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 67.94	
[11/22 07:17:29 visual_prompt]: Stopping early.
[11/22 07:17:29 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 07:17:29 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 07:17:29 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/22 07:17:29 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/22 07:17:29 visual_prompt]: Training with config:
[11/22 07:17:29 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/22 07:17:29 visual_prompt]: Loading training data...
[11/22 07:17:29 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 07:17:29 visual_prompt]: Loading validation data...
[11/22 07:17:29 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 07:17:29 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 07:17:32 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/22 07:17:32 visual_prompt]: tuned percent:0.525
[11/22 07:17:32 visual_prompt]: Device used for model: 0
[11/22 07:17:32 visual_prompt]: Setting up Evaluator...
[11/22 07:17:32 visual_prompt]: Setting up Trainer...
[11/22 07:17:32 visual_prompt]: 	Setting up the optimizer...
[11/22 07:17:32 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 07:19:17 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8311 s / batch. (data: 3.08e-04). ETA=12:44:35, max mem: 20.9 GB 
[11/22 07:20:56 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8513 s / batch. (data: 1.13e-02). ETA=13:01:46, max mem: 20.9 GB 
[11/22 07:22:40 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.1169 s / batch. (data: 2.92e-01). ETA=17:03:47, max mem: 20.9 GB 
[11/22 07:24:19 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8129 s / batch. (data: 3.10e-04). ETA=12:23:50, max mem: 20.9 GB 
[11/22 07:26:03 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8197 s / batch. (data: 5.45e-03). ETA=12:28:41, max mem: 20.9 GB 
[11/22 07:26:56 visual_prompt]: Epoch 1 / 100: avg data time: 1.92e-01, avg batch time: 1.0192, average train loss: 1.5403
[11/22 07:27:54 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3088, average loss: 1.5201
[11/22 07:27:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/22 07:27:54 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/22 07:29:38 visual_prompt]: 	Training 100/553. train loss: 2.9190,	1.0138 s / batch. (data: 1.44e-01). ETA=15:23:19, max mem: 20.9 GB 
[11/22 07:31:19 visual_prompt]: 	Training 200/553. train loss: 0.0037,	0.8418 s / batch. (data: 9.50e-03). ETA=12:45:16, max mem: 20.9 GB 
[11/22 07:33:01 visual_prompt]: 	Training 300/553. train loss: 7.8096,	1.1120 s / batch. (data: 2.70e-01). ETA=16:49:05, max mem: 20.9 GB 
[11/22 07:34:42 visual_prompt]: 	Training 400/553. train loss: 0.6231,	0.8468 s / batch. (data: 5.44e-03). ETA=12:47:01, max mem: 20.9 GB 
[11/22 07:36:23 visual_prompt]: 	Training 500/553. train loss: 3.2929,	0.8396 s / batch. (data: 5.43e-03). ETA=12:39:07, max mem: 20.9 GB 
[11/22 07:37:16 visual_prompt]: Epoch 2 / 100: avg data time: 1.88e-01, avg batch time: 1.0153, average train loss: 3.4209
[11/22 07:38:13 visual_prompt]: Inference (val):avg data time: 6.49e-05, avg batch time: 0.3086, average loss: 19.1750
[11/22 07:38:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.67	
[11/22 07:38:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/22 07:39:57 visual_prompt]: 	Training 100/553. train loss: 0.7196,	0.8200 s / batch. (data: 3.49e-04). ETA=12:19:16, max mem: 20.9 GB 
[11/22 07:41:39 visual_prompt]: 	Training 200/553. train loss: 1.8945,	0.8201 s / batch. (data: 3.02e-04). ETA=12:18:02, max mem: 20.9 GB 
[11/22 07:43:19 visual_prompt]: 	Training 300/553. train loss: 4.1691,	0.8109 s / batch. (data: 3.24e-04). ETA=12:08:24, max mem: 20.9 GB 
[11/22 07:45:00 visual_prompt]: 	Training 400/553. train loss: 55.2133,	0.8360 s / batch. (data: 3.17e-04). ETA=12:29:31, max mem: 20.9 GB 
[11/22 07:46:42 visual_prompt]: 	Training 500/553. train loss: 7.2534,	1.4482 s / batch. (data: 6.21e-01). ETA=21:35:58, max mem: 20.9 GB 
[11/22 07:47:34 visual_prompt]: Epoch 3 / 100: avg data time: 1.86e-01, avg batch time: 1.0134, average train loss: 6.8817
[11/22 07:48:32 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3079, average loss: 4.1820
[11/22 07:48:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.68	
[11/22 07:48:32 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/22 07:50:18 visual_prompt]: 	Training 100/553. train loss: 20.7751,	0.8080 s / batch. (data: 3.03e-04). ETA=12:01:01, max mem: 20.9 GB 
[11/22 07:51:59 visual_prompt]: 	Training 200/553. train loss: 2.4443,	0.8353 s / batch. (data: 3.41e-04). ETA=12:23:58, max mem: 20.9 GB 
[11/22 07:53:41 visual_prompt]: 	Training 300/553. train loss: 2.6300,	1.7560 s / batch. (data: 9.30e-01). ETA=1 day, 2:01:06, max mem: 20.9 GB 
[11/22 07:55:17 visual_prompt]: 	Training 400/553. train loss: 5.1956,	1.1097 s / batch. (data: 2.96e-01). ETA=16:24:43, max mem: 20.9 GB 
[11/22 07:57:00 visual_prompt]: 	Training 500/553. train loss: 49.1465,	3.4931 s / batch. (data: 2.69e+00). ETA=2 days, 3:33:44, max mem: 20.9 GB 
[11/22 07:57:54 visual_prompt]: Epoch 4 / 100: avg data time: 1.91e-01, avg batch time: 1.0163, average train loss: 9.3442
[11/22 07:58:52 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3091, average loss: 2.7230
[11/22 07:58:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.14	
[11/22 07:58:52 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/22 08:00:36 visual_prompt]: 	Training 100/553. train loss: 0.3351,	0.8320 s / batch. (data: 3.10e-04). ETA=12:14:46, max mem: 20.9 GB 
[11/22 08:02:17 visual_prompt]: 	Training 200/553. train loss: 12.2037,	1.6120 s / batch. (data: 7.75e-01). ETA=23:40:54, max mem: 20.9 GB 
[11/22 08:03:59 visual_prompt]: 	Training 300/553. train loss: 47.6977,	0.8244 s / batch. (data: 7.67e-03). ETA=12:05:18, max mem: 20.9 GB 
[11/22 08:05:39 visual_prompt]: 	Training 400/553. train loss: 16.4583,	0.8394 s / batch. (data: 5.46e-03). ETA=12:17:05, max mem: 20.9 GB 
[11/22 08:07:20 visual_prompt]: 	Training 500/553. train loss: 4.7859,	0.8509 s / batch. (data: 7.94e-03). ETA=12:25:49, max mem: 20.9 GB 
[11/22 08:08:13 visual_prompt]: Epoch 5 / 100: avg data time: 1.89e-01, avg batch time: 1.0150, average train loss: 13.9010
[11/22 08:09:11 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3091, average loss: 92.5483
[11/22 08:09:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[11/22 08:09:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/22 08:10:58 visual_prompt]: 	Training 100/553. train loss: 1.8484,	0.8322 s / batch. (data: 5.45e-03). ETA=12:07:18, max mem: 20.9 GB 
[11/22 08:12:37 visual_prompt]: 	Training 200/553. train loss: 11.7367,	0.8325 s / batch. (data: 5.51e-03). ETA=12:06:07, max mem: 20.9 GB 
[11/22 08:14:16 visual_prompt]: 	Training 300/553. train loss: 3.2738,	0.8202 s / batch. (data: 3.50e-04). ETA=11:54:02, max mem: 20.9 GB 
[11/22 08:16:02 visual_prompt]: 	Training 400/553. train loss: 8.4288,	0.8384 s / batch. (data: 1.12e-02). ETA=12:08:29, max mem: 20.9 GB 
[11/22 08:17:41 visual_prompt]: 	Training 500/553. train loss: 2.4603,	0.8213 s / batch. (data: 3.58e-04). ETA=11:52:15, max mem: 20.9 GB 
[11/22 08:18:33 visual_prompt]: Epoch 6 / 100: avg data time: 1.90e-01, avg batch time: 1.0154, average train loss: 21.8691
[11/22 08:19:30 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3078, average loss: 2.3365
[11/22 08:19:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.85	
[11/22 08:19:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/22 08:21:14 visual_prompt]: 	Training 100/553. train loss: 13.2510,	0.8201 s / batch. (data: 3.84e-04). ETA=11:49:09, max mem: 20.9 GB 
[11/22 08:22:54 visual_prompt]: 	Training 200/553. train loss: 23.3658,	1.1485 s / batch. (data: 3.29e-01). ETA=16:31:10, max mem: 20.9 GB 
[11/22 08:24:39 visual_prompt]: 	Training 300/553. train loss: 12.4615,	1.9986 s / batch. (data: 1.18e+00). ETA=1 day, 4:41:34, max mem: 20.9 GB 
[11/22 08:26:20 visual_prompt]: 	Training 400/553. train loss: 9.6258,	1.8651 s / batch. (data: 1.06e+00). ETA=1 day, 2:43:27, max mem: 20.9 GB 
[11/22 08:27:58 visual_prompt]: 	Training 500/553. train loss: 4.9146,	0.8359 s / batch. (data: 5.13e-04). ETA=11:57:15, max mem: 20.9 GB 
[11/22 08:28:50 visual_prompt]: Epoch 7 / 100: avg data time: 1.86e-01, avg batch time: 1.0116, average train loss: 20.0644
[11/22 08:29:48 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.3079, average loss: 8.4642
[11/22 08:29:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.60	
[11/22 08:29:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/22 08:31:31 visual_prompt]: 	Training 100/553. train loss: 9.8224,	0.8439 s / batch. (data: 4.03e-04). ETA=12:01:58, max mem: 20.9 GB 
[11/22 08:33:13 visual_prompt]: 	Training 200/553. train loss: 39.4100,	0.8440 s / batch. (data: 3.40e-04). ETA=12:00:37, max mem: 20.9 GB 
[11/22 08:34:55 visual_prompt]: 	Training 300/553. train loss: 13.7431,	0.8220 s / batch. (data: 3.23e-04). ETA=11:40:29, max mem: 20.9 GB 
[11/22 08:36:36 visual_prompt]: 	Training 400/553. train loss: 51.2663,	0.8314 s / batch. (data: 3.23e-04). ETA=11:47:03, max mem: 20.9 GB 
[11/22 08:38:18 visual_prompt]: 	Training 500/553. train loss: 139.0039,	1.6149 s / batch. (data: 8.07e-01). ETA=22:50:44, max mem: 20.9 GB 
[11/22 08:39:10 visual_prompt]: Epoch 8 / 100: avg data time: 1.92e-01, avg batch time: 1.0163, average train loss: 25.8392
[11/22 08:40:07 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3081, average loss: 7.0291
[11/22 08:40:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.43	
[11/22 08:40:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/22 08:41:52 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8317 s / batch. (data: 1.05e-02). ETA=11:43:48, max mem: 20.9 GB 
[11/22 08:43:32 visual_prompt]: 	Training 200/553. train loss: 13.8168,	0.8213 s / batch. (data: 3.07e-04). ETA=11:33:38, max mem: 20.9 GB 
[11/22 08:45:13 visual_prompt]: 	Training 300/553. train loss: 3.5134,	1.9168 s / batch. (data: 1.10e+00). ETA=1 day, 2:55:45, max mem: 20.9 GB 
[11/22 08:46:56 visual_prompt]: 	Training 400/553. train loss: 10.2903,	0.8230 s / batch. (data: 3.25e-04). ETA=11:32:23, max mem: 20.9 GB 
[11/22 08:48:37 visual_prompt]: 	Training 500/553. train loss: 21.3491,	0.9774 s / batch. (data: 1.59e-01). ETA=13:40:37, max mem: 20.9 GB 
[11/22 08:49:29 visual_prompt]: Epoch 9 / 100: avg data time: 1.89e-01, avg batch time: 1.0146, average train loss: 23.0162
[11/22 08:50:27 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.3069, average loss: 42.8388
[11/22 08:50:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.46	
[11/22 08:50:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/22 08:52:14 visual_prompt]: 	Training 100/553. train loss: 12.5214,	0.8367 s / batch. (data: 4.03e-04). ETA=11:40:24, max mem: 20.9 GB 
[11/22 08:53:53 visual_prompt]: 	Training 200/553. train loss: 6.6046,	0.8182 s / batch. (data: 3.52e-04). ETA=11:23:28, max mem: 20.9 GB 
[11/22 08:55:33 visual_prompt]: 	Training 300/553. train loss: 6.6750,	1.9200 s / batch. (data: 1.09e+00). ETA=1 day, 2:40:43, max mem: 20.9 GB 
[11/22 08:57:12 visual_prompt]: 	Training 400/553. train loss: 5.2920,	0.8404 s / batch. (data: 1.56e-02). ETA=11:39:14, max mem: 20.9 GB 
[11/22 08:58:54 visual_prompt]: 	Training 500/553. train loss: 19.9925,	0.8428 s / batch. (data: 3.18e-02). ETA=11:39:52, max mem: 20.9 GB 
[11/22 08:59:47 visual_prompt]: Epoch 10 / 100: avg data time: 1.89e-01, avg batch time: 1.0141, average train loss: 30.4111
[11/22 09:00:47 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3069, average loss: 0.9898
[11/22 09:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.75	
[11/22 09:00:47 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/22 09:02:34 visual_prompt]: 	Training 100/553. train loss: 15.8778,	0.8198 s / batch. (data: 3.05e-04). ETA=11:18:39, max mem: 20.9 GB 
[11/22 09:04:17 visual_prompt]: 	Training 200/553. train loss: 24.1415,	0.8160 s / batch. (data: 3.55e-04). ETA=11:14:08, max mem: 20.9 GB 
[11/22 09:05:56 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.9404 s / batch. (data: 1.12e-01). ETA=12:55:22, max mem: 20.9 GB 
[11/22 09:07:36 visual_prompt]: 	Training 400/553. train loss: 43.0872,	0.8142 s / batch. (data: 5.44e-03). ETA=11:09:56, max mem: 20.9 GB 
[11/22 09:09:16 visual_prompt]: 	Training 500/553. train loss: 45.0347,	0.8320 s / batch. (data: 7.94e-03). ETA=11:23:12, max mem: 20.9 GB 
[11/22 09:10:08 visual_prompt]: Epoch 11 / 100: avg data time: 1.91e-01, avg batch time: 1.0148, average train loss: 34.2581
[11/22 09:11:06 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3091, average loss: 21.7759
[11/22 09:11:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.63	
[11/22 09:11:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/22 09:12:52 visual_prompt]: 	Training 100/553. train loss: 39.2219,	0.8315 s / batch. (data: 5.48e-03). ETA=11:20:40, max mem: 20.9 GB 
[11/22 09:14:34 visual_prompt]: 	Training 200/553. train loss: 7.6297,	0.8470 s / batch. (data: 1.56e-02). ETA=11:31:58, max mem: 20.9 GB 
[11/22 09:16:14 visual_prompt]: 	Training 300/553. train loss: 12.9869,	0.8280 s / batch. (data: 1.20e-02). ETA=11:15:03, max mem: 20.9 GB 
[11/22 09:17:54 visual_prompt]: 	Training 400/553. train loss: 20.0534,	0.8127 s / batch. (data: 3.32e-04). ETA=11:01:13, max mem: 20.9 GB 
[11/22 09:19:35 visual_prompt]: 	Training 500/553. train loss: 7.4051,	0.8426 s / batch. (data: 8.33e-04). ETA=11:24:09, max mem: 20.9 GB 
[11/22 09:20:26 visual_prompt]: Epoch 12 / 100: avg data time: 1.89e-01, avg batch time: 1.0136, average train loss: 34.2184
[11/22 09:21:24 visual_prompt]: Inference (val):avg data time: 2.60e-04, avg batch time: 0.3060, average loss: 33.8616
[11/22 09:21:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.16	
[11/22 09:21:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/22 09:23:11 visual_prompt]: 	Training 100/553. train loss: 18.5837,	0.8680 s / batch. (data: 7.97e-03). ETA=11:42:33, max mem: 20.9 GB 
[11/22 09:24:49 visual_prompt]: 	Training 200/553. train loss: 119.0935,	0.8280 s / batch. (data: 5.58e-03). ETA=11:08:46, max mem: 20.9 GB 
[11/22 09:26:30 visual_prompt]: 	Training 300/553. train loss: 21.8950,	1.5720 s / batch. (data: 7.42e-01). ETA=21:07:07, max mem: 20.9 GB 
[11/22 09:28:10 visual_prompt]: 	Training 400/553. train loss: 13.5954,	0.8280 s / batch. (data: 3.19e-04). ETA=11:06:02, max mem: 20.9 GB 
[11/22 09:29:52 visual_prompt]: 	Training 500/553. train loss: 20.9357,	0.8293 s / batch. (data: 3.61e-04). ETA=11:05:40, max mem: 20.9 GB 
[11/22 09:30:45 visual_prompt]: Epoch 13 / 100: avg data time: 1.90e-01, avg batch time: 1.0131, average train loss: 36.6054
[11/22 09:31:43 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3054, average loss: 12.1257
[11/22 09:31:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[11/22 09:31:43 visual_prompt]: Best epoch 13: best metric: -12.126
[11/22 09:31:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/22 09:33:29 visual_prompt]: 	Training 100/553. train loss: 1.8615,	0.8441 s / batch. (data: 6.12e-03). ETA=11:15:25, max mem: 20.9 GB 
[11/22 09:35:10 visual_prompt]: 	Training 200/553. train loss: 6.4021,	1.1160 s / batch. (data: 2.89e-01). ETA=14:51:09, max mem: 20.9 GB 
[11/22 09:36:50 visual_prompt]: 	Training 300/553. train loss: 68.5546,	0.8327 s / batch. (data: 3.25e-04). ETA=11:03:32, max mem: 20.9 GB 
[11/22 09:38:31 visual_prompt]: 	Training 400/553. train loss: 5.1330,	0.8162 s / batch. (data: 5.44e-03). ETA=10:49:03, max mem: 20.9 GB 
[11/22 09:40:12 visual_prompt]: 	Training 500/553. train loss: 9.8359,	0.8251 s / batch. (data: 3.17e-04). ETA=10:54:42, max mem: 20.9 GB 
[11/22 09:41:03 visual_prompt]: Epoch 14 / 100: avg data time: 1.88e-01, avg batch time: 1.0120, average train loss: 32.7403
[11/22 09:42:01 visual_prompt]: Inference (val):avg data time: 3.72e-04, avg batch time: 0.3095, average loss: 76.2857
[11/22 09:42:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.90	
[11/22 09:42:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/22 09:43:46 visual_prompt]: 	Training 100/553. train loss: 58.4551,	0.8051 s / batch. (data: 3.55e-04). ETA=10:36:50, max mem: 20.9 GB 
[11/22 09:45:25 visual_prompt]: 	Training 200/553. train loss: 188.4184,	0.8422 s / batch. (data: 3.09e-02). ETA=11:04:44, max mem: 20.9 GB 
[11/22 09:47:08 visual_prompt]: 	Training 300/553. train loss: 24.2248,	0.8387 s / batch. (data: 5.43e-03). ETA=11:00:33, max mem: 20.9 GB 
[11/22 09:48:47 visual_prompt]: 	Training 400/553. train loss: 5.2649,	0.8204 s / batch. (data: 3.35e-04). ETA=10:44:49, max mem: 20.9 GB 
[11/22 09:50:28 visual_prompt]: 	Training 500/553. train loss: 15.9856,	0.8240 s / batch. (data: 3.65e-04). ETA=10:46:14, max mem: 20.9 GB 
[11/22 09:51:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.91e-01, avg batch time: 1.0154, average train loss: 36.2378
[11/22 09:52:20 visual_prompt]: Inference (val):avg data time: 4.47e-05, avg batch time: 0.3065, average loss: 21.1190
[11/22 09:52:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.29	
[11/22 09:52:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/22 09:54:04 visual_prompt]: 	Training 100/553. train loss: 136.7389,	0.8257 s / batch. (data: 1.19e-02). ETA=10:45:31, max mem: 20.9 GB 
[11/22 09:55:44 visual_prompt]: 	Training 200/553. train loss: 59.3486,	0.8179 s / batch. (data: 3.29e-04). ETA=10:38:03, max mem: 20.9 GB 
[11/22 09:57:26 visual_prompt]: 	Training 300/553. train loss: 30.4218,	0.8137 s / batch. (data: 8.37e-04). ETA=10:33:22, max mem: 20.9 GB 
[11/22 09:59:07 visual_prompt]: 	Training 400/553. train loss: 17.3393,	0.8201 s / batch. (data: 3.40e-04). ETA=10:36:58, max mem: 20.9 GB 
[11/22 10:00:47 visual_prompt]: 	Training 500/553. train loss: 11.4081,	0.8160 s / batch. (data: 3.33e-04). ETA=10:32:29, max mem: 20.9 GB 
[11/22 10:01:40 visual_prompt]: Epoch 16 / 100: avg data time: 1.88e-01, avg batch time: 1.0133, average train loss: 32.5930
[11/22 10:02:38 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3080, average loss: 44.3229
[11/22 10:02:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.94	
[11/22 10:02:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/22 10:04:21 visual_prompt]: 	Training 100/553. train loss: 55.8716,	0.8360 s / batch. (data: 3.18e-04). ETA=10:45:50, max mem: 20.9 GB 
[11/22 10:06:03 visual_prompt]: 	Training 200/553. train loss: 54.9315,	0.8307 s / batch. (data: 6.79e-03). ETA=10:40:20, max mem: 20.9 GB 
[11/22 10:07:43 visual_prompt]: 	Training 300/553. train loss: 15.8055,	0.8453 s / batch. (data: 2.12e-02). ETA=10:50:10, max mem: 20.9 GB 
[11/22 10:09:24 visual_prompt]: 	Training 400/553. train loss: 2.8833,	1.1837 s / batch. (data: 3.63e-01). ETA=15:08:31, max mem: 20.9 GB 
[11/22 10:11:05 visual_prompt]: 	Training 500/553. train loss: 5.9224,	1.8007 s / batch. (data: 9.91e-01). ETA=22:59:07, max mem: 20.9 GB 
[11/22 10:11:59 visual_prompt]: Epoch 17 / 100: avg data time: 1.89e-01, avg batch time: 1.0138, average train loss: 31.9066
[11/22 10:12:56 visual_prompt]: Inference (val):avg data time: 1.85e-04, avg batch time: 0.3060, average loss: 22.5387
[11/22 10:12:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.52	
[11/22 10:12:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/22 10:14:41 visual_prompt]: 	Training 100/553. train loss: 33.3468,	0.8366 s / batch. (data: 3.02e-04). ETA=10:38:37, max mem: 20.9 GB 
[11/22 10:16:25 visual_prompt]: 	Training 200/553. train loss: 15.3985,	0.8156 s / batch. (data: 3.39e-04). ETA=10:21:09, max mem: 20.9 GB 
[11/22 10:18:06 visual_prompt]: 	Training 300/553. train loss: 36.3336,	0.8319 s / batch. (data: 2.12e-02). ETA=10:32:14, max mem: 20.9 GB 
[11/22 10:19:46 visual_prompt]: 	Training 400/553. train loss: 26.4739,	0.8320 s / batch. (data: 3.35e-04). ETA=10:30:54, max mem: 20.9 GB 
[11/22 10:21:26 visual_prompt]: 	Training 500/553. train loss: 26.0536,	0.8240 s / batch. (data: 3.32e-04). ETA=10:23:28, max mem: 20.9 GB 
[11/22 10:22:18 visual_prompt]: Epoch 18 / 100: avg data time: 1.91e-01, avg batch time: 1.0147, average train loss: 32.3624
[11/22 10:23:16 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3106, average loss: 28.3415
[11/22 10:23:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[11/22 10:23:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/22 10:25:00 visual_prompt]: 	Training 100/553. train loss: 73.0952,	0.8240 s / batch. (data: 3.18e-04). ETA=10:21:24, max mem: 20.9 GB 
[11/22 10:26:42 visual_prompt]: 	Training 200/553. train loss: 18.0639,	0.8280 s / batch. (data: 3.34e-04). ETA=10:23:00, max mem: 20.9 GB 
[11/22 10:28:23 visual_prompt]: 	Training 300/553. train loss: 0.0051,	0.8359 s / batch. (data: 4.19e-04). ETA=10:27:34, max mem: 20.9 GB 
[11/22 10:30:06 visual_prompt]: 	Training 400/553. train loss: 8.9777,	0.8200 s / batch. (data: 8.07e-04). ETA=10:14:15, max mem: 20.9 GB 
[11/22 10:31:42 visual_prompt]: 	Training 500/553. train loss: 29.4289,	0.8609 s / batch. (data: 1.28e-02). ETA=10:43:26, max mem: 20.9 GB 
[11/22 10:32:35 visual_prompt]: Epoch 19 / 100: avg data time: 1.87e-01, avg batch time: 1.0114, average train loss: 37.7777
[11/22 10:33:33 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3075, average loss: 163.5188
[11/22 10:33:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.06	
[11/22 10:33:33 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/22 10:35:16 visual_prompt]: 	Training 100/553. train loss: 11.4015,	0.8484 s / batch. (data: 2.58e-02). ETA=10:31:57, max mem: 20.9 GB 
[11/22 10:36:58 visual_prompt]: 	Training 200/553. train loss: 34.3164,	0.8280 s / batch. (data: 7.93e-03). ETA=10:15:21, max mem: 20.9 GB 
[11/22 10:38:39 visual_prompt]: 	Training 300/553. train loss: 34.0807,	0.8200 s / batch. (data: 3.25e-04). ETA=10:08:03, max mem: 20.9 GB 
[11/22 10:40:19 visual_prompt]: 	Training 400/553. train loss: 1.3187,	0.8234 s / batch. (data: 5.49e-03). ETA=10:09:11, max mem: 20.9 GB 
[11/22 10:41:59 visual_prompt]: 	Training 500/553. train loss: 16.2321,	0.8360 s / batch. (data: 3.18e-04). ETA=10:17:08, max mem: 20.9 GB 
[11/22 10:42:53 visual_prompt]: Epoch 20 / 100: avg data time: 1.88e-01, avg batch time: 1.0133, average train loss: 33.0191
[11/22 10:43:51 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.3063, average loss: 21.6157
[11/22 10:43:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.95	
[11/22 10:43:51 visual_prompt]: Stopping early.
[11/22 10:43:51 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 10:43:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 10:43:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/22 10:43:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/22 10:43:51 visual_prompt]: Training with config:
[11/22 10:43:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/22 10:43:51 visual_prompt]: Loading training data...
[11/22 10:43:51 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 10:43:51 visual_prompt]: Loading validation data...
[11/22 10:43:51 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 10:43:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 10:43:54 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/22 10:43:54 visual_prompt]: tuned percent:0.525
[11/22 10:43:54 visual_prompt]: Device used for model: 0
[11/22 10:43:54 visual_prompt]: Setting up Evaluator...
[11/22 10:43:54 visual_prompt]: Setting up Trainer...
[11/22 10:43:54 visual_prompt]: 	Setting up the optimizer...
[11/22 10:43:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 10:45:39 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8200 s / batch. (data: 3.28e-04). ETA=12:34:24, max mem: 20.9 GB 
[11/22 10:47:18 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8245 s / batch. (data: 3.07e-04). ETA=12:37:10, max mem: 20.9 GB 
[11/22 10:49:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.5681 s / batch. (data: 7.53e-01). ETA=23:57:24, max mem: 20.9 GB 
[11/22 10:50:41 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8290 s / batch. (data: 1.05e-02). ETA=12:38:33, max mem: 20.9 GB 
[11/22 10:52:24 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8392 s / batch. (data: 7.98e-04). ETA=12:46:26, max mem: 20.9 GB 
[11/22 10:53:17 visual_prompt]: Epoch 1 / 100: avg data time: 1.92e-01, avg batch time: 1.0180, average train loss: 1.5403
[11/22 10:54:15 visual_prompt]: Inference (val):avg data time: 2.37e-04, avg batch time: 0.3078, average loss: 1.5201
[11/22 10:54:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/22 10:54:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/22 10:55:59 visual_prompt]: 	Training 100/553. train loss: 3.8541,	0.8360 s / batch. (data: 3.41e-04). ETA=12:41:23, max mem: 20.9 GB 
[11/22 10:57:40 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8353 s / batch. (data: 5.47e-03). ETA=12:39:22, max mem: 20.9 GB 
[11/22 10:59:23 visual_prompt]: 	Training 300/553. train loss: 2.6410,	1.1400 s / batch. (data: 3.10e-01). ETA=17:14:30, max mem: 20.9 GB 
[11/22 11:01:02 visual_prompt]: 	Training 400/553. train loss: 1.6751,	0.8366 s / batch. (data: 3.28e-04). ETA=12:37:44, max mem: 20.9 GB 
[11/22 11:02:45 visual_prompt]: 	Training 500/553. train loss: 0.5674,	0.8434 s / batch. (data: 3.04e-04). ETA=12:42:29, max mem: 20.9 GB 
[11/22 11:03:36 visual_prompt]: Epoch 2 / 100: avg data time: 1.88e-01, avg batch time: 1.0144, average train loss: 3.2359
[11/22 11:04:34 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3074, average loss: 10.4632
[11/22 11:04:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.18	
[11/22 11:04:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/22 11:06:29 visual_prompt]: 	Training 100/553. train loss: 24.3151,	0.8600 s / batch. (data: 5.43e-03). ETA=12:55:21, max mem: 20.9 GB 
[11/22 11:08:14 visual_prompt]: 	Training 200/553. train loss: 4.6712,	0.8120 s / batch. (data: 3.01e-04). ETA=12:10:44, max mem: 20.9 GB 
[11/22 11:09:55 visual_prompt]: 	Training 300/553. train loss: 2.6836,	0.8570 s / batch. (data: 1.56e-02). ETA=12:49:49, max mem: 20.9 GB 
[11/22 11:11:39 visual_prompt]: 	Training 400/553. train loss: 6.9202,	0.8120 s / batch. (data: 3.31e-04). ETA=12:08:01, max mem: 20.9 GB 
[11/22 11:13:23 visual_prompt]: 	Training 500/553. train loss: 2.7821,	1.1638 s / batch. (data: 3.53e-01). ETA=17:21:26, max mem: 20.9 GB 
[11/22 11:14:16 visual_prompt]: Epoch 3 / 100: avg data time: 2.26e-01, avg batch time: 1.0506, average train loss: 6.4887
[11/22 11:15:15 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3078, average loss: 7.2072
[11/22 11:15:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/22 11:15:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/22 11:17:02 visual_prompt]: 	Training 100/553. train loss: 21.5205,	0.8274 s / batch. (data: 3.12e-04). ETA=12:18:17, max mem: 20.9 GB 
[11/22 11:18:43 visual_prompt]: 	Training 200/553. train loss: 17.5341,	0.8236 s / batch. (data: 5.44e-03). ETA=12:13:31, max mem: 20.9 GB 
[11/22 11:20:22 visual_prompt]: 	Training 300/553. train loss: 1.5394,	1.4591 s / batch. (data: 6.14e-01). ETA=21:37:10, max mem: 20.9 GB 
[11/22 11:21:55 visual_prompt]: 	Training 400/553. train loss: 17.8045,	0.8240 s / batch. (data: 3.23e-04). ETA=12:11:08, max mem: 20.9 GB 
[11/22 11:23:35 visual_prompt]: 	Training 500/553. train loss: 15.0734,	3.3438 s / batch. (data: 2.52e+00). ETA=2 days, 1:21:33, max mem: 20.9 GB 
[11/22 11:24:27 visual_prompt]: Epoch 4 / 100: avg data time: 1.73e-01, avg batch time: 0.9991, average train loss: 8.8992
[11/22 11:25:23 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3080, average loss: 7.4956
[11/22 11:25:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.73	
[11/22 11:25:23 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/22 11:27:03 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8120 s / batch. (data: 2.84e-04). ETA=11:57:06, max mem: 20.9 GB 
[11/22 11:28:42 visual_prompt]: 	Training 200/553. train loss: 26.0106,	0.9600 s / batch. (data: 1.47e-01). ETA=14:06:11, max mem: 20.9 GB 
[11/22 11:30:19 visual_prompt]: 	Training 300/553. train loss: 27.0224,	0.8441 s / batch. (data: 3.12e-04). ETA=12:22:40, max mem: 20.9 GB 
[11/22 11:31:55 visual_prompt]: 	Training 400/553. train loss: 5.8865,	0.8400 s / batch. (data: 3.07e-04). ETA=12:17:38, max mem: 20.9 GB 
[11/22 11:33:32 visual_prompt]: 	Training 500/553. train loss: 20.3104,	0.8440 s / batch. (data: 5.44e-03). ETA=12:19:42, max mem: 20.9 GB 
[11/22 11:34:23 visual_prompt]: Epoch 5 / 100: avg data time: 1.50e-01, avg batch time: 0.9764, average train loss: 13.5538
[11/22 11:35:18 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3075, average loss: 19.2477
[11/22 11:35:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.87	
[11/22 11:35:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/22 11:37:00 visual_prompt]: 	Training 100/553. train loss: 18.3519,	0.8097 s / batch. (data: 2.99e-04). ETA=11:47:38, max mem: 20.9 GB 
[11/22 11:38:36 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8201 s / batch. (data: 7.97e-03). ETA=11:55:19, max mem: 20.9 GB 
[11/22 11:40:10 visual_prompt]: 	Training 300/553. train loss: 10.2809,	0.8437 s / batch. (data: 1.16e-02). ETA=12:14:29, max mem: 20.9 GB 
[11/22 11:41:55 visual_prompt]: 	Training 400/553. train loss: 44.9343,	0.8118 s / batch. (data: 3.59e-04). ETA=11:45:22, max mem: 20.9 GB 
[11/22 11:43:35 visual_prompt]: 	Training 500/553. train loss: 30.3506,	0.8280 s / batch. (data: 3.48e-04). ETA=11:58:03, max mem: 20.9 GB 
[11/22 11:44:28 visual_prompt]: Epoch 6 / 100: avg data time: 1.68e-01, avg batch time: 0.9936, average train loss: 16.0221
[11/22 11:45:27 visual_prompt]: Inference (val):avg data time: 6.59e-04, avg batch time: 0.3097, average loss: 6.9090
[11/22 11:45:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.85	
[11/22 11:45:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/22 11:47:11 visual_prompt]: 	Training 100/553. train loss: 21.6591,	0.8221 s / batch. (data: 2.96e-04). ETA=11:50:52, max mem: 20.9 GB 
[11/22 11:48:51 visual_prompt]: 	Training 200/553. train loss: 10.9828,	1.0000 s / batch. (data: 1.68e-01). ETA=14:22:59, max mem: 20.9 GB 
[11/22 11:50:35 visual_prompt]: 	Training 300/553. train loss: 16.7040,	1.7023 s / batch. (data: 8.81e-01). ETA=1 day, 0:26:18, max mem: 20.9 GB 
[11/22 11:52:16 visual_prompt]: 	Training 400/553. train loss: 3.1928,	1.9224 s / batch. (data: 1.10e+00). ETA=1 day, 3:32:43, max mem: 20.9 GB 
[11/22 11:53:55 visual_prompt]: 	Training 500/553. train loss: 8.6548,	0.8335 s / batch. (data: 3.62e-04). ETA=11:55:09, max mem: 20.9 GB 
[11/22 11:54:46 visual_prompt]: Epoch 7 / 100: avg data time: 1.86e-01, avg batch time: 1.0111, average train loss: 20.5133
[11/22 11:55:44 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.3072, average loss: 20.0773
[11/22 11:55:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.11	
[11/22 11:55:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/22 11:57:26 visual_prompt]: 	Training 100/553. train loss: 65.4587,	0.8174 s / batch. (data: 5.45e-03). ETA=11:39:18, max mem: 20.9 GB 
[11/22 11:59:09 visual_prompt]: 	Training 200/553. train loss: 5.9290,	0.8280 s / batch. (data: 3.27e-04). ETA=11:46:58, max mem: 20.9 GB 
[11/22 12:00:50 visual_prompt]: 	Training 300/553. train loss: 20.1993,	0.8328 s / batch. (data: 5.42e-03). ETA=11:49:38, max mem: 20.9 GB 
[11/22 12:02:30 visual_prompt]: 	Training 400/553. train loss: 4.9946,	0.8809 s / batch. (data: 6.01e-02). ETA=12:29:11, max mem: 20.9 GB 
[11/22 12:04:11 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.5080 s / batch. (data: 6.79e-01). ETA=21:20:02, max mem: 20.9 GB 
[11/22 12:05:04 visual_prompt]: Epoch 8 / 100: avg data time: 1.88e-01, avg batch time: 1.0135, average train loss: 21.8628
[11/22 12:06:02 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3079, average loss: 4.8054
[11/22 12:06:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.08	
[11/22 12:06:02 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/22 12:07:47 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8520 s / batch. (data: 7.96e-03). ETA=12:01:01, max mem: 20.9 GB 
[11/22 12:09:26 visual_prompt]: 	Training 200/553. train loss: 19.8405,	0.8329 s / batch. (data: 2.06e-02). ETA=11:43:26, max mem: 20.9 GB 
[11/22 12:11:06 visual_prompt]: 	Training 300/553. train loss: 5.5630,	1.9960 s / batch. (data: 1.17e+00). ETA=1 day, 4:02:27, max mem: 20.9 GB 
[11/22 12:12:48 visual_prompt]: 	Training 400/553. train loss: 23.2621,	0.8240 s / batch. (data: 5.44e-03). ETA=11:33:12, max mem: 20.9 GB 
[11/22 12:14:30 visual_prompt]: 	Training 500/553. train loss: 11.8249,	1.0210 s / batch. (data: 2.09e-01). ETA=14:17:16, max mem: 20.9 GB 
[11/22 12:15:21 visual_prompt]: Epoch 9 / 100: avg data time: 1.86e-01, avg batch time: 1.0102, average train loss: 22.3752
[11/22 12:16:18 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3061, average loss: 28.7980
[11/22 12:16:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.92	
[11/22 12:16:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/22 12:18:06 visual_prompt]: 	Training 100/553. train loss: 51.8149,	0.8200 s / batch. (data: 8.10e-04). ETA=11:26:22, max mem: 20.9 GB 
[11/22 12:19:45 visual_prompt]: 	Training 200/553. train loss: 40.6482,	0.8296 s / batch. (data: 2.94e-04). ETA=11:33:00, max mem: 20.9 GB 
[11/22 12:21:24 visual_prompt]: 	Training 300/553. train loss: 216.0547,	2.3907 s / batch. (data: 1.57e+00). ETA=1 day, 9:13:08, max mem: 20.9 GB 
[11/22 12:23:03 visual_prompt]: 	Training 400/553. train loss: 9.2732,	0.8453 s / batch. (data: 3.21e-04). ETA=11:43:22, max mem: 20.9 GB 
[11/22 12:24:44 visual_prompt]: 	Training 500/553. train loss: 3.4918,	0.9640 s / batch. (data: 1.37e-01). ETA=13:20:29, max mem: 20.9 GB 
[11/22 12:25:37 visual_prompt]: Epoch 10 / 100: avg data time: 1.87e-01, avg batch time: 1.0100, average train loss: 33.7567
[11/22 12:26:35 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3070, average loss: 27.9118
[11/22 12:26:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 49.09	
[11/22 12:26:35 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/22 12:28:21 visual_prompt]: 	Training 100/553. train loss: 25.8767,	0.8233 s / batch. (data: 2.97e-04). ETA=11:21:34, max mem: 20.9 GB 
[11/22 12:30:03 visual_prompt]: 	Training 200/553. train loss: 43.7321,	0.8440 s / batch. (data: 3.15e-04). ETA=11:37:15, max mem: 20.9 GB 
[11/22 12:31:43 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.5560 s / batch. (data: 7.16e-01). ETA=21:22:55, max mem: 20.9 GB 
[11/22 12:33:22 visual_prompt]: 	Training 400/553. train loss: 3.9212,	0.8173 s / batch. (data: 5.42e-03). ETA=11:12:30, max mem: 20.9 GB 
[11/22 12:35:01 visual_prompt]: 	Training 500/553. train loss: 7.5434,	0.8166 s / batch. (data: 2.98e-04). ETA=11:10:31, max mem: 20.9 GB 
[11/22 12:35:53 visual_prompt]: Epoch 11 / 100: avg data time: 1.86e-01, avg batch time: 1.0101, average train loss: 28.0970
[11/22 12:36:51 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3073, average loss: 22.0926
[11/22 12:36:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.95	
[11/22 12:36:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/22 12:38:38 visual_prompt]: 	Training 100/553. train loss: 13.9978,	1.0645 s / batch. (data: 2.21e-01). ETA=14:31:24, max mem: 20.9 GB 
[11/22 12:40:19 visual_prompt]: 	Training 200/553. train loss: 86.0450,	0.8549 s / batch. (data: 2.38e-02). ETA=11:38:25, max mem: 20.9 GB 
[11/22 12:41:58 visual_prompt]: 	Training 300/553. train loss: 195.4582,	0.8400 s / batch. (data: 3.06e-04). ETA=11:24:50, max mem: 20.9 GB 
[11/22 12:43:39 visual_prompt]: 	Training 400/553. train loss: 32.3267,	0.8345 s / batch. (data: 1.05e-02). ETA=11:18:57, max mem: 20.9 GB 
[11/22 12:45:20 visual_prompt]: 	Training 500/553. train loss: 104.1752,	0.8247 s / batch. (data: 6.21e-03). ETA=11:09:39, max mem: 20.9 GB 
[11/22 12:46:11 visual_prompt]: Epoch 12 / 100: avg data time: 1.90e-01, avg batch time: 1.0127, average train loss: 34.8851
[11/22 12:47:09 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3066, average loss: 8.3360
[11/22 12:47:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[11/22 12:47:09 visual_prompt]: Best epoch 12: best metric: -8.336
[11/22 12:47:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/22 12:48:55 visual_prompt]: 	Training 100/553. train loss: 12.3251,	0.8284 s / batch. (data: 3.49e-04). ETA=11:10:31, max mem: 20.9 GB 
[11/22 12:50:33 visual_prompt]: 	Training 200/553. train loss: 2.0794,	0.8312 s / batch. (data: 3.10e-04). ETA=11:11:20, max mem: 20.9 GB 
[11/22 12:52:14 visual_prompt]: 	Training 300/553. train loss: 61.6420,	1.9480 s / batch. (data: 1.14e+00). ETA=1 day, 2:10:13, max mem: 20.9 GB 
[11/22 12:53:52 visual_prompt]: 	Training 400/553. train loss: 211.3320,	0.8185 s / batch. (data: 3.06e-04). ETA=10:58:22, max mem: 20.9 GB 
[11/22 12:55:34 visual_prompt]: 	Training 500/553. train loss: 41.9996,	0.8119 s / batch. (data: 2.91e-04). ETA=10:51:42, max mem: 20.9 GB 
[11/22 12:56:26 visual_prompt]: Epoch 13 / 100: avg data time: 1.86e-01, avg batch time: 1.0085, average train loss: 33.2361
[11/22 12:57:24 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3092, average loss: 23.4402
[11/22 12:57:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.31	
[11/22 12:57:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/22 12:59:10 visual_prompt]: 	Training 100/553. train loss: 58.2859,	0.8226 s / batch. (data: 1.20e-02). ETA=10:58:12, max mem: 20.9 GB 
[11/22 13:00:51 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2884 s / batch. (data: 4.60e-01). ETA=17:08:47, max mem: 20.9 GB 
[11/22 13:02:31 visual_prompt]: 	Training 300/553. train loss: 8.4469,	0.8792 s / batch. (data: 1.05e-02). ETA=11:40:34, max mem: 20.9 GB 
[11/22 13:04:10 visual_prompt]: 	Training 400/553. train loss: 24.6987,	0.8345 s / batch. (data: 5.42e-03). ETA=11:03:32, max mem: 20.9 GB 
[11/22 13:05:51 visual_prompt]: 	Training 500/553. train loss: 10.9003,	0.8279 s / batch. (data: 1.05e-02). ETA=10:56:59, max mem: 20.9 GB 
[11/22 13:06:43 visual_prompt]: Epoch 14 / 100: avg data time: 1.84e-01, avg batch time: 1.0097, average train loss: 27.9338
[11/22 13:07:41 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3078, average loss: 1.4110
[11/22 13:07:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.01	
[11/22 13:07:41 visual_prompt]: Best epoch 14: best metric: -1.411
[11/22 13:07:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/22 13:09:25 visual_prompt]: 	Training 100/553. train loss: 33.2693,	0.8278 s / batch. (data: 7.96e-03). ETA=10:54:45, max mem: 20.9 GB 
[11/22 13:11:04 visual_prompt]: 	Training 200/553. train loss: 175.5086,	0.8433 s / batch. (data: 3.06e-04). ETA=11:05:36, max mem: 20.9 GB 
[11/22 13:12:47 visual_prompt]: 	Training 300/553. train loss: 14.0868,	0.8280 s / batch. (data: 3.32e-04). ETA=10:52:09, max mem: 20.9 GB 
[11/22 13:14:24 visual_prompt]: 	Training 400/553. train loss: 43.9856,	0.8374 s / batch. (data: 1.05e-02). ETA=10:58:08, max mem: 20.9 GB 
[11/22 13:16:06 visual_prompt]: 	Training 500/553. train loss: 22.7769,	0.8361 s / batch. (data: 2.34e-02). ETA=10:55:43, max mem: 20.9 GB 
[11/22 13:16:59 visual_prompt]: Epoch 15 / 100: avg data time: 1.87e-01, avg batch time: 1.0101, average train loss: 30.8940
[11/22 13:17:57 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3075, average loss: 51.1618
[11/22 13:17:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.25	
[11/22 13:17:57 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/22 13:19:41 visual_prompt]: 	Training 100/553. train loss: 29.5155,	0.8160 s / batch. (data: 3.00e-04). ETA=10:37:54, max mem: 20.9 GB 
[11/22 13:21:22 visual_prompt]: 	Training 200/553. train loss: 37.1712,	0.8490 s / batch. (data: 5.41e-03). ETA=11:02:15, max mem: 20.9 GB 
[11/22 13:23:02 visual_prompt]: 	Training 300/553. train loss: 95.8599,	0.8209 s / batch. (data: 2.96e-04). ETA=10:39:01, max mem: 20.9 GB 
[11/22 13:24:43 visual_prompt]: 	Training 400/553. train loss: 46.6106,	0.8040 s / batch. (data: 3.23e-04). ETA=10:24:29, max mem: 20.9 GB 
[11/22 13:26:22 visual_prompt]: 	Training 500/553. train loss: 5.6730,	1.3451 s / batch. (data: 5.25e-01). ETA=17:22:35, max mem: 20.9 GB 
[11/22 13:27:15 visual_prompt]: Epoch 16 / 100: avg data time: 1.85e-01, avg batch time: 1.0092, average train loss: 30.0371
[11/22 13:28:13 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.3094, average loss: 16.7110
[11/22 13:28:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.45	
[11/22 13:28:13 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/22 13:29:56 visual_prompt]: 	Training 100/553. train loss: 0.9629,	0.8270 s / batch. (data: 3.03e-04). ETA=10:38:53, max mem: 20.9 GB 
[11/22 13:31:39 visual_prompt]: 	Training 200/553. train loss: 1.0105,	0.8198 s / batch. (data: 2.95e-04). ETA=10:31:56, max mem: 20.9 GB 
[11/22 13:33:18 visual_prompt]: 	Training 300/553. train loss: 31.9760,	0.8664 s / batch. (data: 5.42e-03). ETA=11:06:25, max mem: 20.9 GB 
[11/22 13:34:58 visual_prompt]: 	Training 400/553. train loss: 60.9852,	1.0758 s / batch. (data: 2.62e-01). ETA=13:45:41, max mem: 20.9 GB 
[11/22 13:36:38 visual_prompt]: 	Training 500/553. train loss: 10.4717,	1.4042 s / batch. (data: 5.47e-01). ETA=17:55:27, max mem: 20.9 GB 
[11/22 13:37:32 visual_prompt]: Epoch 17 / 100: avg data time: 1.86e-01, avg batch time: 1.0100, average train loss: 32.3750
[11/22 13:38:30 visual_prompt]: Inference (val):avg data time: 2.29e-04, avg batch time: 0.3104, average loss: 26.2978
[11/22 13:38:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.64	
[11/22 13:38:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/22 13:40:14 visual_prompt]: 	Training 100/553. train loss: 37.6888,	0.8240 s / batch. (data: 3.04e-04). ETA=10:28:58, max mem: 20.9 GB 
[11/22 13:41:57 visual_prompt]: 	Training 200/553. train loss: 47.4924,	0.8240 s / batch. (data: 3.17e-04). ETA=10:27:37, max mem: 20.9 GB 
[11/22 13:43:38 visual_prompt]: 	Training 300/553. train loss: 42.6714,	0.8160 s / batch. (data: 2.88e-04). ETA=10:20:08, max mem: 20.9 GB 
[11/22 13:45:18 visual_prompt]: 	Training 400/553. train loss: 5.9467,	0.8520 s / batch. (data: 3.03e-04). ETA=10:46:05, max mem: 20.9 GB 
[11/22 13:46:58 visual_prompt]: 	Training 500/553. train loss: 25.6926,	0.8457 s / batch. (data: 9.69e-03). ETA=10:39:55, max mem: 20.9 GB 
[11/22 13:47:49 visual_prompt]: Epoch 18 / 100: avg data time: 1.89e-01, avg batch time: 1.0123, average train loss: 33.0730
[11/22 13:48:47 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3079, average loss: 18.0114
[11/22 13:48:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.93	
[11/22 13:48:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/22 13:50:31 visual_prompt]: 	Training 100/553. train loss: 6.7463,	0.8455 s / batch. (data: 1.56e-02). ETA=10:37:35, max mem: 20.9 GB 
[11/22 13:52:13 visual_prompt]: 	Training 200/553. train loss: 18.9321,	0.8401 s / batch. (data: 8.45e-04). ETA=10:32:05, max mem: 20.9 GB 
[11/22 13:53:53 visual_prompt]: 	Training 300/553. train loss: 18.4357,	0.8187 s / batch. (data: 7.95e-03). ETA=10:14:41, max mem: 20.9 GB 
[11/22 13:55:35 visual_prompt]: 	Training 400/553. train loss: 20.8769,	0.8180 s / batch. (data: 1.67e-02). ETA=10:12:47, max mem: 20.9 GB 
[11/22 13:57:11 visual_prompt]: 	Training 500/553. train loss: 38.7176,	0.8246 s / batch. (data: 3.28e-04). ETA=10:16:22, max mem: 20.9 GB 
[11/22 13:58:04 visual_prompt]: Epoch 19 / 100: avg data time: 1.83e-01, avg batch time: 1.0062, average train loss: 33.2714
[11/22 13:59:01 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3060, average loss: 45.4579
[11/22 13:59:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.63	
[11/22 13:59:01 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/22 14:00:44 visual_prompt]: 	Training 100/553. train loss: 12.9582,	0.8334 s / batch. (data: 3.19e-04). ETA=10:20:47, max mem: 20.9 GB 
[11/22 14:02:26 visual_prompt]: 	Training 200/553. train loss: 39.6115,	0.8365 s / batch. (data: 1.07e-02). ETA=10:21:41, max mem: 20.9 GB 
[11/22 14:04:07 visual_prompt]: 	Training 300/553. train loss: 121.8446,	0.8287 s / batch. (data: 3.12e-04). ETA=10:14:29, max mem: 20.9 GB 
[11/22 14:05:48 visual_prompt]: 	Training 400/553. train loss: 47.2035,	0.8067 s / batch. (data: 3.37e-04). ETA=9:56:51, max mem: 20.9 GB 
[11/22 14:07:29 visual_prompt]: 	Training 500/553. train loss: 39.6276,	0.8164 s / batch. (data: 3.13e-04). ETA=10:02:41, max mem: 20.9 GB 
[11/22 14:08:24 visual_prompt]: Epoch 20 / 100: avg data time: 1.93e-01, avg batch time: 1.0172, average train loss: 35.4635
[11/22 14:09:22 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3081, average loss: 42.7701
[11/22 14:09:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.73	
[11/22 14:09:22 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/22 14:11:11 visual_prompt]: 	Training 100/553. train loss: 47.9980,	0.8533 s / batch. (data: 9.22e-03). ETA=10:27:43, max mem: 20.9 GB 
[11/22 14:12:51 visual_prompt]: 	Training 200/553. train loss: 94.1435,	0.8253 s / batch. (data: 5.90e-04). ETA=10:05:47, max mem: 20.9 GB 
[11/22 14:14:32 visual_prompt]: 	Training 300/553. train loss: 185.8542,	0.9040 s / batch. (data: 9.46e-02). ETA=11:02:02, max mem: 20.9 GB 
[11/22 14:16:14 visual_prompt]: 	Training 400/553. train loss: 1.6639,	0.8333 s / batch. (data: 1.05e-02). ETA=10:08:50, max mem: 20.9 GB 
[11/22 14:17:56 visual_prompt]: 	Training 500/553. train loss: 20.4397,	0.8211 s / batch. (data: 3.72e-03). ETA=9:58:35, max mem: 20.9 GB 
[11/22 14:18:49 visual_prompt]: Epoch 21 / 100: avg data time: 2.01e-01, avg batch time: 1.0238, average train loss: 36.2581
[11/22 14:19:47 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3090, average loss: 52.4051
[11/22 14:19:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.97	
[11/22 14:19:47 visual_prompt]: Stopping early.
[11/22 14:19:47 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 14:19:47 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 14:19:47 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/22 14:19:47 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/22 14:19:47 visual_prompt]: Training with config:
[11/22 14:19:47 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/22 14:19:47 visual_prompt]: Loading training data...
[11/22 14:19:47 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 14:19:47 visual_prompt]: Loading validation data...
[11/22 14:19:47 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 14:19:47 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 14:19:52 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/22 14:19:52 visual_prompt]: tuned percent:0.525
[11/22 14:19:52 visual_prompt]: Device used for model: 0
[11/22 14:19:52 visual_prompt]: Setting up Evaluator...
[11/22 14:19:52 visual_prompt]: Setting up Trainer...
[11/22 14:19:52 visual_prompt]: 	Setting up the optimizer...
[11/22 14:19:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 14:21:37 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8181 s / batch. (data: 2.97e-04). ETA=12:32:37, max mem: 20.9 GB 
[11/22 14:23:17 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8359 s / batch. (data: 4.91e-04). ETA=12:47:39, max mem: 20.9 GB 
[11/22 14:25:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.4248 s / batch. (data: 5.99e-01). ETA=21:46:06, max mem: 20.9 GB 
[11/22 14:26:42 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8101 s / batch. (data: 3.30e-04). ETA=12:21:16, max mem: 20.9 GB 
[11/22 14:28:26 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8360 s / batch. (data: 3.19e-04). ETA=12:43:31, max mem: 20.9 GB 
[11/22 14:29:20 visual_prompt]: Epoch 1 / 100: avg data time: 2.01e-01, avg batch time: 1.0270, average train loss: 1.5403
[11/22 14:30:18 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3071, average loss: 1.5201
[11/22 14:30:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/22 14:30:18 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/22 14:32:03 visual_prompt]: 	Training 100/553. train loss: 10.4643,	0.8240 s / batch. (data: 3.19e-04). ETA=12:30:30, max mem: 20.9 GB 
[11/22 14:33:44 visual_prompt]: 	Training 200/553. train loss: 0.0010,	0.8320 s / batch. (data: 3.31e-04). ETA=12:36:21, max mem: 20.9 GB 
[11/22 14:35:28 visual_prompt]: 	Training 300/553. train loss: 5.4646,	1.1007 s / batch. (data: 2.77e-01). ETA=16:38:48, max mem: 20.9 GB 
[11/22 14:37:09 visual_prompt]: 	Training 400/553. train loss: 5.3334,	0.8204 s / batch. (data: 3.13e-04). ETA=12:23:08, max mem: 20.9 GB 
[11/22 14:38:52 visual_prompt]: 	Training 500/553. train loss: 9.4859,	0.8124 s / batch. (data: 3.31e-04). ETA=12:14:30, max mem: 20.9 GB 
[11/22 14:39:43 visual_prompt]: Epoch 2 / 100: avg data time: 1.95e-01, avg batch time: 1.0210, average train loss: 4.9669
[11/22 14:40:41 visual_prompt]: Inference (val):avg data time: 4.02e-04, avg batch time: 0.3068, average loss: 17.1788
[11/22 14:40:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.88	
[11/22 14:40:41 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/22 14:42:25 visual_prompt]: 	Training 100/553. train loss: 5.1646,	0.8163 s / batch. (data: 3.19e-04). ETA=12:15:56, max mem: 20.9 GB 
[11/22 14:44:08 visual_prompt]: 	Training 200/553. train loss: 2.0174,	0.8197 s / batch. (data: 3.30e-04). ETA=12:17:40, max mem: 20.9 GB 
[11/22 14:45:50 visual_prompt]: 	Training 300/553. train loss: 3.7236,	0.8091 s / batch. (data: 3.22e-04). ETA=12:06:48, max mem: 20.9 GB 
[11/22 14:47:31 visual_prompt]: 	Training 400/553. train loss: 2.2168,	0.8276 s / batch. (data: 5.50e-03). ETA=12:21:59, max mem: 20.9 GB 
[11/22 14:49:15 visual_prompt]: 	Training 500/553. train loss: 6.8552,	1.4886 s / batch. (data: 6.50e-01). ETA=22:12:09, max mem: 20.9 GB 
[11/22 14:50:07 visual_prompt]: Epoch 3 / 100: avg data time: 1.97e-01, avg batch time: 1.0221, average train loss: 5.8513
[11/22 14:51:05 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3075, average loss: 14.7225
[11/22 14:51:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.89	
[11/22 14:51:05 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/22 14:52:52 visual_prompt]: 	Training 100/553. train loss: 0.7517,	0.8479 s / batch. (data: 9.23e-04). ETA=12:36:39, max mem: 20.9 GB 
[11/22 14:54:33 visual_prompt]: 	Training 200/553. train loss: 6.1720,	0.8324 s / batch. (data: 5.47e-03). ETA=12:21:26, max mem: 20.9 GB 
[11/22 14:56:15 visual_prompt]: 	Training 300/553. train loss: 2.6758,	1.6119 s / batch. (data: 7.91e-01). ETA=23:53:02, max mem: 20.9 GB 
[11/22 14:57:52 visual_prompt]: 	Training 400/553. train loss: 13.2228,	1.4241 s / batch. (data: 6.03e-01). ETA=21:03:38, max mem: 20.9 GB 
[11/22 14:59:36 visual_prompt]: 	Training 500/553. train loss: 0.6358,	3.6333 s / batch. (data: 2.81e+00). ETA=2 days, 5:37:56, max mem: 20.9 GB 
[11/22 15:00:29 visual_prompt]: Epoch 4 / 100: avg data time: 1.94e-01, avg batch time: 1.0207, average train loss: 10.6225
[11/22 15:01:28 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3078, average loss: 5.6601
[11/22 15:01:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.18	
[11/22 15:01:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/22 15:03:12 visual_prompt]: 	Training 100/553. train loss: 28.5359,	0.8391 s / batch. (data: 1.05e-02). ETA=12:21:01, max mem: 20.9 GB 
[11/22 15:04:54 visual_prompt]: 	Training 200/553. train loss: 2.2963,	1.3199 s / batch. (data: 4.94e-01). ETA=19:23:27, max mem: 20.9 GB 
[11/22 15:06:37 visual_prompt]: 	Training 300/553. train loss: 5.5521,	0.8400 s / batch. (data: 3.14e-04). ETA=12:19:02, max mem: 20.9 GB 
[11/22 15:08:18 visual_prompt]: 	Training 400/553. train loss: 3.1498,	0.8244 s / batch. (data: 3.04e-04). ETA=12:03:54, max mem: 20.9 GB 
[11/22 15:09:59 visual_prompt]: 	Training 500/553. train loss: 14.7823,	0.8168 s / batch. (data: 3.06e-04). ETA=11:55:54, max mem: 20.9 GB 
[11/22 15:10:53 visual_prompt]: Epoch 5 / 100: avg data time: 1.96e-01, avg batch time: 1.0221, average train loss: 11.0127
[11/22 15:11:52 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3072, average loss: 19.4735
[11/22 15:11:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.91	
[11/22 15:11:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/22 15:13:39 visual_prompt]: 	Training 100/553. train loss: 5.0224,	0.8358 s / batch. (data: 8.75e-04). ETA=12:10:26, max mem: 20.9 GB 
[11/22 15:15:20 visual_prompt]: 	Training 200/553. train loss: 19.3753,	0.8279 s / batch. (data: 3.82e-04). ETA=12:02:07, max mem: 20.9 GB 
[11/22 15:17:00 visual_prompt]: 	Training 300/553. train loss: 3.0734,	0.8106 s / batch. (data: 3.44e-04). ETA=11:45:40, max mem: 20.9 GB 
[11/22 15:18:46 visual_prompt]: 	Training 400/553. train loss: 9.9597,	0.8480 s / batch. (data: 3.36e-04). ETA=12:16:50, max mem: 20.9 GB 
[11/22 15:20:26 visual_prompt]: 	Training 500/553. train loss: 16.4667,	0.8188 s / batch. (data: 3.67e-04). ETA=11:50:08, max mem: 20.9 GB 
[11/22 15:21:19 visual_prompt]: Epoch 6 / 100: avg data time: 1.99e-01, avg batch time: 1.0251, average train loss: 10.0695
[11/22 15:22:17 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.3080, average loss: 8.6839
[11/22 15:22:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.73	
[11/22 15:22:17 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/22 15:24:02 visual_prompt]: 	Training 100/553. train loss: 8.0542,	0.8543 s / batch. (data: 5.44e-03). ETA=12:18:44, max mem: 20.9 GB 
[11/22 15:25:43 visual_prompt]: 	Training 200/553. train loss: 2.1701,	0.8124 s / batch. (data: 3.40e-04). ETA=11:41:07, max mem: 20.9 GB 
[11/22 15:27:29 visual_prompt]: 	Training 300/553. train loss: 27.3659,	1.9560 s / batch. (data: 1.13e+00). ETA=1 day, 4:04:51, max mem: 20.9 GB 
[11/22 15:29:10 visual_prompt]: 	Training 400/553. train loss: 0.6175,	2.1351 s / batch. (data: 1.28e+00). ETA=1 day, 6:35:33, max mem: 20.9 GB 
[11/22 15:30:49 visual_prompt]: 	Training 500/553. train loss: 5.7167,	0.8680 s / batch. (data: 3.35e-04). ETA=12:24:46, max mem: 20.9 GB 
[11/22 15:31:41 visual_prompt]: Epoch 7 / 100: avg data time: 1.94e-01, avg batch time: 1.0198, average train loss: 12.7131
[11/22 15:32:40 visual_prompt]: Inference (val):avg data time: 1.96e-04, avg batch time: 0.3083, average loss: 3.9841
[11/22 15:32:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.10	
[11/22 15:32:40 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/22 15:34:23 visual_prompt]: 	Training 100/553. train loss: 7.3276,	0.8278 s / batch. (data: 5.46e-03). ETA=11:48:12, max mem: 20.9 GB 
[11/22 15:36:07 visual_prompt]: 	Training 200/553. train loss: 51.3404,	0.8355 s / batch. (data: 3.33e-04). ETA=11:53:22, max mem: 20.9 GB 
[11/22 15:37:49 visual_prompt]: 	Training 300/553. train loss: 0.8880,	0.8303 s / batch. (data: 3.27e-04). ETA=11:47:31, max mem: 20.9 GB 
[11/22 15:39:31 visual_prompt]: 	Training 400/553. train loss: 19.9931,	1.0325 s / batch. (data: 1.89e-01). ETA=14:38:07, max mem: 20.9 GB 
[11/22 15:41:13 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.5746 s / batch. (data: 7.51e-01). ETA=22:16:34, max mem: 20.9 GB 
[11/22 15:42:06 visual_prompt]: Epoch 8 / 100: avg data time: 1.99e-01, avg batch time: 1.0235, average train loss: 25.0387
[11/22 15:43:04 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.3081, average loss: 11.9663
[11/22 15:43:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.23	
[11/22 15:43:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/22 15:44:49 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8135 s / batch. (data: 5.45e-03). ETA=11:28:25, max mem: 20.9 GB 
[11/22 15:46:30 visual_prompt]: 	Training 200/553. train loss: 18.9466,	0.8238 s / batch. (data: 7.61e-03). ETA=11:35:47, max mem: 20.9 GB 
[11/22 15:48:12 visual_prompt]: 	Training 300/553. train loss: 1.4170,	1.8080 s / batch. (data: 9.77e-01). ETA=1 day, 1:24:01, max mem: 20.9 GB 
[11/22 15:49:55 visual_prompt]: 	Training 400/553. train loss: 2.0662,	0.8240 s / batch. (data: 8.31e-04). ETA=11:33:13, max mem: 20.9 GB 
[11/22 15:51:37 visual_prompt]: 	Training 500/553. train loss: 12.9637,	0.8297 s / batch. (data: 3.32e-04). ETA=11:36:35, max mem: 20.9 GB 
[11/22 15:52:29 visual_prompt]: Epoch 9 / 100: avg data time: 1.96e-01, avg batch time: 1.0208, average train loss: 21.0203
[11/22 15:53:27 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3069, average loss: 80.6455
[11/22 15:53:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.99	
[11/22 15:53:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/22 15:55:16 visual_prompt]: 	Training 100/553. train loss: 54.4525,	0.8280 s / batch. (data: 8.47e-04). ETA=11:33:04, max mem: 20.9 GB 
[11/22 15:56:56 visual_prompt]: 	Training 200/553. train loss: 18.4336,	0.8280 s / batch. (data: 2.99e-04). ETA=11:31:41, max mem: 20.9 GB 
[11/22 15:58:37 visual_prompt]: 	Training 300/553. train loss: 29.5771,	0.8178 s / batch. (data: 3.22e-04). ETA=11:21:47, max mem: 20.9 GB 
[11/22 16:00:16 visual_prompt]: 	Training 400/553. train loss: 51.3899,	0.8360 s / batch. (data: 1.20e-02). ETA=11:35:35, max mem: 20.9 GB 
[11/22 16:01:59 visual_prompt]: 	Training 500/553. train loss: 3.2221,	0.8880 s / batch. (data: 7.31e-02). ETA=12:17:22, max mem: 20.9 GB 
[11/22 16:02:52 visual_prompt]: Epoch 10 / 100: avg data time: 1.97e-01, avg batch time: 1.0211, average train loss: 23.4597
[11/22 16:03:51 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3086, average loss: 35.3130
[11/22 16:03:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.92	
[11/22 16:03:51 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/22 16:05:39 visual_prompt]: 	Training 100/553. train loss: 49.0889,	0.8131 s / batch. (data: 3.20e-04). ETA=11:13:05, max mem: 20.9 GB 
[11/22 16:07:22 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8238 s / batch. (data: 7.94e-03). ETA=11:20:34, max mem: 20.9 GB 
[11/22 16:09:03 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2871 s / batch. (data: 1.49e+00). ETA=1 day, 7:25:43, max mem: 20.9 GB 
[11/22 16:10:43 visual_prompt]: 	Training 400/553. train loss: 47.5715,	0.8466 s / batch. (data: 2.46e-02). ETA=11:36:39, max mem: 20.9 GB 
[11/22 16:12:23 visual_prompt]: 	Training 500/553. train loss: 90.2490,	0.8116 s / batch. (data: 3.27e-04). ETA=11:06:29, max mem: 20.9 GB 
[11/22 16:13:15 visual_prompt]: Epoch 11 / 100: avg data time: 1.99e-01, avg batch time: 1.0207, average train loss: 40.8844
[11/22 16:14:14 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3067, average loss: 19.9013
[11/22 16:14:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.81	
[11/22 16:14:14 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/22 16:16:00 visual_prompt]: 	Training 100/553. train loss: 33.3353,	0.8200 s / batch. (data: 3.26e-04). ETA=11:11:14, max mem: 20.9 GB 
[11/22 16:17:43 visual_prompt]: 	Training 200/553. train loss: 8.5810,	1.6387 s / batch. (data: 8.22e-01). ETA=22:18:45, max mem: 20.9 GB 
[11/22 16:19:23 visual_prompt]: 	Training 300/553. train loss: 37.9235,	0.8125 s / batch. (data: 4.68e-04). ETA=11:02:24, max mem: 20.9 GB 
[11/22 16:21:05 visual_prompt]: 	Training 400/553. train loss: 5.5246,	0.8204 s / batch. (data: 7.97e-03). ETA=11:07:27, max mem: 20.9 GB 
[11/22 16:22:47 visual_prompt]: 	Training 500/553. train loss: 361.9593,	0.8320 s / batch. (data: 1.20e-02). ETA=11:15:32, max mem: 20.9 GB 
[11/22 16:23:39 visual_prompt]: Epoch 12 / 100: avg data time: 1.98e-01, avg batch time: 1.0218, average train loss: 33.7319
[11/22 16:24:37 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3082, average loss: 6.8668
[11/22 16:24:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.22	
[11/22 16:24:37 visual_prompt]: Best epoch 12: best metric: -6.867
[11/22 16:24:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/22 16:26:24 visual_prompt]: 	Training 100/553. train loss: 23.7033,	0.8347 s / batch. (data: 1.87e-02). ETA=11:15:35, max mem: 20.9 GB 
[11/22 16:28:02 visual_prompt]: 	Training 200/553. train loss: 24.2083,	0.8596 s / batch. (data: 4.49e-04). ETA=11:34:20, max mem: 20.9 GB 
[11/22 16:29:45 visual_prompt]: 	Training 300/553. train loss: 11.9505,	1.8640 s / batch. (data: 1.03e+00). ETA=1 day, 1:02:29, max mem: 20.9 GB 
[11/22 16:31:25 visual_prompt]: 	Training 400/553. train loss: 68.1326,	0.8360 s / batch. (data: 7.99e-03). ETA=11:12:29, max mem: 20.9 GB 
[11/22 16:33:08 visual_prompt]: 	Training 500/553. train loss: 43.9326,	0.8360 s / batch. (data: 3.86e-04). ETA=11:11:04, max mem: 20.9 GB 
[11/22 16:34:01 visual_prompt]: Epoch 13 / 100: avg data time: 1.96e-01, avg batch time: 1.0200, average train loss: 31.1751
[11/22 16:34:59 visual_prompt]: Inference (val):avg data time: 2.24e-04, avg batch time: 0.3075, average loss: 9.2663
[11/22 16:34:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.03	
[11/22 16:34:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/22 16:36:46 visual_prompt]: 	Training 100/553. train loss: 9.2130,	0.8280 s / batch. (data: 7.97e-03). ETA=11:02:33, max mem: 20.9 GB 
[11/22 16:38:28 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.5800 s / batch. (data: 7.33e-01). ETA=21:01:40, max mem: 20.9 GB 
[11/22 16:40:09 visual_prompt]: 	Training 300/553. train loss: 19.4283,	0.8494 s / batch. (data: 1.68e-02). ETA=11:16:51, max mem: 20.9 GB 
[11/22 16:41:50 visual_prompt]: 	Training 400/553. train loss: 13.2197,	0.8236 s / batch. (data: 3.09e-04). ETA=10:54:56, max mem: 20.9 GB 
[11/22 16:43:31 visual_prompt]: 	Training 500/553. train loss: 6.1606,	0.8367 s / batch. (data: 3.28e-04). ETA=11:03:56, max mem: 20.9 GB 
[11/22 16:44:23 visual_prompt]: Epoch 14 / 100: avg data time: 1.94e-01, avg batch time: 1.0185, average train loss: 30.6143
[11/22 16:45:21 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3091, average loss: 9.9501
[11/22 16:45:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/22 16:45:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/22 16:47:07 visual_prompt]: 	Training 100/553. train loss: 16.6267,	0.8163 s / batch. (data: 3.28e-04). ETA=10:45:39, max mem: 20.9 GB 
[11/22 16:48:47 visual_prompt]: 	Training 200/553. train loss: 84.0703,	0.8325 s / batch. (data: 3.17e-04). ETA=10:57:05, max mem: 20.9 GB 
[11/22 16:50:31 visual_prompt]: 	Training 300/553. train loss: 69.5441,	0.8312 s / batch. (data: 8.36e-04). ETA=10:54:42, max mem: 20.9 GB 
[11/22 16:52:10 visual_prompt]: 	Training 400/553. train loss: 11.5677,	0.8470 s / batch. (data: 1.50e-02). ETA=11:05:40, max mem: 20.9 GB 
[11/22 16:53:52 visual_prompt]: 	Training 500/553. train loss: 1.6814,	0.8640 s / batch. (data: 3.18e-04). ETA=11:17:37, max mem: 20.9 GB 
[11/22 16:54:45 visual_prompt]: Epoch 15 / 100: avg data time: 1.95e-01, avg batch time: 1.0192, average train loss: 28.5416
[11/22 16:55:43 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3079, average loss: 5.6738
[11/22 16:55:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.66	
[11/22 16:55:43 visual_prompt]: Best epoch 15: best metric: -5.674
[11/22 16:55:43 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/22 16:57:29 visual_prompt]: 	Training 100/553. train loss: 2.0761,	0.8197 s / batch. (data: 3.32e-04). ETA=10:40:49, max mem: 20.9 GB 
[11/22 16:59:12 visual_prompt]: 	Training 200/553. train loss: 74.7176,	0.8123 s / batch. (data: 5.45e-03). ETA=10:33:37, max mem: 20.9 GB 
[11/22 17:01:01 visual_prompt]: 	Training 300/553. train loss: 70.4559,	0.8188 s / batch. (data: 3.05e-04). ETA=10:37:23, max mem: 20.9 GB 
[11/22 17:02:48 visual_prompt]: 	Training 400/553. train loss: 32.9066,	0.8291 s / batch. (data: 3.18e-04). ETA=10:43:59, max mem: 20.9 GB 
[11/22 17:04:30 visual_prompt]: 	Training 500/553. train loss: 10.4075,	1.4563 s / batch. (data: 6.32e-01). ETA=18:48:46, max mem: 20.9 GB 
[11/22 17:05:23 visual_prompt]: Epoch 16 / 100: avg data time: 2.25e-01, avg batch time: 1.0480, average train loss: 35.6242
[11/22 17:06:22 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3070, average loss: 27.4504
[11/22 17:06:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.72	
[11/22 17:06:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/22 17:08:08 visual_prompt]: 	Training 100/553. train loss: 10.0498,	0.8157 s / batch. (data: 3.12e-04). ETA=10:30:08, max mem: 20.9 GB 
[11/22 17:09:53 visual_prompt]: 	Training 200/553. train loss: 2.1062,	0.8159 s / batch. (data: 3.10e-04). ETA=10:28:58, max mem: 20.9 GB 
[11/22 17:11:33 visual_prompt]: 	Training 300/553. train loss: 47.0998,	0.8309 s / batch. (data: 1.05e-02). ETA=10:39:06, max mem: 20.9 GB 
[11/22 17:13:14 visual_prompt]: 	Training 400/553. train loss: 48.3595,	1.2772 s / batch. (data: 4.68e-01). ETA=16:20:15, max mem: 20.9 GB 
[11/22 17:14:56 visual_prompt]: 	Training 500/553. train loss: 1.4778,	1.8225 s / batch. (data: 1.00e+00). ETA=23:15:48, max mem: 20.9 GB 
[11/22 17:15:50 visual_prompt]: Epoch 17 / 100: avg data time: 2.04e-01, avg batch time: 1.0275, average train loss: 27.8454
[11/22 17:16:49 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3065, average loss: 27.0778
[11/22 17:16:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.98	
[11/22 17:16:49 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/22 17:18:34 visual_prompt]: 	Training 100/553. train loss: 38.1946,	0.8309 s / batch. (data: 1.21e-02). ETA=10:34:13, max mem: 20.9 GB 
[11/22 17:20:19 visual_prompt]: 	Training 200/553. train loss: 16.9148,	0.8239 s / batch. (data: 1.20e-02). ETA=10:27:31, max mem: 20.9 GB 
[11/22 17:22:01 visual_prompt]: 	Training 300/553. train loss: 4.3457,	0.8240 s / batch. (data: 3.29e-04). ETA=10:26:14, max mem: 20.9 GB 
[11/22 17:23:42 visual_prompt]: 	Training 400/553. train loss: 22.5713,	0.8477 s / batch. (data: 1.56e-02). ETA=10:42:50, max mem: 20.9 GB 
[11/22 17:25:24 visual_prompt]: 	Training 500/553. train loss: 4.5779,	0.8248 s / batch. (data: 3.20e-04). ETA=10:24:03, max mem: 20.9 GB 
[11/22 17:26:15 visual_prompt]: Epoch 18 / 100: avg data time: 2.02e-01, avg batch time: 1.0245, average train loss: 34.1536
[11/22 17:27:14 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.3079, average loss: 8.4810
[11/22 17:27:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[11/22 17:27:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/22 17:29:00 visual_prompt]: 	Training 100/553. train loss: 9.2575,	0.8760 s / batch. (data: 4.48e-02). ETA=11:00:35, max mem: 20.9 GB 
[11/22 17:30:42 visual_prompt]: 	Training 200/553. train loss: 21.5479,	0.8266 s / batch. (data: 3.08e-04). ETA=10:21:57, max mem: 20.9 GB 
[11/22 17:32:24 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8338 s / batch. (data: 5.46e-03). ETA=10:25:58, max mem: 20.9 GB 
[11/22 17:34:07 visual_prompt]: 	Training 400/553. train loss: 9.2315,	0.8557 s / batch. (data: 1.05e-02). ETA=10:41:02, max mem: 20.9 GB 
[11/22 17:35:44 visual_prompt]: 	Training 500/553. train loss: 0.2463,	0.8100 s / batch. (data: 3.25e-04). ETA=10:05:25, max mem: 20.9 GB 
[11/22 17:36:38 visual_prompt]: Epoch 19 / 100: avg data time: 1.95e-01, avg batch time: 1.0198, average train loss: 26.0521
[11/22 17:37:36 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3082, average loss: 82.6336
[11/22 17:37:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.47	
[11/22 17:37:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/22 17:39:20 visual_prompt]: 	Training 100/553. train loss: 8.7580,	0.8260 s / batch. (data: 3.27e-04). ETA=10:15:16, max mem: 20.9 GB 
[11/22 17:41:03 visual_prompt]: 	Training 200/553. train loss: 1.7225,	0.8240 s / batch. (data: 5.46e-03). ETA=10:12:23, max mem: 20.9 GB 
[11/22 17:42:44 visual_prompt]: 	Training 300/553. train loss: 5.0633,	0.8507 s / batch. (data: 1.20e-02). ETA=10:30:51, max mem: 20.9 GB 
[11/22 17:44:26 visual_prompt]: 	Training 400/553. train loss: 18.6988,	0.8277 s / batch. (data: 5.46e-03). ETA=10:12:23, max mem: 20.9 GB 
[11/22 17:46:07 visual_prompt]: 	Training 500/553. train loss: 0.4188,	0.8349 s / batch. (data: 3.27e-04). ETA=10:16:21, max mem: 20.9 GB 
[11/22 17:47:01 visual_prompt]: Epoch 20 / 100: avg data time: 1.97e-01, avg batch time: 1.0216, average train loss: 28.4255
[11/22 17:47:59 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3081, average loss: 20.9842
[11/22 17:47:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.62	
[11/22 17:47:59 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/22 17:49:47 visual_prompt]: 	Training 100/553. train loss: 31.0035,	0.8287 s / batch. (data: 5.45e-03). ETA=10:09:39, max mem: 20.9 GB 
[11/22 17:51:28 visual_prompt]: 	Training 200/553. train loss: 56.9735,	0.8320 s / batch. (data: 8.81e-04). ETA=10:10:40, max mem: 20.9 GB 
[11/22 17:53:09 visual_prompt]: 	Training 300/553. train loss: 62.3923,	1.1280 s / batch. (data: 2.91e-01). ETA=13:46:04, max mem: 20.9 GB 
[11/22 17:54:49 visual_prompt]: 	Training 400/553. train loss: 42.8589,	0.8565 s / batch. (data: 3.08e-02). ETA=10:25:49, max mem: 20.9 GB 
[11/22 17:56:32 visual_prompt]: 	Training 500/553. train loss: 15.1508,	0.8360 s / batch. (data: 5.50e-03). ETA=10:09:27, max mem: 20.9 GB 
[11/22 17:57:24 visual_prompt]: Epoch 21 / 100: avg data time: 1.97e-01, avg batch time: 1.0212, average train loss: 22.8141
[11/22 17:58:23 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.3089, average loss: 1.3783
[11/22 17:58:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/22 17:58:23 visual_prompt]: Best epoch 21: best metric: -1.378
[11/22 17:58:23 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/22 18:00:08 visual_prompt]: 	Training 100/553. train loss: 19.0938,	0.8448 s / batch. (data: 1.56e-02). ETA=10:13:41, max mem: 20.9 GB 
[11/22 18:01:50 visual_prompt]: 	Training 200/553. train loss: 4.2181,	0.8240 s / batch. (data: 3.13e-04). ETA=9:57:13, max mem: 20.9 GB 
[11/22 18:03:29 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8232 s / batch. (data: 1.20e-02). ETA=9:55:15, max mem: 20.9 GB 
[11/22 18:05:12 visual_prompt]: 	Training 400/553. train loss: 35.0689,	0.8520 s / batch. (data: 3.27e-04). ETA=10:14:39, max mem: 20.9 GB 
[11/22 18:06:53 visual_prompt]: 	Training 500/553. train loss: 61.7992,	0.8165 s / batch. (data: 3.04e-04). ETA=9:47:42, max mem: 20.9 GB 
[11/22 18:07:48 visual_prompt]: Epoch 22 / 100: avg data time: 1.96e-01, avg batch time: 1.0213, average train loss: 28.6100
[11/22 18:08:46 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3077, average loss: 40.7791
[11/22 18:08:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.47	
[11/22 18:08:46 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/22 18:10:33 visual_prompt]: 	Training 100/553. train loss: 17.7667,	0.8319 s / batch. (data: 8.31e-03). ETA=9:56:38, max mem: 20.9 GB 
[11/22 18:12:15 visual_prompt]: 	Training 200/553. train loss: 75.8020,	0.8154 s / batch. (data: 3.80e-04). ETA=9:43:29, max mem: 20.9 GB 
[11/22 18:13:59 visual_prompt]: 	Training 300/553. train loss: 12.9878,	0.8480 s / batch. (data: 8.44e-04). ETA=10:05:24, max mem: 20.9 GB 
[11/22 18:15:39 visual_prompt]: 	Training 400/553. train loss: 5.6556,	0.8252 s / batch. (data: 8.99e-04). ETA=9:47:46, max mem: 20.9 GB 
[11/22 18:17:18 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.21e-04). ETA=9:51:12, max mem: 20.9 GB 
[11/22 18:18:11 visual_prompt]: Epoch 23 / 100: avg data time: 1.96e-01, avg batch time: 1.0213, average train loss: 26.6718
[11/22 18:19:09 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3087, average loss: 2.5826
[11/22 18:19:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.97	
[11/22 18:19:09 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/22 18:20:52 visual_prompt]: 	Training 100/553. train loss: 99.1741,	0.8240 s / batch. (data: 3.58e-04). ETA=9:43:24, max mem: 20.9 GB 
[11/22 18:22:34 visual_prompt]: 	Training 200/553. train loss: 30.3081,	0.8253 s / batch. (data: 2.94e-04). ETA=9:42:57, max mem: 20.9 GB 
[11/22 18:24:16 visual_prompt]: 	Training 300/553. train loss: 11.2261,	1.0477 s / batch. (data: 2.19e-01). ETA=12:18:18, max mem: 20.9 GB 
[11/22 18:25:58 visual_prompt]: 	Training 400/553. train loss: 1.6055,	0.8360 s / batch. (data: 3.21e-04). ETA=9:47:44, max mem: 20.9 GB 
[11/22 18:27:42 visual_prompt]: 	Training 500/553. train loss: 91.2325,	0.8195 s / batch. (data: 3.25e-04). ETA=9:34:43, max mem: 20.9 GB 
[11/22 18:28:36 visual_prompt]: Epoch 24 / 100: avg data time: 2.01e-01, avg batch time: 1.0239, average train loss: 29.2039
[11/22 18:29:34 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.3101, average loss: 2.2943
[11/22 18:29:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.61	
[11/22 18:29:34 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/22 18:31:23 visual_prompt]: 	Training 100/553. train loss: 11.0729,	0.8360 s / batch. (data: 5.65e-03). ETA=9:44:11, max mem: 20.9 GB 
[11/22 18:33:02 visual_prompt]: 	Training 200/553. train loss: 41.8671,	0.8248 s / batch. (data: 7.96e-03). ETA=9:34:58, max mem: 20.9 GB 
[11/22 18:34:42 visual_prompt]: 	Training 300/553. train loss: 31.9652,	1.3029 s / batch. (data: 4.95e-01). ETA=15:06:05, max mem: 20.9 GB 
[11/22 18:36:24 visual_prompt]: 	Training 400/553. train loss: 5.9064,	1.2161 s / batch. (data: 4.01e-01). ETA=14:03:44, max mem: 20.9 GB 
[11/22 18:38:06 visual_prompt]: 	Training 500/553. train loss: 37.1354,	1.5520 s / batch. (data: 7.18e-01). ETA=17:54:11, max mem: 20.9 GB 
[11/22 18:38:59 visual_prompt]: Epoch 25 / 100: avg data time: 1.98e-01, avg batch time: 1.0213, average train loss: 39.3230
[11/22 18:39:57 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3061, average loss: 95.5960
[11/22 18:39:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.02	
[11/22 18:39:57 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/22 18:41:43 visual_prompt]: 	Training 100/553. train loss: 20.8582,	0.8088 s / batch. (data: 3.35e-04). ETA=9:17:45, max mem: 20.9 GB 
[11/22 18:43:25 visual_prompt]: 	Training 200/553. train loss: 25.1038,	1.8201 s / batch. (data: 1.01e+00). ETA=20:52:02, max mem: 20.9 GB 
[11/22 18:45:09 visual_prompt]: 	Training 300/553. train loss: 26.8676,	0.8200 s / batch. (data: 3.15e-04). ETA=9:22:43, max mem: 20.9 GB 
[11/22 18:46:49 visual_prompt]: 	Training 400/553. train loss: 48.3365,	0.8134 s / batch. (data: 3.10e-04). ETA=9:16:48, max mem: 20.9 GB 
[11/22 18:48:28 visual_prompt]: 	Training 500/553. train loss: 8.3600,	0.8400 s / batch. (data: 7.95e-03). ETA=9:33:38, max mem: 20.9 GB 
[11/22 18:49:21 visual_prompt]: Epoch 26 / 100: avg data time: 1.96e-01, avg batch time: 1.0193, average train loss: 25.6008
[11/22 18:50:20 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3076, average loss: 31.2810
[11/22 18:50:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.85	
[11/22 18:50:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/22 18:52:06 visual_prompt]: 	Training 100/553. train loss: 1.9237,	0.8320 s / batch. (data: 7.96e-03). ETA=9:26:02, max mem: 20.9 GB 
[11/22 18:53:47 visual_prompt]: 	Training 200/553. train loss: 16.5352,	1.4440 s / batch. (data: 6.15e-01). ETA=16:20:01, max mem: 20.9 GB 
[11/22 18:55:28 visual_prompt]: 	Training 300/553. train loss: 26.6875,	0.8142 s / batch. (data: 5.49e-03). ETA=9:11:14, max mem: 20.9 GB 
[11/22 18:57:11 visual_prompt]: 	Training 400/553. train loss: 22.0815,	0.8159 s / batch. (data: 3.14e-04). ETA=9:11:03, max mem: 20.9 GB 
[11/22 18:58:53 visual_prompt]: 	Training 500/553. train loss: 22.2407,	0.8330 s / batch. (data: 9.48e-04). ETA=9:21:10, max mem: 20.9 GB 
[11/22 18:59:45 visual_prompt]: Epoch 27 / 100: avg data time: 1.97e-01, avg batch time: 1.0215, average train loss: 29.5681
[11/22 19:00:43 visual_prompt]: Inference (val):avg data time: 4.81e-04, avg batch time: 0.3073, average loss: 9.5758
[11/22 19:00:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.81	
[11/22 19:00:43 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/22 19:02:27 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.9658 s / batch. (data: 1.34e-01). ETA=10:48:12, max mem: 20.9 GB 
[11/22 19:04:10 visual_prompt]: 	Training 200/553. train loss: 6.5409,	0.8160 s / batch. (data: 3.36e-04). ETA=9:06:19, max mem: 20.9 GB 
[11/22 19:05:53 visual_prompt]: 	Training 300/553. train loss: 50.0483,	1.7720 s / batch. (data: 9.45e-01). ETA=19:43:22, max mem: 20.9 GB 
[11/22 19:07:33 visual_prompt]: 	Training 400/553. train loss: 83.2198,	0.8163 s / batch. (data: 3.10e-04). ETA=9:03:48, max mem: 20.9 GB 
[11/22 19:09:13 visual_prompt]: 	Training 500/553. train loss: 151.5318,	0.8400 s / batch. (data: 3.03e-04). ETA=9:18:09, max mem: 20.9 GB 
[11/22 19:10:07 visual_prompt]: Epoch 28 / 100: avg data time: 1.97e-01, avg batch time: 1.0199, average train loss: 33.4609
[11/22 19:11:05 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3074, average loss: 80.3306
[11/22 19:11:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.43	
[11/22 19:11:05 visual_prompt]: Stopping early.
[11/22 19:11:06 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 19:11:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 19:11:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/22 19:11:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/22 19:11:06 visual_prompt]: Training with config:
[11/22 19:11:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/22 19:11:06 visual_prompt]: Loading training data...
[11/22 19:11:06 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 19:11:06 visual_prompt]: Loading validation data...
[11/22 19:11:06 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 19:11:06 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 19:11:11 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/22 19:11:11 visual_prompt]: tuned percent:0.525
[11/22 19:11:11 visual_prompt]: Device used for model: 0
[11/22 19:11:11 visual_prompt]: Setting up Evaluator...
[11/22 19:11:11 visual_prompt]: Setting up Trainer...
[11/22 19:11:11 visual_prompt]: 	Setting up the optimizer...
[11/22 19:11:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 19:12:55 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8240 s / batch. (data: 1.20e-02). ETA=12:38:03, max mem: 20.9 GB 
[11/22 19:14:35 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8308 s / batch. (data: 1.47e-02). ETA=12:42:55, max mem: 20.9 GB 
[11/22 19:16:19 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.6947 s / batch. (data: 8.71e-01). ETA=1 day, 1:53:26, max mem: 20.9 GB 
[11/22 19:17:58 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8361 s / batch. (data: 3.28e-04). ETA=12:45:00, max mem: 20.9 GB 
[11/22 19:19:42 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8240 s / batch. (data: 3.17e-04). ETA=12:32:35, max mem: 20.9 GB 
[11/22 19:20:36 visual_prompt]: Epoch 1 / 100: avg data time: 1.96e-01, avg batch time: 1.0224, average train loss: 1.5403
[11/22 19:21:34 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3082, average loss: 1.5201
[11/22 19:21:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/22 19:21:34 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/22 19:23:19 visual_prompt]: 	Training 100/553. train loss: 2.4547,	0.9953 s / batch. (data: 1.77e-01). ETA=15:06:28, max mem: 20.9 GB 
[11/22 19:25:01 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2890 s / batch. (data: 4.49e-01). ETA=19:31:52, max mem: 20.9 GB 
[11/22 19:26:44 visual_prompt]: 	Training 300/553. train loss: 7.6994,	1.2867 s / batch. (data: 4.50e-01). ETA=19:27:37, max mem: 20.9 GB 
[11/22 19:28:25 visual_prompt]: 	Training 400/553. train loss: 0.7945,	0.8400 s / batch. (data: 8.01e-03). ETA=12:40:51, max mem: 20.9 GB 
[11/22 19:30:09 visual_prompt]: 	Training 500/553. train loss: 0.8968,	0.8380 s / batch. (data: 1.05e-02). ETA=12:37:38, max mem: 20.9 GB 
[11/22 19:31:01 visual_prompt]: Epoch 2 / 100: avg data time: 1.98e-01, avg batch time: 1.0242, average train loss: 4.2426
[11/22 19:31:59 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3073, average loss: 0.7099
[11/22 19:31:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 59.89	
[11/22 19:31:59 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/22 19:33:44 visual_prompt]: 	Training 100/553. train loss: 6.4990,	0.8120 s / batch. (data: 3.32e-04). ETA=12:12:06, max mem: 20.9 GB 
[11/22 19:35:27 visual_prompt]: 	Training 200/553. train loss: 4.0049,	0.8169 s / batch. (data: 3.15e-04). ETA=12:15:07, max mem: 20.9 GB 
[11/22 19:37:08 visual_prompt]: 	Training 300/553. train loss: 0.8738,	0.8460 s / batch. (data: 1.56e-02). ETA=12:39:55, max mem: 20.9 GB 
[11/22 19:38:50 visual_prompt]: 	Training 400/553. train loss: 8.9774,	0.8360 s / batch. (data: 3.19e-04). ETA=12:29:30, max mem: 20.9 GB 
[11/22 19:40:33 visual_prompt]: 	Training 500/553. train loss: 7.3340,	1.3761 s / batch. (data: 5.53e-01). ETA=20:31:26, max mem: 20.9 GB 
[11/22 19:41:25 visual_prompt]: Epoch 3 / 100: avg data time: 1.96e-01, avg batch time: 1.0223, average train loss: 7.6226
[11/22 19:42:23 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3084, average loss: 5.2789
[11/22 19:42:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.01	
[11/22 19:42:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/22 19:44:10 visual_prompt]: 	Training 100/553. train loss: 7.9321,	0.8612 s / batch. (data: 1.05e-02). ETA=12:48:27, max mem: 20.9 GB 
[11/22 19:45:52 visual_prompt]: 	Training 200/553. train loss: 12.9808,	0.8320 s / batch. (data: 7.97e-03). ETA=12:21:01, max mem: 20.9 GB 
[11/22 19:47:35 visual_prompt]: 	Training 300/553. train loss: 8.3782,	1.6335 s / batch. (data: 8.22e-01). ETA=1 day, 0:12:12, max mem: 20.9 GB 
[11/22 19:49:12 visual_prompt]: 	Training 400/553. train loss: 0.5039,	0.8113 s / batch. (data: 3.34e-04). ETA=11:59:53, max mem: 20.9 GB 
[11/22 19:50:56 visual_prompt]: 	Training 500/553. train loss: 31.1022,	3.7395 s / batch. (data: 2.91e+00). ETA=2 days, 7:11:59, max mem: 20.9 GB 
[11/22 19:51:50 visual_prompt]: Epoch 4 / 100: avg data time: 1.99e-01, avg batch time: 1.0257, average train loss: 8.4598
[11/22 19:52:49 visual_prompt]: Inference (val):avg data time: 5.67e-04, avg batch time: 0.3085, average loss: 31.7809
[11/22 19:52:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/22 19:52:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/22 19:54:33 visual_prompt]: 	Training 100/553. train loss: 0.0982,	0.8347 s / batch. (data: 1.05e-02). ETA=12:17:09, max mem: 20.9 GB 
[11/22 19:56:15 visual_prompt]: 	Training 200/553. train loss: 5.2413,	1.2389 s / batch. (data: 4.12e-01). ETA=18:12:05, max mem: 20.9 GB 
[11/22 19:57:57 visual_prompt]: 	Training 300/553. train loss: 3.9706,	0.8361 s / batch. (data: 3.02e-04). ETA=12:15:34, max mem: 20.9 GB 
[11/22 19:59:37 visual_prompt]: 	Training 400/553. train loss: 0.8584,	0.8223 s / batch. (data: 3.09e-04). ETA=12:02:03, max mem: 20.9 GB 
[11/22 20:01:19 visual_prompt]: 	Training 500/553. train loss: 8.9242,	0.8160 s / batch. (data: 3.03e-04). ETA=11:55:09, max mem: 20.9 GB 
[11/22 20:02:13 visual_prompt]: Epoch 5 / 100: avg data time: 1.95e-01, avg batch time: 1.0203, average train loss: 10.1281
[11/22 20:03:11 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3082, average loss: 22.4373
[11/22 20:03:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/22 20:03:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/22 20:04:59 visual_prompt]: 	Training 100/553. train loss: 3.7443,	0.8606 s / batch. (data: 1.61e-02). ETA=12:32:07, max mem: 20.9 GB 
[11/22 20:06:40 visual_prompt]: 	Training 200/553. train loss: 5.5833,	0.8377 s / batch. (data: 2.76e-04). ETA=12:10:39, max mem: 20.9 GB 
[11/22 20:08:20 visual_prompt]: 	Training 300/553. train loss: 2.3348,	0.8268 s / batch. (data: 3.45e-04). ETA=11:59:47, max mem: 20.9 GB 
[11/22 20:10:06 visual_prompt]: 	Training 400/553. train loss: 26.8347,	0.8163 s / batch. (data: 3.35e-04). ETA=11:49:15, max mem: 20.9 GB 
[11/22 20:11:46 visual_prompt]: 	Training 500/553. train loss: 1.0403,	0.8470 s / batch. (data: 2.34e-02). ETA=12:14:35, max mem: 20.9 GB 
[11/22 20:12:38 visual_prompt]: Epoch 6 / 100: avg data time: 1.99e-01, avg batch time: 1.0249, average train loss: 11.6939
[11/22 20:13:37 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3068, average loss: 5.0968
[11/22 20:13:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.36	
[11/22 20:13:37 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/22 20:15:21 visual_prompt]: 	Training 100/553. train loss: 10.6278,	0.8288 s / batch. (data: 3.29e-04). ETA=11:56:38, max mem: 20.9 GB 
[11/22 20:17:02 visual_prompt]: 	Training 200/553. train loss: 8.8718,	0.8241 s / batch. (data: 3.28e-04). ETA=11:51:13, max mem: 20.9 GB 
[11/22 20:18:47 visual_prompt]: 	Training 300/553. train loss: 16.6753,	1.9172 s / batch. (data: 1.10e+00). ETA=1 day, 3:31:23, max mem: 20.9 GB 
[11/22 20:20:29 visual_prompt]: 	Training 400/553. train loss: 9.0358,	2.0645 s / batch. (data: 1.24e+00). ETA=1 day, 5:34:52, max mem: 20.9 GB 
[11/22 20:22:08 visual_prompt]: 	Training 500/553. train loss: 48.6467,	0.8399 s / batch. (data: 7.98e-03). ETA=12:00:40, max mem: 20.9 GB 
[11/22 20:23:01 visual_prompt]: Epoch 7 / 100: avg data time: 1.95e-01, avg batch time: 1.0205, average train loss: 14.1636
[11/22 20:23:59 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3078, average loss: 17.4630
[11/22 20:23:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.64	
[11/22 20:23:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/22 20:25:43 visual_prompt]: 	Training 100/553. train loss: 50.7500,	0.8276 s / batch. (data: 2.07e-02). ETA=11:48:02, max mem: 20.9 GB 
[11/22 20:27:26 visual_prompt]: 	Training 200/553. train loss: 3.8612,	0.8360 s / batch. (data: 1.20e-02). ETA=11:53:49, max mem: 20.9 GB 
[11/22 20:29:09 visual_prompt]: 	Training 300/553. train loss: 33.2820,	0.8480 s / batch. (data: 8.53e-04). ETA=12:02:36, max mem: 20.9 GB 
[11/22 20:30:50 visual_prompt]: 	Training 400/553. train loss: 1.3699,	0.8281 s / batch. (data: 3.34e-04). ETA=11:44:19, max mem: 20.9 GB 
[11/22 20:32:33 visual_prompt]: 	Training 500/553. train loss: 0.3061,	1.5680 s / batch. (data: 7.27e-01). ETA=22:10:56, max mem: 20.9 GB 
[11/22 20:33:26 visual_prompt]: Epoch 8 / 100: avg data time: 1.97e-01, avg batch time: 1.0235, average train loss: 14.8445
[11/22 20:34:24 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3079, average loss: 2.0214
[11/22 20:34:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 62.54	
[11/22 20:34:24 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/22 20:36:10 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8330 s / batch. (data: 3.01e-04). ETA=11:44:57, max mem: 20.9 GB 
[11/22 20:37:50 visual_prompt]: 	Training 200/553. train loss: 3.6899,	0.8315 s / batch. (data: 3.31e-04). ETA=11:42:19, max mem: 20.9 GB 
[11/22 20:39:33 visual_prompt]: 	Training 300/553. train loss: 13.8040,	1.6756 s / batch. (data: 8.47e-01). ETA=23:32:24, max mem: 20.9 GB 
[11/22 20:41:15 visual_prompt]: 	Training 400/553. train loss: 12.5228,	0.8516 s / batch. (data: 1.06e-02). ETA=11:56:24, max mem: 20.9 GB 
[11/22 20:42:58 visual_prompt]: 	Training 500/553. train loss: 8.6952,	0.9417 s / batch. (data: 1.26e-01). ETA=13:10:37, max mem: 20.9 GB 
[11/22 20:43:50 visual_prompt]: Epoch 9 / 100: avg data time: 1.96e-01, avg batch time: 1.0229, average train loss: 14.6695
[11/22 20:44:48 visual_prompt]: Inference (val):avg data time: 2.23e-04, avg batch time: 0.3095, average loss: 9.3240
[11/22 20:44:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.57	
[11/22 20:44:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/22 20:46:36 visual_prompt]: 	Training 100/553. train loss: 36.7253,	0.8073 s / batch. (data: 3.26e-04). ETA=11:15:43, max mem: 20.9 GB 
[11/22 20:48:17 visual_prompt]: 	Training 200/553. train loss: 19.9705,	0.8105 s / batch. (data: 3.03e-04). ETA=11:17:03, max mem: 20.9 GB 
[11/22 20:49:58 visual_prompt]: 	Training 300/553. train loss: 46.9929,	1.2040 s / batch. (data: 3.66e-01). ETA=16:43:49, max mem: 20.9 GB 
[11/22 20:51:38 visual_prompt]: 	Training 400/553. train loss: 38.0591,	0.8160 s / batch. (data: 3.30e-04). ETA=11:18:57, max mem: 20.9 GB 
[11/22 20:53:21 visual_prompt]: 	Training 500/553. train loss: 6.8261,	0.8320 s / batch. (data: 7.96e-03). ETA=11:30:52, max mem: 20.9 GB 
[11/22 20:54:14 visual_prompt]: Epoch 10 / 100: avg data time: 1.98e-01, avg batch time: 1.0232, average train loss: 22.4447
[11/22 20:55:12 visual_prompt]: Inference (val):avg data time: 5.24e-05, avg batch time: 0.3078, average loss: 16.9751
[11/22 20:55:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.92	
[11/22 20:55:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/22 20:57:00 visual_prompt]: 	Training 100/553. train loss: 74.9341,	0.8527 s / batch. (data: 1.56e-02). ETA=11:45:55, max mem: 20.9 GB 
[11/22 20:58:43 visual_prompt]: 	Training 200/553. train loss: 29.2614,	0.8280 s / batch. (data: 1.19e-02). ETA=11:24:03, max mem: 20.9 GB 
[11/22 21:00:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.1761 s / batch. (data: 3.43e-01). ETA=16:09:40, max mem: 20.9 GB 
[11/22 21:02:04 visual_prompt]: 	Training 400/553. train loss: 18.4664,	0.8337 s / batch. (data: 3.25e-04). ETA=11:26:00, max mem: 20.9 GB 
[11/22 21:03:45 visual_prompt]: 	Training 500/553. train loss: 7.8388,	0.8278 s / batch. (data: 3.09e-04). ETA=11:19:43, max mem: 20.9 GB 
[11/22 21:04:37 visual_prompt]: Epoch 11 / 100: avg data time: 1.95e-01, avg batch time: 1.0201, average train loss: 22.0659
[11/22 21:05:35 visual_prompt]: Inference (val):avg data time: 2.23e-04, avg batch time: 0.3062, average loss: 2.4523
[11/22 21:05:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 69.60	
[11/22 21:05:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/22 21:07:22 visual_prompt]: 	Training 100/553. train loss: 3.9416,	0.8320 s / batch. (data: 7.96e-03). ETA=11:21:06, max mem: 20.9 GB 
[11/22 21:09:04 visual_prompt]: 	Training 200/553. train loss: 24.5018,	0.8409 s / batch. (data: 1.06e-02). ETA=11:26:58, max mem: 20.9 GB 
[11/22 21:10:44 visual_prompt]: 	Training 300/553. train loss: 32.0297,	0.8522 s / batch. (data: 3.44e-04). ETA=11:34:48, max mem: 20.9 GB 
[11/22 21:12:26 visual_prompt]: 	Training 400/553. train loss: 28.2493,	0.8280 s / batch. (data: 3.35e-04). ETA=11:13:41, max mem: 20.9 GB 
[11/22 21:14:08 visual_prompt]: 	Training 500/553. train loss: 82.2206,	0.8521 s / batch. (data: 8.24e-04). ETA=11:31:50, max mem: 20.9 GB 
[11/22 21:15:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.96e-01, avg batch time: 1.0216, average train loss: 18.6137
[11/22 21:15:59 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.3062, average loss: 22.1416
[11/22 21:15:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.73	
[11/22 21:15:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/22 21:17:46 visual_prompt]: 	Training 100/553. train loss: 10.2265,	0.8480 s / batch. (data: 2.94e-04). ETA=11:26:22, max mem: 20.9 GB 
[11/22 21:19:25 visual_prompt]: 	Training 200/553. train loss: 12.1518,	0.8189 s / batch. (data: 5.42e-03). ETA=11:01:25, max mem: 20.9 GB 
[11/22 21:21:08 visual_prompt]: 	Training 300/553. train loss: 6.5078,	1.8565 s / batch. (data: 1.02e+00). ETA=1 day, 0:56:30, max mem: 20.9 GB 
[11/22 21:22:48 visual_prompt]: 	Training 400/553. train loss: 5.5350,	0.8242 s / batch. (data: 3.27e-04). ETA=11:03:00, max mem: 20.9 GB 
[11/22 21:24:31 visual_prompt]: 	Training 500/553. train loss: 30.8013,	0.8205 s / batch. (data: 4.25e-03). ETA=10:58:39, max mem: 20.9 GB 
[11/22 21:25:24 visual_prompt]: Epoch 13 / 100: avg data time: 1.96e-01, avg batch time: 1.0217, average train loss: 19.2707
[11/22 21:26:22 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3073, average loss: 9.5355
[11/22 21:26:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.06	
[11/22 21:26:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/22 21:28:09 visual_prompt]: 	Training 100/553. train loss: 3.6630,	0.8583 s / batch. (data: 3.03e-02). ETA=11:26:48, max mem: 20.9 GB 
[11/22 21:29:50 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2428 s / batch. (data: 4.14e-01). ETA=16:32:23, max mem: 20.9 GB 
[11/22 21:31:32 visual_prompt]: 	Training 300/553. train loss: 13.1360,	0.8400 s / batch. (data: 4.26e-04). ETA=11:09:21, max mem: 20.9 GB 
[11/22 21:33:13 visual_prompt]: 	Training 400/553. train loss: 8.2468,	0.8441 s / batch. (data: 3.14e-04). ETA=11:11:12, max mem: 20.9 GB 
[11/22 21:34:56 visual_prompt]: 	Training 500/553. train loss: 6.0314,	0.8186 s / batch. (data: 1.06e-02). ETA=10:49:33, max mem: 20.9 GB 
[11/22 21:35:46 visual_prompt]: Epoch 14 / 100: avg data time: 1.95e-01, avg batch time: 1.0202, average train loss: 20.2889
[11/22 21:36:45 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3088, average loss: 5.4824
[11/22 21:36:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.59	
[11/22 21:36:45 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/22 21:38:31 visual_prompt]: 	Training 100/553. train loss: 2.4841,	0.8159 s / batch. (data: 3.46e-04). ETA=10:45:22, max mem: 20.9 GB 
[11/22 21:40:11 visual_prompt]: 	Training 200/553. train loss: 0.0061,	0.8201 s / batch. (data: 3.43e-04). ETA=10:47:17, max mem: 20.9 GB 
[11/22 21:41:55 visual_prompt]: 	Training 300/553. train loss: 34.1132,	0.8080 s / batch. (data: 3.25e-04). ETA=10:36:24, max mem: 20.9 GB 
[11/22 21:43:34 visual_prompt]: 	Training 400/553. train loss: 30.7254,	0.8102 s / batch. (data: 5.48e-03). ETA=10:36:49, max mem: 20.9 GB 
[11/22 21:45:16 visual_prompt]: 	Training 500/553. train loss: 8.6655,	0.8560 s / batch. (data: 3.22e-04). ETA=11:11:22, max mem: 20.9 GB 
[11/22 21:46:11 visual_prompt]: Epoch 15 / 100: avg data time: 1.98e-01, avg batch time: 1.0231, average train loss: 21.3577
[11/22 21:47:09 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3069, average loss: 4.9152
[11/22 21:47:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.58	
[11/22 21:47:09 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/22 21:48:53 visual_prompt]: 	Training 100/553. train loss: 25.2839,	0.8238 s / batch. (data: 3.35e-04). ETA=10:44:02, max mem: 20.9 GB 
[11/22 21:50:36 visual_prompt]: 	Training 200/553. train loss: 20.3574,	0.8186 s / batch. (data: 3.28e-04). ETA=10:38:34, max mem: 20.9 GB 
[11/22 21:52:18 visual_prompt]: 	Training 300/553. train loss: 11.8521,	0.8123 s / batch. (data: 3.12e-04). ETA=10:32:16, max mem: 20.9 GB 
[11/22 21:54:00 visual_prompt]: 	Training 400/553. train loss: 19.4738,	0.8135 s / batch. (data: 3.41e-04). ETA=10:31:51, max mem: 20.9 GB 
[11/22 21:55:41 visual_prompt]: 	Training 500/553. train loss: 3.9495,	1.0640 s / batch. (data: 2.24e-01). ETA=13:44:42, max mem: 20.9 GB 
[11/22 21:56:35 visual_prompt]: Epoch 16 / 100: avg data time: 1.98e-01, avg batch time: 1.0230, average train loss: 17.7663
[11/22 21:57:33 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.3060, average loss: 2.0187
[11/22 21:57:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 64.72	
[11/22 21:57:33 visual_prompt]: Best epoch 16: best metric: -2.019
[11/22 21:57:33 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/22 21:59:18 visual_prompt]: 	Training 100/553. train loss: 8.9375,	0.8234 s / batch. (data: 2.98e-04). ETA=10:36:06, max mem: 20.9 GB 
[11/22 22:01:01 visual_prompt]: 	Training 200/553. train loss: 45.9730,	0.8136 s / batch. (data: 3.51e-03). ETA=10:27:12, max mem: 20.9 GB 
[11/22 22:02:43 visual_prompt]: 	Training 300/553. train loss: 22.9622,	0.8193 s / batch. (data: 5.56e-03). ETA=10:30:13, max mem: 20.9 GB 
[11/22 22:04:24 visual_prompt]: 	Training 400/553. train loss: 14.7286,	1.1441 s / batch. (data: 3.16e-01). ETA=14:38:07, max mem: 20.9 GB 
[11/22 22:06:05 visual_prompt]: 	Training 500/553. train loss: 7.3184,	0.9915 s / batch. (data: 1.62e-01). ETA=12:39:20, max mem: 20.9 GB 
[11/22 22:06:59 visual_prompt]: Epoch 17 / 100: avg data time: 1.98e-01, avg batch time: 1.0228, average train loss: 19.2786
[11/22 22:07:57 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3096, average loss: 27.4367
[11/22 22:07:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.51	
[11/22 22:07:57 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/22 22:09:43 visual_prompt]: 	Training 100/553. train loss: 5.2559,	0.8360 s / batch. (data: 3.10e-04). ETA=10:38:07, max mem: 20.9 GB 
[11/22 22:11:28 visual_prompt]: 	Training 200/553. train loss: 3.6647,	0.8349 s / batch. (data: 1.05e-02). ETA=10:35:56, max mem: 20.9 GB 
[11/22 22:13:10 visual_prompt]: 	Training 300/553. train loss: 5.1940,	0.8359 s / batch. (data: 3.46e-04). ETA=10:35:15, max mem: 20.9 GB 
[11/22 22:14:51 visual_prompt]: 	Training 400/553. train loss: 25.8491,	0.8320 s / batch. (data: 2.94e-04). ETA=10:30:55, max mem: 20.9 GB 
[11/22 22:16:32 visual_prompt]: 	Training 500/553. train loss: 14.6559,	0.8395 s / batch. (data: 3.23e-04). ETA=10:35:12, max mem: 20.9 GB 
[11/22 22:17:24 visual_prompt]: Epoch 18 / 100: avg data time: 1.99e-01, avg batch time: 1.0239, average train loss: 21.2044
[11/22 22:18:22 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3060, average loss: 31.1111
[11/22 22:18:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.99	
[11/22 22:18:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/22 22:20:07 visual_prompt]: 	Training 100/553. train loss: 8.5300,	0.8287 s / batch. (data: 3.20e-04). ETA=10:24:57, max mem: 20.9 GB 
[11/22 22:21:50 visual_prompt]: 	Training 200/553. train loss: 10.2427,	0.8279 s / batch. (data: 3.26e-04). ETA=10:22:57, max mem: 20.9 GB 
[11/22 22:23:31 visual_prompt]: 	Training 300/553. train loss: 0.0001,	0.8360 s / batch. (data: 1.20e-02). ETA=10:27:38, max mem: 20.9 GB 
[11/22 22:25:15 visual_prompt]: 	Training 400/553. train loss: 9.9992,	0.8544 s / batch. (data: 8.50e-04). ETA=10:39:59, max mem: 20.9 GB 
[11/22 22:26:52 visual_prompt]: 	Training 500/553. train loss: 8.1673,	0.8280 s / batch. (data: 3.29e-04). ETA=10:18:52, max mem: 20.9 GB 
[11/22 22:27:46 visual_prompt]: Epoch 19 / 100: avg data time: 1.94e-01, avg batch time: 1.0199, average train loss: 16.9489
[11/22 22:28:44 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3058, average loss: 9.2377
[11/22 22:28:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 66.11	
[11/22 22:28:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/22 22:30:29 visual_prompt]: 	Training 100/553. train loss: 25.5115,	0.8316 s / batch. (data: 3.14e-04). ETA=10:19:25, max mem: 20.9 GB 
[11/22 22:32:11 visual_prompt]: 	Training 200/553. train loss: 2.2696,	0.8222 s / batch. (data: 3.25e-04). ETA=10:11:02, max mem: 20.9 GB 
[11/22 22:33:52 visual_prompt]: 	Training 300/553. train loss: 13.7544,	0.8400 s / batch. (data: 9.22e-03). ETA=10:22:54, max mem: 20.9 GB 
[11/22 22:35:34 visual_prompt]: 	Training 400/553. train loss: 30.7075,	0.8517 s / batch. (data: 1.56e-02). ETA=10:30:07, max mem: 20.9 GB 
[11/22 22:37:14 visual_prompt]: 	Training 500/553. train loss: 84.3555,	0.8281 s / batch. (data: 3.10e-04). ETA=10:11:17, max mem: 20.9 GB 
[11/22 22:38:09 visual_prompt]: Epoch 20 / 100: avg data time: 1.97e-01, avg batch time: 1.0204, average train loss: 21.6830
[11/22 22:39:07 visual_prompt]: Inference (val):avg data time: 1.94e-04, avg batch time: 0.3076, average loss: 7.5366
[11/22 22:39:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.91	
[11/22 22:39:07 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/22 22:40:56 visual_prompt]: 	Training 100/553. train loss: 3.1234,	0.8413 s / batch. (data: 9.25e-03). ETA=10:18:53, max mem: 20.9 GB 
[11/22 22:42:36 visual_prompt]: 	Training 200/553. train loss: 36.0146,	0.8100 s / batch. (data: 3.00e-04). ETA=9:54:34, max mem: 20.9 GB 
[11/22 22:44:18 visual_prompt]: 	Training 300/553. train loss: 81.9775,	1.0966 s / batch. (data: 2.75e-01). ETA=13:23:05, max mem: 20.9 GB 
[11/22 22:45:58 visual_prompt]: 	Training 400/553. train loss: 1.2556,	0.8122 s / batch. (data: 3.43e-04). ETA=9:53:28, max mem: 20.9 GB 
[11/22 22:47:41 visual_prompt]: 	Training 500/553. train loss: 7.6277,	0.8314 s / batch. (data: 5.46e-03). ETA=10:06:06, max mem: 20.9 GB 
[11/22 22:48:33 visual_prompt]: Epoch 21 / 100: avg data time: 1.98e-01, avg batch time: 1.0228, average train loss: 16.5011
[11/22 22:49:31 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3081, average loss: 7.3796
[11/22 22:49:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 67.41	
[11/22 22:49:31 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/22 22:51:16 visual_prompt]: 	Training 100/553. train loss: 47.6375,	0.8320 s / batch. (data: 3.02e-04). ETA=10:04:25, max mem: 20.9 GB 
[11/22 22:52:58 visual_prompt]: 	Training 200/553. train loss: 13.8386,	0.8360 s / batch. (data: 3.37e-04). ETA=10:05:54, max mem: 20.9 GB 
[11/22 22:54:37 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8429 s / batch. (data: 3.17e-04). ETA=10:09:28, max mem: 20.9 GB 
[11/22 22:56:20 visual_prompt]: 	Training 400/553. train loss: 4.3740,	0.8291 s / batch. (data: 3.13e-04). ETA=9:58:09, max mem: 20.9 GB 
[11/22 22:58:02 visual_prompt]: 	Training 500/553. train loss: 23.4144,	0.8550 s / batch. (data: 1.05e-02). ETA=10:15:25, max mem: 20.9 GB 
[11/22 22:58:56 visual_prompt]: Epoch 22 / 100: avg data time: 1.96e-01, avg batch time: 1.0211, average train loss: 15.0954
[11/22 22:59:54 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3067, average loss: 4.8684
[11/22 22:59:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 68.48	
[11/22 22:59:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/22 23:01:41 visual_prompt]: 	Training 100/553. train loss: 9.3038,	0.8301 s / batch. (data: 3.40e-04). ETA=9:55:23, max mem: 20.9 GB 
[11/22 23:03:23 visual_prompt]: 	Training 200/553. train loss: 66.7489,	0.8240 s / batch. (data: 3.16e-04). ETA=9:49:37, max mem: 20.9 GB 
[11/22 23:05:07 visual_prompt]: 	Training 300/553. train loss: 12.7685,	0.8201 s / batch. (data: 4.45e-04). ETA=9:45:26, max mem: 20.9 GB 
[11/22 23:06:47 visual_prompt]: 	Training 400/553. train loss: 9.6662,	0.8376 s / batch. (data: 1.05e-02). ETA=9:56:34, max mem: 20.9 GB 
[11/22 23:08:26 visual_prompt]: 	Training 500/553. train loss: 29.2016,	0.8280 s / batch. (data: 3.17e-04). ETA=9:48:21, max mem: 20.9 GB 
[11/22 23:09:19 visual_prompt]: Epoch 23 / 100: avg data time: 1.96e-01, avg batch time: 1.0215, average train loss: 19.5543
[11/22 23:10:18 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3071, average loss: 8.3050
[11/22 23:10:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 67.63	
[11/22 23:10:18 visual_prompt]: Stopping early.
[11/22 23:10:18 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 23:10:18 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 23:10:18 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/22 23:10:18 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/22 23:10:18 visual_prompt]: Training with config:
[11/22 23:10:18 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/22 23:10:18 visual_prompt]: Loading training data...
[11/22 23:10:18 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 23:10:18 visual_prompt]: Loading validation data...
[11/22 23:10:18 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 23:10:18 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 23:10:21 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/22 23:10:21 visual_prompt]: tuned percent:0.525
[11/22 23:10:21 visual_prompt]: Device used for model: 0
[11/22 23:10:21 visual_prompt]: Setting up Evaluator...
[11/22 23:10:21 visual_prompt]: Setting up Trainer...
[11/22 23:10:21 visual_prompt]: 	Setting up the optimizer...
[11/22 23:10:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 23:12:05 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8520 s / batch. (data: 1.20e-02). ETA=13:03:49, max mem: 20.9 GB 
[11/22 23:13:46 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8277 s / batch. (data: 3.05e-04). ETA=12:40:04, max mem: 20.9 GB 
[11/22 23:15:30 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.1300 s / batch. (data: 1.32e+00). ETA=1 day, 8:32:27, max mem: 20.9 GB 
[11/22 23:17:10 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8178 s / batch. (data: 3.00e-04). ETA=12:28:18, max mem: 20.9 GB 
[11/22 23:18:54 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8280 s / batch. (data: 8.06e-04). ETA=12:36:14, max mem: 20.9 GB 
[11/22 23:19:48 visual_prompt]: Epoch 1 / 100: avg data time: 1.99e-01, avg batch time: 1.0256, average train loss: 1.5403
[11/22 23:20:46 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3086, average loss: 1.5201
[11/22 23:20:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/22 23:20:46 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/22 23:22:32 visual_prompt]: 	Training 100/553. train loss: 0.7798,	1.5880 s / batch. (data: 7.71e-01). ETA=1 day, 0:06:19, max mem: 20.9 GB 
[11/22 23:24:13 visual_prompt]: 	Training 200/553. train loss: 0.0007,	0.8200 s / batch. (data: 3.33e-04). ETA=12:25:28, max mem: 20.9 GB 
[11/22 23:25:57 visual_prompt]: 	Training 300/553. train loss: 0.8430,	1.2680 s / batch. (data: 4.35e-01). ETA=19:10:39, max mem: 20.9 GB 
[11/22 23:27:37 visual_prompt]: 	Training 400/553. train loss: 0.8061,	0.8207 s / batch. (data: 3.19e-04). ETA=12:23:24, max mem: 20.9 GB 
[11/22 23:29:20 visual_prompt]: 	Training 500/553. train loss: 0.6105,	0.8279 s / batch. (data: 3.06e-04). ETA=12:28:30, max mem: 20.9 GB 
[11/22 23:30:13 visual_prompt]: Epoch 2 / 100: avg data time: 1.97e-01, avg batch time: 1.0237, average train loss: 1.5226
[11/22 23:31:11 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3079, average loss: 0.8135
[11/22 23:31:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.50	
[11/22 23:31:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/22 23:32:55 visual_prompt]: 	Training 100/553. train loss: 3.1812,	0.9918 s / batch. (data: 1.62e-01). ETA=14:54:10, max mem: 20.9 GB 
[11/22 23:34:37 visual_prompt]: 	Training 200/553. train loss: 2.0711,	0.8274 s / batch. (data: 5.48e-03). ETA=12:24:32, max mem: 20.9 GB 
[11/22 23:36:18 visual_prompt]: 	Training 300/553. train loss: 0.9746,	0.8515 s / batch. (data: 5.47e-03). ETA=12:44:52, max mem: 20.9 GB 
[11/22 23:38:01 visual_prompt]: 	Training 400/553. train loss: 0.0115,	0.8360 s / batch. (data: 3.10e-04). ETA=12:29:31, max mem: 20.9 GB 
[11/22 23:39:44 visual_prompt]: 	Training 500/553. train loss: 4.7185,	1.3827 s / batch. (data: 5.75e-01). ETA=20:37:20, max mem: 20.9 GB 
[11/22 23:40:36 visual_prompt]: Epoch 3 / 100: avg data time: 1.96e-01, avg batch time: 1.0220, average train loss: 2.5513
[11/22 23:41:35 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.3055, average loss: 7.7699
[11/22 23:41:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.54	
[11/22 23:41:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/22 23:43:22 visual_prompt]: 	Training 100/553. train loss: 8.1876,	0.8320 s / batch. (data: 7.96e-03). ETA=12:22:25, max mem: 20.9 GB 
[11/22 23:45:04 visual_prompt]: 	Training 200/553. train loss: 2.3320,	0.8357 s / batch. (data: 3.86e-04). ETA=12:24:21, max mem: 20.9 GB 
[11/22 23:46:47 visual_prompt]: 	Training 300/553. train loss: 1.5272,	1.3000 s / batch. (data: 4.80e-01). ETA=19:15:41, max mem: 20.9 GB 
[11/22 23:48:23 visual_prompt]: 	Training 400/553. train loss: 2.3384,	1.0298 s / batch. (data: 1.95e-01). ETA=15:13:47, max mem: 20.9 GB 
[11/22 23:50:06 visual_prompt]: 	Training 500/553. train loss: 15.5396,	3.3240 s / batch. (data: 2.50e+00). ETA=2 days, 1:04:01, max mem: 20.9 GB 
[11/22 23:51:01 visual_prompt]: Epoch 4 / 100: avg data time: 1.97e-01, avg batch time: 1.0239, average train loss: 4.5100
[11/22 23:51:59 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3081, average loss: 4.4083
[11/22 23:51:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.16	
[11/22 23:51:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/22 23:53:44 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8440 s / batch. (data: 7.96e-03). ETA=12:25:21, max mem: 20.9 GB 
[11/22 23:55:25 visual_prompt]: 	Training 200/553. train loss: 6.5708,	1.4032 s / batch. (data: 5.84e-01). ETA=20:36:54, max mem: 20.9 GB 
[11/22 23:57:08 visual_prompt]: 	Training 300/553. train loss: 5.6033,	0.8128 s / batch. (data: 2.87e-04). ETA=11:55:05, max mem: 20.9 GB 
[11/22 23:58:48 visual_prompt]: 	Training 400/553. train loss: 16.1834,	0.8560 s / batch. (data: 5.45e-03). ETA=12:31:43, max mem: 20.9 GB 
[11/23 00:00:30 visual_prompt]: 	Training 500/553. train loss: 1.6618,	0.8514 s / batch. (data: 3.17e-04). ETA=12:26:13, max mem: 20.9 GB 
[11/23 00:01:24 visual_prompt]: Epoch 5 / 100: avg data time: 1.95e-01, avg batch time: 1.0211, average train loss: 5.5591
[11/23 00:02:23 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3063, average loss: 6.7012
[11/23 00:02:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.04	
[11/23 00:02:23 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/23 00:04:10 visual_prompt]: 	Training 100/553. train loss: 14.3782,	0.8249 s / batch. (data: 3.96e-04). ETA=12:00:51, max mem: 20.9 GB 
[11/23 00:05:51 visual_prompt]: 	Training 200/553. train loss: 7.5807,	0.8677 s / batch. (data: 2.60e-02). ETA=12:36:50, max mem: 20.9 GB 
[11/23 00:07:31 visual_prompt]: 	Training 300/553. train loss: 8.5515,	0.8309 s / batch. (data: 3.27e-04). ETA=12:03:21, max mem: 20.9 GB 
[11/23 00:09:16 visual_prompt]: 	Training 400/553. train loss: 6.8434,	0.8189 s / batch. (data: 3.41e-04). ETA=11:51:35, max mem: 20.9 GB 
[11/23 00:10:56 visual_prompt]: 	Training 500/553. train loss: 6.3768,	0.8343 s / batch. (data: 2.31e-02). ETA=12:03:30, max mem: 20.9 GB 
[11/23 00:11:49 visual_prompt]: Epoch 6 / 100: avg data time: 1.98e-01, avg batch time: 1.0234, average train loss: 7.4405
[11/23 00:12:47 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.3081, average loss: 0.7440
[11/23 00:12:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.26	
[11/23 00:12:47 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/23 00:14:31 visual_prompt]: 	Training 100/553. train loss: 4.6580,	0.8240 s / batch. (data: 3.01e-04). ETA=11:52:32, max mem: 20.9 GB 
[11/23 00:16:13 visual_prompt]: 	Training 200/553. train loss: 6.2999,	1.1140 s / batch. (data: 3.05e-01). ETA=16:01:26, max mem: 20.9 GB 
[11/23 00:17:58 visual_prompt]: 	Training 300/553. train loss: 1.2756,	1.5600 s / batch. (data: 7.18e-01). ETA=22:23:43, max mem: 20.9 GB 
[11/23 00:19:40 visual_prompt]: 	Training 400/553. train loss: 2.8603,	1.9980 s / batch. (data: 1.18e+00). ETA=1 day, 4:37:39, max mem: 20.9 GB 
[11/23 00:21:20 visual_prompt]: 	Training 500/553. train loss: 7.7095,	0.8400 s / batch. (data: 7.95e-03). ETA=12:00:44, max mem: 20.9 GB 
[11/23 00:22:12 visual_prompt]: Epoch 7 / 100: avg data time: 1.95e-01, avg batch time: 1.0212, average train loss: 8.8655
[11/23 00:23:10 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.3089, average loss: 0.8059
[11/23 00:23:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.19	
[11/23 00:23:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/23 00:24:54 visual_prompt]: 	Training 100/553. train loss: 5.4048,	0.8256 s / batch. (data: 8.42e-03). ETA=11:46:16, max mem: 20.9 GB 
[11/23 00:26:38 visual_prompt]: 	Training 200/553. train loss: 13.6669,	0.8329 s / batch. (data: 7.96e-03). ETA=11:51:10, max mem: 20.9 GB 
[11/23 00:28:21 visual_prompt]: 	Training 300/553. train loss: 3.8111,	0.8235 s / batch. (data: 1.05e-02). ETA=11:41:44, max mem: 20.9 GB 
[11/23 00:30:03 visual_prompt]: 	Training 400/553. train loss: 13.1761,	0.8400 s / batch. (data: 3.25e-04). ETA=11:54:23, max mem: 20.9 GB 
[11/23 00:31:45 visual_prompt]: 	Training 500/553. train loss: 58.6027,	1.5400 s / batch. (data: 7.11e-01). ETA=21:47:10, max mem: 20.9 GB 
[11/23 00:32:39 visual_prompt]: Epoch 8 / 100: avg data time: 2.01e-01, avg batch time: 1.0278, average train loss: 10.5169
[11/23 00:33:37 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3074, average loss: 7.4940
[11/23 00:33:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.82	
[11/23 00:33:37 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/23 00:35:23 visual_prompt]: 	Training 100/553. train loss: 11.2248,	0.8274 s / batch. (data: 1.20e-02). ETA=11:40:09, max mem: 20.9 GB 
[11/23 00:37:03 visual_prompt]: 	Training 200/553. train loss: 17.2262,	0.8240 s / batch. (data: 3.20e-04). ETA=11:35:57, max mem: 20.9 GB 
[11/23 00:38:45 visual_prompt]: 	Training 300/553. train loss: 1.2406,	2.0236 s / batch. (data: 1.20e+00). ETA=1 day, 4:25:47, max mem: 20.9 GB 
[11/23 00:40:29 visual_prompt]: 	Training 400/553. train loss: 14.1322,	0.8240 s / batch. (data: 3.04e-04). ETA=11:33:11, max mem: 20.9 GB 
[11/23 00:42:11 visual_prompt]: 	Training 500/553. train loss: 28.9069,	1.0855 s / batch. (data: 2.73e-01). ETA=15:11:21, max mem: 20.9 GB 
[11/23 00:43:04 visual_prompt]: Epoch 9 / 100: avg data time: 1.98e-01, avg batch time: 1.0244, average train loss: 14.5256
[11/23 00:44:02 visual_prompt]: Inference (val):avg data time: 3.22e-04, avg batch time: 0.3072, average loss: 7.8257
[11/23 00:44:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.12	
[11/23 00:44:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/23 00:45:50 visual_prompt]: 	Training 100/553. train loss: 14.5590,	0.8399 s / batch. (data: 4.58e-04). ETA=11:43:03, max mem: 20.9 GB 
[11/23 00:47:30 visual_prompt]: 	Training 200/553. train loss: 1.0110,	0.8235 s / batch. (data: 3.74e-04). ETA=11:27:58, max mem: 20.9 GB 
[11/23 00:49:11 visual_prompt]: 	Training 300/553. train loss: 35.9825,	1.1320 s / batch. (data: 2.80e-01). ETA=15:43:44, max mem: 20.9 GB 
[11/23 00:50:51 visual_prompt]: 	Training 400/553. train loss: 25.0067,	0.8240 s / batch. (data: 3.30e-04). ETA=11:25:37, max mem: 20.9 GB 
[11/23 00:52:33 visual_prompt]: 	Training 500/553. train loss: 3.2665,	0.8714 s / batch. (data: 5.81e-02). ETA=12:03:38, max mem: 20.9 GB 
[11/23 00:53:27 visual_prompt]: Epoch 10 / 100: avg data time: 1.94e-01, avg batch time: 1.0202, average train loss: 15.3921
[11/23 00:54:25 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3079, average loss: 8.6444
[11/23 00:54:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.01	
[11/23 00:54:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/23 00:56:12 visual_prompt]: 	Training 100/553. train loss: 2.8445,	0.8236 s / batch. (data: 3.05e-04). ETA=11:21:48, max mem: 20.9 GB 
[11/23 00:57:56 visual_prompt]: 	Training 200/553. train loss: 48.9576,	0.8611 s / batch. (data: 3.31e-02). ETA=11:51:26, max mem: 20.9 GB 
[11/23 00:59:37 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.3949 s / batch. (data: 1.56e+00). ETA=1 day, 8:54:36, max mem: 20.9 GB 
[11/23 01:01:18 visual_prompt]: 	Training 400/553. train loss: 2.8517,	0.8526 s / batch. (data: 1.66e-02). ETA=11:41:31, max mem: 20.9 GB 
[11/23 01:02:58 visual_prompt]: 	Training 500/553. train loss: 48.7725,	0.8123 s / batch. (data: 2.97e-04). ETA=11:07:01, max mem: 20.9 GB 
[11/23 01:03:51 visual_prompt]: Epoch 11 / 100: avg data time: 1.97e-01, avg batch time: 1.0228, average train loss: 15.8937
[11/23 01:04:49 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3069, average loss: 2.6196
[11/23 01:04:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.89	
[11/23 01:04:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/23 01:06:36 visual_prompt]: 	Training 100/553. train loss: 16.8725,	1.0880 s / batch. (data: 2.50e-01). ETA=14:50:39, max mem: 20.9 GB 
[11/23 01:08:19 visual_prompt]: 	Training 200/553. train loss: 10.2299,	0.8330 s / batch. (data: 3.23e-04). ETA=11:20:31, max mem: 20.9 GB 
[11/23 01:09:59 visual_prompt]: 	Training 300/553. train loss: 23.4843,	0.8335 s / batch. (data: 1.06e-02). ETA=11:19:31, max mem: 20.9 GB 
[11/23 01:11:42 visual_prompt]: 	Training 400/553. train loss: 21.7349,	0.8364 s / batch. (data: 2.56e-02). ETA=11:20:30, max mem: 20.9 GB 
[11/23 01:13:24 visual_prompt]: 	Training 500/553. train loss: 6.1085,	0.8693 s / batch. (data: 3.55e-04). ETA=11:45:47, max mem: 20.9 GB 
[11/23 01:14:15 visual_prompt]: Epoch 12 / 100: avg data time: 1.97e-01, avg batch time: 1.0236, average train loss: 15.7183
[11/23 01:15:13 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3062, average loss: 9.6142
[11/23 01:15:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[11/23 01:15:13 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/23 01:17:00 visual_prompt]: 	Training 100/553. train loss: 5.4204,	0.8320 s / batch. (data: 3.24e-04). ETA=11:13:24, max mem: 20.9 GB 
[11/23 01:18:38 visual_prompt]: 	Training 200/553. train loss: 18.7901,	0.8388 s / batch. (data: 1.06e-02). ETA=11:17:30, max mem: 20.9 GB 
[11/23 01:20:21 visual_prompt]: 	Training 300/553. train loss: 4.6744,	1.9160 s / batch. (data: 1.06e+00). ETA=1 day, 1:44:25, max mem: 20.9 GB 
[11/23 01:22:01 visual_prompt]: 	Training 400/553. train loss: 22.9821,	0.8324 s / batch. (data: 1.06e-02). ETA=11:09:33, max mem: 20.9 GB 
[11/23 01:23:44 visual_prompt]: 	Training 500/553. train loss: 55.6965,	0.8240 s / batch. (data: 3.22e-04). ETA=11:01:26, max mem: 20.9 GB 
[11/23 01:24:37 visual_prompt]: Epoch 13 / 100: avg data time: 1.92e-01, avg batch time: 1.0186, average train loss: 18.4516
[11/23 01:25:35 visual_prompt]: Inference (val):avg data time: 2.23e-04, avg batch time: 0.3066, average loss: 39.4750
[11/23 01:25:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.53	
[11/23 01:25:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/23 01:27:22 visual_prompt]: 	Training 100/553. train loss: 7.5923,	0.8152 s / batch. (data: 3.02e-04). ETA=10:52:19, max mem: 20.9 GB 
[11/23 01:29:03 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.3207 s / batch. (data: 5.03e-01). ETA=17:34:37, max mem: 20.9 GB 
[11/23 01:30:44 visual_prompt]: 	Training 300/553. train loss: 13.0603,	0.8159 s / batch. (data: 3.46e-04). ETA=10:50:11, max mem: 20.9 GB 
[11/23 01:32:25 visual_prompt]: 	Training 400/553. train loss: 15.3959,	0.8244 s / batch. (data: 3.31e-04). ETA=10:55:34, max mem: 20.9 GB 
[11/23 01:34:08 visual_prompt]: 	Training 500/553. train loss: 9.6030,	0.8298 s / batch. (data: 3.03e-04). ETA=10:58:29, max mem: 20.9 GB 
[11/23 01:34:59 visual_prompt]: Epoch 14 / 100: avg data time: 1.94e-01, avg batch time: 1.0200, average train loss: 17.0693
[11/23 01:35:58 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3073, average loss: 30.5229
[11/23 01:35:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.87	
[11/23 01:35:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/23 01:37:43 visual_prompt]: 	Training 100/553. train loss: 9.4049,	0.8660 s / batch. (data: 1.00e-02). ETA=11:24:59, max mem: 20.9 GB 
[11/23 01:39:22 visual_prompt]: 	Training 200/553. train loss: 97.4315,	0.8152 s / batch. (data: 3.10e-04). ETA=10:43:26, max mem: 20.9 GB 
[11/23 01:41:06 visual_prompt]: 	Training 300/553. train loss: 53.9615,	0.8471 s / batch. (data: 1.11e-02). ETA=11:07:14, max mem: 20.9 GB 
[11/23 01:42:45 visual_prompt]: 	Training 400/553. train loss: 5.0474,	1.2995 s / batch. (data: 4.45e-01). ETA=17:01:21, max mem: 20.9 GB 
[11/23 01:44:28 visual_prompt]: 	Training 500/553. train loss: 2.5343,	0.8501 s / batch. (data: 1.53e-02). ETA=11:06:42, max mem: 20.9 GB 
[11/23 01:45:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.94e-01, avg batch time: 1.0207, average train loss: 15.8687
[11/23 01:46:20 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.3072, average loss: 5.2078
[11/23 01:46:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.60	
[11/23 01:46:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/23 01:48:05 visual_prompt]: 	Training 100/553. train loss: 2.0542,	0.8138 s / batch. (data: 5.43e-03). ETA=10:36:09, max mem: 20.9 GB 
[11/23 01:49:46 visual_prompt]: 	Training 200/553. train loss: 7.6370,	0.8157 s / batch. (data: 4.45e-04). ETA=10:36:20, max mem: 20.9 GB 
[11/23 01:51:28 visual_prompt]: 	Training 300/553. train loss: 33.3452,	0.8377 s / batch. (data: 5.46e-03). ETA=10:52:04, max mem: 20.9 GB 
[11/23 01:53:10 visual_prompt]: 	Training 400/553. train loss: 29.9088,	0.8320 s / batch. (data: 7.98e-03). ETA=10:46:14, max mem: 20.9 GB 
[11/23 01:54:51 visual_prompt]: 	Training 500/553. train loss: 51.7052,	1.3543 s / batch. (data: 5.44e-01). ETA=17:29:42, max mem: 20.9 GB 
[11/23 01:55:45 visual_prompt]: Epoch 16 / 100: avg data time: 1.95e-01, avg batch time: 1.0207, average train loss: 16.8948
[11/23 01:56:43 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3059, average loss: 15.7914
[11/23 01:56:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.27	
[11/23 01:56:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/23 01:58:27 visual_prompt]: 	Training 100/553. train loss: 4.7427,	0.8130 s / batch. (data: 5.49e-03). ETA=10:28:02, max mem: 20.9 GB 
[11/23 02:00:10 visual_prompt]: 	Training 200/553. train loss: 38.9712,	0.8200 s / batch. (data: 3.10e-04). ETA=10:32:07, max mem: 20.9 GB 
[11/23 02:01:52 visual_prompt]: 	Training 300/553. train loss: 31.7634,	0.8280 s / batch. (data: 3.19e-04). ETA=10:36:53, max mem: 20.9 GB 
[11/23 02:03:32 visual_prompt]: 	Training 400/553. train loss: 12.3765,	1.3392 s / batch. (data: 5.12e-01). ETA=17:07:53, max mem: 20.9 GB 
[11/23 02:05:13 visual_prompt]: 	Training 500/553. train loss: 48.7165,	1.2713 s / batch. (data: 4.56e-01). ETA=16:13:38, max mem: 20.9 GB 
[11/23 02:06:08 visual_prompt]: Epoch 17 / 100: avg data time: 1.95e-01, avg batch time: 1.0207, average train loss: 15.6109
[11/23 02:07:06 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3071, average loss: 55.4827
[11/23 02:07:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.53	
[11/23 02:07:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/23 02:08:53 visual_prompt]: 	Training 100/553. train loss: 9.5563,	0.8458 s / batch. (data: 3.06e-04). ETA=10:45:34, max mem: 20.9 GB 
[11/23 02:10:36 visual_prompt]: 	Training 200/553. train loss: 3.4535,	0.8345 s / batch. (data: 8.46e-04). ETA=10:35:37, max mem: 20.9 GB 
[11/23 02:12:19 visual_prompt]: 	Training 300/553. train loss: 1.8644,	0.8160 s / batch. (data: 3.36e-04). ETA=10:20:08, max mem: 20.9 GB 
[11/23 02:14:00 visual_prompt]: 	Training 400/553. train loss: 1.3397,	0.8104 s / batch. (data: 3.07e-04). ETA=10:14:33, max mem: 20.9 GB 
[11/23 02:15:41 visual_prompt]: 	Training 500/553. train loss: 4.7031,	0.8520 s / batch. (data: 1.20e-02). ETA=10:44:39, max mem: 20.9 GB 
[11/23 02:16:33 visual_prompt]: Epoch 18 / 100: avg data time: 1.99e-01, avg batch time: 1.0241, average train loss: 14.6306
[11/23 02:17:31 visual_prompt]: Inference (val):avg data time: 4.06e-04, avg batch time: 0.3091, average loss: 12.1063
[11/23 02:17:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.99	
[11/23 02:17:31 visual_prompt]: Stopping early.
[11/23 02:17:31 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 02:17:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 02:17:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/23 02:17:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 02:17:31 visual_prompt]: Training with config:
[11/23 02:17:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 02:17:31 visual_prompt]: Loading training data...
[11/23 02:17:31 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 02:17:31 visual_prompt]: Loading validation data...
[11/23 02:17:31 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 02:17:31 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 02:17:34 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 02:17:34 visual_prompt]: tuned percent:0.525
[11/23 02:17:34 visual_prompt]: Device used for model: 0
[11/23 02:17:34 visual_prompt]: Setting up Evaluator...
[11/23 02:17:34 visual_prompt]: Setting up Trainer...
[11/23 02:17:34 visual_prompt]: 	Setting up the optimizer...
[11/23 02:17:34 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 02:19:19 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8199 s / batch. (data: 1.06e-02). ETA=12:34:16, max mem: 20.9 GB 
[11/23 02:20:59 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 3.06e-04). ETA=12:47:42, max mem: 20.9 GB 
[11/23 02:22:43 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.5627 s / batch. (data: 7.39e-01). ETA=23:52:26, max mem: 20.9 GB 
[11/23 02:24:24 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8171 s / batch. (data: 7.95e-03). ETA=12:27:41, max mem: 20.9 GB 
[11/23 02:26:07 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8278 s / batch. (data: 5.46e-03). ETA=12:36:04, max mem: 20.9 GB 
[11/23 02:27:01 visual_prompt]: Epoch 1 / 100: avg data time: 1.99e-01, avg batch time: 1.0252, average train loss: 1.5403
[11/23 02:28:00 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3081, average loss: 1.5201
[11/23 02:28:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 02:28:00 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/23 02:29:44 visual_prompt]: 	Training 100/553. train loss: 0.9062,	0.9657 s / batch. (data: 1.40e-01). ETA=14:39:34, max mem: 20.9 GB 
[11/23 02:31:26 visual_prompt]: 	Training 200/553. train loss: 0.0004,	1.5344 s / batch. (data: 7.13e-01). ETA=23:14:54, max mem: 20.9 GB 
[11/23 02:33:09 visual_prompt]: 	Training 300/553. train loss: 2.3715,	1.1716 s / batch. (data: 3.17e-01). ETA=17:43:10, max mem: 20.9 GB 
[11/23 02:34:49 visual_prompt]: 	Training 400/553. train loss: 0.8650,	0.8108 s / batch. (data: 3.44e-04). ETA=12:14:26, max mem: 20.9 GB 
[11/23 02:36:33 visual_prompt]: 	Training 500/553. train loss: 0.7718,	0.8250 s / batch. (data: 1.20e-02). ETA=12:25:54, max mem: 20.9 GB 
[11/23 02:37:25 visual_prompt]: Epoch 2 / 100: avg data time: 1.98e-01, avg batch time: 1.0225, average train loss: 1.8079
[11/23 02:38:24 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3092, average loss: 2.7599
[11/23 02:38:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.04	
[11/23 02:38:24 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/23 02:40:07 visual_prompt]: 	Training 100/553. train loss: 0.8204,	0.9597 s / batch. (data: 1.23e-01). ETA=14:25:16, max mem: 20.9 GB 
[11/23 02:41:50 visual_prompt]: 	Training 200/553. train loss: 0.8124,	1.3801 s / batch. (data: 5.46e-01). ETA=20:41:56, max mem: 20.9 GB 
[11/23 02:43:30 visual_prompt]: 	Training 300/553. train loss: 1.1878,	0.8560 s / batch. (data: 1.20e-02). ETA=12:48:52, max mem: 20.9 GB 
[11/23 02:45:12 visual_prompt]: 	Training 400/553. train loss: 3.5394,	0.8293 s / batch. (data: 1.05e-02). ETA=12:23:31, max mem: 20.9 GB 
[11/23 02:46:55 visual_prompt]: 	Training 500/553. train loss: 0.9536,	1.4236 s / batch. (data: 5.94e-01). ETA=21:13:58, max mem: 20.9 GB 
[11/23 02:47:47 visual_prompt]: Epoch 3 / 100: avg data time: 1.92e-01, avg batch time: 1.0182, average train loss: 2.4124
[11/23 02:48:45 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.3066, average loss: 3.8518
[11/23 02:48:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.31	
[11/23 02:48:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/23 02:50:33 visual_prompt]: 	Training 100/553. train loss: 5.8539,	0.8507 s / batch. (data: 1.05e-02). ETA=12:39:06, max mem: 20.9 GB 
[11/23 02:52:15 visual_prompt]: 	Training 200/553. train loss: 4.1352,	0.8400 s / batch. (data: 3.60e-04). ETA=12:28:09, max mem: 20.9 GB 
[11/23 02:53:56 visual_prompt]: 	Training 300/553. train loss: 1.8282,	1.3817 s / batch. (data: 5.54e-01). ETA=20:28:19, max mem: 20.9 GB 
[11/23 02:55:34 visual_prompt]: 	Training 400/553. train loss: 22.1794,	1.4208 s / batch. (data: 6.14e-01). ETA=21:00:45, max mem: 20.9 GB 
[11/23 02:57:18 visual_prompt]: 	Training 500/553. train loss: 0.6721,	3.5217 s / batch. (data: 2.71e+00). ETA=2 days, 3:59:08, max mem: 20.9 GB 
[11/23 02:58:12 visual_prompt]: Epoch 4 / 100: avg data time: 1.99e-01, avg batch time: 1.0244, average train loss: 3.9943
[11/23 02:59:10 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3078, average loss: 1.0283
[11/23 02:59:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.76	
[11/23 02:59:10 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/23 03:00:55 visual_prompt]: 	Training 100/553. train loss: 15.8534,	0.8280 s / batch. (data: 2.96e-04). ETA=12:11:15, max mem: 20.9 GB 
[11/23 03:02:35 visual_prompt]: 	Training 200/553. train loss: 0.5989,	1.5342 s / batch. (data: 6.98e-01). ETA=22:32:19, max mem: 20.9 GB 
[11/23 03:04:18 visual_prompt]: 	Training 300/553. train loss: 20.0197,	0.8391 s / batch. (data: 1.51e-02). ETA=12:18:15, max mem: 20.9 GB 
[11/23 03:05:59 visual_prompt]: 	Training 400/553. train loss: 3.1582,	0.8200 s / batch. (data: 3.17e-04). ETA=12:00:05, max mem: 20.9 GB 
[11/23 03:07:41 visual_prompt]: 	Training 500/553. train loss: 6.6953,	0.8275 s / batch. (data: 7.97e-03). ETA=12:05:16, max mem: 20.9 GB 
[11/23 03:08:35 visual_prompt]: Epoch 5 / 100: avg data time: 1.95e-01, avg batch time: 1.0214, average train loss: 5.6487
[11/23 03:09:34 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.3082, average loss: 10.8251
[11/23 03:09:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/23 03:09:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/23 03:11:21 visual_prompt]: 	Training 100/553. train loss: 0.5371,	0.8160 s / batch. (data: 3.19e-04). ETA=11:53:07, max mem: 20.9 GB 
[11/23 03:13:02 visual_prompt]: 	Training 200/553. train loss: 22.6648,	0.8360 s / batch. (data: 1.05e-02). ETA=12:09:14, max mem: 20.9 GB 
[11/23 03:14:43 visual_prompt]: 	Training 300/553. train loss: 5.7618,	0.8280 s / batch. (data: 7.95e-03). ETA=12:00:50, max mem: 20.9 GB 
[11/23 03:16:28 visual_prompt]: 	Training 400/553. train loss: 2.8443,	0.8360 s / batch. (data: 3.36e-04). ETA=12:06:23, max mem: 20.9 GB 
[11/23 03:18:09 visual_prompt]: 	Training 500/553. train loss: 16.9213,	0.8241 s / batch. (data: 4.86e-04). ETA=11:54:40, max mem: 20.9 GB 
[11/23 03:19:01 visual_prompt]: Epoch 6 / 100: avg data time: 2.00e-01, avg batch time: 1.0255, average train loss: 7.5020
[11/23 03:19:59 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.3080, average loss: 3.5944
[11/23 03:19:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.87	
[11/23 03:19:59 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/23 03:21:44 visual_prompt]: 	Training 100/553. train loss: 8.8278,	0.8760 s / batch. (data: 7.97e-03). ETA=12:37:28, max mem: 20.9 GB 
[11/23 03:23:25 visual_prompt]: 	Training 200/553. train loss: 3.8332,	0.8360 s / batch. (data: 3.43e-04). ETA=12:01:28, max mem: 20.9 GB 
[11/23 03:25:11 visual_prompt]: 	Training 300/553. train loss: 4.1304,	1.6120 s / batch. (data: 7.78e-01). ETA=23:08:29, max mem: 20.9 GB 
[11/23 03:26:52 visual_prompt]: 	Training 400/553. train loss: 4.4043,	2.0040 s / batch. (data: 1.18e+00). ETA=1 day, 4:42:49, max mem: 20.9 GB 
[11/23 03:28:33 visual_prompt]: 	Training 500/553. train loss: 33.8506,	0.8307 s / batch. (data: 1.44e-02). ETA=11:52:47, max mem: 20.9 GB 
[11/23 03:29:25 visual_prompt]: Epoch 7 / 100: avg data time: 1.97e-01, avg batch time: 1.0228, average train loss: 9.9244
[11/23 03:30:23 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.3082, average loss: 0.8425
[11/23 03:30:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.95	
[11/23 03:30:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/23 03:32:07 visual_prompt]: 	Training 100/553. train loss: 7.2772,	0.8269 s / batch. (data: 3.32e-04). ETA=11:47:25, max mem: 20.9 GB 
[11/23 03:33:51 visual_prompt]: 	Training 200/553. train loss: 4.0536,	0.8484 s / batch. (data: 2.39e-02). ETA=12:04:24, max mem: 20.9 GB 
[11/23 03:35:33 visual_prompt]: 	Training 300/553. train loss: 4.9816,	0.8555 s / batch. (data: 5.97e-03). ETA=12:09:02, max mem: 20.9 GB 
[11/23 03:37:15 visual_prompt]: 	Training 400/553. train loss: 9.5666,	0.8360 s / batch. (data: 3.46e-04). ETA=11:50:59, max mem: 20.9 GB 
[11/23 03:38:58 visual_prompt]: 	Training 500/553. train loss: 32.3815,	1.6040 s / batch. (data: 7.80e-01). ETA=22:41:31, max mem: 20.9 GB 
[11/23 03:39:51 visual_prompt]: Epoch 8 / 100: avg data time: 2.01e-01, avg batch time: 1.0269, average train loss: 9.7738
[11/23 03:40:49 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3060, average loss: 8.3319
[11/23 03:40:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.51	
[11/23 03:40:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/23 03:42:35 visual_prompt]: 	Training 100/553. train loss: 8.8853,	0.8282 s / batch. (data: 3.51e-04). ETA=11:40:50, max mem: 20.9 GB 
[11/23 03:44:15 visual_prompt]: 	Training 200/553. train loss: 10.7656,	0.8440 s / batch. (data: 1.19e-02). ETA=11:52:51, max mem: 20.9 GB 
[11/23 03:45:57 visual_prompt]: 	Training 300/553. train loss: 3.2274,	1.8881 s / batch. (data: 1.07e+00). ETA=1 day, 2:31:32, max mem: 20.9 GB 
[11/23 03:47:41 visual_prompt]: 	Training 400/553. train loss: 3.7891,	0.8271 s / batch. (data: 8.43e-04). ETA=11:35:46, max mem: 20.9 GB 
[11/23 03:49:23 visual_prompt]: 	Training 500/553. train loss: 5.6567,	1.0520 s / batch. (data: 2.37e-01). ETA=14:43:15, max mem: 20.9 GB 
[11/23 03:50:15 visual_prompt]: Epoch 9 / 100: avg data time: 1.97e-01, avg batch time: 1.0231, average train loss: 9.8596
[11/23 03:51:13 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3067, average loss: 9.2470
[11/23 03:51:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.70	
[11/23 03:51:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/23 03:53:01 visual_prompt]: 	Training 100/553. train loss: 79.6099,	0.8265 s / batch. (data: 3.11e-04). ETA=11:31:48, max mem: 20.9 GB 
[11/23 03:54:42 visual_prompt]: 	Training 200/553. train loss: 2.5538,	0.8401 s / batch. (data: 1.05e-02). ETA=11:41:47, max mem: 20.9 GB 
[11/23 03:56:22 visual_prompt]: 	Training 300/553. train loss: 30.6063,	0.8440 s / batch. (data: 3.23e-04). ETA=11:43:39, max mem: 20.9 GB 
[11/23 03:58:03 visual_prompt]: 	Training 400/553. train loss: 14.8067,	0.8103 s / batch. (data: 3.34e-04). ETA=11:14:13, max mem: 20.9 GB 
[11/23 03:59:45 visual_prompt]: 	Training 500/553. train loss: 11.6864,	1.0500 s / batch. (data: 2.17e-01). ETA=14:31:52, max mem: 20.9 GB 
[11/23 04:00:39 visual_prompt]: Epoch 10 / 100: avg data time: 1.96e-01, avg batch time: 1.0218, average train loss: 16.9430
[11/23 04:01:37 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.3075, average loss: 3.1175
[11/23 04:01:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[11/23 04:01:37 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/23 04:03:25 visual_prompt]: 	Training 100/553. train loss: 8.8875,	0.8427 s / batch. (data: 2.04e-02). ETA=11:37:38, max mem: 20.9 GB 
[11/23 04:05:08 visual_prompt]: 	Training 200/553. train loss: 1.5306,	0.8681 s / batch. (data: 8.49e-04). ETA=11:57:12, max mem: 20.9 GB 
[11/23 04:06:49 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9201 s / batch. (data: 1.09e+00). ETA=1 day, 2:23:08, max mem: 20.9 GB 
[11/23 04:08:29 visual_prompt]: 	Training 400/553. train loss: 8.2091,	0.8440 s / batch. (data: 7.99e-03). ETA=11:34:28, max mem: 20.9 GB 
[11/23 04:10:09 visual_prompt]: 	Training 500/553. train loss: 4.4132,	0.8280 s / batch. (data: 3.38e-04). ETA=11:19:56, max mem: 20.9 GB 
[11/23 04:11:02 visual_prompt]: Epoch 11 / 100: avg data time: 1.95e-01, avg batch time: 1.0216, average train loss: 17.7631
[11/23 04:12:00 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3071, average loss: 2.8473
[11/23 04:12:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.31	
[11/23 04:12:00 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/23 04:13:48 visual_prompt]: 	Training 100/553. train loss: 6.9889,	0.8369 s / batch. (data: 2.45e-02). ETA=11:25:07, max mem: 20.9 GB 
[11/23 04:15:30 visual_prompt]: 	Training 200/553. train loss: 3.7040,	0.8319 s / batch. (data: 7.98e-03). ETA=11:19:38, max mem: 20.9 GB 
[11/23 04:17:11 visual_prompt]: 	Training 300/553. train loss: 6.6281,	0.8240 s / batch. (data: 5.43e-03). ETA=11:11:47, max mem: 20.9 GB 
[11/23 04:18:53 visual_prompt]: 	Training 400/553. train loss: 1.5824,	0.8393 s / batch. (data: 3.11e-04). ETA=11:22:50, max mem: 20.9 GB 
[11/23 04:20:35 visual_prompt]: 	Training 500/553. train loss: 67.3412,	0.8454 s / batch. (data: 8.36e-04). ETA=11:26:23, max mem: 20.9 GB 
[11/23 04:21:27 visual_prompt]: Epoch 12 / 100: avg data time: 1.98e-01, avg batch time: 1.0243, average train loss: 17.0899
[11/23 04:22:25 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3064, average loss: 4.5959
[11/23 04:22:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.78	
[11/23 04:22:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/23 04:24:12 visual_prompt]: 	Training 100/553. train loss: 5.7661,	0.8080 s / batch. (data: 3.34e-04). ETA=10:53:58, max mem: 20.9 GB 
[11/23 04:25:51 visual_prompt]: 	Training 200/553. train loss: 3.9897,	0.8360 s / batch. (data: 3.17e-04). ETA=11:15:15, max mem: 20.9 GB 
[11/23 04:27:33 visual_prompt]: 	Training 300/553. train loss: 12.3657,	2.0323 s / batch. (data: 1.21e+00). ETA=1 day, 3:18:11, max mem: 20.9 GB 
[11/23 04:29:14 visual_prompt]: 	Training 400/553. train loss: 35.4608,	0.8432 s / batch. (data: 4.67e-03). ETA=11:18:17, max mem: 20.9 GB 
[11/23 04:30:57 visual_prompt]: 	Training 500/553. train loss: 4.5832,	0.8360 s / batch. (data: 3.65e-04). ETA=11:11:07, max mem: 20.9 GB 
[11/23 04:31:50 visual_prompt]: Epoch 13 / 100: avg data time: 1.96e-01, avg batch time: 1.0214, average train loss: 15.4765
[11/23 04:32:48 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.3074, average loss: 11.7818
[11/23 04:32:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.04	
[11/23 04:32:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/23 04:34:35 visual_prompt]: 	Training 100/553. train loss: 21.9701,	0.8421 s / batch. (data: 3.12e-04). ETA=11:13:48, max mem: 20.9 GB 
[11/23 04:36:17 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1703 s / batch. (data: 3.61e-01). ETA=15:34:29, max mem: 20.9 GB 
[11/23 04:37:58 visual_prompt]: 	Training 300/553. train loss: 42.3929,	0.8197 s / batch. (data: 7.99e-03). ETA=10:53:09, max mem: 20.9 GB 
[11/23 04:39:40 visual_prompt]: 	Training 400/553. train loss: 13.3594,	0.8400 s / batch. (data: 3.24e-04). ETA=11:07:55, max mem: 20.9 GB 
[11/23 04:41:22 visual_prompt]: 	Training 500/553. train loss: 32.8339,	0.8302 s / batch. (data: 7.95e-03). ETA=10:58:46, max mem: 20.9 GB 
[11/23 04:42:14 visual_prompt]: Epoch 14 / 100: avg data time: 1.97e-01, avg batch time: 1.0228, average train loss: 14.9519
[11/23 04:43:13 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3080, average loss: 8.8228
[11/23 04:43:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.32	
[11/23 04:43:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/23 04:44:58 visual_prompt]: 	Training 100/553. train loss: 8.3991,	0.8325 s / batch. (data: 1.64e-02). ETA=10:58:30, max mem: 20.9 GB 
[11/23 04:46:39 visual_prompt]: 	Training 200/553. train loss: 28.2339,	0.8185 s / batch. (data: 7.96e-03). ETA=10:46:04, max mem: 20.9 GB 
[11/23 04:48:22 visual_prompt]: 	Training 300/553. train loss: 34.7296,	0.8360 s / batch. (data: 2.41e-02). ETA=10:58:26, max mem: 20.9 GB 
[11/23 04:50:01 visual_prompt]: 	Training 400/553. train loss: 0.6504,	1.1007 s / batch. (data: 2.90e-01). ETA=14:25:07, max mem: 20.9 GB 
[11/23 04:51:44 visual_prompt]: 	Training 500/553. train loss: 9.5385,	0.8480 s / batch. (data: 3.72e-04). ETA=11:05:04, max mem: 20.9 GB 
[11/23 04:52:38 visual_prompt]: Epoch 15 / 100: avg data time: 1.96e-01, avg batch time: 1.0221, average train loss: 18.0007
[11/23 04:53:37 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3077, average loss: 21.3152
[11/23 04:53:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.15	
[11/23 04:53:37 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/23 04:55:21 visual_prompt]: 	Training 100/553. train loss: 24.1684,	0.8360 s / batch. (data: 3.08e-04). ETA=10:53:31, max mem: 20.9 GB 
[11/23 04:57:03 visual_prompt]: 	Training 200/553. train loss: 22.2943,	0.8701 s / batch. (data: 1.40e-02). ETA=11:18:42, max mem: 20.9 GB 
[11/23 04:58:45 visual_prompt]: 	Training 300/553. train loss: 19.5646,	0.8269 s / batch. (data: 1.49e-02). ETA=10:43:40, max mem: 20.9 GB 
[11/23 05:00:27 visual_prompt]: 	Training 400/553. train loss: 10.7294,	0.8280 s / batch. (data: 8.29e-04). ETA=10:43:08, max mem: 20.9 GB 
[11/23 05:02:08 visual_prompt]: 	Training 500/553. train loss: 7.4963,	1.4521 s / batch. (data: 6.31e-01). ETA=18:45:28, max mem: 20.9 GB 
[11/23 05:03:02 visual_prompt]: Epoch 16 / 100: avg data time: 1.96e-01, avg batch time: 1.0219, average train loss: 16.8012
[11/23 05:04:00 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3090, average loss: 1.8160
[11/23 05:04:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.65	
[11/23 05:04:00 visual_prompt]: Best epoch 16: best metric: -1.816
[11/23 05:04:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/23 05:05:46 visual_prompt]: 	Training 100/553. train loss: 3.3493,	0.8360 s / batch. (data: 1.61e-02). ETA=10:45:49, max mem: 20.9 GB 
[11/23 05:07:28 visual_prompt]: 	Training 200/553. train loss: 6.1398,	0.8280 s / batch. (data: 3.16e-04). ETA=10:38:16, max mem: 20.9 GB 
[11/23 05:09:10 visual_prompt]: 	Training 300/553. train loss: 0.5005,	0.8218 s / batch. (data: 3.08e-04). ETA=10:32:06, max mem: 20.9 GB 
[11/23 05:10:51 visual_prompt]: 	Training 400/553. train loss: 5.0081,	1.3087 s / batch. (data: 4.88e-01). ETA=16:44:27, max mem: 20.9 GB 
[11/23 05:12:32 visual_prompt]: 	Training 500/553. train loss: 4.7998,	1.7307 s / batch. (data: 9.11e-01). ETA=22:05:29, max mem: 20.9 GB 
[11/23 05:13:27 visual_prompt]: Epoch 17 / 100: avg data time: 2.00e-01, avg batch time: 1.0247, average train loss: 15.2727
[11/23 05:14:25 visual_prompt]: Inference (val):avg data time: 4.44e-04, avg batch time: 0.3096, average loss: 18.4460
[11/23 05:14:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/23 05:14:25 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/23 05:16:11 visual_prompt]: 	Training 100/553. train loss: 28.6263,	0.8373 s / batch. (data: 1.19e-02). ETA=10:39:08, max mem: 20.9 GB 
[11/23 05:17:55 visual_prompt]: 	Training 200/553. train loss: 32.5979,	0.8153 s / batch. (data: 2.99e-04). ETA=10:20:57, max mem: 20.9 GB 
[11/23 05:19:37 visual_prompt]: 	Training 300/553. train loss: 8.1456,	0.8360 s / batch. (data: 3.18e-04). ETA=10:35:20, max mem: 20.9 GB 
[11/23 05:21:19 visual_prompt]: 	Training 400/553. train loss: 0.7908,	0.8182 s / batch. (data: 3.16e-04). ETA=10:20:26, max mem: 20.9 GB 
[11/23 05:23:01 visual_prompt]: 	Training 500/553. train loss: 7.3902,	0.8280 s / batch. (data: 5.19e-04). ETA=10:26:28, max mem: 20.9 GB 
[11/23 05:23:53 visual_prompt]: Epoch 18 / 100: avg data time: 1.99e-01, avg batch time: 1.0259, average train loss: 14.4648
[11/23 05:24:51 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.3085, average loss: 4.8361
[11/23 05:24:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.09	
[11/23 05:24:51 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/23 05:26:37 visual_prompt]: 	Training 100/553. train loss: 2.9938,	0.9570 s / batch. (data: 1.35e-01). ETA=12:01:38, max mem: 20.9 GB 
[11/23 05:28:19 visual_prompt]: 	Training 200/553. train loss: 9.4797,	0.8320 s / batch. (data: 3.11e-04). ETA=10:26:02, max mem: 20.9 GB 
[11/23 05:30:01 visual_prompt]: 	Training 300/553. train loss: 67.5854,	0.8353 s / batch. (data: 1.17e-02). ETA=10:27:05, max mem: 20.9 GB 
[11/23 05:31:44 visual_prompt]: 	Training 400/553. train loss: 6.8751,	0.8125 s / batch. (data: 3.07e-04). ETA=10:08:39, max mem: 20.9 GB 
[11/23 05:33:21 visual_prompt]: 	Training 500/553. train loss: 5.4041,	0.8240 s / batch. (data: 3.30e-04). ETA=10:15:54, max mem: 20.9 GB 
[11/23 05:34:15 visual_prompt]: Epoch 19 / 100: avg data time: 1.93e-01, avg batch time: 1.0190, average train loss: 14.8815
[11/23 05:35:13 visual_prompt]: Inference (val):avg data time: 4.00e-04, avg batch time: 0.3099, average loss: 20.3297
[11/23 05:35:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.58	
[11/23 05:35:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/23 05:36:57 visual_prompt]: 	Training 100/553. train loss: 3.2001,	0.8207 s / batch. (data: 8.55e-03). ETA=10:11:19, max mem: 20.9 GB 
[11/23 05:38:40 visual_prompt]: 	Training 200/553. train loss: 48.3757,	0.8600 s / batch. (data: 3.17e-04). ETA=10:39:11, max mem: 20.9 GB 
[11/23 05:40:23 visual_prompt]: 	Training 300/553. train loss: 2.0789,	0.8680 s / batch. (data: 8.71e-04). ETA=10:43:39, max mem: 20.9 GB 
[11/23 05:42:04 visual_prompt]: 	Training 400/553. train loss: 1.1873,	0.8393 s / batch. (data: 5.46e-03). ETA=10:20:57, max mem: 20.9 GB 
[11/23 05:43:46 visual_prompt]: 	Training 500/553. train loss: 11.6085,	0.8280 s / batch. (data: 3.06e-04). ETA=10:11:15, max mem: 20.9 GB 
[11/23 05:44:40 visual_prompt]: Epoch 20 / 100: avg data time: 1.99e-01, avg batch time: 1.0257, average train loss: 16.6486
[11/23 05:45:39 visual_prompt]: Inference (val):avg data time: 3.78e-04, avg batch time: 0.3100, average loss: 26.0773
[11/23 05:45:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.82	
[11/23 05:45:39 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/23 05:47:26 visual_prompt]: 	Training 100/553. train loss: 11.1664,	0.8352 s / batch. (data: 1.53e-02). ETA=10:14:24, max mem: 20.9 GB 
[11/23 05:49:07 visual_prompt]: 	Training 200/553. train loss: 27.7361,	0.8477 s / batch. (data: 1.44e-02). ETA=10:22:11, max mem: 20.9 GB 
[11/23 05:50:48 visual_prompt]: 	Training 300/553. train loss: 31.1452,	0.9421 s / batch. (data: 1.24e-01). ETA=11:29:55, max mem: 20.9 GB 
[11/23 05:52:29 visual_prompt]: 	Training 400/553. train loss: 5.8189,	0.8105 s / batch. (data: 3.17e-04). ETA=9:52:11, max mem: 20.9 GB 
[11/23 05:54:12 visual_prompt]: 	Training 500/553. train loss: 26.2486,	0.8277 s / batch. (data: 3.21e-04). ETA=10:03:23, max mem: 20.9 GB 
[11/23 05:55:05 visual_prompt]: Epoch 21 / 100: avg data time: 1.97e-01, avg batch time: 1.0233, average train loss: 15.1601
[11/23 05:56:03 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3092, average loss: 8.8783
[11/23 05:56:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 51.47	
[11/23 05:56:03 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/23 05:57:48 visual_prompt]: 	Training 100/553. train loss: 46.0157,	0.8038 s / batch. (data: 3.24e-04). ETA=9:43:54, max mem: 20.9 GB 
[11/23 05:59:30 visual_prompt]: 	Training 200/553. train loss: 3.3613,	0.8410 s / batch. (data: 2.01e-02). ETA=10:09:34, max mem: 20.9 GB 
[11/23 06:01:10 visual_prompt]: 	Training 300/553. train loss: 0.0013,	0.8380 s / batch. (data: 3.17e-04). ETA=10:05:57, max mem: 20.9 GB 
[11/23 06:02:53 visual_prompt]: 	Training 400/553. train loss: 23.6424,	0.8184 s / batch. (data: 7.94e-03). ETA=9:50:27, max mem: 20.9 GB 
[11/23 06:04:36 visual_prompt]: 	Training 500/553. train loss: 4.7286,	0.8203 s / batch. (data: 3.18e-04). ETA=9:50:24, max mem: 20.9 GB 
[11/23 06:05:30 visual_prompt]: Epoch 22 / 100: avg data time: 2.00e-01, avg batch time: 1.0258, average train loss: 13.4969
[11/23 06:06:29 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.3062, average loss: 11.3088
[11/23 06:06:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.83	
[11/23 06:06:29 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/23 06:08:16 visual_prompt]: 	Training 100/553. train loss: 2.1520,	1.2360 s / batch. (data: 3.76e-01). ETA=14:46:30, max mem: 20.9 GB 
[11/23 06:09:59 visual_prompt]: 	Training 200/553. train loss: 30.8022,	0.9520 s / batch. (data: 1.21e-01). ETA=11:21:13, max mem: 20.9 GB 
[11/23 06:11:42 visual_prompt]: 	Training 300/553. train loss: 8.1021,	0.8319 s / batch. (data: 3.34e-04). ETA=9:53:54, max mem: 20.9 GB 
[11/23 06:13:22 visual_prompt]: 	Training 400/553. train loss: 5.6357,	0.8468 s / batch. (data: 8.43e-04). ETA=10:03:08, max mem: 20.9 GB 
[11/23 06:15:02 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.01e-04). ETA=9:42:40, max mem: 20.9 GB 
[11/23 06:15:55 visual_prompt]: Epoch 23 / 100: avg data time: 1.98e-01, avg batch time: 1.0243, average train loss: 16.6052
[11/23 06:16:54 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.3069, average loss: 17.7758
[11/23 06:16:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.79	
[11/23 06:16:54 visual_prompt]: Stopping early.
[11/23 06:16:54 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 06:16:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 06:16:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/23 06:16:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 06:16:54 visual_prompt]: Training with config:
[11/23 06:16:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 06:16:54 visual_prompt]: Loading training data...
[11/23 06:16:54 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 06:16:54 visual_prompt]: Loading validation data...
[11/23 06:16:54 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 06:16:54 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 06:16:56 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 06:16:56 visual_prompt]: tuned percent:0.525
[11/23 06:16:57 visual_prompt]: Device used for model: 0
[11/23 06:16:57 visual_prompt]: Setting up Evaluator...
[11/23 06:16:57 visual_prompt]: Setting up Trainer...
[11/23 06:16:57 visual_prompt]: 	Setting up the optimizer...
[11/23 06:16:57 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 06:18:42 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8137 s / batch. (data: 5.46e-03). ETA=12:28:36, max mem: 20.9 GB 
[11/23 06:20:23 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8280 s / batch. (data: 3.24e-04). ETA=12:40:22, max mem: 20.9 GB 
[11/23 06:22:07 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.9920 s / batch. (data: 1.58e-01). ETA=15:09:19, max mem: 20.9 GB 
[11/23 06:23:47 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8240 s / batch. (data: 3.29e-04). ETA=12:33:55, max mem: 20.9 GB 
[11/23 06:25:32 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8292 s / batch. (data: 8.40e-04). ETA=12:37:21, max mem: 20.9 GB 
[11/23 06:26:26 visual_prompt]: Epoch 1 / 100: avg data time: 2.02e-01, avg batch time: 1.0288, average train loss: 1.5403
[11/23 06:27:24 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3077, average loss: 1.5201
[11/23 06:27:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 06:27:24 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/23 06:29:09 visual_prompt]: 	Training 100/553. train loss: 3.5035,	0.8306 s / batch. (data: 3.29e-04). ETA=12:36:27, max mem: 20.9 GB 
[11/23 06:30:50 visual_prompt]: 	Training 200/553. train loss: 0.0003,	0.8319 s / batch. (data: 5.74e-04). ETA=12:36:20, max mem: 20.9 GB 
[11/23 06:32:34 visual_prompt]: 	Training 300/553. train loss: 0.8492,	1.2022 s / batch. (data: 3.76e-01). ETA=18:10:54, max mem: 20.9 GB 
[11/23 06:34:15 visual_prompt]: 	Training 400/553. train loss: 4.5294,	0.8494 s / batch. (data: 5.63e-03). ETA=12:49:24, max mem: 20.9 GB 
[11/23 06:35:58 visual_prompt]: 	Training 500/553. train loss: 0.4798,	0.8281 s / batch. (data: 3.13e-04). ETA=12:28:39, max mem: 20.9 GB 
[11/23 06:36:51 visual_prompt]: Epoch 2 / 100: avg data time: 1.99e-01, avg batch time: 1.0250, average train loss: 2.0718
[11/23 06:37:49 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3066, average loss: 1.9715
[11/23 06:37:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.22	
[11/23 06:37:49 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/23 06:39:33 visual_prompt]: 	Training 100/553. train loss: 0.6295,	0.8337 s / batch. (data: 5.43e-03). ETA=12:31:40, max mem: 20.9 GB 
[11/23 06:41:16 visual_prompt]: 	Training 200/553. train loss: 0.9610,	1.5395 s / batch. (data: 7.14e-01). ETA=23:05:22, max mem: 20.9 GB 
[11/23 06:42:57 visual_prompt]: 	Training 300/553. train loss: 2.4749,	0.8205 s / batch. (data: 1.12e-02). ETA=12:16:57, max mem: 20.9 GB 
[11/23 06:44:40 visual_prompt]: 	Training 400/553. train loss: 0.2250,	0.8207 s / batch. (data: 3.12e-04). ETA=12:15:48, max mem: 20.9 GB 
[11/23 06:46:23 visual_prompt]: 	Training 500/553. train loss: 1.0738,	1.5126 s / batch. (data: 6.84e-01). ETA=22:33:39, max mem: 20.9 GB 
[11/23 06:47:15 visual_prompt]: Epoch 3 / 100: avg data time: 1.98e-01, avg batch time: 1.0232, average train loss: 2.3930
[11/23 06:48:13 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3081, average loss: 2.9331
[11/23 06:48:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.74	
[11/23 06:48:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/23 06:50:00 visual_prompt]: 	Training 100/553. train loss: 2.0854,	0.8400 s / batch. (data: 1.20e-02). ETA=12:29:35, max mem: 20.9 GB 
[11/23 06:51:43 visual_prompt]: 	Training 200/553. train loss: 1.2467,	0.8320 s / batch. (data: 3.33e-04). ETA=12:21:02, max mem: 20.9 GB 
[11/23 06:53:24 visual_prompt]: 	Training 300/553. train loss: 0.7352,	1.5253 s / batch. (data: 7.03e-01). ETA=22:36:02, max mem: 20.9 GB 
[11/23 06:55:02 visual_prompt]: 	Training 400/553. train loss: 3.3312,	1.6326 s / batch. (data: 8.19e-01). ETA=1 day, 0:08:39, max mem: 20.9 GB 
[11/23 06:56:46 visual_prompt]: 	Training 500/553. train loss: 13.1551,	3.6278 s / batch. (data: 2.81e+00). ETA=2 days, 5:33:05, max mem: 20.9 GB 
[11/23 06:57:40 visual_prompt]: Epoch 4 / 100: avg data time: 1.99e-01, avg batch time: 1.0253, average train loss: 3.2573
[11/23 06:58:39 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3069, average loss: 3.1964
[11/23 06:58:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.71	
[11/23 06:58:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/23 07:00:24 visual_prompt]: 	Training 100/553. train loss: 4.0519,	0.8390 s / batch. (data: 1.56e-02). ETA=12:20:55, max mem: 20.9 GB 
[11/23 07:02:05 visual_prompt]: 	Training 200/553. train loss: 8.2089,	1.3040 s / batch. (data: 4.88e-01). ETA=19:09:26, max mem: 20.9 GB 
[11/23 07:03:48 visual_prompt]: 	Training 300/553. train loss: 2.9179,	0.8430 s / batch. (data: 3.27e-04). ETA=12:21:39, max mem: 20.9 GB 
[11/23 07:05:29 visual_prompt]: 	Training 400/553. train loss: 3.8703,	0.8280 s / batch. (data: 2.95e-04). ETA=12:07:05, max mem: 20.9 GB 
[11/23 07:07:11 visual_prompt]: 	Training 500/553. train loss: 9.9950,	0.8400 s / batch. (data: 1.20e-02). ETA=12:16:14, max mem: 20.9 GB 
[11/23 07:08:05 visual_prompt]: Epoch 5 / 100: avg data time: 1.98e-01, avg batch time: 1.0236, average train loss: 6.9270
[11/23 07:09:03 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3093, average loss: 4.2445
[11/23 07:09:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.30	
[11/23 07:09:03 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/23 07:10:51 visual_prompt]: 	Training 100/553. train loss: 4.6621,	0.8228 s / batch. (data: 5.46e-03). ETA=11:59:05, max mem: 20.9 GB 
[11/23 07:12:31 visual_prompt]: 	Training 200/553. train loss: 21.5458,	0.8360 s / batch. (data: 5.47e-03). ETA=12:09:11, max mem: 20.9 GB 
[11/23 07:14:12 visual_prompt]: 	Training 300/553. train loss: 1.0072,	0.8477 s / batch. (data: 2.39e-02). ETA=12:18:01, max mem: 20.9 GB 
[11/23 07:15:57 visual_prompt]: 	Training 400/553. train loss: 11.3812,	0.8402 s / batch. (data: 2.04e-02). ETA=12:10:06, max mem: 20.9 GB 
[11/23 07:17:38 visual_prompt]: 	Training 500/553. train loss: 8.4550,	0.8440 s / batch. (data: 4.12e-04). ETA=12:11:59, max mem: 20.9 GB 
[11/23 07:18:30 visual_prompt]: Epoch 6 / 100: avg data time: 1.99e-01, avg batch time: 1.0244, average train loss: 6.8165
[11/23 07:19:29 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3074, average loss: 2.8051
[11/23 07:19:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.58	
[11/23 07:19:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/23 07:21:13 visual_prompt]: 	Training 100/553. train loss: 6.3153,	0.8440 s / batch. (data: 3.06e-04). ETA=12:09:46, max mem: 20.9 GB 
[11/23 07:22:54 visual_prompt]: 	Training 200/553. train loss: 0.6921,	0.8384 s / batch. (data: 1.20e-02). ETA=12:03:31, max mem: 20.9 GB 
[11/23 07:24:40 visual_prompt]: 	Training 300/553. train loss: 3.9191,	2.2039 s / batch. (data: 1.37e+00). ETA=1 day, 7:38:21, max mem: 20.9 GB 
[11/23 07:26:22 visual_prompt]: 	Training 400/553. train loss: 2.8755,	1.5815 s / batch. (data: 7.36e-01). ETA=22:39:34, max mem: 20.9 GB 
[11/23 07:28:03 visual_prompt]: 	Training 500/553. train loss: 3.8488,	0.8349 s / batch. (data: 5.44e-03). ETA=11:56:24, max mem: 20.9 GB 
[11/23 07:28:54 visual_prompt]: Epoch 7 / 100: avg data time: 1.96e-01, avg batch time: 1.0224, average train loss: 4.8161
[11/23 07:29:53 visual_prompt]: Inference (val):avg data time: 8.22e-05, avg batch time: 0.3078, average loss: 4.5820
[11/23 07:29:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.39	
[11/23 07:29:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/23 07:31:36 visual_prompt]: 	Training 100/553. train loss: 3.1925,	0.8105 s / batch. (data: 3.02e-04). ETA=11:33:22, max mem: 20.9 GB 
[11/23 07:33:19 visual_prompt]: 	Training 200/553. train loss: 21.3880,	0.8205 s / batch. (data: 3.17e-04). ETA=11:40:32, max mem: 20.9 GB 
[11/23 07:35:02 visual_prompt]: 	Training 300/553. train loss: 1.0200,	0.8445 s / batch. (data: 3.09e-04). ETA=11:59:36, max mem: 20.9 GB 
[11/23 07:36:45 visual_prompt]: 	Training 400/553. train loss: 13.3662,	1.0160 s / batch. (data: 1.97e-01). ETA=14:24:05, max mem: 20.9 GB 
[11/23 07:38:28 visual_prompt]: 	Training 500/553. train loss: 0.2706,	1.5043 s / batch. (data: 6.95e-01). ETA=21:16:54, max mem: 20.9 GB 
[11/23 07:39:21 visual_prompt]: Epoch 8 / 100: avg data time: 2.02e-01, avg batch time: 1.0281, average train loss: 8.8876
[11/23 07:40:20 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3081, average loss: 1.0892
[11/23 07:40:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[11/23 07:40:20 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/23 07:42:05 visual_prompt]: 	Training 100/553. train loss: 0.0001,	0.8096 s / batch. (data: 3.17e-04). ETA=11:25:07, max mem: 20.9 GB 
[11/23 07:43:46 visual_prompt]: 	Training 200/553. train loss: 3.8978,	0.8224 s / batch. (data: 5.23e-04). ETA=11:34:35, max mem: 20.9 GB 
[11/23 07:45:28 visual_prompt]: 	Training 300/553. train loss: 3.0510,	1.9640 s / batch. (data: 1.15e+00). ETA=1 day, 3:35:32, max mem: 20.9 GB 
[11/23 07:47:12 visual_prompt]: 	Training 400/553. train loss: 1.7741,	0.8574 s / batch. (data: 3.16e-04). ETA=12:01:17, max mem: 20.9 GB 
[11/23 07:48:54 visual_prompt]: 	Training 500/553. train loss: 5.9535,	0.9803 s / batch. (data: 1.70e-01). ETA=13:43:01, max mem: 20.9 GB 
[11/23 07:49:47 visual_prompt]: Epoch 9 / 100: avg data time: 2.00e-01, avg batch time: 1.0261, average train loss: 7.6257
[11/23 07:50:45 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3085, average loss: 15.7628
[11/23 07:50:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.76	
[11/23 07:50:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/23 07:52:33 visual_prompt]: 	Training 100/553. train loss: 19.4007,	0.8237 s / batch. (data: 3.33e-04). ETA=11:29:29, max mem: 20.9 GB 
[11/23 07:54:14 visual_prompt]: 	Training 200/553. train loss: 0.5049,	0.8351 s / batch. (data: 3.14e-04). ETA=11:37:39, max mem: 20.9 GB 
[11/23 07:55:55 visual_prompt]: 	Training 300/553. train loss: 6.9126,	1.7360 s / batch. (data: 9.05e-01). ETA=1 day, 0:07:18, max mem: 20.9 GB 
[11/23 07:57:34 visual_prompt]: 	Training 400/553. train loss: 13.4654,	0.8224 s / batch. (data: 1.23e-02). ETA=11:24:14, max mem: 20.9 GB 
[11/23 07:59:17 visual_prompt]: 	Training 500/553. train loss: 2.0865,	0.8280 s / batch. (data: 3.47e-04). ETA=11:27:34, max mem: 20.9 GB 
[11/23 08:00:11 visual_prompt]: Epoch 10 / 100: avg data time: 1.97e-01, avg batch time: 1.0224, average train loss: 10.7126
[11/23 08:01:09 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3089, average loss: 5.2410
[11/23 08:01:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.73	
[11/23 08:01:09 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/23 08:02:56 visual_prompt]: 	Training 100/553. train loss: 12.9361,	0.8320 s / batch. (data: 2.97e-04). ETA=11:28:45, max mem: 20.9 GB 
[11/23 08:04:40 visual_prompt]: 	Training 200/553. train loss: 5.3582,	0.8377 s / batch. (data: 1.57e-02). ETA=11:32:06, max mem: 20.9 GB 
[11/23 08:06:21 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2151 s / batch. (data: 1.38e+00). ETA=1 day, 6:26:21, max mem: 20.9 GB 
[11/23 08:08:01 visual_prompt]: 	Training 400/553. train loss: 7.4933,	0.8240 s / batch. (data: 7.96e-03). ETA=11:18:01, max mem: 20.9 GB 
[11/23 08:09:42 visual_prompt]: 	Training 500/553. train loss: 0.5588,	0.8270 s / batch. (data: 3.32e-04). ETA=11:19:06, max mem: 20.9 GB 
[11/23 08:10:35 visual_prompt]: Epoch 11 / 100: avg data time: 1.95e-01, avg batch time: 1.0228, average train loss: 8.1792
[11/23 08:11:33 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3100, average loss: 11.4596
[11/23 08:11:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.25	
[11/23 08:11:33 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/23 08:13:20 visual_prompt]: 	Training 100/553. train loss: 0.3006,	0.9938 s / batch. (data: 1.50e-01). ETA=13:33:34, max mem: 20.9 GB 
[11/23 08:15:03 visual_prompt]: 	Training 200/553. train loss: 5.7857,	0.8085 s / batch. (data: 3.36e-04). ETA=11:00:28, max mem: 20.9 GB 
[11/23 08:16:43 visual_prompt]: 	Training 300/553. train loss: 13.7108,	0.8495 s / batch. (data: 3.34e-02). ETA=11:32:33, max mem: 20.9 GB 
[11/23 08:18:25 visual_prompt]: 	Training 400/553. train loss: 19.6701,	0.8440 s / batch. (data: 3.60e-04). ETA=11:26:40, max mem: 20.9 GB 
[11/23 08:20:08 visual_prompt]: 	Training 500/553. train loss: 23.8542,	0.8211 s / batch. (data: 3.33e-04). ETA=11:06:42, max mem: 20.9 GB 
[11/23 08:21:00 visual_prompt]: Epoch 12 / 100: avg data time: 2.00e-01, avg batch time: 1.0246, average train loss: 15.1743
[11/23 08:21:58 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3094, average loss: 90.2101
[11/23 08:21:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.14	
[11/23 08:21:58 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/23 08:23:45 visual_prompt]: 	Training 100/553. train loss: 1.5743,	0.8170 s / batch. (data: 3.32e-04). ETA=11:01:18, max mem: 20.9 GB 
[11/23 08:25:24 visual_prompt]: 	Training 200/553. train loss: 3.3038,	0.8200 s / batch. (data: 3.20e-04). ETA=11:02:21, max mem: 20.9 GB 
[11/23 08:27:06 visual_prompt]: 	Training 300/553. train loss: 8.8213,	1.9079 s / batch. (data: 1.08e+00). ETA=1 day, 1:37:55, max mem: 20.9 GB 
[11/23 08:28:47 visual_prompt]: 	Training 400/553. train loss: 14.4938,	0.8251 s / batch. (data: 9.05e-03). ETA=11:03:41, max mem: 20.9 GB 
[11/23 08:30:31 visual_prompt]: 	Training 500/553. train loss: 3.1417,	0.8364 s / batch. (data: 5.46e-03). ETA=11:11:26, max mem: 20.9 GB 
[11/23 08:31:24 visual_prompt]: Epoch 13 / 100: avg data time: 1.97e-01, avg batch time: 1.0230, average train loss: 10.2507
[11/23 08:32:22 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3085, average loss: 3.3825
[11/23 08:32:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.35	
[11/23 08:32:22 visual_prompt]: Best epoch 13: best metric: -3.383
[11/23 08:32:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/23 08:34:09 visual_prompt]: 	Training 100/553. train loss: 1.9361,	0.8282 s / batch. (data: 7.95e-03). ETA=11:02:41, max mem: 20.9 GB 
[11/23 08:35:51 visual_prompt]: 	Training 200/553. train loss: 0.0023,	1.2478 s / batch. (data: 4.19e-01). ETA=16:36:25, max mem: 20.9 GB 
[11/23 08:37:33 visual_prompt]: 	Training 300/553. train loss: 0.9843,	0.8464 s / batch. (data: 1.04e-02). ETA=11:14:28, max mem: 20.9 GB 
[11/23 08:39:14 visual_prompt]: 	Training 400/553. train loss: 11.0351,	0.8148 s / batch. (data: 3.27e-04). ETA=10:47:56, max mem: 20.9 GB 
[11/23 08:40:57 visual_prompt]: 	Training 500/553. train loss: 21.5263,	0.8360 s / batch. (data: 3.48e-04). ETA=11:03:21, max mem: 20.9 GB 
[11/23 08:41:49 visual_prompt]: Epoch 14 / 100: avg data time: 1.98e-01, avg batch time: 1.0245, average train loss: 9.0660
[11/23 08:42:47 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3073, average loss: 5.8439
[11/23 08:42:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.18	
[11/23 08:42:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/23 08:44:33 visual_prompt]: 	Training 100/553. train loss: 15.9422,	0.8440 s / batch. (data: 3.22e-04). ETA=11:07:35, max mem: 20.9 GB 
[11/23 08:46:14 visual_prompt]: 	Training 200/553. train loss: 24.2661,	0.8804 s / batch. (data: 5.90e-02). ETA=11:34:55, max mem: 20.9 GB 
[11/23 08:47:57 visual_prompt]: 	Training 300/553. train loss: 1.1437,	0.8238 s / batch. (data: 3.17e-04). ETA=10:48:52, max mem: 20.9 GB 
[11/23 08:49:38 visual_prompt]: 	Training 400/553. train loss: 1.0355,	1.2600 s / batch. (data: 4.22e-01). ETA=16:30:20, max mem: 20.9 GB 
[11/23 08:51:21 visual_prompt]: 	Training 500/553. train loss: 90.8781,	0.8065 s / batch. (data: 4.37e-04). ETA=10:32:33, max mem: 20.9 GB 
[11/23 08:52:15 visual_prompt]: Epoch 15 / 100: avg data time: 2.00e-01, avg batch time: 1.0260, average train loss: 17.6134
[11/23 08:53:14 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3066, average loss: 23.9815
[11/23 08:53:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.29	
[11/23 08:53:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/23 08:54:58 visual_prompt]: 	Training 100/553. train loss: 25.2271,	0.8037 s / batch. (data: 5.43e-03). ETA=10:28:17, max mem: 20.9 GB 
[11/23 08:56:39 visual_prompt]: 	Training 200/553. train loss: 25.7270,	0.8480 s / batch. (data: 3.43e-04). ETA=11:01:29, max mem: 20.9 GB 
[11/23 08:58:22 visual_prompt]: 	Training 300/553. train loss: 57.3220,	0.8408 s / batch. (data: 1.59e-02). ETA=10:54:27, max mem: 20.9 GB 
[11/23 09:00:04 visual_prompt]: 	Training 400/553. train loss: 5.9560,	0.8290 s / batch. (data: 5.46e-03). ETA=10:43:55, max mem: 20.9 GB 
[11/23 09:01:45 visual_prompt]: 	Training 500/553. train loss: 8.8824,	1.4092 s / batch. (data: 5.94e-01). ETA=18:12:14, max mem: 20.9 GB 
[11/23 09:02:39 visual_prompt]: Epoch 16 / 100: avg data time: 1.96e-01, avg batch time: 1.0226, average train loss: 14.8550
[11/23 09:03:37 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3068, average loss: 13.6446
[11/23 09:03:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.17	
[11/23 09:03:37 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/23 09:05:22 visual_prompt]: 	Training 100/553. train loss: 26.1969,	0.8286 s / batch. (data: 1.05e-02). ETA=10:40:05, max mem: 20.9 GB 
[11/23 09:07:05 visual_prompt]: 	Training 200/553. train loss: 56.1493,	0.8281 s / batch. (data: 5.51e-03). ETA=10:38:19, max mem: 20.9 GB 
[11/23 09:08:47 visual_prompt]: 	Training 300/553. train loss: 1.6608,	0.8233 s / batch. (data: 3.08e-04). ETA=10:33:17, max mem: 20.9 GB 
[11/23 09:10:28 visual_prompt]: 	Training 400/553. train loss: 58.0613,	1.3210 s / batch. (data: 4.96e-01). ETA=16:53:53, max mem: 20.9 GB 
[11/23 09:12:10 visual_prompt]: 	Training 500/553. train loss: 10.7328,	1.3154 s / batch. (data: 4.81e-01). ETA=16:47:23, max mem: 20.9 GB 
[11/23 09:13:04 visual_prompt]: Epoch 17 / 100: avg data time: 1.99e-01, avg batch time: 1.0236, average train loss: 18.0756
[11/23 09:14:02 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3081, average loss: 18.8841
[11/23 09:14:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.52	
[11/23 09:14:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/23 09:15:48 visual_prompt]: 	Training 100/553. train loss: 27.9208,	0.8440 s / batch. (data: 7.96e-03). ETA=10:44:14, max mem: 20.9 GB 
[11/23 09:17:32 visual_prompt]: 	Training 200/553. train loss: 27.2477,	0.8496 s / batch. (data: 6.14e-03). ETA=10:47:08, max mem: 20.9 GB 
[11/23 09:19:14 visual_prompt]: 	Training 300/553. train loss: 2.7293,	0.8313 s / batch. (data: 3.30e-04). ETA=10:31:46, max mem: 20.9 GB 
[11/23 09:20:55 visual_prompt]: 	Training 400/553. train loss: 2.6912,	0.8520 s / batch. (data: 1.20e-02). ETA=10:46:05, max mem: 20.9 GB 
[11/23 09:22:36 visual_prompt]: 	Training 500/553. train loss: 39.4254,	0.8247 s / batch. (data: 7.56e-03). ETA=10:24:02, max mem: 20.9 GB 
[11/23 09:23:28 visual_prompt]: Epoch 18 / 100: avg data time: 1.98e-01, avg batch time: 1.0225, average train loss: 21.2450
[11/23 09:24:26 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3084, average loss: 20.1407
[11/23 09:24:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.29	
[11/23 09:24:26 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/23 09:26:12 visual_prompt]: 	Training 100/553. train loss: 1.8607,	0.8160 s / batch. (data: 3.42e-04). ETA=10:15:19, max mem: 20.9 GB 
[11/23 09:27:54 visual_prompt]: 	Training 200/553. train loss: 2.2904,	0.8353 s / batch. (data: 5.51e-03). ETA=10:28:31, max mem: 20.9 GB 
[11/23 09:29:36 visual_prompt]: 	Training 300/553. train loss: 7.7977,	0.8200 s / batch. (data: 3.54e-04). ETA=10:15:38, max mem: 20.9 GB 
[11/23 09:31:20 visual_prompt]: 	Training 400/553. train loss: 4.5759,	0.8251 s / batch. (data: 3.13e-04). ETA=10:18:04, max mem: 20.9 GB 
[11/23 09:32:58 visual_prompt]: 	Training 500/553. train loss: 25.7462,	0.8460 s / batch. (data: 5.44e-03). ETA=10:32:19, max mem: 20.9 GB 
[11/23 09:33:51 visual_prompt]: Epoch 19 / 100: avg data time: 1.96e-01, avg batch time: 1.0205, average train loss: 13.8323
[11/23 09:34:49 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3085, average loss: 33.4310
[11/23 09:34:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.05	
[11/23 09:34:49 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/23 09:36:34 visual_prompt]: 	Training 100/553. train loss: 5.0464,	0.8483 s / batch. (data: 2.43e-02). ETA=10:31:51, max mem: 20.9 GB 
[11/23 09:38:16 visual_prompt]: 	Training 200/553. train loss: 8.0982,	0.8518 s / batch. (data: 1.57e-02). ETA=10:33:05, max mem: 20.9 GB 
[11/23 09:39:58 visual_prompt]: 	Training 300/553. train loss: 6.0087,	0.8200 s / batch. (data: 3.03e-04). ETA=10:08:05, max mem: 20.9 GB 
[11/23 09:41:40 visual_prompt]: 	Training 400/553. train loss: 1.3213,	0.8745 s / batch. (data: 5.48e-03). ETA=10:47:03, max mem: 20.9 GB 
[11/23 09:43:20 visual_prompt]: 	Training 500/553. train loss: 14.1554,	0.8400 s / batch. (data: 3.20e-04). ETA=10:20:06, max mem: 20.9 GB 
[11/23 09:44:15 visual_prompt]: Epoch 20 / 100: avg data time: 1.98e-01, avg batch time: 1.0230, average train loss: 15.2412
[11/23 09:45:13 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3070, average loss: 41.8419
[11/23 09:45:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.45	
[11/23 09:45:13 visual_prompt]: Stopping early.
[11/23 09:45:13 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 09:45:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 09:45:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/23 09:45:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 09:45:13 visual_prompt]: Training with config:
[11/23 09:45:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 09:45:13 visual_prompt]: Loading training data...
[11/23 09:45:13 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 09:45:13 visual_prompt]: Loading validation data...
[11/23 09:45:13 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 09:45:13 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 09:45:16 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 09:45:16 visual_prompt]: tuned percent:0.525
[11/23 09:45:17 visual_prompt]: Device used for model: 0
[11/23 09:45:17 visual_prompt]: Setting up Evaluator...
[11/23 09:45:17 visual_prompt]: Setting up Trainer...
[11/23 09:45:17 visual_prompt]: 	Setting up the optimizer...
[11/23 09:45:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 09:47:01 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8360 s / batch. (data: 3.14e-04). ETA=12:49:08, max mem: 20.9 GB 
[11/23 09:48:41 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8136 s / batch. (data: 3.15e-04). ETA=12:27:07, max mem: 20.9 GB 
[11/23 09:50:26 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.4160 s / batch. (data: 5.95e-01). ETA=21:37:58, max mem: 20.9 GB 
[11/23 09:52:06 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8200 s / batch. (data: 3.09e-04). ETA=12:30:17, max mem: 20.9 GB 
[11/23 09:53:50 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8501 s / batch. (data: 3.69e-02). ETA=12:56:25, max mem: 20.9 GB 
[11/23 09:54:44 visual_prompt]: Epoch 1 / 100: avg data time: 2.01e-01, avg batch time: 1.0258, average train loss: 1.5403
[11/23 09:55:42 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3082, average loss: 1.5201
[11/23 09:55:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 09:55:42 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/23 09:57:28 visual_prompt]: 	Training 100/553. train loss: 3.0043,	0.8264 s / batch. (data: 1.56e-02). ETA=12:32:40, max mem: 20.9 GB 
[11/23 09:59:09 visual_prompt]: 	Training 200/553. train loss: 0.0004,	1.1639 s / batch. (data: 3.48e-01). ETA=17:38:07, max mem: 20.9 GB 
[11/23 10:00:53 visual_prompt]: 	Training 300/553. train loss: 1.0276,	1.1520 s / batch. (data: 3.12e-01). ETA=17:25:23, max mem: 20.9 GB 
[11/23 10:02:34 visual_prompt]: 	Training 400/553. train loss: 2.9529,	0.8121 s / batch. (data: 3.06e-04). ETA=12:15:35, max mem: 20.9 GB 
[11/23 10:04:18 visual_prompt]: 	Training 500/553. train loss: 1.0274,	0.8240 s / batch. (data: 2.80e-04). ETA=12:24:59, max mem: 20.9 GB 
[11/23 10:05:10 visual_prompt]: Epoch 2 / 100: avg data time: 2.00e-01, avg batch time: 1.0257, average train loss: 1.8527
[11/23 10:06:08 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.3071, average loss: 3.8356
[11/23 10:06:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.38	
[11/23 10:06:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/23 10:07:52 visual_prompt]: 	Training 100/553. train loss: 4.4338,	0.8360 s / batch. (data: 3.36e-04). ETA=12:33:41, max mem: 20.9 GB 
[11/23 10:09:35 visual_prompt]: 	Training 200/553. train loss: 0.7001,	0.8440 s / batch. (data: 5.55e-03). ETA=12:39:32, max mem: 20.9 GB 
[11/23 10:11:16 visual_prompt]: 	Training 300/553. train loss: 2.6151,	0.8460 s / batch. (data: 8.39e-04). ETA=12:39:54, max mem: 20.9 GB 
[11/23 10:12:58 visual_prompt]: 	Training 400/553. train loss: 1.4173,	0.8346 s / batch. (data: 3.74e-03). ETA=12:28:16, max mem: 20.9 GB 
[11/23 10:14:41 visual_prompt]: 	Training 500/553. train loss: 1.0324,	1.3360 s / batch. (data: 5.12e-01). ETA=19:55:34, max mem: 20.9 GB 
[11/23 10:15:34 visual_prompt]: Epoch 3 / 100: avg data time: 1.97e-01, avg batch time: 1.0229, average train loss: 2.5510
[11/23 10:16:32 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3068, average loss: 1.8970
[11/23 10:16:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.23	
[11/23 10:16:32 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/23 10:18:19 visual_prompt]: 	Training 100/553. train loss: 2.2802,	0.8242 s / batch. (data: 1.05e-02). ETA=12:15:30, max mem: 20.9 GB 
[11/23 10:20:01 visual_prompt]: 	Training 200/553. train loss: 2.3283,	0.8350 s / batch. (data: 3.27e-04). ETA=12:23:45, max mem: 20.9 GB 
[11/23 10:21:44 visual_prompt]: 	Training 300/553. train loss: 1.8069,	1.4744 s / batch. (data: 6.65e-01). ETA=21:50:44, max mem: 20.9 GB 
[11/23 10:23:22 visual_prompt]: 	Training 400/553. train loss: 1.2089,	0.8120 s / batch. (data: 3.40e-04). ETA=12:00:29, max mem: 20.9 GB 
[11/23 10:25:05 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.7967 s / batch. (data: 2.97e+00). ETA=2 days, 8:02:38, max mem: 20.9 GB 
[11/23 10:26:00 visual_prompt]: Epoch 4 / 100: avg data time: 2.00e-01, avg batch time: 1.0258, average train loss: 2.9854
[11/23 10:26:58 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3087, average loss: 12.4888
[11/23 10:26:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.00	
[11/23 10:26:58 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/23 10:28:43 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8440 s / batch. (data: 4.05e-04). ETA=12:25:22, max mem: 20.9 GB 
[11/23 10:30:24 visual_prompt]: 	Training 200/553. train loss: 6.0476,	1.4289 s / batch. (data: 6.11e-01). ETA=20:59:33, max mem: 20.9 GB 
[11/23 10:32:08 visual_prompt]: 	Training 300/553. train loss: 16.7818,	0.8402 s / batch. (data: 1.22e-02). ETA=12:19:11, max mem: 20.9 GB 
[11/23 10:33:49 visual_prompt]: 	Training 400/553. train loss: 16.1333,	0.8442 s / batch. (data: 2.44e-02). ETA=12:21:20, max mem: 20.9 GB 
[11/23 10:35:31 visual_prompt]: 	Training 500/553. train loss: 4.3187,	0.8557 s / batch. (data: 3.07e-04). ETA=12:29:58, max mem: 20.9 GB 
[11/23 10:36:25 visual_prompt]: Epoch 5 / 100: avg data time: 1.97e-01, avg batch time: 1.0242, average train loss: 5.7854
[11/23 10:37:23 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.3076, average loss: 14.4886
[11/23 10:37:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.19	
[11/23 10:37:23 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/23 10:39:10 visual_prompt]: 	Training 100/553. train loss: 11.8735,	0.8440 s / batch. (data: 8.36e-04). ETA=12:17:34, max mem: 20.9 GB 
[11/23 10:40:52 visual_prompt]: 	Training 200/553. train loss: 8.3384,	0.8217 s / batch. (data: 3.19e-04). ETA=11:56:42, max mem: 20.9 GB 
[11/23 10:42:32 visual_prompt]: 	Training 300/553. train loss: 5.6462,	0.8230 s / batch. (data: 3.40e-04). ETA=11:56:27, max mem: 20.9 GB 
[11/23 10:44:18 visual_prompt]: 	Training 400/553. train loss: 4.8664,	0.8293 s / batch. (data: 3.81e-04). ETA=12:00:37, max mem: 20.9 GB 
[11/23 10:45:58 visual_prompt]: 	Training 500/553. train loss: 1.8894,	0.8399 s / batch. (data: 5.47e-03). ETA=12:08:24, max mem: 20.9 GB 
[11/23 10:46:51 visual_prompt]: Epoch 6 / 100: avg data time: 2.01e-01, avg batch time: 1.0273, average train loss: 6.9691
[11/23 10:47:50 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3079, average loss: 13.6566
[11/23 10:47:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.65	
[11/23 10:47:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/23 10:49:34 visual_prompt]: 	Training 100/553. train loss: 7.1549,	0.8199 s / batch. (data: 3.96e-04). ETA=11:48:58, max mem: 20.9 GB 
[11/23 10:51:16 visual_prompt]: 	Training 200/553. train loss: 2.6272,	0.8125 s / batch. (data: 2.99e-04). ETA=11:41:13, max mem: 20.9 GB 
[11/23 10:53:01 visual_prompt]: 	Training 300/553. train loss: 2.2763,	1.5360 s / batch. (data: 6.98e-01). ETA=22:03:03, max mem: 20.9 GB 
[11/23 10:54:43 visual_prompt]: 	Training 400/553. train loss: 0.9221,	2.1320 s / batch. (data: 1.30e+00). ETA=1 day, 6:32:53, max mem: 20.9 GB 
[11/23 10:56:23 visual_prompt]: 	Training 500/553. train loss: 0.9348,	0.8400 s / batch. (data: 7.98e-03). ETA=12:00:45, max mem: 20.9 GB 
[11/23 10:57:16 visual_prompt]: Epoch 7 / 100: avg data time: 1.97e-01, avg batch time: 1.0239, average train loss: 6.1081
[11/23 10:58:15 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3067, average loss: 3.2896
[11/23 10:58:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[11/23 10:58:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/23 10:59:58 visual_prompt]: 	Training 100/553. train loss: 16.5645,	0.8240 s / batch. (data: 3.80e-04). ETA=11:44:53, max mem: 20.9 GB 
[11/23 11:01:42 visual_prompt]: 	Training 200/553. train loss: 1.7785,	0.8560 s / batch. (data: 3.40e-04). ETA=12:10:50, max mem: 20.9 GB 
[11/23 11:03:25 visual_prompt]: 	Training 300/553. train loss: 17.5561,	0.8597 s / batch. (data: 2.37e-02). ETA=12:12:33, max mem: 20.9 GB 
[11/23 11:05:07 visual_prompt]: 	Training 400/553. train loss: 4.2794,	1.0628 s / batch. (data: 2.26e-01). ETA=15:03:55, max mem: 20.9 GB 
[11/23 11:06:49 visual_prompt]: 	Training 500/553. train loss: 36.0258,	1.5838 s / batch. (data: 7.62e-01). ETA=22:24:21, max mem: 20.9 GB 
[11/23 11:07:42 visual_prompt]: Epoch 8 / 100: avg data time: 1.99e-01, avg batch time: 1.0264, average train loss: 7.0746
[11/23 11:08:41 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3086, average loss: 1.8158
[11/23 11:08:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.71	
[11/23 11:08:41 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/23 11:10:27 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8326 s / batch. (data: 3.29e-04). ETA=11:44:37, max mem: 20.9 GB 
[11/23 11:12:08 visual_prompt]: 	Training 200/553. train loss: 1.3922,	0.8176 s / batch. (data: 4.55e-04). ETA=11:30:30, max mem: 20.9 GB 
[11/23 11:13:50 visual_prompt]: 	Training 300/553. train loss: 9.8045,	1.7640 s / batch. (data: 9.28e-01). ETA=1 day, 0:46:55, max mem: 20.9 GB 
[11/23 11:15:34 visual_prompt]: 	Training 400/553. train loss: 16.5693,	0.8360 s / batch. (data: 1.20e-02). ETA=11:43:17, max mem: 20.9 GB 
[11/23 11:17:17 visual_prompt]: 	Training 500/553. train loss: 1.3801,	0.9560 s / batch. (data: 1.29e-01). ETA=13:22:39, max mem: 20.9 GB 
[11/23 11:18:09 visual_prompt]: Epoch 9 / 100: avg data time: 2.01e-01, avg batch time: 1.0261, average train loss: 5.9887
[11/23 11:19:07 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3087, average loss: 4.5217
[11/23 11:19:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.49	
[11/23 11:19:07 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/23 11:20:55 visual_prompt]: 	Training 100/553. train loss: 20.1672,	0.8410 s / batch. (data: 1.11e-02). ETA=11:43:59, max mem: 20.9 GB 
[11/23 11:22:35 visual_prompt]: 	Training 200/553. train loss: 7.2029,	0.8286 s / batch. (data: 7.69e-03). ETA=11:32:10, max mem: 20.9 GB 
[11/23 11:24:17 visual_prompt]: 	Training 300/553. train loss: 16.4779,	0.8314 s / batch. (data: 3.12e-04). ETA=11:33:07, max mem: 20.9 GB 
[11/23 11:25:57 visual_prompt]: 	Training 400/553. train loss: 4.8384,	0.8552 s / batch. (data: 3.05e-02). ETA=11:51:35, max mem: 20.9 GB 
[11/23 11:27:40 visual_prompt]: 	Training 500/553. train loss: 27.9044,	1.1152 s / batch. (data: 2.99e-01). ETA=15:26:04, max mem: 20.9 GB 
[11/23 11:28:33 visual_prompt]: Epoch 10 / 100: avg data time: 1.97e-01, avg batch time: 1.0234, average train loss: 9.6479
[11/23 11:29:31 visual_prompt]: Inference (val):avg data time: 2.23e-04, avg batch time: 0.3067, average loss: 12.2237
[11/23 11:29:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.87	
[11/23 11:29:31 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/23 11:31:19 visual_prompt]: 	Training 100/553. train loss: 0.4391,	0.8440 s / batch. (data: 2.96e-04). ETA=11:38:40, max mem: 20.9 GB 
[11/23 11:33:03 visual_prompt]: 	Training 200/553. train loss: 0.8890,	0.8337 s / batch. (data: 3.05e-04). ETA=11:28:48, max mem: 20.9 GB 
[11/23 11:34:44 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.3005 s / batch. (data: 1.48e+00). ETA=1 day, 7:36:45, max mem: 20.9 GB 
[11/23 11:36:24 visual_prompt]: 	Training 400/553. train loss: 2.5199,	0.8528 s / batch. (data: 7.99e-04). ETA=11:41:40, max mem: 20.9 GB 
[11/23 11:38:04 visual_prompt]: 	Training 500/553. train loss: 3.1253,	0.8320 s / batch. (data: 3.09e-04). ETA=11:23:11, max mem: 20.9 GB 
[11/23 11:38:57 visual_prompt]: Epoch 11 / 100: avg data time: 1.96e-01, avg batch time: 1.0224, average train loss: 5.9548
[11/23 11:39:55 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3081, average loss: 8.7608
[11/23 11:39:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.98	
[11/23 11:39:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/23 11:41:43 visual_prompt]: 	Training 100/553. train loss: 4.0551,	0.8370 s / batch. (data: 3.22e-04). ETA=11:25:08, max mem: 20.9 GB 
[11/23 11:43:25 visual_prompt]: 	Training 200/553. train loss: 3.1000,	1.0600 s / batch. (data: 2.29e-01). ETA=14:25:59, max mem: 20.9 GB 
[11/23 11:45:06 visual_prompt]: 	Training 300/553. train loss: 2.7414,	0.8399 s / batch. (data: 5.46e-03). ETA=11:24:43, max mem: 20.9 GB 
[11/23 11:46:48 visual_prompt]: 	Training 400/553. train loss: 3.0529,	0.8360 s / batch. (data: 1.20e-02). ETA=11:20:11, max mem: 20.9 GB 
[11/23 11:48:31 visual_prompt]: 	Training 500/553. train loss: 39.8382,	0.8283 s / batch. (data: 5.45e-03). ETA=11:12:30, max mem: 20.9 GB 
[11/23 11:49:23 visual_prompt]: Epoch 12 / 100: avg data time: 1.99e-01, avg batch time: 1.0259, average train loss: 7.5559
[11/23 11:50:22 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3093, average loss: 7.9266
[11/23 11:50:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.23	
[11/23 11:50:22 visual_prompt]: Best epoch 12: best metric: -7.927
[11/23 11:50:22 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/23 11:52:09 visual_prompt]: 	Training 100/553. train loss: 1.6612,	0.8300 s / batch. (data: 3.33e-04). ETA=11:11:47, max mem: 20.9 GB 
[11/23 11:53:47 visual_prompt]: 	Training 200/553. train loss: 1.3221,	0.8400 s / batch. (data: 3.15e-04). ETA=11:18:29, max mem: 20.9 GB 
[11/23 11:55:30 visual_prompt]: 	Training 300/553. train loss: 1.4190,	1.7153 s / batch. (data: 8.62e-01). ETA=23:02:40, max mem: 20.9 GB 
[11/23 11:57:11 visual_prompt]: 	Training 400/553. train loss: 28.4845,	0.8466 s / batch. (data: 2.99e-04). ETA=11:21:00, max mem: 20.9 GB 
[11/23 11:58:54 visual_prompt]: 	Training 500/553. train loss: 0.2549,	0.8361 s / batch. (data: 2.18e-04). ETA=11:11:08, max mem: 20.9 GB 
[11/23 11:59:48 visual_prompt]: Epoch 13 / 100: avg data time: 1.97e-01, avg batch time: 1.0236, average train loss: 6.1947
[11/23 12:00:46 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3086, average loss: 1.4006
[11/23 12:00:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.02	
[11/23 12:00:46 visual_prompt]: Best epoch 13: best metric: -1.401
[11/23 12:00:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/23 12:02:33 visual_prompt]: 	Training 100/553. train loss: 8.7893,	0.8158 s / batch. (data: 3.10e-04). ETA=10:52:48, max mem: 20.9 GB 
[11/23 12:04:14 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1918 s / batch. (data: 3.64e-01). ETA=15:51:41, max mem: 20.9 GB 
[11/23 12:05:57 visual_prompt]: 	Training 300/553. train loss: 0.4008,	0.8280 s / batch. (data: 3.36e-04). ETA=10:59:47, max mem: 20.9 GB 
[11/23 12:07:38 visual_prompt]: 	Training 400/553. train loss: 2.3632,	0.8207 s / batch. (data: 7.19e-03). ETA=10:52:34, max mem: 20.9 GB 
[11/23 12:09:21 visual_prompt]: 	Training 500/553. train loss: 16.9422,	0.8334 s / batch. (data: 5.44e-03). ETA=11:01:21, max mem: 20.9 GB 
[11/23 12:10:13 visual_prompt]: Epoch 14 / 100: avg data time: 1.98e-01, avg batch time: 1.0239, average train loss: 4.8561
[11/23 12:11:11 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3081, average loss: 5.0464
[11/23 12:11:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.00	
[11/23 12:11:11 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/23 12:12:57 visual_prompt]: 	Training 100/553. train loss: 9.6415,	0.8357 s / batch. (data: 8.94e-03). ETA=11:01:02, max mem: 20.9 GB 
[11/23 12:14:37 visual_prompt]: 	Training 200/553. train loss: 0.2219,	0.8312 s / batch. (data: 3.15e-04). ETA=10:56:03, max mem: 20.9 GB 
[11/23 12:16:21 visual_prompt]: 	Training 300/553. train loss: 9.0622,	0.8388 s / batch. (data: 8.36e-04). ETA=11:00:38, max mem: 20.9 GB 
[11/23 12:18:00 visual_prompt]: 	Training 400/553. train loss: 7.6745,	0.8280 s / batch. (data: 3.45e-04). ETA=10:50:47, max mem: 20.9 GB 
[11/23 12:19:43 visual_prompt]: 	Training 500/553. train loss: 3.0960,	0.8397 s / batch. (data: 1.05e-02). ETA=10:58:34, max mem: 20.9 GB 
[11/23 12:20:37 visual_prompt]: Epoch 15 / 100: avg data time: 1.97e-01, avg batch time: 1.0231, average train loss: 8.3364
[11/23 12:21:35 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.3072, average loss: 7.1108
[11/23 12:21:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.17	
[11/23 12:21:35 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/23 12:23:20 visual_prompt]: 	Training 100/553. train loss: 1.6939,	0.8487 s / batch. (data: 8.67e-03). ETA=11:03:27, max mem: 20.9 GB 
[11/23 12:25:02 visual_prompt]: 	Training 200/553. train loss: 35.5122,	0.8271 s / batch. (data: 3.11e-04). ETA=10:45:13, max mem: 20.9 GB 
[11/23 12:26:44 visual_prompt]: 	Training 300/553. train loss: 23.5092,	0.8232 s / batch. (data: 3.20e-04). ETA=10:40:45, max mem: 20.9 GB 
[11/23 12:28:25 visual_prompt]: 	Training 400/553. train loss: 0.4262,	0.8539 s / batch. (data: 2.19e-02). ETA=11:03:17, max mem: 20.9 GB 
[11/23 12:30:06 visual_prompt]: 	Training 500/553. train loss: 1.3423,	0.8280 s / batch. (data: 3.59e-04). ETA=10:41:45, max mem: 20.9 GB 
[11/23 12:31:01 visual_prompt]: Epoch 16 / 100: avg data time: 1.97e-01, avg batch time: 1.0229, average train loss: 6.4424
[11/23 12:31:59 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3086, average loss: 1.2462
[11/23 12:31:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 63.31	
[11/23 12:31:59 visual_prompt]: Best epoch 16: best metric: -1.246
[11/23 12:31:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/23 12:33:45 visual_prompt]: 	Training 100/553. train loss: 3.2804,	0.8207 s / batch. (data: 2.83e-04). ETA=10:34:01, max mem: 20.9 GB 
[11/23 12:35:28 visual_prompt]: 	Training 200/553. train loss: 15.9472,	0.8171 s / batch. (data: 7.91e-03). ETA=10:29:51, max mem: 20.9 GB 
[11/23 12:37:10 visual_prompt]: 	Training 300/553. train loss: 2.5579,	0.8600 s / batch. (data: 2.92e-04). ETA=11:01:30, max mem: 20.9 GB 
[11/23 12:38:51 visual_prompt]: 	Training 400/553. train loss: 2.8205,	1.1855 s / batch. (data: 3.73e-01). ETA=15:09:54, max mem: 20.9 GB 
[11/23 12:40:32 visual_prompt]: 	Training 500/553. train loss: 9.4810,	1.4640 s / batch. (data: 6.38e-01). ETA=18:41:13, max mem: 20.9 GB 
[11/23 12:41:27 visual_prompt]: Epoch 17 / 100: avg data time: 1.99e-01, avg batch time: 1.0258, average train loss: 5.8217
[11/23 12:42:25 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.3099, average loss: 2.4304
[11/23 12:42:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 65.42	
[11/23 12:42:25 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/23 12:44:11 visual_prompt]: 	Training 100/553. train loss: 5.2362,	0.8320 s / batch. (data: 7.89e-03). ETA=10:35:02, max mem: 20.9 GB 
[11/23 12:45:55 visual_prompt]: 	Training 200/553. train loss: 10.0353,	0.8160 s / batch. (data: 3.43e-04). ETA=10:21:29, max mem: 20.9 GB 
[11/23 12:47:37 visual_prompt]: 	Training 300/553. train loss: 3.7636,	0.8202 s / batch. (data: 3.10e-04). ETA=10:23:22, max mem: 20.9 GB 
[11/23 12:49:20 visual_prompt]: 	Training 400/553. train loss: 2.8356,	0.8277 s / batch. (data: 1.06e-02). ETA=10:27:40, max mem: 20.9 GB 
[11/23 12:51:00 visual_prompt]: 	Training 500/553. train loss: 3.3602,	0.8213 s / batch. (data: 2.98e-04). ETA=10:21:28, max mem: 20.9 GB 
[11/23 12:51:52 visual_prompt]: Epoch 18 / 100: avg data time: 1.98e-01, avg batch time: 1.0245, average train loss: 4.8105
[11/23 12:52:50 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3085, average loss: 3.5719
[11/23 12:52:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.67	
[11/23 12:52:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/23 12:54:36 visual_prompt]: 	Training 100/553. train loss: 10.9852,	1.0006 s / batch. (data: 1.78e-01). ETA=12:34:33, max mem: 20.9 GB 
[11/23 12:56:18 visual_prompt]: 	Training 200/553. train loss: 2.1276,	0.8160 s / batch. (data: 3.09e-04). ETA=10:13:57, max mem: 20.9 GB 
[11/23 12:58:00 visual_prompt]: 	Training 300/553. train loss: 16.7522,	0.8314 s / batch. (data: 5.69e-03). ETA=10:24:11, max mem: 20.9 GB 
[11/23 12:59:43 visual_prompt]: 	Training 400/553. train loss: 0.8231,	0.8246 s / batch. (data: 4.79e-04). ETA=10:17:42, max mem: 20.9 GB 
[11/23 13:01:21 visual_prompt]: 	Training 500/553. train loss: 5.8514,	0.8166 s / batch. (data: 3.03e-04). ETA=10:10:19, max mem: 20.9 GB 
[11/23 13:02:14 visual_prompt]: Epoch 19 / 100: avg data time: 1.92e-01, avg batch time: 1.0189, average train loss: 5.4547
[11/23 13:03:13 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3088, average loss: 15.1166
[11/23 13:03:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.62	
[11/23 13:03:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/23 13:04:56 visual_prompt]: 	Training 100/553. train loss: 7.5258,	0.8304 s / batch. (data: 1.04e-02). ETA=10:18:33, max mem: 20.9 GB 
[11/23 13:06:39 visual_prompt]: 	Training 200/553. train loss: 0.0937,	0.8144 s / batch. (data: 3.43e-04). ETA=10:05:18, max mem: 20.9 GB 
[11/23 13:08:22 visual_prompt]: 	Training 300/553. train loss: 11.4970,	0.8286 s / batch. (data: 1.21e-02). ETA=10:14:29, max mem: 20.9 GB 
[11/23 13:10:04 visual_prompt]: 	Training 400/553. train loss: 7.0201,	0.8120 s / batch. (data: 3.62e-04). ETA=10:00:47, max mem: 20.9 GB 
[11/23 13:11:45 visual_prompt]: 	Training 500/553. train loss: 37.8318,	0.8211 s / batch. (data: 3.04e-04). ETA=10:06:10, max mem: 20.9 GB 
[11/23 13:12:40 visual_prompt]: Epoch 20 / 100: avg data time: 2.00e-01, avg batch time: 1.0258, average train loss: 7.4035
[11/23 13:13:39 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3088, average loss: 4.7180
[11/23 13:13:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.29	
[11/23 13:13:39 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/23 13:15:28 visual_prompt]: 	Training 100/553. train loss: 1.1744,	0.8201 s / batch. (data: 3.14e-04). ETA=10:03:20, max mem: 20.9 GB 
[11/23 13:17:08 visual_prompt]: 	Training 200/553. train loss: 15.9846,	0.8145 s / batch. (data: 2.97e-04). ETA=9:57:51, max mem: 20.9 GB 
[11/23 13:18:48 visual_prompt]: 	Training 300/553. train loss: 3.3645,	0.8239 s / batch. (data: 1.63e-03). ETA=10:03:22, max mem: 20.9 GB 
[11/23 13:20:29 visual_prompt]: 	Training 400/553. train loss: 11.6615,	0.8370 s / batch. (data: 8.98e-03). ETA=10:11:34, max mem: 20.9 GB 
[11/23 13:22:11 visual_prompt]: 	Training 500/553. train loss: 7.7146,	0.8449 s / batch. (data: 3.23e-02). ETA=10:15:54, max mem: 20.9 GB 
[11/23 13:23:03 visual_prompt]: Epoch 21 / 100: avg data time: 1.93e-01, avg batch time: 1.0191, average train loss: 7.2807
[11/23 13:24:02 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3076, average loss: 4.3996
[11/23 13:24:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.04	
[11/23 13:24:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/23 13:25:47 visual_prompt]: 	Training 100/553. train loss: 11.0785,	0.8240 s / batch. (data: 4.34e-04). ETA=9:58:34, max mem: 20.9 GB 
[11/23 13:27:30 visual_prompt]: 	Training 200/553. train loss: 3.7789,	0.8406 s / batch. (data: 3.07e-04). ETA=10:09:14, max mem: 20.9 GB 
[11/23 13:29:10 visual_prompt]: 	Training 300/553. train loss: 0.0005,	0.8774 s / batch. (data: 5.45e-03). ETA=10:34:26, max mem: 20.9 GB 
[11/23 13:30:52 visual_prompt]: 	Training 400/553. train loss: 1.1607,	0.8393 s / batch. (data: 3.02e-04). ETA=10:05:32, max mem: 20.9 GB 
[11/23 13:32:34 visual_prompt]: 	Training 500/553. train loss: 1.8572,	0.8395 s / batch. (data: 5.41e-03). ETA=10:04:16, max mem: 20.9 GB 
[11/23 13:33:28 visual_prompt]: Epoch 22 / 100: avg data time: 1.97e-01, avg batch time: 1.0236, average train loss: 5.3035
[11/23 13:34:27 visual_prompt]: Inference (val):avg data time: 2.30e-04, avg batch time: 0.3078, average loss: 5.3951
[11/23 13:34:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.83	
[11/23 13:34:27 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/23 13:36:16 visual_prompt]: 	Training 100/553. train loss: 2.2837,	0.8243 s / batch. (data: 3.33e-04). ETA=9:51:13, max mem: 20.9 GB 
[11/23 13:37:57 visual_prompt]: 	Training 200/553. train loss: 3.1618,	0.8244 s / batch. (data: 3.52e-04). ETA=9:49:54, max mem: 20.9 GB 
[11/23 13:39:41 visual_prompt]: 	Training 300/553. train loss: 0.9660,	0.8360 s / batch. (data: 8.06e-04). ETA=9:56:49, max mem: 20.9 GB 
[11/23 13:41:20 visual_prompt]: 	Training 400/553. train loss: 0.7535,	0.8400 s / batch. (data: 7.92e-04). ETA=9:58:15, max mem: 20.9 GB 
[11/23 13:43:00 visual_prompt]: 	Training 500/553. train loss: 17.1139,	0.8417 s / batch. (data: 2.07e-02). ETA=9:58:03, max mem: 20.9 GB 
[11/23 13:43:53 visual_prompt]: Epoch 23 / 100: avg data time: 1.96e-01, avg batch time: 1.0223, average train loss: 6.0506
[11/23 13:44:50 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3072, average loss: 1.1173
[11/23 13:44:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 69.92	
[11/23 13:44:50 visual_prompt]: Best epoch 23: best metric: -1.117
[11/23 13:44:50 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/23 13:46:33 visual_prompt]: 	Training 100/553. train loss: 3.6433,	0.8240 s / batch. (data: 3.22e-04). ETA=9:43:24, max mem: 20.9 GB 
[11/23 13:48:12 visual_prompt]: 	Training 200/553. train loss: 2.6525,	0.8116 s / batch. (data: 3.12e-04). ETA=9:33:15, max mem: 20.9 GB 
[11/23 13:49:55 visual_prompt]: 	Training 300/553. train loss: 4.9809,	0.9873 s / batch. (data: 1.29e-01). ETA=11:35:44, max mem: 20.9 GB 
[11/23 13:51:38 visual_prompt]: 	Training 400/553. train loss: 0.7819,	0.8475 s / batch. (data: 1.56e-02). ETA=9:55:48, max mem: 20.9 GB 
[11/23 13:53:22 visual_prompt]: 	Training 500/553. train loss: 0.2565,	0.8240 s / batch. (data: 7.96e-03). ETA=9:37:55, max mem: 20.9 GB 
[11/23 13:54:15 visual_prompt]: Epoch 24 / 100: avg data time: 1.94e-01, avg batch time: 1.0208, average train loss: 4.8728
[11/23 13:55:13 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3087, average loss: 8.6880
[11/23 13:55:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.08	
[11/23 13:55:13 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/23 13:57:01 visual_prompt]: 	Training 100/553. train loss: 11.0764,	0.8320 s / batch. (data: 2.92e-04). ETA=9:41:25, max mem: 20.9 GB 
[11/23 13:58:39 visual_prompt]: 	Training 200/553. train loss: 3.3035,	0.8293 s / batch. (data: 4.93e-03). ETA=9:38:06, max mem: 20.9 GB 
[11/23 14:00:19 visual_prompt]: 	Training 300/553. train loss: 4.5802,	0.8466 s / batch. (data: 5.48e-03). ETA=9:48:46, max mem: 20.9 GB 
[11/23 14:02:00 visual_prompt]: 	Training 400/553. train loss: 6.3984,	1.4240 s / batch. (data: 5.88e-01). ETA=16:27:58, max mem: 20.9 GB 
[11/23 14:03:41 visual_prompt]: 	Training 500/553. train loss: 2.0448,	1.5756 s / batch. (data: 7.56e-01). ETA=18:10:31, max mem: 20.9 GB 
[11/23 14:04:33 visual_prompt]: Epoch 25 / 100: avg data time: 1.87e-01, avg batch time: 1.0130, average train loss: 6.7018
[11/23 14:05:31 visual_prompt]: Inference (val):avg data time: 2.06e-04, avg batch time: 0.3086, average loss: 15.9009
[11/23 14:05:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.48	
[11/23 14:05:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/23 14:07:15 visual_prompt]: 	Training 100/553. train loss: 10.9048,	0.8359 s / batch. (data: 3.05e-04). ETA=9:36:26, max mem: 20.9 GB 
[11/23 14:08:57 visual_prompt]: 	Training 200/553. train loss: 16.0287,	1.6430 s / batch. (data: 8.17e-01). ETA=18:50:16, max mem: 20.9 GB 
[11/23 14:10:39 visual_prompt]: 	Training 300/553. train loss: 5.3045,	0.8360 s / batch. (data: 3.18e-04). ETA=9:33:41, max mem: 20.9 GB 
[11/23 14:12:18 visual_prompt]: 	Training 400/553. train loss: 1.7387,	0.8160 s / batch. (data: 2.92e-04). ETA=9:18:37, max mem: 20.9 GB 
[11/23 14:13:57 visual_prompt]: 	Training 500/553. train loss: 5.1397,	0.8320 s / batch. (data: 2.97e-04). ETA=9:28:10, max mem: 20.9 GB 
[11/23 14:14:49 visual_prompt]: Epoch 26 / 100: avg data time: 1.83e-01, avg batch time: 1.0092, average train loss: 5.4226
[11/23 14:15:47 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.3075, average loss: 1.9294
[11/23 14:15:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 71.24	
[11/23 14:15:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/23 14:17:32 visual_prompt]: 	Training 100/553. train loss: 7.7995,	0.8297 s / batch. (data: 3.01e-04). ETA=9:24:29, max mem: 20.9 GB 
[11/23 14:19:12 visual_prompt]: 	Training 200/553. train loss: 17.9861,	1.7225 s / batch. (data: 8.95e-01). ETA=19:29:01, max mem: 20.9 GB 
[11/23 14:20:53 visual_prompt]: 	Training 300/553. train loss: 6.7700,	0.8320 s / batch. (data: 3.20e-04). ETA=9:23:17, max mem: 20.9 GB 
[11/23 14:22:35 visual_prompt]: 	Training 400/553. train loss: 9.3514,	0.8400 s / batch. (data: 8.38e-04). ETA=9:27:18, max mem: 20.9 GB 
[11/23 14:24:15 visual_prompt]: 	Training 500/553. train loss: 5.1421,	0.8302 s / batch. (data: 3.00e-04). ETA=9:19:17, max mem: 20.9 GB 
[11/23 14:25:06 visual_prompt]: Epoch 27 / 100: avg data time: 1.84e-01, avg batch time: 1.0106, average train loss: 5.3024
[11/23 14:26:03 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3087, average loss: 2.4663
[11/23 14:26:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 72.00	
[11/23 14:26:03 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/23 14:27:47 visual_prompt]: 	Training 100/553. train loss: 3.7675,	1.1050 s / batch. (data: 2.59e-01). ETA=12:21:38, max mem: 20.9 GB 
[11/23 14:29:29 visual_prompt]: 	Training 200/553. train loss: 23.0866,	0.8106 s / batch. (data: 2.94e-04). ETA=9:02:39, max mem: 20.9 GB 
[11/23 14:31:10 visual_prompt]: 	Training 300/553. train loss: 22.1772,	1.6872 s / batch. (data: 8.78e-01). ETA=18:46:44, max mem: 20.9 GB 
[11/23 14:32:50 visual_prompt]: 	Training 400/553. train loss: 19.7659,	0.8318 s / batch. (data: 3.05e-04). ETA=9:14:06, max mem: 20.9 GB 
[11/23 14:34:29 visual_prompt]: 	Training 500/553. train loss: 0.9137,	0.8118 s / batch. (data: 3.22e-04). ETA=8:59:26, max mem: 20.9 GB 
[11/23 14:35:23 visual_prompt]: Epoch 28 / 100: avg data time: 1.87e-01, avg batch time: 1.0123, average train loss: 5.5347
[11/23 14:36:21 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.3060, average loss: 1.6315
[11/23 14:36:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.93	
[11/23 14:36:21 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/23 14:38:13 visual_prompt]: 	Training 100/553. train loss: 0.6146,	0.8240 s / batch. (data: 3.00e-04). ETA=9:05:25, max mem: 20.9 GB 
[11/23 14:39:53 visual_prompt]: 	Training 200/553. train loss: 22.7147,	1.9323 s / batch. (data: 1.11e+00). ETA=21:15:50, max mem: 20.9 GB 
[11/23 14:41:32 visual_prompt]: 	Training 300/553. train loss: 14.2245,	0.8365 s / batch. (data: 1.05e-02). ETA=9:10:54, max mem: 20.9 GB 
[11/23 14:43:09 visual_prompt]: 	Training 400/553. train loss: 11.3983,	1.5200 s / batch. (data: 6.89e-01). ETA=16:38:32, max mem: 20.9 GB 
[11/23 14:44:51 visual_prompt]: 	Training 500/553. train loss: 14.5313,	0.8230 s / batch. (data: 1.06e-02). ETA=8:59:16, max mem: 20.9 GB 
[11/23 14:45:43 visual_prompt]: Epoch 29 / 100: avg data time: 1.90e-01, avg batch time: 1.0163, average train loss: 6.5519
[11/23 14:46:43 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3088, average loss: 5.4480
[11/23 14:46:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.10	
[11/23 14:46:43 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/23 14:48:28 visual_prompt]: 	Training 100/553. train loss: 4.4012,	0.8280 s / batch. (data: 3.21e-04). ETA=9:00:27, max mem: 20.9 GB 
[11/23 14:50:10 visual_prompt]: 	Training 200/553. train loss: 2.6239,	0.8440 s / batch. (data: 5.48e-03). ETA=9:09:31, max mem: 20.9 GB 
[11/23 14:51:49 visual_prompt]: 	Training 300/553. train loss: 0.0003,	1.2157 s / batch. (data: 3.65e-01). ETA=13:09:26, max mem: 20.9 GB 
[11/23 14:53:30 visual_prompt]: 	Training 400/553. train loss: 9.0745,	1.2099 s / batch. (data: 3.99e-01). ETA=13:03:40, max mem: 20.9 GB 
[11/23 14:55:11 visual_prompt]: 	Training 500/553. train loss: 1.3446,	1.6661 s / batch. (data: 8.32e-01). ETA=17:56:24, max mem: 20.9 GB 
[11/23 14:56:06 visual_prompt]: Epoch 30 / 100: avg data time: 1.93e-01, avg batch time: 1.0180, average train loss: 4.8709
[11/23 14:57:04 visual_prompt]: Inference (val):avg data time: 6.97e-05, avg batch time: 0.3075, average loss: 1.7702
[11/23 14:57:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 71.46	
[11/23 14:57:04 visual_prompt]: Stopping early.
[11/23 14:57:04 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 14:57:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 14:57:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/23 14:57:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 14:57:04 visual_prompt]: Training with config:
[11/23 14:57:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 14:57:04 visual_prompt]: Loading training data...
[11/23 14:57:04 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 14:57:04 visual_prompt]: Loading validation data...
[11/23 14:57:04 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 14:57:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 14:57:07 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 14:57:07 visual_prompt]: tuned percent:0.525
[11/23 14:57:07 visual_prompt]: Device used for model: 0
[11/23 14:57:07 visual_prompt]: Setting up Evaluator...
[11/23 14:57:07 visual_prompt]: Setting up Trainer...
[11/23 14:57:07 visual_prompt]: 	Setting up the optimizer...
[11/23 14:57:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 14:58:52 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8120 s / batch. (data: 3.07e-04). ETA=12:27:00, max mem: 20.9 GB 
[11/23 15:00:31 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8379 s / batch. (data: 3.10e-04). ETA=12:49:29, max mem: 20.9 GB 
[11/23 15:02:15 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.8100 s / batch. (data: 9.92e-01). ETA=1 day, 3:39:08, max mem: 20.9 GB 
[11/23 15:03:54 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8240 s / batch. (data: 3.26e-04). ETA=12:33:57, max mem: 20.9 GB 
[11/23 15:05:37 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8400 s / batch. (data: 7.97e-03). ETA=12:47:12, max mem: 20.9 GB 
[11/23 15:06:30 visual_prompt]: Epoch 1 / 100: avg data time: 1.91e-01, avg batch time: 1.0173, average train loss: 1.5403
[11/23 15:07:28 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3076, average loss: 1.5201
[11/23 15:07:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 15:07:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/23 15:09:11 visual_prompt]: 	Training 100/553. train loss: 1.3178,	0.9108 s / batch. (data: 8.91e-02). ETA=13:49:30, max mem: 20.9 GB 
[11/23 15:10:51 visual_prompt]: 	Training 200/553. train loss: 0.2776,	0.8240 s / batch. (data: 7.99e-03). ETA=12:29:06, max mem: 20.9 GB 
[11/23 15:12:34 visual_prompt]: 	Training 300/553. train loss: 1.1687,	1.1480 s / batch. (data: 3.14e-01). ETA=17:21:47, max mem: 20.9 GB 
[11/23 15:14:14 visual_prompt]: 	Training 400/553. train loss: 1.4125,	0.8280 s / batch. (data: 5.43e-03). ETA=12:29:59, max mem: 20.9 GB 
[11/23 15:15:56 visual_prompt]: 	Training 500/553. train loss: 0.5618,	0.8320 s / batch. (data: 5.44e-03). ETA=12:32:13, max mem: 20.9 GB 
[11/23 15:16:47 visual_prompt]: Epoch 2 / 100: avg data time: 1.86e-01, avg batch time: 1.0120, average train loss: 1.0792
[11/23 15:17:45 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3086, average loss: 2.9495
[11/23 15:17:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.70	
[11/23 15:17:45 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/23 15:19:28 visual_prompt]: 	Training 100/553. train loss: 1.1325,	0.8323 s / batch. (data: 3.04e-04). ETA=12:30:22, max mem: 20.9 GB 
[11/23 15:21:10 visual_prompt]: 	Training 200/553. train loss: 0.7032,	1.1697 s / batch. (data: 3.39e-01). ETA=17:32:36, max mem: 20.9 GB 
[11/23 15:22:50 visual_prompt]: 	Training 300/553. train loss: 0.8751,	0.8243 s / batch. (data: 2.97e-04). ETA=12:20:25, max mem: 20.9 GB 
[11/23 15:24:31 visual_prompt]: 	Training 400/553. train loss: 0.4530,	0.8253 s / batch. (data: 1.20e-02). ETA=12:19:54, max mem: 20.9 GB 
[11/23 15:26:13 visual_prompt]: 	Training 500/553. train loss: 0.7412,	1.1064 s / batch. (data: 2.75e-01). ETA=16:30:05, max mem: 20.9 GB 
[11/23 15:27:05 visual_prompt]: Epoch 3 / 100: avg data time: 1.87e-01, avg batch time: 1.0121, average train loss: 1.4825
[11/23 15:28:03 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3074, average loss: 1.3046
[11/23 15:28:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.65	
[11/23 15:28:03 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/23 15:29:49 visual_prompt]: 	Training 100/553. train loss: 2.1249,	0.8205 s / batch. (data: 5.42e-03). ETA=12:12:10, max mem: 20.9 GB 
[11/23 15:31:30 visual_prompt]: 	Training 200/553. train loss: 3.7853,	0.8320 s / batch. (data: 3.16e-04). ETA=12:21:02, max mem: 20.9 GB 
[11/23 15:33:11 visual_prompt]: 	Training 300/553. train loss: 0.7608,	1.5179 s / batch. (data: 6.95e-01). ETA=22:29:24, max mem: 20.9 GB 
[11/23 15:34:47 visual_prompt]: 	Training 400/553. train loss: 0.5434,	0.8277 s / batch. (data: 3.33e-04). ETA=12:14:28, max mem: 20.9 GB 
[11/23 15:36:29 visual_prompt]: 	Training 500/553. train loss: 0.2530,	3.5040 s / batch. (data: 2.67e+00). ETA=2 days, 3:43:26, max mem: 20.9 GB 
[11/23 15:37:24 visual_prompt]: Epoch 4 / 100: avg data time: 1.88e-01, avg batch time: 1.0139, average train loss: 2.4336
[11/23 15:38:21 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3077, average loss: 0.9127
[11/23 15:38:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.69	
[11/23 15:38:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/23 15:40:05 visual_prompt]: 	Training 100/553. train loss: 0.0051,	0.8103 s / batch. (data: 2.95e-04). ETA=11:55:37, max mem: 20.9 GB 
[11/23 15:41:45 visual_prompt]: 	Training 200/553. train loss: 1.9763,	1.0560 s / batch. (data: 2.35e-01). ETA=15:30:47, max mem: 20.9 GB 
[11/23 15:43:27 visual_prompt]: 	Training 300/553. train loss: 1.9640,	0.8239 s / batch. (data: 3.94e-04). ETA=12:04:54, max mem: 20.9 GB 
[11/23 15:45:07 visual_prompt]: 	Training 400/553. train loss: 0.7200,	0.8237 s / batch. (data: 3.17e-04). ETA=12:03:19, max mem: 20.9 GB 
[11/23 15:46:48 visual_prompt]: 	Training 500/553. train loss: 0.5754,	0.8542 s / batch. (data: 1.02e-02). ETA=12:28:40, max mem: 20.9 GB 
[11/23 15:47:42 visual_prompt]: Epoch 5 / 100: avg data time: 1.88e-01, avg batch time: 1.0136, average train loss: 2.6534
[11/23 15:48:40 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3085, average loss: 5.0650
[11/23 15:48:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.42	
[11/23 15:48:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/23 15:50:26 visual_prompt]: 	Training 100/553. train loss: 2.6954,	0.8503 s / batch. (data: 5.95e-03). ETA=12:23:05, max mem: 20.9 GB 
[11/23 15:52:06 visual_prompt]: 	Training 200/553. train loss: 6.5801,	0.8369 s / batch. (data: 5.45e-03). ETA=12:10:00, max mem: 20.9 GB 
[11/23 15:53:46 visual_prompt]: 	Training 300/553. train loss: 5.0656,	0.8113 s / batch. (data: 3.06e-04). ETA=11:46:15, max mem: 20.9 GB 
[11/23 15:55:30 visual_prompt]: 	Training 400/553. train loss: 1.3784,	0.8382 s / batch. (data: 5.45e-03). ETA=12:08:19, max mem: 20.9 GB 
[11/23 15:57:09 visual_prompt]: 	Training 500/553. train loss: 0.1031,	0.8362 s / batch. (data: 5.49e-03). ETA=12:05:12, max mem: 20.9 GB 
[11/23 15:58:01 visual_prompt]: Epoch 6 / 100: avg data time: 1.90e-01, avg batch time: 1.0154, average train loss: 3.5055
[11/23 15:58:59 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3062, average loss: 6.6380
[11/23 15:58:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 37.33	
[11/23 15:58:59 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/23 16:00:43 visual_prompt]: 	Training 100/553. train loss: 12.1488,	0.8201 s / batch. (data: 5.47e-03). ETA=11:49:06, max mem: 20.9 GB 
[11/23 16:02:24 visual_prompt]: 	Training 200/553. train loss: 1.0307,	0.8484 s / batch. (data: 7.97e-03). ETA=12:12:13, max mem: 20.9 GB 
[11/23 16:04:08 visual_prompt]: 	Training 300/553. train loss: 3.5153,	1.8322 s / batch. (data: 1.01e+00). ETA=1 day, 2:18:10, max mem: 20.9 GB 
[11/23 16:05:50 visual_prompt]: 	Training 400/553. train loss: 2.3256,	1.8447 s / batch. (data: 1.02e+00). ETA=1 day, 2:25:55, max mem: 20.9 GB 
[11/23 16:07:28 visual_prompt]: 	Training 500/553. train loss: 3.2859,	0.8311 s / batch. (data: 7.98e-03). ETA=11:53:04, max mem: 20.9 GB 
[11/23 16:08:20 visual_prompt]: Epoch 7 / 100: avg data time: 1.88e-01, avg batch time: 1.0143, average train loss: 4.0773
[11/23 16:09:18 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3076, average loss: 2.2820
[11/23 16:09:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.19	
[11/23 16:09:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/23 16:11:01 visual_prompt]: 	Training 100/553. train loss: 2.5110,	0.8247 s / batch. (data: 3.06e-04). ETA=11:45:28, max mem: 20.9 GB 
[11/23 16:12:43 visual_prompt]: 	Training 200/553. train loss: 3.3374,	0.8254 s / batch. (data: 1.61e-02). ETA=11:44:44, max mem: 20.9 GB 
[11/23 16:14:25 visual_prompt]: 	Training 300/553. train loss: 2.0184,	0.8405 s / batch. (data: 5.45e-03). ETA=11:56:11, max mem: 20.9 GB 
[11/23 16:16:06 visual_prompt]: 	Training 400/553. train loss: 0.7383,	0.9800 s / batch. (data: 1.48e-01). ETA=13:53:30, max mem: 20.9 GB 
[11/23 16:17:47 visual_prompt]: 	Training 500/553. train loss: 2.7546,	1.4966 s / batch. (data: 6.87e-01). ETA=21:10:18, max mem: 20.9 GB 
[11/23 16:18:40 visual_prompt]: Epoch 8 / 100: avg data time: 1.90e-01, avg batch time: 1.0160, average train loss: 5.1077
[11/23 16:19:38 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3088, average loss: 0.8597
[11/23 16:19:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.15	
[11/23 16:19:38 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/23 16:21:24 visual_prompt]: 	Training 100/553. train loss: 4.1649,	0.8473 s / batch. (data: 3.94e-04). ETA=11:57:01, max mem: 20.9 GB 
[11/23 16:23:05 visual_prompt]: 	Training 200/553. train loss: 0.6673,	0.8120 s / batch. (data: 3.11e-04). ETA=11:25:49, max mem: 20.9 GB 
[11/23 16:24:47 visual_prompt]: 	Training 300/553. train loss: 7.3893,	1.8233 s / batch. (data: 9.93e-01). ETA=1 day, 1:36:53, max mem: 20.9 GB 
[11/23 16:26:30 visual_prompt]: 	Training 400/553. train loss: 0.6317,	0.8385 s / batch. (data: 8.34e-04). ETA=11:45:24, max mem: 20.9 GB 
[11/23 16:28:11 visual_prompt]: 	Training 500/553. train loss: 3.4955,	0.8443 s / batch. (data: 3.24e-02). ETA=11:48:53, max mem: 20.9 GB 
[11/23 16:29:04 visual_prompt]: Epoch 9 / 100: avg data time: 1.97e-01, avg batch time: 1.0233, average train loss: 5.4163
[11/23 16:30:03 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3079, average loss: 1.2256
[11/23 16:30:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.59	
[11/23 16:30:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/23 16:31:52 visual_prompt]: 	Training 100/553. train loss: 28.6720,	0.8222 s / batch. (data: 8.54e-04). ETA=11:28:13, max mem: 20.9 GB 
[11/23 16:33:32 visual_prompt]: 	Training 200/553. train loss: 5.4005,	0.8160 s / batch. (data: 3.38e-04). ETA=11:21:39, max mem: 20.9 GB 
[11/23 16:35:12 visual_prompt]: 	Training 300/553. train loss: 17.1056,	0.8123 s / batch. (data: 3.03e-04). ETA=11:17:15, max mem: 20.9 GB 
[11/23 16:36:52 visual_prompt]: 	Training 400/553. train loss: 6.5896,	0.8639 s / batch. (data: 2.57e-02). ETA=11:58:48, max mem: 20.9 GB 
[11/23 16:38:35 visual_prompt]: 	Training 500/553. train loss: 0.4526,	1.0720 s / batch. (data: 2.34e-01). ETA=14:50:11, max mem: 20.9 GB 
[11/23 16:39:29 visual_prompt]: Epoch 10 / 100: avg data time: 1.98e-01, avg batch time: 1.0238, average train loss: 7.0842
[11/23 16:40:27 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3076, average loss: 1.9893
[11/23 16:40:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.73	
[11/23 16:40:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/23 16:42:15 visual_prompt]: 	Training 100/553. train loss: 2.3298,	0.8109 s / batch. (data: 4.89e-04). ETA=11:11:19, max mem: 20.9 GB 
[11/23 16:43:59 visual_prompt]: 	Training 200/553. train loss: 27.7675,	0.8280 s / batch. (data: 3.23e-04). ETA=11:24:04, max mem: 20.9 GB 
[11/23 16:45:40 visual_prompt]: 	Training 300/553. train loss: 34.4139,	2.1675 s / batch. (data: 1.33e+00). ETA=1 day, 5:47:06, max mem: 20.9 GB 
[11/23 16:47:20 visual_prompt]: 	Training 400/553. train loss: 3.9413,	0.8166 s / batch. (data: 2.74e-04). ETA=11:11:55, max mem: 20.9 GB 
[11/23 16:49:00 visual_prompt]: 	Training 500/553. train loss: 1.0671,	0.8440 s / batch. (data: 3.06e-04). ETA=11:33:04, max mem: 20.9 GB 
[11/23 16:49:52 visual_prompt]: Epoch 11 / 100: avg data time: 1.93e-01, avg batch time: 1.0205, average train loss: 7.7834
[11/23 16:50:50 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3083, average loss: 0.8970
[11/23 16:50:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.27	
[11/23 16:50:50 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/23 16:52:37 visual_prompt]: 	Training 100/553. train loss: 8.2900,	1.0666 s / batch. (data: 2.52e-01). ETA=14:33:09, max mem: 20.9 GB 
[11/23 16:54:20 visual_prompt]: 	Training 200/553. train loss: 4.5576,	0.8153 s / batch. (data: 5.45e-03). ETA=11:06:01, max mem: 20.9 GB 
[11/23 16:56:00 visual_prompt]: 	Training 300/553. train loss: 6.9376,	0.8278 s / batch. (data: 3.25e-04). ETA=11:14:54, max mem: 20.9 GB 
[11/23 16:57:42 visual_prompt]: 	Training 400/553. train loss: 4.4879,	0.8252 s / batch. (data: 1.20e-02). ETA=11:11:25, max mem: 20.9 GB 
[11/23 16:59:23 visual_prompt]: 	Training 500/553. train loss: 0.7148,	0.8318 s / batch. (data: 5.47e-03). ETA=11:15:21, max mem: 20.9 GB 
[11/23 17:00:16 visual_prompt]: Epoch 12 / 100: avg data time: 1.97e-01, avg batch time: 1.0231, average train loss: 7.2357
[11/23 17:01:14 visual_prompt]: Inference (val):avg data time: 3.18e-04, avg batch time: 0.3077, average loss: 3.4477
[11/23 17:01:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.88	
[11/23 17:01:14 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/23 17:03:02 visual_prompt]: 	Training 100/553. train loss: 3.1823,	0.8320 s / batch. (data: 7.95e-03). ETA=11:13:24, max mem: 20.9 GB 
[11/23 17:04:41 visual_prompt]: 	Training 200/553. train loss: 5.5554,	0.8360 s / batch. (data: 3.12e-04). ETA=11:15:16, max mem: 20.9 GB 
[11/23 17:06:23 visual_prompt]: 	Training 300/553. train loss: 0.9770,	1.7921 s / batch. (data: 9.46e-01). ETA=1 day, 0:04:30, max mem: 20.9 GB 
[11/23 17:08:04 visual_prompt]: 	Training 400/553. train loss: 24.2262,	0.8226 s / batch. (data: 1.15e-02). ETA=11:01:43, max mem: 20.9 GB 
[11/23 17:09:47 visual_prompt]: 	Training 500/553. train loss: 4.8191,	0.8396 s / batch. (data: 7.50e-03). ETA=11:13:56, max mem: 20.9 GB 
[11/23 17:10:39 visual_prompt]: Epoch 13 / 100: avg data time: 1.95e-01, avg batch time: 1.0215, average train loss: 7.4968
[11/23 17:11:38 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3092, average loss: 4.0477
[11/23 17:11:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.12	
[11/23 17:11:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/23 17:13:25 visual_prompt]: 	Training 100/553. train loss: 3.8470,	0.8320 s / batch. (data: 3.14e-04). ETA=11:05:43, max mem: 20.9 GB 
[11/23 17:15:06 visual_prompt]: 	Training 200/553. train loss: 31.0490,	1.0267 s / batch. (data: 2.04e-01). ETA=13:39:51, max mem: 20.9 GB 
[11/23 17:16:48 visual_prompt]: 	Training 300/553. train loss: 3.5751,	0.8321 s / batch. (data: 1.21e-02). ETA=11:03:03, max mem: 20.9 GB 
[11/23 17:18:29 visual_prompt]: 	Training 400/553. train loss: 1.4584,	0.8360 s / batch. (data: 3.40e-04). ETA=11:04:45, max mem: 20.9 GB 
[11/23 17:20:11 visual_prompt]: 	Training 500/553. train loss: 5.5519,	0.8103 s / batch. (data: 3.10e-04). ETA=10:42:57, max mem: 20.9 GB 
[11/23 17:21:03 visual_prompt]: Epoch 14 / 100: avg data time: 1.96e-01, avg batch time: 1.0228, average train loss: 6.7657
[11/23 17:22:02 visual_prompt]: Inference (val):avg data time: 3.67e-04, avg batch time: 0.3061, average loss: 1.5707
[11/23 17:22:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.87	
[11/23 17:22:02 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/23 17:23:48 visual_prompt]: 	Training 100/553. train loss: 8.5192,	0.8450 s / batch. (data: 1.74e-02). ETA=11:08:23, max mem: 20.9 GB 
[11/23 17:25:28 visual_prompt]: 	Training 200/553. train loss: 7.0463,	0.8104 s / batch. (data: 3.32e-04). ETA=10:39:40, max mem: 20.9 GB 
[11/23 17:27:12 visual_prompt]: 	Training 300/553. train loss: 14.5544,	0.8287 s / batch. (data: 1.20e-02). ETA=10:52:43, max mem: 20.9 GB 
[11/23 17:28:52 visual_prompt]: 	Training 400/553. train loss: 6.5952,	1.1441 s / batch. (data: 3.06e-01). ETA=14:59:15, max mem: 20.9 GB 
[11/23 17:30:35 visual_prompt]: 	Training 500/553. train loss: 8.2057,	0.8386 s / batch. (data: 3.25e-04). ETA=10:57:40, max mem: 20.9 GB 
[11/23 17:31:28 visual_prompt]: Epoch 15 / 100: avg data time: 1.97e-01, avg batch time: 1.0236, average train loss: 8.7889
[11/23 17:32:27 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3066, average loss: 2.6206
[11/23 17:32:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.29	
[11/23 17:32:27 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/23 17:34:11 visual_prompt]: 	Training 100/553. train loss: 2.7006,	0.8360 s / batch. (data: 3.34e-04). ETA=10:53:31, max mem: 20.9 GB 
[11/23 17:35:53 visual_prompt]: 	Training 200/553. train loss: 13.8312,	0.8121 s / batch. (data: 3.23e-04). ETA=10:33:28, max mem: 20.9 GB 
[11/23 17:37:35 visual_prompt]: 	Training 300/553. train loss: 1.7735,	0.8320 s / batch. (data: 1.07e-03). ETA=10:47:38, max mem: 20.9 GB 
[11/23 17:39:17 visual_prompt]: 	Training 400/553. train loss: 2.1459,	0.8368 s / batch. (data: 8.21e-04). ETA=10:50:00, max mem: 20.9 GB 
[11/23 17:40:58 visual_prompt]: 	Training 500/553. train loss: 13.0191,	1.5380 s / batch. (data: 7.29e-01). ETA=19:52:03, max mem: 20.9 GB 
[11/23 17:41:51 visual_prompt]: Epoch 16 / 100: avg data time: 1.95e-01, avg batch time: 1.0211, average train loss: 8.9952
[11/23 17:42:50 visual_prompt]: Inference (val):avg data time: 1.67e-04, avg batch time: 0.3087, average loss: 15.6493
[11/23 17:42:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.58	
[11/23 17:42:50 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/23 17:44:37 visual_prompt]: 	Training 100/553. train loss: 3.8247,	0.8358 s / batch. (data: 3.10e-04). ETA=10:45:41, max mem: 20.9 GB 
[11/23 17:46:20 visual_prompt]: 	Training 200/553. train loss: 1.4259,	0.8257 s / batch. (data: 5.47e-03). ETA=10:36:31, max mem: 20.9 GB 
[11/23 17:48:02 visual_prompt]: 	Training 300/553. train loss: 10.5930,	0.8280 s / batch. (data: 5.43e-03). ETA=10:36:53, max mem: 20.9 GB 
[11/23 17:49:43 visual_prompt]: 	Training 400/553. train loss: 14.6420,	0.9467 s / batch. (data: 1.12e-01). ETA=12:06:36, max mem: 20.9 GB 
[11/23 17:51:24 visual_prompt]: 	Training 500/553. train loss: 5.6674,	1.6440 s / batch. (data: 8.14e-01). ETA=20:59:05, max mem: 20.9 GB 
[11/23 17:52:18 visual_prompt]: Epoch 17 / 100: avg data time: 2.01e-01, avg batch time: 1.0279, average train loss: 7.7397
[11/23 17:53:17 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3063, average loss: 1.6430
[11/23 17:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.38	
[11/23 17:53:17 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/23 17:55:02 visual_prompt]: 	Training 100/553. train loss: 17.1106,	0.8097 s / batch. (data: 3.17e-04). ETA=10:18:01, max mem: 20.9 GB 
[11/23 17:56:46 visual_prompt]: 	Training 200/553. train loss: 8.3440,	0.8360 s / batch. (data: 3.11e-04). ETA=10:36:45, max mem: 20.9 GB 
[11/23 17:58:28 visual_prompt]: 	Training 300/553. train loss: 0.9150,	0.8320 s / batch. (data: 3.26e-04). ETA=10:32:17, max mem: 20.9 GB 
[11/23 18:00:10 visual_prompt]: 	Training 400/553. train loss: 4.5661,	0.8199 s / batch. (data: 5.45e-03). ETA=10:21:46, max mem: 20.9 GB 
[11/23 18:01:51 visual_prompt]: 	Training 500/553. train loss: 0.8066,	0.8450 s / batch. (data: 3.55e-04). ETA=10:39:23, max mem: 20.9 GB 
[11/23 18:02:43 visual_prompt]: Epoch 18 / 100: avg data time: 1.97e-01, avg batch time: 1.0230, average train loss: 6.6641
[11/23 18:03:41 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3073, average loss: 4.8269
[11/23 18:03:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.34	
[11/23 18:03:41 visual_prompt]: Stopping early.
[11/23 18:03:41 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 18:03:41 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 18:03:41 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/23 18:03:41 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 18:03:41 visual_prompt]: Training with config:
[11/23 18:03:41 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 18:03:41 visual_prompt]: Loading training data...
[11/23 18:03:41 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 18:03:41 visual_prompt]: Loading validation data...
[11/23 18:03:41 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 18:03:41 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 18:03:44 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 18:03:44 visual_prompt]: tuned percent:0.525
[11/23 18:03:44 visual_prompt]: Device used for model: 0
[11/23 18:03:44 visual_prompt]: Setting up Evaluator...
[11/23 18:03:44 visual_prompt]: Setting up Trainer...
[11/23 18:03:44 visual_prompt]: 	Setting up the optimizer...
[11/23 18:03:44 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 18:05:29 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8436 s / batch. (data: 5.44e-03). ETA=12:56:08, max mem: 20.9 GB 
[11/23 18:07:10 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8320 s / batch. (data: 3.19e-04). ETA=12:44:03, max mem: 20.9 GB 
[11/23 18:08:53 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0788 s / batch. (data: 2.70e-01). ETA=16:28:55, max mem: 20.9 GB 
[11/23 18:10:33 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8192 s / batch. (data: 3.10e-04). ETA=12:29:31, max mem: 20.9 GB 
[11/23 18:12:17 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8354 s / batch. (data: 3.19e-04). ETA=12:42:58, max mem: 20.9 GB 
[11/23 18:13:11 visual_prompt]: Epoch 1 / 100: avg data time: 1.99e-01, avg batch time: 1.0249, average train loss: 1.5403
[11/23 18:14:10 visual_prompt]: Inference (val):avg data time: 4.93e-05, avg batch time: 0.3076, average loss: 1.5201
[11/23 18:14:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 18:14:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/23 18:15:54 visual_prompt]: 	Training 100/553. train loss: 1.1712,	0.8795 s / batch. (data: 5.39e-02). ETA=13:21:00, max mem: 20.9 GB 
[11/23 18:17:36 visual_prompt]: 	Training 200/553. train loss: 0.0979,	0.8321 s / batch. (data: 3.33e-04). ETA=12:36:27, max mem: 20.9 GB 
[11/23 18:19:18 visual_prompt]: 	Training 300/553. train loss: 1.4903,	1.0598 s / batch. (data: 2.24e-01). ETA=16:01:44, max mem: 20.9 GB 
[11/23 18:20:58 visual_prompt]: 	Training 400/553. train loss: 0.5685,	0.8428 s / batch. (data: 1.56e-02). ETA=12:43:26, max mem: 20.9 GB 
[11/23 18:22:40 visual_prompt]: 	Training 500/553. train loss: 0.7332,	0.8104 s / batch. (data: 3.05e-04). ETA=12:12:39, max mem: 20.9 GB 
[11/23 18:23:32 visual_prompt]: Epoch 2 / 100: avg data time: 1.90e-01, avg batch time: 1.0161, average train loss: 1.2897
[11/23 18:24:30 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3060, average loss: 2.6171
[11/23 18:24:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.83	
[11/23 18:24:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/23 18:26:13 visual_prompt]: 	Training 100/553. train loss: 1.3173,	0.8280 s / batch. (data: 3.20e-04). ETA=12:26:29, max mem: 20.9 GB 
[11/23 18:27:54 visual_prompt]: 	Training 200/553. train loss: 0.7036,	0.8490 s / batch. (data: 5.44e-03). ETA=12:44:00, max mem: 20.9 GB 
[11/23 18:29:33 visual_prompt]: 	Training 300/553. train loss: 1.2212,	0.8198 s / batch. (data: 3.04e-04). ETA=12:16:20, max mem: 20.9 GB 
[11/23 18:31:14 visual_prompt]: 	Training 400/553. train loss: 0.3644,	0.8480 s / batch. (data: 1.60e-02). ETA=12:40:15, max mem: 20.9 GB 
[11/23 18:32:56 visual_prompt]: 	Training 500/553. train loss: 0.7960,	1.2569 s / batch. (data: 4.38e-01). ETA=18:44:46, max mem: 20.9 GB 
[11/23 18:33:47 visual_prompt]: Epoch 3 / 100: avg data time: 1.82e-01, avg batch time: 1.0076, average train loss: 1.3528
[11/23 18:34:44 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3104, average loss: 1.8503
[11/23 18:34:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.62	
[11/23 18:34:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/23 18:36:29 visual_prompt]: 	Training 100/553. train loss: 1.2718,	0.8360 s / batch. (data: 2.99e-04). ETA=12:26:00, max mem: 20.9 GB 
[11/23 18:38:08 visual_prompt]: 	Training 200/553. train loss: 0.9991,	0.8180 s / batch. (data: 5.45e-03). ETA=12:08:36, max mem: 20.9 GB 
[11/23 18:39:48 visual_prompt]: 	Training 300/553. train loss: 0.6045,	0.8884 s / batch. (data: 7.87e-02). ETA=13:09:48, max mem: 20.9 GB 
[11/23 18:41:23 visual_prompt]: 	Training 400/553. train loss: 1.0981,	0.8361 s / batch. (data: 1.06e-02). ETA=12:21:55, max mem: 20.9 GB 
[11/23 18:43:03 visual_prompt]: 	Training 500/553. train loss: 2.7090,	3.5263 s / batch. (data: 2.72e+00). ETA=2 days, 4:03:12, max mem: 20.9 GB 
[11/23 18:43:57 visual_prompt]: Epoch 4 / 100: avg data time: 1.75e-01, avg batch time: 1.0005, average train loss: 1.2842
[11/23 18:44:54 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3083, average loss: 1.5540
[11/23 18:44:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.66	
[11/23 18:44:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/23 18:46:36 visual_prompt]: 	Training 100/553. train loss: 16.0523,	0.8299 s / batch. (data: 5.43e-03). ETA=12:12:56, max mem: 20.9 GB 
[11/23 18:48:14 visual_prompt]: 	Training 200/553. train loss: 2.0396,	1.0120 s / batch. (data: 1.84e-01). ETA=14:52:01, max mem: 20.9 GB 
[11/23 18:49:54 visual_prompt]: 	Training 300/553. train loss: 0.7127,	0.8283 s / batch. (data: 8.26e-04). ETA=12:08:46, max mem: 20.9 GB 
[11/23 18:51:32 visual_prompt]: 	Training 400/553. train loss: 5.3987,	0.8280 s / batch. (data: 3.09e-04). ETA=12:07:05, max mem: 20.9 GB 
[11/23 18:53:10 visual_prompt]: 	Training 500/553. train loss: 2.3383,	0.8319 s / batch. (data: 4.77e-04). ETA=12:09:10, max mem: 20.9 GB 
[11/23 18:54:02 visual_prompt]: Epoch 5 / 100: avg data time: 1.63e-01, avg batch time: 0.9904, average train loss: 2.3967
[11/23 18:54:59 visual_prompt]: Inference (val):avg data time: 4.03e-04, avg batch time: 0.3095, average loss: 1.1091
[11/23 18:54:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.43	
[11/23 18:54:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/23 18:56:42 visual_prompt]: 	Training 100/553. train loss: 3.2001,	0.8240 s / batch. (data: 2.97e-04). ETA=12:00:04, max mem: 20.9 GB 
[11/23 18:58:20 visual_prompt]: 	Training 200/553. train loss: 12.0924,	0.8414 s / batch. (data: 5.58e-03). ETA=12:13:52, max mem: 20.9 GB 
[11/23 18:59:57 visual_prompt]: 	Training 300/553. train loss: 0.6165,	0.8120 s / batch. (data: 3.31e-04). ETA=11:46:54, max mem: 20.9 GB 
[11/23 19:01:39 visual_prompt]: 	Training 400/553. train loss: 9.0347,	0.8320 s / batch. (data: 3.59e-04). ETA=12:02:55, max mem: 20.9 GB 
[11/23 19:03:15 visual_prompt]: 	Training 500/553. train loss: 0.9203,	0.8167 s / batch. (data: 5.62e-03). ETA=11:48:18, max mem: 20.9 GB 
[11/23 19:04:07 visual_prompt]: Epoch 6 / 100: avg data time: 1.66e-01, avg batch time: 0.9908, average train loss: 3.5688
[11/23 19:05:03 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3077, average loss: 2.7631
[11/23 19:05:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.84	
[11/23 19:05:03 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/23 19:06:44 visual_prompt]: 	Training 100/553. train loss: 3.1863,	0.8196 s / batch. (data: 9.65e-03). ETA=11:48:44, max mem: 20.9 GB 
[11/23 19:08:21 visual_prompt]: 	Training 200/553. train loss: 1.2053,	0.8237 s / batch. (data: 7.96e-03). ETA=11:50:51, max mem: 20.9 GB 
[11/23 19:10:02 visual_prompt]: 	Training 300/553. train loss: 2.6570,	1.3920 s / batch. (data: 5.63e-01). ETA=19:59:00, max mem: 20.9 GB 
[11/23 19:11:40 visual_prompt]: 	Training 400/553. train loss: 1.3255,	1.6143 s / batch. (data: 7.86e-01). ETA=23:07:50, max mem: 20.9 GB 
[11/23 19:13:17 visual_prompt]: 	Training 500/553. train loss: 2.3669,	0.8123 s / batch. (data: 3.30e-04). ETA=11:36:59, max mem: 20.9 GB 
[11/23 19:14:07 visual_prompt]: Epoch 7 / 100: avg data time: 1.56e-01, avg batch time: 0.9834, average train loss: 4.0666
[11/23 19:15:03 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3083, average loss: 5.2876
[11/23 19:15:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.18	
[11/23 19:15:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/23 19:16:42 visual_prompt]: 	Training 100/553. train loss: 2.6818,	0.8360 s / batch. (data: 7.95e-03). ETA=11:55:12, max mem: 20.9 GB 
[11/23 19:18:22 visual_prompt]: 	Training 200/553. train loss: 0.5606,	0.8425 s / batch. (data: 3.41e-04). ETA=11:59:22, max mem: 20.9 GB 
[11/23 19:20:01 visual_prompt]: 	Training 300/553. train loss: 3.3904,	0.8202 s / batch. (data: 3.05e-04). ETA=11:38:55, max mem: 20.9 GB 
[11/23 19:21:39 visual_prompt]: 	Training 400/553. train loss: 3.0755,	0.8403 s / batch. (data: 1.56e-02). ETA=11:54:40, max mem: 20.9 GB 
[11/23 19:23:16 visual_prompt]: 	Training 500/553. train loss: 18.5463,	1.3809 s / batch. (data: 5.64e-01). ETA=19:32:05, max mem: 20.9 GB 
[11/23 19:24:08 visual_prompt]: Epoch 8 / 100: avg data time: 1.59e-01, avg batch time: 0.9851, average train loss: 4.4663
[11/23 19:25:04 visual_prompt]: Inference (val):avg data time: 2.01e-04, avg batch time: 0.3081, average loss: 1.9721
[11/23 19:25:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.10	
[11/23 19:25:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/23 19:26:46 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8240 s / batch. (data: 3.04e-04). ETA=11:37:19, max mem: 20.9 GB 
[11/23 19:28:24 visual_prompt]: 	Training 200/553. train loss: 0.9589,	0.8169 s / batch. (data: 3.04e-04). ETA=11:29:58, max mem: 20.9 GB 
[11/23 19:30:02 visual_prompt]: 	Training 300/553. train loss: 11.3712,	1.4571 s / batch. (data: 6.18e-01). ETA=20:28:16, max mem: 20.9 GB 
[11/23 19:31:41 visual_prompt]: 	Training 400/553. train loss: 2.1421,	0.8320 s / batch. (data: 2.96e-04). ETA=11:39:56, max mem: 20.9 GB 
[11/23 19:33:19 visual_prompt]: 	Training 500/553. train loss: 2.3915,	0.8240 s / batch. (data: 3.29e-04). ETA=11:31:49, max mem: 20.9 GB 
[11/23 19:34:09 visual_prompt]: Epoch 9 / 100: avg data time: 1.60e-01, avg batch time: 0.9856, average train loss: 4.8825
[11/23 19:35:06 visual_prompt]: Inference (val):avg data time: 1.81e-04, avg batch time: 0.3062, average loss: 0.7921
[11/23 19:35:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 40.65	rocauc: 41.97	
[11/23 19:35:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/23 19:36:50 visual_prompt]: 	Training 100/553. train loss: 5.6283,	0.8172 s / batch. (data: 2.95e-04). ETA=11:24:01, max mem: 20.9 GB 
[11/23 19:38:27 visual_prompt]: 	Training 200/553. train loss: 1.0567,	0.8400 s / batch. (data: 1.05e-02). ETA=11:41:42, max mem: 20.9 GB 
[11/23 19:40:05 visual_prompt]: 	Training 300/553. train loss: 7.1913,	0.8238 s / batch. (data: 3.19e-04). ETA=11:26:48, max mem: 20.9 GB 
[11/23 19:41:41 visual_prompt]: 	Training 400/553. train loss: 9.4222,	0.8520 s / batch. (data: 7.96e-03). ETA=11:48:54, max mem: 20.9 GB 
[11/23 19:43:20 visual_prompt]: 	Training 500/553. train loss: 12.0612,	0.8320 s / batch. (data: 3.35e-04). ETA=11:30:53, max mem: 20.9 GB 
[11/23 19:44:12 visual_prompt]: Epoch 10 / 100: avg data time: 1.62e-01, avg batch time: 0.9871, average train loss: 6.9467
[11/23 19:45:08 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.3092, average loss: 4.1296
[11/23 19:45:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.30	
[11/23 19:45:08 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/23 19:46:50 visual_prompt]: 	Training 100/553. train loss: 3.6753,	0.8400 s / batch. (data: 1.20e-02). ETA=11:35:21, max mem: 20.9 GB 
[11/23 19:48:28 visual_prompt]: 	Training 200/553. train loss: 4.9292,	0.8194 s / batch. (data: 2.99e-04). ETA=11:16:56, max mem: 20.9 GB 
[11/23 19:50:05 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.5922 s / batch. (data: 7.79e-01). ETA=21:52:46, max mem: 20.9 GB 
[11/23 19:51:44 visual_prompt]: 	Training 400/553. train loss: 0.7602,	0.8157 s / batch. (data: 2.96e-04). ETA=11:11:11, max mem: 20.9 GB 
[11/23 19:53:23 visual_prompt]: 	Training 500/553. train loss: 3.7707,	0.8109 s / batch. (data: 3.12e-04). ETA=11:05:55, max mem: 20.9 GB 
[11/23 19:54:16 visual_prompt]: Epoch 11 / 100: avg data time: 1.65e-01, avg batch time: 0.9908, average train loss: 6.0666
[11/23 19:55:13 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3079, average loss: 1.8684
[11/23 19:55:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.24	
[11/23 19:55:13 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/23 19:56:59 visual_prompt]: 	Training 100/553. train loss: 2.4200,	0.8360 s / batch. (data: 8.82e-04). ETA=11:24:22, max mem: 20.9 GB 
[11/23 19:58:42 visual_prompt]: 	Training 200/553. train loss: 2.7326,	2.2032 s / batch. (data: 1.39e+00). ETA=1 day, 5:59:52, max mem: 20.9 GB 
[11/23 20:00:20 visual_prompt]: 	Training 300/553. train loss: 10.3288,	0.8308 s / batch. (data: 3.21e-04). ETA=11:17:20, max mem: 20.9 GB 
[11/23 20:02:02 visual_prompt]: 	Training 400/553. train loss: 6.6342,	0.8255 s / batch. (data: 5.44e-03). ETA=11:11:39, max mem: 20.9 GB 
[11/23 20:03:42 visual_prompt]: 	Training 500/553. train loss: 3.1335,	0.8422 s / batch. (data: 5.50e-03). ETA=11:23:48, max mem: 20.9 GB 
[11/23 20:04:34 visual_prompt]: Epoch 12 / 100: avg data time: 1.87e-01, avg batch time: 1.0134, average train loss: 6.9875
[11/23 20:05:31 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3084, average loss: 1.9784
[11/23 20:05:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.19	
[11/23 20:05:31 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/23 20:07:18 visual_prompt]: 	Training 100/553. train loss: 6.9440,	0.8200 s / batch. (data: 3.38e-04). ETA=11:03:42, max mem: 20.9 GB 
[11/23 20:08:56 visual_prompt]: 	Training 200/553. train loss: 3.8198,	0.8527 s / batch. (data: 2.06e-02). ETA=11:28:44, max mem: 20.9 GB 
[11/23 20:10:37 visual_prompt]: 	Training 300/553. train loss: 3.7713,	1.8994 s / batch. (data: 1.07e+00). ETA=1 day, 1:31:02, max mem: 20.9 GB 
[11/23 20:12:16 visual_prompt]: 	Training 400/553. train loss: 33.9767,	0.8226 s / batch. (data: 1.16e-02). ETA=11:01:41, max mem: 20.9 GB 
[11/23 20:13:58 visual_prompt]: 	Training 500/553. train loss: 18.2886,	0.8360 s / batch. (data: 3.14e-04). ETA=11:11:03, max mem: 20.9 GB 
[11/23 20:14:51 visual_prompt]: Epoch 13 / 100: avg data time: 1.85e-01, avg batch time: 1.0109, average train loss: 8.3601
[11/23 20:15:48 visual_prompt]: Inference (val):avg data time: 2.28e-04, avg batch time: 0.3075, average loss: 2.4024
[11/23 20:15:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.31	
[11/23 20:15:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/23 20:17:35 visual_prompt]: 	Training 100/553. train loss: 5.6784,	0.8184 s / batch. (data: 3.17e-04). ETA=10:54:53, max mem: 20.9 GB 
[11/23 20:19:15 visual_prompt]: 	Training 200/553. train loss: 3.5062,	1.3880 s / batch. (data: 5.43e-01). ETA=18:28:21, max mem: 20.9 GB 
[11/23 20:20:56 visual_prompt]: 	Training 300/553. train loss: 2.7180,	0.8190 s / batch. (data: 3.53e-04). ETA=10:52:37, max mem: 20.9 GB 
[11/23 20:22:36 visual_prompt]: 	Training 400/553. train loss: 2.3671,	0.8560 s / batch. (data: 1.20e-02). ETA=11:20:39, max mem: 20.9 GB 
[11/23 20:24:16 visual_prompt]: 	Training 500/553. train loss: 3.3707,	0.8440 s / batch. (data: 3.06e-04). ETA=11:09:43, max mem: 20.9 GB 
[11/23 20:25:09 visual_prompt]: Epoch 14 / 100: avg data time: 1.87e-01, avg batch time: 1.0128, average train loss: 7.4798
[11/23 20:26:06 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3065, average loss: 6.0543
[11/23 20:26:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.13	
[11/23 20:26:06 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/23 20:27:51 visual_prompt]: 	Training 100/553. train loss: 7.5818,	1.1563 s / batch. (data: 3.34e-01). ETA=15:14:37, max mem: 20.9 GB 
[11/23 20:29:30 visual_prompt]: 	Training 200/553. train loss: 37.2050,	0.8278 s / batch. (data: 3.13e-04). ETA=10:53:23, max mem: 20.9 GB 
[11/23 20:31:12 visual_prompt]: 	Training 300/553. train loss: 0.9968,	0.8209 s / batch. (data: 5.49e-03). ETA=10:46:33, max mem: 20.9 GB 
[11/23 20:32:50 visual_prompt]: 	Training 400/553. train loss: 15.8997,	1.2430 s / batch. (data: 4.13e-01). ETA=16:16:55, max mem: 20.9 GB 
[11/23 20:34:32 visual_prompt]: 	Training 500/553. train loss: 0.9633,	0.8520 s / batch. (data: 3.35e-04). ETA=11:08:13, max mem: 20.9 GB 
[11/23 20:35:25 visual_prompt]: Epoch 15 / 100: avg data time: 1.85e-01, avg batch time: 1.0106, average train loss: 7.0170
[11/23 20:36:23 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3102, average loss: 79.1104
[11/23 20:36:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.74	
[11/23 20:36:23 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/23 20:38:06 visual_prompt]: 	Training 100/553. train loss: 12.6038,	0.8374 s / batch. (data: 2.99e-04). ETA=10:54:36, max mem: 20.9 GB 
[11/23 20:39:47 visual_prompt]: 	Training 200/553. train loss: 41.1485,	0.8343 s / batch. (data: 5.45e-03). ETA=10:50:50, max mem: 20.9 GB 
[11/23 20:41:29 visual_prompt]: 	Training 300/553. train loss: 0.7949,	0.8400 s / batch. (data: 3.03e-04). ETA=10:53:52, max mem: 20.9 GB 
[11/23 20:43:09 visual_prompt]: 	Training 400/553. train loss: 1.5314,	0.8280 s / batch. (data: 8.18e-04). ETA=10:43:09, max mem: 20.9 GB 
[11/23 20:44:49 visual_prompt]: 	Training 500/553. train loss: 3.2104,	1.1040 s / batch. (data: 2.75e-01). ETA=14:15:40, max mem: 20.9 GB 
[11/23 20:45:42 visual_prompt]: Epoch 16 / 100: avg data time: 1.85e-01, avg batch time: 1.0107, average train loss: 6.9581
[11/23 20:46:40 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.3064, average loss: 6.3874
[11/23 20:46:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.45	
[11/23 20:46:40 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/23 20:48:24 visual_prompt]: 	Training 100/553. train loss: 3.1416,	0.8240 s / batch. (data: 3.21e-04). ETA=10:36:33, max mem: 20.9 GB 
[11/23 20:50:05 visual_prompt]: 	Training 200/553. train loss: 1.7053,	0.8237 s / batch. (data: 3.25e-04). ETA=10:34:59, max mem: 20.9 GB 
[11/23 20:51:46 visual_prompt]: 	Training 300/553. train loss: 2.6357,	0.8152 s / batch. (data: 5.47e-03). ETA=10:27:03, max mem: 20.9 GB 
[11/23 20:53:25 visual_prompt]: 	Training 400/553. train loss: 7.6595,	1.1497 s / batch. (data: 3.29e-01). ETA=14:42:27, max mem: 20.9 GB 
[11/23 20:55:05 visual_prompt]: 	Training 500/553. train loss: 3.5254,	1.7396 s / batch. (data: 9.30e-01). ETA=22:12:15, max mem: 20.9 GB 
[11/23 20:55:59 visual_prompt]: Epoch 17 / 100: avg data time: 1.87e-01, avg batch time: 1.0122, average train loss: 6.7164
[11/23 20:56:57 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3068, average loss: 1.1450
[11/23 20:56:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.21	
[11/23 20:56:57 visual_prompt]: Best epoch 17: best metric: -1.145
[11/23 20:56:57 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/23 20:58:42 visual_prompt]: 	Training 100/553. train loss: 18.2026,	0.8315 s / batch. (data: 3.26e-04). ETA=10:34:43, max mem: 20.9 GB 
[11/23 21:00:25 visual_prompt]: 	Training 200/553. train loss: 4.7025,	0.8280 s / batch. (data: 8.61e-04). ETA=10:30:39, max mem: 20.9 GB 
[11/23 21:02:06 visual_prompt]: 	Training 300/553. train loss: 1.9836,	0.8083 s / batch. (data: 3.04e-04). ETA=10:14:19, max mem: 20.9 GB 
[11/23 21:03:46 visual_prompt]: 	Training 400/553. train loss: 5.2338,	0.8505 s / batch. (data: 8.63e-03). ETA=10:44:55, max mem: 20.9 GB 
[11/23 21:05:26 visual_prompt]: 	Training 500/553. train loss: 8.3156,	0.8269 s / batch. (data: 3.05e-04). ETA=10:25:41, max mem: 20.9 GB 
[11/23 21:06:17 visual_prompt]: Epoch 18 / 100: avg data time: 1.87e-01, avg batch time: 1.0116, average train loss: 7.3623
[11/23 21:07:14 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3081, average loss: 9.6024
[11/23 21:07:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.61	
[11/23 21:07:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/23 21:08:59 visual_prompt]: 	Training 100/553. train loss: 0.8589,	1.0716 s / batch. (data: 2.40e-01). ETA=13:28:05, max mem: 20.9 GB 
[11/23 21:10:40 visual_prompt]: 	Training 200/553. train loss: 2.2295,	0.8200 s / batch. (data: 3.09e-04). ETA=10:17:00, max mem: 20.9 GB 
[11/23 21:12:21 visual_prompt]: 	Training 300/553. train loss: 17.0633,	0.8479 s / batch. (data: 1.05e-02). ETA=10:36:35, max mem: 20.9 GB 
[11/23 21:14:03 visual_prompt]: 	Training 400/553. train loss: 2.2336,	0.8271 s / batch. (data: 8.61e-04). ETA=10:19:36, max mem: 20.9 GB 
[11/23 21:15:39 visual_prompt]: 	Training 500/553. train loss: 7.0014,	0.8166 s / batch. (data: 5.46e-03). ETA=10:10:23, max mem: 20.9 GB 
[11/23 21:16:32 visual_prompt]: Epoch 19 / 100: avg data time: 1.81e-01, avg batch time: 1.0078, average train loss: 6.6630
[11/23 21:17:30 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3068, average loss: 1.8048
[11/23 21:17:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.98	
[11/23 21:17:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/23 21:19:13 visual_prompt]: 	Training 100/553. train loss: 1.3331,	0.8198 s / batch. (data: 3.37e-04). ETA=10:10:40, max mem: 20.9 GB 
[11/23 21:20:54 visual_prompt]: 	Training 200/553. train loss: 3.7003,	0.8440 s / batch. (data: 3.28e-04). ETA=10:27:16, max mem: 20.9 GB 
[11/23 21:22:35 visual_prompt]: 	Training 300/553. train loss: 0.5766,	0.8472 s / batch. (data: 3.78e-04). ETA=10:28:14, max mem: 20.9 GB 
[11/23 21:24:15 visual_prompt]: 	Training 400/553. train loss: 2.2929,	0.8713 s / batch. (data: 1.12e-02). ETA=10:44:38, max mem: 20.9 GB 
[11/23 21:25:54 visual_prompt]: 	Training 500/553. train loss: 0.8760,	0.8397 s / batch. (data: 3.07e-04). ETA=10:19:51, max mem: 20.9 GB 
[11/23 21:26:48 visual_prompt]: Epoch 20 / 100: avg data time: 1.85e-01, avg batch time: 1.0104, average train loss: 7.9536
[11/23 21:27:46 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3073, average loss: 0.7894
[11/23 21:27:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.55	
[11/23 21:27:46 visual_prompt]: Best epoch 20: best metric: -0.789
[11/23 21:27:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/23 21:29:33 visual_prompt]: 	Training 100/553. train loss: 3.4122,	0.8198 s / batch. (data: 3.14e-04). ETA=10:03:06, max mem: 20.9 GB 
[11/23 21:31:13 visual_prompt]: 	Training 200/553. train loss: 15.1083,	0.8166 s / batch. (data: 3.02e-04). ETA=9:59:22, max mem: 20.9 GB 
[11/23 21:32:54 visual_prompt]: 	Training 300/553. train loss: 36.6907,	1.0639 s / batch. (data: 2.55e-01). ETA=12:59:05, max mem: 20.9 GB 
[11/23 21:34:32 visual_prompt]: 	Training 400/553. train loss: 22.1858,	0.8240 s / batch. (data: 3.18e-04). ETA=10:02:03, max mem: 20.9 GB 
[11/23 21:36:15 visual_prompt]: 	Training 500/553. train loss: 1.8073,	0.8262 s / batch. (data: 3.20e-04). ETA=10:02:17, max mem: 20.9 GB 
[11/23 21:37:07 visual_prompt]: Epoch 21 / 100: avg data time: 1.87e-01, avg batch time: 1.0129, average train loss: 6.9093
[11/23 21:38:04 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3079, average loss: 6.4880
[11/23 21:38:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.66	
[11/23 21:38:04 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/23 21:39:49 visual_prompt]: 	Training 100/553. train loss: 3.0801,	0.8089 s / batch. (data: 3.22e-04). ETA=9:47:37, max mem: 20.9 GB 
[11/23 21:41:30 visual_prompt]: 	Training 200/553. train loss: 7.0918,	0.8169 s / batch. (data: 5.44e-03). ETA=9:52:06, max mem: 20.9 GB 
[11/23 21:43:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8434 s / batch. (data: 2.33e-02). ETA=10:09:50, max mem: 20.9 GB 
[11/23 21:44:50 visual_prompt]: 	Training 400/553. train loss: 20.0441,	0.8370 s / batch. (data: 8.91e-03). ETA=10:03:49, max mem: 20.9 GB 
[11/23 21:46:31 visual_prompt]: 	Training 500/553. train loss: 2.1695,	0.8320 s / batch. (data: 7.95e-03). ETA=9:58:51, max mem: 20.9 GB 
[11/23 21:47:25 visual_prompt]: Epoch 22 / 100: avg data time: 1.86e-01, avg batch time: 1.0132, average train loss: 7.2443
[11/23 21:48:23 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3086, average loss: 6.5371
[11/23 21:48:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.52	
[11/23 21:48:23 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/23 21:50:10 visual_prompt]: 	Training 100/553. train loss: 0.7161,	0.8312 s / batch. (data: 5.64e-03). ETA=9:56:08, max mem: 20.9 GB 
[11/23 21:51:50 visual_prompt]: 	Training 200/553. train loss: 0.8525,	0.8786 s / batch. (data: 6.57e-02). ETA=10:28:42, max mem: 20.9 GB 
[11/23 21:53:33 visual_prompt]: 	Training 300/553. train loss: 1.7063,	0.8484 s / batch. (data: 8.38e-04). ETA=10:05:38, max mem: 20.9 GB 
[11/23 21:55:11 visual_prompt]: 	Training 400/553. train loss: 7.9508,	0.8356 s / batch. (data: 8.25e-04). ETA=9:55:08, max mem: 20.9 GB 
[11/23 21:56:51 visual_prompt]: 	Training 500/553. train loss: 3.5188,	0.8466 s / batch. (data: 1.59e-02). ETA=10:01:34, max mem: 20.9 GB 
[11/23 21:57:42 visual_prompt]: Epoch 23 / 100: avg data time: 1.86e-01, avg batch time: 1.0123, average train loss: 6.2641
[11/23 21:58:40 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.3092, average loss: 9.3899
[11/23 21:58:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.78	
[11/23 21:58:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/23 22:00:23 visual_prompt]: 	Training 100/553. train loss: 0.9012,	0.8360 s / batch. (data: 5.25e-04). ETA=9:51:53, max mem: 20.9 GB 
[11/23 22:02:03 visual_prompt]: 	Training 200/553. train loss: 1.0382,	0.8120 s / batch. (data: 3.09e-04). ETA=9:33:33, max mem: 20.9 GB 
[11/23 22:03:44 visual_prompt]: 	Training 300/553. train loss: 5.4717,	0.8633 s / batch. (data: 4.64e-02). ETA=10:08:20, max mem: 20.9 GB 
[11/23 22:05:26 visual_prompt]: 	Training 400/553. train loss: 4.3375,	0.8377 s / batch. (data: 1.20e-02). ETA=9:48:53, max mem: 20.9 GB 
[11/23 22:07:07 visual_prompt]: 	Training 500/553. train loss: 11.1186,	0.8573 s / batch. (data: 3.74e-02). ETA=10:01:15, max mem: 20.9 GB 
[11/23 22:08:01 visual_prompt]: Epoch 24 / 100: avg data time: 1.87e-01, avg batch time: 1.0131, average train loss: 7.0337
[11/23 22:08:58 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3070, average loss: 6.6399
[11/23 22:08:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.51	
[11/23 22:08:58 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/23 22:10:47 visual_prompt]: 	Training 100/553. train loss: 4.3921,	0.8268 s / batch. (data: 5.66e-03). ETA=9:37:45, max mem: 20.9 GB 
[11/23 22:12:24 visual_prompt]: 	Training 200/553. train loss: 1.0410,	0.8433 s / batch. (data: 2.33e-02). ETA=9:47:54, max mem: 20.9 GB 
[11/23 22:14:04 visual_prompt]: 	Training 300/553. train loss: 3.6681,	0.8103 s / batch. (data: 4.11e-04). ETA=9:23:33, max mem: 20.9 GB 
[11/23 22:15:45 visual_prompt]: 	Training 400/553. train loss: 0.9376,	1.3200 s / batch. (data: 5.01e-01). ETA=15:15:49, max mem: 20.9 GB 
[11/23 22:17:27 visual_prompt]: 	Training 500/553. train loss: 4.8600,	1.4640 s / batch. (data: 6.40e-01). ETA=16:53:17, max mem: 20.9 GB 
[11/23 22:18:19 visual_prompt]: Epoch 25 / 100: avg data time: 1.87e-01, avg batch time: 1.0134, average train loss: 4.7027
[11/23 22:19:17 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3066, average loss: 8.6686
[11/23 22:19:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.27	
[11/23 22:19:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/23 22:21:01 visual_prompt]: 	Training 100/553. train loss: 13.1341,	0.8085 s / batch. (data: 3.16e-04). ETA=9:17:30, max mem: 20.9 GB 
[11/23 22:22:43 visual_prompt]: 	Training 200/553. train loss: 3.6936,	1.8384 s / batch. (data: 1.02e+00). ETA=21:04:40, max mem: 20.9 GB 
[11/23 22:24:26 visual_prompt]: 	Training 300/553. train loss: 5.4963,	0.8277 s / batch. (data: 3.30e-04). ETA=9:28:00, max mem: 20.9 GB 
[11/23 22:26:05 visual_prompt]: 	Training 400/553. train loss: 2.1597,	0.8356 s / batch. (data: 5.45e-03). ETA=9:32:02, max mem: 20.9 GB 
[11/23 22:27:44 visual_prompt]: 	Training 500/553. train loss: 12.5324,	0.8104 s / batch. (data: 3.12e-04). ETA=9:13:25, max mem: 20.9 GB 
[11/23 22:28:36 visual_prompt]: Epoch 26 / 100: avg data time: 1.85e-01, avg batch time: 1.0109, average train loss: 4.8536
[11/23 22:29:34 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3082, average loss: 8.5867
[11/23 22:29:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.31	
[11/23 22:29:34 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/23 22:31:20 visual_prompt]: 	Training 100/553. train loss: 1.0692,	0.8346 s / batch. (data: 4.39e-04). ETA=9:27:51, max mem: 20.9 GB 
[11/23 22:33:00 visual_prompt]: 	Training 200/553. train loss: 10.7444,	1.4639 s / batch. (data: 6.53e-01). ETA=16:33:33, max mem: 20.9 GB 
[11/23 22:34:41 visual_prompt]: 	Training 300/553. train loss: 7.6850,	0.8320 s / batch. (data: 3.41e-04). ETA=9:23:16, max mem: 20.9 GB 
[11/23 22:36:23 visual_prompt]: 	Training 400/553. train loss: 2.4567,	0.8109 s / batch. (data: 3.39e-04). ETA=9:07:39, max mem: 20.9 GB 
[11/23 22:38:04 visual_prompt]: 	Training 500/553. train loss: 0.8005,	0.8280 s / batch. (data: 3.20e-04). ETA=9:17:48, max mem: 20.9 GB 
[11/23 22:38:54 visual_prompt]: Epoch 27 / 100: avg data time: 1.87e-01, avg batch time: 1.0137, average train loss: 5.8253
[11/23 22:39:52 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.3060, average loss: 11.6900
[11/23 22:39:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.86	
[11/23 22:39:52 visual_prompt]: Stopping early.
[11/23 22:39:52 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 22:39:52 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 22:39:52 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/23 22:39:52 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 22:39:52 visual_prompt]: Training with config:
[11/23 22:39:52 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 22:39:52 visual_prompt]: Loading training data...
[11/23 22:39:52 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 22:39:52 visual_prompt]: Loading validation data...
[11/23 22:39:52 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 22:39:52 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 22:39:58 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 22:39:58 visual_prompt]: tuned percent:0.525
[11/23 22:39:58 visual_prompt]: Device used for model: 0
[11/23 22:39:58 visual_prompt]: Setting up Evaluator...
[11/23 22:39:58 visual_prompt]: Setting up Trainer...
[11/23 22:39:58 visual_prompt]: 	Setting up the optimizer...
[11/23 22:39:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 22:41:42 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8487 s / batch. (data: 1.05e-02). ETA=13:00:48, max mem: 20.9 GB 
[11/23 22:43:22 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8400 s / batch. (data: 3.14e-04). ETA=12:51:23, max mem: 20.9 GB 
[11/23 22:45:05 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3341 s / batch. (data: 5.01e-01). ETA=20:22:55, max mem: 20.9 GB 
[11/23 22:46:44 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8171 s / batch. (data: 5.45e-03). ETA=12:27:36, max mem: 20.9 GB 
[11/23 22:48:27 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8400 s / batch. (data: 3.21e-04). ETA=12:47:11, max mem: 20.9 GB 
[11/23 22:49:20 visual_prompt]: Epoch 1 / 100: avg data time: 1.90e-01, avg batch time: 1.0169, average train loss: 1.5403
[11/23 22:50:18 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3071, average loss: 1.5201
[11/23 22:50:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 22:50:18 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/23 22:52:03 visual_prompt]: 	Training 100/553. train loss: 1.2142,	0.8400 s / batch. (data: 5.47e-03). ETA=12:45:03, max mem: 20.9 GB 
[11/23 22:53:43 visual_prompt]: 	Training 200/553. train loss: 0.0920,	0.8240 s / batch. (data: 3.25e-04). ETA=12:29:05, max mem: 20.9 GB 
[11/23 22:55:26 visual_prompt]: 	Training 300/553. train loss: 1.6199,	1.2768 s / batch. (data: 4.50e-01). ETA=19:18:39, max mem: 20.9 GB 
[11/23 22:57:04 visual_prompt]: 	Training 400/553. train loss: 1.0970,	0.8416 s / batch. (data: 3.19e-04). ETA=12:42:19, max mem: 20.9 GB 
[11/23 22:58:47 visual_prompt]: 	Training 500/553. train loss: 0.5494,	0.8365 s / batch. (data: 1.05e-02). ETA=12:36:19, max mem: 20.9 GB 
[11/23 22:59:38 visual_prompt]: Epoch 2 / 100: avg data time: 1.86e-01, avg batch time: 1.0125, average train loss: 1.3718
[11/23 23:00:36 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3076, average loss: 4.0659
[11/23 23:00:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.87	
[11/23 23:00:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/23 23:02:20 visual_prompt]: 	Training 100/553. train loss: 0.9441,	0.8240 s / batch. (data: 3.11e-04). ETA=12:22:53, max mem: 20.9 GB 
[11/23 23:04:02 visual_prompt]: 	Training 200/553. train loss: 0.7023,	0.8320 s / batch. (data: 1.20e-02). ETA=12:28:41, max mem: 20.9 GB 
[11/23 23:05:41 visual_prompt]: 	Training 300/553. train loss: 1.4231,	0.8496 s / batch. (data: 3.15e-04). ETA=12:43:07, max mem: 20.9 GB 
[11/23 23:07:23 visual_prompt]: 	Training 400/553. train loss: 4.3751,	0.8120 s / batch. (data: 3.07e-04). ETA=12:08:01, max mem: 20.9 GB 
[11/23 23:09:05 visual_prompt]: 	Training 500/553. train loss: 0.7843,	1.2561 s / batch. (data: 4.28e-01). ETA=18:44:03, max mem: 20.9 GB 
[11/23 23:09:56 visual_prompt]: Epoch 3 / 100: avg data time: 1.87e-01, avg batch time: 1.0129, average train loss: 1.6100
[11/23 23:10:54 visual_prompt]: Inference (val):avg data time: 5.01e-05, avg batch time: 0.3069, average loss: 1.9609
[11/23 23:10:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.84	
[11/23 23:10:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/23 23:12:41 visual_prompt]: 	Training 100/553. train loss: 2.2299,	0.8200 s / batch. (data: 3.26e-04). ETA=12:11:43, max mem: 20.9 GB 
[11/23 23:14:22 visual_prompt]: 	Training 200/553. train loss: 1.0641,	0.8440 s / batch. (data: 5.46e-03). ETA=12:31:43, max mem: 20.9 GB 
[11/23 23:16:03 visual_prompt]: 	Training 300/553. train loss: 0.8041,	1.3084 s / batch. (data: 4.98e-01). ETA=19:23:10, max mem: 20.9 GB 
[11/23 23:17:39 visual_prompt]: 	Training 400/553. train loss: 2.0559,	1.3339 s / batch. (data: 5.13e-01). ETA=19:43:39, max mem: 20.9 GB 
[11/23 23:19:22 visual_prompt]: 	Training 500/553. train loss: 0.0003,	3.5280 s / batch. (data: 2.70e+00). ETA=2 days, 4:04:41, max mem: 20.9 GB 
[11/23 23:20:16 visual_prompt]: Epoch 4 / 100: avg data time: 1.89e-01, avg batch time: 1.0149, average train loss: 2.5073
[11/23 23:21:14 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3086, average loss: 1.3046
[11/23 23:21:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.22	
[11/23 23:21:14 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/23 23:22:56 visual_prompt]: 	Training 100/553. train loss: 0.0006,	0.8447 s / batch. (data: 5.46e-03). ETA=12:25:59, max mem: 20.9 GB 
[11/23 23:24:37 visual_prompt]: 	Training 200/553. train loss: 3.6976,	0.8512 s / batch. (data: 1.56e-02). ETA=12:30:20, max mem: 20.9 GB 
[11/23 23:26:21 visual_prompt]: 	Training 300/553. train loss: 8.8004,	0.8138 s / batch. (data: 3.16e-04). ETA=11:55:59, max mem: 20.9 GB 
[11/23 23:28:04 visual_prompt]: 	Training 400/553. train loss: 0.6081,	0.8334 s / batch. (data: 1.05e-02). ETA=12:11:48, max mem: 20.9 GB 
[11/23 23:29:47 visual_prompt]: 	Training 500/553. train loss: 1.3376,	0.8216 s / batch. (data: 1.05e-02). ETA=12:00:06, max mem: 20.9 GB 
[11/23 23:30:42 visual_prompt]: Epoch 5 / 100: avg data time: 2.01e-01, avg batch time: 1.0265, average train loss: 2.8567
[11/23 23:31:41 visual_prompt]: Inference (val):avg data time: 6.02e-04, avg batch time: 0.3075, average loss: 5.3650
[11/23 23:31:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.52	
[11/23 23:31:41 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/23 23:33:29 visual_prompt]: 	Training 100/553. train loss: 1.1663,	0.8520 s / batch. (data: 3.18e-04). ETA=12:24:33, max mem: 20.9 GB 
[11/23 23:35:12 visual_prompt]: 	Training 200/553. train loss: 3.3154,	0.8480 s / batch. (data: 7.97e-03). ETA=12:19:39, max mem: 20.9 GB 
[11/23 23:36:53 visual_prompt]: 	Training 300/553. train loss: 0.7684,	0.8320 s / batch. (data: 3.11e-04). ETA=12:04:19, max mem: 20.9 GB 
[11/23 23:38:40 visual_prompt]: 	Training 400/553. train loss: 1.2629,	0.8268 s / batch. (data: 3.43e-04). ETA=11:58:27, max mem: 20.9 GB 
[11/23 23:40:21 visual_prompt]: 	Training 500/553. train loss: 5.1409,	0.9439 s / batch. (data: 9.71e-02). ETA=13:38:35, max mem: 20.9 GB 
[11/23 23:41:14 visual_prompt]: Epoch 6 / 100: avg data time: 2.09e-01, avg batch time: 1.0358, average train loss: 2.0619
[11/23 23:42:13 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3070, average loss: 0.9227
[11/23 23:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.91	
[11/23 23:42:13 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/23 23:43:58 visual_prompt]: 	Training 100/553. train loss: 4.5842,	0.8578 s / batch. (data: 1.05e-02). ETA=12:21:44, max mem: 20.9 GB 
[11/23 23:45:40 visual_prompt]: 	Training 200/553. train loss: 0.7425,	0.8480 s / batch. (data: 3.14e-04). ETA=12:11:51, max mem: 20.9 GB 
[11/23 23:47:26 visual_prompt]: 	Training 300/553. train loss: 1.6967,	2.0240 s / batch. (data: 1.21e+00). ETA=1 day, 5:03:23, max mem: 20.9 GB 
[11/23 23:49:08 visual_prompt]: 	Training 400/553. train loss: 0.6390,	1.9014 s / batch. (data: 1.06e+00). ETA=1 day, 3:14:38, max mem: 20.9 GB 
[11/23 23:50:49 visual_prompt]: 	Training 500/553. train loss: 0.6944,	0.8263 s / batch. (data: 3.23e-04). ETA=11:48:58, max mem: 20.9 GB 
[11/23 23:51:40 visual_prompt]: Epoch 7 / 100: avg data time: 2.00e-01, avg batch time: 1.0263, average train loss: 2.1026
[11/23 23:52:39 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3071, average loss: 1.9892
[11/23 23:52:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.76	
[11/23 23:52:39 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/23 23:54:24 visual_prompt]: 	Training 100/553. train loss: 8.7400,	0.8191 s / batch. (data: 3.21e-04). ETA=11:40:45, max mem: 20.9 GB 
[11/23 23:56:07 visual_prompt]: 	Training 200/553. train loss: 11.3453,	0.8129 s / batch. (data: 3.26e-04). ETA=11:34:05, max mem: 20.9 GB 
[11/23 23:57:50 visual_prompt]: 	Training 300/553. train loss: 2.3540,	0.8320 s / batch. (data: 3.14e-04). ETA=11:48:59, max mem: 20.9 GB 
[11/23 23:59:32 visual_prompt]: 	Training 400/553. train loss: 3.7570,	0.8838 s / batch. (data: 6.93e-02). ETA=12:31:39, max mem: 20.9 GB 
[11/24 00:01:14 visual_prompt]: 	Training 500/553. train loss: 6.8638,	1.5364 s / batch. (data: 7.26e-01). ETA=21:44:08, max mem: 20.9 GB 
[11/24 00:02:07 visual_prompt]: Epoch 8 / 100: avg data time: 2.00e-01, avg batch time: 1.0269, average train loss: 5.9094
[11/24 00:03:06 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3060, average loss: 3.1904
[11/24 00:03:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.02	
[11/24 00:03:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/24 00:04:52 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8155 s / batch. (data: 3.12e-04). ETA=11:30:07, max mem: 20.9 GB 
[11/24 00:06:32 visual_prompt]: 	Training 200/553. train loss: 0.5124,	0.8338 s / batch. (data: 3.03e-04). ETA=11:44:14, max mem: 20.9 GB 
[11/24 00:08:15 visual_prompt]: 	Training 300/553. train loss: 2.3567,	1.9080 s / batch. (data: 1.08e+00). ETA=1 day, 2:48:19, max mem: 20.9 GB 
[11/24 00:09:58 visual_prompt]: 	Training 400/553. train loss: 2.7290,	0.8400 s / batch. (data: 1.20e-02). ETA=11:46:39, max mem: 20.9 GB 
[11/24 00:11:40 visual_prompt]: 	Training 500/553. train loss: 0.4908,	0.8293 s / batch. (data: 3.24e-04). ETA=11:36:19, max mem: 20.9 GB 
[11/24 00:12:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.98e-01, avg batch time: 1.0243, average train loss: 2.7112
[11/24 00:13:31 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.3065, average loss: 0.8254
[11/24 00:13:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.13	
[11/24 00:13:31 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/24 00:15:20 visual_prompt]: 	Training 100/553. train loss: 11.5821,	0.8097 s / batch. (data: 3.25e-04). ETA=11:17:47, max mem: 20.9 GB 
[11/24 00:17:01 visual_prompt]: 	Training 200/553. train loss: 0.5792,	0.8390 s / batch. (data: 1.20e-02). ETA=11:40:54, max mem: 20.9 GB 
[11/24 00:18:41 visual_prompt]: 	Training 300/553. train loss: 1.7898,	1.0040 s / batch. (data: 1.67e-01). ETA=13:57:03, max mem: 20.9 GB 
[11/24 00:20:22 visual_prompt]: 	Training 400/553. train loss: 4.9888,	0.8182 s / batch. (data: 7.96e-03). ETA=11:20:49, max mem: 20.9 GB 
[11/24 00:22:05 visual_prompt]: 	Training 500/553. train loss: 1.6759,	1.0963 s / batch. (data: 2.70e-01). ETA=15:10:21, max mem: 20.9 GB 
[11/24 00:22:59 visual_prompt]: Epoch 10 / 100: avg data time: 2.01e-01, avg batch time: 1.0262, average train loss: 4.3058
[11/24 00:23:58 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.3074, average loss: 1.0591
[11/24 00:23:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.98	
[11/24 00:23:58 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/24 00:25:46 visual_prompt]: 	Training 100/553. train loss: 5.5329,	0.8280 s / batch. (data: 3.01e-04). ETA=11:25:26, max mem: 20.9 GB 
[11/24 00:27:30 visual_prompt]: 	Training 200/553. train loss: 8.8807,	0.8464 s / batch. (data: 2.76e-02). ETA=11:39:17, max mem: 20.9 GB 
[11/24 00:29:12 visual_prompt]: 	Training 300/553. train loss: 0.0029,	2.2228 s / batch. (data: 1.41e+00). ETA=1 day, 6:32:42, max mem: 20.9 GB 
[11/24 00:30:51 visual_prompt]: 	Training 400/553. train loss: 2.8477,	0.8360 s / batch. (data: 8.64e-04). ETA=11:27:53, max mem: 20.9 GB 
[11/24 00:32:32 visual_prompt]: 	Training 500/553. train loss: 2.4086,	0.8160 s / batch. (data: 3.02e-04). ETA=11:10:03, max mem: 20.9 GB 
[11/24 00:33:24 visual_prompt]: Epoch 11 / 100: avg data time: 1.98e-01, avg batch time: 1.0249, average train loss: 2.4816
[11/24 00:34:23 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3080, average loss: 4.5119
[11/24 00:34:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.09	
[11/24 00:34:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/24 00:36:10 visual_prompt]: 	Training 100/553. train loss: 1.4814,	0.8376 s / batch. (data: 3.53e-04). ETA=11:25:42, max mem: 20.9 GB 
[11/24 00:37:54 visual_prompt]: 	Training 200/553. train loss: 1.5926,	0.8200 s / batch. (data: 5.43e-03). ETA=11:09:53, max mem: 20.9 GB 
[11/24 00:39:35 visual_prompt]: 	Training 300/553. train loss: 4.6174,	0.8212 s / batch. (data: 7.96e-03). ETA=11:09:31, max mem: 20.9 GB 
[11/24 00:41:16 visual_prompt]: 	Training 400/553. train loss: 8.3687,	0.8286 s / batch. (data: 3.53e-04). ETA=11:14:09, max mem: 20.9 GB 
[11/24 00:42:59 visual_prompt]: 	Training 500/553. train loss: 23.9205,	0.8390 s / batch. (data: 8.68e-04). ETA=11:21:14, max mem: 20.9 GB 
[11/24 00:43:51 visual_prompt]: Epoch 12 / 100: avg data time: 2.01e-01, avg batch time: 1.0265, average train loss: 7.8640
[11/24 00:44:49 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3077, average loss: 0.8345
[11/24 00:44:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[11/24 00:44:49 visual_prompt]: Best epoch 12: best metric: -0.834
[11/24 00:44:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/24 00:46:37 visual_prompt]: 	Training 100/553. train loss: 4.5378,	0.8200 s / batch. (data: 5.45e-03). ETA=11:03:42, max mem: 20.9 GB 
[11/24 00:48:15 visual_prompt]: 	Training 200/553. train loss: 6.0635,	0.8245 s / batch. (data: 4.57e-04). ETA=11:05:58, max mem: 20.9 GB 
[11/24 00:49:59 visual_prompt]: 	Training 300/553. train loss: 3.3955,	1.9520 s / batch. (data: 1.11e+00). ETA=1 day, 2:13:26, max mem: 20.9 GB 
[11/24 00:51:39 visual_prompt]: 	Training 400/553. train loss: 10.6050,	0.8328 s / batch. (data: 3.45e-04). ETA=11:09:54, max mem: 20.9 GB 
[11/24 00:53:21 visual_prompt]: 	Training 500/553. train loss: 9.7557,	0.8200 s / batch. (data: 3.06e-04). ETA=10:58:16, max mem: 20.9 GB 
[11/24 00:54:14 visual_prompt]: Epoch 13 / 100: avg data time: 1.95e-01, avg batch time: 1.0211, average train loss: 6.9726
[11/24 00:55:13 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3064, average loss: 1.6762
[11/24 00:55:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.13	
[11/24 00:55:13 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/24 00:57:00 visual_prompt]: 	Training 100/553. train loss: 21.1755,	0.8155 s / batch. (data: 5.44e-03). ETA=10:52:33, max mem: 20.9 GB 
[11/24 00:58:42 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2524 s / batch. (data: 4.42e-01). ETA=16:40:03, max mem: 20.9 GB 
[11/24 01:00:23 visual_prompt]: 	Training 300/553. train loss: 2.2619,	0.8345 s / batch. (data: 1.20e-02). ETA=11:04:58, max mem: 20.9 GB 
[11/24 01:02:04 visual_prompt]: 	Training 400/553. train loss: 2.6300,	1.4040 s / batch. (data: 5.72e-01). ETA=18:36:27, max mem: 20.9 GB 
[11/24 01:03:46 visual_prompt]: 	Training 500/553. train loss: 4.8886,	0.8209 s / batch. (data: 1.05e-02). ETA=10:51:25, max mem: 20.9 GB 
[11/24 01:04:37 visual_prompt]: Epoch 14 / 100: avg data time: 1.97e-01, avg batch time: 1.0211, average train loss: 5.9034
[11/24 01:05:36 visual_prompt]: Inference (val):avg data time: 3.79e-04, avg batch time: 0.3062, average loss: 5.0850
[11/24 01:05:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.40	
[11/24 01:05:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/24 01:07:23 visual_prompt]: 	Training 100/553. train loss: 6.5107,	0.8320 s / batch. (data: 3.54e-04). ETA=10:58:04, max mem: 20.9 GB 
[11/24 01:09:04 visual_prompt]: 	Training 200/553. train loss: 1.9310,	0.8400 s / batch. (data: 1.19e-02). ETA=11:02:59, max mem: 20.9 GB 
[11/24 01:10:49 visual_prompt]: 	Training 300/553. train loss: 13.6328,	0.8120 s / batch. (data: 3.13e-04). ETA=10:39:34, max mem: 20.9 GB 
[11/24 01:12:29 visual_prompt]: 	Training 400/553. train loss: 5.5660,	0.8409 s / batch. (data: 1.56e-02). ETA=11:00:54, max mem: 20.9 GB 
[11/24 01:14:13 visual_prompt]: 	Training 500/553. train loss: 13.2362,	0.8360 s / batch. (data: 3.39e-04). ETA=10:55:41, max mem: 20.9 GB 
[11/24 01:15:06 visual_prompt]: Epoch 15 / 100: avg data time: 2.04e-01, avg batch time: 1.0303, average train loss: 8.9296
[11/24 01:16:05 visual_prompt]: Inference (val):avg data time: 4.77e-05, avg batch time: 0.3079, average loss: 0.7932
[11/24 01:16:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.85	
[11/24 01:16:05 visual_prompt]: Best epoch 15: best metric: -0.793
[11/24 01:16:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/24 01:17:52 visual_prompt]: 	Training 100/553. train loss: 6.6863,	0.8240 s / batch. (data: 3.21e-04). ETA=10:44:09, max mem: 20.9 GB 
[11/24 01:19:34 visual_prompt]: 	Training 200/553. train loss: 0.8400,	0.8361 s / batch. (data: 7.97e-03). ETA=10:52:12, max mem: 20.9 GB 
[11/24 01:21:18 visual_prompt]: 	Training 300/553. train loss: 1.1524,	0.8240 s / batch. (data: 3.06e-04). ETA=10:41:25, max mem: 20.9 GB 
[11/24 01:23:00 visual_prompt]: 	Training 400/553. train loss: 5.8702,	0.8281 s / batch. (data: 1.20e-02). ETA=10:43:13, max mem: 20.9 GB 
[11/24 01:24:42 visual_prompt]: 	Training 500/553. train loss: 10.1401,	1.1520 s / batch. (data: 3.11e-01). ETA=14:52:54, max mem: 20.9 GB 
[11/24 01:25:36 visual_prompt]: Epoch 16 / 100: avg data time: 2.06e-01, avg batch time: 1.0317, average train loss: 4.0607
[11/24 01:26:35 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.3090, average loss: 1.6048
[11/24 01:26:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.91	
[11/24 01:26:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/24 01:28:22 visual_prompt]: 	Training 100/553. train loss: 7.7257,	0.8313 s / batch. (data: 3.50e-04). ETA=10:42:12, max mem: 20.9 GB 
[11/24 01:30:05 visual_prompt]: 	Training 200/553. train loss: 2.0183,	0.8127 s / batch. (data: 3.14e-04). ETA=10:26:27, max mem: 20.9 GB 
[11/24 01:31:47 visual_prompt]: 	Training 300/553. train loss: 6.1852,	0.8301 s / batch. (data: 4.37e-04). ETA=10:38:29, max mem: 20.9 GB 
[11/24 01:33:29 visual_prompt]: 	Training 400/553. train loss: 0.6028,	1.2320 s / batch. (data: 4.22e-01). ETA=15:45:35, max mem: 20.9 GB 
[11/24 01:35:12 visual_prompt]: 	Training 500/553. train loss: 0.8317,	1.7880 s / batch. (data: 9.47e-01). ETA=22:49:22, max mem: 20.9 GB 
[11/24 01:36:06 visual_prompt]: Epoch 17 / 100: avg data time: 2.06e-01, avg batch time: 1.0321, average train loss: 5.6851
[11/24 01:37:05 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3089, average loss: 3.2075
[11/24 01:37:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.88	
[11/24 01:37:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/24 01:38:53 visual_prompt]: 	Training 100/553. train loss: 8.7789,	0.8235 s / batch. (data: 3.52e-04). ETA=10:28:36, max mem: 20.9 GB 
[11/24 01:40:37 visual_prompt]: 	Training 200/553. train loss: 6.0809,	0.8160 s / batch. (data: 3.19e-04). ETA=10:21:32, max mem: 20.9 GB 
[11/24 01:42:20 visual_prompt]: 	Training 300/553. train loss: 2.3445,	0.8403 s / batch. (data: 2.03e-02). ETA=10:38:38, max mem: 20.9 GB 
[11/24 01:44:02 visual_prompt]: 	Training 400/553. train loss: 4.4757,	0.8403 s / batch. (data: 2.44e-02). ETA=10:37:11, max mem: 20.9 GB 
[11/24 01:45:44 visual_prompt]: 	Training 500/553. train loss: 1.6489,	0.8400 s / batch. (data: 2.45e-03). ETA=10:35:34, max mem: 20.9 GB 
[11/24 01:46:36 visual_prompt]: Epoch 18 / 100: avg data time: 2.06e-01, avg batch time: 1.0325, average train loss: 4.2608
[11/24 01:47:35 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3067, average loss: 2.0046
[11/24 01:47:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.59	
[11/24 01:47:35 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/24 01:49:22 visual_prompt]: 	Training 100/553. train loss: 1.2388,	0.8629 s / batch. (data: 4.25e-02). ETA=10:50:43, max mem: 20.9 GB 
[11/24 01:51:05 visual_prompt]: 	Training 200/553. train loss: 2.4127,	0.8360 s / batch. (data: 3.53e-04). ETA=10:29:01, max mem: 20.9 GB 
[11/24 01:52:48 visual_prompt]: 	Training 300/553. train loss: 8.9535,	0.8268 s / batch. (data: 7.97e-03). ETA=10:20:42, max mem: 20.9 GB 
[11/24 01:54:32 visual_prompt]: 	Training 400/553. train loss: 2.3611,	0.8241 s / batch. (data: 8.12e-04). ETA=10:17:20, max mem: 20.9 GB 
[11/24 01:56:10 visual_prompt]: 	Training 500/553. train loss: 0.6586,	0.8400 s / batch. (data: 1.20e-02). ETA=10:27:50, max mem: 20.9 GB 
[11/24 01:57:04 visual_prompt]: Epoch 19 / 100: avg data time: 2.01e-01, avg batch time: 1.0274, average train loss: 2.8640
[11/24 01:58:03 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.3081, average loss: 8.3813
[11/24 01:58:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.01	
[11/24 01:58:03 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/24 01:59:48 visual_prompt]: 	Training 100/553. train loss: 0.6519,	0.8314 s / batch. (data: 3.42e-04). ETA=10:19:17, max mem: 20.9 GB 
[11/24 02:01:32 visual_prompt]: 	Training 200/553. train loss: 1.2496,	0.8217 s / batch. (data: 3.37e-04). ETA=10:10:40, max mem: 20.9 GB 
[11/24 02:03:15 visual_prompt]: 	Training 300/553. train loss: 0.8050,	0.8265 s / batch. (data: 1.15e-03). ETA=10:12:54, max mem: 20.9 GB 
[11/24 02:04:57 visual_prompt]: 	Training 400/553. train loss: 1.9979,	0.8354 s / batch. (data: 5.65e-03). ETA=10:18:04, max mem: 20.9 GB 
[11/24 02:06:38 visual_prompt]: 	Training 500/553. train loss: 1.2681,	0.8400 s / batch. (data: 3.79e-04). ETA=10:20:06, max mem: 20.9 GB 
[11/24 02:07:33 visual_prompt]: Epoch 20 / 100: avg data time: 2.06e-01, avg batch time: 1.0319, average train loss: 5.0237
[11/24 02:08:32 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3068, average loss: 3.8102
[11/24 02:08:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.32	
[11/24 02:08:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/24 02:10:22 visual_prompt]: 	Training 100/553. train loss: 7.4528,	0.8213 s / batch. (data: 3.69e-04). ETA=10:04:11, max mem: 20.9 GB 
[11/24 02:12:04 visual_prompt]: 	Training 200/553. train loss: 5.4756,	0.8268 s / batch. (data: 3.24e-04). ETA=10:06:50, max mem: 20.9 GB 
[11/24 02:13:46 visual_prompt]: 	Training 300/553. train loss: 21.7955,	1.1910 s / batch. (data: 3.74e-01). ETA=14:32:14, max mem: 20.9 GB 
[11/24 02:15:28 visual_prompt]: 	Training 400/553. train loss: 6.5294,	0.8365 s / batch. (data: 3.22e-04). ETA=10:11:12, max mem: 20.9 GB 
[11/24 02:17:12 visual_prompt]: 	Training 500/553. train loss: 5.4555,	0.8295 s / batch. (data: 3.48e-04). ETA=10:04:43, max mem: 20.9 GB 
[11/24 02:18:04 visual_prompt]: Epoch 21 / 100: avg data time: 2.08e-01, avg batch time: 1.0340, average train loss: 5.0771
[11/24 02:19:03 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3050, average loss: 0.6898
[11/24 02:19:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.28	
[11/24 02:19:03 visual_prompt]: Best epoch 21: best metric: -0.690
[11/24 02:19:03 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/24 02:20:49 visual_prompt]: 	Training 100/553. train loss: 1.6547,	0.8384 s / batch. (data: 1.06e-02). ETA=10:09:05, max mem: 20.9 GB 
[11/24 02:22:32 visual_prompt]: 	Training 200/553. train loss: 4.8469,	0.8400 s / batch. (data: 3.18e-04). ETA=10:08:48, max mem: 20.9 GB 
[11/24 02:24:12 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8315 s / batch. (data: 7.96e-03). ETA=10:01:16, max mem: 20.9 GB 
[11/24 02:25:55 visual_prompt]: 	Training 400/553. train loss: 21.3525,	0.8287 s / batch. (data: 3.19e-04). ETA=9:57:53, max mem: 20.9 GB 
[11/24 02:27:38 visual_prompt]: 	Training 500/553. train loss: 3.4759,	0.8491 s / batch. (data: 9.05e-03). ETA=10:11:09, max mem: 20.9 GB 
[11/24 02:28:34 visual_prompt]: Epoch 22 / 100: avg data time: 2.05e-01, avg batch time: 1.0309, average train loss: 6.3588
[11/24 02:29:33 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.3074, average loss: 1.1448
[11/24 02:29:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.40	
[11/24 02:29:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/24 02:31:20 visual_prompt]: 	Training 100/553. train loss: 1.8340,	0.8218 s / batch. (data: 5.28e-03). ETA=9:49:25, max mem: 20.9 GB 
[11/24 02:33:04 visual_prompt]: 	Training 200/553. train loss: 1.3014,	0.8280 s / batch. (data: 3.33e-04). ETA=9:52:29, max mem: 20.9 GB 
[11/24 02:34:48 visual_prompt]: 	Training 300/553. train loss: 1.6809,	0.8484 s / batch. (data: 8.29e-04). ETA=10:05:39, max mem: 20.9 GB 
[11/24 02:36:28 visual_prompt]: 	Training 400/553. train loss: 2.2285,	0.8100 s / batch. (data: 4.21e-04). ETA=9:36:54, max mem: 20.9 GB 
[11/24 02:38:08 visual_prompt]: 	Training 500/553. train loss: 0.0010,	0.8520 s / batch. (data: 1.20e-02). ETA=10:05:25, max mem: 20.9 GB 
[11/24 02:39:02 visual_prompt]: Epoch 23 / 100: avg data time: 2.04e-01, avg batch time: 1.0296, average train loss: 3.0160
[11/24 02:40:01 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.3068, average loss: 1.4121
[11/24 02:40:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.34	
[11/24 02:40:01 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/24 02:41:46 visual_prompt]: 	Training 100/553. train loss: 5.5223,	0.8440 s / batch. (data: 5.48e-03). ETA=9:57:33, max mem: 20.9 GB 
[11/24 02:43:28 visual_prompt]: 	Training 200/553. train loss: 0.9238,	0.8190 s / batch. (data: 3.43e-04). ETA=9:38:32, max mem: 20.9 GB 
[11/24 02:45:11 visual_prompt]: 	Training 300/553. train loss: 2.2984,	1.0078 s / batch. (data: 1.85e-01). ETA=11:50:12, max mem: 20.9 GB 
[11/24 02:46:54 visual_prompt]: 	Training 400/553. train loss: 2.7778,	0.8323 s / batch. (data: 1.06e-02). ETA=9:45:06, max mem: 20.9 GB 
[11/24 02:48:38 visual_prompt]: 	Training 500/553. train loss: 8.6358,	0.8169 s / batch. (data: 5.70e-03). ETA=9:32:54, max mem: 20.9 GB 
[11/24 02:49:33 visual_prompt]: Epoch 24 / 100: avg data time: 2.09e-01, avg batch time: 1.0338, average train loss: 4.2892
[11/24 02:50:32 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.3083, average loss: 15.8930
[11/24 02:50:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.24	
[11/24 02:50:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/24 02:52:23 visual_prompt]: 	Training 100/553. train loss: 16.2428,	0.8224 s / batch. (data: 3.21e-04). ETA=9:34:41, max mem: 20.9 GB 
[11/24 02:54:02 visual_prompt]: 	Training 200/553. train loss: 2.1483,	0.8483 s / batch. (data: 5.47e-03). ETA=9:51:23, max mem: 20.9 GB 
[11/24 02:55:45 visual_prompt]: 	Training 300/553. train loss: 3.0458,	0.8474 s / batch. (data: 1.14e-02). ETA=9:49:22, max mem: 20.9 GB 
[11/24 02:57:28 visual_prompt]: 	Training 400/553. train loss: 3.4441,	1.3522 s / batch. (data: 5.35e-01). ETA=15:38:09, max mem: 20.9 GB 
[11/24 02:59:10 visual_prompt]: 	Training 500/553. train loss: 6.8680,	1.5467 s / batch. (data: 7.11e-01). ETA=17:50:30, max mem: 20.9 GB 
[11/24 03:00:04 visual_prompt]: Epoch 25 / 100: avg data time: 2.08e-01, avg batch time: 1.0331, average train loss: 6.1061
[11/24 03:01:03 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3075, average loss: 9.2456
[11/24 03:01:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.45	
[11/24 03:01:03 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/24 03:02:50 visual_prompt]: 	Training 100/553. train loss: 10.9275,	0.8234 s / batch. (data: 3.13e-04). ETA=9:27:50, max mem: 20.9 GB 
[11/24 03:04:34 visual_prompt]: 	Training 200/553. train loss: 27.0044,	2.0240 s / batch. (data: 1.18e+00). ETA=23:12:20, max mem: 20.9 GB 
[11/24 03:06:18 visual_prompt]: 	Training 300/553. train loss: 0.0091,	0.8412 s / batch. (data: 1.31e-02). ETA=9:37:14, max mem: 20.9 GB 
[11/24 03:08:00 visual_prompt]: 	Training 400/553. train loss: 0.7513,	0.8317 s / batch. (data: 3.22e-04). ETA=9:29:20, max mem: 20.9 GB 
[11/24 03:09:40 visual_prompt]: 	Training 500/553. train loss: 1.5391,	0.8520 s / batch. (data: 4.02e-04). ETA=9:41:51, max mem: 20.9 GB 
[11/24 03:10:34 visual_prompt]: Epoch 26 / 100: avg data time: 2.06e-01, avg batch time: 1.0318, average train loss: 3.9692
[11/24 03:11:33 visual_prompt]: Inference (val):avg data time: 1.60e-04, avg batch time: 0.3085, average loss: 1.9148
[11/24 03:11:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.57	
[11/24 03:11:33 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/24 03:13:21 visual_prompt]: 	Training 100/553. train loss: 7.9751,	0.8440 s / batch. (data: 3.26e-04). ETA=9:34:13, max mem: 20.9 GB 
[11/24 03:15:04 visual_prompt]: 	Training 200/553. train loss: 4.2559,	1.7040 s / batch. (data: 8.60e-01). ETA=19:16:29, max mem: 20.9 GB 
[11/24 03:16:46 visual_prompt]: 	Training 300/553. train loss: 3.7082,	0.8385 s / batch. (data: 3.45e-04). ETA=9:27:42, max mem: 20.9 GB 
[11/24 03:18:30 visual_prompt]: 	Training 400/553. train loss: 12.4739,	0.8344 s / batch. (data: 8.48e-04). ETA=9:23:32, max mem: 20.9 GB 
[11/24 03:20:14 visual_prompt]: 	Training 500/553. train loss: 2.9141,	0.8244 s / batch. (data: 3.37e-04). ETA=9:15:22, max mem: 20.9 GB 
[11/24 03:21:05 visual_prompt]: Epoch 27 / 100: avg data time: 2.08e-01, avg batch time: 1.0350, average train loss: 5.5327
[11/24 03:22:05 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3080, average loss: 2.9857
[11/24 03:22:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.25	
[11/24 03:22:05 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/24 03:23:50 visual_prompt]: 	Training 100/553. train loss: 0.0005,	0.8440 s / batch. (data: 3.33e-04). ETA=9:26:26, max mem: 20.9 GB 
[11/24 03:25:34 visual_prompt]: 	Training 200/553. train loss: 14.3116,	0.8683 s / batch. (data: 4.04e-02). ETA=9:41:16, max mem: 20.9 GB 
[11/24 03:27:18 visual_prompt]: 	Training 300/553. train loss: 3.0044,	1.6989 s / batch. (data: 8.65e-01). ETA=18:54:34, max mem: 20.9 GB 
[11/24 03:28:59 visual_prompt]: 	Training 400/553. train loss: 7.5925,	0.8479 s / batch. (data: 4.15e-04). ETA=9:24:50, max mem: 20.9 GB 
[11/24 03:30:40 visual_prompt]: 	Training 500/553. train loss: 0.0967,	0.8240 s / batch. (data: 3.17e-04). ETA=9:07:32, max mem: 20.9 GB 
[11/24 03:31:35 visual_prompt]: Epoch 28 / 100: avg data time: 2.06e-01, avg batch time: 1.0307, average train loss: 3.8640
[11/24 03:32:34 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3073, average loss: 0.9024
[11/24 03:32:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.51	
[11/24 03:32:34 visual_prompt]: Stopping early.
[11/24 03:32:34 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 03:32:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 03:32:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/24 03:32:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 03:32:34 visual_prompt]: Training with config:
[11/24 03:32:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/24 03:32:34 visual_prompt]: Loading training data...
[11/24 03:32:34 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 03:32:34 visual_prompt]: Loading validation data...
[11/24 03:32:34 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 03:32:34 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 03:32:37 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 03:32:37 visual_prompt]: tuned percent:0.525
[11/24 03:32:37 visual_prompt]: Device used for model: 0
[11/24 03:32:37 visual_prompt]: Setting up Evaluator...
[11/24 03:32:37 visual_prompt]: Setting up Trainer...
[11/24 03:32:37 visual_prompt]: 	Setting up the optimizer...
[11/24 03:32:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 03:34:24 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8360 s / batch. (data: 3.16e-04). ETA=12:49:06, max mem: 20.9 GB 
[11/24 03:36:05 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8280 s / batch. (data: 3.14e-04). ETA=12:40:23, max mem: 20.9 GB 
[11/24 03:37:50 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0520 s / batch. (data: 2.30e-01). ETA=16:04:20, max mem: 20.9 GB 
[11/24 03:39:31 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8183 s / batch. (data: 3.57e-04). ETA=12:28:46, max mem: 20.9 GB 
[11/24 03:41:16 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8158 s / batch. (data: 3.91e-04). ETA=12:25:06, max mem: 20.9 GB 
[11/24 03:42:10 visual_prompt]: Epoch 1 / 100: avg data time: 2.10e-01, avg batch time: 1.0360, average train loss: 1.5403
[11/24 03:43:09 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3079, average loss: 1.5201
[11/24 03:43:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 03:43:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/24 03:44:56 visual_prompt]: 	Training 100/553. train loss: 1.2469,	0.9108 s / batch. (data: 9.10e-02). ETA=13:49:34, max mem: 20.9 GB 
[11/24 03:46:38 visual_prompt]: 	Training 200/553. train loss: 0.1243,	0.8103 s / batch. (data: 3.51e-04). ETA=12:16:41, max mem: 20.9 GB 
[11/24 03:48:22 visual_prompt]: 	Training 300/553. train loss: 1.6256,	1.1836 s / batch. (data: 3.57e-01). ETA=17:54:05, max mem: 20.9 GB 
[11/24 03:50:04 visual_prompt]: 	Training 400/553. train loss: 1.0894,	0.8414 s / batch. (data: 5.50e-03). ETA=12:42:06, max mem: 20.9 GB 
[11/24 03:51:47 visual_prompt]: 	Training 500/553. train loss: 0.5472,	0.8306 s / batch. (data: 3.05e-04). ETA=12:30:59, max mem: 20.9 GB 
[11/24 03:52:40 visual_prompt]: Epoch 2 / 100: avg data time: 2.05e-01, avg batch time: 1.0315, average train loss: 1.3787
[11/24 03:53:39 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3069, average loss: 4.4359
[11/24 03:53:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.97	
[11/24 03:53:39 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/24 03:55:24 visual_prompt]: 	Training 100/553. train loss: 3.5525,	0.8361 s / batch. (data: 8.50e-03). ETA=12:33:48, max mem: 20.9 GB 
[11/24 03:57:08 visual_prompt]: 	Training 200/553. train loss: 0.7205,	0.8440 s / batch. (data: 3.06e-04). ETA=12:39:30, max mem: 20.9 GB 
[11/24 03:58:50 visual_prompt]: 	Training 300/553. train loss: 1.3486,	0.8440 s / batch. (data: 3.41e-04). ETA=12:38:04, max mem: 20.9 GB 
[11/24 04:00:34 visual_prompt]: 	Training 400/553. train loss: 5.5128,	0.8592 s / batch. (data: 2.07e-02). ETA=12:50:19, max mem: 20.9 GB 
[11/24 04:02:18 visual_prompt]: 	Training 500/553. train loss: 0.8341,	1.5583 s / batch. (data: 7.31e-01). ETA=23:14:34, max mem: 20.9 GB 
[11/24 04:03:09 visual_prompt]: Epoch 3 / 100: avg data time: 2.04e-01, avg batch time: 1.0309, average train loss: 1.7772
[11/24 04:04:09 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3092, average loss: 2.0011
[11/24 04:04:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.93	
[11/24 04:04:09 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/24 04:05:57 visual_prompt]: 	Training 100/553. train loss: 1.9634,	0.8553 s / batch. (data: 2.33e-02). ETA=12:43:11, max mem: 20.9 GB 
[11/24 04:07:40 visual_prompt]: 	Training 200/553. train loss: 1.2338,	0.8278 s / batch. (data: 3.36e-04). ETA=12:17:16, max mem: 20.9 GB 
[11/24 04:09:23 visual_prompt]: 	Training 300/553. train loss: 0.9824,	1.7067 s / batch. (data: 8.99e-01). ETA=1 day, 1:17:14, max mem: 20.9 GB 
[11/24 04:11:01 visual_prompt]: 	Training 400/553. train loss: 0.5550,	0.8280 s / batch. (data: 3.23e-04). ETA=12:14:42, max mem: 20.9 GB 
[11/24 04:12:44 visual_prompt]: 	Training 500/553. train loss: 1.8908,	0.8134 s / batch. (data: 3.54e-04). ETA=12:00:23, max mem: 20.9 GB 
[11/24 04:13:42 visual_prompt]: Epoch 4 / 100: avg data time: 2.10e-01, avg batch time: 1.0358, average train loss: 1.6000
[11/24 04:14:41 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3080, average loss: 0.9835
[11/24 04:14:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.53	
[11/24 04:14:41 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/24 04:16:27 visual_prompt]: 	Training 100/553. train loss: 0.3163,	0.8547 s / batch. (data: 1.05e-02). ETA=12:34:46, max mem: 20.9 GB 
[11/24 04:18:09 visual_prompt]: 	Training 200/553. train loss: 3.4402,	1.4228 s / batch. (data: 6.13e-01). ETA=20:54:10, max mem: 20.9 GB 
[11/24 04:19:53 visual_prompt]: 	Training 300/553. train loss: 11.1062,	0.8765 s / batch. (data: 5.97e-03). ETA=12:51:07, max mem: 20.9 GB 
[11/24 04:21:35 visual_prompt]: 	Training 400/553. train loss: 4.8068,	0.8364 s / batch. (data: 1.05e-02). ETA=12:14:29, max mem: 20.9 GB 
[11/24 04:23:18 visual_prompt]: 	Training 500/553. train loss: 3.0015,	0.8520 s / batch. (data: 7.97e-03). ETA=12:26:45, max mem: 20.9 GB 
[11/24 04:24:12 visual_prompt]: Epoch 5 / 100: avg data time: 2.06e-01, avg batch time: 1.0330, average train loss: 3.1452
[11/24 04:25:11 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3075, average loss: 7.8595
[11/24 04:25:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.86	
[11/24 04:25:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/24 04:27:00 visual_prompt]: 	Training 100/553. train loss: 1.0219,	0.8415 s / batch. (data: 5.45e-03). ETA=12:15:23, max mem: 20.9 GB 
[11/24 04:28:42 visual_prompt]: 	Training 200/553. train loss: 6.9314,	0.8201 s / batch. (data: 3.32e-04). ETA=11:55:18, max mem: 20.9 GB 
[11/24 04:30:24 visual_prompt]: 	Training 300/553. train loss: 2.0502,	0.8564 s / batch. (data: 1.05e-02). ETA=12:25:36, max mem: 20.9 GB 
[11/24 04:32:10 visual_prompt]: 	Training 400/553. train loss: 2.9298,	0.8285 s / batch. (data: 3.55e-04). ETA=11:59:52, max mem: 20.9 GB 
[11/24 04:33:51 visual_prompt]: 	Training 500/553. train loss: 8.1810,	0.8437 s / batch. (data: 1.57e-02). ETA=12:11:39, max mem: 20.9 GB 
[11/24 04:34:44 visual_prompt]: Epoch 6 / 100: avg data time: 2.09e-01, avg batch time: 1.0355, average train loss: 3.0157
[11/24 04:35:43 visual_prompt]: Inference (val):avg data time: 4.62e-05, avg batch time: 0.3083, average loss: 1.0766
[11/24 04:35:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.61	
[11/24 04:35:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/24 04:37:28 visual_prompt]: 	Training 100/553. train loss: 4.1645,	0.8360 s / batch. (data: 1.20e-02). ETA=12:02:52, max mem: 20.9 GB 
[11/24 04:39:11 visual_prompt]: 	Training 200/553. train loss: 0.8076,	0.8328 s / batch. (data: 5.49e-03). ETA=11:58:43, max mem: 20.9 GB 
[11/24 04:40:58 visual_prompt]: 	Training 300/553. train loss: 1.0695,	2.1200 s / batch. (data: 1.29e+00). ETA=1 day, 6:26:05, max mem: 20.9 GB 
[11/24 04:42:40 visual_prompt]: 	Training 400/553. train loss: 0.9894,	1.9799 s / batch. (data: 1.15e+00). ETA=1 day, 4:22:07, max mem: 20.9 GB 
[11/24 04:44:21 visual_prompt]: 	Training 500/553. train loss: 1.4231,	0.8320 s / batch. (data: 3.19e-04). ETA=11:53:54, max mem: 20.9 GB 
[11/24 04:45:14 visual_prompt]: Epoch 7 / 100: avg data time: 2.06e-01, avg batch time: 1.0316, average train loss: 2.4139
[11/24 04:46:13 visual_prompt]: Inference (val):avg data time: 1.71e-04, avg batch time: 0.3056, average loss: 1.2369
[11/24 04:46:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.16	
[11/24 04:46:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/24 04:47:58 visual_prompt]: 	Training 100/553. train loss: 3.9532,	0.8120 s / batch. (data: 3.48e-04). ETA=11:34:38, max mem: 20.9 GB 
[11/24 04:49:42 visual_prompt]: 	Training 200/553. train loss: 4.5629,	0.8363 s / batch. (data: 3.91e-04). ETA=11:54:04, max mem: 20.9 GB 
[11/24 04:51:26 visual_prompt]: 	Training 300/553. train loss: 10.5372,	0.8332 s / batch. (data: 2.33e-02). ETA=11:50:03, max mem: 20.9 GB 
[11/24 04:53:09 visual_prompt]: 	Training 400/553. train loss: 2.3542,	1.0283 s / batch. (data: 2.15e-01). ETA=14:34:33, max mem: 20.9 GB 
[11/24 04:54:52 visual_prompt]: 	Training 500/553. train loss: 6.8852,	1.5037 s / batch. (data: 6.56e-01). ETA=21:16:21, max mem: 20.9 GB 
[11/24 04:55:46 visual_prompt]: Epoch 8 / 100: avg data time: 2.10e-01, avg batch time: 1.0357, average train loss: 3.2039
[11/24 04:56:45 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.3078, average loss: 0.7341
[11/24 04:56:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.82	
[11/24 04:56:45 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/24 04:58:32 visual_prompt]: 	Training 100/553. train loss: 0.0003,	0.8444 s / batch. (data: 1.06e-02). ETA=11:54:36, max mem: 20.9 GB 
[11/24 05:00:14 visual_prompt]: 	Training 200/553. train loss: 3.1278,	0.8277 s / batch. (data: 3.14e-04). ETA=11:39:05, max mem: 20.9 GB 
[11/24 05:01:57 visual_prompt]: 	Training 300/553. train loss: 0.8057,	2.0194 s / batch. (data: 1.20e+00). ETA=1 day, 4:22:14, max mem: 20.9 GB 
[11/24 05:03:41 visual_prompt]: 	Training 400/553. train loss: 1.2837,	0.8520 s / batch. (data: 8.36e-04). ETA=11:56:45, max mem: 20.9 GB 
[11/24 05:05:24 visual_prompt]: 	Training 500/553. train loss: 0.6201,	1.1560 s / batch. (data: 3.21e-01). ETA=16:10:34, max mem: 20.9 GB 
[11/24 05:06:17 visual_prompt]: Epoch 9 / 100: avg data time: 2.08e-01, avg batch time: 1.0340, average train loss: 2.7151
[11/24 05:07:16 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3072, average loss: 0.7821
[11/24 05:07:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.84	
[11/24 05:07:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/24 05:09:06 visual_prompt]: 	Training 100/553. train loss: 6.0433,	0.8344 s / batch. (data: 8.71e-04). ETA=11:38:23, max mem: 20.9 GB 
[11/24 05:10:47 visual_prompt]: 	Training 200/553. train loss: 0.9900,	0.8353 s / batch. (data: 5.49e-03). ETA=11:37:48, max mem: 20.9 GB 
[11/24 05:12:29 visual_prompt]: 	Training 300/553. train loss: 2.5430,	2.2651 s / batch. (data: 1.45e+00). ETA=1 day, 7:28:28, max mem: 20.9 GB 
[11/24 05:14:10 visual_prompt]: 	Training 400/553. train loss: 1.1189,	0.8400 s / batch. (data: 7.94e-03). ETA=11:38:54, max mem: 20.9 GB 
[11/24 05:15:54 visual_prompt]: 	Training 500/553. train loss: 0.8032,	0.8530 s / batch. (data: 2.44e-02). ETA=11:48:17, max mem: 20.9 GB 
[11/24 05:16:47 visual_prompt]: Epoch 10 / 100: avg data time: 2.05e-01, avg batch time: 1.0322, average train loss: 3.6532
[11/24 05:17:46 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3080, average loss: 0.7692
[11/24 05:17:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[11/24 05:17:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/24 05:19:35 visual_prompt]: 	Training 100/553. train loss: 1.9915,	0.8209 s / batch. (data: 3.12e-04). ETA=11:19:34, max mem: 20.9 GB 
[11/24 05:21:20 visual_prompt]: 	Training 200/553. train loss: 1.4055,	0.8360 s / batch. (data: 3.18e-04). ETA=11:30:40, max mem: 20.9 GB 
[11/24 05:23:02 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.4640 s / batch. (data: 1.63e+00). ETA=1 day, 9:51:32, max mem: 20.9 GB 
[11/24 05:24:43 visual_prompt]: 	Training 400/553. train loss: 3.1107,	0.9019 s / batch. (data: 6.19e-02). ETA=12:22:08, max mem: 20.9 GB 
[11/24 05:26:24 visual_prompt]: 	Training 500/553. train loss: 2.3858,	0.8160 s / batch. (data: 3.22e-04). ETA=11:10:04, max mem: 20.9 GB 
[11/24 05:27:17 visual_prompt]: Epoch 11 / 100: avg data time: 2.05e-01, avg batch time: 1.0314, average train loss: 2.7395
[11/24 05:28:16 visual_prompt]: Inference (val):avg data time: 2.23e-04, avg batch time: 0.3061, average loss: 2.1884
[11/24 05:28:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.54	
[11/24 05:28:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/24 05:30:04 visual_prompt]: 	Training 100/553. train loss: 0.7223,	0.9103 s / batch. (data: 8.20e-02). ETA=12:25:09, max mem: 20.9 GB 
[11/24 05:31:47 visual_prompt]: 	Training 200/553. train loss: 2.2740,	0.8200 s / batch. (data: 3.94e-04). ETA=11:09:51, max mem: 20.9 GB 
[11/24 05:33:29 visual_prompt]: 	Training 300/553. train loss: 8.0912,	0.8389 s / batch. (data: 2.15e-02). ETA=11:23:56, max mem: 20.9 GB 
[11/24 05:35:12 visual_prompt]: 	Training 400/553. train loss: 1.6407,	0.8200 s / batch. (data: 3.14e-04). ETA=11:07:09, max mem: 20.9 GB 
[11/24 05:36:55 visual_prompt]: 	Training 500/553. train loss: 12.1122,	0.8360 s / batch. (data: 3.32e-04). ETA=11:18:47, max mem: 20.9 GB 
[11/24 05:37:47 visual_prompt]: Epoch 12 / 100: avg data time: 2.08e-01, avg batch time: 1.0337, average train loss: 2.7763
[11/24 05:38:47 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.3068, average loss: 7.2946
[11/24 05:38:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.00	
[11/24 05:38:47 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/24 05:40:35 visual_prompt]: 	Training 100/553. train loss: 1.2997,	0.8280 s / batch. (data: 7.93e-03). ETA=11:10:09, max mem: 20.9 GB 
[11/24 05:42:15 visual_prompt]: 	Training 200/553. train loss: 0.6424,	0.8240 s / batch. (data: 3.17e-04). ETA=11:05:33, max mem: 20.9 GB 
[11/24 05:43:59 visual_prompt]: 	Training 300/553. train loss: 1.9891,	1.9989 s / batch. (data: 1.19e+00). ETA=1 day, 2:51:12, max mem: 20.9 GB 
[11/24 05:45:40 visual_prompt]: 	Training 400/553. train loss: 1.8692,	0.8294 s / batch. (data: 3.52e-04). ETA=11:07:10, max mem: 20.9 GB 
[11/24 05:47:23 visual_prompt]: 	Training 500/553. train loss: 5.3363,	0.8348 s / batch. (data: 1.20e-02). ETA=11:10:04, max mem: 20.9 GB 
[11/24 05:48:18 visual_prompt]: Epoch 13 / 100: avg data time: 2.06e-01, avg batch time: 1.0326, average train loss: 3.2620
[11/24 05:49:17 visual_prompt]: Inference (val):avg data time: 3.31e-04, avg batch time: 0.3059, average loss: 2.3255
[11/24 05:49:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.87	
[11/24 05:49:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/24 05:51:05 visual_prompt]: 	Training 100/553. train loss: 0.6281,	0.8616 s / batch. (data: 2.16e-02). ETA=11:29:27, max mem: 20.9 GB 
[11/24 05:52:47 visual_prompt]: 	Training 200/553. train loss: 0.0045,	1.1534 s / batch. (data: 3.41e-01). ETA=15:20:58, max mem: 20.9 GB 
[11/24 05:54:29 visual_prompt]: 	Training 300/553. train loss: 1.0539,	0.8318 s / batch. (data: 3.39e-04). ETA=11:02:51, max mem: 20.9 GB 
[11/24 05:56:12 visual_prompt]: 	Training 400/553. train loss: 0.6801,	0.8262 s / batch. (data: 9.32e-03). ETA=10:57:00, max mem: 20.9 GB 
[11/24 05:57:55 visual_prompt]: 	Training 500/553. train loss: 2.5838,	0.8356 s / batch. (data: 5.52e-03). ETA=11:03:03, max mem: 20.9 GB 
[11/24 05:58:48 visual_prompt]: Epoch 14 / 100: avg data time: 2.05e-01, avg batch time: 1.0318, average train loss: 2.5301
[11/24 05:59:47 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3090, average loss: 0.6842
[11/24 05:59:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.50	
[11/24 05:59:47 visual_prompt]: Best epoch 14: best metric: -0.684
[11/24 05:59:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/24 06:01:33 visual_prompt]: 	Training 100/553. train loss: 4.3418,	0.9200 s / batch. (data: 1.05e-01). ETA=12:07:41, max mem: 20.9 GB 
[11/24 06:03:15 visual_prompt]: 	Training 200/553. train loss: 1.6729,	0.8204 s / batch. (data: 3.31e-04). ETA=10:47:32, max mem: 20.9 GB 
[11/24 06:05:00 visual_prompt]: 	Training 300/553. train loss: 2.9823,	0.8280 s / batch. (data: 3.18e-04). ETA=10:52:10, max mem: 20.9 GB 
[11/24 06:06:40 visual_prompt]: 	Training 400/553. train loss: 0.5084,	0.8591 s / batch. (data: 1.51e-02). ETA=11:15:12, max mem: 20.9 GB 
[11/24 06:08:24 visual_prompt]: 	Training 500/553. train loss: 1.0066,	0.8199 s / batch. (data: 5.50e-03). ETA=10:43:03, max mem: 20.9 GB 
[11/24 06:09:18 visual_prompt]: Epoch 15 / 100: avg data time: 2.07e-01, avg batch time: 1.0327, average train loss: 3.7963
[11/24 06:10:17 visual_prompt]: Inference (val):avg data time: 4.56e-05, avg batch time: 0.3078, average loss: 3.8092
[11/24 06:10:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.50	
[11/24 06:10:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/24 06:12:03 visual_prompt]: 	Training 100/553. train loss: 5.3203,	0.8274 s / batch. (data: 5.45e-03). ETA=10:46:48, max mem: 20.9 GB 
[11/24 06:13:46 visual_prompt]: 	Training 200/553. train loss: 0.4610,	0.8379 s / batch. (data: 1.56e-02). ETA=10:53:38, max mem: 20.9 GB 
[11/24 06:15:29 visual_prompt]: 	Training 300/553. train loss: 3.5131,	0.8320 s / batch. (data: 3.46e-04). ETA=10:47:38, max mem: 20.9 GB 
[11/24 06:17:12 visual_prompt]: 	Training 400/553. train loss: 7.2067,	0.8404 s / batch. (data: 8.45e-04). ETA=10:52:46, max mem: 20.9 GB 
[11/24 06:18:54 visual_prompt]: 	Training 500/553. train loss: 1.3315,	1.4136 s / batch. (data: 6.04e-01). ETA=18:15:37, max mem: 20.9 GB 
[11/24 06:19:48 visual_prompt]: Epoch 16 / 100: avg data time: 2.07e-01, avg batch time: 1.0329, average train loss: 2.9806
[11/24 06:20:48 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3072, average loss: 0.7077
[11/24 06:20:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.44	
[11/24 06:20:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/24 06:22:34 visual_prompt]: 	Training 100/553. train loss: 6.2435,	0.8165 s / batch. (data: 3.09e-04). ETA=10:30:44, max mem: 20.9 GB 
[11/24 06:24:19 visual_prompt]: 	Training 200/553. train loss: 8.1347,	0.8447 s / batch. (data: 1.05e-02). ETA=10:51:08, max mem: 20.9 GB 
[11/24 06:26:01 visual_prompt]: 	Training 300/553. train loss: 12.6241,	0.8385 s / batch. (data: 2.13e-02). ETA=10:44:57, max mem: 20.9 GB 
[11/24 06:27:42 visual_prompt]: 	Training 400/553. train loss: 3.3180,	1.1027 s / batch. (data: 2.81e-01). ETA=14:06:19, max mem: 20.9 GB 
[11/24 06:29:25 visual_prompt]: 	Training 500/553. train loss: 2.3273,	1.7815 s / batch. (data: 9.65e-01). ETA=22:44:23, max mem: 20.9 GB 
[11/24 06:30:19 visual_prompt]: Epoch 17 / 100: avg data time: 2.08e-01, avg batch time: 1.0338, average train loss: 3.0233
[11/24 06:31:19 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3047, average loss: 3.4167
[11/24 06:31:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.38	
[11/24 06:31:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/24 06:33:06 visual_prompt]: 	Training 100/553. train loss: 1.2508,	0.8262 s / batch. (data: 5.48e-03). ETA=10:30:41, max mem: 20.9 GB 
[11/24 06:34:51 visual_prompt]: 	Training 200/553. train loss: 5.2008,	0.8318 s / batch. (data: 8.50e-04). ETA=10:33:34, max mem: 20.9 GB 
[11/24 06:36:34 visual_prompt]: 	Training 300/553. train loss: 1.5172,	0.8341 s / batch. (data: 5.45e-03). ETA=10:33:53, max mem: 20.9 GB 
[11/24 06:38:16 visual_prompt]: 	Training 400/553. train loss: 2.1699,	0.8136 s / batch. (data: 3.17e-04). ETA=10:16:56, max mem: 20.9 GB 
[11/24 06:39:58 visual_prompt]: 	Training 500/553. train loss: 2.3278,	0.8271 s / batch. (data: 3.20e-04). ETA=10:25:49, max mem: 20.9 GB 
[11/24 06:40:51 visual_prompt]: Epoch 18 / 100: avg data time: 2.07e-01, avg batch time: 1.0341, average train loss: 3.2838
[11/24 06:41:50 visual_prompt]: Inference (val):avg data time: 5.00e-05, avg batch time: 0.3074, average loss: 3.8529
[11/24 06:41:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.17	
[11/24 06:41:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/24 06:43:36 visual_prompt]: 	Training 100/553. train loss: 1.1073,	0.8320 s / batch. (data: 3.29e-04). ETA=10:27:24, max mem: 20.9 GB 
[11/24 06:45:20 visual_prompt]: 	Training 200/553. train loss: 0.5272,	0.8477 s / batch. (data: 5.50e-03). ETA=10:37:49, max mem: 20.9 GB 
[11/24 06:47:03 visual_prompt]: 	Training 300/553. train loss: 0.7648,	0.8208 s / batch. (data: 1.05e-02). ETA=10:16:14, max mem: 20.9 GB 
[11/24 06:48:47 visual_prompt]: 	Training 400/553. train loss: 0.9542,	0.8202 s / batch. (data: 3.24e-04). ETA=10:14:24, max mem: 20.9 GB 
[11/24 06:50:25 visual_prompt]: 	Training 500/553. train loss: 1.7440,	0.8316 s / batch. (data: 3.25e-04). ETA=10:21:32, max mem: 20.9 GB 
[11/24 06:51:19 visual_prompt]: Epoch 19 / 100: avg data time: 2.02e-01, avg batch time: 1.0288, average train loss: 2.5396
[11/24 06:52:18 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3071, average loss: 8.6541
[11/24 06:52:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.80	
[11/24 06:52:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/24 06:54:03 visual_prompt]: 	Training 100/553. train loss: 1.4667,	1.5586 s / batch. (data: 7.15e-01). ETA=19:20:57, max mem: 20.9 GB 
[11/24 06:55:47 visual_prompt]: 	Training 200/553. train loss: 0.4625,	0.8240 s / batch. (data: 3.14e-04). ETA=10:12:25, max mem: 20.9 GB 
[11/24 06:57:30 visual_prompt]: 	Training 300/553. train loss: 11.2653,	0.8207 s / batch. (data: 3.20e-04). ETA=10:08:35, max mem: 20.9 GB 
[11/24 06:59:12 visual_prompt]: 	Training 400/553. train loss: 2.3421,	0.8490 s / batch. (data: 3.30e-04). ETA=10:28:09, max mem: 20.9 GB 
[11/24 07:00:54 visual_prompt]: 	Training 500/553. train loss: 2.3442,	0.8713 s / batch. (data: 1.13e-02). ETA=10:43:12, max mem: 20.9 GB 
[11/24 07:01:49 visual_prompt]: Epoch 20 / 100: avg data time: 2.07e-01, avg batch time: 1.0328, average train loss: 3.0663
[11/24 07:02:48 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.3070, average loss: 1.1841
[11/24 07:02:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 66.69	
[11/24 07:02:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/24 07:04:38 visual_prompt]: 	Training 100/553. train loss: 4.4882,	0.8240 s / batch. (data: 3.43e-04). ETA=10:06:10, max mem: 20.9 GB 
[11/24 07:06:20 visual_prompt]: 	Training 200/553. train loss: 0.4055,	0.8360 s / batch. (data: 3.36e-04). ETA=10:13:37, max mem: 20.9 GB 
[11/24 07:08:01 visual_prompt]: 	Training 300/553. train loss: 11.5161,	1.2960 s / batch. (data: 4.54e-01). ETA=15:49:06, max mem: 20.9 GB 
[11/24 07:09:42 visual_prompt]: 	Training 400/553. train loss: 3.4505,	0.8517 s / batch. (data: 1.17e-02). ETA=10:22:18, max mem: 20.9 GB 
[11/24 07:11:27 visual_prompt]: 	Training 500/553. train loss: 4.3983,	0.8183 s / batch. (data: 5.47e-03). ETA=9:56:33, max mem: 20.9 GB 
[11/24 07:12:20 visual_prompt]: Epoch 21 / 100: avg data time: 2.07e-01, avg batch time: 1.0331, average train loss: 3.2743
[11/24 07:13:19 visual_prompt]: Inference (val):avg data time: 2.26e-04, avg batch time: 0.3072, average loss: 1.0218
[11/24 07:13:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 64.63	
[11/24 07:13:19 visual_prompt]: Stopping early.
[11/24 07:13:19 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 07:13:19 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 07:13:19 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/24 07:13:19 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 07:13:19 visual_prompt]: Training with config:
[11/24 07:13:19 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/24 07:13:19 visual_prompt]: Loading training data...
[11/24 07:13:19 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 07:13:19 visual_prompt]: Loading validation data...
[11/24 07:13:19 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 07:13:19 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 07:13:22 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 07:13:22 visual_prompt]: tuned percent:0.525
[11/24 07:13:22 visual_prompt]: Device used for model: 0
[11/24 07:13:22 visual_prompt]: Setting up Evaluator...
[11/24 07:13:22 visual_prompt]: Setting up Trainer...
[11/24 07:13:22 visual_prompt]: 	Setting up the optimizer...
[11/24 07:13:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 07:15:08 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8160 s / batch. (data: 3.17e-04). ETA=12:30:44, max mem: 20.9 GB 
[11/24 07:16:49 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8615 s / batch. (data: 9.51e-03). ETA=13:11:11, max mem: 20.9 GB 
[11/24 07:18:35 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3160 s / batch. (data: 4.73e-01). ETA=20:06:19, max mem: 20.9 GB 
[11/24 07:20:15 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8323 s / batch. (data: 1.64e-02). ETA=12:41:35, max mem: 20.9 GB 
[11/24 07:22:01 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8280 s / batch. (data: 3.19e-04). ETA=12:36:17, max mem: 20.9 GB 
[11/24 07:22:55 visual_prompt]: Epoch 1 / 100: avg data time: 2.11e-01, avg batch time: 1.0367, average train loss: 1.5403
[11/24 07:23:54 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3082, average loss: 1.5201
[11/24 07:23:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 07:23:54 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/24 07:25:41 visual_prompt]: 	Training 100/553. train loss: 0.7319,	1.0667 s / batch. (data: 2.53e-01). ETA=16:11:34, max mem: 20.9 GB 
[11/24 07:27:23 visual_prompt]: 	Training 200/553. train loss: 0.0588,	0.8316 s / batch. (data: 5.38e-03). ETA=12:36:01, max mem: 20.9 GB 
[11/24 07:29:08 visual_prompt]: 	Training 300/553. train loss: 0.7024,	1.2506 s / batch. (data: 4.27e-01). ETA=18:54:53, max mem: 20.9 GB 
[11/24 07:30:49 visual_prompt]: 	Training 400/553. train loss: 1.2103,	0.8103 s / batch. (data: 3.45e-04). ETA=12:13:56, max mem: 20.9 GB 
[11/24 07:32:34 visual_prompt]: 	Training 500/553. train loss: 0.6010,	0.8400 s / batch. (data: 5.46e-03). ETA=12:39:27, max mem: 20.9 GB 
[11/24 07:33:26 visual_prompt]: Epoch 2 / 100: avg data time: 2.07e-01, avg batch time: 1.0334, average train loss: 0.9073
[11/24 07:34:25 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.3082, average loss: 1.4984
[11/24 07:34:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.17	
[11/24 07:34:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/24 07:36:11 visual_prompt]: 	Training 100/553. train loss: 1.1307,	0.8392 s / batch. (data: 5.46e-03). ETA=12:36:38, max mem: 20.9 GB 
[11/24 07:37:55 visual_prompt]: 	Training 200/553. train loss: 0.7188,	0.8261 s / batch. (data: 5.45e-03). ETA=12:23:27, max mem: 20.9 GB 
[11/24 07:39:37 visual_prompt]: 	Training 300/553. train loss: 1.0751,	0.8403 s / batch. (data: 3.40e-04). ETA=12:34:45, max mem: 20.9 GB 
[11/24 07:41:19 visual_prompt]: 	Training 400/553. train loss: 1.2364,	0.8120 s / batch. (data: 3.20e-04). ETA=12:07:59, max mem: 20.9 GB 
[11/24 07:43:03 visual_prompt]: 	Training 500/553. train loss: 0.7146,	1.3000 s / batch. (data: 4.81e-01). ETA=19:23:20, max mem: 20.9 GB 
[11/24 07:43:55 visual_prompt]: Epoch 3 / 100: avg data time: 2.04e-01, avg batch time: 1.0306, average train loss: 1.0137
[11/24 07:44:54 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3059, average loss: 0.7381
[11/24 07:44:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.10	
[11/24 07:44:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/24 07:46:42 visual_prompt]: 	Training 100/553. train loss: 0.7098,	0.8277 s / batch. (data: 5.47e-03). ETA=12:18:34, max mem: 20.9 GB 
[11/24 07:48:26 visual_prompt]: 	Training 200/553. train loss: 0.6045,	0.8269 s / batch. (data: 6.95e-03). ETA=12:16:33, max mem: 20.9 GB 
[11/24 07:50:09 visual_prompt]: 	Training 300/553. train loss: 0.6077,	1.3234 s / batch. (data: 5.12e-01). ETA=19:36:33, max mem: 20.9 GB 
[11/24 07:51:48 visual_prompt]: 	Training 400/553. train loss: 0.5648,	1.5032 s / batch. (data: 6.77e-01). ETA=22:13:54, max mem: 20.9 GB 
[11/24 07:53:32 visual_prompt]: 	Training 500/553. train loss: 1.8408,	3.1496 s / batch. (data: 2.33e+00). ETA=1 day, 22:29:35, max mem: 20.9 GB 
[11/24 07:54:27 visual_prompt]: Epoch 4 / 100: avg data time: 2.10e-01, avg batch time: 1.0355, average train loss: 1.0741
[11/24 07:55:26 visual_prompt]: Inference (val):avg data time: 3.38e-04, avg batch time: 0.3067, average loss: 0.6947
[11/24 07:55:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.97	
[11/24 07:55:26 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/24 07:57:11 visual_prompt]: 	Training 100/553. train loss: 3.1885,	0.8520 s / batch. (data: 3.06e-04). ETA=12:32:25, max mem: 20.9 GB 
[11/24 07:58:54 visual_prompt]: 	Training 200/553. train loss: 0.6860,	1.2880 s / batch. (data: 4.65e-01). ETA=18:55:21, max mem: 20.9 GB 
[11/24 08:00:38 visual_prompt]: 	Training 300/553. train loss: 0.9858,	0.8280 s / batch. (data: 3.27e-04). ETA=12:08:26, max mem: 20.9 GB 
[11/24 08:02:20 visual_prompt]: 	Training 400/553. train loss: 2.0841,	0.8277 s / batch. (data: 1.20e-02). ETA=12:06:50, max mem: 20.9 GB 
[11/24 08:04:03 visual_prompt]: 	Training 500/553. train loss: 0.5765,	0.8323 s / batch. (data: 3.26e-04). ETA=12:09:29, max mem: 20.9 GB 
[11/24 08:04:57 visual_prompt]: Epoch 5 / 100: avg data time: 2.07e-01, avg batch time: 1.0329, average train loss: 1.3406
[11/24 08:05:56 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3060, average loss: 1.1708
[11/24 08:05:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.42	
[11/24 08:05:56 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/24 08:07:45 visual_prompt]: 	Training 100/553. train loss: 1.4281,	0.8320 s / batch. (data: 3.16e-04). ETA=12:07:06, max mem: 20.9 GB 
[11/24 08:09:26 visual_prompt]: 	Training 200/553. train loss: 0.3868,	0.8280 s / batch. (data: 3.29e-04). ETA=12:02:12, max mem: 20.9 GB 
[11/24 08:11:08 visual_prompt]: 	Training 300/553. train loss: 0.5484,	0.8360 s / batch. (data: 5.47e-03). ETA=12:07:48, max mem: 20.9 GB 
[11/24 08:12:55 visual_prompt]: 	Training 400/553. train loss: 1.1725,	0.8225 s / batch. (data: 3.26e-04). ETA=11:54:38, max mem: 20.9 GB 
[11/24 08:14:35 visual_prompt]: 	Training 500/553. train loss: 1.1503,	0.8560 s / batch. (data: 1.15e-02). ETA=12:22:20, max mem: 20.9 GB 
[11/24 08:15:28 visual_prompt]: Epoch 6 / 100: avg data time: 2.08e-01, avg batch time: 1.0334, average train loss: 1.4767
[11/24 08:16:27 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.3079, average loss: 0.7122
[11/24 08:16:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.23	
[11/24 08:16:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/24 08:18:13 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8125 s / batch. (data: 3.20e-04). ETA=11:42:36, max mem: 20.9 GB 
[11/24 08:19:55 visual_prompt]: 	Training 200/553. train loss: 0.5725,	0.8320 s / batch. (data: 3.40e-04). ETA=11:58:02, max mem: 20.9 GB 
[11/24 08:21:40 visual_prompt]: 	Training 300/553. train loss: 1.1049,	1.3584 s / batch. (data: 5.51e-01). ETA=19:30:03, max mem: 20.9 GB 
[11/24 08:23:23 visual_prompt]: 	Training 400/553. train loss: 0.8149,	2.0475 s / batch. (data: 1.21e+00). ETA=1 day, 5:20:14, max mem: 20.9 GB 
[11/24 08:25:05 visual_prompt]: 	Training 500/553. train loss: 2.5716,	0.8603 s / batch. (data: 1.75e-02). ETA=12:18:08, max mem: 20.9 GB 
[11/24 08:25:57 visual_prompt]: Epoch 7 / 100: avg data time: 2.04e-01, avg batch time: 1.0311, average train loss: 1.7214
[11/24 08:26:56 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3078, average loss: 2.4383
[11/24 08:26:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.17	
[11/24 08:26:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/24 08:28:41 visual_prompt]: 	Training 100/553. train loss: 1.4058,	0.8462 s / batch. (data: 1.81e-02). ETA=12:03:53, max mem: 20.9 GB 
[11/24 08:30:26 visual_prompt]: 	Training 200/553. train loss: 1.2943,	0.8205 s / batch. (data: 9.75e-03). ETA=11:40:31, max mem: 20.9 GB 
[11/24 08:32:09 visual_prompt]: 	Training 300/553. train loss: 1.1910,	0.8113 s / batch. (data: 3.13e-04). ETA=11:31:21, max mem: 20.9 GB 
[11/24 08:33:52 visual_prompt]: 	Training 400/553. train loss: 1.4699,	0.8363 s / batch. (data: 3.24e-04). ETA=11:51:16, max mem: 20.9 GB 
[11/24 08:35:34 visual_prompt]: 	Training 500/553. train loss: 2.6714,	1.5520 s / batch. (data: 7.38e-01). ETA=21:57:22, max mem: 20.9 GB 
[11/24 08:36:29 visual_prompt]: Epoch 8 / 100: avg data time: 2.09e-01, avg batch time: 1.0346, average train loss: 1.7855
[11/24 08:37:28 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.3064, average loss: 1.4725
[11/24 08:37:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.95	
[11/24 08:37:28 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/24 08:39:14 visual_prompt]: 	Training 100/553. train loss: 0.1074,	0.8098 s / batch. (data: 3.50e-04). ETA=11:25:17, max mem: 20.9 GB 
[11/24 08:40:56 visual_prompt]: 	Training 200/553. train loss: 0.8484,	0.8199 s / batch. (data: 3.72e-04). ETA=11:32:28, max mem: 20.9 GB 
[11/24 08:42:39 visual_prompt]: 	Training 300/553. train loss: 0.5530,	1.9085 s / batch. (data: 1.10e+00). ETA=1 day, 2:48:44, max mem: 20.9 GB 
[11/24 08:44:23 visual_prompt]: 	Training 400/553. train loss: 1.3158,	0.8471 s / batch. (data: 1.53e-02). ETA=11:52:39, max mem: 20.9 GB 
[11/24 08:46:07 visual_prompt]: 	Training 500/553. train loss: 1.6353,	1.0146 s / batch. (data: 1.92e-01). ETA=14:11:51, max mem: 20.9 GB 
[11/24 08:46:59 visual_prompt]: Epoch 9 / 100: avg data time: 2.07e-01, avg batch time: 1.0333, average train loss: 2.0420
[11/24 08:47:58 visual_prompt]: Inference (val):avg data time: 4.56e-05, avg batch time: 0.3082, average loss: 2.6569
[11/24 08:47:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.63	
[11/24 08:47:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/24 08:49:48 visual_prompt]: 	Training 100/553. train loss: 4.6758,	0.8308 s / batch. (data: 2.08e-02). ETA=11:35:25, max mem: 20.9 GB 
[11/24 08:51:30 visual_prompt]: 	Training 200/553. train loss: 4.1651,	0.8360 s / batch. (data: 7.96e-03). ETA=11:38:23, max mem: 20.9 GB 
[11/24 08:53:12 visual_prompt]: 	Training 300/553. train loss: 1.9748,	0.8120 s / batch. (data: 3.14e-04). ETA=11:16:58, max mem: 20.9 GB 
[11/24 08:54:52 visual_prompt]: 	Training 400/553. train loss: 0.8070,	0.9440 s / batch. (data: 1.12e-01). ETA=13:05:27, max mem: 20.9 GB 
[11/24 08:56:36 visual_prompt]: 	Training 500/553. train loss: 0.5712,	0.9804 s / batch. (data: 1.61e-01). ETA=13:34:04, max mem: 20.9 GB 
[11/24 08:57:30 visual_prompt]: Epoch 10 / 100: avg data time: 2.09e-01, avg batch time: 1.0338, average train loss: 2.7223
[11/24 08:58:29 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3075, average loss: 0.9930
[11/24 08:58:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.69	
[11/24 08:58:29 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/24 09:00:18 visual_prompt]: 	Training 100/553. train loss: 1.3468,	0.8126 s / batch. (data: 3.27e-04). ETA=11:12:44, max mem: 20.9 GB 
[11/24 09:02:03 visual_prompt]: 	Training 200/553. train loss: 7.5440,	0.8240 s / batch. (data: 5.94e-04). ETA=11:20:44, max mem: 20.9 GB 
[11/24 09:03:45 visual_prompt]: 	Training 300/553. train loss: 0.0002,	2.3600 s / batch. (data: 1.52e+00). ETA=1 day, 8:25:47, max mem: 20.9 GB 
[11/24 09:05:26 visual_prompt]: 	Training 400/553. train loss: 5.0701,	0.8255 s / batch. (data: 5.48e-03). ETA=11:19:13, max mem: 20.9 GB 
[11/24 09:07:08 visual_prompt]: 	Training 500/553. train loss: 4.2936,	0.8440 s / batch. (data: 7.97e-03). ETA=11:33:04, max mem: 20.9 GB 
[11/24 09:08:00 visual_prompt]: Epoch 11 / 100: avg data time: 2.06e-01, avg batch time: 1.0321, average train loss: 2.7813
[11/24 09:08:59 visual_prompt]: Inference (val):avg data time: 4.47e-05, avg batch time: 0.3081, average loss: 0.7190
[11/24 09:08:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.25	
[11/24 09:08:59 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/24 09:10:48 visual_prompt]: 	Training 100/553. train loss: 1.2974,	0.8559 s / batch. (data: 4.53e-04). ETA=11:40:40, max mem: 20.9 GB 
[11/24 09:12:31 visual_prompt]: 	Training 200/553. train loss: 1.2570,	0.8148 s / batch. (data: 5.49e-03). ETA=11:05:40, max mem: 20.9 GB 
[11/24 09:14:12 visual_prompt]: 	Training 300/553. train loss: 1.0898,	0.8324 s / batch. (data: 3.65e-04). ETA=11:18:38, max mem: 20.9 GB 
[11/24 09:15:55 visual_prompt]: 	Training 400/553. train loss: 2.4149,	0.8280 s / batch. (data: 1.20e-02). ETA=11:13:40, max mem: 20.9 GB 
[11/24 09:17:38 visual_prompt]: 	Training 500/553. train loss: 0.8156,	0.8360 s / batch. (data: 3.35e-04). ETA=11:18:46, max mem: 20.9 GB 
[11/24 09:18:30 visual_prompt]: Epoch 12 / 100: avg data time: 2.06e-01, avg batch time: 1.0322, average train loss: 2.2124
[11/24 09:19:29 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3074, average loss: 1.9518
[11/24 09:19:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 42.65	
[11/24 09:19:29 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/24 09:21:18 visual_prompt]: 	Training 100/553. train loss: 1.6673,	0.8430 s / batch. (data: 1.05e-02). ETA=11:22:21, max mem: 20.9 GB 
[11/24 09:22:57 visual_prompt]: 	Training 200/553. train loss: 1.0907,	0.8398 s / batch. (data: 1.05e-02). ETA=11:18:20, max mem: 20.9 GB 
[11/24 09:24:41 visual_prompt]: 	Training 300/553. train loss: 11.3819,	1.8600 s / batch. (data: 1.03e+00). ETA=1 day, 0:59:14, max mem: 20.9 GB 
[11/24 09:26:22 visual_prompt]: 	Training 400/553. train loss: 0.8201,	0.8480 s / batch. (data: 3.53e-04). ETA=11:22:08, max mem: 20.9 GB 
[11/24 09:28:06 visual_prompt]: 	Training 500/553. train loss: 4.5766,	0.8320 s / batch. (data: 3.26e-04). ETA=11:07:52, max mem: 20.9 GB 
[11/24 09:29:00 visual_prompt]: Epoch 13 / 100: avg data time: 2.06e-01, avg batch time: 1.0323, average train loss: 3.3906
[11/24 09:29:59 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.3068, average loss: 1.8345
[11/24 09:29:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 54.85	
[11/24 09:29:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/24 09:31:47 visual_prompt]: 	Training 100/553. train loss: 1.4706,	0.8332 s / batch. (data: 3.23e-04). ETA=11:06:41, max mem: 20.9 GB 
[11/24 09:33:30 visual_prompt]: 	Training 200/553. train loss: 0.0002,	1.3642 s / batch. (data: 5.43e-01). ETA=18:09:19, max mem: 20.9 GB 
[11/24 09:35:12 visual_prompt]: 	Training 300/553. train loss: 0.9847,	0.9385 s / batch. (data: 1.13e-01). ETA=12:27:51, max mem: 20.9 GB 
[11/24 09:36:54 visual_prompt]: 	Training 400/553. train loss: 0.6098,	0.8360 s / batch. (data: 1.20e-02). ETA=11:04:45, max mem: 20.9 GB 
[11/24 09:38:38 visual_prompt]: 	Training 500/553. train loss: 0.8445,	0.8320 s / batch. (data: 7.95e-03). ETA=11:00:10, max mem: 20.9 GB 
[11/24 09:39:29 visual_prompt]: Epoch 14 / 100: avg data time: 2.04e-01, avg batch time: 1.0304, average train loss: 2.9013
[11/24 09:40:28 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3082, average loss: 2.6851
[11/24 09:40:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.51	
[11/24 09:40:28 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/24 09:42:15 visual_prompt]: 	Training 100/553. train loss: 0.7502,	0.8219 s / batch. (data: 3.30e-04). ETA=10:50:04, max mem: 20.9 GB 
[11/24 09:43:56 visual_prompt]: 	Training 200/553. train loss: 6.2084,	0.8236 s / batch. (data: 5.48e-03). ETA=10:50:03, max mem: 20.9 GB 
[11/24 09:45:41 visual_prompt]: 	Training 300/553. train loss: 6.2141,	0.8321 s / batch. (data: 3.18e-04). ETA=10:55:23, max mem: 20.9 GB 
[11/24 09:47:21 visual_prompt]: 	Training 400/553. train loss: 4.9723,	0.8360 s / batch. (data: 3.44e-04). ETA=10:57:04, max mem: 20.9 GB 
[11/24 09:49:05 visual_prompt]: 	Training 500/553. train loss: 0.5792,	0.8190 s / batch. (data: 3.63e-04). ETA=10:42:19, max mem: 20.9 GB 
[11/24 09:49:59 visual_prompt]: Epoch 15 / 100: avg data time: 2.07e-01, avg batch time: 1.0320, average train loss: 2.8800
[11/24 09:50:58 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3065, average loss: 1.1222
[11/24 09:50:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.96	
[11/24 09:50:58 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/24 09:52:44 visual_prompt]: 	Training 100/553. train loss: 0.9188,	0.8440 s / batch. (data: 3.16e-04). ETA=10:59:47, max mem: 20.9 GB 
[11/24 09:54:27 visual_prompt]: 	Training 200/553. train loss: 1.6138,	0.8105 s / batch. (data: 3.00e-04). ETA=10:32:16, max mem: 20.9 GB 
[11/24 09:56:10 visual_prompt]: 	Training 300/553. train loss: 2.6054,	0.8320 s / batch. (data: 3.25e-04). ETA=10:47:36, max mem: 20.9 GB 
[11/24 09:57:52 visual_prompt]: 	Training 400/553. train loss: 11.0380,	0.8320 s / batch. (data: 3.96e-04). ETA=10:46:15, max mem: 20.9 GB 
[11/24 09:59:34 visual_prompt]: 	Training 500/553. train loss: 1.0261,	1.4794 s / batch. (data: 6.58e-01). ETA=19:06:37, max mem: 20.9 GB 
[11/24 10:00:28 visual_prompt]: Epoch 16 / 100: avg data time: 2.05e-01, avg batch time: 1.0313, average train loss: 3.0492
[11/24 10:01:28 visual_prompt]: Inference (val):avg data time: 5.21e-05, avg batch time: 0.3069, average loss: 1.8886
[11/24 10:01:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.51	
[11/24 10:01:28 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/24 10:03:14 visual_prompt]: 	Training 100/553. train loss: 3.0164,	0.8098 s / batch. (data: 3.18e-04). ETA=10:25:34, max mem: 20.9 GB 
[11/24 10:04:58 visual_prompt]: 	Training 200/553. train loss: 0.7377,	0.8440 s / batch. (data: 3.23e-04). ETA=10:50:36, max mem: 20.9 GB 
[11/24 10:06:40 visual_prompt]: 	Training 300/553. train loss: 2.3622,	0.8237 s / batch. (data: 3.18e-04). ETA=10:33:37, max mem: 20.9 GB 
[11/24 10:08:23 visual_prompt]: 	Training 400/553. train loss: 3.6778,	1.3200 s / batch. (data: 4.81e-01). ETA=16:53:07, max mem: 20.9 GB 
[11/24 10:10:05 visual_prompt]: 	Training 500/553. train loss: 2.8617,	1.7269 s / batch. (data: 8.83e-01). ETA=22:02:35, max mem: 20.9 GB 
[11/24 10:10:59 visual_prompt]: Epoch 17 / 100: avg data time: 2.07e-01, avg batch time: 1.0333, average train loss: 3.0789
[11/24 10:11:58 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3083, average loss: 2.8243
[11/24 10:11:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.42	
[11/24 10:11:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/24 10:13:45 visual_prompt]: 	Training 100/553. train loss: 2.2617,	0.8400 s / batch. (data: 7.94e-03). ETA=10:41:11, max mem: 20.9 GB 
[11/24 10:15:31 visual_prompt]: 	Training 200/553. train loss: 1.3063,	0.8393 s / batch. (data: 8.66e-04). ETA=10:39:14, max mem: 20.9 GB 
[11/24 10:17:13 visual_prompt]: 	Training 300/553. train loss: 1.5358,	0.8673 s / batch. (data: 2.33e-02). ETA=10:59:08, max mem: 20.9 GB 
[11/24 10:18:56 visual_prompt]: 	Training 400/553. train loss: 4.9620,	0.8243 s / batch. (data: 7.96e-03). ETA=10:25:06, max mem: 20.9 GB 
[11/24 10:20:38 visual_prompt]: 	Training 500/553. train loss: 1.2236,	0.8160 s / batch. (data: 3.48e-04). ETA=10:17:25, max mem: 20.9 GB 
[11/24 10:21:31 visual_prompt]: Epoch 18 / 100: avg data time: 2.10e-01, avg batch time: 1.0346, average train loss: 3.1646
[11/24 10:22:30 visual_prompt]: Inference (val):avg data time: 2.32e-04, avg batch time: 0.3097, average loss: 2.9035
[11/24 10:22:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.02	
[11/24 10:22:30 visual_prompt]: Stopping early.
[11/24 10:22:30 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 10:22:30 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 10:22:30 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/24 10:22:30 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 10:22:30 visual_prompt]: Training with config:
[11/24 10:22:30 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/24 10:22:30 visual_prompt]: Loading training data...
[11/24 10:22:30 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 10:22:30 visual_prompt]: Loading validation data...
[11/24 10:22:30 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 10:22:30 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 10:22:33 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 10:22:33 visual_prompt]: tuned percent:0.525
[11/24 10:22:33 visual_prompt]: Device used for model: 0
[11/24 10:22:33 visual_prompt]: Setting up Evaluator...
[11/24 10:22:33 visual_prompt]: Setting up Trainer...
[11/24 10:22:33 visual_prompt]: 	Setting up the optimizer...
[11/24 10:22:33 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 10:24:20 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8140 s / batch. (data: 5.45e-03). ETA=12:28:51, max mem: 20.9 GB 
[11/24 10:26:00 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8251 s / batch. (data: 3.23e-04). ETA=12:37:43, max mem: 20.9 GB 
[11/24 10:27:46 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.9000 s / batch. (data: 1.05e+00). ETA=1 day, 5:01:39, max mem: 20.9 GB 
[11/24 10:29:27 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8216 s / batch. (data: 3.12e-04). ETA=12:31:48, max mem: 20.9 GB 
[11/24 10:31:12 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8137 s / batch. (data: 3.21e-04). ETA=12:23:11, max mem: 20.9 GB 
[11/24 10:32:06 visual_prompt]: Epoch 1 / 100: avg data time: 2.09e-01, avg batch time: 1.0357, average train loss: 1.5403
[11/24 10:33:05 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.3082, average loss: 1.5201
[11/24 10:33:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 10:33:05 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/24 10:34:52 visual_prompt]: 	Training 100/553. train loss: 0.7431,	0.8560 s / batch. (data: 3.55e-04). ETA=12:59:36, max mem: 20.9 GB 
[11/24 10:36:34 visual_prompt]: 	Training 200/553. train loss: 0.0229,	0.8408 s / batch. (data: 2.27e-02). ETA=12:44:23, max mem: 20.9 GB 
[11/24 10:38:19 visual_prompt]: 	Training 300/553. train loss: 0.7115,	1.2040 s / batch. (data: 3.43e-01). ETA=18:12:34, max mem: 20.9 GB 
[11/24 10:40:01 visual_prompt]: 	Training 400/553. train loss: 1.0806,	0.8540 s / batch. (data: 9.96e-03). ETA=12:53:31, max mem: 20.9 GB 
[11/24 10:41:44 visual_prompt]: 	Training 500/553. train loss: 0.6006,	0.8320 s / batch. (data: 4.55e-04). ETA=12:32:10, max mem: 20.9 GB 
[11/24 10:42:37 visual_prompt]: Epoch 2 / 100: avg data time: 2.07e-01, avg batch time: 1.0341, average train loss: 0.9692
[11/24 10:43:36 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3074, average loss: 1.2669
[11/24 10:43:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.96	
[11/24 10:43:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/24 10:45:21 visual_prompt]: 	Training 100/553. train loss: 1.2298,	0.9114 s / batch. (data: 8.14e-02). ETA=13:41:43, max mem: 20.9 GB 
[11/24 10:47:06 visual_prompt]: 	Training 200/553. train loss: 0.9757,	1.5932 s / batch. (data: 7.45e-01). ETA=23:53:41, max mem: 20.9 GB 
[11/24 10:48:48 visual_prompt]: 	Training 300/553. train loss: 0.7386,	0.8280 s / batch. (data: 7.96e-03). ETA=12:23:43, max mem: 20.9 GB 
[11/24 10:50:31 visual_prompt]: 	Training 400/553. train loss: 3.2934,	0.8252 s / batch. (data: 3.22e-04). ETA=12:19:51, max mem: 20.9 GB 
[11/24 10:52:15 visual_prompt]: 	Training 500/553. train loss: 0.6973,	1.3080 s / batch. (data: 4.80e-01). ETA=19:30:31, max mem: 20.9 GB 
[11/24 10:53:07 visual_prompt]: Epoch 3 / 100: avg data time: 2.05e-01, avg batch time: 1.0324, average train loss: 0.9728
[11/24 10:54:06 visual_prompt]: Inference (val):avg data time: 4.58e-05, avg batch time: 0.3081, average loss: 0.8207
[11/24 10:54:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.52	
[11/24 10:54:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/24 10:55:55 visual_prompt]: 	Training 100/553. train loss: 0.7498,	0.8400 s / batch. (data: 7.97e-03). ETA=12:29:34, max mem: 20.9 GB 
[11/24 10:57:37 visual_prompt]: 	Training 200/553. train loss: 0.6275,	0.8360 s / batch. (data: 3.35e-04). ETA=12:24:35, max mem: 20.9 GB 
[11/24 10:59:21 visual_prompt]: 	Training 300/553. train loss: 0.6057,	1.2633 s / batch. (data: 4.27e-01). ETA=18:43:07, max mem: 20.9 GB 
[11/24 11:00:59 visual_prompt]: 	Training 400/553. train loss: 0.5973,	1.2760 s / batch. (data: 4.54e-01). ETA=18:52:14, max mem: 20.9 GB 
[11/24 11:02:44 visual_prompt]: 	Training 500/553. train loss: 0.3909,	3.6055 s / batch. (data: 2.80e+00). ETA=2 days, 5:13:21, max mem: 20.9 GB 
[11/24 11:03:39 visual_prompt]: Epoch 4 / 100: avg data time: 2.08e-01, avg batch time: 1.0349, average train loss: 1.0162
[11/24 11:04:38 visual_prompt]: Inference (val):avg data time: 2.25e-04, avg batch time: 0.3082, average loss: 0.6778
[11/24 11:04:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 58.80	
[11/24 11:04:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/24 11:06:24 visual_prompt]: 	Training 100/553. train loss: 3.2099,	0.8280 s / batch. (data: 3.10e-04). ETA=12:11:14, max mem: 20.9 GB 
[11/24 11:08:07 visual_prompt]: 	Training 200/553. train loss: 0.9763,	1.1360 s / batch. (data: 3.14e-01). ETA=16:41:19, max mem: 20.9 GB 
[11/24 11:09:50 visual_prompt]: 	Training 300/553. train loss: 2.7767,	0.8280 s / batch. (data: 3.35e-04). ETA=12:08:26, max mem: 20.9 GB 
[11/24 11:11:32 visual_prompt]: 	Training 400/553. train loss: 2.3731,	0.8400 s / batch. (data: 7.98e-03). ETA=12:17:38, max mem: 20.9 GB 
[11/24 11:13:15 visual_prompt]: 	Training 500/553. train loss: 0.6466,	0.8320 s / batch. (data: 3.40e-04). ETA=12:09:11, max mem: 20.9 GB 
[11/24 11:14:09 visual_prompt]: Epoch 5 / 100: avg data time: 2.06e-01, avg batch time: 1.0324, average train loss: 1.1417
[11/24 11:15:08 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.3078, average loss: 2.2109
[11/24 11:15:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.25	
[11/24 11:15:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/24 11:16:57 visual_prompt]: 	Training 100/553. train loss: 1.4888,	0.8480 s / batch. (data: 8.63e-04). ETA=12:21:04, max mem: 20.9 GB 
[11/24 11:18:39 visual_prompt]: 	Training 200/553. train loss: 2.5385,	0.8203 s / batch. (data: 3.05e-04). ETA=11:55:31, max mem: 20.9 GB 
[11/24 11:20:21 visual_prompt]: 	Training 300/553. train loss: 2.6572,	0.8248 s / batch. (data: 3.14e-04). ETA=11:58:02, max mem: 20.9 GB 
[11/24 11:22:07 visual_prompt]: 	Training 400/553. train loss: 2.0656,	0.8240 s / batch. (data: 3.84e-04). ETA=11:55:59, max mem: 20.9 GB 
[11/24 11:23:48 visual_prompt]: 	Training 500/553. train loss: 2.2061,	0.8227 s / batch. (data: 3.41e-04). ETA=11:53:28, max mem: 20.9 GB 
[11/24 11:24:42 visual_prompt]: Epoch 6 / 100: avg data time: 2.09e-01, avg batch time: 1.0371, average train loss: 1.4448
[11/24 11:25:41 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3084, average loss: 0.7829
[11/24 11:25:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.25	
[11/24 11:25:41 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/24 11:27:27 visual_prompt]: 	Training 100/553. train loss: 2.1696,	0.8202 s / batch. (data: 3.62e-04). ETA=11:49:13, max mem: 20.9 GB 
[11/24 11:29:09 visual_prompt]: 	Training 200/553. train loss: 0.5902,	1.4607 s / batch. (data: 6.23e-01). ETA=21:00:37, max mem: 20.9 GB 
[11/24 11:30:56 visual_prompt]: 	Training 300/553. train loss: 1.1093,	2.1569 s / batch. (data: 1.35e+00). ETA=1 day, 6:57:50, max mem: 20.9 GB 
[11/24 11:32:39 visual_prompt]: 	Training 400/553. train loss: 1.1678,	1.9593 s / batch. (data: 1.14e+00). ETA=1 day, 4:04:27, max mem: 20.9 GB 
[11/24 11:34:20 visual_prompt]: 	Training 500/553. train loss: 0.6711,	0.8451 s / batch. (data: 3.17e-04). ETA=12:05:08, max mem: 20.9 GB 
[11/24 11:35:12 visual_prompt]: Epoch 7 / 100: avg data time: 2.05e-01, avg batch time: 1.0314, average train loss: 1.4688
[11/24 11:36:11 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.3070, average loss: 0.9110
[11/24 11:36:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.41	
[11/24 11:36:11 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/24 11:37:56 visual_prompt]: 	Training 100/553. train loss: 3.7135,	0.8440 s / batch. (data: 3.29e-04). ETA=12:02:03, max mem: 20.9 GB 
[11/24 11:39:40 visual_prompt]: 	Training 200/553. train loss: 0.6631,	0.8598 s / batch. (data: 1.65e-02). ETA=12:14:05, max mem: 20.9 GB 
[11/24 11:41:23 visual_prompt]: 	Training 300/553. train loss: 1.7093,	0.8408 s / batch. (data: 1.05e-02). ETA=11:56:27, max mem: 20.9 GB 
[11/24 11:43:06 visual_prompt]: 	Training 400/553. train loss: 3.7947,	0.8417 s / batch. (data: 9.70e-03). ETA=11:55:52, max mem: 20.9 GB 
[11/24 11:44:49 visual_prompt]: 	Training 500/553. train loss: 4.6051,	1.5882 s / batch. (data: 7.80e-01). ETA=22:28:04, max mem: 20.9 GB 
[11/24 11:45:43 visual_prompt]: Epoch 8 / 100: avg data time: 2.09e-01, avg batch time: 1.0341, average train loss: 1.9594
[11/24 11:46:42 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.3080, average loss: 0.8549
[11/24 11:46:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/24 11:46:42 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/24 11:48:29 visual_prompt]: 	Training 100/553. train loss: 0.9613,	0.8308 s / batch. (data: 3.09e-04). ETA=11:43:05, max mem: 20.9 GB 
[11/24 11:50:10 visual_prompt]: 	Training 200/553. train loss: 0.5460,	0.8172 s / batch. (data: 3.25e-04). ETA=11:30:12, max mem: 20.9 GB 
[11/24 11:51:53 visual_prompt]: 	Training 300/553. train loss: 0.7530,	1.9188 s / batch. (data: 1.08e+00). ETA=1 day, 2:57:27, max mem: 20.9 GB 
[11/24 11:53:37 visual_prompt]: 	Training 400/553. train loss: 0.6412,	0.8240 s / batch. (data: 3.25e-04). ETA=11:33:11, max mem: 20.9 GB 
[11/24 11:55:20 visual_prompt]: 	Training 500/553. train loss: 0.6179,	0.8218 s / batch. (data: 3.57e-04). ETA=11:30:00, max mem: 20.9 GB 
[11/24 11:56:13 visual_prompt]: Epoch 9 / 100: avg data time: 2.08e-01, avg batch time: 1.0326, average train loss: 1.6057
[11/24 11:57:12 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3068, average loss: 0.8593
[11/24 11:57:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.55	
[11/24 11:57:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/24 11:59:02 visual_prompt]: 	Training 100/553. train loss: 4.1423,	0.8320 s / batch. (data: 3.34e-04). ETA=11:36:25, max mem: 20.9 GB 
[11/24 12:00:44 visual_prompt]: 	Training 200/553. train loss: 0.9189,	0.8362 s / batch. (data: 7.91e-03). ETA=11:38:31, max mem: 20.9 GB 
[11/24 12:02:26 visual_prompt]: 	Training 300/553. train loss: 2.8276,	0.8264 s / batch. (data: 5.45e-03). ETA=11:28:57, max mem: 20.9 GB 
[11/24 12:04:05 visual_prompt]: 	Training 400/553. train loss: 1.9134,	0.8960 s / batch. (data: 5.38e-02). ETA=12:25:30, max mem: 20.9 GB 
[11/24 12:05:50 visual_prompt]: 	Training 500/553. train loss: 0.9033,	1.1444 s / batch. (data: 3.25e-01). ETA=15:50:15, max mem: 20.9 GB 
[11/24 12:06:44 visual_prompt]: Epoch 10 / 100: avg data time: 2.08e-01, avg batch time: 1.0344, average train loss: 2.1337
[11/24 12:07:43 visual_prompt]: Inference (val):avg data time: 4.93e-05, avg batch time: 0.3088, average loss: 1.3788
[11/24 12:07:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.90	
[11/24 12:07:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/24 12:09:32 visual_prompt]: 	Training 100/553. train loss: 2.5256,	0.8245 s / batch. (data: 3.27e-04). ETA=11:22:30, max mem: 20.9 GB 
[11/24 12:11:17 visual_prompt]: 	Training 200/553. train loss: 1.4719,	0.8320 s / batch. (data: 3.36e-04). ETA=11:27:21, max mem: 20.9 GB 
[11/24 12:12:59 visual_prompt]: 	Training 300/553. train loss: 0.0326,	1.7997 s / batch. (data: 9.71e-01). ETA=1 day, 0:43:53, max mem: 20.9 GB 
[11/24 12:14:40 visual_prompt]: 	Training 400/553. train loss: 1.0791,	0.8520 s / batch. (data: 7.97e-03). ETA=11:41:03, max mem: 20.9 GB 
[11/24 12:16:21 visual_prompt]: 	Training 500/553. train loss: 2.7138,	0.8469 s / batch. (data: 3.11e-04). ETA=11:35:26, max mem: 20.9 GB 
[11/24 12:17:14 visual_prompt]: Epoch 11 / 100: avg data time: 2.05e-01, avg batch time: 1.0313, average train loss: 2.4819
[11/24 12:18:13 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3076, average loss: 1.0858
[11/24 12:18:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.37	
[11/24 12:18:13 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/24 12:20:01 visual_prompt]: 	Training 100/553. train loss: 0.6847,	0.8280 s / batch. (data: 3.56e-04). ETA=11:17:49, max mem: 20.9 GB 
[11/24 12:21:45 visual_prompt]: 	Training 200/553. train loss: 0.5447,	0.8359 s / batch. (data: 3.46e-03). ETA=11:22:54, max mem: 20.9 GB 
[11/24 12:23:27 visual_prompt]: 	Training 300/553. train loss: 1.9428,	0.8280 s / batch. (data: 3.34e-04). ETA=11:15:04, max mem: 20.9 GB 
[11/24 12:25:10 visual_prompt]: 	Training 400/553. train loss: 1.3645,	0.8383 s / batch. (data: 9.94e-03). ETA=11:22:02, max mem: 20.9 GB 
[11/24 12:26:53 visual_prompt]: 	Training 500/553. train loss: 5.1667,	0.8480 s / batch. (data: 8.73e-04). ETA=11:28:31, max mem: 20.9 GB 
[11/24 12:27:45 visual_prompt]: Epoch 12 / 100: avg data time: 2.09e-01, avg batch time: 1.0345, average train loss: 1.8395
[11/24 12:28:44 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3076, average loss: 1.0636
[11/24 12:28:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.40	
[11/24 12:28:44 visual_prompt]: Best epoch 12: best metric: -1.064
[11/24 12:28:44 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/24 12:30:34 visual_prompt]: 	Training 100/553. train loss: 0.5006,	0.8345 s / batch. (data: 5.52e-03). ETA=11:15:25, max mem: 20.9 GB 
[11/24 12:32:13 visual_prompt]: 	Training 200/553. train loss: 0.7680,	0.8125 s / batch. (data: 3.28e-04). ETA=10:56:16, max mem: 20.9 GB 
[11/24 12:33:56 visual_prompt]: 	Training 300/553. train loss: 0.7476,	1.6400 s / batch. (data: 8.01e-01). ETA=22:01:56, max mem: 20.9 GB 
[11/24 12:35:38 visual_prompt]: 	Training 400/553. train loss: 1.3625,	0.8163 s / batch. (data: 3.32e-04). ETA=10:56:38, max mem: 20.9 GB 
[11/24 12:37:22 visual_prompt]: 	Training 500/553. train loss: 3.0766,	0.8120 s / batch. (data: 3.19e-04). ETA=10:51:48, max mem: 20.9 GB 
[11/24 12:38:15 visual_prompt]: Epoch 13 / 100: avg data time: 2.07e-01, avg batch time: 1.0321, average train loss: 2.0376
[11/24 12:39:14 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.3072, average loss: 0.8844
[11/24 12:39:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.78	
[11/24 12:39:14 visual_prompt]: Best epoch 13: best metric: -0.884
[11/24 12:39:14 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/24 12:41:02 visual_prompt]: 	Training 100/553. train loss: 8.4550,	0.8360 s / batch. (data: 7.96e-03). ETA=11:08:57, max mem: 20.9 GB 
[11/24 12:42:45 visual_prompt]: 	Training 200/553. train loss: 0.1337,	1.2891 s / batch. (data: 4.65e-01). ETA=17:09:24, max mem: 20.9 GB 
[11/24 12:44:28 visual_prompt]: 	Training 300/553. train loss: 0.7719,	0.8104 s / batch. (data: 3.62e-04). ETA=10:45:43, max mem: 20.9 GB 
[11/24 12:46:10 visual_prompt]: 	Training 400/553. train loss: 0.6683,	0.8490 s / batch. (data: 2.90e-02). ETA=11:15:07, max mem: 20.9 GB 
[11/24 12:47:53 visual_prompt]: 	Training 500/553. train loss: 3.9133,	0.8160 s / batch. (data: 3.34e-04). ETA=10:47:31, max mem: 20.9 GB 
[11/24 12:48:46 visual_prompt]: Epoch 14 / 100: avg data time: 2.07e-01, avg batch time: 1.0336, average train loss: 1.7534
[11/24 12:49:45 visual_prompt]: Inference (val):avg data time: 1.27e-04, avg batch time: 0.3074, average loss: 1.6784
[11/24 12:49:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.75	
[11/24 12:49:45 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/24 12:51:32 visual_prompt]: 	Training 100/553. train loss: 0.9750,	1.3869 s / batch. (data: 5.68e-01). ETA=18:16:59, max mem: 20.9 GB 
[11/24 12:53:14 visual_prompt]: 	Training 200/553. train loss: 12.8571,	0.8280 s / batch. (data: 3.06e-04). ETA=10:53:32, max mem: 20.9 GB 
[11/24 12:54:58 visual_prompt]: 	Training 300/553. train loss: 1.9245,	0.8800 s / batch. (data: 1.17e-03). ETA=11:33:06, max mem: 20.9 GB 
[11/24 12:56:38 visual_prompt]: 	Training 400/553. train loss: 0.6197,	0.8120 s / batch. (data: 3.62e-04). ETA=10:38:11, max mem: 20.9 GB 
[11/24 12:58:22 visual_prompt]: 	Training 500/553. train loss: 1.6630,	0.8162 s / batch. (data: 5.46e-03). ETA=10:40:08, max mem: 20.9 GB 
[11/24 12:59:16 visual_prompt]: Epoch 15 / 100: avg data time: 2.07e-01, avg batch time: 1.0327, average train loss: 2.6104
[11/24 13:00:15 visual_prompt]: Inference (val):avg data time: 4.64e-05, avg batch time: 0.3058, average loss: 2.2748
[11/24 13:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.09	
[11/24 13:00:15 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/24 13:02:01 visual_prompt]: 	Training 100/553. train loss: 0.5710,	0.8320 s / batch. (data: 4.28e-04). ETA=10:50:23, max mem: 20.9 GB 
[11/24 13:03:44 visual_prompt]: 	Training 200/553. train loss: 2.6514,	0.8277 s / batch. (data: 5.27e-04). ETA=10:45:41, max mem: 20.9 GB 
[11/24 13:05:27 visual_prompt]: 	Training 300/553. train loss: 5.6168,	0.8120 s / batch. (data: 3.28e-04). ETA=10:32:06, max mem: 20.9 GB 
[11/24 13:07:10 visual_prompt]: 	Training 400/553. train loss: 4.9681,	0.8238 s / batch. (data: 3.34e-04). ETA=10:39:52, max mem: 20.9 GB 
[11/24 13:08:52 visual_prompt]: 	Training 500/553. train loss: 0.8915,	1.3435 s / batch. (data: 5.33e-01). ETA=17:21:18, max mem: 20.9 GB 
[11/24 13:09:46 visual_prompt]: Epoch 16 / 100: avg data time: 2.06e-01, avg batch time: 1.0323, average train loss: 2.6295
[11/24 13:10:45 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.3080, average loss: 0.7354
[11/24 13:10:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.72	
[11/24 13:10:45 visual_prompt]: Best epoch 16: best metric: -0.735
[11/24 13:10:45 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/24 13:12:32 visual_prompt]: 	Training 100/553. train loss: 0.5410,	0.8253 s / batch. (data: 3.37e-04). ETA=10:37:35, max mem: 20.9 GB 
[11/24 13:14:16 visual_prompt]: 	Training 200/553. train loss: 0.8182,	0.8200 s / batch. (data: 3.53e-04). ETA=10:32:05, max mem: 20.9 GB 
[11/24 13:15:58 visual_prompt]: 	Training 300/553. train loss: 1.0049,	0.8531 s / batch. (data: 3.31e-02). ETA=10:56:10, max mem: 20.9 GB 
[11/24 13:17:40 visual_prompt]: 	Training 400/553. train loss: 6.2289,	1.3320 s / batch. (data: 5.11e-01). ETA=17:02:19, max mem: 20.9 GB 
[11/24 13:19:23 visual_prompt]: 	Training 500/553. train loss: 4.5001,	1.6909 s / batch. (data: 8.65e-01). ETA=21:35:01, max mem: 20.9 GB 
[11/24 13:20:17 visual_prompt]: Epoch 17 / 100: avg data time: 2.08e-01, avg batch time: 1.0333, average train loss: 2.3484
[11/24 13:21:16 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.3062, average loss: 3.8605
[11/24 13:21:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/24 13:21:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/24 13:23:04 visual_prompt]: 	Training 100/553. train loss: 5.0259,	0.8560 s / batch. (data: 8.92e-04). ETA=10:53:23, max mem: 20.9 GB 
[11/24 13:24:49 visual_prompt]: 	Training 200/553. train loss: 16.7930,	0.8329 s / batch. (data: 1.05e-02). ETA=10:34:23, max mem: 20.9 GB 
[11/24 13:26:32 visual_prompt]: 	Training 300/553. train loss: 0.7122,	0.8280 s / batch. (data: 3.31e-04). ETA=10:29:16, max mem: 20.9 GB 
[11/24 13:28:13 visual_prompt]: 	Training 400/553. train loss: 1.0092,	0.8440 s / batch. (data: 3.13e-04). ETA=10:40:00, max mem: 20.9 GB 
[11/24 13:29:55 visual_prompt]: 	Training 500/553. train loss: 0.7725,	0.8248 s / batch. (data: 1.06e-02). ETA=10:24:06, max mem: 20.9 GB 
[11/24 13:30:48 visual_prompt]: Epoch 18 / 100: avg data time: 2.08e-01, avg batch time: 1.0334, average train loss: 3.1650
[11/24 13:31:47 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3074, average loss: 0.9248
[11/24 13:31:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/24 13:31:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/24 13:33:33 visual_prompt]: 	Training 100/553. train loss: 0.6262,	0.9920 s / batch. (data: 1.41e-01). ETA=12:28:04, max mem: 20.9 GB 
[11/24 13:35:17 visual_prompt]: 	Training 200/553. train loss: 1.7280,	0.8280 s / batch. (data: 3.18e-04). ETA=10:23:00, max mem: 20.9 GB 
[11/24 13:37:00 visual_prompt]: 	Training 300/553. train loss: 5.0892,	0.8280 s / batch. (data: 7.97e-03). ETA=10:21:37, max mem: 20.9 GB 
[11/24 13:38:43 visual_prompt]: 	Training 400/553. train loss: 1.3262,	0.8610 s / batch. (data: 9.29e-04). ETA=10:44:58, max mem: 20.9 GB 
[11/24 13:40:23 visual_prompt]: 	Training 500/553. train loss: 1.1437,	0.8180 s / batch. (data: 3.55e-04). ETA=10:11:22, max mem: 20.9 GB 
[11/24 13:41:16 visual_prompt]: Epoch 19 / 100: avg data time: 2.03e-01, avg batch time: 1.0290, average train loss: 1.7645
[11/24 13:42:15 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.3076, average loss: 3.4530
[11/24 13:42:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.46	
[11/24 13:42:15 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/24 13:44:01 visual_prompt]: 	Training 100/553. train loss: 0.5834,	0.8320 s / batch. (data: 3.39e-04). ETA=10:19:44, max mem: 20.9 GB 
[11/24 13:45:45 visual_prompt]: 	Training 200/553. train loss: 0.6940,	0.8526 s / batch. (data: 1.06e-02). ETA=10:33:39, max mem: 20.9 GB 
[11/24 13:47:27 visual_prompt]: 	Training 300/553. train loss: 1.6089,	0.8214 s / batch. (data: 3.97e-03). ETA=10:09:06, max mem: 20.9 GB 
[11/24 13:49:10 visual_prompt]: 	Training 400/553. train loss: 0.6775,	0.8440 s / batch. (data: 3.34e-04). ETA=10:24:28, max mem: 20.9 GB 
[11/24 13:50:52 visual_prompt]: 	Training 500/553. train loss: 1.3922,	0.8386 s / batch. (data: 7.95e-03). ETA=10:19:02, max mem: 20.9 GB 
[11/24 13:51:47 visual_prompt]: Epoch 20 / 100: avg data time: 2.07e-01, avg batch time: 1.0339, average train loss: 2.1300
[11/24 13:52:46 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3068, average loss: 0.7118
[11/24 13:52:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.56	
[11/24 13:52:46 visual_prompt]: Best epoch 20: best metric: -0.712
[11/24 13:52:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[11/24 13:54:35 visual_prompt]: 	Training 100/553. train loss: 0.5810,	1.1680 s / batch. (data: 3.39e-01). ETA=14:19:15, max mem: 20.9 GB 
[11/24 13:56:17 visual_prompt]: 	Training 200/553. train loss: 6.4126,	0.8100 s / batch. (data: 2.99e-04). ETA=9:54:33, max mem: 20.9 GB 
[11/24 13:58:01 visual_prompt]: 	Training 300/553. train loss: 0.8785,	1.1280 s / batch. (data: 3.00e-01). ETA=13:46:03, max mem: 20.9 GB 
[11/24 13:59:41 visual_prompt]: 	Training 400/553. train loss: 3.0139,	0.8401 s / batch. (data: 3.25e-04). ETA=10:13:49, max mem: 20.9 GB 
[11/24 14:01:25 visual_prompt]: 	Training 500/553. train loss: 2.0082,	0.8207 s / batch. (data: 3.91e-04). ETA=9:58:18, max mem: 20.9 GB 
[11/24 14:02:18 visual_prompt]: Epoch 21 / 100: avg data time: 2.06e-01, avg batch time: 1.0330, average train loss: 2.1354
[11/24 14:03:17 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3091, average loss: 1.3491
[11/24 14:03:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.65	
[11/24 14:03:17 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[11/24 14:05:02 visual_prompt]: 	Training 100/553. train loss: 1.5063,	0.8560 s / batch. (data: 3.19e-04). ETA=10:21:51, max mem: 20.9 GB 
[11/24 14:06:45 visual_prompt]: 	Training 200/553. train loss: 0.6319,	0.8388 s / batch. (data: 7.96e-03). ETA=10:07:55, max mem: 20.9 GB 
[11/24 14:08:26 visual_prompt]: 	Training 300/553. train loss: 0.0657,	0.8360 s / batch. (data: 3.12e-04). ETA=10:04:30, max mem: 20.9 GB 
[11/24 14:10:10 visual_prompt]: 	Training 400/553. train loss: 10.1777,	0.8648 s / batch. (data: 2.49e-02). ETA=10:23:54, max mem: 20.9 GB 
[11/24 14:11:52 visual_prompt]: 	Training 500/553. train loss: 0.8846,	0.8360 s / batch. (data: 7.93e-03). ETA=10:01:42, max mem: 20.9 GB 
[11/24 14:12:47 visual_prompt]: Epoch 22 / 100: avg data time: 2.06e-01, avg batch time: 1.0319, average train loss: 1.8319
[11/24 14:13:47 visual_prompt]: Inference (val):avg data time: 2.75e-04, avg batch time: 0.3070, average loss: 3.1889
[11/24 14:13:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.64	
[11/24 14:13:47 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[11/24 14:15:35 visual_prompt]: 	Training 100/553. train loss: 1.2534,	0.8161 s / batch. (data: 3.29e-04). ETA=9:45:21, max mem: 20.9 GB 
[11/24 14:17:18 visual_prompt]: 	Training 200/553. train loss: 0.5634,	0.9075 s / batch. (data: 9.75e-02). ETA=10:49:20, max mem: 20.9 GB 
[11/24 14:19:02 visual_prompt]: 	Training 300/553. train loss: 0.5942,	0.8156 s / batch. (data: 5.57e-03). ETA=9:42:16, max mem: 20.9 GB 
[11/24 14:20:43 visual_prompt]: 	Training 400/553. train loss: 1.5386,	0.8359 s / batch. (data: 7.92e-03). ETA=9:55:22, max mem: 20.9 GB 
[11/24 14:22:24 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8198 s / batch. (data: 3.17e-04). ETA=9:42:30, max mem: 20.9 GB 
[11/24 14:23:17 visual_prompt]: Epoch 23 / 100: avg data time: 2.05e-01, avg batch time: 1.0312, average train loss: 2.2013
[11/24 14:24:16 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3098, average loss: 2.4110
[11/24 14:24:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.72	
[11/24 14:24:16 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[11/24 14:26:00 visual_prompt]: 	Training 100/553. train loss: 6.5537,	0.8348 s / batch. (data: 3.37e-04). ETA=9:51:01, max mem: 20.9 GB 
[11/24 14:27:42 visual_prompt]: 	Training 200/553. train loss: 0.6106,	0.8160 s / batch. (data: 3.02e-04). ETA=9:36:22, max mem: 20.9 GB 
[11/24 14:29:26 visual_prompt]: 	Training 300/553. train loss: 1.3971,	1.2033 s / batch. (data: 3.78e-01). ETA=14:07:58, max mem: 20.9 GB 
[11/24 14:31:09 visual_prompt]: 	Training 400/553. train loss: 0.6552,	0.8368 s / batch. (data: 3.40e-04). ETA=9:48:18, max mem: 20.9 GB 
[11/24 14:32:53 visual_prompt]: 	Training 500/553. train loss: 0.9322,	0.8315 s / batch. (data: 4.70e-04). ETA=9:43:08, max mem: 20.9 GB 
[11/24 14:33:48 visual_prompt]: Epoch 24 / 100: avg data time: 2.08e-01, avg batch time: 1.0337, average train loss: 2.2871
[11/24 14:34:47 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.3103, average loss: 0.7321
[11/24 14:34:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.61	
[11/24 14:34:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[11/24 14:36:37 visual_prompt]: 	Training 100/553. train loss: 2.2837,	0.8072 s / batch. (data: 3.28e-04). ETA=9:24:05, max mem: 20.9 GB 
[11/24 14:38:17 visual_prompt]: 	Training 200/553. train loss: 1.1692,	0.8201 s / batch. (data: 3.57e-04). ETA=9:31:41, max mem: 20.9 GB 
[11/24 14:39:59 visual_prompt]: 	Training 300/553. train loss: 1.5923,	0.8240 s / batch. (data: 7.95e-03). ETA=9:33:03, max mem: 20.9 GB 
[11/24 14:41:42 visual_prompt]: 	Training 400/553. train loss: 1.2355,	1.4320 s / batch. (data: 5.88e-01). ETA=16:33:31, max mem: 20.9 GB 
[11/24 14:43:25 visual_prompt]: 	Training 500/553. train loss: 0.7242,	1.6563 s / batch. (data: 8.46e-01). ETA=19:06:21, max mem: 20.9 GB 
[11/24 14:44:19 visual_prompt]: Epoch 25 / 100: avg data time: 2.08e-01, avg batch time: 1.0332, average train loss: 1.6836
[11/24 14:45:18 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.3079, average loss: 3.6299
[11/24 14:45:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.64	
[11/24 14:45:18 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[11/24 14:47:05 visual_prompt]: 	Training 100/553. train loss: 2.0128,	0.8284 s / batch. (data: 3.26e-04). ETA=9:31:16, max mem: 20.9 GB 
[11/24 14:48:49 visual_prompt]: 	Training 200/553. train loss: 11.8396,	1.7760 s / batch. (data: 9.47e-01). ETA=20:21:43, max mem: 20.9 GB 
[11/24 14:50:33 visual_prompt]: 	Training 300/553. train loss: 0.1117,	0.8681 s / batch. (data: 8.29e-04). ETA=9:55:43, max mem: 20.9 GB 
[11/24 14:52:14 visual_prompt]: 	Training 400/553. train loss: 0.5824,	0.8480 s / batch. (data: 4.71e-04). ETA=9:40:29, max mem: 20.9 GB 
[11/24 14:53:55 visual_prompt]: 	Training 500/553. train loss: 5.2843,	0.8244 s / batch. (data: 5.47e-03). ETA=9:22:57, max mem: 20.9 GB 
[11/24 14:54:48 visual_prompt]: Epoch 26 / 100: avg data time: 2.06e-01, avg batch time: 1.0319, average train loss: 2.4047
[11/24 14:55:48 visual_prompt]: Inference (val):avg data time: 4.69e-05, avg batch time: 0.3087, average loss: 0.8601
[11/24 14:55:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.71	
[11/24 14:55:48 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[11/24 14:57:36 visual_prompt]: 	Training 100/553. train loss: 0.6568,	0.8534 s / batch. (data: 3.17e-04). ETA=9:40:38, max mem: 20.9 GB 
[11/24 14:59:18 visual_prompt]: 	Training 200/553. train loss: 5.6375,	1.5400 s / batch. (data: 7.18e-01). ETA=17:25:11, max mem: 20.9 GB 
[11/24 15:01:00 visual_prompt]: 	Training 300/553. train loss: 2.6742,	0.8240 s / batch. (data: 5.49e-03). ETA=9:17:51, max mem: 20.9 GB 
[11/24 15:02:45 visual_prompt]: 	Training 400/553. train loss: 22.6630,	0.8317 s / batch. (data: 8.44e-04). ETA=9:21:41, max mem: 20.9 GB 
[11/24 15:04:28 visual_prompt]: 	Training 500/553. train loss: 0.7050,	0.8481 s / batch. (data: 8.37e-04). ETA=9:31:20, max mem: 20.9 GB 
[11/24 15:05:19 visual_prompt]: Epoch 27 / 100: avg data time: 2.06e-01, avg batch time: 1.0324, average train loss: 2.5300
[11/24 15:06:18 visual_prompt]: Inference (val):avg data time: 4.05e-04, avg batch time: 0.3073, average loss: 4.6716
[11/24 15:06:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.49	
[11/24 15:06:18 visual_prompt]: Stopping early.
[11/24 15:06:18 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 15:06:18 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 15:06:18 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/24 15:06:18 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 15:06:18 visual_prompt]: Training with config:
[11/24 15:06:18 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/24 15:06:18 visual_prompt]: Loading training data...
[11/24 15:06:18 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 15:06:18 visual_prompt]: Loading validation data...
[11/24 15:06:18 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 15:06:18 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 15:06:21 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 15:06:21 visual_prompt]: tuned percent:0.525
[11/24 15:06:21 visual_prompt]: Device used for model: 0
[11/24 15:06:21 visual_prompt]: Setting up Evaluator...
[11/24 15:06:21 visual_prompt]: Setting up Trainer...
[11/24 15:06:21 visual_prompt]: 	Setting up the optimizer...
[11/24 15:06:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 15:08:07 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8240 s / batch. (data: 3.10e-04). ETA=12:38:05, max mem: 20.9 GB 
[11/24 15:09:49 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8286 s / batch. (data: 7.97e-03). ETA=12:40:53, max mem: 20.9 GB 
[11/24 15:11:34 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.5437 s / batch. (data: 7.03e-01). ETA=23:35:04, max mem: 20.9 GB 
[11/24 15:13:14 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8400 s / batch. (data: 7.97e-03). ETA=12:48:36, max mem: 20.9 GB 
[11/24 15:15:00 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8553 s / batch. (data: 3.98e-02). ETA=13:01:09, max mem: 20.9 GB 
[11/24 15:15:54 visual_prompt]: Epoch 1 / 100: avg data time: 2.11e-01, avg batch time: 1.0358, average train loss: 1.5403
[11/24 15:16:53 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.3059, average loss: 1.5201
[11/24 15:16:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 15:16:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/24 15:18:39 visual_prompt]: 	Training 100/553. train loss: 0.7444,	1.4960 s / batch. (data: 6.65e-01). ETA=22:42:31, max mem: 20.9 GB 
[11/24 15:20:21 visual_prompt]: 	Training 200/553. train loss: 0.0235,	1.2320 s / batch. (data: 3.96e-01). ETA=18:40:00, max mem: 20.9 GB 
[11/24 15:22:05 visual_prompt]: 	Training 300/553. train loss: 0.7303,	1.1559 s / batch. (data: 3.33e-01). ETA=17:28:52, max mem: 20.9 GB 
[11/24 15:23:47 visual_prompt]: 	Training 400/553. train loss: 1.0344,	0.8210 s / batch. (data: 1.05e-02). ETA=12:23:38, max mem: 20.9 GB 
[11/24 15:25:31 visual_prompt]: 	Training 500/553. train loss: 0.6514,	0.8214 s / batch. (data: 1.06e-02). ETA=12:22:39, max mem: 20.9 GB 
[11/24 15:26:23 visual_prompt]: Epoch 2 / 100: avg data time: 2.06e-01, avg batch time: 1.0313, average train loss: 0.9801
[11/24 15:27:23 visual_prompt]: Inference (val):avg data time: 4.47e-05, avg batch time: 0.3085, average loss: 1.2716
[11/24 15:27:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.56	
[11/24 15:27:23 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/24 15:29:08 visual_prompt]: 	Training 100/553. train loss: 1.4377,	0.8480 s / batch. (data: 5.47e-03). ETA=12:44:29, max mem: 20.9 GB 
[11/24 15:30:52 visual_prompt]: 	Training 200/553. train loss: 0.8695,	1.7750 s / batch. (data: 9.38e-01). ETA=1 day, 2:37:17, max mem: 20.9 GB 
[11/24 15:32:34 visual_prompt]: 	Training 300/553. train loss: 0.6381,	0.8441 s / batch. (data: 3.21e-04). ETA=12:38:09, max mem: 20.9 GB 
[11/24 15:34:18 visual_prompt]: 	Training 400/553. train loss: 3.7800,	0.8440 s / batch. (data: 3.14e-04). ETA=12:36:41, max mem: 20.9 GB 
[11/24 15:36:01 visual_prompt]: 	Training 500/553. train loss: 0.7114,	1.3390 s / batch. (data: 5.17e-01). ETA=19:58:16, max mem: 20.9 GB 
[11/24 15:36:54 visual_prompt]: Epoch 3 / 100: avg data time: 2.06e-01, avg batch time: 1.0325, average train loss: 1.0415
[11/24 15:37:53 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.3086, average loss: 0.7156
[11/24 15:37:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 59.24	
[11/24 15:37:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/24 15:39:42 visual_prompt]: 	Training 100/553. train loss: 0.7622,	0.8082 s / batch. (data: 3.24e-04). ETA=12:01:10, max mem: 20.9 GB 
[11/24 15:41:25 visual_prompt]: 	Training 200/553. train loss: 0.5349,	0.8101 s / batch. (data: 3.35e-04). ETA=12:01:32, max mem: 20.9 GB 
[11/24 15:43:08 visual_prompt]: 	Training 300/553. train loss: 0.5132,	1.5363 s / batch. (data: 7.24e-01). ETA=22:45:46, max mem: 20.9 GB 
[11/24 15:44:46 visual_prompt]: 	Training 400/553. train loss: 1.0355,	0.9115 s / batch. (data: 8.38e-02). ETA=13:28:48, max mem: 20.9 GB 
[11/24 15:46:32 visual_prompt]: 	Training 500/553. train loss: 0.2275,	3.7784 s / batch. (data: 2.96e+00). ETA=2 days, 7:46:28, max mem: 20.9 GB 
[11/24 15:47:26 visual_prompt]: Epoch 4 / 100: avg data time: 2.09e-01, avg batch time: 1.0355, average train loss: 1.0871
[11/24 15:48:25 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3062, average loss: 1.6708
[11/24 15:48:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.13	
[11/24 15:48:25 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/24 15:50:11 visual_prompt]: 	Training 100/553. train loss: 2.6226,	0.8427 s / batch. (data: 1.05e-02). ETA=12:24:11, max mem: 20.9 GB 
[11/24 15:51:54 visual_prompt]: 	Training 200/553. train loss: 1.5608,	1.5451 s / batch. (data: 6.86e-01). ETA=22:41:57, max mem: 20.9 GB 
[11/24 15:53:39 visual_prompt]: 	Training 300/553. train loss: 1.2797,	0.8160 s / batch. (data: 3.18e-04). ETA=11:57:53, max mem: 20.9 GB 
[11/24 15:55:20 visual_prompt]: 	Training 400/553. train loss: 2.5061,	0.8240 s / batch. (data: 3.33e-04). ETA=12:03:36, max mem: 20.9 GB 
[11/24 15:57:04 visual_prompt]: 	Training 500/553. train loss: 1.0235,	0.8400 s / batch. (data: 7.97e-03). ETA=12:16:12, max mem: 20.9 GB 
[11/24 15:57:59 visual_prompt]: Epoch 5 / 100: avg data time: 2.10e-01, avg batch time: 1.0368, average train loss: 1.4804
[11/24 15:58:58 visual_prompt]: Inference (val):avg data time: 1.30e-04, avg batch time: 0.3075, average loss: 3.6272
[11/24 15:58:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.28	
[11/24 15:58:58 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/24 16:00:46 visual_prompt]: 	Training 100/553. train loss: 0.5887,	0.8241 s / batch. (data: 5.46e-03). ETA=12:00:11, max mem: 20.9 GB 
[11/24 16:02:27 visual_prompt]: 	Training 200/553. train loss: 3.9305,	0.8559 s / batch. (data: 7.77e-03). ETA=12:26:33, max mem: 20.9 GB 
[11/24 16:04:08 visual_prompt]: 	Training 300/553. train loss: 1.8119,	0.8202 s / batch. (data: 3.08e-04). ETA=11:54:03, max mem: 20.9 GB 
[11/24 16:05:54 visual_prompt]: 	Training 400/553. train loss: 1.6268,	0.8234 s / batch. (data: 4.27e-04). ETA=11:55:29, max mem: 20.9 GB 
[11/24 16:07:34 visual_prompt]: 	Training 500/553. train loss: 3.0176,	0.8235 s / batch. (data: 3.33e-04). ETA=11:54:13, max mem: 20.9 GB 
[11/24 16:08:27 visual_prompt]: Epoch 6 / 100: avg data time: 2.03e-01, avg batch time: 1.0294, average train loss: 1.3675
[11/24 16:09:26 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3071, average loss: 0.9233
[11/24 16:09:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.50	
[11/24 16:09:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/24 16:11:11 visual_prompt]: 	Training 100/553. train loss: 1.1389,	0.8098 s / batch. (data: 4.06e-04). ETA=11:40:11, max mem: 20.9 GB 
[11/24 16:12:52 visual_prompt]: 	Training 200/553. train loss: 0.5374,	0.8171 s / batch. (data: 3.20e-04). ETA=11:45:12, max mem: 20.9 GB 
[11/24 16:14:39 visual_prompt]: 	Training 300/553. train loss: 0.6987,	2.0863 s / batch. (data: 1.25e+00). ETA=1 day, 5:57:04, max mem: 20.9 GB 
[11/24 16:16:18 visual_prompt]: 	Training 400/553. train loss: 0.8380,	1.5316 s / batch. (data: 6.97e-01). ETA=21:56:41, max mem: 20.9 GB 
[11/24 16:17:58 visual_prompt]: 	Training 500/553. train loss: 1.3916,	0.8555 s / batch. (data: 1.14e-02). ETA=12:14:01, max mem: 20.9 GB 
[11/24 16:18:50 visual_prompt]: Epoch 7 / 100: avg data time: 1.96e-01, avg batch time: 1.0212, average train loss: 1.1268
[11/24 16:19:49 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3073, average loss: 0.6818
[11/24 16:19:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 61.98	
[11/24 16:19:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/24 16:21:32 visual_prompt]: 	Training 100/553. train loss: 2.6913,	0.8240 s / batch. (data: 3.18e-04). ETA=11:44:55, max mem: 20.9 GB 
[11/24 16:23:15 visual_prompt]: 	Training 200/553. train loss: 1.1695,	0.8105 s / batch. (data: 3.48e-04). ETA=11:32:01, max mem: 20.9 GB 
[11/24 16:24:57 visual_prompt]: 	Training 300/553. train loss: 0.5651,	0.8495 s / batch. (data: 2.15e-02). ETA=12:03:56, max mem: 20.9 GB 
[11/24 16:26:39 visual_prompt]: 	Training 400/553. train loss: 0.8290,	0.8880 s / batch. (data: 4.48e-02). ETA=12:35:14, max mem: 20.9 GB 
[11/24 16:28:21 visual_prompt]: 	Training 500/553. train loss: 1.5774,	1.4796 s / batch. (data: 6.62e-01). ETA=20:55:54, max mem: 20.9 GB 
[11/24 16:29:14 visual_prompt]: Epoch 8 / 100: avg data time: 1.95e-01, avg batch time: 1.0213, average train loss: 1.4651
[11/24 16:30:12 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.3062, average loss: 1.0596
[11/24 16:30:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.58	
[11/24 16:30:12 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/24 16:31:58 visual_prompt]: 	Training 100/553. train loss: 0.0075,	0.8400 s / batch. (data: 1.20e-02). ETA=11:50:51, max mem: 20.9 GB 
[11/24 16:33:39 visual_prompt]: 	Training 200/553. train loss: 0.5798,	0.8235 s / batch. (data: 3.12e-04). ETA=11:35:31, max mem: 20.9 GB 
[11/24 16:35:20 visual_prompt]: 	Training 300/553. train loss: 0.9115,	1.9459 s / batch. (data: 1.13e+00). ETA=1 day, 3:20:16, max mem: 20.9 GB 
[11/24 16:37:03 visual_prompt]: 	Training 400/553. train loss: 1.0908,	0.8425 s / batch. (data: 2.27e-02). ETA=11:48:47, max mem: 20.9 GB 
[11/24 16:38:44 visual_prompt]: 	Training 500/553. train loss: 0.8527,	0.8285 s / batch. (data: 1.19e-02). ETA=11:35:34, max mem: 20.9 GB 
[11/24 16:39:36 visual_prompt]: Epoch 9 / 100: avg data time: 1.93e-01, avg batch time: 1.0201, average train loss: 1.1514
[11/24 16:40:34 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3076, average loss: 0.8764
[11/24 16:40:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.72	
[11/24 16:40:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/24 16:42:23 visual_prompt]: 	Training 100/553. train loss: 2.6237,	0.8389 s / batch. (data: 5.49e-03). ETA=11:42:13, max mem: 20.9 GB 
[11/24 16:44:03 visual_prompt]: 	Training 200/553. train loss: 1.0801,	0.8273 s / batch. (data: 3.46e-04). ETA=11:31:08, max mem: 20.9 GB 
[11/24 16:45:43 visual_prompt]: 	Training 300/553. train loss: 0.8928,	1.0043 s / batch. (data: 1.61e-01). ETA=13:57:19, max mem: 20.9 GB 
[11/24 16:47:22 visual_prompt]: 	Training 400/553. train loss: 3.2987,	0.8560 s / batch. (data: 2.69e-02). ETA=11:52:12, max mem: 20.9 GB 
[11/24 16:49:05 visual_prompt]: 	Training 500/553. train loss: 0.9837,	0.8206 s / batch. (data: 3.39e-04). ETA=11:21:26, max mem: 20.9 GB 
[11/24 16:49:57 visual_prompt]: Epoch 10 / 100: avg data time: 1.92e-01, avg batch time: 1.0174, average train loss: 1.7488
[11/24 16:50:55 visual_prompt]: Inference (val):avg data time: 1.61e-04, avg batch time: 0.3091, average loss: 0.7217
[11/24 16:50:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.37	
[11/24 16:50:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/24 16:52:42 visual_prompt]: 	Training 100/553. train loss: 0.7583,	0.8280 s / batch. (data: 7.97e-03). ETA=11:25:27, max mem: 20.9 GB 
[11/24 16:54:26 visual_prompt]: 	Training 200/553. train loss: 0.4333,	0.8440 s / batch. (data: 7.97e-03). ETA=11:37:17, max mem: 20.9 GB 
[11/24 16:56:06 visual_prompt]: 	Training 300/553. train loss: 0.0232,	1.7922 s / batch. (data: 9.73e-01). ETA=1 day, 0:37:40, max mem: 20.9 GB 
[11/24 16:57:46 visual_prompt]: 	Training 400/553. train loss: 0.6679,	0.8092 s / batch. (data: 3.20e-04). ETA=11:05:48, max mem: 20.9 GB 
[11/24 16:59:26 visual_prompt]: 	Training 500/553. train loss: 0.6420,	0.8320 s / batch. (data: 2.98e-04). ETA=11:23:13, max mem: 20.9 GB 
[11/24 17:00:18 visual_prompt]: Epoch 11 / 100: avg data time: 1.91e-01, avg batch time: 1.0175, average train loss: 1.3496
[11/24 17:01:16 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.3082, average loss: 2.7328
[11/24 17:01:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.00	
[11/24 17:01:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/24 17:03:04 visual_prompt]: 	Training 100/553. train loss: 0.7005,	0.8078 s / batch. (data: 3.40e-04). ETA=11:01:15, max mem: 20.9 GB 
[11/24 17:04:46 visual_prompt]: 	Training 200/553. train loss: 0.5731,	0.8320 s / batch. (data: 1.20e-02). ETA=11:19:42, max mem: 20.9 GB 
[11/24 17:06:25 visual_prompt]: 	Training 300/553. train loss: 0.7164,	0.8299 s / batch. (data: 5.45e-03). ETA=11:16:37, max mem: 20.9 GB 
[11/24 17:08:07 visual_prompt]: 	Training 400/553. train loss: 1.8411,	0.8185 s / batch. (data: 3.32e-04). ETA=11:05:57, max mem: 20.9 GB 
[11/24 17:09:49 visual_prompt]: 	Training 500/553. train loss: 7.3812,	0.8120 s / batch. (data: 4.54e-04). ETA=10:59:17, max mem: 20.9 GB 
[11/24 17:10:41 visual_prompt]: Epoch 12 / 100: avg data time: 1.95e-01, avg batch time: 1.0204, average train loss: 1.7641
[11/24 17:11:39 visual_prompt]: Inference (val):avg data time: 2.25e-04, avg batch time: 0.3076, average loss: 5.1254
[11/24 17:11:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.50	
[11/24 17:11:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/24 17:13:26 visual_prompt]: 	Training 100/553. train loss: 1.0007,	0.8225 s / batch. (data: 3.36e-04). ETA=11:05:45, max mem: 20.9 GB 
[11/24 17:15:05 visual_prompt]: 	Training 200/553. train loss: 0.7884,	0.8127 s / batch. (data: 3.26e-04). ETA=10:56:26, max mem: 20.9 GB 
[11/24 17:16:48 visual_prompt]: 	Training 300/553. train loss: 1.5693,	1.9069 s / batch. (data: 1.09e+00). ETA=1 day, 1:37:02, max mem: 20.9 GB 
[11/24 17:18:28 visual_prompt]: 	Training 400/553. train loss: 2.8366,	0.8441 s / batch. (data: 7.97e-03). ETA=11:19:01, max mem: 20.9 GB 
[11/24 17:20:10 visual_prompt]: 	Training 500/553. train loss: 3.0108,	0.8125 s / batch. (data: 3.16e-04). ETA=10:52:11, max mem: 20.9 GB 
[11/24 17:21:03 visual_prompt]: Epoch 13 / 100: avg data time: 1.93e-01, avg batch time: 1.0196, average train loss: 1.9001
[11/24 17:22:01 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3079, average loss: 1.2067
[11/24 17:22:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.46	
[11/24 17:22:01 visual_prompt]: Best epoch 13: best metric: -1.207
[11/24 17:22:01 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/24 17:23:49 visual_prompt]: 	Training 100/553. train loss: 1.5523,	0.8400 s / batch. (data: 3.05e-04). ETA=11:12:09, max mem: 20.9 GB 
[11/24 17:25:31 visual_prompt]: 	Training 200/553. train loss: 0.0387,	1.3968 s / batch. (data: 5.87e-01). ETA=18:35:22, max mem: 20.9 GB 
[11/24 17:27:11 visual_prompt]: 	Training 300/553. train loss: 0.9315,	0.8400 s / batch. (data: 2.28e-02). ETA=11:09:21, max mem: 20.9 GB 
[11/24 17:28:53 visual_prompt]: 	Training 400/553. train loss: 0.7454,	0.8440 s / batch. (data: 7.98e-03). ETA=11:11:10, max mem: 20.9 GB 
[11/24 17:30:34 visual_prompt]: 	Training 500/553. train loss: 1.9779,	0.8288 s / batch. (data: 1.05e-02). ETA=10:57:40, max mem: 20.9 GB 
[11/24 17:31:27 visual_prompt]: Epoch 14 / 100: avg data time: 1.96e-01, avg batch time: 1.0220, average train loss: 1.2273
[11/24 17:32:25 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.3068, average loss: 0.7427
[11/24 17:32:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.20	
[11/24 17:32:25 visual_prompt]: Best epoch 14: best metric: -0.743
[11/24 17:32:25 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/24 17:34:10 visual_prompt]: 	Training 100/553. train loss: 1.0834,	0.8320 s / batch. (data: 3.58e-04). ETA=10:58:04, max mem: 20.9 GB 
[11/24 17:35:49 visual_prompt]: 	Training 200/553. train loss: 4.6378,	0.8170 s / batch. (data: 3.59e-04). ETA=10:44:52, max mem: 20.9 GB 
[11/24 17:37:31 visual_prompt]: 	Training 300/553. train loss: 4.5705,	0.8275 s / batch. (data: 3.21e-04). ETA=10:51:44, max mem: 20.9 GB 
[11/24 17:39:10 visual_prompt]: 	Training 400/553. train loss: 0.6600,	1.3071 s / batch. (data: 4.78e-01). ETA=17:07:20, max mem: 20.9 GB 
[11/24 17:40:53 visual_prompt]: 	Training 500/553. train loss: 1.6384,	0.8360 s / batch. (data: 3.44e-04). ETA=10:55:39, max mem: 20.9 GB 
[11/24 17:41:47 visual_prompt]: Epoch 15 / 100: avg data time: 1.89e-01, avg batch time: 1.0154, average train loss: 2.1151
[11/24 17:42:45 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.3076, average loss: 1.9401
[11/24 17:42:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.86	
[11/24 17:42:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/24 17:44:30 visual_prompt]: 	Training 100/553. train loss: 0.5644,	0.8163 s / batch. (data: 5.45e-03). ETA=10:38:06, max mem: 20.9 GB 
[11/24 17:46:13 visual_prompt]: 	Training 200/553. train loss: 2.3663,	0.8305 s / batch. (data: 5.27e-04). ETA=10:47:49, max mem: 20.9 GB 
[11/24 17:47:54 visual_prompt]: 	Training 300/553. train loss: 1.1997,	0.8374 s / batch. (data: 1.06e-02). ETA=10:51:51, max mem: 20.9 GB 
[11/24 17:49:36 visual_prompt]: 	Training 400/553. train loss: 1.6563,	0.8342 s / batch. (data: 1.05e-02). ETA=10:47:57, max mem: 20.9 GB 
[11/24 17:51:17 visual_prompt]: 	Training 500/553. train loss: 0.5822,	1.1528 s / batch. (data: 3.40e-01). ETA=14:53:28, max mem: 20.9 GB 
[11/24 17:52:11 visual_prompt]: Epoch 16 / 100: avg data time: 1.96e-01, avg batch time: 1.0229, average train loss: 1.4400
[11/24 17:53:10 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3085, average loss: 0.7450
[11/24 17:53:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.94	
[11/24 17:53:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/24 17:54:55 visual_prompt]: 	Training 100/553. train loss: 1.6136,	0.8263 s / batch. (data: 3.24e-04). ETA=10:38:20, max mem: 20.9 GB 
[11/24 17:56:39 visual_prompt]: 	Training 200/553. train loss: 3.2109,	0.8273 s / batch. (data: 3.34e-04). ETA=10:37:42, max mem: 20.9 GB 
[11/24 17:58:20 visual_prompt]: 	Training 300/553. train loss: 2.5880,	0.8360 s / batch. (data: 3.18e-04). ETA=10:43:03, max mem: 20.9 GB 
[11/24 18:00:02 visual_prompt]: 	Training 400/553. train loss: 0.9729,	1.3525 s / batch. (data: 5.35e-01). ETA=17:18:04, max mem: 20.9 GB 
[11/24 18:01:43 visual_prompt]: 	Training 500/553. train loss: 1.1869,	1.6062 s / batch. (data: 7.81e-01). ETA=20:30:10, max mem: 20.9 GB 
[11/24 18:02:37 visual_prompt]: Epoch 17 / 100: avg data time: 1.99e-01, avg batch time: 1.0263, average train loss: 1.4579
[11/24 18:03:36 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3085, average loss: 1.4424
[11/24 18:03:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.36	
[11/24 18:03:36 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/24 18:05:23 visual_prompt]: 	Training 100/553. train loss: 0.6193,	0.8226 s / batch. (data: 1.05e-02). ETA=10:27:56, max mem: 20.9 GB 
[11/24 18:07:07 visual_prompt]: 	Training 200/553. train loss: 1.5820,	0.8491 s / batch. (data: 5.98e-03). ETA=10:46:42, max mem: 20.9 GB 
[11/24 18:08:49 visual_prompt]: 	Training 300/553. train loss: 0.5427,	0.8240 s / batch. (data: 3.11e-04). ETA=10:26:14, max mem: 20.9 GB 
[11/24 18:10:31 visual_prompt]: 	Training 400/553. train loss: 1.4705,	0.8520 s / batch. (data: 7.96e-03). ETA=10:46:04, max mem: 20.9 GB 
[11/24 18:12:12 visual_prompt]: 	Training 500/553. train loss: 0.7507,	0.8400 s / batch. (data: 3.28e-04). ETA=10:35:34, max mem: 20.9 GB 
[11/24 18:13:04 visual_prompt]: Epoch 18 / 100: avg data time: 1.99e-01, avg batch time: 1.0257, average train loss: 1.3950
[11/24 18:14:02 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3066, average loss: 0.8686
[11/24 18:14:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.83	
[11/24 18:14:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/24 18:15:48 visual_prompt]: 	Training 100/553. train loss: 0.5782,	0.8346 s / batch. (data: 5.52e-03). ETA=10:29:20, max mem: 20.9 GB 
[11/24 18:17:30 visual_prompt]: 	Training 200/553. train loss: 0.7036,	0.8400 s / batch. (data: 7.95e-03). ETA=10:32:02, max mem: 20.9 GB 
[11/24 18:19:13 visual_prompt]: 	Training 300/553. train loss: 3.1288,	0.8157 s / batch. (data: 5.47e-03). ETA=10:12:22, max mem: 20.9 GB 
[11/24 18:20:56 visual_prompt]: 	Training 400/553. train loss: 0.5579,	0.8273 s / batch. (data: 1.56e-02). ETA=10:19:42, max mem: 20.9 GB 
[11/24 18:22:34 visual_prompt]: 	Training 500/553. train loss: 0.6502,	0.8240 s / batch. (data: 3.47e-04). ETA=10:15:53, max mem: 20.9 GB 
[11/24 18:23:27 visual_prompt]: Epoch 19 / 100: avg data time: 1.95e-01, avg batch time: 1.0212, average train loss: 1.2007
[11/24 18:24:26 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3074, average loss: 2.6943
[11/24 18:24:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.90	
[11/24 18:24:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/24 18:26:11 visual_prompt]: 	Training 100/553. train loss: 0.5632,	0.8260 s / batch. (data: 6.10e-03). ETA=10:15:14, max mem: 20.9 GB 
[11/24 18:27:53 visual_prompt]: 	Training 200/553. train loss: 0.8049,	0.8240 s / batch. (data: 3.39e-04). ETA=10:12:24, max mem: 20.9 GB 
[11/24 18:29:36 visual_prompt]: 	Training 300/553. train loss: 3.2648,	0.8159 s / batch. (data: 4.88e-04). ETA=10:05:00, max mem: 20.9 GB 
[11/24 18:31:17 visual_prompt]: 	Training 400/553. train loss: 0.8737,	0.8280 s / batch. (data: 3.72e-04). ETA=10:12:37, max mem: 20.9 GB 
[11/24 18:32:58 visual_prompt]: 	Training 500/553. train loss: 1.3796,	0.8441 s / batch. (data: 1.05e-02). ETA=10:23:06, max mem: 20.9 GB 
[11/24 18:33:53 visual_prompt]: Epoch 20 / 100: avg data time: 2.00e-01, avg batch time: 1.0254, average train loss: 1.4975
[11/24 18:34:52 visual_prompt]: Inference (val):avg data time: 1.01e-04, avg batch time: 0.3068, average loss: 0.8022
[11/24 18:34:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.56	
[11/24 18:34:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[11/24 18:36:40 visual_prompt]: 	Training 100/553. train loss: 1.7491,	0.8320 s / batch. (data: 3.82e-04). ETA=10:12:03, max mem: 20.9 GB 
[11/24 18:38:21 visual_prompt]: 	Training 200/553. train loss: 1.3258,	0.8240 s / batch. (data: 3.14e-04). ETA=10:04:48, max mem: 20.9 GB 
[11/24 18:40:03 visual_prompt]: 	Training 300/553. train loss: 1.9252,	1.2554 s / batch. (data: 4.28e-01). ETA=15:19:21, max mem: 20.9 GB 
[11/24 18:41:43 visual_prompt]: 	Training 400/553. train loss: 1.4301,	0.8697 s / batch. (data: 3.42e-04). ETA=10:35:29, max mem: 20.9 GB 
[11/24 18:43:27 visual_prompt]: 	Training 500/553. train loss: 0.6992,	0.8360 s / batch. (data: 3.66e-04). ETA=10:09:25, max mem: 20.9 GB 
[11/24 18:44:19 visual_prompt]: Epoch 21 / 100: avg data time: 2.01e-01, avg batch time: 1.0259, average train loss: 1.3935
[11/24 18:45:18 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3096, average loss: 1.4628
[11/24 18:45:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.11	
[11/24 18:45:18 visual_prompt]: Stopping early.
[11/24 18:45:18 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 18:45:18 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 18:45:18 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/24 18:45:18 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 18:45:18 visual_prompt]: Training with config:
[11/24 18:45:18 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/24 18:45:18 visual_prompt]: Loading training data...
[11/24 18:45:18 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 18:45:18 visual_prompt]: Loading validation data...
[11/24 18:45:18 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 18:45:18 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 18:45:25 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 18:45:25 visual_prompt]: tuned percent:0.525
[11/24 18:45:25 visual_prompt]: Device used for model: 0
[11/24 18:45:25 visual_prompt]: Setting up Evaluator...
[11/24 18:45:25 visual_prompt]: Setting up Trainer...
[11/24 18:45:25 visual_prompt]: 	Setting up the optimizer...
[11/24 18:45:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 18:47:11 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8643 s / batch. (data: 1.23e-02). ETA=13:15:09, max mem: 20.9 GB 
[11/24 18:48:51 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8132 s / batch. (data: 3.34e-04). ETA=12:26:47, max mem: 20.9 GB 
[11/24 18:50:35 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3367 s / batch. (data: 5.26e-01). ETA=20:25:21, max mem: 20.9 GB 
[11/24 18:52:16 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8282 s / batch. (data: 7.96e-03). ETA=12:37:50, max mem: 20.9 GB 
[11/24 18:54:00 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8365 s / batch. (data: 3.05e-04). ETA=12:44:00, max mem: 20.9 GB 
[11/24 18:54:54 visual_prompt]: Epoch 1 / 100: avg data time: 2.02e-01, avg batch time: 1.0291, average train loss: 1.5403
[11/24 18:55:53 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3083, average loss: 1.5201
[11/24 18:55:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 18:55:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/24 18:57:38 visual_prompt]: 	Training 100/553. train loss: 0.7442,	0.8320 s / batch. (data: 7.96e-03). ETA=12:37:45, max mem: 20.9 GB 
[11/24 18:59:20 visual_prompt]: 	Training 200/553. train loss: 0.0236,	0.8392 s / batch. (data: 5.47e-03). ETA=12:42:53, max mem: 20.9 GB 
[11/24 19:01:03 visual_prompt]: 	Training 300/553. train loss: 0.7331,	1.2041 s / batch. (data: 3.86e-01). ETA=18:12:37, max mem: 20.9 GB 
[11/24 19:02:43 visual_prompt]: 	Training 400/553. train loss: 1.0297,	0.8154 s / batch. (data: 5.45e-03). ETA=12:18:35, max mem: 20.9 GB 
[11/24 19:04:28 visual_prompt]: 	Training 500/553. train loss: 0.6588,	0.8318 s / batch. (data: 3.17e-04). ETA=12:32:00, max mem: 20.9 GB 
[11/24 19:05:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.99e-01, avg batch time: 1.0249, average train loss: 0.9814
[11/24 19:06:18 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3087, average loss: 1.2745
[11/24 19:06:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.69	
[11/24 19:06:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/24 19:08:03 visual_prompt]: 	Training 100/553. train loss: 1.4617,	0.8320 s / batch. (data: 3.15e-04). ETA=12:30:05, max mem: 20.9 GB 
[11/24 19:09:46 visual_prompt]: 	Training 200/553. train loss: 0.8717,	0.8704 s / batch. (data: 6.21e-02). ETA=13:03:14, max mem: 20.9 GB 
[11/24 19:11:27 visual_prompt]: 	Training 300/553. train loss: 0.6260,	0.8196 s / batch. (data: 7.97e-03). ETA=12:16:14, max mem: 20.9 GB 
[11/24 19:13:10 visual_prompt]: 	Training 400/553. train loss: 3.8072,	0.8389 s / batch. (data: 7.97e-03). ETA=12:32:10, max mem: 20.9 GB 
[11/24 19:14:52 visual_prompt]: 	Training 500/553. train loss: 0.7178,	1.2494 s / batch. (data: 4.41e-01). ETA=18:38:04, max mem: 20.9 GB 
[11/24 19:15:44 visual_prompt]: Epoch 3 / 100: avg data time: 1.97e-01, avg batch time: 1.0233, average train loss: 1.0506
[11/24 19:16:43 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3089, average loss: 0.7089
[11/24 19:16:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 59.41	
[11/24 19:16:43 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/24 19:18:30 visual_prompt]: 	Training 100/553. train loss: 0.7360,	0.8215 s / batch. (data: 3.12e-04). ETA=12:13:06, max mem: 20.9 GB 
[11/24 19:20:13 visual_prompt]: 	Training 200/553. train loss: 0.5707,	0.8640 s / batch. (data: 7.97e-03). ETA=12:49:32, max mem: 20.9 GB 
[11/24 19:21:55 visual_prompt]: 	Training 300/553. train loss: 0.6907,	1.6713 s / batch. (data: 8.65e-01). ETA=1 day, 0:45:51, max mem: 20.9 GB 
[11/24 19:23:32 visual_prompt]: 	Training 400/553. train loss: 0.9989,	1.3107 s / batch. (data: 4.59e-01). ETA=19:23:01, max mem: 20.9 GB 
[11/24 19:25:16 visual_prompt]: 	Training 500/553. train loss: 0.1562,	3.5760 s / batch. (data: 2.75e+00). ETA=2 days, 4:47:11, max mem: 20.9 GB 
[11/24 19:26:11 visual_prompt]: Epoch 4 / 100: avg data time: 2.01e-01, avg batch time: 1.0268, average train loss: 1.1074
[11/24 19:27:09 visual_prompt]: Inference (val):avg data time: 3.23e-04, avg batch time: 0.3072, average loss: 1.4242
[11/24 19:27:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.75	
[11/24 19:27:09 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/24 19:28:54 visual_prompt]: 	Training 100/553. train loss: 2.8956,	0.8411 s / batch. (data: 1.05e-02). ETA=12:22:47, max mem: 20.9 GB 
[11/24 19:30:36 visual_prompt]: 	Training 200/553. train loss: 1.7048,	1.3583 s / batch. (data: 5.50e-01). ETA=19:57:17, max mem: 20.9 GB 
[11/24 19:32:19 visual_prompt]: 	Training 300/553. train loss: 3.8031,	0.8204 s / batch. (data: 3.13e-04). ETA=12:01:45, max mem: 20.9 GB 
[11/24 19:34:01 visual_prompt]: 	Training 400/553. train loss: 2.9447,	0.8276 s / batch. (data: 3.23e-04). ETA=12:06:44, max mem: 20.9 GB 
[11/24 19:35:43 visual_prompt]: 	Training 500/553. train loss: 1.0125,	0.8201 s / batch. (data: 3.09e-04). ETA=11:58:49, max mem: 20.9 GB 
[11/24 19:36:37 visual_prompt]: Epoch 5 / 100: avg data time: 2.01e-01, avg batch time: 1.0268, average train loss: 1.3246
[11/24 19:37:36 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3065, average loss: 1.9187
[11/24 19:37:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.00	
[11/24 19:37:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/24 19:39:23 visual_prompt]: 	Training 100/553. train loss: 0.7886,	0.8240 s / batch. (data: 3.22e-04). ETA=12:00:05, max mem: 20.9 GB 
[11/24 19:41:05 visual_prompt]: 	Training 200/553. train loss: 4.9891,	0.8481 s / batch. (data: 5.46e-03). ETA=12:19:47, max mem: 20.9 GB 
[11/24 19:42:45 visual_prompt]: 	Training 300/553. train loss: 0.6830,	0.8362 s / batch. (data: 5.18e-04). ETA=12:07:57, max mem: 20.9 GB 
[11/24 19:44:31 visual_prompt]: 	Training 400/553. train loss: 0.6291,	0.8200 s / batch. (data: 4.23e-04). ETA=11:52:31, max mem: 20.9 GB 
[11/24 19:46:11 visual_prompt]: 	Training 500/553. train loss: 3.6114,	0.8162 s / batch. (data: 3.55e-04). ETA=11:47:53, max mem: 20.9 GB 
[11/24 19:47:04 visual_prompt]: Epoch 6 / 100: avg data time: 2.02e-01, avg batch time: 1.0280, average train loss: 1.4182
[11/24 19:48:03 visual_prompt]: Inference (val):avg data time: 1.65e-04, avg batch time: 0.3068, average loss: 1.3055
[11/24 19:48:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.08	
[11/24 19:48:03 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/24 19:49:47 visual_prompt]: 	Training 100/553. train loss: 1.0775,	0.8398 s / batch. (data: 7.78e-03). ETA=12:06:10, max mem: 20.9 GB 
[11/24 19:51:29 visual_prompt]: 	Training 200/553. train loss: 0.6079,	0.8440 s / batch. (data: 7.90e-03). ETA=12:08:23, max mem: 20.9 GB 
[11/24 19:53:14 visual_prompt]: 	Training 300/553. train loss: 0.7782,	2.1121 s / batch. (data: 1.28e+00). ETA=1 day, 6:19:19, max mem: 20.9 GB 
[11/24 19:54:56 visual_prompt]: 	Training 400/553. train loss: 0.8291,	2.1523 s / batch. (data: 1.34e+00). ETA=1 day, 6:50:20, max mem: 20.9 GB 
[11/24 19:56:36 visual_prompt]: 	Training 500/553. train loss: 1.8916,	0.8216 s / batch. (data: 1.20e-02). ETA=11:44:56, max mem: 20.9 GB 
[11/24 19:57:28 visual_prompt]: Epoch 7 / 100: avg data time: 1.95e-01, avg batch time: 1.0205, average train loss: 1.3133
[11/24 19:58:26 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3071, average loss: 0.6972
[11/24 19:58:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 64.01	
[11/24 19:58:26 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/24 20:00:11 visual_prompt]: 	Training 100/553. train loss: 3.1443,	0.8322 s / batch. (data: 5.49e-03). ETA=11:51:55, max mem: 20.9 GB 
[11/24 20:01:54 visual_prompt]: 	Training 200/553. train loss: 1.8945,	0.8225 s / batch. (data: 3.63e-04). ETA=11:42:14, max mem: 20.9 GB 
[11/24 20:03:37 visual_prompt]: 	Training 300/553. train loss: 3.2202,	0.8439 s / batch. (data: 1.24e-02). ETA=11:59:09, max mem: 20.9 GB 
[11/24 20:05:20 visual_prompt]: 	Training 400/553. train loss: 0.7549,	1.0203 s / batch. (data: 2.00e-01). ETA=14:27:45, max mem: 20.9 GB 
[11/24 20:07:02 visual_prompt]: 	Training 500/553. train loss: 2.4947,	1.6880 s / batch. (data: 8.70e-01). ETA=23:52:48, max mem: 20.9 GB 
[11/24 20:07:55 visual_prompt]: Epoch 8 / 100: avg data time: 2.03e-01, avg batch time: 1.0283, average train loss: 1.8896
[11/24 20:08:54 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3078, average loss: 1.0552
[11/24 20:08:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.93	
[11/24 20:08:54 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/24 20:10:40 visual_prompt]: 	Training 100/553. train loss: 0.0021,	0.8234 s / batch. (data: 1.53e-02). ETA=11:36:48, max mem: 20.9 GB 
[11/24 20:12:21 visual_prompt]: 	Training 200/553. train loss: 0.5775,	0.8292 s / batch. (data: 3.27e-04). ETA=11:40:19, max mem: 20.9 GB 
[11/24 20:14:03 visual_prompt]: 	Training 300/553. train loss: 1.3700,	1.9432 s / batch. (data: 1.12e+00). ETA=1 day, 3:17:57, max mem: 20.9 GB 
[11/24 20:15:47 visual_prompt]: 	Training 400/553. train loss: 1.1137,	0.8560 s / batch. (data: 8.14e-04). ETA=12:00:07, max mem: 20.9 GB 
[11/24 20:17:29 visual_prompt]: 	Training 500/553. train loss: 1.7083,	1.0560 s / batch. (data: 2.19e-01). ETA=14:46:38, max mem: 20.9 GB 
[11/24 20:18:22 visual_prompt]: Epoch 9 / 100: avg data time: 2.01e-01, avg batch time: 1.0268, average train loss: 1.4716
[11/24 20:19:20 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3076, average loss: 1.2125
[11/24 20:19:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.94	
[11/24 20:19:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/24 20:21:09 visual_prompt]: 	Training 100/553. train loss: 3.3886,	0.8305 s / batch. (data: 2.17e-02). ETA=11:35:12, max mem: 20.9 GB 
[11/24 20:22:50 visual_prompt]: 	Training 200/553. train loss: 0.6082,	0.8098 s / batch. (data: 4.72e-04). ETA=11:16:29, max mem: 20.9 GB 
[11/24 20:24:31 visual_prompt]: 	Training 300/553. train loss: 0.6965,	1.4247 s / batch. (data: 5.82e-01). ETA=19:47:48, max mem: 20.9 GB 
[11/24 20:26:11 visual_prompt]: 	Training 400/553. train loss: 1.5656,	0.8198 s / batch. (data: 7.72e-03). ETA=11:22:06, max mem: 20.9 GB 
[11/24 20:27:53 visual_prompt]: 	Training 500/553. train loss: 0.6589,	0.8280 s / batch. (data: 3.35e-04). ETA=11:27:35, max mem: 20.9 GB 
[11/24 20:28:46 visual_prompt]: Epoch 10 / 100: avg data time: 1.98e-01, avg batch time: 1.0227, average train loss: 2.1908
[11/24 20:29:44 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3068, average loss: 1.2408
[11/24 20:29:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.24	
[11/24 20:29:44 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/24 20:31:33 visual_prompt]: 	Training 100/553. train loss: 0.5043,	0.8287 s / batch. (data: 1.66e-02). ETA=11:26:02, max mem: 20.9 GB 
[11/24 20:33:17 visual_prompt]: 	Training 200/553. train loss: 1.3819,	0.8201 s / batch. (data: 3.29e-04). ETA=11:17:32, max mem: 20.9 GB 
[11/24 20:34:58 visual_prompt]: 	Training 300/553. train loss: 0.1835,	2.1474 s / batch. (data: 1.33e+00). ETA=1 day, 5:30:31, max mem: 20.9 GB 
[11/24 20:36:38 visual_prompt]: 	Training 400/553. train loss: 0.8650,	0.8212 s / batch. (data: 3.09e-04). ETA=11:15:42, max mem: 20.9 GB 
[11/24 20:38:19 visual_prompt]: 	Training 500/553. train loss: 1.4664,	0.8109 s / batch. (data: 3.40e-04). ETA=11:05:52, max mem: 20.9 GB 
[11/24 20:39:12 visual_prompt]: Epoch 11 / 100: avg data time: 2.00e-01, avg batch time: 1.0252, average train loss: 1.3125
[11/24 20:40:10 visual_prompt]: Inference (val):avg data time: 4.66e-05, avg batch time: 0.3069, average loss: 0.6698
[11/24 20:40:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 63.50	
[11/24 20:40:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/24 20:41:58 visual_prompt]: 	Training 100/553. train loss: 1.0039,	0.8400 s / batch. (data: 2.46e-02). ETA=11:27:38, max mem: 20.9 GB 
[11/24 20:43:41 visual_prompt]: 	Training 200/553. train loss: 1.3046,	0.8200 s / batch. (data: 3.40e-04). ETA=11:09:54, max mem: 20.9 GB 
[11/24 20:45:21 visual_prompt]: 	Training 300/553. train loss: 2.3753,	0.8260 s / batch. (data: 3.86e-03). ETA=11:13:25, max mem: 20.9 GB 
[11/24 20:47:04 visual_prompt]: 	Training 400/553. train loss: 2.2551,	0.8282 s / batch. (data: 3.25e-04). ETA=11:13:51, max mem: 20.9 GB 
[11/24 20:48:45 visual_prompt]: 	Training 500/553. train loss: 7.8036,	0.8280 s / batch. (data: 3.05e-04). ETA=11:12:16, max mem: 20.9 GB 
[11/24 20:49:37 visual_prompt]: Epoch 12 / 100: avg data time: 1.99e-01, avg batch time: 1.0249, average train loss: 1.5844
[11/24 20:50:35 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.3078, average loss: 4.4920
[11/24 20:50:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.22	
[11/24 20:50:35 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/24 20:52:23 visual_prompt]: 	Training 100/553. train loss: 0.9622,	0.8240 s / batch. (data: 3.07e-04). ETA=11:06:56, max mem: 20.9 GB 
[11/24 20:54:02 visual_prompt]: 	Training 200/553. train loss: 0.6140,	0.8279 s / batch. (data: 5.30e-04). ETA=11:08:45, max mem: 20.9 GB 
[11/24 20:55:45 visual_prompt]: 	Training 300/553. train loss: 0.6270,	1.8602 s / batch. (data: 1.04e+00). ETA=1 day, 0:59:26, max mem: 20.9 GB 
[11/24 20:57:26 visual_prompt]: 	Training 400/553. train loss: 0.5155,	0.8594 s / batch. (data: 2.34e-02). ETA=11:31:18, max mem: 20.9 GB 
[11/24 20:59:09 visual_prompt]: 	Training 500/553. train loss: 3.0772,	0.8280 s / batch. (data: 3.07e-04). ETA=11:04:40, max mem: 20.9 GB 
[11/24 21:00:01 visual_prompt]: Epoch 13 / 100: avg data time: 1.97e-01, avg batch time: 1.0234, average train loss: 1.5807
[11/24 21:01:00 visual_prompt]: Inference (val):avg data time: 4.56e-05, avg batch time: 0.3075, average loss: 0.7782
[11/24 21:01:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 62.49	
[11/24 21:01:00 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/24 21:02:47 visual_prompt]: 	Training 100/553. train loss: 0.9836,	0.8360 s / batch. (data: 7.95e-03). ETA=11:08:56, max mem: 20.9 GB 
[11/24 21:04:30 visual_prompt]: 	Training 200/553. train loss: 0.0590,	1.2440 s / batch. (data: 4.11e-01). ETA=16:33:21, max mem: 20.9 GB 
[11/24 21:06:12 visual_prompt]: 	Training 300/553. train loss: 0.4164,	0.8295 s / batch. (data: 3.66e-04). ETA=11:00:58, max mem: 20.9 GB 
[11/24 21:07:53 visual_prompt]: 	Training 400/553. train loss: 0.8832,	0.8285 s / batch. (data: 3.31e-04). ETA=10:58:46, max mem: 20.9 GB 
[11/24 21:09:35 visual_prompt]: 	Training 500/553. train loss: 4.5468,	0.8161 s / batch. (data: 3.12e-04). ETA=10:47:34, max mem: 20.9 GB 
[11/24 21:10:28 visual_prompt]: Epoch 14 / 100: avg data time: 2.00e-01, avg batch time: 1.0257, average train loss: 1.4852
[11/24 21:11:26 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3086, average loss: 0.9987
[11/24 21:11:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 66.16	
[11/24 21:11:26 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/24 21:13:12 visual_prompt]: 	Training 100/553. train loss: 1.3619,	0.8360 s / batch. (data: 7.95e-03). ETA=11:01:14, max mem: 20.9 GB 
[11/24 21:14:53 visual_prompt]: 	Training 200/553. train loss: 0.2890,	0.8360 s / batch. (data: 3.15e-04). ETA=10:59:51, max mem: 20.9 GB 
[11/24 21:16:36 visual_prompt]: 	Training 300/553. train loss: 1.6031,	0.8319 s / batch. (data: 3.44e-04). ETA=10:55:15, max mem: 20.9 GB 
[11/24 21:18:16 visual_prompt]: 	Training 400/553. train loss: 1.2904,	1.0921 s / batch. (data: 2.81e-01). ETA=14:18:20, max mem: 20.9 GB 
[11/24 21:19:59 visual_prompt]: 	Training 500/553. train loss: 0.8546,	0.8360 s / batch. (data: 3.42e-04). ETA=10:55:40, max mem: 20.9 GB 
[11/24 21:20:53 visual_prompt]: Epoch 15 / 100: avg data time: 1.98e-01, avg batch time: 1.0245, average train loss: 1.8137
[11/24 21:21:52 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3067, average loss: 1.5462
[11/24 21:21:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.27	
[11/24 21:21:52 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/24 21:23:37 visual_prompt]: 	Training 100/553. train loss: 1.7917,	0.8680 s / batch. (data: 1.20e-02). ETA=11:18:33, max mem: 20.9 GB 
[11/24 21:25:19 visual_prompt]: 	Training 200/553. train loss: 0.9200,	0.8245 s / batch. (data: 1.20e-02). ETA=10:43:13, max mem: 20.9 GB 
[11/24 21:27:01 visual_prompt]: 	Training 300/553. train loss: 2.8723,	0.8156 s / batch. (data: 3.29e-04). ETA=10:34:53, max mem: 20.9 GB 
[11/24 21:28:43 visual_prompt]: 	Training 400/553. train loss: 1.8306,	0.8356 s / batch. (data: 3.34e-04). ETA=10:49:04, max mem: 20.9 GB 
[11/24 21:30:24 visual_prompt]: 	Training 500/553. train loss: 0.5853,	0.8760 s / batch. (data: 6.44e-02). ETA=11:18:58, max mem: 20.9 GB 
[11/24 21:31:18 visual_prompt]: Epoch 16 / 100: avg data time: 1.98e-01, avg batch time: 1.0233, average train loss: 1.3513
[11/24 21:32:17 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3103, average loss: 0.8820
[11/24 21:32:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.44	
[11/24 21:32:17 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/24 21:34:02 visual_prompt]: 	Training 100/553. train loss: 0.9979,	0.8192 s / batch. (data: 3.33e-04). ETA=10:32:50, max mem: 20.9 GB 
[11/24 21:35:45 visual_prompt]: 	Training 200/553. train loss: 5.0759,	0.8107 s / batch. (data: 3.63e-04). ETA=10:24:55, max mem: 20.9 GB 
[11/24 21:37:27 visual_prompt]: 	Training 300/553. train loss: 1.2097,	0.8207 s / batch. (data: 3.23e-04). ETA=10:31:16, max mem: 20.9 GB 
[11/24 21:39:08 visual_prompt]: 	Training 400/553. train loss: 0.6966,	1.3403 s / batch. (data: 5.26e-01). ETA=17:08:44, max mem: 20.9 GB 
[11/24 21:40:49 visual_prompt]: 	Training 500/553. train loss: 0.9693,	1.7360 s / batch. (data: 8.96e-01). ETA=22:09:32, max mem: 20.9 GB 
[11/24 21:41:44 visual_prompt]: Epoch 17 / 100: avg data time: 2.00e-01, avg batch time: 1.0257, average train loss: 1.5792
[11/24 21:42:42 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.3064, average loss: 0.6952
[11/24 21:42:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 69.91	
[11/24 21:42:42 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/24 21:44:29 visual_prompt]: 	Training 100/553. train loss: 0.2510,	0.8169 s / batch. (data: 3.26e-04). ETA=10:23:33, max mem: 20.9 GB 
[11/24 21:46:13 visual_prompt]: 	Training 200/553. train loss: 2.5037,	0.8351 s / batch. (data: 3.23e-04). ETA=10:36:01, max mem: 20.9 GB 
[11/24 21:47:55 visual_prompt]: 	Training 300/553. train loss: 0.5159,	0.8284 s / batch. (data: 1.05e-02). ETA=10:29:33, max mem: 20.9 GB 
[11/24 21:49:36 visual_prompt]: 	Training 400/553. train loss: 1.4378,	0.8097 s / batch. (data: 3.06e-04). ETA=10:14:01, max mem: 20.9 GB 
[11/24 21:51:17 visual_prompt]: 	Training 500/553. train loss: 1.6077,	0.8440 s / batch. (data: 1.20e-02). ETA=10:38:35, max mem: 20.9 GB 
[11/24 21:52:09 visual_prompt]: Epoch 18 / 100: avg data time: 1.99e-01, avg batch time: 1.0251, average train loss: 1.5942
[11/24 21:53:08 visual_prompt]: Inference (val):avg data time: 2.27e-04, avg batch time: 0.3082, average loss: 0.9914
[11/24 21:53:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.78	
[11/24 21:53:08 visual_prompt]: Stopping early.
[11/24 21:53:08 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 21:53:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 21:53:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/24 21:53:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 21:53:08 visual_prompt]: Training with config:
[11/24 21:53:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/24 21:53:08 visual_prompt]: Loading training data...
[11/24 21:53:08 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 21:53:08 visual_prompt]: Loading validation data...
[11/24 21:53:08 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 21:53:08 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 21:53:11 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 21:53:11 visual_prompt]: tuned percent:0.525
[11/24 21:53:11 visual_prompt]: Device used for model: 0
[11/24 21:53:11 visual_prompt]: Setting up Evaluator...
[11/24 21:53:11 visual_prompt]: Setting up Trainer...
[11/24 21:53:11 visual_prompt]: 	Setting up the optimizer...
[11/24 21:53:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 21:54:57 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8440 s / batch. (data: 3.29e-04). ETA=12:56:28, max mem: 20.9 GB 
[11/24 21:56:37 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8345 s / batch. (data: 5.45e-03). ETA=12:46:19, max mem: 20.9 GB 
[11/24 21:58:22 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.8272 s / batch. (data: 1.01e+00). ETA=1 day, 3:54:57, max mem: 20.9 GB 
[11/24 22:00:02 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8307 s / batch. (data: 3.36e-04). ETA=12:40:06, max mem: 20.9 GB 
[11/24 22:01:46 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8320 s / batch. (data: 1.20e-02). ETA=12:39:54, max mem: 20.9 GB 
[11/24 22:02:39 visual_prompt]: Epoch 1 / 100: avg data time: 2.01e-01, avg batch time: 1.0275, average train loss: 1.5403
[11/24 22:03:38 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3077, average loss: 1.5201
[11/24 22:03:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 22:03:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/24 22:05:24 visual_prompt]: 	Training 100/553. train loss: 0.7700,	0.8600 s / batch. (data: 3.20e-04). ETA=13:03:16, max mem: 20.9 GB 
[11/24 22:07:05 visual_prompt]: 	Training 200/553. train loss: 0.1688,	1.2263 s / batch. (data: 3.89e-01). ETA=18:34:48, max mem: 20.9 GB 
[11/24 22:08:49 visual_prompt]: 	Training 300/553. train loss: 0.9670,	1.1280 s / batch. (data: 3.05e-01). ETA=17:03:35, max mem: 20.9 GB 
[11/24 22:10:30 visual_prompt]: 	Training 400/553. train loss: 1.0565,	0.8315 s / batch. (data: 5.50e-03). ETA=12:33:07, max mem: 20.9 GB 
[11/24 22:12:13 visual_prompt]: 	Training 500/553. train loss: 0.6524,	0.8246 s / batch. (data: 3.12e-04). ETA=12:25:33, max mem: 20.9 GB 
[11/24 22:13:05 visual_prompt]: Epoch 2 / 100: avg data time: 1.98e-01, avg batch time: 1.0240, average train loss: 0.8609
[11/24 22:14:03 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3085, average loss: 0.7371
[11/24 22:14:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.87	
[11/24 22:14:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/24 22:15:47 visual_prompt]: 	Training 100/553. train loss: 0.7855,	0.8245 s / batch. (data: 5.48e-03). ETA=12:23:18, max mem: 20.9 GB 
[11/24 22:17:31 visual_prompt]: 	Training 200/553. train loss: 0.8414,	1.4200 s / batch. (data: 5.89e-01). ETA=21:17:51, max mem: 20.9 GB 
[11/24 22:19:11 visual_prompt]: 	Training 300/553. train loss: 0.6527,	0.8337 s / batch. (data: 3.27e-04). ETA=12:28:48, max mem: 20.9 GB 
[11/24 22:20:54 visual_prompt]: 	Training 400/553. train loss: 0.9671,	0.8203 s / batch. (data: 7.97e-03). ETA=12:15:26, max mem: 20.9 GB 
[11/24 22:22:39 visual_prompt]: 	Training 500/553. train loss: 0.7584,	1.7808 s / batch. (data: 9.56e-01). ETA=1 day, 2:33:36, max mem: 20.9 GB 
[11/24 22:23:32 visual_prompt]: Epoch 3 / 100: avg data time: 2.02e-01, avg batch time: 1.0276, average train loss: 0.7676
[11/24 22:24:31 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3064, average loss: 0.7342
[11/24 22:24:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.38	
[11/24 22:24:31 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/24 22:26:23 visual_prompt]: 	Training 100/553. train loss: 0.8624,	0.8400 s / batch. (data: 5.64e-03). ETA=12:29:33, max mem: 20.9 GB 
[11/24 22:28:10 visual_prompt]: 	Training 200/553. train loss: 0.9227,	0.8160 s / batch. (data: 3.19e-04). ETA=12:06:49, max mem: 20.9 GB 
[11/24 22:29:56 visual_prompt]: 	Training 300/553. train loss: 0.7043,	1.6040 s / batch. (data: 7.71e-01). ETA=23:45:58, max mem: 20.9 GB 
[11/24 22:31:37 visual_prompt]: 	Training 400/553. train loss: 0.6346,	1.4843 s / batch. (data: 6.53e-01). ETA=21:57:05, max mem: 20.9 GB 
[11/24 22:33:24 visual_prompt]: 	Training 500/553. train loss: 0.1718,	4.1104 s / batch. (data: 3.30e+00). ETA=2 days, 12:40:31, max mem: 20.9 GB 
[11/24 22:34:20 visual_prompt]: Epoch 4 / 100: avg data time: 2.38e-01, avg batch time: 1.0632, average train loss: 0.9653
[11/24 22:35:20 visual_prompt]: Inference (val):avg data time: 4.39e-04, avg batch time: 0.3086, average loss: 1.0900
[11/24 22:35:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.41	
[11/24 22:35:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/24 22:37:07 visual_prompt]: 	Training 100/553. train loss: 2.2221,	0.8084 s / batch. (data: 3.84e-04). ETA=11:53:54, max mem: 20.9 GB 
[11/24 22:38:52 visual_prompt]: 	Training 200/553. train loss: 0.6535,	1.6279 s / batch. (data: 7.78e-01). ETA=23:54:58, max mem: 20.9 GB 
[11/24 22:40:37 visual_prompt]: 	Training 300/553. train loss: 1.4512,	0.8207 s / batch. (data: 1.07e-02). ETA=12:02:01, max mem: 20.9 GB 
[11/24 22:42:20 visual_prompt]: 	Training 400/553. train loss: 2.0240,	0.8460 s / batch. (data: 7.97e-03). ETA=12:22:55, max mem: 20.9 GB 
[11/24 22:44:05 visual_prompt]: 	Training 500/553. train loss: 0.5817,	0.8137 s / batch. (data: 3.97e-04). ETA=11:53:12, max mem: 20.9 GB 
[11/24 22:45:00 visual_prompt]: Epoch 5 / 100: avg data time: 2.24e-01, avg batch time: 1.0493, average train loss: 0.9193
[11/24 22:46:00 visual_prompt]: Inference (val):avg data time: 5.24e-05, avg batch time: 0.3086, average loss: 1.2579
[11/24 22:46:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.52	
[11/24 22:46:00 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/24 22:47:51 visual_prompt]: 	Training 100/553. train loss: 0.7583,	0.8320 s / batch. (data: 3.45e-04). ETA=12:07:07, max mem: 20.9 GB 
[11/24 22:49:35 visual_prompt]: 	Training 200/553. train loss: 2.2897,	0.8200 s / batch. (data: 4.17e-04). ETA=11:55:14, max mem: 20.9 GB 
[11/24 22:51:17 visual_prompt]: 	Training 300/553. train loss: 0.5654,	0.8439 s / batch. (data: 1.19e-02). ETA=12:14:43, max mem: 20.9 GB 
[11/24 22:53:05 visual_prompt]: 	Training 400/553. train loss: 0.5669,	0.8280 s / batch. (data: 4.46e-04). ETA=11:59:26, max mem: 20.9 GB 
[11/24 22:54:47 visual_prompt]: 	Training 500/553. train loss: 0.7552,	0.8216 s / batch. (data: 1.20e-02). ETA=11:52:29, max mem: 20.9 GB 
[11/24 22:55:41 visual_prompt]: Epoch 6 / 100: avg data time: 2.23e-01, avg batch time: 1.0492, average train loss: 0.9855
[11/24 22:56:41 visual_prompt]: Inference (val):avg data time: 5.11e-05, avg batch time: 0.3083, average loss: 0.9076
[11/24 22:56:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.66	
[11/24 22:56:41 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/24 22:58:28 visual_prompt]: 	Training 100/553. train loss: 2.4193,	0.8161 s / batch. (data: 3.55e-04). ETA=11:45:40, max mem: 20.9 GB 
[11/24 23:00:12 visual_prompt]: 	Training 200/553. train loss: 0.5644,	0.8475 s / batch. (data: 3.39e-04). ETA=12:11:26, max mem: 20.9 GB 
[11/24 23:01:59 visual_prompt]: 	Training 300/553. train loss: 0.5432,	1.9777 s / batch. (data: 1.15e+00). ETA=1 day, 4:23:31, max mem: 20.9 GB 
[11/24 23:03:44 visual_prompt]: 	Training 400/553. train loss: 0.5758,	2.1561 s / batch. (data: 1.33e+00). ETA=1 day, 6:53:35, max mem: 20.9 GB 
[11/24 23:05:26 visual_prompt]: 	Training 500/553. train loss: 1.1166,	0.8154 s / batch. (data: 4.72e-04). ETA=11:39:39, max mem: 20.9 GB 
[11/24 23:06:19 visual_prompt]: Epoch 7 / 100: avg data time: 2.18e-01, avg batch time: 1.0451, average train loss: 1.0383
[11/24 23:07:19 visual_prompt]: Inference (val):avg data time: 4.01e-04, avg batch time: 0.3067, average loss: 0.7151
[11/24 23:07:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.52	
[11/24 23:07:19 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/24 23:09:05 visual_prompt]: 	Training 100/553. train loss: 2.1714,	0.8199 s / batch. (data: 4.56e-04). ETA=11:41:26, max mem: 20.9 GB 
[11/24 23:10:51 visual_prompt]: 	Training 200/553. train loss: 0.6697,	0.8203 s / batch. (data: 3.82e-04). ETA=11:40:24, max mem: 20.9 GB 
[11/24 23:12:36 visual_prompt]: 	Training 300/553. train loss: 0.6618,	0.8440 s / batch. (data: 5.57e-03). ETA=11:59:13, max mem: 20.9 GB 
[11/24 23:14:20 visual_prompt]: 	Training 400/553. train loss: 0.6970,	0.8567 s / batch. (data: 3.71e-02). ETA=12:08:37, max mem: 20.9 GB 
[11/24 23:16:05 visual_prompt]: 	Training 500/553. train loss: 3.6742,	1.6760 s / batch. (data: 8.47e-01). ETA=23:42:34, max mem: 20.9 GB 
[11/24 23:16:59 visual_prompt]: Epoch 8 / 100: avg data time: 2.22e-01, avg batch time: 1.0490, average train loss: 1.1705
[11/24 23:18:00 visual_prompt]: Inference (val):avg data time: 1.69e-04, avg batch time: 0.3096, average loss: 0.9263
[11/24 23:18:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.42	
[11/24 23:18:00 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/24 23:19:48 visual_prompt]: 	Training 100/553. train loss: 0.3339,	0.8384 s / batch. (data: 7.93e-03). ETA=11:49:30, max mem: 20.9 GB 
[11/24 23:21:31 visual_prompt]: 	Training 200/553. train loss: 0.8443,	0.8200 s / batch. (data: 3.50e-04). ETA=11:32:35, max mem: 20.9 GB 
[11/24 23:23:16 visual_prompt]: 	Training 300/553. train loss: 0.6550,	1.9637 s / batch. (data: 1.16e+00). ETA=1 day, 3:35:17, max mem: 20.9 GB 
[11/24 23:25:01 visual_prompt]: 	Training 400/553. train loss: 0.9674,	0.8383 s / batch. (data: 1.05e-02). ETA=11:45:14, max mem: 20.9 GB 
[11/24 23:26:46 visual_prompt]: 	Training 500/553. train loss: 0.7006,	0.8606 s / batch. (data: 1.06e-02). ETA=12:02:32, max mem: 20.9 GB 
[11/24 23:27:39 visual_prompt]: Epoch 9 / 100: avg data time: 2.22e-01, avg batch time: 1.0484, average train loss: 1.0496
[11/24 23:28:40 visual_prompt]: Inference (val):avg data time: 5.66e-05, avg batch time: 0.3071, average loss: 0.6961
[11/24 23:28:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.60	
[11/24 23:28:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/24 23:30:30 visual_prompt]: 	Training 100/553. train loss: 3.6842,	0.8334 s / batch. (data: 7.96e-03). ETA=11:37:35, max mem: 20.9 GB 
[11/24 23:32:14 visual_prompt]: 	Training 200/553. train loss: 2.0461,	0.8359 s / batch. (data: 3.39e-04). ETA=11:38:20, max mem: 20.9 GB 
[11/24 23:33:57 visual_prompt]: 	Training 300/553. train loss: 3.3785,	1.3080 s / batch. (data: 4.68e-01). ETA=18:10:29, max mem: 20.9 GB 
[11/24 23:35:39 visual_prompt]: 	Training 400/553. train loss: 0.9255,	0.8760 s / batch. (data: 4.57e-02). ETA=12:08:50, max mem: 20.9 GB 
[11/24 23:37:24 visual_prompt]: 	Training 500/553. train loss: 0.5967,	1.0560 s / batch. (data: 2.38e-01). ETA=14:36:52, max mem: 20.9 GB 
[11/24 23:38:19 visual_prompt]: Epoch 10 / 100: avg data time: 2.21e-01, avg batch time: 1.0471, average train loss: 1.3934
[11/24 23:39:19 visual_prompt]: Inference (val):avg data time: 3.42e-04, avg batch time: 0.3078, average loss: 0.7303
[11/24 23:39:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.89	
[11/24 23:39:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/24 23:41:10 visual_prompt]: 	Training 100/553. train loss: 0.8313,	0.8403 s / batch. (data: 1.33e-02). ETA=11:35:39, max mem: 20.9 GB 
[11/24 23:42:56 visual_prompt]: 	Training 200/553. train loss: 0.6824,	0.8475 s / batch. (data: 1.13e-02). ETA=11:40:08, max mem: 20.9 GB 
[11/24 23:44:38 visual_prompt]: 	Training 300/553. train loss: 0.1487,	1.3776 s / batch. (data: 5.67e-01). ETA=18:55:50, max mem: 20.9 GB 
[11/24 23:46:22 visual_prompt]: 	Training 400/553. train loss: 0.6376,	0.8277 s / batch. (data: 5.48e-03). ETA=11:21:01, max mem: 20.9 GB 
[11/24 23:48:04 visual_prompt]: 	Training 500/553. train loss: 1.5787,	0.8223 s / batch. (data: 3.45e-04). ETA=11:15:12, max mem: 20.9 GB 
[11/24 23:48:58 visual_prompt]: Epoch 11 / 100: avg data time: 2.20e-01, avg batch time: 1.0468, average train loss: 1.1421
[11/24 23:49:58 visual_prompt]: Inference (val):avg data time: 5.19e-05, avg batch time: 0.3083, average loss: 1.3272
[11/24 23:49:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[11/24 23:49:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/24 23:51:48 visual_prompt]: 	Training 100/553. train loss: 0.7520,	0.8407 s / batch. (data: 9.33e-03). ETA=11:28:15, max mem: 20.9 GB 
[11/24 23:53:34 visual_prompt]: 	Training 200/553. train loss: 0.8649,	0.8236 s / batch. (data: 3.64e-04). ETA=11:12:49, max mem: 20.9 GB 
[11/24 23:55:17 visual_prompt]: 	Training 300/553. train loss: 1.3444,	0.8173 s / batch. (data: 4.28e-04). ETA=11:06:17, max mem: 20.9 GB 
[11/24 23:57:01 visual_prompt]: 	Training 400/553. train loss: 0.7673,	0.8320 s / batch. (data: 7.92e-03). ETA=11:16:54, max mem: 20.9 GB 
[11/24 23:58:46 visual_prompt]: 	Training 500/553. train loss: 1.1665,	0.8360 s / batch. (data: 1.06e-02). ETA=11:18:45, max mem: 20.9 GB 
[11/24 23:59:39 visual_prompt]: Epoch 12 / 100: avg data time: 2.23e-01, avg batch time: 1.0499, average train loss: 1.3947
[11/25 00:00:39 visual_prompt]: Inference (val):avg data time: 5.54e-05, avg batch time: 0.3102, average loss: 0.8203
[11/25 00:00:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.77	
[11/25 00:00:39 visual_prompt]: Best epoch 12: best metric: -0.820
[11/25 00:00:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/25 00:02:30 visual_prompt]: 	Training 100/553. train loss: 0.7714,	0.8360 s / batch. (data: 4.96e-04). ETA=11:16:39, max mem: 20.9 GB 
[11/25 00:04:10 visual_prompt]: 	Training 200/553. train loss: 0.7119,	0.8760 s / batch. (data: 3.65e-04). ETA=11:47:35, max mem: 20.9 GB 
[11/25 00:05:56 visual_prompt]: 	Training 300/553. train loss: 0.6117,	1.8699 s / batch. (data: 1.05e+00). ETA=1 day, 1:07:17, max mem: 20.9 GB 
[11/25 00:07:39 visual_prompt]: 	Training 400/553. train loss: 3.0195,	0.8121 s / batch. (data: 5.19e-04). ETA=10:53:16, max mem: 20.9 GB 
[11/25 00:09:24 visual_prompt]: 	Training 500/553. train loss: 2.4177,	0.8112 s / batch. (data: 3.88e-04). ETA=10:51:10, max mem: 20.9 GB 
[11/25 00:10:19 visual_prompt]: Epoch 13 / 100: avg data time: 2.21e-01, avg batch time: 1.0477, average train loss: 1.3389
[11/25 00:11:19 visual_prompt]: Inference (val):avg data time: 4.94e-05, avg batch time: 0.3065, average loss: 0.8071
[11/25 00:11:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 45.86	
[11/25 00:11:19 visual_prompt]: Best epoch 13: best metric: -0.807
[11/25 00:11:19 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/25 00:13:09 visual_prompt]: 	Training 100/553. train loss: 0.5203,	0.8282 s / batch. (data: 5.49e-03). ETA=11:02:44, max mem: 20.9 GB 
[11/25 00:14:53 visual_prompt]: 	Training 200/553. train loss: 0.1359,	1.3478 s / batch. (data: 5.30e-01). ETA=17:56:12, max mem: 20.9 GB 
[11/25 00:16:37 visual_prompt]: 	Training 300/553. train loss: 0.7159,	0.8448 s / batch. (data: 3.66e-04). ETA=11:13:11, max mem: 20.9 GB 
[11/25 00:18:21 visual_prompt]: 	Training 400/553. train loss: 0.4982,	0.8128 s / batch. (data: 3.43e-04). ETA=10:46:19, max mem: 20.9 GB 
[11/25 00:20:06 visual_prompt]: 	Training 500/553. train loss: 1.7130,	0.8368 s / batch. (data: 3.87e-04). ETA=11:04:01, max mem: 20.9 GB 
[11/25 00:20:59 visual_prompt]: Epoch 14 / 100: avg data time: 2.20e-01, avg batch time: 1.0477, average train loss: 1.2353
[11/25 00:21:59 visual_prompt]: Inference (val):avg data time: 5.30e-05, avg batch time: 0.3074, average loss: 0.8232
[11/25 00:21:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.43	
[11/25 00:21:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/25 00:23:48 visual_prompt]: 	Training 100/553. train loss: 0.7051,	0.9478 s / batch. (data: 1.01e-01). ETA=12:29:39, max mem: 20.9 GB 
[11/25 00:25:31 visual_prompt]: 	Training 200/553. train loss: 5.6507,	0.8463 s / batch. (data: 2.22e-02). ETA=11:07:57, max mem: 20.9 GB 
[11/25 00:27:17 visual_prompt]: 	Training 300/553. train loss: 1.0801,	0.8159 s / batch. (data: 3.92e-04). ETA=10:42:38, max mem: 20.9 GB 
[11/25 00:28:59 visual_prompt]: 	Training 400/553. train loss: 1.5442,	1.3240 s / batch. (data: 4.74e-01). ETA=17:20:37, max mem: 20.9 GB 
[11/25 00:30:44 visual_prompt]: 	Training 500/553. train loss: 0.6505,	0.8218 s / batch. (data: 5.70e-03). ETA=10:44:30, max mem: 20.9 GB 
[11/25 00:31:39 visual_prompt]: Epoch 15 / 100: avg data time: 2.22e-01, avg batch time: 1.0495, average train loss: 1.3959
[11/25 00:32:40 visual_prompt]: Inference (val):avg data time: 5.38e-05, avg batch time: 0.3086, average loss: 1.1552
[11/25 00:32:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.79	
[11/25 00:32:40 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/25 00:34:27 visual_prompt]: 	Training 100/553. train loss: 3.9407,	0.8160 s / batch. (data: 3.55e-04). ETA=10:37:53, max mem: 20.9 GB 
[11/25 00:36:11 visual_prompt]: 	Training 200/553. train loss: 2.2138,	0.8352 s / batch. (data: 1.20e-02). ETA=10:51:32, max mem: 20.9 GB 
[11/25 00:37:56 visual_prompt]: 	Training 300/553. train loss: 0.6724,	0.8420 s / batch. (data: 3.55e-04). ETA=10:55:26, max mem: 20.9 GB 
[11/25 00:39:40 visual_prompt]: 	Training 400/553. train loss: 1.2174,	0.8313 s / batch. (data: 4.27e-04). ETA=10:45:42, max mem: 20.9 GB 
[11/25 00:41:24 visual_prompt]: 	Training 500/553. train loss: 0.8232,	1.7435 s / batch. (data: 9.35e-01). ETA=22:31:19, max mem: 20.9 GB 
[11/25 00:42:19 visual_prompt]: Epoch 16 / 100: avg data time: 2.21e-01, avg batch time: 1.0477, average train loss: 1.4743
[11/25 00:43:19 visual_prompt]: Inference (val):avg data time: 5.10e-05, avg batch time: 0.3070, average loss: 0.6890
[11/25 00:43:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.91	
[11/25 00:43:19 visual_prompt]: Best epoch 16: best metric: -0.689
[11/25 00:43:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/25 00:45:07 visual_prompt]: 	Training 100/553. train loss: 0.8575,	0.8200 s / batch. (data: 3.55e-04). ETA=10:33:27, max mem: 20.9 GB 
[11/25 00:46:53 visual_prompt]: 	Training 200/553. train loss: 3.3039,	0.8388 s / batch. (data: 5.40e-04). ETA=10:46:37, max mem: 20.9 GB 
[11/25 00:48:38 visual_prompt]: 	Training 300/553. train loss: 0.9985,	0.8480 s / batch. (data: 6.72e-04). ETA=10:52:18, max mem: 20.9 GB 
[11/25 00:50:22 visual_prompt]: 	Training 400/553. train loss: 2.4057,	0.8320 s / batch. (data: 8.00e-03). ETA=10:38:35, max mem: 20.9 GB 
[11/25 00:52:08 visual_prompt]: 	Training 500/553. train loss: 0.5941,	1.8320 s / batch. (data: 9.95e-01). ETA=23:23:04, max mem: 20.9 GB 
[11/25 00:53:03 visual_prompt]: Epoch 17 / 100: avg data time: 2.29e-01, avg batch time: 1.0552, average train loss: 1.4583
[11/25 00:54:04 visual_prompt]: Inference (val):avg data time: 5.34e-05, avg batch time: 0.3079, average loss: 1.1845
[11/25 00:54:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.50	
[11/25 00:54:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/25 00:55:54 visual_prompt]: 	Training 100/553. train loss: 0.8823,	0.8319 s / batch. (data: 7.92e-03). ETA=10:35:02, max mem: 20.9 GB 
[11/25 00:57:42 visual_prompt]: 	Training 200/553. train loss: 2.5699,	0.8687 s / batch. (data: 9.69e-04). ETA=11:01:36, max mem: 20.9 GB 
[11/25 00:59:27 visual_prompt]: 	Training 300/553. train loss: 0.5949,	0.8235 s / batch. (data: 4.77e-04). ETA=10:25:50, max mem: 20.9 GB 
[11/25 01:01:12 visual_prompt]: 	Training 400/553. train loss: 0.9589,	0.8120 s / batch. (data: 3.24e-04). ETA=10:15:45, max mem: 20.9 GB 
[11/25 01:02:56 visual_prompt]: 	Training 500/553. train loss: 0.9227,	0.8320 s / batch. (data: 3.67e-04). ETA=10:29:31, max mem: 20.9 GB 
[11/25 01:03:50 visual_prompt]: Epoch 18 / 100: avg data time: 2.34e-01, avg batch time: 1.0594, average train loss: 1.4059
[11/25 01:04:51 visual_prompt]: Inference (val):avg data time: 5.33e-05, avg batch time: 0.3091, average loss: 1.4272
[11/25 01:04:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.14	
[11/25 01:04:51 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[11/25 01:06:41 visual_prompt]: 	Training 100/553. train loss: 0.5746,	0.8374 s / batch. (data: 4.45e-04). ETA=10:31:30, max mem: 20.9 GB 
[11/25 01:08:27 visual_prompt]: 	Training 200/553. train loss: 0.7530,	0.8440 s / batch. (data: 1.01e-03). ETA=10:35:02, max mem: 20.9 GB 
[11/25 01:10:12 visual_prompt]: 	Training 300/553. train loss: 0.3898,	0.8177 s / batch. (data: 3.66e-04). ETA=10:13:52, max mem: 20.9 GB 
[11/25 01:11:59 visual_prompt]: 	Training 400/553. train loss: 0.9271,	0.8165 s / batch. (data: 4.15e-04). ETA=10:11:40, max mem: 20.9 GB 
[11/25 01:13:40 visual_prompt]: 	Training 500/553. train loss: 0.5652,	0.8241 s / batch. (data: 3.41e-04). ETA=10:15:56, max mem: 20.9 GB 
[11/25 01:14:35 visual_prompt]: Epoch 19 / 100: avg data time: 2.29e-01, avg batch time: 1.0557, average train loss: 1.3115
[11/25 01:15:35 visual_prompt]: Inference (val):avg data time: 5.02e-05, avg batch time: 0.3077, average loss: 5.8312
[11/25 01:15:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.63	
[11/25 01:15:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[11/25 01:17:24 visual_prompt]: 	Training 100/553. train loss: 0.7514,	0.8279 s / batch. (data: 5.13e-04). ETA=10:16:42, max mem: 20.9 GB 
[11/25 01:19:10 visual_prompt]: 	Training 200/553. train loss: 0.6134,	0.8382 s / batch. (data: 2.09e-04). ETA=10:22:57, max mem: 20.9 GB 
[11/25 01:20:55 visual_prompt]: 	Training 300/553. train loss: 0.6458,	0.8240 s / batch. (data: 4.74e-04). ETA=10:11:03, max mem: 20.9 GB 
[11/25 01:22:40 visual_prompt]: 	Training 400/553. train loss: 1.0217,	0.8674 s / batch. (data: 1.95e-02). ETA=10:41:47, max mem: 20.9 GB 
[11/25 01:24:24 visual_prompt]: 	Training 500/553. train loss: 1.3167,	0.8478 s / batch. (data: 1.06e-02). ETA=10:25:49, max mem: 20.9 GB 
[11/25 01:25:21 visual_prompt]: Epoch 20 / 100: avg data time: 2.32e-01, avg batch time: 1.0581, average train loss: 1.2826
[11/25 01:26:22 visual_prompt]: Inference (val):avg data time: 4.16e-04, avg batch time: 0.3086, average loss: 0.6973
[11/25 01:26:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.44	
[11/25 01:26:22 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[11/25 01:28:14 visual_prompt]: 	Training 100/553. train loss: 1.3835,	1.0920 s / batch. (data: 2.71e-01). ETA=13:23:21, max mem: 20.9 GB 
[11/25 01:29:59 visual_prompt]: 	Training 200/553. train loss: 1.9942,	0.8219 s / batch. (data: 5.57e-03). ETA=10:03:16, max mem: 20.9 GB 
[11/25 01:31:44 visual_prompt]: 	Training 300/553. train loss: 1.2501,	1.2166 s / batch. (data: 4.01e-01). ETA=14:50:55, max mem: 20.9 GB 
[11/25 01:33:28 visual_prompt]: 	Training 400/553. train loss: 1.1687,	0.8200 s / batch. (data: 3.52e-04). ETA=9:59:07, max mem: 20.9 GB 
[11/25 01:35:15 visual_prompt]: 	Training 500/553. train loss: 1.5510,	0.8400 s / batch. (data: 3.41e-04). ETA=10:12:22, max mem: 20.9 GB 
[11/25 01:36:08 visual_prompt]: Epoch 21 / 100: avg data time: 2.32e-01, avg batch time: 1.0595, average train loss: 1.3929
[11/25 01:37:09 visual_prompt]: Inference (val):avg data time: 5.19e-05, avg batch time: 0.3077, average loss: 0.8058
[11/25 01:37:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.46	
[11/25 01:37:09 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[11/25 01:38:57 visual_prompt]: 	Training 100/553. train loss: 0.7388,	0.8228 s / batch. (data: 4.22e-04). ETA=9:57:45, max mem: 20.9 GB 
[11/25 01:40:43 visual_prompt]: 	Training 200/553. train loss: 0.6400,	0.8440 s / batch. (data: 7.98e-03). ETA=10:11:43, max mem: 20.9 GB 
[11/25 01:42:26 visual_prompt]: 	Training 300/553. train loss: 0.2082,	0.8319 s / batch. (data: 1.61e-02). ETA=10:01:35, max mem: 20.9 GB 
[11/25 01:44:11 visual_prompt]: 	Training 400/553. train loss: 3.0826,	0.8453 s / batch. (data: 2.48e-02). ETA=10:09:50, max mem: 20.9 GB 
[11/25 01:45:57 visual_prompt]: 	Training 500/553. train loss: 0.8451,	0.8640 s / batch. (data: 5.57e-03). ETA=10:21:53, max mem: 20.9 GB 
[11/25 01:46:53 visual_prompt]: Epoch 22 / 100: avg data time: 2.30e-01, avg batch time: 1.0565, average train loss: 1.4194
[11/25 01:47:54 visual_prompt]: Inference (val):avg data time: 5.47e-05, avg batch time: 0.3078, average loss: 1.3268
[11/25 01:47:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.88	
[11/25 01:47:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[11/25 01:49:45 visual_prompt]: 	Training 100/553. train loss: 0.9541,	1.0206 s / batch. (data: 2.02e-01). ETA=12:12:00, max mem: 20.9 GB 
[11/25 01:51:32 visual_prompt]: 	Training 200/553. train loss: 0.5850,	1.1320 s / batch. (data: 2.86e-01). ETA=13:30:01, max mem: 20.9 GB 
[11/25 01:53:16 visual_prompt]: 	Training 300/553. train loss: 0.5825,	0.8747 s / batch. (data: 5.96e-03). ETA=10:24:27, max mem: 20.9 GB 
[11/25 01:54:56 visual_prompt]: 	Training 400/553. train loss: 0.9522,	0.8280 s / batch. (data: 8.39e-04). ETA=9:49:43, max mem: 20.9 GB 
[11/25 01:56:35 visual_prompt]: 	Training 500/553. train loss: 0.2300,	0.8412 s / batch. (data: 3.06e-04). ETA=9:57:45, max mem: 20.9 GB 
[11/25 01:57:29 visual_prompt]: Epoch 23 / 100: avg data time: 2.12e-01, avg batch time: 1.0401, average train loss: 1.2357
[11/25 01:58:28 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3085, average loss: 0.9518
[11/25 01:58:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.56	
[11/25 01:58:28 visual_prompt]: Stopping early.
[11/25 01:58:28 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 01:58:28 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 01:58:28 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/25 01:58:28 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 01:58:28 visual_prompt]: Training with config:
[11/25 01:58:28 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/25 01:58:28 visual_prompt]: Loading training data...
[11/25 01:58:28 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 01:58:28 visual_prompt]: Loading validation data...
[11/25 01:58:28 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 01:58:28 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 01:58:31 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 01:58:31 visual_prompt]: tuned percent:0.525
[11/25 01:58:31 visual_prompt]: Device used for model: 0
[11/25 01:58:31 visual_prompt]: Setting up Evaluator...
[11/25 01:58:31 visual_prompt]: Setting up Trainer...
[11/25 01:58:31 visual_prompt]: 	Setting up the optimizer...
[11/25 01:58:31 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 02:00:17 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8186 s / batch. (data: 3.21e-04). ETA=12:33:06, max mem: 20.9 GB 
[11/25 02:01:58 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8194 s / batch. (data: 3.58e-04). ETA=12:32:29, max mem: 20.9 GB 
[11/25 02:03:42 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.9961 s / batch. (data: 1.75e-01). ETA=15:13:02, max mem: 20.9 GB 
[11/25 02:05:23 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8200 s / batch. (data: 3.16e-04). ETA=12:30:18, max mem: 20.9 GB 
[11/25 02:07:07 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8480 s / batch. (data: 8.38e-04). ETA=12:54:30, max mem: 20.9 GB 
[11/25 02:08:01 visual_prompt]: Epoch 1 / 100: avg data time: 2.03e-01, avg batch time: 1.0294, average train loss: 1.5403
[11/25 02:08:59 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3075, average loss: 1.5201
[11/25 02:08:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 02:08:59 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/25 02:10:45 visual_prompt]: 	Training 100/553. train loss: 0.7742,	0.8338 s / batch. (data: 3.25e-04). ETA=12:39:25, max mem: 20.9 GB 
[11/25 02:12:26 visual_prompt]: 	Training 200/553. train loss: 0.1321,	0.9983 s / batch. (data: 1.76e-01). ETA=15:07:33, max mem: 20.9 GB 
[11/25 02:14:11 visual_prompt]: 	Training 300/553. train loss: 0.9265,	1.2225 s / batch. (data: 4.12e-01). ETA=18:29:22, max mem: 20.9 GB 
[11/25 02:15:51 visual_prompt]: 	Training 400/553. train loss: 1.5258,	0.8160 s / batch. (data: 3.70e-04). ETA=12:19:06, max mem: 20.9 GB 
[11/25 02:17:35 visual_prompt]: 	Training 500/553. train loss: 0.5747,	0.8157 s / batch. (data: 5.46e-03). ETA=12:17:28, max mem: 20.9 GB 
[11/25 02:18:26 visual_prompt]: Epoch 2 / 100: avg data time: 1.99e-01, avg batch time: 1.0254, average train loss: 0.9021
[11/25 02:19:25 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.3089, average loss: 1.0807
[11/25 02:19:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.77	
[11/25 02:19:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/25 02:21:10 visual_prompt]: 	Training 100/553. train loss: 0.7503,	1.6644 s / batch. (data: 8.47e-01). ETA=1 day, 1:00:36, max mem: 20.9 GB 
[11/25 02:22:53 visual_prompt]: 	Training 200/553. train loss: 0.7792,	0.9291 s / batch. (data: 1.03e-01). ETA=13:56:04, max mem: 20.9 GB 
[11/25 02:24:34 visual_prompt]: 	Training 300/553. train loss: 0.5905,	0.8363 s / batch. (data: 5.46e-03). ETA=12:31:10, max mem: 20.9 GB 
[11/25 02:26:17 visual_prompt]: 	Training 400/553. train loss: 1.6339,	0.8120 s / batch. (data: 3.29e-04). ETA=12:08:01, max mem: 20.9 GB 
[11/25 02:28:00 visual_prompt]: 	Training 500/553. train loss: 0.8624,	1.3793 s / batch. (data: 5.56e-01). ETA=20:34:19, max mem: 20.9 GB 
[11/25 02:28:52 visual_prompt]: Epoch 3 / 100: avg data time: 1.99e-01, avg batch time: 1.0250, average train loss: 0.8498
[11/25 02:29:51 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3083, average loss: 0.7327
[11/25 02:29:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.58	
[11/25 02:29:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/25 02:31:38 visual_prompt]: 	Training 100/553. train loss: 0.8313,	0.8626 s / batch. (data: 1.56e-02). ETA=12:49:43, max mem: 20.9 GB 
[11/25 02:33:21 visual_prompt]: 	Training 200/553. train loss: 1.4157,	0.8198 s / batch. (data: 7.88e-03). ETA=12:10:09, max mem: 20.9 GB 
[11/25 02:35:04 visual_prompt]: 	Training 300/553. train loss: 0.7560,	1.5962 s / batch. (data: 7.85e-01). ETA=23:39:02, max mem: 20.9 GB 
[11/25 02:36:41 visual_prompt]: 	Training 400/553. train loss: 0.6816,	0.8574 s / batch. (data: 3.08e-02). ETA=12:40:47, max mem: 20.9 GB 
[11/25 02:38:25 visual_prompt]: 	Training 500/553. train loss: 0.6022,	3.5357 s / batch. (data: 2.72e+00). ETA=2 days, 4:11:28, max mem: 20.9 GB 
[11/25 02:39:20 visual_prompt]: Epoch 4 / 100: avg data time: 2.03e-01, avg batch time: 1.0292, average train loss: 0.9386
[11/25 02:40:19 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.3100, average loss: 1.0216
[11/25 02:40:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.34	
[11/25 02:40:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/25 02:42:03 visual_prompt]: 	Training 100/553. train loss: 0.6818,	0.8240 s / batch. (data: 3.32e-04). ETA=12:07:41, max mem: 20.9 GB 
[11/25 02:43:46 visual_prompt]: 	Training 200/553. train loss: 0.6144,	1.3840 s / batch. (data: 5.66e-01). ETA=20:19:57, max mem: 20.9 GB 
[11/25 02:45:29 visual_prompt]: 	Training 300/553. train loss: 1.4252,	0.8634 s / batch. (data: 5.44e-03). ETA=12:39:39, max mem: 20.9 GB 
[11/25 02:47:10 visual_prompt]: 	Training 400/553. train loss: 1.0239,	0.8422 s / batch. (data: 7.98e-03). ETA=12:19:35, max mem: 20.9 GB 
[11/25 02:48:52 visual_prompt]: 	Training 500/553. train loss: 0.5849,	0.8172 s / batch. (data: 3.13e-04). ETA=11:56:17, max mem: 20.9 GB 
[11/25 02:49:46 visual_prompt]: Epoch 5 / 100: avg data time: 2.00e-01, avg batch time: 1.0267, average train loss: 0.8772
[11/25 02:50:45 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3078, average loss: 1.6199
[11/25 02:50:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.20	
[11/25 02:50:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/25 02:52:33 visual_prompt]: 	Training 100/553. train loss: 0.5677,	0.8577 s / batch. (data: 1.62e-02). ETA=12:29:34, max mem: 20.9 GB 
[11/25 02:54:15 visual_prompt]: 	Training 200/553. train loss: 0.9987,	0.8108 s / batch. (data: 3.46e-04). ETA=11:47:14, max mem: 20.9 GB 
[11/25 02:55:54 visual_prompt]: 	Training 300/553. train loss: 0.5979,	0.8560 s / batch. (data: 3.17e-04). ETA=12:25:13, max mem: 20.9 GB 
[11/25 02:57:41 visual_prompt]: 	Training 400/553. train loss: 0.6772,	0.8219 s / batch. (data: 3.60e-04). ETA=11:54:12, max mem: 20.9 GB 
[11/25 02:59:21 visual_prompt]: 	Training 500/553. train loss: 0.9436,	0.8480 s / batch. (data: 3.35e-04). ETA=12:15:24, max mem: 20.9 GB 
[11/25 03:00:14 visual_prompt]: Epoch 6 / 100: avg data time: 2.02e-01, avg batch time: 1.0286, average train loss: 0.9098
[11/25 03:01:13 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3082, average loss: 1.0490
[11/25 03:01:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.71	
[11/25 03:01:13 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/25 03:02:58 visual_prompt]: 	Training 100/553. train loss: 2.2867,	0.8142 s / batch. (data: 4.00e-04). ETA=11:44:04, max mem: 20.9 GB 
[11/25 03:04:39 visual_prompt]: 	Training 200/553. train loss: 0.5891,	0.8630 s / batch. (data: 1.06e-02). ETA=12:24:49, max mem: 20.9 GB 
[11/25 03:06:25 visual_prompt]: 	Training 300/553. train loss: 0.5617,	1.8640 s / batch. (data: 1.03e+00). ETA=1 day, 2:45:35, max mem: 20.9 GB 
[11/25 03:08:07 visual_prompt]: 	Training 400/553. train loss: 0.5960,	1.8679 s / batch. (data: 1.06e+00). ETA=1 day, 2:45:48, max mem: 20.9 GB 
[11/25 03:09:47 visual_prompt]: 	Training 500/553. train loss: 1.2674,	0.8437 s / batch. (data: 1.05e-02). ETA=12:03:54, max mem: 20.9 GB 
[11/25 03:10:39 visual_prompt]: Epoch 7 / 100: avg data time: 1.97e-01, avg batch time: 1.0244, average train loss: 0.9300
[11/25 03:11:38 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3084, average loss: 0.7171
[11/25 03:11:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.19	
[11/25 03:11:38 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/25 03:13:22 visual_prompt]: 	Training 100/553. train loss: 0.7912,	0.9279 s / batch. (data: 9.86e-02). ETA=13:13:50, max mem: 20.9 GB 
[11/25 03:15:06 visual_prompt]: 	Training 200/553. train loss: 0.7433,	0.8374 s / batch. (data: 2.07e-02). ETA=11:54:59, max mem: 20.9 GB 
[11/25 03:16:49 visual_prompt]: 	Training 300/553. train loss: 0.7953,	0.8280 s / batch. (data: 3.08e-04). ETA=11:45:36, max mem: 20.9 GB 
[11/25 03:18:31 visual_prompt]: 	Training 400/553. train loss: 0.7159,	0.9085 s / batch. (data: 9.15e-02). ETA=12:52:38, max mem: 20.9 GB 
[11/25 03:20:14 visual_prompt]: 	Training 500/553. train loss: 1.2213,	1.5965 s / batch. (data: 7.86e-01). ETA=22:35:07, max mem: 20.9 GB 
[11/25 03:21:07 visual_prompt]: Epoch 8 / 100: avg data time: 2.03e-01, avg batch time: 1.0297, average train loss: 0.9477
[11/25 03:22:06 visual_prompt]: Inference (val):avg data time: 5.02e-04, avg batch time: 0.3076, average loss: 1.1595
[11/25 03:22:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.49	
[11/25 03:22:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/25 03:23:52 visual_prompt]: 	Training 100/553. train loss: 0.1267,	0.8290 s / batch. (data: 1.06e-02). ETA=11:41:31, max mem: 20.9 GB 
[11/25 03:25:33 visual_prompt]: 	Training 200/553. train loss: 0.7180,	0.8147 s / batch. (data: 3.57e-03). ETA=11:28:07, max mem: 20.9 GB 
[11/25 03:27:15 visual_prompt]: 	Training 300/553. train loss: 0.5589,	1.9299 s / batch. (data: 1.11e+00). ETA=1 day, 3:06:48, max mem: 20.9 GB 
[11/25 03:28:59 visual_prompt]: 	Training 400/553. train loss: 0.6192,	0.8299 s / batch. (data: 5.49e-03). ETA=11:38:09, max mem: 20.9 GB 
[11/25 03:30:41 visual_prompt]: 	Training 500/553. train loss: 0.6840,	0.9675 s / batch. (data: 1.31e-01). ETA=13:32:19, max mem: 20.9 GB 
[11/25 03:31:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.99e-01, avg batch time: 1.0258, average train loss: 0.9406
[11/25 03:32:32 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.3060, average loss: 0.6886
[11/25 03:32:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.76	
[11/25 03:32:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/25 03:34:21 visual_prompt]: 	Training 100/553. train loss: 1.4953,	0.8350 s / batch. (data: 1.20e-02). ETA=11:38:58, max mem: 20.9 GB 
[11/25 03:36:02 visual_prompt]: 	Training 200/553. train loss: 0.7161,	0.8241 s / batch. (data: 3.17e-04). ETA=11:28:27, max mem: 20.9 GB 
[11/25 03:37:42 visual_prompt]: 	Training 300/553. train loss: 0.5708,	0.9989 s / batch. (data: 1.89e-01). ETA=13:52:46, max mem: 20.9 GB 
[11/25 03:39:23 visual_prompt]: 	Training 400/553. train loss: 0.8661,	0.8240 s / batch. (data: 3.48e-04). ETA=11:25:38, max mem: 20.9 GB 
[11/25 03:41:06 visual_prompt]: 	Training 500/553. train loss: 0.5701,	0.8440 s / batch. (data: 3.28e-04). ETA=11:40:49, max mem: 20.9 GB 
[11/25 03:41:59 visual_prompt]: Epoch 10 / 100: avg data time: 1.98e-01, avg batch time: 1.0254, average train loss: 1.1865
[11/25 03:42:58 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3091, average loss: 0.6896
[11/25 03:42:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.88	
[11/25 03:42:58 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/25 03:44:46 visual_prompt]: 	Training 100/553. train loss: 1.0289,	0.8369 s / batch. (data: 1.56e-02). ETA=11:32:50, max mem: 20.9 GB 
[11/25 03:46:30 visual_prompt]: 	Training 200/553. train loss: 1.3758,	0.8109 s / batch. (data: 1.04e-03). ETA=11:09:56, max mem: 20.9 GB 
[11/25 03:48:11 visual_prompt]: 	Training 300/553. train loss: 0.1677,	2.4074 s / batch. (data: 1.57e+00). ETA=1 day, 9:04:55, max mem: 20.9 GB 
[11/25 03:49:52 visual_prompt]: 	Training 400/553. train loss: 0.6386,	0.8328 s / batch. (data: 1.05e-02). ETA=11:25:16, max mem: 20.9 GB 
[11/25 03:51:33 visual_prompt]: 	Training 500/553. train loss: 0.8986,	0.8155 s / batch. (data: 5.43e-03). ETA=11:09:40, max mem: 20.9 GB 
[11/25 03:52:25 visual_prompt]: Epoch 11 / 100: avg data time: 1.98e-01, avg batch time: 1.0248, average train loss: 0.9486
[11/25 03:53:23 visual_prompt]: Inference (val):avg data time: 1.61e-04, avg batch time: 0.3070, average loss: 0.7804
[11/25 03:53:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.26	
[11/25 03:53:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/25 03:55:11 visual_prompt]: 	Training 100/553. train loss: 0.8570,	0.8231 s / batch. (data: 3.45e-04). ETA=11:13:50, max mem: 20.9 GB 
[11/25 03:56:54 visual_prompt]: 	Training 200/553. train loss: 0.5702,	0.8360 s / batch. (data: 3.87e-04). ETA=11:22:58, max mem: 20.9 GB 
[11/25 03:58:35 visual_prompt]: 	Training 300/553. train loss: 1.0247,	0.8200 s / batch. (data: 3.25e-04). ETA=11:08:32, max mem: 20.9 GB 
[11/25 04:00:17 visual_prompt]: 	Training 400/553. train loss: 0.7868,	0.8480 s / batch. (data: 5.48e-03). ETA=11:29:55, max mem: 20.9 GB 
[11/25 04:01:59 visual_prompt]: 	Training 500/553. train loss: 3.7251,	0.8472 s / batch. (data: 3.78e-04). ETA=11:27:53, max mem: 20.9 GB 
[11/25 04:02:51 visual_prompt]: Epoch 12 / 100: avg data time: 2.01e-01, avg batch time: 1.0271, average train loss: 1.0984
[11/25 04:03:50 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.3074, average loss: 1.8046
[11/25 04:03:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.26	
[11/25 04:03:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/25 04:05:38 visual_prompt]: 	Training 100/553. train loss: 0.6844,	0.8320 s / batch. (data: 1.20e-02). ETA=11:13:25, max mem: 20.9 GB 
[11/25 04:07:17 visual_prompt]: 	Training 200/553. train loss: 0.7033,	0.8221 s / batch. (data: 6.09e-03). ETA=11:04:00, max mem: 20.9 GB 
[11/25 04:09:00 visual_prompt]: 	Training 300/553. train loss: 0.9773,	1.8915 s / batch. (data: 1.07e+00). ETA=1 day, 1:24:40, max mem: 20.9 GB 
[11/25 04:10:41 visual_prompt]: 	Training 400/553. train loss: 3.7853,	0.8596 s / batch. (data: 2.36e-02). ETA=11:31:26, max mem: 20.9 GB 
[11/25 04:12:24 visual_prompt]: 	Training 500/553. train loss: 1.1160,	0.8215 s / batch. (data: 5.50e-03). ETA=10:59:28, max mem: 20.9 GB 
[11/25 04:13:17 visual_prompt]: Epoch 13 / 100: avg data time: 1.98e-01, avg batch time: 1.0247, average train loss: 1.2169
[11/25 04:14:16 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3079, average loss: 0.9227
[11/25 04:14:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.10	
[11/25 04:14:16 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/25 04:16:03 visual_prompt]: 	Training 100/553. train loss: 0.9061,	0.8152 s / batch. (data: 3.77e-04). ETA=10:52:19, max mem: 20.9 GB 
[11/25 04:17:44 visual_prompt]: 	Training 200/553. train loss: 0.0361,	0.8414 s / batch. (data: 5.48e-03). ETA=11:11:50, max mem: 20.9 GB 
[11/25 04:19:27 visual_prompt]: 	Training 300/553. train loss: 0.6723,	0.8468 s / batch. (data: 3.37e-04). ETA=11:14:48, max mem: 20.9 GB 
[11/25 04:21:08 visual_prompt]: 	Training 400/553. train loss: 0.6289,	0.8400 s / batch. (data: 1.20e-02). ETA=11:07:57, max mem: 20.9 GB 
[11/25 04:22:51 visual_prompt]: 	Training 500/553. train loss: 1.1140,	0.8725 s / batch. (data: 2.86e-02). ETA=11:32:19, max mem: 20.9 GB 
[11/25 04:23:42 visual_prompt]: Epoch 14 / 100: avg data time: 1.99e-01, avg batch time: 1.0244, average train loss: 1.2065
[11/25 04:24:41 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3068, average loss: 0.6908
[11/25 04:24:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.98	
[11/25 04:24:41 visual_prompt]: Best epoch 14: best metric: -0.691
[11/25 04:24:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/25 04:26:27 visual_prompt]: 	Training 100/553. train loss: 0.8596,	0.8364 s / batch. (data: 1.56e-02). ETA=11:01:33, max mem: 20.9 GB 
[11/25 04:28:08 visual_prompt]: 	Training 200/553. train loss: 4.4659,	0.8400 s / batch. (data: 7.97e-03). ETA=11:03:00, max mem: 20.9 GB 
[11/25 04:29:52 visual_prompt]: 	Training 300/553. train loss: 0.6629,	0.8480 s / batch. (data: 2.99e-04). ETA=11:07:53, max mem: 20.9 GB 
[11/25 04:31:31 visual_prompt]: 	Training 400/553. train loss: 1.2398,	0.8320 s / batch. (data: 3.38e-04). ETA=10:53:55, max mem: 20.9 GB 
[11/25 04:33:14 visual_prompt]: 	Training 500/553. train loss: 0.5912,	0.8120 s / batch. (data: 3.66e-04). ETA=10:36:49, max mem: 20.9 GB 
[11/25 04:34:08 visual_prompt]: Epoch 15 / 100: avg data time: 1.98e-01, avg batch time: 1.0248, average train loss: 1.2034
[11/25 04:35:07 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3076, average loss: 1.0720
[11/25 04:35:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.47	
[11/25 04:35:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/25 04:36:51 visual_prompt]: 	Training 100/553. train loss: 0.7801,	0.8192 s / batch. (data: 3.29e-04). ETA=10:40:24, max mem: 20.9 GB 
[11/25 04:38:34 visual_prompt]: 	Training 200/553. train loss: 1.8266,	0.8302 s / batch. (data: 1.05e-02). ETA=10:47:38, max mem: 20.9 GB 
[11/25 04:40:16 visual_prompt]: 	Training 300/553. train loss: 1.1724,	0.8269 s / batch. (data: 3.13e-04). ETA=10:43:39, max mem: 20.9 GB 
[11/25 04:41:58 visual_prompt]: 	Training 400/553. train loss: 0.9592,	0.8341 s / batch. (data: 3.32e-04). ETA=10:47:53, max mem: 20.9 GB 
[11/25 04:43:39 visual_prompt]: 	Training 500/553. train loss: 0.6301,	1.3991 s / batch. (data: 5.50e-01). ETA=18:04:27, max mem: 20.9 GB 
[11/25 04:44:33 visual_prompt]: Epoch 16 / 100: avg data time: 1.98e-01, avg batch time: 1.0241, average train loss: 0.9768
[11/25 04:45:32 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.3074, average loss: 0.8237
[11/25 04:45:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.02	
[11/25 04:45:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/25 04:47:17 visual_prompt]: 	Training 100/553. train loss: 0.9535,	0.8400 s / batch. (data: 3.45e-04). ETA=10:48:55, max mem: 20.9 GB 
[11/25 04:49:00 visual_prompt]: 	Training 200/553. train loss: 2.9774,	0.8248 s / batch. (data: 3.11e-04). ETA=10:35:49, max mem: 20.9 GB 
[11/25 04:50:42 visual_prompt]: 	Training 300/553. train loss: 1.5363,	0.8102 s / batch. (data: 4.19e-04). ETA=10:23:14, max mem: 20.9 GB 
[11/25 04:52:23 visual_prompt]: 	Training 400/553. train loss: 0.6225,	0.8135 s / batch. (data: 3.21e-04). ETA=10:24:21, max mem: 20.9 GB 
[11/25 04:54:04 visual_prompt]: 	Training 500/553. train loss: 1.1883,	1.6440 s / batch. (data: 8.18e-01). ETA=20:59:05, max mem: 20.9 GB 
[11/25 04:54:59 visual_prompt]: Epoch 17 / 100: avg data time: 2.00e-01, avg batch time: 1.0260, average train loss: 1.1318
[11/25 04:55:58 visual_prompt]: Inference (val):avg data time: 4.59e-05, avg batch time: 0.3087, average loss: 0.8863
[11/25 04:55:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.78	
[11/25 04:55:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/25 04:57:44 visual_prompt]: 	Training 100/553. train loss: 0.7490,	0.8222 s / batch. (data: 1.20e-02). ETA=10:27:34, max mem: 20.9 GB 
[11/25 04:59:29 visual_prompt]: 	Training 200/553. train loss: 0.6182,	0.8236 s / batch. (data: 3.23e-04). ETA=10:27:19, max mem: 20.9 GB 
[11/25 05:01:11 visual_prompt]: 	Training 300/553. train loss: 0.6582,	0.8440 s / batch. (data: 1.20e-02). ETA=10:41:27, max mem: 20.9 GB 
[11/25 05:02:51 visual_prompt]: 	Training 400/553. train loss: 2.0270,	0.8114 s / batch. (data: 3.24e-04). ETA=10:15:18, max mem: 20.9 GB 
[11/25 05:04:32 visual_prompt]: 	Training 500/553. train loss: 0.7826,	0.8400 s / batch. (data: 3.98e-03). ETA=10:35:35, max mem: 20.9 GB 
[11/25 05:05:24 visual_prompt]: Epoch 18 / 100: avg data time: 1.97e-01, avg batch time: 1.0234, average train loss: 1.3795
[11/25 05:06:22 visual_prompt]: Inference (val):avg data time: 2.83e-04, avg batch time: 0.3091, average loss: 0.6943
[11/25 05:06:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.74	
[11/25 05:06:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[11/25 05:08:08 visual_prompt]: 	Training 100/553. train loss: 0.6935,	0.8108 s / batch. (data: 1.31e-03). ETA=10:11:25, max mem: 20.9 GB 
[11/25 05:09:51 visual_prompt]: 	Training 200/553. train loss: 0.7440,	0.8240 s / batch. (data: 3.18e-04). ETA=10:20:00, max mem: 20.9 GB 
[11/25 05:11:33 visual_prompt]: 	Training 300/553. train loss: 0.3663,	0.8200 s / batch. (data: 3.38e-04). ETA=10:15:38, max mem: 20.9 GB 
[11/25 05:13:17 visual_prompt]: 	Training 400/553. train loss: 0.6592,	0.8400 s / batch. (data: 8.27e-04). ETA=10:29:14, max mem: 20.9 GB 
[11/25 05:14:54 visual_prompt]: 	Training 500/553. train loss: 0.8467,	0.8475 s / batch. (data: 1.56e-02). ETA=10:33:26, max mem: 20.9 GB 
[11/25 05:15:48 visual_prompt]: Epoch 19 / 100: avg data time: 1.95e-01, avg batch time: 1.0222, average train loss: 1.1908
[11/25 05:16:46 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.3065, average loss: 4.5945
[11/25 05:16:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.63	
[11/25 05:16:46 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[11/25 05:18:30 visual_prompt]: 	Training 100/553. train loss: 0.6789,	0.8239 s / batch. (data: 3.40e-04). ETA=10:13:43, max mem: 20.9 GB 
[11/25 05:20:14 visual_prompt]: 	Training 200/553. train loss: 0.6506,	0.8480 s / batch. (data: 7.95e-03). ETA=10:30:14, max mem: 20.9 GB 
[11/25 05:21:56 visual_prompt]: 	Training 300/553. train loss: 0.6597,	0.8400 s / batch. (data: 3.05e-04). ETA=10:22:55, max mem: 20.9 GB 
[11/25 05:23:37 visual_prompt]: 	Training 400/553. train loss: 0.7239,	0.8122 s / batch. (data: 3.54e-04). ETA=10:00:57, max mem: 20.9 GB 
[11/25 05:25:19 visual_prompt]: 	Training 500/553. train loss: 0.7731,	0.8178 s / batch. (data: 7.96e-03). ETA=10:03:44, max mem: 20.9 GB 
[11/25 05:26:13 visual_prompt]: Epoch 20 / 100: avg data time: 2.00e-01, avg batch time: 1.0253, average train loss: 1.4464
[11/25 05:27:12 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.3076, average loss: 0.8541
[11/25 05:27:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.39	
[11/25 05:27:12 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[11/25 05:29:00 visual_prompt]: 	Training 100/553. train loss: 1.5329,	0.9840 s / batch. (data: 1.38e-01). ETA=12:03:54, max mem: 20.9 GB 
[11/25 05:30:41 visual_prompt]: 	Training 200/553. train loss: 0.8960,	0.8273 s / batch. (data: 3.13e-04). ETA=10:07:15, max mem: 20.9 GB 
[11/25 05:32:23 visual_prompt]: 	Training 300/553. train loss: 3.4251,	0.9640 s / batch. (data: 1.42e-01). ETA=11:45:58, max mem: 20.9 GB 
[11/25 05:34:04 visual_prompt]: 	Training 400/553. train loss: 1.6836,	0.8298 s / batch. (data: 3.09e-04). ETA=10:06:19, max mem: 20.9 GB 
[11/25 05:35:47 visual_prompt]: 	Training 500/553. train loss: 0.7203,	0.8664 s / batch. (data: 3.29e-04). ETA=10:31:36, max mem: 20.9 GB 
[11/25 05:36:39 visual_prompt]: Epoch 21 / 100: avg data time: 1.99e-01, avg batch time: 1.0248, average train loss: 1.2872
[11/25 05:37:37 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3077, average loss: 0.8163
[11/25 05:37:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.15	
[11/25 05:37:37 visual_prompt]: Stopping early.
[11/25 05:37:37 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 05:37:37 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 05:37:37 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/25 05:37:37 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 05:37:37 visual_prompt]: Training with config:
[11/25 05:37:37 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/25 05:37:37 visual_prompt]: Loading training data...
[11/25 05:37:37 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 05:37:37 visual_prompt]: Loading validation data...
[11/25 05:37:37 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 05:37:37 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 05:37:40 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 05:37:40 visual_prompt]: tuned percent:0.525
[11/25 05:37:40 visual_prompt]: Device used for model: 0
[11/25 05:37:40 visual_prompt]: Setting up Evaluator...
[11/25 05:37:40 visual_prompt]: Setting up Trainer...
[11/25 05:37:40 visual_prompt]: 	Setting up the optimizer...
[11/25 05:37:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 05:39:26 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8360 s / batch. (data: 7.98e-03). ETA=12:49:07, max mem: 20.9 GB 
[11/25 05:41:07 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8303 s / batch. (data: 1.20e-02). ETA=12:42:31, max mem: 20.9 GB 
[11/25 05:42:52 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.5545 s / batch. (data: 7.31e-01). ETA=23:44:55, max mem: 20.9 GB 
[11/25 05:44:32 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8320 s / batch. (data: 3.12e-04). ETA=12:41:17, max mem: 20.9 GB 
[11/25 05:46:16 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8239 s / batch. (data: 3.14e-04). ETA=12:32:30, max mem: 20.9 GB 
[11/25 05:47:10 visual_prompt]: Epoch 1 / 100: avg data time: 2.04e-01, avg batch time: 1.0300, average train loss: 1.5403
[11/25 05:48:09 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3085, average loss: 1.5201
[11/25 05:48:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 05:48:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/25 05:49:55 visual_prompt]: 	Training 100/553. train loss: 0.7679,	0.8189 s / batch. (data: 8.19e-03). ETA=12:25:50, max mem: 20.9 GB 
[11/25 05:51:37 visual_prompt]: 	Training 200/553. train loss: 0.0953,	0.8400 s / batch. (data: 3.37e-04). ETA=12:43:38, max mem: 20.9 GB 
[11/25 05:53:20 visual_prompt]: 	Training 300/553. train loss: 0.8468,	1.0459 s / batch. (data: 2.29e-01). ETA=15:49:05, max mem: 20.9 GB 
[11/25 05:55:01 visual_prompt]: 	Training 400/553. train loss: 1.5797,	0.8158 s / batch. (data: 4.28e-04). ETA=12:18:53, max mem: 20.9 GB 
[11/25 05:56:45 visual_prompt]: 	Training 500/553. train loss: 0.5381,	0.8232 s / batch. (data: 3.15e-04). ETA=12:24:15, max mem: 20.9 GB 
[11/25 05:57:37 visual_prompt]: Epoch 2 / 100: avg data time: 2.01e-01, avg batch time: 1.0271, average train loss: 0.9151
[11/25 05:58:36 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3083, average loss: 1.2192
[11/25 05:58:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.14	
[11/25 05:58:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/25 06:00:20 visual_prompt]: 	Training 100/553. train loss: 0.7689,	0.8865 s / batch. (data: 6.71e-02). ETA=13:19:15, max mem: 20.9 GB 
[11/25 06:02:03 visual_prompt]: 	Training 200/553. train loss: 0.8610,	1.5456 s / batch. (data: 7.19e-01). ETA=23:10:51, max mem: 20.9 GB 
[11/25 06:03:44 visual_prompt]: 	Training 300/553. train loss: 0.6059,	0.8397 s / batch. (data: 1.58e-02). ETA=12:34:13, max mem: 20.9 GB 
[11/25 06:05:27 visual_prompt]: 	Training 400/553. train loss: 1.8079,	0.8274 s / batch. (data: 3.76e-04). ETA=12:21:49, max mem: 20.9 GB 
[11/25 06:07:10 visual_prompt]: 	Training 500/553. train loss: 0.8238,	1.3920 s / batch. (data: 5.57e-01). ETA=20:45:41, max mem: 20.9 GB 
[11/25 06:08:02 visual_prompt]: Epoch 3 / 100: avg data time: 1.98e-01, avg batch time: 1.0243, average train loss: 0.8850
[11/25 06:09:01 visual_prompt]: Inference (val):avg data time: 4.56e-05, avg batch time: 0.3072, average loss: 0.7381
[11/25 06:09:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.87	
[11/25 06:09:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/25 06:10:49 visual_prompt]: 	Training 100/553. train loss: 0.8136,	0.8148 s / batch. (data: 3.11e-04). ETA=12:07:05, max mem: 20.9 GB 
[11/25 06:12:31 visual_prompt]: 	Training 200/553. train loss: 0.5487,	0.8445 s / batch. (data: 3.40e-04). ETA=12:32:10, max mem: 20.9 GB 
[11/25 06:14:13 visual_prompt]: 	Training 300/553. train loss: 0.9565,	1.5318 s / batch. (data: 7.17e-01). ETA=22:41:45, max mem: 20.9 GB 
[11/25 06:15:51 visual_prompt]: 	Training 400/553. train loss: 1.2003,	1.4367 s / batch. (data: 6.02e-01). ETA=21:14:50, max mem: 20.9 GB 
[11/25 06:17:35 visual_prompt]: 	Training 500/553. train loss: 0.2921,	3.9315 s / batch. (data: 3.12e+00). ETA=2 days, 10:02:04, max mem: 20.9 GB 
[11/25 06:18:29 visual_prompt]: Epoch 4 / 100: avg data time: 2.01e-01, avg batch time: 1.0282, average train loss: 0.9867
[11/25 06:19:28 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3073, average loss: 1.5527
[11/25 06:19:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.95	
[11/25 06:19:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/25 06:21:13 visual_prompt]: 	Training 100/553. train loss: 2.0901,	0.8221 s / batch. (data: 3.22e-04). ETA=12:06:01, max mem: 20.9 GB 
[11/25 06:22:55 visual_prompt]: 	Training 200/553. train loss: 0.8485,	1.3411 s / batch. (data: 5.31e-01). ETA=19:42:10, max mem: 20.9 GB 
[11/25 06:24:38 visual_prompt]: 	Training 300/553. train loss: 1.4419,	0.8373 s / batch. (data: 3.15e-04). ETA=12:16:40, max mem: 20.9 GB 
[11/25 06:26:19 visual_prompt]: 	Training 400/553. train loss: 1.5371,	0.8360 s / batch. (data: 3.19e-04). ETA=12:14:07, max mem: 20.9 GB 
[11/25 06:28:01 visual_prompt]: 	Training 500/553. train loss: 0.5277,	0.8197 s / batch. (data: 5.44e-03). ETA=11:58:28, max mem: 20.9 GB 
[11/25 06:28:55 visual_prompt]: Epoch 5 / 100: avg data time: 1.99e-01, avg batch time: 1.0252, average train loss: 1.0075
[11/25 06:29:54 visual_prompt]: Inference (val):avg data time: 1.63e-04, avg batch time: 0.3080, average loss: 1.4005
[11/25 06:29:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.86	
[11/25 06:29:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/25 06:31:41 visual_prompt]: 	Training 100/553. train loss: 0.6448,	0.8537 s / batch. (data: 5.98e-03). ETA=12:26:03, max mem: 20.9 GB 
[11/25 06:33:23 visual_prompt]: 	Training 200/553. train loss: 2.5418,	0.8248 s / batch. (data: 3.16e-04). ETA=11:59:26, max mem: 20.9 GB 
[11/25 06:35:03 visual_prompt]: 	Training 300/553. train loss: 0.5570,	0.8200 s / batch. (data: 3.21e-04). ETA=11:53:53, max mem: 20.9 GB 
[11/25 06:36:49 visual_prompt]: 	Training 400/553. train loss: 0.8850,	0.8203 s / batch. (data: 3.59e-04). ETA=11:52:45, max mem: 20.9 GB 
[11/25 06:38:29 visual_prompt]: 	Training 500/553. train loss: 1.2508,	0.8760 s / batch. (data: 5.93e-02). ETA=12:39:42, max mem: 20.9 GB 
[11/25 06:39:22 visual_prompt]: Epoch 6 / 100: avg data time: 2.03e-01, avg batch time: 1.0286, average train loss: 1.0922
[11/25 06:40:21 visual_prompt]: Inference (val):avg data time: 2.65e-04, avg batch time: 0.3081, average loss: 1.2903
[11/25 06:40:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.99	
[11/25 06:40:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/25 06:42:06 visual_prompt]: 	Training 100/553. train loss: 2.6519,	0.8414 s / batch. (data: 9.34e-03). ETA=12:07:32, max mem: 20.9 GB 
[11/25 06:43:47 visual_prompt]: 	Training 200/553. train loss: 0.5422,	0.9352 s / batch. (data: 1.08e-01). ETA=13:27:08, max mem: 20.9 GB 
[11/25 06:45:33 visual_prompt]: 	Training 300/553. train loss: 0.7642,	1.6389 s / batch. (data: 8.14e-01). ETA=23:31:40, max mem: 20.9 GB 
[11/25 06:47:14 visual_prompt]: 	Training 400/553. train loss: 0.5958,	1.3389 s / batch. (data: 5.17e-01). ETA=19:11:02, max mem: 20.9 GB 
[11/25 06:48:54 visual_prompt]: 	Training 500/553. train loss: 1.4260,	0.8455 s / batch. (data: 1.05e-02). ETA=12:05:30, max mem: 20.9 GB 
[11/25 06:49:47 visual_prompt]: Epoch 7 / 100: avg data time: 1.97e-01, avg batch time: 1.0225, average train loss: 0.9920
[11/25 06:50:45 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.3083, average loss: 0.6865
[11/25 06:50:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 63.35	
[11/25 06:50:45 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/25 06:52:29 visual_prompt]: 	Training 100/553. train loss: 1.1387,	0.8114 s / batch. (data: 3.37e-04). ETA=11:34:10, max mem: 20.9 GB 
[11/25 06:54:13 visual_prompt]: 	Training 200/553. train loss: 1.0875,	0.8480 s / batch. (data: 2.48e-04). ETA=12:04:02, max mem: 20.9 GB 
[11/25 06:55:56 visual_prompt]: 	Training 300/553. train loss: 1.8679,	0.8451 s / batch. (data: 9.10e-03). ETA=12:00:10, max mem: 20.9 GB 
[11/25 06:57:38 visual_prompt]: 	Training 400/553. train loss: 0.7211,	0.9833 s / batch. (data: 1.57e-01). ETA=13:56:17, max mem: 20.9 GB 
[11/25 06:59:21 visual_prompt]: 	Training 500/553. train loss: 1.6014,	1.6800 s / batch. (data: 8.57e-01). ETA=23:45:59, max mem: 20.9 GB 
[11/25 07:00:14 visual_prompt]: Epoch 8 / 100: avg data time: 2.01e-01, avg batch time: 1.0276, average train loss: 1.1053
[11/25 07:01:13 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3083, average loss: 0.6950
[11/25 07:01:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 60.88	
[11/25 07:01:13 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/25 07:02:59 visual_prompt]: 	Training 100/553. train loss: 0.1361,	0.8383 s / batch. (data: 5.46e-03). ETA=11:49:27, max mem: 20.9 GB 
[11/25 07:04:40 visual_prompt]: 	Training 200/553. train loss: 0.7272,	0.8234 s / batch. (data: 3.07e-04). ETA=11:35:26, max mem: 20.9 GB 
[11/25 07:06:22 visual_prompt]: 	Training 300/553. train loss: 0.7011,	1.9200 s / batch. (data: 1.08e+00). ETA=1 day, 2:58:26, max mem: 20.9 GB 
[11/25 07:08:06 visual_prompt]: 	Training 400/553. train loss: 0.6741,	0.8360 s / batch. (data: 8.76e-04). ETA=11:43:16, max mem: 20.9 GB 
[11/25 07:09:49 visual_prompt]: 	Training 500/553. train loss: 1.0512,	1.1213 s / batch. (data: 3.05e-01). ETA=15:41:25, max mem: 20.9 GB 
[11/25 07:10:41 visual_prompt]: Epoch 9 / 100: avg data time: 2.01e-01, avg batch time: 1.0269, average train loss: 0.8821
[11/25 07:11:39 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.3067, average loss: 1.2095
[11/25 07:11:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.04	
[11/25 07:11:39 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/25 07:13:28 visual_prompt]: 	Training 100/553. train loss: 1.7716,	0.8672 s / batch. (data: 1.07e-02). ETA=12:05:51, max mem: 20.9 GB 
[11/25 07:15:08 visual_prompt]: 	Training 200/553. train loss: 0.7371,	0.8352 s / batch. (data: 3.26e-04). ETA=11:37:44, max mem: 20.9 GB 
[11/25 07:16:49 visual_prompt]: 	Training 300/553. train loss: 0.4131,	1.2948 s / batch. (data: 4.84e-01). ETA=17:59:30, max mem: 20.9 GB 
[11/25 07:18:29 visual_prompt]: 	Training 400/553. train loss: 0.6812,	0.8237 s / batch. (data: 3.26e-04). ETA=11:25:20, max mem: 20.9 GB 
[11/25 07:20:12 visual_prompt]: 	Training 500/553. train loss: 0.5299,	0.8245 s / batch. (data: 3.29e-04). ETA=11:24:38, max mem: 20.9 GB 
[11/25 07:21:06 visual_prompt]: Epoch 10 / 100: avg data time: 1.98e-01, avg batch time: 1.0242, average train loss: 1.1561
[11/25 07:22:05 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3083, average loss: 0.7289
[11/25 07:22:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.17	
[11/25 07:22:05 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/25 07:23:53 visual_prompt]: 	Training 100/553. train loss: 2.1311,	0.8304 s / batch. (data: 8.40e-03). ETA=11:27:27, max mem: 20.9 GB 
[11/25 07:25:37 visual_prompt]: 	Training 200/553. train loss: 1.6184,	0.8605 s / batch. (data: 2.07e-02). ETA=11:50:52, max mem: 20.9 GB 
[11/25 07:27:18 visual_prompt]: 	Training 300/553. train loss: 0.0302,	2.2840 s / batch. (data: 1.46e+00). ETA=1 day, 7:23:09, max mem: 20.9 GB 
[11/25 07:28:58 visual_prompt]: 	Training 400/553. train loss: 0.6176,	0.8280 s / batch. (data: 5.45e-03). ETA=11:21:17, max mem: 20.9 GB 
[11/25 07:30:39 visual_prompt]: 	Training 500/553. train loss: 0.7791,	0.8200 s / batch. (data: 3.11e-04). ETA=11:13:21, max mem: 20.9 GB 
[11/25 07:31:31 visual_prompt]: Epoch 11 / 100: avg data time: 1.97e-01, avg batch time: 1.0247, average train loss: 0.9843
[11/25 07:32:30 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3083, average loss: 0.6849
[11/25 07:32:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.17	
[11/25 07:32:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/25 07:34:18 visual_prompt]: 	Training 100/553. train loss: 0.7765,	0.9063 s / batch. (data: 9.86e-02). ETA=12:21:52, max mem: 20.9 GB 
[11/25 07:36:01 visual_prompt]: 	Training 200/553. train loss: 0.6089,	0.8095 s / batch. (data: 3.38e-04). ETA=11:01:21, max mem: 20.9 GB 
[11/25 07:37:42 visual_prompt]: 	Training 300/553. train loss: 0.7602,	0.8370 s / batch. (data: 1.05e-02). ETA=11:22:22, max mem: 20.9 GB 
[11/25 07:39:23 visual_prompt]: 	Training 400/553. train loss: 0.5513,	0.8464 s / batch. (data: 3.30e-04). ETA=11:28:39, max mem: 20.9 GB 
[11/25 07:41:06 visual_prompt]: 	Training 500/553. train loss: 4.2357,	0.8461 s / batch. (data: 1.56e-02). ETA=11:27:01, max mem: 20.9 GB 
[11/25 07:41:58 visual_prompt]: Epoch 12 / 100: avg data time: 2.01e-01, avg batch time: 1.0271, average train loss: 1.0818
[11/25 07:42:57 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3071, average loss: 1.6687
[11/25 07:42:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.49	
[11/25 07:42:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/25 07:44:45 visual_prompt]: 	Training 100/553. train loss: 0.5888,	0.8208 s / batch. (data: 1.05e-02). ETA=11:04:21, max mem: 20.9 GB 
[11/25 07:46:24 visual_prompt]: 	Training 200/553. train loss: 0.6360,	0.8560 s / batch. (data: 3.22e-04). ETA=11:31:25, max mem: 20.9 GB 
[11/25 07:48:07 visual_prompt]: 	Training 300/553. train loss: 0.5082,	1.8445 s / batch. (data: 1.04e+00). ETA=1 day, 0:46:47, max mem: 20.9 GB 
[11/25 07:49:47 visual_prompt]: 	Training 400/553. train loss: 4.0660,	0.8435 s / batch. (data: 2.88e-04). ETA=11:18:30, max mem: 20.9 GB 
[11/25 07:51:31 visual_prompt]: 	Training 500/553. train loss: 0.8391,	0.8520 s / batch. (data: 1.19e-02). ETA=11:23:54, max mem: 20.9 GB 
[11/25 07:52:24 visual_prompt]: Epoch 13 / 100: avg data time: 1.99e-01, avg batch time: 1.0250, average train loss: 1.0679
[11/25 07:53:22 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.3070, average loss: 1.2025
[11/25 07:53:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.58	
[11/25 07:53:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/25 07:55:10 visual_prompt]: 	Training 100/553. train loss: 0.6127,	0.8225 s / batch. (data: 3.09e-04). ETA=10:58:10, max mem: 20.9 GB 
[11/25 07:56:51 visual_prompt]: 	Training 200/553. train loss: 0.1792,	1.4080 s / batch. (data: 5.89e-01). ETA=18:44:18, max mem: 20.9 GB 
[11/25 07:58:33 visual_prompt]: 	Training 300/553. train loss: 0.7110,	0.8340 s / batch. (data: 1.20e-02). ETA=11:04:33, max mem: 20.9 GB 
[11/25 08:00:16 visual_prompt]: 	Training 400/553. train loss: 0.6591,	0.8683 s / batch. (data: 2.07e-02). ETA=11:30:25, max mem: 20.9 GB 
[11/25 08:01:57 visual_prompt]: 	Training 500/553. train loss: 2.0891,	0.8264 s / batch. (data: 5.45e-03). ETA=10:55:46, max mem: 20.9 GB 
[11/25 08:02:51 visual_prompt]: Epoch 14 / 100: avg data time: 2.02e-01, avg batch time: 1.0271, average train loss: 1.1317
[11/25 08:03:49 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3052, average loss: 0.7286
[11/25 08:03:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.69	
[11/25 08:03:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/25 08:05:35 visual_prompt]: 	Training 100/553. train loss: 1.1146,	0.9150 s / batch. (data: 1.05e-01). ETA=12:03:46, max mem: 20.9 GB 
[11/25 08:07:16 visual_prompt]: 	Training 200/553. train loss: 6.5137,	0.8480 s / batch. (data: 1.20e-02). ETA=11:09:18, max mem: 20.9 GB 
[11/25 08:09:00 visual_prompt]: 	Training 300/553. train loss: 1.6192,	0.8124 s / batch. (data: 3.15e-04). ETA=10:39:51, max mem: 20.9 GB 
[11/25 08:10:39 visual_prompt]: 	Training 400/553. train loss: 1.1032,	0.8118 s / batch. (data: 3.20e-04). ETA=10:38:04, max mem: 20.9 GB 
[11/25 08:12:22 visual_prompt]: 	Training 500/553. train loss: 0.6259,	0.8520 s / batch. (data: 3.44e-04). ETA=11:08:12, max mem: 20.9 GB 
[11/25 08:13:16 visual_prompt]: Epoch 15 / 100: avg data time: 1.98e-01, avg batch time: 1.0242, average train loss: 1.3634
[11/25 08:14:14 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3058, average loss: 2.0656
[11/25 08:14:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.88	
[11/25 08:14:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/25 08:15:59 visual_prompt]: 	Training 100/553. train loss: 0.8009,	0.8356 s / batch. (data: 1.20e-02). ETA=10:53:11, max mem: 20.9 GB 
[11/25 08:17:42 visual_prompt]: 	Training 200/553. train loss: 1.4988,	0.8321 s / batch. (data: 1.20e-02). ETA=10:49:04, max mem: 20.9 GB 
[11/25 08:19:24 visual_prompt]: 	Training 300/553. train loss: 1.0280,	0.8518 s / batch. (data: 3.17e-04). ETA=11:03:04, max mem: 20.9 GB 
[11/25 08:21:06 visual_prompt]: 	Training 400/553. train loss: 0.6460,	0.8178 s / batch. (data: 8.00e-03). ETA=10:35:11, max mem: 20.9 GB 
[11/25 08:22:47 visual_prompt]: 	Training 500/553. train loss: 0.9618,	1.2306 s / batch. (data: 4.06e-01). ETA=15:53:48, max mem: 20.9 GB 
[11/25 08:23:41 visual_prompt]: Epoch 16 / 100: avg data time: 1.99e-01, avg batch time: 1.0254, average train loss: 0.9734
[11/25 08:24:40 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3083, average loss: 0.6828
[11/25 08:24:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.31	
[11/25 08:24:40 visual_prompt]: Best epoch 16: best metric: -0.683
[11/25 08:24:40 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/25 08:26:25 visual_prompt]: 	Training 100/553. train loss: 0.8205,	0.8371 s / batch. (data: 1.56e-02). ETA=10:46:42, max mem: 20.9 GB 
[11/25 08:28:09 visual_prompt]: 	Training 200/553. train loss: 1.7765,	0.8142 s / batch. (data: 3.48e-04). ETA=10:27:37, max mem: 20.9 GB 
[11/25 08:29:50 visual_prompt]: 	Training 300/553. train loss: 1.4992,	0.8440 s / batch. (data: 5.48e-03). ETA=10:49:13, max mem: 20.9 GB 
[11/25 08:31:31 visual_prompt]: 	Training 400/553. train loss: 0.9255,	0.9360 s / batch. (data: 9.51e-02). ETA=11:58:25, max mem: 20.9 GB 
[11/25 08:33:13 visual_prompt]: 	Training 500/553. train loss: 0.6938,	1.3145 s / batch. (data: 4.90e-01). ETA=16:46:43, max mem: 20.9 GB 
[11/25 08:34:07 visual_prompt]: Epoch 17 / 100: avg data time: 2.01e-01, avg batch time: 1.0257, average train loss: 0.9912
[11/25 08:35:14 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3067, average loss: 0.7131
[11/25 08:35:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.57	
[11/25 08:35:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/25 08:37:00 visual_prompt]: 	Training 100/553. train loss: 0.7639,	0.8280 s / batch. (data: 3.33e-04). ETA=10:32:00, max mem: 20.9 GB 
[11/25 08:38:45 visual_prompt]: 	Training 200/553. train loss: 1.1218,	0.8476 s / batch. (data: 5.48e-03). ETA=10:45:34, max mem: 20.9 GB 
[11/25 08:40:27 visual_prompt]: 	Training 300/553. train loss: 0.4815,	0.8098 s / batch. (data: 3.46e-04). ETA=10:15:28, max mem: 20.9 GB 
[11/25 08:42:09 visual_prompt]: 	Training 400/553. train loss: 0.9788,	0.8240 s / batch. (data: 7.71e-03). ETA=10:24:52, max mem: 20.9 GB 
[11/25 08:43:50 visual_prompt]: 	Training 500/553. train loss: 1.5766,	1.0308 s / batch. (data: 1.95e-01). ETA=12:59:55, max mem: 20.9 GB 
[11/25 08:44:42 visual_prompt]: Epoch 18 / 100: avg data time: 1.99e-01, avg batch time: 1.0264, average train loss: 1.3131
[11/25 08:45:40 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3083, average loss: 0.9938
[11/25 08:45:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.91	
[11/25 08:45:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[11/25 08:47:26 visual_prompt]: 	Training 100/553. train loss: 0.8804,	1.3201 s / batch. (data: 5.03e-01). ETA=16:35:27, max mem: 20.9 GB 
[11/25 08:49:09 visual_prompt]: 	Training 200/553. train loss: 0.7377,	0.8300 s / batch. (data: 7.96e-03). ETA=10:24:30, max mem: 20.9 GB 
[11/25 08:50:51 visual_prompt]: 	Training 300/553. train loss: 3.0461,	0.8884 s / batch. (data: 6.65e-02). ETA=11:06:57, max mem: 20.9 GB 
[11/25 08:52:35 visual_prompt]: 	Training 400/553. train loss: 0.6691,	0.8240 s / batch. (data: 3.17e-04). ETA=10:17:16, max mem: 20.9 GB 
[11/25 08:54:12 visual_prompt]: 	Training 500/553. train loss: 0.5790,	0.8394 s / batch. (data: 1.13e-02). ETA=10:27:21, max mem: 20.9 GB 
[11/25 08:55:05 visual_prompt]: Epoch 19 / 100: avg data time: 1.95e-01, avg batch time: 1.0213, average train loss: 0.9104
[11/25 08:56:04 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3092, average loss: 1.7240
[11/25 08:56:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.08	
[11/25 08:56:04 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[11/25 08:57:47 visual_prompt]: 	Training 100/553. train loss: 0.7645,	0.8434 s / batch. (data: 3.30e-04). ETA=10:28:12, max mem: 20.9 GB 
[11/25 08:59:31 visual_prompt]: 	Training 200/553. train loss: 0.4673,	0.8280 s / batch. (data: 3.33e-04). ETA=10:15:23, max mem: 20.9 GB 
[11/25 09:01:14 visual_prompt]: 	Training 300/553. train loss: 1.8378,	0.8301 s / batch. (data: 9.69e-03). ETA=10:15:34, max mem: 20.9 GB 
[11/25 09:02:55 visual_prompt]: 	Training 400/553. train loss: 0.5997,	0.8489 s / batch. (data: 2.50e-02). ETA=10:28:03, max mem: 20.9 GB 
[11/25 09:04:36 visual_prompt]: 	Training 500/553. train loss: 0.7122,	0.8397 s / batch. (data: 3.19e-04). ETA=10:19:53, max mem: 20.9 GB 
[11/25 09:05:31 visual_prompt]: Epoch 20 / 100: avg data time: 2.00e-01, avg batch time: 1.0256, average train loss: 0.9603
[11/25 09:06:29 visual_prompt]: Inference (val):avg data time: 2.23e-04, avg batch time: 0.3076, average loss: 0.6917
[11/25 09:06:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 64.64	
[11/25 09:06:29 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[11/25 09:08:18 visual_prompt]: 	Training 100/553. train loss: 0.9778,	0.8360 s / batch. (data: 1.19e-02). ETA=10:15:00, max mem: 20.9 GB 
[11/25 09:09:59 visual_prompt]: 	Training 200/553. train loss: 1.2474,	0.8500 s / batch. (data: 5.45e-03). ETA=10:23:55, max mem: 20.9 GB 
[11/25 09:11:41 visual_prompt]: 	Training 300/553. train loss: 2.5566,	1.1392 s / batch. (data: 3.11e-01). ETA=13:54:16, max mem: 20.9 GB 
[11/25 09:13:21 visual_prompt]: 	Training 400/553. train loss: 1.3980,	0.8320 s / batch. (data: 3.03e-04). ETA=10:07:54, max mem: 20.9 GB 
[11/25 09:15:05 visual_prompt]: 	Training 500/553. train loss: 0.8321,	0.8400 s / batch. (data: 3.15e-04). ETA=10:12:20, max mem: 20.9 GB 
[11/25 09:15:57 visual_prompt]: Epoch 21 / 100: avg data time: 2.00e-01, avg batch time: 1.0256, average train loss: 0.9988
[11/25 09:16:55 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3068, average loss: 0.7338
[11/25 09:16:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 59.84	
[11/25 09:16:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[11/25 09:18:40 visual_prompt]: 	Training 100/553. train loss: 1.1736,	0.8112 s / batch. (data: 3.08e-04). ETA=9:49:17, max mem: 20.9 GB 
[11/25 09:20:23 visual_prompt]: 	Training 200/553. train loss: 0.6435,	0.8401 s / batch. (data: 3.15e-04). ETA=10:08:51, max mem: 20.9 GB 
[11/25 09:22:03 visual_prompt]: 	Training 300/553. train loss: 0.2492,	0.8245 s / batch. (data: 4.20e-04). ETA=9:56:11, max mem: 20.9 GB 
[11/25 09:23:45 visual_prompt]: 	Training 400/553. train loss: 0.5551,	0.8208 s / batch. (data: 7.97e-03). ETA=9:52:10, max mem: 20.9 GB 
[11/25 09:25:27 visual_prompt]: 	Training 500/553. train loss: 0.5989,	0.8311 s / batch. (data: 5.47e-03). ETA=9:58:11, max mem: 20.9 GB 
[11/25 09:26:22 visual_prompt]: Epoch 22 / 100: avg data time: 1.98e-01, avg batch time: 1.0242, average train loss: 0.9580
[11/25 09:27:20 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3076, average loss: 0.8673
[11/25 09:27:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.81	
[11/25 09:27:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[11/25 09:29:07 visual_prompt]: 	Training 100/553. train loss: 1.5493,	0.8360 s / batch. (data: 3.19e-04). ETA=9:59:35, max mem: 20.9 GB 
[11/25 09:30:51 visual_prompt]: 	Training 200/553. train loss: 1.3994,	0.8459 s / batch. (data: 2.39e-02). ETA=10:05:18, max mem: 20.9 GB 
[11/25 09:32:34 visual_prompt]: 	Training 300/553. train loss: 0.6830,	0.8331 s / batch. (data: 3.22e-04). ETA=9:54:45, max mem: 20.9 GB 
[11/25 09:34:14 visual_prompt]: 	Training 400/553. train loss: 0.5141,	0.8290 s / batch. (data: 5.94e-03). ETA=9:50:27, max mem: 20.9 GB 
[11/25 09:35:54 visual_prompt]: 	Training 500/553. train loss: 1.0323,	0.8160 s / batch. (data: 3.19e-04). ETA=9:39:49, max mem: 20.9 GB 
[11/25 09:36:47 visual_prompt]: Epoch 23 / 100: avg data time: 1.98e-01, avg batch time: 1.0244, average train loss: 0.9491
[11/25 09:37:46 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3085, average loss: 0.7012
[11/25 09:37:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.77	
[11/25 09:37:46 visual_prompt]: Stopping early.
[11/25 09:37:46 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 09:37:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 09:37:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/25 09:37:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 09:37:46 visual_prompt]: Training with config:
[11/25 09:37:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/25 09:37:46 visual_prompt]: Loading training data...
[11/25 09:37:46 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 09:37:46 visual_prompt]: Loading validation data...
[11/25 09:37:46 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 09:37:46 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 09:37:49 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 09:37:49 visual_prompt]: tuned percent:0.525
[11/25 09:37:49 visual_prompt]: Device used for model: 0
[11/25 09:37:49 visual_prompt]: Setting up Evaluator...
[11/25 09:37:49 visual_prompt]: Setting up Trainer...
[11/25 09:37:49 visual_prompt]: 	Setting up the optimizer...
[11/25 09:37:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 09:39:34 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8389 s / batch. (data: 7.61e-03). ETA=12:51:47, max mem: 20.9 GB 
[11/25 09:41:15 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8356 s / batch. (data: 3.08e-04). ETA=12:47:22, max mem: 20.9 GB 
[11/25 09:43:00 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.6080 s / batch. (data: 7.61e-01). ETA=1 day, 0:34:02, max mem: 20.9 GB 
[11/25 09:44:40 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8183 s / batch. (data: 3.09e-04). ETA=12:28:43, max mem: 20.9 GB 
[11/25 09:46:24 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8514 s / batch. (data: 5.95e-03). ETA=12:57:38, max mem: 20.9 GB 
[11/25 09:47:18 visual_prompt]: Epoch 1 / 100: avg data time: 2.02e-01, avg batch time: 1.0287, average train loss: 1.5403
[11/25 09:48:16 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3086, average loss: 1.5201
[11/25 09:48:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 09:48:16 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/25 09:50:02 visual_prompt]: 	Training 100/553. train loss: 0.7692,	1.5560 s / batch. (data: 7.29e-01). ETA=23:37:09, max mem: 20.9 GB 
[11/25 09:51:43 visual_prompt]: 	Training 200/553. train loss: 0.0948,	0.8271 s / batch. (data: 3.49e-04). ETA=12:31:54, max mem: 20.9 GB 
[11/25 09:53:27 visual_prompt]: 	Training 300/553. train loss: 0.8429,	1.2053 s / batch. (data: 3.94e-01). ETA=18:13:42, max mem: 20.9 GB 
[11/25 09:55:08 visual_prompt]: 	Training 400/553. train loss: 1.5886,	0.8400 s / batch. (data: 3.19e-04). ETA=12:40:51, max mem: 20.9 GB 
[11/25 09:56:50 visual_prompt]: 	Training 500/553. train loss: 0.5426,	0.8228 s / batch. (data: 3.12e-04). ETA=12:23:56, max mem: 20.9 GB 
[11/25 09:57:42 visual_prompt]: Epoch 2 / 100: avg data time: 1.98e-01, avg batch time: 1.0235, average train loss: 0.9154
[11/25 09:58:41 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3083, average loss: 1.2315
[11/25 09:58:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.98	
[11/25 09:58:41 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/25 10:00:26 visual_prompt]: 	Training 100/553. train loss: 0.7618,	0.8116 s / batch. (data: 3.05e-04). ETA=12:11:42, max mem: 20.9 GB 
[11/25 10:02:09 visual_prompt]: 	Training 200/553. train loss: 0.8719,	0.8311 s / batch. (data: 5.46e-03). ETA=12:27:51, max mem: 20.9 GB 
[11/25 10:03:50 visual_prompt]: 	Training 300/553. train loss: 0.6061,	0.8280 s / batch. (data: 3.27e-04). ETA=12:23:44, max mem: 20.9 GB 
[11/25 10:05:33 visual_prompt]: 	Training 400/553. train loss: 1.8251,	0.8320 s / batch. (data: 3.14e-04). ETA=12:25:56, max mem: 20.9 GB 
[11/25 10:07:16 visual_prompt]: 	Training 500/553. train loss: 0.8345,	1.5120 s / batch. (data: 6.89e-01). ETA=22:33:04, max mem: 20.9 GB 
[11/25 10:08:08 visual_prompt]: Epoch 3 / 100: avg data time: 1.98e-01, avg batch time: 1.0245, average train loss: 0.8886
[11/25 10:09:06 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.3073, average loss: 0.7379
[11/25 10:09:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.87	
[11/25 10:09:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/25 10:10:54 visual_prompt]: 	Training 100/553. train loss: 0.8040,	0.8236 s / batch. (data: 3.15e-04). ETA=12:14:54, max mem: 20.9 GB 
[11/25 10:12:36 visual_prompt]: 	Training 200/553. train loss: 0.5810,	0.8556 s / batch. (data: 1.16e-02). ETA=12:42:02, max mem: 20.9 GB 
[11/25 10:14:19 visual_prompt]: 	Training 300/553. train loss: 0.9695,	1.3798 s / batch. (data: 5.39e-01). ETA=20:26:41, max mem: 20.9 GB 
[11/25 10:15:56 visual_prompt]: 	Training 400/553. train loss: 1.2132,	1.0878 s / batch. (data: 2.73e-01). ETA=16:05:14, max mem: 20.9 GB 
[11/25 10:17:39 visual_prompt]: 	Training 500/553. train loss: 0.2903,	3.8401 s / batch. (data: 3.03e+00). ETA=2 days, 8:41:06, max mem: 20.9 GB 
[11/25 10:18:34 visual_prompt]: Epoch 4 / 100: avg data time: 2.00e-01, avg batch time: 1.0261, average train loss: 0.9992
[11/25 10:19:33 visual_prompt]: Inference (val):avg data time: 4.74e-05, avg batch time: 0.3071, average loss: 1.5824
[11/25 10:19:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.18	
[11/25 10:19:33 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/25 10:21:17 visual_prompt]: 	Training 100/553. train loss: 2.2033,	0.8346 s / batch. (data: 3.08e-04). ETA=12:17:05, max mem: 20.9 GB 
[11/25 10:22:59 visual_prompt]: 	Training 200/553. train loss: 0.8827,	1.3698 s / batch. (data: 5.43e-01). ETA=20:07:25, max mem: 20.9 GB 
[11/25 10:24:42 visual_prompt]: 	Training 300/553. train loss: 1.2643,	0.8394 s / batch. (data: 5.47e-03). ETA=12:18:31, max mem: 20.9 GB 
[11/25 10:26:23 visual_prompt]: 	Training 400/553. train loss: 1.5535,	0.8370 s / batch. (data: 1.05e-02). ETA=12:15:02, max mem: 20.9 GB 
[11/25 10:28:05 visual_prompt]: 	Training 500/553. train loss: 0.5232,	0.8456 s / batch. (data: 1.56e-02). ETA=12:21:06, max mem: 20.9 GB 
[11/25 10:28:59 visual_prompt]: Epoch 5 / 100: avg data time: 1.98e-01, avg batch time: 1.0243, average train loss: 0.9994
[11/25 10:29:58 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3064, average loss: 1.8342
[11/25 10:29:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.22	
[11/25 10:29:58 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/25 10:31:45 visual_prompt]: 	Training 100/553. train loss: 0.6791,	0.8357 s / batch. (data: 5.97e-03). ETA=12:10:21, max mem: 20.9 GB 
[11/25 10:33:27 visual_prompt]: 	Training 200/553. train loss: 2.7447,	0.8263 s / batch. (data: 1.58e-02). ETA=12:00:43, max mem: 20.9 GB 
[11/25 10:35:07 visual_prompt]: 	Training 300/553. train loss: 0.5407,	0.8276 s / batch. (data: 3.31e-04). ETA=12:00:28, max mem: 20.9 GB 
[11/25 10:36:53 visual_prompt]: 	Training 400/553. train loss: 0.8629,	0.8143 s / batch. (data: 3.42e-04). ETA=11:47:33, max mem: 20.9 GB 
[11/25 10:38:34 visual_prompt]: 	Training 500/553. train loss: 1.2590,	0.8163 s / batch. (data: 5.46e-03). ETA=11:47:56, max mem: 20.9 GB 
[11/25 10:39:26 visual_prompt]: Epoch 6 / 100: avg data time: 2.00e-01, avg batch time: 1.0274, average train loss: 1.1331
[11/25 10:40:24 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3080, average loss: 1.3537
[11/25 10:40:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.22	
[11/25 10:40:24 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/25 10:42:09 visual_prompt]: 	Training 100/553. train loss: 2.4068,	0.8320 s / batch. (data: 3.09e-04). ETA=11:59:24, max mem: 20.9 GB 
[11/25 10:43:50 visual_prompt]: 	Training 200/553. train loss: 0.5849,	0.8528 s / batch. (data: 3.02e-02). ETA=12:15:59, max mem: 20.9 GB 
[11/25 10:45:37 visual_prompt]: 	Training 300/553. train loss: 0.6902,	1.9600 s / batch. (data: 1.13e+00). ETA=1 day, 4:08:15, max mem: 20.9 GB 
[11/25 10:47:18 visual_prompt]: 	Training 400/553. train loss: 0.5755,	1.7527 s / batch. (data: 9.29e-01). ETA=1 day, 1:06:49, max mem: 20.9 GB 
[11/25 10:48:58 visual_prompt]: 	Training 500/553. train loss: 1.5788,	0.8238 s / batch. (data: 3.08e-04). ETA=11:46:49, max mem: 20.9 GB 
[11/25 10:49:51 visual_prompt]: Epoch 7 / 100: avg data time: 1.97e-01, avg batch time: 1.0237, average train loss: 1.0505
[11/25 10:50:49 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.3098, average loss: 0.7697
[11/25 10:50:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.37	
[11/25 10:50:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/25 10:52:33 visual_prompt]: 	Training 100/553. train loss: 1.1272,	0.8312 s / batch. (data: 9.10e-03). ETA=11:51:03, max mem: 20.9 GB 
[11/25 10:54:17 visual_prompt]: 	Training 200/553. train loss: 0.4748,	0.8360 s / batch. (data: 3.29e-04). ETA=11:53:47, max mem: 20.9 GB 
[11/25 10:55:59 visual_prompt]: 	Training 300/553. train loss: 2.0676,	0.8200 s / batch. (data: 7.96e-03). ETA=11:38:46, max mem: 20.9 GB 
[11/25 10:57:41 visual_prompt]: 	Training 400/553. train loss: 0.6976,	0.9949 s / batch. (data: 1.84e-01). ETA=14:06:08, max mem: 20.9 GB 
[11/25 10:59:23 visual_prompt]: 	Training 500/553. train loss: 2.2300,	1.4360 s / batch. (data: 6.11e-01). ETA=20:18:54, max mem: 20.9 GB 
[11/25 11:00:16 visual_prompt]: Epoch 8 / 100: avg data time: 1.99e-01, avg batch time: 1.0256, average train loss: 1.2381
[11/25 11:01:15 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3096, average loss: 1.3317
[11/25 11:01:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.58	
[11/25 11:01:15 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/25 11:03:00 visual_prompt]: 	Training 100/553. train loss: 0.0375,	0.8320 s / batch. (data: 5.43e-03). ETA=11:44:05, max mem: 20.9 GB 
[11/25 11:04:42 visual_prompt]: 	Training 200/553. train loss: 0.5812,	0.8314 s / batch. (data: 5.44e-03). ETA=11:42:13, max mem: 20.9 GB 
[11/25 11:06:24 visual_prompt]: 	Training 300/553. train loss: 0.9770,	1.9599 s / batch. (data: 1.12e+00). ETA=1 day, 3:32:02, max mem: 20.9 GB 
[11/25 11:08:07 visual_prompt]: 	Training 400/553. train loss: 1.0847,	0.8234 s / batch. (data: 3.14e-04). ETA=11:32:43, max mem: 20.9 GB 
[11/25 11:09:49 visual_prompt]: 	Training 500/553. train loss: 1.1847,	0.9728 s / batch. (data: 1.41e-01). ETA=13:36:45, max mem: 20.9 GB 
[11/25 11:10:41 visual_prompt]: Epoch 9 / 100: avg data time: 1.98e-01, avg batch time: 1.0240, average train loss: 0.9965
[11/25 11:11:40 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3088, average loss: 1.3654
[11/25 11:11:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.18	
[11/25 11:11:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/25 11:13:28 visual_prompt]: 	Training 100/553. train loss: 2.1853,	0.8120 s / batch. (data: 3.24e-04). ETA=11:19:40, max mem: 20.9 GB 
[11/25 11:15:08 visual_prompt]: 	Training 200/553. train loss: 0.7298,	0.8221 s / batch. (data: 5.45e-03). ETA=11:26:48, max mem: 20.9 GB 
[11/25 11:16:49 visual_prompt]: 	Training 300/553. train loss: 0.5715,	1.1680 s / batch. (data: 3.43e-01). ETA=16:13:47, max mem: 20.9 GB 
[11/25 11:18:29 visual_prompt]: 	Training 400/553. train loss: 1.1998,	0.8239 s / batch. (data: 3.64e-04). ETA=11:25:33, max mem: 20.9 GB 
[11/25 11:20:12 visual_prompt]: 	Training 500/553. train loss: 0.5270,	0.8286 s / batch. (data: 7.97e-03). ETA=11:28:02, max mem: 20.9 GB 
[11/25 11:21:05 visual_prompt]: Epoch 10 / 100: avg data time: 1.97e-01, avg batch time: 1.0231, average train loss: 1.3032
[11/25 11:22:04 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.3060, average loss: 0.7288
[11/25 11:22:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.18	
[11/25 11:22:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/25 11:23:52 visual_prompt]: 	Training 100/553. train loss: 2.2260,	0.8228 s / batch. (data: 4.30e-04). ETA=11:21:09, max mem: 20.9 GB 
[11/25 11:25:36 visual_prompt]: 	Training 200/553. train loss: 1.4518,	0.8529 s / batch. (data: 1.56e-02). ETA=11:44:40, max mem: 20.9 GB 
[11/25 11:27:18 visual_prompt]: 	Training 300/553. train loss: 0.0190,	2.8050 s / batch. (data: 1.99e+00). ETA=1 day, 14:32:44, max mem: 20.9 GB 
[11/25 11:28:57 visual_prompt]: 	Training 400/553. train loss: 0.8589,	0.8230 s / batch. (data: 3.44e-04). ETA=11:17:13, max mem: 20.9 GB 
[11/25 11:30:38 visual_prompt]: 	Training 500/553. train loss: 0.7390,	0.8252 s / batch. (data: 5.46e-03). ETA=11:17:38, max mem: 20.9 GB 
[11/25 11:31:30 visual_prompt]: Epoch 11 / 100: avg data time: 1.98e-01, avg batch time: 1.0240, average train loss: 1.0203
[11/25 11:32:29 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.3087, average loss: 1.1206
[11/25 11:32:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.96	
[11/25 11:32:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/25 11:34:17 visual_prompt]: 	Training 100/553. train loss: 0.9176,	0.8876 s / batch. (data: 7.79e-02). ETA=12:06:37, max mem: 20.9 GB 
[11/25 11:35:59 visual_prompt]: 	Training 200/553. train loss: 0.5466,	1.3855 s / batch. (data: 5.77e-01). ETA=18:51:54, max mem: 20.9 GB 
[11/25 11:37:40 visual_prompt]: 	Training 300/553. train loss: 0.4498,	0.8337 s / batch. (data: 1.01e-02). ETA=11:19:43, max mem: 20.9 GB 
[11/25 11:39:22 visual_prompt]: 	Training 400/553. train loss: 0.6596,	0.8399 s / batch. (data: 4.99e-04). ETA=11:23:23, max mem: 20.9 GB 
[11/25 11:41:05 visual_prompt]: 	Training 500/553. train loss: 4.8971,	0.8285 s / batch. (data: 8.30e-04). ETA=11:12:42, max mem: 20.9 GB 
[11/25 11:41:56 visual_prompt]: Epoch 12 / 100: avg data time: 1.99e-01, avg batch time: 1.0253, average train loss: 1.0609
[11/25 11:42:55 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.3063, average loss: 2.9049
[11/25 11:42:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.47	
[11/25 11:42:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/25 11:44:42 visual_prompt]: 	Training 100/553. train loss: 0.6522,	0.8114 s / batch. (data: 4.84e-04). ETA=10:56:43, max mem: 20.9 GB 
[11/25 11:46:22 visual_prompt]: 	Training 200/553. train loss: 0.4967,	0.8389 s / batch. (data: 3.35e-04). ETA=11:17:36, max mem: 20.9 GB 
[11/25 11:48:04 visual_prompt]: 	Training 300/553. train loss: 0.4334,	1.9758 s / batch. (data: 1.17e+00). ETA=1 day, 2:32:37, max mem: 20.9 GB 
[11/25 11:49:45 visual_prompt]: 	Training 400/553. train loss: 4.0432,	0.8124 s / batch. (data: 3.26e-04). ETA=10:53:27, max mem: 20.9 GB 
[11/25 11:51:27 visual_prompt]: 	Training 500/553. train loss: 1.3895,	0.8363 s / batch. (data: 5.46e-03). ETA=11:11:17, max mem: 20.9 GB 
[11/25 11:52:21 visual_prompt]: Epoch 13 / 100: avg data time: 1.98e-01, avg batch time: 1.0236, average train loss: 1.1305
[11/25 11:53:19 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3075, average loss: 0.6267
[11/25 11:53:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.49	
[11/25 11:53:19 visual_prompt]: Best epoch 13: best metric: -0.627
[11/25 11:53:19 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/25 11:55:07 visual_prompt]: 	Training 100/553. train loss: 0.6649,	0.8343 s / batch. (data: 1.20e-02). ETA=11:07:36, max mem: 20.9 GB 
[11/25 11:56:49 visual_prompt]: 	Training 200/553. train loss: 0.1900,	1.2321 s / batch. (data: 4.09e-01). ETA=16:23:50, max mem: 20.9 GB 
[11/25 11:58:30 visual_prompt]: 	Training 300/553. train loss: 0.4326,	0.8330 s / batch. (data: 3.49e-04). ETA=11:03:44, max mem: 20.9 GB 
[11/25 12:00:11 visual_prompt]: 	Training 400/553. train loss: 0.7925,	0.9564 s / batch. (data: 1.45e-01). ETA=12:40:29, max mem: 20.9 GB 
[11/25 12:01:53 visual_prompt]: 	Training 500/553. train loss: 3.2284,	0.8275 s / batch. (data: 3.38e-04). ETA=10:56:38, max mem: 20.9 GB 
[11/25 12:02:45 visual_prompt]: Epoch 14 / 100: avg data time: 1.96e-01, avg batch time: 1.0228, average train loss: 1.0063
[11/25 12:03:44 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.3058, average loss: 0.6575
[11/25 12:03:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 71.26	
[11/25 12:03:44 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/25 12:05:28 visual_prompt]: 	Training 100/553. train loss: 0.8357,	0.8281 s / batch. (data: 5.47e-03). ETA=10:55:00, max mem: 20.9 GB 
[11/25 12:07:09 visual_prompt]: 	Training 200/553. train loss: 0.2557,	0.8200 s / batch. (data: 3.27e-04). ETA=10:47:13, max mem: 20.9 GB 
[11/25 12:08:53 visual_prompt]: 	Training 300/553. train loss: 1.0142,	0.8156 s / batch. (data: 5.65e-03). ETA=10:42:25, max mem: 20.9 GB 
[11/25 12:10:33 visual_prompt]: 	Training 400/553. train loss: 0.3907,	1.2850 s / batch. (data: 4.65e-01). ETA=16:49:55, max mem: 20.9 GB 
[11/25 12:12:15 visual_prompt]: 	Training 500/553. train loss: 0.4546,	0.9087 s / batch. (data: 6.76e-02). ETA=11:52:39, max mem: 20.9 GB 
[11/25 12:13:09 visual_prompt]: Epoch 15 / 100: avg data time: 1.95e-01, avg batch time: 1.0217, average train loss: 1.1292
[11/25 12:14:07 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3060, average loss: 2.1255
[11/25 12:14:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.87	
[11/25 12:14:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/25 12:15:53 visual_prompt]: 	Training 100/553. train loss: 0.4559,	0.8501 s / batch. (data: 1.20e-02). ETA=11:04:36, max mem: 20.9 GB 
[11/25 12:17:34 visual_prompt]: 	Training 200/553. train loss: 0.6217,	0.8239 s / batch. (data: 3.15e-04). ETA=10:42:40, max mem: 20.9 GB 
[11/25 12:19:16 visual_prompt]: 	Training 300/553. train loss: 1.2673,	0.8162 s / batch. (data: 3.19e-04). ETA=10:35:21, max mem: 20.9 GB 
[11/25 12:20:58 visual_prompt]: 	Training 400/553. train loss: 0.4661,	0.8320 s / batch. (data: 8.53e-04). ETA=10:46:15, max mem: 20.9 GB 
[11/25 12:22:39 visual_prompt]: 	Training 500/553. train loss: 0.7612,	0.9962 s / batch. (data: 1.70e-01). ETA=12:52:08, max mem: 20.9 GB 
[11/25 12:23:33 visual_prompt]: Epoch 16 / 100: avg data time: 1.96e-01, avg batch time: 1.0221, average train loss: 0.9279
[11/25 12:24:31 visual_prompt]: Inference (val):avg data time: 4.00e-04, avg batch time: 0.3071, average loss: 0.6113
[11/25 12:24:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 71.95	rocauc: 73.55	
[11/25 12:24:31 visual_prompt]: Best epoch 16: best metric: -0.611
[11/25 12:24:31 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/25 12:26:17 visual_prompt]: 	Training 100/553. train loss: 0.2198,	0.8161 s / batch. (data: 4.85e-04). ETA=10:30:28, max mem: 20.9 GB 
[11/25 12:28:00 visual_prompt]: 	Training 200/553. train loss: 3.3822,	0.8242 s / batch. (data: 3.37e-04). ETA=10:35:22, max mem: 20.9 GB 
[11/25 12:29:41 visual_prompt]: 	Training 300/553. train loss: 1.1530,	0.8440 s / batch. (data: 5.48e-03). ETA=10:49:12, max mem: 20.9 GB 
[11/25 12:31:22 visual_prompt]: 	Training 400/553. train loss: 0.7215,	1.1033 s / batch. (data: 2.78e-01). ETA=14:06:47, max mem: 20.9 GB 
[11/25 12:33:03 visual_prompt]: 	Training 500/553. train loss: 1.2809,	1.4963 s / batch. (data: 6.68e-01). ETA=19:05:56, max mem: 20.9 GB 
[11/25 12:33:57 visual_prompt]: Epoch 17 / 100: avg data time: 1.97e-01, avg batch time: 1.0233, average train loss: 0.9485
[11/25 12:34:56 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.3080, average loss: 0.7429
[11/25 12:34:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.38	
[11/25 12:34:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/25 12:36:43 visual_prompt]: 	Training 100/553. train loss: 0.4876,	0.8271 s / batch. (data: 5.45e-03). ETA=10:31:18, max mem: 20.9 GB 
[11/25 12:38:27 visual_prompt]: 	Training 200/553. train loss: 0.6285,	0.8400 s / batch. (data: 8.53e-04). ETA=10:39:47, max mem: 20.9 GB 
[11/25 12:40:09 visual_prompt]: 	Training 300/553. train loss: 0.6465,	0.8180 s / batch. (data: 5.46e-03). ETA=10:21:37, max mem: 20.9 GB 
[11/25 12:41:50 visual_prompt]: 	Training 400/553. train loss: 0.9750,	0.8264 s / batch. (data: 3.78e-04). ETA=10:26:39, max mem: 20.9 GB 
[11/25 12:43:31 visual_prompt]: 	Training 500/553. train loss: 1.3419,	0.8200 s / batch. (data: 7.96e-03). ETA=10:20:25, max mem: 20.9 GB 
[11/25 12:44:23 visual_prompt]: Epoch 18 / 100: avg data time: 1.99e-01, avg batch time: 1.0248, average train loss: 1.0641
[11/25 12:45:22 visual_prompt]: Inference (val):avg data time: 5.80e-04, avg batch time: 0.3084, average loss: 0.7910
[11/25 12:45:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 71.49	
[11/25 12:45:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[11/25 12:47:14 visual_prompt]: 	Training 100/553. train loss: 1.1723,	1.6160 s / batch. (data: 7.72e-01). ETA=20:18:38, max mem: 20.9 GB 
[11/25 12:48:56 visual_prompt]: 	Training 200/553. train loss: 0.3619,	0.8469 s / batch. (data: 1.05e-02). ETA=10:37:14, max mem: 20.9 GB 
[11/25 12:50:38 visual_prompt]: 	Training 300/553. train loss: 1.8000,	0.8321 s / batch. (data: 7.98e-03). ETA=10:24:41, max mem: 20.9 GB 
[11/25 12:52:21 visual_prompt]: 	Training 400/553. train loss: 0.2757,	0.8246 s / batch. (data: 8.25e-04). ETA=10:17:42, max mem: 20.9 GB 
[11/25 12:53:58 visual_prompt]: 	Training 500/553. train loss: 0.6552,	0.8387 s / batch. (data: 1.48e-02). ETA=10:26:52, max mem: 20.9 GB 
[11/25 12:54:52 visual_prompt]: Epoch 19 / 100: avg data time: 2.04e-01, avg batch time: 1.0305, average train loss: 0.8206
[11/25 12:55:50 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.3075, average loss: 2.2406
[11/25 12:55:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.49	
[11/25 12:55:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[11/25 12:57:35 visual_prompt]: 	Training 100/553. train loss: 0.8022,	0.8154 s / batch. (data: 3.16e-04). ETA=10:07:23, max mem: 20.9 GB 
[11/25 12:59:17 visual_prompt]: 	Training 200/553. train loss: 0.1549,	0.8160 s / batch. (data: 3.77e-04). ETA=10:06:27, max mem: 20.9 GB 
[11/25 13:00:59 visual_prompt]: 	Training 300/553. train loss: 1.8837,	0.8371 s / batch. (data: 5.47e-03). ETA=10:20:43, max mem: 20.9 GB 
[11/25 13:02:41 visual_prompt]: 	Training 400/553. train loss: 0.6205,	0.8392 s / batch. (data: 5.47e-03). ETA=10:20:52, max mem: 20.9 GB 
[11/25 13:04:22 visual_prompt]: 	Training 500/553. train loss: 0.5659,	0.8407 s / batch. (data: 1.20e-02). ETA=10:20:37, max mem: 20.9 GB 
[11/25 13:05:16 visual_prompt]: Epoch 20 / 100: avg data time: 1.97e-01, avg batch time: 1.0241, average train loss: 0.9438
[11/25 13:06:15 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.3067, average loss: 0.6474
[11/25 13:06:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 72.75	
[11/25 13:06:15 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[11/25 13:08:03 visual_prompt]: 	Training 100/553. train loss: 0.2255,	0.8520 s / batch. (data: 7.98e-03). ETA=10:26:48, max mem: 20.9 GB 
[11/25 13:09:44 visual_prompt]: 	Training 200/553. train loss: 0.2152,	0.8240 s / batch. (data: 3.05e-04). ETA=10:04:50, max mem: 20.9 GB 
[11/25 13:11:26 visual_prompt]: 	Training 300/553. train loss: 1.1154,	1.0729 s / batch. (data: 2.27e-01). ETA=13:05:41, max mem: 20.9 GB 
[11/25 13:13:06 visual_prompt]: 	Training 400/553. train loss: 1.1809,	0.8424 s / batch. (data: 2.24e-02). ETA=10:15:32, max mem: 20.9 GB 
[11/25 13:14:50 visual_prompt]: 	Training 500/553. train loss: 0.6918,	0.8489 s / batch. (data: 3.12e-04). ETA=10:18:52, max mem: 20.9 GB 
[11/25 13:15:43 visual_prompt]: Epoch 21 / 100: avg data time: 2.01e-01, avg batch time: 1.0264, average train loss: 0.9820
[11/25 13:16:41 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3092, average loss: 0.6259
[11/25 13:16:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 72.51	
[11/25 13:16:41 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[11/25 13:18:27 visual_prompt]: 	Training 100/553. train loss: 1.2514,	0.8120 s / batch. (data: 3.01e-04). ETA=9:49:53, max mem: 20.9 GB 
[11/25 13:20:09 visual_prompt]: 	Training 200/553. train loss: 0.4223,	0.8440 s / batch. (data: 3.24e-04). ETA=10:11:43, max mem: 20.9 GB 
[11/25 13:21:48 visual_prompt]: 	Training 300/553. train loss: 0.1473,	0.8320 s / batch. (data: 3.20e-04). ETA=10:01:37, max mem: 20.9 GB 
[11/25 13:23:31 visual_prompt]: 	Training 400/553. train loss: 0.5592,	0.8240 s / batch. (data: 3.13e-04). ETA=9:54:27, max mem: 20.9 GB 
[11/25 13:25:13 visual_prompt]: 	Training 500/553. train loss: 0.7561,	0.8339 s / batch. (data: 3.30e-04). ETA=10:00:15, max mem: 20.9 GB 
[11/25 13:26:07 visual_prompt]: Epoch 22 / 100: avg data time: 1.97e-01, avg batch time: 1.0234, average train loss: 0.8482
[11/25 13:27:06 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3098, average loss: 0.6143
[11/25 13:27:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.73	rocauc: 72.60	
[11/25 13:27:06 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[11/25 13:28:52 visual_prompt]: 	Training 100/553. train loss: 1.4942,	0.8280 s / batch. (data: 5.45e-03). ETA=9:53:52, max mem: 20.9 GB 
[11/25 13:30:35 visual_prompt]: 	Training 200/553. train loss: 1.7507,	0.8185 s / batch. (data: 3.11e-03). ETA=9:45:43, max mem: 20.9 GB 
[11/25 13:32:19 visual_prompt]: 	Training 300/553. train loss: 0.6437,	0.8468 s / batch. (data: 3.32e-04). ETA=10:04:30, max mem: 20.9 GB 
[11/25 13:33:59 visual_prompt]: 	Training 400/553. train loss: 0.6500,	0.8308 s / batch. (data: 3.19e-04). ETA=9:51:42, max mem: 20.9 GB 
[11/25 13:35:42 visual_prompt]: 	Training 500/553. train loss: 1.2989,	0.8228 s / batch. (data: 3.22e-04). ETA=9:44:38, max mem: 20.9 GB 
[11/25 13:36:38 visual_prompt]: Epoch 23 / 100: avg data time: 2.08e-01, avg batch time: 1.0343, average train loss: 0.9132
[11/25 13:37:39 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3076, average loss: 0.6851
[11/25 13:37:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 71.78	
[11/25 13:37:39 visual_prompt]: Stopping early.
[11/25 13:37:39 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 13:37:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 13:37:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/25 13:37:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 13:37:39 visual_prompt]: Training with config:
[11/25 13:37:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.25_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/25 13:37:39 visual_prompt]: Loading training data...
[11/25 13:37:39 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 13:37:39 visual_prompt]: Loading validation data...
[11/25 13:37:39 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 13:37:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 13:37:46 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 13:37:46 visual_prompt]: tuned percent:0.525
[11/25 13:37:46 visual_prompt]: Device used for model: 0
[11/25 13:37:46 visual_prompt]: Setting up Evaluator...
[11/25 13:37:46 visual_prompt]: Setting up Trainer...
[11/25 13:37:46 visual_prompt]: 	Setting up the optimizer...
[11/25 13:37:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 13:39:33 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8375 s / batch. (data: 5.45e-03). ETA=12:50:32, max mem: 20.9 GB 
[11/25 13:41:16 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8197 s / batch. (data: 5.44e-03). ETA=12:32:45, max mem: 20.9 GB 
[11/25 13:43:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.5567 s / batch. (data: 7.36e-01). ETA=23:46:56, max mem: 20.9 GB 
[11/25 13:44:56 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8164 s / batch. (data: 3.89e-04). ETA=12:27:02, max mem: 20.9 GB 
[11/25 13:46:41 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8240 s / batch. (data: 8.47e-04). ETA=12:32:33, max mem: 20.9 GB 
[11/25 13:47:36 visual_prompt]: Epoch 1 / 100: avg data time: 2.43e-01, avg batch time: 1.0669, average train loss: 1.5403
[11/25 13:48:35 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3071, average loss: 1.5201
[11/25 13:48:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 13:48:35 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/25 13:50:21 visual_prompt]: 	Training 100/553. train loss: 0.7259,	1.1440 s / batch. (data: 3.02e-01). ETA=17:21:54, max mem: 20.9 GB 
[11/25 13:52:02 visual_prompt]: 	Training 200/553. train loss: 0.2324,	0.8280 s / batch. (data: 5.46e-03). ETA=12:32:46, max mem: 20.9 GB 
[11/25 13:53:47 visual_prompt]: 	Training 300/553. train loss: 0.8152,	1.1831 s / batch. (data: 3.59e-01). ETA=17:53:34, max mem: 20.9 GB 
[11/25 13:55:28 visual_prompt]: 	Training 400/553. train loss: 0.9185,	0.8211 s / batch. (data: 1.20e-02). ETA=12:23:42, max mem: 20.9 GB 
[11/25 13:57:12 visual_prompt]: 	Training 500/553. train loss: 0.6950,	0.8520 s / batch. (data: 7.98e-03). ETA=12:50:18, max mem: 20.9 GB 
[11/25 13:58:05 visual_prompt]: Epoch 2 / 100: avg data time: 2.04e-01, avg batch time: 1.0302, average train loss: 0.8104
[11/25 13:59:04 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3083, average loss: 0.7343
[11/25 13:59:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.96	
[11/25 13:59:04 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/25 14:00:49 visual_prompt]: 	Training 100/553. train loss: 0.7299,	0.8388 s / batch. (data: 8.96e-03). ETA=12:36:12, max mem: 20.9 GB 
[11/25 14:02:32 visual_prompt]: 	Training 200/553. train loss: 0.7123,	1.4730 s / batch. (data: 6.43e-01). ETA=22:05:32, max mem: 20.9 GB 
[11/25 14:04:14 visual_prompt]: 	Training 300/553. train loss: 0.5926,	0.8368 s / batch. (data: 3.23e-04). ETA=12:31:39, max mem: 20.9 GB 
[11/25 14:05:57 visual_prompt]: 	Training 400/553. train loss: 0.6351,	0.8379 s / batch. (data: 3.17e-04). ETA=12:31:15, max mem: 20.9 GB 
[11/25 14:07:41 visual_prompt]: 	Training 500/553. train loss: 0.7292,	1.4040 s / batch. (data: 5.85e-01). ETA=20:56:28, max mem: 20.9 GB 
[11/25 14:08:33 visual_prompt]: Epoch 3 / 100: avg data time: 2.02e-01, avg batch time: 1.0290, average train loss: 0.7370
[11/25 14:09:32 visual_prompt]: Inference (val):avg data time: 3.38e-04, avg batch time: 0.3093, average loss: 0.7402
[11/25 14:09:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.22	
[11/25 14:09:32 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/25 14:11:20 visual_prompt]: 	Training 100/553. train loss: 0.7316,	0.8226 s / batch. (data: 3.32e-04). ETA=12:14:03, max mem: 20.9 GB 
[11/25 14:13:02 visual_prompt]: 	Training 200/553. train loss: 0.7393,	1.0543 s / batch. (data: 2.02e-01). ETA=15:39:03, max mem: 20.9 GB 
[11/25 14:14:45 visual_prompt]: 	Training 300/553. train loss: 0.5749,	1.7093 s / batch. (data: 8.94e-01). ETA=1 day, 1:19:35, max mem: 20.9 GB 
[11/25 14:16:23 visual_prompt]: 	Training 400/553. train loss: 0.6431,	1.5355 s / batch. (data: 7.28e-01). ETA=22:42:33, max mem: 20.9 GB 
[11/25 14:18:07 visual_prompt]: 	Training 500/553. train loss: 0.5387,	3.3640 s / batch. (data: 2.54e+00). ETA=2 days, 1:39:25, max mem: 20.9 GB 
[11/25 14:19:02 visual_prompt]: Epoch 4 / 100: avg data time: 2.05e-01, avg batch time: 1.0308, average train loss: 0.7705
[11/25 14:20:01 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3062, average loss: 0.7032
[11/25 14:20:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.59	
[11/25 14:20:01 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/25 14:21:45 visual_prompt]: 	Training 100/553. train loss: 0.4886,	0.8280 s / batch. (data: 3.13e-04). ETA=12:11:13, max mem: 20.9 GB 
[11/25 14:23:28 visual_prompt]: 	Training 200/553. train loss: 0.5989,	1.4568 s / batch. (data: 6.23e-01). ETA=21:24:06, max mem: 20.9 GB 
[11/25 14:25:11 visual_prompt]: 	Training 300/553. train loss: 1.5031,	0.8157 s / batch. (data: 5.43e-03). ETA=11:57:41, max mem: 20.9 GB 
[11/25 14:26:53 visual_prompt]: 	Training 400/553. train loss: 1.0950,	0.8215 s / batch. (data: 4.82e-04). ETA=12:01:24, max mem: 20.9 GB 
[11/25 14:28:36 visual_prompt]: 	Training 500/553. train loss: 0.6009,	0.8401 s / batch. (data: 3.07e-04). ETA=12:16:17, max mem: 20.9 GB 
[11/25 14:29:30 visual_prompt]: Epoch 5 / 100: avg data time: 2.03e-01, avg batch time: 1.0290, average train loss: 0.7995
[11/25 14:30:29 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.3095, average loss: 1.1954
[11/25 14:30:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.96	
[11/25 14:30:29 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/25 14:32:17 visual_prompt]: 	Training 100/553. train loss: 0.5873,	0.8128 s / batch. (data: 3.35e-04). ETA=11:50:17, max mem: 20.9 GB 
[11/25 14:33:58 visual_prompt]: 	Training 200/553. train loss: 1.4488,	0.8155 s / batch. (data: 3.31e-04). ETA=11:51:21, max mem: 20.9 GB 
[11/25 14:35:38 visual_prompt]: 	Training 300/553. train loss: 0.5597,	0.8186 s / batch. (data: 3.10e-04). ETA=11:52:40, max mem: 20.9 GB 
[11/25 14:37:25 visual_prompt]: 	Training 400/553. train loss: 0.6484,	0.8240 s / batch. (data: 3.38e-04). ETA=11:56:00, max mem: 20.9 GB 
[11/25 14:39:05 visual_prompt]: 	Training 500/553. train loss: 1.0040,	0.8239 s / batch. (data: 5.66e-04). ETA=11:54:33, max mem: 20.9 GB 
[11/25 14:39:58 visual_prompt]: Epoch 6 / 100: avg data time: 2.04e-01, avg batch time: 1.0296, average train loss: 0.8197
[11/25 14:40:57 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3061, average loss: 0.7042
[11/25 14:40:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.52	
[11/25 14:40:57 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/25 14:42:43 visual_prompt]: 	Training 100/553. train loss: 0.8747,	0.8181 s / batch. (data: 7.96e-03). ETA=11:47:27, max mem: 20.9 GB 
[11/25 14:44:25 visual_prompt]: 	Training 200/553. train loss: 0.6068,	0.8364 s / batch. (data: 5.46e-03). ETA=12:01:52, max mem: 20.9 GB 
[11/25 14:46:11 visual_prompt]: 	Training 300/553. train loss: 0.5748,	2.0276 s / batch. (data: 1.21e+00). ETA=1 day, 5:06:28, max mem: 20.9 GB 
[11/25 14:47:54 visual_prompt]: 	Training 400/553. train loss: 0.5677,	2.0000 s / batch. (data: 1.15e+00). ETA=1 day, 4:39:23, max mem: 20.9 GB 
[11/25 14:49:35 visual_prompt]: 	Training 500/553. train loss: 0.7092,	0.8100 s / batch. (data: 3.07e-04). ETA=11:34:58, max mem: 20.9 GB 
[11/25 14:50:27 visual_prompt]: Epoch 7 / 100: avg data time: 2.04e-01, avg batch time: 1.0306, average train loss: 0.8428
[11/25 14:51:26 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3080, average loss: 0.7045
[11/25 14:51:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.13	
[11/25 14:51:26 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/25 14:53:10 visual_prompt]: 	Training 100/553. train loss: 1.0183,	0.8163 s / batch. (data: 3.40e-04). ETA=11:38:22, max mem: 20.9 GB 
[11/25 14:54:54 visual_prompt]: 	Training 200/553. train loss: 0.8070,	0.8244 s / batch. (data: 1.08e-02). ETA=11:43:50, max mem: 20.9 GB 
[11/25 14:56:38 visual_prompt]: 	Training 300/553. train loss: 1.3519,	0.8280 s / batch. (data: 3.29e-04). ETA=11:45:34, max mem: 20.9 GB 
[11/25 14:58:21 visual_prompt]: 	Training 400/553. train loss: 0.7351,	1.1680 s / batch. (data: 3.54e-01). ETA=16:33:22, max mem: 20.9 GB 
[11/25 15:00:03 visual_prompt]: 	Training 500/553. train loss: 1.0141,	1.3800 s / batch. (data: 5.57e-01). ETA=19:31:21, max mem: 20.9 GB 
[11/25 15:00:57 visual_prompt]: Epoch 8 / 100: avg data time: 2.08e-01, avg batch time: 1.0336, average train loss: 0.9099
[11/25 15:01:56 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3067, average loss: 0.6914
[11/25 15:01:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.99	
[11/25 15:01:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/25 15:03:43 visual_prompt]: 	Training 100/553. train loss: 0.5348,	0.8101 s / batch. (data: 3.50e-04). ETA=11:25:32, max mem: 20.9 GB 
[11/25 15:05:24 visual_prompt]: 	Training 200/553. train loss: 0.7175,	0.8576 s / batch. (data: 9.53e-03). ETA=12:04:17, max mem: 20.9 GB 
[11/25 15:07:07 visual_prompt]: 	Training 300/553. train loss: 0.5529,	2.0120 s / batch. (data: 1.16e+00). ETA=1 day, 4:15:58, max mem: 20.9 GB 
[11/25 15:08:51 visual_prompt]: 	Training 400/553. train loss: 0.6746,	0.8210 s / batch. (data: 8.74e-04). ETA=11:30:42, max mem: 20.9 GB 
[11/25 15:10:35 visual_prompt]: 	Training 500/553. train loss: 0.6847,	1.1000 s / batch. (data: 2.56e-01). ETA=15:23:35, max mem: 20.9 GB 
[11/25 15:11:26 visual_prompt]: Epoch 9 / 100: avg data time: 2.05e-01, avg batch time: 1.0306, average train loss: 0.8434
[11/25 15:12:25 visual_prompt]: Inference (val):avg data time: 2.54e-04, avg batch time: 0.3074, average loss: 0.7232
[11/25 15:12:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.56	
[11/25 15:12:25 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/25 15:14:15 visual_prompt]: 	Training 100/553. train loss: 1.2551,	0.8317 s / batch. (data: 1.11e-02). ETA=11:36:08, max mem: 20.9 GB 
[11/25 15:15:56 visual_prompt]: 	Training 200/553. train loss: 0.9320,	0.8196 s / batch. (data: 3.55e-04). ETA=11:24:42, max mem: 20.9 GB 
[11/25 15:17:38 visual_prompt]: 	Training 300/553. train loss: 0.5720,	0.8320 s / batch. (data: 2.53e-03). ETA=11:33:38, max mem: 20.9 GB 
[11/25 15:19:18 visual_prompt]: 	Training 400/553. train loss: 0.9727,	0.8480 s / batch. (data: 5.48e-03). ETA=11:45:35, max mem: 20.9 GB 
[11/25 15:21:02 visual_prompt]: 	Training 500/553. train loss: 0.6588,	1.0422 s / batch. (data: 2.09e-01). ETA=14:25:25, max mem: 20.9 GB 
[11/25 15:21:56 visual_prompt]: Epoch 10 / 100: avg data time: 2.06e-01, avg batch time: 1.0323, average train loss: 0.9502
[11/25 15:22:55 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3066, average loss: 0.7259
[11/25 15:22:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.58	
[11/25 15:22:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/25 15:24:44 visual_prompt]: 	Training 100/553. train loss: 1.5573,	0.8344 s / batch. (data: 3.11e-04). ETA=11:30:43, max mem: 20.9 GB 
[11/25 15:26:28 visual_prompt]: 	Training 200/553. train loss: 1.3085,	0.8240 s / batch. (data: 3.14e-04). ETA=11:20:45, max mem: 20.9 GB 
[11/25 15:28:11 visual_prompt]: 	Training 300/553. train loss: 0.1110,	2.4001 s / batch. (data: 1.57e+00). ETA=1 day, 8:58:51, max mem: 20.9 GB 
[11/25 15:29:51 visual_prompt]: 	Training 400/553. train loss: 0.6141,	0.8376 s / batch. (data: 1.05e-02). ETA=11:29:13, max mem: 20.9 GB 
[11/25 15:31:32 visual_prompt]: 	Training 500/553. train loss: 0.8697,	0.8280 s / batch. (data: 3.15e-04). ETA=11:19:55, max mem: 20.9 GB 
[11/25 15:32:25 visual_prompt]: Epoch 11 / 100: avg data time: 2.03e-01, avg batch time: 1.0299, average train loss: 0.8911
[11/25 15:33:24 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.3069, average loss: 0.7500
[11/25 15:33:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.31	
[11/25 15:33:24 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/25 15:35:12 visual_prompt]: 	Training 100/553. train loss: 0.8803,	1.1457 s / batch. (data: 3.18e-01). ETA=15:37:51, max mem: 20.9 GB 
[11/25 15:36:55 visual_prompt]: 	Training 200/553. train loss: 0.5622,	0.9856 s / batch. (data: 1.48e-01). ETA=13:25:12, max mem: 20.9 GB 
[11/25 15:38:36 visual_prompt]: 	Training 300/553. train loss: 0.6052,	0.8365 s / batch. (data: 1.05e-02). ETA=11:22:01, max mem: 20.9 GB 
[11/25 15:40:18 visual_prompt]: 	Training 400/553. train loss: 0.9251,	0.8356 s / batch. (data: 3.40e-04). ETA=11:19:50, max mem: 20.9 GB 
[11/25 15:42:01 visual_prompt]: 	Training 500/553. train loss: 2.1185,	0.8253 s / batch. (data: 3.25e-04). ETA=11:10:05, max mem: 20.9 GB 
[11/25 15:42:53 visual_prompt]: Epoch 12 / 100: avg data time: 2.04e-01, avg batch time: 1.0298, average train loss: 0.9602
[11/25 15:43:52 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3083, average loss: 2.1559
[11/25 15:43:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.15	
[11/25 15:43:52 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/25 15:45:41 visual_prompt]: 	Training 100/553. train loss: 0.5731,	0.8329 s / batch. (data: 2.08e-02). ETA=11:14:06, max mem: 20.9 GB 
[11/25 15:47:19 visual_prompt]: 	Training 200/553. train loss: 0.6871,	0.8247 s / batch. (data: 3.09e-04). ETA=11:06:08, max mem: 20.9 GB 
[11/25 15:49:01 visual_prompt]: 	Training 300/553. train loss: 0.7941,	1.8684 s / batch. (data: 1.06e+00). ETA=1 day, 1:06:05, max mem: 20.9 GB 
[11/25 15:50:41 visual_prompt]: 	Training 400/553. train loss: 0.5058,	0.8399 s / batch. (data: 7.92e-03). ETA=11:15:39, max mem: 20.9 GB 
[11/25 15:52:23 visual_prompt]: 	Training 500/553. train loss: 1.0339,	0.8278 s / batch. (data: 3.24e-04). ETA=11:04:29, max mem: 20.9 GB 
[11/25 15:53:16 visual_prompt]: Epoch 13 / 100: avg data time: 1.94e-01, avg batch time: 1.0195, average train loss: 0.9720
[11/25 15:54:14 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.3079, average loss: 0.9952
[11/25 15:54:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.03	
[11/25 15:54:14 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/25 15:56:01 visual_prompt]: 	Training 100/553. train loss: 0.6778,	0.8440 s / batch. (data: 3.91e-04). ETA=11:15:20, max mem: 20.9 GB 
[11/25 15:57:41 visual_prompt]: 	Training 200/553. train loss: 0.1189,	1.1598 s / batch. (data: 3.21e-01). ETA=15:26:05, max mem: 20.9 GB 
[11/25 15:59:22 visual_prompt]: 	Training 300/553. train loss: 0.6993,	0.8250 s / batch. (data: 3.46e-04). ETA=10:57:24, max mem: 20.9 GB 
[11/25 16:01:04 visual_prompt]: 	Training 400/553. train loss: 0.5773,	0.8350 s / batch. (data: 1.20e-02). ETA=11:03:58, max mem: 20.9 GB 
[11/25 16:02:45 visual_prompt]: 	Training 500/553. train loss: 1.4140,	0.8360 s / batch. (data: 3.14e-04). ETA=11:03:24, max mem: 20.9 GB 
[11/25 16:03:37 visual_prompt]: Epoch 14 / 100: avg data time: 1.91e-01, avg batch time: 1.0183, average train loss: 0.9793
[11/25 16:04:36 visual_prompt]: Inference (val):avg data time: 3.70e-04, avg batch time: 0.3080, average loss: 0.6933
[11/25 16:04:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 56.67	
[11/25 16:04:36 visual_prompt]: Best epoch 14: best metric: -0.693
[11/25 16:04:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/25 16:06:21 visual_prompt]: 	Training 100/553. train loss: 0.7365,	0.8086 s / batch. (data: 3.22e-04). ETA=10:39:33, max mem: 20.9 GB 
[11/25 16:08:00 visual_prompt]: 	Training 200/553. train loss: 9.0314,	0.8240 s / batch. (data: 3.13e-04). ETA=10:50:22, max mem: 20.9 GB 
[11/25 16:09:44 visual_prompt]: 	Training 300/553. train loss: 0.8628,	0.8400 s / batch. (data: 7.30e-04). ETA=11:01:37, max mem: 20.9 GB 
[11/25 16:11:21 visual_prompt]: 	Training 400/553. train loss: 0.8945,	0.8474 s / batch. (data: 7.36e-03). ETA=11:06:01, max mem: 20.9 GB 
[11/25 16:13:01 visual_prompt]: 	Training 500/553. train loss: 0.5552,	0.8480 s / batch. (data: 3.13e-04). ETA=11:05:04, max mem: 20.9 GB 
[11/25 16:13:52 visual_prompt]: Epoch 15 / 100: avg data time: 1.82e-01, avg batch time: 1.0069, average train loss: 1.0462
[11/25 16:14:49 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.3081, average loss: 1.8795
[11/25 16:14:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.07	
[11/25 16:14:49 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/25 16:16:32 visual_prompt]: 	Training 100/553. train loss: 0.6909,	0.8360 s / batch. (data: 7.96e-03). ETA=10:53:33, max mem: 20.9 GB 
[11/25 16:18:10 visual_prompt]: 	Training 200/553. train loss: 1.1942,	0.8200 s / batch. (data: 7.96e-03). ETA=10:39:41, max mem: 20.9 GB 
[11/25 16:19:51 visual_prompt]: 	Training 300/553. train loss: 1.1520,	0.8594 s / batch. (data: 2.75e-02). ETA=11:08:58, max mem: 20.9 GB 
[11/25 16:21:31 visual_prompt]: 	Training 400/553. train loss: 1.0239,	0.8363 s / batch. (data: 3.36e-04). ETA=10:49:37, max mem: 20.9 GB 
[11/25 16:23:11 visual_prompt]: 	Training 500/553. train loss: 0.9960,	1.2350 s / batch. (data: 4.09e-01). ETA=15:57:11, max mem: 20.9 GB 
[11/25 16:24:05 visual_prompt]: Epoch 16 / 100: avg data time: 1.79e-01, avg batch time: 1.0047, average train loss: 0.9705
[11/25 16:25:03 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3093, average loss: 0.6910
[11/25 16:25:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.67	
[11/25 16:25:03 visual_prompt]: Best epoch 16: best metric: -0.691
[11/25 16:25:03 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/25 16:26:48 visual_prompt]: 	Training 100/553. train loss: 0.5105,	0.8507 s / batch. (data: 1.07e-02). ETA=10:57:11, max mem: 20.9 GB 
[11/25 16:28:31 visual_prompt]: 	Training 200/553. train loss: 3.3185,	0.8566 s / batch. (data: 3.10e-04). ETA=11:00:18, max mem: 20.9 GB 
[11/25 16:30:11 visual_prompt]: 	Training 300/553. train loss: 1.2886,	0.8365 s / batch. (data: 5.45e-03). ETA=10:43:27, max mem: 20.9 GB 
[11/25 16:31:52 visual_prompt]: 	Training 400/553. train loss: 0.7003,	1.2636 s / batch. (data: 4.31e-01). ETA=16:09:51, max mem: 20.9 GB 
[11/25 16:33:33 visual_prompt]: 	Training 500/553. train loss: 1.3119,	1.8680 s / batch. (data: 1.04e+00). ETA=23:50:39, max mem: 20.9 GB 
[11/25 16:34:27 visual_prompt]: Epoch 17 / 100: avg data time: 1.92e-01, avg batch time: 1.0187, average train loss: 1.1423
[11/25 16:35:25 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3070, average loss: 0.8402
[11/25 16:35:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.00	
[11/25 16:35:25 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/25 16:37:10 visual_prompt]: 	Training 100/553. train loss: 0.6988,	0.8105 s / batch. (data: 4.06e-04). ETA=10:18:37, max mem: 20.9 GB 
[11/25 16:38:54 visual_prompt]: 	Training 200/553. train loss: 0.9375,	0.8455 s / batch. (data: 8.38e-04). ETA=10:43:58, max mem: 20.9 GB 
[11/25 16:40:35 visual_prompt]: 	Training 300/553. train loss: 0.6529,	0.8347 s / batch. (data: 3.62e-04). ETA=10:34:21, max mem: 20.9 GB 
[11/25 16:42:16 visual_prompt]: 	Training 400/553. train loss: 1.2210,	0.8479 s / batch. (data: 3.26e-04). ETA=10:42:59, max mem: 20.9 GB 
[11/25 16:43:56 visual_prompt]: 	Training 500/553. train loss: 0.7179,	0.8320 s / batch. (data: 3.21e-04). ETA=10:29:30, max mem: 20.9 GB 
[11/25 16:44:48 visual_prompt]: Epoch 18 / 100: avg data time: 1.93e-01, avg batch time: 1.0192, average train loss: 0.9786
[11/25 16:45:47 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3080, average loss: 0.6882
[11/25 16:45:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.22	
[11/25 16:45:47 visual_prompt]: Best epoch 18: best metric: -0.688
[11/25 16:45:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/25 16:47:32 visual_prompt]: 	Training 100/553. train loss: 0.7749,	0.8882 s / batch. (data: 5.54e-02). ETA=11:09:48, max mem: 20.9 GB 
[11/25 16:49:14 visual_prompt]: 	Training 200/553. train loss: 0.6822,	0.8504 s / batch. (data: 1.44e-02). ETA=10:39:53, max mem: 20.9 GB 
[11/25 16:50:55 visual_prompt]: 	Training 300/553. train loss: 2.4590,	0.8347 s / batch. (data: 5.42e-03). ETA=10:26:40, max mem: 20.9 GB 
[11/25 16:52:36 visual_prompt]: 	Training 400/553. train loss: 0.5780,	0.8109 s / batch. (data: 3.20e-04). ETA=10:07:24, max mem: 20.9 GB 
[11/25 16:54:13 visual_prompt]: 	Training 500/553. train loss: 0.5742,	1.2286 s / batch. (data: 4.20e-01). ETA=15:18:15, max mem: 20.9 GB 
[11/25 16:55:10 visual_prompt]: Epoch 19 / 100: avg data time: 1.93e-01, avg batch time: 1.0191, average train loss: 0.9127
[11/25 16:56:12 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3059, average loss: 1.4988
[11/25 16:56:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.03	
[11/25 16:56:12 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[11/25 16:58:01 visual_prompt]: 	Training 100/553. train loss: 0.6583,	0.8247 s / batch. (data: 7.96e-03). ETA=10:14:19, max mem: 20.9 GB 
[11/25 16:59:44 visual_prompt]: 	Training 200/553. train loss: 0.5545,	0.8247 s / batch. (data: 3.22e-04). ETA=10:12:55, max mem: 20.9 GB 
[11/25 17:01:26 visual_prompt]: 	Training 300/553. train loss: 0.6616,	0.8480 s / batch. (data: 3.03e-04). ETA=10:28:50, max mem: 20.9 GB 
[11/25 17:03:10 visual_prompt]: 	Training 400/553. train loss: 0.5651,	0.8125 s / batch. (data: 3.37e-04). ETA=10:01:11, max mem: 20.9 GB 
[11/25 17:04:50 visual_prompt]: 	Training 500/553. train loss: 0.8121,	0.8231 s / batch. (data: 3.27e-04). ETA=10:07:36, max mem: 20.9 GB 
[11/25 17:05:45 visual_prompt]: Epoch 20 / 100: avg data time: 2.10e-01, avg batch time: 1.0349, average train loss: 1.0325
[11/25 17:06:42 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3072, average loss: 0.7373
[11/25 17:06:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.75	
[11/25 17:06:42 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[11/25 17:08:29 visual_prompt]: 	Training 100/553. train loss: 1.2247,	0.8224 s / batch. (data: 3.26e-04). ETA=10:05:02, max mem: 20.9 GB 
[11/25 17:10:09 visual_prompt]: 	Training 200/553. train loss: 0.9440,	0.8120 s / batch. (data: 3.36e-04). ETA=9:56:00, max mem: 20.9 GB 
[11/25 17:11:49 visual_prompt]: 	Training 300/553. train loss: 2.4146,	1.2378 s / batch. (data: 4.04e-01). ETA=15:06:29, max mem: 20.9 GB 
[11/25 17:13:28 visual_prompt]: 	Training 400/553. train loss: 1.4602,	0.8240 s / batch. (data: 3.31e-04). ETA=10:02:04, max mem: 20.9 GB 
[11/25 17:15:09 visual_prompt]: 	Training 500/553. train loss: 0.6879,	0.8305 s / batch. (data: 1.05e-02). ETA=10:05:26, max mem: 20.9 GB 
[11/25 17:16:01 visual_prompt]: Epoch 21 / 100: avg data time: 1.85e-01, avg batch time: 1.0095, average train loss: 1.0103
[11/25 17:16:58 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3077, average loss: 0.6883
[11/25 17:16:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.61	
[11/25 17:16:58 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[11/25 17:18:41 visual_prompt]: 	Training 100/553. train loss: 1.0177,	0.8400 s / batch. (data: 1.64e-02). ETA=10:10:11, max mem: 20.9 GB 
[11/25 17:20:22 visual_prompt]: 	Training 200/553. train loss: 0.5729,	0.8210 s / batch. (data: 7.96e-03). ETA=9:55:01, max mem: 20.9 GB 
[11/25 17:22:00 visual_prompt]: 	Training 300/553. train loss: 0.2426,	0.8128 s / batch. (data: 3.00e-04). ETA=9:47:44, max mem: 20.9 GB 
[11/25 17:23:41 visual_prompt]: 	Training 400/553. train loss: 1.6333,	0.8315 s / batch. (data: 3.02e-04). ETA=9:59:53, max mem: 20.9 GB 
[11/25 17:25:21 visual_prompt]: 	Training 500/553. train loss: 0.5899,	0.8428 s / batch. (data: 1.56e-02). ETA=10:06:38, max mem: 20.9 GB 
[11/25 17:26:15 visual_prompt]: Epoch 22 / 100: avg data time: 1.81e-01, avg batch time: 1.0067, average train loss: 0.9236
[11/25 17:27:12 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3081, average loss: 0.7018
[11/25 17:27:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.49	
[11/25 17:27:12 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[11/25 17:28:58 visual_prompt]: 	Training 100/553. train loss: 1.0391,	0.8240 s / batch. (data: 3.11e-04). ETA=9:50:59, max mem: 20.9 GB 
[11/25 17:30:39 visual_prompt]: 	Training 200/553. train loss: 1.2116,	0.8360 s / batch. (data: 3.32e-04). ETA=9:58:12, max mem: 20.9 GB 
[11/25 17:32:21 visual_prompt]: 	Training 300/553. train loss: 0.6818,	0.8367 s / batch. (data: 5.44e-03). ETA=9:57:21, max mem: 20.9 GB 
[11/25 17:33:59 visual_prompt]: 	Training 400/553. train loss: 0.6489,	0.8251 s / batch. (data: 5.42e-03). ETA=9:47:38, max mem: 20.9 GB 
[11/25 17:35:37 visual_prompt]: 	Training 500/553. train loss: 2.0812,	0.8482 s / batch. (data: 2.03e-02). ETA=10:02:42, max mem: 20.9 GB 
[11/25 17:36:29 visual_prompt]: Epoch 23 / 100: avg data time: 1.82e-01, avg batch time: 1.0071, average train loss: 0.9907
[11/25 17:37:27 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3071, average loss: 0.7896
[11/25 17:37:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.53	
[11/25 17:37:27 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[11/25 17:39:09 visual_prompt]: 	Training 100/553. train loss: 1.0404,	0.8280 s / batch. (data: 3.25e-04). ETA=9:46:14, max mem: 20.9 GB 
[11/25 17:40:48 visual_prompt]: 	Training 200/553. train loss: 0.7835,	0.8098 s / batch. (data: 3.26e-04). ETA=9:32:01, max mem: 20.9 GB 
[11/25 17:42:29 visual_prompt]: 	Training 300/553. train loss: 0.7471,	1.0406 s / batch. (data: 2.17e-01). ETA=12:13:16, max mem: 20.9 GB 
[11/25 17:44:11 visual_prompt]: 	Training 400/553. train loss: 0.8219,	0.8292 s / batch. (data: 5.41e-03). ETA=9:42:55, max mem: 20.9 GB 
[11/25 17:45:52 visual_prompt]: 	Training 500/553. train loss: 0.6914,	0.8342 s / batch. (data: 7.96e-03). ETA=9:45:02, max mem: 20.9 GB 
[11/25 17:46:45 visual_prompt]: Epoch 24 / 100: avg data time: 1.83e-01, avg batch time: 1.0084, average train loss: 0.9448
[11/25 17:47:42 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3074, average loss: 1.5532
[11/25 17:47:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.72	
[11/25 17:47:42 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[11/25 17:49:30 visual_prompt]: 	Training 100/553. train loss: 1.3448,	0.8199 s / batch. (data: 3.91e-04). ETA=9:32:56, max mem: 20.9 GB 
[11/25 17:51:07 visual_prompt]: 	Training 200/553. train loss: 0.6383,	1.0480 s / batch. (data: 2.10e-01). ETA=12:10:35, max mem: 20.9 GB 
[11/25 17:52:47 visual_prompt]: 	Training 300/553. train loss: 0.7216,	0.8223 s / batch. (data: 1.05e-02). ETA=9:31:54, max mem: 20.9 GB 
[11/25 17:54:28 visual_prompt]: 	Training 400/553. train loss: 0.6739,	0.8366 s / batch. (data: 2.81e-02). ETA=9:40:24, max mem: 20.9 GB 
[11/25 17:56:07 visual_prompt]: 	Training 500/553. train loss: 0.9483,	1.5359 s / batch. (data: 7.16e-01). ETA=17:43:04, max mem: 20.9 GB 
[11/25 17:57:00 visual_prompt]: Epoch 25 / 100: avg data time: 1.82e-01, avg batch time: 1.0081, average train loss: 0.9823
[11/25 17:57:57 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.3074, average loss: 1.9188
[11/25 17:57:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.34	
[11/25 17:57:57 visual_prompt]: Stopping early.
[11/25 17:57:57 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 17:57:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 17:57:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/25 17:57:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 17:57:57 visual_prompt]: Training with config:
[11/25 17:57:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.25_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/25 17:57:57 visual_prompt]: Loading training data...
[11/25 17:57:57 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 17:57:57 visual_prompt]: Loading validation data...
[11/25 17:57:57 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 17:57:57 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 17:58:02 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 17:58:02 visual_prompt]: tuned percent:0.525
[11/25 17:58:03 visual_prompt]: Device used for model: 0
[11/25 17:58:03 visual_prompt]: Setting up Evaluator...
[11/25 17:58:03 visual_prompt]: Setting up Trainer...
[11/25 17:58:03 visual_prompt]: 	Setting up the optimizer...
[11/25 17:58:03 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 17:59:46 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8200 s / batch. (data: 2.95e-04). ETA=12:34:26, max mem: 20.9 GB 
[11/25 18:01:25 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8195 s / batch. (data: 7.18e-03). ETA=12:32:32, max mem: 20.9 GB 
[11/25 18:03:08 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.1720 s / batch. (data: 3.38e-01). ETA=17:54:19, max mem: 20.9 GB 
[11/25 18:04:43 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8464 s / batch. (data: 1.05e-02). ETA=12:54:28, max mem: 20.9 GB 
[11/25 18:06:20 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8361 s / batch. (data: 5.45e-03). ETA=12:43:36, max mem: 20.9 GB 
[11/25 18:07:11 visual_prompt]: Epoch 1 / 100: avg data time: 1.66e-01, avg batch time: 0.9923, average train loss: 1.5403
[11/25 18:08:06 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3089, average loss: 1.5201
[11/25 18:08:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 18:08:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/25 18:09:46 visual_prompt]: 	Training 100/553. train loss: 0.7428,	1.0267 s / batch. (data: 2.01e-01). ETA=15:35:07, max mem: 20.9 GB 
[11/25 18:11:26 visual_prompt]: 	Training 200/553. train loss: 0.2153,	1.5997 s / batch. (data: 7.85e-01). ETA=1 day, 0:14:17, max mem: 20.9 GB 
[11/25 18:13:10 visual_prompt]: 	Training 300/553. train loss: 0.9035,	1.2431 s / batch. (data: 4.11e-01). ETA=18:48:05, max mem: 20.9 GB 
[11/25 18:14:51 visual_prompt]: 	Training 400/553. train loss: 1.0799,	0.8397 s / batch. (data: 1.60e-02). ETA=12:40:36, max mem: 20.9 GB 
[11/25 18:16:35 visual_prompt]: 	Training 500/553. train loss: 0.6557,	0.8360 s / batch. (data: 5.43e-03). ETA=12:35:51, max mem: 20.9 GB 
[11/25 18:17:26 visual_prompt]: Epoch 2 / 100: avg data time: 1.88e-01, avg batch time: 1.0127, average train loss: 0.8338
[11/25 18:18:25 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3075, average loss: 0.7538
[11/25 18:18:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.77	
[11/25 18:18:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/25 18:20:08 visual_prompt]: 	Training 100/553. train loss: 0.7588,	0.8521 s / batch. (data: 2.17e-02). ETA=12:48:13, max mem: 20.9 GB 
[11/25 18:21:50 visual_prompt]: 	Training 200/553. train loss: 0.6979,	0.8240 s / batch. (data: 3.00e-04). ETA=12:21:31, max mem: 20.9 GB 
[11/25 18:23:30 visual_prompt]: 	Training 300/553. train loss: 0.6494,	0.8283 s / batch. (data: 1.05e-02). ETA=12:24:03, max mem: 20.9 GB 
[11/25 18:25:11 visual_prompt]: 	Training 400/553. train loss: 0.6664,	0.8480 s / batch. (data: 2.98e-04). ETA=12:40:16, max mem: 20.9 GB 
[11/25 18:26:53 visual_prompt]: 	Training 500/553. train loss: 0.7062,	1.1720 s / batch. (data: 3.29e-01). ETA=17:28:51, max mem: 20.9 GB 
[11/25 18:27:45 visual_prompt]: Epoch 3 / 100: avg data time: 1.87e-01, avg batch time: 1.0117, average train loss: 0.7698
[11/25 18:28:42 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3066, average loss: 0.7097
[11/25 18:28:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 57.19	
[11/25 18:28:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/25 18:30:27 visual_prompt]: 	Training 100/553. train loss: 0.7156,	0.8301 s / batch. (data: 3.11e-04). ETA=12:20:45, max mem: 20.9 GB 
[11/25 18:32:09 visual_prompt]: 	Training 200/553. train loss: 1.4199,	0.8249 s / batch. (data: 3.19e-04). ETA=12:14:42, max mem: 20.9 GB 
[11/25 18:33:49 visual_prompt]: 	Training 300/553. train loss: 0.6373,	1.5134 s / batch. (data: 7.03e-01). ETA=22:25:27, max mem: 20.9 GB 
[11/25 18:35:25 visual_prompt]: 	Training 400/553. train loss: 0.5697,	0.9366 s / batch. (data: 1.03e-01). ETA=13:51:05, max mem: 20.9 GB 
[11/25 18:37:07 visual_prompt]: 	Training 500/553. train loss: 0.7092,	3.5430 s / batch. (data: 2.73e+00). ETA=2 days, 4:17:57, max mem: 20.9 GB 
[11/25 18:38:01 visual_prompt]: Epoch 4 / 100: avg data time: 1.85e-01, avg batch time: 1.0104, average train loss: 0.8536
[11/25 18:38:59 visual_prompt]: Inference (val):avg data time: 2.78e-04, avg batch time: 0.3088, average loss: 0.8408
[11/25 18:38:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.80	
[11/25 18:38:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/25 18:40:42 visual_prompt]: 	Training 100/553. train loss: 0.4815,	0.8320 s / batch. (data: 3.18e-04). ETA=12:14:46, max mem: 20.9 GB 
[11/25 18:42:22 visual_prompt]: 	Training 200/553. train loss: 0.6136,	1.1599 s / batch. (data: 3.27e-01). ETA=17:02:27, max mem: 20.9 GB 
[11/25 18:44:04 visual_prompt]: 	Training 300/553. train loss: 1.3234,	0.8318 s / batch. (data: 3.18e-04). ETA=12:11:46, max mem: 20.9 GB 
[11/25 18:45:43 visual_prompt]: 	Training 400/553. train loss: 1.1613,	0.8411 s / batch. (data: 3.06e-04). ETA=12:18:33, max mem: 20.9 GB 
[11/25 18:47:27 visual_prompt]: 	Training 500/553. train loss: 0.5823,	0.8093 s / batch. (data: 2.95e-04). ETA=11:49:18, max mem: 20.9 GB 
[11/25 18:48:20 visual_prompt]: Epoch 5 / 100: avg data time: 1.89e-01, avg batch time: 1.0139, average train loss: 0.8950
[11/25 18:49:17 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3073, average loss: 0.7901
[11/25 18:49:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.59	
[11/25 18:49:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/25 18:51:02 visual_prompt]: 	Training 100/553. train loss: 0.5826,	0.8240 s / batch. (data: 2.93e-04). ETA=12:00:06, max mem: 20.9 GB 
[11/25 18:52:38 visual_prompt]: 	Training 200/553. train loss: 0.6062,	0.8381 s / batch. (data: 1.20e-02). ETA=12:11:04, max mem: 20.9 GB 
[11/25 18:54:12 visual_prompt]: 	Training 300/553. train loss: 0.5727,	0.8126 s / batch. (data: 3.31e-04). ETA=11:47:23, max mem: 20.9 GB 
[11/25 18:55:52 visual_prompt]: 	Training 400/553. train loss: 0.5861,	0.8280 s / batch. (data: 5.48e-04). ETA=11:59:25, max mem: 20.9 GB 
[11/25 18:57:27 visual_prompt]: 	Training 500/553. train loss: 0.8666,	0.8160 s / batch. (data: 3.37e-04). ETA=11:47:41, max mem: 20.9 GB 
[11/25 18:58:16 visual_prompt]: Epoch 6 / 100: avg data time: 1.48e-01, avg batch time: 0.9744, average train loss: 0.7715
[11/25 18:59:11 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3079, average loss: 0.6998
[11/25 18:59:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.70	
[11/25 18:59:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/25 19:00:51 visual_prompt]: 	Training 100/553. train loss: 0.7248,	0.8351 s / batch. (data: 1.15e-02). ETA=12:02:04, max mem: 20.9 GB 
[11/25 19:02:31 visual_prompt]: 	Training 200/553. train loss: 0.5871,	0.8302 s / batch. (data: 3.16e-04). ETA=11:56:31, max mem: 20.9 GB 
[11/25 19:04:14 visual_prompt]: 	Training 300/553. train loss: 0.6997,	1.9599 s / batch. (data: 1.14e+00). ETA=1 day, 4:08:09, max mem: 20.9 GB 
[11/25 19:05:56 visual_prompt]: 	Training 400/553. train loss: 0.6116,	2.2041 s / batch. (data: 1.35e+00). ETA=1 day, 7:34:49, max mem: 20.9 GB 
[11/25 19:07:36 visual_prompt]: 	Training 500/553. train loss: 1.1290,	0.8600 s / batch. (data: 3.01e-04). ETA=12:17:55, max mem: 20.9 GB 
[11/25 19:08:27 visual_prompt]: Epoch 7 / 100: avg data time: 1.79e-01, avg batch time: 1.0061, average train loss: 0.7805
[11/25 19:09:26 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3087, average loss: 0.7299
[11/25 19:09:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.11	
[11/25 19:09:26 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/25 19:11:09 visual_prompt]: 	Training 100/553. train loss: 0.7028,	0.8240 s / batch. (data: 3.28e-04). ETA=11:44:56, max mem: 20.9 GB 
[11/25 19:12:52 visual_prompt]: 	Training 200/553. train loss: 1.2375,	0.8296 s / batch. (data: 7.48e-03). ETA=11:48:21, max mem: 20.9 GB 
[11/25 19:14:34 visual_prompt]: 	Training 300/553. train loss: 0.9843,	0.8343 s / batch. (data: 7.68e-04). ETA=11:50:56, max mem: 20.9 GB 
[11/25 19:16:15 visual_prompt]: 	Training 400/553. train loss: 0.7567,	1.1606 s / batch. (data: 3.38e-01). ETA=16:27:04, max mem: 20.9 GB 
[11/25 19:17:57 visual_prompt]: 	Training 500/553. train loss: 1.0193,	1.5000 s / batch. (data: 6.72e-01). ETA=21:13:13, max mem: 20.9 GB 
[11/25 19:18:51 visual_prompt]: Epoch 8 / 100: avg data time: 1.96e-01, avg batch time: 1.0217, average train loss: 0.8128
[11/25 19:19:49 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3069, average loss: 0.8603
[11/25 19:19:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.03	
[11/25 19:19:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/25 19:21:33 visual_prompt]: 	Training 100/553. train loss: 0.5982,	0.8305 s / batch. (data: 1.19e-02). ETA=11:42:50, max mem: 20.9 GB 
[11/25 19:23:12 visual_prompt]: 	Training 200/553. train loss: 0.6442,	0.8244 s / batch. (data: 3.07e-04). ETA=11:36:18, max mem: 20.9 GB 
[11/25 19:24:52 visual_prompt]: 	Training 300/553. train loss: 0.6454,	1.8440 s / batch. (data: 9.91e-01). ETA=1 day, 1:54:23, max mem: 20.9 GB 
[11/25 19:26:34 visual_prompt]: 	Training 400/553. train loss: 0.5966,	0.8320 s / batch. (data: 7.95e-03). ETA=11:39:56, max mem: 20.9 GB 
[11/25 19:28:19 visual_prompt]: 	Training 500/553. train loss: 0.6879,	1.0098 s / batch. (data: 1.88e-01). ETA=14:07:50, max mem: 20.9 GB 
[11/25 19:29:11 visual_prompt]: Epoch 9 / 100: avg data time: 1.90e-01, avg batch time: 1.0161, average train loss: 0.7972
[11/25 19:30:12 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3072, average loss: 1.0446
[11/25 19:30:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.04	
[11/25 19:30:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/25 19:32:01 visual_prompt]: 	Training 100/553. train loss: 0.9860,	0.8400 s / batch. (data: 1.20e-02). ETA=11:43:07, max mem: 20.9 GB 
[11/25 19:33:42 visual_prompt]: 	Training 200/553. train loss: 0.5646,	0.8320 s / batch. (data: 3.22e-04). ETA=11:35:01, max mem: 20.9 GB 
[11/25 19:35:24 visual_prompt]: 	Training 300/553. train loss: 0.6824,	0.8320 s / batch. (data: 8.44e-03). ETA=11:33:41, max mem: 20.9 GB 
[11/25 19:37:03 visual_prompt]: 	Training 400/553. train loss: 0.9630,	0.8210 s / batch. (data: 1.07e-02). ETA=11:23:08, max mem: 20.9 GB 
[11/25 19:38:47 visual_prompt]: 	Training 500/553. train loss: 1.9114,	0.8440 s / batch. (data: 5.49e-03). ETA=11:40:48, max mem: 20.9 GB 
[11/25 19:39:41 visual_prompt]: Epoch 10 / 100: avg data time: 2.03e-01, avg batch time: 1.0280, average train loss: 0.9018
[11/25 19:40:39 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3081, average loss: 0.8534
[11/25 19:40:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.42	
[11/25 19:40:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/25 19:42:26 visual_prompt]: 	Training 100/553. train loss: 0.7061,	0.8359 s / batch. (data: 1.19e-02). ETA=11:31:59, max mem: 20.9 GB 
[11/25 19:44:10 visual_prompt]: 	Training 200/553. train loss: 2.2222,	0.8400 s / batch. (data: 2.93e-04). ETA=11:33:59, max mem: 20.9 GB 
[11/25 19:45:51 visual_prompt]: 	Training 300/553. train loss: 0.1973,	1.9826 s / batch. (data: 1.16e+00). ETA=1 day, 3:14:37, max mem: 20.9 GB 
[11/25 19:47:30 visual_prompt]: 	Training 400/553. train loss: 0.5758,	0.8512 s / batch. (data: 1.12e-02). ETA=11:40:25, max mem: 20.9 GB 
[11/25 19:49:10 visual_prompt]: 	Training 500/553. train loss: 0.7289,	0.8371 s / batch. (data: 8.83e-03). ETA=11:27:24, max mem: 20.9 GB 
[11/25 19:50:02 visual_prompt]: Epoch 11 / 100: avg data time: 1.92e-01, avg batch time: 1.0188, average train loss: 0.8837
[11/25 19:51:01 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.3067, average loss: 0.8879
[11/25 19:51:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.53	
[11/25 19:51:01 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/25 19:52:48 visual_prompt]: 	Training 100/553. train loss: 0.7659,	0.8314 s / batch. (data: 7.59e-03). ETA=11:20:34, max mem: 20.9 GB 
[11/25 19:54:30 visual_prompt]: 	Training 200/553. train loss: 0.8116,	0.8187 s / batch. (data: 3.28e-04). ETA=11:08:51, max mem: 20.9 GB 
[11/25 19:56:10 visual_prompt]: 	Training 300/553. train loss: 0.6508,	0.8207 s / batch. (data: 3.11e-04). ETA=11:09:06, max mem: 20.9 GB 
[11/25 19:57:51 visual_prompt]: 	Training 400/553. train loss: 1.0739,	0.8236 s / batch. (data: 1.06e-02). ETA=11:10:03, max mem: 20.9 GB 
[11/25 19:59:33 visual_prompt]: 	Training 500/553. train loss: 1.8426,	0.8144 s / batch. (data: 3.44e-03). ETA=11:01:13, max mem: 20.9 GB 
[11/25 20:00:25 visual_prompt]: Epoch 12 / 100: avg data time: 1.94e-01, avg batch time: 1.0200, average train loss: 0.8246
[11/25 20:01:23 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3085, average loss: 1.5140
[11/25 20:01:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.39	
[11/25 20:01:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/25 20:03:11 visual_prompt]: 	Training 100/553. train loss: 0.6790,	0.8360 s / batch. (data: 2.95e-04). ETA=11:16:38, max mem: 20.9 GB 
[11/25 20:04:48 visual_prompt]: 	Training 200/553. train loss: 0.7078,	0.8511 s / batch. (data: 3.11e-02). ETA=11:27:27, max mem: 20.9 GB 
[11/25 20:06:31 visual_prompt]: 	Training 300/553. train loss: 0.6904,	1.7891 s / batch. (data: 9.46e-01). ETA=1 day, 0:02:07, max mem: 20.9 GB 
[11/25 20:08:11 visual_prompt]: 	Training 400/553. train loss: 1.1358,	0.8123 s / batch. (data: 3.21e-04). ETA=10:53:23, max mem: 20.9 GB 
[11/25 20:09:54 visual_prompt]: 	Training 500/553. train loss: 0.7014,	0.8449 s / batch. (data: 1.56e-02). ETA=11:18:12, max mem: 20.9 GB 
[11/25 20:10:46 visual_prompt]: Epoch 13 / 100: avg data time: 1.93e-01, avg batch time: 1.0185, average train loss: 0.8478
[11/25 20:11:45 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3089, average loss: 0.7749
[11/25 20:11:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.02	
[11/25 20:11:45 visual_prompt]: Best epoch 13: best metric: -0.775
[11/25 20:11:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/25 20:13:32 visual_prompt]: 	Training 100/553. train loss: 0.6213,	0.8327 s / batch. (data: 5.43e-03). ETA=11:06:18, max mem: 20.9 GB 
[11/25 20:15:12 visual_prompt]: 	Training 200/553. train loss: 1.1663,	1.3069 s / batch. (data: 4.84e-01). ETA=17:23:35, max mem: 20.9 GB 
[11/25 20:16:54 visual_prompt]: 	Training 300/553. train loss: 0.7462,	0.8554 s / batch. (data: 2.81e-02). ETA=11:21:39, max mem: 20.9 GB 
[11/25 20:18:35 visual_prompt]: 	Training 400/553. train loss: 0.8124,	0.8347 s / batch. (data: 7.96e-03). ETA=11:03:43, max mem: 20.9 GB 
[11/25 20:20:16 visual_prompt]: 	Training 500/553. train loss: 1.1173,	0.8160 s / batch. (data: 3.10e-04). ETA=10:47:30, max mem: 20.9 GB 
[11/25 20:21:09 visual_prompt]: Epoch 14 / 100: avg data time: 1.94e-01, avg batch time: 1.0199, average train loss: 0.7847
[11/25 20:22:07 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.3080, average loss: 0.6897
[11/25 20:22:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.63	
[11/25 20:22:07 visual_prompt]: Best epoch 14: best metric: -0.690
[11/25 20:22:07 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/25 20:23:53 visual_prompt]: 	Training 100/553. train loss: 0.7982,	0.8433 s / batch. (data: 2.13e-02). ETA=11:06:59, max mem: 20.9 GB 
[11/25 20:25:33 visual_prompt]: 	Training 200/553. train loss: 0.8254,	0.8122 s / batch. (data: 3.12e-04). ETA=10:41:05, max mem: 20.9 GB 
[11/25 20:27:16 visual_prompt]: 	Training 300/553. train loss: 0.6573,	0.8360 s / batch. (data: 2.98e-04). ETA=10:58:28, max mem: 20.9 GB 
[11/25 20:28:55 visual_prompt]: 	Training 400/553. train loss: 0.7014,	1.4075 s / batch. (data: 5.99e-01). ETA=18:26:16, max mem: 20.9 GB 
[11/25 20:30:38 visual_prompt]: 	Training 500/553. train loss: 0.7251,	0.8242 s / batch. (data: 5.48e-03). ETA=10:46:26, max mem: 20.9 GB 
[11/25 20:31:31 visual_prompt]: Epoch 15 / 100: avg data time: 1.94e-01, avg batch time: 1.0199, average train loss: 0.9029
[11/25 20:32:29 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3083, average loss: 0.8379
[11/25 20:32:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.61	
[11/25 20:32:29 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/25 20:34:14 visual_prompt]: 	Training 100/553. train loss: 0.5627,	0.8160 s / batch. (data: 3.10e-04). ETA=10:37:54, max mem: 20.9 GB 
[11/25 20:35:56 visual_prompt]: 	Training 200/553. train loss: 1.0237,	0.8320 s / batch. (data: 6.52e-04). ETA=10:49:01, max mem: 20.9 GB 
[11/25 20:37:37 visual_prompt]: 	Training 300/553. train loss: 0.9025,	0.8633 s / batch. (data: 1.13e-02). ETA=11:12:00, max mem: 20.9 GB 
[11/25 20:39:17 visual_prompt]: 	Training 400/553. train loss: 0.6871,	0.8202 s / batch. (data: 3.08e-04). ETA=10:37:06, max mem: 20.9 GB 
[11/25 20:40:57 visual_prompt]: 	Training 500/553. train loss: 0.7218,	1.6138 s / batch. (data: 8.04e-01). ETA=20:50:51, max mem: 20.9 GB 
[11/25 20:41:49 visual_prompt]: Epoch 16 / 100: avg data time: 1.85e-01, avg batch time: 1.0114, average train loss: 0.8325
[11/25 20:42:48 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3084, average loss: 0.6887
[11/25 20:42:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.81	
[11/25 20:42:48 visual_prompt]: Best epoch 16: best metric: -0.689
[11/25 20:42:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/25 20:44:35 visual_prompt]: 	Training 100/553. train loss: 0.5818,	0.8299 s / batch. (data: 6.66e-03). ETA=10:41:07, max mem: 20.9 GB 
[11/25 20:46:25 visual_prompt]: 	Training 200/553. train loss: 1.9924,	0.8090 s / batch. (data: 3.12e-04). ETA=10:23:37, max mem: 20.9 GB 
[11/25 20:48:07 visual_prompt]: 	Training 300/553. train loss: 1.7095,	0.8501 s / batch. (data: 1.00e-02). ETA=10:53:52, max mem: 20.9 GB 
[11/25 20:49:50 visual_prompt]: 	Training 400/553. train loss: 0.6308,	1.2789 s / batch. (data: 4.63e-01). ETA=16:21:34, max mem: 20.9 GB 
[11/25 20:51:32 visual_prompt]: 	Training 500/553. train loss: 1.0265,	1.8720 s / batch. (data: 1.05e+00). ETA=23:53:43, max mem: 20.9 GB 
[11/25 20:52:27 visual_prompt]: Epoch 17 / 100: avg data time: 2.20e-01, avg batch time: 1.0454, average train loss: 0.9583
[11/25 20:53:25 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3082, average loss: 0.7726
[11/25 20:53:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.10	
[11/25 20:53:25 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/25 20:55:10 visual_prompt]: 	Training 100/553. train loss: 0.7899,	0.8456 s / batch. (data: 1.36e-02). ETA=10:45:29, max mem: 20.9 GB 
[11/25 20:56:55 visual_prompt]: 	Training 200/553. train loss: 0.8118,	0.8480 s / batch. (data: 3.10e-04). ETA=10:45:52, max mem: 20.9 GB 
[11/25 20:58:36 visual_prompt]: 	Training 300/553. train loss: 0.6788,	0.8285 s / batch. (data: 5.44e-03). ETA=10:29:39, max mem: 20.9 GB 
[11/25 21:00:17 visual_prompt]: 	Training 400/553. train loss: 0.7317,	0.8352 s / batch. (data: 7.48e-03). ETA=10:33:22, max mem: 20.9 GB 
[11/25 21:01:57 visual_prompt]: 	Training 500/553. train loss: 1.1800,	0.9881 s / batch. (data: 1.67e-01). ETA=12:27:36, max mem: 20.9 GB 
[11/25 21:02:49 visual_prompt]: Epoch 18 / 100: avg data time: 1.96e-01, avg batch time: 1.0206, average train loss: 1.0210
[11/25 21:03:48 visual_prompt]: Inference (val):avg data time: 5.04e-05, avg batch time: 0.3088, average loss: 0.9951
[11/25 21:03:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.00	
[11/25 21:03:48 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/25 21:05:33 visual_prompt]: 	Training 100/553. train loss: 0.5707,	0.8280 s / batch. (data: 3.66e-04). ETA=10:24:21, max mem: 20.9 GB 
[11/25 21:07:15 visual_prompt]: 	Training 200/553. train loss: 0.5834,	0.8240 s / batch. (data: 3.47e-04). ETA=10:19:58, max mem: 20.9 GB 
[11/25 21:08:57 visual_prompt]: 	Training 300/553. train loss: 1.9944,	0.8560 s / batch. (data: 3.21e-04). ETA=10:42:38, max mem: 20.9 GB 
[11/25 21:10:40 visual_prompt]: 	Training 400/553. train loss: 0.6432,	0.8181 s / batch. (data: 2.98e-04). ETA=10:12:49, max mem: 20.9 GB 
[11/25 21:12:17 visual_prompt]: 	Training 500/553. train loss: 0.6219,	0.8538 s / batch. (data: 2.23e-02). ETA=10:38:10, max mem: 20.9 GB 
[11/25 21:13:10 visual_prompt]: Epoch 19 / 100: avg data time: 1.90e-01, avg batch time: 1.0159, average train loss: 0.8275
[11/25 21:14:08 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.3100, average loss: 1.2959
[11/25 21:14:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.96	
[11/25 21:14:08 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[11/25 21:15:52 visual_prompt]: 	Training 100/553. train loss: 0.5875,	1.6796 s / batch. (data: 8.54e-01). ETA=20:51:07, max mem: 20.9 GB 
[11/25 21:17:35 visual_prompt]: 	Training 200/553. train loss: 0.5600,	0.8200 s / batch. (data: 3.15e-04). ETA=10:09:27, max mem: 20.9 GB 
[11/25 21:19:17 visual_prompt]: 	Training 300/553. train loss: 0.9484,	0.8436 s / batch. (data: 1.05e-02). ETA=10:25:32, max mem: 20.9 GB 
[11/25 21:20:58 visual_prompt]: 	Training 400/553. train loss: 0.6638,	0.8334 s / batch. (data: 3.09e-04). ETA=10:16:35, max mem: 20.9 GB 
[11/25 21:22:38 visual_prompt]: 	Training 500/553. train loss: 0.7173,	0.8240 s / batch. (data: 3.12e-04). ETA=10:08:17, max mem: 20.9 GB 
[11/25 21:23:32 visual_prompt]: Epoch 20 / 100: avg data time: 1.93e-01, avg batch time: 1.0196, average train loss: 0.8286
[11/25 21:24:31 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3082, average loss: 0.9962
[11/25 21:24:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.31	
[11/25 21:24:31 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[11/25 21:26:18 visual_prompt]: 	Training 100/553. train loss: 0.5715,	0.8400 s / batch. (data: 7.07e-03). ETA=10:17:55, max mem: 20.9 GB 
[11/25 21:27:59 visual_prompt]: 	Training 200/553. train loss: 0.9737,	0.8300 s / batch. (data: 1.55e-02). ETA=10:09:14, max mem: 20.9 GB 
[11/25 21:29:40 visual_prompt]: 	Training 300/553. train loss: 2.0618,	1.0407 s / batch. (data: 2.08e-01). ETA=12:42:09, max mem: 20.9 GB 
[11/25 21:31:20 visual_prompt]: 	Training 400/553. train loss: 1.4197,	0.8445 s / batch. (data: 1.24e-02). ETA=10:17:00, max mem: 20.9 GB 
[11/25 21:33:03 visual_prompt]: 	Training 500/553. train loss: 0.6889,	0.8429 s / batch. (data: 1.05e-02). ETA=10:14:29, max mem: 20.9 GB 
[11/25 21:33:55 visual_prompt]: Epoch 21 / 100: avg data time: 1.94e-01, avg batch time: 1.0191, average train loss: 0.8497
[11/25 21:34:53 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.3073, average loss: 0.6998
[11/25 21:34:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.33	
[11/25 21:34:53 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[11/25 21:36:37 visual_prompt]: 	Training 100/553. train loss: 1.6218,	0.8240 s / batch. (data: 3.11e-04). ETA=9:58:36, max mem: 20.9 GB 
[11/25 21:38:19 visual_prompt]: 	Training 200/553. train loss: 0.8761,	0.8360 s / batch. (data: 7.94e-03). ETA=10:05:54, max mem: 20.9 GB 
[11/25 21:39:58 visual_prompt]: 	Training 300/553. train loss: 0.3297,	0.8491 s / batch. (data: 1.39e-02). ETA=10:13:59, max mem: 20.9 GB 
[11/25 21:41:40 visual_prompt]: 	Training 400/553. train loss: 0.7813,	0.8320 s / batch. (data: 3.00e-04). ETA=10:00:16, max mem: 20.9 GB 
[11/25 21:43:21 visual_prompt]: 	Training 500/553. train loss: 0.7249,	0.8480 s / batch. (data: 7.94e-03). ETA=10:10:21, max mem: 20.9 GB 
[11/25 21:44:16 visual_prompt]: Epoch 22 / 100: avg data time: 1.93e-01, avg batch time: 1.0171, average train loss: 0.8943
[11/25 21:45:14 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3082, average loss: 0.7687
[11/25 21:45:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.40	
[11/25 21:45:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[11/25 21:47:01 visual_prompt]: 	Training 100/553. train loss: 0.9184,	0.8407 s / batch. (data: 5.41e-03). ETA=10:02:59, max mem: 20.9 GB 
[11/25 21:48:43 visual_prompt]: 	Training 200/553. train loss: 0.7380,	0.8480 s / batch. (data: 7.93e-03). ETA=10:06:47, max mem: 20.9 GB 
[11/25 21:50:26 visual_prompt]: 	Training 300/553. train loss: 0.5879,	0.8485 s / batch. (data: 7.51e-04). ETA=10:05:45, max mem: 20.9 GB 
[11/25 21:52:05 visual_prompt]: 	Training 400/553. train loss: 0.5716,	0.8205 s / batch. (data: 3.11e-04). ETA=9:44:23, max mem: 20.9 GB 
[11/25 21:53:44 visual_prompt]: 	Training 500/553. train loss: 0.2949,	0.8226 s / batch. (data: 3.01e-04). ETA=9:44:31, max mem: 20.9 GB 
[11/25 21:54:36 visual_prompt]: Epoch 23 / 100: avg data time: 1.91e-01, avg batch time: 1.0164, average train loss: 0.8408
[11/25 21:55:33 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3093, average loss: 1.0228
[11/25 21:55:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.63	
[11/25 21:55:33 visual_prompt]: Stopping early.
[11/25 21:55:34 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 21:55:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 21:55:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/25 21:55:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 21:55:34 visual_prompt]: Training with config:
[11/25 21:55:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.25_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/25 21:55:34 visual_prompt]: Loading training data...
[11/25 21:55:34 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 21:55:34 visual_prompt]: Loading validation data...
[11/25 21:55:34 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 21:55:34 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 21:55:42 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 21:55:42 visual_prompt]: tuned percent:0.525
[11/25 21:55:42 visual_prompt]: Device used for model: 0
[11/25 21:55:42 visual_prompt]: Setting up Evaluator...
[11/25 21:55:42 visual_prompt]: Setting up Trainer...
[11/25 21:55:42 visual_prompt]: 	Setting up the optimizer...
[11/25 21:55:42 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 21:57:25 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8268 s / batch. (data: 4.34e-04). ETA=12:40:37, max mem: 20.9 GB 
[11/25 21:59:04 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8233 s / batch. (data: 2.88e-04). ETA=12:36:06, max mem: 20.9 GB 
[11/25 22:00:46 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8680 s / batch. (data: 4.01e-02). ETA=13:15:39, max mem: 20.9 GB 
[11/25 22:02:23 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 7.96e-03). ETA=12:52:15, max mem: 20.9 GB 
[11/25 22:04:06 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8395 s / batch. (data: 7.91e-04). ETA=12:46:46, max mem: 20.9 GB 
[11/25 22:04:59 visual_prompt]: Epoch 1 / 100: avg data time: 1.81e-01, avg batch time: 1.0070, average train loss: 1.5403
[11/25 22:05:56 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3081, average loss: 1.5201
[11/25 22:05:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 22:05:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/25 22:07:39 visual_prompt]: 	Training 100/553. train loss: 0.7438,	1.0449 s / batch. (data: 2.12e-01). ETA=15:51:39, max mem: 20.9 GB 
[11/25 22:09:18 visual_prompt]: 	Training 200/553. train loss: 0.2134,	0.8201 s / batch. (data: 3.57e-04). ETA=12:25:31, max mem: 20.9 GB 
[11/25 22:11:00 visual_prompt]: 	Training 300/553. train loss: 0.9101,	1.0324 s / batch. (data: 2.22e-01). ETA=15:36:48, max mem: 20.9 GB 
[11/25 22:12:38 visual_prompt]: 	Training 400/553. train loss: 1.0998,	0.8514 s / batch. (data: 1.55e-02). ETA=12:51:10, max mem: 20.9 GB 
[11/25 22:14:19 visual_prompt]: 	Training 500/553. train loss: 0.6529,	0.8122 s / batch. (data: 3.00e-04). ETA=12:14:17, max mem: 20.9 GB 
[11/25 22:15:10 visual_prompt]: Epoch 2 / 100: avg data time: 1.75e-01, avg batch time: 1.0010, average train loss: 0.8370
[11/25 22:16:07 visual_prompt]: Inference (val):avg data time: 1.64e-04, avg batch time: 0.3077, average loss: 0.7615
[11/25 22:16:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.85	
[11/25 22:16:07 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/25 22:17:49 visual_prompt]: 	Training 100/553. train loss: 0.7621,	0.8249 s / batch. (data: 3.19e-04). ETA=12:23:44, max mem: 20.9 GB 
[11/25 22:19:31 visual_prompt]: 	Training 200/553. train loss: 0.7002,	2.2092 s / batch. (data: 1.37e+00). ETA=1 day, 9:08:04, max mem: 20.9 GB 
[11/25 22:21:09 visual_prompt]: 	Training 300/553. train loss: 0.6370,	0.8126 s / batch. (data: 2.97e-04). ETA=12:09:53, max mem: 20.9 GB 
[11/25 22:22:50 visual_prompt]: 	Training 400/553. train loss: 0.7117,	0.8244 s / batch. (data: 3.07e-04). ETA=12:19:09, max mem: 20.9 GB 
[11/25 22:24:31 visual_prompt]: 	Training 500/553. train loss: 0.7734,	1.3040 s / batch. (data: 4.72e-01). ETA=19:26:55, max mem: 20.9 GB 
[11/25 22:25:21 visual_prompt]: Epoch 3 / 100: avg data time: 1.77e-01, avg batch time: 1.0020, average train loss: 0.7755
[11/25 22:26:18 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3077, average loss: 0.7067
[11/25 22:26:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 56.78	
[11/25 22:26:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/25 22:28:03 visual_prompt]: 	Training 100/553. train loss: 0.7445,	0.8240 s / batch. (data: 2.89e-04). ETA=12:15:18, max mem: 20.9 GB 
[11/25 22:29:42 visual_prompt]: 	Training 200/553. train loss: 1.4261,	0.8204 s / batch. (data: 3.15e-04). ETA=12:10:43, max mem: 20.9 GB 
[11/25 22:31:22 visual_prompt]: 	Training 300/553. train loss: 0.6587,	0.8374 s / batch. (data: 9.36e-03). ETA=12:24:29, max mem: 20.9 GB 
[11/25 22:32:57 visual_prompt]: 	Training 400/553. train loss: 0.5731,	1.0752 s / batch. (data: 2.59e-01). ETA=15:54:05, max mem: 20.9 GB 
[11/25 22:34:38 visual_prompt]: 	Training 500/553. train loss: 0.6808,	2.5174 s / batch. (data: 1.67e+00). ETA=1 day, 13:09:35, max mem: 20.9 GB 
[11/25 22:35:33 visual_prompt]: Epoch 4 / 100: avg data time: 1.77e-01, avg batch time: 1.0024, average train loss: 0.8678
[11/25 22:36:30 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3073, average loss: 0.8937
[11/25 22:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.42	
[11/25 22:36:30 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/25 22:38:12 visual_prompt]: 	Training 100/553. train loss: 0.5147,	0.8257 s / batch. (data: 3.05e-04). ETA=12:09:10, max mem: 20.9 GB 
[11/25 22:39:52 visual_prompt]: 	Training 200/553. train loss: 0.6129,	1.3408 s / batch. (data: 4.96e-01). ETA=19:41:53, max mem: 20.9 GB 
[11/25 22:41:33 visual_prompt]: 	Training 300/553. train loss: 1.5914,	0.8104 s / batch. (data: 2.93e-04). ETA=11:52:59, max mem: 20.9 GB 
[11/25 22:43:11 visual_prompt]: 	Training 400/553. train loss: 1.0384,	0.8282 s / batch. (data: 3.58e-04). ETA=12:07:14, max mem: 20.9 GB 
[11/25 22:44:51 visual_prompt]: 	Training 500/553. train loss: 0.5678,	0.8201 s / batch. (data: 3.03e-04). ETA=11:58:45, max mem: 20.9 GB 
[11/25 22:45:44 visual_prompt]: Epoch 5 / 100: avg data time: 1.76e-01, avg batch time: 1.0018, average train loss: 0.9647
[11/25 22:46:41 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.3082, average loss: 0.8294
[11/25 22:46:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.77	
[11/25 22:46:41 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/25 22:48:26 visual_prompt]: 	Training 100/553. train loss: 0.5381,	0.8384 s / batch. (data: 8.57e-04). ETA=12:12:43, max mem: 20.9 GB 
[11/25 22:50:04 visual_prompt]: 	Training 200/553. train loss: 0.7757,	0.8440 s / batch. (data: 3.16e-04). ETA=12:16:09, max mem: 20.9 GB 
[11/25 22:51:42 visual_prompt]: 	Training 300/553. train loss: 0.5882,	0.8187 s / batch. (data: 3.32e-04). ETA=11:52:43, max mem: 20.9 GB 
[11/25 22:53:30 visual_prompt]: 	Training 400/553. train loss: 0.6270,	0.8275 s / batch. (data: 7.92e-03). ETA=11:58:59, max mem: 20.9 GB 
[11/25 22:55:15 visual_prompt]: 	Training 500/553. train loss: 0.9022,	0.9512 s / batch. (data: 1.28e-01). ETA=13:44:55, max mem: 20.9 GB 
[11/25 22:56:11 visual_prompt]: Epoch 6 / 100: avg data time: 2.05e-01, avg batch time: 1.0301, average train loss: 0.8342
[11/25 22:57:11 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3082, average loss: 0.7804
[11/25 22:57:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.93	
[11/25 22:57:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/25 22:58:59 visual_prompt]: 	Training 100/553. train loss: 1.0074,	0.8410 s / batch. (data: 1.05e-02). ETA=12:07:13, max mem: 20.9 GB 
[11/25 23:00:47 visual_prompt]: 	Training 200/553. train loss: 0.5370,	1.3432 s / batch. (data: 5.31e-01). ETA=19:19:12, max mem: 20.9 GB 
[11/25 23:02:36 visual_prompt]: 	Training 300/553. train loss: 0.5949,	2.3407 s / batch. (data: 1.53e+00). ETA=1 day, 9:36:10, max mem: 20.9 GB 
[11/25 23:04:20 visual_prompt]: 	Training 400/553. train loss: 0.5927,	2.1964 s / batch. (data: 1.36e+00). ETA=1 day, 7:28:12, max mem: 20.9 GB 
[11/25 23:06:03 visual_prompt]: 	Training 500/553. train loss: 1.4072,	0.8265 s / batch. (data: 3.22e-04). ETA=11:49:07, max mem: 20.9 GB 
[11/25 23:07:05 visual_prompt]: Epoch 7 / 100: avg data time: 2.50e-01, avg batch time: 1.0741, average train loss: 0.8535
[11/25 23:08:04 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3087, average loss: 0.7186
[11/25 23:08:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.45	
[11/25 23:08:04 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/25 23:09:48 visual_prompt]: 	Training 100/553. train loss: 0.7326,	0.8457 s / batch. (data: 2.95e-04). ETA=12:03:27, max mem: 20.9 GB 
[11/25 23:11:33 visual_prompt]: 	Training 200/553. train loss: 1.6427,	0.8131 s / batch. (data: 3.32e-04). ETA=11:34:12, max mem: 20.9 GB 
[11/25 23:13:18 visual_prompt]: 	Training 300/553. train loss: 0.8773,	0.8169 s / batch. (data: 3.34e-04). ETA=11:36:08, max mem: 20.9 GB 
[11/25 23:15:02 visual_prompt]: 	Training 400/553. train loss: 0.7498,	1.0440 s / batch. (data: 2.19e-01). ETA=14:47:55, max mem: 20.9 GB 
[11/25 23:16:47 visual_prompt]: 	Training 500/553. train loss: 1.2193,	1.6344 s / batch. (data: 8.25e-01). ETA=23:07:18, max mem: 20.9 GB 
[11/25 23:17:41 visual_prompt]: Epoch 8 / 100: avg data time: 2.16e-01, avg batch time: 1.0429, average train loss: 0.9269
[11/25 23:18:41 visual_prompt]: Inference (val):avg data time: 3.94e-04, avg batch time: 0.3094, average loss: 1.3096
[11/25 23:18:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.43	
[11/25 23:18:41 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/25 23:20:29 visual_prompt]: 	Training 100/553. train loss: 0.3751,	0.8316 s / batch. (data: 1.06e-02). ETA=11:43:43, max mem: 20.9 GB 
[11/25 23:22:12 visual_prompt]: 	Training 200/553. train loss: 0.5990,	0.8559 s / batch. (data: 2.40e-02). ETA=12:02:55, max mem: 20.9 GB 
[11/25 23:23:56 visual_prompt]: 	Training 300/553. train loss: 0.7508,	1.9070 s / batch. (data: 1.09e+00). ETA=1 day, 2:47:28, max mem: 20.9 GB 
[11/25 23:25:40 visual_prompt]: 	Training 400/553. train loss: 0.5719,	0.8225 s / batch. (data: 1.06e-02). ETA=11:31:56, max mem: 20.9 GB 
[11/25 23:27:21 visual_prompt]: 	Training 500/553. train loss: 0.6019,	1.0043 s / batch. (data: 1.79e-01). ETA=14:03:11, max mem: 20.9 GB 
[11/25 23:28:12 visual_prompt]: Epoch 9 / 100: avg data time: 2.05e-01, avg batch time: 1.0327, average train loss: 0.8506
[11/25 23:29:11 visual_prompt]: Inference (val):avg data time: 4.02e-04, avg batch time: 0.3068, average loss: 0.9252
[11/25 23:29:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.27	
[11/25 23:29:11 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/25 23:30:58 visual_prompt]: 	Training 100/553. train loss: 1.1945,	0.8240 s / batch. (data: 3.39e-04). ETA=11:29:43, max mem: 20.9 GB 
[11/25 23:32:38 visual_prompt]: 	Training 200/553. train loss: 0.5637,	0.8520 s / batch. (data: 3.74e-04). ETA=11:51:44, max mem: 20.9 GB 
[11/25 23:34:19 visual_prompt]: 	Training 300/553. train loss: 0.6080,	1.0112 s / batch. (data: 1.87e-01). ETA=14:03:02, max mem: 20.9 GB 
[11/25 23:35:59 visual_prompt]: 	Training 400/553. train loss: 0.7187,	0.8278 s / batch. (data: 3.09e-04). ETA=11:28:47, max mem: 20.9 GB 
[11/25 23:37:42 visual_prompt]: 	Training 500/553. train loss: 1.3974,	1.1520 s / batch. (data: 3.08e-01). ETA=15:56:36, max mem: 20.9 GB 
[11/25 23:38:35 visual_prompt]: Epoch 10 / 100: avg data time: 1.92e-01, avg batch time: 1.0192, average train loss: 0.9451
[11/25 23:39:33 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3083, average loss: 0.6622
[11/25 23:39:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.60	
[11/25 23:39:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/25 23:41:21 visual_prompt]: 	Training 100/553. train loss: 1.1209,	0.8263 s / batch. (data: 1.05e-02). ETA=11:24:03, max mem: 20.9 GB 
[11/25 23:43:05 visual_prompt]: 	Training 200/553. train loss: 1.5567,	0.8334 s / batch. (data: 1.56e-02). ETA=11:28:31, max mem: 20.9 GB 
[11/25 23:44:47 visual_prompt]: 	Training 300/553. train loss: 0.1204,	2.2520 s / batch. (data: 1.42e+00). ETA=1 day, 6:56:46, max mem: 20.9 GB 
[11/25 23:46:26 visual_prompt]: 	Training 400/553. train loss: 0.6646,	0.8385 s / batch. (data: 1.07e-02). ETA=11:29:54, max mem: 20.9 GB 
[11/25 23:48:06 visual_prompt]: 	Training 500/553. train loss: 0.7281,	0.8602 s / batch. (data: 2.44e-02). ETA=11:46:23, max mem: 20.9 GB 
[11/25 23:48:58 visual_prompt]: Epoch 11 / 100: avg data time: 1.96e-01, avg batch time: 1.0218, average train loss: 0.9384
[11/25 23:49:56 visual_prompt]: Inference (val):avg data time: 6.05e-04, avg batch time: 0.3089, average loss: 0.6852
[11/25 23:49:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 65.33	
[11/25 23:49:56 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/25 23:51:41 visual_prompt]: 	Training 100/553. train loss: 0.7452,	0.8109 s / batch. (data: 3.18e-04). ETA=11:03:51, max mem: 20.9 GB 
[11/25 23:53:22 visual_prompt]: 	Training 200/553. train loss: 0.7320,	0.8480 s / batch. (data: 7.95e-03). ETA=11:32:46, max mem: 20.9 GB 
[11/25 23:55:02 visual_prompt]: 	Training 300/553. train loss: 0.5731,	0.8342 s / batch. (data: 3.07e-04). ETA=11:20:08, max mem: 20.9 GB 
[11/25 23:56:45 visual_prompt]: 	Training 400/553. train loss: 0.6994,	0.8347 s / batch. (data: 7.97e-03). ETA=11:19:07, max mem: 20.9 GB 
[11/25 23:58:30 visual_prompt]: 	Training 500/553. train loss: 2.1286,	0.8344 s / batch. (data: 3.31e-04). ETA=11:17:27, max mem: 20.9 GB 
[11/25 23:59:24 visual_prompt]: Epoch 12 / 100: avg data time: 2.01e-01, avg batch time: 1.0260, average train loss: 0.8608
[11/26 00:00:23 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3079, average loss: 1.9649
[11/26 00:00:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.59	
[11/26 00:00:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/26 00:02:13 visual_prompt]: 	Training 100/553. train loss: 0.7960,	1.2999 s / batch. (data: 4.75e-01). ETA=17:32:07, max mem: 20.9 GB 
[11/26 00:03:53 visual_prompt]: 	Training 200/553. train loss: 0.6433,	0.8314 s / batch. (data: 3.29e-04). ETA=11:11:32, max mem: 20.9 GB 
[11/26 00:05:38 visual_prompt]: 	Training 300/553. train loss: 0.6373,	1.8271 s / batch. (data: 1.01e+00). ETA=1 day, 0:32:47, max mem: 20.9 GB 
[11/26 00:07:21 visual_prompt]: 	Training 400/553. train loss: 2.1056,	0.8431 s / batch. (data: 3.40e-04). ETA=11:18:13, max mem: 20.9 GB 
[11/26 00:09:06 visual_prompt]: 	Training 500/553. train loss: 0.8410,	0.8480 s / batch. (data: 7.95e-03). ETA=11:20:42, max mem: 20.9 GB 
[11/26 00:10:00 visual_prompt]: Epoch 13 / 100: avg data time: 2.17e-01, avg batch time: 1.0429, average train loss: 0.8787
[11/26 00:11:00 visual_prompt]: Inference (val):avg data time: 1.63e-04, avg batch time: 0.3090, average loss: 0.9326
[11/26 00:11:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.42	
[11/26 00:11:00 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/26 00:12:48 visual_prompt]: 	Training 100/553. train loss: 0.5916,	0.8431 s / batch. (data: 3.05e-04). ETA=11:14:37, max mem: 20.9 GB 
[11/26 00:14:32 visual_prompt]: 	Training 200/553. train loss: 0.5386,	1.2797 s / batch. (data: 4.49e-01). ETA=17:01:49, max mem: 20.9 GB 
[11/26 00:16:15 visual_prompt]: 	Training 300/553. train loss: 0.8136,	0.8920 s / batch. (data: 6.40e-02). ETA=11:50:48, max mem: 20.9 GB 
[11/26 00:17:58 visual_prompt]: 	Training 400/553. train loss: 0.9694,	0.8440 s / batch. (data: 7.95e-03). ETA=11:11:07, max mem: 20.9 GB 
[11/26 00:19:42 visual_prompt]: 	Training 500/553. train loss: 1.3713,	0.8160 s / batch. (data: 3.27e-04). ETA=10:47:31, max mem: 20.9 GB 
[11/26 00:20:33 visual_prompt]: Epoch 14 / 100: avg data time: 2.10e-01, avg batch time: 1.0366, average train loss: 0.8128
[11/26 00:21:32 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3078, average loss: 0.6864
[11/26 00:21:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.89	
[11/26 00:21:32 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/26 00:23:20 visual_prompt]: 	Training 100/553. train loss: 0.6960,	0.8204 s / batch. (data: 3.31e-04). ETA=10:48:55, max mem: 20.9 GB 
[11/26 00:25:01 visual_prompt]: 	Training 200/553. train loss: 1.5204,	0.8160 s / batch. (data: 3.36e-04). ETA=10:44:04, max mem: 20.9 GB 
[11/26 00:26:46 visual_prompt]: 	Training 300/553. train loss: 0.5794,	0.8442 s / batch. (data: 2.82e-02). ETA=11:04:56, max mem: 20.9 GB 
[11/26 00:28:27 visual_prompt]: 	Training 400/553. train loss: 0.4191,	1.2306 s / batch. (data: 4.13e-01). ETA=16:07:14, max mem: 20.9 GB 
[11/26 00:30:11 visual_prompt]: 	Training 500/553. train loss: 0.7216,	0.8360 s / batch. (data: 3.30e-04). ETA=10:55:40, max mem: 20.9 GB 
[11/26 00:31:05 visual_prompt]: Epoch 15 / 100: avg data time: 2.09e-01, avg batch time: 1.0355, average train loss: 0.9148
[11/26 00:32:04 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3075, average loss: 1.8960
[11/26 00:32:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.82	
[11/26 00:32:04 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/26 00:33:51 visual_prompt]: 	Training 100/553. train loss: 0.5959,	0.8321 s / batch. (data: 5.45e-03). ETA=10:50:29, max mem: 20.9 GB 
[11/26 00:35:34 visual_prompt]: 	Training 200/553. train loss: 1.1388,	0.8397 s / batch. (data: 7.96e-03). ETA=10:55:03, max mem: 20.9 GB 
[11/26 00:37:17 visual_prompt]: 	Training 300/553. train loss: 0.8300,	0.8332 s / batch. (data: 2.24e-03). ETA=10:48:35, max mem: 20.9 GB 
[11/26 00:39:00 visual_prompt]: 	Training 400/553. train loss: 0.4894,	0.8360 s / batch. (data: 3.48e-04). ETA=10:49:20, max mem: 20.9 GB 
[11/26 00:40:42 visual_prompt]: 	Training 500/553. train loss: 1.0545,	1.0099 s / batch. (data: 1.89e-01). ETA=13:02:43, max mem: 20.9 GB 
[11/26 00:41:36 visual_prompt]: Epoch 16 / 100: avg data time: 2.08e-01, avg batch time: 1.0346, average train loss: 0.8112
[11/26 00:42:36 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3076, average loss: 0.6750
[11/26 00:42:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.97	
[11/26 00:42:36 visual_prompt]: Best epoch 16: best metric: -0.675
[11/26 00:42:36 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/26 00:44:23 visual_prompt]: 	Training 100/553. train loss: 0.3698,	0.8480 s / batch. (data: 3.10e-04). ETA=10:55:06, max mem: 20.9 GB 
[11/26 00:46:07 visual_prompt]: 	Training 200/553. train loss: 1.2701,	0.8112 s / batch. (data: 2.65e-04). ETA=10:25:17, max mem: 20.9 GB 
[11/26 00:47:49 visual_prompt]: 	Training 300/553. train loss: 1.2673,	0.8171 s / batch. (data: 3.09e-04). ETA=10:28:30, max mem: 20.9 GB 
[11/26 00:49:33 visual_prompt]: 	Training 400/553. train loss: 0.6794,	1.5877 s / batch. (data: 7.60e-01). ETA=20:18:35, max mem: 20.9 GB 
[11/26 00:51:14 visual_prompt]: 	Training 500/553. train loss: 0.8860,	1.7353 s / batch. (data: 9.12e-01). ETA=22:08:59, max mem: 20.9 GB 
[11/26 00:52:09 visual_prompt]: Epoch 17 / 100: avg data time: 2.11e-01, avg batch time: 1.0369, average train loss: 0.8059
[11/26 00:53:08 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.3085, average loss: 0.7225
[11/26 00:53:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.79	
[11/26 00:53:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/26 00:54:56 visual_prompt]: 	Training 100/553. train loss: 0.5705,	0.8429 s / batch. (data: 8.29e-04). ETA=10:43:22, max mem: 20.9 GB 
[11/26 00:56:42 visual_prompt]: 	Training 200/553. train loss: 0.6109,	0.8452 s / batch. (data: 8.18e-04). ETA=10:43:46, max mem: 20.9 GB 
[11/26 00:58:25 visual_prompt]: 	Training 300/553. train loss: 0.3658,	0.8221 s / batch. (data: 3.11e-04). ETA=10:24:48, max mem: 20.9 GB 
[11/26 01:00:07 visual_prompt]: 	Training 400/553. train loss: 0.6038,	0.8520 s / batch. (data: 6.38e-03). ETA=10:46:05, max mem: 20.9 GB 
[11/26 01:01:49 visual_prompt]: 	Training 500/553. train loss: 0.8911,	1.1565 s / batch. (data: 3.32e-01). ETA=14:35:02, max mem: 20.9 GB 
[11/26 01:02:40 visual_prompt]: Epoch 18 / 100: avg data time: 2.08e-01, avg batch time: 1.0339, average train loss: 0.8631
[11/26 01:03:39 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3067, average loss: 0.6621
[11/26 01:03:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.40	
[11/26 01:03:39 visual_prompt]: Best epoch 18: best metric: -0.662
[11/26 01:03:39 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/26 01:05:27 visual_prompt]: 	Training 100/553. train loss: 0.6142,	1.2760 s / batch. (data: 4.52e-01). ETA=16:02:12, max mem: 20.9 GB 
[11/26 01:07:23 visual_prompt]: 	Training 200/553. train loss: 0.4699,	0.8120 s / batch. (data: 3.22e-04). ETA=10:10:58, max mem: 20.9 GB 
[11/26 01:09:08 visual_prompt]: 	Training 300/553. train loss: 1.2461,	0.8244 s / batch. (data: 3.19e-04). ETA=10:18:56, max mem: 20.9 GB 
[11/26 01:10:51 visual_prompt]: 	Training 400/553. train loss: 0.6849,	0.8271 s / batch. (data: 3.21e-04). ETA=10:19:34, max mem: 20.9 GB 
[11/26 01:12:30 visual_prompt]: 	Training 500/553. train loss: 0.8544,	0.8179 s / batch. (data: 7.93e-03). ETA=10:11:18, max mem: 20.9 GB 
[11/26 01:13:23 visual_prompt]: Epoch 19 / 100: avg data time: 2.32e-01, avg batch time: 1.0567, average train loss: 0.7991
[11/26 01:14:22 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3061, average loss: 1.4556
[11/26 01:14:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.28	
[11/26 01:14:22 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[11/26 01:16:05 visual_prompt]: 	Training 100/553. train loss: 0.6127,	0.8201 s / batch. (data: 3.28e-04). ETA=10:10:50, max mem: 20.9 GB 
[11/26 01:17:49 visual_prompt]: 	Training 200/553. train loss: 0.3464,	0.8364 s / batch. (data: 9.44e-03). ETA=10:21:37, max mem: 20.9 GB 
[11/26 01:19:33 visual_prompt]: 	Training 300/553. train loss: 1.0631,	0.8394 s / batch. (data: 7.86e-03). ETA=10:22:29, max mem: 20.9 GB 
[11/26 01:21:16 visual_prompt]: 	Training 400/553. train loss: 0.6841,	0.8160 s / batch. (data: 3.47e-04). ETA=10:03:45, max mem: 20.9 GB 
[11/26 01:23:01 visual_prompt]: 	Training 500/553. train loss: 2.1441,	0.8487 s / batch. (data: 3.12e-04). ETA=10:26:31, max mem: 20.9 GB 
[11/26 01:23:56 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e-01, avg batch time: 1.0385, average train loss: 0.8370
[11/26 01:24:56 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3079, average loss: 0.8575
[11/26 01:24:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 67.19	
[11/26 01:24:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[11/26 01:26:44 visual_prompt]: 	Training 100/553. train loss: 0.4125,	0.8310 s / batch. (data: 3.34e-04). ETA=10:11:22, max mem: 20.9 GB 
[11/26 01:28:24 visual_prompt]: 	Training 200/553. train loss: 0.3525,	0.8179 s / batch. (data: 5.42e-03). ETA=10:00:20, max mem: 20.9 GB 
[11/26 01:30:04 visual_prompt]: 	Training 300/553. train loss: 0.7622,	0.8352 s / batch. (data: 3.21e-04). ETA=10:11:38, max mem: 20.9 GB 
[11/26 01:31:46 visual_prompt]: 	Training 400/553. train loss: 1.0077,	0.8360 s / batch. (data: 2.99e-04). ETA=10:10:49, max mem: 20.9 GB 
[11/26 01:33:29 visual_prompt]: 	Training 500/553. train loss: 0.6925,	0.8254 s / batch. (data: 3.07e-04). ETA=10:01:42, max mem: 20.9 GB 
[11/26 01:34:21 visual_prompt]: Epoch 21 / 100: avg data time: 1.95e-01, avg batch time: 1.0210, average train loss: 0.8104
[11/26 01:35:19 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3076, average loss: 0.6641
[11/26 01:35:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 65.49	
[11/26 01:35:19 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[11/26 01:37:03 visual_prompt]: 	Training 100/553. train loss: 0.6656,	0.8632 s / batch. (data: 1.11e-02). ETA=10:27:04, max mem: 20.9 GB 
[11/26 01:38:47 visual_prompt]: 	Training 200/553. train loss: 0.7585,	0.8249 s / batch. (data: 7.96e-03). ETA=9:57:53, max mem: 20.9 GB 
[11/26 01:40:29 visual_prompt]: 	Training 300/553. train loss: 0.4793,	0.8251 s / batch. (data: 3.53e-04). ETA=9:56:37, max mem: 20.9 GB 
[11/26 01:42:14 visual_prompt]: 	Training 400/553. train loss: 0.9132,	0.8714 s / batch. (data: 2.34e-02). ETA=10:28:40, max mem: 20.9 GB 
[11/26 01:44:00 visual_prompt]: 	Training 500/553. train loss: 0.7591,	0.8299 s / batch. (data: 3.29e-04). ETA=9:57:19, max mem: 20.9 GB 
[11/26 01:44:59 visual_prompt]: Epoch 22 / 100: avg data time: 2.21e-01, avg batch time: 1.0480, average train loss: 0.7916
[11/26 01:46:00 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3088, average loss: 0.7053
[11/26 01:46:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 68.71	
[11/26 01:46:00 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[11/26 01:47:51 visual_prompt]: 	Training 100/553. train loss: 0.9378,	0.8377 s / batch. (data: 2.95e-04). ETA=10:00:48, max mem: 20.9 GB 
[11/26 01:49:35 visual_prompt]: 	Training 200/553. train loss: 0.7419,	0.8317 s / batch. (data: 7.94e-03). ETA=9:55:09, max mem: 20.9 GB 
[11/26 01:51:21 visual_prompt]: 	Training 300/553. train loss: 1.3093,	0.8123 s / batch. (data: 3.38e-04). ETA=9:39:52, max mem: 20.9 GB 
[11/26 01:53:03 visual_prompt]: 	Training 400/553. train loss: 0.5014,	0.8569 s / batch. (data: 8.19e-04). ETA=10:10:18, max mem: 20.9 GB 
[11/26 01:54:46 visual_prompt]: 	Training 500/553. train loss: 0.8242,	0.8240 s / batch. (data: 1.20e-02). ETA=9:45:30, max mem: 20.9 GB 
[11/26 01:55:40 visual_prompt]: Epoch 23 / 100: avg data time: 2.23e-01, avg batch time: 1.0483, average train loss: 0.7951
[11/26 01:56:40 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3093, average loss: 0.6610
[11/26 01:56:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 66.63	
[11/26 01:56:40 visual_prompt]: Best epoch 23: best metric: -0.661
[11/26 01:56:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[11/26 01:58:24 visual_prompt]: 	Training 100/553. train loss: 0.6541,	1.0603 s / batch. (data: 2.41e-01). ETA=12:30:43, max mem: 20.9 GB 
[11/26 02:00:07 visual_prompt]: 	Training 200/553. train loss: 0.7373,	0.8650 s / batch. (data: 3.64e-02). ETA=10:11:00, max mem: 20.9 GB 
[11/26 02:01:51 visual_prompt]: 	Training 300/553. train loss: 0.5749,	1.0678 s / batch. (data: 2.45e-01). ETA=12:32:26, max mem: 20.9 GB 
[11/26 02:03:36 visual_prompt]: 	Training 400/553. train loss: 0.4404,	0.8099 s / batch. (data: 7.10e-04). ETA=9:29:21, max mem: 20.9 GB 
[11/26 02:05:21 visual_prompt]: 	Training 500/553. train loss: 0.7676,	0.8307 s / batch. (data: 3.28e-04). ETA=9:42:37, max mem: 20.9 GB 
[11/26 02:06:15 visual_prompt]: Epoch 24 / 100: avg data time: 2.14e-01, avg batch time: 1.0403, average train loss: 0.8561
[11/26 02:07:14 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3068, average loss: 1.2594
[11/26 02:07:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.59	
[11/26 02:07:14 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[11/26 02:09:05 visual_prompt]: 	Training 100/553. train loss: 0.8078,	0.8087 s / batch. (data: 3.08e-04). ETA=9:25:07, max mem: 20.9 GB 
[11/26 02:10:45 visual_prompt]: 	Training 200/553. train loss: 1.6847,	0.8242 s / batch. (data: 3.14e-04). ETA=9:34:34, max mem: 20.9 GB 
[11/26 02:12:28 visual_prompt]: 	Training 300/553. train loss: 0.6692,	0.8213 s / batch. (data: 1.05e-02). ETA=9:31:09, max mem: 20.9 GB 
[11/26 02:14:11 visual_prompt]: 	Training 400/553. train loss: 1.1286,	1.4036 s / batch. (data: 5.81e-01). ETA=16:13:50, max mem: 20.9 GB 
[11/26 02:15:55 visual_prompt]: 	Training 500/553. train loss: 0.8395,	1.6963 s / batch. (data: 8.72e-01). ETA=19:34:03, max mem: 20.9 GB 
[11/26 02:16:48 visual_prompt]: Epoch 25 / 100: avg data time: 2.11e-01, avg batch time: 1.0377, average train loss: 0.9025
[11/26 02:17:47 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3083, average loss: 1.3245
[11/26 02:17:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.97	
[11/26 02:17:47 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[11/26 02:19:35 visual_prompt]: 	Training 100/553. train loss: 0.3550,	0.8115 s / batch. (data: 3.18e-04). ETA=9:19:34, max mem: 20.9 GB 
[11/26 02:21:20 visual_prompt]: 	Training 200/553. train loss: 1.0625,	1.7720 s / batch. (data: 9.43e-01). ETA=20:18:59, max mem: 20.9 GB 
[11/26 02:23:04 visual_prompt]: 	Training 300/553. train loss: 0.7684,	0.8352 s / batch. (data: 7.78e-04). ETA=9:33:10, max mem: 20.9 GB 
[11/26 02:24:46 visual_prompt]: 	Training 400/553. train loss: 0.9725,	0.8240 s / batch. (data: 3.11e-04). ETA=9:24:07, max mem: 20.9 GB 
[11/26 02:26:27 visual_prompt]: 	Training 500/553. train loss: 0.6735,	0.8360 s / batch. (data: 1.05e-02). ETA=9:30:53, max mem: 20.9 GB 
[11/26 02:27:21 visual_prompt]: Epoch 26 / 100: avg data time: 2.10e-01, avg batch time: 1.0362, average train loss: 0.8532
[11/26 02:28:20 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3095, average loss: 0.7897
[11/26 02:28:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 64.02	
[11/26 02:28:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[11/26 02:30:09 visual_prompt]: 	Training 100/553. train loss: 0.5590,	0.8480 s / batch. (data: 3.24e-04). ETA=9:36:57, max mem: 20.9 GB 
[11/26 02:31:52 visual_prompt]: 	Training 200/553. train loss: 0.6246,	1.4666 s / batch. (data: 6.28e-01). ETA=16:35:23, max mem: 20.9 GB 
[11/26 02:33:35 visual_prompt]: 	Training 300/553. train loss: 0.6184,	0.8442 s / batch. (data: 9.54e-03). ETA=9:31:32, max mem: 20.9 GB 
[11/26 02:35:19 visual_prompt]: 	Training 400/553. train loss: 0.9961,	0.8480 s / batch. (data: 7.94e-04). ETA=9:32:42, max mem: 20.9 GB 
[11/26 02:37:03 visual_prompt]: 	Training 500/553. train loss: 0.6948,	0.8283 s / batch. (data: 7.48e-04). ETA=9:17:59, max mem: 20.9 GB 
[11/26 02:37:55 visual_prompt]: Epoch 27 / 100: avg data time: 2.11e-01, avg batch time: 1.0384, average train loss: 0.7795
[11/26 02:38:54 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3088, average loss: 0.6464
[11/26 02:38:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 70.64	
[11/26 02:38:54 visual_prompt]: Best epoch 27: best metric: -0.646
[11/26 02:38:54 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[11/26 02:40:40 visual_prompt]: 	Training 100/553. train loss: 0.4128,	1.3454 s / batch. (data: 5.09e-01). ETA=15:02:56, max mem: 20.9 GB 
[11/26 02:42:23 visual_prompt]: 	Training 200/553. train loss: 0.3812,	0.8282 s / batch. (data: 1.20e-02). ETA=9:14:28, max mem: 20.9 GB 
[11/26 02:44:08 visual_prompt]: 	Training 300/553. train loss: 0.9904,	1.7446 s / batch. (data: 9.21e-01). ETA=19:25:03, max mem: 20.9 GB 
[11/26 02:45:50 visual_prompt]: 	Training 400/553. train loss: 0.5543,	0.8400 s / batch. (data: 3.11e-04). ETA=9:19:32, max mem: 20.9 GB 
[11/26 02:47:31 visual_prompt]: 	Training 500/553. train loss: 0.4060,	0.8640 s / batch. (data: 3.14e-04). ETA=9:34:06, max mem: 20.9 GB 
[11/26 02:48:27 visual_prompt]: Epoch 28 / 100: avg data time: 2.10e-01, avg batch time: 1.0351, average train loss: 0.7581
[11/26 02:49:26 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3083, average loss: 0.6225
[11/26 02:49:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.70	
[11/26 02:49:26 visual_prompt]: Best epoch 28: best metric: -0.622
[11/26 02:49:26 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[11/26 02:51:20 visual_prompt]: 	Training 100/553. train loss: 0.1783,	0.8609 s / batch. (data: 5.45e-03). ETA=9:29:51, max mem: 20.9 GB 
[11/26 02:53:02 visual_prompt]: 	Training 200/553. train loss: 1.4014,	2.0760 s / batch. (data: 1.26e+00). ETA=22:50:42, max mem: 20.9 GB 
[11/26 02:54:43 visual_prompt]: 	Training 300/553. train loss: 0.7988,	0.8221 s / batch. (data: 5.44e-04). ETA=9:01:25, max mem: 20.9 GB 
[11/26 02:56:24 visual_prompt]: 	Training 400/553. train loss: 0.6739,	1.5480 s / batch. (data: 7.16e-01). ETA=16:56:56, max mem: 20.9 GB 
[11/26 02:58:07 visual_prompt]: 	Training 500/553. train loss: 0.6385,	0.8472 s / batch. (data: 1.11e-02). ETA=9:15:07, max mem: 20.9 GB 
[11/26 02:59:00 visual_prompt]: Epoch 29 / 100: avg data time: 2.12e-01, avg batch time: 1.0382, average train loss: 0.8325
[11/26 02:59:59 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3086, average loss: 0.6432
[11/26 02:59:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 71.15	
[11/26 02:59:59 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[11/26 03:01:45 visual_prompt]: 	Training 100/553. train loss: 1.3914,	0.8266 s / batch. (data: 3.03e-04). ETA=8:59:30, max mem: 20.9 GB 
[11/26 03:03:29 visual_prompt]: 	Training 200/553. train loss: 0.8902,	0.8360 s / batch. (data: 5.48e-03). ETA=9:04:16, max mem: 20.9 GB 
[11/26 03:05:12 visual_prompt]: 	Training 300/553. train loss: 0.3225,	2.4680 s / batch. (data: 1.62e+00). ETA=1 day, 2:42:40, max mem: 20.9 GB 
[11/26 03:06:56 visual_prompt]: 	Training 400/553. train loss: 1.4520,	1.2360 s / batch. (data: 4.06e-01). ETA=13:20:34, max mem: 20.9 GB 
[11/26 03:08:38 visual_prompt]: 	Training 500/553. train loss: 0.4983,	1.6359 s / batch. (data: 8.11e-01). ETA=17:36:51, max mem: 20.9 GB 
[11/26 03:09:33 visual_prompt]: Epoch 30 / 100: avg data time: 2.12e-01, avg batch time: 1.0380, average train loss: 0.7754
[11/26 03:10:33 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3068, average loss: 0.7292
[11/26 03:10:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 72.53	
[11/26 03:10:33 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[11/26 03:12:22 visual_prompt]: 	Training 100/553. train loss: 0.6565,	0.8199 s / batch. (data: 1.20e-02). ETA=8:47:35, max mem: 20.9 GB 
[11/26 03:14:07 visual_prompt]: 	Training 200/553. train loss: 1.0729,	0.8213 s / batch. (data: 7.87e-03). ETA=8:47:09, max mem: 20.9 GB 
[11/26 03:15:47 visual_prompt]: 	Training 300/553. train loss: 0.7365,	0.8200 s / batch. (data: 3.01e-04). ETA=8:44:55, max mem: 20.9 GB 
[11/26 03:17:30 visual_prompt]: 	Training 400/553. train loss: 0.5884,	1.0999 s / batch. (data: 2.65e-01). ETA=11:42:17, max mem: 20.9 GB 
[11/26 03:19:13 visual_prompt]: 	Training 500/553. train loss: 0.7347,	0.8313 s / batch. (data: 3.13e-04). ETA=8:49:22, max mem: 20.9 GB 
[11/26 03:20:06 visual_prompt]: Epoch 31 / 100: avg data time: 2.11e-01, avg batch time: 1.0369, average train loss: 0.7403
[11/26 03:21:06 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3076, average loss: 0.6442
[11/26 03:21:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 68.68	
[11/26 03:21:06 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[11/26 03:22:54 visual_prompt]: 	Training 100/553. train loss: 0.6650,	0.8198 s / batch. (data: 1.05e-02). ETA=8:40:00, max mem: 20.9 GB 
[11/26 03:24:37 visual_prompt]: 	Training 200/553. train loss: 0.4946,	0.8400 s / batch. (data: 5.44e-03). ETA=8:51:24, max mem: 20.9 GB 
[11/26 03:26:24 visual_prompt]: 	Training 300/553. train loss: 0.9610,	0.8096 s / batch. (data: 3.28e-04). ETA=8:30:47, max mem: 20.9 GB 
[11/26 03:28:07 visual_prompt]: 	Training 400/553. train loss: 0.8494,	0.8280 s / batch. (data: 3.01e-04). ETA=8:41:04, max mem: 20.9 GB 
[11/26 03:29:47 visual_prompt]: 	Training 500/553. train loss: 0.9137,	0.8194 s / batch. (data: 3.12e-04). ETA=8:34:15, max mem: 20.9 GB 
[11/26 03:30:39 visual_prompt]: Epoch 32 / 100: avg data time: 2.11e-01, avg batch time: 1.0364, average train loss: 0.7183
[11/26 03:31:38 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3077, average loss: 0.6411
[11/26 03:31:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 70.08	
[11/26 03:31:38 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[11/26 03:33:24 visual_prompt]: 	Training 100/553. train loss: 1.2168,	1.1202 s / batch. (data: 3.12e-01). ETA=11:40:10, max mem: 20.9 GB 
[11/26 03:35:09 visual_prompt]: 	Training 200/553. train loss: 0.7286,	1.5120 s / batch. (data: 6.96e-01). ETA=15:42:34, max mem: 20.9 GB 
[11/26 03:36:51 visual_prompt]: 	Training 300/553. train loss: 0.5560,	0.8614 s / batch. (data: 2.13e-02). ETA=8:55:33, max mem: 20.9 GB 
[11/26 03:38:36 visual_prompt]: 	Training 400/553. train loss: 0.5274,	0.8360 s / batch. (data: 7.97e-03). ETA=8:38:22, max mem: 20.9 GB 
[11/26 03:40:17 visual_prompt]: 	Training 500/553. train loss: 0.6054,	0.8285 s / batch. (data: 1.19e-02). ETA=8:32:21, max mem: 20.9 GB 
[11/26 03:41:11 visual_prompt]: Epoch 33 / 100: avg data time: 2.10e-01, avg batch time: 1.0362, average train loss: 0.7732
[11/26 03:42:11 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3093, average loss: 0.9484
[11/26 03:42:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 69.59	
[11/26 03:42:11 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[11/26 03:44:00 visual_prompt]: 	Training 100/553. train loss: 0.7525,	0.9598 s / batch. (data: 1.38e-01). ETA=9:51:06, max mem: 20.9 GB 
[11/26 03:45:41 visual_prompt]: 	Training 200/553. train loss: 0.9737,	0.8454 s / batch. (data: 3.32e-04). ETA=8:39:12, max mem: 20.9 GB 
[11/26 03:47:24 visual_prompt]: 	Training 300/553. train loss: 0.4960,	0.8431 s / batch. (data: 3.41e-04). ETA=8:36:23, max mem: 20.9 GB 
[11/26 03:49:08 visual_prompt]: 	Training 400/553. train loss: 0.9404,	0.8148 s / batch. (data: 5.43e-03). ETA=8:17:43, max mem: 20.9 GB 
[11/26 03:50:52 visual_prompt]: 	Training 500/553. train loss: 0.5754,	1.4361 s / batch. (data: 6.12e-01). ETA=14:34:51, max mem: 20.9 GB 
[11/26 03:51:45 visual_prompt]: Epoch 34 / 100: avg data time: 2.10e-01, avg batch time: 1.0373, average train loss: 0.7610
[11/26 03:52:44 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3080, average loss: 0.6441
[11/26 03:52:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 71.15	
[11/26 03:52:44 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[11/26 03:54:32 visual_prompt]: 	Training 100/553. train loss: 1.4151,	0.8523 s / batch. (data: 2.03e-02). ETA=8:37:03, max mem: 20.9 GB 
[11/26 03:56:17 visual_prompt]: 	Training 200/553. train loss: 1.2026,	0.8321 s / batch. (data: 3.26e-04). ETA=8:23:24, max mem: 20.9 GB 
[11/26 03:57:58 visual_prompt]: 	Training 300/553. train loss: 0.5898,	0.8208 s / batch. (data: 7.98e-03). ETA=8:15:11, max mem: 20.9 GB 
[11/26 03:59:40 visual_prompt]: 	Training 400/553. train loss: 0.3023,	0.8511 s / batch. (data: 3.21e-02). ETA=8:32:01, max mem: 20.9 GB 
[11/26 04:01:22 visual_prompt]: 	Training 500/553. train loss: 0.7569,	1.2649 s / batch. (data: 4.55e-01). ETA=12:38:52, max mem: 20.9 GB 
[11/26 04:02:15 visual_prompt]: Epoch 35 / 100: avg data time: 2.06e-01, avg batch time: 1.0323, average train loss: 0.7742
[11/26 04:03:13 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3069, average loss: 0.6447
[11/26 04:03:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.83	
[11/26 04:03:13 visual_prompt]: Stopping early.
[11/26 04:03:14 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 04:03:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 04:03:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/26 04:03:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 04:03:14 visual_prompt]: Training with config:
[11/26 04:03:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.25_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/26 04:03:14 visual_prompt]: Loading training data...
[11/26 04:03:14 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 04:03:14 visual_prompt]: Loading validation data...
[11/26 04:03:14 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 04:03:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 04:03:22 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 04:03:22 visual_prompt]: tuned percent:0.525
[11/26 04:03:22 visual_prompt]: Device used for model: 0
[11/26 04:03:22 visual_prompt]: Setting up Evaluator...
[11/26 04:03:22 visual_prompt]: Setting up Trainer...
[11/26 04:03:22 visual_prompt]: 	Setting up the optimizer...
[11/26 04:03:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 04:05:08 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8356 s / batch. (data: 1.05e-02). ETA=12:48:47, max mem: 20.9 GB 
[11/26 04:06:48 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8330 s / batch. (data: 7.91e-03). ETA=12:44:58, max mem: 20.9 GB 
[11/26 04:08:32 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3865 s / batch. (data: 5.62e-01). ETA=21:10:59, max mem: 20.9 GB 
[11/26 04:10:12 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8398 s / batch. (data: 5.44e-03). ETA=12:48:23, max mem: 20.9 GB 
[11/26 04:11:56 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8316 s / batch. (data: 3.16e-04). ETA=12:39:33, max mem: 20.9 GB 
[11/26 04:12:50 visual_prompt]: Epoch 1 / 100: avg data time: 2.00e-01, avg batch time: 1.0265, average train loss: 1.5403
[11/26 04:13:48 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3088, average loss: 1.5201
[11/26 04:13:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 04:13:48 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[11/26 04:15:34 visual_prompt]: 	Training 100/553. train loss: 0.7436,	0.8200 s / batch. (data: 3.05e-04). ETA=12:26:51, max mem: 20.9 GB 
[11/26 04:17:15 visual_prompt]: 	Training 200/553. train loss: 0.2132,	0.8383 s / batch. (data: 1.05e-02). ETA=12:42:05, max mem: 20.9 GB 
[11/26 04:18:59 visual_prompt]: 	Training 300/553. train loss: 0.9111,	1.1174 s / batch. (data: 3.07e-01). ETA=16:54:01, max mem: 20.9 GB 
[11/26 04:20:39 visual_prompt]: 	Training 400/553. train loss: 1.1021,	0.8200 s / batch. (data: 3.10e-04). ETA=12:22:44, max mem: 20.9 GB 
[11/26 04:22:23 visual_prompt]: 	Training 500/553. train loss: 0.6526,	0.8120 s / batch. (data: 3.02e-04). ETA=12:14:09, max mem: 20.9 GB 
[11/26 04:23:15 visual_prompt]: Epoch 2 / 100: avg data time: 1.98e-01, avg batch time: 1.0242, average train loss: 0.8373
[11/26 04:24:13 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3079, average loss: 0.7624
[11/26 04:24:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.87	
[11/26 04:24:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[11/26 04:25:58 visual_prompt]: 	Training 100/553. train loss: 0.7625,	0.8090 s / batch. (data: 3.51e-04). ETA=12:09:23, max mem: 20.9 GB 
[11/26 04:27:41 visual_prompt]: 	Training 200/553. train loss: 0.7006,	0.8478 s / batch. (data: 1.05e-02). ETA=12:42:53, max mem: 20.9 GB 
[11/26 04:29:21 visual_prompt]: 	Training 300/553. train loss: 0.6354,	0.8102 s / batch. (data: 3.12e-04). ETA=12:07:43, max mem: 20.9 GB 
[11/26 04:31:03 visual_prompt]: 	Training 400/553. train loss: 0.7167,	0.8262 s / batch. (data: 5.43e-03). ETA=12:20:44, max mem: 20.9 GB 
[11/26 04:32:46 visual_prompt]: 	Training 500/553. train loss: 0.7742,	1.3680 s / batch. (data: 5.49e-01). ETA=20:24:12, max mem: 20.9 GB 
[11/26 04:33:37 visual_prompt]: Epoch 3 / 100: avg data time: 1.95e-01, avg batch time: 1.0200, average train loss: 0.7757
[11/26 04:34:36 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3096, average loss: 0.7058
[11/26 04:34:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 56.84	
[11/26 04:34:36 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[11/26 04:36:24 visual_prompt]: 	Training 100/553. train loss: 0.7458,	0.8312 s / batch. (data: 1.05e-02). ETA=12:21:41, max mem: 20.9 GB 
[11/26 04:38:06 visual_prompt]: 	Training 200/553. train loss: 1.4286,	0.8320 s / batch. (data: 3.19e-04). ETA=12:21:02, max mem: 20.9 GB 
[11/26 04:39:48 visual_prompt]: 	Training 300/553. train loss: 0.6624,	1.2078 s / batch. (data: 3.86e-01). ETA=17:53:43, max mem: 20.9 GB 
[11/26 04:41:26 visual_prompt]: 	Training 400/553. train loss: 0.5764,	1.3000 s / batch. (data: 4.67e-01). ETA=19:13:32, max mem: 20.9 GB 
[11/26 04:43:11 visual_prompt]: 	Training 500/553. train loss: 0.6758,	3.3999 s / batch. (data: 2.59e+00). ETA=2 days, 2:11:16, max mem: 20.9 GB 
[11/26 04:44:04 visual_prompt]: Epoch 4 / 100: avg data time: 2.01e-01, avg batch time: 1.0268, average train loss: 0.8704
[11/26 04:45:02 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.3086, average loss: 0.8946
[11/26 04:45:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.39	
[11/26 04:45:02 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[11/26 04:46:47 visual_prompt]: 	Training 100/553. train loss: 0.5171,	0.8276 s / batch. (data: 1.20e-02). ETA=12:10:53, max mem: 20.9 GB 
[11/26 04:48:29 visual_prompt]: 	Training 200/553. train loss: 0.6178,	1.4447 s / batch. (data: 6.33e-01). ETA=21:13:28, max mem: 20.9 GB 
[11/26 04:50:11 visual_prompt]: 	Training 300/553. train loss: 1.6098,	0.8275 s / batch. (data: 7.95e-03). ETA=12:08:00, max mem: 20.9 GB 
[11/26 04:51:53 visual_prompt]: 	Training 400/553. train loss: 1.0288,	0.8320 s / batch. (data: 3.17e-04). ETA=12:10:37, max mem: 20.9 GB 
[11/26 04:53:34 visual_prompt]: 	Training 500/553. train loss: 0.5677,	0.8357 s / batch. (data: 3.04e-04). ETA=12:12:27, max mem: 20.9 GB 
[11/26 04:54:28 visual_prompt]: Epoch 5 / 100: avg data time: 1.97e-01, avg batch time: 1.0227, average train loss: 0.9702
[11/26 04:55:27 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3090, average loss: 0.8285
[11/26 04:55:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.83	
[11/26 04:55:27 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[11/26 04:57:14 visual_prompt]: 	Training 100/553. train loss: 0.5388,	0.8445 s / batch. (data: 8.25e-04). ETA=12:17:58, max mem: 20.9 GB 
[11/26 04:58:56 visual_prompt]: 	Training 200/553. train loss: 0.7963,	0.8188 s / batch. (data: 2.92e-04). ETA=11:54:11, max mem: 20.9 GB 
[11/26 05:00:36 visual_prompt]: 	Training 300/553. train loss: 0.6023,	0.8303 s / batch. (data: 3.23e-04). ETA=12:02:50, max mem: 20.9 GB 
[11/26 05:02:22 visual_prompt]: 	Training 400/553. train loss: 0.6314,	0.8400 s / batch. (data: 3.63e-04). ETA=12:09:51, max mem: 20.9 GB 
[11/26 05:04:02 visual_prompt]: 	Training 500/553. train loss: 0.8519,	0.8431 s / batch. (data: 5.46e-03). ETA=12:11:13, max mem: 20.9 GB 
[11/26 05:04:54 visual_prompt]: Epoch 6 / 100: avg data time: 2.00e-01, avg batch time: 1.0256, average train loss: 0.8433
[11/26 05:05:53 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3076, average loss: 0.7642
[11/26 05:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.24	
[11/26 05:05:53 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[11/26 05:07:37 visual_prompt]: 	Training 100/553. train loss: 0.8887,	0.8278 s / batch. (data: 5.45e-03). ETA=11:55:49, max mem: 20.9 GB 
[11/26 05:09:19 visual_prompt]: 	Training 200/553. train loss: 0.5279,	0.8199 s / batch. (data: 5.58e-04). ETA=11:47:38, max mem: 20.9 GB 
[11/26 05:11:04 visual_prompt]: 	Training 300/553. train loss: 0.6200,	2.0522 s / batch. (data: 1.21e+00). ETA=1 day, 5:27:40, max mem: 20.9 GB 
[11/26 05:12:46 visual_prompt]: 	Training 400/553. train loss: 0.6064,	2.0111 s / batch. (data: 1.18e+00). ETA=1 day, 4:48:57, max mem: 20.9 GB 
[11/26 05:14:26 visual_prompt]: 	Training 500/553. train loss: 1.4629,	0.8200 s / batch. (data: 3.19e-04). ETA=11:43:33, max mem: 20.9 GB 
[11/26 05:15:19 visual_prompt]: Epoch 7 / 100: avg data time: 1.98e-01, avg batch time: 1.0240, average train loss: 0.8611
[11/26 05:16:18 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3106, average loss: 0.7455
[11/26 05:16:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.40	
[11/26 05:16:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[11/26 05:18:02 visual_prompt]: 	Training 100/553. train loss: 0.7446,	0.8181 s / batch. (data: 5.45e-03). ETA=11:39:50, max mem: 20.9 GB 
[11/26 05:19:45 visual_prompt]: 	Training 200/553. train loss: 1.5044,	0.8255 s / batch. (data: 3.32e-04). ETA=11:44:51, max mem: 20.9 GB 
[11/26 05:21:28 visual_prompt]: 	Training 300/553. train loss: 0.9343,	0.8117 s / batch. (data: 3.02e-04). ETA=11:31:41, max mem: 20.9 GB 
[11/26 05:23:10 visual_prompt]: 	Training 400/553. train loss: 0.7478,	0.9671 s / batch. (data: 1.37e-01). ETA=13:42:30, max mem: 20.9 GB 
[11/26 05:24:52 visual_prompt]: 	Training 500/553. train loss: 1.2109,	1.5538 s / batch. (data: 7.35e-01). ETA=21:58:54, max mem: 20.9 GB 
[11/26 05:25:45 visual_prompt]: Epoch 8 / 100: avg data time: 2.00e-01, avg batch time: 1.0259, average train loss: 0.9507
[11/26 05:26:44 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3071, average loss: 1.4041
[11/26 05:26:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.19	
[11/26 05:26:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[11/26 05:28:29 visual_prompt]: 	Training 100/553. train loss: 0.2778,	0.8184 s / batch. (data: 3.02e-04). ETA=11:32:32, max mem: 20.9 GB 
[11/26 05:30:10 visual_prompt]: 	Training 200/553. train loss: 0.6187,	0.8400 s / batch. (data: 2.96e-04). ETA=11:49:27, max mem: 20.9 GB 
[11/26 05:31:52 visual_prompt]: 	Training 300/553. train loss: 0.6669,	1.7640 s / batch. (data: 9.39e-01). ETA=1 day, 0:46:56, max mem: 20.9 GB 
[11/26 05:33:35 visual_prompt]: 	Training 400/553. train loss: 0.5471,	0.8173 s / batch. (data: 7.96e-03). ETA=11:27:36, max mem: 20.9 GB 
[11/26 05:35:18 visual_prompt]: 	Training 500/553. train loss: 0.7474,	1.0233 s / batch. (data: 2.02e-01). ETA=14:19:10, max mem: 20.9 GB 
[11/26 05:36:10 visual_prompt]: Epoch 9 / 100: avg data time: 1.98e-01, avg batch time: 1.0232, average train loss: 0.8651
[11/26 05:37:08 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3100, average loss: 1.0266
[11/26 05:37:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.45	
[11/26 05:37:08 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[11/26 05:38:56 visual_prompt]: 	Training 100/553. train loss: 1.5081,	0.8194 s / batch. (data: 9.00e-03). ETA=11:25:54, max mem: 20.9 GB 
[11/26 05:40:37 visual_prompt]: 	Training 200/553. train loss: 0.5658,	0.8320 s / batch. (data: 3.22e-04). ETA=11:35:02, max mem: 20.9 GB 
[11/26 05:42:18 visual_prompt]: 	Training 300/553. train loss: 0.6614,	0.8205 s / batch. (data: 1.05e-02). ETA=11:24:06, max mem: 20.9 GB 
[11/26 05:43:57 visual_prompt]: 	Training 400/553. train loss: 0.6856,	0.8509 s / batch. (data: 5.45e-03). ETA=11:47:59, max mem: 20.9 GB 
[11/26 05:45:41 visual_prompt]: 	Training 500/553. train loss: 0.7624,	0.9734 s / batch. (data: 1.39e-01). ETA=13:28:18, max mem: 20.9 GB 
[11/26 05:46:34 visual_prompt]: Epoch 10 / 100: avg data time: 1.97e-01, avg batch time: 1.0234, average train loss: 0.9914
[11/26 05:47:33 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3079, average loss: 0.6945
[11/26 05:47:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.50	
[11/26 05:47:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[11/26 05:49:21 visual_prompt]: 	Training 100/553. train loss: 1.4991,	0.8360 s / batch. (data: 3.06e-04). ETA=11:32:03, max mem: 20.9 GB 
[11/26 05:51:05 visual_prompt]: 	Training 200/553. train loss: 2.1116,	0.8283 s / batch. (data: 3.23e-04). ETA=11:24:16, max mem: 20.9 GB 
[11/26 05:52:47 visual_prompt]: 	Training 300/553. train loss: 0.2302,	2.1920 s / batch. (data: 1.36e+00). ETA=1 day, 6:07:18, max mem: 20.9 GB 
[11/26 05:54:27 visual_prompt]: 	Training 400/553. train loss: 0.7068,	0.8611 s / batch. (data: 1.60e-02). ETA=11:48:31, max mem: 20.9 GB 
[11/26 05:56:08 visual_prompt]: 	Training 500/553. train loss: 0.6712,	0.8310 s / batch. (data: 5.44e-03). ETA=11:22:22, max mem: 20.9 GB 
[11/26 05:57:00 visual_prompt]: Epoch 11 / 100: avg data time: 1.99e-01, avg batch time: 1.0254, average train loss: 0.9720
[11/26 05:57:59 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3081, average loss: 0.6573
[11/26 05:57:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 63.97	
[11/26 05:57:59 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[11/26 05:59:46 visual_prompt]: 	Training 100/553. train loss: 0.7590,	0.9033 s / batch. (data: 7.03e-02). ETA=12:19:29, max mem: 20.9 GB 
[11/26 06:01:29 visual_prompt]: 	Training 200/553. train loss: 0.7137,	1.0017 s / batch. (data: 1.76e-01). ETA=13:38:22, max mem: 20.9 GB 
[11/26 06:03:09 visual_prompt]: 	Training 300/553. train loss: 0.5980,	0.8245 s / batch. (data: 3.27e-04). ETA=11:12:12, max mem: 20.9 GB 
[11/26 06:04:51 visual_prompt]: 	Training 400/553. train loss: 0.7207,	0.8600 s / batch. (data: 3.12e-04). ETA=11:39:43, max mem: 20.9 GB 
[11/26 06:06:33 visual_prompt]: 	Training 500/553. train loss: 2.8480,	0.8360 s / batch. (data: 3.17e-04). ETA=11:18:47, max mem: 20.9 GB 
[11/26 06:07:25 visual_prompt]: Epoch 12 / 100: avg data time: 1.98e-01, avg batch time: 1.0242, average train loss: 0.8939
[11/26 06:08:24 visual_prompt]: Inference (val):avg data time: 4.00e-04, avg batch time: 0.3085, average loss: 2.2231
[11/26 06:08:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.06	
[11/26 06:08:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[11/26 06:10:11 visual_prompt]: 	Training 100/553. train loss: 0.7936,	1.6138 s / batch. (data: 7.67e-01). ETA=21:46:13, max mem: 20.9 GB 
[11/26 06:11:50 visual_prompt]: 	Training 200/553. train loss: 0.6456,	0.8612 s / batch. (data: 2.57e-02). ETA=11:35:35, max mem: 20.9 GB 
[11/26 06:13:33 visual_prompt]: 	Training 300/553. train loss: 0.4188,	1.9036 s / batch. (data: 1.09e+00). ETA=1 day, 1:34:26, max mem: 20.9 GB 
[11/26 06:15:13 visual_prompt]: 	Training 400/553. train loss: 2.7604,	0.8440 s / batch. (data: 3.21e-04). ETA=11:18:55, max mem: 20.9 GB 
[11/26 06:16:57 visual_prompt]: 	Training 500/553. train loss: 1.1969,	0.8370 s / batch. (data: 1.43e-03). ETA=11:11:53, max mem: 20.9 GB 
[11/26 06:17:50 visual_prompt]: Epoch 13 / 100: avg data time: 1.98e-01, avg batch time: 1.0233, average train loss: 0.9678
[11/26 06:18:48 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3077, average loss: 1.2001
[11/26 06:18:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.99	
[11/26 06:18:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[11/26 06:20:36 visual_prompt]: 	Training 100/553. train loss: 0.5801,	0.8160 s / batch. (data: 3.06e-04). ETA=10:52:56, max mem: 20.9 GB 
[11/26 06:22:18 visual_prompt]: 	Training 200/553. train loss: 1.0630,	1.2349 s / batch. (data: 4.26e-01). ETA=16:26:04, max mem: 20.9 GB 
[11/26 06:23:59 visual_prompt]: 	Training 300/553. train loss: 0.8490,	0.8356 s / batch. (data: 5.54e-03). ETA=11:05:52, max mem: 20.9 GB 
[11/26 06:25:41 visual_prompt]: 	Training 400/553. train loss: 1.0030,	0.8400 s / batch. (data: 3.09e-04). ETA=11:07:55, max mem: 20.9 GB 
[11/26 06:27:22 visual_prompt]: 	Training 500/553. train loss: 1.9111,	0.8345 s / batch. (data: 3.15e-04). ETA=11:02:11, max mem: 20.9 GB 
[11/26 06:28:14 visual_prompt]: Epoch 14 / 100: avg data time: 1.96e-01, avg batch time: 1.0233, average train loss: 0.8577
[11/26 06:29:13 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3094, average loss: 0.6717
[11/26 06:29:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.40	
[11/26 06:29:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[11/26 06:30:59 visual_prompt]: 	Training 100/553. train loss: 0.8441,	0.8230 s / batch. (data: 3.24e-04). ETA=10:50:59, max mem: 20.9 GB 
[11/26 06:32:39 visual_prompt]: 	Training 200/553. train loss: 0.6626,	0.8281 s / batch. (data: 3.03e-04). ETA=10:53:35, max mem: 20.9 GB 
[11/26 06:34:22 visual_prompt]: 	Training 300/553. train loss: 0.4331,	0.8240 s / batch. (data: 3.04e-04). ETA=10:48:58, max mem: 20.9 GB 
[11/26 06:36:01 visual_prompt]: 	Training 400/553. train loss: 0.3525,	0.8247 s / batch. (data: 3.24e-04). ETA=10:48:11, max mem: 20.9 GB 
[11/26 06:37:44 visual_prompt]: 	Training 500/553. train loss: 0.5166,	0.8572 s / batch. (data: 1.56e-02). ETA=11:12:17, max mem: 20.9 GB 
[11/26 06:38:38 visual_prompt]: Epoch 15 / 100: avg data time: 1.94e-01, avg batch time: 1.0209, average train loss: 0.9378
[11/26 06:39:36 visual_prompt]: Inference (val):avg data time: 6.36e-04, avg batch time: 0.3088, average loss: 1.4959
[11/26 06:39:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.26	
[11/26 06:39:36 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[11/26 06:41:21 visual_prompt]: 	Training 100/553. train loss: 0.3294,	0.8280 s / batch. (data: 5.08e-03). ETA=10:47:17, max mem: 20.9 GB 
[11/26 06:43:03 visual_prompt]: 	Training 200/553. train loss: 0.6825,	0.8480 s / batch. (data: 2.99e-04). ETA=11:01:28, max mem: 20.9 GB 
[11/26 06:44:45 visual_prompt]: 	Training 300/553. train loss: 1.0684,	0.8240 s / batch. (data: 3.63e-04). ETA=10:41:25, max mem: 20.9 GB 
[11/26 06:46:27 visual_prompt]: 	Training 400/553. train loss: 0.4496,	0.8480 s / batch. (data: 1.56e-02). ETA=10:58:42, max mem: 20.9 GB 
[11/26 06:48:07 visual_prompt]: 	Training 500/553. train loss: 0.7999,	1.4240 s / batch. (data: 5.94e-01). ETA=18:23:43, max mem: 20.9 GB 
[11/26 06:49:01 visual_prompt]: Epoch 16 / 100: avg data time: 1.94e-01, avg batch time: 1.0210, average train loss: 0.8059
[11/26 06:49:59 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3076, average loss: 0.6313
[11/26 06:49:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 68.88	
[11/26 06:49:59 visual_prompt]: Best epoch 16: best metric: -0.631
[11/26 06:49:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[11/26 06:51:44 visual_prompt]: 	Training 100/553. train loss: 0.2631,	0.8480 s / batch. (data: 3.15e-04). ETA=10:55:07, max mem: 20.9 GB 
[11/26 06:53:28 visual_prompt]: 	Training 200/553. train loss: 1.0558,	0.8280 s / batch. (data: 3.08e-04). ETA=10:38:17, max mem: 20.9 GB 
[11/26 06:55:08 visual_prompt]: 	Training 300/553. train loss: 1.0640,	0.8280 s / batch. (data: 3.06e-04). ETA=10:36:53, max mem: 20.9 GB 
[11/26 06:56:49 visual_prompt]: 	Training 400/553. train loss: 0.6531,	1.0720 s / batch. (data: 2.53e-01). ETA=13:42:46, max mem: 20.9 GB 
[11/26 06:58:30 visual_prompt]: 	Training 500/553. train loss: 1.4022,	1.5449 s / batch. (data: 7.34e-01). ETA=19:43:09, max mem: 20.9 GB 
[11/26 06:59:24 visual_prompt]: Epoch 17 / 100: avg data time: 1.96e-01, avg batch time: 1.0221, average train loss: 0.8274
[11/26 07:00:23 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.3079, average loss: 0.6235
[11/26 07:00:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.11	rocauc: 69.32	
[11/26 07:00:23 visual_prompt]: Best epoch 17: best metric: -0.624
[11/26 07:00:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[11/26 07:02:08 visual_prompt]: 	Training 100/553. train loss: 0.4588,	0.8520 s / batch. (data: 2.88e-04). ETA=10:50:20, max mem: 20.9 GB 
[11/26 07:03:45 visual_prompt]: 	Training 200/553. train loss: 0.4853,	0.8275 s / batch. (data: 2.92e-04). ETA=10:30:14, max mem: 20.9 GB 
[11/26 07:05:20 visual_prompt]: 	Training 300/553. train loss: 0.5312,	0.8172 s / batch. (data: 5.42e-03). ETA=10:21:01, max mem: 20.9 GB 
[11/26 07:06:55 visual_prompt]: 	Training 400/553. train loss: 0.7852,	0.8600 s / batch. (data: 1.59e-02). ETA=10:52:08, max mem: 20.9 GB 
[11/26 07:08:29 visual_prompt]: 	Training 500/553. train loss: 1.0336,	0.8325 s / batch. (data: 1.07e-02). ETA=10:29:53, max mem: 20.9 GB 
[11/26 07:09:18 visual_prompt]: Epoch 18 / 100: avg data time: 1.42e-01, avg batch time: 0.9679, average train loss: 0.8607
[11/26 07:10:13 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3088, average loss: 0.6609
[11/26 07:10:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 69.26	
[11/26 07:10:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[11/26 07:11:51 visual_prompt]: 	Training 100/553. train loss: 0.5239,	0.8917 s / batch. (data: 6.96e-02). ETA=11:12:25, max mem: 20.9 GB 
[11/26 07:13:27 visual_prompt]: 	Training 200/553. train loss: 0.3981,	0.8456 s / batch. (data: 9.54e-03). ETA=10:36:14, max mem: 20.9 GB 
[11/26 07:15:03 visual_prompt]: 	Training 300/553. train loss: 0.9150,	0.8360 s / batch. (data: 2.94e-04). ETA=10:27:38, max mem: 20.9 GB 
[11/26 07:16:39 visual_prompt]: 	Training 400/553. train loss: 0.2576,	0.8109 s / batch. (data: 2.99e-04). ETA=10:07:27, max mem: 20.9 GB 
[11/26 07:18:11 visual_prompt]: 	Training 500/553. train loss: 1.7811,	0.8263 s / batch. (data: 1.05e-02). ETA=10:17:35, max mem: 20.9 GB 
[11/26 07:19:01 visual_prompt]: Epoch 19 / 100: avg data time: 1.28e-01, avg batch time: 0.9545, average train loss: 0.7490
[11/26 07:19:55 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3083, average loss: 1.2840
[11/26 07:19:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.53	
[11/26 07:19:55 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[11/26 07:21:33 visual_prompt]: 	Training 100/553. train loss: 0.4561,	0.8107 s / batch. (data: 3.17e-04). ETA=10:03:51, max mem: 20.9 GB 
[11/26 07:23:09 visual_prompt]: 	Training 200/553. train loss: 0.2753,	0.8195 s / batch. (data: 2.60e-04). ETA=10:09:03, max mem: 20.9 GB 
[11/26 07:24:45 visual_prompt]: 	Training 300/553. train loss: 1.0949,	0.8500 s / batch. (data: 1.56e-02). ETA=10:30:18, max mem: 20.9 GB 
[11/26 07:26:20 visual_prompt]: 	Training 400/553. train loss: 0.8389,	0.8520 s / batch. (data: 1.19e-02). ETA=10:30:22, max mem: 20.9 GB 
[11/26 07:27:54 visual_prompt]: 	Training 500/553. train loss: 0.8288,	0.8277 s / batch. (data: 2.94e-04). ETA=10:11:00, max mem: 20.9 GB 
[11/26 07:28:45 visual_prompt]: Epoch 20 / 100: avg data time: 1.32e-01, avg batch time: 0.9587, average train loss: 0.8507
[11/26 07:29:40 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.3088, average loss: 0.9878
[11/26 07:29:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 67.00	
[11/26 07:29:40 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[11/26 07:31:21 visual_prompt]: 	Training 100/553. train loss: 0.4257,	0.8312 s / batch. (data: 3.05e-04). ETA=10:11:29, max mem: 20.9 GB 
[11/26 07:32:56 visual_prompt]: 	Training 200/553. train loss: 0.3328,	0.8171 s / batch. (data: 3.07e-04). ETA=9:59:45, max mem: 20.9 GB 
[11/26 07:34:31 visual_prompt]: 	Training 300/553. train loss: 0.7221,	0.9245 s / batch. (data: 9.30e-02). ETA=11:17:00, max mem: 20.9 GB 
[11/26 07:36:05 visual_prompt]: 	Training 400/553. train loss: 0.9835,	0.8136 s / batch. (data: 2.92e-04). ETA=9:54:26, max mem: 20.9 GB 
[11/26 07:37:41 visual_prompt]: 	Training 500/553. train loss: 0.6839,	0.8247 s / batch. (data: 3.01e-04). ETA=10:01:13, max mem: 20.9 GB 
[11/26 07:38:30 visual_prompt]: Epoch 21 / 100: avg data time: 1.33e-01, avg batch time: 0.9586, average train loss: 0.8068
[11/26 07:39:24 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3069, average loss: 0.6207
[11/26 07:39:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.51	rocauc: 70.98	
[11/26 07:39:24 visual_prompt]: Best epoch 21: best metric: -0.621
[11/26 07:39:24 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[11/26 07:41:03 visual_prompt]: 	Training 100/553. train loss: 0.6275,	0.8519 s / batch. (data: 1.20e-02). ETA=10:18:53, max mem: 20.9 GB 
[11/26 07:42:38 visual_prompt]: 	Training 200/553. train loss: 0.5135,	0.8240 s / batch. (data: 7.95e-03). ETA=9:57:13, max mem: 20.9 GB 
[11/26 07:44:12 visual_prompt]: 	Training 300/553. train loss: 0.2174,	0.8280 s / batch. (data: 2.95e-04). ETA=9:58:44, max mem: 20.9 GB 
[11/26 07:45:48 visual_prompt]: 	Training 400/553. train loss: 0.7639,	0.8240 s / batch. (data: 2.88e-04). ETA=9:54:28, max mem: 20.9 GB 
[11/26 07:47:23 visual_prompt]: 	Training 500/553. train loss: 0.6402,	0.8160 s / batch. (data: 3.15e-04). ETA=9:47:20, max mem: 20.9 GB 
[11/26 07:48:13 visual_prompt]: Epoch 22 / 100: avg data time: 1.31e-01, avg batch time: 0.9568, average train loss: 0.7254
[11/26 07:49:08 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3078, average loss: 0.6321
[11/26 07:49:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 71.04	
[11/26 07:49:08 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[11/26 07:50:49 visual_prompt]: 	Training 100/553. train loss: 1.1419,	0.8400 s / batch. (data: 3.66e-04). ETA=10:02:27, max mem: 20.9 GB 
[11/26 07:52:24 visual_prompt]: 	Training 200/553. train loss: 1.1796,	0.8181 s / batch. (data: 5.43e-03). ETA=9:45:22, max mem: 20.9 GB 
[11/26 07:54:01 visual_prompt]: 	Training 300/553. train loss: 1.0276,	0.8360 s / batch. (data: 2.93e-04). ETA=9:56:49, max mem: 20.9 GB 
[11/26 07:55:34 visual_prompt]: 	Training 400/553. train loss: 0.3921,	0.8329 s / batch. (data: 7.20e-04). ETA=9:53:11, max mem: 20.9 GB 
[11/26 07:57:08 visual_prompt]: 	Training 500/553. train loss: 0.7357,	0.8242 s / batch. (data: 4.52e-04). ETA=9:45:40, max mem: 20.9 GB 
[11/26 07:57:57 visual_prompt]: Epoch 23 / 100: avg data time: 1.30e-01, avg batch time: 0.9567, average train loss: 0.7772
[11/26 07:58:52 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3086, average loss: 0.6213
[11/26 07:58:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.51	rocauc: 70.51	
[11/26 07:58:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[11/26 08:00:28 visual_prompt]: 	Training 100/553. train loss: 0.7550,	0.8641 s / batch. (data: 2.41e-02). ETA=10:11:49, max mem: 20.9 GB 
[11/26 08:02:03 visual_prompt]: 	Training 200/553. train loss: 1.1172,	0.8320 s / batch. (data: 2.96e-04). ETA=9:47:40, max mem: 20.9 GB 
[11/26 08:03:39 visual_prompt]: 	Training 300/553. train loss: 0.2621,	0.8320 s / batch. (data: 5.46e-03). ETA=9:46:17, max mem: 20.9 GB 
[11/26 08:05:15 visual_prompt]: 	Training 400/553. train loss: 0.1795,	0.8357 s / batch. (data: 1.17e-02). ETA=9:47:30, max mem: 20.9 GB 
[11/26 08:06:52 visual_prompt]: 	Training 500/553. train loss: 0.6438,	0.8359 s / batch. (data: 5.08e-04). ETA=9:46:17, max mem: 20.9 GB 
[11/26 08:07:42 visual_prompt]: Epoch 24 / 100: avg data time: 1.31e-01, avg batch time: 0.9583, average train loss: 0.7560
[11/26 08:08:36 visual_prompt]: Inference (val):avg data time: 2.78e-04, avg batch time: 0.3101, average loss: 0.7242
[11/26 08:08:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 68.20	
[11/26 08:08:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[11/26 08:10:19 visual_prompt]: 	Training 100/553. train loss: 0.6446,	0.8400 s / batch. (data: 7.95e-03). ETA=9:46:58, max mem: 20.9 GB 
[11/26 08:11:52 visual_prompt]: 	Training 200/553. train loss: 1.5657,	0.8336 s / batch. (data: 3.09e-04). ETA=9:41:08, max mem: 20.9 GB 
[11/26 08:13:27 visual_prompt]: 	Training 300/553. train loss: 0.4855,	0.8514 s / batch. (data: 2.32e-02). ETA=9:52:08, max mem: 20.9 GB 
[11/26 08:15:02 visual_prompt]: 	Training 400/553. train loss: 0.8730,	1.1372 s / batch. (data: 3.12e-01). ETA=13:08:59, max mem: 20.9 GB 
[11/26 08:16:37 visual_prompt]: 	Training 500/553. train loss: 0.5101,	1.3040 s / batch. (data: 4.84e-01). ETA=15:02:32, max mem: 20.9 GB 
[11/26 08:17:28 visual_prompt]: Epoch 25 / 100: avg data time: 1.33e-01, avg batch time: 0.9605, average train loss: 0.7445
[11/26 08:18:22 visual_prompt]: Inference (val):avg data time: 1.65e-04, avg batch time: 0.3077, average loss: 0.9815
[11/26 08:18:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 67.64	
[11/26 08:18:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[11/26 08:20:02 visual_prompt]: 	Training 100/553. train loss: 0.0479,	0.8182 s / batch. (data: 7.94e-03). ETA=9:24:11, max mem: 20.9 GB 
[11/26 08:21:38 visual_prompt]: 	Training 200/553. train loss: 0.5228,	1.3796 s / batch. (data: 5.60e-01). ETA=15:49:02, max mem: 20.9 GB 
[11/26 08:23:15 visual_prompt]: 	Training 300/553. train loss: 0.9361,	0.8532 s / batch. (data: 5.90e-03). ETA=9:45:31, max mem: 20.9 GB 
[11/26 08:24:48 visual_prompt]: 	Training 400/553. train loss: 1.1999,	0.8270 s / batch. (data: 2.91e-04). ETA=9:26:08, max mem: 20.9 GB 
[11/26 08:26:22 visual_prompt]: 	Training 500/553. train loss: 0.2359,	0.8444 s / batch. (data: 7.94e-03). ETA=9:36:40, max mem: 20.9 GB 
[11/26 08:27:11 visual_prompt]: Epoch 26 / 100: avg data time: 1.29e-01, avg batch time: 0.9569, average train loss: 0.7377
[11/26 08:28:06 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3068, average loss: 0.6744
[11/26 08:28:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 69.70	
[11/26 08:28:06 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[11/26 08:29:46 visual_prompt]: 	Training 100/553. train loss: 0.4973,	0.8125 s / batch. (data: 2.52e-03). ETA=9:12:48, max mem: 20.9 GB 
[11/26 08:31:21 visual_prompt]: 	Training 200/553. train loss: 0.3193,	0.8240 s / batch. (data: 3.03e-04). ETA=9:19:16, max mem: 20.9 GB 
[11/26 08:32:56 visual_prompt]: 	Training 300/553. train loss: 0.8815,	0.8237 s / batch. (data: 3.00e-04). ETA=9:17:41, max mem: 20.9 GB 
[11/26 08:34:33 visual_prompt]: 	Training 400/553. train loss: 0.8618,	0.8567 s / batch. (data: 1.09e-02). ETA=9:38:34, max mem: 20.9 GB 
[11/26 08:36:08 visual_prompt]: 	Training 500/553. train loss: 0.6133,	0.8189 s / batch. (data: 3.17e-04). ETA=9:11:42, max mem: 20.9 GB 
[11/26 08:36:56 visual_prompt]: Epoch 27 / 100: avg data time: 1.32e-01, avg batch time: 0.9580, average train loss: 0.7071
[11/26 08:37:50 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3075, average loss: 0.6632
[11/26 08:37:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.71	
[11/26 08:37:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[11/26 08:39:29 visual_prompt]: 	Training 100/553. train loss: 0.6293,	0.8519 s / batch. (data: 3.21e-02). ETA=9:31:43, max mem: 20.9 GB 
[11/26 08:41:04 visual_prompt]: 	Training 200/553. train loss: 0.1998,	0.8175 s / batch. (data: 4.65e-04). ETA=9:07:17, max mem: 20.9 GB 
[11/26 08:42:40 visual_prompt]: 	Training 300/553. train loss: 0.8809,	1.2115 s / batch. (data: 3.86e-01). ETA=13:29:05, max mem: 20.9 GB 
[11/26 08:44:15 visual_prompt]: 	Training 400/553. train loss: 0.4145,	0.8560 s / batch. (data: 7.90e-03). ETA=9:30:13, max mem: 20.9 GB 
[11/26 08:45:49 visual_prompt]: 	Training 500/553. train loss: 0.4562,	0.8357 s / batch. (data: 7.96e-04). ETA=9:15:17, max mem: 20.9 GB 
[11/26 08:46:39 visual_prompt]: Epoch 28 / 100: avg data time: 1.29e-01, avg batch time: 0.9561, average train loss: 0.6504
[11/26 08:47:34 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3082, average loss: 0.6399
[11/26 08:47:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.11	rocauc: 73.17	
[11/26 08:47:34 visual_prompt]: Stopping early.
[11/26 08:47:34 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 08:47:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 08:47:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/26 08:47:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 08:47:34 visual_prompt]: Training with config:
[11/26 08:47:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.1_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/26 08:47:34 visual_prompt]: Loading training data...
[11/26 08:47:34 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 08:47:34 visual_prompt]: Loading validation data...
[11/26 08:47:34 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 08:47:34 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 08:47:37 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 08:47:37 visual_prompt]: tuned percent:0.525
[11/26 08:47:37 visual_prompt]: Device used for model: 0
[11/26 08:47:37 visual_prompt]: Setting up Evaluator...
[11/26 08:47:37 visual_prompt]: Setting up Trainer...
[11/26 08:47:37 visual_prompt]: 	Setting up the optimizer...
[11/26 08:47:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 08:49:16 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8250 s / batch. (data: 1.55e-02). ETA=12:38:59, max mem: 20.9 GB 
[11/26 08:50:49 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8182 s / batch. (data: 7.95e-03). ETA=12:31:21, max mem: 20.9 GB 
[11/26 08:52:27 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.2087 s / batch. (data: 3.86e-01). ETA=18:27:57, max mem: 20.9 GB 
[11/26 08:54:01 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8303 s / batch. (data: 7.95e-03). ETA=12:39:45, max mem: 20.9 GB 
[11/26 08:55:37 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8101 s / batch. (data: 3.17e-04). ETA=12:19:52, max mem: 20.9 GB 
[11/26 08:56:28 visual_prompt]: Epoch 1 / 100: avg data time: 1.33e-01, avg batch time: 0.9598, average train loss: 1.5403
[11/26 08:57:22 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3085, average loss: 1.5201
[11/26 08:57:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 08:57:22 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[11/26 08:59:01 visual_prompt]: 	Training 100/553. train loss: 0.6431,	0.8464 s / batch. (data: 1.56e-02). ETA=12:50:53, max mem: 20.9 GB 
[11/26 09:00:36 visual_prompt]: 	Training 200/553. train loss: 0.2374,	0.8360 s / batch. (data: 6.92e-03). ETA=12:40:01, max mem: 20.9 GB 
[11/26 09:02:13 visual_prompt]: 	Training 300/553. train loss: 0.7816,	0.9518 s / batch. (data: 1.34e-01). ETA=14:23:41, max mem: 20.9 GB 
[11/26 09:03:47 visual_prompt]: 	Training 400/553. train loss: 0.9218,	0.8225 s / batch. (data: 1.05e-02). ETA=12:24:57, max mem: 20.9 GB 
[11/26 09:05:24 visual_prompt]: 	Training 500/553. train loss: 0.7228,	0.8135 s / batch. (data: 2.94e-04). ETA=12:15:28, max mem: 20.9 GB 
[11/26 09:06:12 visual_prompt]: Epoch 2 / 100: avg data time: 1.33e-01, avg batch time: 0.9588, average train loss: 0.7701
[11/26 09:07:07 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3084, average loss: 0.7302
[11/26 09:07:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.36	
[11/26 09:07:07 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[11/26 09:08:46 visual_prompt]: 	Training 100/553. train loss: 0.7790,	0.8358 s / batch. (data: 1.05e-02). ETA=12:33:32, max mem: 20.9 GB 
[11/26 09:10:21 visual_prompt]: 	Training 200/553. train loss: 0.7828,	0.8522 s / batch. (data: 1.22e-02). ETA=12:46:53, max mem: 20.9 GB 
[11/26 09:11:56 visual_prompt]: 	Training 300/553. train loss: 0.5731,	0.8359 s / batch. (data: 2.89e-04). ETA=12:30:50, max mem: 20.9 GB 
[11/26 09:13:31 visual_prompt]: 	Training 400/553. train loss: 0.6154,	0.8432 s / batch. (data: 1.11e-02). ETA=12:35:58, max mem: 20.9 GB 
[11/26 09:15:08 visual_prompt]: 	Training 500/553. train loss: 0.7019,	1.1870 s / batch. (data: 3.66e-01). ETA=17:42:16, max mem: 20.9 GB 
[11/26 09:15:56 visual_prompt]: Epoch 3 / 100: avg data time: 1.32e-01, avg batch time: 0.9571, average train loss: 0.7334
[11/26 09:16:51 visual_prompt]: Inference (val):avg data time: 2.54e-04, avg batch time: 0.3088, average loss: 0.7195
[11/26 09:16:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.31	
[11/26 09:16:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[11/26 09:18:31 visual_prompt]: 	Training 100/553. train loss: 0.7262,	0.8346 s / batch. (data: 3.13e-04). ETA=12:24:47, max mem: 20.9 GB 
[11/26 09:20:07 visual_prompt]: 	Training 200/553. train loss: 0.6107,	0.8248 s / batch. (data: 3.05e-04). ETA=12:14:36, max mem: 20.9 GB 
[11/26 09:21:43 visual_prompt]: 	Training 300/553. train loss: 0.6999,	1.2793 s / batch. (data: 4.57e-01). ETA=18:57:18, max mem: 20.9 GB 
[11/26 09:23:13 visual_prompt]: 	Training 400/553. train loss: 0.7789,	0.9320 s / batch. (data: 9.12e-02). ETA=13:47:01, max mem: 20.9 GB 
[11/26 09:24:50 visual_prompt]: 	Training 500/553. train loss: 0.3921,	3.1359 s / batch. (data: 2.32e+00). ETA=1 day, 22:17:26, max mem: 20.9 GB 
[11/26 09:25:42 visual_prompt]: Epoch 4 / 100: avg data time: 1.34e-01, avg batch time: 0.9604, average train loss: 0.7260
[11/26 09:26:36 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3071, average loss: 0.7170
[11/26 09:26:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.38	
[11/26 09:26:36 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[11/26 09:28:14 visual_prompt]: 	Training 100/553. train loss: 0.4793,	0.8320 s / batch. (data: 2.95e-04). ETA=12:14:43, max mem: 20.9 GB 
[11/26 09:29:50 visual_prompt]: 	Training 200/553. train loss: 0.5785,	0.9819 s / batch. (data: 1.60e-01). ETA=14:25:31, max mem: 20.9 GB 
[11/26 09:31:26 visual_prompt]: 	Training 300/553. train loss: 0.7965,	0.8120 s / batch. (data: 2.69e-04). ETA=11:54:24, max mem: 20.9 GB 
[11/26 09:33:01 visual_prompt]: 	Training 400/553. train loss: 0.5750,	0.8226 s / batch. (data: 1.04e-02). ETA=12:02:22, max mem: 20.9 GB 
[11/26 09:34:36 visual_prompt]: 	Training 500/553. train loss: 0.6880,	0.8319 s / batch. (data: 7.89e-03). ETA=12:09:08, max mem: 20.9 GB 
[11/26 09:35:27 visual_prompt]: Epoch 5 / 100: avg data time: 1.32e-01, avg batch time: 0.9589, average train loss: 0.7161
[11/26 09:36:21 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3065, average loss: 0.6887
[11/26 09:36:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.48	
[11/26 09:36:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[11/26 09:38:02 visual_prompt]: 	Training 100/553. train loss: 0.7186,	0.8438 s / batch. (data: 5.87e-03). ETA=12:17:22, max mem: 20.9 GB 
[11/26 09:39:36 visual_prompt]: 	Training 200/553. train loss: 0.6947,	0.8280 s / batch. (data: 3.12e-04). ETA=12:02:12, max mem: 20.9 GB 
[11/26 09:41:09 visual_prompt]: 	Training 300/553. train loss: 0.5645,	0.8122 s / batch. (data: 3.76e-04). ETA=11:47:06, max mem: 20.9 GB 
[11/26 09:42:49 visual_prompt]: 	Training 400/553. train loss: 0.6932,	0.8279 s / batch. (data: 3.38e-04). ETA=11:59:23, max mem: 20.9 GB 
[11/26 09:44:23 visual_prompt]: 	Training 500/553. train loss: 0.7095,	0.8194 s / batch. (data: 3.10e-04). ETA=11:50:37, max mem: 20.9 GB 
[11/26 09:45:12 visual_prompt]: Epoch 6 / 100: avg data time: 1.34e-01, avg batch time: 0.9598, average train loss: 0.7280
[11/26 09:46:06 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3087, average loss: 0.7389
[11/26 09:46:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.21	
[11/26 09:46:06 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[11/26 09:47:44 visual_prompt]: 	Training 100/553. train loss: 0.4931,	0.8252 s / batch. (data: 1.56e-02). ETA=11:53:30, max mem: 20.9 GB 
[11/26 09:49:20 visual_prompt]: 	Training 200/553. train loss: 0.5623,	0.8165 s / batch. (data: 2.88e-04). ETA=11:44:39, max mem: 20.9 GB 
[11/26 09:50:58 visual_prompt]: 	Training 300/553. train loss: 0.8180,	1.5914 s / batch. (data: 7.52e-01). ETA=22:50:46, max mem: 20.9 GB 
[11/26 09:52:32 visual_prompt]: 	Training 400/553. train loss: 0.6170,	1.4000 s / batch. (data: 5.55e-01). ETA=20:03:34, max mem: 20.9 GB 
[11/26 09:54:06 visual_prompt]: 	Training 500/553. train loss: 0.9595,	0.8366 s / batch. (data: 1.11e-02). ETA=11:57:49, max mem: 20.9 GB 
[11/26 09:54:56 visual_prompt]: Epoch 7 / 100: avg data time: 1.30e-01, avg batch time: 0.9565, average train loss: 0.7295
[11/26 09:55:50 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3098, average loss: 0.8218
[11/26 09:55:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.82	
[11/26 09:55:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[11/26 09:57:28 visual_prompt]: 	Training 100/553. train loss: 0.7122,	0.8275 s / batch. (data: 5.43e-03). ETA=11:47:53, max mem: 20.9 GB 
[11/26 09:59:04 visual_prompt]: 	Training 200/553. train loss: 1.2346,	0.8279 s / batch. (data: 1.05e-02). ETA=11:46:52, max mem: 20.9 GB 
[11/26 10:00:40 visual_prompt]: 	Training 300/553. train loss: 0.6719,	0.8396 s / batch. (data: 1.05e-02). ETA=11:55:26, max mem: 20.9 GB 
[11/26 10:02:16 visual_prompt]: 	Training 400/553. train loss: 0.7037,	0.8239 s / batch. (data: 3.06e-04). ETA=11:40:40, max mem: 20.9 GB 
[11/26 10:03:51 visual_prompt]: 	Training 500/553. train loss: 0.9900,	1.2727 s / batch. (data: 4.37e-01). ETA=18:00:19, max mem: 20.9 GB 
[11/26 10:04:41 visual_prompt]: Epoch 8 / 100: avg data time: 1.33e-01, avg batch time: 0.9603, average train loss: 0.7613
[11/26 10:05:36 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3078, average loss: 0.7099
[11/26 10:05:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.16	
[11/26 10:05:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[11/26 10:07:15 visual_prompt]: 	Training 100/553. train loss: 0.4686,	0.8400 s / batch. (data: 2.89e-04). ETA=11:50:51, max mem: 20.9 GB 
[11/26 10:08:50 visual_prompt]: 	Training 200/553. train loss: 0.8585,	0.8101 s / batch. (data: 2.49e-04). ETA=11:24:14, max mem: 20.9 GB 
[11/26 10:10:27 visual_prompt]: 	Training 300/553. train loss: 0.6885,	1.4113 s / batch. (data: 5.88e-01). ETA=19:49:37, max mem: 20.9 GB 
[11/26 10:12:03 visual_prompt]: 	Training 400/553. train loss: 0.5860,	0.8320 s / batch. (data: 3.29e-04). ETA=11:39:54, max mem: 20.9 GB 
[11/26 10:13:39 visual_prompt]: 	Training 500/553. train loss: 0.7607,	0.8798 s / batch. (data: 3.62e-02). ETA=12:18:39, max mem: 20.9 GB 
[11/26 10:14:27 visual_prompt]: Epoch 9 / 100: avg data time: 1.35e-01, avg batch time: 0.9612, average train loss: 0.7489
[11/26 10:15:21 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.3100, average loss: 0.7599
[11/26 10:15:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.02	
[11/26 10:15:22 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[11/26 10:17:03 visual_prompt]: 	Training 100/553. train loss: 0.6948,	0.8360 s / batch. (data: 3.20e-04). ETA=11:39:46, max mem: 20.9 GB 
[11/26 10:18:38 visual_prompt]: 	Training 200/553. train loss: 0.7120,	0.8614 s / batch. (data: 1.56e-02). ETA=11:59:38, max mem: 20.9 GB 
[11/26 10:20:12 visual_prompt]: 	Training 300/553. train loss: 0.6014,	1.0119 s / batch. (data: 1.64e-01). ETA=14:03:40, max mem: 20.9 GB 
[11/26 10:21:45 visual_prompt]: 	Training 400/553. train loss: 0.8461,	0.8440 s / batch. (data: 1.99e-02). ETA=11:42:13, max mem: 20.9 GB 
[11/26 10:23:22 visual_prompt]: 	Training 500/553. train loss: 1.3348,	0.8211 s / batch. (data: 1.19e-02). ETA=11:21:49, max mem: 20.9 GB 
[11/26 10:24:13 visual_prompt]: Epoch 10 / 100: avg data time: 1.33e-01, avg batch time: 0.9605, average train loss: 0.7891
[11/26 10:25:07 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3083, average loss: 0.9843
[11/26 10:25:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.53	
[11/26 10:25:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[11/26 10:26:48 visual_prompt]: 	Training 100/553. train loss: 0.7003,	0.8146 s / batch. (data: 5.44e-03). ETA=11:14:19, max mem: 20.9 GB 
[11/26 10:28:25 visual_prompt]: 	Training 200/553. train loss: 1.8986,	0.8273 s / batch. (data: 3.32e-03). ETA=11:23:28, max mem: 20.9 GB 
[11/26 10:30:00 visual_prompt]: 	Training 300/553. train loss: 0.6453,	2.0121 s / batch. (data: 1.20e+00). ETA=1 day, 3:39:00, max mem: 20.9 GB 
[11/26 10:31:33 visual_prompt]: 	Training 400/553. train loss: 0.5925,	0.8241 s / batch. (data: 2.48e-04). ETA=11:18:03, max mem: 20.9 GB 
[11/26 10:33:07 visual_prompt]: 	Training 500/553. train loss: 0.7808,	0.8246 s / batch. (data: 3.02e-04). ETA=11:17:07, max mem: 20.9 GB 
[11/26 10:33:57 visual_prompt]: Epoch 11 / 100: avg data time: 1.32e-01, avg batch time: 0.9579, average train loss: 0.7997
[11/26 10:34:51 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3077, average loss: 0.7660
[11/26 10:34:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.82	
[11/26 10:34:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[11/26 10:36:32 visual_prompt]: 	Training 100/553. train loss: 0.8860,	0.8329 s / batch. (data: 3.23e-04). ETA=11:21:49, max mem: 20.9 GB 
[11/26 10:38:08 visual_prompt]: 	Training 200/553. train loss: 0.6502,	1.3795 s / batch. (data: 5.70e-01). ETA=18:46:56, max mem: 20.9 GB 
[11/26 10:39:42 visual_prompt]: 	Training 300/553. train loss: 0.6491,	0.8369 s / batch. (data: 3.00e-04). ETA=11:22:20, max mem: 20.9 GB 
[11/26 10:41:18 visual_prompt]: 	Training 400/553. train loss: 0.7330,	0.8201 s / batch. (data: 2.97e-04). ETA=11:07:13, max mem: 20.9 GB 
[11/26 10:42:53 visual_prompt]: 	Training 500/553. train loss: 1.7519,	0.8106 s / batch. (data: 7.40e-04). ETA=10:58:12, max mem: 20.9 GB 
[11/26 10:43:42 visual_prompt]: Epoch 12 / 100: avg data time: 1.33e-01, avg batch time: 0.9600, average train loss: 0.7785
[11/26 10:44:37 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3075, average loss: 1.0841
[11/26 10:44:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.42	
[11/26 10:44:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[11/26 10:46:18 visual_prompt]: 	Training 100/553. train loss: 0.7142,	0.8160 s / batch. (data: 3.10e-04). ETA=11:00:27, max mem: 20.9 GB 
[11/26 10:47:50 visual_prompt]: 	Training 200/553. train loss: 0.6996,	1.2799 s / batch. (data: 4.45e-01). ETA=17:13:49, max mem: 20.9 GB 
[11/26 10:49:26 visual_prompt]: 	Training 300/553. train loss: 0.6431,	1.5407 s / batch. (data: 7.31e-01). ETA=20:41:52, max mem: 20.9 GB 
[11/26 10:51:00 visual_prompt]: 	Training 400/553. train loss: 1.4196,	0.8367 s / batch. (data: 7.95e-03). ETA=11:13:02, max mem: 20.9 GB 
[11/26 10:52:40 visual_prompt]: 	Training 500/553. train loss: 0.7342,	0.8248 s / batch. (data: 5.45e-03). ETA=11:02:05, max mem: 20.9 GB 
[11/26 10:53:34 visual_prompt]: Epoch 13 / 100: avg data time: 1.45e-01, avg batch time: 0.9711, average train loss: 0.8154
[11/26 10:54:30 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3094, average loss: 0.6895
[11/26 10:54:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.09	
[11/26 10:54:30 visual_prompt]: Best epoch 13: best metric: -0.689
[11/26 10:54:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[11/26 10:56:17 visual_prompt]: 	Training 100/553. train loss: 0.6347,	0.8489 s / batch. (data: 1.56e-02). ETA=11:19:18, max mem: 20.9 GB 
[11/26 10:57:55 visual_prompt]: 	Training 200/553. train loss: 0.6719,	1.1360 s / batch. (data: 3.11e-01). ETA=15:07:07, max mem: 20.9 GB 
[11/26 10:59:34 visual_prompt]: 	Training 300/553. train loss: 0.6946,	0.8287 s / batch. (data: 1.03e-02). ETA=11:00:21, max mem: 20.9 GB 
[11/26 11:01:19 visual_prompt]: 	Training 400/553. train loss: 0.6832,	0.8360 s / batch. (data: 7.96e-03). ETA=11:04:46, max mem: 20.9 GB 
[11/26 11:02:56 visual_prompt]: 	Training 500/553. train loss: 1.4012,	0.8368 s / batch. (data: 3.17e-04). ETA=11:04:01, max mem: 20.9 GB 
[11/26 11:03:48 visual_prompt]: Epoch 14 / 100: avg data time: 1.83e-01, avg batch time: 1.0082, average train loss: 0.7529
[11/26 11:04:43 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3089, average loss: 0.7286
[11/26 11:04:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.50	
[11/26 11:04:43 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[11/26 11:06:22 visual_prompt]: 	Training 100/553. train loss: 0.6950,	0.8517 s / batch. (data: 2.75e-03). ETA=11:13:37, max mem: 20.9 GB 
[11/26 11:07:56 visual_prompt]: 	Training 200/553. train loss: 0.8049,	0.8360 s / batch. (data: 7.95e-03). ETA=10:59:51, max mem: 20.9 GB 
[11/26 11:09:34 visual_prompt]: 	Training 300/553. train loss: 0.5667,	0.8270 s / batch. (data: 8.64e-04). ETA=10:51:24, max mem: 20.9 GB 
[11/26 11:11:06 visual_prompt]: 	Training 400/553. train loss: 0.5837,	1.0837 s / batch. (data: 2.61e-01). ETA=14:11:44, max mem: 20.9 GB 
[11/26 11:12:42 visual_prompt]: 	Training 500/553. train loss: 1.0142,	0.8143 s / batch. (data: 3.06e-04). ETA=10:38:37, max mem: 20.9 GB 
[11/26 11:13:32 visual_prompt]: Epoch 15 / 100: avg data time: 1.31e-01, avg batch time: 0.9566, average train loss: 0.7890
[11/26 11:14:27 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3065, average loss: 0.8145
[11/26 11:14:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.86	
[11/26 11:14:27 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[11/26 11:16:04 visual_prompt]: 	Training 100/553. train loss: 0.5642,	0.8120 s / batch. (data: 3.34e-04). ETA=10:34:46, max mem: 20.9 GB 
[11/26 11:17:41 visual_prompt]: 	Training 200/553. train loss: 1.4136,	0.8507 s / batch. (data: 7.95e-03). ETA=11:03:38, max mem: 20.9 GB 
[11/26 11:19:18 visual_prompt]: 	Training 300/553. train loss: 0.9059,	0.8390 s / batch. (data: 9.81e-03). ETA=10:53:05, max mem: 20.9 GB 
[11/26 11:20:58 visual_prompt]: 	Training 400/553. train loss: 0.7279,	0.8360 s / batch. (data: 4.56e-04). ETA=10:49:19, max mem: 20.9 GB 
[11/26 11:22:37 visual_prompt]: 	Training 500/553. train loss: 0.9672,	1.6943 s / batch. (data: 8.64e-01). ETA=21:53:15, max mem: 20.9 GB 
[11/26 11:23:29 visual_prompt]: Epoch 16 / 100: avg data time: 1.56e-01, avg batch time: 0.9808, average train loss: 0.7948
[11/26 11:24:26 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3068, average loss: 0.6926
[11/26 11:24:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.43	
[11/26 11:24:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[11/26 11:26:09 visual_prompt]: 	Training 100/553. train loss: 0.5978,	0.8400 s / batch. (data: 7.98e-03). ETA=10:48:54, max mem: 20.9 GB 
[11/26 11:27:50 visual_prompt]: 	Training 200/553. train loss: 0.9430,	0.8156 s / batch. (data: 2.79e-04). ETA=10:28:41, max mem: 20.9 GB 
[11/26 11:29:28 visual_prompt]: 	Training 300/553. train loss: 1.3583,	0.8274 s / batch. (data: 2.85e-04). ETA=10:36:26, max mem: 20.9 GB 
[11/26 11:31:08 visual_prompt]: 	Training 400/553. train loss: 0.8717,	0.9937 s / batch. (data: 1.83e-01). ETA=12:42:43, max mem: 20.9 GB 
[11/26 11:32:47 visual_prompt]: 	Training 500/553. train loss: 0.5718,	0.8320 s / batch. (data: 2.94e-04). ETA=10:37:13, max mem: 20.9 GB 
[11/26 11:33:41 visual_prompt]: Epoch 17 / 100: avg data time: 1.77e-01, avg batch time: 1.0034, average train loss: 0.7683
[11/26 11:34:38 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3094, average loss: 0.6881
[11/26 11:34:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.96	
[11/26 11:34:38 visual_prompt]: Best epoch 17: best metric: -0.688
[11/26 11:34:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[11/26 11:36:21 visual_prompt]: 	Training 100/553. train loss: 0.7365,	0.8369 s / batch. (data: 2.75e-04). ETA=10:38:49, max mem: 20.9 GB 
[11/26 11:38:02 visual_prompt]: 	Training 200/553. train loss: 0.8259,	0.8160 s / batch. (data: 3.21e-04). ETA=10:21:29, max mem: 20.9 GB 
[11/26 11:39:41 visual_prompt]: 	Training 300/553. train loss: 0.5632,	0.8440 s / batch. (data: 3.36e-04). ETA=10:41:23, max mem: 20.9 GB 
[11/26 11:41:20 visual_prompt]: 	Training 400/553. train loss: 0.7363,	0.8241 s / batch. (data: 5.66e-03). ETA=10:24:56, max mem: 20.9 GB 
[11/26 11:42:58 visual_prompt]: 	Training 500/553. train loss: 0.6900,	0.8520 s / batch. (data: 2.91e-04). ETA=10:44:39, max mem: 20.9 GB 
[11/26 11:43:48 visual_prompt]: Epoch 18 / 100: avg data time: 1.68e-01, avg batch time: 0.9949, average train loss: 0.8341
[11/26 11:44:45 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3084, average loss: 0.7493
[11/26 11:44:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.58	
[11/26 11:44:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[11/26 11:46:28 visual_prompt]: 	Training 100/553. train loss: 0.9425,	1.5182 s / batch. (data: 7.10e-01). ETA=19:04:52, max mem: 20.9 GB 
[11/26 11:48:06 visual_prompt]: 	Training 200/553. train loss: 0.7896,	0.8480 s / batch. (data: 1.20e-02). ETA=10:38:05, max mem: 20.9 GB 
[11/26 11:49:45 visual_prompt]: 	Training 300/553. train loss: 1.1253,	0.9160 s / batch. (data: 7.60e-02). ETA=11:27:42, max mem: 20.9 GB 
[11/26 11:51:26 visual_prompt]: 	Training 400/553. train loss: 0.5754,	0.8121 s / batch. (data: 7.12e-04). ETA=10:08:19, max mem: 20.9 GB 
[11/26 11:53:00 visual_prompt]: 	Training 500/553. train loss: 1.1973,	0.8207 s / batch. (data: 1.19e-02). ETA=10:13:26, max mem: 20.9 GB 
[11/26 11:53:52 visual_prompt]: Epoch 19 / 100: avg data time: 1.63e-01, avg batch time: 0.9887, average train loss: 0.7942
[11/26 11:54:48 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3082, average loss: 1.0754
[11/26 11:54:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.80	
[11/26 11:54:48 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[11/26 11:56:29 visual_prompt]: 	Training 100/553. train loss: 1.3739,	0.8240 s / batch. (data: 3.64e-04). ETA=10:13:46, max mem: 20.9 GB 
[11/26 11:58:09 visual_prompt]: 	Training 200/553. train loss: 0.5721,	0.8194 s / batch. (data: 3.30e-04). ETA=10:09:00, max mem: 20.9 GB 
[11/26 11:59:49 visual_prompt]: 	Training 300/553. train loss: 0.8490,	0.8244 s / batch. (data: 3.28e-04). ETA=10:11:19, max mem: 20.9 GB 
[11/26 12:01:27 visual_prompt]: 	Training 400/553. train loss: 0.6387,	0.8212 s / batch. (data: 1.05e-02). ETA=10:07:37, max mem: 20.9 GB 
[11/26 12:03:05 visual_prompt]: 	Training 500/553. train loss: 0.9564,	0.8800 s / batch. (data: 7.96e-03). ETA=10:49:37, max mem: 20.9 GB 
[11/26 12:03:58 visual_prompt]: Epoch 20 / 100: avg data time: 1.67e-01, avg batch time: 0.9933, average train loss: 0.8001
[11/26 12:04:54 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3070, average loss: 0.7979
[11/26 12:04:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.60	
[11/26 12:04:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[11/26 12:06:40 visual_prompt]: 	Training 100/553. train loss: 0.5625,	0.8301 s / batch. (data: 3.23e-04). ETA=10:10:40, max mem: 20.9 GB 
[11/26 12:08:18 visual_prompt]: 	Training 200/553. train loss: 0.6707,	0.8641 s / batch. (data: 2.41e-02). ETA=10:34:15, max mem: 20.9 GB 
[11/26 12:09:55 visual_prompt]: 	Training 300/553. train loss: 1.3704,	0.8400 s / batch. (data: 2.78e-04). ETA=10:15:11, max mem: 20.9 GB 
[11/26 12:11:34 visual_prompt]: 	Training 400/553. train loss: 0.8066,	0.8476 s / batch. (data: 2.86e-04). ETA=10:19:16, max mem: 20.9 GB 
[11/26 12:13:14 visual_prompt]: 	Training 500/553. train loss: 0.6989,	0.8254 s / batch. (data: 3.08e-04). ETA=10:01:44, max mem: 20.9 GB 
[11/26 12:14:05 visual_prompt]: Epoch 21 / 100: avg data time: 1.69e-01, avg batch time: 0.9952, average train loss: 0.7599
[11/26 12:15:01 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3076, average loss: 0.7515
[11/26 12:15:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/26 12:15:01 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[11/26 12:16:43 visual_prompt]: 	Training 100/553. train loss: 0.6949,	0.8220 s / batch. (data: 4.36e-04). ETA=9:57:08, max mem: 20.9 GB 
[11/26 12:18:22 visual_prompt]: 	Training 200/553. train loss: 0.6059,	0.8261 s / batch. (data: 9.59e-03). ETA=9:58:45, max mem: 20.9 GB 
[11/26 12:19:59 visual_prompt]: 	Training 300/553. train loss: 0.5261,	0.8459 s / batch. (data: 9.82e-03). ETA=10:11:39, max mem: 20.9 GB 
[11/26 12:21:38 visual_prompt]: 	Training 400/553. train loss: 0.8621,	0.8386 s / batch. (data: 5.42e-03). ETA=10:05:01, max mem: 20.9 GB 
[11/26 12:23:16 visual_prompt]: 	Training 500/553. train loss: 0.7520,	0.8320 s / batch. (data: 3.00e-04). ETA=9:58:53, max mem: 20.9 GB 
[11/26 12:24:09 visual_prompt]: Epoch 22 / 100: avg data time: 1.65e-01, avg batch time: 0.9901, average train loss: 0.7927
[11/26 12:25:06 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3081, average loss: 0.7243
[11/26 12:25:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.10	
[11/26 12:25:06 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[11/26 12:26:51 visual_prompt]: 	Training 100/553. train loss: 0.8870,	0.8260 s / batch. (data: 5.63e-03). ETA=9:52:24, max mem: 20.9 GB 
[11/26 12:28:35 visual_prompt]: 	Training 200/553. train loss: 0.8144,	0.8127 s / batch. (data: 3.34e-04). ETA=9:41:34, max mem: 20.9 GB 
[11/26 12:30:20 visual_prompt]: 	Training 300/553. train loss: 0.7957,	0.9099 s / batch. (data: 9.41e-03). ETA=10:49:34, max mem: 20.9 GB 
[11/26 12:31:58 visual_prompt]: 	Training 400/553. train loss: 0.5669,	0.8581 s / batch. (data: 8.26e-04). ETA=10:11:10, max mem: 20.9 GB 
[11/26 12:33:37 visual_prompt]: 	Training 500/553. train loss: 1.5836,	0.8319 s / batch. (data: 3.15e-04). ETA=9:51:05, max mem: 20.9 GB 
[11/26 12:34:29 visual_prompt]: Epoch 23 / 100: avg data time: 1.95e-01, avg batch time: 1.0194, average train loss: 0.8282
[11/26 12:35:28 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3068, average loss: 0.6939
[11/26 12:35:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.29	
[11/26 12:35:28 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[11/26 12:37:10 visual_prompt]: 	Training 100/553. train loss: 0.8737,	0.8624 s / batch. (data: 3.24e-02). ETA=10:10:35, max mem: 20.9 GB 
[11/26 12:38:49 visual_prompt]: 	Training 200/553. train loss: 0.5736,	0.8101 s / batch. (data: 2.96e-04). ETA=9:32:12, max mem: 20.9 GB 
[11/26 12:40:29 visual_prompt]: 	Training 300/553. train loss: 0.6920,	1.0360 s / batch. (data: 2.21e-01). ETA=12:10:03, max mem: 20.9 GB 
[11/26 12:42:09 visual_prompt]: 	Training 400/553. train loss: 0.5643,	0.8097 s / batch. (data: 3.30e-04). ETA=9:29:13, max mem: 20.9 GB 
[11/26 12:43:50 visual_prompt]: 	Training 500/553. train loss: 0.7896,	0.8105 s / batch. (data: 4.99e-04). ETA=9:28:28, max mem: 20.9 GB 
[11/26 12:44:43 visual_prompt]: Epoch 24 / 100: avg data time: 1.77e-01, avg batch time: 1.0025, average train loss: 0.7836
[11/26 12:45:39 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3096, average loss: 0.6891
[11/26 12:45:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.14	
[11/26 12:45:39 visual_prompt]: Stopping early.
[11/26 12:45:39 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 12:45:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 12:45:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/26 12:45:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 12:45:39 visual_prompt]: Training with config:
[11/26 12:45:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.1_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/26 12:45:39 visual_prompt]: Loading training data...
[11/26 12:45:39 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 12:45:39 visual_prompt]: Loading validation data...
[11/26 12:45:39 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 12:45:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 12:45:42 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 12:45:42 visual_prompt]: tuned percent:0.525
[11/26 12:45:43 visual_prompt]: Device used for model: 0
[11/26 12:45:43 visual_prompt]: Setting up Evaluator...
[11/26 12:45:43 visual_prompt]: Setting up Trainer...
[11/26 12:45:43 visual_prompt]: 	Setting up the optimizer...
[11/26 12:45:43 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 12:47:25 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8432 s / batch. (data: 2.10e-02). ETA=12:55:45, max mem: 20.9 GB 
[11/26 12:49:02 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8240 s / batch. (data: 3.01e-04). ETA=12:36:42, max mem: 20.9 GB 
[11/26 12:50:43 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0047 s / batch. (data: 1.87e-01). ETA=15:20:59, max mem: 20.9 GB 
[11/26 12:52:20 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8403 s / batch. (data: 1.56e-02). ETA=12:48:54, max mem: 20.9 GB 
[11/26 12:54:01 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8118 s / batch. (data: 3.12e-04). ETA=12:21:26, max mem: 20.9 GB 
[11/26 12:54:53 visual_prompt]: Epoch 1 / 100: avg data time: 1.68e-01, avg batch time: 0.9946, average train loss: 1.5403
[11/26 12:55:49 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3083, average loss: 1.5201
[11/26 12:55:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 12:55:49 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[11/26 12:57:31 visual_prompt]: 	Training 100/553. train loss: 0.6279,	1.0568 s / batch. (data: 2.47e-01). ETA=16:02:33, max mem: 20.9 GB 
[11/26 12:59:10 visual_prompt]: 	Training 200/553. train loss: 0.2577,	0.8124 s / batch. (data: 3.01e-04). ETA=12:18:32, max mem: 20.9 GB 
[11/26 13:00:50 visual_prompt]: 	Training 300/553. train loss: 0.7430,	1.0079 s / batch. (data: 1.68e-01). ETA=15:14:38, max mem: 20.9 GB 
[11/26 13:02:28 visual_prompt]: 	Training 400/553. train loss: 0.9585,	0.8685 s / batch. (data: 2.45e-02). ETA=13:06:41, max mem: 20.9 GB 
[11/26 13:04:08 visual_prompt]: 	Training 500/553. train loss: 0.6860,	0.8321 s / batch. (data: 2.62e-04). ETA=12:32:16, max mem: 20.9 GB 
[11/26 13:04:58 visual_prompt]: Epoch 2 / 100: avg data time: 1.67e-01, avg batch time: 0.9930, average train loss: 0.7769
[11/26 13:05:54 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3089, average loss: 0.7374
[11/26 13:05:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.00	
[11/26 13:05:54 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[11/26 13:07:36 visual_prompt]: 	Training 100/553. train loss: 0.7862,	0.8166 s / batch. (data: 2.78e-04). ETA=12:16:12, max mem: 20.9 GB 
[11/26 13:09:16 visual_prompt]: 	Training 200/553. train loss: 0.7773,	0.8344 s / batch. (data: 3.05e-04). ETA=12:30:51, max mem: 20.9 GB 
[11/26 13:10:54 visual_prompt]: 	Training 300/553. train loss: 0.7054,	0.8499 s / batch. (data: 2.20e-02). ETA=12:43:22, max mem: 20.9 GB 
[11/26 13:12:33 visual_prompt]: 	Training 400/553. train loss: 0.5651,	0.8123 s / batch. (data: 3.36e-04). ETA=12:08:15, max mem: 20.9 GB 
[11/26 13:14:13 visual_prompt]: 	Training 500/553. train loss: 0.7718,	1.4694 s / batch. (data: 6.31e-01). ETA=21:54:57, max mem: 20.9 GB 
[11/26 13:15:03 visual_prompt]: Epoch 3 / 100: avg data time: 1.66e-01, avg batch time: 0.9916, average train loss: 0.7468
[11/26 13:15:59 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3095, average loss: 0.7184
[11/26 13:15:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.24	
[11/26 13:15:59 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[11/26 13:17:44 visual_prompt]: 	Training 100/553. train loss: 0.6853,	0.8374 s / batch. (data: 5.45e-03). ETA=12:27:14, max mem: 20.9 GB 
[11/26 13:19:22 visual_prompt]: 	Training 200/553. train loss: 0.8473,	0.8440 s / batch. (data: 3.08e-04). ETA=12:31:44, max mem: 20.9 GB 
[11/26 13:21:01 visual_prompt]: 	Training 300/553. train loss: 0.5468,	1.6609 s / batch. (data: 8.30e-01). ETA=1 day, 0:36:36, max mem: 20.9 GB 
[11/26 13:22:36 visual_prompt]: 	Training 400/553. train loss: 0.6213,	1.3435 s / batch. (data: 4.98e-01). ETA=19:52:11, max mem: 20.9 GB 
[11/26 13:24:17 visual_prompt]: 	Training 500/553. train loss: 0.7584,	3.3480 s / batch. (data: 2.53e+00). ETA=2 days, 1:25:15, max mem: 20.9 GB 
[11/26 13:25:09 visual_prompt]: Epoch 4 / 100: avg data time: 1.69e-01, avg batch time: 0.9945, average train loss: 0.7848
[11/26 13:26:06 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3083, average loss: 0.6919
[11/26 13:26:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.59	
[11/26 13:26:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[11/26 13:27:48 visual_prompt]: 	Training 100/553. train loss: 0.4628,	0.8399 s / batch. (data: 9.63e-03). ETA=12:21:44, max mem: 20.9 GB 
[11/26 13:29:26 visual_prompt]: 	Training 200/553. train loss: 0.8532,	1.1440 s / batch. (data: 3.15e-01). ETA=16:48:22, max mem: 20.9 GB 
[11/26 13:31:07 visual_prompt]: 	Training 300/553. train loss: 0.9400,	0.8404 s / batch. (data: 1.05e-02). ETA=12:19:22, max mem: 20.9 GB 
[11/26 13:32:44 visual_prompt]: 	Training 400/553. train loss: 0.7033,	0.8200 s / batch. (data: 2.46e-04). ETA=12:00:06, max mem: 20.9 GB 
[11/26 13:34:24 visual_prompt]: 	Training 500/553. train loss: 0.6027,	0.8283 s / batch. (data: 3.13e-04). ETA=12:06:00, max mem: 20.9 GB 
[11/26 13:35:16 visual_prompt]: Epoch 5 / 100: avg data time: 1.68e-01, avg batch time: 0.9943, average train loss: 0.8043
[11/26 13:36:12 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3071, average loss: 0.7580
[11/26 13:36:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.73	
[11/26 13:36:12 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[11/26 13:37:56 visual_prompt]: 	Training 100/553. train loss: 0.5946,	0.8484 s / batch. (data: 3.50e-04). ETA=12:21:27, max mem: 20.9 GB 
[11/26 13:39:30 visual_prompt]: 	Training 200/553. train loss: 0.6098,	0.8208 s / batch. (data: 3.09e-04). ETA=11:55:58, max mem: 20.9 GB 
[11/26 13:41:03 visual_prompt]: 	Training 300/553. train loss: 0.5594,	0.8200 s / batch. (data: 3.02e-04). ETA=11:53:52, max mem: 20.9 GB 
[11/26 13:42:42 visual_prompt]: 	Training 400/553. train loss: 0.5339,	0.8359 s / batch. (data: 5.22e-04). ETA=12:06:19, max mem: 20.9 GB 
[11/26 13:44:16 visual_prompt]: 	Training 500/553. train loss: 0.7692,	0.8383 s / batch. (data: 3.19e-04). ETA=12:06:58, max mem: 20.9 GB 
[11/26 13:45:07 visual_prompt]: Epoch 6 / 100: avg data time: 1.41e-01, avg batch time: 0.9671, average train loss: 0.7513
[11/26 13:46:05 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3060, average loss: 0.6778
[11/26 13:46:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 60.26	
[11/26 13:46:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[11/26 13:47:49 visual_prompt]: 	Training 100/553. train loss: 0.5749,	0.8256 s / batch. (data: 2.98e-04). ETA=11:53:51, max mem: 20.9 GB 
[11/26 13:49:28 visual_prompt]: 	Training 200/553. train loss: 0.5493,	0.8379 s / batch. (data: 3.08e-04). ETA=12:03:10, max mem: 20.9 GB 
[11/26 13:51:08 visual_prompt]: 	Training 300/553. train loss: 0.7380,	0.8360 s / batch. (data: 5.46e-03). ETA=12:00:06, max mem: 20.9 GB 
[11/26 13:52:47 visual_prompt]: 	Training 400/553. train loss: 0.5772,	1.8592 s / batch. (data: 1.04e+00). ETA=1 day, 2:38:22, max mem: 20.9 GB 
[11/26 13:54:24 visual_prompt]: 	Training 500/553. train loss: 0.9787,	0.8281 s / batch. (data: 7.94e-03). ETA=11:50:33, max mem: 20.9 GB 
[11/26 13:55:13 visual_prompt]: Epoch 7 / 100: avg data time: 1.66e-01, avg batch time: 0.9915, average train loss: 0.7599
[11/26 13:56:09 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3080, average loss: 0.8091
[11/26 13:56:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.96	
[11/26 13:56:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[11/26 13:57:46 visual_prompt]: 	Training 100/553. train loss: 0.6921,	0.8240 s / batch. (data: 2.90e-04). ETA=11:44:55, max mem: 20.9 GB 
[11/26 13:59:23 visual_prompt]: 	Training 200/553. train loss: 1.2244,	0.8440 s / batch. (data: 3.12e-04). ETA=12:00:36, max mem: 20.9 GB 
[11/26 14:00:59 visual_prompt]: 	Training 300/553. train loss: 0.7496,	0.8602 s / batch. (data: 2.54e-02). ETA=12:13:00, max mem: 20.9 GB 
[11/26 14:02:37 visual_prompt]: 	Training 400/553. train loss: 0.7586,	1.0960 s / batch. (data: 2.72e-01). ETA=15:32:07, max mem: 20.9 GB 
[11/26 14:04:19 visual_prompt]: 	Training 500/553. train loss: 0.8660,	0.9720 s / batch. (data: 1.46e-01). ETA=13:45:03, max mem: 20.9 GB 
[11/26 14:05:13 visual_prompt]: Epoch 8 / 100: avg data time: 1.57e-01, avg batch time: 0.9839, average train loss: 0.7670
[11/26 14:06:11 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3090, average loss: 0.6925
[11/26 14:06:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.24	
[11/26 14:06:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[11/26 14:07:56 visual_prompt]: 	Training 100/553. train loss: 0.4150,	0.8440 s / batch. (data: 7.95e-03). ETA=11:54:14, max mem: 20.9 GB 
[11/26 14:09:37 visual_prompt]: 	Training 200/553. train loss: 0.6221,	0.8480 s / batch. (data: 5.39e-04). ETA=11:56:11, max mem: 20.9 GB 
[11/26 14:11:19 visual_prompt]: 	Training 300/553. train loss: 0.7428,	1.7760 s / batch. (data: 9.52e-01). ETA=1 day, 0:57:03, max mem: 20.9 GB 
[11/26 14:13:02 visual_prompt]: 	Training 400/553. train loss: 0.5888,	0.8294 s / batch. (data: 3.35e-04). ETA=11:37:46, max mem: 20.9 GB 
[11/26 14:14:43 visual_prompt]: 	Training 500/553. train loss: 0.7645,	1.0233 s / batch. (data: 2.12e-01). ETA=14:19:09, max mem: 20.9 GB 
[11/26 14:15:35 visual_prompt]: Epoch 9 / 100: avg data time: 1.94e-01, avg batch time: 1.0199, average train loss: 0.7426
[11/26 14:16:32 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3083, average loss: 0.7558
[11/26 14:16:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.53	
[11/26 14:16:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[11/26 14:18:18 visual_prompt]: 	Training 100/553. train loss: 0.6994,	0.8227 s / batch. (data: 3.06e-04). ETA=11:28:39, max mem: 20.9 GB 
[11/26 14:19:57 visual_prompt]: 	Training 200/553. train loss: 0.6694,	0.8229 s / batch. (data: 3.07e-04). ETA=11:27:26, max mem: 20.9 GB 
[11/26 14:21:36 visual_prompt]: 	Training 300/553. train loss: 0.6549,	1.3800 s / batch. (data: 5.43e-01). ETA=19:10:31, max mem: 20.9 GB 
[11/26 14:23:15 visual_prompt]: 	Training 400/553. train loss: 0.7412,	0.8273 s / batch. (data: 5.46e-03). ETA=11:28:21, max mem: 20.9 GB 
[11/26 14:24:57 visual_prompt]: 	Training 500/553. train loss: 1.0304,	0.8316 s / batch. (data: 3.02e-04). ETA=11:30:33, max mem: 20.9 GB 
[11/26 14:25:49 visual_prompt]: Epoch 10 / 100: avg data time: 1.80e-01, avg batch time: 1.0064, average train loss: 0.7587
[11/26 14:26:46 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3075, average loss: 0.9164
[11/26 14:26:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.02	
[11/26 14:26:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[11/26 14:28:32 visual_prompt]: 	Training 100/553. train loss: 0.6765,	0.8360 s / batch. (data: 7.95e-03). ETA=11:32:05, max mem: 20.9 GB 
[11/26 14:30:13 visual_prompt]: 	Training 200/553. train loss: 1.0358,	0.8097 s / batch. (data: 2.78e-04). ETA=11:08:54, max mem: 20.9 GB 
[11/26 14:31:52 visual_prompt]: 	Training 300/553. train loss: 0.6732,	2.0920 s / batch. (data: 1.25e+00). ETA=1 day, 4:44:52, max mem: 20.9 GB 
[11/26 14:33:31 visual_prompt]: 	Training 400/553. train loss: 0.7097,	0.8400 s / batch. (data: 7.97e-03). ETA=11:31:11, max mem: 20.9 GB 
[11/26 14:35:09 visual_prompt]: 	Training 500/553. train loss: 0.6885,	0.8166 s / batch. (data: 3.03e-04). ETA=11:10:34, max mem: 20.9 GB 
[11/26 14:36:00 visual_prompt]: Epoch 11 / 100: avg data time: 1.76e-01, avg batch time: 1.0014, average train loss: 0.7534
[11/26 14:36:57 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3085, average loss: 0.6948
[11/26 14:36:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.59	
[11/26 14:36:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[11/26 14:38:42 visual_prompt]: 	Training 100/553. train loss: 0.9692,	0.9814 s / batch. (data: 1.32e-01). ETA=13:23:21, max mem: 20.9 GB 
[11/26 14:40:22 visual_prompt]: 	Training 200/553. train loss: 0.5911,	1.7359 s / batch. (data: 9.27e-01). ETA=23:38:08, max mem: 20.9 GB 
[11/26 14:41:59 visual_prompt]: 	Training 300/553. train loss: 0.6782,	0.8360 s / batch. (data: 5.44e-03). ETA=11:21:35, max mem: 20.9 GB 
[11/26 14:43:38 visual_prompt]: 	Training 400/553. train loss: 0.7094,	0.8280 s / batch. (data: 3.30e-04). ETA=11:13:40, max mem: 20.9 GB 
[11/26 14:45:17 visual_prompt]: 	Training 500/553. train loss: 2.0742,	0.8236 s / batch. (data: 7.81e-04). ETA=11:08:45, max mem: 20.9 GB 
[11/26 14:46:07 visual_prompt]: Epoch 12 / 100: avg data time: 1.70e-01, avg batch time: 0.9951, average train loss: 0.7615
[11/26 14:47:03 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3100, average loss: 0.8491
[11/26 14:47:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.20	
[11/26 14:47:03 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[11/26 14:48:48 visual_prompt]: 	Training 100/553. train loss: 0.7742,	0.8534 s / batch. (data: 3.08e-04). ETA=11:30:45, max mem: 20.9 GB 
[11/26 14:50:24 visual_prompt]: 	Training 200/553. train loss: 0.6752,	0.8480 s / batch. (data: 2.88e-04). ETA=11:24:58, max mem: 20.9 GB 
[11/26 14:52:03 visual_prompt]: 	Training 300/553. train loss: 0.6694,	1.7280 s / batch. (data: 8.85e-01). ETA=23:12:54, max mem: 20.9 GB 
[11/26 14:53:40 visual_prompt]: 	Training 400/553. train loss: 0.9745,	0.8280 s / batch. (data: 2.93e-04). ETA=11:06:04, max mem: 20.9 GB 
[11/26 14:55:20 visual_prompt]: 	Training 500/553. train loss: 0.7368,	0.8674 s / batch. (data: 1.13e-02). ETA=11:36:15, max mem: 20.9 GB 
[11/26 14:56:12 visual_prompt]: Epoch 13 / 100: avg data time: 1.65e-01, avg batch time: 0.9919, average train loss: 0.7640
[11/26 14:57:09 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3074, average loss: 0.7014
[11/26 14:57:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.98	
[11/26 14:57:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[11/26 14:58:53 visual_prompt]: 	Training 100/553. train loss: 0.6431,	0.8375 s / batch. (data: 1.05e-02). ETA=11:10:08, max mem: 20.9 GB 
[11/26 15:00:31 visual_prompt]: 	Training 200/553. train loss: 0.7708,	1.3040 s / batch. (data: 4.80e-01). ETA=17:21:15, max mem: 20.9 GB 
[11/26 15:02:09 visual_prompt]: 	Training 300/553. train loss: 0.7119,	0.8253 s / batch. (data: 3.24e-04). ETA=10:57:39, max mem: 20.9 GB 
[11/26 15:03:47 visual_prompt]: 	Training 400/553. train loss: 0.6560,	0.8426 s / batch. (data: 4.82e-04). ETA=11:10:01, max mem: 20.9 GB 
[11/26 15:05:26 visual_prompt]: 	Training 500/553. train loss: 0.9734,	0.8249 s / batch. (data: 3.17e-04). ETA=10:54:32, max mem: 20.9 GB 
[11/26 15:06:17 visual_prompt]: Epoch 14 / 100: avg data time: 1.65e-01, avg batch time: 0.9917, average train loss: 0.7485
[11/26 15:07:13 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3065, average loss: 0.6943
[11/26 15:07:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.04	
[11/26 15:07:13 visual_prompt]: Best epoch 14: best metric: -0.694
[11/26 15:07:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[11/26 15:08:56 visual_prompt]: 	Training 100/553. train loss: 0.7109,	0.8160 s / batch. (data: 3.12e-04). ETA=10:45:26, max mem: 20.9 GB 
[11/26 15:10:34 visual_prompt]: 	Training 200/553. train loss: 0.6378,	0.8436 s / batch. (data: 3.59e-04). ETA=11:05:51, max mem: 20.9 GB 
[11/26 15:12:15 visual_prompt]: 	Training 300/553. train loss: 0.7343,	0.8127 s / batch. (data: 2.64e-04). ETA=10:40:04, max mem: 20.9 GB 
[11/26 15:13:51 visual_prompt]: 	Training 400/553. train loss: 0.5733,	1.0840 s / batch. (data: 2.46e-01). ETA=14:11:58, max mem: 20.9 GB 
[11/26 15:15:30 visual_prompt]: 	Training 500/553. train loss: 0.9717,	0.8376 s / batch. (data: 3.17e-04). ETA=10:56:56, max mem: 20.9 GB 
[11/26 15:16:23 visual_prompt]: Epoch 15 / 100: avg data time: 1.67e-01, avg batch time: 0.9931, average train loss: 0.7359
[11/26 15:17:19 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3070, average loss: 0.7089
[11/26 15:17:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.74	
[11/26 15:17:19 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[11/26 15:19:01 visual_prompt]: 	Training 100/553. train loss: 0.5607,	0.8283 s / batch. (data: 2.93e-04). ETA=10:47:31, max mem: 20.9 GB 
[11/26 15:20:39 visual_prompt]: 	Training 200/553. train loss: 0.8435,	0.8104 s / batch. (data: 3.05e-04). ETA=10:32:11, max mem: 20.9 GB 
[11/26 15:22:19 visual_prompt]: 	Training 300/553. train loss: 1.2283,	0.8526 s / batch. (data: 3.03e-04). ETA=11:03:39, max mem: 20.9 GB 
[11/26 15:23:57 visual_prompt]: 	Training 400/553. train loss: 0.6431,	0.8197 s / batch. (data: 3.06e-04). ETA=10:36:40, max mem: 20.9 GB 
[11/26 15:25:35 visual_prompt]: 	Training 500/553. train loss: 0.8496,	1.0497 s / batch. (data: 2.29e-01). ETA=13:33:37, max mem: 20.9 GB 
[11/26 15:26:27 visual_prompt]: Epoch 16 / 100: avg data time: 1.64e-01, avg batch time: 0.9897, average train loss: 0.7228
[11/26 15:27:24 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3084, average loss: 0.7196
[11/26 15:27:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/26 15:27:24 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[11/26 15:29:06 visual_prompt]: 	Training 100/553. train loss: 0.5634,	0.8280 s / batch. (data: 3.13e-04). ETA=10:39:38, max mem: 20.9 GB 
[11/26 15:30:46 visual_prompt]: 	Training 200/553. train loss: 0.6783,	0.8281 s / batch. (data: 2.81e-04). ETA=10:38:21, max mem: 20.9 GB 
[11/26 15:32:24 visual_prompt]: 	Training 300/553. train loss: 1.0435,	0.8400 s / batch. (data: 3.01e-04). ETA=10:46:06, max mem: 20.9 GB 
[11/26 15:34:03 visual_prompt]: 	Training 400/553. train loss: 0.6753,	1.2081 s / batch. (data: 3.70e-01). ETA=15:27:13, max mem: 20.9 GB 
[11/26 15:35:41 visual_prompt]: 	Training 500/553. train loss: 0.6440,	1.3872 s / batch. (data: 5.76e-01). ETA=17:42:25, max mem: 20.9 GB 
[11/26 15:36:34 visual_prompt]: Epoch 17 / 100: avg data time: 1.69e-01, avg batch time: 0.9948, average train loss: 0.7228
[11/26 15:37:30 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3074, average loss: 0.7145
[11/26 15:37:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.94	
[11/26 15:37:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[11/26 15:39:14 visual_prompt]: 	Training 100/553. train loss: 0.7472,	0.8621 s / batch. (data: 1.55e-02). ETA=10:58:03, max mem: 20.9 GB 
[11/26 15:40:54 visual_prompt]: 	Training 200/553. train loss: 0.7476,	0.8438 s / batch. (data: 1.05e-02). ETA=10:42:42, max mem: 20.9 GB 
[11/26 15:42:33 visual_prompt]: 	Training 300/553. train loss: 0.6563,	0.8280 s / batch. (data: 2.69e-04). ETA=10:29:16, max mem: 20.9 GB 
[11/26 15:44:12 visual_prompt]: 	Training 400/553. train loss: 0.6872,	0.8720 s / batch. (data: 5.46e-03). ETA=11:01:15, max mem: 20.9 GB 
[11/26 15:45:49 visual_prompt]: 	Training 500/553. train loss: 0.7037,	0.8280 s / batch. (data: 3.38e-04). ETA=10:26:30, max mem: 20.9 GB 
[11/26 15:46:40 visual_prompt]: Epoch 18 / 100: avg data time: 1.69e-01, avg batch time: 0.9939, average train loss: 0.7249
[11/26 15:47:36 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3084, average loss: 0.7495
[11/26 15:47:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.08	
[11/26 15:47:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[11/26 15:49:20 visual_prompt]: 	Training 100/553. train loss: 1.0922,	0.8364 s / batch. (data: 8.32e-03). ETA=10:30:42, max mem: 20.9 GB 
[11/26 15:51:02 visual_prompt]: 	Training 200/553. train loss: 0.7904,	0.8127 s / batch. (data: 3.01e-04). ETA=10:11:32, max mem: 20.9 GB 
[11/26 15:52:42 visual_prompt]: 	Training 300/553. train loss: 1.0872,	0.8411 s / batch. (data: 3.12e-04). ETA=10:31:27, max mem: 20.9 GB 
[11/26 15:54:24 visual_prompt]: 	Training 400/553. train loss: 0.5629,	0.8240 s / batch. (data: 2.91e-04). ETA=10:17:15, max mem: 20.9 GB 
[11/26 15:56:00 visual_prompt]: 	Training 500/553. train loss: 0.7644,	0.8359 s / batch. (data: 1.05e-02). ETA=10:24:46, max mem: 20.9 GB 
[11/26 15:56:52 visual_prompt]: Epoch 19 / 100: avg data time: 1.79e-01, avg batch time: 1.0048, average train loss: 0.7397
[11/26 15:57:50 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3099, average loss: 0.9610
[11/26 15:57:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.26	
[11/26 15:57:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[11/26 15:59:31 visual_prompt]: 	Training 100/553. train loss: 1.1063,	0.8237 s / batch. (data: 3.75e-03). ETA=10:13:35, max mem: 20.9 GB 
[11/26 16:01:11 visual_prompt]: 	Training 200/553. train loss: 0.5885,	0.8356 s / batch. (data: 7.95e-03). ETA=10:21:03, max mem: 20.9 GB 
[11/26 16:02:51 visual_prompt]: 	Training 300/553. train loss: 0.7721,	0.8237 s / batch. (data: 3.14e-04). ETA=10:10:48, max mem: 20.9 GB 
[11/26 16:04:30 visual_prompt]: 	Training 400/553. train loss: 0.5869,	0.8519 s / batch. (data: 1.29e-02). ETA=10:30:19, max mem: 20.9 GB 
[11/26 16:06:08 visual_prompt]: 	Training 500/553. train loss: 0.7745,	0.8164 s / batch. (data: 3.03e-04). ETA=10:02:39, max mem: 20.9 GB 
[11/26 16:07:01 visual_prompt]: Epoch 20 / 100: avg data time: 1.72e-01, avg batch time: 0.9973, average train loss: 0.7536
[11/26 16:07:58 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3082, average loss: 0.7681
[11/26 16:07:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.59	
[11/26 16:07:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[11/26 16:09:43 visual_prompt]: 	Training 100/553. train loss: 0.5750,	0.8297 s / batch. (data: 3.53e-04). ETA=10:10:23, max mem: 20.9 GB 
[11/26 16:11:22 visual_prompt]: 	Training 200/553. train loss: 0.6979,	0.8372 s / batch. (data: 5.43e-03). ETA=10:14:30, max mem: 20.9 GB 
[11/26 16:13:01 visual_prompt]: 	Training 300/553. train loss: 0.8871,	1.0015 s / batch. (data: 1.84e-01). ETA=12:13:24, max mem: 20.9 GB 
[11/26 16:14:39 visual_prompt]: 	Training 400/553. train loss: 0.6125,	0.8289 s / batch. (data: 1.05e-02). ETA=10:05:39, max mem: 20.9 GB 
[11/26 16:16:20 visual_prompt]: 	Training 500/553. train loss: 0.7041,	0.8393 s / batch. (data: 1.13e-02). ETA=10:11:52, max mem: 20.9 GB 
[11/26 16:17:10 visual_prompt]: Epoch 21 / 100: avg data time: 1.73e-01, avg batch time: 0.9980, average train loss: 0.7327
[11/26 16:18:07 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3068, average loss: 0.8394
[11/26 16:18:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.32	
[11/26 16:18:07 visual_prompt]: Stopping early.
[11/26 16:18:07 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 16:18:07 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 16:18:07 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/26 16:18:07 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 16:18:07 visual_prompt]: Training with config:
[11/26 16:18:07 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.1_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/26 16:18:07 visual_prompt]: Loading training data...
[11/26 16:18:07 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 16:18:07 visual_prompt]: Loading validation data...
[11/26 16:18:07 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 16:18:07 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 16:18:11 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 16:18:11 visual_prompt]: tuned percent:0.525
[11/26 16:18:12 visual_prompt]: Device used for model: 0
[11/26 16:18:12 visual_prompt]: Setting up Evaluator...
[11/26 16:18:12 visual_prompt]: Setting up Trainer...
[11/26 16:18:12 visual_prompt]: 	Setting up the optimizer...
[11/26 16:18:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 16:19:54 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8320 s / batch. (data: 1.20e-02). ETA=12:45:27, max mem: 20.9 GB 
[11/26 16:21:31 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8399 s / batch. (data: 3.14e-04). ETA=12:51:21, max mem: 20.9 GB 
[11/26 16:23:12 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.1599 s / batch. (data: 3.41e-01). ETA=17:43:12, max mem: 20.9 GB 
[11/26 16:24:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8360 s / batch. (data: 4.88e-03). ETA=12:44:58, max mem: 20.9 GB 
[11/26 16:26:29 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8361 s / batch. (data: 3.08e-04). ETA=12:43:36, max mem: 20.9 GB 
[11/26 16:27:21 visual_prompt]: Epoch 1 / 100: avg data time: 1.67e-01, avg batch time: 0.9929, average train loss: 1.5403
[11/26 16:28:17 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3078, average loss: 1.5201
[11/26 16:28:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 16:28:17 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[11/26 16:29:59 visual_prompt]: 	Training 100/553. train loss: 0.6265,	1.1382 s / batch. (data: 3.17e-01). ETA=17:16:41, max mem: 20.9 GB 
[11/26 16:31:38 visual_prompt]: 	Training 200/553. train loss: 0.2616,	0.8320 s / batch. (data: 3.15e-04). ETA=12:36:24, max mem: 20.9 GB 
[11/26 16:33:18 visual_prompt]: 	Training 300/553. train loss: 0.7625,	1.0452 s / batch. (data: 2.25e-01). ETA=15:48:30, max mem: 20.9 GB 
[11/26 16:34:56 visual_prompt]: 	Training 400/553. train loss: 0.9386,	0.8313 s / batch. (data: 1.20e-02). ETA=12:32:59, max mem: 20.9 GB 
[11/26 16:36:37 visual_prompt]: 	Training 500/553. train loss: 0.6741,	0.8491 s / batch. (data: 1.05e-02). ETA=12:47:41, max mem: 20.9 GB 
[11/26 16:37:27 visual_prompt]: Epoch 2 / 100: avg data time: 1.68e-01, avg batch time: 0.9941, average train loss: 0.7773
[11/26 16:38:24 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.3079, average loss: 0.7351
[11/26 16:38:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.42	
[11/26 16:38:24 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[11/26 16:40:05 visual_prompt]: 	Training 100/553. train loss: 0.8010,	0.8341 s / batch. (data: 5.44e-03). ETA=12:32:00, max mem: 20.9 GB 
[11/26 16:41:45 visual_prompt]: 	Training 200/553. train loss: 0.7660,	0.8578 s / batch. (data: 2.17e-02). ETA=12:51:53, max mem: 20.9 GB 
[11/26 16:43:23 visual_prompt]: 	Training 300/553. train loss: 0.5735,	0.8381 s / batch. (data: 2.79e-04). ETA=12:32:49, max mem: 20.9 GB 
[11/26 16:45:02 visual_prompt]: 	Training 400/553. train loss: 1.6354,	0.8520 s / batch. (data: 2.98e-04). ETA=12:43:54, max mem: 20.9 GB 
[11/26 16:46:42 visual_prompt]: 	Training 500/553. train loss: 0.8072,	1.3504 s / batch. (data: 5.12e-01). ETA=20:08:30, max mem: 20.9 GB 
[11/26 16:47:32 visual_prompt]: Epoch 3 / 100: avg data time: 1.65e-01, avg batch time: 0.9912, average train loss: 0.7552
[11/26 16:48:29 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3091, average loss: 0.7268
[11/26 16:48:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 58.34	
[11/26 16:48:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[11/26 16:50:13 visual_prompt]: 	Training 100/553. train loss: 0.6764,	0.8160 s / batch. (data: 2.95e-04). ETA=12:08:11, max mem: 20.9 GB 
[11/26 16:51:53 visual_prompt]: 	Training 200/553. train loss: 0.4517,	0.8293 s / batch. (data: 1.05e-02). ETA=12:18:37, max mem: 20.9 GB 
[11/26 16:53:32 visual_prompt]: 	Training 300/553. train loss: 0.6148,	1.6760 s / batch. (data: 8.60e-01). ETA=1 day, 0:50:00, max mem: 20.9 GB 
[11/26 16:55:07 visual_prompt]: 	Training 400/553. train loss: 0.5692,	0.8418 s / batch. (data: 3.51e-04). ETA=12:26:58, max mem: 20.9 GB 
[11/26 16:56:50 visual_prompt]: 	Training 500/553. train loss: 0.9406,	3.4200 s / batch. (data: 2.61e+00). ETA=2 days, 2:29:01, max mem: 20.9 GB 
[11/26 16:57:43 visual_prompt]: Epoch 4 / 100: avg data time: 1.76e-01, avg batch time: 1.0012, average train loss: 0.8193
[11/26 16:58:39 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3094, average loss: 0.7209
[11/26 16:58:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.16	
[11/26 16:58:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[11/26 17:00:22 visual_prompt]: 	Training 100/553. train loss: 0.4178,	0.8242 s / batch. (data: 3.12e-04). ETA=12:07:54, max mem: 20.9 GB 
[11/26 17:02:01 visual_prompt]: 	Training 200/553. train loss: 0.7487,	1.1067 s / batch. (data: 2.98e-01). ETA=16:15:30, max mem: 20.9 GB 
[11/26 17:03:41 visual_prompt]: 	Training 300/553. train loss: 0.9754,	0.8280 s / batch. (data: 2.80e-04). ETA=12:08:29, max mem: 20.9 GB 
[11/26 17:05:19 visual_prompt]: 	Training 400/553. train loss: 0.9884,	0.8338 s / batch. (data: 5.45e-03). ETA=12:12:10, max mem: 20.9 GB 
[11/26 17:06:58 visual_prompt]: 	Training 500/553. train loss: 0.5832,	0.8360 s / batch. (data: 3.08e-04). ETA=12:12:43, max mem: 20.9 GB 
[11/26 17:07:50 visual_prompt]: Epoch 5 / 100: avg data time: 1.71e-01, avg batch time: 0.9966, average train loss: 0.8314
[11/26 17:08:47 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.3067, average loss: 0.6827
[11/26 17:08:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.66	
[11/26 17:08:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[11/26 17:10:31 visual_prompt]: 	Training 100/553. train loss: 0.6242,	0.8293 s / batch. (data: 7.92e-04). ETA=12:04:42, max mem: 20.9 GB 
[11/26 17:12:09 visual_prompt]: 	Training 200/553. train loss: 0.5877,	0.8240 s / batch. (data: 2.99e-04). ETA=11:58:43, max mem: 20.9 GB 
[11/26 17:13:46 visual_prompt]: 	Training 300/553. train loss: 0.5429,	0.8128 s / batch. (data: 3.12e-04). ETA=11:47:36, max mem: 20.9 GB 
[11/26 17:15:29 visual_prompt]: 	Training 400/553. train loss: 0.5565,	0.8222 s / batch. (data: 3.26e-04). ETA=11:54:27, max mem: 20.9 GB 
[11/26 17:17:06 visual_prompt]: 	Training 500/553. train loss: 0.8099,	0.8200 s / batch. (data: 3.30e-04). ETA=11:51:08, max mem: 20.9 GB 
[11/26 17:17:57 visual_prompt]: Epoch 6 / 100: avg data time: 1.70e-01, avg batch time: 0.9948, average train loss: 0.7602
[11/26 17:18:54 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3076, average loss: 0.6798
[11/26 17:18:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.56	
[11/26 17:18:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[11/26 17:20:36 visual_prompt]: 	Training 100/553. train loss: 0.6582,	0.8200 s / batch. (data: 3.12e-04). ETA=11:49:02, max mem: 20.9 GB 
[11/26 17:22:14 visual_prompt]: 	Training 200/553. train loss: 0.5005,	0.8360 s / batch. (data: 7.99e-03). ETA=12:01:29, max mem: 20.9 GB 
[11/26 17:23:57 visual_prompt]: 	Training 300/553. train loss: 0.7510,	1.9324 s / batch. (data: 1.12e+00). ETA=1 day, 3:44:30, max mem: 20.9 GB 
[11/26 17:25:35 visual_prompt]: 	Training 400/553. train loss: 0.6171,	1.8880 s / batch. (data: 1.04e+00). ETA=1 day, 3:03:07, max mem: 20.9 GB 
[11/26 17:27:12 visual_prompt]: 	Training 500/553. train loss: 1.1128,	0.8223 s / batch. (data: 1.05e-02). ETA=11:45:33, max mem: 20.9 GB 
[11/26 17:28:03 visual_prompt]: Epoch 7 / 100: avg data time: 1.66e-01, avg batch time: 0.9922, average train loss: 0.7771
[11/26 17:28:59 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3075, average loss: 0.7506
[11/26 17:28:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.39	
[11/26 17:28:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[11/26 17:30:40 visual_prompt]: 	Training 100/553. train loss: 0.7150,	0.8445 s / batch. (data: 2.07e-02). ETA=12:02:24, max mem: 20.9 GB 
[11/26 17:32:21 visual_prompt]: 	Training 200/553. train loss: 1.3153,	0.8242 s / batch. (data: 5.47e-03). ETA=11:43:41, max mem: 20.9 GB 
[11/26 17:34:00 visual_prompt]: 	Training 300/553. train loss: 0.8282,	0.8558 s / batch. (data: 5.87e-03). ETA=12:09:17, max mem: 20.9 GB 
[11/26 17:35:39 visual_prompt]: 	Training 400/553. train loss: 0.6084,	0.8153 s / batch. (data: 3.40e-04). ETA=11:33:25, max mem: 20.9 GB 
[11/26 17:37:18 visual_prompt]: 	Training 500/553. train loss: 0.9699,	1.4076 s / batch. (data: 5.99e-01). ETA=19:54:48, max mem: 20.9 GB 
[11/26 17:38:10 visual_prompt]: Epoch 8 / 100: avg data time: 1.70e-01, avg batch time: 0.9958, average train loss: 0.7740
[11/26 17:39:07 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3078, average loss: 0.8966
[11/26 17:39:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.31	
[11/26 17:39:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[11/26 17:40:50 visual_prompt]: 	Training 100/553. train loss: 0.6805,	0.8288 s / batch. (data: 2.97e-04). ETA=11:41:23, max mem: 20.9 GB 
[11/26 17:42:27 visual_prompt]: 	Training 200/553. train loss: 0.6354,	0.8240 s / batch. (data: 5.43e-03). ETA=11:35:55, max mem: 20.9 GB 
[11/26 17:44:06 visual_prompt]: 	Training 300/553. train loss: 0.6115,	1.7680 s / batch. (data: 9.44e-01). ETA=1 day, 0:50:19, max mem: 20.9 GB 
[11/26 17:45:45 visual_prompt]: 	Training 400/553. train loss: 0.5351,	0.8316 s / batch. (data: 9.48e-03). ETA=11:39:34, max mem: 20.9 GB 
[11/26 17:47:25 visual_prompt]: 	Training 500/553. train loss: 0.9439,	0.9180 s / batch. (data: 8.56e-02). ETA=12:50:47, max mem: 20.9 GB 
[11/26 17:48:15 visual_prompt]: Epoch 9 / 100: avg data time: 1.65e-01, avg batch time: 0.9912, average train loss: 0.7596
[11/26 17:49:12 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3081, average loss: 0.7185
[11/26 17:49:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 64.10	
[11/26 17:49:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[11/26 17:50:57 visual_prompt]: 	Training 100/553. train loss: 0.6646,	0.8281 s / batch. (data: 2.41e-04). ETA=11:33:07, max mem: 20.9 GB 
[11/26 17:52:34 visual_prompt]: 	Training 200/553. train loss: 0.5143,	0.8440 s / batch. (data: 1.19e-02). ETA=11:45:02, max mem: 20.9 GB 
[11/26 17:54:11 visual_prompt]: 	Training 300/553. train loss: 0.4989,	0.8520 s / batch. (data: 3.18e-04). ETA=11:50:18, max mem: 20.9 GB 
[11/26 17:55:47 visual_prompt]: 	Training 400/553. train loss: 0.6560,	0.8320 s / batch. (data: 3.22e-04). ETA=11:32:14, max mem: 20.9 GB 
[11/26 17:57:28 visual_prompt]: 	Training 500/553. train loss: 1.9120,	0.8352 s / batch. (data: 3.07e-04). ETA=11:33:31, max mem: 20.9 GB 
[11/26 17:58:19 visual_prompt]: Epoch 10 / 100: avg data time: 1.64e-01, avg batch time: 0.9904, average train loss: 0.7746
[11/26 17:59:16 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3077, average loss: 1.0198
[11/26 17:59:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.63	
[11/26 17:59:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[11/26 18:01:01 visual_prompt]: 	Training 100/553. train loss: 0.5947,	0.8440 s / batch. (data: 3.12e-04). ETA=11:38:40, max mem: 20.9 GB 
[11/26 18:02:42 visual_prompt]: 	Training 200/553. train loss: 1.8940,	0.8169 s / batch. (data: 5.39e-03). ETA=11:14:53, max mem: 20.9 GB 
[11/26 18:04:20 visual_prompt]: 	Training 300/553. train loss: 0.7495,	1.9584 s / batch. (data: 1.12e+00). ETA=1 day, 2:54:43, max mem: 20.9 GB 
[11/26 18:05:57 visual_prompt]: 	Training 400/553. train loss: 0.7169,	0.8120 s / batch. (data: 3.15e-04). ETA=11:08:08, max mem: 20.9 GB 
[11/26 18:07:35 visual_prompt]: 	Training 500/553. train loss: 0.6069,	0.8221 s / batch. (data: 1.20e-02). ETA=11:15:04, max mem: 20.9 GB 
[11/26 18:08:26 visual_prompt]: Epoch 11 / 100: avg data time: 1.70e-01, avg batch time: 0.9953, average train loss: 0.7998
[11/26 18:09:23 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3094, average loss: 0.7757
[11/26 18:09:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 64.18	
[11/26 18:09:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[11/26 18:11:08 visual_prompt]: 	Training 100/553. train loss: 0.7897,	0.8202 s / batch. (data: 1.05e-02). ETA=11:11:24, max mem: 20.9 GB 
[11/26 18:12:48 visual_prompt]: 	Training 200/553. train loss: 0.6715,	0.8220 s / batch. (data: 3.10e-04). ETA=11:11:31, max mem: 20.9 GB 
[11/26 18:14:26 visual_prompt]: 	Training 300/553. train loss: 0.6489,	0.8196 s / batch. (data: 7.63e-03). ETA=11:08:13, max mem: 20.9 GB 
[11/26 18:16:05 visual_prompt]: 	Training 400/553. train loss: 1.0776,	0.8240 s / batch. (data: 3.33e-04). ETA=11:10:24, max mem: 20.9 GB 
[11/26 18:17:44 visual_prompt]: 	Training 500/553. train loss: 1.2660,	0.8400 s / batch. (data: 2.99e-04). ETA=11:22:02, max mem: 20.9 GB 
[11/26 18:18:33 visual_prompt]: Epoch 12 / 100: avg data time: 1.68e-01, avg batch time: 0.9934, average train loss: 0.7510
[11/26 18:19:27 visual_prompt]: Inference (val):avg data time: 5.63e-04, avg batch time: 0.3092, average loss: 1.2077
[11/26 18:19:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.69	
[11/26 18:19:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[11/26 18:21:08 visual_prompt]: 	Training 100/553. train loss: 0.7846,	0.8450 s / batch. (data: 2.73e-04). ETA=11:23:54, max mem: 20.9 GB 
[11/26 18:22:40 visual_prompt]: 	Training 200/553. train loss: 0.6277,	0.8320 s / batch. (data: 1.13e-02). ETA=11:12:03, max mem: 20.9 GB 
[11/26 18:24:15 visual_prompt]: 	Training 300/553. train loss: 0.7024,	1.3851 s / batch. (data: 5.76e-01). ETA=18:36:27, max mem: 20.9 GB 
[11/26 18:25:51 visual_prompt]: 	Training 400/553. train loss: 0.9833,	0.8236 s / batch. (data: 5.43e-03). ETA=11:02:31, max mem: 20.9 GB 
[11/26 18:27:29 visual_prompt]: 	Training 500/553. train loss: 0.6022,	0.8466 s / batch. (data: 1.45e-02). ETA=11:19:36, max mem: 20.9 GB 
[11/26 18:28:22 visual_prompt]: Epoch 13 / 100: avg data time: 1.42e-01, avg batch time: 0.9684, average train loss: 0.7644
[11/26 18:29:19 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3082, average loss: 0.6954
[11/26 18:29:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 63.55	
[11/26 18:29:19 visual_prompt]: Best epoch 13: best metric: -0.695
[11/26 18:29:19 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[11/26 18:31:01 visual_prompt]: 	Training 100/553. train loss: 0.7094,	0.8148 s / batch. (data: 2.93e-04). ETA=10:51:57, max mem: 20.9 GB 
[11/26 18:32:38 visual_prompt]: 	Training 200/553. train loss: 0.5219,	1.1138 s / batch. (data: 2.71e-01). ETA=14:49:23, max mem: 20.9 GB 
[11/26 18:34:15 visual_prompt]: 	Training 300/553. train loss: 0.7927,	0.8284 s / batch. (data: 3.03e-04). ETA=11:00:06, max mem: 20.9 GB 
[11/26 18:35:51 visual_prompt]: 	Training 400/553. train loss: 0.8142,	0.8400 s / batch. (data: 3.06e-04). ETA=11:07:56, max mem: 20.9 GB 
[11/26 18:37:28 visual_prompt]: 	Training 500/553. train loss: 0.9324,	0.8252 s / batch. (data: 3.04e-04). ETA=10:54:49, max mem: 20.9 GB 
[11/26 18:38:17 visual_prompt]: Epoch 14 / 100: avg data time: 1.45e-01, avg batch time: 0.9722, average train loss: 0.7312
[11/26 18:39:11 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3091, average loss: 0.7078
[11/26 18:39:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 67.65	
[11/26 18:39:11 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[11/26 18:40:51 visual_prompt]: 	Training 100/553. train loss: 0.8414,	0.8122 s / batch. (data: 3.04e-04). ETA=10:42:24, max mem: 20.9 GB 
[11/26 18:42:24 visual_prompt]: 	Training 200/553. train loss: 1.0052,	0.8212 s / batch. (data: 1.12e-03). ETA=10:48:11, max mem: 20.9 GB 
[11/26 18:44:01 visual_prompt]: 	Training 300/553. train loss: 0.4177,	0.8281 s / batch. (data: 4.51e-03). ETA=10:52:15, max mem: 20.9 GB 
[11/26 18:45:34 visual_prompt]: 	Training 400/553. train loss: 0.3850,	0.9846 s / batch. (data: 1.48e-01). ETA=12:53:49, max mem: 20.9 GB 
[11/26 18:47:10 visual_prompt]: 	Training 500/553. train loss: 0.8735,	0.8120 s / batch. (data: 3.15e-04). ETA=10:36:51, max mem: 20.9 GB 
[11/26 18:48:00 visual_prompt]: Epoch 15 / 100: avg data time: 1.30e-01, avg batch time: 0.9561, average train loss: 0.7632
[11/26 18:48:54 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3096, average loss: 0.8663
[11/26 18:48:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.07	
[11/26 18:48:54 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[11/26 18:50:33 visual_prompt]: 	Training 100/553. train loss: 0.4721,	0.8238 s / batch. (data: 2.74e-04). ETA=10:44:02, max mem: 20.9 GB 
[11/26 18:52:08 visual_prompt]: 	Training 200/553. train loss: 1.0872,	0.8480 s / batch. (data: 7.95e-03). ETA=11:01:30, max mem: 20.9 GB 
[11/26 18:53:43 visual_prompt]: 	Training 300/553. train loss: 1.1451,	0.8485 s / batch. (data: 1.24e-02). ETA=11:00:27, max mem: 20.9 GB 
[11/26 18:55:18 visual_prompt]: 	Training 400/553. train loss: 0.3235,	0.8456 s / batch. (data: 1.05e-02). ETA=10:56:49, max mem: 20.9 GB 
[11/26 18:56:53 visual_prompt]: 	Training 500/553. train loss: 1.2523,	1.0656 s / batch. (data: 2.29e-01). ETA=13:45:55, max mem: 20.9 GB 
[11/26 18:57:43 visual_prompt]: Epoch 16 / 100: avg data time: 1.30e-01, avg batch time: 0.9557, average train loss: 0.7254
[11/26 18:58:37 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3081, average loss: 0.6363
[11/26 18:58:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 69.57	
[11/26 18:58:37 visual_prompt]: Best epoch 16: best metric: -0.636
[11/26 18:58:37 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[11/26 19:00:16 visual_prompt]: 	Training 100/553. train loss: 0.3604,	0.8115 s / batch. (data: 3.01e-04). ETA=10:26:54, max mem: 20.9 GB 
[11/26 19:01:52 visual_prompt]: 	Training 200/553. train loss: 0.9815,	0.8208 s / batch. (data: 5.32e-03). ETA=10:32:43, max mem: 20.9 GB 
[11/26 19:03:26 visual_prompt]: 	Training 300/553. train loss: 1.2304,	0.8395 s / batch. (data: 3.04e-04). ETA=10:45:46, max mem: 20.9 GB 
[11/26 19:05:01 visual_prompt]: 	Training 400/553. train loss: 0.6718,	1.1720 s / batch. (data: 3.48e-01). ETA=14:59:33, max mem: 20.9 GB 
[11/26 19:06:35 visual_prompt]: 	Training 500/553. train loss: 0.5687,	0.8172 s / batch. (data: 3.17e-04). ETA=10:25:53, max mem: 20.9 GB 
[11/26 19:07:26 visual_prompt]: Epoch 17 / 100: avg data time: 1.30e-01, avg batch time: 0.9558, average train loss: 0.6980
[11/26 19:08:20 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3081, average loss: 0.7770
[11/26 19:08:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.62	
[11/26 19:08:20 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[11/26 19:09:59 visual_prompt]: 	Training 100/553. train loss: 0.5526,	0.8240 s / batch. (data: 2.90e-04). ETA=10:28:58, max mem: 20.9 GB 
[11/26 19:11:37 visual_prompt]: 	Training 200/553. train loss: 0.6147,	0.8320 s / batch. (data: 7.93e-03). ETA=10:33:42, max mem: 20.9 GB 
[11/26 19:13:12 visual_prompt]: 	Training 300/553. train loss: 0.4375,	0.8280 s / batch. (data: 2.91e-04). ETA=10:29:14, max mem: 20.9 GB 
[11/26 19:14:47 visual_prompt]: 	Training 400/553. train loss: 0.7208,	0.8280 s / batch. (data: 3.01e-04). ETA=10:27:52, max mem: 20.9 GB 
[11/26 19:16:21 visual_prompt]: 	Training 500/553. train loss: 0.6741,	0.8120 s / batch. (data: 2.82e-04). ETA=10:14:24, max mem: 20.9 GB 
[11/26 19:17:10 visual_prompt]: Epoch 18 / 100: avg data time: 1.32e-01, avg batch time: 0.9584, average train loss: 0.7153
[11/26 19:18:04 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3080, average loss: 0.6302
[11/26 19:18:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 70.32	
[11/26 19:18:04 visual_prompt]: Best epoch 18: best metric: -0.630
[11/26 19:18:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[11/26 19:19:44 visual_prompt]: 	Training 100/553. train loss: 1.2634,	0.8200 s / batch. (data: 7.94e-03). ETA=10:18:21, max mem: 20.9 GB 
[11/26 19:21:21 visual_prompt]: 	Training 200/553. train loss: 0.5345,	0.8232 s / batch. (data: 1.14e-02). ETA=10:19:23, max mem: 20.9 GB 
[11/26 19:22:58 visual_prompt]: 	Training 300/553. train loss: 0.4904,	0.8200 s / batch. (data: 3.07e-04). ETA=10:15:36, max mem: 20.9 GB 
[11/26 19:24:36 visual_prompt]: 	Training 400/553. train loss: 0.3932,	0.8177 s / batch. (data: 5.42e-03). ETA=10:12:34, max mem: 20.9 GB 
[11/26 19:26:08 visual_prompt]: 	Training 500/553. train loss: 0.7870,	0.8475 s / batch. (data: 1.56e-02). ETA=10:33:28, max mem: 20.9 GB 
[11/26 19:26:59 visual_prompt]: Epoch 19 / 100: avg data time: 1.40e-01, avg batch time: 0.9676, average train loss: 0.7050
[11/26 19:27:55 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3077, average loss: 0.7530
[11/26 19:27:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.12	
[11/26 19:27:55 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[11/26 19:29:32 visual_prompt]: 	Training 100/553. train loss: 1.6583,	0.8400 s / batch. (data: 7.95e-03). ETA=10:25:41, max mem: 20.9 GB 
[11/26 19:31:09 visual_prompt]: 	Training 200/553. train loss: 0.3161,	0.8280 s / batch. (data: 3.22e-04). ETA=10:15:22, max mem: 20.9 GB 
[11/26 19:32:45 visual_prompt]: 	Training 300/553. train loss: 0.7349,	0.8802 s / batch. (data: 1.64e-02). ETA=10:52:44, max mem: 20.9 GB 
[11/26 19:34:20 visual_prompt]: 	Training 400/553. train loss: 0.6471,	0.8280 s / batch. (data: 5.44e-03). ETA=10:12:37, max mem: 20.9 GB 
[11/26 19:35:54 visual_prompt]: 	Training 500/553. train loss: 1.3095,	0.8360 s / batch. (data: 3.01e-04). ETA=10:17:08, max mem: 20.9 GB 
[11/26 19:36:45 visual_prompt]: Epoch 20 / 100: avg data time: 1.33e-01, avg batch time: 0.9595, average train loss: 0.7344
[11/26 19:37:40 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3089, average loss: 0.6733
[11/26 19:37:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.66	
[11/26 19:37:40 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[11/26 19:39:21 visual_prompt]: 	Training 100/553. train loss: 0.4429,	0.8280 s / batch. (data: 7.95e-03). ETA=10:09:08, max mem: 20.9 GB 
[11/26 19:40:56 visual_prompt]: 	Training 200/553. train loss: 0.3585,	0.8284 s / batch. (data: 2.77e-04). ETA=10:08:00, max mem: 20.9 GB 
[11/26 19:42:33 visual_prompt]: 	Training 300/553. train loss: 1.0149,	0.8812 s / batch. (data: 2.93e-02). ETA=10:45:19, max mem: 20.9 GB 
[11/26 19:44:07 visual_prompt]: 	Training 400/553. train loss: 0.5129,	0.8524 s / batch. (data: 1.05e-02). ETA=10:22:51, max mem: 20.9 GB 
[11/26 19:45:43 visual_prompt]: 	Training 500/553. train loss: 0.6940,	0.8386 s / batch. (data: 1.05e-02). ETA=10:11:20, max mem: 20.9 GB 
[11/26 19:46:32 visual_prompt]: Epoch 21 / 100: avg data time: 1.36e-01, avg batch time: 0.9628, average train loss: 0.6990
[11/26 19:47:32 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3065, average loss: 0.6145
[11/26 19:47:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 73.59	
[11/26 19:47:32 visual_prompt]: Best epoch 21: best metric: -0.614
[11/26 19:47:32 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[11/26 19:49:18 visual_prompt]: 	Training 100/553. train loss: 0.7278,	0.8240 s / batch. (data: 2.92e-04). ETA=9:58:35, max mem: 20.9 GB 
[11/26 19:50:55 visual_prompt]: 	Training 200/553. train loss: 0.4200,	0.8404 s / batch. (data: 2.34e-02). ETA=10:09:08, max mem: 20.9 GB 
[11/26 19:52:28 visual_prompt]: 	Training 300/553. train loss: 0.2538,	0.8120 s / batch. (data: 3.02e-04). ETA=9:47:10, max mem: 20.9 GB 
[11/26 19:54:04 visual_prompt]: 	Training 400/553. train loss: 0.7925,	0.8403 s / batch. (data: 1.19e-02). ETA=10:06:12, max mem: 20.9 GB 
[11/26 19:55:40 visual_prompt]: 	Training 500/553. train loss: 1.0197,	0.8209 s / batch. (data: 3.05e-04). ETA=9:50:51, max mem: 20.9 GB 
[11/26 19:56:32 visual_prompt]: Epoch 22 / 100: avg data time: 1.49e-01, avg batch time: 0.9759, average train loss: 0.7016
[11/26 19:57:27 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3067, average loss: 0.6464
[11/26 19:57:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 73.45	
[11/26 19:57:27 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[11/26 19:59:08 visual_prompt]: 	Training 100/553. train loss: 0.8485,	1.0005 s / batch. (data: 1.87e-01). ETA=11:57:37, max mem: 20.9 GB 
[11/26 20:00:46 visual_prompt]: 	Training 200/553. train loss: 0.6734,	0.8260 s / batch. (data: 3.10e-04). ETA=9:51:01, max mem: 20.9 GB 
[11/26 20:02:23 visual_prompt]: 	Training 300/553. train loss: 1.5267,	0.8320 s / batch. (data: 7.85e-04). ETA=9:53:58, max mem: 20.9 GB 
[11/26 20:04:00 visual_prompt]: 	Training 400/553. train loss: 0.5009,	0.8337 s / batch. (data: 5.87e-03). ETA=9:53:46, max mem: 20.9 GB 
[11/26 20:05:39 visual_prompt]: 	Training 500/553. train loss: 0.5974,	0.8320 s / batch. (data: 2.99e-04). ETA=9:51:11, max mem: 20.9 GB 
[11/26 20:06:30 visual_prompt]: Epoch 23 / 100: avg data time: 1.56e-01, avg batch time: 0.9821, average train loss: 0.6758
[11/26 20:07:25 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3076, average loss: 0.6746
[11/26 20:07:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 72.25	
[11/26 20:07:25 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[11/26 20:09:04 visual_prompt]: 	Training 100/553. train loss: 0.9864,	0.8258 s / batch. (data: 3.17e-04). ETA=9:44:40, max mem: 20.9 GB 
[11/26 20:10:41 visual_prompt]: 	Training 200/553. train loss: 0.8586,	0.8320 s / batch. (data: 2.99e-04). ETA=9:47:41, max mem: 20.9 GB 
[11/26 20:12:18 visual_prompt]: 	Training 300/553. train loss: 0.6382,	0.8398 s / batch. (data: 1.22e-02). ETA=9:51:48, max mem: 20.9 GB 
[11/26 20:13:54 visual_prompt]: 	Training 400/553. train loss: 0.3481,	0.8207 s / batch. (data: 3.30e-04). ETA=9:36:57, max mem: 20.9 GB 
[11/26 20:15:32 visual_prompt]: 	Training 500/553. train loss: 0.5297,	0.8440 s / batch. (data: 3.45e-04). ETA=9:51:57, max mem: 20.9 GB 
[11/26 20:16:23 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9736, average train loss: 0.7003
[11/26 20:17:18 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3064, average loss: 0.6441
[11/26 20:17:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 69.99	
[11/26 20:17:18 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[11/26 20:19:02 visual_prompt]: 	Training 100/553. train loss: 0.6753,	0.8196 s / batch. (data: 7.96e-03). ETA=9:32:45, max mem: 20.9 GB 
[11/26 20:20:35 visual_prompt]: 	Training 200/553. train loss: 1.3931,	0.9324 s / batch. (data: 1.23e-01). ETA=10:50:00, max mem: 20.9 GB 
[11/26 20:22:11 visual_prompt]: 	Training 300/553. train loss: 0.5303,	0.8160 s / batch. (data: 3.24e-04). ETA=9:27:30, max mem: 20.9 GB 
[11/26 20:23:47 visual_prompt]: 	Training 400/553. train loss: 0.7206,	1.2912 s / batch. (data: 4.69e-01). ETA=14:55:50, max mem: 20.9 GB 
[11/26 20:25:24 visual_prompt]: 	Training 500/553. train loss: 0.7247,	1.3182 s / batch. (data: 5.02e-01). ETA=15:12:20, max mem: 20.9 GB 
[11/26 20:26:15 visual_prompt]: Epoch 25 / 100: avg data time: 1.42e-01, avg batch time: 0.9698, average train loss: 0.6811
[11/26 20:27:10 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3089, average loss: 0.6921
[11/26 20:27:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 71.80	
[11/26 20:27:10 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[11/26 20:28:50 visual_prompt]: 	Training 100/553. train loss: 0.2271,	0.8341 s / batch. (data: 1.05e-02). ETA=9:35:11, max mem: 20.9 GB 
[11/26 20:30:27 visual_prompt]: 	Training 200/553. train loss: 0.6226,	1.6276 s / batch. (data: 7.95e-01). ETA=18:39:37, max mem: 20.9 GB 
[11/26 20:32:05 visual_prompt]: 	Training 300/553. train loss: 0.7106,	0.8360 s / batch. (data: 2.91e-04). ETA=9:33:43, max mem: 20.9 GB 
[11/26 20:33:40 visual_prompt]: 	Training 400/553. train loss: 0.5168,	0.8172 s / batch. (data: 5.42e-03). ETA=9:19:27, max mem: 20.9 GB 
[11/26 20:35:15 visual_prompt]: 	Training 500/553. train loss: 0.4132,	0.8553 s / batch. (data: 1.53e-02). ETA=9:44:07, max mem: 20.9 GB 
[11/26 20:36:05 visual_prompt]: Epoch 26 / 100: avg data time: 1.41e-01, avg batch time: 0.9678, average train loss: 0.6779
[11/26 20:37:00 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3087, average loss: 0.6556
[11/26 20:37:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 71.91	
[11/26 20:37:00 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[11/26 20:38:41 visual_prompt]: 	Training 100/553. train loss: 0.6711,	0.8240 s / batch. (data: 3.02e-04). ETA=9:20:37, max mem: 20.9 GB 
[11/26 20:40:16 visual_prompt]: 	Training 200/553. train loss: 0.5084,	1.2130 s / batch. (data: 3.81e-01). ETA=13:43:15, max mem: 20.9 GB 
[11/26 20:41:53 visual_prompt]: 	Training 300/553. train loss: 0.6906,	0.8326 s / batch. (data: 3.31e-04). ETA=9:23:39, max mem: 20.9 GB 
[11/26 20:43:31 visual_prompt]: 	Training 400/553. train loss: 0.8445,	0.8185 s / batch. (data: 8.02e-03). ETA=9:12:47, max mem: 20.9 GB 
[11/26 20:45:08 visual_prompt]: 	Training 500/553. train loss: 0.7647,	0.8358 s / batch. (data: 5.42e-03). ETA=9:23:03, max mem: 20.9 GB 
[11/26 20:45:56 visual_prompt]: Epoch 27 / 100: avg data time: 1.43e-01, avg batch time: 0.9695, average train loss: 0.6682
[11/26 20:46:51 visual_prompt]: Inference (val):avg data time: 1.98e-04, avg batch time: 0.3081, average loss: 0.7004
[11/26 20:46:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 75.26	
[11/26 20:46:51 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[11/26 20:48:30 visual_prompt]: 	Training 100/553. train loss: 0.1672,	1.0118 s / batch. (data: 1.90e-01). ETA=11:19:05, max mem: 20.9 GB 
[11/26 20:50:08 visual_prompt]: 	Training 200/553. train loss: 0.3424,	0.8154 s / batch. (data: 3.09e-04). ETA=9:05:54, max mem: 20.9 GB 
[11/26 20:51:44 visual_prompt]: 	Training 300/553. train loss: 0.6658,	1.4320 s / batch. (data: 6.18e-01). ETA=15:56:18, max mem: 20.9 GB 
[11/26 20:53:19 visual_prompt]: 	Training 400/553. train loss: 0.5723,	0.8130 s / batch. (data: 2.94e-04). ETA=9:01:35, max mem: 20.9 GB 
[11/26 20:54:54 visual_prompt]: 	Training 500/553. train loss: 0.4167,	0.8200 s / batch. (data: 3.41e-04). ETA=9:04:51, max mem: 20.9 GB 
[11/26 20:55:45 visual_prompt]: Epoch 28 / 100: avg data time: 1.39e-01, avg batch time: 0.9658, average train loss: 0.6667
[11/26 20:56:40 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3079, average loss: 0.7336
[11/26 20:56:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 72.38	
[11/26 20:56:40 visual_prompt]: Stopping early.
[11/26 20:56:41 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 20:56:41 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 20:56:41 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/26 20:56:41 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 20:56:41 visual_prompt]: Training with config:
[11/26 20:56:41 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.1_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/26 20:56:41 visual_prompt]: Loading training data...
[11/26 20:56:41 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 20:56:41 visual_prompt]: Loading validation data...
[11/26 20:56:41 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 20:56:41 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 20:56:47 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 20:56:47 visual_prompt]: tuned percent:0.525
[11/26 20:56:47 visual_prompt]: Device used for model: 0
[11/26 20:56:47 visual_prompt]: Setting up Evaluator...
[11/26 20:56:47 visual_prompt]: Setting up Trainer...
[11/26 20:56:47 visual_prompt]: 	Setting up the optimizer...
[11/26 20:56:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 20:58:27 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8280 s / batch. (data: 7.96e-03). ETA=12:41:46, max mem: 20.9 GB 
[11/26 21:00:01 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8280 s / batch. (data: 2.95e-04). ETA=12:40:23, max mem: 20.9 GB 
[11/26 21:01:39 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3619 s / batch. (data: 5.53e-01). ETA=20:48:27, max mem: 20.9 GB 
[11/26 21:03:12 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8706 s / batch. (data: 2.27e-02). ETA=13:16:38, max mem: 20.9 GB 
[11/26 21:04:50 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8160 s / batch. (data: 3.00e-04). ETA=12:25:16, max mem: 20.9 GB 
[11/26 21:05:40 visual_prompt]: Epoch 1 / 100: avg data time: 1.36e-01, avg batch time: 0.9625, average train loss: 1.5403
[11/26 21:06:34 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3078, average loss: 1.5201
[11/26 21:06:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 21:06:34 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[11/26 21:08:14 visual_prompt]: 	Training 100/553. train loss: 0.6264,	0.8315 s / batch. (data: 3.06e-04). ETA=12:37:19, max mem: 20.9 GB 
[11/26 21:09:49 visual_prompt]: 	Training 200/553. train loss: 0.2617,	0.8134 s / batch. (data: 3.14e-04). ETA=12:19:26, max mem: 20.9 GB 
[11/26 21:11:26 visual_prompt]: 	Training 300/553. train loss: 0.7624,	0.9667 s / batch. (data: 1.28e-01). ETA=14:37:15, max mem: 20.9 GB 
[11/26 21:13:01 visual_prompt]: 	Training 400/553. train loss: 0.9390,	0.8488 s / batch. (data: 1.05e-02). ETA=12:48:48, max mem: 20.9 GB 
[11/26 21:14:39 visual_prompt]: 	Training 500/553. train loss: 0.6747,	0.8480 s / batch. (data: 7.95e-03). ETA=12:46:40, max mem: 20.9 GB 
[11/26 21:15:27 visual_prompt]: Epoch 2 / 100: avg data time: 1.37e-01, avg batch time: 0.9639, average train loss: 0.7774
[11/26 21:16:21 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3066, average loss: 0.7374
[11/26 21:16:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.51	
[11/26 21:16:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[11/26 21:17:59 visual_prompt]: 	Training 100/553. train loss: 0.7944,	0.8501 s / batch. (data: 3.77e-04). ETA=12:46:22, max mem: 20.9 GB 
[11/26 21:19:35 visual_prompt]: 	Training 200/553. train loss: 0.7782,	0.8208 s / batch. (data: 5.44e-03). ETA=12:18:38, max mem: 20.9 GB 
[11/26 21:21:10 visual_prompt]: 	Training 300/553. train loss: 0.5651,	0.8248 s / batch. (data: 2.95e-04). ETA=12:20:54, max mem: 20.9 GB 
[11/26 21:22:49 visual_prompt]: 	Training 400/553. train loss: 1.7438,	0.8536 s / batch. (data: 1.56e-02). ETA=12:45:17, max mem: 20.9 GB 
[11/26 21:24:29 visual_prompt]: 	Training 500/553. train loss: 0.7807,	1.3245 s / batch. (data: 5.02e-01). ETA=19:45:20, max mem: 20.9 GB 
[11/26 21:25:20 visual_prompt]: Epoch 3 / 100: avg data time: 1.48e-01, avg batch time: 0.9736, average train loss: 0.7553
[11/26 21:26:18 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3076, average loss: 0.7224
[11/26 21:26:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 58.10	
[11/26 21:26:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[11/26 21:28:02 visual_prompt]: 	Training 100/553. train loss: 0.6357,	0.8413 s / batch. (data: 4.83e-04). ETA=12:30:41, max mem: 20.9 GB 
[11/26 21:29:41 visual_prompt]: 	Training 200/553. train loss: 0.8901,	0.8520 s / batch. (data: 1.20e-02). ETA=12:38:51, max mem: 20.9 GB 
[11/26 21:31:19 visual_prompt]: 	Training 300/553. train loss: 0.5571,	0.8803 s / batch. (data: 5.61e-02). ETA=13:02:33, max mem: 20.9 GB 
[11/26 21:32:53 visual_prompt]: 	Training 400/553. train loss: 0.6062,	0.8259 s / batch. (data: 1.05e-02). ETA=12:12:49, max mem: 20.9 GB 
[11/26 21:34:34 visual_prompt]: 	Training 500/553. train loss: 0.9240,	3.2918 s / batch. (data: 2.48e+00). ETA=2 days, 0:35:31, max mem: 20.9 GB 
[11/26 21:35:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.67e-01, avg batch time: 0.9917, average train loss: 0.8020
[11/26 21:36:21 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3058, average loss: 0.6967
[11/26 21:36:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.55	
[11/26 21:36:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[11/26 21:38:01 visual_prompt]: 	Training 100/553. train loss: 0.4366,	0.8113 s / batch. (data: 2.89e-04). ETA=11:56:29, max mem: 20.9 GB 
[11/26 21:39:37 visual_prompt]: 	Training 200/553. train loss: 0.7742,	1.1764 s / batch. (data: 3.36e-01). ETA=17:16:55, max mem: 20.9 GB 
[11/26 21:41:15 visual_prompt]: 	Training 300/553. train loss: 0.8981,	0.8397 s / batch. (data: 2.85e-04). ETA=12:18:47, max mem: 20.9 GB 
[11/26 21:42:50 visual_prompt]: 	Training 400/553. train loss: 0.9549,	0.8480 s / batch. (data: 2.86e-04). ETA=12:24:40, max mem: 20.9 GB 
[11/26 21:44:27 visual_prompt]: 	Training 500/553. train loss: 0.6033,	0.8320 s / batch. (data: 2.96e-04). ETA=12:09:12, max mem: 20.9 GB 
[11/26 21:45:18 visual_prompt]: Epoch 5 / 100: avg data time: 1.43e-01, avg batch time: 0.9694, average train loss: 0.8214
[11/26 21:46:13 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3077, average loss: 0.7553
[11/26 21:46:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.47	
[11/26 21:46:13 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[11/26 21:47:54 visual_prompt]: 	Training 100/553. train loss: 0.6362,	0.8156 s / batch. (data: 5.42e-03). ETA=11:52:47, max mem: 20.9 GB 
[11/26 21:49:30 visual_prompt]: 	Training 200/553. train loss: 0.5682,	0.8284 s / batch. (data: 1.59e-02). ETA=12:02:32, max mem: 20.9 GB 
[11/26 21:51:05 visual_prompt]: 	Training 300/553. train loss: 0.5493,	0.8238 s / batch. (data: 3.05e-04). ETA=11:57:11, max mem: 20.9 GB 
[11/26 21:52:45 visual_prompt]: 	Training 400/553. train loss: 0.5430,	0.8242 s / batch. (data: 3.34e-04). ETA=11:56:10, max mem: 20.9 GB 
[11/26 21:54:20 visual_prompt]: 	Training 500/553. train loss: 0.8655,	0.8440 s / batch. (data: 3.30e-04). ETA=12:11:57, max mem: 20.9 GB 
[11/26 21:55:10 visual_prompt]: Epoch 6 / 100: avg data time: 1.47e-01, avg batch time: 0.9719, average train loss: 0.7631
[11/26 21:56:06 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.3078, average loss: 0.6751
[11/26 21:56:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 62.26	
[11/26 21:56:06 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[11/26 21:57:47 visual_prompt]: 	Training 100/553. train loss: 0.7818,	0.8280 s / batch. (data: 5.44e-03). ETA=11:55:57, max mem: 20.9 GB 
[11/26 21:59:23 visual_prompt]: 	Training 200/553. train loss: 0.5034,	0.8168 s / batch. (data: 2.58e-03). ETA=11:44:56, max mem: 20.9 GB 
[11/26 22:01:03 visual_prompt]: 	Training 300/553. train loss: 0.8170,	1.7543 s / batch. (data: 9.29e-01). ETA=1 day, 1:11:06, max mem: 20.9 GB 
[11/26 22:02:40 visual_prompt]: 	Training 400/553. train loss: 0.6038,	1.7560 s / batch. (data: 9.28e-01). ETA=1 day, 1:09:37, max mem: 20.9 GB 
[11/26 22:04:15 visual_prompt]: 	Training 500/553. train loss: 1.0963,	0.8129 s / batch. (data: 3.04e-04). ETA=11:37:31, max mem: 20.9 GB 
[11/26 22:05:05 visual_prompt]: Epoch 7 / 100: avg data time: 1.49e-01, avg batch time: 0.9745, average train loss: 0.7795
[11/26 22:06:00 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3078, average loss: 0.7335
[11/26 22:06:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.16	
[11/26 22:06:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[11/26 22:07:38 visual_prompt]: 	Training 100/553. train loss: 0.7489,	0.8249 s / batch. (data: 3.28e-04). ETA=11:45:42, max mem: 20.9 GB 
[11/26 22:09:16 visual_prompt]: 	Training 200/553. train loss: 1.3224,	0.8114 s / batch. (data: 3.14e-04). ETA=11:32:48, max mem: 20.9 GB 
[11/26 22:10:53 visual_prompt]: 	Training 300/553. train loss: 0.7265,	0.8553 s / batch. (data: 1.12e-02). ETA=12:08:48, max mem: 20.9 GB 
[11/26 22:12:29 visual_prompt]: 	Training 400/553. train loss: 0.6661,	0.8160 s / batch. (data: 3.09e-04). ETA=11:33:57, max mem: 20.9 GB 
[11/26 22:14:05 visual_prompt]: 	Training 500/553. train loss: 0.9587,	1.4600 s / batch. (data: 6.16e-01). ETA=20:39:16, max mem: 20.9 GB 
[11/26 22:14:56 visual_prompt]: Epoch 8 / 100: avg data time: 1.42e-01, avg batch time: 0.9690, average train loss: 0.7744
[11/26 22:15:51 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3095, average loss: 0.8887
[11/26 22:15:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.15	
[11/26 22:15:51 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[11/26 22:17:30 visual_prompt]: 	Training 100/553. train loss: 0.6589,	0.8620 s / batch. (data: 5.48e-03). ETA=12:09:27, max mem: 20.9 GB 
[11/26 22:19:06 visual_prompt]: 	Training 200/553. train loss: 0.6005,	0.8346 s / batch. (data: 1.06e-02). ETA=11:44:54, max mem: 20.9 GB 
[11/26 22:20:42 visual_prompt]: 	Training 300/553. train loss: 0.6051,	1.5934 s / batch. (data: 7.81e-01). ETA=22:23:07, max mem: 20.9 GB 
[11/26 22:22:20 visual_prompt]: 	Training 400/553. train loss: 0.5555,	0.8216 s / batch. (data: 2.94e-04). ETA=11:31:10, max mem: 20.9 GB 
[11/26 22:23:56 visual_prompt]: 	Training 500/553. train loss: 0.7795,	0.8675 s / batch. (data: 3.91e-02). ETA=12:08:22, max mem: 20.9 GB 
[11/26 22:24:45 visual_prompt]: Epoch 9 / 100: avg data time: 1.39e-01, avg batch time: 0.9663, average train loss: 0.7642
[11/26 22:25:41 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3087, average loss: 0.7245
[11/26 22:25:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 64.88	
[11/26 22:25:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[11/26 22:27:24 visual_prompt]: 	Training 100/553. train loss: 0.6625,	0.8377 s / batch. (data: 7.95e-03). ETA=11:41:10, max mem: 20.9 GB 
[11/26 22:28:58 visual_prompt]: 	Training 200/553. train loss: 0.4965,	0.8280 s / batch. (data: 5.41e-03). ETA=11:31:40, max mem: 20.9 GB 
[11/26 22:30:34 visual_prompt]: 	Training 300/553. train loss: 0.5139,	0.8426 s / batch. (data: 3.27e-04). ETA=11:42:29, max mem: 20.9 GB 
[11/26 22:32:08 visual_prompt]: 	Training 400/553. train loss: 0.6741,	0.8240 s / batch. (data: 3.06e-04). ETA=11:25:36, max mem: 20.9 GB 
[11/26 22:33:46 visual_prompt]: 	Training 500/553. train loss: 1.9074,	0.8449 s / batch. (data: 1.06e-02). ETA=11:41:34, max mem: 20.9 GB 
[11/26 22:34:36 visual_prompt]: Epoch 10 / 100: avg data time: 1.43e-01, avg batch time: 0.9686, average train loss: 0.7730
[11/26 22:35:31 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3054, average loss: 1.0097
[11/26 22:35:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.46	
[11/26 22:35:31 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[11/26 22:37:14 visual_prompt]: 	Training 100/553. train loss: 0.5686,	0.8371 s / batch. (data: 5.41e-03). ETA=11:32:58, max mem: 20.9 GB 
[11/26 22:38:52 visual_prompt]: 	Training 200/553. train loss: 1.8852,	0.8325 s / batch. (data: 5.41e-03). ETA=11:27:49, max mem: 20.9 GB 
[11/26 22:40:29 visual_prompt]: 	Training 300/553. train loss: 0.6599,	1.9076 s / batch. (data: 1.10e+00). ETA=1 day, 2:12:46, max mem: 20.9 GB 
[11/26 22:42:02 visual_prompt]: 	Training 400/553. train loss: 0.7311,	0.8267 s / batch. (data: 5.41e-03). ETA=11:20:11, max mem: 20.9 GB 
[11/26 22:43:37 visual_prompt]: 	Training 500/553. train loss: 0.5769,	0.8400 s / batch. (data: 2.93e-04). ETA=11:29:45, max mem: 20.9 GB 
[11/26 22:44:27 visual_prompt]: Epoch 11 / 100: avg data time: 1.42e-01, avg batch time: 0.9691, average train loss: 0.8002
[11/26 22:45:22 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3071, average loss: 0.7692
[11/26 22:45:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 66.56	
[11/26 22:45:22 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[11/26 22:47:04 visual_prompt]: 	Training 100/553. train loss: 0.8144,	0.8320 s / batch. (data: 3.10e-04). ETA=11:21:05, max mem: 20.9 GB 
[11/26 22:48:41 visual_prompt]: 	Training 200/553. train loss: 0.7279,	0.8131 s / batch. (data: 3.10e-04). ETA=11:04:14, max mem: 20.9 GB 
[11/26 22:50:18 visual_prompt]: 	Training 300/553. train loss: 0.6858,	0.8287 s / batch. (data: 2.99e-04). ETA=11:15:39, max mem: 20.9 GB 
[11/26 22:51:56 visual_prompt]: 	Training 400/553. train loss: 1.1265,	0.8362 s / batch. (data: 2.23e-02). ETA=11:20:19, max mem: 20.9 GB 
[11/26 22:53:32 visual_prompt]: 	Training 500/553. train loss: 1.6917,	0.8278 s / batch. (data: 3.09e-04). ETA=11:12:07, max mem: 20.9 GB 
[11/26 22:54:22 visual_prompt]: Epoch 12 / 100: avg data time: 1.50e-01, avg batch time: 0.9755, average train loss: 0.7647
[11/26 22:55:17 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3075, average loss: 1.2363
[11/26 22:55:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.50	
[11/26 22:55:17 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[11/26 22:56:59 visual_prompt]: 	Training 100/553. train loss: 0.8384,	0.8363 s / batch. (data: 2.40e-02). ETA=11:16:54, max mem: 20.9 GB 
[11/26 22:58:34 visual_prompt]: 	Training 200/553. train loss: 0.6761,	0.8201 s / batch. (data: 5.42e-03). ETA=11:02:23, max mem: 20.9 GB 
[11/26 23:00:10 visual_prompt]: 	Training 300/553. train loss: 0.6832,	1.5727 s / batch. (data: 7.64e-01). ETA=21:07:43, max mem: 20.9 GB 
[11/26 23:01:45 visual_prompt]: 	Training 400/553. train loss: 0.9954,	0.8161 s / batch. (data: 3.26e-04). ETA=10:56:27, max mem: 20.9 GB 
[11/26 23:03:22 visual_prompt]: 	Training 500/553. train loss: 0.6511,	0.8121 s / batch. (data: 2.85e-04). ETA=10:51:51, max mem: 20.9 GB 
[11/26 23:04:12 visual_prompt]: Epoch 13 / 100: avg data time: 1.41e-01, avg batch time: 0.9672, average train loss: 0.7542
[11/26 23:05:06 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3086, average loss: 0.7127
[11/26 23:05:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 67.30	
[11/26 23:05:06 visual_prompt]: Best epoch 13: best metric: -0.713
[11/26 23:05:06 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[11/26 23:06:46 visual_prompt]: 	Training 100/553. train loss: 0.7310,	0.8229 s / batch. (data: 5.41e-03). ETA=10:58:26, max mem: 20.9 GB 
[11/26 23:08:21 visual_prompt]: 	Training 200/553. train loss: 0.7939,	0.8391 s / batch. (data: 3.10e-04). ETA=11:10:00, max mem: 20.9 GB 
[11/26 23:09:56 visual_prompt]: 	Training 300/553. train loss: 0.8042,	0.8400 s / batch. (data: 7.95e-03). ETA=11:09:20, max mem: 20.9 GB 
[11/26 23:11:30 visual_prompt]: 	Training 400/553. train loss: 0.8075,	0.8360 s / batch. (data: 2.91e-04). ETA=11:04:46, max mem: 20.9 GB 
[11/26 23:13:06 visual_prompt]: 	Training 500/553. train loss: 0.8404,	0.8160 s / batch. (data: 2.58e-04). ETA=10:47:31, max mem: 20.9 GB 
[11/26 23:13:54 visual_prompt]: Epoch 14 / 100: avg data time: 1.29e-01, avg batch time: 0.9545, average train loss: 0.7282
[11/26 23:14:49 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3094, average loss: 0.7464
[11/26 23:14:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.41	rocauc: 68.17	
[11/26 23:14:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[11/26 23:16:28 visual_prompt]: 	Training 100/553. train loss: 0.8209,	0.8682 s / batch. (data: 2.44e-02). ETA=11:26:41, max mem: 20.9 GB 
[11/26 23:18:02 visual_prompt]: 	Training 200/553. train loss: 0.7435,	0.8161 s / batch. (data: 4.01e-04). ETA=10:44:09, max mem: 20.9 GB 
[11/26 23:19:39 visual_prompt]: 	Training 300/553. train loss: 0.3367,	0.8592 s / batch. (data: 1.09e-02). ETA=11:16:42, max mem: 20.9 GB 
[11/26 23:21:11 visual_prompt]: 	Training 400/553. train loss: 0.3146,	1.0680 s / batch. (data: 2.39e-01). ETA=13:59:26, max mem: 20.9 GB 
[11/26 23:22:47 visual_prompt]: 	Training 500/553. train loss: 0.8725,	0.8480 s / batch. (data: 1.20e-02). ETA=11:05:05, max mem: 20.9 GB 
[11/26 23:23:38 visual_prompt]: Epoch 15 / 100: avg data time: 1.30e-01, avg batch time: 0.9563, average train loss: 0.7543
[11/26 23:24:32 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3074, average loss: 0.8515
[11/26 23:24:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.77	
[11/26 23:24:32 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[11/26 23:26:11 visual_prompt]: 	Training 100/553. train loss: 0.4603,	0.8181 s / batch. (data: 6.39e-03). ETA=10:39:31, max mem: 20.9 GB 
[11/26 23:27:46 visual_prompt]: 	Training 200/553. train loss: 1.0471,	0.8212 s / batch. (data: 3.01e-04). ETA=10:40:35, max mem: 20.9 GB 
[11/26 23:29:21 visual_prompt]: 	Training 300/553. train loss: 1.1026,	0.8440 s / batch. (data: 2.64e-04). ETA=10:57:00, max mem: 20.9 GB 
[11/26 23:30:57 visual_prompt]: 	Training 400/553. train loss: 0.3462,	0.8232 s / batch. (data: 2.67e-04). ETA=10:39:27, max mem: 20.9 GB 
[11/26 23:32:31 visual_prompt]: 	Training 500/553. train loss: 1.1091,	1.1123 s / batch. (data: 2.98e-01). ETA=14:22:06, max mem: 20.9 GB 
[11/26 23:33:22 visual_prompt]: Epoch 16 / 100: avg data time: 1.30e-01, avg batch time: 0.9572, average train loss: 0.7191
[11/26 23:34:16 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3069, average loss: 0.6568
[11/26 23:34:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 68.32	
[11/26 23:34:16 visual_prompt]: Best epoch 16: best metric: -0.657
[11/26 23:34:16 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[11/26 23:35:54 visual_prompt]: 	Training 100/553. train loss: 0.3619,	0.8168 s / batch. (data: 2.88e-04). ETA=10:31:00, max mem: 20.9 GB 
[11/26 23:37:30 visual_prompt]: 	Training 200/553. train loss: 1.0301,	0.8342 s / batch. (data: 6.19e-03). ETA=10:43:05, max mem: 20.9 GB 
[11/26 23:39:04 visual_prompt]: 	Training 300/553. train loss: 0.9207,	0.8280 s / batch. (data: 2.87e-04). ETA=10:36:53, max mem: 20.9 GB 
[11/26 23:40:40 visual_prompt]: 	Training 400/553. train loss: 0.5209,	1.0387 s / batch. (data: 2.27e-01). ETA=13:17:16, max mem: 20.9 GB 
[11/26 23:42:17 visual_prompt]: 	Training 500/553. train loss: 0.6430,	1.5080 s / batch. (data: 6.89e-01). ETA=19:14:55, max mem: 20.9 GB 
[11/26 23:43:08 visual_prompt]: Epoch 17 / 100: avg data time: 1.35e-01, avg batch time: 0.9619, average train loss: 0.6916
[11/26 23:44:03 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3082, average loss: 0.7379
[11/26 23:44:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 67.68	
[11/26 23:44:03 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[11/26 23:45:43 visual_prompt]: 	Training 100/553. train loss: 0.3458,	0.8094 s / batch. (data: 2.95e-04). ETA=10:17:49, max mem: 20.9 GB 
[11/26 23:47:21 visual_prompt]: 	Training 200/553. train loss: 0.5195,	0.8224 s / batch. (data: 2.97e-04). ETA=10:26:20, max mem: 20.9 GB 
[11/26 23:48:57 visual_prompt]: 	Training 300/553. train loss: 0.3816,	0.8400 s / batch. (data: 3.22e-04). ETA=10:38:22, max mem: 20.9 GB 
[11/26 23:50:32 visual_prompt]: 	Training 400/553. train loss: 0.7200,	0.8435 s / batch. (data: 5.42e-03). ETA=10:39:36, max mem: 20.9 GB 
[11/26 23:52:06 visual_prompt]: 	Training 500/553. train loss: 0.7488,	0.8439 s / batch. (data: 5.41e-03). ETA=10:38:32, max mem: 20.9 GB 
[11/26 23:52:55 visual_prompt]: Epoch 18 / 100: avg data time: 1.35e-01, avg batch time: 0.9613, average train loss: 0.7120
[11/26 23:53:49 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3085, average loss: 0.6401
[11/26 23:53:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 68.25	
[11/26 23:53:49 visual_prompt]: Best epoch 18: best metric: -0.640
[11/26 23:53:49 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[11/26 23:55:28 visual_prompt]: 	Training 100/553. train loss: 0.9963,	0.9120 s / batch. (data: 5.19e-02). ETA=11:27:44, max mem: 20.9 GB 
[11/26 23:57:03 visual_prompt]: 	Training 200/553. train loss: 0.6663,	0.8236 s / batch. (data: 2.48e-04). ETA=10:19:42, max mem: 20.9 GB 
[11/26 23:58:38 visual_prompt]: 	Training 300/553. train loss: 0.6535,	0.8190 s / batch. (data: 7.96e-03). ETA=10:14:50, max mem: 20.9 GB 
[11/27 00:00:15 visual_prompt]: 	Training 400/553. train loss: 0.3001,	0.8320 s / batch. (data: 2.95e-04). ETA=10:23:15, max mem: 20.9 GB 
[11/27 00:01:47 visual_prompt]: 	Training 500/553. train loss: 0.8062,	0.8280 s / batch. (data: 2.75e-04). ETA=10:18:53, max mem: 20.9 GB 
[11/27 00:02:36 visual_prompt]: Epoch 19 / 100: avg data time: 1.27e-01, avg batch time: 0.9537, average train loss: 0.6999
[11/27 00:03:31 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3081, average loss: 0.7455
[11/27 00:03:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 67.82	
[11/27 00:03:31 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[11/27 00:05:09 visual_prompt]: 	Training 100/553. train loss: 1.7617,	0.8160 s / batch. (data: 3.06e-04). ETA=10:07:50, max mem: 20.9 GB 
[11/27 00:06:45 visual_prompt]: 	Training 200/553. train loss: 0.2000,	0.8440 s / batch. (data: 1.18e-02). ETA=10:27:18, max mem: 20.9 GB 
[11/27 00:08:21 visual_prompt]: 	Training 300/553. train loss: 0.7156,	0.8480 s / batch. (data: 2.89e-04). ETA=10:28:50, max mem: 20.9 GB 
[11/27 00:09:55 visual_prompt]: 	Training 400/553. train loss: 0.5717,	0.8160 s / batch. (data: 3.06e-04). ETA=10:03:44, max mem: 20.9 GB 
[11/27 00:11:30 visual_prompt]: 	Training 500/553. train loss: 1.5418,	0.8400 s / batch. (data: 3.03e-04). ETA=10:20:06, max mem: 20.9 GB 
[11/27 00:12:21 visual_prompt]: Epoch 20 / 100: avg data time: 1.31e-01, avg batch time: 0.9586, average train loss: 0.7183
[11/27 00:13:15 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3077, average loss: 0.6451
[11/27 00:13:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.68	
[11/27 00:13:15 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[11/27 00:14:56 visual_prompt]: 	Training 100/553. train loss: 0.4569,	0.8088 s / batch. (data: 3.12e-04). ETA=9:54:59, max mem: 20.9 GB 
[11/27 00:16:31 visual_prompt]: 	Training 200/553. train loss: 0.3799,	0.8296 s / batch. (data: 9.53e-03). ETA=10:08:54, max mem: 20.9 GB 
[11/27 00:18:05 visual_prompt]: 	Training 300/553. train loss: 1.0443,	0.9382 s / batch. (data: 1.27e-01). ETA=11:27:03, max mem: 20.9 GB 
[11/27 00:19:40 visual_prompt]: 	Training 400/553. train loss: 0.5222,	0.8260 s / batch. (data: 2.99e-04). ETA=10:03:30, max mem: 20.9 GB 
[11/27 00:21:15 visual_prompt]: 	Training 500/553. train loss: 0.5550,	0.8361 s / batch. (data: 7.97e-03). ETA=10:09:32, max mem: 20.9 GB 
[11/27 00:22:04 visual_prompt]: Epoch 21 / 100: avg data time: 1.31e-01, avg batch time: 0.9568, average train loss: 0.6889
[11/27 00:22:59 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3073, average loss: 0.6284
[11/27 00:22:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 71.26	
[11/27 00:22:59 visual_prompt]: Best epoch 21: best metric: -0.628
[11/27 00:22:59 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[11/27 00:24:37 visual_prompt]: 	Training 100/553. train loss: 0.5536,	0.8366 s / batch. (data: 8.24e-03). ETA=10:07:44, max mem: 20.9 GB 
[11/27 00:26:12 visual_prompt]: 	Training 200/553. train loss: 0.4044,	0.8265 s / batch. (data: 2.85e-04). ETA=9:59:03, max mem: 20.9 GB 
[11/27 00:27:46 visual_prompt]: 	Training 300/553. train loss: 0.2265,	0.8440 s / batch. (data: 2.85e-04). ETA=10:10:18, max mem: 20.9 GB 
[11/27 00:29:22 visual_prompt]: 	Training 400/553. train loss: 0.6493,	0.8280 s / batch. (data: 2.87e-04). ETA=9:57:23, max mem: 20.9 GB 
[11/27 00:30:57 visual_prompt]: 	Training 500/553. train loss: 1.0980,	0.8260 s / batch. (data: 3.35e-03). ETA=9:54:31, max mem: 20.9 GB 
[11/27 00:31:48 visual_prompt]: Epoch 22 / 100: avg data time: 1.30e-01, avg batch time: 0.9569, average train loss: 0.6781
[11/27 00:32:42 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3095, average loss: 0.6210
[11/27 00:32:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.92	rocauc: 71.15	
[11/27 00:32:42 visual_prompt]: Best epoch 22: best metric: -0.621
[11/27 00:32:42 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[11/27 00:34:23 visual_prompt]: 	Training 100/553. train loss: 1.1778,	0.8483 s / batch. (data: 2.04e-02). ETA=10:08:24, max mem: 20.9 GB 
[11/27 00:35:59 visual_prompt]: 	Training 200/553. train loss: 0.6894,	0.8400 s / batch. (data: 2.91e-04). ETA=10:01:04, max mem: 20.9 GB 
[11/27 00:37:35 visual_prompt]: 	Training 300/553. train loss: 1.5652,	0.8437 s / batch. (data: 7.95e-04). ETA=10:02:21, max mem: 20.9 GB 
[11/27 00:39:08 visual_prompt]: 	Training 400/553. train loss: 0.5754,	0.8471 s / batch. (data: 5.83e-03). ETA=10:03:21, max mem: 20.9 GB 
[11/27 00:40:41 visual_prompt]: 	Training 500/553. train loss: 0.5461,	0.8444 s / batch. (data: 2.44e-02). ETA=10:00:01, max mem: 20.9 GB 
[11/27 00:41:31 visual_prompt]: Epoch 23 / 100: avg data time: 1.29e-01, avg batch time: 0.9562, average train loss: 0.6508
[11/27 00:42:25 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3090, average loss: 0.6461
[11/27 00:42:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 71.48	
[11/27 00:42:25 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[11/27 00:44:02 visual_prompt]: 	Training 100/553. train loss: 0.9044,	0.8200 s / batch. (data: 5.43e-03). ETA=9:40:35, max mem: 20.9 GB 
[11/27 00:45:37 visual_prompt]: 	Training 200/553. train loss: 0.8926,	0.8360 s / batch. (data: 2.89e-04). ETA=9:50:29, max mem: 20.9 GB 
[11/27 00:47:12 visual_prompt]: 	Training 300/553. train loss: 0.4972,	0.8449 s / batch. (data: 1.64e-02). ETA=9:55:23, max mem: 20.9 GB 
[11/27 00:48:48 visual_prompt]: 	Training 400/553. train loss: 0.2298,	0.8362 s / batch. (data: 1.20e-02). ETA=9:47:52, max mem: 20.9 GB 
[11/27 00:50:24 visual_prompt]: 	Training 500/553. train loss: 0.5084,	0.8436 s / batch. (data: 3.12e-04). ETA=9:51:40, max mem: 20.9 GB 
[11/27 00:51:14 visual_prompt]: Epoch 24 / 100: avg data time: 1.30e-01, avg batch time: 0.9564, average train loss: 0.6731
[11/27 00:52:09 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3085, average loss: 0.6367
[11/27 00:52:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 70.88	
[11/27 00:52:09 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[11/27 00:53:51 visual_prompt]: 	Training 100/553. train loss: 0.4314,	0.8477 s / batch. (data: 1.97e-02). ETA=9:52:22, max mem: 20.9 GB 
[11/27 00:55:23 visual_prompt]: 	Training 200/553. train loss: 1.2030,	0.8416 s / batch. (data: 1.00e-02). ETA=9:46:42, max mem: 20.9 GB 
[11/27 00:56:57 visual_prompt]: 	Training 300/553. train loss: 0.5260,	0.8360 s / batch. (data: 1.20e-02). ETA=9:41:24, max mem: 20.9 GB 
[11/27 00:58:33 visual_prompt]: 	Training 400/553. train loss: 0.7355,	1.0477 s / batch. (data: 2.35e-01). ETA=12:06:53, max mem: 20.9 GB 
[11/27 01:00:08 visual_prompt]: 	Training 500/553. train loss: 0.5099,	1.1120 s / batch. (data: 2.84e-01). ETA=12:49:37, max mem: 20.9 GB 
[11/27 01:00:58 visual_prompt]: Epoch 25 / 100: avg data time: 1.32e-01, avg batch time: 0.9570, average train loss: 0.6412
[11/27 01:01:52 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3086, average loss: 0.7576
[11/27 01:01:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 69.91	
[11/27 01:01:52 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[11/27 01:03:32 visual_prompt]: 	Training 100/553. train loss: 0.1995,	0.8398 s / batch. (data: 7.96e-03). ETA=9:39:06, max mem: 20.9 GB 
[11/27 01:05:08 visual_prompt]: 	Training 200/553. train loss: 0.3919,	1.5628 s / batch. (data: 7.54e-01). ETA=17:55:03, max mem: 20.9 GB 
[11/27 01:06:45 visual_prompt]: 	Training 300/553. train loss: 1.0048,	0.8242 s / batch. (data: 3.18e-04). ETA=9:25:37, max mem: 20.9 GB 
[11/27 01:08:18 visual_prompt]: 	Training 400/553. train loss: 0.4865,	0.8491 s / batch. (data: 2.85e-04). ETA=9:41:14, max mem: 20.9 GB 
[11/27 01:09:52 visual_prompt]: 	Training 500/553. train loss: 0.2850,	0.8242 s / batch. (data: 2.98e-04). ETA=9:22:52, max mem: 20.9 GB 
[11/27 01:10:42 visual_prompt]: Epoch 26 / 100: avg data time: 1.30e-01, avg batch time: 0.9569, average train loss: 0.6490
[11/27 01:11:36 visual_prompt]: Inference (val):avg data time: 2.98e-04, avg batch time: 0.3094, average loss: 0.6298
[11/27 01:11:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 71.45	
[11/27 01:11:36 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[11/27 01:13:16 visual_prompt]: 	Training 100/553. train loss: 0.5913,	0.8440 s / batch. (data: 2.85e-04). ETA=9:34:15, max mem: 20.9 GB 
[11/27 01:14:51 visual_prompt]: 	Training 200/553. train loss: 0.3232,	0.8248 s / batch. (data: 2.64e-04). ETA=9:19:48, max mem: 20.9 GB 
[11/27 01:16:26 visual_prompt]: 	Training 300/553. train loss: 0.6255,	0.8283 s / batch. (data: 3.25e-04). ETA=9:20:46, max mem: 20.9 GB 
[11/27 01:18:02 visual_prompt]: 	Training 400/553. train loss: 0.8547,	0.8520 s / batch. (data: 7.97e-04). ETA=9:35:24, max mem: 20.9 GB 
[11/27 01:19:38 visual_prompt]: 	Training 500/553. train loss: 0.5204,	0.8483 s / batch. (data: 1.56e-02). ETA=9:31:31, max mem: 20.9 GB 
[11/27 01:20:26 visual_prompt]: Epoch 27 / 100: avg data time: 1.30e-01, avg batch time: 0.9574, average train loss: 0.6418
[11/27 01:21:20 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3102, average loss: 0.6370
[11/27 01:21:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.66	
[11/27 01:21:20 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[11/27 01:22:59 visual_prompt]: 	Training 100/553. train loss: 0.1118,	0.8360 s / batch. (data: 1.55e-02). ETA=9:21:05, max mem: 20.9 GB 
[11/27 01:24:34 visual_prompt]: 	Training 200/553. train loss: 0.1705,	0.8170 s / batch. (data: 5.41e-03). ETA=9:06:58, max mem: 20.9 GB 
[11/27 01:26:10 visual_prompt]: 	Training 300/553. train loss: 1.0033,	1.2880 s / batch. (data: 4.49e-01). ETA=14:20:10, max mem: 20.9 GB 
[11/27 01:27:44 visual_prompt]: 	Training 400/553. train loss: 0.7365,	0.8443 s / batch. (data: 1.18e-02). ETA=9:22:26, max mem: 20.9 GB 
[11/27 01:29:17 visual_prompt]: 	Training 500/553. train loss: 0.2908,	0.8282 s / batch. (data: 1.06e-02). ETA=9:10:19, max mem: 20.9 GB 
[11/27 01:30:08 visual_prompt]: Epoch 28 / 100: avg data time: 1.29e-01, avg batch time: 0.9554, average train loss: 0.6273
[11/27 01:31:03 visual_prompt]: Inference (val):avg data time: 8.77e-05, avg batch time: 0.3093, average loss: 0.6206
[11/27 01:31:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 72.25	
[11/27 01:31:03 visual_prompt]: Best epoch 28: best metric: -0.621
[11/27 01:31:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[11/27 01:32:48 visual_prompt]: 	Training 100/553. train loss: 0.2562,	0.8280 s / batch. (data: 2.98e-04). ETA=9:08:04, max mem: 20.9 GB 
[11/27 01:34:22 visual_prompt]: 	Training 200/553. train loss: 0.7958,	1.6880 s / batch. (data: 8.40e-01). ETA=18:34:31, max mem: 20.9 GB 
[11/27 01:35:56 visual_prompt]: 	Training 300/553. train loss: 0.5266,	0.8339 s / batch. (data: 3.04e-04). ETA=9:09:13, max mem: 20.9 GB 
[11/27 01:37:28 visual_prompt]: 	Training 400/553. train loss: 0.5025,	1.2400 s / batch. (data: 3.96e-01). ETA=13:34:35, max mem: 20.9 GB 
[11/27 01:39:03 visual_prompt]: 	Training 500/553. train loss: 0.4313,	0.8358 s / batch. (data: 1.60e-02). ETA=9:07:41, max mem: 20.9 GB 
[11/27 01:39:52 visual_prompt]: Epoch 29 / 100: avg data time: 1.31e-01, avg batch time: 0.9577, average train loss: 0.6236
[11/27 01:40:47 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3094, average loss: 0.7661
[11/27 01:40:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 70.62	
[11/27 01:40:47 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[11/27 01:42:24 visual_prompt]: 	Training 100/553. train loss: 0.8676,	0.8292 s / batch. (data: 1.56e-02). ETA=9:01:14, max mem: 20.9 GB 
[11/27 01:44:01 visual_prompt]: 	Training 200/553. train loss: 0.3420,	0.8486 s / batch. (data: 2.45e-02). ETA=9:12:27, max mem: 20.9 GB 
[11/27 01:45:34 visual_prompt]: 	Training 300/553. train loss: 0.0243,	1.4703 s / batch. (data: 6.45e-01). ETA=15:54:48, max mem: 20.9 GB 
[11/27 01:47:10 visual_prompt]: 	Training 400/553. train loss: 0.6239,	1.0520 s / batch. (data: 2.21e-01). ETA=11:21:25, max mem: 20.9 GB 
[11/27 01:48:45 visual_prompt]: 	Training 500/553. train loss: 0.1607,	1.3760 s / batch. (data: 5.43e-01). ETA=14:48:57, max mem: 20.9 GB 
[11/27 01:49:36 visual_prompt]: Epoch 30 / 100: avg data time: 1.29e-01, avg batch time: 0.9567, average train loss: 0.6329
[11/27 01:50:30 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3085, average loss: 0.6615
[11/27 01:50:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 72.42	
[11/27 01:50:30 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[11/27 01:52:11 visual_prompt]: 	Training 100/553. train loss: 0.6946,	0.8239 s / batch. (data: 1.06e-02). ETA=8:50:09, max mem: 20.9 GB 
[11/27 01:53:48 visual_prompt]: 	Training 200/553. train loss: 0.7895,	0.8296 s / batch. (data: 1.20e-02). ETA=8:52:27, max mem: 20.9 GB 
[11/27 01:55:20 visual_prompt]: 	Training 300/553. train loss: 0.7634,	0.8479 s / batch. (data: 4.98e-04). ETA=9:02:48, max mem: 20.9 GB 
[11/27 01:56:55 visual_prompt]: 	Training 400/553. train loss: 0.7439,	0.8164 s / batch. (data: 3.06e-04). ETA=8:41:16, max mem: 20.9 GB 
[11/27 01:58:30 visual_prompt]: 	Training 500/553. train loss: 0.7363,	0.8109 s / batch. (data: 3.05e-04). ETA=8:36:26, max mem: 20.9 GB 
[11/27 01:59:19 visual_prompt]: Epoch 31 / 100: avg data time: 1.27e-01, avg batch time: 0.9553, average train loss: 0.5720
[11/27 02:00:13 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3084, average loss: 0.6672
[11/27 02:00:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 70.92	
[11/27 02:00:13 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[11/27 02:01:53 visual_prompt]: 	Training 100/553. train loss: 0.3141,	0.8176 s / batch. (data: 2.88e-04). ETA=8:38:35, max mem: 20.9 GB 
[11/27 02:03:28 visual_prompt]: 	Training 200/553. train loss: 0.2690,	0.8560 s / batch. (data: 3.06e-04). ETA=9:01:32, max mem: 20.9 GB 
[11/27 02:05:06 visual_prompt]: 	Training 300/553. train loss: 1.3009,	0.8440 s / batch. (data: 7.95e-03). ETA=8:52:30, max mem: 20.9 GB 
[11/27 02:06:42 visual_prompt]: 	Training 400/553. train loss: 1.1264,	0.8132 s / batch. (data: 2.92e-04). ETA=8:31:44, max mem: 20.9 GB 
[11/27 02:08:14 visual_prompt]: 	Training 500/553. train loss: 1.2402,	0.8210 s / batch. (data: 3.38e-04). ETA=8:35:16, max mem: 20.9 GB 
[11/27 02:09:02 visual_prompt]: Epoch 32 / 100: avg data time: 1.30e-01, avg batch time: 0.9571, average train loss: 0.5776
[11/27 02:09:57 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3060, average loss: 1.0474
[11/27 02:09:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 69.08	
[11/27 02:09:57 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[11/27 02:11:34 visual_prompt]: 	Training 100/553. train loss: 1.1009,	0.8259 s / batch. (data: 5.36e-03). ETA=8:36:13, max mem: 20.9 GB 
[11/27 02:13:11 visual_prompt]: 	Training 200/553. train loss: 0.2133,	0.8920 s / batch. (data: 5.28e-02). ETA=9:16:04, max mem: 20.9 GB 
[11/27 02:14:45 visual_prompt]: 	Training 300/553. train loss: 0.3145,	0.8458 s / batch. (data: 2.17e-02). ETA=8:45:52, max mem: 20.9 GB 
[11/27 02:16:21 visual_prompt]: 	Training 400/553. train loss: 0.4252,	0.8283 s / batch. (data: 2.99e-04). ETA=8:33:37, max mem: 20.9 GB 
[11/27 02:17:56 visual_prompt]: 	Training 500/553. train loss: 0.2672,	0.8400 s / batch. (data: 3.21e-04). ETA=8:39:27, max mem: 20.9 GB 
[11/27 02:18:45 visual_prompt]: Epoch 33 / 100: avg data time: 1.27e-01, avg batch time: 0.9545, average train loss: 0.5707
[11/27 02:19:39 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3084, average loss: 0.7187
[11/27 02:19:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.01	
[11/27 02:19:39 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[11/27 02:21:19 visual_prompt]: 	Training 100/553. train loss: 0.6803,	0.8265 s / batch. (data: 5.44e-03). ETA=8:28:58, max mem: 20.9 GB 
[11/27 02:22:53 visual_prompt]: 	Training 200/553. train loss: 0.7152,	0.8159 s / batch. (data: 3.10e-04). ETA=8:21:05, max mem: 20.9 GB 
[11/27 02:24:27 visual_prompt]: 	Training 300/553. train loss: 0.3883,	0.8280 s / batch. (data: 5.73e-03). ETA=8:27:10, max mem: 20.9 GB 
[11/27 02:26:04 visual_prompt]: 	Training 400/553. train loss: 0.2337,	0.8428 s / batch. (data: 5.42e-03). ETA=8:34:50, max mem: 20.9 GB 
[11/27 02:27:39 visual_prompt]: 	Training 500/553. train loss: 0.2938,	1.3715 s / batch. (data: 5.50e-01). ETA=13:55:28, max mem: 20.9 GB 
[11/27 02:28:28 visual_prompt]: Epoch 34 / 100: avg data time: 1.28e-01, avg batch time: 0.9558, average train loss: 0.5468
[11/27 02:29:22 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3070, average loss: 0.7190
[11/27 02:29:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 70.60	
[11/27 02:29:22 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[11/27 02:31:03 visual_prompt]: 	Training 100/553. train loss: 0.9457,	0.8280 s / batch. (data: 3.06e-04). ETA=8:22:18, max mem: 20.9 GB 
[11/27 02:32:39 visual_prompt]: 	Training 200/553. train loss: 0.8906,	0.8248 s / batch. (data: 3.25e-04). ETA=8:18:57, max mem: 20.9 GB 
[11/27 02:34:12 visual_prompt]: 	Training 300/553. train loss: 0.3034,	0.8280 s / batch. (data: 2.98e-04). ETA=8:19:31, max mem: 20.9 GB 
[11/27 02:35:46 visual_prompt]: 	Training 400/553. train loss: 0.5313,	0.8328 s / batch. (data: 3.29e-04). ETA=8:21:03, max mem: 20.9 GB 
[11/27 02:37:20 visual_prompt]: 	Training 500/553. train loss: 1.3182,	1.1440 s / batch. (data: 3.09e-01). ETA=11:26:22, max mem: 20.9 GB 
[11/27 02:38:10 visual_prompt]: Epoch 35 / 100: avg data time: 1.29e-01, avg batch time: 0.9555, average train loss: 0.5476
[11/27 02:39:05 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3069, average loss: 0.8022
[11/27 02:39:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 71.44	
[11/27 02:39:05 visual_prompt]: Stopping early.
[11/27 02:39:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 02:39:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 02:39:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/27 02:39:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 02:39:05 visual_prompt]: Training with config:
[11/27 02:39:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.05_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/27 02:39:05 visual_prompt]: Loading training data...
[11/27 02:39:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 02:39:05 visual_prompt]: Loading validation data...
[11/27 02:39:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 02:39:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 02:39:11 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 02:39:11 visual_prompt]: tuned percent:0.525
[11/27 02:39:11 visual_prompt]: Device used for model: 0
[11/27 02:39:11 visual_prompt]: Setting up Evaluator...
[11/27 02:39:11 visual_prompt]: Setting up Trainer...
[11/27 02:39:11 visual_prompt]: 	Setting up the optimizer...
[11/27 02:39:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 02:40:50 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8280 s / batch. (data: 2.86e-04). ETA=12:41:45, max mem: 20.9 GB 
[11/27 02:42:23 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8240 s / batch. (data: 1.19e-02). ETA=12:36:41, max mem: 20.9 GB 
[11/27 02:44:01 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3023 s / batch. (data: 4.80e-01). ETA=19:53:47, max mem: 20.9 GB 
[11/27 02:45:35 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8254 s / batch. (data: 9.54e-03). ETA=12:35:14, max mem: 20.9 GB 
[11/27 02:47:12 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8557 s / batch. (data: 1.60e-02). ETA=13:01:31, max mem: 20.9 GB 
[11/27 02:48:02 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.9607, average train loss: 1.5403
[11/27 02:48:56 visual_prompt]: Inference (val):avg data time: 2.35e-04, avg batch time: 0.3073, average loss: 1.5201
[11/27 02:48:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 02:48:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[11/27 02:50:35 visual_prompt]: 	Training 100/553. train loss: 0.7362,	1.6916 s / batch. (data: 8.65e-01). ETA=1 day, 1:40:40, max mem: 20.9 GB 
[11/27 02:52:09 visual_prompt]: 	Training 200/553. train loss: 0.4418,	0.8920 s / batch. (data: 6.12e-02). ETA=13:30:56, max mem: 20.9 GB 
[11/27 02:53:46 visual_prompt]: 	Training 300/553. train loss: 0.6967,	0.9440 s / batch. (data: 1.19e-01). ETA=14:16:40, max mem: 20.9 GB 
[11/27 02:55:20 visual_prompt]: 	Training 400/553. train loss: 0.7413,	0.8344 s / batch. (data: 2.90e-04). ETA=12:35:45, max mem: 20.9 GB 
[11/27 02:56:56 visual_prompt]: 	Training 500/553. train loss: 0.6909,	0.8399 s / batch. (data: 1.62e-02). ETA=12:39:20, max mem: 20.9 GB 
[11/27 02:57:45 visual_prompt]: Epoch 2 / 100: avg data time: 1.30e-01, avg batch time: 0.9560, average train loss: 0.7611
[11/27 02:58:39 visual_prompt]: Inference (val):avg data time: 4.60e-04, avg batch time: 0.3082, average loss: 0.7329
[11/27 02:58:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.20	
[11/27 02:58:39 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[11/27 03:00:17 visual_prompt]: 	Training 100/553. train loss: 0.7497,	0.8523 s / batch. (data: 1.05e-02). ETA=12:48:26, max mem: 20.9 GB 
[11/27 03:01:53 visual_prompt]: 	Training 200/553. train loss: 0.7420,	0.8400 s / batch. (data: 1.20e-02). ETA=12:35:55, max mem: 20.9 GB 
[11/27 03:03:28 visual_prompt]: 	Training 300/553. train loss: 0.5553,	0.8205 s / batch. (data: 2.79e-04). ETA=12:16:58, max mem: 20.9 GB 
[11/27 03:05:02 visual_prompt]: 	Training 400/553. train loss: 0.6189,	0.8467 s / batch. (data: 1.05e-02). ETA=12:39:05, max mem: 20.9 GB 
[11/27 03:06:39 visual_prompt]: 	Training 500/553. train loss: 0.7347,	1.1094 s / batch. (data: 2.55e-01). ETA=16:32:48, max mem: 20.9 GB 
[11/27 03:07:28 visual_prompt]: Epoch 3 / 100: avg data time: 1.29e-01, avg batch time: 0.9554, average train loss: 0.7369
[11/27 03:08:22 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3089, average loss: 0.7264
[11/27 03:08:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.65	
[11/27 03:08:22 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[11/27 03:10:02 visual_prompt]: 	Training 100/553. train loss: 0.7391,	0.8546 s / batch. (data: 3.22e-04). ETA=12:42:38, max mem: 20.9 GB 
[11/27 03:11:37 visual_prompt]: 	Training 200/553. train loss: 0.6014,	0.8165 s / batch. (data: 5.44e-03). ETA=12:07:12, max mem: 20.9 GB 
[11/27 03:13:12 visual_prompt]: 	Training 300/553. train loss: 0.6650,	0.8200 s / batch. (data: 3.09e-04). ETA=12:08:58, max mem: 20.9 GB 
[11/27 03:14:44 visual_prompt]: 	Training 400/553. train loss: 0.7954,	1.0495 s / batch. (data: 2.39e-01). ETA=15:31:16, max mem: 20.9 GB 
[11/27 03:16:21 visual_prompt]: 	Training 500/553. train loss: 0.4476,	3.0460 s / batch. (data: 2.23e+00). ETA=1 day, 20:57:45, max mem: 20.9 GB 
[11/27 03:17:11 visual_prompt]: Epoch 4 / 100: avg data time: 1.31e-01, avg batch time: 0.9573, average train loss: 0.7284
[11/27 03:18:05 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3076, average loss: 0.6993
[11/27 03:18:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.44	
[11/27 03:18:05 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[11/27 03:19:43 visual_prompt]: 	Training 100/553. train loss: 0.5385,	0.8320 s / batch. (data: 7.57e-03). ETA=12:14:45, max mem: 20.9 GB 
[11/27 03:21:18 visual_prompt]: 	Training 200/553. train loss: 0.5671,	1.2643 s / batch. (data: 4.44e-01). ETA=18:34:27, max mem: 20.9 GB 
[11/27 03:22:54 visual_prompt]: 	Training 300/553. train loss: 0.7473,	0.8170 s / batch. (data: 5.41e-03). ETA=11:58:47, max mem: 20.9 GB 
[11/27 03:24:28 visual_prompt]: 	Training 400/553. train loss: 0.5758,	0.8108 s / batch. (data: 2.89e-04). ETA=11:52:00, max mem: 20.9 GB 
[11/27 03:26:04 visual_prompt]: 	Training 500/553. train loss: 0.6920,	0.8409 s / batch. (data: 2.92e-04). ETA=12:17:02, max mem: 20.9 GB 
[11/27 03:26:54 visual_prompt]: Epoch 5 / 100: avg data time: 1.29e-01, avg batch time: 0.9562, average train loss: 0.7159
[11/27 03:27:48 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3073, average loss: 0.6982
[11/27 03:27:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.16	
[11/27 03:27:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[11/27 03:29:28 visual_prompt]: 	Training 100/553. train loss: 0.9852,	0.8561 s / batch. (data: 3.66e-02). ETA=12:28:08, max mem: 20.9 GB 
[11/27 03:31:03 visual_prompt]: 	Training 200/553. train loss: 0.5738,	0.8521 s / batch. (data: 1.20e-02). ETA=12:23:13, max mem: 20.9 GB 
[11/27 03:32:36 visual_prompt]: 	Training 300/553. train loss: 0.6145,	0.8320 s / batch. (data: 3.70e-04). ETA=12:04:19, max mem: 20.9 GB 
[11/27 03:34:15 visual_prompt]: 	Training 400/553. train loss: 0.6492,	0.8231 s / batch. (data: 3.26e-04). ETA=11:55:13, max mem: 20.9 GB 
[11/27 03:35:48 visual_prompt]: 	Training 500/553. train loss: 0.6936,	0.8200 s / batch. (data: 3.12e-04). ETA=11:51:08, max mem: 20.9 GB 
[11/27 03:36:37 visual_prompt]: Epoch 6 / 100: avg data time: 1.31e-01, avg batch time: 0.9564, average train loss: 0.7148
[11/27 03:37:31 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3074, average loss: 0.7517
[11/27 03:37:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.21	
[11/27 03:37:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[11/27 03:39:09 visual_prompt]: 	Training 100/553. train loss: 0.5537,	0.8520 s / batch. (data: 2.95e-04). ETA=12:16:43, max mem: 20.9 GB 
[11/27 03:40:44 visual_prompt]: 	Training 200/553. train loss: 0.6174,	0.8245 s / batch. (data: 7.97e-03). ETA=11:51:36, max mem: 20.9 GB 
[11/27 03:42:22 visual_prompt]: 	Training 300/553. train loss: 0.6421,	1.5720 s / batch. (data: 7.48e-01). ETA=22:34:02, max mem: 20.9 GB 
[11/27 03:43:57 visual_prompt]: 	Training 400/553. train loss: 0.6692,	1.8200 s / batch. (data: 9.83e-01). ETA=1 day, 2:04:39, max mem: 20.9 GB 
[11/27 03:45:31 visual_prompt]: 	Training 500/553. train loss: 0.6996,	0.8207 s / batch. (data: 5.41e-03). ETA=11:44:09, max mem: 20.9 GB 
[11/27 03:46:20 visual_prompt]: Epoch 7 / 100: avg data time: 1.29e-01, avg batch time: 0.9550, average train loss: 0.7063
[11/27 03:47:14 visual_prompt]: Inference (val):avg data time: 1.42e-04, avg batch time: 0.3083, average loss: 0.7846
[11/27 03:47:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.72	
[11/27 03:47:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[11/27 03:48:50 visual_prompt]: 	Training 100/553. train loss: 0.7271,	0.8400 s / batch. (data: 7.95e-03). ETA=11:58:36, max mem: 20.9 GB 
[11/27 03:50:27 visual_prompt]: 	Training 200/553. train loss: 1.0237,	0.8480 s / batch. (data: 4.03e-04). ETA=12:04:01, max mem: 20.9 GB 
[11/27 03:52:03 visual_prompt]: 	Training 300/553. train loss: 0.6814,	0.8388 s / batch. (data: 1.07e-02). ETA=11:54:45, max mem: 20.9 GB 
[11/27 03:53:37 visual_prompt]: 	Training 400/553. train loss: 0.6991,	0.8331 s / batch. (data: 3.11e-04). ETA=11:48:34, max mem: 20.9 GB 
[11/27 03:55:13 visual_prompt]: 	Training 500/553. train loss: 0.9823,	1.4360 s / batch. (data: 6.13e-01). ETA=20:18:53, max mem: 20.9 GB 
[11/27 03:56:03 visual_prompt]: Epoch 8 / 100: avg data time: 1.30e-01, avg batch time: 0.9565, average train loss: 0.7204
[11/27 03:56:57 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3098, average loss: 0.7744
[11/27 03:56:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.18	
[11/27 03:56:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[11/27 03:58:36 visual_prompt]: 	Training 100/553. train loss: 0.6898,	0.8236 s / batch. (data: 3.02e-04). ETA=11:36:57, max mem: 20.9 GB 
[11/27 04:00:10 visual_prompt]: 	Training 200/553. train loss: 0.5737,	0.8096 s / batch. (data: 3.09e-04). ETA=11:23:48, max mem: 20.9 GB 
[11/27 04:01:45 visual_prompt]: 	Training 300/553. train loss: 0.6114,	1.6807 s / batch. (data: 8.59e-01). ETA=23:36:41, max mem: 20.9 GB 
[11/27 04:03:21 visual_prompt]: 	Training 400/553. train loss: 0.6397,	0.8320 s / batch. (data: 2.93e-04). ETA=11:39:55, max mem: 20.9 GB 
[11/27 04:04:57 visual_prompt]: 	Training 500/553. train loss: 0.5622,	0.8280 s / batch. (data: 3.15e-04). ETA=11:35:10, max mem: 20.9 GB 
[11/27 04:05:45 visual_prompt]: Epoch 9 / 100: avg data time: 1.29e-01, avg batch time: 0.9557, average train loss: 0.7289
[11/27 04:06:40 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3099, average loss: 0.7123
[11/27 04:06:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.67	
[11/27 04:06:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[11/27 04:08:21 visual_prompt]: 	Training 100/553. train loss: 0.7076,	0.8440 s / batch. (data: 5.42e-03). ETA=11:46:26, max mem: 20.9 GB 
[11/27 04:09:54 visual_prompt]: 	Training 200/553. train loss: 0.6601,	0.8280 s / batch. (data: 1.20e-02). ETA=11:31:41, max mem: 20.9 GB 
[11/27 04:11:29 visual_prompt]: 	Training 300/553. train loss: 0.6690,	1.5645 s / batch. (data: 7.36e-01). ETA=21:44:23, max mem: 20.9 GB 
[11/27 04:13:03 visual_prompt]: 	Training 400/553. train loss: 0.8959,	0.8240 s / batch. (data: 1.05e-02). ETA=11:25:36, max mem: 20.9 GB 
[11/27 04:14:39 visual_prompt]: 	Training 500/553. train loss: 0.9823,	0.8360 s / batch. (data: 2.97e-04). ETA=11:34:12, max mem: 20.9 GB 
[11/27 04:15:29 visual_prompt]: Epoch 10 / 100: avg data time: 1.30e-01, avg batch time: 0.9565, average train loss: 0.7426
[11/27 04:16:23 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3106, average loss: 0.7593
[11/27 04:16:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.14	
[11/27 04:16:23 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[11/27 04:18:03 visual_prompt]: 	Training 100/553. train loss: 0.7617,	0.8303 s / batch. (data: 1.05e-02). ETA=11:27:18, max mem: 20.9 GB 
[11/27 04:19:40 visual_prompt]: 	Training 200/553. train loss: 1.0110,	0.8398 s / batch. (data: 2.88e-04). ETA=11:33:49, max mem: 20.9 GB 
[11/27 04:21:15 visual_prompt]: 	Training 300/553. train loss: 0.4844,	1.8041 s / batch. (data: 9.86e-01). ETA=1 day, 0:47:31, max mem: 20.9 GB 
[11/27 04:22:48 visual_prompt]: 	Training 400/553. train loss: 0.7756,	0.8727 s / batch. (data: 4.22e-02). ETA=11:58:04, max mem: 20.9 GB 
[11/27 04:24:21 visual_prompt]: 	Training 500/553. train loss: 0.7049,	0.8124 s / batch. (data: 2.92e-04). ETA=11:07:07, max mem: 20.9 GB 
[11/27 04:25:11 visual_prompt]: Epoch 11 / 100: avg data time: 1.28e-01, avg batch time: 0.9544, average train loss: 0.7302
[11/27 04:26:05 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3097, average loss: 0.7035
[11/27 04:26:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.18	
[11/27 04:26:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[11/27 04:27:45 visual_prompt]: 	Training 100/553. train loss: 0.8556,	0.9599 s / batch. (data: 1.17e-01). ETA=13:05:45, max mem: 20.9 GB 
[11/27 04:29:21 visual_prompt]: 	Training 200/553. train loss: 0.5623,	1.2563 s / batch. (data: 4.50e-01). ETA=17:06:17, max mem: 20.9 GB 
[11/27 04:30:55 visual_prompt]: 	Training 300/553. train loss: 0.7245,	0.8409 s / batch. (data: 2.36e-02). ETA=11:25:34, max mem: 20.9 GB 
[11/27 04:32:30 visual_prompt]: 	Training 400/553. train loss: 0.7212,	0.8312 s / batch. (data: 5.42e-03). ETA=11:16:16, max mem: 20.9 GB 
[11/27 04:34:05 visual_prompt]: 	Training 500/553. train loss: 1.1302,	0.8400 s / batch. (data: 2.94e-04). ETA=11:22:02, max mem: 20.9 GB 
[11/27 04:34:54 visual_prompt]: Epoch 12 / 100: avg data time: 1.31e-01, avg batch time: 0.9565, average train loss: 0.7411
[11/27 04:35:48 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.3088, average loss: 0.7780
[11/27 04:35:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.04	
[11/27 04:35:48 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[11/27 04:37:29 visual_prompt]: 	Training 100/553. train loss: 0.5924,	0.8120 s / batch. (data: 3.01e-04). ETA=10:57:14, max mem: 20.9 GB 
[11/27 04:39:01 visual_prompt]: 	Training 200/553. train loss: 0.7854,	0.8481 s / batch. (data: 1.55e-02). ETA=11:25:02, max mem: 20.9 GB 
[11/27 04:40:37 visual_prompt]: 	Training 300/553. train loss: 0.6264,	1.4553 s / batch. (data: 6.36e-01). ETA=19:33:01, max mem: 20.9 GB 
[11/27 04:42:11 visual_prompt]: 	Training 400/553. train loss: 0.9327,	0.8251 s / batch. (data: 3.05e-04). ETA=11:03:40, max mem: 20.9 GB 
[11/27 04:43:47 visual_prompt]: 	Training 500/553. train loss: 0.7167,	0.8385 s / batch. (data: 2.96e-04). ETA=11:13:07, max mem: 20.9 GB 
[11/27 04:44:37 visual_prompt]: Epoch 13 / 100: avg data time: 1.30e-01, avg batch time: 0.9561, average train loss: 0.7423
[11/27 04:45:31 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3076, average loss: 0.6906
[11/27 04:45:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.40	
[11/27 04:45:31 visual_prompt]: Best epoch 13: best metric: -0.691
[11/27 04:45:31 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[11/27 04:47:12 visual_prompt]: 	Training 100/553. train loss: 0.7079,	0.8242 s / batch. (data: 2.86e-04). ETA=10:59:31, max mem: 20.9 GB 
[11/27 04:48:47 visual_prompt]: 	Training 200/553. train loss: 0.7104,	0.8516 s / batch. (data: 1.85e-02). ETA=11:20:01, max mem: 20.9 GB 
[11/27 04:50:22 visual_prompt]: 	Training 300/553. train loss: 0.6932,	0.8240 s / batch. (data: 3.09e-04). ETA=10:56:35, max mem: 20.9 GB 
[11/27 04:51:56 visual_prompt]: 	Training 400/553. train loss: 0.6018,	0.8275 s / batch. (data: 2.90e-04). ETA=10:58:01, max mem: 20.9 GB 
[11/27 04:53:31 visual_prompt]: 	Training 500/553. train loss: 0.8810,	0.8619 s / batch. (data: 9.87e-03). ETA=11:23:56, max mem: 20.9 GB 
[11/27 04:54:20 visual_prompt]: Epoch 14 / 100: avg data time: 1.29e-01, avg batch time: 0.9554, average train loss: 0.7261
[11/27 04:55:14 visual_prompt]: Inference (val):avg data time: 4.81e-05, avg batch time: 0.3075, average loss: 0.7525
[11/27 04:55:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.57	
[11/27 04:55:14 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[11/27 04:56:54 visual_prompt]: 	Training 100/553. train loss: 0.7093,	0.8193 s / batch. (data: 3.03e-04). ETA=10:48:04, max mem: 20.9 GB 
[11/27 04:58:27 visual_prompt]: 	Training 200/553. train loss: 0.7900,	0.8107 s / batch. (data: 2.93e-04). ETA=10:39:54, max mem: 20.9 GB 
[11/27 05:00:04 visual_prompt]: 	Training 300/553. train loss: 0.7789,	0.8268 s / batch. (data: 3.12e-04). ETA=10:51:12, max mem: 20.9 GB 
[11/27 05:01:36 visual_prompt]: 	Training 400/553. train loss: 0.6269,	0.9840 s / batch. (data: 1.52e-01). ETA=12:53:23, max mem: 20.9 GB 
[11/27 05:03:12 visual_prompt]: 	Training 500/553. train loss: 0.8421,	0.8180 s / batch. (data: 7.95e-03). ETA=10:41:31, max mem: 20.9 GB 
[11/27 05:04:02 visual_prompt]: Epoch 15 / 100: avg data time: 1.28e-01, avg batch time: 0.9543, average train loss: 0.7268
[11/27 05:04:56 visual_prompt]: Inference (val):avg data time: 1.94e-04, avg batch time: 0.3076, average loss: 0.6970
[11/27 05:04:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.52	
[11/27 05:04:56 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[11/27 05:06:34 visual_prompt]: 	Training 100/553. train loss: 0.5824,	0.8164 s / batch. (data: 2.96e-04). ETA=10:38:14, max mem: 20.9 GB 
[11/27 05:08:10 visual_prompt]: 	Training 200/553. train loss: 0.8093,	0.8104 s / batch. (data: 2.95e-04). ETA=10:32:12, max mem: 20.9 GB 
[11/27 05:09:45 visual_prompt]: 	Training 300/553. train loss: 0.8776,	0.8321 s / batch. (data: 3.10e-04). ETA=10:47:44, max mem: 20.9 GB 
[11/27 05:11:20 visual_prompt]: 	Training 400/553. train loss: 0.7453,	0.8398 s / batch. (data: 7.65e-04). ETA=10:52:19, max mem: 20.9 GB 
[11/27 05:12:54 visual_prompt]: 	Training 500/553. train loss: 0.7290,	1.5069 s / batch. (data: 6.84e-01). ETA=19:27:59, max mem: 20.9 GB 
[11/27 05:13:44 visual_prompt]: Epoch 16 / 100: avg data time: 1.28e-01, avg batch time: 0.9542, average train loss: 0.7222
[11/27 05:14:38 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3076, average loss: 0.7414
[11/27 05:14:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.75	
[11/27 05:14:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[11/27 05:16:17 visual_prompt]: 	Training 100/553. train loss: 0.5557,	0.8348 s / batch. (data: 5.44e-03). ETA=10:44:54, max mem: 20.9 GB 
[11/27 05:17:53 visual_prompt]: 	Training 200/553. train loss: 0.6924,	0.8153 s / batch. (data: 2.88e-04). ETA=10:28:28, max mem: 20.9 GB 
[11/27 05:19:27 visual_prompt]: 	Training 300/553. train loss: 0.9320,	0.8320 s / batch. (data: 2.94e-04). ETA=10:39:58, max mem: 20.9 GB 
[11/27 05:21:02 visual_prompt]: 	Training 400/553. train loss: 0.7058,	1.0758 s / batch. (data: 2.48e-01). ETA=13:45:41, max mem: 20.9 GB 
[11/27 05:22:37 visual_prompt]: 	Training 500/553. train loss: 0.6474,	1.4295 s / batch. (data: 6.00e-01). ETA=18:14:50, max mem: 20.9 GB 
[11/27 05:23:28 visual_prompt]: Epoch 17 / 100: avg data time: 1.32e-01, avg batch time: 0.9581, average train loss: 0.7232
[11/27 05:24:22 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3103, average loss: 0.7062
[11/27 05:24:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.74	
[11/27 05:24:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[11/27 05:26:01 visual_prompt]: 	Training 100/553. train loss: 0.7177,	0.8179 s / batch. (data: 2.99e-04). ETA=10:24:17, max mem: 20.9 GB 
[11/27 05:27:39 visual_prompt]: 	Training 200/553. train loss: 0.7552,	0.8401 s / batch. (data: 8.10e-04). ETA=10:39:51, max mem: 20.9 GB 
[11/27 05:29:14 visual_prompt]: 	Training 300/553. train loss: 0.6352,	0.8247 s / batch. (data: 3.58e-04). ETA=10:26:43, max mem: 20.9 GB 
[11/27 05:30:49 visual_prompt]: 	Training 400/553. train loss: 0.6973,	0.8401 s / batch. (data: 1.20e-02). ETA=10:37:02, max mem: 20.9 GB 
[11/27 05:32:23 visual_prompt]: 	Training 500/553. train loss: 0.6885,	0.9578 s / batch. (data: 1.44e-01). ETA=12:04:41, max mem: 20.9 GB 
[11/27 05:33:12 visual_prompt]: Epoch 18 / 100: avg data time: 1.31e-01, avg batch time: 0.9576, average train loss: 0.7277
[11/27 05:34:06 visual_prompt]: Inference (val):avg data time: 1.69e-04, avg batch time: 0.3072, average loss: 0.7176
[11/27 05:34:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.01	
[11/27 05:34:06 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[11/27 05:35:45 visual_prompt]: 	Training 100/553. train loss: 1.1004,	0.8363 s / batch. (data: 3.22e-04). ETA=10:30:40, max mem: 20.9 GB 
[11/27 05:37:21 visual_prompt]: 	Training 200/553. train loss: 0.7952,	0.8191 s / batch. (data: 2.88e-04). ETA=10:16:17, max mem: 20.9 GB 
[11/27 05:38:56 visual_prompt]: 	Training 300/553. train loss: 1.0886,	0.8106 s / batch. (data: 3.97e-04). ETA=10:08:33, max mem: 20.9 GB 
[11/27 05:40:32 visual_prompt]: 	Training 400/553. train loss: 0.5627,	0.8318 s / batch. (data: 1.09e-02). ETA=10:23:07, max mem: 20.9 GB 
[11/27 05:42:04 visual_prompt]: 	Training 500/553. train loss: 0.8933,	0.8360 s / batch. (data: 1.19e-02). ETA=10:24:50, max mem: 20.9 GB 
[11/27 05:42:53 visual_prompt]: Epoch 19 / 100: avg data time: 1.26e-01, avg batch time: 0.9524, average train loss: 0.7233
[11/27 05:43:47 visual_prompt]: Inference (val):avg data time: 2.08e-04, avg batch time: 0.3061, average loss: 0.6883
[11/27 05:43:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.65	
[11/27 05:43:47 visual_prompt]: Best epoch 19: best metric: -0.688
[11/27 05:43:47 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[11/27 05:45:25 visual_prompt]: 	Training 100/553. train loss: 0.6585,	0.8355 s / batch. (data: 5.43e-03). ETA=10:22:20, max mem: 20.9 GB 
[11/27 05:47:01 visual_prompt]: 	Training 200/553. train loss: 0.6598,	0.8507 s / batch. (data: 1.47e-02). ETA=10:32:15, max mem: 20.9 GB 
[11/27 05:48:36 visual_prompt]: 	Training 300/553. train loss: 0.9129,	0.8280 s / batch. (data: 2.94e-04). ETA=10:14:00, max mem: 20.9 GB 
[11/27 05:50:11 visual_prompt]: 	Training 400/553. train loss: 0.5675,	0.8177 s / batch. (data: 3.05e-04). ETA=10:05:00, max mem: 20.9 GB 
[11/27 05:51:45 visual_prompt]: 	Training 500/553. train loss: 0.8200,	0.8242 s / batch. (data: 2.98e-04). ETA=10:08:27, max mem: 20.9 GB 
[11/27 05:52:36 visual_prompt]: Epoch 20 / 100: avg data time: 1.29e-01, avg batch time: 0.9557, average train loss: 0.7394
[11/27 05:53:30 visual_prompt]: Inference (val):avg data time: 2.48e-04, avg batch time: 0.3070, average loss: 0.8410
[11/27 05:53:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.62	
[11/27 05:53:30 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[11/27 05:55:12 visual_prompt]: 	Training 100/553. train loss: 0.5638,	0.8242 s / batch. (data: 3.18e-04). ETA=10:06:21, max mem: 20.9 GB 
[11/27 05:56:46 visual_prompt]: 	Training 200/553. train loss: 0.6998,	0.8378 s / batch. (data: 9.80e-03). ETA=10:14:58, max mem: 20.9 GB 
[11/27 05:58:21 visual_prompt]: 	Training 300/553. train loss: 0.8890,	0.9996 s / batch. (data: 1.57e-01). ETA=12:12:01, max mem: 20.9 GB 
[11/27 05:59:54 visual_prompt]: 	Training 400/553. train loss: 0.6447,	0.8280 s / batch. (data: 2.99e-04). ETA=10:04:59, max mem: 20.9 GB 
[11/27 06:01:31 visual_prompt]: 	Training 500/553. train loss: 0.7090,	0.8511 s / batch. (data: 3.04e-04). ETA=10:20:27, max mem: 20.9 GB 
[11/27 06:02:19 visual_prompt]: Epoch 21 / 100: avg data time: 1.29e-01, avg batch time: 0.9567, average train loss: 0.7324
[11/27 06:03:14 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3078, average loss: 0.8239
[11/27 06:03:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.72	
[11/27 06:03:14 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[11/27 06:04:52 visual_prompt]: 	Training 100/553. train loss: 0.7025,	0.8480 s / batch. (data: 2.90e-04). ETA=10:16:02, max mem: 20.9 GB 
[11/27 06:06:27 visual_prompt]: 	Training 200/553. train loss: 0.5765,	0.8295 s / batch. (data: 5.41e-03). ETA=10:01:10, max mem: 20.9 GB 
[11/27 06:08:01 visual_prompt]: 	Training 300/553. train loss: 0.4743,	0.8446 s / batch. (data: 8.57e-03). ETA=10:10:45, max mem: 20.9 GB 
[11/27 06:09:37 visual_prompt]: 	Training 400/553. train loss: 0.6852,	0.8283 s / batch. (data: 2.89e-04). ETA=9:57:34, max mem: 20.9 GB 
[11/27 06:11:12 visual_prompt]: 	Training 500/553. train loss: 0.7153,	0.8240 s / batch. (data: 3.09e-04). ETA=9:53:06, max mem: 20.9 GB 
[11/27 06:12:03 visual_prompt]: Epoch 22 / 100: avg data time: 1.29e-01, avg batch time: 0.9561, average train loss: 0.7361
[11/27 06:12:57 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3076, average loss: 0.7459
[11/27 06:12:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.08	
[11/27 06:12:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[11/27 06:14:37 visual_prompt]: 	Training 100/553. train loss: 0.7706,	0.8477 s / batch. (data: 9.88e-03). ETA=10:08:00, max mem: 20.9 GB 
[11/27 06:16:13 visual_prompt]: 	Training 200/553. train loss: 0.5909,	0.8320 s / batch. (data: 3.60e-04). ETA=9:55:21, max mem: 20.9 GB 
[11/27 06:17:50 visual_prompt]: 	Training 300/553. train loss: 0.9823,	0.8114 s / batch. (data: 3.07e-04). ETA=9:39:16, max mem: 20.9 GB 
[11/27 06:19:23 visual_prompt]: 	Training 400/553. train loss: 0.5615,	0.8358 s / batch. (data: 1.05e-02). ETA=9:55:18, max mem: 20.9 GB 
[11/27 06:20:57 visual_prompt]: 	Training 500/553. train loss: 0.8922,	0.8320 s / batch. (data: 3.24e-04). ETA=9:51:11, max mem: 20.9 GB 
[11/27 06:21:46 visual_prompt]: Epoch 23 / 100: avg data time: 1.30e-01, avg batch time: 0.9575, average train loss: 0.7355
[11/27 06:22:41 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3079, average loss: 0.6890
[11/27 06:22:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.04	
[11/27 06:22:41 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[11/27 06:24:17 visual_prompt]: 	Training 100/553. train loss: 0.8287,	0.8400 s / batch. (data: 3.11e-04). ETA=9:54:43, max mem: 20.9 GB 
[11/27 06:25:53 visual_prompt]: 	Training 200/553. train loss: 0.8403,	0.8412 s / batch. (data: 7.71e-04). ETA=9:54:12, max mem: 20.9 GB 
[11/27 06:27:28 visual_prompt]: 	Training 300/553. train loss: 0.7115,	0.8288 s / batch. (data: 5.61e-03). ETA=9:44:03, max mem: 20.9 GB 
[11/27 06:29:03 visual_prompt]: 	Training 400/553. train loss: 0.6434,	0.8321 s / batch. (data: 3.02e-04). ETA=9:44:59, max mem: 20.9 GB 
[11/27 06:30:39 visual_prompt]: 	Training 500/553. train loss: 0.6983,	0.8274 s / batch. (data: 1.05e-02). ETA=9:40:16, max mem: 20.9 GB 
[11/27 06:31:30 visual_prompt]: Epoch 24 / 100: avg data time: 1.30e-01, avg batch time: 0.9562, average train loss: 0.7322
[11/27 06:32:24 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3087, average loss: 0.7030
[11/27 06:32:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.87	
[11/27 06:32:24 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[11/27 06:34:06 visual_prompt]: 	Training 100/553. train loss: 0.6338,	0.8350 s / batch. (data: 1.05e-02). ETA=9:43:31, max mem: 20.9 GB 
[11/27 06:35:38 visual_prompt]: 	Training 200/553. train loss: 0.7358,	0.9843 s / batch. (data: 1.60e-01). ETA=11:26:12, max mem: 20.9 GB 
[11/27 06:37:13 visual_prompt]: 	Training 300/553. train loss: 0.6945,	0.8280 s / batch. (data: 3.08e-04). ETA=9:35:50, max mem: 20.9 GB 
[11/27 06:38:47 visual_prompt]: 	Training 400/553. train loss: 0.6581,	1.1701 s / batch. (data: 3.08e-01). ETA=13:31:50, max mem: 20.9 GB 
[11/27 06:40:23 visual_prompt]: 	Training 500/553. train loss: 0.8074,	1.2781 s / batch. (data: 4.68e-01). ETA=14:44:35, max mem: 20.9 GB 
[11/27 06:41:13 visual_prompt]: Epoch 25 / 100: avg data time: 1.31e-01, avg batch time: 0.9571, average train loss: 0.7230
[11/27 06:42:07 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3081, average loss: 0.7684
[11/27 06:42:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.02	
[11/27 06:42:07 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[11/27 06:43:46 visual_prompt]: 	Training 100/553. train loss: 0.6225,	0.8094 s / batch. (data: 2.88e-04). ETA=9:18:08, max mem: 20.9 GB 
[11/27 06:45:23 visual_prompt]: 	Training 200/553. train loss: 0.7678,	1.6004 s / batch. (data: 7.64e-01). ETA=18:20:56, max mem: 20.9 GB 
[11/27 06:47:00 visual_prompt]: 	Training 300/553. train loss: 0.4548,	0.8318 s / batch. (data: 1.10e-02). ETA=9:30:47, max mem: 20.9 GB 
[11/27 06:48:33 visual_prompt]: 	Training 400/553. train loss: 0.6088,	0.8415 s / batch. (data: 7.95e-03). ETA=9:36:03, max mem: 20.9 GB 
[11/27 06:50:07 visual_prompt]: 	Training 500/553. train loss: 0.6843,	0.8371 s / batch. (data: 1.05e-02). ETA=9:31:38, max mem: 20.9 GB 
[11/27 06:50:57 visual_prompt]: Epoch 26 / 100: avg data time: 1.31e-01, avg batch time: 0.9566, average train loss: 0.7344
[11/27 06:51:51 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3083, average loss: 0.7692
[11/27 06:51:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.33	
[11/27 06:51:51 visual_prompt]: Stopping early.
[11/27 06:51:51 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 06:51:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 06:51:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/27 06:51:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 06:51:51 visual_prompt]: Training with config:
[11/27 06:51:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.05_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/27 06:51:51 visual_prompt]: Loading training data...
[11/27 06:51:51 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 06:51:51 visual_prompt]: Loading validation data...
[11/27 06:51:51 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 06:51:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 06:51:54 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 06:51:54 visual_prompt]: tuned percent:0.525
[11/27 06:51:54 visual_prompt]: Device used for model: 0
[11/27 06:51:54 visual_prompt]: Setting up Evaluator...
[11/27 06:51:54 visual_prompt]: Setting up Trainer...
[11/27 06:51:54 visual_prompt]: 	Setting up the optimizer...
[11/27 06:51:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 06:53:32 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8760 s / batch. (data: 1.60e-02). ETA=13:25:55, max mem: 20.9 GB 
[11/27 06:55:06 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8428 s / batch. (data: 5.42e-03). ETA=12:53:58, max mem: 20.9 GB 
[11/27 06:56:44 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8505 s / batch. (data: 2.38e-02). ETA=12:59:36, max mem: 20.9 GB 
[11/27 06:58:17 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 3.06e-04). ETA=12:52:14, max mem: 20.9 GB 
[11/27 06:59:54 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8519 s / batch. (data: 3.31e-04). ETA=12:58:05, max mem: 20.9 GB 
[11/27 07:00:44 visual_prompt]: Epoch 1 / 100: avg data time: 1.33e-01, avg batch time: 0.9594, average train loss: 1.5403
[11/27 07:01:39 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3070, average loss: 1.5201
[11/27 07:01:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 07:01:39 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[11/27 07:03:18 visual_prompt]: 	Training 100/553. train loss: 0.7377,	0.8229 s / batch. (data: 3.25e-04). ETA=12:29:26, max mem: 20.9 GB 
[11/27 07:04:52 visual_prompt]: 	Training 200/553. train loss: 0.4376,	0.8395 s / batch. (data: 1.69e-02). ETA=12:43:14, max mem: 20.9 GB 
[11/27 07:06:29 visual_prompt]: 	Training 300/553. train loss: 0.7006,	0.8770 s / batch. (data: 3.75e-02). ETA=13:15:48, max mem: 20.9 GB 
[11/27 07:08:03 visual_prompt]: 	Training 400/553. train loss: 0.7505,	0.8374 s / batch. (data: 5.47e-03). ETA=12:38:27, max mem: 20.9 GB 
[11/27 07:09:40 visual_prompt]: 	Training 500/553. train loss: 0.6929,	0.8374 s / batch. (data: 2.86e-04). ETA=12:37:04, max mem: 20.9 GB 
[11/27 07:10:28 visual_prompt]: Epoch 2 / 100: avg data time: 1.31e-01, avg batch time: 0.9574, average train loss: 0.7638
[11/27 07:11:22 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3079, average loss: 0.7317
[11/27 07:11:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.88	
[11/27 07:11:22 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[11/27 07:13:00 visual_prompt]: 	Training 100/553. train loss: 0.7687,	0.8401 s / batch. (data: 5.41e-03). ETA=12:37:26, max mem: 20.9 GB 
[11/27 07:14:37 visual_prompt]: 	Training 200/553. train loss: 0.7558,	1.3817 s / batch. (data: 5.52e-01). ETA=20:43:21, max mem: 20.9 GB 
[11/27 07:16:11 visual_prompt]: 	Training 300/553. train loss: 0.5115,	0.8357 s / batch. (data: 3.66e-03). ETA=12:30:37, max mem: 20.9 GB 
[11/27 07:17:46 visual_prompt]: 	Training 400/553. train loss: 0.5871,	0.8229 s / batch. (data: 3.11e-04). ETA=12:17:47, max mem: 20.9 GB 
[11/27 07:19:23 visual_prompt]: 	Training 500/553. train loss: 0.6927,	1.0742 s / batch. (data: 2.45e-01). ETA=16:01:16, max mem: 20.9 GB 
[11/27 07:20:11 visual_prompt]: Epoch 3 / 100: avg data time: 1.30e-01, avg batch time: 0.9564, average train loss: 0.7450
[11/27 07:21:06 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3108, average loss: 0.7359
[11/27 07:21:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.11	
[11/27 07:21:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[11/27 07:22:46 visual_prompt]: 	Training 100/553. train loss: 0.7596,	0.8413 s / batch. (data: 2.06e-02). ETA=12:30:44, max mem: 20.9 GB 
[11/27 07:24:21 visual_prompt]: 	Training 200/553. train loss: 0.6530,	0.8332 s / batch. (data: 2.96e-04). ETA=12:22:09, max mem: 20.9 GB 
[11/27 07:25:57 visual_prompt]: 	Training 300/553. train loss: 0.6034,	1.0476 s / batch. (data: 2.25e-01). ETA=15:31:18, max mem: 20.9 GB 
[11/27 07:27:28 visual_prompt]: 	Training 400/553. train loss: 0.7585,	0.9313 s / batch. (data: 1.13e-01). ETA=13:46:22, max mem: 20.9 GB 
[11/27 07:29:05 visual_prompt]: 	Training 500/553. train loss: 0.5431,	3.3400 s / batch. (data: 2.52e+00). ETA=2 days, 1:18:10, max mem: 20.9 GB 
[11/27 07:29:56 visual_prompt]: Epoch 4 / 100: avg data time: 1.32e-01, avg batch time: 0.9586, average train loss: 0.7482
[11/27 07:30:50 visual_prompt]: Inference (val):avg data time: 2.93e-04, avg batch time: 0.3095, average loss: 0.6863
[11/27 07:30:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.11	
[11/27 07:30:50 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[11/27 07:32:28 visual_prompt]: 	Training 100/553. train loss: 0.4273,	0.8311 s / batch. (data: 2.84e-04). ETA=12:13:59, max mem: 20.9 GB 
[11/27 07:34:03 visual_prompt]: 	Training 200/553. train loss: 0.7193,	0.8960 s / batch. (data: 8.15e-02). ETA=13:09:47, max mem: 20.9 GB 
[11/27 07:35:40 visual_prompt]: 	Training 300/553. train loss: 0.8296,	0.8213 s / batch. (data: 5.45e-03). ETA=12:02:34, max mem: 20.9 GB 
[11/27 07:37:14 visual_prompt]: 	Training 400/553. train loss: 0.5730,	0.8300 s / batch. (data: 9.94e-03). ETA=12:08:50, max mem: 20.9 GB 
[11/27 07:38:50 visual_prompt]: 	Training 500/553. train loss: 0.6621,	0.8257 s / batch. (data: 1.05e-02). ETA=12:03:42, max mem: 20.9 GB 
[11/27 07:39:40 visual_prompt]: Epoch 5 / 100: avg data time: 1.31e-01, avg batch time: 0.9580, average train loss: 0.7475
[11/27 07:40:34 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3088, average loss: 0.6859
[11/27 07:40:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.79	
[11/27 07:40:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[11/27 07:42:15 visual_prompt]: 	Training 100/553. train loss: 0.5581,	0.8233 s / batch. (data: 5.43e-03). ETA=11:59:28, max mem: 20.9 GB 
[11/27 07:43:49 visual_prompt]: 	Training 200/553. train loss: 0.7856,	0.8234 s / batch. (data: 2.95e-04). ETA=11:58:15, max mem: 20.9 GB 
[11/27 07:45:23 visual_prompt]: 	Training 300/553. train loss: 0.5082,	0.8440 s / batch. (data: 3.06e-04). ETA=12:14:46, max mem: 20.9 GB 
[11/27 07:47:01 visual_prompt]: 	Training 400/553. train loss: 0.6244,	0.8371 s / batch. (data: 2.48e-02). ETA=12:07:20, max mem: 20.9 GB 
[11/27 07:48:35 visual_prompt]: 	Training 500/553. train loss: 0.6543,	0.8480 s / batch. (data: 3.19e-04). ETA=12:15:26, max mem: 20.9 GB 
[11/27 07:49:24 visual_prompt]: Epoch 6 / 100: avg data time: 1.33e-01, avg batch time: 0.9584, average train loss: 0.7310
[11/27 07:50:19 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3097, average loss: 0.6828
[11/27 07:50:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.77	
[11/27 07:50:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[11/27 07:51:56 visual_prompt]: 	Training 100/553. train loss: 0.4717,	0.8360 s / batch. (data: 3.01e-04). ETA=12:02:53, max mem: 20.9 GB 
[11/27 07:53:31 visual_prompt]: 	Training 200/553. train loss: 0.5221,	1.3566 s / batch. (data: 5.41e-01). ETA=19:30:49, max mem: 20.9 GB 
[11/27 07:55:09 visual_prompt]: 	Training 300/553. train loss: 0.8645,	1.2361 s / batch. (data: 4.27e-01). ETA=17:44:44, max mem: 20.9 GB 
[11/27 07:56:44 visual_prompt]: 	Training 400/553. train loss: 0.7020,	1.4708 s / batch. (data: 6.62e-01). ETA=21:04:26, max mem: 20.9 GB 
[11/27 07:58:18 visual_prompt]: 	Training 500/553. train loss: 0.8363,	0.8274 s / batch. (data: 1.85e-04). ETA=11:49:55, max mem: 20.9 GB 
[11/27 07:59:07 visual_prompt]: Epoch 7 / 100: avg data time: 1.28e-01, avg batch time: 0.9548, average train loss: 0.7344
[11/27 08:00:01 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3090, average loss: 0.7813
[11/27 08:00:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.20	
[11/27 08:00:01 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[11/27 08:01:38 visual_prompt]: 	Training 100/553. train loss: 0.7149,	0.8102 s / batch. (data: 3.58e-04). ETA=11:33:05, max mem: 20.9 GB 
[11/27 08:03:15 visual_prompt]: 	Training 200/553. train loss: 1.1066,	0.8355 s / batch. (data: 1.61e-02). ETA=11:53:21, max mem: 20.9 GB 
[11/27 08:04:50 visual_prompt]: 	Training 300/553. train loss: 0.6177,	0.8795 s / batch. (data: 1.15e-02). ETA=12:29:28, max mem: 20.9 GB 
[11/27 08:06:25 visual_prompt]: 	Training 400/553. train loss: 0.7028,	0.8360 s / batch. (data: 2.96e-04). ETA=11:51:00, max mem: 20.9 GB 
[11/27 08:08:00 visual_prompt]: 	Training 500/553. train loss: 0.7957,	1.3216 s / batch. (data: 4.84e-01). ETA=18:41:50, max mem: 20.9 GB 
[11/27 08:08:51 visual_prompt]: Epoch 8 / 100: avg data time: 1.31e-01, avg batch time: 0.9573, average train loss: 0.7526
[11/27 08:09:45 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3070, average loss: 0.7457
[11/27 08:09:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.94	
[11/27 08:09:45 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[11/27 08:11:24 visual_prompt]: 	Training 100/553. train loss: 0.4952,	0.8320 s / batch. (data: 3.02e-04). ETA=11:44:05, max mem: 20.9 GB 
[11/27 08:12:58 visual_prompt]: 	Training 200/553. train loss: 0.5725,	0.8230 s / batch. (data: 3.10e-04). ETA=11:35:03, max mem: 20.9 GB 
[11/27 08:14:35 visual_prompt]: 	Training 300/553. train loss: 0.6663,	1.4920 s / batch. (data: 6.81e-01). ETA=20:57:39, max mem: 20.9 GB 
[11/27 08:16:25 visual_prompt]: 	Training 400/553. train loss: 0.5779,	0.8329 s / batch. (data: 1.64e-02). ETA=11:40:43, max mem: 20.9 GB 
[11/27 08:18:14 visual_prompt]: 	Training 500/553. train loss: 0.6268,	0.8480 s / batch. (data: 6.65e-03). ETA=11:51:58, max mem: 20.9 GB 
[11/27 08:19:05 visual_prompt]: Epoch 9 / 100: avg data time: 1.88e-01, avg batch time: 1.0127, average train loss: 0.7348
[11/27 08:20:01 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3072, average loss: 0.6957
[11/27 08:20:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 58.67	
[11/27 08:20:01 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[11/27 08:21:45 visual_prompt]: 	Training 100/553. train loss: 0.6762,	0.8415 s / batch. (data: 3.41e-04). ETA=11:44:22, max mem: 20.9 GB 
[11/27 08:23:20 visual_prompt]: 	Training 200/553. train loss: 0.6689,	0.8120 s / batch. (data: 2.90e-04). ETA=11:18:19, max mem: 20.9 GB 
[11/27 08:24:56 visual_prompt]: 	Training 300/553. train loss: 0.7099,	0.8301 s / batch. (data: 1.01e-02). ETA=11:32:04, max mem: 20.9 GB 
[11/27 08:26:31 visual_prompt]: 	Training 400/553. train loss: 0.8645,	0.8231 s / batch. (data: 2.17e-04). ETA=11:24:49, max mem: 20.9 GB 
[11/27 08:28:13 visual_prompt]: 	Training 500/553. train loss: 0.9624,	1.1412 s / batch. (data: 3.14e-01). ETA=15:47:36, max mem: 20.9 GB 
[11/27 08:29:05 visual_prompt]: Epoch 10 / 100: avg data time: 1.58e-01, avg batch time: 0.9822, average train loss: 0.7458
[11/27 08:30:00 visual_prompt]: Inference (val):avg data time: 4.06e-05, avg batch time: 0.3089, average loss: 0.8842
[11/27 08:30:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.55	
[11/27 08:30:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[11/27 08:31:43 visual_prompt]: 	Training 100/553. train loss: 0.6035,	0.8320 s / batch. (data: 3.32e-04). ETA=11:28:45, max mem: 20.9 GB 
[11/27 08:33:22 visual_prompt]: 	Training 200/553. train loss: 1.2721,	0.8369 s / batch. (data: 7.92e-03). ETA=11:31:24, max mem: 20.9 GB 
[11/27 08:34:59 visual_prompt]: 	Training 300/553. train loss: 0.4828,	2.2166 s / batch. (data: 1.40e+00). ETA=1 day, 6:27:34, max mem: 20.9 GB 
[11/27 08:36:34 visual_prompt]: 	Training 400/553. train loss: 0.6977,	0.8211 s / batch. (data: 2.88e-04). ETA=11:15:40, max mem: 20.9 GB 
[11/27 08:38:11 visual_prompt]: 	Training 500/553. train loss: 0.6663,	0.8227 s / batch. (data: 2.88e-04). ETA=11:15:33, max mem: 20.9 GB 
[11/27 08:39:01 visual_prompt]: Epoch 11 / 100: avg data time: 1.51e-01, avg batch time: 0.9774, average train loss: 0.7363
[11/27 08:39:55 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3098, average loss: 0.6960
[11/27 08:39:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 65.81	
[11/27 08:39:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[11/27 08:41:37 visual_prompt]: 	Training 100/553. train loss: 1.0449,	0.8960 s / batch. (data: 8.34e-02). ETA=12:13:30, max mem: 20.9 GB 
[11/27 08:43:15 visual_prompt]: 	Training 200/553. train loss: 0.5499,	0.8239 s / batch. (data: 3.39e-04). ETA=11:13:06, max mem: 20.9 GB 
[11/27 08:44:53 visual_prompt]: 	Training 300/553. train loss: 0.7099,	0.8593 s / batch. (data: 1.12e-02). ETA=11:40:33, max mem: 20.9 GB 
[11/27 08:46:30 visual_prompt]: 	Training 400/553. train loss: 0.7403,	0.8217 s / batch. (data: 8.50e-03). ETA=11:08:34, max mem: 20.9 GB 
[11/27 08:48:05 visual_prompt]: 	Training 500/553. train loss: 1.3390,	0.8320 s / batch. (data: 2.71e-04). ETA=11:15:32, max mem: 20.9 GB 
[11/27 08:48:54 visual_prompt]: Epoch 12 / 100: avg data time: 1.49e-01, avg batch time: 0.9744, average train loss: 0.7357
[11/27 08:49:49 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3080, average loss: 0.8484
[11/27 08:49:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.39	
[11/27 08:49:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[11/27 08:51:29 visual_prompt]: 	Training 100/553. train loss: 0.6136,	0.8191 s / batch. (data: 3.06e-04). ETA=11:02:59, max mem: 20.9 GB 
[11/27 08:53:01 visual_prompt]: 	Training 200/553. train loss: 0.7408,	0.8522 s / batch. (data: 7.95e-03). ETA=11:28:20, max mem: 20.9 GB 
[11/27 08:54:38 visual_prompt]: 	Training 300/553. train loss: 0.6415,	1.6339 s / batch. (data: 8.15e-01). ETA=21:57:02, max mem: 20.9 GB 
[11/27 08:56:12 visual_prompt]: 	Training 400/553. train loss: 0.9823,	0.8240 s / batch. (data: 3.18e-04). ETA=11:02:49, max mem: 20.9 GB 
[11/27 08:57:48 visual_prompt]: 	Training 500/553. train loss: 0.7093,	0.8296 s / batch. (data: 2.96e-04). ETA=11:05:58, max mem: 20.9 GB 
[11/27 08:58:38 visual_prompt]: Epoch 13 / 100: avg data time: 1.32e-01, avg batch time: 0.9580, average train loss: 0.7425
[11/27 08:59:33 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3068, average loss: 0.6855
[11/27 08:59:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.59	
[11/27 08:59:33 visual_prompt]: Best epoch 13: best metric: -0.685
[11/27 08:59:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[11/27 09:01:13 visual_prompt]: 	Training 100/553. train loss: 0.8391,	0.8099 s / batch. (data: 2.79e-04). ETA=10:48:05, max mem: 20.9 GB 
[11/27 09:02:47 visual_prompt]: 	Training 200/553. train loss: 0.6977,	0.8463 s / batch. (data: 3.29e-04). ETA=11:15:46, max mem: 20.9 GB 
[11/27 09:04:24 visual_prompt]: 	Training 300/553. train loss: 0.6982,	0.8247 s / batch. (data: 7.94e-03). ETA=10:57:11, max mem: 20.9 GB 
[11/27 09:05:58 visual_prompt]: 	Training 400/553. train loss: 0.6335,	0.8479 s / batch. (data: 7.89e-03). ETA=11:14:15, max mem: 20.9 GB 
[11/27 09:07:33 visual_prompt]: 	Training 500/553. train loss: 0.8423,	0.8318 s / batch. (data: 7.95e-03). ETA=11:00:04, max mem: 20.9 GB 
[11/27 09:08:23 visual_prompt]: Epoch 14 / 100: avg data time: 1.33e-01, avg batch time: 0.9585, average train loss: 0.7145
[11/27 09:09:17 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3087, average loss: 0.7704
[11/27 09:09:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.79	
[11/27 09:09:17 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[11/27 09:10:57 visual_prompt]: 	Training 100/553. train loss: 0.6784,	0.8480 s / batch. (data: 1.20e-02). ETA=11:10:44, max mem: 20.9 GB 
[11/27 09:12:30 visual_prompt]: 	Training 200/553. train loss: 0.6306,	0.8826 s / batch. (data: 6.13e-02). ETA=11:36:37, max mem: 20.9 GB 
[11/27 09:14:08 visual_prompt]: 	Training 300/553. train loss: 0.7302,	0.8287 s / batch. (data: 7.74e-04). ETA=10:52:43, max mem: 20.9 GB 
[11/27 09:15:40 visual_prompt]: 	Training 400/553. train loss: 0.5427,	1.1116 s / batch. (data: 3.00e-01). ETA=14:33:42, max mem: 20.9 GB 
[11/27 09:17:16 visual_prompt]: 	Training 500/553. train loss: 0.9513,	0.8234 s / batch. (data: 3.05e-04). ETA=10:45:46, max mem: 20.9 GB 
[11/27 09:18:07 visual_prompt]: Epoch 15 / 100: avg data time: 1.31e-01, avg batch time: 0.9576, average train loss: 0.7157
[11/27 09:19:01 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3079, average loss: 0.6965
[11/27 09:19:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.65	
[11/27 09:19:01 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[11/27 09:20:39 visual_prompt]: 	Training 100/553. train loss: 0.5941,	0.8440 s / batch. (data: 3.01e-04). ETA=10:59:46, max mem: 20.9 GB 
[11/27 09:22:14 visual_prompt]: 	Training 200/553. train loss: 0.7367,	0.8247 s / batch. (data: 5.25e-04). ETA=10:43:21, max mem: 20.9 GB 
[11/27 09:23:50 visual_prompt]: 	Training 300/553. train loss: 1.1750,	0.8253 s / batch. (data: 2.89e-04). ETA=10:42:23, max mem: 20.9 GB 
[11/27 09:25:25 visual_prompt]: 	Training 400/553. train loss: 0.5177,	0.8252 s / batch. (data: 7.71e-04). ETA=10:40:57, max mem: 20.9 GB 
[11/27 09:27:00 visual_prompt]: 	Training 500/553. train loss: 0.6695,	1.3757 s / batch. (data: 5.68e-01). ETA=17:46:14, max mem: 20.9 GB 
[11/27 09:27:50 visual_prompt]: Epoch 16 / 100: avg data time: 1.30e-01, avg batch time: 0.9565, average train loss: 0.7071
[11/27 09:28:44 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3069, average loss: 0.7639
[11/27 09:28:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.04	
[11/27 09:28:44 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[11/27 09:30:22 visual_prompt]: 	Training 100/553. train loss: 0.4896,	0.8229 s / batch. (data: 3.12e-04). ETA=10:35:44, max mem: 20.9 GB 
[11/27 09:31:59 visual_prompt]: 	Training 200/553. train loss: 0.6011,	0.8529 s / batch. (data: 1.55e-02). ETA=10:57:29, max mem: 20.9 GB 
[11/27 09:33:33 visual_prompt]: 	Training 300/553. train loss: 0.9392,	0.8279 s / batch. (data: 5.41e-03). ETA=10:36:50, max mem: 20.9 GB 
[11/27 09:35:08 visual_prompt]: 	Training 400/553. train loss: 0.5487,	1.0037 s / batch. (data: 1.94e-01). ETA=12:50:24, max mem: 20.9 GB 
[11/27 09:36:43 visual_prompt]: 	Training 500/553. train loss: 0.6076,	1.3343 s / batch. (data: 5.21e-01). ETA=17:01:55, max mem: 20.9 GB 
[11/27 09:37:34 visual_prompt]: Epoch 17 / 100: avg data time: 1.32e-01, avg batch time: 0.9585, average train loss: 0.7054
[11/27 09:38:29 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3076, average loss: 0.7308
[11/27 09:38:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.88	
[11/27 09:38:29 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[11/27 09:40:08 visual_prompt]: 	Training 100/553. train loss: 0.6871,	0.8274 s / batch. (data: 2.88e-04). ETA=10:31:35, max mem: 20.9 GB 
[11/27 09:41:45 visual_prompt]: 	Training 200/553. train loss: 0.7586,	0.8522 s / batch. (data: 1.05e-02). ETA=10:49:04, max mem: 20.9 GB 
[11/27 09:43:21 visual_prompt]: 	Training 300/553. train loss: 0.5201,	0.8248 s / batch. (data: 2.87e-04). ETA=10:26:51, max mem: 20.9 GB 
[11/27 09:44:56 visual_prompt]: 	Training 400/553. train loss: 0.6085,	0.8241 s / batch. (data: 2.93e-04). ETA=10:24:55, max mem: 20.9 GB 
[11/27 09:46:31 visual_prompt]: 	Training 500/553. train loss: 0.7352,	0.8280 s / batch. (data: 2.93e-04). ETA=10:26:30, max mem: 20.9 GB 
[11/27 09:47:22 visual_prompt]: Epoch 18 / 100: avg data time: 1.38e-01, avg batch time: 0.9646, average train loss: 0.7040
[11/27 09:48:21 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3079, average loss: 0.6981
[11/27 09:48:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.49	
[11/27 09:48:21 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[11/27 09:50:16 visual_prompt]: 	Training 100/553. train loss: 1.1094,	0.8280 s / batch. (data: 2.66e-04). ETA=10:24:24, max mem: 20.9 GB 
[11/27 09:52:04 visual_prompt]: 	Training 200/553. train loss: 0.6763,	0.8138 s / batch. (data: 3.22e-04). ETA=10:12:20, max mem: 20.9 GB 
[11/27 09:53:42 visual_prompt]: 	Training 300/553. train loss: 1.0866,	0.8360 s / batch. (data: 3.00e-04). ETA=10:27:39, max mem: 20.9 GB 
[11/27 09:55:21 visual_prompt]: 	Training 400/553. train loss: 0.5175,	0.8559 s / batch. (data: 7.72e-04). ETA=10:41:09, max mem: 20.9 GB 
[11/27 09:56:55 visual_prompt]: 	Training 500/553. train loss: 0.9461,	0.8171 s / batch. (data: 3.01e-04). ETA=10:10:43, max mem: 20.9 GB 
[11/27 09:57:47 visual_prompt]: Epoch 19 / 100: avg data time: 1.99e-01, avg batch time: 1.0235, average train loss: 0.6995
[11/27 09:58:41 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3091, average loss: 0.6759
[11/27 09:58:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.89	
[11/27 09:58:41 visual_prompt]: Best epoch 19: best metric: -0.676
[11/27 09:58:41 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[11/27 10:00:19 visual_prompt]: 	Training 100/553. train loss: 0.8614,	0.8121 s / batch. (data: 2.91e-04). ETA=10:04:55, max mem: 20.9 GB 
[11/27 10:01:56 visual_prompt]: 	Training 200/553. train loss: 0.6265,	1.0311 s / batch. (data: 1.99e-01). ETA=12:46:20, max mem: 20.9 GB 
[11/27 10:03:31 visual_prompt]: 	Training 300/553. train loss: 0.6069,	0.8315 s / batch. (data: 1.89e-04). ETA=10:16:33, max mem: 20.9 GB 
[11/27 10:05:06 visual_prompt]: 	Training 400/553. train loss: 0.7811,	0.8560 s / batch. (data: 1.20e-02). ETA=10:33:21, max mem: 20.9 GB 
[11/27 10:06:40 visual_prompt]: 	Training 500/553. train loss: 0.7462,	0.8320 s / batch. (data: 5.42e-03). ETA=10:14:11, max mem: 20.9 GB 
[11/27 10:07:31 visual_prompt]: Epoch 20 / 100: avg data time: 1.32e-01, avg batch time: 0.9590, average train loss: 0.7158
[11/27 10:08:39 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3081, average loss: 0.7135
[11/27 10:08:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 59.63	
[11/27 10:08:39 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[11/27 10:10:34 visual_prompt]: 	Training 100/553. train loss: 0.5148,	0.8459 s / batch. (data: 2.37e-02). ETA=10:22:17, max mem: 20.9 GB 
[11/27 10:12:24 visual_prompt]: 	Training 200/553. train loss: 0.6576,	0.8079 s / batch. (data: 3.35e-04). ETA=9:52:57, max mem: 20.9 GB 
[11/27 10:14:16 visual_prompt]: 	Training 300/553. train loss: 0.7701,	0.8070 s / batch. (data: 2.48e-04). ETA=9:50:58, max mem: 20.9 GB 
[11/27 10:16:23 visual_prompt]: 	Training 400/553. train loss: 0.6121,	0.8359 s / batch. (data: 4.60e-04). ETA=10:10:46, max mem: 20.9 GB 
[11/27 10:18:03 visual_prompt]: 	Training 500/553. train loss: 0.7018,	0.8376 s / batch. (data: 5.47e-03). ETA=10:10:36, max mem: 20.9 GB 
[11/27 10:18:54 visual_prompt]: Epoch 21 / 100: avg data time: 2.90e-01, avg batch time: 1.1124, average train loss: 0.7203
[11/27 10:19:51 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3082, average loss: 0.7989
[11/27 10:19:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.46	
[11/27 10:19:51 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[11/27 10:21:37 visual_prompt]: 	Training 100/553. train loss: 0.6547,	0.8320 s / batch. (data: 3.01e-04). ETA=10:04:24, max mem: 20.9 GB 
[11/27 10:23:13 visual_prompt]: 	Training 200/553. train loss: 0.5790,	0.8368 s / batch. (data: 1.19e-02). ETA=10:06:28, max mem: 20.9 GB 
[11/27 10:24:47 visual_prompt]: 	Training 300/553. train loss: 0.4737,	1.3563 s / batch. (data: 5.23e-01). ETA=16:20:46, max mem: 20.9 GB 
[11/27 10:26:25 visual_prompt]: 	Training 400/553. train loss: 0.6152,	0.8277 s / batch. (data: 1.57e-02). ETA=9:57:07, max mem: 20.9 GB 
[11/27 10:28:04 visual_prompt]: 	Training 500/553. train loss: 0.8641,	1.2440 s / batch. (data: 3.96e-01). ETA=14:55:24, max mem: 20.9 GB 
[11/27 10:28:56 visual_prompt]: Epoch 22 / 100: avg data time: 1.61e-01, avg batch time: 0.9864, average train loss: 0.7176
[11/27 10:29:50 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3078, average loss: 0.7094
[11/27 10:29:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.11	
[11/27 10:29:50 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[11/27 10:31:31 visual_prompt]: 	Training 100/553. train loss: 0.7815,	0.8169 s / batch. (data: 5.43e-03). ETA=9:45:54, max mem: 20.9 GB 
[11/27 10:33:07 visual_prompt]: 	Training 200/553. train loss: 0.7190,	0.8283 s / batch. (data: 1.20e-02). ETA=9:52:40, max mem: 20.9 GB 
[11/27 10:34:43 visual_prompt]: 	Training 300/553. train loss: 0.7676,	0.8172 s / batch. (data: 3.04e-04). ETA=9:43:24, max mem: 20.9 GB 
[11/27 10:36:16 visual_prompt]: 	Training 400/553. train loss: 0.4908,	0.8480 s / batch. (data: 7.17e-04). ETA=10:03:58, max mem: 20.9 GB 
[11/27 10:37:49 visual_prompt]: 	Training 500/553. train loss: 0.7832,	0.8240 s / batch. (data: 3.19e-04). ETA=9:45:30, max mem: 20.9 GB 
[11/27 10:38:39 visual_prompt]: Epoch 23 / 100: avg data time: 1.29e-01, avg batch time: 0.9557, average train loss: 0.7149
[11/27 10:39:33 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3091, average loss: 0.6814
[11/27 10:39:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 60.36	
[11/27 10:39:33 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[11/27 10:41:10 visual_prompt]: 	Training 100/553. train loss: 0.8074,	0.8122 s / batch. (data: 2.88e-04). ETA=9:35:03, max mem: 20.9 GB 
[11/27 10:42:44 visual_prompt]: 	Training 200/553. train loss: 0.7910,	0.8440 s / batch. (data: 5.41e-03). ETA=9:56:09, max mem: 20.9 GB 
[11/27 10:44:20 visual_prompt]: 	Training 300/553. train loss: 0.6703,	0.8430 s / batch. (data: 1.20e-02). ETA=9:54:02, max mem: 20.9 GB 
[11/27 10:45:55 visual_prompt]: 	Training 400/553. train loss: 0.7196,	0.8360 s / batch. (data: 3.06e-04). ETA=9:47:43, max mem: 20.9 GB 
[11/27 10:47:32 visual_prompt]: 	Training 500/553. train loss: 0.9053,	0.8360 s / batch. (data: 3.02e-04). ETA=9:46:19, max mem: 20.9 GB 
[11/27 10:48:23 visual_prompt]: Epoch 24 / 100: avg data time: 1.31e-01, avg batch time: 0.9570, average train loss: 0.7228
[11/27 10:49:17 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3066, average loss: 0.6752
[11/27 10:49:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.10	
[11/27 10:49:17 visual_prompt]: Best epoch 24: best metric: -0.675
[11/27 10:49:17 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[11/27 10:50:59 visual_prompt]: 	Training 100/553. train loss: 0.5708,	0.8372 s / batch. (data: 5.42e-03). ETA=9:45:00, max mem: 20.9 GB 
[11/27 10:52:31 visual_prompt]: 	Training 200/553. train loss: 0.8857,	0.8335 s / batch. (data: 1.56e-02). ETA=9:41:05, max mem: 20.9 GB 
[11/27 10:54:06 visual_prompt]: 	Training 300/553. train loss: 0.6814,	0.8219 s / batch. (data: 3.24e-04). ETA=9:31:36, max mem: 20.9 GB 
[11/27 10:55:41 visual_prompt]: 	Training 400/553. train loss: 0.6599,	0.9938 s / batch. (data: 1.57e-01). ETA=11:29:31, max mem: 20.9 GB 
[11/27 10:57:16 visual_prompt]: 	Training 500/553. train loss: 0.7255,	1.3601 s / batch. (data: 5.50e-01). ETA=15:41:22, max mem: 20.9 GB 
[11/27 10:58:06 visual_prompt]: Epoch 25 / 100: avg data time: 1.30e-01, avg batch time: 0.9575, average train loss: 0.7036
[11/27 10:59:01 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3068, average loss: 0.6781
[11/27 10:59:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.18	
[11/27 10:59:01 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[11/27 11:00:40 visual_prompt]: 	Training 100/553. train loss: 0.5859,	0.8164 s / batch. (data: 2.90e-04). ETA=9:23:00, max mem: 20.9 GB 
[11/27 11:02:16 visual_prompt]: 	Training 200/553. train loss: 0.5317,	1.6598 s / batch. (data: 8.47e-01). ETA=19:01:46, max mem: 20.9 GB 
[11/27 11:03:54 visual_prompt]: 	Training 300/553. train loss: 0.5353,	0.8402 s / batch. (data: 2.97e-02). ETA=9:36:35, max mem: 20.9 GB 
[11/27 11:05:30 visual_prompt]: 	Training 400/553. train loss: 0.5722,	0.8359 s / batch. (data: 1.16e-02). ETA=9:32:15, max mem: 20.9 GB 
[11/27 11:07:06 visual_prompt]: 	Training 500/553. train loss: 0.7408,	0.8668 s / batch. (data: 3.04e-04). ETA=9:51:58, max mem: 20.9 GB 
[11/27 11:07:56 visual_prompt]: Epoch 26 / 100: avg data time: 1.41e-01, avg batch time: 0.9679, average train loss: 0.7099
[11/27 11:08:50 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3087, average loss: 0.7223
[11/27 11:08:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.64	
[11/27 11:08:50 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[11/27 11:10:31 visual_prompt]: 	Training 100/553. train loss: 0.5194,	0.8340 s / batch. (data: 2.89e-04). ETA=9:27:25, max mem: 20.9 GB 
[11/27 11:12:06 visual_prompt]: 	Training 200/553. train loss: 0.6762,	0.9669 s / batch. (data: 1.31e-01). ETA=10:56:15, max mem: 20.9 GB 
[11/27 11:13:41 visual_prompt]: 	Training 300/553. train loss: 0.7751,	0.8400 s / batch. (data: 5.45e-03). ETA=9:28:42, max mem: 20.9 GB 
[11/27 11:15:19 visual_prompt]: 	Training 400/553. train loss: 0.7658,	0.8354 s / batch. (data: 8.07e-04). ETA=9:24:13, max mem: 20.9 GB 
[11/27 11:16:57 visual_prompt]: 	Training 500/553. train loss: 1.0038,	0.8323 s / batch. (data: 7.19e-04). ETA=9:20:42, max mem: 20.9 GB 
[11/27 11:17:45 visual_prompt]: Epoch 27 / 100: avg data time: 1.39e-01, avg batch time: 0.9668, average train loss: 0.7126
[11/27 11:18:39 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3091, average loss: 0.7041
[11/27 11:18:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 60.33	
[11/27 11:18:39 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[11/27 11:20:17 visual_prompt]: 	Training 100/553. train loss: 0.5460,	0.8206 s / batch. (data: 2.82e-04). ETA=9:10:44, max mem: 20.9 GB 
[11/27 11:21:53 visual_prompt]: 	Training 200/553. train loss: 0.4671,	0.8117 s / batch. (data: 2.96e-04). ETA=9:03:25, max mem: 20.9 GB 
[11/27 11:23:29 visual_prompt]: 	Training 300/553. train loss: 0.6622,	1.3614 s / batch. (data: 5.41e-01). ETA=15:09:09, max mem: 20.9 GB 
[11/27 11:25:03 visual_prompt]: 	Training 400/553. train loss: 0.8144,	0.8312 s / batch. (data: 1.16e-02). ETA=9:13:41, max mem: 20.9 GB 
[11/27 11:26:37 visual_prompt]: 	Training 500/553. train loss: 0.4774,	0.8396 s / batch. (data: 5.41e-03). ETA=9:17:54, max mem: 20.9 GB 
[11/27 11:27:28 visual_prompt]: Epoch 28 / 100: avg data time: 1.29e-01, avg batch time: 0.9564, average train loss: 0.7016
[11/27 11:28:22 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3081, average loss: 0.6792
[11/27 11:28:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.01	
[11/27 11:28:22 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[11/27 11:30:07 visual_prompt]: 	Training 100/553. train loss: 0.4954,	0.8267 s / batch. (data: 4.10e-03). ETA=9:07:13, max mem: 20.9 GB 
[11/27 11:31:41 visual_prompt]: 	Training 200/553. train loss: 0.7008,	1.4200 s / batch. (data: 6.03e-01). ETA=15:37:35, max mem: 20.9 GB 
[11/27 11:33:15 visual_prompt]: 	Training 300/553. train loss: 0.6531,	0.8344 s / batch. (data: 3.17e-04). ETA=9:09:32, max mem: 20.9 GB 
[11/27 11:34:46 visual_prompt]: 	Training 400/553. train loss: 0.6115,	0.9440 s / batch. (data: 1.16e-01). ETA=10:20:09, max mem: 20.9 GB 
[11/27 11:36:22 visual_prompt]: 	Training 500/553. train loss: 0.6648,	0.8245 s / batch. (data: 3.25e-04). ETA=9:00:16, max mem: 20.9 GB 
[11/27 11:37:12 visual_prompt]: Epoch 29 / 100: avg data time: 1.31e-01, avg batch time: 0.9579, average train loss: 0.7010
[11/27 11:38:06 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3085, average loss: 0.6879
[11/27 11:38:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 64.27	
[11/27 11:38:06 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[11/27 11:39:44 visual_prompt]: 	Training 100/553. train loss: 0.7262,	0.8520 s / batch. (data: 1.19e-02). ETA=9:16:05, max mem: 20.9 GB 
[11/27 11:41:20 visual_prompt]: 	Training 200/553. train loss: 0.5897,	0.8235 s / batch. (data: 5.40e-03). ETA=8:56:08, max mem: 20.9 GB 
[11/27 11:42:53 visual_prompt]: 	Training 300/553. train loss: 0.7719,	0.8176 s / batch. (data: 5.43e-03). ETA=8:50:57, max mem: 20.9 GB 
[11/27 11:44:30 visual_prompt]: 	Training 400/553. train loss: 0.7983,	0.9517 s / batch. (data: 1.25e-01). ETA=10:16:25, max mem: 20.9 GB 
[11/27 11:46:03 visual_prompt]: 	Training 500/553. train loss: 0.5583,	1.3345 s / batch. (data: 5.05e-01). ETA=14:22:07, max mem: 20.9 GB 
[11/27 11:46:55 visual_prompt]: Epoch 30 / 100: avg data time: 1.28e-01, avg batch time: 0.9553, average train loss: 0.7033
[11/27 11:47:49 visual_prompt]: Inference (val):avg data time: 1.62e-04, avg batch time: 0.3066, average loss: 0.6767
[11/27 11:47:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.39	
[11/27 11:47:49 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[11/27 11:49:29 visual_prompt]: 	Training 100/553. train loss: 0.5774,	0.8480 s / batch. (data: 2.60e-04). ETA=9:05:42, max mem: 20.9 GB 
[11/27 11:51:06 visual_prompt]: 	Training 200/553. train loss: 0.6788,	0.8494 s / batch. (data: 1.56e-02). ETA=9:05:09, max mem: 20.9 GB 
[11/27 11:52:38 visual_prompt]: 	Training 300/553. train loss: 0.7060,	0.8190 s / batch. (data: 2.97e-04). ETA=8:44:19, max mem: 20.9 GB 
[11/27 11:54:13 visual_prompt]: 	Training 400/553. train loss: 0.5595,	0.9560 s / batch. (data: 1.25e-01). ETA=10:10:26, max mem: 20.9 GB 
[11/27 11:55:48 visual_prompt]: 	Training 500/553. train loss: 0.9313,	0.8577 s / batch. (data: 2.06e-02). ETA=9:06:14, max mem: 20.9 GB 
[11/27 11:56:37 visual_prompt]: Epoch 31 / 100: avg data time: 1.29e-01, avg batch time: 0.9548, average train loss: 0.7032
[11/27 11:57:31 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3098, average loss: 0.7206
[11/27 11:57:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.26	
[11/27 11:57:31 visual_prompt]: Stopping early.
[11/27 11:57:31 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 11:57:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 11:57:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/27 11:57:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 11:57:31 visual_prompt]: Training with config:
[11/27 11:57:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.05_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/27 11:57:31 visual_prompt]: Loading training data...
[11/27 11:57:31 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 11:57:31 visual_prompt]: Loading validation data...
[11/27 11:57:31 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 11:57:31 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 11:57:37 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 11:57:37 visual_prompt]: tuned percent:0.525
[11/27 11:57:37 visual_prompt]: Device used for model: 0
[11/27 11:57:37 visual_prompt]: Setting up Evaluator...
[11/27 11:57:37 visual_prompt]: Setting up Trainer...
[11/27 11:57:37 visual_prompt]: 	Setting up the optimizer...
[11/27 11:57:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 11:59:15 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8301 s / batch. (data: 7.96e-03). ETA=12:43:40, max mem: 20.9 GB 
[11/27 12:00:49 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8362 s / batch. (data: 1.21e-02). ETA=12:47:52, max mem: 20.9 GB 
[11/27 12:02:26 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.1200 s / batch. (data: 2.98e-01). ETA=17:06:42, max mem: 20.9 GB 
[11/27 12:03:59 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8240 s / batch. (data: 5.81e-03). ETA=12:33:57, max mem: 20.9 GB 
[11/27 12:05:37 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8539 s / batch. (data: 6.88e-04). ETA=12:59:51, max mem: 20.9 GB 
[11/27 12:06:27 visual_prompt]: Epoch 1 / 100: avg data time: 1.31e-01, avg batch time: 0.9581, average train loss: 1.5403
[11/27 12:07:21 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3084, average loss: 1.5201
[11/27 12:07:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 12:07:21 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[11/27 12:08:59 visual_prompt]: 	Training 100/553. train loss: 0.7379,	0.8320 s / batch. (data: 1.20e-02). ETA=12:37:45, max mem: 20.9 GB 
[11/27 12:10:33 visual_prompt]: 	Training 200/553. train loss: 0.4373,	0.8320 s / batch. (data: 3.08e-04). ETA=12:36:23, max mem: 20.9 GB 
[11/27 12:12:10 visual_prompt]: 	Training 300/553. train loss: 0.7011,	0.9200 s / batch. (data: 6.89e-02). ETA=13:54:53, max mem: 20.9 GB 
[11/27 12:13:44 visual_prompt]: 	Training 400/553. train loss: 0.7513,	0.8160 s / batch. (data: 3.22e-04). ETA=12:19:07, max mem: 20.9 GB 
[11/27 12:15:20 visual_prompt]: 	Training 500/553. train loss: 0.6931,	0.8212 s / batch. (data: 2.90e-04). ETA=12:22:28, max mem: 20.9 GB 
[11/27 12:16:09 visual_prompt]: Epoch 2 / 100: avg data time: 1.29e-01, avg batch time: 0.9547, average train loss: 0.7641
[11/27 12:17:03 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3086, average loss: 0.7316
[11/27 12:17:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.86	
[11/27 12:17:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[11/27 12:18:41 visual_prompt]: 	Training 100/553. train loss: 0.7707,	0.8348 s / batch. (data: 1.05e-02). ETA=12:32:38, max mem: 20.9 GB 
[11/27 12:20:17 visual_prompt]: 	Training 200/553. train loss: 0.7565,	0.8398 s / batch. (data: 1.05e-02). ETA=12:35:42, max mem: 20.9 GB 
[11/27 12:21:51 visual_prompt]: 	Training 300/553. train loss: 0.5069,	0.8320 s / batch. (data: 3.09e-04). ETA=12:27:19, max mem: 20.9 GB 
[11/27 12:23:27 visual_prompt]: 	Training 400/553. train loss: 0.5824,	0.8210 s / batch. (data: 2.89e-04). ETA=12:16:06, max mem: 20.9 GB 
[11/27 12:25:03 visual_prompt]: 	Training 500/553. train loss: 0.6903,	1.2166 s / batch. (data: 4.04e-01). ETA=18:08:43, max mem: 20.9 GB 
[11/27 12:25:52 visual_prompt]: Epoch 3 / 100: avg data time: 1.29e-01, avg batch time: 0.9564, average train loss: 0.7459
[11/27 12:26:46 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3077, average loss: 0.7411
[11/27 12:26:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.16	
[11/27 12:26:46 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[11/27 12:28:27 visual_prompt]: 	Training 100/553. train loss: 0.7598,	0.8838 s / batch. (data: 2.78e-02). ETA=13:08:41, max mem: 20.9 GB 
[11/27 12:30:01 visual_prompt]: 	Training 200/553. train loss: 0.6674,	0.9000 s / batch. (data: 6.72e-02). ETA=13:21:37, max mem: 20.9 GB 
[11/27 12:31:38 visual_prompt]: 	Training 300/553. train loss: 0.6007,	1.4769 s / batch. (data: 6.59e-01). ETA=21:53:01, max mem: 20.9 GB 
[11/27 12:33:09 visual_prompt]: 	Training 400/553. train loss: 0.7485,	0.8368 s / batch. (data: 1.11e-02). ETA=12:22:32, max mem: 20.9 GB 
[11/27 12:34:44 visual_prompt]: 	Training 500/553. train loss: 0.5842,	2.5354 s / batch. (data: 1.73e+00). ETA=1 day, 13:25:31, max mem: 20.9 GB 
[11/27 12:35:36 visual_prompt]: Epoch 4 / 100: avg data time: 1.31e-01, avg batch time: 0.9579, average train loss: 0.7522
[11/27 12:36:30 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3076, average loss: 0.6865
[11/27 12:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.73	
[11/27 12:36:30 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[11/27 12:38:08 visual_prompt]: 	Training 100/553. train loss: 0.4313,	0.8280 s / batch. (data: 5.42e-03). ETA=12:11:11, max mem: 20.9 GB 
[11/27 12:39:43 visual_prompt]: 	Training 200/553. train loss: 0.7558,	1.0591 s / batch. (data: 2.34e-01). ETA=15:33:35, max mem: 20.9 GB 
[11/27 12:41:19 visual_prompt]: 	Training 300/553. train loss: 0.8254,	0.8204 s / batch. (data: 2.87e-04). ETA=12:01:49, max mem: 20.9 GB 
[11/27 12:42:53 visual_prompt]: 	Training 400/553. train loss: 0.5879,	0.8281 s / batch. (data: 2.49e-04). ETA=12:07:11, max mem: 20.9 GB 
[11/27 12:44:29 visual_prompt]: 	Training 500/553. train loss: 0.6680,	0.8402 s / batch. (data: 3.13e-04). ETA=12:16:22, max mem: 20.9 GB 
[11/27 12:45:19 visual_prompt]: Epoch 5 / 100: avg data time: 1.28e-01, avg batch time: 0.9559, average train loss: 0.7630
[11/27 12:46:13 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3097, average loss: 0.7648
[11/27 12:46:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.54	
[11/27 12:46:13 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[11/27 12:47:53 visual_prompt]: 	Training 100/553. train loss: 0.5386,	0.8236 s / batch. (data: 7.60e-04). ETA=11:59:45, max mem: 20.9 GB 
[11/27 12:49:28 visual_prompt]: 	Training 200/553. train loss: 0.8288,	0.8395 s / batch. (data: 5.44e-03). ETA=12:12:17, max mem: 20.9 GB 
[11/27 12:51:01 visual_prompt]: 	Training 300/553. train loss: 0.5530,	0.8186 s / batch. (data: 3.31e-04). ETA=11:52:38, max mem: 20.9 GB 
[11/27 12:52:40 visual_prompt]: 	Training 400/553. train loss: 0.5470,	0.8203 s / batch. (data: 2.66e-04). ETA=11:52:44, max mem: 20.9 GB 
[11/27 12:54:14 visual_prompt]: 	Training 500/553. train loss: 0.6596,	0.8280 s / batch. (data: 3.68e-04). ETA=11:58:03, max mem: 20.9 GB 
[11/27 12:55:04 visual_prompt]: Epoch 6 / 100: avg data time: 1.32e-01, avg batch time: 0.9590, average train loss: 0.7430
[11/27 12:55:58 visual_prompt]: Inference (val):avg data time: 1.88e-04, avg batch time: 0.3096, average loss: 0.6778
[11/27 12:55:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 59.58	
[11/27 12:55:58 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[11/27 12:57:35 visual_prompt]: 	Training 100/553. train loss: 0.5129,	0.8114 s / batch. (data: 2.46e-04). ETA=11:41:38, max mem: 20.9 GB 
[11/27 12:59:10 visual_prompt]: 	Training 200/553. train loss: 0.5057,	0.8638 s / batch. (data: 3.28e-02). ETA=12:25:28, max mem: 20.9 GB 
[11/27 13:00:48 visual_prompt]: 	Training 300/553. train loss: 0.7971,	1.6976 s / batch. (data: 8.87e-01). ETA=1 day, 0:22:16, max mem: 20.9 GB 
[11/27 13:02:23 visual_prompt]: 	Training 400/553. train loss: 0.6186,	1.6440 s / batch. (data: 8.12e-01). ETA=23:33:21, max mem: 20.9 GB 
[11/27 13:03:56 visual_prompt]: 	Training 500/553. train loss: 0.8930,	0.8103 s / batch. (data: 3.07e-04). ETA=11:35:15, max mem: 20.9 GB 
[11/27 13:04:45 visual_prompt]: Epoch 7 / 100: avg data time: 1.27e-01, avg batch time: 0.9539, average train loss: 0.7458
[11/27 13:05:39 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3089, average loss: 0.8076
[11/27 13:05:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.11	
[11/27 13:05:39 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[11/27 13:07:17 visual_prompt]: 	Training 100/553. train loss: 0.6455,	0.8161 s / batch. (data: 3.71e-04). ETA=11:38:10, max mem: 20.9 GB 
[11/27 13:08:53 visual_prompt]: 	Training 200/553. train loss: 1.3725,	0.8200 s / batch. (data: 3.16e-04). ETA=11:40:08, max mem: 20.9 GB 
[11/27 13:10:29 visual_prompt]: 	Training 300/553. train loss: 0.6746,	0.8320 s / batch. (data: 3.08e-04). ETA=11:48:58, max mem: 20.9 GB 
[11/27 13:12:04 visual_prompt]: 	Training 400/553. train loss: 0.7391,	0.8440 s / batch. (data: 1.20e-02). ETA=11:57:47, max mem: 20.9 GB 
[11/27 13:13:39 visual_prompt]: 	Training 500/553. train loss: 0.8729,	1.3400 s / batch. (data: 4.96e-01). ETA=18:57:25, max mem: 20.9 GB 
[11/27 13:14:29 visual_prompt]: Epoch 8 / 100: avg data time: 1.30e-01, avg batch time: 0.9569, average train loss: 0.7582
[11/27 13:15:23 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3080, average loss: 0.6746
[11/27 13:15:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 64.18	
[11/27 13:15:23 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[11/27 13:17:02 visual_prompt]: 	Training 100/553. train loss: 0.4333,	0.8279 s / batch. (data: 2.90e-04). ETA=11:40:39, max mem: 20.9 GB 
[11/27 13:18:36 visual_prompt]: 	Training 200/553. train loss: 0.5449,	0.8249 s / batch. (data: 1.18e-02). ETA=11:36:42, max mem: 20.9 GB 
[11/27 13:20:12 visual_prompt]: 	Training 300/553. train loss: 0.6564,	1.5767 s / batch. (data: 7.37e-01). ETA=22:09:03, max mem: 20.9 GB 
[11/27 13:21:48 visual_prompt]: 	Training 400/553. train loss: 0.4933,	0.8457 s / batch. (data: 1.43e-02). ETA=11:51:25, max mem: 20.9 GB 
[11/27 13:23:23 visual_prompt]: 	Training 500/553. train loss: 0.7352,	0.8400 s / batch. (data: 5.43e-03). ETA=11:45:15, max mem: 20.9 GB 
[11/27 13:24:12 visual_prompt]: Epoch 9 / 100: avg data time: 1.30e-01, avg batch time: 0.9566, average train loss: 0.7334
[11/27 13:25:06 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3091, average loss: 0.7651
[11/27 13:25:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.76	
[11/27 13:25:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[11/27 13:26:48 visual_prompt]: 	Training 100/553. train loss: 0.7285,	0.8320 s / batch. (data: 1.20e-02). ETA=11:36:25, max mem: 20.9 GB 
[11/27 13:28:21 visual_prompt]: 	Training 200/553. train loss: 0.5980,	0.8200 s / batch. (data: 3.09e-04). ETA=11:25:00, max mem: 20.9 GB 
[11/27 13:29:55 visual_prompt]: 	Training 300/553. train loss: 0.5571,	1.5803 s / batch. (data: 7.56e-01). ETA=21:57:30, max mem: 20.9 GB 
[11/27 13:31:28 visual_prompt]: 	Training 400/553. train loss: 0.8150,	0.8467 s / batch. (data: 5.54e-03). ETA=11:44:31, max mem: 20.9 GB 
[11/27 13:33:05 visual_prompt]: 	Training 500/553. train loss: 1.3001,	0.8237 s / batch. (data: 2.62e-04). ETA=11:24:01, max mem: 20.9 GB 
[11/27 13:33:55 visual_prompt]: Epoch 10 / 100: avg data time: 1.29e-01, avg batch time: 0.9555, average train loss: 0.7450
[11/27 13:34:49 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3089, average loss: 1.0399
[11/27 13:34:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.79	
[11/27 13:34:49 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[11/27 13:36:29 visual_prompt]: 	Training 100/553. train loss: 0.5772,	0.8240 s / batch. (data: 2.89e-04). ETA=11:22:08, max mem: 20.9 GB 
[11/27 13:38:06 visual_prompt]: 	Training 200/553. train loss: 1.0143,	0.8368 s / batch. (data: 3.37e-04). ETA=11:31:19, max mem: 20.9 GB 
[11/27 13:39:41 visual_prompt]: 	Training 300/553. train loss: 0.7084,	1.8000 s / batch. (data: 9.75e-01). ETA=1 day, 0:44:05, max mem: 20.9 GB 
[11/27 13:41:14 visual_prompt]: 	Training 400/553. train loss: 0.8294,	0.8425 s / batch. (data: 1.04e-02). ETA=11:33:14, max mem: 20.9 GB 
[11/27 13:42:48 visual_prompt]: 	Training 500/553. train loss: 0.6236,	0.8280 s / batch. (data: 2.91e-04). ETA=11:19:56, max mem: 20.9 GB 
[11/27 13:43:37 visual_prompt]: Epoch 11 / 100: avg data time: 1.28e-01, avg batch time: 0.9555, average train loss: 0.7389
[11/27 13:44:31 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3094, average loss: 0.6616
[11/27 13:44:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 66.21	
[11/27 13:44:31 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[11/27 13:46:13 visual_prompt]: 	Training 100/553. train loss: 0.8849,	0.8192 s / batch. (data: 3.04e-04). ETA=11:10:36, max mem: 20.9 GB 
[11/27 13:47:49 visual_prompt]: 	Training 200/553. train loss: 0.5797,	0.8412 s / batch. (data: 1.05e-02). ETA=11:27:14, max mem: 20.9 GB 
[11/27 13:49:23 visual_prompt]: 	Training 300/553. train loss: 0.7144,	0.8360 s / batch. (data: 5.43e-03). ETA=11:21:34, max mem: 20.9 GB 
[11/27 13:50:58 visual_prompt]: 	Training 400/553. train loss: 0.9075,	0.8114 s / batch. (data: 3.03e-04). ETA=11:00:11, max mem: 20.9 GB 
[11/27 13:52:39 visual_prompt]: 	Training 500/553. train loss: 1.3694,	0.8271 s / batch. (data: 3.16e-04). ETA=11:11:31, max mem: 20.9 GB 
[11/27 13:53:28 visual_prompt]: Epoch 12 / 100: avg data time: 1.42e-01, avg batch time: 0.9705, average train loss: 0.7338
[11/27 13:54:22 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3068, average loss: 0.7243
[11/27 13:54:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 67.45	
[11/27 13:54:22 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[11/27 13:56:03 visual_prompt]: 	Training 100/553. train loss: 0.6218,	0.8568 s / batch. (data: 3.15e-04). ETA=11:33:28, max mem: 20.9 GB 
[11/27 13:57:35 visual_prompt]: 	Training 200/553. train loss: 0.7117,	0.8223 s / batch. (data: 3.03e-04). ETA=11:04:13, max mem: 20.9 GB 
[11/27 13:59:11 visual_prompt]: 	Training 300/553. train loss: 0.5433,	1.6600 s / batch. (data: 8.32e-01). ETA=22:18:03, max mem: 20.9 GB 
[11/27 14:00:45 visual_prompt]: 	Training 400/553. train loss: 0.8834,	0.8214 s / batch. (data: 2.99e-04). ETA=11:00:43, max mem: 20.9 GB 
[11/27 14:02:22 visual_prompt]: 	Training 500/553. train loss: 0.6629,	0.8263 s / batch. (data: 4.25e-04). ETA=11:03:18, max mem: 20.9 GB 
[11/27 14:03:11 visual_prompt]: Epoch 13 / 100: avg data time: 1.28e-01, avg batch time: 0.9559, average train loss: 0.7207
[11/27 14:04:05 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3074, average loss: 0.6499
[11/27 14:04:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 66.11	
[11/27 14:04:05 visual_prompt]: Best epoch 13: best metric: -0.650
[11/27 14:04:05 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[11/27 14:05:46 visual_prompt]: 	Training 100/553. train loss: 0.7374,	0.8400 s / batch. (data: 5.43e-03). ETA=11:12:09, max mem: 20.9 GB 
[11/27 14:07:21 visual_prompt]: 	Training 200/553. train loss: 0.4351,	0.8462 s / batch. (data: 3.31e-04). ETA=11:15:42, max mem: 20.9 GB 
[11/27 14:08:56 visual_prompt]: 	Training 300/553. train loss: 0.5685,	0.8356 s / batch. (data: 3.64e-04). ETA=11:05:50, max mem: 20.9 GB 
[11/27 14:10:30 visual_prompt]: 	Training 400/553. train loss: 0.7903,	0.8467 s / batch. (data: 2.96e-04). ETA=11:13:18, max mem: 20.9 GB 
[11/27 14:12:05 visual_prompt]: 	Training 500/553. train loss: 1.1196,	0.8320 s / batch. (data: 2.89e-04). ETA=11:00:12, max mem: 20.9 GB 
[11/27 14:12:53 visual_prompt]: Epoch 14 / 100: avg data time: 1.26e-01, avg batch time: 0.9546, average train loss: 0.7051
[11/27 14:13:47 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3081, average loss: 0.6477
[11/27 14:13:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 67.19	
[11/27 14:13:47 visual_prompt]: Best epoch 14: best metric: -0.648
[11/27 14:13:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[11/27 14:15:26 visual_prompt]: 	Training 100/553. train loss: 0.8803,	1.1362 s / batch. (data: 3.14e-01). ETA=14:58:41, max mem: 20.9 GB 
[11/27 14:17:00 visual_prompt]: 	Training 200/553. train loss: 0.3435,	0.8200 s / batch. (data: 2.81e-04). ETA=10:47:14, max mem: 20.9 GB 
[11/27 14:18:37 visual_prompt]: 	Training 300/553. train loss: 0.3691,	0.8213 s / batch. (data: 2.92e-04). ETA=10:46:52, max mem: 20.9 GB 
[11/27 14:20:10 visual_prompt]: 	Training 400/553. train loss: 0.3353,	1.0600 s / batch. (data: 2.17e-01). ETA=13:53:05, max mem: 20.9 GB 
[11/27 14:21:46 visual_prompt]: 	Training 500/553. train loss: 0.9703,	0.8386 s / batch. (data: 9.36e-03). ETA=10:57:45, max mem: 20.9 GB 
[11/27 14:22:36 visual_prompt]: Epoch 15 / 100: avg data time: 1.29e-01, avg batch time: 0.9561, average train loss: 0.7188
[11/27 14:23:30 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3096, average loss: 0.6639
[11/27 14:23:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 67.06	
[11/27 14:23:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[11/27 14:25:08 visual_prompt]: 	Training 100/553. train loss: 0.4963,	0.8400 s / batch. (data: 1.60e-02). ETA=10:56:40, max mem: 20.9 GB 
[11/27 14:26:43 visual_prompt]: 	Training 200/553. train loss: 0.8777,	0.8413 s / batch. (data: 5.44e-03). ETA=10:56:14, max mem: 20.9 GB 
[11/27 14:28:18 visual_prompt]: 	Training 300/553. train loss: 1.2902,	0.8200 s / batch. (data: 3.16e-04). ETA=10:38:18, max mem: 20.9 GB 
[11/27 14:29:53 visual_prompt]: 	Training 400/553. train loss: 0.5190,	0.8217 s / batch. (data: 7.18e-04). ETA=10:38:14, max mem: 20.9 GB 
[11/27 14:31:28 visual_prompt]: 	Training 500/553. train loss: 0.8659,	1.1320 s / batch. (data: 2.79e-01). ETA=14:37:23, max mem: 20.9 GB 
[11/27 14:32:18 visual_prompt]: Epoch 16 / 100: avg data time: 1.27e-01, avg batch time: 0.9537, average train loss: 0.6875
[11/27 14:33:12 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3089, average loss: 0.6613
[11/27 14:33:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 69.23	
[11/27 14:33:12 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[11/27 14:34:51 visual_prompt]: 	Training 100/553. train loss: 0.3416,	0.8160 s / batch. (data: 2.98e-04). ETA=10:30:23, max mem: 20.9 GB 
[11/27 14:36:27 visual_prompt]: 	Training 200/553. train loss: 0.9490,	0.8400 s / batch. (data: 2.79e-04). ETA=10:47:31, max mem: 20.9 GB 
[11/27 14:38:01 visual_prompt]: 	Training 300/553. train loss: 0.7696,	0.8551 s / batch. (data: 2.99e-04). ETA=10:57:42, max mem: 20.9 GB 
[11/27 14:39:36 visual_prompt]: 	Training 400/553. train loss: 0.6216,	1.0079 s / batch. (data: 1.88e-01). ETA=12:53:36, max mem: 20.9 GB 
[11/27 14:41:11 visual_prompt]: 	Training 500/553. train loss: 0.5149,	1.4885 s / batch. (data: 6.58e-01). ETA=19:00:00, max mem: 20.9 GB 
[11/27 14:42:01 visual_prompt]: Epoch 17 / 100: avg data time: 1.30e-01, avg batch time: 0.9569, average train loss: 0.6875
[11/27 14:42:56 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3079, average loss: 0.7444
[11/27 14:42:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.65	
[11/27 14:42:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[11/27 14:44:35 visual_prompt]: 	Training 100/553. train loss: 0.6447,	0.8199 s / batch. (data: 1.05e-02). ETA=10:25:49, max mem: 20.9 GB 
[11/27 14:46:12 visual_prompt]: 	Training 200/553. train loss: 0.7319,	0.8260 s / batch. (data: 7.54e-04). ETA=10:29:08, max mem: 20.9 GB 
[11/27 14:47:48 visual_prompt]: 	Training 300/553. train loss: 0.4174,	0.8305 s / batch. (data: 3.01e-04). ETA=10:31:08, max mem: 20.9 GB 
[11/27 14:49:23 visual_prompt]: 	Training 400/553. train loss: 0.7353,	0.8279 s / batch. (data: 3.02e-04). ETA=10:27:50, max mem: 20.9 GB 
[11/27 14:50:57 visual_prompt]: 	Training 500/553. train loss: 0.7641,	0.8360 s / batch. (data: 6.23e-03). ETA=10:32:33, max mem: 20.9 GB 
[11/27 14:51:46 visual_prompt]: Epoch 18 / 100: avg data time: 1.33e-01, avg batch time: 0.9596, average train loss: 0.7060
[11/27 14:52:40 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3081, average loss: 0.6895
[11/27 14:52:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 70.10	
[11/27 14:52:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[11/27 14:54:19 visual_prompt]: 	Training 100/553. train loss: 1.1118,	0.8162 s / batch. (data: 3.00e-04). ETA=10:15:30, max mem: 20.9 GB 
[11/27 14:55:55 visual_prompt]: 	Training 200/553. train loss: 0.7361,	0.8320 s / batch. (data: 2.81e-04). ETA=10:26:01, max mem: 20.9 GB 
[11/27 14:57:30 visual_prompt]: 	Training 300/553. train loss: 0.5915,	0.8160 s / batch. (data: 3.05e-04). ETA=10:12:39, max mem: 20.9 GB 
[11/27 14:59:07 visual_prompt]: 	Training 400/553. train loss: 0.3412,	0.8159 s / batch. (data: 5.48e-03). ETA=10:11:10, max mem: 20.9 GB 
[11/27 15:00:38 visual_prompt]: 	Training 500/553. train loss: 0.9036,	0.8244 s / batch. (data: 5.37e-03). ETA=10:16:09, max mem: 20.9 GB 
[11/27 15:01:28 visual_prompt]: Epoch 19 / 100: avg data time: 1.27e-01, avg batch time: 0.9532, average train loss: 0.6873
[11/27 15:02:22 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3071, average loss: 0.6526
[11/27 15:02:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 69.78	
[11/27 15:02:22 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[11/27 15:03:59 visual_prompt]: 	Training 100/553. train loss: 1.6961,	0.8400 s / batch. (data: 3.08e-04). ETA=10:25:42, max mem: 20.9 GB 
[11/27 15:05:35 visual_prompt]: 	Training 200/553. train loss: 0.2387,	0.8120 s / batch. (data: 3.09e-04). ETA=10:03:30, max mem: 20.9 GB 
[11/27 15:07:10 visual_prompt]: 	Training 300/553. train loss: 0.8938,	0.8107 s / batch. (data: 3.14e-04). ETA=10:01:08, max mem: 20.9 GB 
[11/27 15:08:45 visual_prompt]: 	Training 400/553. train loss: 0.5579,	0.8315 s / batch. (data: 5.42e-03). ETA=10:15:14, max mem: 20.9 GB 
[11/27 15:10:19 visual_prompt]: 	Training 500/553. train loss: 0.9938,	0.8360 s / batch. (data: 2.95e-04). ETA=10:17:08, max mem: 20.9 GB 
[11/27 15:11:10 visual_prompt]: Epoch 20 / 100: avg data time: 1.28e-01, avg batch time: 0.9550, average train loss: 0.7012
[11/27 15:12:04 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.3087, average loss: 0.8370
[11/27 15:12:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 71.91	
[11/27 15:12:04 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[11/27 15:13:45 visual_prompt]: 	Training 100/553. train loss: 0.4845,	0.8402 s / batch. (data: 3.16e-04). ETA=10:18:08, max mem: 20.9 GB 
[11/27 15:15:19 visual_prompt]: 	Training 200/553. train loss: 0.3858,	0.8160 s / batch. (data: 2.96e-04). ETA=9:58:56, max mem: 20.9 GB 
[11/27 15:16:54 visual_prompt]: 	Training 300/553. train loss: 0.9036,	0.8920 s / batch. (data: 4.89e-02). ETA=10:53:14, max mem: 20.9 GB 
[11/27 15:18:28 visual_prompt]: 	Training 400/553. train loss: 0.5443,	0.8648 s / batch. (data: 1.05e-02). ETA=10:31:54, max mem: 20.9 GB 
[11/27 15:20:04 visual_prompt]: 	Training 500/553. train loss: 0.7307,	0.8400 s / batch. (data: 3.03e-04). ETA=10:12:21, max mem: 20.9 GB 
[11/27 15:20:53 visual_prompt]: Epoch 21 / 100: avg data time: 1.29e-01, avg batch time: 0.9565, average train loss: 0.6669
[11/27 15:21:47 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3082, average loss: 0.6508
[11/27 15:21:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 71.06	
[11/27 15:21:47 visual_prompt]: Stopping early.
[11/27 15:21:47 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 15:21:47 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 15:21:47 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/27 15:21:47 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 15:21:47 visual_prompt]: Training with config:
[11/27 15:21:47 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.05_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/27 15:21:47 visual_prompt]: Loading training data...
[11/27 15:21:47 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 15:21:47 visual_prompt]: Loading validation data...
[11/27 15:21:47 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 15:21:47 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 15:21:50 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 15:21:50 visual_prompt]: tuned percent:0.525
[11/27 15:21:50 visual_prompt]: Device used for model: 0
[11/27 15:21:50 visual_prompt]: Setting up Evaluator...
[11/27 15:21:50 visual_prompt]: Setting up Trainer...
[11/27 15:21:50 visual_prompt]: 	Setting up the optimizer...
[11/27 15:21:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 15:23:29 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8319 s / batch. (data: 7.95e-03). ETA=12:45:20, max mem: 20.9 GB 

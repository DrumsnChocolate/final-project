[09/15 12:25:26 visual_prompt]: Rank of current process: 0. World size: 1
[09/15 12:25:26 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/15 12:25:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed42'], train_type='')
[09/15 12:25:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/15 12:25:26 visual_prompt]: Training with config:
[09/15 12:25:26 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-smallnorb(predicted_attribute="label_azimuth")',
          'NO_TEST': False,
          'NUMBER_CLASSES': 18,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed42/vtab-smallnorb(predicted_attribute="label_azimuth")/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/15 12:25:26 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-15 12:25:26.406271: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-15 12:25:26.601515: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-15 12:25:27.597286: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 12:25:27.597370: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 12:25:27.597379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-15 12:25:29.964788: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 12:25:29.964896: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 12:25:29.964910: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/15 12:25:29 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
2023-09-15 12:25:29.984068: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 12:25:31 visual_prompt]: Number of images: 1000
[09/15 12:25:31 visual_prompt]: Number of classes: 18 / 18
[09/15 12:25:31 visual_prompt]: Loading validation data...
[09/15 12:25:31 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 12:25:32 visual_prompt]: Number of images: 200
[09/15 12:25:32 visual_prompt]: Number of classes: 18 / 18
[09/15 12:25:32 visual_prompt]: Loading test data...
[09/15 12:25:32 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 12:25:49 visual_prompt]: Number of images: 12150
[09/15 12:25:49 visual_prompt]: Number of classes: 18 / 18
[09/15 12:25:49 visual_prompt]: Constructing models...
[09/15 12:25:52 visual_prompt]: Total Parameters: 86734098	 Gradient Parameters: 935442
[09/15 12:25:52 visual_prompt]: tuned percent:1.079
[09/15 12:25:55 visual_prompt]: Device used for model: 0
[09/15 12:25:55 visual_prompt]: Setting up Evalutator...
[09/15 12:25:55 visual_prompt]: Setting up Trainer...
[09/15 12:25:55 visual_prompt]: 	Setting up the optimizer...
[09/15 12:25:55 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/15 12:26:05 visual_prompt]: Epoch 1 / 100: avg data time: 8.37e-02, avg batch time: 0.5983, average train loss: 3.0892
[09/15 12:26:08 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1432, average loss: 3.0902
[09/15 12:26:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.00	top5: 28.50	
[09/15 12:26:29 visual_prompt]: 	Test 100/190. loss: 3.191, 0.1981 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 12:26:48 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1955, average loss: 3.0918
[09/15 12:26:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.14	top5: 27.75	
[09/15 12:26:48 visual_prompt]: Best epoch 1: best metric: 0.040
[09/15 12:26:48 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/15 12:26:57 visual_prompt]: Epoch 2 / 100: avg data time: 1.01e-01, avg batch time: 0.5049, average train loss: 3.5464
[09/15 12:27:00 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1440, average loss: 2.9962
[09/15 12:27:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 29.50	
[09/15 12:27:21 visual_prompt]: 	Test 100/190. loss: 2.985, 0.2078 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 12:27:39 visual_prompt]: Inference (test):avg data time: 6.56e-03, avg batch time: 0.1968, average loss: 3.0123
[09/15 12:27:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.33	
[09/15 12:27:39 visual_prompt]: Best epoch 2: best metric: 0.055
[09/15 12:27:39 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/15 12:27:48 visual_prompt]: Epoch 3 / 100: avg data time: 9.66e-02, avg batch time: 0.5061, average train loss: 3.1046
[09/15 12:27:51 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1458, average loss: 3.0123
[09/15 12:27:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 28.50	
[09/15 12:28:12 visual_prompt]: 	Test 100/190. loss: 2.976, 0.1866 s / batch. (data: 1.72e-04)max mem: 17.22456 GB 
[09/15 12:28:31 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1975, average loss: 3.0280
[09/15 12:28:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.79	
[09/15 12:28:31 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/15 12:28:40 visual_prompt]: Epoch 4 / 100: avg data time: 9.91e-02, avg batch time: 0.5079, average train loss: 3.1203
[09/15 12:28:43 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1484, average loss: 3.0068
[09/15 12:28:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 32.00	
[09/15 12:29:04 visual_prompt]: 	Test 100/190. loss: 3.086, 0.1857 s / batch. (data: 1.31e-04)max mem: 17.22456 GB 
[09/15 12:29:23 visual_prompt]: Inference (test):avg data time: 8.75e-03, avg batch time: 0.1981, average loss: 3.0298
[09/15 12:29:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.62	
[09/15 12:29:23 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/15 12:29:32 visual_prompt]: Epoch 5 / 100: avg data time: 8.65e-02, avg batch time: 0.4945, average train loss: 3.1466
[09/15 12:29:34 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1447, average loss: 3.0966
[09/15 12:29:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 29.00	
[09/15 12:29:56 visual_prompt]: 	Test 100/190. loss: 3.070, 0.2000 s / batch. (data: 1.54e-02)max mem: 17.22456 GB 
[09/15 12:30:14 visual_prompt]: Inference (test):avg data time: 6.80e-03, avg batch time: 0.1958, average loss: 3.0818
[09/15 12:30:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.92	top5: 28.25	
[09/15 12:30:14 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/15 12:30:23 visual_prompt]: Epoch 6 / 100: avg data time: 9.44e-02, avg batch time: 0.5001, average train loss: 3.2601
[09/15 12:30:26 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1438, average loss: 3.4802
[09/15 12:30:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 36.00	
[09/15 12:30:47 visual_prompt]: 	Test 100/190. loss: 3.758, 0.1958 s / batch. (data: 1.22e-02)max mem: 17.22456 GB 
[09/15 12:31:06 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1965, average loss: 3.5924
[09/15 12:31:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.54	
[09/15 12:31:06 visual_prompt]: Best epoch 6: best metric: 0.070
[09/15 12:31:06 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/15 12:31:14 visual_prompt]: Epoch 7 / 100: avg data time: 9.74e-02, avg batch time: 0.5012, average train loss: 3.5322
[09/15 12:31:17 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1439, average loss: 3.7158
[09/15 12:31:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 25.50	
[09/15 12:31:38 visual_prompt]: 	Test 100/190. loss: 3.623, 0.1949 s / batch. (data: 1.12e-02)max mem: 17.22456 GB 
[09/15 12:31:57 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1953, average loss: 3.6447
[09/15 12:31:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 27.98	
[09/15 12:31:57 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/15 12:32:06 visual_prompt]: Epoch 8 / 100: avg data time: 9.21e-02, avg batch time: 0.4969, average train loss: 3.5382
[09/15 12:32:08 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1439, average loss: 3.3130
[09/15 12:32:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 33.00	
[09/15 12:32:30 visual_prompt]: 	Test 100/190. loss: 3.411, 0.1988 s / batch. (data: 1.49e-02)max mem: 17.22456 GB 
[09/15 12:32:48 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1959, average loss: 3.3923
[09/15 12:32:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.36	
[09/15 12:32:49 visual_prompt]: Best epoch 8: best metric: 0.085
[09/15 12:32:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/15 12:32:57 visual_prompt]: Epoch 9 / 100: avg data time: 9.79e-02, avg batch time: 0.5024, average train loss: 3.4317
[09/15 12:33:00 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1437, average loss: 3.5193
[09/15 12:33:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 30.50	
[09/15 12:33:21 visual_prompt]: 	Test 100/190. loss: 3.929, 0.1837 s / batch. (data: 1.33e-04)max mem: 17.22456 GB 
[09/15 12:33:40 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1950, average loss: 3.7007
[09/15 12:33:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.75	
[09/15 12:33:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/15 12:33:49 visual_prompt]: Epoch 10 / 100: avg data time: 1.01e-01, avg batch time: 0.5056, average train loss: 3.7357
[09/15 12:33:51 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1439, average loss: 3.3765
[09/15 12:33:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 34.00	
[09/15 12:34:12 visual_prompt]: 	Test 100/190. loss: 3.860, 0.1914 s / batch. (data: 1.57e-04)max mem: 17.22456 GB 
[09/15 12:34:31 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1955, average loss: 3.5383
[09/15 12:34:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.66	
[09/15 12:34:31 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/15 12:34:40 visual_prompt]: Epoch 11 / 100: avg data time: 8.52e-02, avg batch time: 0.4920, average train loss: 3.5010
[09/15 12:34:42 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1441, average loss: 4.1123
[09/15 12:34:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 27.50	
[09/15 12:35:04 visual_prompt]: 	Test 100/190. loss: 3.880, 0.1957 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 12:35:22 visual_prompt]: Inference (test):avg data time: 6.84e-03, avg batch time: 0.1954, average loss: 4.0808
[09/15 12:35:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.66	top5: 27.85	
[09/15 12:35:22 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/15 12:35:31 visual_prompt]: Epoch 12 / 100: avg data time: 1.01e-01, avg batch time: 0.5051, average train loss: 3.8170
[09/15 12:35:34 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1440, average loss: 5.3057
[09/15 12:35:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 27.50	
[09/15 12:35:55 visual_prompt]: 	Test 100/190. loss: 5.491, 0.1948 s / batch. (data: 1.06e-02)max mem: 17.22456 GB 
[09/15 12:36:14 visual_prompt]: Inference (test):avg data time: 6.65e-03, avg batch time: 0.1952, average loss: 5.0909
[09/15 12:36:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.87	
[09/15 12:36:14 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/15 12:36:23 visual_prompt]: Epoch 13 / 100: avg data time: 9.93e-02, avg batch time: 0.5041, average train loss: 12.8762
[09/15 12:36:25 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1440, average loss: 25.0664
[09/15 12:36:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 25.50	
[09/15 12:36:46 visual_prompt]: 	Test 100/190. loss: 25.806, 0.2251 s / batch. (data: 1.40e-04)max mem: 17.22456 GB 
[09/15 12:37:05 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1957, average loss: 23.3551
[09/15 12:37:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.99	
[09/15 12:37:05 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/15 12:37:14 visual_prompt]: Epoch 14 / 100: avg data time: 7.46e-02, avg batch time: 0.4802, average train loss: 28.3215
[09/15 12:37:16 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1438, average loss: 27.1314
[09/15 12:37:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 31.50	
[09/15 12:37:37 visual_prompt]: 	Test 100/190. loss: 23.735, 0.1968 s / batch. (data: 1.25e-02)max mem: 17.22456 GB 
[09/15 12:37:56 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1953, average loss: 28.5643
[09/15 12:37:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.97	
[09/15 12:37:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/15 12:38:05 visual_prompt]: Epoch 15 / 100: avg data time: 9.62e-02, avg batch time: 0.5011, average train loss: 24.8429
[09/15 12:38:07 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1438, average loss: 23.7241
[09/15 12:38:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 27.00	
[09/15 12:38:29 visual_prompt]: 	Test 100/190. loss: 24.897, 0.2199 s / batch. (data: 3.67e-02)max mem: 17.22456 GB 
[09/15 12:38:47 visual_prompt]: Inference (test):avg data time: 8.83e-03, avg batch time: 0.1968, average loss: 24.0949
[09/15 12:38:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 27.54	
[09/15 12:38:47 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/15 12:38:56 visual_prompt]: Epoch 16 / 100: avg data time: 8.77e-02, avg batch time: 0.4941, average train loss: 27.0938
[09/15 12:38:59 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1439, average loss: 20.5037
[09/15 12:38:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 28.00	
[09/15 12:39:20 visual_prompt]: 	Test 100/190. loss: 19.477, 0.1992 s / batch. (data: 1.57e-02)max mem: 17.22456 GB 
[09/15 12:39:39 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1955, average loss: 22.1037
[09/15 12:39:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.43	
[09/15 12:39:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/15 12:39:47 visual_prompt]: Epoch 17 / 100: avg data time: 7.94e-02, avg batch time: 0.4848, average train loss: 26.1428
[09/15 12:39:50 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1441, average loss: 22.5106
[09/15 12:39:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 26.50	
[09/15 12:40:11 visual_prompt]: 	Test 100/190. loss: 27.277, 0.1985 s / batch. (data: 1.51e-02)max mem: 17.22456 GB 
[09/15 12:40:29 visual_prompt]: Inference (test):avg data time: 5.78e-03, avg batch time: 0.1946, average loss: 21.9134
[09/15 12:40:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 27.84	
[09/15 12:40:29 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/15 12:40:38 visual_prompt]: Epoch 18 / 100: avg data time: 8.26e-02, avg batch time: 0.4911, average train loss: 16.5991
[09/15 12:40:40 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1437, average loss: 11.7022
[09/15 12:40:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 26.00	
[09/15 12:41:02 visual_prompt]: 	Test 100/190. loss: 11.065, 0.1842 s / batch. (data: 1.34e-04)max mem: 17.22456 GB 
[09/15 12:41:21 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1962, average loss: 11.4255
[09/15 12:41:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 28.13	
[09/15 12:41:21 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/15 12:41:30 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e-01, avg batch time: 0.5078, average train loss: 13.1910
[09/15 12:41:32 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1439, average loss: 13.2343
[09/15 12:41:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 29.50	
[09/15 12:41:53 visual_prompt]: 	Test 100/190. loss: 16.583, 0.2142 s / batch. (data: 2.06e-02)max mem: 17.22456 GB 
[09/15 12:42:12 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1960, average loss: 14.7114
[09/15 12:42:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 27.65	
[09/15 12:42:12 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/15 12:42:21 visual_prompt]: Epoch 20 / 100: avg data time: 9.31e-02, avg batch time: 0.4970, average train loss: 11.6694
[09/15 12:42:23 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1439, average loss: 8.8173
[09/15 12:42:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 24.50	
[09/15 12:42:45 visual_prompt]: 	Test 100/190. loss: 8.720, 0.1842 s / batch. (data: 1.40e-04)max mem: 17.22456 GB 
[09/15 12:43:03 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1959, average loss: 8.5085
[09/15 12:43:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 28.10	
[09/15 12:43:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/15 12:43:12 visual_prompt]: Epoch 21 / 100: avg data time: 9.41e-02, avg batch time: 0.4971, average train loss: 8.4052
[09/15 12:43:15 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1439, average loss: 6.8835
[09/15 12:43:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 27.00	
[09/15 12:43:35 visual_prompt]: 	Test 100/190. loss: 5.145, 0.1848 s / batch. (data: 1.47e-04)max mem: 17.22456 GB 
[09/15 12:43:54 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1955, average loss: 6.7599
[09/15 12:43:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.32	top5: 27.43	
[09/15 12:43:54 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/15 12:44:03 visual_prompt]: Epoch 22 / 100: avg data time: 8.64e-02, avg batch time: 0.4948, average train loss: 5.8380
[09/15 12:44:06 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1437, average loss: 5.0903
[09/15 12:44:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 27.50	
[09/15 12:44:27 visual_prompt]: 	Test 100/190. loss: 5.237, 0.1839 s / batch. (data: 1.32e-04)max mem: 17.22456 GB 
[09/15 12:44:45 visual_prompt]: Inference (test):avg data time: 6.02e-03, avg batch time: 0.1944, average loss: 5.1040
[09/15 12:44:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.37	
[09/15 12:44:45 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/15 12:44:54 visual_prompt]: Epoch 23 / 100: avg data time: 9.41e-02, avg batch time: 0.4972, average train loss: 5.0795
[09/15 12:44:57 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1437, average loss: 4.6337
[09/15 12:44:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 26.50	
[09/15 12:45:18 visual_prompt]: 	Test 100/190. loss: 3.694, 0.1946 s / batch. (data: 1.12e-02)max mem: 17.22456 GB 
[09/15 12:45:37 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1950, average loss: 4.5834
[09/15 12:45:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.65	top5: 28.03	
[09/15 12:45:37 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/15 12:45:45 visual_prompt]: Epoch 24 / 100: avg data time: 9.35e-02, avg batch time: 0.4987, average train loss: 4.4677
[09/15 12:45:48 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1437, average loss: 3.4762
[09/15 12:45:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 26.50	
[09/15 12:46:09 visual_prompt]: 	Test 100/190. loss: 3.126, 0.1986 s / batch. (data: 1.39e-02)max mem: 17.22456 GB 
[09/15 12:46:28 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1956, average loss: 3.4213
[09/15 12:46:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.49	top5: 27.90	
[09/15 12:46:28 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/15 12:46:37 visual_prompt]: Epoch 25 / 100: avg data time: 8.25e-02, avg batch time: 0.4884, average train loss: 3.5662
[09/15 12:46:39 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1440, average loss: 3.5620
[09/15 12:46:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 31.50	
[09/15 12:47:00 visual_prompt]: 	Test 100/190. loss: 3.559, 0.1843 s / batch. (data: 9.04e-05)max mem: 17.22456 GB 
[09/15 12:47:19 visual_prompt]: Inference (test):avg data time: 6.42e-03, avg batch time: 0.1953, average loss: 3.5827
[09/15 12:47:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 27.84	
[09/15 12:47:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/15 12:47:28 visual_prompt]: Epoch 26 / 100: avg data time: 9.67e-02, avg batch time: 0.5016, average train loss: 3.3746
[09/15 12:47:30 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1440, average loss: 3.2618
[09/15 12:47:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 29.50	
[09/15 12:47:52 visual_prompt]: 	Test 100/190. loss: 3.334, 0.2082 s / batch. (data: 1.47e-04)max mem: 17.22456 GB 
[09/15 12:48:11 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1963, average loss: 3.2717
[09/15 12:48:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.54	top5: 27.40	
[09/15 12:48:11 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/15 12:48:19 visual_prompt]: Epoch 27 / 100: avg data time: 9.99e-02, avg batch time: 0.5056, average train loss: 3.5580
[09/15 12:48:22 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1440, average loss: 3.2860
[09/15 12:48:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 28.50	
[09/15 12:48:43 visual_prompt]: 	Test 100/190. loss: 3.134, 0.1917 s / batch. (data: 1.68e-04)max mem: 17.22456 GB 
[09/15 12:49:02 visual_prompt]: Inference (test):avg data time: 6.79e-03, avg batch time: 0.1940, average loss: 3.3210
[09/15 12:49:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.65	top5: 27.84	
[09/15 12:49:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/15 12:49:10 visual_prompt]: Epoch 28 / 100: avg data time: 8.73e-02, avg batch time: 0.4935, average train loss: 3.3858
[09/15 12:49:13 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1439, average loss: 3.3772
[09/15 12:49:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 30.00	
[09/15 12:49:34 visual_prompt]: 	Test 100/190. loss: 3.544, 0.2236 s / batch. (data: 4.01e-02)max mem: 17.22456 GB 
[09/15 12:49:53 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1968, average loss: 3.4473
[09/15 12:49:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.93	
[09/15 12:49:53 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/15 12:50:02 visual_prompt]: Epoch 29 / 100: avg data time: 1.01e-01, avg batch time: 0.5048, average train loss: 3.4620
[09/15 12:50:04 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1437, average loss: 3.7420
[09/15 12:50:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 27.00	
[09/15 12:50:26 visual_prompt]: 	Test 100/190. loss: 3.811, 0.1986 s / batch. (data: 1.50e-02)max mem: 17.22456 GB 
[09/15 12:50:44 visual_prompt]: Inference (test):avg data time: 8.50e-03, avg batch time: 0.1971, average loss: 3.6713
[09/15 12:50:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.67	top5: 27.74	
[09/15 12:50:45 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/15 12:50:53 visual_prompt]: Epoch 30 / 100: avg data time: 9.00e-02, avg batch time: 0.4941, average train loss: 3.4602
[09/15 12:50:56 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1436, average loss: 3.3346
[09/15 12:50:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 33.00	
[09/15 12:51:17 visual_prompt]: 	Test 100/190. loss: 3.417, 0.1839 s / batch. (data: 3.53e-05)max mem: 17.22456 GB 
[09/15 12:51:36 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1952, average loss: 3.3862
[09/15 12:51:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.35	top5: 27.33	
[09/15 12:51:36 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/15 12:51:44 visual_prompt]: Epoch 31 / 100: avg data time: 8.57e-02, avg batch time: 0.4914, average train loss: 3.5252
[09/15 12:51:47 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1437, average loss: 3.6647
[09/15 12:51:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 27.00	
[09/15 12:52:08 visual_prompt]: 	Test 100/190. loss: 3.504, 0.2078 s / batch. (data: 1.51e-02)max mem: 17.22456 GB 
[09/15 12:52:27 visual_prompt]: Inference (test):avg data time: 6.94e-03, avg batch time: 0.1956, average loss: 3.6865
[09/15 12:52:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.36	top5: 27.25	
[09/15 12:52:27 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/15 12:52:36 visual_prompt]: Epoch 32 / 100: avg data time: 9.82e-02, avg batch time: 0.5010, average train loss: 4.1471
[09/15 12:52:38 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1437, average loss: 4.3419
[09/15 12:52:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 23.50	
[09/15 12:52:59 visual_prompt]: 	Test 100/190. loss: 3.091, 0.2122 s / batch. (data: 1.25e-04)max mem: 17.22456 GB 
[09/15 12:53:18 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1957, average loss: 4.0736
[09/15 12:53:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.40	top5: 27.85	
[09/15 12:53:18 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/15 12:53:27 visual_prompt]: Epoch 33 / 100: avg data time: 9.21e-02, avg batch time: 0.4979, average train loss: 3.6023
[09/15 12:53:29 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1439, average loss: 3.1758
[09/15 12:53:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 32.00	
[09/15 12:53:50 visual_prompt]: 	Test 100/190. loss: 3.132, 0.1845 s / batch. (data: 3.41e-05)max mem: 17.22456 GB 
[09/15 12:54:09 visual_prompt]: Inference (test):avg data time: 6.91e-03, avg batch time: 0.1951, average loss: 3.1870
[09/15 12:54:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.84	
[09/15 12:54:09 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/15 12:54:18 visual_prompt]: Epoch 34 / 100: avg data time: 9.64e-02, avg batch time: 0.4997, average train loss: 3.2827
[09/15 12:54:20 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1440, average loss: 3.4386
[09/15 12:54:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 27.00	
[09/15 12:54:42 visual_prompt]: 	Test 100/190. loss: 3.278, 0.1844 s / batch. (data: 1.34e-04)max mem: 17.22456 GB 
[09/15 12:55:00 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1947, average loss: 3.4728
[09/15 12:55:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.56	top5: 27.81	
[09/15 12:55:00 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/15 12:55:09 visual_prompt]: Epoch 35 / 100: avg data time: 9.87e-02, avg batch time: 0.5051, average train loss: 3.5734
[09/15 12:55:12 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1439, average loss: 3.7229
[09/15 12:55:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 29.00	
[09/15 12:55:33 visual_prompt]: 	Test 100/190. loss: 3.481, 0.2091 s / batch. (data: 1.44e-04)max mem: 17.22456 GB 
[09/15 12:55:51 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1946, average loss: 3.7486
[09/15 12:55:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.68	
[09/15 12:55:51 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/15 12:56:00 visual_prompt]: Epoch 36 / 100: avg data time: 1.00e-01, avg batch time: 0.5038, average train loss: 3.4268
[09/15 12:56:03 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1440, average loss: 3.4806
[09/15 12:56:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 23.50	
[09/15 12:56:24 visual_prompt]: 	Test 100/190. loss: 3.300, 0.2119 s / batch. (data: 1.53e-02)max mem: 17.22456 GB 
[09/15 12:56:43 visual_prompt]: Inference (test):avg data time: 6.62e-03, avg batch time: 0.1952, average loss: 3.3505
[09/15 12:56:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 27.76	
[09/15 12:56:43 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/15 12:56:52 visual_prompt]: Epoch 37 / 100: avg data time: 9.64e-02, avg batch time: 0.5015, average train loss: 3.4215
[09/15 12:56:54 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1439, average loss: 3.2847
[09/15 12:56:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 30.00	
[09/15 12:57:15 visual_prompt]: 	Test 100/190. loss: 3.157, 0.2118 s / batch. (data: 1.51e-02)max mem: 17.22456 GB 
[09/15 12:57:34 visual_prompt]: Inference (test):avg data time: 6.45e-03, avg batch time: 0.1952, average loss: 3.3516
[09/15 12:57:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.80	top5: 27.79	
[09/15 12:57:34 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/15 12:57:43 visual_prompt]: Epoch 38 / 100: avg data time: 9.67e-02, avg batch time: 0.5027, average train loss: 3.3152
[09/15 12:57:45 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1439, average loss: 3.7309
[09/15 12:57:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 30.00	
[09/15 12:58:06 visual_prompt]: 	Test 100/190. loss: 3.647, 0.1958 s / batch. (data: 1.25e-02)max mem: 17.22456 GB 
[09/15 12:58:25 visual_prompt]: Inference (test):avg data time: 7.09e-03, avg batch time: 0.1948, average loss: 3.6646
[09/15 12:58:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.42	top5: 27.68	
[09/15 12:58:25 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/15 12:58:34 visual_prompt]: Epoch 39 / 100: avg data time: 9.56e-02, avg batch time: 0.5002, average train loss: 3.4074
[09/15 12:58:36 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1439, average loss: 3.1433
[09/15 12:58:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 29.00	
[09/15 12:58:58 visual_prompt]: 	Test 100/190. loss: 3.117, 0.2035 s / batch. (data: 1.72e-02)max mem: 17.22456 GB 
[09/15 12:59:16 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1957, average loss: 3.1496
[09/15 12:59:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.64	top5: 27.72	
[09/15 12:59:16 visual_prompt]: Best epoch 39: best metric: 0.090
[09/15 12:59:16 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/15 12:59:25 visual_prompt]: Epoch 40 / 100: avg data time: 8.88e-02, avg batch time: 0.4927, average train loss: 3.1392
[09/15 12:59:28 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1438, average loss: 3.1573
[09/15 12:59:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 31.50	
[09/15 12:59:49 visual_prompt]: 	Test 100/190. loss: 3.201, 0.1911 s / batch. (data: 1.17e-04)max mem: 17.22456 GB 
[09/15 13:00:07 visual_prompt]: Inference (test):avg data time: 7.50e-03, avg batch time: 0.1954, average loss: 3.1934
[09/15 13:00:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.70	top5: 28.36	
[09/15 13:00:07 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/15 13:00:16 visual_prompt]: Epoch 41 / 100: avg data time: 9.75e-02, avg batch time: 0.5007, average train loss: 3.1336
[09/15 13:00:19 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1438, average loss: 3.0860
[09/15 13:00:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 24.50	
[09/15 13:00:40 visual_prompt]: 	Test 100/190. loss: 3.057, 0.2083 s / batch. (data: 1.47e-02)max mem: 17.22456 GB 
[09/15 13:00:58 visual_prompt]: Inference (test):avg data time: 6.69e-03, avg batch time: 0.1943, average loss: 3.0950
[09/15 13:00:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.45	top5: 28.03	
[09/15 13:00:58 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/15 13:01:07 visual_prompt]: Epoch 42 / 100: avg data time: 8.97e-02, avg batch time: 0.4954, average train loss: 3.1851
[09/15 13:01:10 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1438, average loss: 2.9597
[09/15 13:01:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 30.50	
[09/15 13:01:31 visual_prompt]: 	Test 100/190. loss: 2.961, 0.1986 s / batch. (data: 1.39e-02)max mem: 17.22456 GB 
[09/15 13:01:50 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1957, average loss: 2.9925
[09/15 13:01:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.83	top5: 27.72	
[09/15 13:01:50 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/15 13:01:58 visual_prompt]: Epoch 43 / 100: avg data time: 9.75e-02, avg batch time: 0.5014, average train loss: 3.0766
[09/15 13:02:01 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1439, average loss: 3.1281
[09/15 13:02:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 25.00	
[09/15 13:02:22 visual_prompt]: 	Test 100/190. loss: 3.373, 0.1905 s / batch. (data: 7.04e-03)max mem: 17.22456 GB 
[09/15 13:02:41 visual_prompt]: Inference (test):avg data time: 6.75e-03, avg batch time: 0.1945, average loss: 3.1805
[09/15 13:02:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.74	top5: 27.83	
[09/15 13:02:41 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/15 13:02:49 visual_prompt]: Epoch 44 / 100: avg data time: 9.43e-02, avg batch time: 0.4985, average train loss: 3.1263
[09/15 13:02:52 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1439, average loss: 3.2442
[09/15 13:02:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 30.00	
[09/15 13:03:13 visual_prompt]: 	Test 100/190. loss: 2.931, 0.1837 s / batch. (data: 3.19e-05)max mem: 17.22456 GB 
[09/15 13:03:32 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1948, average loss: 3.2677
[09/15 13:03:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.60	top5: 27.43	
[09/15 13:03:32 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/15 13:03:41 visual_prompt]: Epoch 45 / 100: avg data time: 9.29e-02, avg batch time: 0.4982, average train loss: 3.1956
[09/15 13:03:43 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1438, average loss: 3.0789
[09/15 13:03:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 27.50	
[09/15 13:04:04 visual_prompt]: 	Test 100/190. loss: 2.979, 0.1984 s / batch. (data: 1.53e-02)max mem: 17.22456 GB 
[09/15 13:04:23 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1961, average loss: 3.0696
[09/15 13:04:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.84	top5: 28.53	
[09/15 13:04:23 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/15 13:04:32 visual_prompt]: Epoch 46 / 100: avg data time: 8.57e-02, avg batch time: 0.4906, average train loss: 3.0777
[09/15 13:04:34 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1438, average loss: 3.0772
[09/15 13:04:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 28.50	
[09/15 13:04:55 visual_prompt]: 	Test 100/190. loss: 3.205, 0.2079 s / batch. (data: 2.47e-02)max mem: 17.22456 GB 
[09/15 13:05:14 visual_prompt]: Inference (test):avg data time: 6.51e-03, avg batch time: 0.1949, average loss: 3.0479
[09/15 13:05:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.79	top5: 27.80	
[09/15 13:05:14 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/15 13:05:23 visual_prompt]: Epoch 47 / 100: avg data time: 9.02e-02, avg batch time: 0.4961, average train loss: 3.0580
[09/15 13:05:25 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1440, average loss: 3.0202
[09/15 13:05:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 28.50	
[09/15 13:05:46 visual_prompt]: 	Test 100/190. loss: 3.141, 0.2376 s / batch. (data: 5.28e-02)max mem: 17.22456 GB 
[09/15 13:06:05 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1960, average loss: 3.0766
[09/15 13:06:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.79	top5: 27.52	
[09/15 13:06:05 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/15 13:06:14 visual_prompt]: Epoch 48 / 100: avg data time: 9.16e-02, avg batch time: 0.4983, average train loss: 3.0390
[09/15 13:06:17 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1437, average loss: 3.1187
[09/15 13:06:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 24.50	
[09/15 13:06:38 visual_prompt]: 	Test 100/190. loss: 3.037, 0.2088 s / batch. (data: 2.56e-02)max mem: 17.22456 GB 
[09/15 13:06:57 visual_prompt]: Inference (test):avg data time: 8.23e-03, avg batch time: 0.1961, average loss: 3.1223
[09/15 13:06:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.42	top5: 27.67	
[09/15 13:06:57 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/15 13:07:05 visual_prompt]: Epoch 49 / 100: avg data time: 9.18e-02, avg batch time: 0.4965, average train loss: 3.0025
[09/15 13:07:08 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1437, average loss: 3.0853
[09/15 13:07:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 32.00	
[09/15 13:07:29 visual_prompt]: 	Test 100/190. loss: 3.165, 0.1964 s / batch. (data: 1.31e-02)max mem: 17.22456 GB 
[09/15 13:07:48 visual_prompt]: Inference (test):avg data time: 6.72e-03, avg batch time: 0.1947, average loss: 3.1639
[09/15 13:07:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.60	top5: 28.55	
[09/15 13:07:48 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/15 13:07:56 visual_prompt]: Epoch 50 / 100: avg data time: 8.82e-02, avg batch time: 0.4927, average train loss: 3.0645
[09/15 13:07:59 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1438, average loss: 3.0624
[09/15 13:07:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 28.00	
[09/15 13:08:20 visual_prompt]: 	Test 100/190. loss: 3.003, 0.1999 s / batch. (data: 1.12e-02)max mem: 17.22456 GB 
[09/15 13:08:39 visual_prompt]: Inference (test):avg data time: 7.07e-03, avg batch time: 0.1960, average loss: 3.0673
[09/15 13:08:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.65	top5: 28.91	
[09/15 13:08:39 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/15 13:08:48 visual_prompt]: Epoch 51 / 100: avg data time: 9.47e-02, avg batch time: 0.5006, average train loss: 3.0738
[09/15 13:08:50 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1449, average loss: 3.0181
[09/15 13:08:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 30.00	
[09/15 13:09:11 visual_prompt]: 	Test 100/190. loss: 3.095, 0.1978 s / batch. (data: 1.44e-02)max mem: 17.22456 GB 
[09/15 13:09:30 visual_prompt]: Inference (test):avg data time: 8.75e-03, avg batch time: 0.1965, average loss: 3.0548
[09/15 13:09:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.55	
[09/15 13:09:30 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/15 13:09:39 visual_prompt]: Epoch 52 / 100: avg data time: 9.10e-02, avg batch time: 0.4950, average train loss: 3.0638
[09/15 13:09:42 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1439, average loss: 3.0927
[09/15 13:09:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 30.00	
[09/15 13:10:03 visual_prompt]: 	Test 100/190. loss: 3.041, 0.1830 s / batch. (data: 1.56e-04)max mem: 17.22456 GB 
[09/15 13:10:21 visual_prompt]: Inference (test):avg data time: 6.19e-03, avg batch time: 0.1953, average loss: 3.0977
[09/15 13:10:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 28.50	
[09/15 13:10:21 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/15 13:10:30 visual_prompt]: Epoch 53 / 100: avg data time: 8.96e-02, avg batch time: 0.4978, average train loss: 3.0971
[09/15 13:10:33 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1438, average loss: 3.0050
[09/15 13:10:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 33.00	
[09/15 13:10:54 visual_prompt]: 	Test 100/190. loss: 2.913, 0.1849 s / batch. (data: 3.55e-05)max mem: 17.22456 GB 
[09/15 13:11:12 visual_prompt]: Inference (test):avg data time: 7.44e-03, avg batch time: 0.1951, average loss: 3.0617
[09/15 13:11:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.45	top5: 29.13	
[09/15 13:11:12 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/15 13:11:21 visual_prompt]: Epoch 54 / 100: avg data time: 9.40e-02, avg batch time: 0.4982, average train loss: 3.0288
[09/15 13:11:24 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1445, average loss: 2.9394
[09/15 13:11:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 32.00	
[09/15 13:11:45 visual_prompt]: 	Test 100/190. loss: 2.999, 0.1964 s / batch. (data: 1.18e-02)max mem: 17.22456 GB 
[09/15 13:12:03 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1957, average loss: 3.0141
[09/15 13:12:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.17	top5: 31.02	
[09/15 13:12:04 visual_prompt]: Best epoch 54: best metric: 0.095
[09/15 13:12:04 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/15 13:12:12 visual_prompt]: Epoch 55 / 100: avg data time: 1.00e-01, avg batch time: 0.5067, average train loss: 2.9930
[09/15 13:12:15 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1437, average loss: 2.9645
[09/15 13:12:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 33.50	
[09/15 13:12:36 visual_prompt]: 	Test 100/190. loss: 2.951, 0.2246 s / batch. (data: 4.12e-02)max mem: 17.22456 GB 
[09/15 13:12:55 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1959, average loss: 3.0324
[09/15 13:12:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.63	top5: 29.17	
[09/15 13:12:55 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/15 13:13:04 visual_prompt]: Epoch 56 / 100: avg data time: 9.20e-02, avg batch time: 0.4964, average train loss: 2.9826
[09/15 13:13:06 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1439, average loss: 2.9644
[09/15 13:13:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 29.00	
[09/15 13:13:27 visual_prompt]: 	Test 100/190. loss: 2.990, 0.1990 s / batch. (data: 1.56e-02)max mem: 17.22456 GB 
[09/15 13:13:46 visual_prompt]: Inference (test):avg data time: 6.84e-03, avg batch time: 0.1942, average loss: 2.9984
[09/15 13:13:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.61	top5: 30.07	
[09/15 13:13:46 visual_prompt]: Best epoch 56: best metric: 0.105
[09/15 13:13:46 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/15 13:13:55 visual_prompt]: Epoch 57 / 100: avg data time: 8.27e-02, avg batch time: 0.4895, average train loss: 2.9861
[09/15 13:13:57 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.1441, average loss: 2.8711
[09/15 13:13:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 38.50	
[09/15 13:14:18 visual_prompt]: 	Test 100/190. loss: 2.976, 0.2090 s / batch. (data: 1.50e-02)max mem: 17.22456 GB 
[09/15 13:14:37 visual_prompt]: Inference (test):avg data time: 7.12e-03, avg batch time: 0.1952, average loss: 2.9331
[09/15 13:14:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.53	top5: 34.70	
[09/15 13:14:37 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/15 13:14:46 visual_prompt]: Epoch 58 / 100: avg data time: 8.68e-02, avg batch time: 0.4929, average train loss: 2.9485
[09/15 13:14:48 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1439, average loss: 2.9633
[09/15 13:14:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 29.50	
[09/15 13:15:09 visual_prompt]: 	Test 100/190. loss: 2.836, 0.1919 s / batch. (data: 1.49e-04)max mem: 17.22456 GB 
[09/15 13:15:28 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1965, average loss: 2.9506
[09/15 13:15:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.99	top5: 33.12	
[09/15 13:15:28 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/15 13:15:37 visual_prompt]: Epoch 59 / 100: avg data time: 8.10e-02, avg batch time: 0.4857, average train loss: 2.9349
[09/15 13:15:40 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1438, average loss: 2.9838
[09/15 13:15:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 31.50	
[09/15 13:16:01 visual_prompt]: 	Test 100/190. loss: 2.944, 0.2316 s / batch. (data: 3.91e-02)max mem: 17.22456 GB 
[09/15 13:16:19 visual_prompt]: Inference (test):avg data time: 6.90e-03, avg batch time: 0.1948, average loss: 2.9623
[09/15 13:16:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.63	top5: 31.79	
[09/15 13:16:19 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/15 13:16:28 visual_prompt]: Epoch 60 / 100: avg data time: 9.54e-02, avg batch time: 0.5018, average train loss: 2.9355
[09/15 13:16:31 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1439, average loss: 2.9107
[09/15 13:16:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 33.50	
[09/15 13:16:52 visual_prompt]: 	Test 100/190. loss: 2.982, 0.1916 s / batch. (data: 1.69e-04)max mem: 17.22456 GB 
[09/15 13:17:10 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1952, average loss: 2.9954
[09/15 13:17:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.08	top5: 33.14	
[09/15 13:17:11 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/15 13:17:19 visual_prompt]: Epoch 61 / 100: avg data time: 9.32e-02, avg batch time: 0.4980, average train loss: 2.9382
[09/15 13:17:22 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1474, average loss: 2.8836
[09/15 13:17:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 39.50	
[09/15 13:17:43 visual_prompt]: 	Test 100/190. loss: 2.996, 0.1983 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 13:18:02 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1954, average loss: 2.9677
[09/15 13:18:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.63	top5: 39.14	
[09/15 13:18:02 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/15 13:18:10 visual_prompt]: Epoch 62 / 100: avg data time: 8.18e-02, avg batch time: 0.4870, average train loss: 2.8874
[09/15 13:18:13 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1436, average loss: 2.7004
[09/15 13:18:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 49.50	
[09/15 13:18:34 visual_prompt]: 	Test 100/190. loss: 3.009, 0.1972 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 13:18:53 visual_prompt]: Inference (test):avg data time: 8.43e-03, avg batch time: 0.1958, average loss: 2.8639
[09/15 13:18:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.77	top5: 39.67	
[09/15 13:18:53 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/15 13:19:02 visual_prompt]: Epoch 63 / 100: avg data time: 9.42e-02, avg batch time: 0.4998, average train loss: 2.8403
[09/15 13:19:04 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1440, average loss: 2.7882
[09/15 13:19:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.00	top5: 44.00	
[09/15 13:19:25 visual_prompt]: 	Test 100/190. loss: 2.973, 0.1906 s / batch. (data: 1.78e-04)max mem: 17.22456 GB 
[09/15 13:19:44 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1952, average loss: 2.8807
[09/15 13:19:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.44	top5: 37.16	
[09/15 13:19:44 visual_prompt]: Best epoch 63: best metric: 0.110
[09/15 13:19:44 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/15 13:19:53 visual_prompt]: Epoch 64 / 100: avg data time: 9.07e-02, avg batch time: 0.4932, average train loss: 2.8663
[09/15 13:19:55 visual_prompt]: Inference (val):avg data time: 4.92e-05, avg batch time: 0.1440, average loss: 2.8557
[09/15 13:19:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 36.00	
[09/15 13:20:17 visual_prompt]: 	Test 100/190. loss: 2.978, 0.1933 s / batch. (data: 9.82e-05)max mem: 17.22456 GB 
[09/15 13:20:35 visual_prompt]: Inference (test):avg data time: 6.83e-03, avg batch time: 0.1949, average loss: 2.9383
[09/15 13:20:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.05	top5: 34.44	
[09/15 13:20:35 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/15 13:20:44 visual_prompt]: Epoch 65 / 100: avg data time: 9.63e-02, avg batch time: 0.5012, average train loss: 2.8789
[09/15 13:20:47 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1440, average loss: 2.7143
[09/15 13:20:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.00	top5: 46.00	
[09/15 13:21:08 visual_prompt]: 	Test 100/190. loss: 2.900, 0.1967 s / batch. (data: 7.20e-03)max mem: 17.22456 GB 
[09/15 13:21:27 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1951, average loss: 2.9020
[09/15 13:21:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.23	top5: 39.26	
[09/15 13:21:27 visual_prompt]: Best epoch 65: best metric: 0.130
[09/15 13:21:27 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/15 13:21:35 visual_prompt]: Epoch 66 / 100: avg data time: 9.50e-02, avg batch time: 0.4997, average train loss: 2.7952
[09/15 13:21:38 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1440, average loss: 2.7032
[09/15 13:21:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.00	top5: 52.00	
[09/15 13:21:59 visual_prompt]: 	Test 100/190. loss: 2.775, 0.1961 s / batch. (data: 1.23e-02)max mem: 17.22456 GB 
[09/15 13:22:18 visual_prompt]: Inference (test):avg data time: 6.32e-03, avg batch time: 0.1950, average loss: 2.8273
[09/15 13:22:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.05	top5: 44.31	
[09/15 13:22:18 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/15 13:22:27 visual_prompt]: Epoch 67 / 100: avg data time: 9.08e-02, avg batch time: 0.4951, average train loss: 2.7054
[09/15 13:22:29 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1439, average loss: 2.6492
[09/15 13:22:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 51.50	
[09/15 13:22:50 visual_prompt]: 	Test 100/190. loss: 2.811, 0.1976 s / batch. (data: 1.44e-02)max mem: 17.22456 GB 
[09/15 13:23:09 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1965, average loss: 2.8678
[09/15 13:23:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.40	top5: 44.53	
[09/15 13:23:09 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/15 13:23:18 visual_prompt]: Epoch 68 / 100: avg data time: 8.77e-02, avg batch time: 0.4960, average train loss: 2.6290
[09/15 13:23:21 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1439, average loss: 2.5438
[09/15 13:23:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.00	top5: 55.00	
[09/15 13:23:41 visual_prompt]: 	Test 100/190. loss: 2.787, 0.2204 s / batch. (data: 1.49e-02)max mem: 17.22456 GB 
[09/15 13:24:00 visual_prompt]: Inference (test):avg data time: 7.26e-03, avg batch time: 0.1948, average loss: 2.7771
[09/15 13:24:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.46	top5: 47.84	
[09/15 13:24:00 visual_prompt]: Best epoch 68: best metric: 0.150
[09/15 13:24:00 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/15 13:24:09 visual_prompt]: Epoch 69 / 100: avg data time: 8.24e-02, avg batch time: 0.4912, average train loss: 2.5449
[09/15 13:24:12 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1439, average loss: 2.6145
[09/15 13:24:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.00	top5: 53.50	
[09/15 13:24:32 visual_prompt]: 	Test 100/190. loss: 2.891, 0.1836 s / batch. (data: 1.53e-04)max mem: 17.22456 GB 
[09/15 13:24:51 visual_prompt]: Inference (test):avg data time: 6.50e-03, avg batch time: 0.1936, average loss: 2.8285
[09/15 13:24:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.28	top5: 47.95	
[09/15 13:24:51 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/15 13:25:00 visual_prompt]: Epoch 70 / 100: avg data time: 9.81e-02, avg batch time: 0.5035, average train loss: 2.4862
[09/15 13:25:02 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1438, average loss: 2.3989
[09/15 13:25:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.00	top5: 66.50	
[09/15 13:25:24 visual_prompt]: 	Test 100/190. loss: 2.658, 0.2111 s / batch. (data: 1.44e-02)max mem: 17.22456 GB 
[09/15 13:25:42 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1961, average loss: 2.6800
[09/15 13:25:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.32	top5: 54.02	
[09/15 13:25:42 visual_prompt]: Best epoch 70: best metric: 0.180
[09/15 13:25:42 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/15 13:25:51 visual_prompt]: Epoch 71 / 100: avg data time: 1.01e-01, avg batch time: 0.5050, average train loss: 2.4903
[09/15 13:25:54 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1445, average loss: 2.3636
[09/15 13:25:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.50	top5: 69.50	
[09/15 13:26:15 visual_prompt]: 	Test 100/190. loss: 2.761, 0.1838 s / batch. (data: 1.33e-04)max mem: 17.22456 GB 
[09/15 13:26:34 visual_prompt]: Inference (test):avg data time: 8.23e-03, avg batch time: 0.1958, average loss: 2.6339
[09/15 13:26:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.65	top5: 55.62	
[09/15 13:26:34 visual_prompt]: Best epoch 71: best metric: 0.185
[09/15 13:26:34 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/15 13:26:42 visual_prompt]: Epoch 72 / 100: avg data time: 9.37e-02, avg batch time: 0.4989, average train loss: 2.3411
[09/15 13:26:45 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1438, average loss: 2.3458
[09/15 13:26:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.00	top5: 67.50	
[09/15 13:27:06 visual_prompt]: 	Test 100/190. loss: 2.994, 0.2156 s / batch. (data: 2.50e-03)max mem: 17.22456 GB 
[09/15 13:27:25 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1954, average loss: 2.7300
[09/15 13:27:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.05	top5: 56.62	
[09/15 13:27:25 visual_prompt]: Best epoch 72: best metric: 0.200
[09/15 13:27:25 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/15 13:27:34 visual_prompt]: Epoch 73 / 100: avg data time: 9.62e-02, avg batch time: 0.5024, average train loss: 2.2486
[09/15 13:27:36 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1438, average loss: 2.3254
[09/15 13:27:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.00	top5: 69.00	
[09/15 13:27:57 visual_prompt]: 	Test 100/190. loss: 2.561, 0.2026 s / batch. (data: 1.34e-02)max mem: 17.22456 GB 
[09/15 13:28:16 visual_prompt]: Inference (test):avg data time: 8.06e-03, avg batch time: 0.1954, average loss: 2.7834
[09/15 13:28:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.02	top5: 55.85	
[09/15 13:28:16 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/15 13:28:25 visual_prompt]: Epoch 74 / 100: avg data time: 9.42e-02, avg batch time: 0.4980, average train loss: 2.2877
[09/15 13:28:28 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1439, average loss: 2.2939
[09/15 13:28:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.50	top5: 73.00	
[09/15 13:28:49 visual_prompt]: 	Test 100/190. loss: 2.777, 0.1837 s / batch. (data: 1.54e-04)max mem: 17.22456 GB 
[09/15 13:29:07 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1941, average loss: 2.6131
[09/15 13:29:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.43	top5: 54.85	
[09/15 13:29:07 visual_prompt]: Best epoch 74: best metric: 0.235
[09/15 13:29:07 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/15 13:29:16 visual_prompt]: Epoch 75 / 100: avg data time: 9.65e-02, avg batch time: 0.5010, average train loss: 2.2217
[09/15 13:29:19 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1438, average loss: 2.0387
[09/15 13:29:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.00	top5: 79.50	
[09/15 13:29:40 visual_prompt]: 	Test 100/190. loss: 2.807, 0.1847 s / batch. (data: 1.20e-04)max mem: 17.22456 GB 
[09/15 13:29:59 visual_prompt]: Inference (test):avg data time: 8.32e-03, avg batch time: 0.1952, average loss: 2.7803
[09/15 13:29:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.23	top5: 60.41	
[09/15 13:29:59 visual_prompt]: Best epoch 75: best metric: 0.290
[09/15 13:29:59 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/15 13:30:07 visual_prompt]: Epoch 76 / 100: avg data time: 8.58e-02, avg batch time: 0.4919, average train loss: 2.0297
[09/15 13:30:10 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1439, average loss: 1.9081
[09/15 13:30:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 34.50	top5: 84.50	
[09/15 13:30:31 visual_prompt]: 	Test 100/190. loss: 2.873, 0.1960 s / batch. (data: 1.26e-02)max mem: 17.22456 GB 
[09/15 13:30:50 visual_prompt]: Inference (test):avg data time: 8.88e-03, avg batch time: 0.1959, average loss: 2.7079
[09/15 13:30:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.11	top5: 63.80	
[09/15 13:30:50 visual_prompt]: Best epoch 76: best metric: 0.345
[09/15 13:30:50 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/15 13:30:59 visual_prompt]: Epoch 77 / 100: avg data time: 9.09e-02, avg batch time: 0.4963, average train loss: 1.9473
[09/15 13:31:01 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1440, average loss: 2.1602
[09/15 13:31:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.50	top5: 77.00	
[09/15 13:31:22 visual_prompt]: 	Test 100/190. loss: 3.150, 0.2013 s / batch. (data: 1.26e-02)max mem: 17.22456 GB 
[09/15 13:31:41 visual_prompt]: Inference (test):avg data time: 8.41e-03, avg batch time: 0.1955, average loss: 2.8249
[09/15 13:31:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.81	top5: 56.72	
[09/15 13:31:41 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/15 13:31:50 visual_prompt]: Epoch 78 / 100: avg data time: 1.01e-01, avg batch time: 0.5045, average train loss: 1.8770
[09/15 13:31:52 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1437, average loss: 2.0655
[09/15 13:31:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.00	top5: 87.50	
[09/15 13:32:13 visual_prompt]: 	Test 100/190. loss: 3.021, 0.2112 s / batch. (data: 1.46e-02)max mem: 17.22456 GB 
[09/15 13:32:32 visual_prompt]: Inference (test):avg data time: 6.30e-03, avg batch time: 0.1943, average loss: 3.0537
[09/15 13:32:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.52	top5: 63.39	
[09/15 13:32:32 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/15 13:32:41 visual_prompt]: Epoch 79 / 100: avg data time: 8.91e-02, avg batch time: 0.4949, average train loss: 1.8794
[09/15 13:32:43 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1439, average loss: 1.7895
[09/15 13:32:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.50	top5: 89.00	
[09/15 13:33:04 visual_prompt]: 	Test 100/190. loss: 3.078, 0.1848 s / batch. (data: 1.23e-04)max mem: 17.22456 GB 
[09/15 13:33:23 visual_prompt]: Inference (test):avg data time: 6.60e-03, avg batch time: 0.1945, average loss: 2.8386
[09/15 13:33:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.26	top5: 64.81	
[09/15 13:33:23 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/15 13:33:32 visual_prompt]: Epoch 80 / 100: avg data time: 9.38e-02, avg batch time: 0.4980, average train loss: 1.6771
[09/15 13:33:34 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1441, average loss: 1.6473
[09/15 13:33:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 36.50	top5: 94.00	
[09/15 13:33:55 visual_prompt]: 	Test 100/190. loss: 3.024, 0.2100 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 13:34:14 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1964, average loss: 2.8877
[09/15 13:34:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.36	top5: 66.28	
[09/15 13:34:14 visual_prompt]: Best epoch 80: best metric: 0.365
[09/15 13:34:14 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/15 13:34:23 visual_prompt]: Epoch 81 / 100: avg data time: 9.60e-02, avg batch time: 0.4998, average train loss: 1.5934
[09/15 13:34:25 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1439, average loss: 1.5396
[09/15 13:34:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 40.50	top5: 96.50	
[09/15 13:34:47 visual_prompt]: 	Test 100/190. loss: 3.086, 0.1836 s / batch. (data: 2.98e-05)max mem: 17.22456 GB 
[09/15 13:35:05 visual_prompt]: Inference (test):avg data time: 8.38e-03, avg batch time: 0.1960, average loss: 3.0264
[09/15 13:35:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.34	top5: 65.35	
[09/15 13:35:05 visual_prompt]: Best epoch 81: best metric: 0.405
[09/15 13:35:05 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/15 13:35:14 visual_prompt]: Epoch 82 / 100: avg data time: 9.34e-02, avg batch time: 0.4983, average train loss: 1.6647
[09/15 13:35:17 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1440, average loss: 1.5484
[09/15 13:35:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 38.00	top5: 95.00	
[09/15 13:35:38 visual_prompt]: 	Test 100/190. loss: 3.124, 0.1997 s / batch. (data: 1.12e-02)max mem: 17.22456 GB 
[09/15 13:35:57 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1954, average loss: 2.8926
[09/15 13:35:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.07	top5: 64.77	
[09/15 13:35:57 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/15 13:36:05 visual_prompt]: Epoch 83 / 100: avg data time: 9.14e-02, avg batch time: 0.4959, average train loss: 1.5321
[09/15 13:36:08 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1439, average loss: 1.5231
[09/15 13:36:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 45.00	top5: 94.00	
[09/15 13:36:29 visual_prompt]: 	Test 100/190. loss: 3.365, 0.1835 s / batch. (data: 2.86e-05)max mem: 17.22456 GB 
[09/15 13:36:48 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1954, average loss: 3.1882
[09/15 13:36:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.97	top5: 65.91	
[09/15 13:36:48 visual_prompt]: Best epoch 83: best metric: 0.450
[09/15 13:36:48 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/15 13:36:56 visual_prompt]: Epoch 84 / 100: avg data time: 9.72e-02, avg batch time: 0.5002, average train loss: 1.4178
[09/15 13:36:59 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1438, average loss: 1.3592
[09/15 13:36:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 48.00	top5: 97.00	
[09/15 13:37:20 visual_prompt]: 	Test 100/190. loss: 3.328, 0.1972 s / batch. (data: 1.36e-02)max mem: 17.22456 GB 
[09/15 13:37:39 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1951, average loss: 3.3411
[09/15 13:37:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.81	top5: 65.79	
[09/15 13:37:39 visual_prompt]: Best epoch 84: best metric: 0.480
[09/15 13:37:39 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/15 13:37:48 visual_prompt]: Epoch 85 / 100: avg data time: 9.43e-02, avg batch time: 0.4990, average train loss: 1.2051
[09/15 13:37:50 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1446, average loss: 1.3332
[09/15 13:37:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 43.50	top5: 98.00	
[09/15 13:38:11 visual_prompt]: 	Test 100/190. loss: 3.536, 0.1833 s / batch. (data: 3.70e-05)max mem: 17.22456 GB 
[09/15 13:38:30 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1949, average loss: 3.5037
[09/15 13:38:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.77	top5: 66.36	
[09/15 13:38:30 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/15 13:38:39 visual_prompt]: Epoch 86 / 100: avg data time: 9.67e-02, avg batch time: 0.4987, average train loss: 1.0906
[09/15 13:38:41 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1441, average loss: 1.1865
[09/15 13:38:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 58.50	top5: 97.00	
[09/15 13:39:02 visual_prompt]: 	Test 100/190. loss: 4.236, 0.1850 s / batch. (data: 1.55e-04)max mem: 17.22456 GB 
[09/15 13:39:21 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1952, average loss: 3.9368
[09/15 13:39:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.74	top5: 67.52	
[09/15 13:39:21 visual_prompt]: Best epoch 86: best metric: 0.585
[09/15 13:39:21 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/15 13:39:30 visual_prompt]: Epoch 87 / 100: avg data time: 9.44e-02, avg batch time: 0.4998, average train loss: 1.1142
[09/15 13:39:33 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1438, average loss: 1.4706
[09/15 13:39:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 50.50	top5: 95.00	
[09/15 13:39:53 visual_prompt]: 	Test 100/190. loss: 4.293, 0.2108 s / batch. (data: 1.77e-04)max mem: 17.22456 GB 
[09/15 13:40:12 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1949, average loss: 4.0330
[09/15 13:40:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.83	top5: 66.00	
[09/15 13:40:12 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/15 13:40:21 visual_prompt]: Epoch 88 / 100: avg data time: 8.99e-02, avg batch time: 0.4966, average train loss: 1.0070
[09/15 13:40:24 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1438, average loss: 0.9870
[09/15 13:40:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 61.00	top5: 98.00	
[09/15 13:40:45 visual_prompt]: 	Test 100/190. loss: 4.420, 0.1843 s / batch. (data: 1.26e-04)max mem: 17.22456 GB 
[09/15 13:41:04 visual_prompt]: Inference (test):avg data time: 8.59e-03, avg batch time: 0.1969, average loss: 4.1786
[09/15 13:41:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.17	top5: 66.86	
[09/15 13:41:04 visual_prompt]: Best epoch 88: best metric: 0.610
[09/15 13:41:04 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/15 13:41:12 visual_prompt]: Epoch 89 / 100: avg data time: 8.95e-02, avg batch time: 0.4952, average train loss: 0.7937
[09/15 13:41:15 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1439, average loss: 0.8822
[09/15 13:41:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 70.50	top5: 98.50	
[09/15 13:41:36 visual_prompt]: 	Test 100/190. loss: 4.667, 0.1997 s / batch. (data: 1.33e-04)max mem: 17.22456 GB 
[09/15 13:41:55 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1948, average loss: 4.8310
[09/15 13:41:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.79	top5: 67.40	
[09/15 13:41:55 visual_prompt]: Best epoch 89: best metric: 0.705
[09/15 13:41:55 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/15 13:42:04 visual_prompt]: Epoch 90 / 100: avg data time: 1.06e-01, avg batch time: 0.5097, average train loss: 0.6963
[09/15 13:42:06 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1439, average loss: 0.6491
[09/15 13:42:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 73.50	top5: 100.00	
[09/15 13:42:27 visual_prompt]: 	Test 100/190. loss: 4.883, 0.1997 s / batch. (data: 1.62e-02)max mem: 17.22456 GB 
[09/15 13:42:46 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1947, average loss: 5.0220
[09/15 13:42:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.56	top5: 68.29	
[09/15 13:42:46 visual_prompt]: Best epoch 90: best metric: 0.735
[09/15 13:42:46 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/15 13:42:55 visual_prompt]: Epoch 91 / 100: avg data time: 1.03e-01, avg batch time: 0.5058, average train loss: 0.6011
[09/15 13:42:57 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1439, average loss: 0.5211
[09/15 13:42:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 80.00	top5: 100.00	
[09/15 13:43:18 visual_prompt]: 	Test 100/190. loss: 5.367, 0.1963 s / batch. (data: 1.25e-02)max mem: 17.22456 GB 
[09/15 13:43:37 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1954, average loss: 5.0706
[09/15 13:43:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.65	top5: 68.37	
[09/15 13:43:37 visual_prompt]: Best epoch 91: best metric: 0.800
[09/15 13:43:37 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/15 13:43:46 visual_prompt]: Epoch 92 / 100: avg data time: 8.86e-02, avg batch time: 0.4953, average train loss: 0.4717
[09/15 13:43:48 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1439, average loss: 0.4667
[09/15 13:43:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 81.50	top5: 99.50	
[09/15 13:44:09 visual_prompt]: 	Test 100/190. loss: 5.312, 0.1884 s / batch. (data: 1.80e-04)max mem: 17.22456 GB 
[09/15 13:44:28 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1947, average loss: 5.3476
[09/15 13:44:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.90	top5: 68.37	
[09/15 13:44:28 visual_prompt]: Best epoch 92: best metric: 0.815
[09/15 13:44:28 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/15 13:44:37 visual_prompt]: Epoch 93 / 100: avg data time: 9.43e-02, avg batch time: 0.4989, average train loss: 0.3645
[09/15 13:44:39 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1437, average loss: 0.4685
[09/15 13:44:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 81.50	top5: 100.00	
[09/15 13:45:00 visual_prompt]: 	Test 100/190. loss: 5.468, 0.1842 s / batch. (data: 1.49e-04)max mem: 17.22456 GB 
[09/15 13:45:19 visual_prompt]: Inference (test):avg data time: 6.78e-03, avg batch time: 0.1947, average loss: 5.4191
[09/15 13:45:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.73	top5: 67.93	
[09/15 13:45:19 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/15 13:45:28 visual_prompt]: Epoch 94 / 100: avg data time: 9.62e-02, avg batch time: 0.5003, average train loss: 0.3170
[09/15 13:45:30 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1437, average loss: 0.3866
[09/15 13:45:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 86.00	top5: 100.00	
[09/15 13:45:51 visual_prompt]: 	Test 100/190. loss: 5.388, 0.1844 s / batch. (data: 1.28e-04)max mem: 17.22456 GB 
[09/15 13:46:10 visual_prompt]: Inference (test):avg data time: 8.88e-03, avg batch time: 0.1961, average loss: 5.3668
[09/15 13:46:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.86	top5: 68.53	
[09/15 13:46:10 visual_prompt]: Best epoch 94: best metric: 0.860
[09/15 13:46:10 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/15 13:46:19 visual_prompt]: Epoch 95 / 100: avg data time: 9.00e-02, avg batch time: 0.4957, average train loss: 0.2737
[09/15 13:46:22 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1439, average loss: 0.3410
[09/15 13:46:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 87.50	top5: 100.00	
[09/15 13:46:43 visual_prompt]: 	Test 100/190. loss: 5.530, 0.1834 s / batch. (data: 1.26e-04)max mem: 17.22456 GB 
[09/15 13:47:01 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1948, average loss: 5.3834
[09/15 13:47:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.80	top5: 68.56	
[09/15 13:47:01 visual_prompt]: Best epoch 95: best metric: 0.875
[09/15 13:47:01 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/15 13:47:10 visual_prompt]: Epoch 96 / 100: avg data time: 9.69e-02, avg batch time: 0.5021, average train loss: 0.2328
[09/15 13:47:13 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1438, average loss: 0.2983
[09/15 13:47:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 88.50	top5: 100.00	
[09/15 13:47:34 visual_prompt]: 	Test 100/190. loss: 5.501, 0.2084 s / batch. (data: 1.58e-02)max mem: 17.22456 GB 
[09/15 13:47:52 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1951, average loss: 5.3919
[09/15 13:47:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.09	top5: 68.54	
[09/15 13:47:53 visual_prompt]: Best epoch 96: best metric: 0.885
[09/15 13:47:53 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/15 13:48:01 visual_prompt]: Epoch 97 / 100: avg data time: 8.87e-02, avg batch time: 0.4942, average train loss: 0.1959
[09/15 13:48:04 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1439, average loss: 0.2873
[09/15 13:48:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 90.00	top5: 100.00	
[09/15 13:48:25 visual_prompt]: 	Test 100/190. loss: 5.460, 0.2111 s / batch. (data: 2.67e-02)max mem: 17.22456 GB 
[09/15 13:48:43 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1941, average loss: 5.4355
[09/15 13:48:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.05	top5: 68.59	
[09/15 13:48:43 visual_prompt]: Best epoch 97: best metric: 0.900
[09/15 13:48:43 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/15 13:48:52 visual_prompt]: Epoch 98 / 100: avg data time: 8.77e-02, avg batch time: 0.4921, average train loss: 0.1966
[09/15 13:48:55 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1440, average loss: 0.2659
[09/15 13:48:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 90.50	top5: 100.00	
[09/15 13:49:16 visual_prompt]: 	Test 100/190. loss: 5.449, 0.1869 s / batch. (data: 1.52e-04)max mem: 17.22456 GB 
[09/15 13:49:34 visual_prompt]: Inference (test):avg data time: 6.47e-03, avg batch time: 0.1943, average loss: 5.4551
[09/15 13:49:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.00	top5: 68.58	
[09/15 13:49:34 visual_prompt]: Best epoch 98: best metric: 0.905
[09/15 13:49:34 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/15 13:49:43 visual_prompt]: Epoch 99 / 100: avg data time: 9.24e-02, avg batch time: 0.4973, average train loss: 0.1832
[09/15 13:49:46 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1438, average loss: 0.2616
[09/15 13:49:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 93.00	top5: 100.00	
[09/15 13:50:07 visual_prompt]: 	Test 100/190. loss: 5.452, 0.1855 s / batch. (data: 1.23e-04)max mem: 17.22456 GB 
[09/15 13:50:25 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1954, average loss: 5.4573
[09/15 13:50:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.03	top5: 68.53	
[09/15 13:50:25 visual_prompt]: Best epoch 99: best metric: 0.930
[09/15 13:50:25 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/15 13:50:34 visual_prompt]: Epoch 100 / 100: avg data time: 9.28e-02, avg batch time: 0.4964, average train loss: 0.1748
[09/15 13:50:37 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1436, average loss: 0.2637
[09/15 13:50:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 93.00	top5: 100.00	
[09/15 13:50:58 visual_prompt]: 	Test 100/190. loss: 5.464, 0.1936 s / batch. (data: 1.30e-04)max mem: 17.22456 GB 
[09/15 13:51:17 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1962, average loss: 5.4623
[09/15 13:51:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.93	top5: 68.48	
[09/15 13:51:45 visual_prompt]: Rank of current process: 0. World size: 1
[09/15 13:51:45 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/15 13:51:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed44'], train_type='')
[09/15 13:51:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/15 13:51:45 visual_prompt]: Training with config:
[09/15 13:51:45 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-smallnorb(predicted_attribute="label_azimuth")',
          'NO_TEST': False,
          'NUMBER_CLASSES': 18,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed44/vtab-smallnorb(predicted_attribute="label_azimuth")/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/15 13:51:45 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-15 13:51:45.896307: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-15 13:51:46.103106: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-15 13:51:47.002926: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 13:51:47.003018: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 13:51:47.003027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-15 13:51:49.053574: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 13:51:49.053718: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 13:51:49.053731: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/15 13:51:49 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
2023-09-15 13:51:49.117042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 13:51:51 visual_prompt]: Number of images: 1000
[09/15 13:51:51 visual_prompt]: Number of classes: 18 / 18
[09/15 13:51:51 visual_prompt]: Loading validation data...
[09/15 13:51:51 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 13:51:51 visual_prompt]: Number of images: 200
[09/15 13:51:51 visual_prompt]: Number of classes: 18 / 18
[09/15 13:51:51 visual_prompt]: Loading test data...
[09/15 13:51:51 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 13:52:08 visual_prompt]: Number of images: 12150
[09/15 13:52:08 visual_prompt]: Number of classes: 18 / 18
[09/15 13:52:08 visual_prompt]: Constructing models...
[09/15 13:52:11 visual_prompt]: Total Parameters: 86734098	 Gradient Parameters: 935442
[09/15 13:52:11 visual_prompt]: tuned percent:1.079
[09/15 13:52:13 visual_prompt]: Device used for model: 0
[09/15 13:52:13 visual_prompt]: Setting up Evalutator...
[09/15 13:52:13 visual_prompt]: Setting up Trainer...
[09/15 13:52:13 visual_prompt]: 	Setting up the optimizer...
[09/15 13:52:13 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/15 13:52:23 visual_prompt]: Epoch 1 / 100: avg data time: 1.10e-01, avg batch time: 0.5843, average train loss: 3.0638
[09/15 13:52:26 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1430, average loss: 3.0335
[09/15 13:52:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 27.00	
[09/15 13:52:47 visual_prompt]: 	Test 100/190. loss: 3.100, 0.1828 s / batch. (data: 1.37e-04)max mem: 17.22456 GB 
[09/15 13:53:05 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1948, average loss: 3.0226
[09/15 13:53:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.83	top5: 29.33	
[09/15 13:53:05 visual_prompt]: Best epoch 1: best metric: 0.050
[09/15 13:53:05 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/15 13:53:14 visual_prompt]: Epoch 2 / 100: avg data time: 8.82e-02, avg batch time: 0.4918, average train loss: 3.1570
[09/15 13:53:17 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1433, average loss: 3.0021
[09/15 13:53:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 27.00	
[09/15 13:53:38 visual_prompt]: 	Test 100/190. loss: 3.012, 0.1979 s / batch. (data: 1.42e-02)max mem: 17.22456 GB 
[09/15 13:53:56 visual_prompt]: Inference (test):avg data time: 6.89e-03, avg batch time: 0.1942, average loss: 2.9973
[09/15 13:53:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.79	
[09/15 13:53:56 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/15 13:54:05 visual_prompt]: Epoch 3 / 100: avg data time: 8.14e-02, avg batch time: 0.4871, average train loss: 3.0438
[09/15 13:54:07 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1437, average loss: 2.9934
[09/15 13:54:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 33.50	
[09/15 13:54:29 visual_prompt]: 	Test 100/190. loss: 3.057, 0.2213 s / batch. (data: 1.59e-02)max mem: 17.22456 GB 
[09/15 13:54:47 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1952, average loss: 3.0406
[09/15 13:54:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.52	top5: 27.56	
[09/15 13:54:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/15 13:54:56 visual_prompt]: Epoch 4 / 100: avg data time: 9.10e-02, avg batch time: 0.4939, average train loss: 3.0419
[09/15 13:54:59 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1437, average loss: 3.0675
[09/15 13:54:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 31.50	
[09/15 13:55:20 visual_prompt]: 	Test 100/190. loss: 3.112, 0.1833 s / batch. (data: 1.32e-04)max mem: 17.22456 GB 
[09/15 13:55:38 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1957, average loss: 3.0926
[09/15 13:55:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.48	
[09/15 13:55:38 visual_prompt]: Best epoch 4: best metric: 0.070
[09/15 13:55:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/15 13:55:47 visual_prompt]: Epoch 5 / 100: avg data time: 8.89e-02, avg batch time: 0.4941, average train loss: 3.1586
[09/15 13:55:50 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1438, average loss: 3.2477
[09/15 13:55:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 30.00	
[09/15 13:56:11 visual_prompt]: 	Test 100/190. loss: 3.134, 0.1998 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 13:56:29 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1951, average loss: 3.2356
[09/15 13:56:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.35	
[09/15 13:56:29 visual_prompt]: Best epoch 5: best metric: 0.085
[09/15 13:56:29 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/15 13:56:38 visual_prompt]: Epoch 6 / 100: avg data time: 8.91e-02, avg batch time: 0.4928, average train loss: 3.1695
[09/15 13:56:41 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1435, average loss: 3.1321
[09/15 13:56:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 29.00	
[09/15 13:57:02 visual_prompt]: 	Test 100/190. loss: 3.033, 0.2127 s / batch. (data: 1.61e-02)max mem: 17.22456 GB 
[09/15 13:57:21 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1965, average loss: 3.1520
[09/15 13:57:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 28.23	
[09/15 13:57:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/15 13:57:29 visual_prompt]: Epoch 7 / 100: avg data time: 9.39e-02, avg batch time: 0.4968, average train loss: 3.2438
[09/15 13:57:32 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1435, average loss: 3.5184
[09/15 13:57:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 29.00	
[09/15 13:57:53 visual_prompt]: 	Test 100/190. loss: 3.646, 0.1986 s / batch. (data: 1.54e-02)max mem: 17.22456 GB 
[09/15 13:58:12 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1954, average loss: 3.5766
[09/15 13:58:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.48	top5: 28.09	
[09/15 13:58:12 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/15 13:58:20 visual_prompt]: Epoch 8 / 100: avg data time: 9.16e-02, avg batch time: 0.4962, average train loss: 3.4357
[09/15 13:58:23 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1435, average loss: 3.3501
[09/15 13:58:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 31.50	
[09/15 13:58:44 visual_prompt]: 	Test 100/190. loss: 3.457, 0.1893 s / batch. (data: 1.06e-04)max mem: 17.22456 GB 
[09/15 13:59:02 visual_prompt]: Inference (test):avg data time: 6.10e-03, avg batch time: 0.1935, average loss: 3.3230
[09/15 13:59:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 28.12	
[09/15 13:59:02 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/15 13:59:11 visual_prompt]: Epoch 9 / 100: avg data time: 8.97e-02, avg batch time: 0.4955, average train loss: 3.5702
[09/15 13:59:14 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1437, average loss: 3.3946
[09/15 13:59:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 32.00	
[09/15 13:59:35 visual_prompt]: 	Test 100/190. loss: 3.282, 0.1960 s / batch. (data: 1.40e-04)max mem: 17.22456 GB 
[09/15 13:59:53 visual_prompt]: Inference (test):avg data time: 5.51e-03, avg batch time: 0.1943, average loss: 3.4472
[09/15 13:59:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.42	top5: 27.47	
[09/15 13:59:53 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/15 14:00:02 visual_prompt]: Epoch 10 / 100: avg data time: 9.31e-02, avg batch time: 0.4978, average train loss: 3.6003
[09/15 14:00:05 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1436, average loss: 3.3869
[09/15 14:00:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 27.50	
[09/15 14:00:26 visual_prompt]: 	Test 100/190. loss: 3.319, 0.1834 s / batch. (data: 3.53e-05)max mem: 17.22456 GB 
[09/15 14:00:44 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1956, average loss: 3.4593
[09/15 14:00:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.72	top5: 27.42	
[09/15 14:00:44 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/15 14:00:53 visual_prompt]: Epoch 11 / 100: avg data time: 8.38e-02, avg batch time: 0.4916, average train loss: 3.2958
[09/15 14:00:56 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1436, average loss: 3.6931
[09/15 14:00:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 30.50	
[09/15 14:01:17 visual_prompt]: 	Test 100/190. loss: 3.445, 0.1962 s / batch. (data: 1.34e-04)max mem: 17.22456 GB 
[09/15 14:01:35 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1957, average loss: 3.6222
[09/15 14:01:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.69	
[09/15 14:01:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/15 14:01:44 visual_prompt]: Epoch 12 / 100: avg data time: 8.47e-02, avg batch time: 0.4887, average train loss: 3.5731
[09/15 14:01:47 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1436, average loss: 4.9531
[09/15 14:01:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 28.00	
[09/15 14:02:08 visual_prompt]: 	Test 100/190. loss: 3.850, 0.2054 s / batch. (data: 1.26e-02)max mem: 17.22456 GB 
[09/15 14:02:26 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1951, average loss: 4.5655
[09/15 14:02:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.26	
[09/15 14:02:26 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/15 14:02:35 visual_prompt]: Epoch 13 / 100: avg data time: 9.81e-02, avg batch time: 0.5014, average train loss: 11.3299
[09/15 14:02:38 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1433, average loss: 11.3612
[09/15 14:02:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 27.00	
[09/15 14:02:59 visual_prompt]: 	Test 100/190. loss: 7.806, 0.2123 s / batch. (data: 2.86e-02)max mem: 17.22456 GB 
[09/15 14:03:18 visual_prompt]: Inference (test):avg data time: 8.38e-03, avg batch time: 0.1954, average loss: 10.8796
[09/15 14:03:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 27.55	
[09/15 14:03:18 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/15 14:03:27 visual_prompt]: Epoch 14 / 100: avg data time: 9.84e-02, avg batch time: 0.5015, average train loss: 11.8486
[09/15 14:03:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1436, average loss: 12.5998
[09/15 14:03:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 26.50	
[09/15 14:03:50 visual_prompt]: 	Test 100/190. loss: 9.255, 0.1994 s / batch. (data: 1.60e-02)max mem: 17.22456 GB 
[09/15 14:04:09 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1956, average loss: 11.6918
[09/15 14:04:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.52	top5: 27.84	
[09/15 14:04:09 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/15 14:04:18 visual_prompt]: Epoch 15 / 100: avg data time: 1.01e-01, avg batch time: 0.5036, average train loss: 9.3150
[09/15 14:04:20 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1435, average loss: 8.4075
[09/15 14:04:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.00	
[09/15 14:04:42 visual_prompt]: 	Test 100/190. loss: 5.418, 0.1952 s / batch. (data: 3.53e-05)max mem: 17.22456 GB 
[09/15 14:05:00 visual_prompt]: Inference (test):avg data time: 8.68e-03, avg batch time: 0.1958, average loss: 7.8630
[09/15 14:05:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.53	top5: 27.91	
[09/15 14:05:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/15 14:05:09 visual_prompt]: Epoch 16 / 100: avg data time: 9.94e-02, avg batch time: 0.5043, average train loss: 6.3410
[09/15 14:05:12 visual_prompt]: Inference (val):avg data time: 4.75e-05, avg batch time: 0.1435, average loss: 5.3956
[09/15 14:05:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 27.50	
[09/15 14:05:33 visual_prompt]: 	Test 100/190. loss: 4.226, 0.1843 s / batch. (data: 1.39e-04)max mem: 17.22456 GB 
[09/15 14:05:51 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1957, average loss: 5.3157
[09/15 14:05:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.66	
[09/15 14:05:51 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/15 14:06:00 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e-01, avg batch time: 0.5069, average train loss: 4.7317
[09/15 14:06:03 visual_prompt]: Inference (val):avg data time: 4.87e-05, avg batch time: 0.1435, average loss: 4.5672
[09/15 14:06:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 24.00	
[09/15 14:06:24 visual_prompt]: 	Test 100/190. loss: 3.863, 0.1973 s / batch. (data: 1.41e-02)max mem: 17.22456 GB 
[09/15 14:06:43 visual_prompt]: Inference (test):avg data time: 8.73e-03, avg batch time: 0.1970, average loss: 4.4533
[09/15 14:06:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 28.00	
[09/15 14:06:43 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/15 14:06:52 visual_prompt]: Epoch 18 / 100: avg data time: 8.14e-02, avg batch time: 0.4903, average train loss: 3.9669
[09/15 14:06:54 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1436, average loss: 3.5530
[09/15 14:06:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 26.50	
[09/15 14:07:15 visual_prompt]: 	Test 100/190. loss: 3.602, 0.1959 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 14:07:34 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1950, average loss: 3.6195
[09/15 14:07:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.49	
[09/15 14:07:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/15 14:07:43 visual_prompt]: Epoch 19 / 100: avg data time: 9.34e-02, avg batch time: 0.4952, average train loss: 3.3954
[09/15 14:07:45 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1434, average loss: 3.0946
[09/15 14:07:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 29.50	
[09/15 14:08:06 visual_prompt]: 	Test 100/190. loss: 3.121, 0.1989 s / batch. (data: 1.55e-02)max mem: 17.22456 GB 
[09/15 14:08:25 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1956, average loss: 3.1000
[09/15 14:08:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.59	
[09/15 14:08:25 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/15 14:08:34 visual_prompt]: Epoch 20 / 100: avg data time: 9.58e-02, avg batch time: 0.5005, average train loss: 3.3459
[09/15 14:08:36 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1435, average loss: 3.2837
[09/15 14:08:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 3.00	top5: 28.50	
[09/15 14:08:57 visual_prompt]: 	Test 100/190. loss: 3.373, 0.1899 s / batch. (data: 1.32e-04)max mem: 17.22456 GB 
[09/15 14:09:16 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1953, average loss: 3.3346
[09/15 14:09:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.43	top5: 27.62	
[09/15 14:09:16 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/15 14:09:25 visual_prompt]: Epoch 21 / 100: avg data time: 8.72e-02, avg batch time: 0.4920, average train loss: 3.3605
[09/15 14:09:27 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1436, average loss: 3.2934
[09/15 14:09:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 32.50	
[09/15 14:09:49 visual_prompt]: 	Test 100/190. loss: 3.173, 0.1836 s / batch. (data: 1.39e-04)max mem: 17.22456 GB 
[09/15 14:10:07 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1961, average loss: 3.3686
[09/15 14:10:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.89	
[09/15 14:10:07 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/15 14:10:16 visual_prompt]: Epoch 22 / 100: avg data time: 9.47e-02, avg batch time: 0.4969, average train loss: 3.1782
[09/15 14:10:19 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1437, average loss: 3.0447
[09/15 14:10:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 30.50	
[09/15 14:10:40 visual_prompt]: 	Test 100/190. loss: 3.252, 0.2200 s / batch. (data: 1.29e-04)max mem: 17.22456 GB 
[09/15 14:10:59 visual_prompt]: Inference (test):avg data time: 6.63e-03, avg batch time: 0.1950, average loss: 3.0541
[09/15 14:10:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.54	top5: 28.24	
[09/15 14:10:59 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/15 14:11:08 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e-01, avg batch time: 0.5055, average train loss: 3.2461
[09/15 14:11:10 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1435, average loss: 3.4192
[09/15 14:11:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 31.00	
[09/15 14:11:31 visual_prompt]: 	Test 100/190. loss: 3.145, 0.2045 s / batch. (data: 1.45e-02)max mem: 17.22456 GB 
[09/15 14:11:50 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1949, average loss: 3.4172
[09/15 14:11:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.49	top5: 27.68	
[09/15 14:11:50 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/15 14:11:59 visual_prompt]: Epoch 24 / 100: avg data time: 8.28e-02, avg batch time: 0.4888, average train loss: 3.3217
[09/15 14:12:01 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1435, average loss: 3.3062
[09/15 14:12:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.50	
[09/15 14:12:22 visual_prompt]: 	Test 100/190. loss: 3.121, 0.2052 s / batch. (data: 1.25e-02)max mem: 17.22456 GB 
[09/15 14:12:41 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1954, average loss: 3.3340
[09/15 14:12:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.67	top5: 27.63	
[09/15 14:12:41 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/15 14:12:50 visual_prompt]: Epoch 25 / 100: avg data time: 8.66e-02, avg batch time: 0.4924, average train loss: 3.3848
[09/15 14:12:52 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1436, average loss: 3.0509
[09/15 14:12:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 31.00	
[09/15 14:13:13 visual_prompt]: 	Test 100/190. loss: 3.152, 0.1978 s / batch. (data: 1.43e-02)max mem: 17.22456 GB 
[09/15 14:13:32 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1958, average loss: 3.1065
[09/15 14:13:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.71	
[09/15 14:13:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/15 14:13:41 visual_prompt]: Epoch 26 / 100: avg data time: 8.73e-02, avg batch time: 0.4929, average train loss: 3.2302
[09/15 14:13:44 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1435, average loss: 3.3545
[09/15 14:13:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 29.50	
[09/15 14:14:04 visual_prompt]: 	Test 100/190. loss: 3.495, 0.1842 s / batch. (data: 9.58e-05)max mem: 17.22456 GB 
[09/15 14:14:23 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1948, average loss: 3.4536
[09/15 14:14:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 28.15	
[09/15 14:14:23 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/15 14:14:32 visual_prompt]: Epoch 27 / 100: avg data time: 9.43e-02, avg batch time: 0.4985, average train loss: 3.3059
[09/15 14:14:35 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1460, average loss: 3.0969
[09/15 14:14:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 3.00	top5: 27.50	
[09/15 14:14:56 visual_prompt]: 	Test 100/190. loss: 3.097, 0.2177 s / batch. (data: 3.43e-02)max mem: 17.22456 GB 
[09/15 14:15:14 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1948, average loss: 3.0816
[09/15 14:15:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.43	top5: 27.55	
[09/15 14:15:14 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/15 14:15:23 visual_prompt]: Epoch 28 / 100: avg data time: 1.02e-01, avg batch time: 0.5073, average train loss: 3.2081
[09/15 14:15:26 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1436, average loss: 3.0629
[09/15 14:15:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 26.50	
[09/15 14:15:47 visual_prompt]: 	Test 100/190. loss: 3.167, 0.1836 s / batch. (data: 1.03e-04)max mem: 17.22456 GB 
[09/15 14:16:06 visual_prompt]: Inference (test):avg data time: 8.72e-03, avg batch time: 0.1972, average loss: 3.0858
[09/15 14:16:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.53	top5: 28.00	
[09/15 14:16:06 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/15 14:16:15 visual_prompt]: Epoch 29 / 100: avg data time: 8.53e-02, avg batch time: 0.4944, average train loss: 3.1936
[09/15 14:16:17 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1435, average loss: 3.2610
[09/15 14:16:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 26.50	
[09/15 14:16:38 visual_prompt]: 	Test 100/190. loss: 3.107, 0.1854 s / batch. (data: 1.22e-04)max mem: 17.22456 GB 
[09/15 14:16:57 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1947, average loss: 3.2289
[09/15 14:16:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.43	top5: 27.82	
[09/15 14:16:57 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/15 14:17:06 visual_prompt]: Epoch 30 / 100: avg data time: 1.07e-01, avg batch time: 0.5103, average train loss: 3.1719
[09/15 14:17:08 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1461, average loss: 3.1304
[09/15 14:17:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 32.50	
[09/15 14:17:29 visual_prompt]: 	Test 100/190. loss: 3.196, 0.2265 s / batch. (data: 3.41e-02)max mem: 17.22456 GB 
[09/15 14:17:48 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1959, average loss: 3.2685
[09/15 14:17:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.78	
[09/15 14:17:48 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/15 14:17:57 visual_prompt]: Epoch 31 / 100: avg data time: 8.91e-02, avg batch time: 0.4971, average train loss: 3.1333
[09/15 14:18:00 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1437, average loss: 3.1960
[09/15 14:18:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 31.50	
[09/15 14:18:21 visual_prompt]: 	Test 100/190. loss: 3.161, 0.1841 s / batch. (data: 1.26e-04)max mem: 17.22456 GB 
[09/15 14:18:39 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1957, average loss: 3.2519
[09/15 14:18:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.48	
[09/15 14:18:39 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/15 14:18:48 visual_prompt]: Epoch 32 / 100: avg data time: 9.49e-02, avg batch time: 0.4981, average train loss: 3.1496
[09/15 14:18:51 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1436, average loss: 3.1697
[09/15 14:18:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 3.00	top5: 25.50	
[09/15 14:19:12 visual_prompt]: 	Test 100/190. loss: 3.110, 0.2008 s / batch. (data: 1.62e-02)max mem: 17.22456 GB 
[09/15 14:19:31 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1959, average loss: 3.1415
[09/15 14:19:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.43	top5: 28.05	
[09/15 14:19:31 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/15 14:19:39 visual_prompt]: Epoch 33 / 100: avg data time: 8.40e-02, avg batch time: 0.4897, average train loss: 3.1624
[09/15 14:19:42 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1446, average loss: 3.1245
[09/15 14:19:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 30.00	
[09/15 14:20:03 visual_prompt]: 	Test 100/190. loss: 3.177, 0.2093 s / batch. (data: 2.49e-02)max mem: 17.22456 GB 
[09/15 14:20:21 visual_prompt]: Inference (test):avg data time: 6.94e-03, avg batch time: 0.1945, average loss: 3.1904
[09/15 14:20:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.52	
[09/15 14:20:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/15 14:20:30 visual_prompt]: Epoch 34 / 100: avg data time: 9.81e-02, avg batch time: 0.5018, average train loss: 3.1399
[09/15 14:20:33 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1435, average loss: 3.1531
[09/15 14:20:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 28.00	
[09/15 14:20:54 visual_prompt]: 	Test 100/190. loss: 3.090, 0.2063 s / batch. (data: 2.31e-02)max mem: 17.22456 GB 
[09/15 14:21:13 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1956, average loss: 3.1746
[09/15 14:21:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.83	top5: 27.61	
[09/15 14:21:13 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/15 14:21:21 visual_prompt]: Epoch 35 / 100: avg data time: 9.33e-02, avg batch time: 0.4984, average train loss: 3.1309
[09/15 14:21:24 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1434, average loss: 3.1468
[09/15 14:21:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 31.50	
[09/15 14:21:45 visual_prompt]: 	Test 100/190. loss: 3.439, 0.1946 s / batch. (data: 3.34e-05)max mem: 17.22456 GB 
[09/15 14:22:04 visual_prompt]: Inference (test):avg data time: 6.91e-03, avg batch time: 0.1965, average loss: 3.2169
[09/15 14:22:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.69	
[09/15 14:22:04 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/15 14:22:13 visual_prompt]: Epoch 36 / 100: avg data time: 9.66e-02, avg batch time: 0.5008, average train loss: 3.1195
[09/15 14:22:15 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1436, average loss: 3.1384
[09/15 14:22:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 32.00	
[09/15 14:22:36 visual_prompt]: 	Test 100/190. loss: 3.064, 0.1833 s / batch. (data: 1.18e-04)max mem: 17.22456 GB 
[09/15 14:22:55 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1958, average loss: 3.1829
[09/15 14:22:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 29.95	
[09/15 14:22:55 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/15 14:23:04 visual_prompt]: Epoch 37 / 100: avg data time: 9.79e-02, avg batch time: 0.5026, average train loss: 3.0870
[09/15 14:23:07 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1435, average loss: 3.0862
[09/15 14:23:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 33.00	
[09/15 14:23:28 visual_prompt]: 	Test 100/190. loss: 3.162, 0.1891 s / batch. (data: 1.20e-04)max mem: 17.22456 GB 
[09/15 14:23:47 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1955, average loss: 3.1714
[09/15 14:23:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.49	top5: 28.72	
[09/15 14:23:47 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/15 14:23:55 visual_prompt]: Epoch 38 / 100: avg data time: 8.48e-02, avg batch time: 0.4914, average train loss: 3.0672
[09/15 14:23:58 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1436, average loss: 3.0746
[09/15 14:23:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 27.50	
[09/15 14:24:19 visual_prompt]: 	Test 100/190. loss: 3.233, 0.1844 s / batch. (data: 1.50e-04)max mem: 17.22456 GB 
[09/15 14:24:37 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1951, average loss: 3.1177
[09/15 14:24:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.32	top5: 28.96	
[09/15 14:24:38 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/15 14:24:46 visual_prompt]: Epoch 39 / 100: avg data time: 8.70e-02, avg batch time: 0.4940, average train loss: 3.0523
[09/15 14:24:49 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1436, average loss: 2.9857
[09/15 14:24:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.00	top5: 38.50	
[09/15 14:25:10 visual_prompt]: 	Test 100/190. loss: 2.948, 0.1840 s / batch. (data: 1.60e-04)max mem: 17.22456 GB 
[09/15 14:25:29 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1952, average loss: 3.0494
[09/15 14:25:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.94	top5: 34.13	
[09/15 14:25:29 visual_prompt]: Best epoch 39: best metric: 0.110
[09/15 14:25:29 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/15 14:25:37 visual_prompt]: Epoch 40 / 100: avg data time: 7.66e-02, avg batch time: 0.4833, average train loss: 3.0176
[09/15 14:25:40 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1436, average loss: 2.8750
[09/15 14:25:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 36.50	
[09/15 14:26:01 visual_prompt]: 	Test 100/190. loss: 3.003, 0.1994 s / batch. (data: 1.60e-02)max mem: 17.22456 GB 
[09/15 14:26:19 visual_prompt]: Inference (test):avg data time: 6.80e-03, avg batch time: 0.1943, average loss: 2.9177
[09/15 14:26:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.52	top5: 33.56	
[09/15 14:26:20 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/15 14:26:28 visual_prompt]: Epoch 41 / 100: avg data time: 8.54e-02, avg batch time: 0.4918, average train loss: 3.0295
[09/15 14:26:31 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1435, average loss: 2.9700
[09/15 14:26:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 36.00	
[09/15 14:26:52 visual_prompt]: 	Test 100/190. loss: 3.056, 0.2187 s / batch. (data: 2.21e-02)max mem: 17.22456 GB 
[09/15 14:27:11 visual_prompt]: Inference (test):avg data time: 8.54e-03, avg batch time: 0.1958, average loss: 2.9858
[09/15 14:27:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 35.17	
[09/15 14:27:11 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/15 14:27:19 visual_prompt]: Epoch 42 / 100: avg data time: 9.43e-02, avg batch time: 0.4986, average train loss: 3.0418
[09/15 14:27:22 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1437, average loss: 2.9002
[09/15 14:27:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.50	top5: 36.00	
[09/15 14:27:43 visual_prompt]: 	Test 100/190. loss: 2.953, 0.2191 s / batch. (data: 2.09e-02)max mem: 17.22456 GB 
[09/15 14:28:02 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1950, average loss: 2.9639
[09/15 14:28:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.77	top5: 34.34	
[09/15 14:28:02 visual_prompt]: Best epoch 42: best metric: 0.125
[09/15 14:28:02 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/15 14:28:10 visual_prompt]: Epoch 43 / 100: avg data time: 8.78e-02, avg batch time: 0.4942, average train loss: 2.9625
[09/15 14:28:13 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1437, average loss: 2.8517
[09/15 14:28:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 40.50	
[09/15 14:28:34 visual_prompt]: 	Test 100/190. loss: 2.831, 0.1958 s / batch. (data: 1.47e-04)max mem: 17.22456 GB 
[09/15 14:28:53 visual_prompt]: Inference (test):avg data time: 8.40e-03, avg batch time: 0.1956, average loss: 2.9051
[09/15 14:28:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.90	top5: 36.63	
[09/15 14:28:53 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/15 14:29:02 visual_prompt]: Epoch 44 / 100: avg data time: 9.38e-02, avg batch time: 0.4978, average train loss: 2.9975
[09/15 14:29:04 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1435, average loss: 2.9278
[09/15 14:29:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.00	top5: 42.00	
[09/15 14:29:25 visual_prompt]: 	Test 100/190. loss: 3.173, 0.1955 s / batch. (data: 1.53e-04)max mem: 17.22456 GB 
[09/15 14:29:44 visual_prompt]: Inference (test):avg data time: 6.89e-03, avg batch time: 0.1951, average loss: 3.0418
[09/15 14:29:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.65	top5: 35.76	
[09/15 14:29:44 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/15 14:29:53 visual_prompt]: Epoch 45 / 100: avg data time: 9.79e-02, avg batch time: 0.5014, average train loss: 2.9365
[09/15 14:29:55 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1435, average loss: 2.9113
[09/15 14:29:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 36.50	
[09/15 14:30:16 visual_prompt]: 	Test 100/190. loss: 2.765, 0.2025 s / batch. (data: 1.19e-02)max mem: 17.22456 GB 
[09/15 14:30:35 visual_prompt]: Inference (test):avg data time: 6.60e-03, avg batch time: 0.1947, average loss: 2.9424
[09/15 14:30:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.93	top5: 34.46	
[09/15 14:30:35 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/15 14:30:44 visual_prompt]: Epoch 46 / 100: avg data time: 9.22e-02, avg batch time: 0.4953, average train loss: 2.9992
[09/15 14:30:46 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1437, average loss: 2.9870
[09/15 14:30:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 41.50	
[09/15 14:31:07 visual_prompt]: 	Test 100/190. loss: 3.057, 0.1838 s / batch. (data: 1.50e-04)max mem: 17.22456 GB 
[09/15 14:31:26 visual_prompt]: Inference (test):avg data time: 6.35e-03, avg batch time: 0.1947, average loss: 3.1255
[09/15 14:31:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.66	top5: 34.30	
[09/15 14:31:26 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/15 14:31:35 visual_prompt]: Epoch 47 / 100: avg data time: 9.25e-02, avg batch time: 0.4988, average train loss: 2.9410
[09/15 14:31:37 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1435, average loss: 2.7220
[09/15 14:31:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 52.50	
[09/15 14:31:58 visual_prompt]: 	Test 100/190. loss: 2.938, 0.1844 s / batch. (data: 1.52e-04)max mem: 17.22456 GB 
[09/15 14:32:17 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1955, average loss: 2.8802
[09/15 14:32:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.12	top5: 40.70	
[09/15 14:32:17 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/15 14:32:26 visual_prompt]: Epoch 48 / 100: avg data time: 1.01e-01, avg batch time: 0.5073, average train loss: 2.8459
[09/15 14:32:29 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1435, average loss: 2.7638
[09/15 14:32:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.50	top5: 45.00	
[09/15 14:32:50 visual_prompt]: 	Test 100/190. loss: 2.860, 0.2122 s / batch. (data: 2.86e-02)max mem: 17.22456 GB 
[09/15 14:33:08 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1949, average loss: 2.8680
[09/15 14:33:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.40	top5: 39.26	
[09/15 14:33:08 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/15 14:33:17 visual_prompt]: Epoch 49 / 100: avg data time: 1.02e-01, avg batch time: 0.5059, average train loss: 2.8610
[09/15 14:33:20 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1436, average loss: 3.0171
[09/15 14:33:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 41.00	
[09/15 14:33:41 visual_prompt]: 	Test 100/190. loss: 3.091, 0.2037 s / batch. (data: 1.33e-04)max mem: 17.22456 GB 
[09/15 14:34:00 visual_prompt]: Inference (test):avg data time: 8.77e-03, avg batch time: 0.1965, average loss: 3.0645
[09/15 14:34:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.36	top5: 38.92	
[09/15 14:34:00 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/15 14:34:09 visual_prompt]: Epoch 50 / 100: avg data time: 9.25e-02, avg batch time: 0.4977, average train loss: 2.8993
[09/15 14:34:11 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1434, average loss: 2.8845
[09/15 14:34:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.50	top5: 47.50	
[09/15 14:34:32 visual_prompt]: 	Test 100/190. loss: 3.153, 0.2039 s / batch. (data: 1.12e-02)max mem: 17.22456 GB 
[09/15 14:34:51 visual_prompt]: Inference (test):avg data time: 6.77e-03, avg batch time: 0.1954, average loss: 3.0158
[09/15 14:34:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.26	top5: 42.63	
[09/15 14:34:51 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/15 14:35:00 visual_prompt]: Epoch 51 / 100: avg data time: 9.85e-02, avg batch time: 0.5027, average train loss: 2.9531
[09/15 14:35:02 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1436, average loss: 2.8115
[09/15 14:35:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.50	top5: 45.00	
[09/15 14:35:23 visual_prompt]: 	Test 100/190. loss: 2.789, 0.1987 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 14:35:42 visual_prompt]: Inference (test):avg data time: 8.06e-03, avg batch time: 0.1951, average loss: 2.9394
[09/15 14:35:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.88	top5: 41.25	
[09/15 14:35:42 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/15 14:35:51 visual_prompt]: Epoch 52 / 100: avg data time: 1.00e-01, avg batch time: 0.5042, average train loss: 2.7715
[09/15 14:35:54 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1436, average loss: 2.6999
[09/15 14:35:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.50	top5: 51.00	
[09/15 14:36:15 visual_prompt]: 	Test 100/190. loss: 2.779, 0.1844 s / batch. (data: 1.43e-04)max mem: 17.22456 GB 
[09/15 14:36:33 visual_prompt]: Inference (test):avg data time: 6.77e-03, avg batch time: 0.1940, average loss: 2.8910
[09/15 14:36:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.28	top5: 42.29	
[09/15 14:36:33 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/15 14:36:42 visual_prompt]: Epoch 53 / 100: avg data time: 8.87e-02, avg batch time: 0.4932, average train loss: 2.8624
[09/15 14:36:45 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1435, average loss: 2.6958
[09/15 14:36:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.50	top5: 49.00	
[09/15 14:37:06 visual_prompt]: 	Test 100/190. loss: 3.039, 0.1878 s / batch. (data: 1.59e-04)max mem: 17.22456 GB 
[09/15 14:37:24 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1958, average loss: 2.9067
[09/15 14:37:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.75	top5: 38.00	
[09/15 14:37:24 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/15 14:37:33 visual_prompt]: Epoch 54 / 100: avg data time: 9.66e-02, avg batch time: 0.5008, average train loss: 2.7766
[09/15 14:37:36 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1436, average loss: 2.6037
[09/15 14:37:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.50	top5: 54.50	
[09/15 14:37:57 visual_prompt]: 	Test 100/190. loss: 2.770, 0.1934 s / batch. (data: 3.43e-05)max mem: 17.22456 GB 
[09/15 14:38:15 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1953, average loss: 2.7728
[09/15 14:38:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.83	top5: 43.18	
[09/15 14:38:15 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/15 14:38:24 visual_prompt]: Epoch 55 / 100: avg data time: 8.77e-02, avg batch time: 0.4935, average train loss: 2.7152
[09/15 14:38:27 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1435, average loss: 2.7179
[09/15 14:38:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.50	top5: 51.00	
[09/15 14:38:48 visual_prompt]: 	Test 100/190. loss: 3.113, 0.1981 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 14:39:07 visual_prompt]: Inference (test):avg data time: 6.49e-03, avg batch time: 0.1954, average loss: 2.9441
[09/15 14:39:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.26	top5: 41.87	
[09/15 14:39:07 visual_prompt]: Best epoch 55: best metric: 0.145
[09/15 14:39:07 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/15 14:39:15 visual_prompt]: Epoch 56 / 100: avg data time: 9.94e-02, avg batch time: 0.5022, average train loss: 2.7539
[09/15 14:39:18 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1437, average loss: 3.0455
[09/15 14:39:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 28.50	
[09/15 14:39:39 visual_prompt]: 	Test 100/190. loss: 2.941, 0.2019 s / batch. (data: 1.18e-02)max mem: 17.22456 GB 
[09/15 14:39:58 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1945, average loss: 3.0429
[09/15 14:39:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.04	top5: 28.53	
[09/15 14:39:58 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/15 14:40:06 visual_prompt]: Epoch 57 / 100: avg data time: 8.37e-02, avg batch time: 0.4892, average train loss: 3.3159
[09/15 14:40:09 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1434, average loss: 3.3479
[09/15 14:40:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 29.50	
[09/15 14:40:30 visual_prompt]: 	Test 100/190. loss: 3.503, 0.1852 s / batch. (data: 1.34e-04)max mem: 17.22456 GB 
[09/15 14:40:49 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1951, average loss: 3.4256
[09/15 14:40:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.49	top5: 27.44	
[09/15 14:40:49 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/15 14:40:57 visual_prompt]: Epoch 58 / 100: avg data time: 9.30e-02, avg batch time: 0.4980, average train loss: 3.1976
[09/15 14:41:00 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1433, average loss: 3.2963
[09/15 14:41:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 28.50	
[09/15 14:41:21 visual_prompt]: 	Test 100/190. loss: 3.270, 0.1888 s / batch. (data: 1.12e-04)max mem: 17.22456 GB 
[09/15 14:41:40 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1963, average loss: 3.3770
[09/15 14:41:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 27.97	
[09/15 14:41:40 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/15 14:41:49 visual_prompt]: Epoch 59 / 100: avg data time: 8.96e-02, avg batch time: 0.4948, average train loss: 3.2954
[09/15 14:41:51 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1436, average loss: 3.1550
[09/15 14:41:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 27.00	
[09/15 14:42:12 visual_prompt]: 	Test 100/190. loss: 3.145, 0.1999 s / batch. (data: 7.23e-03)max mem: 17.22456 GB 
[09/15 14:42:31 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1956, average loss: 3.1602
[09/15 14:42:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.88	top5: 27.90	
[09/15 14:42:31 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/15 14:42:40 visual_prompt]: Epoch 60 / 100: avg data time: 8.99e-02, avg batch time: 0.4957, average train loss: 3.1411
[09/15 14:42:43 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1436, average loss: 3.1433
[09/15 14:42:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 25.00	
[09/15 14:43:04 visual_prompt]: 	Test 100/190. loss: 3.074, 0.1841 s / batch. (data: 1.51e-04)max mem: 17.22456 GB 
[09/15 14:43:22 visual_prompt]: Inference (test):avg data time: 8.52e-03, avg batch time: 0.1961, average loss: 3.1490
[09/15 14:43:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.73	
[09/15 14:43:23 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/15 14:43:31 visual_prompt]: Epoch 61 / 100: avg data time: 9.54e-02, avg batch time: 0.4996, average train loss: 3.1412
[09/15 14:43:34 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1436, average loss: 3.2222
[09/15 14:43:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 30.50	
[09/15 14:43:55 visual_prompt]: 	Test 100/190. loss: 3.525, 0.2185 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 14:44:14 visual_prompt]: Inference (test):avg data time: 6.86e-03, avg batch time: 0.1948, average loss: 3.2659
[09/15 14:44:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.58	
[09/15 14:44:14 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/15 14:44:22 visual_prompt]: Epoch 62 / 100: avg data time: 1.04e-01, avg batch time: 0.5075, average train loss: 3.1570
[09/15 14:44:25 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1436, average loss: 3.0453
[09/15 14:44:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 30.00	
[09/15 14:44:46 visual_prompt]: 	Test 100/190. loss: 3.037, 0.1881 s / batch. (data: 1.44e-04)max mem: 17.22456 GB 
[09/15 14:45:05 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1948, average loss: 3.0501
[09/15 14:45:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.59	
[09/15 14:45:05 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/15 14:45:13 visual_prompt]: Epoch 63 / 100: avg data time: 8.54e-02, avg batch time: 0.4907, average train loss: 3.0975
[09/15 14:45:16 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1447, average loss: 3.0202
[09/15 14:45:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 29.00	
[09/15 14:45:37 visual_prompt]: 	Test 100/190. loss: 3.118, 0.1995 s / batch. (data: 1.59e-02)max mem: 17.22456 GB 
[09/15 14:45:56 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1958, average loss: 3.0420
[09/15 14:45:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.48	
[09/15 14:45:56 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/15 14:46:05 visual_prompt]: Epoch 64 / 100: avg data time: 9.43e-02, avg batch time: 0.4981, average train loss: 3.0549
[09/15 14:46:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1435, average loss: 3.0730
[09/15 14:46:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 31.50	
[09/15 14:46:28 visual_prompt]: 	Test 100/190. loss: 3.169, 0.1871 s / batch. (data: 3.00e-05)max mem: 17.22456 GB 
[09/15 14:46:47 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1950, average loss: 3.1248
[09/15 14:46:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.02	top5: 27.96	
[09/15 14:46:47 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/15 14:46:55 visual_prompt]: Epoch 65 / 100: avg data time: 7.32e-02, avg batch time: 0.4807, average train loss: 3.0668
[09/15 14:46:58 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1436, average loss: 3.1603
[09/15 14:46:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 28.00	
[09/15 14:47:19 visual_prompt]: 	Test 100/190. loss: 3.086, 0.1986 s / batch. (data: 1.56e-02)max mem: 17.22456 GB 
[09/15 14:47:38 visual_prompt]: Inference (test):avg data time: 7.10e-03, avg batch time: 0.1941, average loss: 3.1656
[09/15 14:47:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.80	top5: 28.09	
[09/15 14:47:38 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/15 14:47:46 visual_prompt]: Epoch 66 / 100: avg data time: 8.07e-02, avg batch time: 0.4879, average train loss: 3.0822
[09/15 14:47:49 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1436, average loss: 3.0879
[09/15 14:47:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 31.00	
[09/15 14:48:10 visual_prompt]: 	Test 100/190. loss: 2.886, 0.1974 s / batch. (data: 1.63e-04)max mem: 17.22456 GB 
[09/15 14:48:29 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1955, average loss: 3.1547
[09/15 14:48:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.55	top5: 27.46	
[09/15 14:48:29 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/15 14:48:37 visual_prompt]: Epoch 67 / 100: avg data time: 8.63e-02, avg batch time: 0.4919, average train loss: 3.0552
[09/15 14:48:40 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1439, average loss: 3.0125
[09/15 14:48:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 34.00	
[09/15 14:49:01 visual_prompt]: 	Test 100/190. loss: 3.044, 0.1893 s / batch. (data: 1.14e-04)max mem: 17.22456 GB 
[09/15 14:49:20 visual_prompt]: Inference (test):avg data time: 6.91e-03, avg batch time: 0.1954, average loss: 3.0738
[09/15 14:49:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 28.06	
[09/15 14:49:20 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/15 14:49:29 visual_prompt]: Epoch 68 / 100: avg data time: 9.59e-02, avg batch time: 0.5006, average train loss: 2.9875
[09/15 14:49:31 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1436, average loss: 2.9584
[09/15 14:49:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 32.50	
[09/15 14:49:52 visual_prompt]: 	Test 100/190. loss: 3.094, 0.1839 s / batch. (data: 1.17e-04)max mem: 17.22456 GB 
[09/15 14:50:11 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1950, average loss: 3.0351
[09/15 14:50:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.48	top5: 27.56	
[09/15 14:50:11 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/15 14:50:20 visual_prompt]: Epoch 69 / 100: avg data time: 1.00e-01, avg batch time: 0.5037, average train loss: 3.0333
[09/15 14:50:22 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1435, average loss: 2.9053
[09/15 14:50:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 35.00	
[09/15 14:50:44 visual_prompt]: 	Test 100/190. loss: 2.895, 0.2242 s / batch. (data: 2.53e-02)max mem: 17.22456 GB 
[09/15 14:51:02 visual_prompt]: Inference (test):avg data time: 8.44e-03, avg batch time: 0.1958, average loss: 2.9541
[09/15 14:51:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.84	top5: 30.20	
[09/15 14:51:02 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/15 14:51:11 visual_prompt]: Epoch 70 / 100: avg data time: 9.04e-02, avg batch time: 0.4946, average train loss: 2.9963
[09/15 14:51:14 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1435, average loss: 2.9262
[09/15 14:51:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 35.50	
[09/15 14:51:35 visual_prompt]: 	Test 100/190. loss: 2.752, 0.1841 s / batch. (data: 1.34e-04)max mem: 17.22456 GB 
[09/15 14:51:53 visual_prompt]: Inference (test):avg data time: 5.75e-03, avg batch time: 0.1937, average loss: 2.9271
[09/15 14:51:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.77	top5: 37.09	
[09/15 14:51:53 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/15 14:52:02 visual_prompt]: Epoch 71 / 100: avg data time: 9.90e-02, avg batch time: 0.5034, average train loss: 2.8620
[09/15 14:52:05 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1434, average loss: 2.7662
[09/15 14:52:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.50	top5: 47.50	
[09/15 14:52:26 visual_prompt]: 	Test 100/190. loss: 2.860, 0.1938 s / batch. (data: 1.57e-04)max mem: 17.22456 GB 
[09/15 14:52:45 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1967, average loss: 2.8362
[09/15 14:52:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.74	top5: 41.95	
[09/15 14:52:45 visual_prompt]: Best epoch 71: best metric: 0.185
[09/15 14:52:45 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/15 14:52:53 visual_prompt]: Epoch 72 / 100: avg data time: 8.40e-02, avg batch time: 0.4912, average train loss: 2.8352
[09/15 14:52:56 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1449, average loss: 2.7723
[09/15 14:52:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 45.00	
[09/15 14:53:17 visual_prompt]: 	Test 100/190. loss: 2.743, 0.1959 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 14:53:36 visual_prompt]: Inference (test):avg data time: 9.34e-03, avg batch time: 0.1967, average loss: 2.8685
[09/15 14:53:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.33	top5: 38.59	
[09/15 14:53:36 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/15 14:53:45 visual_prompt]: Epoch 73 / 100: avg data time: 8.08e-02, avg batch time: 0.4884, average train loss: 2.7379
[09/15 14:53:47 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1435, average loss: 2.5925
[09/15 14:53:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.00	top5: 56.00	
[09/15 14:54:08 visual_prompt]: 	Test 100/190. loss: 2.891, 0.1959 s / batch. (data: 1.02e-04)max mem: 17.22456 GB 
[09/15 14:54:27 visual_prompt]: Inference (test):avg data time: 6.55e-03, avg batch time: 0.1946, average loss: 2.9627
[09/15 14:54:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.10	top5: 42.83	
[09/15 14:54:27 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/15 14:54:36 visual_prompt]: Epoch 74 / 100: avg data time: 9.13e-02, avg batch time: 0.4978, average train loss: 2.7199
[09/15 14:54:38 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1435, average loss: 2.6411
[09/15 14:54:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.50	top5: 48.50	
[09/15 14:54:59 visual_prompt]: 	Test 100/190. loss: 2.834, 0.1917 s / batch. (data: 1.68e-04)max mem: 17.22456 GB 
[09/15 14:55:18 visual_prompt]: Inference (test):avg data time: 7.07e-03, avg batch time: 0.1950, average loss: 2.8033
[09/15 14:55:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.58	top5: 41.80	
[09/15 14:55:18 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/15 14:55:27 visual_prompt]: Epoch 75 / 100: avg data time: 9.16e-02, avg batch time: 0.4954, average train loss: 2.6988
[09/15 14:55:29 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1436, average loss: 2.8888
[09/15 14:55:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.50	top5: 43.50	
[09/15 14:55:50 visual_prompt]: 	Test 100/190. loss: 3.266, 0.1867 s / batch. (data: 1.47e-04)max mem: 17.22456 GB 
[09/15 14:56:09 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1949, average loss: 3.2440
[09/15 14:56:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.80	top5: 35.47	
[09/15 14:56:09 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/15 14:56:18 visual_prompt]: Epoch 76 / 100: avg data time: 8.48e-02, avg batch time: 0.4884, average train loss: 2.6952
[09/15 14:56:20 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1440, average loss: 2.5017
[09/15 14:56:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.50	top5: 60.00	
[09/15 14:56:41 visual_prompt]: 	Test 100/190. loss: 2.872, 0.1971 s / batch. (data: 1.38e-02)max mem: 17.22456 GB 
[09/15 14:57:00 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1958, average loss: 2.8097
[09/15 14:57:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.73	top5: 44.79	
[09/15 14:57:00 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/15 14:57:09 visual_prompt]: Epoch 77 / 100: avg data time: 9.00e-02, avg batch time: 0.4982, average train loss: 2.6715
[09/15 14:57:11 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1436, average loss: 2.5921
[09/15 14:57:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.00	top5: 51.00	
[09/15 14:57:32 visual_prompt]: 	Test 100/190. loss: 2.823, 0.1955 s / batch. (data: 1.21e-02)max mem: 17.22456 GB 
[09/15 14:57:51 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1954, average loss: 2.7753
[09/15 14:57:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.15	top5: 43.37	
[09/15 14:57:51 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/15 14:58:00 visual_prompt]: Epoch 78 / 100: avg data time: 9.83e-02, avg batch time: 0.5022, average train loss: 2.6400
[09/15 14:58:02 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1435, average loss: 2.4140
[09/15 14:58:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.50	top5: 65.50	
[09/15 14:58:24 visual_prompt]: 	Test 100/190. loss: 2.911, 0.1983 s / batch. (data: 1.23e-02)max mem: 17.22456 GB 
[09/15 14:58:42 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1961, average loss: 2.7547
[09/15 14:58:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.08	top5: 47.07	
[09/15 14:58:42 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/15 14:58:51 visual_prompt]: Epoch 79 / 100: avg data time: 1.01e-01, avg batch time: 0.5054, average train loss: 2.5405
[09/15 14:58:54 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1435, average loss: 2.5218
[09/15 14:58:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.50	top5: 55.50	
[09/15 14:59:15 visual_prompt]: 	Test 100/190. loss: 2.835, 0.1999 s / batch. (data: 1.29e-04)max mem: 17.22456 GB 
[09/15 14:59:33 visual_prompt]: Inference (test):avg data time: 6.89e-03, avg batch time: 0.1950, average loss: 2.8178
[09/15 14:59:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.33	top5: 45.74	
[09/15 14:59:33 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/15 14:59:42 visual_prompt]: Epoch 80 / 100: avg data time: 8.81e-02, avg batch time: 0.4939, average train loss: 2.4850
[09/15 14:59:45 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1435, average loss: 2.4553
[09/15 14:59:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.00	top5: 61.00	
[09/15 15:00:06 visual_prompt]: 	Test 100/190. loss: 3.122, 0.1873 s / batch. (data: 3.81e-05)max mem: 17.22456 GB 
[09/15 15:00:25 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1962, average loss: 2.9686
[09/15 15:00:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.44	top5: 45.25	
[09/15 15:00:25 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/15 15:00:34 visual_prompt]: Epoch 81 / 100: avg data time: 9.38e-02, avg batch time: 0.4996, average train loss: 2.4513
[09/15 15:00:36 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1434, average loss: 2.2877
[09/15 15:00:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.00	top5: 69.50	
[09/15 15:00:57 visual_prompt]: 	Test 100/190. loss: 2.830, 0.1920 s / batch. (data: 1.38e-04)max mem: 17.22456 GB 
[09/15 15:01:16 visual_prompt]: Inference (test):avg data time: 8.38e-03, avg batch time: 0.1957, average loss: 2.8658
[09/15 15:01:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.07	top5: 48.71	
[09/15 15:01:16 visual_prompt]: Best epoch 81: best metric: 0.200
[09/15 15:01:16 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/15 15:01:25 visual_prompt]: Epoch 82 / 100: avg data time: 9.87e-02, avg batch time: 0.5026, average train loss: 2.4443
[09/15 15:01:27 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1436, average loss: 2.3181
[09/15 15:01:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.00	top5: 68.50	
[09/15 15:01:48 visual_prompt]: 	Test 100/190. loss: 3.042, 0.2102 s / batch. (data: 2.72e-02)max mem: 17.22456 GB 
[09/15 15:02:07 visual_prompt]: Inference (test):avg data time: 5.97e-03, avg batch time: 0.1949, average loss: 2.8995
[09/15 15:02:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.10	top5: 49.73	
[09/15 15:02:07 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/15 15:02:16 visual_prompt]: Epoch 83 / 100: avg data time: 9.49e-02, avg batch time: 0.4986, average train loss: 2.3622
[09/15 15:02:18 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1436, average loss: 2.4153
[09/15 15:02:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.00	top5: 64.50	
[09/15 15:02:40 visual_prompt]: 	Test 100/190. loss: 3.153, 0.1850 s / batch. (data: 1.47e-04)max mem: 17.22456 GB 
[09/15 15:02:58 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1957, average loss: 2.8732
[09/15 15:02:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.52	top5: 49.93	
[09/15 15:02:58 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/15 15:03:07 visual_prompt]: Epoch 84 / 100: avg data time: 8.42e-02, avg batch time: 0.4912, average train loss: 2.3404
[09/15 15:03:10 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1434, average loss: 2.3026
[09/15 15:03:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.00	top5: 70.00	
[09/15 15:03:31 visual_prompt]: 	Test 100/190. loss: 3.199, 0.1929 s / batch. (data: 1.43e-04)max mem: 17.22456 GB 
[09/15 15:03:49 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1952, average loss: 3.1245
[09/15 15:03:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.69	top5: 47.31	
[09/15 15:03:49 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/15 15:03:58 visual_prompt]: Epoch 85 / 100: avg data time: 8.00e-02, avg batch time: 0.4847, average train loss: 2.2303
[09/15 15:04:01 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1434, average loss: 2.1444
[09/15 15:04:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.00	top5: 79.00	
[09/15 15:04:22 visual_prompt]: 	Test 100/190. loss: 2.953, 0.2003 s / batch. (data: 1.30e-04)max mem: 17.22456 GB 
[09/15 15:04:40 visual_prompt]: Inference (test):avg data time: 6.94e-03, avg batch time: 0.1947, average loss: 2.9406
[09/15 15:04:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.61	top5: 53.35	
[09/15 15:04:40 visual_prompt]: Best epoch 85: best metric: 0.250
[09/15 15:04:40 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/15 15:04:49 visual_prompt]: Epoch 86 / 100: avg data time: 8.95e-02, avg batch time: 0.4944, average train loss: 2.2145
[09/15 15:04:51 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1435, average loss: 2.1395
[09/15 15:04:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.50	top5: 78.00	
[09/15 15:05:13 visual_prompt]: 	Test 100/190. loss: 3.112, 0.1918 s / batch. (data: 8.30e-03)max mem: 17.22456 GB 
[09/15 15:05:31 visual_prompt]: Inference (test):avg data time: 6.64e-03, avg batch time: 0.1945, average loss: 2.9170
[09/15 15:05:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.59	top5: 54.68	
[09/15 15:05:31 visual_prompt]: Best epoch 86: best metric: 0.255
[09/15 15:05:31 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/15 15:05:40 visual_prompt]: Epoch 87 / 100: avg data time: 9.80e-02, avg batch time: 0.5052, average train loss: 2.1718
[09/15 15:05:43 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1437, average loss: 2.1282
[09/15 15:05:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.00	top5: 78.50	
[09/15 15:06:04 visual_prompt]: 	Test 100/190. loss: 3.221, 0.1844 s / batch. (data: 4.27e-05)max mem: 17.22456 GB 
[09/15 15:06:23 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1958, average loss: 3.1647
[09/15 15:06:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.60	top5: 51.89	
[09/15 15:06:23 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/15 15:06:31 visual_prompt]: Epoch 88 / 100: avg data time: 9.36e-02, avg batch time: 0.4975, average train loss: 2.1216
[09/15 15:06:34 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1436, average loss: 1.9466
[09/15 15:06:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.00	top5: 85.00	
[09/15 15:06:55 visual_prompt]: 	Test 100/190. loss: 3.177, 0.1903 s / batch. (data: 3.74e-05)max mem: 17.22456 GB 
[09/15 15:07:14 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1955, average loss: 2.9896
[09/15 15:07:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.80	top5: 55.60	
[09/15 15:07:14 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/15 15:07:22 visual_prompt]: Epoch 89 / 100: avg data time: 8.26e-02, avg batch time: 0.4884, average train loss: 2.0267
[09/15 15:07:25 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1437, average loss: 1.9616
[09/15 15:07:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.00	top5: 85.00	
[09/15 15:07:46 visual_prompt]: 	Test 100/190. loss: 3.238, 0.2072 s / batch. (data: 2.36e-02)max mem: 17.22456 GB 
[09/15 15:08:04 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1953, average loss: 3.1357
[09/15 15:08:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.95	top5: 54.82	
[09/15 15:08:05 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/15 15:08:13 visual_prompt]: Epoch 90 / 100: avg data time: 9.67e-02, avg batch time: 0.5002, average train loss: 1.9538
[09/15 15:08:16 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1434, average loss: 1.9309
[09/15 15:08:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.00	top5: 83.00	
[09/15 15:08:37 visual_prompt]: 	Test 100/190. loss: 3.186, 0.1998 s / batch. (data: 1.29e-04)max mem: 17.22456 GB 
[09/15 15:08:56 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1959, average loss: 3.1034
[09/15 15:08:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.26	top5: 57.48	
[09/15 15:08:56 visual_prompt]: Best epoch 90: best metric: 0.300
[09/15 15:08:56 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/15 15:09:04 visual_prompt]: Epoch 91 / 100: avg data time: 8.03e-02, avg batch time: 0.4844, average train loss: 1.9035
[09/15 15:09:07 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1432, average loss: 1.7820
[09/15 15:09:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 32.50	top5: 89.00	
[09/15 15:09:28 visual_prompt]: 	Test 100/190. loss: 3.519, 0.1841 s / batch. (data: 1.29e-04)max mem: 17.22456 GB 
[09/15 15:09:47 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1965, average loss: 3.3128
[09/15 15:09:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.08	top5: 56.79	
[09/15 15:09:47 visual_prompt]: Best epoch 91: best metric: 0.325
[09/15 15:09:47 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/15 15:09:56 visual_prompt]: Epoch 92 / 100: avg data time: 1.04e-01, avg batch time: 0.5073, average train loss: 1.8359
[09/15 15:09:58 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1436, average loss: 1.7261
[09/15 15:09:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 34.00	top5: 91.50	
[09/15 15:10:20 visual_prompt]: 	Test 100/190. loss: 3.515, 0.2194 s / batch. (data: 3.63e-02)max mem: 17.22456 GB 
[09/15 15:10:38 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1949, average loss: 3.3263
[09/15 15:10:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.00	top5: 58.62	
[09/15 15:10:38 visual_prompt]: Best epoch 92: best metric: 0.340
[09/15 15:10:38 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/15 15:10:47 visual_prompt]: Epoch 93 / 100: avg data time: 9.98e-02, avg batch time: 0.5035, average train loss: 1.7857
[09/15 15:10:50 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1437, average loss: 1.7480
[09/15 15:10:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.00	top5: 89.00	
[09/15 15:11:11 visual_prompt]: 	Test 100/190. loss: 3.732, 0.2386 s / batch. (data: 1.21e-02)max mem: 17.22456 GB 
[09/15 15:11:29 visual_prompt]: Inference (test):avg data time: 7.71e-03, avg batch time: 0.1944, average loss: 3.5267
[09/15 15:11:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.72	top5: 57.89	
[09/15 15:11:29 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/15 15:11:38 visual_prompt]: Epoch 94 / 100: avg data time: 8.34e-02, avg batch time: 0.4871, average train loss: 1.7868
[09/15 15:11:40 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1437, average loss: 1.6944
[09/15 15:11:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 33.00	top5: 92.50	
[09/15 15:12:01 visual_prompt]: 	Test 100/190. loss: 3.636, 0.2156 s / batch. (data: 1.50e-02)max mem: 17.22456 GB 
[09/15 15:12:20 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1956, average loss: 3.4924
[09/15 15:12:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.84	top5: 57.32	
[09/15 15:12:20 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/15 15:12:29 visual_prompt]: Epoch 95 / 100: avg data time: 9.49e-02, avg batch time: 0.4989, average train loss: 1.7272
[09/15 15:12:32 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1442, average loss: 1.6387
[09/15 15:12:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.50	top5: 92.50	
[09/15 15:12:53 visual_prompt]: 	Test 100/190. loss: 3.657, 0.1960 s / batch. (data: 1.26e-02)max mem: 17.22456 GB 
[09/15 15:13:11 visual_prompt]: Inference (test):avg data time: 5.87e-03, avg batch time: 0.1938, average loss: 3.4453
[09/15 15:13:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.45	top5: 59.09	
[09/15 15:13:11 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/15 15:13:20 visual_prompt]: Epoch 96 / 100: avg data time: 8.67e-02, avg batch time: 0.4925, average train loss: 1.6919
[09/15 15:13:22 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1436, average loss: 1.6378
[09/15 15:13:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 37.50	top5: 92.00	
[09/15 15:13:43 visual_prompt]: 	Test 100/190. loss: 3.880, 0.2068 s / batch. (data: 2.33e-02)max mem: 17.22456 GB 
[09/15 15:14:02 visual_prompt]: Inference (test):avg data time: 6.66e-03, avg batch time: 0.1944, average loss: 3.5910
[09/15 15:14:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.31	top5: 58.45	
[09/15 15:14:02 visual_prompt]: Best epoch 96: best metric: 0.375
[09/15 15:14:02 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/15 15:14:11 visual_prompt]: Epoch 97 / 100: avg data time: 9.69e-02, avg batch time: 0.5025, average train loss: 1.6263
[09/15 15:14:14 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1436, average loss: 1.5379
[09/15 15:14:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 36.50	top5: 94.00	
[09/15 15:14:35 visual_prompt]: 	Test 100/190. loss: 3.807, 0.2046 s / batch. (data: 1.28e-04)max mem: 17.22456 GB 
[09/15 15:14:53 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1951, average loss: 3.6030
[09/15 15:14:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.59	top5: 58.76	
[09/15 15:14:53 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/15 15:15:02 visual_prompt]: Epoch 98 / 100: avg data time: 9.14e-02, avg batch time: 0.4964, average train loss: 1.5875
[09/15 15:15:05 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1436, average loss: 1.5260
[09/15 15:15:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 39.50	top5: 96.00	
[09/15 15:15:26 visual_prompt]: 	Test 100/190. loss: 3.841, 0.1886 s / batch. (data: 3.08e-05)max mem: 17.22456 GB 
[09/15 15:15:45 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1949, average loss: 3.6516
[09/15 15:15:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.74	top5: 59.11	
[09/15 15:15:45 visual_prompt]: Best epoch 98: best metric: 0.395
[09/15 15:15:45 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/15 15:15:53 visual_prompt]: Epoch 99 / 100: avg data time: 8.42e-02, avg batch time: 0.4891, average train loss: 1.5688
[09/15 15:15:56 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1436, average loss: 1.5184
[09/15 15:15:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 40.00	top5: 95.50	
[09/15 15:16:17 visual_prompt]: 	Test 100/190. loss: 3.839, 0.1948 s / batch. (data: 1.12e-02)max mem: 17.22456 GB 
[09/15 15:16:35 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1947, average loss: 3.6611
[09/15 15:16:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.86	top5: 58.96	
[09/15 15:16:35 visual_prompt]: Best epoch 99: best metric: 0.400
[09/15 15:16:35 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/15 15:16:44 visual_prompt]: Epoch 100 / 100: avg data time: 9.50e-02, avg batch time: 0.4992, average train loss: 1.5626
[09/15 15:16:47 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1435, average loss: 1.5124
[09/15 15:16:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 41.00	top5: 95.50	
[09/15 15:17:08 visual_prompt]: 	Test 100/190. loss: 3.853, 0.1909 s / batch. (data: 7.52e-03)max mem: 17.22456 GB 
[09/15 15:17:26 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1950, average loss: 3.6669
[09/15 15:17:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.88	top5: 59.12	
[09/15 15:17:26 visual_prompt]: Best epoch 100: best metric: 0.410
[09/15 15:17:46 visual_prompt]: Rank of current process: 0. World size: 1
[09/15 15:17:46 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/15 15:17:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed82'], train_type='')
[09/15 15:17:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/15 15:17:46 visual_prompt]: Training with config:
[09/15 15:17:46 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-smallnorb(predicted_attribute="label_azimuth")',
          'NO_TEST': False,
          'NUMBER_CLASSES': 18,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed82/vtab-smallnorb(predicted_attribute="label_azimuth")/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/15 15:17:46 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-15 15:17:46.507598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-15 15:17:46.720527: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-15 15:17:48.730165: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 15:17:48.730253: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 15:17:48.730262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-15 15:17:53.735013: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 15:17:53.735239: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 15:17:53.735265: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/15 15:17:53 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
2023-09-15 15:17:53.796359: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 15:17:55 visual_prompt]: Number of images: 1000
[09/15 15:17:55 visual_prompt]: Number of classes: 18 / 18
[09/15 15:17:55 visual_prompt]: Loading validation data...
[09/15 15:17:55 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 15:17:56 visual_prompt]: Number of images: 200
[09/15 15:17:56 visual_prompt]: Number of classes: 18 / 18
[09/15 15:17:56 visual_prompt]: Loading test data...
[09/15 15:17:56 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 15:18:12 visual_prompt]: Number of images: 12150
[09/15 15:18:12 visual_prompt]: Number of classes: 18 / 18
[09/15 15:18:12 visual_prompt]: Constructing models...
[09/15 15:18:15 visual_prompt]: Total Parameters: 86734098	 Gradient Parameters: 935442
[09/15 15:18:15 visual_prompt]: tuned percent:1.079
[09/15 15:18:18 visual_prompt]: Device used for model: 0
[09/15 15:18:18 visual_prompt]: Setting up Evalutator...
[09/15 15:18:18 visual_prompt]: Setting up Trainer...
[09/15 15:18:18 visual_prompt]: 	Setting up the optimizer...
[09/15 15:18:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/15 15:18:28 visual_prompt]: Epoch 1 / 100: avg data time: 1.05e-01, avg batch time: 0.5825, average train loss: 3.0224
[09/15 15:18:31 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1430, average loss: 3.0121
[09/15 15:18:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 26.50	
[09/15 15:18:52 visual_prompt]: 	Test 100/190. loss: 3.063, 0.1838 s / batch. (data: 1.58e-04)max mem: 17.22456 GB 
[09/15 15:19:10 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1931, average loss: 3.0171
[09/15 15:19:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.84	top5: 27.07	
[09/15 15:19:10 visual_prompt]: Best epoch 1: best metric: 0.080
[09/15 15:19:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/15 15:19:19 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e-01, avg batch time: 0.5061, average train loss: 3.0539
[09/15 15:19:22 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1437, average loss: 2.9646
[09/15 15:19:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 25.00	
[09/15 15:19:43 visual_prompt]: 	Test 100/190. loss: 2.908, 0.2082 s / batch. (data: 2.57e-02)max mem: 17.22456 GB 
[09/15 15:20:02 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1947, average loss: 2.9505
[09/15 15:20:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 27.64	
[09/15 15:20:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/15 15:20:11 visual_prompt]: Epoch 3 / 100: avg data time: 1.01e-01, avg batch time: 0.5031, average train loss: 3.0158
[09/15 15:20:13 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1434, average loss: 3.0018
[09/15 15:20:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 27.50	
[09/15 15:20:34 visual_prompt]: 	Test 100/190. loss: 2.957, 0.1843 s / batch. (data: 1.12e-04)max mem: 17.22456 GB 
[09/15 15:20:53 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1955, average loss: 2.9929
[09/15 15:20:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.49	top5: 27.28	
[09/15 15:20:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/15 15:21:02 visual_prompt]: Epoch 4 / 100: avg data time: 9.27e-02, avg batch time: 0.4987, average train loss: 3.0791
[09/15 15:21:04 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1467, average loss: 3.0596
[09/15 15:21:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.50	
[09/15 15:21:25 visual_prompt]: 	Test 100/190. loss: 2.932, 0.1840 s / batch. (data: 1.65e-04)max mem: 17.22456 GB 
[09/15 15:21:44 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1950, average loss: 3.1110
[09/15 15:21:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.91	top5: 28.11	
[09/15 15:21:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/15 15:21:53 visual_prompt]: Epoch 5 / 100: avg data time: 9.80e-02, avg batch time: 0.5012, average train loss: 3.1616
[09/15 15:21:56 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1434, average loss: 3.1187
[09/15 15:21:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 31.50	
[09/15 15:22:17 visual_prompt]: 	Test 100/190. loss: 3.306, 0.1839 s / batch. (data: 1.15e-04)max mem: 17.22456 GB 
[09/15 15:22:35 visual_prompt]: Inference (test):avg data time: 6.92e-03, avg batch time: 0.1940, average loss: 3.2242
[09/15 15:22:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.42	top5: 27.97	
[09/15 15:22:35 visual_prompt]: Best epoch 5: best metric: 0.100
[09/15 15:22:35 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/15 15:22:44 visual_prompt]: Epoch 6 / 100: avg data time: 1.06e-01, avg batch time: 0.5098, average train loss: 3.3268
[09/15 15:22:47 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1439, average loss: 3.2184
[09/15 15:22:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 30.00	
[09/15 15:23:08 visual_prompt]: 	Test 100/190. loss: 3.186, 0.1940 s / batch. (data: 1.28e-04)max mem: 17.22456 GB 
[09/15 15:23:27 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1952, average loss: 3.2574
[09/15 15:23:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 27.89	
[09/15 15:23:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/15 15:23:36 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e-01, avg batch time: 0.5071, average train loss: 3.2420
[09/15 15:23:38 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1434, average loss: 3.2261
[09/15 15:23:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 34.00	
[09/15 15:23:59 visual_prompt]: 	Test 100/190. loss: 3.356, 0.1839 s / batch. (data: 9.44e-05)max mem: 17.22456 GB 
[09/15 15:24:18 visual_prompt]: Inference (test):avg data time: 6.23e-03, avg batch time: 0.1936, average loss: 3.3207
[09/15 15:24:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.14	
[09/15 15:24:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/15 15:24:27 visual_prompt]: Epoch 8 / 100: avg data time: 9.53e-02, avg batch time: 0.4982, average train loss: 3.2895
[09/15 15:24:29 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1437, average loss: 3.3373
[09/15 15:24:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 29.00	
[09/15 15:24:50 visual_prompt]: 	Test 100/190. loss: 3.363, 0.1849 s / batch. (data: 1.30e-04)max mem: 17.22456 GB 
[09/15 15:25:09 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1951, average loss: 3.4306
[09/15 15:25:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.48	
[09/15 15:25:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/15 15:25:18 visual_prompt]: Epoch 9 / 100: avg data time: 9.49e-02, avg batch time: 0.4976, average train loss: 3.2909
[09/15 15:25:21 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1435, average loss: 3.0188
[09/15 15:25:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 30.50	
[09/15 15:25:42 visual_prompt]: 	Test 100/190. loss: 2.986, 0.1838 s / batch. (data: 1.46e-04)max mem: 17.22456 GB 
[09/15 15:26:00 visual_prompt]: Inference (test):avg data time: 8.40e-03, avg batch time: 0.1945, average loss: 3.0559
[09/15 15:26:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.53	top5: 28.90	
[09/15 15:26:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/15 15:26:09 visual_prompt]: Epoch 10 / 100: avg data time: 8.26e-02, avg batch time: 0.4874, average train loss: 3.4937
[09/15 15:26:12 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1438, average loss: 3.3460
[09/15 15:26:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 32.00	
[09/15 15:26:33 visual_prompt]: 	Test 100/190. loss: 3.301, 0.2224 s / batch. (data: 3.92e-02)max mem: 17.22456 GB 
[09/15 15:26:51 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1941, average loss: 3.4255
[09/15 15:26:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.60	top5: 27.56	
[09/15 15:26:51 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/15 15:27:00 visual_prompt]: Epoch 11 / 100: avg data time: 9.33e-02, avg batch time: 0.4997, average train loss: 3.4041
[09/15 15:27:03 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1437, average loss: 3.3443
[09/15 15:27:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 36.00	
[09/15 15:27:24 visual_prompt]: 	Test 100/190. loss: 3.579, 0.1964 s / batch. (data: 1.15e-04)max mem: 17.22456 GB 
[09/15 15:27:43 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1946, average loss: 3.4686
[09/15 15:27:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 29.88	
[09/15 15:27:43 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/15 15:27:52 visual_prompt]: Epoch 12 / 100: avg data time: 9.75e-02, avg batch time: 0.5012, average train loss: 3.3008
[09/15 15:27:54 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1438, average loss: 3.3132
[09/15 15:27:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 34.00	
[09/15 15:28:15 visual_prompt]: 	Test 100/190. loss: 3.140, 0.1962 s / batch. (data: 1.25e-02)max mem: 17.22456 GB 
[09/15 15:28:34 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1942, average loss: 3.3397
[09/15 15:28:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.42	top5: 28.86	
[09/15 15:28:34 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/15 15:28:43 visual_prompt]: Epoch 13 / 100: avg data time: 1.00e-01, avg batch time: 0.5039, average train loss: 3.2647
[09/15 15:28:46 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1437, average loss: 3.8451
[09/15 15:28:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 3.00	top5: 30.50	
[09/15 15:29:07 visual_prompt]: 	Test 100/190. loss: 3.755, 0.2056 s / batch. (data: 1.23e-02)max mem: 17.22456 GB 
[09/15 15:29:26 visual_prompt]: Inference (test):avg data time: 9.13e-03, avg batch time: 0.1957, average loss: 3.7276
[09/15 15:29:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.43	top5: 28.31	
[09/15 15:29:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/15 15:29:35 visual_prompt]: Epoch 14 / 100: avg data time: 1.06e-01, avg batch time: 0.5098, average train loss: 5.5057
[09/15 15:29:37 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1437, average loss: 5.5170
[09/15 15:29:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 31.50	
[09/15 15:29:58 visual_prompt]: 	Test 100/190. loss: 5.146, 0.1986 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 15:30:17 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1938, average loss: 5.9765
[09/15 15:30:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.67	top5: 28.00	
[09/15 15:30:17 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/15 15:30:25 visual_prompt]: Epoch 15 / 100: avg data time: 9.10e-02, avg batch time: 0.4948, average train loss: 9.9969
[09/15 15:30:28 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1441, average loss: 9.0470
[09/15 15:30:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 32.50	
[09/15 15:30:49 visual_prompt]: 	Test 100/190. loss: 8.689, 0.2260 s / batch. (data: 1.36e-02)max mem: 17.22456 GB 
[09/15 15:31:08 visual_prompt]: Inference (test):avg data time: 6.78e-03, avg batch time: 0.1933, average loss: 9.7732
[09/15 15:31:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.67	top5: 27.48	
[09/15 15:31:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/15 15:31:16 visual_prompt]: Epoch 16 / 100: avg data time: 9.57e-02, avg batch time: 0.4996, average train loss: 10.4251
[09/15 15:31:19 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1437, average loss: 12.8450
[09/15 15:31:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.00	
[09/15 15:31:40 visual_prompt]: 	Test 100/190. loss: 9.359, 0.1847 s / batch. (data: 1.47e-04)max mem: 17.22456 GB 
[09/15 15:31:59 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1945, average loss: 13.0044
[09/15 15:31:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.53	top5: 27.93	
[09/15 15:31:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/15 15:32:08 visual_prompt]: Epoch 17 / 100: avg data time: 8.51e-02, avg batch time: 0.4903, average train loss: 12.9094
[09/15 15:32:10 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1441, average loss: 12.0601
[09/15 15:32:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 22.50	
[09/15 15:32:31 visual_prompt]: 	Test 100/190. loss: 10.427, 0.1838 s / batch. (data: 1.28e-04)max mem: 17.22456 GB 
[09/15 15:32:50 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1950, average loss: 12.3601
[09/15 15:32:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.53	top5: 27.89	
[09/15 15:32:50 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/15 15:32:59 visual_prompt]: Epoch 18 / 100: avg data time: 9.37e-02, avg batch time: 0.4971, average train loss: 8.2259
[09/15 15:33:02 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1440, average loss: 5.1841
[09/15 15:33:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 31.50	
[09/15 15:33:23 visual_prompt]: 	Test 100/190. loss: 5.440, 0.1993 s / batch. (data: 1.49e-04)max mem: 17.22456 GB 
[09/15 15:33:41 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1946, average loss: 5.3993
[09/15 15:33:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.82	
[09/15 15:33:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/15 15:33:50 visual_prompt]: Epoch 19 / 100: avg data time: 8.34e-02, avg batch time: 0.4921, average train loss: 4.3349
[09/15 15:33:53 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1437, average loss: 3.8847
[09/15 15:33:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 26.50	
[09/15 15:34:14 visual_prompt]: 	Test 100/190. loss: 3.448, 0.1978 s / batch. (data: 1.45e-02)max mem: 17.22456 GB 
[09/15 15:34:33 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1951, average loss: 3.8857
[09/15 15:34:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 27.91	
[09/15 15:34:33 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/15 15:34:42 visual_prompt]: Epoch 20 / 100: avg data time: 1.01e-01, avg batch time: 0.5064, average train loss: 3.6652
[09/15 15:34:44 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1436, average loss: 3.5395
[09/15 15:34:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 30.50	
[09/15 15:35:05 visual_prompt]: 	Test 100/190. loss: 3.628, 0.1985 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 15:35:24 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1945, average loss: 3.7408
[09/15 15:35:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.62	top5: 27.56	
[09/15 15:35:24 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/15 15:35:33 visual_prompt]: Epoch 21 / 100: avg data time: 9.17e-02, avg batch time: 0.4972, average train loss: 3.3123
[09/15 15:35:36 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1438, average loss: 3.3626
[09/15 15:35:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 26.50	
[09/15 15:35:57 visual_prompt]: 	Test 100/190. loss: 3.304, 0.2297 s / batch. (data: 4.69e-02)max mem: 17.22456 GB 
[09/15 15:36:15 visual_prompt]: Inference (test):avg data time: 6.54e-03, avg batch time: 0.1934, average loss: 3.3430
[09/15 15:36:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.53	top5: 28.26	
[09/15 15:36:15 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/15 15:36:24 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e-01, avg batch time: 0.5109, average train loss: 3.4667
[09/15 15:36:27 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1437, average loss: 3.6310
[09/15 15:36:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 28.50	
[09/15 15:36:48 visual_prompt]: 	Test 100/190. loss: 3.867, 0.1834 s / batch. (data: 1.31e-04)max mem: 17.22456 GB 
[09/15 15:37:07 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1946, average loss: 3.6856
[09/15 15:37:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.78	
[09/15 15:37:07 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/15 15:37:16 visual_prompt]: Epoch 23 / 100: avg data time: 1.04e-01, avg batch time: 0.5093, average train loss: 3.2649
[09/15 15:37:19 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1440, average loss: 3.2744
[09/15 15:37:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 27.50	
[09/15 15:37:40 visual_prompt]: 	Test 100/190. loss: 3.240, 0.2096 s / batch. (data: 1.43e-02)max mem: 17.22456 GB 
[09/15 15:37:58 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1945, average loss: 3.3545
[09/15 15:37:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.97	
[09/15 15:37:58 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/15 15:38:07 visual_prompt]: Epoch 24 / 100: avg data time: 1.06e-01, avg batch time: 0.5104, average train loss: 3.1567
[09/15 15:38:10 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1439, average loss: 2.9783
[09/15 15:38:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 30.00	
[09/15 15:38:31 visual_prompt]: 	Test 100/190. loss: 2.956, 0.1842 s / batch. (data: 9.75e-05)max mem: 17.22456 GB 
[09/15 15:38:50 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1945, average loss: 3.0216
[09/15 15:38:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.98	
[09/15 15:38:50 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/15 15:38:59 visual_prompt]: Epoch 25 / 100: avg data time: 8.91e-02, avg batch time: 0.4934, average train loss: 3.1650
[09/15 15:39:01 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1438, average loss: 3.1072
[09/15 15:39:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 29.00	
[09/15 15:39:22 visual_prompt]: 	Test 100/190. loss: 3.158, 0.2205 s / batch. (data: 3.76e-02)max mem: 17.22456 GB 
[09/15 15:39:41 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1956, average loss: 3.1252
[09/15 15:39:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.43	top5: 27.60	
[09/15 15:39:41 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/15 15:39:50 visual_prompt]: Epoch 26 / 100: avg data time: 9.65e-02, avg batch time: 0.5013, average train loss: 3.1495
[09/15 15:39:53 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1437, average loss: 3.1441
[09/15 15:39:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 30.50	
[09/15 15:40:14 visual_prompt]: 	Test 100/190. loss: 3.353, 0.1994 s / batch. (data: 1.55e-02)max mem: 17.22456 GB 
[09/15 15:40:33 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1949, average loss: 3.2274
[09/15 15:40:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.29	top5: 27.09	
[09/15 15:40:33 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/15 15:40:41 visual_prompt]: Epoch 27 / 100: avg data time: 8.70e-02, avg batch time: 0.4939, average train loss: 3.1813
[09/15 15:40:44 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1440, average loss: 3.3018
[09/15 15:40:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 3.00	top5: 27.00	
[09/15 15:41:05 visual_prompt]: 	Test 100/190. loss: 3.150, 0.1838 s / batch. (data: 1.50e-04)max mem: 17.22456 GB 
[09/15 15:41:24 visual_prompt]: Inference (test):avg data time: 7.10e-03, avg batch time: 0.1952, average loss: 3.3338
[09/15 15:41:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.43	top5: 27.85	
[09/15 15:41:24 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/15 15:41:32 visual_prompt]: Epoch 28 / 100: avg data time: 8.93e-02, avg batch time: 0.4955, average train loss: 3.1625
[09/15 15:41:35 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1439, average loss: 3.2689
[09/15 15:41:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 28.50	
[09/15 15:41:56 visual_prompt]: 	Test 100/190. loss: 3.474, 0.1842 s / batch. (data: 1.16e-04)max mem: 17.22456 GB 
[09/15 15:42:15 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1951, average loss: 3.3428
[09/15 15:42:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 28.06	
[09/15 15:42:15 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/15 15:42:24 visual_prompt]: Epoch 29 / 100: avg data time: 1.05e-01, avg batch time: 0.5085, average train loss: 3.2629
[09/15 15:42:27 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1437, average loss: 3.3145
[09/15 15:42:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 31.00	
[09/15 15:42:48 visual_prompt]: 	Test 100/190. loss: 3.273, 0.1844 s / batch. (data: 1.53e-04)max mem: 17.22456 GB 
[09/15 15:43:06 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1954, average loss: 3.3272
[09/15 15:43:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.52	top5: 27.61	
[09/15 15:43:07 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/15 15:43:15 visual_prompt]: Epoch 30 / 100: avg data time: 9.38e-02, avg batch time: 0.4988, average train loss: 3.2042
[09/15 15:43:18 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1438, average loss: 3.1787
[09/15 15:43:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 32.00	
[09/15 15:43:39 visual_prompt]: 	Test 100/190. loss: 3.215, 0.1975 s / batch. (data: 1.21e-04)max mem: 17.22456 GB 
[09/15 15:43:58 visual_prompt]: Inference (test):avg data time: 6.61e-03, avg batch time: 0.1941, average loss: 3.2611
[09/15 15:43:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.58	
[09/15 15:43:58 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/15 15:44:06 visual_prompt]: Epoch 31 / 100: avg data time: 7.76e-02, avg batch time: 0.4861, average train loss: 3.0953
[09/15 15:44:09 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1438, average loss: 3.0275
[09/15 15:44:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 32.50	
[09/15 15:44:30 visual_prompt]: 	Test 100/190. loss: 3.164, 0.1973 s / batch. (data: 1.41e-02)max mem: 17.22456 GB 
[09/15 15:44:49 visual_prompt]: Inference (test):avg data time: 8.87e-03, avg batch time: 0.1959, average loss: 3.0885
[09/15 15:44:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.14	
[09/15 15:44:49 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/15 15:44:58 visual_prompt]: Epoch 32 / 100: avg data time: 1.05e-01, avg batch time: 0.5097, average train loss: 3.0959
[09/15 15:45:01 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1437, average loss: 3.0501
[09/15 15:45:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 24.00	
[09/15 15:45:22 visual_prompt]: 	Test 100/190. loss: 2.863, 0.2218 s / batch. (data: 3.87e-02)max mem: 17.22456 GB 
[09/15 15:45:40 visual_prompt]: Inference (test):avg data time: 8.50e-03, avg batch time: 0.1953, average loss: 3.0831
[09/15 15:45:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 27.79	
[09/15 15:45:40 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/15 15:45:49 visual_prompt]: Epoch 33 / 100: avg data time: 9.22e-02, avg batch time: 0.4980, average train loss: 3.1291
[09/15 15:45:52 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1440, average loss: 3.2745
[09/15 15:45:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 25.50	
[09/15 15:46:13 visual_prompt]: 	Test 100/190. loss: 3.412, 0.1921 s / batch. (data: 9.08e-05)max mem: 17.22456 GB 
[09/15 15:46:31 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1937, average loss: 3.2309
[09/15 15:46:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.33	top5: 28.16	
[09/15 15:46:32 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/15 15:46:40 visual_prompt]: Epoch 34 / 100: avg data time: 8.73e-02, avg batch time: 0.4924, average train loss: 3.1336
[09/15 15:46:43 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1441, average loss: 3.1832
[09/15 15:46:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.00	
[09/15 15:47:04 visual_prompt]: 	Test 100/190. loss: 3.102, 0.1847 s / batch. (data: 1.42e-04)max mem: 17.22456 GB 
[09/15 15:47:22 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1935, average loss: 3.2372
[09/15 15:47:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.67	top5: 27.96	
[09/15 15:47:22 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/15 15:47:31 visual_prompt]: Epoch 35 / 100: avg data time: 1.03e-01, avg batch time: 0.5066, average train loss: 3.1230
[09/15 15:47:34 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1439, average loss: 3.0138
[09/15 15:47:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 31.50	
[09/15 15:47:55 visual_prompt]: 	Test 100/190. loss: 3.105, 0.1848 s / batch. (data: 4.72e-05)max mem: 17.22456 GB 
[09/15 15:48:14 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1946, average loss: 3.0846
[09/15 15:48:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.49	top5: 27.94	
[09/15 15:48:14 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/15 15:48:23 visual_prompt]: Epoch 36 / 100: avg data time: 9.59e-02, avg batch time: 0.5013, average train loss: 3.2134
[09/15 15:48:25 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1438, average loss: 3.2783
[09/15 15:48:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 25.00	
[09/15 15:48:46 visual_prompt]: 	Test 100/190. loss: 3.296, 0.1965 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 15:49:05 visual_prompt]: Inference (test):avg data time: 6.77e-03, avg batch time: 0.1948, average loss: 3.2691
[09/15 15:49:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.65	top5: 27.93	
[09/15 15:49:05 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/15 15:49:14 visual_prompt]: Epoch 37 / 100: avg data time: 9.58e-02, avg batch time: 0.5006, average train loss: 3.2197
[09/15 15:49:16 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1437, average loss: 3.2356
[09/15 15:49:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 31.50	
[09/15 15:49:38 visual_prompt]: 	Test 100/190. loss: 3.302, 0.1844 s / batch. (data: 1.18e-04)max mem: 17.22456 GB 
[09/15 15:49:56 visual_prompt]: Inference (test):avg data time: 8.03e-03, avg batch time: 0.1954, average loss: 3.3254
[09/15 15:49:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 27.83	
[09/15 15:49:56 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/15 15:50:05 visual_prompt]: Epoch 38 / 100: avg data time: 1.07e-01, avg batch time: 0.5105, average train loss: 3.2223
[09/15 15:50:08 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1438, average loss: 3.3321
[09/15 15:50:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 30.00	
[09/15 15:50:29 visual_prompt]: 	Test 100/190. loss: 3.461, 0.1849 s / batch. (data: 1.49e-04)max mem: 17.22456 GB 
[09/15 15:50:48 visual_prompt]: Inference (test):avg data time: 9.04e-03, avg batch time: 0.1955, average loss: 3.4148
[09/15 15:50:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.49	top5: 27.74	
[09/15 15:50:48 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/15 15:50:57 visual_prompt]: Epoch 39 / 100: avg data time: 1.03e-01, avg batch time: 0.5083, average train loss: 3.2944
[09/15 15:51:00 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1437, average loss: 3.3674
[09/15 15:51:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 22.50	
[09/15 15:51:21 visual_prompt]: 	Test 100/190. loss: 3.264, 0.1895 s / batch. (data: 1.30e-04)max mem: 17.22456 GB 
[09/15 15:51:39 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1942, average loss: 3.2920
[09/15 15:51:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.80	top5: 28.07	
[09/15 15:51:40 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/15 15:51:48 visual_prompt]: Epoch 40 / 100: avg data time: 1.01e-01, avg batch time: 0.5047, average train loss: 3.1008
[09/15 15:51:51 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1437, average loss: 3.3136
[09/15 15:51:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 3.50	top5: 27.50	
[09/15 15:52:12 visual_prompt]: 	Test 100/190. loss: 3.325, 0.1845 s / batch. (data: 1.50e-04)max mem: 17.22456 GB 
[09/15 15:52:31 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1950, average loss: 3.3010
[09/15 15:52:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.60	top5: 27.66	
[09/15 15:52:31 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/15 15:52:40 visual_prompt]: Epoch 41 / 100: avg data time: 9.52e-02, avg batch time: 0.5002, average train loss: 3.1241
[09/15 15:52:43 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1439, average loss: 3.1647
[09/15 15:52:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 26.50	
[09/15 15:53:04 visual_prompt]: 	Test 100/190. loss: 3.046, 0.1967 s / batch. (data: 1.29e-04)max mem: 17.22456 GB 
[09/15 15:53:22 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1947, average loss: 3.1397
[09/15 15:53:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.18	top5: 28.08	
[09/15 15:53:22 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/15 15:53:31 visual_prompt]: Epoch 42 / 100: avg data time: 1.01e-01, avg batch time: 0.5052, average train loss: 3.1485
[09/15 15:53:34 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1437, average loss: 3.0529
[09/15 15:53:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 36.00	
[09/15 15:53:55 visual_prompt]: 	Test 100/190. loss: 3.047, 0.1978 s / batch. (data: 1.45e-02)max mem: 17.22456 GB 
[09/15 15:54:14 visual_prompt]: Inference (test):avg data time: 8.32e-03, avg batch time: 0.1951, average loss: 3.1233
[09/15 15:54:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.09	top5: 29.28	
[09/15 15:54:14 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/15 15:54:22 visual_prompt]: Epoch 43 / 100: avg data time: 9.23e-02, avg batch time: 0.4963, average train loss: 3.0993
[09/15 15:54:25 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1437, average loss: 3.1478
[09/15 15:54:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.50	
[09/15 15:54:46 visual_prompt]: 	Test 100/190. loss: 3.221, 0.1960 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 15:55:05 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1941, average loss: 3.1517
[09/15 15:55:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.44	top5: 32.81	
[09/15 15:55:05 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/15 15:55:14 visual_prompt]: Epoch 44 / 100: avg data time: 1.02e-01, avg batch time: 0.5066, average train loss: 3.0445
[09/15 15:55:17 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1441, average loss: 3.0945
[09/15 15:55:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 29.00	
[09/15 15:55:38 visual_prompt]: 	Test 100/190. loss: 3.204, 0.1966 s / batch. (data: 1.30e-02)max mem: 17.22456 GB 
[09/15 15:55:56 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1948, average loss: 3.1584
[09/15 15:55:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 27.77	
[09/15 15:55:56 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/15 15:56:05 visual_prompt]: Epoch 45 / 100: avg data time: 9.80e-02, avg batch time: 0.5030, average train loss: 3.0691
[09/15 15:56:08 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1437, average loss: 3.0426
[09/15 15:56:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 27.50	
[09/15 15:56:29 visual_prompt]: 	Test 100/190. loss: 2.886, 0.2009 s / batch. (data: 3.60e-05)max mem: 17.22456 GB 
[09/15 15:56:48 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1944, average loss: 3.0462
[09/15 15:56:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.54	top5: 29.07	
[09/15 15:56:48 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/15 15:56:56 visual_prompt]: Epoch 46 / 100: avg data time: 8.57e-02, avg batch time: 0.4922, average train loss: 2.9922
[09/15 15:56:59 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1438, average loss: 2.9898
[09/15 15:56:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 32.00	
[09/15 15:57:20 visual_prompt]: 	Test 100/190. loss: 3.076, 0.1966 s / batch. (data: 1.33e-02)max mem: 17.22456 GB 
[09/15 15:57:39 visual_prompt]: Inference (test):avg data time: 8.23e-03, avg batch time: 0.1951, average loss: 3.0844
[09/15 15:57:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.74	top5: 28.12	
[09/15 15:57:39 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/15 15:57:48 visual_prompt]: Epoch 47 / 100: avg data time: 9.73e-02, avg batch time: 0.5006, average train loss: 2.9950
[09/15 15:57:50 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1440, average loss: 2.9779
[09/15 15:57:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 30.00	
[09/15 15:58:12 visual_prompt]: 	Test 100/190. loss: 3.085, 0.1994 s / batch. (data: 1.57e-02)max mem: 17.22456 GB 
[09/15 15:58:30 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1947, average loss: 2.9981
[09/15 15:58:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.30	top5: 29.76	
[09/15 15:58:30 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/15 15:58:39 visual_prompt]: Epoch 48 / 100: avg data time: 9.33e-02, avg batch time: 0.4979, average train loss: 3.0365
[09/15 15:58:42 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1438, average loss: 2.9443
[09/15 15:58:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 32.00	
[09/15 15:59:03 visual_prompt]: 	Test 100/190. loss: 2.869, 0.1840 s / batch. (data: 1.43e-04)max mem: 17.22456 GB 
[09/15 15:59:22 visual_prompt]: Inference (test):avg data time: 9.12e-03, avg batch time: 0.1960, average loss: 2.9639
[09/15 15:59:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.57	top5: 34.71	
[09/15 15:59:22 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/15 15:59:31 visual_prompt]: Epoch 49 / 100: avg data time: 1.02e-01, avg batch time: 0.5065, average train loss: 3.0307
[09/15 15:59:33 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1439, average loss: 2.8109
[09/15 15:59:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.00	top5: 39.50	
[09/15 15:59:54 visual_prompt]: 	Test 100/190. loss: 2.891, 0.1943 s / batch. (data: 1.11e-02)max mem: 17.22456 GB 
[09/15 16:00:13 visual_prompt]: Inference (test):avg data time: 7.06e-03, avg batch time: 0.1944, average loss: 2.8951
[09/15 16:00:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.80	top5: 34.11	
[09/15 16:00:13 visual_prompt]: Best epoch 49: best metric: 0.110
[09/15 16:00:13 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/15 16:00:22 visual_prompt]: Epoch 50 / 100: avg data time: 8.90e-02, avg batch time: 0.4920, average train loss: 2.9638
[09/15 16:00:25 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1438, average loss: 3.0466
[09/15 16:00:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 35.50	
[09/15 16:00:46 visual_prompt]: 	Test 100/190. loss: 3.120, 0.1985 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 16:01:04 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1939, average loss: 3.0614
[09/15 16:01:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 30.91	
[09/15 16:01:04 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/15 16:01:13 visual_prompt]: Epoch 51 / 100: avg data time: 8.57e-02, avg batch time: 0.4927, average train loss: 2.9810
[09/15 16:01:16 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1438, average loss: 2.8834
[09/15 16:01:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 31.50	
[09/15 16:01:37 visual_prompt]: 	Test 100/190. loss: 3.074, 0.1970 s / batch. (data: 1.35e-02)max mem: 17.22456 GB 
[09/15 16:01:55 visual_prompt]: Inference (test):avg data time: 8.48e-03, avg batch time: 0.1947, average loss: 2.9465
[09/15 16:01:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.02	top5: 33.39	
[09/15 16:01:55 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/15 16:02:04 visual_prompt]: Epoch 52 / 100: avg data time: 1.01e-01, avg batch time: 0.5058, average train loss: 2.9415
[09/15 16:02:07 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.1440, average loss: 2.8436
[09/15 16:02:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 38.50	
[09/15 16:02:28 visual_prompt]: 	Test 100/190. loss: 2.989, 0.1955 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 16:02:47 visual_prompt]: Inference (test):avg data time: 7.12e-03, avg batch time: 0.1945, average loss: 2.9081
[09/15 16:02:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.65	top5: 34.78	
[09/15 16:02:47 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/15 16:02:56 visual_prompt]: Epoch 53 / 100: avg data time: 9.86e-02, avg batch time: 0.5040, average train loss: 2.9416
[09/15 16:02:58 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1438, average loss: 3.0279
[09/15 16:02:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 34.50	
[09/15 16:03:19 visual_prompt]: 	Test 100/190. loss: 3.002, 0.2100 s / batch. (data: 1.32e-02)max mem: 17.22456 GB 
[09/15 16:03:38 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1943, average loss: 3.0966
[09/15 16:03:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.68	top5: 33.40	
[09/15 16:03:38 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/15 16:03:47 visual_prompt]: Epoch 54 / 100: avg data time: 1.04e-01, avg batch time: 0.5080, average train loss: 2.9626
[09/15 16:03:50 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1439, average loss: 2.9252
[09/15 16:03:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 38.50	
[09/15 16:04:11 visual_prompt]: 	Test 100/190. loss: 2.975, 0.1961 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 16:04:29 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1942, average loss: 3.0621
[09/15 16:04:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.87	top5: 33.83	
[09/15 16:04:29 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/15 16:04:38 visual_prompt]: Epoch 55 / 100: avg data time: 9.25e-02, avg batch time: 0.4978, average train loss: 2.8865
[09/15 16:04:41 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1437, average loss: 2.9965
[09/15 16:04:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 35.50	
[09/15 16:05:02 visual_prompt]: 	Test 100/190. loss: 2.875, 0.1840 s / batch. (data: 1.61e-04)max mem: 17.22456 GB 
[09/15 16:05:21 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1943, average loss: 2.9686
[09/15 16:05:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.49	top5: 36.97	
[09/15 16:05:21 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/15 16:05:30 visual_prompt]: Epoch 56 / 100: avg data time: 9.47e-02, avg batch time: 0.4982, average train loss: 2.9249
[09/15 16:05:32 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1439, average loss: 3.0023
[09/15 16:05:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.50	top5: 35.00	
[09/15 16:05:54 visual_prompt]: 	Test 100/190. loss: 2.899, 0.2076 s / batch. (data: 2.43e-02)max mem: 17.22456 GB 
[09/15 16:06:12 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1954, average loss: 3.0153
[09/15 16:06:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.28	top5: 38.16	
[09/15 16:06:12 visual_prompt]: Best epoch 56: best metric: 0.115
[09/15 16:06:12 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/15 16:06:21 visual_prompt]: Epoch 57 / 100: avg data time: 8.19e-02, avg batch time: 0.4901, average train loss: 2.9244
[09/15 16:06:24 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.1438, average loss: 2.7203
[09/15 16:06:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.00	top5: 45.00	
[09/15 16:06:45 visual_prompt]: 	Test 100/190. loss: 2.813, 0.1879 s / batch. (data: 1.37e-04)max mem: 17.22456 GB 
[09/15 16:07:03 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1946, average loss: 2.8681
[09/15 16:07:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.94	top5: 40.58	
[09/15 16:07:03 visual_prompt]: Best epoch 57: best metric: 0.140
[09/15 16:07:03 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/15 16:07:12 visual_prompt]: Epoch 58 / 100: avg data time: 9.55e-02, avg batch time: 0.5009, average train loss: 2.8461
[09/15 16:07:15 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1436, average loss: 2.6815
[09/15 16:07:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.50	top5: 43.00	
[09/15 16:07:36 visual_prompt]: 	Test 100/190. loss: 2.674, 0.1851 s / batch. (data: 1.48e-04)max mem: 17.22456 GB 
[09/15 16:07:54 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1942, average loss: 2.7952
[09/15 16:07:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 41.19	
[09/15 16:07:55 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/15 16:08:03 visual_prompt]: Epoch 59 / 100: avg data time: 8.90e-02, avg batch time: 0.4951, average train loss: 2.7712
[09/15 16:08:06 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1437, average loss: 2.7712
[09/15 16:08:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 46.00	
[09/15 16:08:27 visual_prompt]: 	Test 100/190. loss: 2.848, 0.1927 s / batch. (data: 1.55e-04)max mem: 17.22456 GB 
[09/15 16:08:46 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1952, average loss: 2.8928
[09/15 16:08:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.89	top5: 42.36	
[09/15 16:08:46 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/15 16:08:55 visual_prompt]: Epoch 60 / 100: avg data time: 9.68e-02, avg batch time: 0.5009, average train loss: 2.8253
[09/15 16:08:57 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1441, average loss: 2.8233
[09/15 16:08:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.50	top5: 47.00	
[09/15 16:09:19 visual_prompt]: 	Test 100/190. loss: 2.881, 0.1960 s / batch. (data: 1.21e-02)max mem: 17.22456 GB 
[09/15 16:09:37 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1950, average loss: 2.9299
[09/15 16:09:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.79	top5: 38.80	
[09/15 16:09:37 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/15 16:09:46 visual_prompt]: Epoch 61 / 100: avg data time: 9.79e-02, avg batch time: 0.5018, average train loss: 2.7629
[09/15 16:09:49 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1440, average loss: 2.7708
[09/15 16:09:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 43.00	
[09/15 16:10:10 visual_prompt]: 	Test 100/190. loss: 2.877, 0.1990 s / batch. (data: 1.55e-02)max mem: 17.22456 GB 
[09/15 16:10:28 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1947, average loss: 2.8779
[09/15 16:10:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 39.05	
[09/15 16:10:29 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/15 16:10:37 visual_prompt]: Epoch 62 / 100: avg data time: 9.88e-02, avg batch time: 0.5039, average train loss: 2.7648
[09/15 16:10:40 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1437, average loss: 2.8068
[09/15 16:10:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 43.00	
[09/15 16:11:01 visual_prompt]: 	Test 100/190. loss: 2.896, 0.1843 s / batch. (data: 1.48e-04)max mem: 17.22456 GB 
[09/15 16:11:20 visual_prompt]: Inference (test):avg data time: 9.13e-03, avg batch time: 0.1953, average loss: 2.9021
[09/15 16:11:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.79	top5: 42.11	
[09/15 16:11:20 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/15 16:11:29 visual_prompt]: Epoch 63 / 100: avg data time: 8.22e-02, avg batch time: 0.4907, average train loss: 2.7366
[09/15 16:11:32 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1438, average loss: 2.6498
[09/15 16:11:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.50	top5: 45.50	
[09/15 16:11:53 visual_prompt]: 	Test 100/190. loss: 2.776, 0.1896 s / batch. (data: 1.17e-04)max mem: 17.22456 GB 
[09/15 16:12:11 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1938, average loss: 2.8213
[09/15 16:12:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.31	top5: 41.93	
[09/15 16:12:11 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/15 16:12:20 visual_prompt]: Epoch 64 / 100: avg data time: 8.92e-02, avg batch time: 0.4946, average train loss: 2.7037
[09/15 16:12:23 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1439, average loss: 2.6827
[09/15 16:12:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.50	top5: 49.50	
[09/15 16:12:44 visual_prompt]: 	Test 100/190. loss: 2.887, 0.1853 s / batch. (data: 1.49e-04)max mem: 17.22456 GB 
[09/15 16:13:02 visual_prompt]: Inference (test):avg data time: 6.54e-03, avg batch time: 0.1938, average loss: 2.9063
[09/15 16:13:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.74	top5: 39.23	
[09/15 16:13:02 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/15 16:13:11 visual_prompt]: Epoch 65 / 100: avg data time: 9.27e-02, avg batch time: 0.4962, average train loss: 2.6725
[09/15 16:13:14 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1436, average loss: 2.7526
[09/15 16:13:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 45.00	
[09/15 16:13:35 visual_prompt]: 	Test 100/190. loss: 2.931, 0.1984 s / batch. (data: 1.49e-02)max mem: 17.22456 GB 
[09/15 16:13:53 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1933, average loss: 2.9523
[09/15 16:13:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 38.35	
[09/15 16:13:53 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/15 16:14:02 visual_prompt]: Epoch 66 / 100: avg data time: 8.01e-02, avg batch time: 0.4850, average train loss: 2.6855
[09/15 16:14:05 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1437, average loss: 2.6546
[09/15 16:14:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.50	top5: 51.50	
[09/15 16:14:26 visual_prompt]: 	Test 100/190. loss: 2.830, 0.1846 s / batch. (data: 1.29e-04)max mem: 17.22456 GB 
[09/15 16:14:45 visual_prompt]: Inference (test):avg data time: 9.01e-03, avg batch time: 0.1958, average loss: 2.8394
[09/15 16:14:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.33	top5: 42.80	
[09/15 16:14:45 visual_prompt]: Best epoch 66: best metric: 0.155
[09/15 16:14:45 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/15 16:14:54 visual_prompt]: Epoch 67 / 100: avg data time: 9.29e-02, avg batch time: 0.5000, average train loss: 2.6852
[09/15 16:14:56 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1443, average loss: 2.5555
[09/15 16:14:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 54.50	
[09/15 16:15:17 visual_prompt]: 	Test 100/190. loss: 2.848, 0.1983 s / batch. (data: 1.48e-02)max mem: 17.22456 GB 
[09/15 16:15:36 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1943, average loss: 2.8277
[09/15 16:15:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.31	top5: 45.50	
[09/15 16:15:36 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/15 16:15:45 visual_prompt]: Epoch 68 / 100: avg data time: 9.77e-02, avg batch time: 0.5009, average train loss: 2.5493
[09/15 16:15:48 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1438, average loss: 2.5041
[09/15 16:15:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.00	top5: 56.50	
[09/15 16:16:09 visual_prompt]: 	Test 100/190. loss: 2.800, 0.1991 s / batch. (data: 1.45e-02)max mem: 17.22456 GB 
[09/15 16:16:27 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1941, average loss: 2.8282
[09/15 16:16:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.36	top5: 42.99	
[09/15 16:16:27 visual_prompt]: Best epoch 68: best metric: 0.170
[09/15 16:16:27 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/15 16:16:36 visual_prompt]: Epoch 69 / 100: avg data time: 1.03e-01, avg batch time: 0.5115, average train loss: 2.5260
[09/15 16:16:39 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1438, average loss: 2.5218
[09/15 16:16:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.00	top5: 59.50	
[09/15 16:17:00 visual_prompt]: 	Test 100/190. loss: 2.855, 0.2055 s / batch. (data: 2.27e-02)max mem: 17.22456 GB 
[09/15 16:17:18 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1947, average loss: 2.8391
[09/15 16:17:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.51	top5: 44.71	
[09/15 16:17:18 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/15 16:17:27 visual_prompt]: Epoch 70 / 100: avg data time: 9.83e-02, avg batch time: 0.5031, average train loss: 2.5642
[09/15 16:17:30 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1439, average loss: 2.4735
[09/15 16:17:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.50	top5: 57.50	
[09/15 16:17:51 visual_prompt]: 	Test 100/190. loss: 2.739, 0.1846 s / batch. (data: 1.67e-04)max mem: 17.22456 GB 
[09/15 16:18:10 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1949, average loss: 2.8109
[09/15 16:18:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.94	top5: 44.46	
[09/15 16:18:10 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/15 16:18:18 visual_prompt]: Epoch 71 / 100: avg data time: 1.00e-01, avg batch time: 0.5045, average train loss: 2.5366
[09/15 16:18:21 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1438, average loss: 2.4934
[09/15 16:18:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.50	top5: 65.00	
[09/15 16:18:42 visual_prompt]: 	Test 100/190. loss: 2.788, 0.2197 s / batch. (data: 3.63e-02)max mem: 17.22456 GB 
[09/15 16:19:01 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1942, average loss: 2.8062
[09/15 16:19:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.08	top5: 45.13	
[09/15 16:19:01 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/15 16:19:10 visual_prompt]: Epoch 72 / 100: avg data time: 8.68e-02, avg batch time: 0.4908, average train loss: 2.5039
[09/15 16:19:12 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1444, average loss: 2.5494
[09/15 16:19:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.00	top5: 58.50	
[09/15 16:19:33 visual_prompt]: 	Test 100/190. loss: 2.798, 0.1960 s / batch. (data: 1.29e-02)max mem: 17.22456 GB 
[09/15 16:19:52 visual_prompt]: Inference (test):avg data time: 6.48e-03, avg batch time: 0.1939, average loss: 2.8673
[09/15 16:19:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.59	top5: 46.67	
[09/15 16:19:52 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/15 16:20:01 visual_prompt]: Epoch 73 / 100: avg data time: 9.61e-02, avg batch time: 0.5018, average train loss: 2.4688
[09/15 16:20:03 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1439, average loss: 2.3391
[09/15 16:20:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.50	top5: 71.50	
[09/15 16:20:24 visual_prompt]: 	Test 100/190. loss: 2.786, 0.2088 s / batch. (data: 2.54e-02)max mem: 17.22456 GB 
[09/15 16:20:43 visual_prompt]: Inference (test):avg data time: 8.32e-03, avg batch time: 0.1956, average loss: 2.8407
[09/15 16:20:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.54	top5: 49.60	
[09/15 16:20:43 visual_prompt]: Best epoch 73: best metric: 0.195
[09/15 16:20:43 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/15 16:20:52 visual_prompt]: Epoch 74 / 100: avg data time: 1.00e-01, avg batch time: 0.5044, average train loss: 2.4347
[09/15 16:20:55 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1440, average loss: 2.4033
[09/15 16:20:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.50	top5: 64.50	
[09/15 16:21:16 visual_prompt]: 	Test 100/190. loss: 2.839, 0.2073 s / batch. (data: 2.45e-02)max mem: 17.22456 GB 
[09/15 16:21:34 visual_prompt]: Inference (test):avg data time: 8.81e-03, avg batch time: 0.1952, average loss: 2.8428
[09/15 16:21:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.46	top5: 47.06	
[09/15 16:21:35 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/15 16:21:43 visual_prompt]: Epoch 75 / 100: avg data time: 1.00e-01, avg batch time: 0.5046, average train loss: 2.4449
[09/15 16:21:46 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1438, average loss: 2.3344
[09/15 16:21:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.50	top5: 72.00	
[09/15 16:22:07 visual_prompt]: 	Test 100/190. loss: 2.778, 0.1839 s / batch. (data: 1.46e-04)max mem: 17.22456 GB 
[09/15 16:22:26 visual_prompt]: Inference (test):avg data time: 8.94e-03, avg batch time: 0.1955, average loss: 2.8226
[09/15 16:22:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.55	top5: 49.58	
[09/15 16:22:26 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/15 16:22:35 visual_prompt]: Epoch 76 / 100: avg data time: 9.59e-02, avg batch time: 0.5003, average train loss: 2.3285
[09/15 16:22:38 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1437, average loss: 2.2458
[09/15 16:22:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.00	top5: 77.00	
[09/15 16:22:59 visual_prompt]: 	Test 100/190. loss: 2.902, 0.1847 s / batch. (data: 1.45e-04)max mem: 17.22456 GB 
[09/15 16:23:17 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1941, average loss: 2.8498
[09/15 16:23:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.52	top5: 50.91	
[09/15 16:23:17 visual_prompt]: Best epoch 76: best metric: 0.200
[09/15 16:23:17 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/15 16:23:26 visual_prompt]: Epoch 77 / 100: avg data time: 9.96e-02, avg batch time: 0.5022, average train loss: 2.3038
[09/15 16:23:29 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1439, average loss: 2.2594
[09/15 16:23:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.00	top5: 75.00	
[09/15 16:23:50 visual_prompt]: 	Test 100/190. loss: 2.927, 0.2255 s / batch. (data: 2.51e-02)max mem: 17.22456 GB 
[09/15 16:24:09 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1948, average loss: 2.9601
[09/15 16:24:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.94	top5: 49.17	
[09/15 16:24:09 visual_prompt]: Best epoch 77: best metric: 0.230
[09/15 16:24:09 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/15 16:24:17 visual_prompt]: Epoch 78 / 100: avg data time: 9.58e-02, avg batch time: 0.4992, average train loss: 2.2461
[09/15 16:24:20 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1437, average loss: 2.2073
[09/15 16:24:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.50	top5: 72.00	
[09/15 16:24:41 visual_prompt]: 	Test 100/190. loss: 2.836, 0.2112 s / batch. (data: 2.77e-02)max mem: 17.22456 GB 
[09/15 16:25:00 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1943, average loss: 2.9642
[09/15 16:25:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.13	top5: 48.96	
[09/15 16:25:00 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/15 16:25:08 visual_prompt]: Epoch 79 / 100: avg data time: 8.10e-02, avg batch time: 0.4865, average train loss: 2.2736
[09/15 16:25:11 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1439, average loss: 2.1693
[09/15 16:25:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.50	top5: 76.00	
[09/15 16:25:32 visual_prompt]: 	Test 100/190. loss: 2.868, 0.1845 s / batch. (data: 1.29e-04)max mem: 17.22456 GB 
[09/15 16:25:51 visual_prompt]: Inference (test):avg data time: 7.50e-03, avg batch time: 0.1945, average loss: 2.9063
[09/15 16:25:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.63	top5: 48.63	
[09/15 16:25:51 visual_prompt]: Best epoch 79: best metric: 0.235
[09/15 16:25:51 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/15 16:26:00 visual_prompt]: Epoch 80 / 100: avg data time: 9.31e-02, avg batch time: 0.4986, average train loss: 2.2026
[09/15 16:26:02 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1438, average loss: 2.2299
[09/15 16:26:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.50	top5: 76.50	
[09/15 16:26:23 visual_prompt]: 	Test 100/190. loss: 3.075, 0.1847 s / batch. (data: 1.57e-04)max mem: 17.22456 GB 
[09/15 16:26:42 visual_prompt]: Inference (test):avg data time: 6.30e-03, avg batch time: 0.1931, average loss: 3.0501
[09/15 16:26:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.65	top5: 49.86	
[09/15 16:26:42 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/15 16:26:51 visual_prompt]: Epoch 81 / 100: avg data time: 9.46e-02, avg batch time: 0.4989, average train loss: 2.2300
[09/15 16:26:53 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1438, average loss: 2.1751
[09/15 16:26:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.50	top5: 76.00	
[09/15 16:27:14 visual_prompt]: 	Test 100/190. loss: 2.953, 0.1844 s / batch. (data: 9.44e-05)max mem: 17.22456 GB 
[09/15 16:27:33 visual_prompt]: Inference (test):avg data time: 6.54e-03, avg batch time: 0.1944, average loss: 2.9520
[09/15 16:27:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.07	top5: 51.70	
[09/15 16:27:33 visual_prompt]: Best epoch 81: best metric: 0.275
[09/15 16:27:33 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/15 16:27:42 visual_prompt]: Epoch 82 / 100: avg data time: 9.87e-02, avg batch time: 0.5039, average train loss: 2.1289
[09/15 16:27:44 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1437, average loss: 2.0367
[09/15 16:27:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 26.50	top5: 79.00	
[09/15 16:28:06 visual_prompt]: 	Test 100/190. loss: 3.080, 0.1962 s / batch. (data: 1.27e-02)max mem: 17.22456 GB 
[09/15 16:28:24 visual_prompt]: Inference (test):avg data time: 7.02e-03, avg batch time: 0.1939, average loss: 2.9986
[09/15 16:28:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.91	top5: 52.11	
[09/15 16:28:24 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/15 16:28:33 visual_prompt]: Epoch 83 / 100: avg data time: 1.02e-01, avg batch time: 0.5049, average train loss: 2.0565
[09/15 16:28:36 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1438, average loss: 1.9775
[09/15 16:28:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.00	top5: 84.00	
[09/15 16:28:57 visual_prompt]: 	Test 100/190. loss: 3.029, 0.1843 s / batch. (data: 1.10e-04)max mem: 17.22456 GB 
[09/15 16:29:15 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1946, average loss: 3.0513
[09/15 16:29:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.70	top5: 51.76	
[09/15 16:29:15 visual_prompt]: Best epoch 83: best metric: 0.300
[09/15 16:29:15 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/15 16:29:24 visual_prompt]: Epoch 84 / 100: avg data time: 9.43e-02, avg batch time: 0.4977, average train loss: 1.9870
[09/15 16:29:27 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1437, average loss: 1.9490
[09/15 16:29:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.50	top5: 86.50	
[09/15 16:29:48 visual_prompt]: 	Test 100/190. loss: 3.199, 0.1842 s / batch. (data: 1.19e-04)max mem: 17.22456 GB 
[09/15 16:30:06 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1940, average loss: 3.1176
[09/15 16:30:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.20	top5: 51.51	
[09/15 16:30:07 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/15 16:30:15 visual_prompt]: Epoch 85 / 100: avg data time: 8.96e-02, avg batch time: 0.4983, average train loss: 1.8897
[09/15 16:30:18 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1435, average loss: 1.7625
[09/15 16:30:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 35.00	top5: 91.00	
[09/15 16:30:39 visual_prompt]: 	Test 100/190. loss: 3.433, 0.1866 s / batch. (data: 1.41e-04)max mem: 17.22456 GB 
[09/15 16:30:58 visual_prompt]: Inference (test):avg data time: 8.97e-03, avg batch time: 0.1955, average loss: 3.3417
[09/15 16:30:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.70	top5: 52.43	
[09/15 16:30:58 visual_prompt]: Best epoch 85: best metric: 0.350
[09/15 16:30:58 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/15 16:31:07 visual_prompt]: Epoch 86 / 100: avg data time: 1.01e-01, avg batch time: 0.5075, average train loss: 1.9093
[09/15 16:31:10 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1436, average loss: 1.8078
[09/15 16:31:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.00	top5: 92.00	
[09/15 16:31:31 visual_prompt]: 	Test 100/190. loss: 3.320, 0.1919 s / batch. (data: 1.34e-04)max mem: 17.22456 GB 
[09/15 16:31:49 visual_prompt]: Inference (test):avg data time: 8.26e-03, avg batch time: 0.1950, average loss: 3.3338
[09/15 16:31:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.84	top5: 52.79	
[09/15 16:31:49 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/15 16:31:58 visual_prompt]: Epoch 87 / 100: avg data time: 9.96e-02, avg batch time: 0.5026, average train loss: 1.8528
[09/15 16:32:01 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1436, average loss: 1.8215
[09/15 16:32:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 33.50	top5: 85.50	
[09/15 16:32:22 visual_prompt]: 	Test 100/190. loss: 3.130, 0.1962 s / batch. (data: 1.30e-02)max mem: 17.22456 GB 
[09/15 16:32:40 visual_prompt]: Inference (test):avg data time: 6.71e-03, avg batch time: 0.1936, average loss: 3.3300
[09/15 16:32:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.83	top5: 52.30	
[09/15 16:32:40 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/15 16:32:49 visual_prompt]: Epoch 88 / 100: avg data time: 8.89e-02, avg batch time: 0.4985, average train loss: 1.8395
[09/15 16:32:52 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1435, average loss: 1.9958
[09/15 16:32:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.50	top5: 83.50	
[09/15 16:33:13 visual_prompt]: 	Test 100/190. loss: 3.454, 0.1875 s / batch. (data: 1.19e-04)max mem: 17.22456 GB 
[09/15 16:33:32 visual_prompt]: Inference (test):avg data time: 9.07e-03, avg batch time: 0.1956, average loss: 3.4804
[09/15 16:33:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.39	top5: 50.86	
[09/15 16:33:32 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/15 16:33:41 visual_prompt]: Epoch 89 / 100: avg data time: 9.49e-02, avg batch time: 0.4981, average train loss: 1.7822
[09/15 16:33:43 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1436, average loss: 1.7044
[09/15 16:33:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.50	top5: 92.50	
[09/15 16:34:04 visual_prompt]: 	Test 100/190. loss: 3.432, 0.1843 s / batch. (data: 1.49e-04)max mem: 17.22456 GB 
[09/15 16:34:23 visual_prompt]: Inference (test):avg data time: 7.26e-03, avg batch time: 0.1942, average loss: 3.4468
[09/15 16:34:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.69	top5: 53.84	
[09/15 16:34:23 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/15 16:34:32 visual_prompt]: Epoch 90 / 100: avg data time: 8.95e-02, avg batch time: 0.4954, average train loss: 1.6401
[09/15 16:34:35 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1435, average loss: 1.5405
[09/15 16:34:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 43.00	top5: 94.50	
[09/15 16:34:56 visual_prompt]: 	Test 100/190. loss: 3.736, 0.1978 s / batch. (data: 1.42e-02)max mem: 17.22456 GB 
[09/15 16:35:14 visual_prompt]: Inference (test):avg data time: 6.94e-03, avg batch time: 0.1941, average loss: 3.6271
[09/15 16:35:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.98	top5: 53.42	
[09/15 16:35:14 visual_prompt]: Best epoch 90: best metric: 0.430
[09/15 16:35:14 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/15 16:35:23 visual_prompt]: Epoch 91 / 100: avg data time: 1.02e-01, avg batch time: 0.5060, average train loss: 1.6015
[09/15 16:35:26 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1439, average loss: 1.4990
[09/15 16:35:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 42.50	top5: 95.50	
[09/15 16:35:47 visual_prompt]: 	Test 100/190. loss: 3.901, 0.1845 s / batch. (data: 1.13e-04)max mem: 17.22456 GB 
[09/15 16:36:05 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1935, average loss: 3.8217
[09/15 16:36:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.20	top5: 53.08	
[09/15 16:36:05 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/15 16:36:14 visual_prompt]: Epoch 92 / 100: avg data time: 1.03e-01, avg batch time: 0.5054, average train loss: 1.5327
[09/15 16:36:17 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1440, average loss: 1.4881
[09/15 16:36:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 45.50	top5: 94.50	
[09/15 16:36:38 visual_prompt]: 	Test 100/190. loss: 3.963, 0.1938 s / batch. (data: 1.72e-04)max mem: 17.22456 GB 
[09/15 16:36:57 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1948, average loss: 3.8851
[09/15 16:36:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.63	top5: 52.54	
[09/15 16:36:57 visual_prompt]: Best epoch 92: best metric: 0.455
[09/15 16:36:57 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/15 16:37:05 visual_prompt]: Epoch 93 / 100: avg data time: 9.57e-02, avg batch time: 0.4983, average train loss: 1.4720
[09/15 16:37:08 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1438, average loss: 1.4527
[09/15 16:37:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 42.00	top5: 95.00	
[09/15 16:37:29 visual_prompt]: 	Test 100/190. loss: 4.229, 0.1841 s / batch. (data: 1.43e-04)max mem: 17.22456 GB 
[09/15 16:37:48 visual_prompt]: Inference (test):avg data time: 8.58e-03, avg batch time: 0.1953, average loss: 4.0409
[09/15 16:37:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.21	top5: 53.51	
[09/15 16:37:48 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/15 16:37:57 visual_prompt]: Epoch 94 / 100: avg data time: 9.41e-02, avg batch time: 0.5001, average train loss: 1.4194
[09/15 16:38:00 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.1436, average loss: 1.3909
[09/15 16:38:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 47.00	top5: 96.50	
[09/15 16:38:21 visual_prompt]: 	Test 100/190. loss: 4.355, 0.1965 s / batch. (data: 1.30e-02)max mem: 17.22456 GB 
[09/15 16:38:40 visual_prompt]: Inference (test):avg data time: 8.49e-03, avg batch time: 0.1954, average loss: 4.1726
[09/15 16:38:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.93	top5: 52.95	
[09/15 16:38:40 visual_prompt]: Best epoch 94: best metric: 0.470
[09/15 16:38:40 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/15 16:38:49 visual_prompt]: Epoch 95 / 100: avg data time: 9.99e-02, avg batch time: 0.5058, average train loss: 1.3529
[09/15 16:38:51 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1435, average loss: 1.3963
[09/15 16:38:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 44.50	top5: 94.50	
[09/15 16:39:12 visual_prompt]: 	Test 100/190. loss: 4.445, 0.2081 s / batch. (data: 1.27e-02)max mem: 17.22456 GB 
[09/15 16:39:31 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1943, average loss: 4.3146
[09/15 16:39:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.81	top5: 52.81	
[09/15 16:39:31 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/15 16:39:40 visual_prompt]: Epoch 96 / 100: avg data time: 9.55e-02, avg batch time: 0.5006, average train loss: 1.3024
[09/15 16:39:43 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1438, average loss: 1.3446
[09/15 16:39:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 46.50	top5: 97.00	
[09/15 16:40:04 visual_prompt]: 	Test 100/190. loss: 4.620, 0.1844 s / batch. (data: 1.43e-04)max mem: 17.22456 GB 
[09/15 16:40:22 visual_prompt]: Inference (test):avg data time: 5.51e-03, avg batch time: 0.1925, average loss: 4.4724
[09/15 16:40:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.07	top5: 53.31	
[09/15 16:40:22 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/15 16:40:31 visual_prompt]: Epoch 97 / 100: avg data time: 8.65e-02, avg batch time: 0.4925, average train loss: 1.2634
[09/15 16:40:34 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1449, average loss: 1.2953
[09/15 16:40:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 48.50	top5: 97.00	
[09/15 16:40:55 visual_prompt]: 	Test 100/190. loss: 4.646, 0.1850 s / batch. (data: 1.39e-04)max mem: 17.22456 GB 
[09/15 16:41:13 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1937, average loss: 4.5148
[09/15 16:41:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.06	top5: 53.30	
[09/15 16:41:13 visual_prompt]: Best epoch 97: best metric: 0.485
[09/15 16:41:13 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/15 16:41:22 visual_prompt]: Epoch 98 / 100: avg data time: 9.65e-02, avg batch time: 0.5017, average train loss: 1.2232
[09/15 16:41:25 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1435, average loss: 1.3279
[09/15 16:41:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 45.50	top5: 97.00	
[09/15 16:41:46 visual_prompt]: 	Test 100/190. loss: 4.713, 0.1838 s / batch. (data: 8.99e-05)max mem: 17.22456 GB 
[09/15 16:42:04 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1949, average loss: 4.6151
[09/15 16:42:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.04	top5: 53.50	
[09/15 16:42:05 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/15 16:42:13 visual_prompt]: Epoch 99 / 100: avg data time: 1.00e-01, avg batch time: 0.5037, average train loss: 1.2306
[09/15 16:42:16 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1438, average loss: 1.3263
[09/15 16:42:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 47.00	top5: 96.50	
[09/15 16:42:37 visual_prompt]: 	Test 100/190. loss: 4.687, 0.2040 s / batch. (data: 2.04e-02)max mem: 17.22456 GB 
[09/15 16:42:56 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1948, average loss: 4.6209
[09/15 16:42:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.08	top5: 53.19	
[09/15 16:42:56 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/15 16:43:04 visual_prompt]: Epoch 100 / 100: avg data time: 8.44e-02, avg batch time: 0.4916, average train loss: 1.2275
[09/15 16:43:07 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1435, average loss: 1.3099
[09/15 16:43:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 48.00	top5: 97.00	
[09/15 16:43:28 visual_prompt]: 	Test 100/190. loss: 4.698, 0.1846 s / batch. (data: 9.20e-05)max mem: 17.22456 GB 
[09/15 16:43:47 visual_prompt]: Inference (test):avg data time: 9.04e-03, avg batch time: 0.1962, average loss: 4.6228
[09/15 16:43:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.87	top5: 53.19	
[09/15 16:44:08 visual_prompt]: Rank of current process: 0. World size: 1
[09/15 16:44:08 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/15 16:44:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed100'], train_type='')
[09/15 16:44:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/15 16:44:08 visual_prompt]: Training with config:
[09/15 16:44:08 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-smallnorb(predicted_attribute="label_azimuth")',
          'NO_TEST': False,
          'NUMBER_CLASSES': 18,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed100/vtab-smallnorb(predicted_attribute="label_azimuth")/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/15 16:44:08 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-15 16:44:08.282283: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-15 16:44:08.486656: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-15 16:44:09.396399: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 16:44:09.396497: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 16:44:09.396506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-15 16:44:11.507022: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 16:44:11.507125: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 16:44:11.507137: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/15 16:44:11 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
2023-09-15 16:44:11.577479: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 16:44:13 visual_prompt]: Number of images: 1000
[09/15 16:44:13 visual_prompt]: Number of classes: 18 / 18
[09/15 16:44:13 visual_prompt]: Loading validation data...
[09/15 16:44:13 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 16:44:13 visual_prompt]: Number of images: 200
[09/15 16:44:13 visual_prompt]: Number of classes: 18 / 18
[09/15 16:44:13 visual_prompt]: Loading test data...
[09/15 16:44:13 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 16:44:30 visual_prompt]: Number of images: 12150
[09/15 16:44:30 visual_prompt]: Number of classes: 18 / 18
[09/15 16:44:30 visual_prompt]: Constructing models...
[09/15 16:44:33 visual_prompt]: Total Parameters: 86734098	 Gradient Parameters: 935442
[09/15 16:44:33 visual_prompt]: tuned percent:1.079
[09/15 16:44:36 visual_prompt]: Device used for model: 0
[09/15 16:44:36 visual_prompt]: Setting up Evalutator...
[09/15 16:44:36 visual_prompt]: Setting up Trainer...
[09/15 16:44:36 visual_prompt]: 	Setting up the optimizer...
[09/15 16:44:36 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/15 16:44:46 visual_prompt]: Epoch 1 / 100: avg data time: 1.18e-01, avg batch time: 0.5889, average train loss: 3.0606
[09/15 16:44:49 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1429, average loss: 3.0277
[09/15 16:44:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 28.00	
[09/15 16:45:10 visual_prompt]: 	Test 100/190. loss: 3.046, 0.2079 s / batch. (data: 2.57e-02)max mem: 17.22456 GB 
[09/15 16:45:28 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1930, average loss: 3.0762
[09/15 16:45:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.38	top5: 28.07	
[09/15 16:45:28 visual_prompt]: Best epoch 1: best metric: 0.060
[09/15 16:45:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/15 16:45:37 visual_prompt]: Epoch 2 / 100: avg data time: 1.06e-01, avg batch time: 0.5083, average train loss: 3.5100
[09/15 16:45:40 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1433, average loss: 3.3363
[09/15 16:45:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 26.50	
[09/15 16:46:01 visual_prompt]: 	Test 100/190. loss: 3.002, 0.1959 s / batch. (data: 1.27e-02)max mem: 17.22456 GB 
[09/15 16:46:20 visual_prompt]: Inference (test):avg data time: 8.79e-03, avg batch time: 0.1938, average loss: 3.2512
[09/15 16:46:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.67	top5: 27.63	
[09/15 16:46:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/15 16:46:29 visual_prompt]: Epoch 3 / 100: avg data time: 1.00e-01, avg batch time: 0.5043, average train loss: 3.1528
[09/15 16:46:32 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1436, average loss: 3.0775
[09/15 16:46:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 24.50	
[09/15 16:46:52 visual_prompt]: 	Test 100/190. loss: 3.067, 0.1834 s / batch. (data: 1.24e-04)max mem: 17.22456 GB 
[09/15 16:47:11 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1927, average loss: 3.0537
[09/15 16:47:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.91	
[09/15 16:47:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/15 16:47:20 visual_prompt]: Epoch 4 / 100: avg data time: 9.70e-02, avg batch time: 0.4992, average train loss: 3.0974
[09/15 16:47:23 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1435, average loss: 3.2073
[09/15 16:47:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 26.50	
[09/15 16:47:44 visual_prompt]: 	Test 100/190. loss: 3.062, 0.2334 s / batch. (data: 4.00e-02)max mem: 17.22456 GB 
[09/15 16:48:02 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1943, average loss: 3.1529
[09/15 16:48:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.52	top5: 27.85	
[09/15 16:48:02 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/15 16:48:11 visual_prompt]: Epoch 5 / 100: avg data time: 1.00e-01, avg batch time: 0.5063, average train loss: 3.2570
[09/15 16:48:14 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1436, average loss: 3.3709
[09/15 16:48:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 26.00	
[09/15 16:48:35 visual_prompt]: 	Test 100/190. loss: 3.410, 0.1982 s / batch. (data: 1.30e-04)max mem: 17.22456 GB 
[09/15 16:48:54 visual_prompt]: Inference (test):avg data time: 6.94e-03, avg batch time: 0.1938, average loss: 3.3988
[09/15 16:48:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.47	
[09/15 16:48:54 visual_prompt]: Best epoch 5: best metric: 0.085
[09/15 16:48:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/15 16:49:03 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e-01, avg batch time: 0.5089, average train loss: 3.2862
[09/15 16:49:06 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1435, average loss: 3.1601
[09/15 16:49:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 30.50	
[09/15 16:49:26 visual_prompt]: 	Test 100/190. loss: 3.161, 0.1965 s / batch. (data: 1.29e-02)max mem: 17.22456 GB 
[09/15 16:49:45 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1937, average loss: 3.2077
[09/15 16:49:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.37	top5: 27.93	
[09/15 16:49:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/15 16:49:54 visual_prompt]: Epoch 7 / 100: avg data time: 9.61e-02, avg batch time: 0.5025, average train loss: 3.2973
[09/15 16:49:57 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1439, average loss: 3.2568
[09/15 16:49:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 28.50	
[09/15 16:50:18 visual_prompt]: 	Test 100/190. loss: 3.255, 0.1947 s / batch. (data: 1.12e-02)max mem: 17.22456 GB 
[09/15 16:50:36 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1934, average loss: 3.2753
[09/15 16:50:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 28.25	
[09/15 16:50:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/15 16:50:45 visual_prompt]: Epoch 8 / 100: avg data time: 1.00e-01, avg batch time: 0.5023, average train loss: 3.3470
[09/15 16:50:48 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1440, average loss: 3.6906
[09/15 16:50:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 28.00	
[09/15 16:51:09 visual_prompt]: 	Test 100/190. loss: 3.168, 0.1962 s / batch. (data: 3.48e-05)max mem: 17.22456 GB 
[09/15 16:51:28 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1930, average loss: 3.5961
[09/15 16:51:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.77	top5: 29.17	
[09/15 16:51:28 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/15 16:51:36 visual_prompt]: Epoch 9 / 100: avg data time: 8.40e-02, avg batch time: 0.4906, average train loss: 3.4370
[09/15 16:51:39 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1440, average loss: 3.3290
[09/15 16:51:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 34.50	
[09/15 16:52:00 visual_prompt]: 	Test 100/190. loss: 3.339, 0.2248 s / batch. (data: 4.12e-02)max mem: 17.22456 GB 
[09/15 16:52:19 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1942, average loss: 3.3616
[09/15 16:52:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 29.92	
[09/15 16:52:19 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/15 16:52:28 visual_prompt]: Epoch 10 / 100: avg data time: 1.06e-01, avg batch time: 0.5102, average train loss: 3.2519
[09/15 16:52:31 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1435, average loss: 3.0810
[09/15 16:52:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 33.50	
[09/15 16:52:52 visual_prompt]: 	Test 100/190. loss: 3.278, 0.1838 s / batch. (data: 2.93e-05)max mem: 17.22456 GB 
[09/15 16:53:10 visual_prompt]: Inference (test):avg data time: 6.31e-03, avg batch time: 0.1932, average loss: 3.1658
[09/15 16:53:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.02	top5: 30.44	
[09/15 16:53:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/15 16:53:19 visual_prompt]: Epoch 11 / 100: avg data time: 9.80e-02, avg batch time: 0.5041, average train loss: 3.7126
[09/15 16:53:22 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1437, average loss: 3.7594
[09/15 16:53:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.00	top5: 29.00	
[09/15 16:53:43 visual_prompt]: 	Test 100/190. loss: 3.628, 0.1890 s / batch. (data: 1.45e-04)max mem: 17.22456 GB 
[09/15 16:54:02 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1936, average loss: 3.6069
[09/15 16:54:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.10	top5: 32.52	
[09/15 16:54:02 visual_prompt]: Best epoch 11: best metric: 0.110
[09/15 16:54:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/15 16:54:11 visual_prompt]: Epoch 12 / 100: avg data time: 1.07e-01, avg batch time: 0.5098, average train loss: 3.5287
[09/15 16:54:14 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1438, average loss: 3.3945
[09/15 16:54:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 29.50	
[09/15 16:54:35 visual_prompt]: 	Test 100/190. loss: 3.405, 0.2102 s / batch. (data: 2.67e-02)max mem: 17.22456 GB 
[09/15 16:54:53 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1931, average loss: 3.3345
[09/15 16:54:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.79	top5: 29.36	
[09/15 16:54:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/15 16:55:02 visual_prompt]: Epoch 13 / 100: avg data time: 1.01e-01, avg batch time: 0.5061, average train loss: 3.2705
[09/15 16:55:05 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1435, average loss: 2.9913
[09/15 16:55:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 43.00	
[09/15 16:55:26 visual_prompt]: 	Test 100/190. loss: 3.170, 0.2005 s / batch. (data: 1.41e-02)max mem: 17.22456 GB 
[09/15 16:55:45 visual_prompt]: Inference (test):avg data time: 6.61e-03, avg batch time: 0.1932, average loss: 3.0652
[09/15 16:55:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.15	top5: 36.91	
[09/15 16:55:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/15 16:55:54 visual_prompt]: Epoch 14 / 100: avg data time: 1.01e-01, avg batch time: 0.5042, average train loss: 3.3108
[09/15 16:55:57 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1439, average loss: 3.1779
[09/15 16:55:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 38.00	
[09/15 16:56:18 visual_prompt]: 	Test 100/190. loss: 3.124, 0.1982 s / batch. (data: 1.48e-02)max mem: 17.22456 GB 
[09/15 16:56:36 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1941, average loss: 3.2963
[09/15 16:56:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 30.99	
[09/15 16:56:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/15 16:56:45 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e-01, avg batch time: 0.5065, average train loss: 3.1327
[09/15 16:56:48 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1437, average loss: 2.9288
[09/15 16:56:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.00	top5: 47.50	
[09/15 16:57:09 visual_prompt]: 	Test 100/190. loss: 2.936, 0.1838 s / batch. (data: 1.29e-04)max mem: 17.22456 GB 
[09/15 16:57:28 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1938, average loss: 3.0872
[09/15 16:57:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.40	top5: 39.88	
[09/15 16:57:28 visual_prompt]: Best epoch 15: best metric: 0.150
[09/15 16:57:28 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/15 16:57:37 visual_prompt]: Epoch 16 / 100: avg data time: 9.51e-02, avg batch time: 0.4987, average train loss: 3.0424
[09/15 16:57:39 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1437, average loss: 3.0981
[09/15 16:57:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 48.00	
[09/15 16:58:01 visual_prompt]: 	Test 100/190. loss: 3.171, 0.2282 s / batch. (data: 1.54e-02)max mem: 17.22456 GB 
[09/15 16:58:19 visual_prompt]: Inference (test):avg data time: 8.29e-03, avg batch time: 0.1943, average loss: 3.2040
[09/15 16:58:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.70	top5: 43.78	
[09/15 16:58:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/15 16:58:28 visual_prompt]: Epoch 17 / 100: avg data time: 9.07e-02, avg batch time: 0.4986, average train loss: 3.3474
[09/15 16:58:31 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1474, average loss: 3.3847
[09/15 16:58:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.50	top5: 53.00	
[09/15 16:58:52 visual_prompt]: 	Test 100/190. loss: 3.352, 0.1836 s / batch. (data: 1.52e-04)max mem: 17.22456 GB 
[09/15 16:59:11 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1930, average loss: 3.5539
[09/15 16:59:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.71	top5: 46.27	
[09/15 16:59:11 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/15 16:59:19 visual_prompt]: Epoch 18 / 100: avg data time: 8.99e-02, avg batch time: 0.4988, average train loss: 4.4929
[09/15 16:59:23 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1438, average loss: 5.3603
[09/15 16:59:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 36.50	
[09/15 16:59:44 visual_prompt]: 	Test 100/190. loss: 4.794, 0.1962 s / batch. (data: 1.32e-04)max mem: 17.22456 GB 
[09/15 17:00:02 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1940, average loss: 5.4166
[09/15 17:00:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.71	top5: 36.12	
[09/15 17:00:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/15 17:00:11 visual_prompt]: Epoch 19 / 100: avg data time: 8.63e-02, avg batch time: 0.4959, average train loss: 7.8155
[09/15 17:00:14 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1439, average loss: 17.1555
[09/15 17:00:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 28.00	
[09/15 17:00:35 visual_prompt]: 	Test 100/190. loss: 17.374, 0.1843 s / batch. (data: 1.28e-04)max mem: 17.22456 GB 
[09/15 17:00:53 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1934, average loss: 17.0451
[09/15 17:00:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 28.27	
[09/15 17:00:53 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/15 17:01:02 visual_prompt]: Epoch 20 / 100: avg data time: 9.36e-02, avg batch time: 0.5021, average train loss: 13.4724
[09/15 17:01:05 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1437, average loss: 13.7513
[09/15 17:01:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 35.50	
[09/15 17:01:26 visual_prompt]: 	Test 100/190. loss: 16.050, 0.1984 s / batch. (data: 1.51e-02)max mem: 17.22456 GB 
[09/15 17:01:45 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1938, average loss: 14.6771
[09/15 17:01:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.67	top5: 27.62	
[09/15 17:01:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/15 17:01:54 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e-01, avg batch time: 0.5079, average train loss: 16.1938
[09/15 17:01:57 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1435, average loss: 16.8702
[09/15 17:01:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 26.00	
[09/15 17:02:18 visual_prompt]: 	Test 100/190. loss: 14.813, 0.1849 s / batch. (data: 1.41e-04)max mem: 17.22456 GB 
[09/15 17:02:36 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1935, average loss: 16.6177
[09/15 17:02:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.65	top5: 28.04	
[09/15 17:02:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/15 17:02:45 visual_prompt]: Epoch 22 / 100: avg data time: 1.04e-01, avg batch time: 0.5074, average train loss: 18.0215
[09/15 17:02:48 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1437, average loss: 15.0778
[09/15 17:02:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 28.00	
[09/15 17:03:09 visual_prompt]: 	Test 100/190. loss: 15.111, 0.1836 s / batch. (data: 8.85e-05)max mem: 17.22456 GB 
[09/15 17:03:28 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1945, average loss: 15.7397
[09/15 17:03:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.65	top5: 27.87	
[09/15 17:03:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/15 17:03:37 visual_prompt]: Epoch 23 / 100: avg data time: 9.19e-02, avg batch time: 0.4956, average train loss: 13.5153
[09/15 17:03:40 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1437, average loss: 9.1757
[09/15 17:03:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 25.00	
[09/15 17:04:01 visual_prompt]: 	Test 100/190. loss: 8.469, 0.2134 s / batch. (data: 3.01e-02)max mem: 17.22456 GB 
[09/15 17:04:19 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1939, average loss: 8.8218
[09/15 17:04:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.65	top5: 27.97	
[09/15 17:04:19 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/15 17:04:28 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e-01, avg batch time: 0.5079, average train loss: 7.0127
[09/15 17:04:31 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1439, average loss: 5.3911
[09/15 17:04:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 32.00	
[09/15 17:04:52 visual_prompt]: 	Test 100/190. loss: 5.455, 0.1860 s / batch. (data: 1.30e-04)max mem: 17.22456 GB 
[09/15 17:05:10 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1936, average loss: 5.7083
[09/15 17:05:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.63	
[09/15 17:05:10 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/15 17:05:19 visual_prompt]: Epoch 25 / 100: avg data time: 8.78e-02, avg batch time: 0.4918, average train loss: 5.1716
[09/15 17:05:22 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1438, average loss: 4.0118
[09/15 17:05:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 29.00	
[09/15 17:05:43 visual_prompt]: 	Test 100/190. loss: 4.067, 0.1987 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 17:06:02 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1943, average loss: 4.0157
[09/15 17:06:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.59	top5: 28.40	
[09/15 17:06:02 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/15 17:06:11 visual_prompt]: Epoch 26 / 100: avg data time: 9.61e-02, avg batch time: 0.5006, average train loss: 4.0483
[09/15 17:06:14 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1438, average loss: 4.3550
[09/15 17:06:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 25.50	
[09/15 17:06:35 visual_prompt]: 	Test 100/190. loss: 4.678, 0.2184 s / batch. (data: 3.00e-02)max mem: 17.22456 GB 
[09/15 17:06:53 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1939, average loss: 4.2801
[09/15 17:06:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.65	top5: 27.61	
[09/15 17:06:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/15 17:07:02 visual_prompt]: Epoch 27 / 100: avg data time: 1.00e-01, avg batch time: 0.5052, average train loss: 3.7279
[09/15 17:07:05 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1437, average loss: 3.3934
[09/15 17:07:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 23.00	
[09/15 17:07:26 visual_prompt]: 	Test 100/190. loss: 3.312, 0.2074 s / batch. (data: 2.41e-02)max mem: 17.22456 GB 
[09/15 17:07:45 visual_prompt]: Inference (test):avg data time: 6.49e-03, avg batch time: 0.1934, average loss: 3.3190
[09/15 17:07:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 28.18	
[09/15 17:07:45 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/15 17:07:54 visual_prompt]: Epoch 28 / 100: avg data time: 9.94e-02, avg batch time: 0.5023, average train loss: 3.5246
[09/15 17:07:57 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1466, average loss: 3.7499
[09/15 17:07:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 25.00	
[09/15 17:08:17 visual_prompt]: 	Test 100/190. loss: 3.816, 0.1845 s / batch. (data: 1.31e-04)max mem: 17.22456 GB 
[09/15 17:08:36 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1936, average loss: 3.6674
[09/15 17:08:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 27.53	
[09/15 17:08:36 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/15 17:08:45 visual_prompt]: Epoch 29 / 100: avg data time: 9.20e-02, avg batch time: 0.4987, average train loss: 3.4033
[09/15 17:08:48 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1439, average loss: 3.3005
[09/15 17:08:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 28.50	
[09/15 17:09:09 visual_prompt]: 	Test 100/190. loss: 3.451, 0.1956 s / batch. (data: 1.25e-02)max mem: 17.22456 GB 
[09/15 17:09:27 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1936, average loss: 3.3372
[09/15 17:09:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 28.03	
[09/15 17:09:27 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/15 17:09:36 visual_prompt]: Epoch 30 / 100: avg data time: 9.97e-02, avg batch time: 0.5042, average train loss: 3.4783
[09/15 17:09:39 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1475, average loss: 3.3695
[09/15 17:09:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 25.50	
[09/15 17:10:00 visual_prompt]: 	Test 100/190. loss: 3.193, 0.1880 s / batch. (data: 1.31e-04)max mem: 17.22456 GB 
[09/15 17:10:19 visual_prompt]: Inference (test):avg data time: 8.26e-03, avg batch time: 0.1946, average loss: 3.3330
[09/15 17:10:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.53	top5: 27.71	
[09/15 17:10:19 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/15 17:10:28 visual_prompt]: Epoch 31 / 100: avg data time: 1.03e-01, avg batch time: 0.5069, average train loss: 3.3914
[09/15 17:10:30 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1438, average loss: 3.1020
[09/15 17:10:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 32.00	
[09/15 17:10:51 visual_prompt]: 	Test 100/190. loss: 3.146, 0.1847 s / batch. (data: 1.26e-04)max mem: 17.22456 GB 
[09/15 17:11:10 visual_prompt]: Inference (test):avg data time: 6.67e-03, avg batch time: 0.1929, average loss: 3.1254
[09/15 17:11:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.83	
[09/15 17:11:10 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/15 17:11:19 visual_prompt]: Epoch 32 / 100: avg data time: 1.06e-01, avg batch time: 0.5114, average train loss: 3.1968
[09/15 17:11:22 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1438, average loss: 3.2819
[09/15 17:11:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 26.00	
[09/15 17:11:43 visual_prompt]: 	Test 100/190. loss: 3.133, 0.1942 s / batch. (data: 3.77e-05)max mem: 17.22456 GB 
[09/15 17:12:01 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1933, average loss: 3.2529
[09/15 17:12:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 27.88	
[09/15 17:12:01 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/15 17:12:10 visual_prompt]: Epoch 33 / 100: avg data time: 9.77e-02, avg batch time: 0.5035, average train loss: 3.1481
[09/15 17:12:13 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1437, average loss: 3.1978
[09/15 17:12:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 28.00	
[09/15 17:12:34 visual_prompt]: 	Test 100/190. loss: 3.189, 0.1840 s / batch. (data: 1.16e-04)max mem: 17.22456 GB 
[09/15 17:12:53 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1943, average loss: 3.2775
[09/15 17:12:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.79	top5: 27.98	
[09/15 17:12:53 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/15 17:13:02 visual_prompt]: Epoch 34 / 100: avg data time: 1.06e-01, avg batch time: 0.5106, average train loss: 3.2407
[09/15 17:13:05 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1438, average loss: 3.3741
[09/15 17:13:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.00	
[09/15 17:13:26 visual_prompt]: 	Test 100/190. loss: 3.579, 0.1967 s / batch. (data: 1.33e-02)max mem: 17.22456 GB 
[09/15 17:13:44 visual_prompt]: Inference (test):avg data time: 8.50e-03, avg batch time: 0.1946, average loss: 3.3499
[09/15 17:13:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.28	
[09/15 17:13:44 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/15 17:13:53 visual_prompt]: Epoch 35 / 100: avg data time: 8.93e-02, avg batch time: 0.4928, average train loss: 3.2552
[09/15 17:13:56 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1439, average loss: 3.2702
[09/15 17:13:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 25.00	
[09/15 17:14:17 visual_prompt]: 	Test 100/190. loss: 3.356, 0.1931 s / batch. (data: 1.24e-04)max mem: 17.22456 GB 
[09/15 17:14:36 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1935, average loss: 3.2566
[09/15 17:14:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 27.79	
[09/15 17:14:36 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/15 17:14:45 visual_prompt]: Epoch 36 / 100: avg data time: 1.00e-01, avg batch time: 0.5042, average train loss: 3.3030
[09/15 17:14:48 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1439, average loss: 3.2470
[09/15 17:14:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 28.50	
[09/15 17:15:09 visual_prompt]: 	Test 100/190. loss: 3.417, 0.1959 s / batch. (data: 1.23e-02)max mem: 17.22456 GB 
[09/15 17:15:27 visual_prompt]: Inference (test):avg data time: 8.50e-03, avg batch time: 0.1940, average loss: 3.2570
[09/15 17:15:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 28.10	
[09/15 17:15:27 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/15 17:15:36 visual_prompt]: Epoch 37 / 100: avg data time: 1.06e-01, avg batch time: 0.5092, average train loss: 3.1552
[09/15 17:15:39 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1465, average loss: 3.2455
[09/15 17:15:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 24.00	
[09/15 17:16:01 visual_prompt]: 	Test 100/190. loss: 3.278, 0.1857 s / batch. (data: 1.32e-04)max mem: 17.22456 GB 
[09/15 17:16:19 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1943, average loss: 3.1677
[09/15 17:16:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.52	top5: 27.34	
[09/15 17:16:19 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/15 17:16:28 visual_prompt]: Epoch 38 / 100: avg data time: 1.04e-01, avg batch time: 0.5137, average train loss: 3.2277
[09/15 17:16:31 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1438, average loss: 3.1925
[09/15 17:16:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 28.50	
[09/15 17:16:52 visual_prompt]: 	Test 100/190. loss: 3.343, 0.2085 s / batch. (data: 2.50e-02)max mem: 17.22456 GB 
[09/15 17:17:10 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1934, average loss: 3.2480
[09/15 17:17:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.79	top5: 27.88	
[09/15 17:17:10 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/15 17:17:19 visual_prompt]: Epoch 39 / 100: avg data time: 9.05e-02, avg batch time: 0.4969, average train loss: 3.2217
[09/15 17:17:22 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1464, average loss: 3.1945
[09/15 17:17:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 28.00	
[09/15 17:17:44 visual_prompt]: 	Test 100/190. loss: 3.152, 0.2120 s / batch. (data: 2.87e-02)max mem: 17.22456 GB 
[09/15 17:18:02 visual_prompt]: Inference (test):avg data time: 9.14e-03, avg batch time: 0.1950, average loss: 3.2132
[09/15 17:18:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 27.96	
[09/15 17:18:02 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/15 17:18:11 visual_prompt]: Epoch 40 / 100: avg data time: 1.01e-01, avg batch time: 0.5060, average train loss: 3.1424
[09/15 17:18:14 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1436, average loss: 3.1167
[09/15 17:18:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 31.50	
[09/15 17:18:35 visual_prompt]: 	Test 100/190. loss: 3.074, 0.1845 s / batch. (data: 1.01e-04)max mem: 17.22456 GB 
[09/15 17:18:54 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1948, average loss: 3.1801
[09/15 17:18:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 28.40	
[09/15 17:18:54 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/15 17:19:03 visual_prompt]: Epoch 41 / 100: avg data time: 1.06e-01, avg batch time: 0.5118, average train loss: 3.2013
[09/15 17:19:06 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1437, average loss: 3.2399
[09/15 17:19:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 30.00	
[09/15 17:19:27 visual_prompt]: 	Test 100/190. loss: 3.332, 0.1845 s / batch. (data: 1.70e-04)max mem: 17.22456 GB 
[09/15 17:19:45 visual_prompt]: Inference (test):avg data time: 6.48e-03, avg batch time: 0.1925, average loss: 3.2977
[09/15 17:19:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 27.44	
[09/15 17:19:45 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/15 17:19:54 visual_prompt]: Epoch 42 / 100: avg data time: 1.05e-01, avg batch time: 0.5079, average train loss: 3.1203
[09/15 17:19:57 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1437, average loss: 3.0711
[09/15 17:19:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 26.50	
[09/15 17:20:18 visual_prompt]: 	Test 100/190. loss: 2.980, 0.1959 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 17:20:36 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1940, average loss: 3.0838
[09/15 17:20:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.08	top5: 27.63	
[09/15 17:20:37 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/15 17:20:45 visual_prompt]: Epoch 43 / 100: avg data time: 1.01e-01, avg batch time: 0.5049, average train loss: 3.0798
[09/15 17:20:48 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1438, average loss: 3.2244
[09/15 17:20:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 24.00	
[09/15 17:21:09 visual_prompt]: 	Test 100/190. loss: 2.969, 0.2077 s / batch. (data: 1.11e-02)max mem: 17.22456 GB 
[09/15 17:21:28 visual_prompt]: Inference (test):avg data time: 8.39e-03, avg batch time: 0.1939, average loss: 3.1930
[09/15 17:21:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 27.79	
[09/15 17:21:28 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/15 17:21:37 visual_prompt]: Epoch 44 / 100: avg data time: 9.83e-02, avg batch time: 0.5041, average train loss: 3.0974
[09/15 17:21:40 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1439, average loss: 3.0163
[09/15 17:21:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 32.00	
[09/15 17:22:01 visual_prompt]: 	Test 100/190. loss: 3.141, 0.1849 s / batch. (data: 1.44e-04)max mem: 17.22456 GB 
[09/15 17:22:19 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1938, average loss: 3.0662
[09/15 17:22:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.70	top5: 29.19	
[09/15 17:22:19 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/15 17:22:28 visual_prompt]: Epoch 45 / 100: avg data time: 1.05e-01, avg batch time: 0.5097, average train loss: 3.0525
[09/15 17:22:31 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1438, average loss: 2.9197
[09/15 17:22:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 28.00	
[09/15 17:22:52 visual_prompt]: 	Test 100/190. loss: 2.972, 0.1844 s / batch. (data: 1.24e-04)max mem: 17.22456 GB 
[09/15 17:23:11 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1941, average loss: 2.9474
[09/15 17:23:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.79	top5: 29.89	
[09/15 17:23:11 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/15 17:23:20 visual_prompt]: Epoch 46 / 100: avg data time: 1.04e-01, avg batch time: 0.5082, average train loss: 3.0329
[09/15 17:23:23 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1438, average loss: 3.1103
[09/15 17:23:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 28.50	
[09/15 17:23:44 visual_prompt]: 	Test 100/190. loss: 3.385, 0.1975 s / batch. (data: 1.42e-02)max mem: 17.22456 GB 
[09/15 17:24:02 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1937, average loss: 3.1384
[09/15 17:24:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.32	top5: 29.23	
[09/15 17:24:02 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/15 17:24:11 visual_prompt]: Epoch 47 / 100: avg data time: 8.80e-02, avg batch time: 0.4956, average train loss: 3.0609
[09/15 17:24:14 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1438, average loss: 3.0150
[09/15 17:24:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.50	
[09/15 17:24:35 visual_prompt]: 	Test 100/190. loss: 3.032, 0.1992 s / batch. (data: 1.59e-02)max mem: 17.22456 GB 
[09/15 17:24:54 visual_prompt]: Inference (test):avg data time: 7.02e-03, avg batch time: 0.1937, average loss: 3.0329
[09/15 17:24:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.53	top5: 28.17	
[09/15 17:24:54 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/15 17:25:03 visual_prompt]: Epoch 48 / 100: avg data time: 9.96e-02, avg batch time: 0.5028, average train loss: 3.0059
[09/15 17:25:05 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1437, average loss: 3.0039
[09/15 17:25:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 32.00	
[09/15 17:25:26 visual_prompt]: 	Test 100/190. loss: 3.061, 0.2078 s / batch. (data: 2.47e-02)max mem: 17.22456 GB 
[09/15 17:25:45 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1935, average loss: 3.1039
[09/15 17:25:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.61	top5: 28.81	
[09/15 17:25:45 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/15 17:25:54 visual_prompt]: Epoch 49 / 100: avg data time: 9.81e-02, avg batch time: 0.5014, average train loss: 3.0209
[09/15 17:25:57 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1439, average loss: 2.9468
[09/15 17:25:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 31.50	
[09/15 17:26:18 visual_prompt]: 	Test 100/190. loss: 2.934, 0.1842 s / batch. (data: 1.48e-04)max mem: 17.22456 GB 
[09/15 17:26:37 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1952, average loss: 2.9563
[09/15 17:26:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.49	top5: 29.11	
[09/15 17:26:37 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/15 17:26:46 visual_prompt]: Epoch 50 / 100: avg data time: 9.65e-02, avg batch time: 0.5018, average train loss: 3.0496
[09/15 17:26:49 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1436, average loss: 2.8587
[09/15 17:26:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 44.50	
[09/15 17:27:10 visual_prompt]: 	Test 100/190. loss: 2.818, 0.1991 s / batch. (data: 1.60e-02)max mem: 17.22456 GB 
[09/15 17:27:28 visual_prompt]: Inference (test):avg data time: 9.28e-03, avg batch time: 0.1947, average loss: 2.9136
[09/15 17:27:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.30	top5: 35.19	
[09/15 17:27:28 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/15 17:27:37 visual_prompt]: Epoch 51 / 100: avg data time: 8.50e-02, avg batch time: 0.4942, average train loss: 2.9498
[09/15 17:27:40 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1462, average loss: 2.8950
[09/15 17:27:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 37.50	
[09/15 17:28:01 visual_prompt]: 	Test 100/190. loss: 2.900, 0.1888 s / batch. (data: 1.41e-04)max mem: 17.22456 GB 
[09/15 17:28:20 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1935, average loss: 2.9333
[09/15 17:28:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.94	top5: 32.02	
[09/15 17:28:20 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/15 17:28:29 visual_prompt]: Epoch 52 / 100: avg data time: 1.00e-01, avg batch time: 0.5077, average train loss: 3.0151
[09/15 17:28:31 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1438, average loss: 3.0635
[09/15 17:28:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 34.50	
[09/15 17:28:53 visual_prompt]: 	Test 100/190. loss: 3.085, 0.1958 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 17:29:11 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1936, average loss: 3.1532
[09/15 17:29:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.06	top5: 30.44	
[09/15 17:29:11 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/15 17:29:20 visual_prompt]: Epoch 53 / 100: avg data time: 1.02e-01, avg batch time: 0.5063, average train loss: 3.0070
[09/15 17:29:23 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1451, average loss: 3.0568
[09/15 17:29:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 30.50	
[09/15 17:29:44 visual_prompt]: 	Test 100/190. loss: 3.003, 0.1851 s / batch. (data: 4.18e-04)max mem: 17.22456 GB 
[09/15 17:30:03 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1943, average loss: 3.0477
[09/15 17:30:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.76	top5: 32.31	
[09/15 17:30:03 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/15 17:30:12 visual_prompt]: Epoch 54 / 100: avg data time: 1.05e-01, avg batch time: 0.5080, average train loss: 2.9768
[09/15 17:30:15 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1439, average loss: 2.9149
[09/15 17:30:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 39.00	
[09/15 17:30:36 visual_prompt]: 	Test 100/190. loss: 2.977, 0.1961 s / batch. (data: 1.26e-02)max mem: 17.22456 GB 
[09/15 17:30:54 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1938, average loss: 2.9776
[09/15 17:30:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.23	top5: 35.19	
[09/15 17:30:54 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/15 17:31:03 visual_prompt]: Epoch 55 / 100: avg data time: 9.48e-02, avg batch time: 0.5006, average train loss: 3.0104
[09/15 17:31:06 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1439, average loss: 2.8112
[09/15 17:31:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 37.00	
[09/15 17:31:28 visual_prompt]: 	Test 100/190. loss: 2.841, 0.1839 s / batch. (data: 1.19e-04)max mem: 17.22456 GB 
[09/15 17:31:46 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1945, average loss: 2.8853
[09/15 17:31:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.41	top5: 32.22	
[09/15 17:31:46 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/15 17:31:55 visual_prompt]: Epoch 56 / 100: avg data time: 8.90e-02, avg batch time: 0.4957, average train loss: 2.8936
[09/15 17:31:58 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1437, average loss: 2.8672
[09/15 17:31:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 35.00	
[09/15 17:32:19 visual_prompt]: 	Test 100/190. loss: 2.847, 0.1991 s / batch. (data: 1.55e-02)max mem: 17.22456 GB 
[09/15 17:32:37 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1939, average loss: 2.9701
[09/15 17:32:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.09	top5: 33.20	
[09/15 17:32:37 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/15 17:32:46 visual_prompt]: Epoch 57 / 100: avg data time: 1.09e-01, avg batch time: 0.5148, average train loss: 2.8690
[09/15 17:32:49 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1438, average loss: 2.8091
[09/15 17:32:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.50	top5: 41.00	
[09/15 17:33:10 visual_prompt]: 	Test 100/190. loss: 2.891, 0.1970 s / batch. (data: 1.48e-04)max mem: 17.22456 GB 
[09/15 17:33:29 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1939, average loss: 2.8992
[09/15 17:33:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.37	top5: 37.50	
[09/15 17:33:29 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/15 17:33:38 visual_prompt]: Epoch 58 / 100: avg data time: 1.02e-01, avg batch time: 0.5091, average train loss: 2.9006
[09/15 17:33:41 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1438, average loss: 2.8000
[09/15 17:33:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.00	top5: 37.50	
[09/15 17:34:02 visual_prompt]: 	Test 100/190. loss: 2.905, 0.1983 s / batch. (data: 1.26e-02)max mem: 17.22456 GB 
[09/15 17:34:20 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1935, average loss: 2.9027
[09/15 17:34:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 35.79	
[09/15 17:34:20 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/15 17:34:29 visual_prompt]: Epoch 59 / 100: avg data time: 9.19e-02, avg batch time: 0.4976, average train loss: 2.9467
[09/15 17:34:32 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1437, average loss: 2.8697
[09/15 17:34:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.00	top5: 43.50	
[09/15 17:34:53 visual_prompt]: 	Test 100/190. loss: 2.990, 0.1960 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 17:35:12 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1947, average loss: 2.9502
[09/15 17:35:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.93	top5: 34.79	
[09/15 17:35:12 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/15 17:35:21 visual_prompt]: Epoch 60 / 100: avg data time: 1.08e-01, avg batch time: 0.5119, average train loss: 2.9501
[09/15 17:35:24 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1437, average loss: 2.8164
[09/15 17:35:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.00	top5: 38.00	
[09/15 17:35:45 visual_prompt]: 	Test 100/190. loss: 2.901, 0.1846 s / batch. (data: 1.35e-04)max mem: 17.22456 GB 
[09/15 17:36:03 visual_prompt]: Inference (test):avg data time: 8.78e-03, avg batch time: 0.1945, average loss: 2.9172
[09/15 17:36:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.74	top5: 35.08	
[09/15 17:36:03 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/15 17:36:12 visual_prompt]: Epoch 61 / 100: avg data time: 9.55e-02, avg batch time: 0.4992, average train loss: 2.9740
[09/15 17:36:15 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1437, average loss: 2.7987
[09/15 17:36:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 42.00	
[09/15 17:36:36 visual_prompt]: 	Test 100/190. loss: 2.859, 0.2118 s / batch. (data: 2.80e-02)max mem: 17.22456 GB 
[09/15 17:36:55 visual_prompt]: Inference (test):avg data time: 8.57e-03, avg batch time: 0.1948, average loss: 2.9390
[09/15 17:36:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.51	top5: 34.40	
[09/15 17:36:55 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/15 17:37:04 visual_prompt]: Epoch 62 / 100: avg data time: 8.91e-02, avg batch time: 0.4975, average train loss: 2.8355
[09/15 17:37:07 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1440, average loss: 2.9372
[09/15 17:37:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 38.00	
[09/15 17:37:27 visual_prompt]: 	Test 100/190. loss: 3.149, 0.1863 s / batch. (data: 1.42e-04)max mem: 17.22456 GB 
[09/15 17:37:46 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1934, average loss: 3.1970
[09/15 17:37:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.18	top5: 32.74	
[09/15 17:37:46 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/15 17:37:55 visual_prompt]: Epoch 63 / 100: avg data time: 1.09e-01, avg batch time: 0.5126, average train loss: 2.8665
[09/15 17:37:58 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1438, average loss: 2.8539
[09/15 17:37:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 41.50	
[09/15 17:38:19 visual_prompt]: 	Test 100/190. loss: 2.906, 0.1988 s / batch. (data: 1.59e-02)max mem: 17.22456 GB 
[09/15 17:38:38 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1934, average loss: 2.9240
[09/15 17:38:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.95	top5: 36.74	
[09/15 17:38:38 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/15 17:38:47 visual_prompt]: Epoch 64 / 100: avg data time: 1.06e-01, avg batch time: 0.5117, average train loss: 2.8338
[09/15 17:38:50 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1437, average loss: 2.7469
[09/15 17:38:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.00	top5: 45.00	
[09/15 17:39:11 visual_prompt]: 	Test 100/190. loss: 2.825, 0.2075 s / batch. (data: 2.45e-02)max mem: 17.22456 GB 
[09/15 17:39:29 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1944, average loss: 2.8512
[09/15 17:39:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.76	top5: 39.10	
[09/15 17:39:30 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/15 17:39:39 visual_prompt]: Epoch 65 / 100: avg data time: 9.85e-02, avg batch time: 0.5083, average train loss: 2.8127
[09/15 17:39:41 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1438, average loss: 2.8615
[09/15 17:39:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 37.00	
[09/15 17:40:02 visual_prompt]: 	Test 100/190. loss: 2.817, 0.1971 s / batch. (data: 8.70e-05)max mem: 17.22456 GB 
[09/15 17:40:21 visual_prompt]: Inference (test):avg data time: 6.56e-03, avg batch time: 0.1935, average loss: 2.9020
[09/15 17:40:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.05	top5: 37.63	
[09/15 17:40:21 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/15 17:40:30 visual_prompt]: Epoch 66 / 100: avg data time: 1.11e-01, avg batch time: 0.5154, average train loss: 2.7735
[09/15 17:40:33 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1437, average loss: 2.7100
[09/15 17:40:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 48.50	
[09/15 17:40:54 visual_prompt]: 	Test 100/190. loss: 2.808, 0.1982 s / batch. (data: 1.51e-02)max mem: 17.22456 GB 
[09/15 17:41:12 visual_prompt]: Inference (test):avg data time: 6.41e-03, avg batch time: 0.1933, average loss: 2.8808
[09/15 17:41:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.67	top5: 39.55	
[09/15 17:41:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/15 17:41:22 visual_prompt]: Epoch 67 / 100: avg data time: 1.08e-01, avg batch time: 0.5105, average train loss: 2.7691
[09/15 17:41:24 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1440, average loss: 2.7205
[09/15 17:41:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.00	top5: 48.00	
[09/15 17:41:45 visual_prompt]: 	Test 100/190. loss: 2.740, 0.1925 s / batch. (data: 1.25e-04)max mem: 17.22456 GB 
[09/15 17:42:04 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1936, average loss: 2.8531
[09/15 17:42:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.34	top5: 39.78	
[09/15 17:42:04 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/15 17:42:13 visual_prompt]: Epoch 68 / 100: avg data time: 9.93e-02, avg batch time: 0.5059, average train loss: 2.7406
[09/15 17:42:16 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1439, average loss: 2.5974
[09/15 17:42:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.50	top5: 52.50	
[09/15 17:42:37 visual_prompt]: 	Test 100/190. loss: 3.019, 0.1844 s / batch. (data: 1.27e-04)max mem: 17.22456 GB 
[09/15 17:42:55 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1940, average loss: 2.8504
[09/15 17:42:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.66	top5: 45.13	
[09/15 17:42:55 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/15 17:43:04 visual_prompt]: Epoch 69 / 100: avg data time: 1.02e-01, avg batch time: 0.5080, average train loss: 2.7493
[09/15 17:43:07 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1439, average loss: 2.7027
[09/15 17:43:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.00	top5: 47.50	
[09/15 17:43:28 visual_prompt]: 	Test 100/190. loss: 2.780, 0.1842 s / batch. (data: 1.36e-04)max mem: 17.22456 GB 
[09/15 17:43:47 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1937, average loss: 2.8234
[09/15 17:43:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.01	top5: 44.05	
[09/15 17:43:47 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/15 17:43:56 visual_prompt]: Epoch 70 / 100: avg data time: 1.10e-01, avg batch time: 0.5147, average train loss: 2.6617
[09/15 17:43:59 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1438, average loss: 2.6453
[09/15 17:43:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.50	top5: 52.50	
[09/15 17:44:20 visual_prompt]: 	Test 100/190. loss: 2.917, 0.1844 s / batch. (data: 1.02e-04)max mem: 17.22456 GB 
[09/15 17:44:39 visual_prompt]: Inference (test):avg data time: 8.31e-03, avg batch time: 0.1945, average loss: 2.9247
[09/15 17:44:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.73	top5: 46.47	
[09/15 17:44:39 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/15 17:44:48 visual_prompt]: Epoch 71 / 100: avg data time: 1.04e-01, avg batch time: 0.5077, average train loss: 2.6719
[09/15 17:44:51 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1437, average loss: 2.6646
[09/15 17:44:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.00	top5: 48.50	
[09/15 17:45:12 visual_prompt]: 	Test 100/190. loss: 2.753, 0.1993 s / batch. (data: 1.60e-02)max mem: 17.22456 GB 
[09/15 17:45:30 visual_prompt]: Inference (test):avg data time: 6.85e-03, avg batch time: 0.1935, average loss: 2.7749
[09/15 17:45:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.36	top5: 44.54	
[09/15 17:45:30 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/15 17:45:39 visual_prompt]: Epoch 72 / 100: avg data time: 1.04e-01, avg batch time: 0.5059, average train loss: 2.6450
[09/15 17:45:42 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1441, average loss: 2.5530
[09/15 17:45:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.50	top5: 56.00	
[09/15 17:46:03 visual_prompt]: 	Test 100/190. loss: 2.691, 0.1861 s / batch. (data: 1.39e-04)max mem: 17.22456 GB 
[09/15 17:46:21 visual_prompt]: Inference (test):avg data time: 6.14e-03, avg batch time: 0.1920, average loss: 2.7164
[09/15 17:46:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.13	top5: 52.16	
[09/15 17:46:21 visual_prompt]: Best epoch 72: best metric: 0.175
[09/15 17:46:21 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/15 17:46:31 visual_prompt]: Epoch 73 / 100: avg data time: 1.09e-01, avg batch time: 0.5145, average train loss: 2.6195
[09/15 17:46:33 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1438, average loss: 2.5297
[09/15 17:46:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.00	top5: 55.50	
[09/15 17:46:55 visual_prompt]: 	Test 100/190. loss: 2.782, 0.1844 s / batch. (data: 1.33e-04)max mem: 17.22456 GB 
[09/15 17:47:13 visual_prompt]: Inference (test):avg data time: 8.50e-03, avg batch time: 0.1944, average loss: 2.7487
[09/15 17:47:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.19	top5: 50.35	
[09/15 17:47:13 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/15 17:47:22 visual_prompt]: Epoch 74 / 100: avg data time: 9.04e-02, avg batch time: 0.4952, average train loss: 2.5739
[09/15 17:47:25 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1439, average loss: 2.4557
[09/15 17:47:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.00	top5: 60.00	
[09/15 17:47:46 visual_prompt]: 	Test 100/190. loss: 2.848, 0.1962 s / batch. (data: 1.25e-02)max mem: 17.22456 GB 
[09/15 17:48:04 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1943, average loss: 2.7403
[09/15 17:48:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.91	top5: 50.72	
[09/15 17:48:04 visual_prompt]: Best epoch 74: best metric: 0.190
[09/15 17:48:04 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/15 17:48:13 visual_prompt]: Epoch 75 / 100: avg data time: 8.73e-02, avg batch time: 0.4929, average train loss: 2.6032
[09/15 17:48:16 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1437, average loss: 2.5620
[09/15 17:48:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.50	top5: 57.00	
[09/15 17:48:37 visual_prompt]: 	Test 100/190. loss: 2.867, 0.1987 s / batch. (data: 1.35e-04)max mem: 17.22456 GB 
[09/15 17:48:55 visual_prompt]: Inference (test):avg data time: 6.67e-03, avg batch time: 0.1930, average loss: 2.7655
[09/15 17:48:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.69	top5: 47.87	
[09/15 17:48:55 visual_prompt]: Best epoch 75: best metric: 0.195
[09/15 17:48:55 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/15 17:49:04 visual_prompt]: Epoch 76 / 100: avg data time: 1.04e-01, avg batch time: 0.5089, average train loss: 2.5468
[09/15 17:49:07 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1440, average loss: 2.3418
[09/15 17:49:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.50	top5: 67.50	
[09/15 17:49:28 visual_prompt]: 	Test 100/190. loss: 2.636, 0.1839 s / batch. (data: 1.67e-04)max mem: 17.22456 GB 
[09/15 17:49:46 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1927, average loss: 2.6240
[09/15 17:49:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.88	top5: 57.08	
[09/15 17:49:47 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/15 17:49:56 visual_prompt]: Epoch 77 / 100: avg data time: 1.05e-01, avg batch time: 0.5107, average train loss: 2.4666
[09/15 17:49:59 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1449, average loss: 2.4233
[09/15 17:49:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.00	top5: 66.00	
[09/15 17:50:20 visual_prompt]: 	Test 100/190. loss: 2.561, 0.2018 s / batch. (data: 1.47e-02)max mem: 17.22456 GB 
[09/15 17:50:38 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1938, average loss: 2.6993
[09/15 17:50:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.63	top5: 53.62	
[09/15 17:50:38 visual_prompt]: Best epoch 77: best metric: 0.200
[09/15 17:50:38 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/15 17:50:47 visual_prompt]: Epoch 78 / 100: avg data time: 8.32e-02, avg batch time: 0.4909, average train loss: 2.4176
[09/15 17:50:50 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1437, average loss: 2.3306
[09/15 17:50:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.00	top5: 71.50	
[09/15 17:51:11 visual_prompt]: 	Test 100/190. loss: 2.668, 0.1842 s / batch. (data: 1.44e-04)max mem: 17.22456 GB 
[09/15 17:51:29 visual_prompt]: Inference (test):avg data time: 6.81e-03, avg batch time: 0.1928, average loss: 2.6555
[09/15 17:51:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.14	top5: 57.44	
[09/15 17:51:29 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/15 17:51:38 visual_prompt]: Epoch 79 / 100: avg data time: 8.36e-02, avg batch time: 0.4893, average train loss: 2.3903
[09/15 17:51:41 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1438, average loss: 2.4418
[09/15 17:51:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.00	top5: 63.00	
[09/15 17:52:02 visual_prompt]: 	Test 100/190. loss: 2.791, 0.2112 s / batch. (data: 2.80e-02)max mem: 17.22456 GB 
[09/15 17:52:20 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1936, average loss: 2.8734
[09/15 17:52:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.77	top5: 52.21	
[09/15 17:52:20 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/15 17:52:30 visual_prompt]: Epoch 80 / 100: avg data time: 1.07e-01, avg batch time: 0.5140, average train loss: 2.3549
[09/15 17:52:32 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1441, average loss: 2.4142
[09/15 17:52:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.50	top5: 66.00	
[09/15 17:52:53 visual_prompt]: 	Test 100/190. loss: 2.859, 0.1969 s / batch. (data: 1.28e-02)max mem: 17.22456 GB 
[09/15 17:53:12 visual_prompt]: Inference (test):avg data time: 8.39e-03, avg batch time: 0.1939, average loss: 2.7972
[09/15 17:53:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.96	top5: 55.14	
[09/15 17:53:12 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/15 17:53:21 visual_prompt]: Epoch 81 / 100: avg data time: 9.91e-02, avg batch time: 0.5032, average train loss: 2.3110
[09/15 17:53:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1439, average loss: 2.1680
[09/15 17:53:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.50	top5: 76.50	
[09/15 17:53:45 visual_prompt]: 	Test 100/190. loss: 2.586, 0.2081 s / batch. (data: 2.46e-02)max mem: 17.22456 GB 
[09/15 17:54:04 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1941, average loss: 2.6704
[09/15 17:54:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.13	top5: 59.98	
[09/15 17:54:04 visual_prompt]: Best epoch 81: best metric: 0.215
[09/15 17:54:04 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/15 17:54:13 visual_prompt]: Epoch 82 / 100: avg data time: 9.62e-02, avg batch time: 0.5040, average train loss: 2.1963
[09/15 17:54:15 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1437, average loss: 2.1284
[09/15 17:54:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.50	top5: 80.50	
[09/15 17:54:37 visual_prompt]: 	Test 100/190. loss: 2.693, 0.1967 s / batch. (data: 1.33e-02)max mem: 17.22456 GB 
[09/15 17:54:55 visual_prompt]: Inference (test):avg data time: 8.32e-03, avg batch time: 0.1949, average loss: 2.6654
[09/15 17:54:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.47	top5: 60.34	
[09/15 17:54:55 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/15 17:55:04 visual_prompt]: Epoch 83 / 100: avg data time: 9.96e-02, avg batch time: 0.5033, average train loss: 2.1536
[09/15 17:55:07 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1436, average loss: 2.1399
[09/15 17:55:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.50	top5: 77.00	
[09/15 17:55:28 visual_prompt]: 	Test 100/190. loss: 2.639, 0.1845 s / batch. (data: 1.11e-04)max mem: 17.22456 GB 
[09/15 17:55:47 visual_prompt]: Inference (test):avg data time: 6.37e-03, avg batch time: 0.1930, average loss: 2.6615
[09/15 17:55:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.05	top5: 61.78	
[09/15 17:55:47 visual_prompt]: Best epoch 83: best metric: 0.225
[09/15 17:55:47 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/15 17:55:56 visual_prompt]: Epoch 84 / 100: avg data time: 9.66e-02, avg batch time: 0.5020, average train loss: 2.0833
[09/15 17:55:59 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1438, average loss: 1.9880
[09/15 17:55:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 34.00	top5: 81.50	
[09/15 17:56:20 visual_prompt]: 	Test 100/190. loss: 2.702, 0.2113 s / batch. (data: 2.82e-02)max mem: 17.22456 GB 
[09/15 17:56:38 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1934, average loss: 2.7364
[09/15 17:56:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.70	top5: 62.20	
[09/15 17:56:38 visual_prompt]: Best epoch 84: best metric: 0.340
[09/15 17:56:38 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/15 17:56:47 visual_prompt]: Epoch 85 / 100: avg data time: 1.03e-01, avg batch time: 0.5066, average train loss: 2.1152
[09/15 17:56:50 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1438, average loss: 2.0670
[09/15 17:56:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.00	top5: 80.50	
[09/15 17:57:11 visual_prompt]: 	Test 100/190. loss: 2.790, 0.1843 s / batch. (data: 1.26e-04)max mem: 17.22456 GB 
[09/15 17:57:30 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1941, average loss: 2.7336
[09/15 17:57:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.27	top5: 61.01	
[09/15 17:57:30 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/15 17:57:39 visual_prompt]: Epoch 86 / 100: avg data time: 1.08e-01, avg batch time: 0.5132, average train loss: 2.0415
[09/15 17:57:42 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1439, average loss: 1.9836
[09/15 17:57:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 34.00	top5: 82.00	
[09/15 17:58:03 visual_prompt]: 	Test 100/190. loss: 2.694, 0.1853 s / batch. (data: 1.21e-04)max mem: 17.22456 GB 
[09/15 17:58:21 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1939, average loss: 2.8080
[09/15 17:58:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.99	top5: 60.30	
[09/15 17:58:21 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/15 17:58:30 visual_prompt]: Epoch 87 / 100: avg data time: 1.02e-01, avg batch time: 0.5051, average train loss: 1.9467
[09/15 17:58:33 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1440, average loss: 2.1066
[09/15 17:58:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.50	top5: 78.50	
[09/15 17:58:54 visual_prompt]: 	Test 100/190. loss: 2.885, 0.1977 s / batch. (data: 1.44e-02)max mem: 17.22456 GB 
[09/15 17:59:13 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1944, average loss: 2.9716
[09/15 17:59:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.95	top5: 60.45	
[09/15 17:59:13 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/15 17:59:21 visual_prompt]: Epoch 88 / 100: avg data time: 9.18e-02, avg batch time: 0.5001, average train loss: 1.8721
[09/15 17:59:24 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1438, average loss: 1.9320
[09/15 17:59:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.50	top5: 87.50	
[09/15 17:59:46 visual_prompt]: 	Test 100/190. loss: 3.060, 0.1844 s / batch. (data: 1.25e-04)max mem: 17.22456 GB 
[09/15 18:00:04 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1939, average loss: 2.9289
[09/15 18:00:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.47	top5: 62.36	
[09/15 18:00:04 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/15 18:00:13 visual_prompt]: Epoch 89 / 100: avg data time: 9.11e-02, avg batch time: 0.4984, average train loss: 1.8267
[09/15 18:00:16 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1438, average loss: 1.8571
[09/15 18:00:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.50	top5: 86.00	
[09/15 18:00:37 visual_prompt]: 	Test 100/190. loss: 3.021, 0.1848 s / batch. (data: 1.49e-04)max mem: 17.22456 GB 
[09/15 18:00:55 visual_prompt]: Inference (test):avg data time: 7.02e-03, avg batch time: 0.1931, average loss: 2.9579
[09/15 18:00:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.56	top5: 61.22	
[09/15 18:00:55 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/15 18:01:04 visual_prompt]: Epoch 90 / 100: avg data time: 1.03e-01, avg batch time: 0.5087, average train loss: 1.7435
[09/15 18:01:07 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1436, average loss: 1.7862
[09/15 18:01:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 32.00	top5: 88.00	
[09/15 18:01:28 visual_prompt]: 	Test 100/190. loss: 3.033, 0.1854 s / batch. (data: 1.34e-04)max mem: 17.22456 GB 
[09/15 18:01:47 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1933, average loss: 3.1066
[09/15 18:01:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.80	top5: 62.00	
[09/15 18:01:47 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/15 18:01:56 visual_prompt]: Epoch 91 / 100: avg data time: 1.04e-01, avg batch time: 0.5088, average train loss: 1.6734
[09/15 18:01:59 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1437, average loss: 1.6402
[09/15 18:01:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 36.50	top5: 91.50	
[09/15 18:02:20 visual_prompt]: 	Test 100/190. loss: 3.143, 0.1909 s / batch. (data: 7.27e-03)max mem: 17.22456 GB 
[09/15 18:02:38 visual_prompt]: Inference (test):avg data time: 7.44e-03, avg batch time: 0.1931, average loss: 3.2126
[09/15 18:02:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.57	top5: 61.51	
[09/15 18:02:38 visual_prompt]: Best epoch 91: best metric: 0.365
[09/15 18:02:38 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/15 18:02:47 visual_prompt]: Epoch 92 / 100: avg data time: 8.81e-02, avg batch time: 0.4936, average train loss: 1.6354
[09/15 18:02:50 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1439, average loss: 1.5817
[09/15 18:02:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 40.50	top5: 90.50	
[09/15 18:03:11 visual_prompt]: 	Test 100/190. loss: 3.229, 0.1961 s / batch. (data: 1.53e-04)max mem: 17.22456 GB 
[09/15 18:03:29 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1943, average loss: 3.2548
[09/15 18:03:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.19	top5: 63.28	
[09/15 18:03:29 visual_prompt]: Best epoch 92: best metric: 0.405
[09/15 18:03:29 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/15 18:03:38 visual_prompt]: Epoch 93 / 100: avg data time: 1.05e-01, avg batch time: 0.5097, average train loss: 1.5556
[09/15 18:03:41 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1437, average loss: 1.6776
[09/15 18:03:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 39.50	top5: 93.00	
[09/15 18:04:03 visual_prompt]: 	Test 100/190. loss: 3.415, 0.1846 s / batch. (data: 8.51e-05)max mem: 17.22456 GB 
[09/15 18:04:21 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1945, average loss: 3.4085
[09/15 18:04:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.61	top5: 61.57	
[09/15 18:04:21 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/15 18:04:30 visual_prompt]: Epoch 94 / 100: avg data time: 1.01e-01, avg batch time: 0.5067, average train loss: 1.5444
[09/15 18:04:33 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1438, average loss: 1.5054
[09/15 18:04:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 41.50	top5: 93.50	
[09/15 18:04:54 visual_prompt]: 	Test 100/190. loss: 3.435, 0.1899 s / batch. (data: 1.31e-04)max mem: 17.22456 GB 
[09/15 18:05:12 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1937, average loss: 3.4129
[09/15 18:05:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.76	top5: 62.88	
[09/15 18:05:12 visual_prompt]: Best epoch 94: best metric: 0.415
[09/15 18:05:12 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/15 18:05:21 visual_prompt]: Epoch 95 / 100: avg data time: 1.06e-01, avg batch time: 0.5090, average train loss: 1.4780
[09/15 18:05:24 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1443, average loss: 1.6625
[09/15 18:05:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 39.50	top5: 92.00	
[09/15 18:05:45 visual_prompt]: 	Test 100/190. loss: 3.684, 0.1840 s / batch. (data: 1.37e-04)max mem: 17.22456 GB 
[09/15 18:06:04 visual_prompt]: Inference (test):avg data time: 7.04e-03, avg batch time: 0.1930, average loss: 3.7757
[09/15 18:06:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.16	top5: 61.21	
[09/15 18:06:04 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/15 18:06:13 visual_prompt]: Epoch 96 / 100: avg data time: 1.06e-01, avg batch time: 0.5104, average train loss: 1.4200
[09/15 18:06:16 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1437, average loss: 1.5178
[09/15 18:06:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 44.00	top5: 92.50	
[09/15 18:06:37 visual_prompt]: 	Test 100/190. loss: 3.664, 0.1958 s / batch. (data: 1.26e-02)max mem: 17.22456 GB 
[09/15 18:06:55 visual_prompt]: Inference (test):avg data time: 9.12e-03, avg batch time: 0.1949, average loss: 3.7243
[09/15 18:06:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.88	top5: 62.43	
[09/15 18:06:55 visual_prompt]: Best epoch 96: best metric: 0.440
[09/15 18:06:55 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/15 18:07:04 visual_prompt]: Epoch 97 / 100: avg data time: 8.62e-02, avg batch time: 0.4931, average train loss: 1.3735
[09/15 18:07:07 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1439, average loss: 1.4929
[09/15 18:07:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 43.50	top5: 93.50	
[09/15 18:07:28 visual_prompt]: 	Test 100/190. loss: 3.676, 0.1905 s / batch. (data: 1.50e-04)max mem: 17.22456 GB 
[09/15 18:07:46 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1938, average loss: 3.7846
[09/15 18:07:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.03	top5: 62.35	
[09/15 18:07:47 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/15 18:07:55 visual_prompt]: Epoch 98 / 100: avg data time: 9.61e-02, avg batch time: 0.5022, average train loss: 1.3213
[09/15 18:07:58 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1440, average loss: 1.5010
[09/15 18:07:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 45.00	top5: 93.00	
[09/15 18:08:20 visual_prompt]: 	Test 100/190. loss: 3.803, 0.1958 s / batch. (data: 1.22e-02)max mem: 17.22456 GB 
[09/15 18:08:38 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1939, average loss: 3.8859
[09/15 18:08:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.16	top5: 62.12	
[09/15 18:08:38 visual_prompt]: Best epoch 98: best metric: 0.450
[09/15 18:08:38 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/15 18:08:47 visual_prompt]: Epoch 99 / 100: avg data time: 1.10e-01, avg batch time: 0.5174, average train loss: 1.3000
[09/15 18:08:50 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1438, average loss: 1.4486
[09/15 18:08:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 48.00	top5: 94.00	
[09/15 18:09:11 visual_prompt]: 	Test 100/190. loss: 3.828, 0.1841 s / batch. (data: 1.49e-04)max mem: 17.22456 GB 
[09/15 18:09:29 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1933, average loss: 3.8614
[09/15 18:09:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.18	top5: 62.34	
[09/15 18:09:29 visual_prompt]: Best epoch 99: best metric: 0.480
[09/15 18:09:29 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/15 18:09:38 visual_prompt]: Epoch 100 / 100: avg data time: 9.91e-02, avg batch time: 0.5038, average train loss: 1.2915
[09/15 18:09:41 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1439, average loss: 1.4561
[09/15 18:09:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 48.00	top5: 93.50	
[09/15 18:10:02 visual_prompt]: 	Test 100/190. loss: 3.849, 0.1952 s / batch. (data: 1.17e-02)max mem: 17.22456 GB 
[09/15 18:10:21 visual_prompt]: Inference (test):avg data time: 6.96e-03, avg batch time: 0.1934, average loss: 3.8859
[09/15 18:10:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.25	top5: 62.43	
[09/15 18:10:46 visual_prompt]: Rank of current process: 0. World size: 1
[09/15 18:10:46 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/15 18:10:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed800'], train_type='')
[09/15 18:10:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/15 18:10:46 visual_prompt]: Training with config:
[09/15 18:10:46 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-smallnorb(predicted_attribute="label_azimuth")',
          'NO_TEST': False,
          'NUMBER_CLASSES': 18,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed800/vtab-smallnorb(predicted_attribute="label_azimuth")/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/15 18:10:46 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-15 18:10:47.000996: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-15 18:10:47.191154: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-15 18:10:48.082571: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 18:10:48.082660: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 18:10:48.082670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-15 18:10:50.158357: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 18:10:50.158463: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-15 18:10:50.158476: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/15 18:10:50 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
2023-09-15 18:10:50.369862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 18:10:52 visual_prompt]: Number of images: 1000
[09/15 18:10:52 visual_prompt]: Number of classes: 18 / 18
[09/15 18:10:52 visual_prompt]: Loading validation data...
[09/15 18:10:52 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 18:10:52 visual_prompt]: Number of images: 200
[09/15 18:10:52 visual_prompt]: Number of classes: 18 / 18
[09/15 18:10:52 visual_prompt]: Loading test data...
[09/15 18:10:52 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/15 18:11:09 visual_prompt]: Number of images: 12150
[09/15 18:11:09 visual_prompt]: Number of classes: 18 / 18
[09/15 18:11:09 visual_prompt]: Constructing models...
[09/15 18:11:12 visual_prompt]: Total Parameters: 86734098	 Gradient Parameters: 935442
[09/15 18:11:12 visual_prompt]: tuned percent:1.079
[09/15 18:11:15 visual_prompt]: Device used for model: 0
[09/15 18:11:15 visual_prompt]: Setting up Evalutator...
[09/15 18:11:15 visual_prompt]: Setting up Trainer...
[09/15 18:11:15 visual_prompt]: 	Setting up the optimizer...
[09/15 18:11:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/15 18:11:25 visual_prompt]: Epoch 1 / 100: avg data time: 1.06e-01, avg batch time: 0.5951, average train loss: 3.0810
[09/15 18:11:28 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1432, average loss: 3.0701
[09/15 18:11:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 27.50	
[09/15 18:11:49 visual_prompt]: 	Test 100/190. loss: 3.017, 0.1958 s / batch. (data: 1.31e-02)max mem: 17.22456 GB 
[09/15 18:12:07 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1930, average loss: 3.0930
[09/15 18:12:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.47	top5: 27.00	
[09/15 18:12:07 visual_prompt]: Best epoch 1: best metric: 0.050
[09/15 18:12:07 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/15 18:12:16 visual_prompt]: Epoch 2 / 100: avg data time: 9.40e-02, avg batch time: 0.4977, average train loss: 3.2909
[09/15 18:12:19 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1433, average loss: 2.9616
[09/15 18:12:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 33.50	
[09/15 18:12:40 visual_prompt]: 	Test 100/190. loss: 3.090, 0.1932 s / batch. (data: 1.15e-04)max mem: 17.22456 GB 
[09/15 18:12:59 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1941, average loss: 3.0132
[09/15 18:12:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.32	top5: 27.51	
[09/15 18:12:59 visual_prompt]: Best epoch 2: best metric: 0.065
[09/15 18:12:59 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/15 18:13:08 visual_prompt]: Epoch 3 / 100: avg data time: 1.12e-01, avg batch time: 0.5159, average train loss: 3.0550
[09/15 18:13:11 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1436, average loss: 3.0104
[09/15 18:13:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 30.00	
[09/15 18:13:32 visual_prompt]: 	Test 100/190. loss: 3.023, 0.1991 s / batch. (data: 1.60e-02)max mem: 17.22456 GB 
[09/15 18:13:50 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1940, average loss: 3.0626
[09/15 18:13:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.96	top5: 28.09	
[09/15 18:13:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/15 18:14:00 visual_prompt]: Epoch 4 / 100: avg data time: 1.08e-01, avg batch time: 0.5124, average train loss: 3.2262
[09/15 18:14:03 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1433, average loss: 3.0952
[09/15 18:14:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 28.50	
[09/15 18:14:24 visual_prompt]: 	Test 100/190. loss: 3.132, 0.1846 s / batch. (data: 1.30e-04)max mem: 17.22456 GB 
[09/15 18:14:42 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1939, average loss: 3.1321
[09/15 18:14:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 27.67	
[09/15 18:14:42 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/15 18:14:51 visual_prompt]: Epoch 5 / 100: avg data time: 1.08e-01, avg batch time: 0.5127, average train loss: 3.2872
[09/15 18:14:54 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1434, average loss: 3.1335
[09/15 18:14:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 30.00	
[09/15 18:15:16 visual_prompt]: 	Test 100/190. loss: 3.143, 0.1963 s / batch. (data: 1.30e-02)max mem: 17.22456 GB 
[09/15 18:15:34 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1939, average loss: 3.2047
[09/15 18:15:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.61	
[09/15 18:15:34 visual_prompt]: Best epoch 5: best metric: 0.085
[09/15 18:15:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/15 18:15:43 visual_prompt]: Epoch 6 / 100: avg data time: 9.76e-02, avg batch time: 0.5057, average train loss: 3.3028
[09/15 18:15:46 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1435, average loss: 3.1881
[09/15 18:15:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 33.00	
[09/15 18:16:07 visual_prompt]: 	Test 100/190. loss: 3.528, 0.2076 s / batch. (data: 2.44e-02)max mem: 17.22456 GB 
[09/15 18:16:25 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1930, average loss: 3.2977
[09/15 18:16:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 27.45	
[09/15 18:16:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/15 18:16:35 visual_prompt]: Epoch 7 / 100: avg data time: 1.08e-01, avg batch time: 0.5133, average train loss: 3.4111
[09/15 18:16:38 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1466, average loss: 3.3125
[09/15 18:16:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 29.50	
[09/15 18:16:59 visual_prompt]: 	Test 100/190. loss: 3.484, 0.2036 s / batch. (data: 2.03e-02)max mem: 17.22456 GB 
[09/15 18:17:17 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1933, average loss: 3.3757
[09/15 18:17:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.76	
[09/15 18:17:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/15 18:17:26 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e-01, avg batch time: 0.5086, average train loss: 3.2716
[09/15 18:17:29 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1435, average loss: 3.2311
[09/15 18:17:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 29.50	
[09/15 18:17:50 visual_prompt]: 	Test 100/190. loss: 3.055, 0.1833 s / batch. (data: 1.23e-04)max mem: 17.22456 GB 
[09/15 18:18:09 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1937, average loss: 3.2390
[09/15 18:18:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.95	
[09/15 18:18:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/15 18:18:18 visual_prompt]: Epoch 9 / 100: avg data time: 1.05e-01, avg batch time: 0.5110, average train loss: 3.9819
[09/15 18:18:21 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1439, average loss: 3.3595
[09/15 18:18:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 26.50	
[09/15 18:18:41 visual_prompt]: 	Test 100/190. loss: 3.173, 0.1861 s / batch. (data: 1.51e-04)max mem: 17.22456 GB 
[09/15 18:19:00 visual_prompt]: Inference (test):avg data time: 6.70e-03, avg batch time: 0.1930, average loss: 3.3583
[09/15 18:19:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 27.73	
[09/15 18:19:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/15 18:19:09 visual_prompt]: Epoch 10 / 100: avg data time: 1.01e-01, avg batch time: 0.5069, average train loss: 3.5474
[09/15 18:19:12 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1439, average loss: 3.6930
[09/15 18:19:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 23.50	
[09/15 18:19:33 visual_prompt]: 	Test 100/190. loss: 3.354, 0.1961 s / batch. (data: 1.13e-04)max mem: 17.22456 GB 
[09/15 18:19:51 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1937, average loss: 3.4940
[09/15 18:19:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.52	top5: 27.81	
[09/15 18:19:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/15 18:20:00 visual_prompt]: Epoch 11 / 100: avg data time: 9.81e-02, avg batch time: 0.5015, average train loss: 3.5211
[09/15 18:20:03 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1435, average loss: 3.4717
[09/15 18:20:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 31.50	
[09/15 18:20:25 visual_prompt]: 	Test 100/190. loss: 3.146, 0.2114 s / batch. (data: 2.85e-02)max mem: 17.22456 GB 
[09/15 18:20:43 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1944, average loss: 3.4789
[09/15 18:20:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 27.56	
[09/15 18:20:43 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/15 18:20:52 visual_prompt]: Epoch 12 / 100: avg data time: 1.07e-01, avg batch time: 0.5111, average train loss: 3.3130
[09/15 18:20:55 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1435, average loss: 3.2375
[09/15 18:20:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.50	
[09/15 18:21:16 visual_prompt]: 	Test 100/190. loss: 3.294, 0.1956 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 18:21:35 visual_prompt]: Inference (test):avg data time: 8.37e-03, avg batch time: 0.1948, average loss: 3.2551
[09/15 18:21:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 27.83	
[09/15 18:21:35 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/15 18:21:44 visual_prompt]: Epoch 13 / 100: avg data time: 1.00e-01, avg batch time: 0.5042, average train loss: 3.1770
[09/15 18:21:47 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1447, average loss: 3.1324
[09/15 18:21:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 27.00	
[09/15 18:22:08 visual_prompt]: 	Test 100/190. loss: 3.151, 0.1842 s / batch. (data: 1.59e-04)max mem: 17.22456 GB 
[09/15 18:22:26 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1948, average loss: 3.1928
[09/15 18:22:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.32	top5: 27.16	
[09/15 18:22:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/15 18:22:35 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e-01, avg batch time: 0.5041, average train loss: 3.2576
[09/15 18:22:38 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1435, average loss: 3.0711
[09/15 18:22:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 30.00	
[09/15 18:23:00 visual_prompt]: 	Test 100/190. loss: 3.127, 0.1894 s / batch. (data: 9.61e-05)max mem: 17.22456 GB 
[09/15 18:23:18 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1931, average loss: 3.1024
[09/15 18:23:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.88	
[09/15 18:23:18 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/15 18:23:27 visual_prompt]: Epoch 15 / 100: avg data time: 1.17e-01, avg batch time: 0.5190, average train loss: 3.2272
[09/15 18:23:30 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1434, average loss: 3.2787
[09/15 18:23:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 34.50	
[09/15 18:23:51 visual_prompt]: 	Test 100/190. loss: 3.213, 0.2107 s / batch. (data: 2.73e-02)max mem: 17.22456 GB 
[09/15 18:24:10 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1941, average loss: 3.3731
[09/15 18:24:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.47	top5: 27.75	
[09/15 18:24:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/15 18:24:19 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e-01, avg batch time: 0.5079, average train loss: 3.2604
[09/15 18:24:22 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1450, average loss: 3.3992
[09/15 18:24:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 27.50	
[09/15 18:24:43 visual_prompt]: 	Test 100/190. loss: 3.559, 0.1989 s / batch. (data: 1.53e-02)max mem: 17.22456 GB 
[09/15 18:25:01 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1936, average loss: 3.3679
[09/15 18:25:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.89	top5: 28.04	
[09/15 18:25:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/15 18:25:10 visual_prompt]: Epoch 17 / 100: avg data time: 9.98e-02, avg batch time: 0.5047, average train loss: 3.2838
[09/15 18:25:13 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1435, average loss: 3.1600
[09/15 18:25:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.00	top5: 26.50	
[09/15 18:25:34 visual_prompt]: 	Test 100/190. loss: 3.121, 0.2197 s / batch. (data: 1.43e-02)max mem: 17.22456 GB 
[09/15 18:25:53 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1931, average loss: 3.1292
[09/15 18:25:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.66	top5: 28.21	
[09/15 18:25:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/15 18:26:02 visual_prompt]: Epoch 18 / 100: avg data time: 1.20e-01, avg batch time: 0.5232, average train loss: 3.1971
[09/15 18:26:05 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1436, average loss: 3.0664
[09/15 18:26:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 32.00	
[09/15 18:26:26 visual_prompt]: 	Test 100/190. loss: 3.300, 0.1842 s / batch. (data: 1.42e-04)max mem: 17.22456 GB 
[09/15 18:26:45 visual_prompt]: Inference (test):avg data time: 6.17e-03, avg batch time: 0.1929, average loss: 3.2319
[09/15 18:26:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.32	top5: 27.28	
[09/15 18:26:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/15 18:26:54 visual_prompt]: Epoch 19 / 100: avg data time: 1.07e-01, avg batch time: 0.5089, average train loss: 3.2793
[09/15 18:26:57 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1434, average loss: 3.0449
[09/15 18:26:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 28.00	
[09/15 18:27:18 visual_prompt]: 	Test 100/190. loss: 2.939, 0.2036 s / batch. (data: 2.04e-02)max mem: 17.22456 GB 
[09/15 18:27:36 visual_prompt]: Inference (test):avg data time: 8.26e-03, avg batch time: 0.1943, average loss: 3.0659
[09/15 18:27:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 29.90	
[09/15 18:27:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/15 18:27:45 visual_prompt]: Epoch 20 / 100: avg data time: 1.13e-01, avg batch time: 0.5161, average train loss: 3.2098
[09/15 18:27:48 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1433, average loss: 3.2021
[09/15 18:27:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 30.50	
[09/15 18:28:10 visual_prompt]: 	Test 100/190. loss: 3.240, 0.1986 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 18:28:28 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1936, average loss: 3.2413
[09/15 18:28:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.39	top5: 27.47	
[09/15 18:28:28 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/15 18:28:37 visual_prompt]: Epoch 21 / 100: avg data time: 9.91e-02, avg batch time: 0.5036, average train loss: 3.2666
[09/15 18:28:40 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1435, average loss: 3.0375
[09/15 18:28:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 30.50	
[09/15 18:29:01 visual_prompt]: 	Test 100/190. loss: 3.123, 0.2000 s / batch. (data: 1.46e-02)max mem: 17.22456 GB 
[09/15 18:29:20 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1940, average loss: 3.1186
[09/15 18:29:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.95	top5: 27.72	
[09/15 18:29:20 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/15 18:29:29 visual_prompt]: Epoch 22 / 100: avg data time: 1.09e-01, avg batch time: 0.5120, average train loss: 3.1345
[09/15 18:29:32 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1435, average loss: 3.0573
[09/15 18:29:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 26.50	
[09/15 18:29:53 visual_prompt]: 	Test 100/190. loss: 3.154, 0.1960 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 18:30:11 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1935, average loss: 3.0958
[09/15 18:30:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.30	top5: 27.75	
[09/15 18:30:11 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/15 18:30:20 visual_prompt]: Epoch 23 / 100: avg data time: 1.01e-01, avg batch time: 0.5052, average train loss: 3.2213
[09/15 18:30:23 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1436, average loss: 3.1941
[09/15 18:30:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 27.50	
[09/15 18:30:45 visual_prompt]: 	Test 100/190. loss: 3.394, 0.1958 s / batch. (data: 1.23e-02)max mem: 17.22456 GB 
[09/15 18:31:03 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1940, average loss: 3.2288
[09/15 18:31:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.92	top5: 28.03	
[09/15 18:31:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/15 18:31:12 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e-01, avg batch time: 0.5067, average train loss: 3.1044
[09/15 18:31:15 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1435, average loss: 3.1426
[09/15 18:31:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 27.50	
[09/15 18:31:36 visual_prompt]: 	Test 100/190. loss: 3.376, 0.1957 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 18:31:55 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1944, average loss: 3.1421
[09/15 18:31:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.69	top5: 28.25	
[09/15 18:31:55 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/15 18:32:04 visual_prompt]: Epoch 25 / 100: avg data time: 1.05e-01, avg batch time: 0.5094, average train loss: 3.1197
[09/15 18:32:07 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1436, average loss: 3.4134
[09/15 18:32:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 27.50	
[09/15 18:32:28 visual_prompt]: 	Test 100/190. loss: 3.278, 0.1971 s / batch. (data: 1.24e-02)max mem: 17.22456 GB 
[09/15 18:32:47 visual_prompt]: Inference (test):avg data time: 9.23e-03, avg batch time: 0.1951, average loss: 3.3636
[09/15 18:32:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.02	top5: 27.87	
[09/15 18:32:47 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/15 18:32:56 visual_prompt]: Epoch 26 / 100: avg data time: 9.60e-02, avg batch time: 0.5009, average train loss: 3.1312
[09/15 18:32:59 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1436, average loss: 3.1982
[09/15 18:32:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 26.50	
[09/15 18:33:20 visual_prompt]: 	Test 100/190. loss: 3.261, 0.1847 s / batch. (data: 1.27e-04)max mem: 17.22456 GB 
[09/15 18:33:38 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1932, average loss: 3.1850
[09/15 18:33:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.80	top5: 27.88	
[09/15 18:33:38 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/15 18:33:47 visual_prompt]: Epoch 27 / 100: avg data time: 1.01e-01, avg batch time: 0.5057, average train loss: 3.2271
[09/15 18:33:50 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1436, average loss: 3.1967
[09/15 18:33:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 28.50	
[09/15 18:34:11 visual_prompt]: 	Test 100/190. loss: 3.068, 0.1933 s / batch. (data: 1.41e-04)max mem: 17.22456 GB 
[09/15 18:34:30 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1937, average loss: 3.2399
[09/15 18:34:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.49	top5: 27.79	
[09/15 18:34:30 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/15 18:34:39 visual_prompt]: Epoch 28 / 100: avg data time: 1.08e-01, avg batch time: 0.5130, average train loss: 3.2431
[09/15 18:34:42 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1434, average loss: 3.3241
[09/15 18:34:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 27.00	
[09/15 18:35:03 visual_prompt]: 	Test 100/190. loss: 3.486, 0.1839 s / batch. (data: 1.28e-04)max mem: 17.22456 GB 
[09/15 18:35:21 visual_prompt]: Inference (test):avg data time: 7.01e-03, avg batch time: 0.1940, average loss: 3.3583
[09/15 18:35:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.46	top5: 27.93	
[09/15 18:35:21 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/15 18:35:31 visual_prompt]: Epoch 29 / 100: avg data time: 1.07e-01, avg batch time: 0.5117, average train loss: 3.2292
[09/15 18:35:34 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1435, average loss: 3.2168
[09/15 18:35:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.00	top5: 26.50	
[09/15 18:35:55 visual_prompt]: 	Test 100/190. loss: 2.946, 0.2097 s / batch. (data: 1.31e-02)max mem: 17.22456 GB 
[09/15 18:36:13 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1941, average loss: 3.2120
[09/15 18:36:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.84	
[09/15 18:36:13 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/15 18:36:22 visual_prompt]: Epoch 30 / 100: avg data time: 9.71e-02, avg batch time: 0.5025, average train loss: 3.1854
[09/15 18:36:25 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1437, average loss: 3.1050
[09/15 18:36:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 32.50	
[09/15 18:36:46 visual_prompt]: 	Test 100/190. loss: 3.180, 0.1840 s / batch. (data: 1.63e-04)max mem: 17.22456 GB 
[09/15 18:37:05 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1935, average loss: 3.2087
[09/15 18:37:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 28.50	
[09/15 18:37:05 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/15 18:37:14 visual_prompt]: Epoch 31 / 100: avg data time: 1.07e-01, avg batch time: 0.5098, average train loss: 3.1391
[09/15 18:37:17 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1433, average loss: 2.9998
[09/15 18:37:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 33.50	
[09/15 18:37:38 visual_prompt]: 	Test 100/190. loss: 3.010, 0.1840 s / batch. (data: 1.25e-04)max mem: 17.22456 GB 
[09/15 18:37:57 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1937, average loss: 3.0838
[09/15 18:37:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.38	top5: 28.16	
[09/15 18:37:57 visual_prompt]: Best epoch 31: best metric: 0.095
[09/15 18:37:57 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/15 18:38:06 visual_prompt]: Epoch 32 / 100: avg data time: 1.07e-01, avg batch time: 0.5112, average train loss: 3.0678
[09/15 18:38:09 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1435, average loss: 3.1151
[09/15 18:38:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 25.50	
[09/15 18:38:30 visual_prompt]: 	Test 100/190. loss: 3.233, 0.1834 s / batch. (data: 3.74e-05)max mem: 17.22456 GB 
[09/15 18:38:48 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1944, average loss: 3.0855
[09/15 18:38:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.82	top5: 27.99	
[09/15 18:38:48 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/15 18:38:58 visual_prompt]: Epoch 33 / 100: avg data time: 1.07e-01, avg batch time: 0.5106, average train loss: 3.1149
[09/15 18:39:00 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1434, average loss: 3.0504
[09/15 18:39:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 32.50	
[09/15 18:39:21 visual_prompt]: 	Test 100/190. loss: 3.239, 0.1849 s / batch. (data: 1.44e-04)max mem: 17.22456 GB 
[09/15 18:39:40 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1938, average loss: 3.1640
[09/15 18:39:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.91	top5: 27.70	
[09/15 18:39:40 visual_prompt]: Best epoch 33: best metric: 0.100
[09/15 18:39:40 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/15 18:39:49 visual_prompt]: Epoch 34 / 100: avg data time: 1.03e-01, avg batch time: 0.5067, average train loss: 3.1261
[09/15 18:39:52 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1436, average loss: 3.0168
[09/15 18:39:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.50	
[09/15 18:40:13 visual_prompt]: 	Test 100/190. loss: 3.090, 0.2008 s / batch. (data: 1.59e-02)max mem: 17.22456 GB 
[09/15 18:40:31 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1943, average loss: 3.0419
[09/15 18:40:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.67	top5: 28.26	
[09/15 18:40:31 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/15 18:40:40 visual_prompt]: Epoch 35 / 100: avg data time: 9.91e-02, avg batch time: 0.5043, average train loss: 3.0697
[09/15 18:40:43 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1435, average loss: 3.0282
[09/15 18:40:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 34.00	
[09/15 18:41:05 visual_prompt]: 	Test 100/190. loss: 3.141, 0.1962 s / batch. (data: 1.27e-02)max mem: 17.22456 GB 
[09/15 18:41:23 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1934, average loss: 3.0860
[09/15 18:41:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.96	top5: 30.31	
[09/15 18:41:23 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/15 18:41:32 visual_prompt]: Epoch 36 / 100: avg data time: 1.05e-01, avg batch time: 0.5093, average train loss: 3.0424
[09/15 18:41:35 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1435, average loss: 2.9060
[09/15 18:41:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 36.00	
[09/15 18:41:56 visual_prompt]: 	Test 100/190. loss: 2.982, 0.2112 s / batch. (data: 2.81e-02)max mem: 17.22456 GB 
[09/15 18:42:15 visual_prompt]: Inference (test):avg data time: 6.55e-03, avg batch time: 0.1935, average loss: 2.9863
[09/15 18:42:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.72	top5: 31.30	
[09/15 18:42:15 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/15 18:42:24 visual_prompt]: Epoch 37 / 100: avg data time: 1.04e-01, avg batch time: 0.5089, average train loss: 3.0229
[09/15 18:42:27 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1465, average loss: 2.9732
[09/15 18:42:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 36.50	
[09/15 18:42:48 visual_prompt]: 	Test 100/190. loss: 3.166, 0.1990 s / batch. (data: 1.53e-02)max mem: 17.22456 GB 
[09/15 18:43:07 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1946, average loss: 3.1078
[09/15 18:43:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.34	top5: 28.34	
[09/15 18:43:07 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/15 18:43:16 visual_prompt]: Epoch 38 / 100: avg data time: 1.02e-01, avg batch time: 0.5079, average train loss: 3.0920
[09/15 18:43:19 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1437, average loss: 3.1368
[09/15 18:43:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 29.00	
[09/15 18:43:40 visual_prompt]: 	Test 100/190. loss: 3.130, 0.1843 s / batch. (data: 1.63e-04)max mem: 17.22456 GB 
[09/15 18:43:58 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1939, average loss: 3.1690
[09/15 18:43:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.44	top5: 27.72	
[09/15 18:43:58 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/15 18:44:07 visual_prompt]: Epoch 39 / 100: avg data time: 1.12e-01, avg batch time: 0.5136, average train loss: 3.1900
[09/15 18:44:10 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1433, average loss: 3.1876
[09/15 18:44:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 25.50	
[09/15 18:44:31 visual_prompt]: 	Test 100/190. loss: 3.235, 0.1983 s / batch. (data: 1.49e-02)max mem: 17.22456 GB 
[09/15 18:44:50 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1937, average loss: 3.1867
[09/15 18:44:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.60	top5: 27.62	
[09/15 18:44:50 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/15 18:44:59 visual_prompt]: Epoch 40 / 100: avg data time: 1.01e-01, avg batch time: 0.5050, average train loss: 3.1127
[09/15 18:45:02 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1435, average loss: 3.1302
[09/15 18:45:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 28.50	
[09/15 18:45:23 visual_prompt]: 	Test 100/190. loss: 3.185, 0.1963 s / batch. (data: 1.28e-02)max mem: 17.22456 GB 
[09/15 18:45:41 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1931, average loss: 3.0943
[09/15 18:45:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.36	top5: 28.98	
[09/15 18:45:41 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/15 18:45:50 visual_prompt]: Epoch 41 / 100: avg data time: 1.04e-01, avg batch time: 0.5070, average train loss: 3.1085
[09/15 18:45:53 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1436, average loss: 3.1622
[09/15 18:45:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 30.50	
[09/15 18:46:15 visual_prompt]: 	Test 100/190. loss: 3.213, 0.1849 s / batch. (data: 1.41e-04)max mem: 17.22456 GB 
[09/15 18:46:33 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1939, average loss: 3.2060
[09/15 18:46:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.70	top5: 29.30	
[09/15 18:46:33 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/15 18:46:42 visual_prompt]: Epoch 42 / 100: avg data time: 1.02e-01, avg batch time: 0.5060, average train loss: 3.0748
[09/15 18:46:45 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1436, average loss: 3.0052
[09/15 18:46:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.50	top5: 33.50	
[09/15 18:47:06 visual_prompt]: 	Test 100/190. loss: 3.241, 0.1838 s / batch. (data: 1.15e-04)max mem: 17.22456 GB 
[09/15 18:47:25 visual_prompt]: Inference (test):avg data time: 6.82e-03, avg batch time: 0.1932, average loss: 3.1333
[09/15 18:47:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 31.36	
[09/15 18:47:25 visual_prompt]: Best epoch 42: best metric: 0.115
[09/15 18:47:25 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/15 18:47:34 visual_prompt]: Epoch 43 / 100: avg data time: 1.05e-01, avg batch time: 0.5093, average train loss: 3.0558
[09/15 18:47:37 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1435, average loss: 2.9842
[09/15 18:47:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 35.00	
[09/15 18:47:58 visual_prompt]: 	Test 100/190. loss: 3.108, 0.1966 s / batch. (data: 1.30e-02)max mem: 17.22456 GB 
[09/15 18:48:16 visual_prompt]: Inference (test):avg data time: 6.78e-03, avg batch time: 0.1934, average loss: 3.0399
[09/15 18:48:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.06	top5: 31.84	
[09/15 18:48:16 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/15 18:48:25 visual_prompt]: Epoch 44 / 100: avg data time: 9.42e-02, avg batch time: 0.5003, average train loss: 3.0819
[09/15 18:48:28 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1434, average loss: 3.0231
[09/15 18:48:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 32.00	
[09/15 18:48:50 visual_prompt]: 	Test 100/190. loss: 3.108, 0.2038 s / batch. (data: 1.45e-04)max mem: 17.22456 GB 
[09/15 18:49:08 visual_prompt]: Inference (test):avg data time: 8.10e-03, avg batch time: 0.1942, average loss: 3.0709
[09/15 18:49:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.98	top5: 31.10	
[09/15 18:49:08 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/15 18:49:17 visual_prompt]: Epoch 45 / 100: avg data time: 1.05e-01, avg batch time: 0.5084, average train loss: 3.0164
[09/15 18:49:20 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1432, average loss: 3.0977
[09/15 18:49:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 34.50	
[09/15 18:49:41 visual_prompt]: 	Test 100/190. loss: 3.109, 0.1848 s / batch. (data: 1.27e-04)max mem: 17.22456 GB 
[09/15 18:50:00 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1939, average loss: 3.1455
[09/15 18:50:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 31.56	
[09/15 18:50:00 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/15 18:50:09 visual_prompt]: Epoch 46 / 100: avg data time: 1.08e-01, avg batch time: 0.5116, average train loss: 3.0167
[09/15 18:50:12 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1435, average loss: 2.8986
[09/15 18:50:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 41.00	
[09/15 18:50:33 visual_prompt]: 	Test 100/190. loss: 3.073, 0.2231 s / batch. (data: 4.02e-02)max mem: 17.22456 GB 
[09/15 18:50:51 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1932, average loss: 3.0379
[09/15 18:50:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.33	top5: 32.90	
[09/15 18:50:51 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/15 18:51:00 visual_prompt]: Epoch 47 / 100: avg data time: 1.05e-01, avg batch time: 0.5084, average train loss: 2.9991
[09/15 18:51:03 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1436, average loss: 2.9093
[09/15 18:51:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 40.50	
[09/15 18:51:24 visual_prompt]: 	Test 100/190. loss: 3.020, 0.2090 s / batch. (data: 2.57e-02)max mem: 17.22456 GB 
[09/15 18:51:43 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1942, average loss: 3.0101
[09/15 18:51:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.32	top5: 31.79	
[09/15 18:51:43 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/15 18:51:52 visual_prompt]: Epoch 48 / 100: avg data time: 1.06e-01, avg batch time: 0.5093, average train loss: 3.0225
[09/15 18:51:55 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1435, average loss: 3.0559
[09/15 18:51:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.00	
[09/15 18:52:16 visual_prompt]: 	Test 100/190. loss: 2.859, 0.1839 s / batch. (data: 1.31e-04)max mem: 17.22456 GB 
[09/15 18:52:34 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1938, average loss: 2.9965
[09/15 18:52:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.33	top5: 31.21	
[09/15 18:52:34 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/15 18:52:43 visual_prompt]: Epoch 49 / 100: avg data time: 9.09e-02, avg batch time: 0.4981, average train loss: 3.0193
[09/15 18:52:46 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1436, average loss: 2.9129
[09/15 18:52:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 36.50	
[09/15 18:53:08 visual_prompt]: 	Test 100/190. loss: 2.943, 0.1959 s / batch. (data: 1.23e-02)max mem: 17.22456 GB 
[09/15 18:53:26 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1937, average loss: 2.9344
[09/15 18:53:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.07	top5: 35.20	
[09/15 18:53:26 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/15 18:53:35 visual_prompt]: Epoch 50 / 100: avg data time: 1.05e-01, avg batch time: 0.5078, average train loss: 2.9193
[09/15 18:53:38 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1443, average loss: 3.0290
[09/15 18:53:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 32.50	
[09/15 18:53:59 visual_prompt]: 	Test 100/190. loss: 3.100, 0.1836 s / batch. (data: 1.22e-04)max mem: 17.22456 GB 
[09/15 18:54:17 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1930, average loss: 3.1292
[09/15 18:54:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.58	top5: 28.95	
[09/15 18:54:17 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/15 18:54:27 visual_prompt]: Epoch 51 / 100: avg data time: 1.08e-01, avg batch time: 0.5122, average train loss: 2.9364
[09/15 18:54:30 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1435, average loss: 2.9196
[09/15 18:54:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 38.50	
[09/15 18:54:51 visual_prompt]: 	Test 100/190. loss: 3.065, 0.1831 s / batch. (data: 1.40e-04)max mem: 17.22456 GB 
[09/15 18:55:09 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1939, average loss: 2.9602
[09/15 18:55:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.52	top5: 36.63	
[09/15 18:55:09 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/15 18:55:18 visual_prompt]: Epoch 52 / 100: avg data time: 1.07e-01, avg batch time: 0.5107, average train loss: 2.9449
[09/15 18:55:21 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1436, average loss: 3.0293
[09/15 18:55:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.50	top5: 38.00	
[09/15 18:55:43 visual_prompt]: 	Test 100/190. loss: 3.130, 0.2073 s / batch. (data: 1.03e-02)max mem: 17.22456 GB 
[09/15 18:56:01 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1935, average loss: 3.0800
[09/15 18:56:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.47	top5: 36.67	
[09/15 18:56:01 visual_prompt]: Best epoch 52: best metric: 0.125
[09/15 18:56:01 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/15 18:56:10 visual_prompt]: Epoch 53 / 100: avg data time: 1.00e-01, avg batch time: 0.5038, average train loss: 2.9119
[09/15 18:56:13 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1441, average loss: 2.8189
[09/15 18:56:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.00	top5: 40.50	
[09/15 18:56:34 visual_prompt]: 	Test 100/190. loss: 2.832, 0.1850 s / batch. (data: 1.07e-04)max mem: 17.22456 GB 
[09/15 18:56:53 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1932, average loss: 2.8935
[09/15 18:56:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.85	top5: 35.43	
[09/15 18:56:53 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/15 18:57:02 visual_prompt]: Epoch 54 / 100: avg data time: 1.07e-01, avg batch time: 0.5096, average train loss: 2.9502
[09/15 18:57:05 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1435, average loss: 3.1031
[09/15 18:57:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 26.00	
[09/15 18:57:26 visual_prompt]: 	Test 100/190. loss: 3.104, 0.1840 s / batch. (data: 1.31e-04)max mem: 17.22456 GB 
[09/15 18:57:44 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1936, average loss: 3.0968
[09/15 18:57:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.98	top5: 28.91	
[09/15 18:57:44 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/15 18:57:53 visual_prompt]: Epoch 55 / 100: avg data time: 1.16e-01, avg batch time: 0.5197, average train loss: 2.9083
[09/15 18:57:56 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1438, average loss: 2.8391
[09/15 18:57:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 38.00	
[09/15 18:58:18 visual_prompt]: 	Test 100/190. loss: 3.019, 0.1857 s / batch. (data: 1.29e-04)max mem: 17.22456 GB 
[09/15 18:58:36 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1937, average loss: 2.9103
[09/15 18:58:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.99	top5: 38.12	
[09/15 18:58:36 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/15 18:58:45 visual_prompt]: Epoch 56 / 100: avg data time: 1.12e-01, avg batch time: 0.5166, average train loss: 2.9321
[09/15 18:58:48 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1451, average loss: 2.8047
[09/15 18:58:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 43.00	
[09/15 18:59:09 visual_prompt]: 	Test 100/190. loss: 2.841, 0.1960 s / batch. (data: 1.25e-02)max mem: 17.22456 GB 
[09/15 18:59:28 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1936, average loss: 2.8695
[09/15 18:59:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.94	top5: 38.30	
[09/15 18:59:28 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/15 18:59:37 visual_prompt]: Epoch 57 / 100: avg data time: 1.11e-01, avg batch time: 0.5141, average train loss: 2.8612
[09/15 18:59:40 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1440, average loss: 2.8129
[09/15 18:59:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 44.50	
[09/15 19:00:01 visual_prompt]: 	Test 100/190. loss: 2.883, 0.1998 s / batch. (data: 1.67e-02)max mem: 17.22456 GB 
[09/15 19:00:19 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1937, average loss: 2.9030
[09/15 19:00:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.31	top5: 39.23	
[09/15 19:00:19 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/15 19:00:28 visual_prompt]: Epoch 58 / 100: avg data time: 9.75e-02, avg batch time: 0.5019, average train loss: 2.8458
[09/15 19:00:31 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1433, average loss: 2.8055
[09/15 19:00:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 48.00	
[09/15 19:00:53 visual_prompt]: 	Test 100/190. loss: 2.943, 0.1840 s / batch. (data: 1.32e-04)max mem: 17.22456 GB 
[09/15 19:01:11 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1945, average loss: 2.9201
[09/15 19:01:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.71	top5: 40.71	
[09/15 19:01:11 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/15 19:01:20 visual_prompt]: Epoch 59 / 100: avg data time: 1.07e-01, avg batch time: 0.5122, average train loss: 2.8598
[09/15 19:01:23 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1434, average loss: 2.7843
[09/15 19:01:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 42.50	
[09/15 19:01:44 visual_prompt]: 	Test 100/190. loss: 2.922, 0.1843 s / batch. (data: 1.23e-04)max mem: 17.22456 GB 
[09/15 19:02:03 visual_prompt]: Inference (test):avg data time: 7.71e-03, avg batch time: 0.1940, average loss: 2.9329
[09/15 19:02:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.25	top5: 39.05	
[09/15 19:02:03 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/15 19:02:12 visual_prompt]: Epoch 60 / 100: avg data time: 1.00e-01, avg batch time: 0.5065, average train loss: 2.8377
[09/15 19:02:15 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1434, average loss: 2.6475
[09/15 19:02:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.50	top5: 53.50	
[09/15 19:02:36 visual_prompt]: 	Test 100/190. loss: 2.882, 0.1846 s / batch. (data: 2.49e-04)max mem: 17.22456 GB 
[09/15 19:02:54 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1935, average loss: 2.8638
[09/15 19:02:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.29	top5: 42.12	
[09/15 19:02:54 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/15 19:03:03 visual_prompt]: Epoch 61 / 100: avg data time: 9.24e-02, avg batch time: 0.4965, average train loss: 2.7223
[09/15 19:03:06 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1436, average loss: 2.6629
[09/15 19:03:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.00	top5: 47.00	
[09/15 19:03:27 visual_prompt]: 	Test 100/190. loss: 2.901, 0.1844 s / batch. (data: 1.64e-04)max mem: 17.22456 GB 
[09/15 19:03:46 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1942, average loss: 2.8438
[09/15 19:03:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.09	top5: 43.21	
[09/15 19:03:46 visual_prompt]: Best epoch 61: best metric: 0.140
[09/15 19:03:46 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/15 19:03:55 visual_prompt]: Epoch 62 / 100: avg data time: 9.69e-02, avg batch time: 0.5030, average train loss: 2.7463
[09/15 19:03:58 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1435, average loss: 2.6777
[09/15 19:03:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.00	top5: 49.00	
[09/15 19:04:19 visual_prompt]: 	Test 100/190. loss: 2.879, 0.1842 s / batch. (data: 1.22e-04)max mem: 17.22456 GB 
[09/15 19:04:37 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1941, average loss: 2.8227
[09/15 19:04:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.56	top5: 46.33	
[09/15 19:04:37 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/15 19:04:46 visual_prompt]: Epoch 63 / 100: avg data time: 1.02e-01, avg batch time: 0.5055, average train loss: 2.7096
[09/15 19:04:49 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1436, average loss: 2.7426
[09/15 19:04:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 50.00	
[09/15 19:05:10 visual_prompt]: 	Test 100/190. loss: 2.740, 0.1843 s / batch. (data: 3.24e-05)max mem: 17.22456 GB 
[09/15 19:05:29 visual_prompt]: Inference (test):avg data time: 7.71e-03, avg batch time: 0.1936, average loss: 2.8974
[09/15 19:05:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.51	top5: 43.26	
[09/15 19:05:29 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/15 19:05:38 visual_prompt]: Epoch 64 / 100: avg data time: 1.02e-01, avg batch time: 0.5048, average train loss: 2.7122
[09/15 19:05:41 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1436, average loss: 2.5546
[09/15 19:05:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.50	top5: 58.00	
[09/15 19:06:02 visual_prompt]: 	Test 100/190. loss: 2.842, 0.1985 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 19:06:21 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1944, average loss: 2.7195
[09/15 19:06:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.67	top5: 51.29	
[09/15 19:06:21 visual_prompt]: Best epoch 64: best metric: 0.155
[09/15 19:06:21 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/15 19:06:30 visual_prompt]: Epoch 65 / 100: avg data time: 9.70e-02, avg batch time: 0.5038, average train loss: 2.6143
[09/15 19:06:32 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1434, average loss: 2.6093
[09/15 19:06:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.00	top5: 52.00	
[09/15 19:06:54 visual_prompt]: 	Test 100/190. loss: 2.858, 0.1951 s / batch. (data: 1.34e-04)max mem: 17.22456 GB 
[09/15 19:07:12 visual_prompt]: Inference (test):avg data time: 6.77e-03, avg batch time: 0.1932, average loss: 2.8426
[09/15 19:07:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.42	top5: 44.72	
[09/15 19:07:12 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/15 19:07:21 visual_prompt]: Epoch 66 / 100: avg data time: 1.10e-01, avg batch time: 0.5146, average train loss: 2.6306
[09/15 19:07:24 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1435, average loss: 2.6752
[09/15 19:07:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.00	top5: 55.00	
[09/15 19:07:45 visual_prompt]: 	Test 100/190. loss: 2.872, 0.1989 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 19:08:04 visual_prompt]: Inference (test):avg data time: 8.56e-03, avg batch time: 0.1946, average loss: 2.9132
[09/15 19:08:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.71	top5: 46.48	
[09/15 19:08:04 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/15 19:08:13 visual_prompt]: Epoch 67 / 100: avg data time: 1.01e-01, avg batch time: 0.5041, average train loss: 2.6255
[09/15 19:08:16 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1438, average loss: 2.4941
[09/15 19:08:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.50	top5: 61.00	
[09/15 19:08:37 visual_prompt]: 	Test 100/190. loss: 2.582, 0.1970 s / batch. (data: 1.38e-02)max mem: 17.22456 GB 
[09/15 19:08:56 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1945, average loss: 2.7287
[09/15 19:08:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.93	top5: 50.89	
[09/15 19:08:56 visual_prompt]: Best epoch 67: best metric: 0.175
[09/15 19:08:56 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/15 19:09:05 visual_prompt]: Epoch 68 / 100: avg data time: 1.00e-01, avg batch time: 0.5066, average train loss: 2.5101
[09/15 19:09:08 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1433, average loss: 2.4091
[09/15 19:09:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.00	top5: 64.50	
[09/15 19:09:29 visual_prompt]: 	Test 100/190. loss: 2.732, 0.1994 s / batch. (data: 1.60e-02)max mem: 17.22456 GB 
[09/15 19:09:47 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1939, average loss: 2.6837
[09/15 19:09:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.70	top5: 54.77	
[09/15 19:09:47 visual_prompt]: Best epoch 68: best metric: 0.190
[09/15 19:09:47 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/15 19:09:56 visual_prompt]: Epoch 69 / 100: avg data time: 9.70e-02, avg batch time: 0.5034, average train loss: 2.5015
[09/15 19:09:59 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1434, average loss: 2.5849
[09/15 19:09:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.50	top5: 56.50	
[09/15 19:10:21 visual_prompt]: 	Test 100/190. loss: 3.019, 0.2046 s / batch. (data: 1.13e-02)max mem: 17.22456 GB 
[09/15 19:10:39 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1943, average loss: 2.8539
[09/15 19:10:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.21	top5: 51.01	
[09/15 19:10:39 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/15 19:10:48 visual_prompt]: Epoch 70 / 100: avg data time: 1.17e-01, avg batch time: 0.5198, average train loss: 2.4937
[09/15 19:10:51 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1438, average loss: 2.3885
[09/15 19:10:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.50	top5: 68.50	
[09/15 19:11:12 visual_prompt]: 	Test 100/190. loss: 2.778, 0.2092 s / batch. (data: 2.59e-02)max mem: 17.22456 GB 
[09/15 19:11:31 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1933, average loss: 2.6802
[09/15 19:11:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.95	top5: 55.30	
[09/15 19:11:31 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/15 19:11:40 visual_prompt]: Epoch 71 / 100: avg data time: 9.82e-02, avg batch time: 0.5037, average train loss: 2.4295
[09/15 19:11:43 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1436, average loss: 2.3368
[09/15 19:11:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.50	top5: 68.00	
[09/15 19:12:04 visual_prompt]: 	Test 100/190. loss: 2.725, 0.2179 s / batch. (data: 3.51e-02)max mem: 17.22456 GB 
[09/15 19:12:22 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1933, average loss: 2.6802
[09/15 19:12:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.05	top5: 55.58	
[09/15 19:12:22 visual_prompt]: Best epoch 71: best metric: 0.195
[09/15 19:12:22 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/15 19:12:31 visual_prompt]: Epoch 72 / 100: avg data time: 1.07e-01, avg batch time: 0.5117, average train loss: 2.3250
[09/15 19:12:34 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1436, average loss: 2.1929
[09/15 19:12:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.00	top5: 74.50	
[09/15 19:12:56 visual_prompt]: 	Test 100/190. loss: 2.834, 0.2136 s / batch. (data: 3.00e-02)max mem: 17.22456 GB 
[09/15 19:13:14 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1942, average loss: 2.6080
[09/15 19:13:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.86	top5: 61.25	
[09/15 19:13:14 visual_prompt]: Best epoch 72: best metric: 0.240
[09/15 19:13:14 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/15 19:13:23 visual_prompt]: Epoch 73 / 100: avg data time: 1.05e-01, avg batch time: 0.5082, average train loss: 2.3085
[09/15 19:13:26 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1435, average loss: 2.3921
[09/15 19:13:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.50	top5: 70.50	
[09/15 19:13:48 visual_prompt]: 	Test 100/190. loss: 2.750, 0.1845 s / batch. (data: 1.42e-04)max mem: 17.22456 GB 
[09/15 19:14:06 visual_prompt]: Inference (test):avg data time: 8.40e-03, avg batch time: 0.1941, average loss: 2.8049
[09/15 19:14:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.69	top5: 54.37	
[09/15 19:14:06 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/15 19:14:15 visual_prompt]: Epoch 74 / 100: avg data time: 1.08e-01, avg batch time: 0.5134, average train loss: 2.2784
[09/15 19:14:18 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1439, average loss: 2.4175
[09/15 19:14:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.00	top5: 67.00	
[09/15 19:14:40 visual_prompt]: 	Test 100/190. loss: 2.610, 0.1850 s / batch. (data: 1.52e-04)max mem: 17.22456 GB 
[09/15 19:14:58 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1941, average loss: 2.7946
[09/15 19:14:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.30	top5: 55.07	
[09/15 19:14:58 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/15 19:15:07 visual_prompt]: Epoch 75 / 100: avg data time: 8.75e-02, avg batch time: 0.4938, average train loss: 2.2512
[09/15 19:15:10 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1473, average loss: 2.2060
[09/15 19:15:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.00	top5: 78.00	
[09/15 19:15:31 visual_prompt]: 	Test 100/190. loss: 2.701, 0.2104 s / batch. (data: 2.71e-02)max mem: 17.22456 GB 
[09/15 19:15:50 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1943, average loss: 2.7686
[09/15 19:15:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.89	top5: 58.03	
[09/15 19:15:50 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/15 19:15:59 visual_prompt]: Epoch 76 / 100: avg data time: 9.85e-02, avg batch time: 0.5012, average train loss: 2.1198
[09/15 19:16:01 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1437, average loss: 2.0177
[09/15 19:16:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.50	top5: 80.50	
[09/15 19:16:23 visual_prompt]: 	Test 100/190. loss: 2.868, 0.2199 s / batch. (data: 3.67e-02)max mem: 17.22456 GB 
[09/15 19:16:41 visual_prompt]: Inference (test):avg data time: 7.44e-03, avg batch time: 0.1941, average loss: 2.7634
[09/15 19:16:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.13	top5: 61.57	
[09/15 19:16:41 visual_prompt]: Best epoch 76: best metric: 0.285
[09/15 19:16:41 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/15 19:16:50 visual_prompt]: Epoch 77 / 100: avg data time: 8.76e-02, avg batch time: 0.4970, average train loss: 2.0542
[09/15 19:16:53 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1434, average loss: 2.1360
[09/15 19:16:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.00	top5: 76.50	
[09/15 19:17:14 visual_prompt]: 	Test 100/190. loss: 2.777, 0.1845 s / batch. (data: 1.33e-04)max mem: 17.22456 GB 
[09/15 19:17:33 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1933, average loss: 2.8102
[09/15 19:17:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.53	top5: 60.74	
[09/15 19:17:33 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/15 19:17:42 visual_prompt]: Epoch 78 / 100: avg data time: 1.10e-01, avg batch time: 0.5140, average train loss: 2.0434
[09/15 19:17:45 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1436, average loss: 2.2012
[09/15 19:17:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.00	top5: 76.00	
[09/15 19:18:06 visual_prompt]: 	Test 100/190. loss: 2.964, 0.1846 s / batch. (data: 1.44e-04)max mem: 17.22456 GB 
[09/15 19:18:25 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1930, average loss: 2.9096
[09/15 19:18:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.66	top5: 60.68	
[09/15 19:18:25 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/15 19:18:34 visual_prompt]: Epoch 79 / 100: avg data time: 1.13e-01, avg batch time: 0.5182, average train loss: 2.1344
[09/15 19:18:37 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1435, average loss: 2.1186
[09/15 19:18:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.00	top5: 76.50	
[09/15 19:18:58 visual_prompt]: 	Test 100/190. loss: 2.982, 0.2062 s / batch. (data: 2.29e-02)max mem: 17.22456 GB 
[09/15 19:19:16 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1934, average loss: 2.8313
[09/15 19:19:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.96	top5: 60.66	
[09/15 19:19:17 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/15 19:19:26 visual_prompt]: Epoch 80 / 100: avg data time: 1.08e-01, avg batch time: 0.5118, average train loss: 1.9630
[09/15 19:19:29 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1441, average loss: 1.9106
[09/15 19:19:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.50	top5: 89.00	
[09/15 19:19:50 visual_prompt]: 	Test 100/190. loss: 2.582, 0.1849 s / batch. (data: 1.53e-04)max mem: 17.22456 GB 
[09/15 19:20:08 visual_prompt]: Inference (test):avg data time: 8.46e-03, avg batch time: 0.1946, average loss: 2.7793
[09/15 19:20:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.24	top5: 63.61	
[09/15 19:20:08 visual_prompt]: Best epoch 80: best metric: 0.315
[09/15 19:20:08 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/15 19:20:17 visual_prompt]: Epoch 81 / 100: avg data time: 1.00e-01, avg batch time: 0.5050, average train loss: 1.8446
[09/15 19:20:20 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1482, average loss: 1.7561
[09/15 19:20:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 35.50	top5: 92.00	
[09/15 19:20:42 visual_prompt]: 	Test 100/190. loss: 2.872, 0.1994 s / batch. (data: 1.58e-02)max mem: 17.22456 GB 
[09/15 19:21:00 visual_prompt]: Inference (test):avg data time: 7.44e-03, avg batch time: 0.1940, average loss: 2.9609
[09/15 19:21:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.62	top5: 63.18	
[09/15 19:21:00 visual_prompt]: Best epoch 81: best metric: 0.355
[09/15 19:21:00 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/15 19:21:09 visual_prompt]: Epoch 82 / 100: avg data time: 1.14e-01, avg batch time: 0.5163, average train loss: 1.8064
[09/15 19:21:12 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1434, average loss: 2.0202
[09/15 19:21:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.50	top5: 84.00	
[09/15 19:21:33 visual_prompt]: 	Test 100/190. loss: 3.076, 0.1934 s / batch. (data: 9.51e-05)max mem: 17.22456 GB 
[09/15 19:21:52 visual_prompt]: Inference (test):avg data time: 6.52e-03, avg batch time: 0.1931, average loss: 3.2802
[09/15 19:21:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.86	top5: 59.60	
[09/15 19:21:52 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/15 19:22:01 visual_prompt]: Epoch 83 / 100: avg data time: 9.73e-02, avg batch time: 0.5034, average train loss: 1.8031
[09/15 19:22:04 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1436, average loss: 1.7299
[09/15 19:22:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 35.50	top5: 90.00	
[09/15 19:22:25 visual_prompt]: 	Test 100/190. loss: 2.792, 0.1846 s / batch. (data: 9.35e-05)max mem: 17.22456 GB 
[09/15 19:22:43 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1942, average loss: 2.9341
[09/15 19:22:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.02	top5: 64.95	
[09/15 19:22:43 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/15 19:22:53 visual_prompt]: Epoch 84 / 100: avg data time: 1.08e-01, avg batch time: 0.5130, average train loss: 1.6834
[09/15 19:22:56 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1434, average loss: 1.8893
[09/15 19:22:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 33.50	top5: 87.00	
[09/15 19:23:17 visual_prompt]: 	Test 100/190. loss: 3.465, 0.1849 s / batch. (data: 1.23e-04)max mem: 17.22456 GB 
[09/15 19:23:35 visual_prompt]: Inference (test):avg data time: 6.46e-03, avg batch time: 0.1930, average loss: 3.3153
[09/15 19:23:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.13	top5: 61.93	
[09/15 19:23:35 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/15 19:23:44 visual_prompt]: Epoch 85 / 100: avg data time: 1.13e-01, avg batch time: 0.5163, average train loss: 1.6415
[09/15 19:23:47 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1435, average loss: 1.6532
[09/15 19:23:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 36.00	top5: 91.00	
[09/15 19:24:08 visual_prompt]: 	Test 100/190. loss: 2.903, 0.1832 s / batch. (data: 1.19e-04)max mem: 17.22456 GB 
[09/15 19:24:27 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1934, average loss: 2.9907
[09/15 19:24:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.46	top5: 65.09	
[09/15 19:24:27 visual_prompt]: Best epoch 85: best metric: 0.360
[09/15 19:24:27 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/15 19:24:36 visual_prompt]: Epoch 86 / 100: avg data time: 1.03e-01, avg batch time: 0.5099, average train loss: 1.6271
[09/15 19:24:39 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1435, average loss: 1.7937
[09/15 19:24:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 35.50	top5: 90.50	
[09/15 19:25:00 visual_prompt]: 	Test 100/190. loss: 3.102, 0.1962 s / batch. (data: 1.26e-02)max mem: 17.22456 GB 
[09/15 19:25:18 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1935, average loss: 3.0627
[09/15 19:25:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.78	top5: 64.56	
[09/15 19:25:18 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/15 19:25:27 visual_prompt]: Epoch 87 / 100: avg data time: 1.05e-01, avg batch time: 0.5092, average train loss: 1.5370
[09/15 19:25:30 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1436, average loss: 1.4211
[09/15 19:25:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 48.00	top5: 94.50	
[09/15 19:25:52 visual_prompt]: 	Test 100/190. loss: 3.137, 0.2018 s / batch. (data: 1.86e-02)max mem: 17.22456 GB 
[09/15 19:26:10 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1935, average loss: 3.1828
[09/15 19:26:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.74	top5: 65.95	
[09/15 19:26:10 visual_prompt]: Best epoch 87: best metric: 0.480
[09/15 19:26:10 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/15 19:26:19 visual_prompt]: Epoch 88 / 100: avg data time: 1.03e-01, avg batch time: 0.5074, average train loss: 1.3853
[09/15 19:26:22 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1435, average loss: 1.3514
[09/15 19:26:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 47.50	top5: 96.00	
[09/15 19:26:43 visual_prompt]: 	Test 100/190. loss: 3.563, 0.1850 s / batch. (data: 1.48e-04)max mem: 17.22456 GB 
[09/15 19:27:02 visual_prompt]: Inference (test):avg data time: 8.37e-03, avg batch time: 0.1942, average loss: 3.4367
[09/15 19:27:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.99	top5: 65.38	
[09/15 19:27:02 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/15 19:27:11 visual_prompt]: Epoch 89 / 100: avg data time: 9.30e-02, avg batch time: 0.4979, average train loss: 1.3399
[09/15 19:27:14 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1436, average loss: 1.3429
[09/15 19:27:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 47.00	top5: 97.50	
[09/15 19:27:35 visual_prompt]: 	Test 100/190. loss: 3.600, 0.1964 s / batch. (data: 1.25e-02)max mem: 17.22456 GB 
[09/15 19:27:53 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1942, average loss: 3.4901
[09/15 19:27:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.00	top5: 66.63	
[09/15 19:27:53 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/15 19:28:02 visual_prompt]: Epoch 90 / 100: avg data time: 1.08e-01, avg batch time: 0.5113, average train loss: 1.2293
[09/15 19:28:05 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1435, average loss: 1.3091
[09/15 19:28:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 48.00	top5: 97.50	
[09/15 19:28:27 visual_prompt]: 	Test 100/190. loss: 3.995, 0.1920 s / batch. (data: 8.57e-03)max mem: 17.22456 GB 
[09/15 19:28:45 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1935, average loss: 3.7478
[09/15 19:28:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.22	top5: 66.27	
[09/15 19:28:45 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/15 19:28:54 visual_prompt]: Epoch 91 / 100: avg data time: 1.09e-01, avg batch time: 0.5124, average train loss: 1.1782
[09/15 19:28:57 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1436, average loss: 1.2302
[09/15 19:28:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 48.00	top5: 99.50	
[09/15 19:29:18 visual_prompt]: 	Test 100/190. loss: 3.754, 0.1988 s / batch. (data: 1.52e-02)max mem: 17.22456 GB 
[09/15 19:29:37 visual_prompt]: Inference (test):avg data time: 7.06e-03, avg batch time: 0.1933, average loss: 3.9520
[09/15 19:29:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.95	top5: 65.89	
[09/15 19:29:37 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/15 19:29:46 visual_prompt]: Epoch 92 / 100: avg data time: 1.05e-01, avg batch time: 0.5090, average train loss: 1.0890
[09/15 19:29:49 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1439, average loss: 1.1635
[09/15 19:29:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 59.00	top5: 98.00	
[09/15 19:30:10 visual_prompt]: 	Test 100/190. loss: 4.252, 0.1844 s / batch. (data: 1.62e-04)max mem: 17.22456 GB 
[09/15 19:30:29 visual_prompt]: Inference (test):avg data time: 7.26e-03, avg batch time: 0.1941, average loss: 4.0779
[09/15 19:30:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.77	top5: 66.84	
[09/15 19:30:29 visual_prompt]: Best epoch 92: best metric: 0.590
[09/15 19:30:29 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/15 19:30:38 visual_prompt]: Epoch 93 / 100: avg data time: 1.14e-01, avg batch time: 0.5173, average train loss: 1.0158
[09/15 19:30:41 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1437, average loss: 1.1845
[09/15 19:30:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 53.00	top5: 97.00	
[09/15 19:31:02 visual_prompt]: 	Test 100/190. loss: 4.594, 0.1855 s / batch. (data: 1.22e-04)max mem: 17.22456 GB 
[09/15 19:31:20 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1936, average loss: 4.4782
[09/15 19:31:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.49	top5: 65.88	
[09/15 19:31:20 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/15 19:31:29 visual_prompt]: Epoch 94 / 100: avg data time: 9.36e-02, avg batch time: 0.5006, average train loss: 0.9560
[09/15 19:31:32 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.1434, average loss: 0.9622
[09/15 19:31:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 65.00	top5: 98.00	
[09/15 19:31:54 visual_prompt]: 	Test 100/190. loss: 4.403, 0.1846 s / batch. (data: 1.72e-04)max mem: 17.22456 GB 
[09/15 19:32:12 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1944, average loss: 4.5311
[09/15 19:32:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.33	top5: 65.99	
[09/15 19:32:12 visual_prompt]: Best epoch 94: best metric: 0.650
[09/15 19:32:12 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/15 19:32:21 visual_prompt]: Epoch 95 / 100: avg data time: 8.86e-02, avg batch time: 0.4942, average train loss: 0.8910
[09/15 19:32:24 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1434, average loss: 0.9305
[09/15 19:32:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 66.50	top5: 99.00	
[09/15 19:32:45 visual_prompt]: 	Test 100/190. loss: 4.858, 0.1865 s / batch. (data: 1.33e-04)max mem: 17.22456 GB 
[09/15 19:33:04 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1943, average loss: 4.6652
[09/15 19:33:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.14	top5: 65.75	
[09/15 19:33:04 visual_prompt]: Best epoch 95: best metric: 0.665
[09/15 19:33:04 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/15 19:33:13 visual_prompt]: Epoch 96 / 100: avg data time: 1.02e-01, avg batch time: 0.5063, average train loss: 0.8529
[09/15 19:33:16 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1435, average loss: 0.9084
[09/15 19:33:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 67.00	top5: 99.00	
[09/15 19:33:37 visual_prompt]: 	Test 100/190. loss: 4.756, 0.2070 s / batch. (data: 2.41e-02)max mem: 17.22456 GB 
[09/15 19:33:56 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1940, average loss: 4.8606
[09/15 19:33:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.56	top5: 65.77	
[09/15 19:33:56 visual_prompt]: Best epoch 96: best metric: 0.670
[09/15 19:33:56 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/15 19:34:05 visual_prompt]: Epoch 97 / 100: avg data time: 1.00e-01, avg batch time: 0.5038, average train loss: 0.7929
[09/15 19:34:07 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1435, average loss: 0.8602
[09/15 19:34:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 67.00	top5: 99.00	
[09/15 19:34:28 visual_prompt]: 	Test 100/190. loss: 4.865, 0.1844 s / batch. (data: 1.42e-04)max mem: 17.22456 GB 
[09/15 19:34:47 visual_prompt]: Inference (test):avg data time: 6.23e-03, avg batch time: 0.1933, average loss: 4.8617
[09/15 19:34:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.32	top5: 65.96	
[09/15 19:34:47 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/15 19:34:56 visual_prompt]: Epoch 98 / 100: avg data time: 1.08e-01, avg batch time: 0.5129, average train loss: 0.7604
[09/15 19:34:59 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1435, average loss: 0.8512
[09/15 19:34:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 71.00	top5: 99.00	
[09/15 19:35:20 visual_prompt]: 	Test 100/190. loss: 5.019, 0.1963 s / batch. (data: 1.25e-02)max mem: 17.22456 GB 
[09/15 19:35:39 visual_prompt]: Inference (test):avg data time: 6.63e-03, avg batch time: 0.1933, average loss: 4.8996
[09/15 19:35:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.67	top5: 66.27	
[09/15 19:35:39 visual_prompt]: Best epoch 98: best metric: 0.710
[09/15 19:35:39 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/15 19:35:48 visual_prompt]: Epoch 99 / 100: avg data time: 1.12e-01, avg batch time: 0.5144, average train loss: 0.7178
[09/15 19:35:51 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1438, average loss: 0.8355
[09/15 19:35:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 70.00	top5: 98.50	
[09/15 19:36:12 visual_prompt]: 	Test 100/190. loss: 5.043, 0.2061 s / batch. (data: 1.02e-02)max mem: 17.22456 GB 
[09/15 19:36:30 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1937, average loss: 4.9399
[09/15 19:36:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.60	top5: 66.30	
[09/15 19:36:30 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/15 19:36:39 visual_prompt]: Epoch 100 / 100: avg data time: 1.07e-01, avg batch time: 0.5136, average train loss: 0.7069
[09/15 19:36:42 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1439, average loss: 0.8462
[09/15 19:36:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 68.50	top5: 98.50	
[09/15 19:37:03 visual_prompt]: 	Test 100/190. loss: 5.042, 0.1855 s / batch. (data: 1.29e-04)max mem: 17.22456 GB 
[09/15 19:37:22 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1940, average loss: 4.9521
[09/15 19:37:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.40	top5: 66.21	

[09/16 01:28:43 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 01:28:43 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 01:28:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-cifar(num_classes=100)', 'DATA.NUMBER_CLASSES', '100', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed42'], train_type='')
[09/16 01:28:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 01:28:43 visual_prompt]: Training with config:
[09/16 01:28:43 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-cifar(num_classes=100)',
          'NO_TEST': False,
          'NUMBER_CLASSES': 100,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed42/vtab-cifar(num_classes=100)/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 01:28:43 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 01:28:43.600492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 01:28:43.800453: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 01:28:44.816924: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 01:28:44.817005: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 01:28:44.817014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 01:28:47.173718: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 01:28:47.173858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 01:28:47.173875: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 01:28:47 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
2023-09-16 01:28:47.246779: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split train[:800]+train[45000:45200], from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 01:28:49 visual_prompt]: Number of images: 1000
[09/16 01:28:49 visual_prompt]: Number of classes: 100 / 100
[09/16 01:28:49 visual_prompt]: Loading validation data...
[09/16 01:28:49 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split train[45000:45200], from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 01:28:49 visual_prompt]: Number of images: 200
[09/16 01:28:49 visual_prompt]: Number of classes: 90 / 100
[09/16 01:28:49 visual_prompt]: Loading test data...
[09/16 01:28:49 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split test, from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 01:29:02 visual_prompt]: Number of images: 10000
[09/16 01:29:02 visual_prompt]: Number of classes: 100 / 100
[09/16 01:29:02 visual_prompt]: Constructing models...
[09/16 01:29:05 visual_prompt]: Total Parameters: 86797156	 Gradient Parameters: 998500
[09/16 01:29:05 visual_prompt]: tuned percent:1.150
[09/16 01:29:07 visual_prompt]: Device used for model: 0
[09/16 01:29:07 visual_prompt]: Setting up Evalutator...
[09/16 01:29:07 visual_prompt]: Setting up Trainer...
[09/16 01:29:07 visual_prompt]: 	Setting up the optimizer...
[09/16 01:29:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 01:29:18 visual_prompt]: Epoch 1 / 100: avg data time: 1.06e-01, avg batch time: 0.5916, average train loss: 4.6387
[09/16 01:29:21 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1415, average loss: 4.6253
[09/16 01:29:21 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 7.00	
[09/16 01:29:41 visual_prompt]: 	Test 100/157. loss: 4.628, 0.1814 s / batch. (data: 1.41e-04)max mem: 17.22530 GB 
[09/16 01:29:53 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1907, average loss: 4.6444
[09/16 01:29:53 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 0.92	top5: 5.11	
[09/16 01:29:53 visual_prompt]: Best epoch 1: best metric: 0.015
[09/16 01:29:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 01:30:02 visual_prompt]: Epoch 2 / 100: avg data time: 1.00e-01, avg batch time: 0.5026, average train loss: 4.6438
[09/16 01:30:05 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1421, average loss: 4.5890
[09/16 01:30:05 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.00	top5: 7.00	
[09/16 01:30:26 visual_prompt]: 	Test 100/157. loss: 4.586, 0.1952 s / batch. (data: 1.38e-02)max mem: 17.22530 GB 
[09/16 01:30:38 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1919, average loss: 4.6777
[09/16 01:30:38 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.19	top5: 5.38	
[09/16 01:30:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 01:30:46 visual_prompt]: Epoch 3 / 100: avg data time: 8.36e-02, avg batch time: 0.4893, average train loss: 4.6833
[09/16 01:30:49 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1424, average loss: 4.5721
[09/16 01:30:49 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 8.50	
[09/16 01:31:10 visual_prompt]: 	Test 100/157. loss: 4.661, 0.1977 s / batch. (data: 1.59e-02)max mem: 17.22530 GB 
[09/16 01:31:22 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1913, average loss: 4.6920
[09/16 01:31:22 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.43	
[09/16 01:31:22 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 01:31:31 visual_prompt]: Epoch 4 / 100: avg data time: 9.99e-02, avg batch time: 0.4996, average train loss: 4.7263
[09/16 01:31:33 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1426, average loss: 4.6396
[09/16 01:31:33 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 6.00	top5: 10.00	
[09/16 01:31:54 visual_prompt]: 	Test 100/157. loss: 4.816, 0.1824 s / batch. (data: 1.46e-04)max mem: 17.22530 GB 
[09/16 01:32:06 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1919, average loss: 4.7526
[09/16 01:32:06 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 2.59	top5: 7.02	
[09/16 01:32:06 visual_prompt]: Best epoch 4: best metric: 0.060
[09/16 01:32:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 01:32:15 visual_prompt]: Epoch 5 / 100: avg data time: 9.58e-02, avg batch time: 0.4981, average train loss: 4.6539
[09/16 01:32:18 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1428, average loss: 4.4101
[09/16 01:32:18 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 6.00	top5: 12.50	
[09/16 01:32:39 visual_prompt]: 	Test 100/157. loss: 4.405, 0.1823 s / batch. (data: 1.50e-04)max mem: 17.22530 GB 
[09/16 01:32:50 visual_prompt]: Inference (test):avg data time: 8.39e-03, avg batch time: 0.1925, average loss: 4.6668
[09/16 01:32:51 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 2.58	top5: 9.83	
[09/16 01:32:51 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 01:33:00 visual_prompt]: Epoch 6 / 100: avg data time: 9.91e-02, avg batch time: 0.5035, average train loss: 4.6246
[09/16 01:33:02 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1427, average loss: 4.7286
[09/16 01:33:02 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 2.00	top5: 8.50	
[09/16 01:33:23 visual_prompt]: 	Test 100/157. loss: 4.642, 0.2018 s / batch. (data: 1.07e-04)max mem: 17.22530 GB 
[09/16 01:33:35 visual_prompt]: Inference (test):avg data time: 7.04e-03, avg batch time: 0.1912, average loss: 4.8179
[09/16 01:33:35 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.62	top5: 7.86	
[09/16 01:33:35 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 01:33:44 visual_prompt]: Epoch 7 / 100: avg data time: 1.00e-01, avg batch time: 0.5008, average train loss: 4.3216
[09/16 01:33:47 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1427, average loss: 5.4021
[09/16 01:33:47 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 4.50	top5: 15.00	
[09/16 01:34:08 visual_prompt]: 	Test 100/157. loss: 5.326, 0.1875 s / batch. (data: 1.18e-04)max mem: 17.22530 GB 
[09/16 01:34:19 visual_prompt]: Inference (test):avg data time: 6.75e-03, avg batch time: 0.1917, average loss: 5.5910
[09/16 01:34:20 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 3.29	top5: 13.08	
[09/16 01:34:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 01:34:28 visual_prompt]: Epoch 8 / 100: avg data time: 8.70e-02, avg batch time: 0.4903, average train loss: 4.6445
[09/16 01:34:31 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1424, average loss: 4.4066
[09/16 01:34:31 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 5.00	top5: 29.50	
[09/16 01:34:52 visual_prompt]: 	Test 100/157. loss: 4.213, 0.1895 s / batch. (data: 2.36e-04)max mem: 17.22530 GB 
[09/16 01:35:04 visual_prompt]: Inference (test):avg data time: 6.45e-03, avg batch time: 0.1919, average loss: 4.4704
[09/16 01:35:04 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 5.50	top5: 20.24	
[09/16 01:35:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 01:35:13 visual_prompt]: Epoch 9 / 100: avg data time: 1.10e-01, avg batch time: 0.5102, average train loss: 4.1707
[09/16 01:35:16 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1426, average loss: 3.8432
[09/16 01:35:16 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 7.00	top5: 36.00	
[09/16 01:35:37 visual_prompt]: 	Test 100/157. loss: 3.727, 0.1906 s / batch. (data: 3.62e-05)max mem: 17.22530 GB 
[09/16 01:35:49 visual_prompt]: Inference (test):avg data time: 8.87e-03, avg batch time: 0.1930, average loss: 4.0026
[09/16 01:35:49 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 7.18	top5: 28.76	
[09/16 01:35:49 visual_prompt]: Best epoch 9: best metric: 0.070
[09/16 01:35:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 01:35:58 visual_prompt]: Epoch 10 / 100: avg data time: 9.73e-02, avg batch time: 0.4983, average train loss: 3.8633
[09/16 01:36:01 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1426, average loss: 4.8600
[09/16 01:36:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 3.50	top5: 16.50	
[09/16 01:36:21 visual_prompt]: 	Test 100/157. loss: 4.794, 0.1828 s / batch. (data: 9.37e-05)max mem: 17.22530 GB 
[09/16 01:36:33 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1911, average loss: 4.9683
[09/16 01:36:33 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 3.23	top5: 13.83	
[09/16 01:36:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 01:36:42 visual_prompt]: Epoch 11 / 100: avg data time: 9.38e-02, avg batch time: 0.4979, average train loss: 3.5797
[09/16 01:36:45 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1427, average loss: 3.1230
[09/16 01:36:45 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 29.00	top5: 53.50	
[09/16 01:37:06 visual_prompt]: 	Test 100/157. loss: 3.121, 0.2040 s / batch. (data: 1.51e-02)max mem: 17.22530 GB 
[09/16 01:37:18 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1919, average loss: 3.5914
[09/16 01:37:18 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 18.63	top5: 43.58	
[09/16 01:37:18 visual_prompt]: Best epoch 11: best metric: 0.290
[09/16 01:37:18 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 01:37:27 visual_prompt]: Epoch 12 / 100: avg data time: 1.06e-01, avg batch time: 0.5058, average train loss: 2.3159
[09/16 01:37:30 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1428, average loss: 1.8527
[09/16 01:37:30 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 51.00	top5: 84.00	
[09/16 01:37:50 visual_prompt]: 	Test 100/157. loss: 2.903, 0.1839 s / batch. (data: 1.26e-04)max mem: 17.22530 GB 
[09/16 01:38:02 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1913, average loss: 2.9432
[09/16 01:38:02 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 33.57	top5: 66.78	
[09/16 01:38:02 visual_prompt]: Best epoch 12: best metric: 0.510
[09/16 01:38:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 01:38:11 visual_prompt]: Epoch 13 / 100: avg data time: 1.07e-01, avg batch time: 0.5076, average train loss: 1.3736
[09/16 01:38:14 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1428, average loss: 0.6819
[09/16 01:38:14 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 83.00	top5: 97.00	
[09/16 01:38:35 visual_prompt]: 	Test 100/157. loss: 1.899, 0.1960 s / batch. (data: 1.39e-02)max mem: 17.22530 GB 
[09/16 01:38:47 visual_prompt]: Inference (test):avg data time: 6.73e-03, avg batch time: 0.1914, average loss: 2.1811
[09/16 01:38:47 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 49.36	top5: 81.91	
[09/16 01:38:47 visual_prompt]: Best epoch 13: best metric: 0.830
[09/16 01:38:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 01:38:56 visual_prompt]: Epoch 14 / 100: avg data time: 9.56e-02, avg batch time: 0.4962, average train loss: 0.7008
[09/16 01:38:59 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1428, average loss: 0.3919
[09/16 01:38:59 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 91.00	top5: 98.50	
[09/16 01:39:20 visual_prompt]: 	Test 100/157. loss: 2.622, 0.1832 s / batch. (data: 1.66e-04)max mem: 17.22530 GB 
[09/16 01:39:31 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1914, average loss: 2.2030
[09/16 01:39:31 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 52.39	top5: 80.79	
[09/16 01:39:31 visual_prompt]: Best epoch 14: best metric: 0.910
[09/16 01:39:31 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 01:39:40 visual_prompt]: Epoch 15 / 100: avg data time: 9.89e-02, avg batch time: 0.5009, average train loss: 0.5422
[09/16 01:39:43 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1427, average loss: 0.3081
[09/16 01:39:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 90.50	top5: 99.50	
[09/16 01:40:04 visual_prompt]: 	Test 100/157. loss: 1.860, 0.2011 s / batch. (data: 1.62e-02)max mem: 17.22530 GB 
[09/16 01:40:16 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1922, average loss: 2.2690
[09/16 01:40:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 55.55	top5: 83.58	
[09/16 01:40:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 01:40:25 visual_prompt]: Epoch 16 / 100: avg data time: 1.15e-01, avg batch time: 0.5158, average train loss: 0.4048
[09/16 01:40:28 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1427, average loss: 0.1898
[09/16 01:40:28 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.50	top5: 99.50	
[09/16 01:40:49 visual_prompt]: 	Test 100/157. loss: 1.685, 0.1826 s / batch. (data: 1.31e-04)max mem: 17.22530 GB 
[09/16 01:41:00 visual_prompt]: Inference (test):avg data time: 6.25e-03, avg batch time: 0.1908, average loss: 2.1007
[09/16 01:41:00 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 55.54	top5: 83.21	
[09/16 01:41:00 visual_prompt]: Best epoch 16: best metric: 0.935
[09/16 01:41:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 01:41:09 visual_prompt]: Epoch 17 / 100: avg data time: 9.90e-02, avg batch time: 0.5006, average train loss: 0.2621
[09/16 01:41:12 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1428, average loss: 0.1854
[09/16 01:41:12 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.50	top5: 100.00	
[09/16 01:41:33 visual_prompt]: 	Test 100/157. loss: 2.103, 0.1950 s / batch. (data: 1.15e-04)max mem: 17.22530 GB 
[09/16 01:41:45 visual_prompt]: Inference (test):avg data time: 6.82e-03, avg batch time: 0.1913, average loss: 2.3534
[09/16 01:41:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.20	top5: 82.98	
[09/16 01:41:45 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 01:41:54 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e-01, avg batch time: 0.5044, average train loss: 0.3267
[09/16 01:41:56 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1427, average loss: 0.3103
[09/16 01:41:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.00	top5: 99.00	
[09/16 01:42:17 visual_prompt]: 	Test 100/157. loss: 2.476, 0.1831 s / batch. (data: 1.11e-04)max mem: 17.22530 GB 
[09/16 01:42:29 visual_prompt]: Inference (test):avg data time: 7.10e-03, avg batch time: 0.1915, average loss: 2.5914
[09/16 01:42:29 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 50.91	top5: 79.77	
[09/16 01:42:29 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 01:42:38 visual_prompt]: Epoch 19 / 100: avg data time: 1.09e-01, avg batch time: 0.5081, average train loss: 0.3410
[09/16 01:42:41 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1426, average loss: 0.2878
[09/16 01:42:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 94.50	top5: 99.00	
[09/16 01:43:02 visual_prompt]: 	Test 100/157. loss: 2.191, 0.1889 s / batch. (data: 3.31e-05)max mem: 17.22530 GB 
[09/16 01:43:14 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1917, average loss: 2.3750
[09/16 01:43:14 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 54.27	top5: 81.81	
[09/16 01:43:14 visual_prompt]: Best epoch 19: best metric: 0.945
[09/16 01:43:14 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 01:43:23 visual_prompt]: Epoch 20 / 100: avg data time: 1.01e-01, avg batch time: 0.5028, average train loss: 0.3342
[09/16 01:43:26 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1428, average loss: 0.3096
[09/16 01:43:26 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 90.50	top5: 98.50	
[09/16 01:43:47 visual_prompt]: 	Test 100/157. loss: 2.055, 0.1826 s / batch. (data: 2.11e-04)max mem: 17.22530 GB 
[09/16 01:43:59 visual_prompt]: Inference (test):avg data time: 8.90e-03, avg batch time: 0.1934, average loss: 2.2425
[09/16 01:43:59 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 54.94	top5: 81.93	
[09/16 01:43:59 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 01:44:07 visual_prompt]: Epoch 21 / 100: avg data time: 9.27e-02, avg batch time: 0.4969, average train loss: 0.2629
[09/16 01:44:10 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.1427, average loss: 0.2232
[09/16 01:44:10 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.50	top5: 100.00	
[09/16 01:44:31 visual_prompt]: 	Test 100/157. loss: 2.005, 0.1833 s / batch. (data: 1.48e-04)max mem: 17.22530 GB 
[09/16 01:44:43 visual_prompt]: Inference (test):avg data time: 6.96e-03, avg batch time: 0.1925, average loss: 2.2275
[09/16 01:44:43 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 56.30	top5: 82.76	
[09/16 01:44:43 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 01:44:52 visual_prompt]: Epoch 22 / 100: avg data time: 1.07e-01, avg batch time: 0.5103, average train loss: 0.1820
[09/16 01:44:55 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1431, average loss: 0.1473
[09/16 01:44:55 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.50	top5: 100.00	
[09/16 01:45:16 visual_prompt]: 	Test 100/157. loss: 2.652, 0.1931 s / batch. (data: 1.11e-02)max mem: 17.22530 GB 
[09/16 01:45:28 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1915, average loss: 2.3258
[09/16 01:45:28 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 56.22	top5: 83.83	
[09/16 01:45:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 01:45:37 visual_prompt]: Epoch 23 / 100: avg data time: 9.44e-02, avg batch time: 0.4965, average train loss: 0.1937
[09/16 01:45:39 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.1427, average loss: 0.2039
[09/16 01:45:39 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 94.50	top5: 99.00	
[09/16 01:46:01 visual_prompt]: 	Test 100/157. loss: 1.993, 0.1824 s / batch. (data: 1.25e-04)max mem: 17.22530 GB 
[09/16 01:46:12 visual_prompt]: Inference (test):avg data time: 8.81e-03, avg batch time: 0.1919, average loss: 2.1174
[09/16 01:46:12 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.54	top5: 83.79	
[09/16 01:46:12 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 01:46:21 visual_prompt]: Epoch 24 / 100: avg data time: 9.79e-02, avg batch time: 0.5017, average train loss: 0.3178
[09/16 01:46:24 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1426, average loss: 1.2672
[09/16 01:46:24 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 68.00	top5: 92.50	
[09/16 01:46:45 visual_prompt]: 	Test 100/157. loss: 3.492, 0.1895 s / batch. (data: 1.46e-04)max mem: 17.22530 GB 
[09/16 01:46:57 visual_prompt]: Inference (test):avg data time: 6.57e-03, avg batch time: 0.1912, average loss: 3.4648
[09/16 01:46:57 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 39.97	top5: 67.82	
[09/16 01:46:57 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 01:47:06 visual_prompt]: Epoch 25 / 100: avg data time: 1.00e-01, avg batch time: 0.5006, average train loss: 0.3744
[09/16 01:47:09 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1425, average loss: 0.2798
[09/16 01:47:09 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 92.50	top5: 99.50	
[09/16 01:47:30 visual_prompt]: 	Test 100/157. loss: 2.811, 0.1824 s / batch. (data: 1.36e-04)max mem: 17.22530 GB 
[09/16 01:47:42 visual_prompt]: Inference (test):avg data time: 7.11e-03, avg batch time: 0.1914, average loss: 2.3791
[09/16 01:47:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 56.69	top5: 82.68	
[09/16 01:47:42 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 01:47:51 visual_prompt]: Epoch 26 / 100: avg data time: 9.54e-02, avg batch time: 0.4966, average train loss: 0.1694
[09/16 01:47:54 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1427, average loss: 0.0680
[09/16 01:47:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.50	top5: 100.00	
[09/16 01:48:15 visual_prompt]: 	Test 100/157. loss: 1.773, 0.1855 s / batch. (data: 1.36e-04)max mem: 17.22530 GB 
[09/16 01:48:26 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1929, average loss: 2.0137
[09/16 01:48:27 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.70	top5: 84.03	
[09/16 01:48:27 visual_prompt]: Best epoch 26: best metric: 0.985
[09/16 01:48:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 01:48:35 visual_prompt]: Epoch 27 / 100: avg data time: 9.24e-02, avg batch time: 0.4969, average train loss: 0.1920
[09/16 01:48:38 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1426, average loss: 0.2527
[09/16 01:48:38 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.00	top5: 98.50	
[09/16 01:48:59 visual_prompt]: 	Test 100/157. loss: 2.656, 0.1952 s / batch. (data: 1.33e-02)max mem: 17.22530 GB 
[09/16 01:49:11 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1914, average loss: 2.4218
[09/16 01:49:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 54.32	top5: 78.73	
[09/16 01:49:11 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 01:49:20 visual_prompt]: Epoch 28 / 100: avg data time: 1.01e-01, avg batch time: 0.5017, average train loss: 0.1630
[09/16 01:49:23 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1424, average loss: 0.1017
[09/16 01:49:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.00	top5: 99.50	
[09/16 01:49:44 visual_prompt]: 	Test 100/157. loss: 1.739, 0.1828 s / batch. (data: 1.41e-04)max mem: 17.22530 GB 
[09/16 01:49:55 visual_prompt]: Inference (test):avg data time: 7.09e-03, avg batch time: 0.1918, average loss: 2.0242
[09/16 01:49:55 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.80	top5: 82.71	
[09/16 01:49:55 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 01:50:04 visual_prompt]: Epoch 29 / 100: avg data time: 1.01e-01, avg batch time: 0.5011, average train loss: 0.1387
[09/16 01:50:07 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1426, average loss: 0.0823
[09/16 01:50:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.50	top5: 99.50	
[09/16 01:50:28 visual_prompt]: 	Test 100/157. loss: 2.141, 0.1907 s / batch. (data: 1.41e-04)max mem: 17.22530 GB 
[09/16 01:50:40 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1919, average loss: 1.9061
[09/16 01:50:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 59.98	top5: 84.84	
[09/16 01:50:40 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 01:50:49 visual_prompt]: Epoch 30 / 100: avg data time: 9.46e-02, avg batch time: 0.4953, average train loss: 0.0753
[09/16 01:50:52 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1427, average loss: 0.0106
[09/16 01:50:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 01:51:12 visual_prompt]: 	Test 100/157. loss: 1.697, 0.1953 s / batch. (data: 1.29e-02)max mem: 17.22530 GB 
[09/16 01:51:24 visual_prompt]: Inference (test):avg data time: 7.02e-03, avg batch time: 0.1907, average loss: 1.8180
[09/16 01:51:24 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 61.99	top5: 85.71	
[09/16 01:51:24 visual_prompt]: Best epoch 30: best metric: 1.000
[09/16 01:51:24 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 01:51:33 visual_prompt]: Epoch 31 / 100: avg data time: 9.22e-02, avg batch time: 0.4942, average train loss: 0.0609
[09/16 01:51:36 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1427, average loss: 0.1728
[09/16 01:51:36 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.50	top5: 99.50	
[09/16 01:51:57 visual_prompt]: 	Test 100/157. loss: 1.713, 0.1918 s / batch. (data: 1.07e-04)max mem: 17.22530 GB 
[09/16 01:52:08 visual_prompt]: Inference (test):avg data time: 6.32e-03, avg batch time: 0.1906, average loss: 1.8872
[09/16 01:52:08 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 61.91	top5: 84.52	
[09/16 01:52:08 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 01:52:17 visual_prompt]: Epoch 32 / 100: avg data time: 9.60e-02, avg batch time: 0.4990, average train loss: 0.0979
[09/16 01:52:20 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.1428, average loss: 0.1885
[09/16 01:52:20 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 95.50	top5: 99.50	
[09/16 01:52:41 visual_prompt]: 	Test 100/157. loss: 2.004, 0.1956 s / batch. (data: 1.29e-02)max mem: 17.22530 GB 
[09/16 01:52:53 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1925, average loss: 1.8856
[09/16 01:52:53 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.80	top5: 84.26	
[09/16 01:52:53 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 01:53:02 visual_prompt]: Epoch 33 / 100: avg data time: 1.07e-01, avg batch time: 0.5094, average train loss: 0.1568
[09/16 01:53:05 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1427, average loss: 0.2294
[09/16 01:53:05 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 95.00	top5: 99.50	
[09/16 01:53:26 visual_prompt]: 	Test 100/157. loss: 1.951, 0.1962 s / batch. (data: 1.38e-02)max mem: 17.22530 GB 
[09/16 01:53:38 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1911, average loss: 2.0308
[09/16 01:53:38 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.87	top5: 82.54	
[09/16 01:53:38 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 01:53:47 visual_prompt]: Epoch 34 / 100: avg data time: 9.79e-02, avg batch time: 0.4984, average train loss: 0.0845
[09/16 01:53:49 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1427, average loss: 0.0229
[09/16 01:53:49 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 01:54:10 visual_prompt]: 	Test 100/157. loss: 1.555, 0.1898 s / batch. (data: 1.49e-04)max mem: 17.22530 GB 
[09/16 01:54:22 visual_prompt]: Inference (test):avg data time: 7.20e-03, avg batch time: 0.1920, average loss: 1.7436
[09/16 01:54:22 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 63.54	top5: 86.08	
[09/16 01:54:22 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 01:54:31 visual_prompt]: Epoch 35 / 100: avg data time: 1.01e-01, avg batch time: 0.5011, average train loss: 0.0456
[09/16 01:54:34 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1428, average loss: 0.0639
[09/16 01:54:34 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.00	top5: 100.00	
[09/16 01:54:55 visual_prompt]: 	Test 100/157. loss: 1.716, 0.1826 s / batch. (data: 1.45e-04)max mem: 17.22530 GB 
[09/16 01:55:07 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1925, average loss: 1.6265
[09/16 01:55:07 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 65.44	top5: 87.21	
[09/16 01:55:07 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 01:55:16 visual_prompt]: Epoch 36 / 100: avg data time: 8.79e-02, avg batch time: 0.4924, average train loss: 0.0279
[09/16 01:55:18 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1426, average loss: 0.0111
[09/16 01:55:18 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 100.00	
[09/16 01:55:39 visual_prompt]: 	Test 100/157. loss: 1.346, 0.1823 s / batch. (data: 1.28e-04)max mem: 17.22530 GB 
[09/16 01:55:51 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1918, average loss: 1.4167
[09/16 01:55:51 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 68.58	top5: 89.56	
[09/16 01:55:51 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 01:56:00 visual_prompt]: Epoch 37 / 100: avg data time: 8.93e-02, avg batch time: 0.4954, average train loss: 0.0165
[09/16 01:56:03 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1427, average loss: 0.0146
[09/16 01:56:03 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 100.00	
[09/16 01:56:24 visual_prompt]: 	Test 100/157. loss: 1.284, 0.1826 s / batch. (data: 1.38e-04)max mem: 17.22530 GB 
[09/16 01:56:36 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1922, average loss: 1.3500
[09/16 01:56:36 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 69.10	top5: 89.29	
[09/16 01:56:36 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 01:56:44 visual_prompt]: Epoch 38 / 100: avg data time: 9.29e-02, avg batch time: 0.4934, average train loss: 0.0110
[09/16 01:56:47 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1427, average loss: 0.0037
[09/16 01:56:47 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 01:57:08 visual_prompt]: 	Test 100/157. loss: 1.192, 0.2034 s / batch. (data: 1.38e-02)max mem: 17.22530 GB 
[09/16 01:57:20 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1917, average loss: 1.2381
[09/16 01:57:20 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 70.88	top5: 91.00	
[09/16 01:57:20 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 01:57:29 visual_prompt]: Epoch 39 / 100: avg data time: 9.55e-02, avg batch time: 0.4966, average train loss: 0.0056
[09/16 01:57:32 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1427, average loss: 0.0034
[09/16 01:57:32 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 01:57:53 visual_prompt]: 	Test 100/157. loss: 1.133, 0.1895 s / batch. (data: 1.16e-04)max mem: 17.22530 GB 
[09/16 01:58:05 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1924, average loss: 1.1839
[09/16 01:58:05 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 71.72	top5: 91.34	
[09/16 01:58:05 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 01:58:13 visual_prompt]: Epoch 40 / 100: avg data time: 9.45e-02, avg batch time: 0.4960, average train loss: 0.0044
[09/16 01:58:16 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1425, average loss: 0.0038
[09/16 01:58:16 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 01:58:37 visual_prompt]: 	Test 100/157. loss: 1.072, 0.2144 s / batch. (data: 2.57e-02)max mem: 17.22530 GB 
[09/16 01:58:49 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1920, average loss: 1.1417
[09/16 01:58:49 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.32	top5: 91.68	
[09/16 01:58:49 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 01:58:58 visual_prompt]: Epoch 41 / 100: avg data time: 1.03e-01, avg batch time: 0.5051, average train loss: 0.0048
[09/16 01:59:01 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1424, average loss: 0.0041
[09/16 01:59:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 01:59:22 visual_prompt]: 	Test 100/157. loss: 1.025, 0.2000 s / batch. (data: 1.82e-02)max mem: 17.22530 GB 
[09/16 01:59:33 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1930, average loss: 1.1208
[09/16 01:59:34 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.89	top5: 91.98	
[09/16 01:59:34 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 01:59:42 visual_prompt]: Epoch 42 / 100: avg data time: 9.51e-02, avg batch time: 0.4966, average train loss: 0.0051
[09/16 01:59:45 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1425, average loss: 0.0048
[09/16 01:59:45 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:00:06 visual_prompt]: 	Test 100/157. loss: 1.017, 0.1962 s / batch. (data: 1.40e-02)max mem: 17.22530 GB 
[09/16 02:00:18 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1927, average loss: 1.0983
[09/16 02:00:18 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.03	top5: 92.21	
[09/16 02:00:18 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 02:00:27 visual_prompt]: Epoch 43 / 100: avg data time: 1.08e-01, avg batch time: 0.5092, average train loss: 0.0057
[09/16 02:00:30 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1426, average loss: 0.0052
[09/16 02:00:30 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:00:51 visual_prompt]: 	Test 100/157. loss: 1.010, 0.2060 s / batch. (data: 1.47e-02)max mem: 17.22530 GB 
[09/16 02:01:03 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1922, average loss: 1.0772
[09/16 02:01:03 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.35	top5: 92.59	
[09/16 02:01:03 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 02:01:12 visual_prompt]: Epoch 44 / 100: avg data time: 9.92e-02, avg batch time: 0.4983, average train loss: 0.0061
[09/16 02:01:14 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1425, average loss: 0.0056
[09/16 02:01:14 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:01:35 visual_prompt]: 	Test 100/157. loss: 1.003, 0.1883 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 02:01:47 visual_prompt]: Inference (test):avg data time: 8.23e-03, avg batch time: 0.1924, average loss: 1.0660
[09/16 02:01:47 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.48	top5: 92.58	
[09/16 02:01:47 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 02:01:56 visual_prompt]: Epoch 45 / 100: avg data time: 1.00e-01, avg batch time: 0.4994, average train loss: 0.0068
[09/16 02:01:59 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1427, average loss: 0.0057
[09/16 02:01:59 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:02:20 visual_prompt]: 	Test 100/157. loss: 0.979, 0.1825 s / batch. (data: 1.55e-04)max mem: 17.22530 GB 
[09/16 02:02:31 visual_prompt]: Inference (test):avg data time: 6.38e-03, avg batch time: 0.1907, average loss: 1.0433
[09/16 02:02:31 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.95	top5: 92.92	
[09/16 02:02:31 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 02:02:40 visual_prompt]: Epoch 46 / 100: avg data time: 8.62e-02, avg batch time: 0.4892, average train loss: 0.0068
[09/16 02:02:43 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1428, average loss: 0.0059
[09/16 02:02:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:03:04 visual_prompt]: 	Test 100/157. loss: 0.957, 0.1846 s / batch. (data: 1.44e-04)max mem: 17.22530 GB 
[09/16 02:03:16 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1918, average loss: 1.0314
[09/16 02:03:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.28	top5: 93.15	
[09/16 02:03:16 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 02:03:24 visual_prompt]: Epoch 47 / 100: avg data time: 9.83e-02, avg batch time: 0.4993, average train loss: 0.0071
[09/16 02:03:27 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1426, average loss: 0.0059
[09/16 02:03:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:03:48 visual_prompt]: 	Test 100/157. loss: 0.938, 0.2126 s / batch. (data: 3.10e-02)max mem: 17.22530 GB 
[09/16 02:04:00 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1916, average loss: 1.0200
[09/16 02:04:00 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.43	top5: 93.23	
[09/16 02:04:00 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 02:04:09 visual_prompt]: Epoch 48 / 100: avg data time: 1.05e-01, avg batch time: 0.5069, average train loss: 0.0070
[09/16 02:04:11 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1426, average loss: 0.0059
[09/16 02:04:11 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:04:32 visual_prompt]: 	Test 100/157. loss: 0.946, 0.1873 s / batch. (data: 6.58e-05)max mem: 17.22530 GB 
[09/16 02:04:44 visual_prompt]: Inference (test):avg data time: 5.56e-03, avg batch time: 0.1911, average loss: 1.0050
[09/16 02:04:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.55	top5: 93.57	
[09/16 02:04:44 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 02:04:53 visual_prompt]: Epoch 49 / 100: avg data time: 9.40e-02, avg batch time: 0.4967, average train loss: 0.0069
[09/16 02:04:56 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1426, average loss: 0.0056
[09/16 02:04:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:05:16 visual_prompt]: 	Test 100/157. loss: 0.946, 0.1824 s / batch. (data: 1.31e-04)max mem: 17.22530 GB 
[09/16 02:05:28 visual_prompt]: Inference (test):avg data time: 6.74e-03, avg batch time: 0.1916, average loss: 0.9976
[09/16 02:05:28 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.90	top5: 93.66	
[09/16 02:05:28 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 02:05:38 visual_prompt]: Epoch 50 / 100: avg data time: 9.47e-02, avg batch time: 0.5472, average train loss: 0.0069
[09/16 02:05:41 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1428, average loss: 0.0057
[09/16 02:05:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:06:02 visual_prompt]: 	Test 100/157. loss: 0.886, 0.1825 s / batch. (data: 1.11e-04)max mem: 17.22530 GB 
[09/16 02:06:13 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1916, average loss: 0.9898
[09/16 02:06:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.97	top5: 93.97	
[09/16 02:06:13 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 02:06:22 visual_prompt]: Epoch 51 / 100: avg data time: 9.69e-02, avg batch time: 0.4982, average train loss: 0.0068
[09/16 02:06:25 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1427, average loss: 0.0055
[09/16 02:06:25 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:06:46 visual_prompt]: 	Test 100/157. loss: 0.888, 0.2117 s / batch. (data: 5.60e-05)max mem: 17.22530 GB 
[09/16 02:06:58 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1926, average loss: 0.9749
[09/16 02:06:58 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.36	top5: 94.09	
[09/16 02:06:58 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 02:07:07 visual_prompt]: Epoch 52 / 100: avg data time: 1.06e-01, avg batch time: 0.5074, average train loss: 0.0066
[09/16 02:07:10 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1426, average loss: 0.0051
[09/16 02:07:10 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:07:32 visual_prompt]: 	Test 100/157. loss: 0.878, 0.1949 s / batch. (data: 1.28e-02)max mem: 17.22530 GB 
[09/16 02:07:44 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1953, average loss: 0.9728
[09/16 02:07:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.19	top5: 94.16	
[09/16 02:07:44 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 02:07:53 visual_prompt]: Epoch 53 / 100: avg data time: 1.03e-01, avg batch time: 0.5048, average train loss: 0.0063
[09/16 02:07:55 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1425, average loss: 0.0050
[09/16 02:07:55 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:08:16 visual_prompt]: 	Test 100/157. loss: 0.889, 0.1833 s / batch. (data: 8.92e-05)max mem: 17.22530 GB 
[09/16 02:08:28 visual_prompt]: Inference (test):avg data time: 6.84e-03, avg batch time: 0.1919, average loss: 0.9671
[09/16 02:08:28 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.43	top5: 94.31	
[09/16 02:08:28 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 02:08:37 visual_prompt]: Epoch 54 / 100: avg data time: 1.01e-01, avg batch time: 0.5032, average train loss: 0.0063
[09/16 02:08:40 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1427, average loss: 0.0049
[09/16 02:08:40 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:09:01 visual_prompt]: 	Test 100/157. loss: 0.892, 0.2135 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 02:09:13 visual_prompt]: Inference (test):avg data time: 6.73e-03, avg batch time: 0.1942, average loss: 0.9590
[09/16 02:09:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.49	top5: 94.44	
[09/16 02:09:13 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 02:09:23 visual_prompt]: Epoch 55 / 100: avg data time: 1.05e-01, avg batch time: 0.5449, average train loss: 0.0061
[09/16 02:09:25 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1427, average loss: 0.0046
[09/16 02:09:25 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:09:46 visual_prompt]: 	Test 100/157. loss: 0.886, 0.1931 s / batch. (data: 1.44e-04)max mem: 17.22530 GB 
[09/16 02:09:58 visual_prompt]: Inference (test):avg data time: 6.89e-03, avg batch time: 0.1926, average loss: 0.9577
[09/16 02:09:58 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.39	top5: 94.48	
[09/16 02:09:58 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 02:10:07 visual_prompt]: Epoch 56 / 100: avg data time: 1.02e-01, avg batch time: 0.5021, average train loss: 0.0059
[09/16 02:10:10 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1426, average loss: 0.0046
[09/16 02:10:10 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:10:31 visual_prompt]: 	Test 100/157. loss: 0.879, 0.1827 s / batch. (data: 1.35e-04)max mem: 17.22530 GB 
[09/16 02:10:42 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1909, average loss: 0.9672
[09/16 02:10:43 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.07	top5: 94.57	
[09/16 02:10:43 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 02:10:51 visual_prompt]: Epoch 57 / 100: avg data time: 1.07e-01, avg batch time: 0.5094, average train loss: 0.0060
[09/16 02:10:54 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1427, average loss: 0.0050
[09/16 02:10:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:11:15 visual_prompt]: 	Test 100/157. loss: 0.918, 0.1976 s / batch. (data: 1.59e-02)max mem: 17.22530 GB 
[09/16 02:11:27 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1912, average loss: 0.9626
[09/16 02:11:27 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.24	top5: 94.47	
[09/16 02:11:27 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 02:11:36 visual_prompt]: Epoch 58 / 100: avg data time: 9.63e-02, avg batch time: 0.4995, average train loss: 0.0059
[09/16 02:11:39 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1428, average loss: 0.0043
[09/16 02:11:39 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:12:00 visual_prompt]: 	Test 100/157. loss: 0.840, 0.1995 s / batch. (data: 1.77e-02)max mem: 17.22530 GB 
[09/16 02:12:11 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1912, average loss: 0.9520
[09/16 02:12:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.85	top5: 94.68	
[09/16 02:12:11 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 02:12:20 visual_prompt]: Epoch 59 / 100: avg data time: 9.51e-02, avg batch time: 0.4950, average train loss: 0.0055
[09/16 02:12:23 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1426, average loss: 0.0042
[09/16 02:12:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:12:44 visual_prompt]: 	Test 100/157. loss: 0.885, 0.1826 s / batch. (data: 1.39e-04)max mem: 17.22530 GB 
[09/16 02:12:56 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1919, average loss: 0.9780
[09/16 02:12:56 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.99	top5: 94.68	
[09/16 02:12:56 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 02:13:05 visual_prompt]: Epoch 60 / 100: avg data time: 1.06e-01, avg batch time: 0.5067, average train loss: 0.0053
[09/16 02:13:08 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1426, average loss: 0.0040
[09/16 02:13:08 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:13:29 visual_prompt]: 	Test 100/157. loss: 0.906, 0.1827 s / batch. (data: 1.20e-04)max mem: 17.22530 GB 
[09/16 02:13:40 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1916, average loss: 0.9688
[09/16 02:13:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.28	top5: 94.46	
[09/16 02:13:40 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 02:13:49 visual_prompt]: Epoch 61 / 100: avg data time: 1.04e-01, avg batch time: 0.5081, average train loss: 0.0051
[09/16 02:13:52 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1426, average loss: 0.0039
[09/16 02:13:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:14:13 visual_prompt]: 	Test 100/157. loss: 0.876, 0.1919 s / batch. (data: 1.12e-04)max mem: 17.22530 GB 
[09/16 02:14:25 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1921, average loss: 0.9757
[09/16 02:14:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.08	top5: 94.59	
[09/16 02:14:25 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 02:14:34 visual_prompt]: Epoch 62 / 100: avg data time: 1.03e-01, avg batch time: 0.5046, average train loss: 0.0049
[09/16 02:14:37 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1428, average loss: 0.0038
[09/16 02:14:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:14:58 visual_prompt]: 	Test 100/157. loss: 0.883, 0.2253 s / batch. (data: 4.32e-02)max mem: 17.22530 GB 
[09/16 02:15:09 visual_prompt]: Inference (test):avg data time: 8.54e-03, avg batch time: 0.1928, average loss: 0.9716
[09/16 02:15:09 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.53	top5: 94.78	
[09/16 02:15:09 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 02:15:18 visual_prompt]: Epoch 63 / 100: avg data time: 1.01e-01, avg batch time: 0.5022, average train loss: 0.0049
[09/16 02:15:21 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 0.0037
[09/16 02:15:21 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:15:42 visual_prompt]: 	Test 100/157. loss: 0.914, 0.2096 s / batch. (data: 1.30e-02)max mem: 17.22530 GB 
[09/16 02:15:54 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1915, average loss: 0.9715
[09/16 02:15:54 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.61	top5: 94.66	
[09/16 02:15:54 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 02:16:03 visual_prompt]: Epoch 64 / 100: avg data time: 9.21e-02, avg batch time: 0.4945, average train loss: 0.0048
[09/16 02:16:05 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1427, average loss: 0.0036
[09/16 02:16:05 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:16:26 visual_prompt]: 	Test 100/157. loss: 0.841, 0.1984 s / batch. (data: 1.28e-02)max mem: 17.22530 GB 
[09/16 02:16:38 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1913, average loss: 0.9761
[09/16 02:16:38 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.11	top5: 94.63	
[09/16 02:16:38 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 02:16:47 visual_prompt]: Epoch 65 / 100: avg data time: 9.18e-02, avg batch time: 0.4941, average train loss: 0.0048
[09/16 02:16:50 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1426, average loss: 0.0035
[09/16 02:16:50 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:17:11 visual_prompt]: 	Test 100/157. loss: 0.882, 0.1973 s / batch. (data: 1.54e-02)max mem: 17.22530 GB 
[09/16 02:17:23 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1920, average loss: 0.9998
[09/16 02:17:23 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.83	top5: 94.56	
[09/16 02:17:23 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 02:17:32 visual_prompt]: Epoch 66 / 100: avg data time: 9.91e-02, avg batch time: 0.5012, average train loss: 0.0046
[09/16 02:17:34 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1427, average loss: 0.0034
[09/16 02:17:34 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:17:56 visual_prompt]: 	Test 100/157. loss: 0.840, 0.2041 s / batch. (data: 2.22e-02)max mem: 17.22530 GB 
[09/16 02:18:07 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1925, average loss: 0.9911
[09/16 02:18:07 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.12	top5: 94.82	
[09/16 02:18:07 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 02:18:16 visual_prompt]: Epoch 67 / 100: avg data time: 8.61e-02, avg batch time: 0.4912, average train loss: 0.0044
[09/16 02:18:19 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1426, average loss: 0.0032
[09/16 02:18:19 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:18:40 visual_prompt]: 	Test 100/157. loss: 0.873, 0.1930 s / batch. (data: 1.11e-02)max mem: 17.22530 GB 
[09/16 02:18:52 visual_prompt]: Inference (test):avg data time: 6.26e-03, avg batch time: 0.1915, average loss: 0.9973
[09/16 02:18:52 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.07	top5: 94.49	
[09/16 02:18:52 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 02:19:01 visual_prompt]: Epoch 68 / 100: avg data time: 1.07e-01, avg batch time: 0.5098, average train loss: 0.0042
[09/16 02:19:03 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1428, average loss: 0.0031
[09/16 02:19:03 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:19:24 visual_prompt]: 	Test 100/157. loss: 0.907, 0.1828 s / batch. (data: 1.47e-04)max mem: 17.22530 GB 
[09/16 02:19:36 visual_prompt]: Inference (test):avg data time: 7.10e-03, avg batch time: 0.1919, average loss: 1.0077
[09/16 02:19:36 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.01	top5: 94.68	
[09/16 02:19:36 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 02:19:45 visual_prompt]: Epoch 69 / 100: avg data time: 1.04e-01, avg batch time: 0.5068, average train loss: 0.0042
[09/16 02:19:48 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1425, average loss: 0.0031
[09/16 02:19:48 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:20:09 visual_prompt]: 	Test 100/157. loss: 0.910, 0.1827 s / batch. (data: 1.07e-04)max mem: 17.22530 GB 
[09/16 02:20:21 visual_prompt]: Inference (test):avg data time: 6.68e-03, avg batch time: 0.1919, average loss: 0.9989
[09/16 02:20:21 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.05	top5: 94.83	
[09/16 02:20:21 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 02:20:30 visual_prompt]: Epoch 70 / 100: avg data time: 9.16e-02, avg batch time: 0.4939, average train loss: 0.0041
[09/16 02:20:33 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1427, average loss: 0.0032
[09/16 02:20:33 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:20:54 visual_prompt]: 	Test 100/157. loss: 0.913, 0.1824 s / batch. (data: 1.31e-04)max mem: 17.22530 GB 
[09/16 02:21:05 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1922, average loss: 1.0119
[09/16 02:21:05 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.79	top5: 94.62	
[09/16 02:21:05 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 02:21:14 visual_prompt]: Epoch 71 / 100: avg data time: 9.97e-02, avg batch time: 0.5008, average train loss: 0.0041
[09/16 02:21:17 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1428, average loss: 0.0030
[09/16 02:21:17 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:21:38 visual_prompt]: 	Test 100/157. loss: 0.900, 0.1938 s / batch. (data: 1.12e-02)max mem: 17.22530 GB 
[09/16 02:21:50 visual_prompt]: Inference (test):avg data time: 6.70e-03, avg batch time: 0.1912, average loss: 1.0227
[09/16 02:21:50 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.59	top5: 94.44	
[09/16 02:21:50 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 02:21:59 visual_prompt]: Epoch 72 / 100: avg data time: 8.18e-02, avg batch time: 0.4848, average train loss: 0.0042
[09/16 02:22:01 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1428, average loss: 0.0035
[09/16 02:22:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:22:22 visual_prompt]: 	Test 100/157. loss: 0.981, 0.1837 s / batch. (data: 1.26e-04)max mem: 17.22530 GB 
[09/16 02:22:34 visual_prompt]: Inference (test):avg data time: 8.72e-03, avg batch time: 0.1937, average loss: 1.0383
[09/16 02:22:34 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.98	top5: 94.40	
[09/16 02:22:34 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 02:22:43 visual_prompt]: Epoch 73 / 100: avg data time: 1.05e-01, avg batch time: 0.5042, average train loss: 0.0043
[09/16 02:22:46 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1429, average loss: 0.0031
[09/16 02:22:46 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:23:08 visual_prompt]: 	Test 100/157. loss: 0.870, 0.1971 s / batch. (data: 1.43e-02)max mem: 17.22530 GB 
[09/16 02:23:20 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1985, average loss: 1.0142
[09/16 02:23:20 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.02	top5: 94.52	
[09/16 02:23:20 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 02:23:28 visual_prompt]: Epoch 74 / 100: avg data time: 8.09e-02, avg batch time: 0.4854, average train loss: 0.0041
[09/16 02:23:31 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1428, average loss: 0.0031
[09/16 02:23:31 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:23:52 visual_prompt]: 	Test 100/157. loss: 0.877, 0.2014 s / batch. (data: 9.47e-05)max mem: 17.22530 GB 
[09/16 02:24:04 visual_prompt]: Inference (test):avg data time: 5.75e-03, avg batch time: 0.1908, average loss: 1.0471
[09/16 02:24:04 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.96	top5: 94.41	
[09/16 02:24:04 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 02:24:13 visual_prompt]: Epoch 75 / 100: avg data time: 9.37e-02, avg batch time: 0.4961, average train loss: 0.0039
[09/16 02:24:15 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1426, average loss: 0.0031
[09/16 02:24:15 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:24:36 visual_prompt]: 	Test 100/157. loss: 0.865, 0.1967 s / batch. (data: 1.46e-02)max mem: 17.22530 GB 
[09/16 02:24:48 visual_prompt]: Inference (test):avg data time: 6.57e-03, avg batch time: 0.1933, average loss: 1.0473
[09/16 02:24:49 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.40	top5: 94.40	
[09/16 02:24:49 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 02:24:58 visual_prompt]: Epoch 76 / 100: avg data time: 9.00e-02, avg batch time: 0.5486, average train loss: 0.0039
[09/16 02:25:01 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1426, average loss: 0.0029
[09/16 02:25:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:25:22 visual_prompt]: 	Test 100/157. loss: 0.898, 0.1825 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 02:25:34 visual_prompt]: Inference (test):avg data time: 6.29e-03, avg batch time: 0.1908, average loss: 1.0455
[09/16 02:25:34 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.90	top5: 94.51	
[09/16 02:25:34 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 02:25:43 visual_prompt]: Epoch 77 / 100: avg data time: 9.37e-02, avg batch time: 0.4949, average train loss: 0.0038
[09/16 02:25:46 visual_prompt]: Inference (val):avg data time: 1.89e-05, avg batch time: 0.1425, average loss: 0.0029
[09/16 02:25:46 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:26:07 visual_prompt]: 	Test 100/157. loss: 0.906, 0.1827 s / batch. (data: 1.37e-04)max mem: 17.22530 GB 
[09/16 02:26:19 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1949, average loss: 1.0647
[09/16 02:26:19 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.98	top5: 94.34	
[09/16 02:26:19 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 02:26:28 visual_prompt]: Epoch 78 / 100: avg data time: 1.03e-01, avg batch time: 0.5051, average train loss: 0.0038
[09/16 02:26:31 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1427, average loss: 0.0028
[09/16 02:26:31 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:26:51 visual_prompt]: 	Test 100/157. loss: 0.888, 0.1981 s / batch. (data: 1.57e-02)max mem: 17.22530 GB 
[09/16 02:27:03 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1920, average loss: 1.0619
[09/16 02:27:03 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.21	top5: 94.49	
[09/16 02:27:03 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 02:27:12 visual_prompt]: Epoch 79 / 100: avg data time: 1.03e-01, avg batch time: 0.5037, average train loss: 0.0037
[09/16 02:27:15 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1426, average loss: 0.0029
[09/16 02:27:15 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:27:36 visual_prompt]: 	Test 100/157. loss: 0.874, 0.1828 s / batch. (data: 3.31e-05)max mem: 17.22530 GB 
[09/16 02:27:48 visual_prompt]: Inference (test):avg data time: 5.77e-03, avg batch time: 0.1909, average loss: 1.0533
[09/16 02:27:48 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.11	top5: 94.42	
[09/16 02:27:48 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 02:27:57 visual_prompt]: Epoch 80 / 100: avg data time: 1.05e-01, avg batch time: 0.5053, average train loss: 0.0037
[09/16 02:28:00 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1427, average loss: 0.0029
[09/16 02:28:00 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:28:21 visual_prompt]: 	Test 100/157. loss: 0.878, 0.2065 s / batch. (data: 1.15e-04)max mem: 17.22530 GB 
[09/16 02:28:32 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1925, average loss: 1.0542
[09/16 02:28:32 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.15	top5: 94.44	
[09/16 02:28:32 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 02:28:42 visual_prompt]: Epoch 81 / 100: avg data time: 1.08e-01, avg batch time: 0.5105, average train loss: 0.0036
[09/16 02:28:44 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1427, average loss: 0.0029
[09/16 02:28:44 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:29:05 visual_prompt]: 	Test 100/157. loss: 0.915, 0.1875 s / batch. (data: 5.43e-03)max mem: 17.22530 GB 
[09/16 02:29:17 visual_prompt]: Inference (test):avg data time: 6.19e-03, avg batch time: 0.1923, average loss: 1.0676
[09/16 02:29:17 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.80	top5: 94.35	
[09/16 02:29:17 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 02:29:26 visual_prompt]: Epoch 82 / 100: avg data time: 9.99e-02, avg batch time: 0.5012, average train loss: 0.0036
[09/16 02:29:29 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1425, average loss: 0.0028
[09/16 02:29:29 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:29:50 visual_prompt]: 	Test 100/157. loss: 0.917, 0.1966 s / batch. (data: 1.51e-02)max mem: 17.22530 GB 
[09/16 02:30:01 visual_prompt]: Inference (test):avg data time: 5.56e-03, avg batch time: 0.1912, average loss: 1.0717
[09/16 02:30:01 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.85	top5: 94.30	
[09/16 02:30:01 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 02:30:10 visual_prompt]: Epoch 83 / 100: avg data time: 9.76e-02, avg batch time: 0.4986, average train loss: 0.0036
[09/16 02:30:13 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1425, average loss: 0.0028
[09/16 02:30:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:30:34 visual_prompt]: 	Test 100/157. loss: 0.905, 0.1830 s / batch. (data: 1.18e-04)max mem: 17.22530 GB 
[09/16 02:30:46 visual_prompt]: Inference (test):avg data time: 6.68e-03, avg batch time: 0.1921, average loss: 1.0802
[09/16 02:30:46 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.66	top5: 94.31	
[09/16 02:30:46 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 02:30:55 visual_prompt]: Epoch 84 / 100: avg data time: 1.06e-01, avg batch time: 0.5056, average train loss: 0.0036
[09/16 02:30:58 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1426, average loss: 0.0028
[09/16 02:30:58 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:31:19 visual_prompt]: 	Test 100/157. loss: 0.891, 0.1944 s / batch. (data: 1.13e-02)max mem: 17.22530 GB 
[09/16 02:31:30 visual_prompt]: Inference (test):avg data time: 7.50e-03, avg batch time: 0.1928, average loss: 1.0878
[09/16 02:31:31 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.72	top5: 94.25	
[09/16 02:31:31 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 02:31:39 visual_prompt]: Epoch 85 / 100: avg data time: 1.06e-01, avg batch time: 0.5078, average train loss: 0.0036
[09/16 02:31:42 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1433, average loss: 0.0028
[09/16 02:31:42 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:32:03 visual_prompt]: 	Test 100/157. loss: 0.880, 0.1832 s / batch. (data: 1.10e-04)max mem: 17.22530 GB 
[09/16 02:32:15 visual_prompt]: Inference (test):avg data time: 6.34e-03, avg batch time: 0.1907, average loss: 1.0818
[09/16 02:32:15 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.75	top5: 94.31	
[09/16 02:32:15 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 02:32:24 visual_prompt]: Epoch 86 / 100: avg data time: 1.06e-01, avg batch time: 0.5062, average train loss: 0.0035
[09/16 02:32:27 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1426, average loss: 0.0028
[09/16 02:32:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:32:47 visual_prompt]: 	Test 100/157. loss: 0.899, 0.1982 s / batch. (data: 1.60e-02)max mem: 17.22530 GB 
[09/16 02:32:59 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1925, average loss: 1.0920
[09/16 02:32:59 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.61	top5: 94.23	
[09/16 02:32:59 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 02:33:09 visual_prompt]: Epoch 87 / 100: avg data time: 1.06e-01, avg batch time: 0.5364, average train loss: 0.0035
[09/16 02:33:12 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1426, average loss: 0.0028
[09/16 02:33:12 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:33:32 visual_prompt]: 	Test 100/157. loss: 0.901, 0.1972 s / batch. (data: 1.40e-02)max mem: 17.22530 GB 
[09/16 02:33:44 visual_prompt]: Inference (test):avg data time: 6.71e-03, avg batch time: 0.1916, average loss: 1.0882
[09/16 02:33:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.72	top5: 94.30	
[09/16 02:33:44 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 02:33:53 visual_prompt]: Epoch 88 / 100: avg data time: 1.04e-01, avg batch time: 0.5065, average train loss: 0.0035
[09/16 02:33:56 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1426, average loss: 0.0028
[09/16 02:33:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:34:17 visual_prompt]: 	Test 100/157. loss: 0.923, 0.1967 s / batch. (data: 1.44e-02)max mem: 17.22530 GB 
[09/16 02:34:29 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1922, average loss: 1.0949
[09/16 02:34:29 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.47	top5: 94.20	
[09/16 02:34:29 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 02:34:38 visual_prompt]: Epoch 89 / 100: avg data time: 8.97e-02, avg batch time: 0.4947, average train loss: 0.0036
[09/16 02:34:41 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1426, average loss: 0.0028
[09/16 02:34:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:35:01 visual_prompt]: 	Test 100/157. loss: 0.933, 0.1956 s / batch. (data: 1.36e-02)max mem: 17.22530 GB 
[09/16 02:35:13 visual_prompt]: Inference (test):avg data time: 6.28e-03, avg batch time: 0.1914, average loss: 1.0988
[09/16 02:35:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.22	top5: 94.17	
[09/16 02:35:13 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 02:35:22 visual_prompt]: Epoch 90 / 100: avg data time: 1.10e-01, avg batch time: 0.5096, average train loss: 0.0035
[09/16 02:35:25 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1426, average loss: 0.0028
[09/16 02:35:25 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:35:46 visual_prompt]: 	Test 100/157. loss: 0.917, 0.1828 s / batch. (data: 1.34e-04)max mem: 17.22530 GB 
[09/16 02:35:58 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1918, average loss: 1.1015
[09/16 02:35:58 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.31	top5: 94.18	
[09/16 02:35:58 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 02:36:07 visual_prompt]: Epoch 91 / 100: avg data time: 9.99e-02, avg batch time: 0.5014, average train loss: 0.0035
[09/16 02:36:10 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1428, average loss: 0.0028
[09/16 02:36:10 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:36:30 visual_prompt]: 	Test 100/157. loss: 0.907, 0.1839 s / batch. (data: 1.28e-04)max mem: 17.22530 GB 
[09/16 02:36:42 visual_prompt]: Inference (test):avg data time: 6.65e-03, avg batch time: 0.1918, average loss: 1.0976
[09/16 02:36:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.36	top5: 94.20	
[09/16 02:36:42 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 02:36:51 visual_prompt]: Epoch 92 / 100: avg data time: 1.03e-01, avg batch time: 0.5039, average train loss: 0.0035
[09/16 02:36:54 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1426, average loss: 0.0027
[09/16 02:36:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:37:15 visual_prompt]: 	Test 100/157. loss: 0.897, 0.1908 s / batch. (data: 1.13e-04)max mem: 17.22530 GB 
[09/16 02:37:27 visual_prompt]: Inference (test):avg data time: 6.36e-03, avg batch time: 0.1934, average loss: 1.0962
[09/16 02:37:27 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.46	top5: 94.26	
[09/16 02:37:27 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 02:37:36 visual_prompt]: Epoch 93 / 100: avg data time: 9.36e-02, avg batch time: 0.4938, average train loss: 0.0035
[09/16 02:37:39 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1425, average loss: 0.0027
[09/16 02:37:39 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:38:00 visual_prompt]: 	Test 100/157. loss: 0.899, 0.2013 s / batch. (data: 1.86e-02)max mem: 17.22530 GB 
[09/16 02:38:11 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1919, average loss: 1.0976
[09/16 02:38:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.40	top5: 94.21	
[09/16 02:38:11 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 02:38:20 visual_prompt]: Epoch 94 / 100: avg data time: 1.06e-01, avg batch time: 0.5099, average train loss: 0.0035
[09/16 02:38:23 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1427, average loss: 0.0028
[09/16 02:38:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:38:44 visual_prompt]: 	Test 100/157. loss: 0.909, 0.2010 s / batch. (data: 1.92e-02)max mem: 17.22530 GB 
[09/16 02:38:56 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1916, average loss: 1.0990
[09/16 02:38:56 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.39	top5: 94.21	
[09/16 02:38:56 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 02:39:05 visual_prompt]: Epoch 95 / 100: avg data time: 9.96e-02, avg batch time: 0.4989, average train loss: 0.0035
[09/16 02:39:07 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1424, average loss: 0.0028
[09/16 02:39:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:39:28 visual_prompt]: 	Test 100/157. loss: 0.910, 0.2029 s / batch. (data: 4.10e-05)max mem: 17.22530 GB 
[09/16 02:39:40 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1914, average loss: 1.1012
[09/16 02:39:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.30	top5: 94.17	
[09/16 02:39:40 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 02:39:49 visual_prompt]: Epoch 96 / 100: avg data time: 9.01e-02, avg batch time: 0.5158, average train loss: 0.0035
[09/16 02:39:52 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1427, average loss: 0.0027
[09/16 02:39:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:40:13 visual_prompt]: 	Test 100/157. loss: 0.910, 0.1825 s / batch. (data: 8.23e-05)max mem: 17.22530 GB 
[09/16 02:40:25 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1926, average loss: 1.1008
[09/16 02:40:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.35	top5: 94.19	
[09/16 02:40:25 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 02:40:34 visual_prompt]: Epoch 97 / 100: avg data time: 9.27e-02, avg batch time: 0.4926, average train loss: 0.0035
[09/16 02:40:37 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1426, average loss: 0.0027
[09/16 02:40:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:40:57 visual_prompt]: 	Test 100/157. loss: 0.909, 0.1839 s / batch. (data: 1.49e-04)max mem: 17.22530 GB 
[09/16 02:41:09 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1919, average loss: 1.1006
[09/16 02:41:09 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.33	top5: 94.22	
[09/16 02:41:09 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 02:41:18 visual_prompt]: Epoch 98 / 100: avg data time: 9.76e-02, avg batch time: 0.5001, average train loss: 0.0035
[09/16 02:41:21 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1427, average loss: 0.0027
[09/16 02:41:21 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:41:42 visual_prompt]: 	Test 100/157. loss: 0.907, 0.1823 s / batch. (data: 1.23e-04)max mem: 17.22530 GB 
[09/16 02:41:54 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1943, average loss: 1.1004
[09/16 02:41:54 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.32	top5: 94.22	
[09/16 02:41:54 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 02:42:03 visual_prompt]: Epoch 99 / 100: avg data time: 1.02e-01, avg batch time: 0.5037, average train loss: 0.0035
[09/16 02:42:05 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 0.0027
[09/16 02:42:05 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:42:26 visual_prompt]: 	Test 100/157. loss: 0.908, 0.1943 s / batch. (data: 9.16e-05)max mem: 17.22530 GB 
[09/16 02:42:38 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1924, average loss: 1.1006
[09/16 02:42:38 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.32	top5: 94.23	
[09/16 02:42:38 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 02:42:47 visual_prompt]: Epoch 100 / 100: avg data time: 8.35e-02, avg batch time: 0.4904, average train loss: 0.0035
[09/16 02:42:50 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1425, average loss: 0.0027
[09/16 02:42:50 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 02:43:11 visual_prompt]: 	Test 100/157. loss: 0.908, 0.2040 s / batch. (data: 1.55e-02)max mem: 17.22530 GB 
[09/16 02:43:23 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1924, average loss: 1.1007
[09/16 02:43:23 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.32	top5: 94.23	
[09/16 02:43:37 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 02:43:37 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 02:43:37 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-cifar(num_classes=100)', 'DATA.NUMBER_CLASSES', '100', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed44'], train_type='')
[09/16 02:43:37 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 02:43:37 visual_prompt]: Training with config:
[09/16 02:43:37 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-cifar(num_classes=100)',
          'NO_TEST': False,
          'NUMBER_CLASSES': 100,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed44/vtab-cifar(num_classes=100)/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 02:43:37 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 02:43:37.922974: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 02:43:38.106567: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 02:43:38.952357: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 02:43:38.952434: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 02:43:38.952443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 02:43:40.704044: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 02:43:40.704159: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 02:43:40.704175: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 02:43:40 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
2023-09-16 02:43:40.721479: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split train[:800]+train[45000:45200], from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 02:43:42 visual_prompt]: Number of images: 1000
[09/16 02:43:42 visual_prompt]: Number of classes: 100 / 100
[09/16 02:43:42 visual_prompt]: Loading validation data...
[09/16 02:43:42 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split train[45000:45200], from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 02:43:42 visual_prompt]: Number of images: 200
[09/16 02:43:42 visual_prompt]: Number of classes: 90 / 100
[09/16 02:43:42 visual_prompt]: Loading test data...
[09/16 02:43:42 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split test, from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 02:43:55 visual_prompt]: Number of images: 10000
[09/16 02:43:55 visual_prompt]: Number of classes: 100 / 100
[09/16 02:43:55 visual_prompt]: Constructing models...
[09/16 02:43:58 visual_prompt]: Total Parameters: 86797156	 Gradient Parameters: 998500
[09/16 02:43:58 visual_prompt]: tuned percent:1.150
[09/16 02:44:00 visual_prompt]: Device used for model: 0
[09/16 02:44:00 visual_prompt]: Setting up Evalutator...
[09/16 02:44:00 visual_prompt]: Setting up Trainer...
[09/16 02:44:00 visual_prompt]: 	Setting up the optimizer...
[09/16 02:44:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 02:44:11 visual_prompt]: Epoch 1 / 100: avg data time: 1.05e-01, avg batch time: 0.5873, average train loss: 4.6408
[09/16 02:44:13 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1418, average loss: 4.6356
[09/16 02:44:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 2.00	top5: 4.50	
[09/16 02:44:34 visual_prompt]: 	Test 100/157. loss: 4.649, 0.2048 s / batch. (data: 2.37e-02)max mem: 17.22530 GB 
[09/16 02:44:46 visual_prompt]: Inference (test):avg data time: 8.41e-03, avg batch time: 0.1920, average loss: 4.6427
[09/16 02:44:46 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.17	top5: 4.65	
[09/16 02:44:46 visual_prompt]: Best epoch 1: best metric: 0.020
[09/16 02:44:46 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 02:44:55 visual_prompt]: Epoch 2 / 100: avg data time: 9.70e-02, avg batch time: 0.4976, average train loss: 4.6363
[09/16 02:44:58 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1420, average loss: 4.5795
[09/16 02:44:58 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 0.50	top5: 6.00	
[09/16 02:45:19 visual_prompt]: 	Test 100/157. loss: 4.669, 0.1826 s / batch. (data: 1.16e-04)max mem: 17.22530 GB 
[09/16 02:45:31 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1964, average loss: 4.6976
[09/16 02:45:31 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.00	
[09/16 02:45:31 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 02:45:40 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e-01, avg batch time: 0.5040, average train loss: 4.6808
[09/16 02:45:43 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1424, average loss: 4.5931
[09/16 02:45:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.00	top5: 6.00	
[09/16 02:46:04 visual_prompt]: 	Test 100/157. loss: 4.690, 0.1824 s / batch. (data: 1.44e-04)max mem: 17.22530 GB 
[09/16 02:46:16 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1911, average loss: 4.6902
[09/16 02:46:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.25	
[09/16 02:46:16 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 02:46:24 visual_prompt]: Epoch 4 / 100: avg data time: 9.82e-02, avg batch time: 0.5025, average train loss: 4.7068
[09/16 02:46:27 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1426, average loss: 4.5757
[09/16 02:46:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 0.50	top5: 7.00	
[09/16 02:46:48 visual_prompt]: 	Test 100/157. loss: 4.585, 0.1962 s / batch. (data: 1.45e-02)max mem: 17.22530 GB 
[09/16 02:47:00 visual_prompt]: Inference (test):avg data time: 8.61e-03, avg batch time: 0.1926, average loss: 4.6762
[09/16 02:47:00 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.01	top5: 6.13	
[09/16 02:47:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 02:47:09 visual_prompt]: Epoch 5 / 100: avg data time: 1.10e-01, avg batch time: 0.5096, average train loss: 4.6897
[09/16 02:47:12 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1425, average loss: 4.7278
[09/16 02:47:12 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 2.00	top5: 11.00	
[09/16 02:47:33 visual_prompt]: 	Test 100/157. loss: 4.580, 0.1916 s / batch. (data: 1.63e-04)max mem: 17.22530 GB 
[09/16 02:47:45 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1932, average loss: 4.8050
[09/16 02:47:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 2.15	top5: 8.39	
[09/16 02:47:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 02:47:54 visual_prompt]: Epoch 6 / 100: avg data time: 8.44e-02, avg batch time: 0.4892, average train loss: 4.7152
[09/16 02:47:56 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1428, average loss: 4.5649
[09/16 02:47:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 2.50	top5: 9.00	
[09/16 02:48:17 visual_prompt]: 	Test 100/157. loss: 4.469, 0.1830 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 02:48:29 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1922, average loss: 4.7379
[09/16 02:48:29 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.53	top5: 7.06	
[09/16 02:48:29 visual_prompt]: Best epoch 6: best metric: 0.025
[09/16 02:48:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 02:48:38 visual_prompt]: Epoch 7 / 100: avg data time: 9.10e-02, avg batch time: 0.4936, average train loss: 4.5895
[09/16 02:48:41 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1428, average loss: 4.6188
[09/16 02:48:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 4.50	top5: 15.50	
[09/16 02:49:02 visual_prompt]: 	Test 100/157. loss: 5.016, 0.1824 s / batch. (data: 1.13e-04)max mem: 17.22530 GB 
[09/16 02:49:13 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1909, average loss: 4.7774
[09/16 02:49:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 3.24	top5: 10.87	
[09/16 02:49:13 visual_prompt]: Best epoch 7: best metric: 0.045
[09/16 02:49:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 02:49:22 visual_prompt]: Epoch 8 / 100: avg data time: 8.61e-02, avg batch time: 0.4881, average train loss: 4.6384
[09/16 02:49:25 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1427, average loss: 4.9175
[09/16 02:49:25 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.00	top5: 8.00	
[09/16 02:49:46 visual_prompt]: 	Test 100/157. loss: 5.012, 0.1827 s / batch. (data: 1.54e-04)max mem: 17.22530 GB 
[09/16 02:49:58 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1913, average loss: 5.0626
[09/16 02:49:58 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.54	top5: 6.68	
[09/16 02:49:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 02:50:06 visual_prompt]: Epoch 9 / 100: avg data time: 9.00e-02, avg batch time: 0.4920, average train loss: 4.7532
[09/16 02:50:09 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1434, average loss: 4.4376
[09/16 02:50:09 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 4.50	top5: 21.50	
[09/16 02:50:30 visual_prompt]: 	Test 100/157. loss: 4.736, 0.1824 s / batch. (data: 1.57e-04)max mem: 17.22530 GB 
[09/16 02:50:42 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1922, average loss: 4.6355
[09/16 02:50:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 3.27	top5: 16.62	
[09/16 02:50:42 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 02:50:51 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e-01, avg batch time: 0.5066, average train loss: 4.2583
[09/16 02:50:53 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1427, average loss: 4.2837
[09/16 02:50:53 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 5.00	top5: 28.00	
[09/16 02:51:14 visual_prompt]: 	Test 100/157. loss: 4.320, 0.2037 s / batch. (data: 1.73e-02)max mem: 17.22530 GB 
[09/16 02:51:26 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1925, average loss: 4.4524
[09/16 02:51:26 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 4.69	top5: 23.18	
[09/16 02:51:26 visual_prompt]: Best epoch 10: best metric: 0.050
[09/16 02:51:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 02:51:35 visual_prompt]: Epoch 11 / 100: avg data time: 9.20e-02, avg batch time: 0.5014, average train loss: 3.8815
[09/16 02:51:38 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1428, average loss: 3.7191
[09/16 02:51:38 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 15.50	top5: 39.50	
[09/16 02:51:59 visual_prompt]: 	Test 100/157. loss: 4.011, 0.1956 s / batch. (data: 1.36e-02)max mem: 17.22530 GB 
[09/16 02:52:11 visual_prompt]: Inference (test):avg data time: 6.29e-03, avg batch time: 0.1919, average loss: 4.0743
[09/16 02:52:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 10.49	top5: 31.23	
[09/16 02:52:11 visual_prompt]: Best epoch 11: best metric: 0.155
[09/16 02:52:11 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 02:52:20 visual_prompt]: Epoch 12 / 100: avg data time: 1.01e-01, avg batch time: 0.5043, average train loss: 3.3626
[09/16 02:52:22 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1425, average loss: 2.8145
[09/16 02:52:22 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 32.00	top5: 62.00	
[09/16 02:52:43 visual_prompt]: 	Test 100/157. loss: 3.268, 0.2019 s / batch. (data: 1.51e-02)max mem: 17.22530 GB 
[09/16 02:52:55 visual_prompt]: Inference (test):avg data time: 6.31e-03, avg batch time: 0.1912, average loss: 3.5941
[09/16 02:52:55 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 19.16	top5: 45.08	
[09/16 02:52:55 visual_prompt]: Best epoch 12: best metric: 0.320
[09/16 02:52:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 02:53:04 visual_prompt]: Epoch 13 / 100: avg data time: 9.32e-02, avg batch time: 0.4946, average train loss: 2.2752
[09/16 02:53:07 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1426, average loss: 1.7822
[09/16 02:53:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 53.00	top5: 79.00	
[09/16 02:53:28 visual_prompt]: 	Test 100/157. loss: 2.416, 0.1980 s / batch. (data: 1.60e-02)max mem: 17.22530 GB 
[09/16 02:53:39 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1922, average loss: 2.8913
[09/16 02:53:39 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 32.80	top5: 64.27	
[09/16 02:53:39 visual_prompt]: Best epoch 13: best metric: 0.530
[09/16 02:53:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 02:53:48 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e-01, avg batch time: 0.5046, average train loss: 1.5788
[09/16 02:53:51 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.1428, average loss: 5.7625
[09/16 02:53:51 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 10.00	top5: 22.00	
[09/16 02:54:12 visual_prompt]: 	Test 100/157. loss: 6.445, 0.1958 s / batch. (data: 1.41e-02)max mem: 17.22530 GB 
[09/16 02:54:24 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1920, average loss: 6.1144
[09/16 02:54:24 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 7.83	top5: 22.11	
[09/16 02:54:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 02:54:33 visual_prompt]: Epoch 15 / 100: avg data time: 9.72e-02, avg batch time: 0.5002, average train loss: 1.5634
[09/16 02:54:35 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1426, average loss: 1.2838
[09/16 02:54:35 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 67.00	top5: 95.00	
[09/16 02:54:56 visual_prompt]: 	Test 100/157. loss: 3.813, 0.2046 s / batch. (data: 2.30e-02)max mem: 17.22530 GB 
[09/16 02:55:08 visual_prompt]: Inference (test):avg data time: 6.50e-03, avg batch time: 0.1909, average loss: 3.3364
[09/16 02:55:08 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 40.03	top5: 75.44	
[09/16 02:55:08 visual_prompt]: Best epoch 15: best metric: 0.670
[09/16 02:55:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 02:55:17 visual_prompt]: Epoch 16 / 100: avg data time: 1.05e-01, avg batch time: 0.5068, average train loss: 0.9237
[09/16 02:55:20 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1429, average loss: 0.9658
[09/16 02:55:20 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 74.50	top5: 94.00	
[09/16 02:55:41 visual_prompt]: 	Test 100/157. loss: 2.664, 0.1959 s / batch. (data: 1.37e-02)max mem: 17.22530 GB 
[09/16 02:55:53 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1930, average loss: 2.6633
[09/16 02:55:53 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 44.90	top5: 74.06	
[09/16 02:55:53 visual_prompt]: Best epoch 16: best metric: 0.745
[09/16 02:55:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 02:56:02 visual_prompt]: Epoch 17 / 100: avg data time: 9.72e-02, avg batch time: 0.5051, average train loss: 0.6134
[09/16 02:56:04 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1427, average loss: 0.6575
[09/16 02:56:04 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 83.50	top5: 98.50	
[09/16 02:56:25 visual_prompt]: 	Test 100/157. loss: 2.391, 0.1823 s / batch. (data: 1.26e-04)max mem: 17.22530 GB 
[09/16 02:56:37 visual_prompt]: Inference (test):avg data time: 6.50e-03, avg batch time: 0.1909, average loss: 2.7662
[09/16 02:56:37 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 49.66	top5: 82.44	
[09/16 02:56:37 visual_prompt]: Best epoch 17: best metric: 0.835
[09/16 02:56:37 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 02:56:46 visual_prompt]: Epoch 18 / 100: avg data time: 9.74e-02, avg batch time: 0.4982, average train loss: 0.3101
[09/16 02:56:48 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1428, average loss: 0.3271
[09/16 02:56:48 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 92.00	top5: 100.00	
[09/16 02:57:09 visual_prompt]: 	Test 100/157. loss: 2.873, 0.1912 s / batch. (data: 9.36e-03)max mem: 17.22530 GB 
[09/16 02:57:21 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1922, average loss: 2.6037
[09/16 02:57:21 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 55.37	top5: 83.46	
[09/16 02:57:21 visual_prompt]: Best epoch 18: best metric: 0.920
[09/16 02:57:21 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 02:57:30 visual_prompt]: Epoch 19 / 100: avg data time: 1.05e-01, avg batch time: 0.5291, average train loss: 0.3337
[09/16 02:57:33 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1426, average loss: 0.2377
[09/16 02:57:33 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 94.00	top5: 99.50	
[09/16 02:57:55 visual_prompt]: 	Test 100/157. loss: 1.930, 0.1833 s / batch. (data: 1.46e-04)max mem: 17.22530 GB 
[09/16 02:58:06 visual_prompt]: Inference (test):avg data time: 6.10e-03, avg batch time: 0.1947, average loss: 2.0405
[09/16 02:58:06 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.35	top5: 84.55	
[09/16 02:58:06 visual_prompt]: Best epoch 19: best metric: 0.940
[09/16 02:58:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 02:58:15 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e-01, avg batch time: 0.5043, average train loss: 0.1967
[09/16 02:58:18 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1477, average loss: 0.3517
[09/16 02:58:18 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 95.00	top5: 99.50	
[09/16 02:58:39 visual_prompt]: 	Test 100/157. loss: 2.248, 0.2172 s / batch. (data: 3.37e-02)max mem: 17.22530 GB 
[09/16 02:58:51 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1914, average loss: 2.2437
[09/16 02:58:51 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.83	top5: 84.12	
[09/16 02:58:51 visual_prompt]: Best epoch 20: best metric: 0.950
[09/16 02:58:51 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 02:59:00 visual_prompt]: Epoch 21 / 100: avg data time: 9.89e-02, avg batch time: 0.4994, average train loss: 0.2345
[09/16 02:59:03 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 0.2107
[09/16 02:59:03 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 94.00	top5: 99.00	
[09/16 02:59:24 visual_prompt]: 	Test 100/157. loss: 2.249, 0.1969 s / batch. (data: 1.49e-02)max mem: 17.22530 GB 
[09/16 02:59:36 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1940, average loss: 2.2672
[09/16 02:59:36 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 56.86	top5: 84.44	
[09/16 02:59:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 02:59:45 visual_prompt]: Epoch 22 / 100: avg data time: 8.75e-02, avg batch time: 0.4912, average train loss: 0.1327
[09/16 02:59:47 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1427, average loss: 0.1565
[09/16 02:59:47 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.50	top5: 100.00	
[09/16 03:00:08 visual_prompt]: 	Test 100/157. loss: 1.483, 0.1827 s / batch. (data: 1.25e-04)max mem: 17.22530 GB 
[09/16 03:00:20 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1914, average loss: 1.9764
[09/16 03:00:20 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.52	top5: 86.90	
[09/16 03:00:20 visual_prompt]: Best epoch 22: best metric: 0.965
[09/16 03:00:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 03:00:29 visual_prompt]: Epoch 23 / 100: avg data time: 9.68e-02, avg batch time: 0.5002, average train loss: 0.1441
[09/16 03:00:32 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1427, average loss: 0.0430
[09/16 03:00:32 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 100.00	
[09/16 03:00:53 visual_prompt]: 	Test 100/157. loss: 1.601, 0.2072 s / batch. (data: 1.45e-02)max mem: 17.22530 GB 
[09/16 03:01:04 visual_prompt]: Inference (test):avg data time: 6.51e-03, avg batch time: 0.1915, average loss: 2.0122
[09/16 03:01:04 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 60.73	top5: 85.99	
[09/16 03:01:04 visual_prompt]: Best epoch 23: best metric: 0.995
[09/16 03:01:04 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 03:01:13 visual_prompt]: Epoch 24 / 100: avg data time: 9.79e-02, avg batch time: 0.4980, average train loss: 0.0722
[09/16 03:01:16 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1427, average loss: 0.0281
[09/16 03:01:16 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 100.00	
[09/16 03:01:37 visual_prompt]: 	Test 100/157. loss: 1.807, 0.1958 s / batch. (data: 1.46e-04)max mem: 17.22530 GB 
[09/16 03:01:49 visual_prompt]: Inference (test):avg data time: 6.18e-03, avg batch time: 0.1912, average loss: 1.8200
[09/16 03:01:49 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.67	top5: 86.66	
[09/16 03:01:49 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 03:01:58 visual_prompt]: Epoch 25 / 100: avg data time: 1.08e-01, avg batch time: 0.5101, average train loss: 0.1227
[09/16 03:02:01 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1427, average loss: 0.1409
[09/16 03:02:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 95.00	top5: 100.00	
[09/16 03:02:22 visual_prompt]: 	Test 100/157. loss: 2.093, 0.1824 s / batch. (data: 1.15e-04)max mem: 17.22530 GB 
[09/16 03:02:33 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1919, average loss: 2.1454
[09/16 03:02:33 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.43	top5: 82.48	
[09/16 03:02:33 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 03:02:42 visual_prompt]: Epoch 26 / 100: avg data time: 9.56e-02, avg batch time: 0.5009, average train loss: 0.1492
[09/16 03:02:45 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1424, average loss: 0.0715
[09/16 03:02:45 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.50	top5: 99.50	
[09/16 03:03:06 visual_prompt]: 	Test 100/157. loss: 1.846, 0.1963 s / batch. (data: 1.42e-04)max mem: 17.22530 GB 
[09/16 03:03:18 visual_prompt]: Inference (test):avg data time: 7.01e-03, avg batch time: 0.1913, average loss: 1.7769
[09/16 03:03:18 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 61.62	top5: 85.61	
[09/16 03:03:18 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 03:03:27 visual_prompt]: Epoch 27 / 100: avg data time: 1.03e-01, avg batch time: 0.5033, average train loss: 0.0995
[09/16 03:03:30 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1426, average loss: 0.0826
[09/16 03:03:30 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.50	top5: 100.00	
[09/16 03:03:50 visual_prompt]: 	Test 100/157. loss: 1.660, 0.1826 s / batch. (data: 1.83e-04)max mem: 17.22530 GB 
[09/16 03:04:02 visual_prompt]: Inference (test):avg data time: 6.48e-03, avg batch time: 0.1917, average loss: 1.7144
[09/16 03:04:02 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 63.52	top5: 86.72	
[09/16 03:04:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 03:04:11 visual_prompt]: Epoch 28 / 100: avg data time: 1.04e-01, avg batch time: 0.5052, average train loss: 0.1065
[09/16 03:04:14 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1429, average loss: 0.0786
[09/16 03:04:14 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.50	top5: 100.00	
[09/16 03:04:36 visual_prompt]: 	Test 100/157. loss: 1.448, 0.1964 s / batch. (data: 1.43e-02)max mem: 17.22530 GB 
[09/16 03:04:47 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1951, average loss: 1.8091
[09/16 03:04:47 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 60.78	top5: 84.77	
[09/16 03:04:47 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 03:04:56 visual_prompt]: Epoch 29 / 100: avg data time: 9.89e-02, avg batch time: 0.5000, average train loss: 0.0642
[09/16 03:04:59 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1438, average loss: 0.0595
[09/16 03:04:59 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.50	top5: 99.50	
[09/16 03:05:20 visual_prompt]: 	Test 100/157. loss: 1.836, 0.1828 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 03:05:32 visual_prompt]: Inference (test):avg data time: 5.94e-03, avg batch time: 0.1905, average loss: 1.6205
[09/16 03:05:32 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 63.98	top5: 88.02	
[09/16 03:05:32 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 03:05:41 visual_prompt]: Epoch 30 / 100: avg data time: 1.08e-01, avg batch time: 0.5101, average train loss: 0.0631
[09/16 03:05:43 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1426, average loss: 0.0387
[09/16 03:05:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 100.00	
[09/16 03:06:04 visual_prompt]: 	Test 100/157. loss: 1.395, 0.1956 s / batch. (data: 1.37e-02)max mem: 17.22530 GB 
[09/16 03:06:16 visual_prompt]: Inference (test):avg data time: 5.48e-03, avg batch time: 0.1925, average loss: 1.5859
[09/16 03:06:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 64.72	top5: 87.44	
[09/16 03:06:16 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 03:06:25 visual_prompt]: Epoch 31 / 100: avg data time: 1.09e-01, avg batch time: 0.5118, average train loss: 0.0594
[09/16 03:06:28 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.1432, average loss: 0.0861
[09/16 03:06:28 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.00	top5: 100.00	
[09/16 03:06:50 visual_prompt]: 	Test 100/157. loss: 1.582, 0.1960 s / batch. (data: 1.39e-02)max mem: 17.22530 GB 
[09/16 03:07:02 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1960, average loss: 1.6587
[09/16 03:07:02 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 63.90	top5: 85.90	
[09/16 03:07:02 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 03:07:11 visual_prompt]: Epoch 32 / 100: avg data time: 1.16e-01, avg batch time: 0.5165, average train loss: 0.0450
[09/16 03:07:14 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1426, average loss: 0.0220
[09/16 03:07:14 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.00	top5: 100.00	
[09/16 03:07:35 visual_prompt]: 	Test 100/157. loss: 1.286, 0.1825 s / batch. (data: 1.38e-04)max mem: 17.22530 GB 
[09/16 03:07:47 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1942, average loss: 1.5321
[09/16 03:07:47 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 66.50	top5: 87.58	
[09/16 03:07:47 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 03:07:56 visual_prompt]: Epoch 33 / 100: avg data time: 9.06e-02, avg batch time: 0.5023, average train loss: 0.0705
[09/16 03:07:58 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1428, average loss: 0.1379
[09/16 03:07:58 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.00	top5: 100.00	
[09/16 03:08:19 visual_prompt]: 	Test 100/157. loss: 1.711, 0.1826 s / batch. (data: 1.52e-04)max mem: 17.22530 GB 
[09/16 03:08:31 visual_prompt]: Inference (test):avg data time: 6.71e-03, avg batch time: 0.1909, average loss: 1.5671
[09/16 03:08:31 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 65.55	top5: 87.27	
[09/16 03:08:31 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 03:08:40 visual_prompt]: Epoch 34 / 100: avg data time: 9.13e-02, avg batch time: 0.4938, average train loss: 0.1036
[09/16 03:08:43 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1427, average loss: 0.1173
[09/16 03:08:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.50	top5: 99.50	
[09/16 03:09:04 visual_prompt]: 	Test 100/157. loss: 1.565, 0.1829 s / batch. (data: 1.37e-04)max mem: 17.22530 GB 
[09/16 03:09:16 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1922, average loss: 1.5806
[09/16 03:09:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 65.99	top5: 87.32	
[09/16 03:09:16 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 03:09:25 visual_prompt]: Epoch 35 / 100: avg data time: 9.05e-02, avg batch time: 0.4996, average train loss: 0.1491
[09/16 03:09:27 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1426, average loss: 0.1317
[09/16 03:09:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.50	top5: 99.50	
[09/16 03:09:48 visual_prompt]: 	Test 100/157. loss: 1.390, 0.1821 s / batch. (data: 1.79e-04)max mem: 17.22530 GB 
[09/16 03:10:01 visual_prompt]: Inference (test):avg data time: 7.11e-03, avg batch time: 0.1972, average loss: 1.5717
[09/16 03:10:01 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 64.42	top5: 86.02	
[09/16 03:10:01 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 03:10:10 visual_prompt]: Epoch 36 / 100: avg data time: 9.16e-02, avg batch time: 0.4967, average train loss: 0.0492
[09/16 03:10:13 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1428, average loss: 0.2209
[09/16 03:10:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.50	top5: 98.50	
[09/16 03:10:33 visual_prompt]: 	Test 100/157. loss: 1.473, 0.1826 s / batch. (data: 1.05e-04)max mem: 17.22530 GB 
[09/16 03:10:45 visual_prompt]: Inference (test):avg data time: 6.67e-03, avg batch time: 0.1922, average loss: 1.5100
[09/16 03:10:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 67.58	top5: 88.99	
[09/16 03:10:45 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 03:10:54 visual_prompt]: Epoch 37 / 100: avg data time: 9.99e-02, avg batch time: 0.5035, average train loss: 0.0720
[09/16 03:10:57 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1426, average loss: 0.0198
[09/16 03:10:57 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:11:18 visual_prompt]: 	Test 100/157. loss: 1.076, 0.1827 s / batch. (data: 1.33e-04)max mem: 17.22530 GB 
[09/16 03:11:30 visual_prompt]: Inference (test):avg data time: 5.64e-03, avg batch time: 0.1902, average loss: 1.2967
[09/16 03:11:30 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 69.25	top5: 89.57	
[09/16 03:11:30 visual_prompt]: Best epoch 37: best metric: 1.000
[09/16 03:11:30 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 03:11:38 visual_prompt]: Epoch 38 / 100: avg data time: 8.52e-02, avg batch time: 0.4945, average train loss: 0.0325
[09/16 03:11:41 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1428, average loss: 0.0117
[09/16 03:11:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:12:02 visual_prompt]: 	Test 100/157. loss: 1.135, 0.1970 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 03:12:14 visual_prompt]: Inference (test):avg data time: 6.50e-03, avg batch time: 0.1920, average loss: 1.3382
[09/16 03:12:14 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 69.11	top5: 90.20	
[09/16 03:12:14 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 03:12:23 visual_prompt]: Epoch 39 / 100: avg data time: 8.31e-02, avg batch time: 0.4879, average train loss: 0.0131
[09/16 03:12:26 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1428, average loss: 0.0083
[09/16 03:12:26 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:12:46 visual_prompt]: 	Test 100/157. loss: 1.064, 0.1832 s / batch. (data: 1.62e-04)max mem: 17.22530 GB 
[09/16 03:12:58 visual_prompt]: Inference (test):avg data time: 6.36e-03, avg batch time: 0.1915, average loss: 1.2606
[09/16 03:12:58 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 70.94	top5: 90.42	
[09/16 03:12:58 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 03:13:07 visual_prompt]: Epoch 40 / 100: avg data time: 9.98e-02, avg batch time: 0.5008, average train loss: 0.0121
[09/16 03:13:10 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1427, average loss: 0.0055
[09/16 03:13:10 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:13:31 visual_prompt]: 	Test 100/157. loss: 0.955, 0.1825 s / batch. (data: 1.60e-04)max mem: 17.22530 GB 
[09/16 03:13:42 visual_prompt]: Inference (test):avg data time: 5.74e-03, avg batch time: 0.1899, average loss: 1.1176
[09/16 03:13:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.05	top5: 91.86	
[09/16 03:13:42 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 03:13:51 visual_prompt]: Epoch 41 / 100: avg data time: 8.76e-02, avg batch time: 0.4895, average train loss: 0.0196
[09/16 03:13:54 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1426, average loss: 0.0694
[09/16 03:13:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.00	top5: 100.00	
[09/16 03:14:15 visual_prompt]: 	Test 100/157. loss: 0.919, 0.1953 s / batch. (data: 1.34e-02)max mem: 17.22530 GB 
[09/16 03:14:26 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1916, average loss: 1.2330
[09/16 03:14:26 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.05	top5: 90.46	
[09/16 03:14:26 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 03:14:35 visual_prompt]: Epoch 42 / 100: avg data time: 9.40e-02, avg batch time: 0.4959, average train loss: 0.0214
[09/16 03:14:38 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1426, average loss: 0.0090
[09/16 03:14:38 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:14:59 visual_prompt]: 	Test 100/157. loss: 0.994, 0.1998 s / batch. (data: 1.11e-04)max mem: 17.22530 GB 
[09/16 03:15:11 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1914, average loss: 1.2262
[09/16 03:15:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 71.77	top5: 90.68	
[09/16 03:15:11 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 03:15:20 visual_prompt]: Epoch 43 / 100: avg data time: 1.04e-01, avg batch time: 0.5042, average train loss: 0.0146
[09/16 03:15:23 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1435, average loss: 0.0435
[09/16 03:15:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.00	top5: 100.00	
[09/16 03:15:44 visual_prompt]: 	Test 100/157. loss: 1.140, 0.1827 s / batch. (data: 1.21e-04)max mem: 17.22530 GB 
[09/16 03:15:55 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1920, average loss: 1.2264
[09/16 03:15:56 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 71.04	top5: 90.51	
[09/16 03:15:56 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 03:16:04 visual_prompt]: Epoch 44 / 100: avg data time: 1.05e-01, avg batch time: 0.5055, average train loss: 0.0204
[09/16 03:16:07 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1427, average loss: 0.0167
[09/16 03:16:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:16:28 visual_prompt]: 	Test 100/157. loss: 1.003, 0.1831 s / batch. (data: 1.22e-04)max mem: 17.22530 GB 
[09/16 03:16:40 visual_prompt]: Inference (test):avg data time: 6.09e-03, avg batch time: 0.1906, average loss: 1.1572
[09/16 03:16:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.10	top5: 91.36	
[09/16 03:16:40 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 03:16:49 visual_prompt]: Epoch 45 / 100: avg data time: 1.04e-01, avg batch time: 0.5038, average train loss: 0.0548
[09/16 03:16:52 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1430, average loss: 0.0337
[09/16 03:16:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 100.00	
[09/16 03:17:12 visual_prompt]: 	Test 100/157. loss: 1.025, 0.1821 s / batch. (data: 8.73e-05)max mem: 17.22530 GB 
[09/16 03:17:24 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1917, average loss: 1.1882
[09/16 03:17:24 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 71.65	top5: 90.38	
[09/16 03:17:24 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 03:17:33 visual_prompt]: Epoch 46 / 100: avg data time: 8.64e-02, avg batch time: 0.4880, average train loss: 0.0309
[09/16 03:17:36 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1426, average loss: 0.0196
[09/16 03:17:36 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 100.00	
[09/16 03:17:57 visual_prompt]: 	Test 100/157. loss: 1.008, 0.1978 s / batch. (data: 9.80e-05)max mem: 17.22530 GB 
[09/16 03:18:08 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1924, average loss: 1.0723
[09/16 03:18:08 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.79	top5: 92.53	
[09/16 03:18:08 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 03:18:17 visual_prompt]: Epoch 47 / 100: avg data time: 1.02e-01, avg batch time: 0.5046, average train loss: 0.0144
[09/16 03:18:20 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.1430, average loss: 0.0093
[09/16 03:18:20 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:18:41 visual_prompt]: 	Test 100/157. loss: 1.042, 0.1954 s / batch. (data: 1.27e-04)max mem: 17.22530 GB 
[09/16 03:18:53 visual_prompt]: Inference (test):avg data time: 6.29e-03, avg batch time: 0.1915, average loss: 1.0156
[09/16 03:18:53 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.37	top5: 93.17	
[09/16 03:18:53 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 03:19:02 visual_prompt]: Epoch 48 / 100: avg data time: 9.98e-02, avg batch time: 0.5007, average train loss: 0.0129
[09/16 03:19:04 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1427, average loss: 0.0445
[09/16 03:19:04 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.00	top5: 99.50	
[09/16 03:19:26 visual_prompt]: 	Test 100/157. loss: 0.941, 0.1858 s / batch. (data: 1.10e-04)max mem: 17.22530 GB 
[09/16 03:19:38 visual_prompt]: Inference (test):avg data time: 9.00e-03, avg batch time: 0.1947, average loss: 1.1092
[09/16 03:19:38 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.56	top5: 92.22	
[09/16 03:19:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 03:19:46 visual_prompt]: Epoch 49 / 100: avg data time: 9.41e-02, avg batch time: 0.4954, average train loss: 0.0131
[09/16 03:19:49 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1464, average loss: 0.0074
[09/16 03:19:49 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:20:10 visual_prompt]: 	Test 100/157. loss: 0.998, 0.2078 s / batch. (data: 2.60e-02)max mem: 17.22530 GB 
[09/16 03:20:22 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1923, average loss: 1.0075
[09/16 03:20:22 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.39	top5: 93.43	
[09/16 03:20:22 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 03:20:31 visual_prompt]: Epoch 50 / 100: avg data time: 1.14e-01, avg batch time: 0.5123, average train loss: 0.0069
[09/16 03:20:34 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1427, average loss: 0.0061
[09/16 03:20:34 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:20:55 visual_prompt]: 	Test 100/157. loss: 0.959, 0.1831 s / batch. (data: 1.24e-04)max mem: 17.22530 GB 
[09/16 03:21:07 visual_prompt]: Inference (test):avg data time: 7.09e-03, avg batch time: 0.1969, average loss: 0.9741
[09/16 03:21:07 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.75	top5: 93.74	
[09/16 03:21:07 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 03:21:16 visual_prompt]: Epoch 51 / 100: avg data time: 1.02e-01, avg batch time: 0.5036, average train loss: 0.0057
[09/16 03:21:19 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1423, average loss: 0.0055
[09/16 03:21:19 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:21:40 visual_prompt]: 	Test 100/157. loss: 0.902, 0.1825 s / batch. (data: 1.39e-04)max mem: 17.22530 GB 
[09/16 03:21:52 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1922, average loss: 0.9478
[09/16 03:21:52 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.17	top5: 93.99	
[09/16 03:21:52 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 03:22:01 visual_prompt]: Epoch 52 / 100: avg data time: 9.55e-02, avg batch time: 0.4981, average train loss: 0.0055
[09/16 03:22:03 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1427, average loss: 0.0054
[09/16 03:22:03 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:22:25 visual_prompt]: 	Test 100/157. loss: 0.845, 0.1956 s / batch. (data: 1.37e-02)max mem: 17.22530 GB 
[09/16 03:22:37 visual_prompt]: Inference (test):avg data time: 6.75e-03, avg batch time: 0.1946, average loss: 0.9250
[09/16 03:22:37 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.61	top5: 94.19	
[09/16 03:22:37 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 03:22:46 visual_prompt]: Epoch 53 / 100: avg data time: 9.28e-02, avg batch time: 0.5489, average train loss: 0.0055
[09/16 03:22:49 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1425, average loss: 0.0054
[09/16 03:22:49 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:23:10 visual_prompt]: 	Test 100/157. loss: 0.809, 0.2082 s / batch. (data: 2.62e-02)max mem: 17.22530 GB 
[09/16 03:23:22 visual_prompt]: Inference (test):avg data time: 7.03e-03, avg batch time: 0.1923, average loss: 0.9204
[09/16 03:23:22 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.74	top5: 94.30	
[09/16 03:23:22 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 03:23:31 visual_prompt]: Epoch 54 / 100: avg data time: 8.58e-02, avg batch time: 0.4916, average train loss: 0.0058
[09/16 03:23:33 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1425, average loss: 0.0055
[09/16 03:23:33 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:23:54 visual_prompt]: 	Test 100/157. loss: 0.785, 0.2036 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 03:24:06 visual_prompt]: Inference (test):avg data time: 7.20e-03, avg batch time: 0.1913, average loss: 0.9079
[09/16 03:24:06 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.96	top5: 94.45	
[09/16 03:24:06 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 03:24:15 visual_prompt]: Epoch 55 / 100: avg data time: 1.02e-01, avg batch time: 0.5035, average train loss: 0.0058
[09/16 03:24:18 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1461, average loss: 0.0054
[09/16 03:24:18 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:24:39 visual_prompt]: 	Test 100/157. loss: 0.787, 0.1832 s / batch. (data: 1.14e-04)max mem: 17.22530 GB 
[09/16 03:24:50 visual_prompt]: Inference (test):avg data time: 6.33e-03, avg batch time: 0.1907, average loss: 0.9051
[09/16 03:24:50 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 77.21	top5: 94.52	
[09/16 03:24:50 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 03:24:59 visual_prompt]: Epoch 56 / 100: avg data time: 9.28e-02, avg batch time: 0.4962, average train loss: 0.0058
[09/16 03:25:02 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1426, average loss: 0.0055
[09/16 03:25:02 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:25:23 visual_prompt]: 	Test 100/157. loss: 0.755, 0.1972 s / batch. (data: 1.51e-02)max mem: 17.22530 GB 
[09/16 03:25:35 visual_prompt]: Inference (test):avg data time: 8.93e-03, avg batch time: 0.1931, average loss: 0.8994
[09/16 03:25:35 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 77.00	top5: 94.71	
[09/16 03:25:35 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 03:25:44 visual_prompt]: Epoch 57 / 100: avg data time: 9.76e-02, avg batch time: 0.4994, average train loss: 0.0060
[09/16 03:25:47 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1429, average loss: 0.0055
[09/16 03:25:47 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:26:08 visual_prompt]: 	Test 100/157. loss: 0.754, 0.1958 s / batch. (data: 1.38e-02)max mem: 17.22530 GB 
[09/16 03:26:19 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1909, average loss: 0.8997
[09/16 03:26:19 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 77.33	top5: 94.83	
[09/16 03:26:19 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 03:26:28 visual_prompt]: Epoch 58 / 100: avg data time: 1.02e-01, avg batch time: 0.5036, average train loss: 0.0058
[09/16 03:26:31 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1427, average loss: 0.0053
[09/16 03:26:31 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:26:52 visual_prompt]: 	Test 100/157. loss: 0.751, 0.1971 s / batch. (data: 1.54e-02)max mem: 17.22530 GB 
[09/16 03:27:04 visual_prompt]: Inference (test):avg data time: 6.64e-03, avg batch time: 0.1919, average loss: 0.8937
[09/16 03:27:04 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 77.13	top5: 94.92	
[09/16 03:27:04 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 03:27:13 visual_prompt]: Epoch 59 / 100: avg data time: 9.67e-02, avg batch time: 0.5008, average train loss: 0.0057
[09/16 03:27:15 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1426, average loss: 0.0051
[09/16 03:27:15 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:27:36 visual_prompt]: 	Test 100/157. loss: 0.761, 0.1854 s / batch. (data: 1.09e-04)max mem: 17.22530 GB 
[09/16 03:27:48 visual_prompt]: Inference (test):avg data time: 7.71e-03, avg batch time: 0.1922, average loss: 0.8920
[09/16 03:27:48 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 77.22	top5: 94.96	
[09/16 03:27:48 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 03:27:57 visual_prompt]: Epoch 60 / 100: avg data time: 9.89e-02, avg batch time: 0.4981, average train loss: 0.0056
[09/16 03:28:00 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1428, average loss: 0.0050
[09/16 03:28:00 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:28:21 visual_prompt]: 	Test 100/157. loss: 0.715, 0.1867 s / batch. (data: 1.08e-04)max mem: 17.22530 GB 
[09/16 03:28:33 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1919, average loss: 0.8894
[09/16 03:28:33 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 77.14	top5: 95.10	
[09/16 03:28:33 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 03:28:41 visual_prompt]: Epoch 61 / 100: avg data time: 8.98e-02, avg batch time: 0.4895, average train loss: 0.0056
[09/16 03:28:44 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1423, average loss: 0.0050
[09/16 03:28:44 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:29:05 visual_prompt]: 	Test 100/157. loss: 0.686, 0.1866 s / batch. (data: 1.23e-04)max mem: 17.22530 GB 
[09/16 03:29:17 visual_prompt]: Inference (test):avg data time: 6.51e-03, avg batch time: 0.1931, average loss: 0.8930
[09/16 03:29:17 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 77.16	top5: 95.03	
[09/16 03:29:17 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 03:29:26 visual_prompt]: Epoch 62 / 100: avg data time: 1.00e-01, avg batch time: 0.5012, average train loss: 0.0055
[09/16 03:29:29 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1426, average loss: 0.0049
[09/16 03:29:29 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:29:50 visual_prompt]: 	Test 100/157. loss: 0.700, 0.1838 s / batch. (data: 1.07e-04)max mem: 17.22530 GB 
[09/16 03:30:01 visual_prompt]: Inference (test):avg data time: 6.31e-03, avg batch time: 0.1916, average loss: 0.8966
[09/16 03:30:01 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.89	top5: 95.10	
[09/16 03:30:01 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 03:30:10 visual_prompt]: Epoch 63 / 100: avg data time: 1.04e-01, avg batch time: 0.5045, average train loss: 0.0053
[09/16 03:30:13 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1428, average loss: 0.0045
[09/16 03:30:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:30:34 visual_prompt]: 	Test 100/157. loss: 0.695, 0.1957 s / batch. (data: 1.37e-02)max mem: 17.22530 GB 
[09/16 03:30:46 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1916, average loss: 0.8871
[09/16 03:30:46 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 77.14	top5: 95.13	
[09/16 03:30:46 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 03:30:54 visual_prompt]: Epoch 64 / 100: avg data time: 9.17e-02, avg batch time: 0.4944, average train loss: 0.0052
[09/16 03:30:57 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1425, average loss: 0.0046
[09/16 03:30:57 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:31:18 visual_prompt]: 	Test 100/157. loss: 0.686, 0.2551 s / batch. (data: 9.99e-05)max mem: 17.22530 GB 
[09/16 03:31:30 visual_prompt]: Inference (test):avg data time: 6.60e-03, avg batch time: 0.1918, average loss: 0.8920
[09/16 03:31:30 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.77	top5: 95.12	
[09/16 03:31:30 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 03:31:39 visual_prompt]: Epoch 65 / 100: avg data time: 9.76e-02, avg batch time: 0.4999, average train loss: 0.0051
[09/16 03:31:42 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1426, average loss: 0.0048
[09/16 03:31:42 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:32:03 visual_prompt]: 	Test 100/157. loss: 0.671, 0.1958 s / batch. (data: 1.37e-02)max mem: 17.22530 GB 
[09/16 03:32:15 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1935, average loss: 0.8985
[09/16 03:32:15 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.65	top5: 95.19	
[09/16 03:32:15 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 03:32:23 visual_prompt]: Epoch 66 / 100: avg data time: 9.36e-02, avg batch time: 0.4937, average train loss: 0.0050
[09/16 03:32:26 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1426, average loss: 0.0044
[09/16 03:32:26 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:32:47 visual_prompt]: 	Test 100/157. loss: 0.705, 0.1852 s / batch. (data: 1.03e-04)max mem: 17.22530 GB 
[09/16 03:32:59 visual_prompt]: Inference (test):avg data time: 6.29e-03, avg batch time: 0.1916, average loss: 0.8963
[09/16 03:32:59 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.88	top5: 95.27	
[09/16 03:32:59 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 03:33:08 visual_prompt]: Epoch 67 / 100: avg data time: 9.45e-02, avg batch time: 0.5014, average train loss: 0.0049
[09/16 03:33:11 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1428, average loss: 0.0044
[09/16 03:33:11 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:33:31 visual_prompt]: 	Test 100/157. loss: 0.676, 0.1826 s / batch. (data: 1.20e-04)max mem: 17.22530 GB 
[09/16 03:33:43 visual_prompt]: Inference (test):avg data time: 6.04e-03, avg batch time: 0.1904, average loss: 0.8879
[09/16 03:33:43 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.81	top5: 95.31	
[09/16 03:33:43 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 03:33:52 visual_prompt]: Epoch 68 / 100: avg data time: 9.80e-02, avg batch time: 0.4987, average train loss: 0.0048
[09/16 03:33:55 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1425, average loss: 0.0042
[09/16 03:33:55 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:34:16 visual_prompt]: 	Test 100/157. loss: 0.678, 0.1831 s / batch. (data: 1.24e-04)max mem: 17.22530 GB 
[09/16 03:34:28 visual_prompt]: Inference (test):avg data time: 5.68e-03, avg batch time: 0.1904, average loss: 0.9009
[09/16 03:34:28 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.61	top5: 95.29	
[09/16 03:34:28 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 03:34:36 visual_prompt]: Epoch 69 / 100: avg data time: 9.27e-02, avg batch time: 0.4957, average train loss: 0.0047
[09/16 03:34:39 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1427, average loss: 0.0041
[09/16 03:34:39 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:35:01 visual_prompt]: 	Test 100/157. loss: 0.667, 0.1825 s / batch. (data: 6.18e-05)max mem: 17.22530 GB 
[09/16 03:35:13 visual_prompt]: Inference (test):avg data time: 8.45e-03, avg batch time: 0.1949, average loss: 0.9043
[09/16 03:35:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.50	top5: 95.27	
[09/16 03:35:13 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 03:35:22 visual_prompt]: Epoch 70 / 100: avg data time: 1.03e-01, avg batch time: 0.5050, average train loss: 0.0046
[09/16 03:35:24 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1473, average loss: 0.0041
[09/16 03:35:24 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:35:45 visual_prompt]: 	Test 100/157. loss: 0.652, 0.2056 s / batch. (data: 2.34e-02)max mem: 17.22530 GB 
[09/16 03:35:57 visual_prompt]: Inference (test):avg data time: 6.77e-03, avg batch time: 0.1913, average loss: 0.9051
[09/16 03:35:57 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.40	top5: 95.29	
[09/16 03:35:57 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 03:36:06 visual_prompt]: Epoch 71 / 100: avg data time: 1.05e-01, avg batch time: 0.5071, average train loss: 0.0046
[09/16 03:36:09 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1425, average loss: 0.0040
[09/16 03:36:09 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:36:30 visual_prompt]: 	Test 100/157. loss: 0.676, 0.1973 s / batch. (data: 1.52e-02)max mem: 17.22530 GB 
[09/16 03:36:42 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1950, average loss: 0.9105
[09/16 03:36:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.26	top5: 95.29	
[09/16 03:36:42 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 03:36:51 visual_prompt]: Epoch 72 / 100: avg data time: 1.08e-01, avg batch time: 0.5112, average train loss: 0.0045
[09/16 03:36:54 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1426, average loss: 0.0040
[09/16 03:36:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:37:15 visual_prompt]: 	Test 100/157. loss: 0.634, 0.1967 s / batch. (data: 1.47e-02)max mem: 17.22530 GB 
[09/16 03:37:27 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1919, average loss: 0.9140
[09/16 03:37:27 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.21	top5: 95.31	
[09/16 03:37:27 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 03:37:35 visual_prompt]: Epoch 73 / 100: avg data time: 9.24e-02, avg batch time: 0.4945, average train loss: 0.0044
[09/16 03:37:38 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1425, average loss: 0.0038
[09/16 03:37:38 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:37:59 visual_prompt]: 	Test 100/157. loss: 0.666, 0.2080 s / batch. (data: 2.60e-02)max mem: 17.22530 GB 
[09/16 03:38:11 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1924, average loss: 0.9205
[09/16 03:38:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.09	top5: 95.16	
[09/16 03:38:11 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 03:38:20 visual_prompt]: Epoch 74 / 100: avg data time: 1.07e-01, avg batch time: 0.5089, average train loss: 0.0043
[09/16 03:38:23 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1431, average loss: 0.0038
[09/16 03:38:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:38:44 visual_prompt]: 	Test 100/157. loss: 0.656, 0.2080 s / batch. (data: 2.59e-02)max mem: 17.22530 GB 
[09/16 03:38:55 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1918, average loss: 0.9071
[09/16 03:38:55 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.53	top5: 95.41	
[09/16 03:38:55 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 03:39:05 visual_prompt]: Epoch 75 / 100: avg data time: 1.12e-01, avg batch time: 0.5396, average train loss: 0.0043
[09/16 03:39:08 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1426, average loss: 0.0036
[09/16 03:39:08 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:39:29 visual_prompt]: 	Test 100/157. loss: 0.634, 0.1921 s / batch. (data: 1.01e-02)max mem: 17.22530 GB 
[09/16 03:39:41 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1943, average loss: 0.9168
[09/16 03:39:41 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.44	top5: 95.27	
[09/16 03:39:41 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 03:39:50 visual_prompt]: Epoch 76 / 100: avg data time: 9.83e-02, avg batch time: 0.5004, average train loss: 0.0042
[09/16 03:39:52 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.1431, average loss: 0.0038
[09/16 03:39:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:40:13 visual_prompt]: 	Test 100/157. loss: 0.663, 0.1958 s / batch. (data: 1.38e-02)max mem: 17.22530 GB 
[09/16 03:40:25 visual_prompt]: Inference (test):avg data time: 6.60e-03, avg batch time: 0.1919, average loss: 0.9280
[09/16 03:40:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.89	top5: 95.15	
[09/16 03:40:25 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 03:40:34 visual_prompt]: Epoch 77 / 100: avg data time: 1.05e-01, avg batch time: 0.5050, average train loss: 0.0042
[09/16 03:40:37 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1428, average loss: 0.0036
[09/16 03:40:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:40:58 visual_prompt]: 	Test 100/157. loss: 0.647, 0.1830 s / batch. (data: 1.05e-04)max mem: 17.22530 GB 
[09/16 03:41:10 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1923, average loss: 0.9268
[09/16 03:41:10 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.35	top5: 95.25	
[09/16 03:41:10 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 03:41:19 visual_prompt]: Epoch 78 / 100: avg data time: 1.00e-01, avg batch time: 0.5035, average train loss: 0.0041
[09/16 03:41:22 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1426, average loss: 0.0036
[09/16 03:41:22 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:41:43 visual_prompt]: 	Test 100/157. loss: 0.635, 0.1956 s / batch. (data: 1.36e-02)max mem: 17.22530 GB 
[09/16 03:41:55 visual_prompt]: Inference (test):avg data time: 8.40e-03, avg batch time: 0.1931, average loss: 0.9275
[09/16 03:41:55 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.12	top5: 95.31	
[09/16 03:41:55 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 03:42:04 visual_prompt]: Epoch 79 / 100: avg data time: 9.96e-02, avg batch time: 0.5046, average train loss: 0.0041
[09/16 03:42:07 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.1435, average loss: 0.0035
[09/16 03:42:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:42:28 visual_prompt]: 	Test 100/157. loss: 0.635, 0.1978 s / batch. (data: 1.59e-02)max mem: 17.22530 GB 
[09/16 03:42:39 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1923, average loss: 0.9345
[09/16 03:42:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.10	top5: 94.98	
[09/16 03:42:40 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 03:42:48 visual_prompt]: Epoch 80 / 100: avg data time: 9.67e-02, avg batch time: 0.4995, average train loss: 0.0040
[09/16 03:42:51 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1427, average loss: 0.0036
[09/16 03:42:51 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:43:12 visual_prompt]: 	Test 100/157. loss: 0.653, 0.1822 s / batch. (data: 1.38e-04)max mem: 17.22530 GB 
[09/16 03:43:24 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1924, average loss: 0.9403
[09/16 03:43:24 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.89	top5: 95.24	
[09/16 03:43:24 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 03:43:33 visual_prompt]: Epoch 81 / 100: avg data time: 8.74e-02, avg batch time: 0.4895, average train loss: 0.0040
[09/16 03:43:36 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1428, average loss: 0.0034
[09/16 03:43:36 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:43:57 visual_prompt]: 	Test 100/157. loss: 0.638, 0.1968 s / batch. (data: 1.41e-02)max mem: 17.22530 GB 
[09/16 03:44:09 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1955, average loss: 0.9328
[09/16 03:44:09 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.06	top5: 95.16	
[09/16 03:44:09 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 03:44:18 visual_prompt]: Epoch 82 / 100: avg data time: 8.87e-02, avg batch time: 0.4933, average train loss: 0.0040
[09/16 03:44:20 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1426, average loss: 0.0034
[09/16 03:44:20 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:44:41 visual_prompt]: 	Test 100/157. loss: 0.648, 0.1826 s / batch. (data: 1.58e-04)max mem: 17.22530 GB 
[09/16 03:44:53 visual_prompt]: Inference (test):avg data time: 6.16e-03, avg batch time: 0.1935, average loss: 0.9371
[09/16 03:44:53 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.87	top5: 95.21	
[09/16 03:44:53 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 03:45:02 visual_prompt]: Epoch 83 / 100: avg data time: 1.01e-01, avg batch time: 0.5020, average train loss: 0.0040
[09/16 03:45:05 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1428, average loss: 0.0035
[09/16 03:45:05 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:45:26 visual_prompt]: 	Test 100/157. loss: 0.660, 0.1946 s / batch. (data: 1.24e-04)max mem: 17.22530 GB 
[09/16 03:45:38 visual_prompt]: Inference (test):avg data time: 6.55e-03, avg batch time: 0.1913, average loss: 0.9466
[09/16 03:45:38 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.84	top5: 95.08	
[09/16 03:45:38 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 03:45:47 visual_prompt]: Epoch 84 / 100: avg data time: 1.06e-01, avg batch time: 0.5071, average train loss: 0.0039
[09/16 03:45:50 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1427, average loss: 0.0034
[09/16 03:45:50 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:46:10 visual_prompt]: 	Test 100/157. loss: 0.648, 0.1958 s / batch. (data: 1.34e-02)max mem: 17.22530 GB 
[09/16 03:46:22 visual_prompt]: Inference (test):avg data time: 6.91e-03, avg batch time: 0.1917, average loss: 0.9421
[09/16 03:46:22 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.98	top5: 95.11	
[09/16 03:46:22 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 03:46:31 visual_prompt]: Epoch 85 / 100: avg data time: 1.01e-01, avg batch time: 0.5001, average train loss: 0.0039
[09/16 03:46:34 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1428, average loss: 0.0034
[09/16 03:46:34 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:46:55 visual_prompt]: 	Test 100/157. loss: 0.658, 0.1957 s / batch. (data: 1.35e-02)max mem: 17.22530 GB 
[09/16 03:47:07 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1927, average loss: 0.9440
[09/16 03:47:07 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.67	top5: 95.11	
[09/16 03:47:07 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 03:47:16 visual_prompt]: Epoch 86 / 100: avg data time: 9.73e-02, avg batch time: 0.4985, average train loss: 0.0039
[09/16 03:47:18 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1433, average loss: 0.0034
[09/16 03:47:18 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:47:39 visual_prompt]: 	Test 100/157. loss: 0.644, 0.1825 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 03:47:51 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1926, average loss: 0.9494
[09/16 03:47:51 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.77	top5: 95.03	
[09/16 03:47:51 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 03:48:00 visual_prompt]: Epoch 87 / 100: avg data time: 1.01e-01, avg batch time: 0.5015, average train loss: 0.0039
[09/16 03:48:03 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1426, average loss: 0.0034
[09/16 03:48:03 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:48:24 visual_prompt]: 	Test 100/157. loss: 0.653, 0.2086 s / batch. (data: 1.38e-02)max mem: 17.22530 GB 
[09/16 03:48:36 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1928, average loss: 0.9478
[09/16 03:48:36 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.82	top5: 95.08	
[09/16 03:48:36 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 03:48:45 visual_prompt]: Epoch 88 / 100: avg data time: 9.11e-02, avg batch time: 0.4936, average train loss: 0.0038
[09/16 03:48:47 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1434, average loss: 0.0034
[09/16 03:48:47 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:49:08 visual_prompt]: 	Test 100/157. loss: 0.650, 0.1959 s / batch. (data: 1.40e-02)max mem: 17.22530 GB 
[09/16 03:49:20 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1923, average loss: 0.9550
[09/16 03:49:20 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.81	top5: 95.06	
[09/16 03:49:20 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 03:49:29 visual_prompt]: Epoch 89 / 100: avg data time: 1.03e-01, avg batch time: 0.5055, average train loss: 0.0038
[09/16 03:49:32 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1424, average loss: 0.0034
[09/16 03:49:32 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:49:53 visual_prompt]: 	Test 100/157. loss: 0.645, 0.2071 s / batch. (data: 2.56e-02)max mem: 17.22530 GB 
[09/16 03:50:05 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1911, average loss: 0.9560
[09/16 03:50:05 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.76	top5: 94.99	
[09/16 03:50:05 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 03:50:14 visual_prompt]: Epoch 90 / 100: avg data time: 1.02e-01, avg batch time: 0.5048, average train loss: 0.0038
[09/16 03:50:16 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1426, average loss: 0.0034
[09/16 03:50:16 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:50:37 visual_prompt]: 	Test 100/157. loss: 0.651, 0.1903 s / batch. (data: 1.18e-04)max mem: 17.22530 GB 
[09/16 03:50:49 visual_prompt]: Inference (test):avg data time: 6.83e-03, avg batch time: 0.1905, average loss: 0.9561
[09/16 03:50:49 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.75	top5: 94.97	
[09/16 03:50:49 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 03:50:58 visual_prompt]: Epoch 91 / 100: avg data time: 1.06e-01, avg batch time: 0.5060, average train loss: 0.0038
[09/16 03:51:01 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1427, average loss: 0.0034
[09/16 03:51:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:51:22 visual_prompt]: 	Test 100/157. loss: 0.658, 0.2006 s / batch. (data: 1.42e-02)max mem: 17.22530 GB 
[09/16 03:51:34 visual_prompt]: Inference (test):avg data time: 8.82e-03, avg batch time: 0.1926, average loss: 0.9565
[09/16 03:51:34 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.67	top5: 95.03	
[09/16 03:51:34 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 03:51:43 visual_prompt]: Epoch 92 / 100: avg data time: 9.93e-02, avg batch time: 0.5010, average train loss: 0.0038
[09/16 03:51:46 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1425, average loss: 0.0034
[09/16 03:51:46 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:52:06 visual_prompt]: 	Test 100/157. loss: 0.653, 0.1865 s / batch. (data: 1.36e-04)max mem: 17.22530 GB 
[09/16 03:52:18 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1912, average loss: 0.9555
[09/16 03:52:18 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.65	top5: 95.03	
[09/16 03:52:18 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 03:52:27 visual_prompt]: Epoch 93 / 100: avg data time: 1.03e-01, avg batch time: 0.5038, average train loss: 0.0038
[09/16 03:52:30 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1427, average loss: 0.0034
[09/16 03:52:30 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:52:51 visual_prompt]: 	Test 100/157. loss: 0.658, 0.2192 s / batch. (data: 2.26e-02)max mem: 17.22530 GB 
[09/16 03:53:03 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1917, average loss: 0.9552
[09/16 03:53:03 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.60	top5: 95.12	
[09/16 03:53:03 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 03:53:12 visual_prompt]: Epoch 94 / 100: avg data time: 9.56e-02, avg batch time: 0.4967, average train loss: 0.0038
[09/16 03:53:14 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1427, average loss: 0.0033
[09/16 03:53:14 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:53:35 visual_prompt]: 	Test 100/157. loss: 0.659, 0.2136 s / batch. (data: 3.21e-02)max mem: 17.22530 GB 
[09/16 03:53:47 visual_prompt]: Inference (test):avg data time: 6.06e-03, avg batch time: 0.1903, average loss: 0.9562
[09/16 03:53:47 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.67	top5: 95.00	
[09/16 03:53:47 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 03:53:56 visual_prompt]: Epoch 95 / 100: avg data time: 1.02e-01, avg batch time: 0.5035, average train loss: 0.0038
[09/16 03:53:59 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1425, average loss: 0.0033
[09/16 03:53:59 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:54:20 visual_prompt]: 	Test 100/157. loss: 0.656, 0.1828 s / batch. (data: 1.19e-04)max mem: 17.22530 GB 
[09/16 03:54:31 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1919, average loss: 0.9563
[09/16 03:54:32 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.73	top5: 95.00	
[09/16 03:54:32 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 03:54:40 visual_prompt]: Epoch 96 / 100: avg data time: 1.02e-01, avg batch time: 0.5047, average train loss: 0.0038
[09/16 03:54:43 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1425, average loss: 0.0033
[09/16 03:54:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:55:04 visual_prompt]: 	Test 100/157. loss: 0.654, 0.1951 s / batch. (data: 9.04e-05)max mem: 17.22530 GB 
[09/16 03:55:16 visual_prompt]: Inference (test):avg data time: 6.85e-03, avg batch time: 0.1912, average loss: 0.9557
[09/16 03:55:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.66	top5: 94.99	
[09/16 03:55:16 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 03:55:25 visual_prompt]: Epoch 97 / 100: avg data time: 9.84e-02, avg batch time: 0.5014, average train loss: 0.0038
[09/16 03:55:27 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1440, average loss: 0.0033
[09/16 03:55:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:55:48 visual_prompt]: 	Test 100/157. loss: 0.655, 0.1961 s / batch. (data: 1.39e-02)max mem: 17.22530 GB 
[09/16 03:56:00 visual_prompt]: Inference (test):avg data time: 8.31e-03, avg batch time: 0.1923, average loss: 0.9561
[09/16 03:56:00 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.67	top5: 94.99	
[09/16 03:56:00 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 03:56:09 visual_prompt]: Epoch 98 / 100: avg data time: 9.07e-02, avg batch time: 0.4931, average train loss: 0.0038
[09/16 03:56:12 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1437, average loss: 0.0033
[09/16 03:56:12 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:56:32 visual_prompt]: 	Test 100/157. loss: 0.655, 0.1959 s / batch. (data: 1.33e-02)max mem: 17.22530 GB 
[09/16 03:56:44 visual_prompt]: Inference (test):avg data time: 6.65e-03, avg batch time: 0.1903, average loss: 0.9567
[09/16 03:56:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.69	top5: 95.00	
[09/16 03:56:44 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 03:56:53 visual_prompt]: Epoch 99 / 100: avg data time: 1.04e-01, avg batch time: 0.5048, average train loss: 0.0038
[09/16 03:56:56 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1427, average loss: 0.0033
[09/16 03:56:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:57:17 visual_prompt]: 	Test 100/157. loss: 0.655, 0.1826 s / batch. (data: 1.73e-04)max mem: 17.22530 GB 
[09/16 03:57:29 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1922, average loss: 0.9571
[09/16 03:57:29 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.69	top5: 95.00	
[09/16 03:57:29 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 03:57:38 visual_prompt]: Epoch 100 / 100: avg data time: 1.02e-01, avg batch time: 0.5029, average train loss: 0.0038
[09/16 03:57:41 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1425, average loss: 0.0033
[09/16 03:57:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 03:58:01 visual_prompt]: 	Test 100/157. loss: 0.655, 0.1981 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 03:58:13 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1929, average loss: 0.9571
[09/16 03:58:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.69	top5: 95.00	
[09/16 03:58:23 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 03:58:23 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 03:58:23 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-cifar(num_classes=100)', 'DATA.NUMBER_CLASSES', '100', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed82'], train_type='')
[09/16 03:58:23 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 03:58:23 visual_prompt]: Training with config:
[09/16 03:58:23 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-cifar(num_classes=100)',
          'NO_TEST': False,
          'NUMBER_CLASSES': 100,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed82/vtab-cifar(num_classes=100)/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 03:58:23 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 03:58:23.593938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 03:58:23.764209: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 03:58:24.641771: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 03:58:24.641860: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 03:58:24.641869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 03:58:26.681711: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 03:58:26.681832: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 03:58:26.681847: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 03:58:26 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
2023-09-16 03:58:26.697253: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split train[:800]+train[45000:45200], from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 03:58:28 visual_prompt]: Number of images: 1000
[09/16 03:58:28 visual_prompt]: Number of classes: 100 / 100
[09/16 03:58:28 visual_prompt]: Loading validation data...
[09/16 03:58:28 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split train[45000:45200], from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 03:58:28 visual_prompt]: Number of images: 200
[09/16 03:58:28 visual_prompt]: Number of classes: 90 / 100
[09/16 03:58:28 visual_prompt]: Loading test data...
[09/16 03:58:28 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split test, from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 03:58:41 visual_prompt]: Number of images: 10000
[09/16 03:58:41 visual_prompt]: Number of classes: 100 / 100
[09/16 03:58:41 visual_prompt]: Constructing models...
[09/16 03:58:44 visual_prompt]: Total Parameters: 86797156	 Gradient Parameters: 998500
[09/16 03:58:44 visual_prompt]: tuned percent:1.150
[09/16 03:58:47 visual_prompt]: Device used for model: 0
[09/16 03:58:47 visual_prompt]: Setting up Evalutator...
[09/16 03:58:47 visual_prompt]: Setting up Trainer...
[09/16 03:58:47 visual_prompt]: 	Setting up the optimizer...
[09/16 03:58:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 03:58:57 visual_prompt]: Epoch 1 / 100: avg data time: 1.17e-01, avg batch time: 0.5869, average train loss: 4.6599
[09/16 03:59:00 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1419, average loss: 4.6281
[09/16 03:59:00 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 4.50	
[09/16 03:59:21 visual_prompt]: 	Test 100/157. loss: 4.583, 0.1960 s / batch. (data: 1.47e-02)max mem: 17.22530 GB 
[09/16 03:59:32 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1920, average loss: 4.6464
[09/16 03:59:32 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.02	
[09/16 03:59:32 visual_prompt]: Best epoch 1: best metric: 0.015
[09/16 03:59:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 03:59:41 visual_prompt]: Epoch 2 / 100: avg data time: 9.71e-02, avg batch time: 0.4987, average train loss: 4.6469
[09/16 03:59:44 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1422, average loss: 4.5705
[09/16 03:59:44 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 7.00	
[09/16 04:00:05 visual_prompt]: 	Test 100/157. loss: 4.645, 0.1958 s / batch. (data: 1.39e-02)max mem: 17.22530 GB 
[09/16 04:00:17 visual_prompt]: Inference (test):avg data time: 7.09e-03, avg batch time: 0.1918, average loss: 4.7020
[09/16 04:00:17 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.01	top5: 5.30	
[09/16 04:00:17 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 04:00:26 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e-01, avg batch time: 0.5029, average train loss: 4.6810
[09/16 04:00:29 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1426, average loss: 4.5699
[09/16 04:00:29 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 5.50	
[09/16 04:00:50 visual_prompt]: 	Test 100/157. loss: 4.616, 0.1996 s / batch. (data: 1.23e-04)max mem: 17.22530 GB 
[09/16 04:01:02 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1920, average loss: 4.7180
[09/16 04:01:02 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.02	
[09/16 04:01:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 04:01:11 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e-01, avg batch time: 0.5034, average train loss: 4.7456
[09/16 04:01:13 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1425, average loss: 4.6186
[09/16 04:01:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 8.50	
[09/16 04:01:34 visual_prompt]: 	Test 100/157. loss: 4.719, 0.1957 s / batch. (data: 1.37e-02)max mem: 17.22530 GB 
[09/16 04:01:46 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1934, average loss: 4.7120
[09/16 04:01:46 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 6.26	
[09/16 04:01:46 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 04:01:55 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e-01, avg batch time: 0.5042, average train loss: 4.7397
[09/16 04:01:58 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1426, average loss: 4.5601
[09/16 04:01:58 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 3.00	top5: 12.00	
[09/16 04:02:19 visual_prompt]: 	Test 100/157. loss: 4.746, 0.1847 s / batch. (data: 1.32e-04)max mem: 17.22530 GB 
[09/16 04:02:31 visual_prompt]: Inference (test):avg data time: 6.26e-03, avg batch time: 0.1910, average loss: 4.7131
[09/16 04:02:31 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.91	top5: 8.84	
[09/16 04:02:31 visual_prompt]: Best epoch 5: best metric: 0.030
[09/16 04:02:31 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 04:02:40 visual_prompt]: Epoch 6 / 100: avg data time: 9.87e-02, avg batch time: 0.5009, average train loss: 4.6306
[09/16 04:02:42 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1425, average loss: 4.4391
[09/16 04:02:42 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 2.00	top5: 13.00	
[09/16 04:03:03 visual_prompt]: 	Test 100/157. loss: 4.611, 0.1826 s / batch. (data: 1.49e-04)max mem: 17.22530 GB 
[09/16 04:03:15 visual_prompt]: Inference (test):avg data time: 5.63e-03, avg batch time: 0.1919, average loss: 4.5584
[09/16 04:03:15 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 2.07	top5: 11.28	
[09/16 04:03:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 04:03:24 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e-01, avg batch time: 0.5039, average train loss: 4.6993
[09/16 04:03:27 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1428, average loss: 4.7849
[09/16 04:03:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 10.00	
[09/16 04:03:48 visual_prompt]: 	Test 100/157. loss: 4.956, 0.2148 s / batch. (data: 1.59e-02)max mem: 17.22530 GB 
[09/16 04:04:00 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1907, average loss: 4.7516
[09/16 04:04:00 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 2.57	top5: 9.15	
[09/16 04:04:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 04:04:09 visual_prompt]: Epoch 8 / 100: avg data time: 1.00e-01, avg batch time: 0.5009, average train loss: 4.5899
[09/16 04:04:11 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 4.3091
[09/16 04:04:11 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 7.00	top5: 25.00	
[09/16 04:04:32 visual_prompt]: 	Test 100/157. loss: 4.565, 0.1957 s / batch. (data: 1.33e-02)max mem: 17.22530 GB 
[09/16 04:04:44 visual_prompt]: Inference (test):avg data time: 8.37e-03, avg batch time: 0.1921, average loss: 4.5055
[09/16 04:04:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 4.02	top5: 18.29	
[09/16 04:04:44 visual_prompt]: Best epoch 8: best metric: 0.070
[09/16 04:04:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 04:04:53 visual_prompt]: Epoch 9 / 100: avg data time: 8.92e-02, avg batch time: 0.4908, average train loss: 4.4001
[09/16 04:04:56 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1426, average loss: 4.2262
[09/16 04:04:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 11.50	top5: 25.50	
[09/16 04:05:16 visual_prompt]: 	Test 100/157. loss: 4.457, 0.1979 s / batch. (data: 1.61e-02)max mem: 17.22530 GB 
[09/16 04:05:28 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1912, average loss: 4.4107
[09/16 04:05:28 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 6.22	top5: 20.55	
[09/16 04:05:28 visual_prompt]: Best epoch 9: best metric: 0.115
[09/16 04:05:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 04:05:37 visual_prompt]: Epoch 10 / 100: avg data time: 1.11e-01, avg batch time: 0.5120, average train loss: 4.2097
[09/16 04:05:40 visual_prompt]: Inference (val):avg data time: 4.85e-05, avg batch time: 0.1431, average loss: 4.1047
[09/16 04:05:40 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 12.50	top5: 35.00	
[09/16 04:06:01 visual_prompt]: 	Test 100/157. loss: 4.312, 0.2172 s / batch. (data: 9.99e-03)max mem: 17.22530 GB 
[09/16 04:06:13 visual_prompt]: Inference (test):avg data time: 6.46e-03, avg batch time: 0.1911, average loss: 4.2693
[09/16 04:06:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 8.18	top5: 28.07	
[09/16 04:06:13 visual_prompt]: Best epoch 10: best metric: 0.125
[09/16 04:06:13 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 04:06:22 visual_prompt]: Epoch 11 / 100: avg data time: 9.30e-02, avg batch time: 0.4930, average train loss: 3.6093
[09/16 04:06:24 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1424, average loss: 3.4983
[09/16 04:06:24 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 19.50	top5: 49.50	
[09/16 04:06:45 visual_prompt]: 	Test 100/157. loss: 3.653, 0.1896 s / batch. (data: 1.36e-04)max mem: 17.22530 GB 
[09/16 04:06:57 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1918, average loss: 3.9976
[09/16 04:06:57 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 13.53	top5: 38.88	
[09/16 04:06:57 visual_prompt]: Best epoch 11: best metric: 0.195
[09/16 04:06:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 04:07:06 visual_prompt]: Epoch 12 / 100: avg data time: 9.41e-02, avg batch time: 0.4980, average train loss: 3.3227
[09/16 04:07:09 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1426, average loss: 2.5619
[09/16 04:07:09 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 31.00	top5: 66.00	
[09/16 04:07:30 visual_prompt]: 	Test 100/157. loss: 3.276, 0.1833 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 04:07:42 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1925, average loss: 3.4226
[09/16 04:07:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 21.40	top5: 48.84	
[09/16 04:07:42 visual_prompt]: Best epoch 12: best metric: 0.310
[09/16 04:07:42 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 04:07:51 visual_prompt]: Epoch 13 / 100: avg data time: 1.06e-01, avg batch time: 0.5085, average train loss: 2.7287
[09/16 04:07:54 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1427, average loss: 1.6831
[09/16 04:07:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 55.00	top5: 84.00	
[09/16 04:08:14 visual_prompt]: 	Test 100/157. loss: 2.269, 0.1821 s / batch. (data: 1.20e-04)max mem: 17.22530 GB 
[09/16 04:08:26 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1927, average loss: 2.7511
[09/16 04:08:26 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 32.67	top5: 67.05	
[09/16 04:08:26 visual_prompt]: Best epoch 13: best metric: 0.550
[09/16 04:08:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 04:08:35 visual_prompt]: Epoch 14 / 100: avg data time: 8.52e-02, avg batch time: 0.4897, average train loss: 2.0749
[09/16 04:08:38 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1426, average loss: 1.7108
[09/16 04:08:38 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 59.50	top5: 85.00	
[09/16 04:09:00 visual_prompt]: 	Test 100/157. loss: 2.781, 0.1824 s / batch. (data: 3.72e-05)max mem: 17.22530 GB 
[09/16 04:09:12 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1960, average loss: 2.8713
[09/16 04:09:12 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 33.48	top5: 66.68	
[09/16 04:09:12 visual_prompt]: Best epoch 14: best metric: 0.595
[09/16 04:09:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 04:09:20 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e-01, avg batch time: 0.5059, average train loss: 1.2381
[09/16 04:09:23 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1429, average loss: 1.2087
[09/16 04:09:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 67.50	top5: 89.00	
[09/16 04:09:44 visual_prompt]: 	Test 100/157. loss: 2.310, 0.1958 s / batch. (data: 1.40e-02)max mem: 17.22530 GB 
[09/16 04:09:57 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1969, average loss: 2.7618
[09/16 04:09:57 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 41.96	top5: 71.60	
[09/16 04:09:57 visual_prompt]: Best epoch 15: best metric: 0.675
[09/16 04:09:57 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 04:10:06 visual_prompt]: Epoch 16 / 100: avg data time: 1.08e-01, avg batch time: 0.5072, average train loss: 0.9470
[09/16 04:10:09 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1425, average loss: 0.4945
[09/16 04:10:09 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 88.00	top5: 98.50	
[09/16 04:10:30 visual_prompt]: 	Test 100/157. loss: 2.057, 0.2002 s / batch. (data: 1.86e-02)max mem: 17.22530 GB 
[09/16 04:10:42 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1916, average loss: 2.0987
[09/16 04:10:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 51.52	top5: 79.91	
[09/16 04:10:42 visual_prompt]: Best epoch 16: best metric: 0.880
[09/16 04:10:42 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 04:10:50 visual_prompt]: Epoch 17 / 100: avg data time: 9.48e-02, avg batch time: 0.4969, average train loss: 0.4339
[09/16 04:10:53 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1428, average loss: 0.3501
[09/16 04:10:53 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 86.50	top5: 99.50	
[09/16 04:11:14 visual_prompt]: 	Test 100/157. loss: 2.426, 0.1833 s / batch. (data: 1.09e-04)max mem: 17.22530 GB 
[09/16 04:11:26 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1918, average loss: 2.2802
[09/16 04:11:26 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 50.92	top5: 82.02	
[09/16 04:11:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 04:11:35 visual_prompt]: Epoch 18 / 100: avg data time: 1.01e-01, avg batch time: 0.5030, average train loss: 0.2579
[09/16 04:11:38 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1428, average loss: 0.2077
[09/16 04:11:38 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.50	top5: 100.00	
[09/16 04:11:59 visual_prompt]: 	Test 100/157. loss: 1.926, 0.1832 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 04:12:11 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1924, average loss: 2.2504
[09/16 04:12:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 59.07	top5: 85.14	
[09/16 04:12:11 visual_prompt]: Best epoch 18: best metric: 0.935
[09/16 04:12:11 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 04:12:19 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e-01, avg batch time: 0.5048, average train loss: 0.4053
[09/16 04:12:22 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1427, average loss: 0.5018
[09/16 04:12:22 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 90.50	top5: 98.00	
[09/16 04:12:43 visual_prompt]: 	Test 100/157. loss: 2.459, 0.1971 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 04:12:55 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1945, average loss: 2.3750
[09/16 04:12:55 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 52.08	top5: 80.64	
[09/16 04:12:55 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 04:13:04 visual_prompt]: Epoch 20 / 100: avg data time: 9.61e-02, avg batch time: 0.4976, average train loss: 0.3823
[09/16 04:13:07 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1425, average loss: 0.2179
[09/16 04:13:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 95.00	top5: 100.00	
[09/16 04:13:28 visual_prompt]: 	Test 100/157. loss: 2.281, 0.2231 s / batch. (data: 2.30e-02)max mem: 17.22530 GB 
[09/16 04:13:40 visual_prompt]: Inference (test):avg data time: 6.66e-03, avg batch time: 0.1915, average loss: 2.1558
[09/16 04:13:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.10	top5: 83.82	
[09/16 04:13:40 visual_prompt]: Best epoch 20: best metric: 0.950
[09/16 04:13:40 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 04:13:49 visual_prompt]: Epoch 21 / 100: avg data time: 9.53e-02, avg batch time: 0.5011, average train loss: 0.3358
[09/16 04:13:52 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1427, average loss: 0.1181
[09/16 04:13:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.00	top5: 100.00	
[09/16 04:14:12 visual_prompt]: 	Test 100/157. loss: 1.962, 0.1882 s / batch. (data: 1.28e-04)max mem: 17.22530 GB 
[09/16 04:14:24 visual_prompt]: Inference (test):avg data time: 6.94e-03, avg batch time: 0.1922, average loss: 2.1982
[09/16 04:14:24 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.19	top5: 82.71	
[09/16 04:14:24 visual_prompt]: Best epoch 21: best metric: 0.970
[09/16 04:14:24 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 04:14:33 visual_prompt]: Epoch 22 / 100: avg data time: 1.02e-01, avg batch time: 0.5029, average train loss: 0.2042
[09/16 04:14:36 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1428, average loss: 0.1342
[09/16 04:14:36 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.00	top5: 99.50	
[09/16 04:14:57 visual_prompt]: 	Test 100/157. loss: 2.184, 0.2107 s / batch. (data: 1.99e-02)max mem: 17.22530 GB 
[09/16 04:15:09 visual_prompt]: Inference (test):avg data time: 8.10e-03, avg batch time: 0.1921, average loss: 2.0823
[09/16 04:15:09 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 59.41	top5: 83.77	
[09/16 04:15:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 04:15:18 visual_prompt]: Epoch 23 / 100: avg data time: 9.90e-02, avg batch time: 0.5004, average train loss: 0.1947
[09/16 04:15:21 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1425, average loss: 0.0460
[09/16 04:15:21 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.50	top5: 100.00	
[09/16 04:15:42 visual_prompt]: 	Test 100/157. loss: 1.850, 0.2006 s / batch. (data: 1.51e-02)max mem: 17.22530 GB 
[09/16 04:15:53 visual_prompt]: Inference (test):avg data time: 6.32e-03, avg batch time: 0.1903, average loss: 2.1549
[09/16 04:15:53 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 60.73	top5: 83.97	
[09/16 04:15:53 visual_prompt]: Best epoch 23: best metric: 0.985
[09/16 04:15:53 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 04:16:02 visual_prompt]: Epoch 24 / 100: avg data time: 1.02e-01, avg batch time: 0.5036, average train loss: 0.1010
[09/16 04:16:05 visual_prompt]: Inference (val):avg data time: 1.89e-05, avg batch time: 0.1426, average loss: 0.0153
[09/16 04:16:05 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:16:27 visual_prompt]: 	Test 100/157. loss: 1.752, 0.1977 s / batch. (data: 1.56e-02)max mem: 17.22530 GB 
[09/16 04:16:39 visual_prompt]: Inference (test):avg data time: 9.19e-03, avg batch time: 0.1962, average loss: 1.7808
[09/16 04:16:39 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 64.02	top5: 87.50	
[09/16 04:16:39 visual_prompt]: Best epoch 24: best metric: 1.000
[09/16 04:16:39 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 04:16:48 visual_prompt]: Epoch 25 / 100: avg data time: 1.04e-01, avg batch time: 0.5052, average train loss: 0.1460
[09/16 04:16:50 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1427, average loss: 0.0583
[09/16 04:16:50 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.00	top5: 99.50	
[09/16 04:17:11 visual_prompt]: 	Test 100/157. loss: 2.177, 0.1829 s / batch. (data: 9.66e-05)max mem: 17.22530 GB 
[09/16 04:17:23 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1924, average loss: 1.9184
[09/16 04:17:23 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.13	top5: 85.12	
[09/16 04:17:23 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 04:17:32 visual_prompt]: Epoch 26 / 100: avg data time: 1.07e-01, avg batch time: 0.5073, average train loss: 0.0882
[09/16 04:17:35 visual_prompt]: Inference (val):avg data time: 3.09e-04, avg batch time: 0.2336, average loss: 0.0439
[09/16 04:17:35 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.00	top5: 100.00	
[09/16 04:17:56 visual_prompt]: 	Test 100/157. loss: 2.008, 0.1963 s / batch. (data: 1.40e-02)max mem: 17.22530 GB 
[09/16 04:18:08 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1931, average loss: 1.8965
[09/16 04:18:09 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.52	top5: 85.59	
[09/16 04:18:09 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 04:18:17 visual_prompt]: Epoch 27 / 100: avg data time: 9.06e-02, avg batch time: 0.4922, average train loss: 0.1014
[09/16 04:18:20 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1429, average loss: 0.1618
[09/16 04:18:20 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.00	top5: 99.00	
[09/16 04:18:41 visual_prompt]: 	Test 100/157. loss: 2.197, 0.1831 s / batch. (data: 1.09e-04)max mem: 17.22530 GB 
[09/16 04:18:53 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1920, average loss: 1.9146
[09/16 04:18:53 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 61.57	top5: 84.87	
[09/16 04:18:53 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 04:19:02 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e-01, avg batch time: 0.5035, average train loss: 0.2309
[09/16 04:19:05 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1426, average loss: 0.0887
[09/16 04:19:05 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.00	top5: 100.00	
[09/16 04:19:25 visual_prompt]: 	Test 100/157. loss: 2.193, 0.1963 s / batch. (data: 1.43e-02)max mem: 17.22530 GB 
[09/16 04:19:37 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1912, average loss: 2.0059
[09/16 04:19:37 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 60.29	top5: 82.77	
[09/16 04:19:37 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 04:19:46 visual_prompt]: Epoch 29 / 100: avg data time: 1.10e-01, avg batch time: 0.5098, average train loss: 0.1349
[09/16 04:19:49 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1428, average loss: 0.1510
[09/16 04:19:49 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 95.50	top5: 99.50	
[09/16 04:20:10 visual_prompt]: 	Test 100/157. loss: 2.348, 0.1838 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 04:20:21 visual_prompt]: Inference (test):avg data time: 6.86e-03, avg batch time: 0.1913, average loss: 1.8562
[09/16 04:20:22 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.95	top5: 86.32	
[09/16 04:20:22 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 04:20:30 visual_prompt]: Epoch 30 / 100: avg data time: 8.89e-02, avg batch time: 0.4916, average train loss: 0.0779
[09/16 04:20:33 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1426, average loss: 0.0919
[09/16 04:20:33 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.00	top5: 100.00	
[09/16 04:20:54 visual_prompt]: 	Test 100/157. loss: 2.069, 0.1845 s / batch. (data: 9.32e-05)max mem: 17.22530 GB 
[09/16 04:21:06 visual_prompt]: Inference (test):avg data time: 7.07e-03, avg batch time: 0.1912, average loss: 1.6475
[09/16 04:21:06 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 65.46	top5: 88.01	
[09/16 04:21:06 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 04:21:15 visual_prompt]: Epoch 31 / 100: avg data time: 9.54e-02, avg batch time: 0.4975, average train loss: 0.0781
[09/16 04:21:17 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1427, average loss: 0.0863
[09/16 04:21:17 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.50	top5: 99.50	
[09/16 04:21:38 visual_prompt]: 	Test 100/157. loss: 1.644, 0.1824 s / batch. (data: 1.47e-04)max mem: 17.22530 GB 
[09/16 04:21:50 visual_prompt]: Inference (test):avg data time: 6.53e-03, avg batch time: 0.1911, average loss: 1.7884
[09/16 04:21:50 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.74	top5: 85.42	
[09/16 04:21:50 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 04:21:59 visual_prompt]: Epoch 32 / 100: avg data time: 1.02e-01, avg batch time: 0.5017, average train loss: 0.0842
[09/16 04:22:02 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1425, average loss: 0.0375
[09/16 04:22:02 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.50	top5: 100.00	
[09/16 04:22:23 visual_prompt]: 	Test 100/157. loss: 1.689, 0.1835 s / batch. (data: 9.85e-05)max mem: 17.22530 GB 
[09/16 04:22:34 visual_prompt]: Inference (test):avg data time: 6.81e-03, avg batch time: 0.1917, average loss: 1.6882
[09/16 04:22:34 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 64.97	top5: 87.41	
[09/16 04:22:34 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 04:22:43 visual_prompt]: Epoch 33 / 100: avg data time: 1.05e-01, avg batch time: 0.5071, average train loss: 0.0365
[09/16 04:22:46 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1425, average loss: 0.0414
[09/16 04:22:46 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 99.50	
[09/16 04:23:07 visual_prompt]: 	Test 100/157. loss: 1.576, 0.1828 s / batch. (data: 1.23e-04)max mem: 17.22530 GB 
[09/16 04:23:19 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1919, average loss: 1.4945
[09/16 04:23:19 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 67.29	top5: 87.58	
[09/16 04:23:19 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 04:23:28 visual_prompt]: Epoch 34 / 100: avg data time: 1.03e-01, avg batch time: 0.5036, average train loss: 0.0206
[09/16 04:23:31 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1426, average loss: 0.0204
[09/16 04:23:31 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.00	top5: 100.00	
[09/16 04:23:52 visual_prompt]: 	Test 100/157. loss: 1.449, 0.2066 s / batch. (data: 1.40e-02)max mem: 17.22530 GB 
[09/16 04:24:04 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1923, average loss: 1.4315
[09/16 04:24:04 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 69.48	top5: 90.04	
[09/16 04:24:04 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 04:24:12 visual_prompt]: Epoch 35 / 100: avg data time: 8.51e-02, avg batch time: 0.4885, average train loss: 0.0181
[09/16 04:24:15 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1426, average loss: 0.0176
[09/16 04:24:15 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 100.00	
[09/16 04:24:36 visual_prompt]: 	Test 100/157. loss: 1.511, 0.1825 s / batch. (data: 1.06e-04)max mem: 17.22530 GB 
[09/16 04:24:48 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1918, average loss: 1.3900
[09/16 04:24:48 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 68.95	top5: 89.83	
[09/16 04:24:48 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 04:24:57 visual_prompt]: Epoch 36 / 100: avg data time: 1.05e-01, avg batch time: 0.5072, average train loss: 0.0176
[09/16 04:25:00 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1428, average loss: 0.0062
[09/16 04:25:00 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:25:21 visual_prompt]: 	Test 100/157. loss: 1.294, 0.1880 s / batch. (data: 1.03e-04)max mem: 17.22530 GB 
[09/16 04:25:33 visual_prompt]: Inference (test):avg data time: 8.74e-03, avg batch time: 0.1930, average loss: 1.2112
[09/16 04:25:33 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 71.81	top5: 91.20	
[09/16 04:25:33 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 04:25:42 visual_prompt]: Epoch 37 / 100: avg data time: 9.79e-02, avg batch time: 0.5002, average train loss: 0.0100
[09/16 04:25:44 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1428, average loss: 0.0062
[09/16 04:25:44 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:26:05 visual_prompt]: 	Test 100/157. loss: 1.094, 0.1823 s / batch. (data: 3.55e-05)max mem: 17.22530 GB 
[09/16 04:26:17 visual_prompt]: Inference (test):avg data time: 6.02e-03, avg batch time: 0.1909, average loss: 1.1649
[09/16 04:26:17 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.32	top5: 91.40	
[09/16 04:26:17 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 04:26:26 visual_prompt]: Epoch 38 / 100: avg data time: 1.05e-01, avg batch time: 0.5057, average train loss: 0.0167
[09/16 04:26:29 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1427, average loss: 0.0165
[09/16 04:26:29 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:26:50 visual_prompt]: 	Test 100/157. loss: 1.114, 0.1987 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 04:27:02 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1924, average loss: 1.2119
[09/16 04:27:02 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 71.04	top5: 91.12	
[09/16 04:27:02 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 04:27:11 visual_prompt]: Epoch 39 / 100: avg data time: 9.80e-02, avg batch time: 0.5050, average train loss: 0.0082
[09/16 04:27:14 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1426, average loss: 0.0063
[09/16 04:27:14 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:27:35 visual_prompt]: 	Test 100/157. loss: 1.097, 0.1940 s / batch. (data: 1.22e-02)max mem: 17.22530 GB 
[09/16 04:27:46 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1934, average loss: 1.0991
[09/16 04:27:46 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.58	top5: 92.13	
[09/16 04:27:46 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 04:27:55 visual_prompt]: Epoch 40 / 100: avg data time: 8.52e-02, avg batch time: 0.4896, average train loss: 0.0056
[09/16 04:27:58 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1426, average loss: 0.0055
[09/16 04:27:58 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:28:19 visual_prompt]: 	Test 100/157. loss: 1.079, 0.1830 s / batch. (data: 1.12e-04)max mem: 17.22530 GB 
[09/16 04:28:31 visual_prompt]: Inference (test):avg data time: 8.29e-03, avg batch time: 0.1922, average loss: 1.0577
[09/16 04:28:31 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.14	top5: 92.63	
[09/16 04:28:31 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 04:28:40 visual_prompt]: Epoch 41 / 100: avg data time: 1.01e-01, avg batch time: 0.5024, average train loss: 0.0054
[09/16 04:28:43 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1428, average loss: 0.0057
[09/16 04:28:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:29:04 visual_prompt]: 	Test 100/157. loss: 1.055, 0.2104 s / batch. (data: 1.34e-04)max mem: 17.22530 GB 
[09/16 04:29:16 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1924, average loss: 1.0350
[09/16 04:29:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.75	top5: 92.97	
[09/16 04:29:16 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 04:29:25 visual_prompt]: Epoch 42 / 100: avg data time: 9.90e-02, avg batch time: 0.5012, average train loss: 0.0056
[09/16 04:29:28 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1428, average loss: 0.0061
[09/16 04:29:28 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:29:49 visual_prompt]: 	Test 100/157. loss: 1.054, 0.1979 s / batch. (data: 1.60e-02)max mem: 17.22530 GB 
[09/16 04:30:00 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1933, average loss: 1.0160
[09/16 04:30:01 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.97	top5: 93.22	
[09/16 04:30:01 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 04:30:09 visual_prompt]: Epoch 43 / 100: avg data time: 1.00e-01, avg batch time: 0.5031, average train loss: 0.0061
[09/16 04:30:12 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1428, average loss: 0.0061
[09/16 04:30:12 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:30:33 visual_prompt]: 	Test 100/157. loss: 1.004, 0.1829 s / batch. (data: 1.12e-04)max mem: 17.22530 GB 
[09/16 04:30:45 visual_prompt]: Inference (test):avg data time: 6.29e-03, avg batch time: 0.1921, average loss: 1.0004
[09/16 04:30:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.30	top5: 93.47	
[09/16 04:30:45 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 04:30:54 visual_prompt]: Epoch 44 / 100: avg data time: 1.09e-01, avg batch time: 0.5100, average train loss: 0.0067
[09/16 04:30:57 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1428, average loss: 0.0062
[09/16 04:30:57 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:31:18 visual_prompt]: 	Test 100/157. loss: 1.043, 0.2087 s / batch. (data: 2.69e-02)max mem: 17.22530 GB 
[09/16 04:31:30 visual_prompt]: Inference (test):avg data time: 8.72e-03, avg batch time: 0.1930, average loss: 0.9916
[09/16 04:31:30 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.31	top5: 93.67	
[09/16 04:31:30 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 04:31:39 visual_prompt]: Epoch 45 / 100: avg data time: 9.78e-02, avg batch time: 0.5014, average train loss: 0.0067
[09/16 04:31:41 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1428, average loss: 0.0065
[09/16 04:31:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:32:02 visual_prompt]: 	Test 100/157. loss: 1.046, 0.1828 s / batch. (data: 1.42e-04)max mem: 17.22530 GB 
[09/16 04:32:14 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1918, average loss: 0.9830
[09/16 04:32:14 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.30	top5: 93.83	
[09/16 04:32:14 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 04:32:23 visual_prompt]: Epoch 46 / 100: avg data time: 1.03e-01, avg batch time: 0.5038, average train loss: 0.0067
[09/16 04:32:26 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1427, average loss: 0.0063
[09/16 04:32:26 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:32:47 visual_prompt]: 	Test 100/157. loss: 1.004, 0.1827 s / batch. (data: 1.39e-04)max mem: 17.22530 GB 
[09/16 04:32:59 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1928, average loss: 0.9706
[09/16 04:32:59 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.71	top5: 94.04	
[09/16 04:32:59 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 04:33:08 visual_prompt]: Epoch 47 / 100: avg data time: 1.04e-01, avg batch time: 0.5068, average train loss: 0.0067
[09/16 04:33:11 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1429, average loss: 0.0060
[09/16 04:33:11 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:33:32 visual_prompt]: 	Test 100/157. loss: 1.002, 0.1828 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 04:33:43 visual_prompt]: Inference (test):avg data time: 6.57e-03, avg batch time: 0.1920, average loss: 0.9551
[09/16 04:33:43 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.85	top5: 94.18	
[09/16 04:33:43 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 04:33:52 visual_prompt]: Epoch 48 / 100: avg data time: 8.45e-02, avg batch time: 0.4886, average train loss: 0.0068
[09/16 04:33:55 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1428, average loss: 0.0061
[09/16 04:33:55 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:34:16 visual_prompt]: 	Test 100/157. loss: 1.016, 0.1825 s / batch. (data: 1.59e-04)max mem: 17.22530 GB 
[09/16 04:34:28 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1926, average loss: 0.9480
[09/16 04:34:28 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.23	top5: 94.31	
[09/16 04:34:28 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 04:34:37 visual_prompt]: Epoch 49 / 100: avg data time: 1.05e-01, avg batch time: 0.5059, average train loss: 0.0066
[09/16 04:34:39 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1426, average loss: 0.0058
[09/16 04:34:39 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:35:00 visual_prompt]: 	Test 100/157. loss: 0.978, 0.1965 s / batch. (data: 1.43e-02)max mem: 17.22530 GB 
[09/16 04:35:12 visual_prompt]: Inference (test):avg data time: 8.53e-03, avg batch time: 0.1929, average loss: 0.9362
[09/16 04:35:12 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.51	top5: 94.30	
[09/16 04:35:12 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 04:35:21 visual_prompt]: Epoch 50 / 100: avg data time: 9.24e-02, avg batch time: 0.4959, average train loss: 0.0064
[09/16 04:35:24 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1428, average loss: 0.0056
[09/16 04:35:24 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:35:45 visual_prompt]: 	Test 100/157. loss: 0.999, 0.1966 s / batch. (data: 1.41e-02)max mem: 17.22530 GB 
[09/16 04:35:57 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1928, average loss: 0.9328
[09/16 04:35:57 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.49	top5: 94.39	
[09/16 04:35:57 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 04:36:06 visual_prompt]: Epoch 51 / 100: avg data time: 9.61e-02, avg batch time: 0.5029, average train loss: 0.0065
[09/16 04:36:09 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1425, average loss: 0.0054
[09/16 04:36:09 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:36:29 visual_prompt]: 	Test 100/157. loss: 0.938, 0.1816 s / batch. (data: 1.18e-04)max mem: 17.22530 GB 
[09/16 04:36:41 visual_prompt]: Inference (test):avg data time: 6.66e-03, avg batch time: 0.1910, average loss: 0.9296
[09/16 04:36:41 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.45	top5: 94.59	
[09/16 04:36:41 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 04:36:50 visual_prompt]: Epoch 52 / 100: avg data time: 8.69e-02, avg batch time: 0.4905, average train loss: 0.0063
[09/16 04:36:53 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1424, average loss: 0.0057
[09/16 04:36:53 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:37:14 visual_prompt]: 	Test 100/157. loss: 0.891, 0.1823 s / batch. (data: 3.53e-05)max mem: 17.22530 GB 
[09/16 04:37:25 visual_prompt]: Inference (test):avg data time: 6.72e-03, avg batch time: 0.1922, average loss: 0.9411
[09/16 04:37:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.60	top5: 94.69	
[09/16 04:37:25 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 04:37:34 visual_prompt]: Epoch 53 / 100: avg data time: 8.77e-02, avg batch time: 0.4909, average train loss: 0.0061
[09/16 04:37:37 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1428, average loss: 0.0051
[09/16 04:37:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:37:58 visual_prompt]: 	Test 100/157. loss: 0.943, 0.1995 s / batch. (data: 1.74e-02)max mem: 17.22530 GB 
[09/16 04:38:09 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1919, average loss: 0.9298
[09/16 04:38:10 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.03	top5: 94.72	
[09/16 04:38:10 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 04:38:19 visual_prompt]: Epoch 54 / 100: avg data time: 1.09e-01, avg batch time: 0.5103, average train loss: 0.0061
[09/16 04:38:21 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1425, average loss: 0.0049
[09/16 04:38:21 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:38:42 visual_prompt]: 	Test 100/157. loss: 0.903, 0.2112 s / batch. (data: 2.58e-02)max mem: 17.22530 GB 
[09/16 04:38:55 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1945, average loss: 0.9231
[09/16 04:38:55 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.41	top5: 94.71	
[09/16 04:38:55 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 04:39:04 visual_prompt]: Epoch 55 / 100: avg data time: 9.64e-02, avg batch time: 0.5000, average train loss: 0.0060
[09/16 04:39:06 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1427, average loss: 0.0050
[09/16 04:39:06 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:39:27 visual_prompt]: 	Test 100/157. loss: 0.926, 0.2019 s / batch. (data: 2.02e-02)max mem: 17.22530 GB 
[09/16 04:39:39 visual_prompt]: Inference (test):avg data time: 8.76e-03, avg batch time: 0.1921, average loss: 0.9321
[09/16 04:39:39 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.33	top5: 94.86	
[09/16 04:39:39 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 04:39:48 visual_prompt]: Epoch 56 / 100: avg data time: 1.02e-01, avg batch time: 0.5032, average train loss: 0.0056
[09/16 04:39:51 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1428, average loss: 0.0046
[09/16 04:39:51 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:40:12 visual_prompt]: 	Test 100/157. loss: 0.957, 0.1950 s / batch. (data: 1.30e-02)max mem: 17.22530 GB 
[09/16 04:40:23 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1908, average loss: 0.9268
[09/16 04:40:23 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.45	top5: 94.89	
[09/16 04:40:23 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 04:40:32 visual_prompt]: Epoch 57 / 100: avg data time: 8.77e-02, avg batch time: 0.4891, average train loss: 0.0053
[09/16 04:40:35 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1428, average loss: 0.0044
[09/16 04:40:35 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:40:56 visual_prompt]: 	Test 100/157. loss: 0.910, 0.1862 s / batch. (data: 3.98e-05)max mem: 17.22530 GB 
[09/16 04:41:08 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1958, average loss: 0.9287
[09/16 04:41:08 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.29	top5: 94.79	
[09/16 04:41:08 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 04:41:17 visual_prompt]: Epoch 58 / 100: avg data time: 1.01e-01, avg batch time: 0.5024, average train loss: 0.0051
[09/16 04:41:20 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1426, average loss: 0.0043
[09/16 04:41:20 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:41:41 visual_prompt]: 	Test 100/157. loss: 0.887, 0.1829 s / batch. (data: 1.03e-04)max mem: 17.22530 GB 
[09/16 04:41:53 visual_prompt]: Inference (test):avg data time: 6.43e-03, avg batch time: 0.1914, average loss: 0.9312
[09/16 04:41:53 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.51	top5: 94.98	
[09/16 04:41:53 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 04:42:02 visual_prompt]: Epoch 59 / 100: avg data time: 1.05e-01, avg batch time: 0.5088, average train loss: 0.0049
[09/16 04:42:04 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1427, average loss: 0.0041
[09/16 04:42:04 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:42:25 visual_prompt]: 	Test 100/157. loss: 0.864, 0.1831 s / batch. (data: 1.39e-04)max mem: 17.22530 GB 
[09/16 04:42:37 visual_prompt]: Inference (test):avg data time: 6.63e-03, avg batch time: 0.1917, average loss: 0.9296
[09/16 04:42:37 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.30	top5: 94.96	
[09/16 04:42:37 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 04:42:46 visual_prompt]: Epoch 60 / 100: avg data time: 1.05e-01, avg batch time: 0.5089, average train loss: 0.0048
[09/16 04:42:49 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1426, average loss: 0.0041
[09/16 04:42:49 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:43:10 visual_prompt]: 	Test 100/157. loss: 0.906, 0.1957 s / batch. (data: 1.37e-02)max mem: 17.22530 GB 
[09/16 04:43:22 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1924, average loss: 0.9380
[09/16 04:43:22 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.33	top5: 94.95	
[09/16 04:43:22 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 04:43:31 visual_prompt]: Epoch 61 / 100: avg data time: 9.32e-02, avg batch time: 0.4953, average train loss: 0.0047
[09/16 04:43:33 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.1432, average loss: 0.0040
[09/16 04:43:33 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:43:54 visual_prompt]: 	Test 100/157. loss: 0.904, 0.1829 s / batch. (data: 1.31e-04)max mem: 17.22530 GB 
[09/16 04:44:06 visual_prompt]: Inference (test):avg data time: 6.70e-03, avg batch time: 0.1918, average loss: 0.9442
[09/16 04:44:06 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.79	top5: 95.05	
[09/16 04:44:06 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 04:44:15 visual_prompt]: Epoch 62 / 100: avg data time: 1.03e-01, avg batch time: 0.5044, average train loss: 0.0048
[09/16 04:44:18 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1426, average loss: 0.0041
[09/16 04:44:18 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:44:39 visual_prompt]: 	Test 100/157. loss: 0.970, 0.1829 s / batch. (data: 1.37e-04)max mem: 17.22530 GB 
[09/16 04:44:51 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1931, average loss: 0.9518
[09/16 04:44:51 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.94	top5: 94.98	
[09/16 04:44:51 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 04:45:00 visual_prompt]: Epoch 63 / 100: avg data time: 1.01e-01, avg batch time: 0.5031, average train loss: 0.0051
[09/16 04:45:03 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1425, average loss: 0.0039
[09/16 04:45:03 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:45:24 visual_prompt]: 	Test 100/157. loss: 0.919, 0.2081 s / batch. (data: 2.57e-02)max mem: 17.22530 GB 
[09/16 04:45:35 visual_prompt]: Inference (test):avg data time: 6.34e-03, avg batch time: 0.1918, average loss: 0.9373
[09/16 04:45:35 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.49	top5: 95.07	
[09/16 04:45:35 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 04:45:44 visual_prompt]: Epoch 64 / 100: avg data time: 1.03e-01, avg batch time: 0.5046, average train loss: 0.0050
[09/16 04:45:48 visual_prompt]: Inference (val):avg data time: 4.31e-04, avg batch time: 0.2357, average loss: 0.0038
[09/16 04:45:48 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:46:08 visual_prompt]: 	Test 100/157. loss: 0.958, 0.1916 s / batch. (data: 1.44e-04)max mem: 17.22530 GB 
[09/16 04:46:20 visual_prompt]: Inference (test):avg data time: 6.44e-03, avg batch time: 0.1912, average loss: 0.9592
[09/16 04:46:20 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.91	top5: 94.78	
[09/16 04:46:20 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 04:46:29 visual_prompt]: Epoch 65 / 100: avg data time: 9.92e-02, avg batch time: 0.5002, average train loss: 0.0046
[09/16 04:46:32 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1427, average loss: 0.0035
[09/16 04:46:32 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:46:53 visual_prompt]: 	Test 100/157. loss: 0.944, 0.1828 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 04:47:05 visual_prompt]: Inference (test):avg data time: 8.92e-03, avg batch time: 0.1929, average loss: 0.9516
[09/16 04:47:05 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.96	top5: 95.00	
[09/16 04:47:05 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 04:47:14 visual_prompt]: Epoch 66 / 100: avg data time: 9.12e-02, avg batch time: 0.4947, average train loss: 0.0043
[09/16 04:47:17 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1426, average loss: 0.0036
[09/16 04:47:17 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:47:38 visual_prompt]: 	Test 100/157. loss: 0.937, 0.1830 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 04:47:49 visual_prompt]: Inference (test):avg data time: 6.54e-03, avg batch time: 0.1918, average loss: 0.9620
[09/16 04:47:50 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.84	top5: 95.01	
[09/16 04:47:50 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 04:47:58 visual_prompt]: Epoch 67 / 100: avg data time: 9.21e-02, avg batch time: 0.4985, average train loss: 0.0041
[09/16 04:48:01 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1427, average loss: 0.0033
[09/16 04:48:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:48:22 visual_prompt]: 	Test 100/157. loss: 0.913, 0.1834 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 04:48:34 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1916, average loss: 0.9737
[09/16 04:48:34 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.61	top5: 94.88	
[09/16 04:48:34 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 04:48:43 visual_prompt]: Epoch 68 / 100: avg data time: 9.95e-02, avg batch time: 0.5051, average train loss: 0.0040
[09/16 04:48:46 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1427, average loss: 0.0033
[09/16 04:48:46 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:49:07 visual_prompt]: 	Test 100/157. loss: 0.969, 0.1958 s / batch. (data: 1.35e-02)max mem: 17.22530 GB 
[09/16 04:49:19 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1957, average loss: 0.9838
[09/16 04:49:19 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.34	top5: 94.94	
[09/16 04:49:19 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 04:49:28 visual_prompt]: Epoch 69 / 100: avg data time: 1.04e-01, avg batch time: 0.5046, average train loss: 0.0040
[09/16 04:49:31 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1427, average loss: 0.0033
[09/16 04:49:31 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:49:52 visual_prompt]: 	Test 100/157. loss: 0.950, 0.1823 s / batch. (data: 1.66e-04)max mem: 17.22530 GB 
[09/16 04:50:04 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1929, average loss: 0.9865
[09/16 04:50:04 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.56	top5: 94.85	
[09/16 04:50:04 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 04:50:13 visual_prompt]: Epoch 70 / 100: avg data time: 9.79e-02, avg batch time: 0.4992, average train loss: 0.0039
[09/16 04:50:15 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1427, average loss: 0.0033
[09/16 04:50:16 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:50:37 visual_prompt]: 	Test 100/157. loss: 0.974, 0.1999 s / batch. (data: 1.36e-04)max mem: 17.22530 GB 
[09/16 04:50:49 visual_prompt]: Inference (test):avg data time: 6.78e-03, avg batch time: 0.1942, average loss: 1.0037
[09/16 04:50:49 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.95	top5: 94.66	
[09/16 04:50:49 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 04:50:58 visual_prompt]: Epoch 71 / 100: avg data time: 1.04e-01, avg batch time: 0.5074, average train loss: 0.0039
[09/16 04:51:01 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1426, average loss: 0.0032
[09/16 04:51:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:51:22 visual_prompt]: 	Test 100/157. loss: 0.972, 0.2062 s / batch. (data: 4.84e-05)max mem: 17.22530 GB 
[09/16 04:51:34 visual_prompt]: Inference (test):avg data time: 9.34e-03, avg batch time: 0.1928, average loss: 0.9898
[09/16 04:51:34 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.61	top5: 94.76	
[09/16 04:51:34 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 04:51:43 visual_prompt]: Epoch 72 / 100: avg data time: 1.03e-01, avg batch time: 0.5047, average train loss: 0.0039
[09/16 04:51:45 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1429, average loss: 0.0032
[09/16 04:51:45 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:52:06 visual_prompt]: 	Test 100/157. loss: 0.986, 0.1824 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 04:52:18 visual_prompt]: Inference (test):avg data time: 8.97e-03, avg batch time: 0.1928, average loss: 1.0051
[09/16 04:52:18 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.26	top5: 94.59	
[09/16 04:52:18 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 04:52:27 visual_prompt]: Epoch 73 / 100: avg data time: 1.07e-01, avg batch time: 0.5075, average train loss: 0.0038
[09/16 04:52:30 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1427, average loss: 0.0032
[09/16 04:52:30 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:52:51 visual_prompt]: 	Test 100/157. loss: 1.036, 0.1832 s / batch. (data: 1.36e-04)max mem: 17.22530 GB 
[09/16 04:53:03 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1917, average loss: 1.0106
[09/16 04:53:03 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.28	top5: 94.76	
[09/16 04:53:03 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 04:53:12 visual_prompt]: Epoch 74 / 100: avg data time: 9.92e-02, avg batch time: 0.4994, average train loss: 0.0038
[09/16 04:53:15 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1428, average loss: 0.0032
[09/16 04:53:15 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:53:36 visual_prompt]: 	Test 100/157. loss: 1.004, 0.1975 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 04:53:47 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1927, average loss: 1.0158
[09/16 04:53:48 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.09	top5: 94.82	
[09/16 04:53:48 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 04:53:56 visual_prompt]: Epoch 75 / 100: avg data time: 1.05e-01, avg batch time: 0.5051, average train loss: 0.0038
[09/16 04:53:59 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1426, average loss: 0.0031
[09/16 04:53:59 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:54:20 visual_prompt]: 	Test 100/157. loss: 1.047, 0.2212 s / batch. (data: 3.61e-02)max mem: 17.22530 GB 
[09/16 04:54:32 visual_prompt]: Inference (test):avg data time: 6.67e-03, avg batch time: 0.1914, average loss: 1.0341
[09/16 04:54:32 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.80	top5: 94.65	
[09/16 04:54:32 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 04:54:41 visual_prompt]: Epoch 76 / 100: avg data time: 1.04e-01, avg batch time: 0.5056, average train loss: 0.0038
[09/16 04:54:44 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1426, average loss: 0.0031
[09/16 04:54:44 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:55:04 visual_prompt]: 	Test 100/157. loss: 1.044, 0.1826 s / batch. (data: 1.16e-04)max mem: 17.22530 GB 
[09/16 04:55:16 visual_prompt]: Inference (test):avg data time: 6.89e-03, avg batch time: 0.1914, average loss: 1.0268
[09/16 04:55:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.75	top5: 94.75	
[09/16 04:55:16 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 04:55:25 visual_prompt]: Epoch 77 / 100: avg data time: 1.09e-01, avg batch time: 0.5143, average train loss: 0.0038
[09/16 04:55:28 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1428, average loss: 0.0031
[09/16 04:55:28 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:55:49 visual_prompt]: 	Test 100/157. loss: 1.019, 0.2208 s / batch. (data: 2.53e-02)max mem: 17.22530 GB 
[09/16 04:56:01 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1924, average loss: 1.0429
[09/16 04:56:01 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.35	top5: 94.63	
[09/16 04:56:01 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 04:56:10 visual_prompt]: Epoch 78 / 100: avg data time: 8.79e-02, avg batch time: 0.4898, average train loss: 0.0038
[09/16 04:56:13 visual_prompt]: Inference (val):avg data time: 3.28e-04, avg batch time: 0.2661, average loss: 0.0031
[09/16 04:56:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:56:34 visual_prompt]: 	Test 100/157. loss: 1.092, 0.1830 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 04:56:46 visual_prompt]: Inference (test):avg data time: 4.97e-03, avg batch time: 0.1935, average loss: 1.0428
[09/16 04:56:46 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.68	top5: 94.54	
[09/16 04:56:46 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 04:56:55 visual_prompt]: Epoch 79 / 100: avg data time: 1.01e-01, avg batch time: 0.5033, average train loss: 0.0037
[09/16 04:56:58 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1427, average loss: 0.0031
[09/16 04:56:58 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:57:19 visual_prompt]: 	Test 100/157. loss: 1.045, 0.1958 s / batch. (data: 1.54e-04)max mem: 17.22530 GB 
[09/16 04:57:31 visual_prompt]: Inference (test):avg data time: 6.15e-03, avg batch time: 0.1949, average loss: 1.0485
[09/16 04:57:31 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.59	top5: 94.53	
[09/16 04:57:31 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 04:57:39 visual_prompt]: Epoch 80 / 100: avg data time: 8.32e-02, avg batch time: 0.4868, average train loss: 0.0037
[09/16 04:57:42 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1427, average loss: 0.0031
[09/16 04:57:42 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:58:03 visual_prompt]: 	Test 100/157. loss: 1.064, 0.1836 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 04:58:15 visual_prompt]: Inference (test):avg data time: 9.72e-03, avg batch time: 0.1937, average loss: 1.0458
[09/16 04:58:15 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.61	top5: 94.45	
[09/16 04:58:15 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 04:58:24 visual_prompt]: Epoch 81 / 100: avg data time: 9.93e-02, avg batch time: 0.5012, average train loss: 0.0037
[09/16 04:58:27 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1427, average loss: 0.0031
[09/16 04:58:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:58:48 visual_prompt]: 	Test 100/157. loss: 1.027, 0.1949 s / batch. (data: 1.34e-04)max mem: 17.22530 GB 
[09/16 04:59:00 visual_prompt]: Inference (test):avg data time: 9.09e-03, avg batch time: 0.1953, average loss: 1.0502
[09/16 04:59:00 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.65	top5: 94.43	
[09/16 04:59:00 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 04:59:09 visual_prompt]: Epoch 82 / 100: avg data time: 1.06e-01, avg batch time: 0.5066, average train loss: 0.0036
[09/16 04:59:12 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1427, average loss: 0.0031
[09/16 04:59:12 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 04:59:33 visual_prompt]: 	Test 100/157. loss: 1.071, 0.1825 s / batch. (data: 1.42e-04)max mem: 17.22530 GB 
[09/16 04:59:45 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1916, average loss: 1.0512
[09/16 04:59:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.61	top5: 94.51	
[09/16 04:59:45 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 04:59:54 visual_prompt]: Epoch 83 / 100: avg data time: 1.07e-01, avg batch time: 0.5071, average train loss: 0.0036
[09/16 04:59:57 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1426, average loss: 0.0031
[09/16 04:59:57 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:00:18 visual_prompt]: 	Test 100/157. loss: 1.078, 0.1964 s / batch. (data: 1.46e-02)max mem: 17.22530 GB 
[09/16 05:00:30 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1922, average loss: 1.0608
[09/16 05:00:30 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.32	top5: 94.33	
[09/16 05:00:30 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 05:00:39 visual_prompt]: Epoch 84 / 100: avg data time: 1.01e-01, avg batch time: 0.5023, average train loss: 0.0036
[09/16 05:00:41 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1436, average loss: 0.0030
[09/16 05:00:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:01:02 visual_prompt]: 	Test 100/157. loss: 1.086, 0.1829 s / batch. (data: 1.75e-04)max mem: 17.22530 GB 
[09/16 05:01:14 visual_prompt]: Inference (test):avg data time: 6.68e-03, avg batch time: 0.1917, average loss: 1.0758
[09/16 05:01:14 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.92	top5: 94.38	
[09/16 05:01:14 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 05:01:23 visual_prompt]: Epoch 85 / 100: avg data time: 1.05e-01, avg batch time: 0.5082, average train loss: 0.0036
[09/16 05:01:26 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1425, average loss: 0.0030
[09/16 05:01:26 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:01:48 visual_prompt]: 	Test 100/157. loss: 1.098, 0.2071 s / batch. (data: 2.56e-02)max mem: 17.22530 GB 
[09/16 05:01:59 visual_prompt]: Inference (test):avg data time: 8.46e-03, avg batch time: 0.1960, average loss: 1.0767
[09/16 05:01:59 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.07	top5: 94.31	
[09/16 05:01:59 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 05:02:08 visual_prompt]: Epoch 86 / 100: avg data time: 9.62e-02, avg batch time: 0.4990, average train loss: 0.0036
[09/16 05:02:11 visual_prompt]: Inference (val):avg data time: 4.71e-05, avg batch time: 0.1533, average loss: 0.0030
[09/16 05:02:11 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:02:32 visual_prompt]: 	Test 100/157. loss: 1.089, 0.1967 s / batch. (data: 1.01e-02)max mem: 17.22530 GB 
[09/16 05:02:44 visual_prompt]: Inference (test):avg data time: 6.66e-03, avg batch time: 0.1915, average loss: 1.0803
[09/16 05:02:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.00	top5: 94.28	
[09/16 05:02:44 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 05:02:53 visual_prompt]: Epoch 87 / 100: avg data time: 1.08e-01, avg batch time: 0.5096, average train loss: 0.0036
[09/16 05:02:56 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1430, average loss: 0.0030
[09/16 05:02:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:03:17 visual_prompt]: 	Test 100/157. loss: 1.115, 0.1825 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 05:03:29 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1936, average loss: 1.0846
[09/16 05:03:29 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.93	top5: 94.23	
[09/16 05:03:29 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 05:03:38 visual_prompt]: Epoch 88 / 100: avg data time: 8.39e-02, avg batch time: 0.4866, average train loss: 0.0035
[09/16 05:03:40 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1452, average loss: 0.0030
[09/16 05:03:40 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:04:02 visual_prompt]: 	Test 100/157. loss: 1.133, 0.1820 s / batch. (data: 1.77e-04)max mem: 17.22530 GB 
[09/16 05:04:13 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1927, average loss: 1.0870
[09/16 05:04:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.02	top5: 94.19	
[09/16 05:04:13 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 05:04:22 visual_prompt]: Epoch 89 / 100: avg data time: 9.18e-02, avg batch time: 0.4928, average train loss: 0.0035
[09/16 05:04:25 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1425, average loss: 0.0030
[09/16 05:04:25 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:04:46 visual_prompt]: 	Test 100/157. loss: 1.130, 0.1830 s / batch. (data: 1.18e-04)max mem: 17.22530 GB 
[09/16 05:04:58 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1917, average loss: 1.0933
[09/16 05:04:58 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.77	top5: 94.23	
[09/16 05:04:58 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 05:05:07 visual_prompt]: Epoch 90 / 100: avg data time: 9.94e-02, avg batch time: 0.5019, average train loss: 0.0035
[09/16 05:05:10 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1429, average loss: 0.0030
[09/16 05:05:10 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:05:31 visual_prompt]: 	Test 100/157. loss: 1.120, 0.1831 s / batch. (data: 9.23e-05)max mem: 17.22530 GB 
[09/16 05:05:43 visual_prompt]: Inference (test):avg data time: 8.29e-03, avg batch time: 0.1923, average loss: 1.0933
[09/16 05:05:43 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.79	top5: 94.25	
[09/16 05:05:43 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 05:05:52 visual_prompt]: Epoch 91 / 100: avg data time: 1.06e-01, avg batch time: 0.5129, average train loss: 0.0035
[09/16 05:05:54 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1428, average loss: 0.0030
[09/16 05:05:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:06:15 visual_prompt]: 	Test 100/157. loss: 1.132, 0.1823 s / batch. (data: 1.09e-04)max mem: 17.22530 GB 
[09/16 05:06:27 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1944, average loss: 1.0912
[09/16 05:06:28 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.94	top5: 94.25	
[09/16 05:06:28 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 05:06:36 visual_prompt]: Epoch 92 / 100: avg data time: 1.06e-01, avg batch time: 0.5061, average train loss: 0.0035
[09/16 05:06:39 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1465, average loss: 0.0030
[09/16 05:06:39 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:07:00 visual_prompt]: 	Test 100/157. loss: 1.144, 0.1827 s / batch. (data: 1.34e-04)max mem: 17.22530 GB 
[09/16 05:07:12 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1924, average loss: 1.0938
[09/16 05:07:12 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.86	top5: 94.21	
[09/16 05:07:12 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 05:07:21 visual_prompt]: Epoch 93 / 100: avg data time: 1.03e-01, avg batch time: 0.5030, average train loss: 0.0035
[09/16 05:07:24 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1428, average loss: 0.0030
[09/16 05:07:24 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:07:45 visual_prompt]: 	Test 100/157. loss: 1.132, 0.1963 s / batch. (data: 1.43e-02)max mem: 17.22530 GB 
[09/16 05:07:57 visual_prompt]: Inference (test):avg data time: 5.75e-03, avg batch time: 0.1906, average loss: 1.0962
[09/16 05:07:57 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.72	top5: 94.19	
[09/16 05:07:57 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 05:08:06 visual_prompt]: Epoch 94 / 100: avg data time: 1.02e-01, avg batch time: 0.5024, average train loss: 0.0035
[09/16 05:08:08 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1425, average loss: 0.0030
[09/16 05:08:08 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:08:30 visual_prompt]: 	Test 100/157. loss: 1.120, 0.1821 s / batch. (data: 9.70e-05)max mem: 17.22530 GB 
[09/16 05:08:41 visual_prompt]: Inference (test):avg data time: 6.90e-03, avg batch time: 0.1934, average loss: 1.0959
[09/16 05:08:41 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.81	top5: 94.17	
[09/16 05:08:41 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 05:08:51 visual_prompt]: Epoch 95 / 100: avg data time: 8.83e-02, avg batch time: 0.4942, average train loss: 0.0035
[09/16 05:08:54 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1426, average loss: 0.0030
[09/16 05:08:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:09:15 visual_prompt]: 	Test 100/157. loss: 1.118, 0.1974 s / batch. (data: 1.21e-02)max mem: 17.22530 GB 
[09/16 05:09:27 visual_prompt]: Inference (test):avg data time: 8.45e-03, avg batch time: 0.1947, average loss: 1.0962
[09/16 05:09:27 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.76	top5: 94.23	
[09/16 05:09:27 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 05:09:36 visual_prompt]: Epoch 96 / 100: avg data time: 8.83e-02, avg batch time: 0.4937, average train loss: 0.0035
[09/16 05:09:38 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1426, average loss: 0.0030
[09/16 05:09:38 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:09:59 visual_prompt]: 	Test 100/157. loss: 1.122, 0.2288 s / batch. (data: 4.71e-02)max mem: 17.22530 GB 
[09/16 05:10:11 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1925, average loss: 1.0968
[09/16 05:10:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.76	top5: 94.23	
[09/16 05:10:11 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 05:10:20 visual_prompt]: Epoch 97 / 100: avg data time: 9.28e-02, avg batch time: 0.4969, average train loss: 0.0035
[09/16 05:10:22 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1426, average loss: 0.0030
[09/16 05:10:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:10:44 visual_prompt]: 	Test 100/157. loss: 1.125, 0.1961 s / batch. (data: 1.40e-02)max mem: 17.22530 GB 
[09/16 05:10:56 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1949, average loss: 1.0977
[09/16 05:10:56 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.72	top5: 94.23	
[09/16 05:10:56 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 05:11:05 visual_prompt]: Epoch 98 / 100: avg data time: 1.04e-01, avg batch time: 0.5083, average train loss: 0.0035
[09/16 05:11:08 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1427, average loss: 0.0030
[09/16 05:11:08 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:11:29 visual_prompt]: 	Test 100/157. loss: 1.126, 0.1915 s / batch. (data: 5.87e-05)max mem: 17.22530 GB 
[09/16 05:11:40 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1923, average loss: 1.0984
[09/16 05:11:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.71	top5: 94.22	
[09/16 05:11:40 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 05:11:49 visual_prompt]: Epoch 99 / 100: avg data time: 9.40e-02, avg batch time: 0.4944, average train loss: 0.0035
[09/16 05:11:52 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1427, average loss: 0.0030
[09/16 05:11:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:12:13 visual_prompt]: 	Test 100/157. loss: 1.126, 0.1824 s / batch. (data: 1.64e-04)max mem: 17.22530 GB 
[09/16 05:12:25 visual_prompt]: Inference (test):avg data time: 7.01e-03, avg batch time: 0.1911, average loss: 1.0984
[09/16 05:12:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.74	top5: 94.23	
[09/16 05:12:25 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 05:12:34 visual_prompt]: Epoch 100 / 100: avg data time: 1.01e-01, avg batch time: 0.5042, average train loss: 0.0035
[09/16 05:12:36 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1429, average loss: 0.0030
[09/16 05:12:36 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:12:57 visual_prompt]: 	Test 100/157. loss: 1.126, 0.2077 s / batch. (data: 1.37e-02)max mem: 17.22530 GB 
[09/16 05:13:09 visual_prompt]: Inference (test):avg data time: 8.56e-03, avg batch time: 0.1923, average loss: 1.0984
[09/16 05:13:09 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.73	top5: 94.23	
[09/16 05:13:38 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 05:13:38 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 05:13:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-cifar(num_classes=100)', 'DATA.NUMBER_CLASSES', '100', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed100'], train_type='')
[09/16 05:13:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 05:13:38 visual_prompt]: Training with config:
[09/16 05:13:38 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-cifar(num_classes=100)',
          'NO_TEST': False,
          'NUMBER_CLASSES': 100,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed100/vtab-cifar(num_classes=100)/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 05:13:38 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 05:13:38.523258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 05:13:38.690902: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 05:13:39.600533: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 05:13:39.600613: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 05:13:39.600623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 05:13:41.650705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 05:13:41.650809: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 05:13:41.650823: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 05:13:41 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
2023-09-16 05:13:41.665570: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split train[:800]+train[45000:45200], from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 05:13:43 visual_prompt]: Number of images: 1000
[09/16 05:13:43 visual_prompt]: Number of classes: 100 / 100
[09/16 05:13:43 visual_prompt]: Loading validation data...
[09/16 05:13:43 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split train[45000:45200], from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 05:13:43 visual_prompt]: Number of images: 200
[09/16 05:13:43 visual_prompt]: Number of classes: 90 / 100
[09/16 05:13:43 visual_prompt]: Loading test data...
[09/16 05:13:43 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split test, from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 05:13:56 visual_prompt]: Number of images: 10000
[09/16 05:13:56 visual_prompt]: Number of classes: 100 / 100
[09/16 05:13:56 visual_prompt]: Constructing models...
[09/16 05:13:59 visual_prompt]: Total Parameters: 86797156	 Gradient Parameters: 998500
[09/16 05:13:59 visual_prompt]: tuned percent:1.150
[09/16 05:14:01 visual_prompt]: Device used for model: 0
[09/16 05:14:01 visual_prompt]: Setting up Evalutator...
[09/16 05:14:01 visual_prompt]: Setting up Trainer...
[09/16 05:14:01 visual_prompt]: 	Setting up the optimizer...
[09/16 05:14:01 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 05:14:11 visual_prompt]: Epoch 1 / 100: avg data time: 9.43e-02, avg batch time: 0.5815, average train loss: 4.6587
[09/16 05:14:14 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1419, average loss: 4.6106
[09/16 05:14:14 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 5.50	
[09/16 05:14:35 visual_prompt]: 	Test 100/157. loss: 4.642, 0.1938 s / batch. (data: 1.26e-02)max mem: 17.22530 GB 
[09/16 05:14:47 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1920, average loss: 4.6481
[09/16 05:14:47 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 4.61	
[09/16 05:14:47 visual_prompt]: Best epoch 1: best metric: 0.015
[09/16 05:14:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 05:14:56 visual_prompt]: Epoch 2 / 100: avg data time: 9.76e-02, avg batch time: 0.4977, average train loss: 4.6466
[09/16 05:14:59 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1423, average loss: 4.6073
[09/16 05:14:59 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 0.50	top5: 5.00	
[09/16 05:15:20 visual_prompt]: 	Test 100/157. loss: 4.721, 0.1962 s / batch. (data: 1.49e-04)max mem: 17.22530 GB 
[09/16 05:15:32 visual_prompt]: Inference (test):avg data time: 8.11e-03, avg batch time: 0.1924, average loss: 4.7224
[09/16 05:15:32 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.59	
[09/16 05:15:32 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 05:15:40 visual_prompt]: Epoch 3 / 100: avg data time: 9.97e-02, avg batch time: 0.4991, average train loss: 4.6769
[09/16 05:15:43 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 4.5678
[09/16 05:15:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 3.00	top5: 9.00	
[09/16 05:16:04 visual_prompt]: 	Test 100/157. loss: 4.570, 0.1922 s / batch. (data: 1.44e-04)max mem: 17.22530 GB 
[09/16 05:16:16 visual_prompt]: Inference (test):avg data time: 6.81e-03, avg batch time: 0.1917, average loss: 4.6773
[09/16 05:16:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.93	top5: 5.22	
[09/16 05:16:16 visual_prompt]: Best epoch 3: best metric: 0.030
[09/16 05:16:16 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 05:16:25 visual_prompt]: Epoch 4 / 100: avg data time: 1.00e-01, avg batch time: 0.5011, average train loss: 4.7327
[09/16 05:16:28 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1427, average loss: 4.6188
[09/16 05:16:28 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 0.50	top5: 6.00	
[09/16 05:16:49 visual_prompt]: 	Test 100/157. loss: 4.711, 0.1891 s / batch. (data: 1.70e-04)max mem: 17.22530 GB 
[09/16 05:17:01 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1931, average loss: 4.6679
[09/16 05:17:01 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.31	top5: 5.43	
[09/16 05:17:01 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 05:17:10 visual_prompt]: Epoch 5 / 100: avg data time: 1.01e-01, avg batch time: 0.5016, average train loss: 4.6945
[09/16 05:17:13 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1427, average loss: 4.5444
[09/16 05:17:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 8.50	
[09/16 05:17:33 visual_prompt]: 	Test 100/157. loss: 4.589, 0.1828 s / batch. (data: 1.16e-04)max mem: 17.22530 GB 
[09/16 05:17:45 visual_prompt]: Inference (test):avg data time: 6.05e-03, avg batch time: 0.1910, average loss: 4.6544
[09/16 05:17:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.46	top5: 8.31	
[09/16 05:17:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 05:17:54 visual_prompt]: Epoch 6 / 100: avg data time: 1.05e-01, avg batch time: 0.5077, average train loss: 4.5679
[09/16 05:17:57 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1426, average loss: 4.2133
[09/16 05:17:57 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 5.50	top5: 25.00	
[09/16 05:18:18 visual_prompt]: 	Test 100/157. loss: 4.236, 0.2015 s / batch. (data: 1.29e-02)max mem: 17.22530 GB 
[09/16 05:18:30 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1907, average loss: 4.3459
[09/16 05:18:30 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 4.01	top5: 17.44	
[09/16 05:18:30 visual_prompt]: Best epoch 6: best metric: 0.055
[09/16 05:18:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 05:18:38 visual_prompt]: Epoch 7 / 100: avg data time: 8.64e-02, avg batch time: 0.4861, average train loss: 4.1935
[09/16 05:18:41 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1425, average loss: 3.8116
[09/16 05:18:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 7.50	top5: 28.50	
[09/16 05:19:03 visual_prompt]: 	Test 100/157. loss: 3.900, 0.2145 s / batch. (data: 3.31e-05)max mem: 17.22530 GB 
[09/16 05:19:15 visual_prompt]: Inference (test):avg data time: 8.41e-03, avg batch time: 0.1978, average loss: 4.0351
[09/16 05:19:15 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 7.62	top5: 25.48	
[09/16 05:19:15 visual_prompt]: Best epoch 7: best metric: 0.075
[09/16 05:19:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 05:19:24 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e-01, avg batch time: 0.5060, average train loss: 3.9703
[09/16 05:19:27 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1425, average loss: 3.5170
[09/16 05:19:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 11.00	top5: 39.50	
[09/16 05:19:48 visual_prompt]: 	Test 100/157. loss: 3.850, 0.1826 s / batch. (data: 1.24e-04)max mem: 17.22530 GB 
[09/16 05:20:00 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1935, average loss: 3.8057
[09/16 05:20:00 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 9.85	top5: 32.50	
[09/16 05:20:00 visual_prompt]: Best epoch 8: best metric: 0.110
[09/16 05:20:00 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 05:20:09 visual_prompt]: Epoch 9 / 100: avg data time: 9.95e-02, avg batch time: 0.5010, average train loss: 3.6323
[09/16 05:20:12 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1427, average loss: 6.3557
[09/16 05:20:12 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 4.00	top5: 10.00	
[09/16 05:20:32 visual_prompt]: 	Test 100/157. loss: 6.758, 0.1972 s / batch. (data: 1.52e-02)max mem: 17.22530 GB 
[09/16 05:20:44 visual_prompt]: Inference (test):avg data time: 6.92e-03, avg batch time: 0.1935, average loss: 6.4529
[09/16 05:20:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 3.02	top5: 9.68	
[09/16 05:20:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 05:20:53 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e-01, avg batch time: 0.5037, average train loss: 4.2073
[09/16 05:20:56 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1427, average loss: 4.2706
[09/16 05:20:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 10.50	top5: 35.00	
[09/16 05:21:17 visual_prompt]: 	Test 100/157. loss: 4.479, 0.1875 s / batch. (data: 5.20e-03)max mem: 17.22530 GB 
[09/16 05:21:29 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1930, average loss: 4.5971
[09/16 05:21:29 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 8.18	top5: 25.91	
[09/16 05:21:29 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 05:21:38 visual_prompt]: Epoch 11 / 100: avg data time: 9.44e-02, avg batch time: 0.4952, average train loss: 3.2732
[09/16 05:21:41 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1429, average loss: 2.8493
[09/16 05:21:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 29.50	top5: 65.50	
[09/16 05:22:02 visual_prompt]: 	Test 100/157. loss: 3.273, 0.1822 s / batch. (data: 1.48e-04)max mem: 17.22530 GB 
[09/16 05:22:14 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1918, average loss: 3.5502
[09/16 05:22:14 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 20.79	top5: 50.29	
[09/16 05:22:14 visual_prompt]: Best epoch 11: best metric: 0.295
[09/16 05:22:14 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 05:22:23 visual_prompt]: Epoch 12 / 100: avg data time: 1.11e-01, avg batch time: 0.5099, average train loss: 2.1689
[09/16 05:22:26 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1427, average loss: 1.6247
[09/16 05:22:26 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 55.50	top5: 83.50	
[09/16 05:22:47 visual_prompt]: 	Test 100/157. loss: 2.215, 0.1824 s / batch. (data: 9.89e-05)max mem: 17.22530 GB 
[09/16 05:22:58 visual_prompt]: Inference (test):avg data time: 6.03e-03, avg batch time: 0.1920, average loss: 2.8955
[09/16 05:22:59 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 36.55	top5: 67.48	
[09/16 05:22:59 visual_prompt]: Best epoch 12: best metric: 0.555
[09/16 05:22:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 05:23:07 visual_prompt]: Epoch 13 / 100: avg data time: 9.09e-02, avg batch time: 0.4954, average train loss: 1.3329
[09/16 05:23:10 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1427, average loss: 0.6993
[09/16 05:23:10 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 78.50	top5: 96.00	
[09/16 05:23:31 visual_prompt]: 	Test 100/157. loss: 2.445, 0.1965 s / batch. (data: 1.49e-02)max mem: 17.22530 GB 
[09/16 05:23:43 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1928, average loss: 2.4446
[09/16 05:23:43 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 45.56	top5: 81.01	
[09/16 05:23:43 visual_prompt]: Best epoch 13: best metric: 0.785
[09/16 05:23:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 05:23:52 visual_prompt]: Epoch 14 / 100: avg data time: 9.28e-02, avg batch time: 0.4968, average train loss: 0.7683
[09/16 05:23:55 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1426, average loss: 0.5932
[09/16 05:23:55 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 81.50	top5: 98.00	
[09/16 05:24:15 visual_prompt]: 	Test 100/157. loss: 2.509, 0.1972 s / batch. (data: 1.52e-02)max mem: 17.22530 GB 
[09/16 05:24:27 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1915, average loss: 2.6810
[09/16 05:24:27 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 48.20	top5: 77.73	
[09/16 05:24:27 visual_prompt]: Best epoch 14: best metric: 0.815
[09/16 05:24:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 05:24:36 visual_prompt]: Epoch 15 / 100: avg data time: 9.80e-02, avg batch time: 0.4992, average train loss: 0.5527
[09/16 05:24:39 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1427, average loss: 0.6423
[09/16 05:24:39 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 84.00	top5: 98.50	
[09/16 05:25:00 visual_prompt]: 	Test 100/157. loss: 2.632, 0.1887 s / batch. (data: 9.92e-05)max mem: 17.22530 GB 
[09/16 05:25:12 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1915, average loss: 2.7442
[09/16 05:25:12 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 51.48	top5: 80.91	
[09/16 05:25:12 visual_prompt]: Best epoch 15: best metric: 0.840
[09/16 05:25:12 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 05:25:21 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e-01, avg batch time: 0.5045, average train loss: 0.4019
[09/16 05:25:24 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1428, average loss: 0.4613
[09/16 05:25:24 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 90.50	top5: 99.00	
[09/16 05:25:45 visual_prompt]: 	Test 100/157. loss: 2.674, 0.1992 s / batch. (data: 1.49e-02)max mem: 17.22530 GB 
[09/16 05:25:56 visual_prompt]: Inference (test):avg data time: 6.54e-03, avg batch time: 0.1930, average loss: 2.6558
[09/16 05:25:57 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 53.04	top5: 79.86	
[09/16 05:25:57 visual_prompt]: Best epoch 16: best metric: 0.905
[09/16 05:25:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 05:26:05 visual_prompt]: Epoch 17 / 100: avg data time: 8.98e-02, avg batch time: 0.4922, average train loss: 0.2733
[09/16 05:26:08 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1427, average loss: 0.1724
[09/16 05:26:08 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 94.00	top5: 100.00	
[09/16 05:26:29 visual_prompt]: 	Test 100/157. loss: 2.160, 0.1958 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 05:26:41 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1916, average loss: 2.0588
[09/16 05:26:41 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 59.07	top5: 85.29	
[09/16 05:26:41 visual_prompt]: Best epoch 17: best metric: 0.940
[09/16 05:26:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 05:26:49 visual_prompt]: Epoch 18 / 100: avg data time: 8.92e-02, avg batch time: 0.4913, average train loss: 0.2406
[09/16 05:26:52 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1429, average loss: 0.2573
[09/16 05:26:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.00	top5: 99.50	
[09/16 05:27:13 visual_prompt]: 	Test 100/157. loss: 1.701, 0.2029 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 05:27:25 visual_prompt]: Inference (test):avg data time: 6.22e-03, avg batch time: 0.1913, average loss: 2.0465
[09/16 05:27:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 59.35	top5: 84.13	
[09/16 05:27:25 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 05:27:34 visual_prompt]: Epoch 19 / 100: avg data time: 1.01e-01, avg batch time: 0.5026, average train loss: 0.2172
[09/16 05:27:37 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1443, average loss: 0.2509
[09/16 05:27:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.00	top5: 98.00	
[09/16 05:27:58 visual_prompt]: 	Test 100/157. loss: 1.830, 0.1821 s / batch. (data: 2.96e-05)max mem: 17.22530 GB 
[09/16 05:28:10 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1917, average loss: 2.1275
[09/16 05:28:10 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 59.86	top5: 84.25	
[09/16 05:28:10 visual_prompt]: Best epoch 19: best metric: 0.960
[09/16 05:28:10 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 05:28:19 visual_prompt]: Epoch 20 / 100: avg data time: 9.88e-02, avg batch time: 0.5036, average train loss: 0.2324
[09/16 05:28:22 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1427, average loss: 0.1549
[09/16 05:28:22 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.00	top5: 100.00	
[09/16 05:28:42 visual_prompt]: 	Test 100/157. loss: 2.009, 0.2104 s / batch. (data: 2.58e-02)max mem: 17.22530 GB 
[09/16 05:28:54 visual_prompt]: Inference (test):avg data time: 6.73e-03, avg batch time: 0.1923, average loss: 2.0903
[09/16 05:28:54 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 59.48	top5: 84.93	
[09/16 05:28:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 05:29:03 visual_prompt]: Epoch 21 / 100: avg data time: 9.57e-02, avg batch time: 0.4999, average train loss: 0.1489
[09/16 05:29:06 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1428, average loss: 0.2315
[09/16 05:29:06 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 94.50	top5: 98.50	
[09/16 05:29:27 visual_prompt]: 	Test 100/157. loss: 1.611, 0.1961 s / batch. (data: 1.40e-02)max mem: 17.22530 GB 
[09/16 05:29:39 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1920, average loss: 2.0538
[09/16 05:29:39 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 60.94	top5: 85.48	
[09/16 05:29:39 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 05:29:48 visual_prompt]: Epoch 22 / 100: avg data time: 1.09e-01, avg batch time: 0.5094, average train loss: 0.1181
[09/16 05:29:51 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1426, average loss: 0.1958
[09/16 05:29:51 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 94.00	top5: 99.50	
[09/16 05:30:12 visual_prompt]: 	Test 100/157. loss: 1.776, 0.1828 s / batch. (data: 1.40e-04)max mem: 17.22530 GB 
[09/16 05:30:24 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1939, average loss: 1.8764
[09/16 05:30:24 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.13	top5: 84.47	
[09/16 05:30:24 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 05:30:33 visual_prompt]: Epoch 23 / 100: avg data time: 9.56e-02, avg batch time: 0.4992, average train loss: 0.1092
[09/16 05:30:36 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1428, average loss: 0.1402
[09/16 05:30:36 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.00	top5: 100.00	
[09/16 05:30:56 visual_prompt]: 	Test 100/157. loss: 1.871, 0.1830 s / batch. (data: 1.26e-04)max mem: 17.22530 GB 
[09/16 05:31:08 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1913, average loss: 2.1053
[09/16 05:31:08 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 60.06	top5: 84.97	
[09/16 05:31:08 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 05:31:17 visual_prompt]: Epoch 24 / 100: avg data time: 1.08e-01, avg batch time: 0.5082, average train loss: 0.1200
[09/16 05:31:20 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1427, average loss: 0.2786
[09/16 05:31:20 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 90.00	top5: 99.00	
[09/16 05:31:41 visual_prompt]: 	Test 100/157. loss: 1.377, 0.1831 s / batch. (data: 1.43e-04)max mem: 17.22530 GB 
[09/16 05:31:53 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1924, average loss: 1.9213
[09/16 05:31:53 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 63.45	top5: 86.46	
[09/16 05:31:53 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 05:32:02 visual_prompt]: Epoch 25 / 100: avg data time: 9.55e-02, avg batch time: 0.4969, average train loss: 0.1438
[09/16 05:32:04 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1427, average loss: 0.2233
[09/16 05:32:04 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 91.00	top5: 99.50	
[09/16 05:32:25 visual_prompt]: 	Test 100/157. loss: 2.084, 0.1932 s / batch. (data: 1.13e-02)max mem: 17.22530 GB 
[09/16 05:32:37 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1926, average loss: 2.2340
[09/16 05:32:37 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.58	top5: 81.83	
[09/16 05:32:37 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 05:32:46 visual_prompt]: Epoch 26 / 100: avg data time: 8.85e-02, avg batch time: 0.4932, average train loss: 0.1314
[09/16 05:32:49 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1428, average loss: 0.1493
[09/16 05:32:49 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.00	top5: 100.00	
[09/16 05:33:11 visual_prompt]: 	Test 100/157. loss: 1.488, 0.2113 s / batch. (data: 1.73e-02)max mem: 17.22530 GB 
[09/16 05:33:23 visual_prompt]: Inference (test):avg data time: 8.23e-03, avg batch time: 0.1970, average loss: 1.8520
[09/16 05:33:23 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.10	top5: 86.65	
[09/16 05:33:23 visual_prompt]: Best epoch 26: best metric: 0.970
[09/16 05:33:23 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 05:33:32 visual_prompt]: Epoch 27 / 100: avg data time: 1.00e-01, avg batch time: 0.5008, average train loss: 0.1288
[09/16 05:33:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1429, average loss: 0.1291
[09/16 05:33:34 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.00	top5: 100.00	
[09/16 05:33:55 visual_prompt]: 	Test 100/157. loss: 1.008, 0.1975 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 05:34:07 visual_prompt]: Inference (test):avg data time: 6.67e-03, avg batch time: 0.1928, average loss: 1.5890
[09/16 05:34:07 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 66.15	top5: 87.62	
[09/16 05:34:07 visual_prompt]: Best epoch 27: best metric: 0.980
[09/16 05:34:07 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 05:34:16 visual_prompt]: Epoch 28 / 100: avg data time: 9.40e-02, avg batch time: 0.4949, average train loss: 0.0810
[09/16 05:34:19 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.1428, average loss: 0.0498
[09/16 05:34:19 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.50	top5: 100.00	
[09/16 05:34:40 visual_prompt]: 	Test 100/157. loss: 1.207, 0.1911 s / batch. (data: 1.45e-04)max mem: 17.22530 GB 
[09/16 05:34:52 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1938, average loss: 1.5627
[09/16 05:34:52 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 66.55	top5: 88.23	
[09/16 05:34:52 visual_prompt]: Best epoch 28: best metric: 0.985
[09/16 05:34:52 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 05:35:01 visual_prompt]: Epoch 29 / 100: avg data time: 9.43e-02, avg batch time: 0.4978, average train loss: 0.0334
[09/16 05:35:04 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1427, average loss: 0.0084
[09/16 05:35:04 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:35:25 visual_prompt]: 	Test 100/157. loss: 1.085, 0.1826 s / batch. (data: 1.42e-04)max mem: 17.22530 GB 
[09/16 05:35:37 visual_prompt]: Inference (test):avg data time: 8.85e-03, avg batch time: 0.1925, average loss: 1.3906
[09/16 05:35:37 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 69.50	top5: 90.13	
[09/16 05:35:37 visual_prompt]: Best epoch 29: best metric: 1.000
[09/16 05:35:37 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 05:35:46 visual_prompt]: Epoch 30 / 100: avg data time: 9.68e-02, avg batch time: 0.4990, average train loss: 0.0107
[09/16 05:35:48 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1429, average loss: 0.0063
[09/16 05:35:48 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:36:09 visual_prompt]: 	Test 100/157. loss: 1.016, 0.1826 s / batch. (data: 1.62e-04)max mem: 17.22530 GB 
[09/16 05:36:21 visual_prompt]: Inference (test):avg data time: 8.29e-03, avg batch time: 0.1922, average loss: 1.2928
[09/16 05:36:21 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 70.49	top5: 90.71	
[09/16 05:36:21 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 05:36:30 visual_prompt]: Epoch 31 / 100: avg data time: 1.05e-01, avg batch time: 0.5059, average train loss: 0.0061
[09/16 05:36:33 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1424, average loss: 0.0045
[09/16 05:36:33 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:36:54 visual_prompt]: 	Test 100/157. loss: 0.856, 0.2158 s / batch. (data: 3.39e-02)max mem: 17.22530 GB 
[09/16 05:37:06 visual_prompt]: Inference (test):avg data time: 8.98e-03, avg batch time: 0.1928, average loss: 1.1547
[09/16 05:37:06 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.48	top5: 91.76	
[09/16 05:37:06 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 05:37:15 visual_prompt]: Epoch 32 / 100: avg data time: 9.30e-02, avg batch time: 0.4957, average train loss: 0.0056
[09/16 05:37:18 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1426, average loss: 0.0046
[09/16 05:37:18 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:37:39 visual_prompt]: 	Test 100/157. loss: 0.834, 0.1820 s / batch. (data: 1.15e-04)max mem: 17.22530 GB 
[09/16 05:37:50 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1923, average loss: 1.1248
[09/16 05:37:50 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.46	top5: 92.14	
[09/16 05:37:50 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 05:37:59 visual_prompt]: Epoch 33 / 100: avg data time: 8.68e-02, avg batch time: 0.4912, average train loss: 0.0080
[09/16 05:38:02 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1426, average loss: 0.0070
[09/16 05:38:02 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:38:23 visual_prompt]: 	Test 100/157. loss: 0.835, 0.2040 s / batch. (data: 1.15e-04)max mem: 17.22530 GB 
[09/16 05:38:35 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1925, average loss: 1.1643
[09/16 05:38:35 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.51	top5: 91.66	
[09/16 05:38:35 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 05:38:44 visual_prompt]: Epoch 34 / 100: avg data time: 1.04e-01, avg batch time: 0.5079, average train loss: 0.0077
[09/16 05:38:47 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1428, average loss: 0.0065
[09/16 05:38:47 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:39:08 visual_prompt]: 	Test 100/157. loss: 0.876, 0.2206 s / batch. (data: 1.52e-02)max mem: 17.22530 GB 
[09/16 05:39:19 visual_prompt]: Inference (test):avg data time: 6.20e-03, avg batch time: 0.1921, average loss: 1.1053
[09/16 05:39:19 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.14	top5: 92.09	
[09/16 05:39:19 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 05:39:28 visual_prompt]: Epoch 35 / 100: avg data time: 1.07e-01, avg batch time: 0.5096, average train loss: 0.0061
[09/16 05:39:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1424, average loss: 0.0057
[09/16 05:39:31 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:39:52 visual_prompt]: 	Test 100/157. loss: 0.863, 0.1958 s / batch. (data: 1.40e-02)max mem: 17.22530 GB 
[09/16 05:40:04 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1919, average loss: 1.0527
[09/16 05:40:04 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.03	top5: 92.51	
[09/16 05:40:04 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 05:40:13 visual_prompt]: Epoch 36 / 100: avg data time: 1.06e-01, avg batch time: 0.5072, average train loss: 0.0060
[09/16 05:40:16 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1425, average loss: 0.0059
[09/16 05:40:16 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:40:37 visual_prompt]: 	Test 100/157. loss: 0.818, 0.1948 s / batch. (data: 1.36e-04)max mem: 17.22530 GB 
[09/16 05:40:49 visual_prompt]: Inference (test):avg data time: 6.49e-03, avg batch time: 0.1914, average loss: 1.0141
[09/16 05:40:49 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.89	top5: 92.84	
[09/16 05:40:49 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 05:40:58 visual_prompt]: Epoch 37 / 100: avg data time: 1.08e-01, avg batch time: 0.5130, average train loss: 0.0063
[09/16 05:41:01 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1426, average loss: 0.0061
[09/16 05:41:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:41:22 visual_prompt]: 	Test 100/157. loss: 0.832, 0.1828 s / batch. (data: 1.41e-04)max mem: 17.22530 GB 
[09/16 05:41:34 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1944, average loss: 0.9961
[09/16 05:41:34 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.37	top5: 93.12	
[09/16 05:41:34 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 05:41:43 visual_prompt]: Epoch 38 / 100: avg data time: 8.88e-02, avg batch time: 0.4926, average train loss: 0.0067
[09/16 05:41:45 visual_prompt]: Inference (val):avg data time: 4.96e-05, avg batch time: 0.1428, average loss: 0.0065
[09/16 05:41:45 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:42:06 visual_prompt]: 	Test 100/157. loss: 0.808, 0.1824 s / batch. (data: 1.51e-04)max mem: 17.22530 GB 
[09/16 05:42:18 visual_prompt]: Inference (test):avg data time: 6.64e-03, avg batch time: 0.1915, average loss: 0.9814
[09/16 05:42:18 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.81	top5: 93.37	
[09/16 05:42:18 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 05:42:27 visual_prompt]: Epoch 39 / 100: avg data time: 1.02e-01, avg batch time: 0.5082, average train loss: 0.0072
[09/16 05:42:30 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1428, average loss: 0.0066
[09/16 05:42:30 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:42:51 visual_prompt]: 	Test 100/157. loss: 0.805, 0.2050 s / batch. (data: 2.30e-02)max mem: 17.22530 GB 
[09/16 05:43:03 visual_prompt]: Inference (test):avg data time: 8.39e-03, avg batch time: 0.1926, average loss: 0.9605
[09/16 05:43:03 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.13	top5: 93.69	
[09/16 05:43:03 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 05:43:11 visual_prompt]: Epoch 40 / 100: avg data time: 9.42e-02, avg batch time: 0.4952, average train loss: 0.0074
[09/16 05:43:14 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1427, average loss: 0.0069
[09/16 05:43:14 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:43:35 visual_prompt]: 	Test 100/157. loss: 0.814, 0.1827 s / batch. (data: 9.82e-05)max mem: 17.22530 GB 
[09/16 05:43:47 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1913, average loss: 0.9455
[09/16 05:43:47 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.31	top5: 93.92	
[09/16 05:43:47 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 05:43:56 visual_prompt]: Epoch 41 / 100: avg data time: 9.02e-02, avg batch time: 0.4951, average train loss: 0.0074
[09/16 05:43:59 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1425, average loss: 0.0067
[09/16 05:43:59 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:44:20 visual_prompt]: 	Test 100/157. loss: 0.838, 0.1826 s / batch. (data: 1.21e-04)max mem: 17.22530 GB 
[09/16 05:44:31 visual_prompt]: Inference (test):avg data time: 6.73e-03, avg batch time: 0.1911, average loss: 0.9390
[09/16 05:44:32 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.29	top5: 94.12	
[09/16 05:44:32 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 05:44:40 visual_prompt]: Epoch 42 / 100: avg data time: 8.95e-02, avg batch time: 0.4927, average train loss: 0.0073
[09/16 05:44:43 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1428, average loss: 0.0064
[09/16 05:44:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:45:04 visual_prompt]: 	Test 100/157. loss: 0.805, 0.2012 s / batch. (data: 1.26e-02)max mem: 17.22530 GB 
[09/16 05:45:16 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1925, average loss: 0.9232
[09/16 05:45:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.44	top5: 94.50	
[09/16 05:45:16 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 05:45:25 visual_prompt]: Epoch 43 / 100: avg data time: 1.00e-01, avg batch time: 0.5013, average train loss: 0.0070
[09/16 05:45:28 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1426, average loss: 0.0059
[09/16 05:45:28 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:45:49 visual_prompt]: 	Test 100/157. loss: 0.821, 0.2069 s / batch. (data: 1.04e-02)max mem: 17.22530 GB 
[09/16 05:46:01 visual_prompt]: Inference (test):avg data time: 6.31e-03, avg batch time: 0.1942, average loss: 0.9171
[09/16 05:46:01 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.62	top5: 94.64	
[09/16 05:46:01 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 05:46:10 visual_prompt]: Epoch 44 / 100: avg data time: 9.74e-02, avg batch time: 0.5001, average train loss: 0.0068
[09/16 05:46:13 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1425, average loss: 0.0059
[09/16 05:46:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:46:34 visual_prompt]: 	Test 100/157. loss: 0.804, 0.2034 s / batch. (data: 1.40e-02)max mem: 17.22530 GB 
[09/16 05:46:45 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1928, average loss: 0.9059
[09/16 05:46:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.99	top5: 94.71	
[09/16 05:46:45 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 05:46:54 visual_prompt]: Epoch 45 / 100: avg data time: 1.02e-01, avg batch time: 0.5046, average train loss: 0.0067
[09/16 05:46:57 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1427, average loss: 0.0059
[09/16 05:46:57 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:47:18 visual_prompt]: 	Test 100/157. loss: 0.842, 0.1966 s / batch. (data: 1.45e-02)max mem: 17.22530 GB 
[09/16 05:47:30 visual_prompt]: Inference (test):avg data time: 8.40e-03, avg batch time: 0.1923, average loss: 0.9056
[09/16 05:47:30 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.85	top5: 94.84	
[09/16 05:47:30 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 05:47:39 visual_prompt]: Epoch 46 / 100: avg data time: 9.62e-02, avg batch time: 0.4993, average train loss: 0.0066
[09/16 05:47:42 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1428, average loss: 0.0059
[09/16 05:47:42 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:48:03 visual_prompt]: 	Test 100/157. loss: 0.806, 0.1823 s / batch. (data: 8.51e-05)max mem: 17.22530 GB 
[09/16 05:48:15 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1952, average loss: 0.9041
[09/16 05:48:15 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.71	top5: 95.04	
[09/16 05:48:15 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 05:48:24 visual_prompt]: Epoch 47 / 100: avg data time: 1.01e-01, avg batch time: 0.5031, average train loss: 0.0064
[09/16 05:48:27 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1427, average loss: 0.0055
[09/16 05:48:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:48:49 visual_prompt]: 	Test 100/157. loss: 0.814, 0.2144 s / batch. (data: 1.56e-02)max mem: 17.22530 GB 
[09/16 05:49:01 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1986, average loss: 0.9001
[09/16 05:49:01 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.68	top5: 95.19	
[09/16 05:49:01 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 05:49:10 visual_prompt]: Epoch 48 / 100: avg data time: 1.03e-01, avg batch time: 0.5055, average train loss: 0.0063
[09/16 05:49:13 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1464, average loss: 0.0058
[09/16 05:49:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:49:34 visual_prompt]: 	Test 100/157. loss: 0.834, 0.1951 s / batch. (data: 4.03e-05)max mem: 17.22530 GB 
[09/16 05:49:45 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1919, average loss: 0.9153
[09/16 05:49:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.31	top5: 95.06	
[09/16 05:49:45 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 05:49:54 visual_prompt]: Epoch 49 / 100: avg data time: 9.69e-02, avg batch time: 0.4975, average train loss: 0.0062
[09/16 05:49:57 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1424, average loss: 0.0051
[09/16 05:49:57 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:50:18 visual_prompt]: 	Test 100/157. loss: 0.791, 0.1832 s / batch. (data: 1.46e-04)max mem: 17.22530 GB 
[09/16 05:50:29 visual_prompt]: Inference (test):avg data time: 6.99e-03, avg batch time: 0.1912, average loss: 0.9003
[09/16 05:50:30 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.61	top5: 95.26	
[09/16 05:50:30 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 05:50:38 visual_prompt]: Epoch 50 / 100: avg data time: 9.67e-02, avg batch time: 0.4973, average train loss: 0.0059
[09/16 05:50:41 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1427, average loss: 0.0050
[09/16 05:50:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:51:02 visual_prompt]: 	Test 100/157. loss: 0.844, 0.1978 s / batch. (data: 1.61e-02)max mem: 17.22530 GB 
[09/16 05:51:14 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1921, average loss: 0.9047
[09/16 05:51:14 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.35	top5: 95.19	
[09/16 05:51:14 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 05:51:23 visual_prompt]: Epoch 51 / 100: avg data time: 1.12e-01, avg batch time: 0.5123, average train loss: 0.0057
[09/16 05:51:26 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1426, average loss: 0.0049
[09/16 05:51:26 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:51:47 visual_prompt]: 	Test 100/157. loss: 0.821, 0.1976 s / batch. (data: 1.55e-02)max mem: 17.22530 GB 
[09/16 05:51:59 visual_prompt]: Inference (test):avg data time: 7.01e-03, avg batch time: 0.1915, average loss: 0.9282
[09/16 05:51:59 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.27	top5: 95.38	
[09/16 05:51:59 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 05:52:08 visual_prompt]: Epoch 52 / 100: avg data time: 9.95e-02, avg batch time: 0.5007, average train loss: 0.0056
[09/16 05:52:11 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1427, average loss: 0.0048
[09/16 05:52:11 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:52:32 visual_prompt]: 	Test 100/157. loss: 0.776, 0.1826 s / batch. (data: 1.21e-04)max mem: 17.22530 GB 
[09/16 05:52:43 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1936, average loss: 0.9058
[09/16 05:52:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.48	top5: 95.26	
[09/16 05:52:44 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 05:52:53 visual_prompt]: Epoch 53 / 100: avg data time: 1.06e-01, avg batch time: 0.5084, average train loss: 0.0055
[09/16 05:52:55 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1427, average loss: 0.0045
[09/16 05:52:55 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:53:16 visual_prompt]: 	Test 100/157. loss: 0.817, 0.1838 s / batch. (data: 1.11e-04)max mem: 17.22530 GB 
[09/16 05:53:29 visual_prompt]: Inference (test):avg data time: 7.12e-03, avg batch time: 0.1968, average loss: 0.9163
[09/16 05:53:29 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.27	top5: 95.31	
[09/16 05:53:29 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 05:53:38 visual_prompt]: Epoch 54 / 100: avg data time: 9.36e-02, avg batch time: 0.4963, average train loss: 0.0052
[09/16 05:53:40 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1441, average loss: 0.0043
[09/16 05:53:40 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:54:01 visual_prompt]: 	Test 100/157. loss: 0.804, 0.1964 s / batch. (data: 1.33e-02)max mem: 17.22530 GB 
[09/16 05:54:13 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1930, average loss: 0.9135
[09/16 05:54:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 76.33	top5: 95.39	
[09/16 05:54:13 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 05:54:22 visual_prompt]: Epoch 55 / 100: avg data time: 1.03e-01, avg batch time: 0.5031, average train loss: 0.0050
[09/16 05:54:25 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1428, average loss: 0.0043
[09/16 05:54:25 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:54:46 visual_prompt]: 	Test 100/157. loss: 0.802, 0.1831 s / batch. (data: 1.50e-04)max mem: 17.22530 GB 
[09/16 05:54:58 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1918, average loss: 0.9428
[09/16 05:54:58 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.76	top5: 95.37	
[09/16 05:54:58 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 05:55:07 visual_prompt]: Epoch 56 / 100: avg data time: 1.06e-01, avg batch time: 0.5083, average train loss: 0.0048
[09/16 05:55:10 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1430, average loss: 0.0041
[09/16 05:55:10 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:55:31 visual_prompt]: 	Test 100/157. loss: 0.810, 0.1953 s / batch. (data: 1.35e-02)max mem: 17.22530 GB 
[09/16 05:55:42 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1922, average loss: 0.9340
[09/16 05:55:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.77	top5: 95.35	
[09/16 05:55:42 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 05:55:52 visual_prompt]: Epoch 57 / 100: avg data time: 1.03e-01, avg batch time: 0.5212, average train loss: 0.0046
[09/16 05:55:54 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1429, average loss: 0.0036
[09/16 05:55:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:56:15 visual_prompt]: 	Test 100/157. loss: 0.787, 0.1959 s / batch. (data: 1.35e-02)max mem: 17.22530 GB 
[09/16 05:56:27 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1923, average loss: 0.9384
[09/16 05:56:27 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.72	top5: 95.32	
[09/16 05:56:27 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 05:56:36 visual_prompt]: Epoch 58 / 100: avg data time: 9.99e-02, avg batch time: 0.5001, average train loss: 0.0045
[09/16 05:56:39 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1428, average loss: 0.0038
[09/16 05:56:39 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:57:00 visual_prompt]: 	Test 100/157. loss: 0.783, 0.1964 s / batch. (data: 4.43e-05)max mem: 17.22530 GB 
[09/16 05:57:12 visual_prompt]: Inference (test):avg data time: 6.75e-03, avg batch time: 0.1909, average loss: 0.9285
[09/16 05:57:12 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.96	top5: 95.39	
[09/16 05:57:12 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 05:57:20 visual_prompt]: Epoch 59 / 100: avg data time: 8.97e-02, avg batch time: 0.4911, average train loss: 0.0044
[09/16 05:57:23 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1427, average loss: 0.0040
[09/16 05:57:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:57:44 visual_prompt]: 	Test 100/157. loss: 0.759, 0.1973 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 05:57:56 visual_prompt]: Inference (test):avg data time: 6.70e-03, avg batch time: 0.1927, average loss: 0.9641
[09/16 05:57:56 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.27	top5: 95.14	
[09/16 05:57:56 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 05:58:05 visual_prompt]: Epoch 60 / 100: avg data time: 9.66e-02, avg batch time: 0.4976, average train loss: 0.0047
[09/16 05:58:08 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1427, average loss: 0.0039
[09/16 05:58:08 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:58:29 visual_prompt]: 	Test 100/157. loss: 0.827, 0.1821 s / batch. (data: 1.47e-04)max mem: 17.22530 GB 
[09/16 05:58:40 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1921, average loss: 0.9532
[09/16 05:58:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.33	top5: 95.32	
[09/16 05:58:40 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 05:58:49 visual_prompt]: Epoch 61 / 100: avg data time: 9.56e-02, avg batch time: 0.4987, average train loss: 0.0046
[09/16 05:58:52 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1429, average loss: 0.0037
[09/16 05:58:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:59:13 visual_prompt]: 	Test 100/157. loss: 0.753, 0.1944 s / batch. (data: 1.34e-04)max mem: 17.22530 GB 
[09/16 05:59:25 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1917, average loss: 0.9634
[09/16 05:59:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 75.47	top5: 95.18	
[09/16 05:59:25 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 05:59:34 visual_prompt]: Epoch 62 / 100: avg data time: 1.00e-01, avg batch time: 0.5013, average train loss: 0.0043
[09/16 05:59:37 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1427, average loss: 0.0039
[09/16 05:59:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 05:59:58 visual_prompt]: 	Test 100/157. loss: 0.860, 0.1951 s / batch. (data: 2.45e-04)max mem: 17.22530 GB 
[09/16 06:00:09 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1918, average loss: 0.9875
[09/16 06:00:10 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.75	top5: 95.00	
[09/16 06:00:10 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 06:00:18 visual_prompt]: Epoch 63 / 100: avg data time: 9.96e-02, avg batch time: 0.4989, average train loss: 0.0074
[09/16 06:00:21 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1428, average loss: 0.1404
[09/16 06:00:21 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.50	top5: 100.00	
[09/16 06:00:42 visual_prompt]: 	Test 100/157. loss: 1.063, 0.2037 s / batch. (data: 2.17e-02)max mem: 17.22530 GB 
[09/16 06:00:54 visual_prompt]: Inference (test):avg data time: 7.06e-03, avg batch time: 0.1915, average loss: 1.1980
[09/16 06:00:54 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 70.32	top5: 93.06	
[09/16 06:00:54 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 06:01:03 visual_prompt]: Epoch 64 / 100: avg data time: 1.05e-01, avg batch time: 0.5063, average train loss: 4.1026
[09/16 06:01:06 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1428, average loss: 4.6122
[09/16 06:01:06 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 2.00	top5: 7.50	
[09/16 06:01:27 visual_prompt]: 	Test 100/157. loss: 4.623, 0.1884 s / batch. (data: 6.03e-03)max mem: 17.22530 GB 
[09/16 06:01:39 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1937, average loss: 4.7189
[09/16 06:01:39 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.00	
[09/16 06:01:39 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 06:01:47 visual_prompt]: Epoch 65 / 100: avg data time: 9.00e-02, avg batch time: 0.4935, average train loss: 4.7971
[09/16 06:01:50 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1427, average loss: 4.6837
[09/16 06:01:50 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.00	top5: 6.50	
[09/16 06:02:11 visual_prompt]: 	Test 100/157. loss: 4.741, 0.1981 s / batch. (data: 1.21e-04)max mem: 17.22530 GB 
[09/16 06:02:23 visual_prompt]: Inference (test):avg data time: 6.30e-03, avg batch time: 0.1906, average loss: 4.7945
[09/16 06:02:23 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.00	
[09/16 06:02:23 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 06:02:32 visual_prompt]: Epoch 66 / 100: avg data time: 8.97e-02, avg batch time: 0.4927, average train loss: 4.8897
[09/16 06:02:35 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1426, average loss: 4.7707
[09/16 06:02:35 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 0.50	top5: 3.50	
[09/16 06:02:56 visual_prompt]: 	Test 100/157. loss: 4.781, 0.1960 s / batch. (data: 1.41e-02)max mem: 17.22530 GB 
[09/16 06:03:08 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1922, average loss: 4.8561
[09/16 06:03:08 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.00	
[09/16 06:03:08 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 06:03:16 visual_prompt]: Epoch 67 / 100: avg data time: 9.79e-02, avg batch time: 0.4988, average train loss: 4.8523
[09/16 06:03:19 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1426, average loss: 4.6782
[09/16 06:03:19 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 0.50	top5: 6.00	
[09/16 06:03:40 visual_prompt]: 	Test 100/157. loss: 4.684, 0.2022 s / batch. (data: 1.31e-02)max mem: 17.22530 GB 
[09/16 06:03:52 visual_prompt]: Inference (test):avg data time: 6.18e-03, avg batch time: 0.1918, average loss: 4.7628
[09/16 06:03:52 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.31	
[09/16 06:03:52 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 06:04:01 visual_prompt]: Epoch 68 / 100: avg data time: 8.57e-02, avg batch time: 0.4869, average train loss: 4.8233
[09/16 06:04:04 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1426, average loss: 4.6706
[09/16 06:04:04 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 7.50	
[09/16 06:04:24 visual_prompt]: 	Test 100/157. loss: 4.883, 0.1960 s / batch. (data: 1.13e-02)max mem: 17.22530 GB 
[09/16 06:04:36 visual_prompt]: Inference (test):avg data time: 6.30e-03, avg batch time: 0.1925, average loss: 4.7865
[09/16 06:04:36 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.42	
[09/16 06:04:36 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 06:04:45 visual_prompt]: Epoch 69 / 100: avg data time: 1.01e-01, avg batch time: 0.5011, average train loss: 4.8050
[09/16 06:04:48 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1425, average loss: 4.6521
[09/16 06:04:48 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.00	top5: 8.00	
[09/16 06:05:09 visual_prompt]: 	Test 100/157. loss: 4.717, 0.2069 s / batch. (data: 2.04e-02)max mem: 17.22530 GB 
[09/16 06:05:21 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1920, average loss: 4.7636
[09/16 06:05:21 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.14	top5: 5.09	
[09/16 06:05:21 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 06:05:30 visual_prompt]: Epoch 70 / 100: avg data time: 9.19e-02, avg batch time: 0.4971, average train loss: 4.7378
[09/16 06:05:32 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1427, average loss: 4.6072
[09/16 06:05:32 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 7.50	
[09/16 06:05:53 visual_prompt]: 	Test 100/157. loss: 4.720, 0.1829 s / batch. (data: 1.12e-04)max mem: 17.22530 GB 
[09/16 06:06:05 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1920, average loss: 4.6928
[09/16 06:06:05 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.47	top5: 5.97	
[09/16 06:06:05 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 06:06:14 visual_prompt]: Epoch 71 / 100: avg data time: 9.24e-02, avg batch time: 0.4938, average train loss: 4.6845
[09/16 06:06:17 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1425, average loss: 4.5809
[09/16 06:06:17 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 7.00	
[09/16 06:06:38 visual_prompt]: 	Test 100/157. loss: 4.644, 0.1971 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 06:06:49 visual_prompt]: Inference (test):avg data time: 6.33e-03, avg batch time: 0.1908, average loss: 4.7005
[09/16 06:06:50 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.81	top5: 5.35	
[09/16 06:06:50 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 06:06:58 visual_prompt]: Epoch 72 / 100: avg data time: 1.01e-01, avg batch time: 0.5048, average train loss: 4.6636
[09/16 06:07:01 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1426, average loss: 4.5108
[09/16 06:07:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 3.00	top5: 13.00	
[09/16 06:07:22 visual_prompt]: 	Test 100/157. loss: 4.627, 0.1820 s / batch. (data: 3.60e-05)max mem: 17.22530 GB 
[09/16 06:07:34 visual_prompt]: Inference (test):avg data time: 6.92e-03, avg batch time: 0.1920, average loss: 4.6201
[09/16 06:07:34 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.76	top5: 8.40	
[09/16 06:07:34 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 06:07:43 visual_prompt]: Epoch 73 / 100: avg data time: 9.68e-02, avg batch time: 0.5296, average train loss: 4.5895
[09/16 06:07:46 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1424, average loss: 4.4671
[09/16 06:07:46 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 3.00	top5: 11.50	
[09/16 06:08:07 visual_prompt]: 	Test 100/157. loss: 4.462, 0.2059 s / batch. (data: 1.35e-02)max mem: 17.22530 GB 
[09/16 06:08:19 visual_prompt]: Inference (test):avg data time: 8.45e-03, avg batch time: 0.1923, average loss: 4.6701
[09/16 06:08:19 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.86	top5: 10.13	
[09/16 06:08:19 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 06:08:28 visual_prompt]: Epoch 74 / 100: avg data time: 9.96e-02, avg batch time: 0.5018, average train loss: 4.5141
[09/16 06:08:31 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1422, average loss: 4.3612
[09/16 06:08:31 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 3.50	top5: 15.50	
[09/16 06:08:52 visual_prompt]: 	Test 100/157. loss: 4.460, 0.1866 s / batch. (data: 1.08e-04)max mem: 17.22530 GB 
[09/16 06:09:04 visual_prompt]: Inference (test):avg data time: 8.29e-03, avg batch time: 0.1928, average loss: 4.5617
[09/16 06:09:04 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 2.28	top5: 9.97	
[09/16 06:09:04 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 06:09:12 visual_prompt]: Epoch 75 / 100: avg data time: 8.70e-02, avg batch time: 0.4890, average train loss: 4.4630
[09/16 06:09:15 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1426, average loss: 4.3911
[09/16 06:09:15 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.00	top5: 11.00	
[09/16 06:09:37 visual_prompt]: 	Test 100/157. loss: 4.528, 0.1890 s / batch. (data: 1.57e-04)max mem: 17.22530 GB 
[09/16 06:09:49 visual_prompt]: Inference (test):avg data time: 6.87e-03, avg batch time: 0.1947, average loss: 4.5423
[09/16 06:09:49 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 2.13	top5: 9.99	
[09/16 06:09:49 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 06:09:57 visual_prompt]: Epoch 76 / 100: avg data time: 9.95e-02, avg batch time: 0.5003, average train loss: 4.3846
[09/16 06:10:00 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1426, average loss: 4.2249
[09/16 06:10:00 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 5.50	top5: 17.50	
[09/16 06:10:21 visual_prompt]: 	Test 100/157. loss: 4.388, 0.2160 s / batch. (data: 3.45e-02)max mem: 17.22530 GB 
[09/16 06:10:33 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1928, average loss: 4.4659
[09/16 06:10:33 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 3.06	top5: 12.58	
[09/16 06:10:33 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 06:10:42 visual_prompt]: Epoch 77 / 100: avg data time: 8.68e-02, avg batch time: 0.4882, average train loss: 4.2603
[09/16 06:10:45 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1427, average loss: 4.3342
[09/16 06:10:45 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 3.00	top5: 14.00	
[09/16 06:11:06 visual_prompt]: 	Test 100/157. loss: 4.500, 0.1827 s / batch. (data: 1.44e-04)max mem: 17.22530 GB 
[09/16 06:11:18 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1915, average loss: 4.5194
[09/16 06:11:18 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 2.35	top5: 10.46	
[09/16 06:11:18 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 06:11:27 visual_prompt]: Epoch 78 / 100: avg data time: 9.49e-02, avg batch time: 0.5073, average train loss: 4.3128
[09/16 06:11:29 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1426, average loss: 4.1320
[09/16 06:11:29 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 8.00	top5: 22.50	
[09/16 06:11:50 visual_prompt]: 	Test 100/157. loss: 4.313, 0.2112 s / batch. (data: 1.41e-02)max mem: 17.22530 GB 
[09/16 06:12:02 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1913, average loss: 4.3889
[09/16 06:12:02 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 3.71	top5: 16.19	
[09/16 06:12:02 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 06:12:11 visual_prompt]: Epoch 79 / 100: avg data time: 9.39e-02, avg batch time: 0.4952, average train loss: 4.0641
[09/16 06:12:14 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1426, average loss: 3.8928
[09/16 06:12:14 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 9.00	top5: 31.50	
[09/16 06:12:35 visual_prompt]: 	Test 100/157. loss: 4.140, 0.2007 s / batch. (data: 1.60e-02)max mem: 17.22530 GB 
[09/16 06:12:47 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1945, average loss: 4.3017
[09/16 06:12:47 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 5.62	top5: 19.16	
[09/16 06:12:47 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 06:12:56 visual_prompt]: Epoch 80 / 100: avg data time: 9.66e-02, avg batch time: 0.4986, average train loss: 3.7493
[09/16 06:12:59 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1427, average loss: 3.7095
[09/16 06:12:59 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 14.50	top5: 43.00	
[09/16 06:13:20 visual_prompt]: 	Test 100/157. loss: 4.010, 0.2018 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 06:13:31 visual_prompt]: Inference (test):avg data time: 5.78e-03, avg batch time: 0.1918, average loss: 4.0829
[09/16 06:13:31 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 8.69	top5: 25.32	
[09/16 06:13:31 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 06:13:40 visual_prompt]: Epoch 81 / 100: avg data time: 1.02e-01, avg batch time: 0.5017, average train loss: 3.2358
[09/16 06:13:43 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1427, average loss: 2.7361
[09/16 06:13:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 37.50	top5: 65.00	
[09/16 06:14:04 visual_prompt]: 	Test 100/157. loss: 3.370, 0.1820 s / batch. (data: 1.06e-04)max mem: 17.22530 GB 
[09/16 06:14:16 visual_prompt]: Inference (test):avg data time: 8.82e-03, avg batch time: 0.1922, average loss: 3.6234
[09/16 06:14:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 14.97	top5: 38.27	
[09/16 06:14:16 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 06:14:25 visual_prompt]: Epoch 82 / 100: avg data time: 9.26e-02, avg batch time: 0.4950, average train loss: 2.5579
[09/16 06:14:28 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.1430, average loss: 1.7676
[09/16 06:14:28 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 53.50	top5: 85.50	
[09/16 06:14:49 visual_prompt]: 	Test 100/157. loss: 2.519, 0.1824 s / batch. (data: 1.38e-04)max mem: 17.22530 GB 
[09/16 06:15:01 visual_prompt]: Inference (test):avg data time: 6.54e-03, avg batch time: 0.1925, average loss: 2.9958
[09/16 06:15:01 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 28.61	top5: 56.59	
[09/16 06:15:01 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 06:15:10 visual_prompt]: Epoch 83 / 100: avg data time: 1.09e-01, avg batch time: 0.5107, average train loss: 1.4440
[09/16 06:15:13 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1426, average loss: 0.8089
[09/16 06:15:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 84.50	top5: 96.00	
[09/16 06:15:34 visual_prompt]: 	Test 100/157. loss: 1.940, 0.1830 s / batch. (data: 1.23e-04)max mem: 17.22530 GB 
[09/16 06:15:46 visual_prompt]: Inference (test):avg data time: 8.45e-03, avg batch time: 0.1935, average loss: 2.2709
[09/16 06:15:46 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 42.55	top5: 72.95	
[09/16 06:15:46 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 06:15:55 visual_prompt]: Epoch 84 / 100: avg data time: 1.04e-01, avg batch time: 0.5058, average train loss: 0.6726
[09/16 06:15:57 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1426, average loss: 0.3536
[09/16 06:15:57 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.50	top5: 99.50	
[09/16 06:16:18 visual_prompt]: 	Test 100/157. loss: 1.542, 0.2145 s / batch. (data: 3.30e-02)max mem: 17.22530 GB 
[09/16 06:16:30 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1922, average loss: 1.8865
[09/16 06:16:30 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 50.60	top5: 79.96	
[09/16 06:16:30 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 06:16:39 visual_prompt]: Epoch 85 / 100: avg data time: 8.83e-02, avg batch time: 0.4954, average train loss: 0.3130
[09/16 06:16:42 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1425, average loss: 0.1744
[09/16 06:16:42 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.00	top5: 100.00	
[09/16 06:17:03 visual_prompt]: 	Test 100/157. loss: 1.495, 0.1946 s / batch. (data: 1.25e-02)max mem: 17.22530 GB 
[09/16 06:17:15 visual_prompt]: Inference (test):avg data time: 8.94e-03, avg batch time: 0.1937, average loss: 1.7596
[09/16 06:17:15 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 53.31	top5: 82.16	
[09/16 06:17:15 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 06:17:24 visual_prompt]: Epoch 86 / 100: avg data time: 9.63e-02, avg batch time: 0.4966, average train loss: 0.1558
[09/16 06:17:27 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1426, average loss: 0.0945
[09/16 06:17:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 100.00	
[09/16 06:17:48 visual_prompt]: 	Test 100/157. loss: 1.423, 0.2009 s / batch. (data: 1.23e-02)max mem: 17.22530 GB 
[09/16 06:18:00 visual_prompt]: Inference (test):avg data time: 8.55e-03, avg batch time: 0.1925, average loss: 1.6294
[09/16 06:18:00 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 56.44	top5: 84.30	
[09/16 06:18:00 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 06:18:09 visual_prompt]: Epoch 87 / 100: avg data time: 9.96e-02, avg batch time: 0.5008, average train loss: 0.1002
[09/16 06:18:11 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1429, average loss: 0.0717
[09/16 06:18:11 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:18:32 visual_prompt]: 	Test 100/157. loss: 1.435, 0.1826 s / batch. (data: 4.60e-05)max mem: 17.22530 GB 
[09/16 06:18:44 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1924, average loss: 1.6072
[09/16 06:18:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.21	top5: 84.69	
[09/16 06:18:44 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 06:18:54 visual_prompt]: Epoch 88 / 100: avg data time: 9.22e-02, avg batch time: 0.5304, average train loss: 0.0706
[09/16 06:18:56 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1426, average loss: 0.0558
[09/16 06:18:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:19:17 visual_prompt]: 	Test 100/157. loss: 1.398, 0.1971 s / batch. (data: 1.54e-02)max mem: 17.22530 GB 
[09/16 06:19:30 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1964, average loss: 1.5811
[09/16 06:19:30 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.71	top5: 85.17	
[09/16 06:19:30 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 06:19:39 visual_prompt]: Epoch 89 / 100: avg data time: 1.05e-01, avg batch time: 0.5066, average train loss: 0.0555
[09/16 06:19:42 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1427, average loss: 0.0448
[09/16 06:19:42 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:20:03 visual_prompt]: 	Test 100/157. loss: 1.350, 0.1879 s / batch. (data: 5.25e-03)max mem: 17.22530 GB 
[09/16 06:20:15 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1939, average loss: 1.5754
[09/16 06:20:15 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.64	top5: 85.25	
[09/16 06:20:15 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 06:20:24 visual_prompt]: Epoch 90 / 100: avg data time: 1.05e-01, avg batch time: 0.5062, average train loss: 0.0476
[09/16 06:20:27 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1425, average loss: 0.0404
[09/16 06:20:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:20:48 visual_prompt]: 	Test 100/157. loss: 1.354, 0.1933 s / batch. (data: 1.11e-02)max mem: 17.22530 GB 
[09/16 06:20:59 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1921, average loss: 1.5587
[09/16 06:20:59 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.02	top5: 85.50	
[09/16 06:20:59 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 06:21:08 visual_prompt]: Epoch 91 / 100: avg data time: 9.48e-02, avg batch time: 0.4977, average train loss: 0.0440
[09/16 06:21:11 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1436, average loss: 0.0364
[09/16 06:21:11 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:21:32 visual_prompt]: 	Test 100/157. loss: 1.360, 0.1834 s / batch. (data: 1.14e-04)max mem: 17.22530 GB 
[09/16 06:21:44 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1915, average loss: 1.5504
[09/16 06:21:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.35	top5: 85.66	
[09/16 06:21:44 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 06:21:53 visual_prompt]: Epoch 92 / 100: avg data time: 9.94e-02, avg batch time: 0.5025, average train loss: 0.0391
[09/16 06:21:56 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1428, average loss: 0.0340
[09/16 06:21:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:22:17 visual_prompt]: 	Test 100/157. loss: 1.346, 0.1949 s / batch. (data: 1.30e-02)max mem: 17.22530 GB 
[09/16 06:22:29 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1936, average loss: 1.5414
[09/16 06:22:29 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.52	top5: 85.84	
[09/16 06:22:29 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 06:22:38 visual_prompt]: Epoch 93 / 100: avg data time: 9.57e-02, avg batch time: 0.5000, average train loss: 0.0369
[09/16 06:22:41 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1433, average loss: 0.0323
[09/16 06:22:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:23:02 visual_prompt]: 	Test 100/157. loss: 1.344, 0.1830 s / batch. (data: 9.99e-05)max mem: 17.22530 GB 
[09/16 06:23:14 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1961, average loss: 1.5394
[09/16 06:23:14 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.53	top5: 85.89	
[09/16 06:23:14 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 06:23:23 visual_prompt]: Epoch 94 / 100: avg data time: 1.08e-01, avg batch time: 0.5080, average train loss: 0.0351
[09/16 06:23:26 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1427, average loss: 0.0311
[09/16 06:23:26 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:23:47 visual_prompt]: 	Test 100/157. loss: 1.334, 0.1827 s / batch. (data: 1.25e-04)max mem: 17.22530 GB 
[09/16 06:23:59 visual_prompt]: Inference (test):avg data time: 8.78e-03, avg batch time: 0.1934, average loss: 1.5359
[09/16 06:23:59 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.68	top5: 85.97	
[09/16 06:23:59 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 06:24:08 visual_prompt]: Epoch 95 / 100: avg data time: 1.02e-01, avg batch time: 0.5041, average train loss: 0.0349
[09/16 06:24:11 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1428, average loss: 0.0302
[09/16 06:24:11 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:24:32 visual_prompt]: 	Test 100/157. loss: 1.331, 0.1961 s / batch. (data: 1.41e-02)max mem: 17.22530 GB 
[09/16 06:24:43 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1918, average loss: 1.5343
[09/16 06:24:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.82	top5: 85.97	
[09/16 06:24:44 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 06:24:52 visual_prompt]: Epoch 96 / 100: avg data time: 9.57e-02, avg batch time: 0.4999, average train loss: 0.0338
[09/16 06:24:55 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1428, average loss: 0.0299
[09/16 06:24:55 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:25:16 visual_prompt]: 	Test 100/157. loss: 1.336, 0.2192 s / batch. (data: 3.69e-02)max mem: 17.22530 GB 
[09/16 06:25:28 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1926, average loss: 1.5352
[09/16 06:25:28 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.73	top5: 86.02	
[09/16 06:25:28 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 06:25:37 visual_prompt]: Epoch 97 / 100: avg data time: 9.38e-02, avg batch time: 0.4987, average train loss: 0.0332
[09/16 06:25:40 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1427, average loss: 0.0296
[09/16 06:25:40 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:26:01 visual_prompt]: 	Test 100/157. loss: 1.336, 0.1827 s / batch. (data: 1.31e-04)max mem: 17.22530 GB 
[09/16 06:26:13 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1929, average loss: 1.5347
[09/16 06:26:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.70	top5: 86.06	
[09/16 06:26:13 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 06:26:22 visual_prompt]: Epoch 98 / 100: avg data time: 9.93e-02, avg batch time: 0.5010, average train loss: 0.0330
[09/16 06:26:24 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1428, average loss: 0.0293
[09/16 06:26:24 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:26:45 visual_prompt]: 	Test 100/157. loss: 1.337, 0.1829 s / batch. (data: 1.56e-04)max mem: 17.22530 GB 
[09/16 06:26:57 visual_prompt]: Inference (test):avg data time: 6.91e-03, avg batch time: 0.1915, average loss: 1.5341
[09/16 06:26:57 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.68	top5: 86.09	
[09/16 06:26:57 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 06:27:06 visual_prompt]: Epoch 99 / 100: avg data time: 8.69e-02, avg batch time: 0.4896, average train loss: 0.0317
[09/16 06:27:09 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1427, average loss: 0.0293
[09/16 06:27:09 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:27:30 visual_prompt]: 	Test 100/157. loss: 1.337, 0.1828 s / batch. (data: 1.34e-04)max mem: 17.22530 GB 
[09/16 06:27:42 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1917, average loss: 1.5340
[09/16 06:27:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.67	top5: 86.08	
[09/16 06:27:42 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 06:27:51 visual_prompt]: Epoch 100 / 100: avg data time: 9.37e-02, avg batch time: 0.4972, average train loss: 0.0325
[09/16 06:27:53 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1430, average loss: 0.0293
[09/16 06:27:53 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 06:28:14 visual_prompt]: 	Test 100/157. loss: 1.337, 0.1963 s / batch. (data: 1.41e-02)max mem: 17.22530 GB 
[09/16 06:28:26 visual_prompt]: Inference (test):avg data time: 7.11e-03, avg batch time: 0.1919, average loss: 1.5340
[09/16 06:28:26 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.68	top5: 86.09	
[09/16 06:28:33 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 06:28:33 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 06:28:33 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-cifar(num_classes=100)', 'DATA.NUMBER_CLASSES', '100', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed800'], train_type='')
[09/16 06:28:33 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 06:28:33 visual_prompt]: Training with config:
[09/16 06:28:33 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-cifar(num_classes=100)',
          'NO_TEST': False,
          'NUMBER_CLASSES': 100,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed800/vtab-cifar(num_classes=100)/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 06:28:33 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 06:28:33.590541: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 06:28:33.784960: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 06:28:34.681353: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 06:28:34.681441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 06:28:34.681450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 06:28:36.714274: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 06:28:36.714382: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 06:28:36.714395: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 06:28:36 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
2023-09-16 06:28:36.729211: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split train[:800]+train[45000:45200], from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 06:28:38 visual_prompt]: Number of images: 1000
[09/16 06:28:38 visual_prompt]: Number of classes: 100 / 100
[09/16 06:28:38 visual_prompt]: Loading validation data...
[09/16 06:28:38 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split train[45000:45200], from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 06:28:38 visual_prompt]: Number of images: 200
[09/16 06:28:38 visual_prompt]: Number of classes: 90 / 100
[09/16 06:28:38 visual_prompt]: Loading test data...
[09/16 06:28:38 visual_prompt]: Constructing vtab-cifar(num_classes=100) dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/cifar100/3.0.2
[INFO: dataset_builder.py:  510]: Reusing dataset cifar100 (visual_prompt_tuning/data_path/cifar100/3.0.2)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset cifar100 for split test, from visual_prompt_tuning/data_path/cifar100/3.0.2
[09/16 06:28:51 visual_prompt]: Number of images: 10000
[09/16 06:28:51 visual_prompt]: Number of classes: 100 / 100
[09/16 06:28:51 visual_prompt]: Constructing models...
[09/16 06:28:54 visual_prompt]: Total Parameters: 86797156	 Gradient Parameters: 998500
[09/16 06:28:54 visual_prompt]: tuned percent:1.150
[09/16 06:28:56 visual_prompt]: Device used for model: 0
[09/16 06:28:56 visual_prompt]: Setting up Evalutator...
[09/16 06:28:56 visual_prompt]: Setting up Trainer...
[09/16 06:28:56 visual_prompt]: 	Setting up the optimizer...
[09/16 06:28:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 06:29:06 visual_prompt]: Epoch 1 / 100: avg data time: 1.16e-01, avg batch time: 0.5830, average train loss: 4.6471
[09/16 06:29:09 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1423, average loss: 4.6579
[09/16 06:29:09 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 3.00	top5: 6.00	
[09/16 06:29:30 visual_prompt]: 	Test 100/157. loss: 4.635, 0.1966 s / batch. (data: 1.52e-02)max mem: 17.22530 GB 
[09/16 06:29:42 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1918, average loss: 4.6437
[09/16 06:29:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.41	top5: 5.00	
[09/16 06:29:42 visual_prompt]: Best epoch 1: best metric: 0.030
[09/16 06:29:42 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 06:29:52 visual_prompt]: Epoch 2 / 100: avg data time: 1.05e-01, avg batch time: 0.5539, average train loss: 4.6389
[09/16 06:29:54 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1423, average loss: 4.5493
[09/16 06:29:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 7.50	
[09/16 06:30:15 visual_prompt]: 	Test 100/157. loss: 4.683, 0.1827 s / batch. (data: 1.33e-04)max mem: 17.22530 GB 
[09/16 06:30:27 visual_prompt]: Inference (test):avg data time: 6.38e-03, avg batch time: 0.1903, average loss: 4.7010
[09/16 06:30:27 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.41	
[09/16 06:30:27 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 06:30:36 visual_prompt]: Epoch 3 / 100: avg data time: 9.67e-02, avg batch time: 0.4995, average train loss: 4.6705
[09/16 06:30:39 visual_prompt]: Inference (val):avg data time: 1.71e-05, avg batch time: 0.1424, average loss: 4.5833
[09/16 06:30:39 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 0.50	top5: 5.50	
[09/16 06:30:59 visual_prompt]: 	Test 100/157. loss: 4.665, 0.1945 s / batch. (data: 1.05e-04)max mem: 17.22530 GB 
[09/16 06:31:11 visual_prompt]: Inference (test):avg data time: 6.09e-03, avg batch time: 0.1907, average loss: 4.6964
[09/16 06:31:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.54	
[09/16 06:31:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 06:31:20 visual_prompt]: Epoch 4 / 100: avg data time: 9.36e-02, avg batch time: 0.4933, average train loss: 4.7280
[09/16 06:31:23 visual_prompt]: Inference (val):avg data time: 1.79e-05, avg batch time: 0.1426, average loss: 4.6483
[09/16 06:31:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 8.00	
[09/16 06:31:44 visual_prompt]: 	Test 100/157. loss: 4.650, 0.1830 s / batch. (data: 8.32e-05)max mem: 17.22530 GB 
[09/16 06:31:56 visual_prompt]: Inference (test):avg data time: 8.31e-03, avg batch time: 0.1925, average loss: 4.7151
[09/16 06:31:56 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.01	top5: 5.27	
[09/16 06:31:56 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 06:32:05 visual_prompt]: Epoch 5 / 100: avg data time: 9.74e-02, avg batch time: 0.5007, average train loss: 4.7595
[09/16 06:32:07 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1427, average loss: 4.6938
[09/16 06:32:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 1.50	top5: 7.00	
[09/16 06:32:29 visual_prompt]: 	Test 100/157. loss: 4.727, 0.2073 s / batch. (data: 2.58e-02)max mem: 17.22530 GB 
[09/16 06:32:41 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1952, average loss: 4.7776
[09/16 06:32:41 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 6.17	
[09/16 06:32:41 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 06:32:49 visual_prompt]: Epoch 6 / 100: avg data time: 9.07e-02, avg batch time: 0.4933, average train loss: 4.7648
[09/16 06:32:52 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1427, average loss: 4.4835
[09/16 06:32:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 0.50	top5: 13.00	
[09/16 06:33:13 visual_prompt]: 	Test 100/157. loss: 4.681, 0.1988 s / batch. (data: 1.19e-02)max mem: 17.22530 GB 
[09/16 06:33:25 visual_prompt]: Inference (test):avg data time: 6.92e-03, avg batch time: 0.1924, average loss: 4.6248
[09/16 06:33:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 9.38	
[09/16 06:33:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 06:33:34 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e-01, avg batch time: 0.5031, average train loss: 4.5629
[09/16 06:33:37 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1427, average loss: 4.3989
[09/16 06:33:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 4.00	top5: 18.50	
[09/16 06:33:58 visual_prompt]: 	Test 100/157. loss: 4.727, 0.1952 s / batch. (data: 1.34e-02)max mem: 17.22530 GB 
[09/16 06:34:10 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1944, average loss: 4.5984
[09/16 06:34:10 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 3.44	top5: 13.66	
[09/16 06:34:10 visual_prompt]: Best epoch 7: best metric: 0.040
[09/16 06:34:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 06:34:19 visual_prompt]: Epoch 8 / 100: avg data time: 9.78e-02, avg batch time: 0.4997, average train loss: 4.4348
[09/16 06:34:22 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1427, average loss: 4.3308
[09/16 06:34:22 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 5.50	top5: 17.00	
[09/16 06:34:43 visual_prompt]: 	Test 100/157. loss: 4.386, 0.2077 s / batch. (data: 2.58e-02)max mem: 17.22530 GB 
[09/16 06:34:54 visual_prompt]: Inference (test):avg data time: 8.98e-03, avg batch time: 0.1935, average loss: 4.5976
[09/16 06:34:54 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 3.23	top5: 11.83	
[09/16 06:34:54 visual_prompt]: Best epoch 8: best metric: 0.055
[09/16 06:34:54 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 06:35:03 visual_prompt]: Epoch 9 / 100: avg data time: 9.74e-02, avg batch time: 0.4976, average train loss: 4.9289
[09/16 06:35:06 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1429, average loss: 5.1812
[09/16 06:35:06 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 2.00	top5: 9.50	
[09/16 06:35:27 visual_prompt]: 	Test 100/157. loss: 5.192, 0.2540 s / batch. (data: 1.43e-02)max mem: 17.22530 GB 
[09/16 06:35:39 visual_prompt]: Inference (test):avg data time: 6.59e-03, avg batch time: 0.1914, average loss: 5.2551
[09/16 06:35:39 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.00	top5: 5.74	
[09/16 06:35:39 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 06:35:48 visual_prompt]: Epoch 10 / 100: avg data time: 9.88e-02, avg batch time: 0.5242, average train loss: 5.1085
[09/16 06:35:51 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1425, average loss: 4.8489
[09/16 06:35:51 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 3.00	top5: 5.50	
[09/16 06:36:12 visual_prompt]: 	Test 100/157. loss: 4.969, 0.1828 s / batch. (data: 1.15e-04)max mem: 17.22530 GB 
[09/16 06:36:23 visual_prompt]: Inference (test):avg data time: 6.00e-03, avg batch time: 0.1906, average loss: 4.9552
[09/16 06:36:23 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 1.71	top5: 6.13	
[09/16 06:36:23 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 06:36:32 visual_prompt]: Epoch 11 / 100: avg data time: 8.46e-02, avg batch time: 0.4883, average train loss: 4.6756
[09/16 06:36:35 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1428, average loss: 4.7083
[09/16 06:36:35 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 4.00	top5: 14.50	
[09/16 06:36:56 visual_prompt]: 	Test 100/157. loss: 4.785, 0.1824 s / batch. (data: 1.18e-04)max mem: 17.22530 GB 
[09/16 06:37:08 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1923, average loss: 4.8794
[09/16 06:37:08 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 3.60	top5: 12.83	
[09/16 06:37:08 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 06:37:17 visual_prompt]: Epoch 12 / 100: avg data time: 1.10e-01, avg batch time: 0.5087, average train loss: 4.8200
[09/16 06:37:19 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1428, average loss: 4.3990
[09/16 06:37:19 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 3.50	top5: 18.50	
[09/16 06:37:41 visual_prompt]: 	Test 100/157. loss: 4.594, 0.1980 s / batch. (data: 1.58e-02)max mem: 17.22530 GB 
[09/16 06:37:53 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1966, average loss: 4.6480
[09/16 06:37:53 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 2.96	top5: 13.85	
[09/16 06:37:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 06:38:02 visual_prompt]: Epoch 13 / 100: avg data time: 9.19e-02, avg batch time: 0.4933, average train loss: 4.1368
[09/16 06:38:04 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1424, average loss: 4.0874
[09/16 06:38:04 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 5.50	top5: 25.00	
[09/16 06:38:25 visual_prompt]: 	Test 100/157. loss: 4.414, 0.2083 s / batch. (data: 1.66e-02)max mem: 17.22530 GB 
[09/16 06:38:37 visual_prompt]: Inference (test):avg data time: 6.52e-03, avg batch time: 0.1914, average loss: 4.4421
[09/16 06:38:37 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 5.75	top5: 20.00	
[09/16 06:38:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 06:38:46 visual_prompt]: Epoch 14 / 100: avg data time: 9.93e-02, avg batch time: 0.5017, average train loss: 3.9412
[09/16 06:38:49 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1426, average loss: 3.2110
[09/16 06:38:49 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 22.50	top5: 54.00	
[09/16 06:39:10 visual_prompt]: 	Test 100/157. loss: 3.320, 0.2103 s / batch. (data: 1.36e-02)max mem: 17.22530 GB 
[09/16 06:39:22 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1916, average loss: 3.7626
[09/16 06:39:22 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 14.90	top5: 36.47	
[09/16 06:39:22 visual_prompt]: Best epoch 14: best metric: 0.225
[09/16 06:39:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 06:39:30 visual_prompt]: Epoch 15 / 100: avg data time: 8.26e-02, avg batch time: 0.4854, average train loss: 2.8939
[09/16 06:39:33 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1426, average loss: 2.2206
[09/16 06:39:33 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 46.50	top5: 75.00	
[09/16 06:39:54 visual_prompt]: 	Test 100/157. loss: 2.822, 0.1969 s / batch. (data: 1.47e-02)max mem: 17.22530 GB 
[09/16 06:40:06 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1918, average loss: 2.9909
[09/16 06:40:06 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 28.64	top5: 60.14	
[09/16 06:40:06 visual_prompt]: Best epoch 15: best metric: 0.465
[09/16 06:40:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 06:40:15 visual_prompt]: Epoch 16 / 100: avg data time: 1.05e-01, avg batch time: 0.5071, average train loss: 2.0955
[09/16 06:40:18 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1428, average loss: 1.9200
[09/16 06:40:18 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 42.00	top5: 82.50	
[09/16 06:40:39 visual_prompt]: 	Test 100/157. loss: 2.704, 0.1955 s / batch. (data: 1.27e-02)max mem: 17.22530 GB 
[09/16 06:40:51 visual_prompt]: Inference (test):avg data time: 6.44e-03, avg batch time: 0.1927, average loss: 2.9899
[09/16 06:40:51 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 28.23	top5: 60.59	
[09/16 06:40:51 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 06:41:00 visual_prompt]: Epoch 17 / 100: avg data time: 1.01e-01, avg batch time: 0.5022, average train loss: 1.5784
[09/16 06:41:02 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1427, average loss: 1.1158
[09/16 06:41:02 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 69.00	top5: 93.50	
[09/16 06:41:23 visual_prompt]: 	Test 100/157. loss: 2.334, 0.1909 s / batch. (data: 1.57e-04)max mem: 17.22530 GB 
[09/16 06:41:35 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1911, average loss: 2.3957
[09/16 06:41:35 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 41.66	top5: 73.01	
[09/16 06:41:35 visual_prompt]: Best epoch 17: best metric: 0.690
[09/16 06:41:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 06:41:44 visual_prompt]: Epoch 18 / 100: avg data time: 9.51e-02, avg batch time: 0.4980, average train loss: 0.8673
[09/16 06:41:47 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1427, average loss: 0.6042
[09/16 06:41:47 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 82.00	top5: 99.00	
[09/16 06:42:07 visual_prompt]: 	Test 100/157. loss: 2.883, 0.1834 s / batch. (data: 1.28e-04)max mem: 17.22530 GB 
[09/16 06:42:19 visual_prompt]: Inference (test):avg data time: 6.20e-03, avg batch time: 0.1918, average loss: 2.6636
[09/16 06:42:19 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 45.93	top5: 77.72	
[09/16 06:42:19 visual_prompt]: Best epoch 18: best metric: 0.820
[09/16 06:42:19 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 06:42:28 visual_prompt]: Epoch 19 / 100: avg data time: 9.52e-02, avg batch time: 0.4958, average train loss: 0.7169
[09/16 06:42:31 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1434, average loss: 0.5234
[09/16 06:42:31 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 84.50	top5: 98.50	
[09/16 06:42:52 visual_prompt]: 	Test 100/157. loss: 2.395, 0.1832 s / batch. (data: 1.44e-04)max mem: 17.22530 GB 
[09/16 06:43:04 visual_prompt]: Inference (test):avg data time: 6.05e-03, avg batch time: 0.1904, average loss: 2.2923
[09/16 06:43:04 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 47.40	top5: 79.21	
[09/16 06:43:04 visual_prompt]: Best epoch 19: best metric: 0.845
[09/16 06:43:04 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 06:43:13 visual_prompt]: Epoch 20 / 100: avg data time: 1.06e-01, avg batch time: 0.5078, average train loss: 0.4091
[09/16 06:43:16 visual_prompt]: Inference (val):avg data time: 8.72e-05, avg batch time: 0.1663, average loss: 0.3364
[09/16 06:43:16 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 90.00	top5: 99.50	
[09/16 06:43:37 visual_prompt]: 	Test 100/157. loss: 2.271, 0.2082 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 06:43:48 visual_prompt]: Inference (test):avg data time: 6.91e-03, avg batch time: 0.1925, average loss: 2.3335
[09/16 06:43:49 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 51.51	top5: 80.72	
[09/16 06:43:49 visual_prompt]: Best epoch 20: best metric: 0.900
[09/16 06:43:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 06:43:57 visual_prompt]: Epoch 21 / 100: avg data time: 1.03e-01, avg batch time: 0.5032, average train loss: 0.2786
[09/16 06:44:00 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1430, average loss: 0.3724
[09/16 06:44:00 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 91.50	top5: 98.00	
[09/16 06:44:21 visual_prompt]: 	Test 100/157. loss: 3.444, 0.1823 s / batch. (data: 1.59e-04)max mem: 17.22530 GB 
[09/16 06:44:33 visual_prompt]: Inference (test):avg data time: 9.02e-03, avg batch time: 0.1929, average loss: 2.7284
[09/16 06:44:33 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 49.11	top5: 79.25	
[09/16 06:44:33 visual_prompt]: Best epoch 21: best metric: 0.915
[09/16 06:44:33 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 06:44:42 visual_prompt]: Epoch 22 / 100: avg data time: 9.18e-02, avg batch time: 0.4933, average train loss: 0.3879
[09/16 06:44:45 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1427, average loss: 0.1580
[09/16 06:44:45 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 95.00	top5: 99.50	
[09/16 06:45:06 visual_prompt]: 	Test 100/157. loss: 2.829, 0.1819 s / batch. (data: 1.19e-04)max mem: 17.22530 GB 
[09/16 06:45:18 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1956, average loss: 2.6603
[09/16 06:45:18 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 52.33	top5: 80.05	
[09/16 06:45:18 visual_prompt]: Best epoch 22: best metric: 0.950
[09/16 06:45:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 06:45:27 visual_prompt]: Epoch 23 / 100: avg data time: 1.05e-01, avg batch time: 0.5045, average train loss: 0.2689
[09/16 06:45:30 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1427, average loss: 0.1082
[09/16 06:45:30 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.50	top5: 99.50	
[09/16 06:45:51 visual_prompt]: 	Test 100/157. loss: 2.676, 0.1822 s / batch. (data: 1.23e-04)max mem: 17.22530 GB 
[09/16 06:46:02 visual_prompt]: Inference (test):avg data time: 6.58e-03, avg batch time: 0.1915, average loss: 2.3839
[09/16 06:46:03 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 54.69	top5: 81.83	
[09/16 06:46:03 visual_prompt]: Best epoch 23: best metric: 0.975
[09/16 06:46:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 06:46:12 visual_prompt]: Epoch 24 / 100: avg data time: 1.10e-01, avg batch time: 0.5107, average train loss: 0.1881
[09/16 06:46:14 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1427, average loss: 0.2575
[09/16 06:46:14 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 91.00	top5: 99.50	
[09/16 06:46:35 visual_prompt]: 	Test 100/157. loss: 2.211, 0.1965 s / batch. (data: 1.43e-02)max mem: 17.22530 GB 
[09/16 06:46:47 visual_prompt]: Inference (test):avg data time: 6.56e-03, avg batch time: 0.1918, average loss: 2.4208
[09/16 06:46:47 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 55.15	top5: 82.09	
[09/16 06:46:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 06:46:56 visual_prompt]: Epoch 25 / 100: avg data time: 8.92e-02, avg batch time: 0.4955, average train loss: 0.1783
[09/16 06:46:59 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1426, average loss: 0.1563
[09/16 06:46:59 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.00	top5: 100.00	
[09/16 06:47:20 visual_prompt]: 	Test 100/157. loss: 2.379, 0.1821 s / batch. (data: 1.04e-04)max mem: 17.22530 GB 
[09/16 06:47:31 visual_prompt]: Inference (test):avg data time: 8.52e-03, avg batch time: 0.1929, average loss: 2.3518
[09/16 06:47:32 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 56.61	top5: 82.39	
[09/16 06:47:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 06:47:40 visual_prompt]: Epoch 26 / 100: avg data time: 1.02e-01, avg batch time: 0.5035, average train loss: 0.1618
[09/16 06:47:43 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1427, average loss: 0.2166
[09/16 06:47:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 91.50	top5: 100.00	
[09/16 06:48:04 visual_prompt]: 	Test 100/157. loss: 2.273, 0.1960 s / batch. (data: 1.38e-02)max mem: 17.22530 GB 
[09/16 06:48:16 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1927, average loss: 2.6038
[09/16 06:48:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 54.43	top5: 81.50	
[09/16 06:48:16 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 06:48:25 visual_prompt]: Epoch 27 / 100: avg data time: 1.01e-01, avg batch time: 0.5010, average train loss: 0.1700
[09/16 06:48:28 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1426, average loss: 0.1105
[09/16 06:48:28 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.00	top5: 100.00	
[09/16 06:48:49 visual_prompt]: 	Test 100/157. loss: 2.071, 0.1817 s / batch. (data: 1.26e-04)max mem: 17.22530 GB 
[09/16 06:49:01 visual_prompt]: Inference (test):avg data time: 6.36e-03, avg batch time: 0.1922, average loss: 2.2095
[09/16 06:49:01 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 56.98	top5: 83.14	
[09/16 06:49:01 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 06:49:09 visual_prompt]: Epoch 28 / 100: avg data time: 9.52e-02, avg batch time: 0.4965, average train loss: 0.0910
[09/16 06:49:12 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1438, average loss: 0.1533
[09/16 06:49:12 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.00	top5: 99.00	
[09/16 06:49:33 visual_prompt]: 	Test 100/157. loss: 2.044, 0.1833 s / batch. (data: 1.18e-04)max mem: 17.22530 GB 
[09/16 06:49:45 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1930, average loss: 2.2546
[09/16 06:49:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.89	top5: 83.15	
[09/16 06:49:45 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 06:49:54 visual_prompt]: Epoch 29 / 100: avg data time: 1.06e-01, avg batch time: 0.5066, average train loss: 0.1025
[09/16 06:49:57 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1431, average loss: 0.0685
[09/16 06:49:57 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 98.50	top5: 100.00	
[09/16 06:50:18 visual_prompt]: 	Test 100/157. loss: 1.833, 0.1836 s / batch. (data: 1.24e-04)max mem: 17.22530 GB 
[09/16 06:50:29 visual_prompt]: Inference (test):avg data time: 7.06e-03, avg batch time: 0.1912, average loss: 1.8659
[09/16 06:50:29 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.69	top5: 86.25	
[09/16 06:50:29 visual_prompt]: Best epoch 29: best metric: 0.985
[09/16 06:50:29 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 06:50:38 visual_prompt]: Epoch 30 / 100: avg data time: 9.37e-02, avg batch time: 0.4975, average train loss: 0.0947
[09/16 06:50:41 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1426, average loss: 0.1892
[09/16 06:50:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 94.50	top5: 99.50	
[09/16 06:51:02 visual_prompt]: 	Test 100/157. loss: 2.371, 0.1821 s / batch. (data: 1.19e-04)max mem: 17.22530 GB 
[09/16 06:51:13 visual_prompt]: Inference (test):avg data time: 5.74e-03, avg batch time: 0.1903, average loss: 2.0961
[09/16 06:51:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.20	top5: 81.61	
[09/16 06:51:13 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 06:51:22 visual_prompt]: Epoch 31 / 100: avg data time: 9.12e-02, avg batch time: 0.4947, average train loss: 0.0965
[09/16 06:51:25 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1428, average loss: 0.4241
[09/16 06:51:25 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 87.00	top5: 97.00	
[09/16 06:51:46 visual_prompt]: 	Test 100/157. loss: 2.922, 0.1972 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 06:51:57 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1910, average loss: 2.5765
[09/16 06:51:58 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 49.40	top5: 75.01	
[09/16 06:51:58 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 06:52:06 visual_prompt]: Epoch 32 / 100: avg data time: 9.93e-02, avg batch time: 0.5004, average train loss: 0.2506
[09/16 06:52:09 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1425, average loss: 0.1614
[09/16 06:52:09 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 94.50	top5: 100.00	
[09/16 06:52:30 visual_prompt]: 	Test 100/157. loss: 1.762, 0.1832 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 06:52:42 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1916, average loss: 2.0773
[09/16 06:52:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 56.36	top5: 80.85	
[09/16 06:52:42 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 06:52:51 visual_prompt]: Epoch 33 / 100: avg data time: 9.99e-02, avg batch time: 0.5005, average train loss: 0.2428
[09/16 06:52:53 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1425, average loss: 0.3452
[09/16 06:52:53 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 93.50	top5: 98.50	
[09/16 06:53:15 visual_prompt]: 	Test 100/157. loss: 2.533, 0.1864 s / batch. (data: 1.14e-04)max mem: 17.22530 GB 
[09/16 06:53:26 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1924, average loss: 2.2291
[09/16 06:53:26 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.09	top5: 80.35	
[09/16 06:53:26 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 06:53:35 visual_prompt]: Epoch 34 / 100: avg data time: 9.02e-02, avg batch time: 0.4915, average train loss: 0.2398
[09/16 06:53:38 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1426, average loss: 0.2599
[09/16 06:53:38 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 92.50	top5: 99.50	
[09/16 06:53:59 visual_prompt]: 	Test 100/157. loss: 2.183, 0.1828 s / batch. (data: 2.91e-05)max mem: 17.22530 GB 
[09/16 06:54:11 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1950, average loss: 1.9985
[09/16 06:54:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 57.69	top5: 82.60	
[09/16 06:54:11 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 06:54:20 visual_prompt]: Epoch 35 / 100: avg data time: 9.81e-02, avg batch time: 0.4983, average train loss: 0.1470
[09/16 06:54:23 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1425, average loss: 0.0581
[09/16 06:54:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.00	top5: 99.50	
[09/16 06:54:44 visual_prompt]: 	Test 100/157. loss: 1.649, 0.2035 s / batch. (data: 1.49e-02)max mem: 17.22530 GB 
[09/16 06:54:56 visual_prompt]: Inference (test):avg data time: 7.02e-03, avg batch time: 0.1943, average loss: 1.7021
[09/16 06:54:56 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.92	top5: 87.30	
[09/16 06:54:56 visual_prompt]: Best epoch 35: best metric: 0.990
[09/16 06:54:56 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 06:55:05 visual_prompt]: Epoch 36 / 100: avg data time: 9.70e-02, avg batch time: 0.4972, average train loss: 0.0972
[09/16 06:55:07 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1425, average loss: 0.1549
[09/16 06:55:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.50	top5: 99.00	
[09/16 06:55:29 visual_prompt]: 	Test 100/157. loss: 1.920, 0.1820 s / batch. (data: 1.82e-04)max mem: 17.22530 GB 
[09/16 06:55:41 visual_prompt]: Inference (test):avg data time: 6.84e-03, avg batch time: 0.1959, average loss: 1.7931
[09/16 06:55:41 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 62.39	top5: 86.47	
[09/16 06:55:41 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 06:55:50 visual_prompt]: Epoch 37 / 100: avg data time: 9.49e-02, avg batch time: 0.4966, average train loss: 0.1122
[09/16 06:55:52 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1427, average loss: 0.1328
[09/16 06:55:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.00	top5: 100.00	
[09/16 06:56:13 visual_prompt]: 	Test 100/157. loss: 1.957, 0.1929 s / batch. (data: 1.08e-02)max mem: 17.22530 GB 
[09/16 06:56:25 visual_prompt]: Inference (test):avg data time: 8.47e-03, avg batch time: 0.1926, average loss: 1.8864
[09/16 06:56:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 61.72	top5: 84.98	
[09/16 06:56:25 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 06:56:34 visual_prompt]: Epoch 38 / 100: avg data time: 9.68e-02, avg batch time: 0.5217, average train loss: 0.1328
[09/16 06:56:37 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1426, average loss: 0.0554
[09/16 06:56:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.50	top5: 100.00	
[09/16 06:56:58 visual_prompt]: 	Test 100/157. loss: 1.894, 0.1974 s / batch. (data: 1.51e-02)max mem: 17.22530 GB 
[09/16 06:57:11 visual_prompt]: Inference (test):avg data time: 8.64e-03, avg batch time: 0.1954, average loss: 1.7450
[09/16 06:57:11 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 63.69	top5: 86.25	
[09/16 06:57:11 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 06:57:19 visual_prompt]: Epoch 39 / 100: avg data time: 9.78e-02, avg batch time: 0.5010, average train loss: 0.0857
[09/16 06:57:22 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1426, average loss: 0.9047
[09/16 06:57:22 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 77.50	top5: 93.50	
[09/16 06:57:43 visual_prompt]: 	Test 100/157. loss: 2.841, 0.1957 s / batch. (data: 1.38e-02)max mem: 17.22530 GB 
[09/16 06:57:55 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1932, average loss: 2.7313
[09/16 06:57:55 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 43.90	top5: 69.88	
[09/16 06:57:55 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 06:58:04 visual_prompt]: Epoch 40 / 100: avg data time: 9.23e-02, avg batch time: 0.5018, average train loss: 1.4340
[09/16 06:58:07 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1426, average loss: 0.4815
[09/16 06:58:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 85.50	top5: 98.00	
[09/16 06:58:28 visual_prompt]: 	Test 100/157. loss: 2.902, 0.2050 s / batch. (data: 2.04e-02)max mem: 17.22530 GB 
[09/16 06:58:40 visual_prompt]: Inference (test):avg data time: 7.20e-03, avg batch time: 0.1915, average loss: 2.7492
[09/16 06:58:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 46.01	top5: 73.02	
[09/16 06:58:40 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 06:58:49 visual_prompt]: Epoch 41 / 100: avg data time: 9.84e-02, avg batch time: 0.4996, average train loss: 0.3179
[09/16 06:58:52 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1428, average loss: 0.1703
[09/16 06:58:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 96.50	top5: 99.50	
[09/16 06:59:13 visual_prompt]: 	Test 100/157. loss: 1.708, 0.1823 s / batch. (data: 1.59e-04)max mem: 17.22530 GB 
[09/16 06:59:25 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1929, average loss: 1.9731
[09/16 06:59:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 59.08	top5: 83.16	
[09/16 06:59:25 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 06:59:34 visual_prompt]: Epoch 42 / 100: avg data time: 1.01e-01, avg batch time: 0.5021, average train loss: 0.1470
[09/16 06:59:37 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1454, average loss: 0.0974
[09/16 06:59:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 97.00	top5: 100.00	
[09/16 06:59:58 visual_prompt]: 	Test 100/157. loss: 2.369, 0.2059 s / batch. (data: 2.38e-02)max mem: 17.22530 GB 
[09/16 07:00:09 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1924, average loss: 2.0818
[09/16 07:00:09 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 58.53	top5: 83.52	
[09/16 07:00:09 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 07:00:18 visual_prompt]: Epoch 43 / 100: avg data time: 9.25e-02, avg batch time: 0.4985, average train loss: 0.0834
[09/16 07:00:21 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1426, average loss: 0.0683
[09/16 07:00:21 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.00	top5: 100.00	
[09/16 07:00:42 visual_prompt]: 	Test 100/157. loss: 1.721, 0.1951 s / batch. (data: 1.30e-02)max mem: 17.22530 GB 
[09/16 07:00:54 visual_prompt]: Inference (test):avg data time: 6.66e-03, avg batch time: 0.1922, average loss: 1.7523
[09/16 07:00:54 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 63.03	top5: 85.82	
[09/16 07:00:54 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 07:01:03 visual_prompt]: Epoch 44 / 100: avg data time: 9.41e-02, avg batch time: 0.4985, average train loss: 0.0534
[09/16 07:01:05 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1425, average loss: 0.0133
[09/16 07:01:05 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:01:26 visual_prompt]: 	Test 100/157. loss: 1.811, 0.2485 s / batch. (data: 3.98e-02)max mem: 17.22530 GB 
[09/16 07:01:38 visual_prompt]: Inference (test):avg data time: 6.12e-03, avg batch time: 0.1920, average loss: 1.6970
[09/16 07:01:38 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 64.77	top5: 87.08	
[09/16 07:01:38 visual_prompt]: Best epoch 44: best metric: 1.000
[09/16 07:01:38 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 07:01:47 visual_prompt]: Epoch 45 / 100: avg data time: 9.51e-02, avg batch time: 0.4960, average train loss: 0.0346
[09/16 07:01:50 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1426, average loss: 0.0213
[09/16 07:01:50 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 99.50	top5: 100.00	
[09/16 07:02:11 visual_prompt]: 	Test 100/157. loss: 1.807, 0.1957 s / batch. (data: 1.34e-02)max mem: 17.22530 GB 
[09/16 07:02:23 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1925, average loss: 1.6800
[09/16 07:02:23 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 63.48	top5: 86.57	
[09/16 07:02:23 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 07:02:32 visual_prompt]: Epoch 46 / 100: avg data time: 1.09e-01, avg batch time: 0.5105, average train loss: 0.0166
[09/16 07:02:34 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1428, average loss: 0.0145
[09/16 07:02:34 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:02:55 visual_prompt]: 	Test 100/157. loss: 1.860, 0.1973 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 07:03:07 visual_prompt]: Inference (test):avg data time: 6.24e-03, avg batch time: 0.1909, average loss: 1.6226
[09/16 07:03:07 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 65.10	top5: 86.24	
[09/16 07:03:07 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 07:03:16 visual_prompt]: Epoch 47 / 100: avg data time: 1.04e-01, avg batch time: 0.5049, average train loss: 0.0079
[09/16 07:03:19 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1425, average loss: 0.0038
[09/16 07:03:19 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:03:40 visual_prompt]: 	Test 100/157. loss: 1.585, 0.1833 s / batch. (data: 1.23e-04)max mem: 17.22530 GB 
[09/16 07:03:51 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1923, average loss: 1.4581
[09/16 07:03:51 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 67.21	top5: 87.77	
[09/16 07:03:51 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 07:04:00 visual_prompt]: Epoch 48 / 100: avg data time: 1.02e-01, avg batch time: 0.5025, average train loss: 0.0038
[09/16 07:04:03 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1427, average loss: 0.0031
[09/16 07:04:03 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:04:24 visual_prompt]: 	Test 100/157. loss: 1.504, 0.2004 s / batch. (data: 1.44e-04)max mem: 17.22530 GB 
[09/16 07:04:36 visual_prompt]: Inference (test):avg data time: 5.91e-03, avg batch time: 0.1920, average loss: 1.3808
[09/16 07:04:36 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 68.18	top5: 88.52	
[09/16 07:04:36 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 07:04:44 visual_prompt]: Epoch 49 / 100: avg data time: 8.15e-02, avg batch time: 0.4851, average train loss: 0.0030
[09/16 07:04:47 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1426, average loss: 0.0030
[09/16 07:04:47 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:05:08 visual_prompt]: 	Test 100/157. loss: 1.451, 0.1949 s / batch. (data: 1.31e-02)max mem: 17.22530 GB 
[09/16 07:05:20 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1926, average loss: 1.3078
[09/16 07:05:20 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 69.14	top5: 89.29	
[09/16 07:05:20 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 07:05:29 visual_prompt]: Epoch 50 / 100: avg data time: 9.94e-02, avg batch time: 0.5017, average train loss: 0.0030
[09/16 07:05:32 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1427, average loss: 0.0034
[09/16 07:05:32 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:05:53 visual_prompt]: 	Test 100/157. loss: 1.410, 0.1966 s / batch. (data: 1.45e-02)max mem: 17.22530 GB 
[09/16 07:06:05 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1928, average loss: 1.2706
[09/16 07:06:05 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 69.74	top5: 89.49	
[09/16 07:06:05 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 07:06:13 visual_prompt]: Epoch 51 / 100: avg data time: 9.66e-02, avg batch time: 0.4979, average train loss: 0.0034
[09/16 07:06:17 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1427, average loss: 0.0039
[09/16 07:06:17 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:06:38 visual_prompt]: 	Test 100/157. loss: 1.379, 0.1859 s / batch. (data: 1.68e-04)max mem: 17.22530 GB 
[09/16 07:06:50 visual_prompt]: Inference (test):avg data time: 6.83e-03, avg batch time: 0.1923, average loss: 1.2473
[09/16 07:06:50 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 70.09	top5: 89.73	
[09/16 07:06:50 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 07:06:58 visual_prompt]: Epoch 52 / 100: avg data time: 8.43e-02, avg batch time: 0.4876, average train loss: 0.0038
[09/16 07:07:01 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1429, average loss: 0.0042
[09/16 07:07:01 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:07:22 visual_prompt]: 	Test 100/157. loss: 1.323, 0.1956 s / batch. (data: 1.26e-04)max mem: 17.22530 GB 
[09/16 07:07:34 visual_prompt]: Inference (test):avg data time: 6.77e-03, avg batch time: 0.1913, average loss: 1.2228
[09/16 07:07:34 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 70.16	top5: 90.11	
[09/16 07:07:34 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 07:07:43 visual_prompt]: Epoch 53 / 100: avg data time: 9.33e-02, avg batch time: 0.5169, average train loss: 0.0042
[09/16 07:07:46 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1428, average loss: 0.0047
[09/16 07:07:46 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:08:07 visual_prompt]: 	Test 100/157. loss: 1.289, 0.1944 s / batch. (data: 1.22e-02)max mem: 17.22530 GB 
[09/16 07:08:19 visual_prompt]: Inference (test):avg data time: 6.04e-03, avg batch time: 0.1909, average loss: 1.1998
[09/16 07:08:19 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 70.49	top5: 90.34	
[09/16 07:08:19 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 07:08:28 visual_prompt]: Epoch 54 / 100: avg data time: 1.02e-01, avg batch time: 0.5038, average train loss: 0.0048
[09/16 07:08:31 visual_prompt]: Inference (val):avg data time: 3.72e-04, avg batch time: 0.2800, average loss: 0.0051
[09/16 07:08:31 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:08:52 visual_prompt]: 	Test 100/157. loss: 1.260, 0.1975 s / batch. (data: 1.54e-02)max mem: 17.22530 GB 
[09/16 07:09:04 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1920, average loss: 1.1777
[09/16 07:09:04 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 71.04	top5: 90.69	
[09/16 07:09:04 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 07:09:13 visual_prompt]: Epoch 55 / 100: avg data time: 1.07e-01, avg batch time: 0.5276, average train loss: 0.0052
[09/16 07:09:16 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.1435, average loss: 0.0054
[09/16 07:09:16 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:09:37 visual_prompt]: 	Test 100/157. loss: 1.233, 0.2075 s / batch. (data: 2.54e-02)max mem: 17.22530 GB 
[09/16 07:09:49 visual_prompt]: Inference (test):avg data time: 6.57e-03, avg batch time: 0.1926, average loss: 1.1621
[09/16 07:09:49 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 71.26	top5: 90.83	
[09/16 07:09:49 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 07:09:58 visual_prompt]: Epoch 56 / 100: avg data time: 8.51e-02, avg batch time: 0.4877, average train loss: 0.0055
[09/16 07:10:00 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1426, average loss: 0.0057
[09/16 07:10:00 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:10:21 visual_prompt]: 	Test 100/157. loss: 1.203, 0.2003 s / batch. (data: 1.85e-02)max mem: 17.22530 GB 
[09/16 07:10:33 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1925, average loss: 1.1401
[09/16 07:10:33 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 71.52	top5: 91.48	
[09/16 07:10:33 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 07:10:42 visual_prompt]: Epoch 57 / 100: avg data time: 1.03e-01, avg batch time: 0.5036, average train loss: 0.0058
[09/16 07:10:45 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1428, average loss: 0.0058
[09/16 07:10:45 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:11:06 visual_prompt]: 	Test 100/157. loss: 1.202, 0.1824 s / batch. (data: 1.17e-04)max mem: 17.22530 GB 
[09/16 07:11:18 visual_prompt]: Inference (test):avg data time: 6.37e-03, avg batch time: 0.1906, average loss: 1.1241
[09/16 07:11:18 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.03	top5: 91.66	
[09/16 07:11:18 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 07:11:26 visual_prompt]: Epoch 58 / 100: avg data time: 8.88e-02, avg batch time: 0.4922, average train loss: 0.0058
[09/16 07:11:29 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1429, average loss: 0.0058
[09/16 07:11:29 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:11:50 visual_prompt]: 	Test 100/157. loss: 1.184, 0.1970 s / batch. (data: 8.05e-03)max mem: 17.22530 GB 
[09/16 07:12:02 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1914, average loss: 1.1113
[09/16 07:12:02 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.10	top5: 91.89	
[09/16 07:12:02 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 07:12:11 visual_prompt]: Epoch 59 / 100: avg data time: 8.30e-02, avg batch time: 0.4850, average train loss: 0.0061
[09/16 07:12:13 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1428, average loss: 0.0059
[09/16 07:12:13 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:12:34 visual_prompt]: 	Test 100/157. loss: 1.165, 0.2017 s / batch. (data: 1.22e-04)max mem: 17.22530 GB 
[09/16 07:12:46 visual_prompt]: Inference (test):avg data time: 6.43e-03, avg batch time: 0.1920, average loss: 1.0956
[09/16 07:12:46 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.38	top5: 92.22	
[09/16 07:12:46 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 07:12:55 visual_prompt]: Epoch 60 / 100: avg data time: 1.08e-01, avg batch time: 0.5091, average train loss: 0.0062
[09/16 07:12:58 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1428, average loss: 0.0062
[09/16 07:12:58 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:13:19 visual_prompt]: 	Test 100/157. loss: 1.187, 0.2445 s / batch. (data: 1.33e-04)max mem: 17.22530 GB 
[09/16 07:13:31 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1932, average loss: 1.0903
[09/16 07:13:31 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.21	top5: 92.29	
[09/16 07:13:31 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 07:13:40 visual_prompt]: Epoch 61 / 100: avg data time: 1.05e-01, avg batch time: 0.5064, average train loss: 0.0063
[09/16 07:13:43 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1429, average loss: 0.0062
[09/16 07:13:43 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:14:04 visual_prompt]: 	Test 100/157. loss: 1.158, 0.1950 s / batch. (data: 1.24e-02)max mem: 17.22530 GB 
[09/16 07:14:15 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1918, average loss: 1.0816
[09/16 07:14:16 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.45	top5: 92.64	
[09/16 07:14:16 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 07:14:24 visual_prompt]: Epoch 62 / 100: avg data time: 1.01e-01, avg batch time: 0.5001, average train loss: 0.0066
[09/16 07:14:27 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1426, average loss: 0.0063
[09/16 07:14:27 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:14:48 visual_prompt]: 	Test 100/157. loss: 1.162, 0.1987 s / batch. (data: 1.44e-04)max mem: 17.22530 GB 
[09/16 07:15:00 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1924, average loss: 1.0710
[09/16 07:15:00 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.99	top5: 92.72	
[09/16 07:15:00 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 07:15:09 visual_prompt]: Epoch 63 / 100: avg data time: 1.02e-01, avg batch time: 0.5014, average train loss: 0.0066
[09/16 07:15:12 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1427, average loss: 0.0061
[09/16 07:15:12 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:15:33 visual_prompt]: 	Test 100/157. loss: 1.147, 0.1969 s / batch. (data: 3.46e-05)max mem: 17.22530 GB 
[09/16 07:15:45 visual_prompt]: Inference (test):avg data time: 6.30e-03, avg batch time: 0.1916, average loss: 1.0643
[09/16 07:15:45 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 72.90	top5: 92.78	
[09/16 07:15:45 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 07:15:54 visual_prompt]: Epoch 64 / 100: avg data time: 1.01e-01, avg batch time: 0.5036, average train loss: 0.0065
[09/16 07:15:56 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1426, average loss: 0.0059
[09/16 07:15:56 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:16:18 visual_prompt]: 	Test 100/157. loss: 1.132, 0.1820 s / batch. (data: 8.49e-05)max mem: 17.22530 GB 
[09/16 07:16:29 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1920, average loss: 1.0453
[09/16 07:16:29 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.35	top5: 93.07	
[09/16 07:16:29 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 07:16:38 visual_prompt]: Epoch 65 / 100: avg data time: 9.56e-02, avg batch time: 0.4971, average train loss: 0.0064
[09/16 07:16:41 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1427, average loss: 0.0058
[09/16 07:16:41 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:17:02 visual_prompt]: 	Test 100/157. loss: 1.120, 0.1954 s / batch. (data: 1.31e-02)max mem: 17.22530 GB 
[09/16 07:17:14 visual_prompt]: Inference (test):avg data time: 8.10e-03, avg batch time: 0.1919, average loss: 1.0393
[09/16 07:17:14 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.60	top5: 93.10	
[09/16 07:17:14 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 07:17:22 visual_prompt]: Epoch 66 / 100: avg data time: 8.59e-02, avg batch time: 0.4885, average train loss: 0.0064
[09/16 07:17:25 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1427, average loss: 0.0058
[09/16 07:17:25 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:17:47 visual_prompt]: 	Test 100/157. loss: 1.110, 0.2099 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 07:17:59 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1945, average loss: 1.0369
[09/16 07:17:59 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.54	top5: 93.25	
[09/16 07:17:59 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 07:18:07 visual_prompt]: Epoch 67 / 100: avg data time: 9.69e-02, avg batch time: 0.4985, average train loss: 0.0063
[09/16 07:18:10 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1427, average loss: 0.0056
[09/16 07:18:10 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:18:32 visual_prompt]: 	Test 100/157. loss: 1.066, 0.1980 s / batch. (data: 1.53e-02)max mem: 17.22530 GB 
[09/16 07:18:44 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1943, average loss: 1.0244
[09/16 07:18:44 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.72	top5: 93.32	
[09/16 07:18:44 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 07:18:52 visual_prompt]: Epoch 68 / 100: avg data time: 9.09e-02, avg batch time: 0.4929, average train loss: 0.0065
[09/16 07:18:55 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 0.0056
[09/16 07:18:55 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:19:16 visual_prompt]: 	Test 100/157. loss: 1.097, 0.1924 s / batch. (data: 1.03e-02)max mem: 17.22530 GB 
[09/16 07:19:28 visual_prompt]: Inference (test):avg data time: 7.01e-03, avg batch time: 0.1917, average loss: 1.0308
[09/16 07:19:28 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.58	top5: 93.37	
[09/16 07:19:28 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 07:19:37 visual_prompt]: Epoch 69 / 100: avg data time: 1.01e-01, avg batch time: 0.5037, average train loss: 0.0064
[09/16 07:19:40 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1427, average loss: 0.0056
[09/16 07:19:40 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:20:01 visual_prompt]: 	Test 100/157. loss: 1.086, 0.1983 s / batch. (data: 1.61e-02)max mem: 17.22530 GB 
[09/16 07:20:13 visual_prompt]: Inference (test):avg data time: 6.00e-03, avg batch time: 0.1915, average loss: 1.0174
[09/16 07:20:13 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.92	top5: 93.59	
[09/16 07:20:13 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 07:20:22 visual_prompt]: Epoch 70 / 100: avg data time: 1.17e-01, avg batch time: 0.5170, average train loss: 0.0064
[09/16 07:20:25 visual_prompt]: Inference (val):avg data time: 3.56e-04, avg batch time: 0.2243, average loss: 0.0054
[09/16 07:20:25 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:20:46 visual_prompt]: 	Test 100/157. loss: 1.056, 0.1988 s / batch. (data: 1.67e-02)max mem: 17.22530 GB 
[09/16 07:20:58 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1923, average loss: 1.0137
[09/16 07:20:58 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.05	top5: 93.57	
[09/16 07:20:58 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 07:21:07 visual_prompt]: Epoch 71 / 100: avg data time: 9.87e-02, avg batch time: 0.5018, average train loss: 0.0064
[09/16 07:21:09 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1435, average loss: 0.0054
[09/16 07:21:09 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:21:30 visual_prompt]: 	Test 100/157. loss: 1.045, 0.1945 s / batch. (data: 1.13e-04)max mem: 17.22530 GB 
[09/16 07:21:42 visual_prompt]: Inference (test):avg data time: 6.78e-03, avg batch time: 0.1914, average loss: 1.0129
[09/16 07:21:42 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.05	top5: 93.75	
[09/16 07:21:42 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 07:21:51 visual_prompt]: Epoch 72 / 100: avg data time: 8.82e-02, avg batch time: 0.4897, average train loss: 0.0061
[09/16 07:21:54 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1428, average loss: 0.0052
[09/16 07:21:54 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:22:15 visual_prompt]: 	Test 100/157. loss: 1.039, 0.1827 s / batch. (data: 1.34e-04)max mem: 17.22530 GB 
[09/16 07:22:26 visual_prompt]: Inference (test):avg data time: 6.46e-03, avg batch time: 0.1932, average loss: 1.0038
[09/16 07:22:27 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.11	top5: 93.81	
[09/16 07:22:27 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 07:22:35 visual_prompt]: Epoch 73 / 100: avg data time: 9.21e-02, avg batch time: 0.4955, average train loss: 0.0061
[09/16 07:22:38 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1426, average loss: 0.0052
[09/16 07:22:38 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:23:00 visual_prompt]: 	Test 100/157. loss: 1.021, 0.1829 s / batch. (data: 8.23e-05)max mem: 17.22530 GB 
[09/16 07:23:12 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1958, average loss: 0.9981
[09/16 07:23:12 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 73.89	top5: 93.93	
[09/16 07:23:12 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 07:23:21 visual_prompt]: Epoch 74 / 100: avg data time: 9.51e-02, avg batch time: 0.4954, average train loss: 0.0059
[09/16 07:23:23 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1451, average loss: 0.0051
[09/16 07:23:23 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:23:44 visual_prompt]: 	Test 100/157. loss: 1.031, 0.1983 s / batch. (data: 1.60e-02)max mem: 17.22530 GB 
[09/16 07:23:56 visual_prompt]: Inference (test):avg data time: 6.33e-03, avg batch time: 0.1918, average loss: 0.9992
[09/16 07:23:56 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.15	top5: 93.85	
[09/16 07:23:56 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 07:24:05 visual_prompt]: Epoch 75 / 100: avg data time: 9.95e-02, avg batch time: 0.5007, average train loss: 0.0058
[09/16 07:24:08 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1428, average loss: 0.0050
[09/16 07:24:08 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:24:29 visual_prompt]: 	Test 100/157. loss: 1.036, 0.2093 s / batch. (data: 2.46e-02)max mem: 17.22530 GB 
[09/16 07:24:41 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1938, average loss: 1.0018
[09/16 07:24:41 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.01	top5: 93.86	
[09/16 07:24:41 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 07:24:50 visual_prompt]: Epoch 76 / 100: avg data time: 9.94e-02, avg batch time: 0.5023, average train loss: 0.0057
[09/16 07:24:52 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1429, average loss: 0.0049
[09/16 07:24:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:25:13 visual_prompt]: 	Test 100/157. loss: 1.031, 0.1963 s / batch. (data: 1.39e-02)max mem: 17.22530 GB 
[09/16 07:25:25 visual_prompt]: Inference (test):avg data time: 6.88e-03, avg batch time: 0.1929, average loss: 0.9991
[09/16 07:25:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.17	top5: 93.81	
[09/16 07:25:25 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 07:25:34 visual_prompt]: Epoch 77 / 100: avg data time: 9.02e-02, avg batch time: 0.4917, average train loss: 0.0057
[09/16 07:25:37 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1427, average loss: 0.0049
[09/16 07:25:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:25:58 visual_prompt]: 	Test 100/157. loss: 1.033, 0.1989 s / batch. (data: 1.68e-02)max mem: 17.22530 GB 
[09/16 07:26:10 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1958, average loss: 0.9940
[09/16 07:26:10 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.09	top5: 93.95	
[09/16 07:26:10 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 07:26:19 visual_prompt]: Epoch 78 / 100: avg data time: 9.88e-02, avg batch time: 0.4991, average train loss: 0.0057
[09/16 07:26:22 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1427, average loss: 0.0049
[09/16 07:26:22 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:26:44 visual_prompt]: 	Test 100/157. loss: 1.035, 0.2230 s / batch. (data: 4.14e-02)max mem: 17.22530 GB 
[09/16 07:26:56 visual_prompt]: Inference (test):avg data time: 6.33e-03, avg batch time: 0.1967, average loss: 0.9946
[09/16 07:26:56 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.25	top5: 93.96	
[09/16 07:26:56 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 07:27:05 visual_prompt]: Epoch 79 / 100: avg data time: 1.06e-01, avg batch time: 0.5058, average train loss: 0.0056
[09/16 07:27:07 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1426, average loss: 0.0048
[09/16 07:27:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:27:28 visual_prompt]: 	Test 100/157. loss: 1.034, 0.1819 s / batch. (data: 9.23e-05)max mem: 17.22530 GB 
[09/16 07:27:40 visual_prompt]: Inference (test):avg data time: 6.78e-03, avg batch time: 0.1923, average loss: 0.9932
[09/16 07:27:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.08	top5: 93.93	
[09/16 07:27:40 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 07:27:49 visual_prompt]: Epoch 80 / 100: avg data time: 1.07e-01, avg batch time: 0.5091, average train loss: 0.0055
[09/16 07:27:52 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1425, average loss: 0.0048
[09/16 07:27:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:28:13 visual_prompt]: 	Test 100/157. loss: 1.023, 0.1829 s / batch. (data: 1.29e-04)max mem: 17.22530 GB 
[09/16 07:28:25 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1926, average loss: 1.0006
[09/16 07:28:25 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.16	top5: 93.97	
[09/16 07:28:25 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 07:28:34 visual_prompt]: Epoch 81 / 100: avg data time: 9.71e-02, avg batch time: 0.5159, average train loss: 0.0056
[09/16 07:28:37 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1426, average loss: 0.0047
[09/16 07:28:37 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:28:58 visual_prompt]: 	Test 100/157. loss: 1.017, 0.1829 s / batch. (data: 1.19e-04)max mem: 17.22530 GB 
[09/16 07:29:10 visual_prompt]: Inference (test):avg data time: 6.94e-03, avg batch time: 0.1916, average loss: 0.9875
[09/16 07:29:10 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.27	top5: 94.11	
[09/16 07:29:10 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 07:29:19 visual_prompt]: Epoch 82 / 100: avg data time: 1.09e-01, avg batch time: 0.5094, average train loss: 0.0054
[09/16 07:29:22 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1424, average loss: 0.0047
[09/16 07:29:22 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:29:43 visual_prompt]: 	Test 100/157. loss: 1.009, 0.1956 s / batch. (data: 1.35e-02)max mem: 17.22530 GB 
[09/16 07:29:55 visual_prompt]: Inference (test):avg data time: 6.86e-03, avg batch time: 0.1915, average loss: 0.9941
[09/16 07:29:55 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.15	top5: 93.96	
[09/16 07:29:55 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 07:30:04 visual_prompt]: Epoch 83 / 100: avg data time: 9.69e-02, avg batch time: 0.5091, average train loss: 0.0053
[09/16 07:30:07 visual_prompt]: Inference (val):avg data time: 3.66e-04, avg batch time: 0.2150, average loss: 0.0046
[09/16 07:30:07 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:30:28 visual_prompt]: 	Test 100/157. loss: 1.014, 0.1823 s / batch. (data: 8.82e-05)max mem: 17.22530 GB 
[09/16 07:30:40 visual_prompt]: Inference (test):avg data time: 8.46e-03, avg batch time: 0.1942, average loss: 0.9918
[09/16 07:30:40 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.36	top5: 94.07	
[09/16 07:30:40 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 07:30:49 visual_prompt]: Epoch 84 / 100: avg data time: 8.42e-02, avg batch time: 0.4878, average train loss: 0.0054
[09/16 07:30:52 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1427, average loss: 0.0045
[09/16 07:30:52 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:31:13 visual_prompt]: 	Test 100/157. loss: 1.011, 0.1825 s / batch. (data: 1.36e-04)max mem: 17.22530 GB 
[09/16 07:31:24 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1914, average loss: 0.9886
[09/16 07:31:24 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.37	top5: 94.06	
[09/16 07:31:24 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 07:31:33 visual_prompt]: Epoch 85 / 100: avg data time: 1.00e-01, avg batch time: 0.5041, average train loss: 0.0053
[09/16 07:31:36 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1426, average loss: 0.0045
[09/16 07:31:36 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:31:57 visual_prompt]: 	Test 100/157. loss: 0.999, 0.1956 s / batch. (data: 1.32e-02)max mem: 17.22530 GB 
[09/16 07:32:09 visual_prompt]: Inference (test):avg data time: 6.50e-03, avg batch time: 0.1913, average loss: 0.9875
[09/16 07:32:09 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.27	top5: 94.05	
[09/16 07:32:09 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 07:32:18 visual_prompt]: Epoch 86 / 100: avg data time: 9.87e-02, avg batch time: 0.5008, average train loss: 0.0052
[09/16 07:32:21 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1429, average loss: 0.0045
[09/16 07:32:21 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:32:42 visual_prompt]: 	Test 100/157. loss: 1.009, 0.2069 s / batch. (data: 2.50e-02)max mem: 17.22530 GB 
[09/16 07:32:54 visual_prompt]: Inference (test):avg data time: 6.46e-03, avg batch time: 0.1914, average loss: 0.9935
[09/16 07:32:54 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.12	top5: 94.07	
[09/16 07:32:54 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 07:33:03 visual_prompt]: Epoch 87 / 100: avg data time: 1.05e-01, avg batch time: 0.5063, average train loss: 0.0052
[09/16 07:33:05 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1427, average loss: 0.0045
[09/16 07:33:05 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:33:26 visual_prompt]: 	Test 100/157. loss: 1.011, 0.1826 s / batch. (data: 1.44e-04)max mem: 17.22530 GB 
[09/16 07:33:38 visual_prompt]: Inference (test):avg data time: 8.60e-03, avg batch time: 0.1937, average loss: 0.9925
[09/16 07:33:39 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.23	top5: 94.08	
[09/16 07:33:39 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 07:33:47 visual_prompt]: Epoch 88 / 100: avg data time: 9.23e-02, avg batch time: 0.4993, average train loss: 0.0052
[09/16 07:33:50 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1427, average loss: 0.0044
[09/16 07:33:50 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:34:11 visual_prompt]: 	Test 100/157. loss: 1.003, 0.2067 s / batch. (data: 1.31e-04)max mem: 17.22530 GB 
[09/16 07:34:23 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1926, average loss: 0.9921
[09/16 07:34:23 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.23	top5: 94.07	
[09/16 07:34:23 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 07:34:32 visual_prompt]: Epoch 89 / 100: avg data time: 9.08e-02, avg batch time: 0.4931, average train loss: 0.0051
[09/16 07:34:35 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1428, average loss: 0.0045
[09/16 07:34:35 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:34:56 visual_prompt]: 	Test 100/157. loss: 1.005, 0.1826 s / batch. (data: 1.26e-04)max mem: 17.22530 GB 
[09/16 07:35:08 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1927, average loss: 0.9946
[09/16 07:35:08 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.06	top5: 94.12	
[09/16 07:35:08 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 07:35:17 visual_prompt]: Epoch 90 / 100: avg data time: 9.57e-02, avg batch time: 0.5006, average train loss: 0.0051
[09/16 07:35:19 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1429, average loss: 0.0044
[09/16 07:35:19 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:35:40 visual_prompt]: 	Test 100/157. loss: 1.005, 0.2037 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 07:35:52 visual_prompt]: Inference (test):avg data time: 7.20e-03, avg batch time: 0.1920, average loss: 0.9946
[09/16 07:35:52 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.15	top5: 94.13	
[09/16 07:35:52 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 07:36:01 visual_prompt]: Epoch 91 / 100: avg data time: 9.94e-02, avg batch time: 0.5025, average train loss: 0.0050
[09/16 07:36:04 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1427, average loss: 0.0044
[09/16 07:36:04 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:36:25 visual_prompt]: 	Test 100/157. loss: 1.008, 0.1828 s / batch. (data: 1.51e-04)max mem: 17.22530 GB 
[09/16 07:36:37 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1931, average loss: 0.9939
[09/16 07:36:37 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.14	top5: 94.20	
[09/16 07:36:37 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 07:36:46 visual_prompt]: Epoch 92 / 100: avg data time: 1.09e-01, avg batch time: 0.5104, average train loss: 0.0050
[09/16 07:36:49 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1428, average loss: 0.0044
[09/16 07:36:49 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:37:10 visual_prompt]: 	Test 100/157. loss: 1.008, 0.1947 s / batch. (data: 3.39e-05)max mem: 17.22530 GB 
[09/16 07:37:22 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1931, average loss: 0.9924
[09/16 07:37:22 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.15	top5: 94.24	
[09/16 07:37:22 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 07:37:31 visual_prompt]: Epoch 93 / 100: avg data time: 1.08e-01, avg batch time: 0.5129, average train loss: 0.0051
[09/16 07:37:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1427, average loss: 0.0044
[09/16 07:37:34 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:37:55 visual_prompt]: 	Test 100/157. loss: 1.010, 0.1831 s / batch. (data: 1.39e-04)max mem: 17.22530 GB 
[09/16 07:38:07 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1922, average loss: 0.9928
[09/16 07:38:07 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.21	top5: 94.20	
[09/16 07:38:07 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 07:38:16 visual_prompt]: Epoch 94 / 100: avg data time: 1.08e-01, avg batch time: 0.5116, average train loss: 0.0051
[09/16 07:38:19 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1427, average loss: 0.0043
[09/16 07:38:19 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:38:40 visual_prompt]: 	Test 100/157. loss: 1.012, 0.1958 s / batch. (data: 1.38e-02)max mem: 17.22530 GB 
[09/16 07:38:52 visual_prompt]: Inference (test):avg data time: 8.62e-03, avg batch time: 0.1933, average loss: 0.9926
[09/16 07:38:52 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.23	top5: 94.20	
[09/16 07:38:52 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 07:39:01 visual_prompt]: Epoch 95 / 100: avg data time: 1.04e-01, avg batch time: 0.5046, average train loss: 0.0050
[09/16 07:39:03 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1426, average loss: 0.0044
[09/16 07:39:03 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:39:25 visual_prompt]: 	Test 100/157. loss: 1.015, 0.2069 s / batch. (data: 1.02e-02)max mem: 17.22530 GB 
[09/16 07:39:36 visual_prompt]: Inference (test):avg data time: 7.07e-03, avg batch time: 0.1917, average loss: 0.9928
[09/16 07:39:36 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.18	top5: 94.18	
[09/16 07:39:36 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 07:39:45 visual_prompt]: Epoch 96 / 100: avg data time: 1.02e-01, avg batch time: 0.5038, average train loss: 0.0051
[09/16 07:39:48 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1427, average loss: 0.0043
[09/16 07:39:48 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:40:09 visual_prompt]: 	Test 100/157. loss: 1.017, 0.1836 s / batch. (data: 1.28e-04)max mem: 17.22530 GB 
[09/16 07:40:21 visual_prompt]: Inference (test):avg data time: 6.93e-03, avg batch time: 0.1920, average loss: 0.9925
[09/16 07:40:21 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.16	top5: 94.21	
[09/16 07:40:21 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 07:40:30 visual_prompt]: Epoch 97 / 100: avg data time: 1.03e-01, avg batch time: 0.5043, average train loss: 0.0051
[09/16 07:40:33 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1427, average loss: 0.0043
[09/16 07:40:33 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:40:54 visual_prompt]: 	Test 100/157. loss: 1.016, 0.1912 s / batch. (data: 1.63e-04)max mem: 17.22530 GB 
[09/16 07:41:05 visual_prompt]: Inference (test):avg data time: 6.26e-03, avg batch time: 0.1903, average loss: 0.9926
[09/16 07:41:05 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.08	top5: 94.22	
[09/16 07:41:05 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 07:41:14 visual_prompt]: Epoch 98 / 100: avg data time: 1.00e-01, avg batch time: 0.5024, average train loss: 0.0051
[09/16 07:41:17 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1427, average loss: 0.0043
[09/16 07:41:17 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:41:38 visual_prompt]: 	Test 100/157. loss: 1.015, 0.1823 s / batch. (data: 1.33e-04)max mem: 17.22530 GB 
[09/16 07:41:50 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1917, average loss: 0.9928
[09/16 07:41:50 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.10	top5: 94.21	
[09/16 07:41:50 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 07:41:59 visual_prompt]: Epoch 99 / 100: avg data time: 9.67e-02, avg batch time: 0.4972, average train loss: 0.0050
[09/16 07:42:02 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1429, average loss: 0.0043
[09/16 07:42:02 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:42:23 visual_prompt]: 	Test 100/157. loss: 1.015, 0.1827 s / batch. (data: 1.30e-04)max mem: 17.22530 GB 
[09/16 07:42:34 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1925, average loss: 0.9929
[09/16 07:42:35 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.09	top5: 94.20	
[09/16 07:42:35 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 07:42:44 visual_prompt]: Epoch 100 / 100: avg data time: 1.10e-01, avg batch time: 0.5093, average train loss: 0.0050
[09/16 07:42:46 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1428, average loss: 0.0043
[09/16 07:42:46 visual_prompt]: Classification results with val_vtab-cifar(num_classes=100): top1: 100.00	top5: 100.00	
[09/16 07:43:07 visual_prompt]: 	Test 100/157. loss: 1.015, 0.1956 s / batch. (data: 1.36e-02)max mem: 17.22530 GB 
[09/16 07:43:19 visual_prompt]: Inference (test):avg data time: 7.50e-03, avg batch time: 0.1920, average loss: 0.9929
[09/16 07:43:19 visual_prompt]: Classification results with test_vtab-cifar(num_classes=100): top1: 74.08	top5: 94.20	

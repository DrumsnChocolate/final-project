/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/23 17:29:20 visual_prompt]: Rank of current process: 0. World size: 1
[10/23 17:29:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/23 17:29:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/23 17:29:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/23 17:29:21 visual_prompt]: Training with config:
[10/23 17:29:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr50.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/23 17:29:21 visual_prompt]: Loading training data...
[10/23 17:29:21 visual_prompt]: Constructing mammo-cbis dataset train...
[10/23 17:29:21 visual_prompt]: Loading validation data...
[10/23 17:29:21 visual_prompt]: Constructing mammo-cbis dataset val...
[10/23 17:29:21 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/23 17:29:25 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/23 17:29:25 visual_prompt]: tuned percent:0.529
[10/23 17:29:25 visual_prompt]: Device used for model: 0
[10/23 17:29:25 visual_prompt]: Setting up Evaluator...
[10/23 17:29:25 visual_prompt]: Setting up Trainer...
[10/23 17:29:25 visual_prompt]: 	Setting up the optimizer...
[10/23 17:29:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/23 17:31:02 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4946 s / batch. (data: 3.18e-04). ETA=7:35:00, max mem: 11.4 GB 
[10/23 17:32:33 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5074 s / batch. (data: 5.38e-03). ETA=7:45:56, max mem: 11.4 GB 
[10/23 17:34:10 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.2400 s / batch. (data: 2.73e+00). ETA=2 days, 1:30:01, max mem: 11.4 GB 
[10/23 17:35:39 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5018 s / batch. (data: 7.98e-03). ETA=7:39:09, max mem: 11.4 GB 
[10/23 17:37:15 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4919 s / batch. (data: 4.13e-04). ETA=7:29:18, max mem: 11.4 GB 
[10/23 17:38:03 visual_prompt]: Epoch 1 / 100: avg data time: 4.36e-01, avg batch time: 0.9359, average train loss: 1.3966
[10/23 17:38:57 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1904, average loss: 1.3454
[10/23 17:38:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/23 17:38:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/23 17:40:42 visual_prompt]: 	Training 100/553. train loss: 7.5797,	0.5200 s / batch. (data: 2.61e-04). ETA=7:53:37, max mem: 11.4 GB 
[10/23 17:42:14 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1393 s / batch. (data: 6.39e-01). ETA=17:15:43, max mem: 11.4 GB 
[10/23 17:43:47 visual_prompt]: 	Training 300/553. train loss: 8.9756,	1.8231 s / batch. (data: 1.31e+00). ETA=1 day, 3:34:22, max mem: 11.4 GB 
[10/23 17:45:16 visual_prompt]: 	Training 400/553. train loss: 9.0264,	0.7916 s / batch. (data: 3.08e-01). ETA=11:57:03, max mem: 11.4 GB 
[10/23 17:46:50 visual_prompt]: 	Training 500/553. train loss: 3.8020,	0.4999 s / batch. (data: 2.75e-04). ETA=7:32:00, max mem: 11.4 GB 
[10/23 17:47:35 visual_prompt]: Epoch 2 / 100: avg data time: 4.41e-01, avg batch time: 0.9366, average train loss: 18.0974
[10/23 17:48:29 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.1911, average loss: 0.9561
[10/23 17:48:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.47	
[10/23 17:48:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/23 17:50:01 visual_prompt]: 	Training 100/553. train loss: 13.6798,	0.5012 s / batch. (data: 3.28e-04). ETA=7:31:50, max mem: 11.4 GB 
[10/23 17:51:34 visual_prompt]: 	Training 200/553. train loss: 26.6610,	0.8840 s / batch. (data: 3.65e-01). ETA=13:15:31, max mem: 11.4 GB 
[10/23 17:53:03 visual_prompt]: 	Training 300/553. train loss: 25.8675,	0.5000 s / batch. (data: 2.73e-04). ETA=7:29:07, max mem: 11.4 GB 
[10/23 17:54:36 visual_prompt]: 	Training 400/553. train loss: 68.9261,	0.4960 s / batch. (data: 2.24e-04). ETA=7:24:43, max mem: 11.4 GB 
[10/23 17:56:08 visual_prompt]: 	Training 500/553. train loss: 10.0027,	2.0914 s / batch. (data: 1.60e+00). ETA=1 day, 7:11:33, max mem: 11.4 GB 
[10/23 17:56:53 visual_prompt]: Epoch 3 / 100: avg data time: 4.17e-01, avg batch time: 0.9119, average train loss: 30.5267
[10/23 17:57:47 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1932, average loss: 45.0389
[10/23 17:57:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.48	
[10/23 17:57:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/23 17:59:22 visual_prompt]: 	Training 100/553. train loss: 40.6780,	0.4870 s / batch. (data: 2.80e-04). ETA=7:14:32, max mem: 11.4 GB 
[10/23 18:00:54 visual_prompt]: 	Training 200/553. train loss: 36.8292,	0.4946 s / batch. (data: 2.86e-04). ETA=7:20:32, max mem: 11.4 GB 
[10/23 18:02:25 visual_prompt]: 	Training 300/553. train loss: 5.6168,	1.9598 s / batch. (data: 1.47e+00). ETA=1 day, 5:02:18, max mem: 11.4 GB 
[10/23 18:03:53 visual_prompt]: 	Training 400/553. train loss: 24.9486,	1.9720 s / batch. (data: 1.48e+00). ETA=1 day, 5:09:49, max mem: 11.4 GB 
[10/23 18:05:26 visual_prompt]: 	Training 500/553. train loss: 27.5489,	4.0440 s / batch. (data: 3.54e+00). ETA=2 days, 11:41:42, max mem: 11.4 GB 
[10/23 18:06:12 visual_prompt]: Epoch 4 / 100: avg data time: 4.19e-01, avg batch time: 0.9131, average train loss: 49.8694
[10/23 18:07:06 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1901, average loss: 39.3894
[10/23 18:07:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.59	
[10/23 18:07:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/23 18:08:39 visual_prompt]: 	Training 100/553. train loss: 0.0001,	0.4801 s / batch. (data: 2.78e-04). ETA=7:03:56, max mem: 11.4 GB 
[10/23 18:10:09 visual_prompt]: 	Training 200/553. train loss: 9.8716,	1.1555 s / batch. (data: 6.44e-01). ETA=16:58:34, max mem: 11.4 GB 
[10/23 18:11:41 visual_prompt]: 	Training 300/553. train loss: 49.5656,	0.5040 s / batch. (data: 2.41e-04). ETA=7:23:25, max mem: 11.4 GB 
[10/23 18:13:18 visual_prompt]: 	Training 400/553. train loss: 71.0345,	0.4840 s / batch. (data: 2.52e-04). ETA=7:05:03, max mem: 11.4 GB 
[10/23 18:14:50 visual_prompt]: 	Training 500/553. train loss: 248.1970,	0.5360 s / batch. (data: 2.43e-04). ETA=7:49:48, max mem: 11.4 GB 
[10/23 18:15:37 visual_prompt]: Epoch 5 / 100: avg data time: 4.30e-01, avg batch time: 0.9247, average train loss: 67.2436
[10/23 18:16:31 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1916, average loss: 112.3346
[10/23 18:16:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.72	
[10/23 18:16:31 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/23 18:18:06 visual_prompt]: 	Training 100/553. train loss: 23.8877,	0.5240 s / batch. (data: 7.11e-04). ETA=7:37:55, max mem: 11.4 GB 
[10/23 18:19:38 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.5082 s / batch. (data: 2.22e-02). ETA=7:23:15, max mem: 11.4 GB 
[10/23 18:21:08 visual_prompt]: 	Training 300/553. train loss: 125.4984,	0.5320 s / batch. (data: 2.19e-04). ETA=7:43:11, max mem: 11.4 GB 
[10/23 18:22:42 visual_prompt]: 	Training 400/553. train loss: 84.6284,	0.5800 s / batch. (data: 9.63e-02). ETA=8:24:00, max mem: 11.4 GB 
[10/23 18:24:12 visual_prompt]: 	Training 500/553. train loss: 114.5862,	1.5494 s / batch. (data: 1.06e+00). ETA=22:23:43, max mem: 11.4 GB 
[10/23 18:24:58 visual_prompt]: Epoch 6 / 100: avg data time: 4.23e-01, avg batch time: 0.9173, average train loss: 93.6537
[10/23 18:25:52 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1937, average loss: 45.2910
[10/23 18:25:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.17	
[10/23 18:25:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/23 18:27:24 visual_prompt]: 	Training 100/553. train loss: 340.5126,	1.0375 s / batch. (data: 5.49e-01). ETA=14:57:06, max mem: 11.4 GB 
[10/23 18:28:55 visual_prompt]: 	Training 200/553. train loss: 51.2697,	0.8206 s / batch. (data: 3.41e-01). ETA=11:48:11, max mem: 11.4 GB 
[10/23 18:30:29 visual_prompt]: 	Training 300/553. train loss: 27.4396,	1.3204 s / batch. (data: 8.29e-01). ETA=18:57:20, max mem: 11.4 GB 
[10/23 18:32:00 visual_prompt]: 	Training 400/553. train loss: 306.8625,	2.5161 s / batch. (data: 2.03e+00). ETA=1 day, 12:03:07, max mem: 11.4 GB 
[10/23 18:33:29 visual_prompt]: 	Training 500/553. train loss: 75.2421,	0.5034 s / batch. (data: 7.95e-03). ETA=7:11:54, max mem: 11.4 GB 
[10/23 18:34:15 visual_prompt]: Epoch 7 / 100: avg data time: 4.16e-01, avg batch time: 0.9095, average train loss: 106.7715
[10/23 18:35:09 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1915, average loss: 136.7675
[10/23 18:35:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.71	
[10/23 18:35:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/23 18:36:40 visual_prompt]: 	Training 100/553. train loss: 21.9908,	0.4880 s / batch. (data: 2.86e-04). ETA=6:57:28, max mem: 11.4 GB 
[10/23 18:38:12 visual_prompt]: 	Training 200/553. train loss: 97.0921,	0.4944 s / batch. (data: 5.42e-03). ETA=7:02:05, max mem: 11.4 GB 
[10/23 18:39:44 visual_prompt]: 	Training 300/553. train loss: 23.6963,	0.5246 s / batch. (data: 1.67e-02). ETA=7:27:01, max mem: 11.4 GB 
[10/23 18:41:15 visual_prompt]: 	Training 400/553. train loss: 316.5627,	1.0099 s / batch. (data: 5.21e-01). ETA=14:18:56, max mem: 11.4 GB 
[10/23 18:42:47 visual_prompt]: 	Training 500/553. train loss: 2.9843,	2.2016 s / batch. (data: 1.72e+00). ETA=1 day, 7:08:43, max mem: 11.4 GB 
[10/23 18:43:32 visual_prompt]: Epoch 8 / 100: avg data time: 4.17e-01, avg batch time: 0.9107, average train loss: 122.7536
[10/23 18:44:38 visual_prompt]: Inference (val):avg data time: 3.61e-04, avg batch time: 0.1906, average loss: 272.4704
[10/23 18:44:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.32	
[10/23 18:44:38 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/23 18:46:12 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.5120 s / batch. (data: 2.72e-04). ETA=7:13:16, max mem: 11.4 GB 
[10/23 18:47:45 visual_prompt]: 	Training 200/553. train loss: 38.6049,	0.5208 s / batch. (data: 3.28e-02). ETA=7:19:52, max mem: 11.4 GB 
[10/23 18:49:15 visual_prompt]: 	Training 300/553. train loss: 34.2913,	1.4689 s / batch. (data: 9.75e-01). ETA=20:38:09, max mem: 11.4 GB 
[10/23 18:50:49 visual_prompt]: 	Training 400/553. train loss: 171.4275,	0.5159 s / batch. (data: 2.64e-04). ETA=7:14:02, max mem: 11.4 GB 
[10/23 18:52:20 visual_prompt]: 	Training 500/553. train loss: 81.1689,	1.0110 s / batch. (data: 5.35e-01). ETA=14:08:51, max mem: 11.4 GB 
[10/23 18:53:05 visual_prompt]: Epoch 9 / 100: avg data time: 4.24e-01, avg batch time: 0.9169, average train loss: 147.4935
[10/23 18:53:58 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1914, average loss: 239.1836
[10/23 18:53:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.92	
[10/23 18:53:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/23 18:55:34 visual_prompt]: 	Training 100/553. train loss: 206.8668,	0.4800 s / batch. (data: 2.78e-04). ETA=6:41:46, max mem: 11.4 GB 
[10/23 18:57:04 visual_prompt]: 	Training 200/553. train loss: 401.0653,	0.4828 s / batch. (data: 2.55e-04). ETA=6:43:18, max mem: 11.4 GB 
[10/23 18:58:34 visual_prompt]: 	Training 300/553. train loss: 11.6500,	0.9770 s / batch. (data: 4.76e-01). ETA=13:34:33, max mem: 11.4 GB 
[10/23 19:00:04 visual_prompt]: 	Training 400/553. train loss: 117.9170,	1.4320 s / batch. (data: 8.99e-01). ETA=19:51:30, max mem: 11.4 GB 
[10/23 19:01:36 visual_prompt]: 	Training 500/553. train loss: 38.9581,	1.8461 s / batch. (data: 1.33e+00). ETA=1 day, 1:33:00, max mem: 11.4 GB 
[10/23 19:02:22 visual_prompt]: Epoch 10 / 100: avg data time: 4.18e-01, avg batch time: 0.9112, average train loss: 138.1613
[10/23 19:03:16 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1913, average loss: 14.8161
[10/23 19:03:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 56.42	
[10/23 19:03:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/23 19:04:55 visual_prompt]: 	Training 100/553. train loss: 104.4702,	0.4932 s / batch. (data: 2.74e-04). ETA=6:48:19, max mem: 11.4 GB 
[10/23 19:06:27 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.4936 s / batch. (data: 7.91e-03). ETA=6:47:50, max mem: 11.4 GB 
[10/23 19:07:58 visual_prompt]: 	Training 300/553. train loss: 455.8028,	2.2654 s / batch. (data: 1.79e+00). ETA=1 day, 7:07:49, max mem: 11.4 GB 
[10/23 19:09:27 visual_prompt]: 	Training 400/553. train loss: 172.7478,	0.5152 s / batch. (data: 1.59e-02). ETA=7:03:56, max mem: 11.4 GB 
[10/23 19:10:57 visual_prompt]: 	Training 500/553. train loss: 105.5214,	0.4952 s / batch. (data: 2.65e-04). ETA=6:46:37, max mem: 11.4 GB 
[10/23 19:11:43 visual_prompt]: Epoch 11 / 100: avg data time: 4.25e-01, avg batch time: 0.9167, average train loss: 172.1445
[10/23 19:12:37 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1918, average loss: 327.1889
[10/23 19:12:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.24	
[10/23 19:12:37 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/23 19:14:12 visual_prompt]: 	Training 100/553. train loss: 41.4395,	0.4830 s / batch. (data: 2.94e-04). ETA=6:35:22, max mem: 11.4 GB 
[10/23 19:15:44 visual_prompt]: 	Training 200/553. train loss: 78.9318,	0.4919 s / batch. (data: 2.88e-04). ETA=6:41:49, max mem: 11.4 GB 
[10/23 19:17:13 visual_prompt]: 	Training 300/553. train loss: 118.8898,	0.4822 s / batch. (data: 2.71e-04). ETA=6:33:06, max mem: 11.4 GB 
[10/23 19:18:44 visual_prompt]: 	Training 400/553. train loss: 9.4556,	0.4811 s / batch. (data: 2.67e-04). ETA=6:31:23, max mem: 11.4 GB 
[10/23 19:20:16 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.5298 s / batch. (data: 2.17e-02). ETA=7:10:09, max mem: 11.4 GB 
[10/23 19:21:01 visual_prompt]: Epoch 12 / 100: avg data time: 4.18e-01, avg batch time: 0.9112, average train loss: 192.9835
[10/23 19:21:54 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1923, average loss: 173.9502
[10/23 19:21:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.91	
[10/23 19:21:54 visual_prompt]: Best epoch 12: best metric: -173.950
[10/23 19:21:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/23 19:23:29 visual_prompt]: 	Training 100/553. train loss: 103.5077,	0.5000 s / batch. (data: 2.88e-04). ETA=6:44:41, max mem: 11.4 GB 
[10/23 19:24:58 visual_prompt]: 	Training 200/553. train loss: 208.7661,	0.4871 s / batch. (data: 1.20e-02). ETA=6:33:24, max mem: 11.4 GB 
[10/23 19:26:29 visual_prompt]: 	Training 300/553. train loss: 126.6659,	1.6920 s / batch. (data: 1.19e+00). ETA=22:43:50, max mem: 11.4 GB 
[10/23 19:28:10 visual_prompt]: 	Training 400/553. train loss: 50.7753,	0.4901 s / batch. (data: 2.88e-04). ETA=6:34:13, max mem: 11.4 GB 
[10/23 19:29:42 visual_prompt]: 	Training 500/553. train loss: 379.2424,	0.4863 s / batch. (data: 2.16e-04). ETA=6:30:20, max mem: 11.4 GB 
[10/23 19:30:29 visual_prompt]: Epoch 13 / 100: avg data time: 4.37e-01, avg batch time: 0.9301, average train loss: 170.4146
[10/23 19:31:23 visual_prompt]: Inference (val):avg data time: 2.76e-04, avg batch time: 0.1910, average loss: 320.4713
[10/23 19:31:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.12	
[10/23 19:31:23 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/23 19:32:56 visual_prompt]: 	Training 100/553. train loss: 354.6418,	0.5005 s / batch. (data: 3.73e-03). ETA=6:40:31, max mem: 11.4 GB 
[10/23 19:34:29 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.9360 s / batch. (data: 1.45e+00). ETA=1 day, 1:45:55, max mem: 11.4 GB 
[10/23 19:35:59 visual_prompt]: 	Training 300/553. train loss: 14.7310,	1.4080 s / batch. (data: 8.91e-01). ETA=18:41:55, max mem: 11.4 GB 
[10/23 19:37:29 visual_prompt]: 	Training 400/553. train loss: 30.9013,	0.5120 s / batch. (data: 7.99e-03). ETA=6:47:07, max mem: 11.4 GB 
[10/23 19:38:59 visual_prompt]: 	Training 500/553. train loss: 251.5918,	0.5040 s / batch. (data: 2.84e-04). ETA=6:39:54, max mem: 11.4 GB 
[10/23 19:39:45 visual_prompt]: Epoch 14 / 100: avg data time: 4.16e-01, avg batch time: 0.9088, average train loss: 185.6612
[10/23 19:40:39 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1924, average loss: 105.9525
[10/23 19:40:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.70	
[10/23 19:40:39 visual_prompt]: Best epoch 14: best metric: -105.953
[10/23 19:40:39 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/23 19:42:13 visual_prompt]: 	Training 100/553. train loss: 473.7285,	0.5270 s / batch. (data: 1.08e-02). ETA=6:56:50, max mem: 11.4 GB 
[10/23 19:43:42 visual_prompt]: 	Training 200/553. train loss: 545.8245,	0.5121 s / batch. (data: 2.21e-04). ETA=6:44:11, max mem: 11.4 GB 
[10/23 19:45:14 visual_prompt]: 	Training 300/553. train loss: 179.3405,	0.4961 s / batch. (data: 3.13e-03). ETA=6:30:42, max mem: 11.4 GB 
[10/23 19:46:42 visual_prompt]: 	Training 400/553. train loss: 426.8895,	0.4911 s / batch. (data: 2.80e-04). ETA=6:25:59, max mem: 11.4 GB 
[10/23 19:48:14 visual_prompt]: 	Training 500/553. train loss: 124.7463,	0.5409 s / batch. (data: 5.39e-03). ETA=7:04:11, max mem: 11.4 GB 
[10/23 19:49:01 visual_prompt]: Epoch 15 / 100: avg data time: 4.15e-01, avg batch time: 0.9088, average train loss: 187.6236
[10/23 19:49:55 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1922, average loss: 192.1853
[10/23 19:49:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.51	
[10/23 19:49:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/23 19:51:27 visual_prompt]: 	Training 100/553. train loss: 209.7523,	0.4880 s / batch. (data: 2.68e-04). ETA=6:21:28, max mem: 11.4 GB 
[10/23 19:52:59 visual_prompt]: 	Training 200/553. train loss: 602.2291,	0.5051 s / batch. (data: 1.06e-02). ETA=6:34:02, max mem: 11.4 GB 
[10/23 19:54:30 visual_prompt]: 	Training 300/553. train loss: 264.7664,	0.5119 s / batch. (data: 1.05e-03). ETA=6:38:28, max mem: 11.4 GB 
[10/23 19:56:00 visual_prompt]: 	Training 400/553. train loss: 101.0584,	0.4860 s / batch. (data: 2.54e-04). ETA=6:17:31, max mem: 11.4 GB 
[10/23 19:57:30 visual_prompt]: 	Training 500/553. train loss: 329.3840,	1.8466 s / batch. (data: 1.37e+00). ETA=23:51:15, max mem: 11.4 GB 
[10/23 19:58:17 visual_prompt]: Epoch 16 / 100: avg data time: 4.15e-01, avg batch time: 0.9076, average train loss: 195.4497
[10/23 19:59:10 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1910, average loss: 75.8560
[10/23 19:59:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.62	
[10/23 19:59:10 visual_prompt]: Best epoch 16: best metric: -75.856
[10/23 19:59:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/23 20:00:47 visual_prompt]: 	Training 100/553. train loss: 67.8591,	0.4783 s / batch. (data: 2.48e-04). ETA=6:09:31, max mem: 11.4 GB 
[10/23 20:02:20 visual_prompt]: 	Training 200/553. train loss: 49.8137,	0.4928 s / batch. (data: 2.89e-04). ETA=6:19:54, max mem: 11.4 GB 
[10/23 20:03:50 visual_prompt]: 	Training 300/553. train loss: 64.1356,	0.4768 s / batch. (data: 2.55e-04). ETA=6:06:45, max mem: 11.4 GB 
[10/23 20:05:20 visual_prompt]: 	Training 400/553. train loss: 49.0948,	0.5039 s / batch. (data: 5.00e-04). ETA=6:26:46, max mem: 11.4 GB 
[10/23 20:06:50 visual_prompt]: 	Training 500/553. train loss: 76.5268,	2.2440 s / batch. (data: 1.75e+00). ETA=1 day, 4:38:37, max mem: 11.4 GB 
[10/23 20:07:37 visual_prompt]: Epoch 17 / 100: avg data time: 4.23e-01, avg batch time: 0.9160, average train loss: 148.1922
[10/23 20:08:30 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1908, average loss: 298.3003
[10/23 20:08:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.31	
[10/23 20:08:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/23 20:10:05 visual_prompt]: 	Training 100/553. train loss: 249.6975,	0.5052 s / batch. (data: 2.05e-02). ETA=6:25:37, max mem: 11.4 GB 
[10/23 20:11:37 visual_prompt]: 	Training 200/553. train loss: 43.5750,	0.4880 s / batch. (data: 2.84e-04). ETA=6:11:38, max mem: 11.4 GB 
[10/23 20:13:08 visual_prompt]: 	Training 300/553. train loss: 204.9143,	0.4960 s / batch. (data: 2.22e-04). ETA=6:16:58, max mem: 11.4 GB 
[10/23 20:14:38 visual_prompt]: 	Training 400/553. train loss: 40.1228,	0.4800 s / batch. (data: 2.82e-04). ETA=6:03:59, max mem: 11.4 GB 
[10/23 20:16:09 visual_prompt]: 	Training 500/553. train loss: 77.1152,	0.5049 s / batch. (data: 1.04e-02). ETA=6:22:02, max mem: 11.4 GB 
[10/23 20:16:54 visual_prompt]: Epoch 18 / 100: avg data time: 4.16e-01, avg batch time: 0.9099, average train loss: 156.7787
[10/23 20:17:47 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1920, average loss: 10.0945
[10/23 20:17:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 54.31	
[10/23 20:17:47 visual_prompt]: Best epoch 18: best metric: -10.094
[10/23 20:17:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[10/23 20:19:21 visual_prompt]: 	Training 100/553. train loss: 90.6569,	0.5040 s / batch. (data: 2.23e-04). ETA=6:20:05, max mem: 11.4 GB 
[10/23 20:20:52 visual_prompt]: 	Training 200/553. train loss: 226.4519,	0.4758 s / batch. (data: 2.67e-04). ETA=5:58:02, max mem: 11.4 GB 
[10/23 20:22:23 visual_prompt]: 	Training 300/553. train loss: 149.5363,	0.5022 s / batch. (data: 2.64e-04). ETA=6:17:02, max mem: 11.4 GB 
[10/23 20:23:55 visual_prompt]: 	Training 400/553. train loss: 0.0057,	0.5000 s / batch. (data: 6.73e-04). ETA=6:14:33, max mem: 11.4 GB 
[10/23 20:25:22 visual_prompt]: 	Training 500/553. train loss: 549.4352,	0.4952 s / batch. (data: 2.73e-04). ETA=6:10:09, max mem: 11.4 GB 
[10/23 20:26:09 visual_prompt]: Epoch 19 / 100: avg data time: 4.15e-01, avg batch time: 0.9075, average train loss: 154.7703
[10/23 20:27:03 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1923, average loss: 56.4449
[10/23 20:27:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.39	
[10/23 20:27:03 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[10/23 20:28:34 visual_prompt]: 	Training 100/553. train loss: 75.1056,	0.9154 s / batch. (data: 4.30e-01). ETA=11:21:52, max mem: 11.4 GB 
[10/23 20:30:07 visual_prompt]: 	Training 200/553. train loss: 14.5995,	0.5000 s / batch. (data: 2.28e-04). ETA=6:11:38, max mem: 11.4 GB 
[10/23 20:31:37 visual_prompt]: 	Training 300/553. train loss: 74.1048,	0.5000 s / batch. (data: 2.79e-04). ETA=6:10:44, max mem: 11.4 GB 
[10/23 20:33:08 visual_prompt]: 	Training 400/553. train loss: 122.9810,	0.4880 s / batch. (data: 3.71e-04). ETA=6:01:02, max mem: 11.4 GB 
[10/23 20:34:37 visual_prompt]: 	Training 500/553. train loss: 301.5468,	0.5280 s / batch. (data: 2.35e-04). ETA=6:29:48, max mem: 11.4 GB 
[10/23 20:35:25 visual_prompt]: Epoch 20 / 100: avg data time: 4.16e-01, avg batch time: 0.9091, average train loss: 161.9391
[10/23 20:36:19 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1894, average loss: 42.4982
[10/23 20:36:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 47.75	
[10/23 20:36:19 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[10/23 20:37:54 visual_prompt]: 	Training 100/553. train loss: 64.2322,	0.5624 s / batch. (data: 6.86e-02). ETA=6:53:42, max mem: 11.4 GB 
[10/23 20:39:24 visual_prompt]: 	Training 200/553. train loss: 200.4295,	0.4936 s / batch. (data: 5.37e-03). ETA=6:02:16, max mem: 11.4 GB 
[10/23 20:40:55 visual_prompt]: 	Training 300/553. train loss: 204.2472,	1.6320 s / batch. (data: 1.13e+00). ETA=19:55:09, max mem: 11.4 GB 
[10/23 20:42:24 visual_prompt]: 	Training 400/553. train loss: 329.7251,	0.4928 s / batch. (data: 1.04e-02). ETA=6:00:04, max mem: 11.4 GB 
[10/23 20:43:56 visual_prompt]: 	Training 500/553. train loss: 373.0549,	0.4880 s / batch. (data: 1.20e-02). ETA=5:55:45, max mem: 11.4 GB 
[10/23 20:44:41 visual_prompt]: Epoch 21 / 100: avg data time: 4.15e-01, avg batch time: 0.9082, average train loss: 154.5615
[10/23 20:45:35 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1925, average loss: 62.4301
[10/23 20:45:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.88	
[10/23 20:45:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[10/23 20:47:07 visual_prompt]: 	Training 100/553. train loss: 10.7979,	0.5120 s / batch. (data: 7.98e-03). ETA=6:11:58, max mem: 11.4 GB 
[10/23 20:48:38 visual_prompt]: 	Training 200/553. train loss: 914.3804,	0.5069 s / batch. (data: 5.39e-03). ETA=6:07:23, max mem: 11.4 GB 
[10/23 20:50:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.4837 s / batch. (data: 5.41e-03). ETA=5:49:47, max mem: 11.4 GB 
[10/23 20:51:38 visual_prompt]: 	Training 400/553. train loss: 30.6346,	0.4889 s / batch. (data: 2.85e-04). ETA=5:52:43, max mem: 11.4 GB 
[10/23 20:53:08 visual_prompt]: 	Training 500/553. train loss: 189.3260,	0.4840 s / batch. (data: 2.81e-04). ETA=5:48:22, max mem: 11.4 GB 
[10/23 20:53:57 visual_prompt]: Epoch 22 / 100: avg data time: 4.14e-01, avg batch time: 0.9077, average train loss: 164.5143
[10/23 20:54:50 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1921, average loss: 67.3277
[10/23 20:54:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.51	
[10/23 20:54:50 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[10/23 20:56:25 visual_prompt]: 	Training 100/553. train loss: 140.5399,	0.4861 s / batch. (data: 2.14e-04). ETA=5:48:37, max mem: 11.4 GB 
[10/23 20:57:56 visual_prompt]: 	Training 200/553. train loss: 256.7495,	1.0908 s / batch. (data: 6.02e-01). ETA=13:00:30, max mem: 11.4 GB 
[10/23 20:59:28 visual_prompt]: 	Training 300/553. train loss: 163.5918,	0.5137 s / batch. (data: 1.04e-02). ETA=6:06:45, max mem: 11.4 GB 
[10/23 21:00:57 visual_prompt]: 	Training 400/553. train loss: 52.2091,	0.4746 s / batch. (data: 4.42e-04). ETA=5:38:00, max mem: 11.4 GB 
[10/23 21:02:30 visual_prompt]: 	Training 500/553. train loss: 7.4214,	0.5096 s / batch. (data: 5.38e-03). ETA=6:02:05, max mem: 11.4 GB 
[10/23 21:03:17 visual_prompt]: Epoch 23 / 100: avg data time: 4.24e-01, avg batch time: 0.9165, average train loss: 182.8315
[10/23 21:04:11 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1911, average loss: 114.1531
[10/23 21:04:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.43	
[10/23 21:04:11 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[10/23 21:05:42 visual_prompt]: 	Training 100/553. train loss: 44.6604,	0.5108 s / batch. (data: 2.07e-02). ETA=6:01:37, max mem: 11.4 GB 
[10/23 21:07:14 visual_prompt]: 	Training 200/553. train loss: 146.3458,	0.5110 s / batch. (data: 8.85e-03). ETA=6:00:55, max mem: 11.4 GB 
[10/23 21:08:44 visual_prompt]: 	Training 300/553. train loss: 139.6820,	1.5320 s / batch. (data: 1.03e+00). ETA=17:59:35, max mem: 11.4 GB 
[10/23 21:10:17 visual_prompt]: 	Training 400/553. train loss: 20.8986,	0.4928 s / batch. (data: 2.55e-04). ETA=5:46:28, max mem: 11.4 GB 
[10/23 21:11:52 visual_prompt]: 	Training 500/553. train loss: 501.0214,	3.4094 s / batch. (data: 2.92e+00). ETA=1 day, 15:51:10, max mem: 11.4 GB 
[10/23 21:12:37 visual_prompt]: Epoch 24 / 100: avg data time: 4.21e-01, avg batch time: 0.9145, average train loss: 167.3474
[10/23 21:13:31 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1898, average loss: 104.9937
[10/23 21:13:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.08	
[10/23 21:13:31 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[10/23 21:15:08 visual_prompt]: 	Training 100/553. train loss: 78.4694,	0.5000 s / batch. (data: 2.79e-04). ETA=5:49:22, max mem: 11.4 GB 
[10/23 21:16:36 visual_prompt]: 	Training 200/553. train loss: 142.4663,	0.5024 s / batch. (data: 1.05e-02). ETA=5:50:14, max mem: 11.4 GB 
[10/23 21:18:05 visual_prompt]: 	Training 300/553. train loss: 281.0510,	0.5000 s / batch. (data: 5.41e-03). ETA=5:47:44, max mem: 11.4 GB 
[10/23 21:19:37 visual_prompt]: 	Training 400/553. train loss: 97.8225,	2.0260 s / batch. (data: 1.54e+00). ETA=23:25:37, max mem: 11.4 GB 
[10/23 21:21:08 visual_prompt]: 	Training 500/553. train loss: 37.0374,	2.1755 s / batch. (data: 1.68e+00). ETA=1 day, 1:05:44, max mem: 11.4 GB 
[10/23 21:21:54 visual_prompt]: Epoch 25 / 100: avg data time: 4.17e-01, avg batch time: 0.9103, average train loss: 145.4787
[10/23 21:22:48 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1920, average loss: 27.5519
[10/23 21:22:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.24	
[10/23 21:22:48 visual_prompt]: Stopping early.
[10/23 21:22:48 visual_prompt]: Rank of current process: 0. World size: 1
[10/23 21:22:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/23 21:22:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/23 21:22:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/23 21:22:48 visual_prompt]: Training with config:
[10/23 21:22:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr50.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/23 21:22:48 visual_prompt]: Loading training data...
[10/23 21:22:48 visual_prompt]: Constructing mammo-cbis dataset train...
[10/23 21:22:48 visual_prompt]: Loading validation data...
[10/23 21:22:48 visual_prompt]: Constructing mammo-cbis dataset val...
[10/23 21:22:48 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/23 21:22:51 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/23 21:22:51 visual_prompt]: tuned percent:0.529
[10/23 21:22:51 visual_prompt]: Device used for model: 0
[10/23 21:22:51 visual_prompt]: Setting up Evaluator...
[10/23 21:22:51 visual_prompt]: Setting up Trainer...
[10/23 21:22:51 visual_prompt]: 	Setting up the optimizer...
[10/23 21:22:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/23 21:24:24 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4960 s / batch. (data: 7.97e-03). ETA=7:36:20, max mem: 11.4 GB 
[10/23 21:25:52 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5021 s / batch. (data: 2.58e-04). ETA=7:41:05, max mem: 11.4 GB 
[10/23 21:27:27 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.0570 s / batch. (data: 2.56e+00). ETA=1 day, 22:42:17, max mem: 11.4 GB 
[10/23 21:28:54 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4914 s / batch. (data: 9.85e-03). ETA=7:29:35, max mem: 11.4 GB 
[10/23 21:30:27 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5129 s / batch. (data: 7.46e-04). ETA=7:48:25, max mem: 11.4 GB 
[10/23 21:31:14 visual_prompt]: Epoch 1 / 100: avg data time: 4.12e-01, avg batch time: 0.9100, average train loss: 1.3966
[10/23 21:32:14 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1900, average loss: 1.3454
[10/23 21:32:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/23 21:32:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/23 21:33:59 visual_prompt]: 	Training 100/553. train loss: 11.2174,	0.8616 s / batch. (data: 3.74e-01). ETA=13:04:43, max mem: 11.4 GB 
[10/23 21:35:39 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.9880 s / batch. (data: 1.50e+00). ETA=1 day, 6:07:18, max mem: 11.4 GB 
[10/23 21:37:20 visual_prompt]: 	Training 300/553. train loss: 6.7924,	2.0396 s / batch. (data: 1.55e+00). ETA=1 day, 6:50:49, max mem: 11.4 GB 
[10/23 21:38:54 visual_prompt]: 	Training 400/553. train loss: 21.6601,	0.8948 s / batch. (data: 4.05e-01). ETA=13:30:29, max mem: 11.4 GB 
[10/23 21:40:33 visual_prompt]: 	Training 500/553. train loss: 3.1162,	0.4991 s / batch. (data: 7.96e-03). ETA=7:31:13, max mem: 11.4 GB 
[10/23 21:41:19 visual_prompt]: Epoch 2 / 100: avg data time: 4.90e-01, avg batch time: 0.9846, average train loss: 18.8621
[10/23 21:42:12 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1915, average loss: 38.8424
[10/23 21:42:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.25	
[10/23 21:42:12 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/23 21:43:44 visual_prompt]: 	Training 100/553. train loss: 6.0756,	1.0796 s / batch. (data: 5.95e-01). ETA=16:13:21, max mem: 11.4 GB 
[10/23 21:45:16 visual_prompt]: 	Training 200/553. train loss: 55.6808,	0.4914 s / batch. (data: 3.19e-04). ETA=7:22:14, max mem: 11.4 GB 
[10/23 21:46:46 visual_prompt]: 	Training 300/553. train loss: 1.0125,	0.4960 s / batch. (data: 2.92e-04). ETA=7:25:31, max mem: 11.4 GB 
[10/23 21:48:18 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.5000 s / batch. (data: 2.84e-04). ETA=7:28:16, max mem: 11.4 GB 
[10/23 21:49:50 visual_prompt]: 	Training 500/553. train loss: 26.0339,	1.8566 s / batch. (data: 1.38e+00). ETA=1 day, 3:41:29, max mem: 11.4 GB 
[10/23 21:50:35 visual_prompt]: Epoch 3 / 100: avg data time: 4.16e-01, avg batch time: 0.9092, average train loss: 31.7978
[10/23 21:51:29 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1917, average loss: 11.9392
[10/23 21:51:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.25	
[10/23 21:51:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/23 21:53:04 visual_prompt]: 	Training 100/553. train loss: 56.7022,	0.4760 s / batch. (data: 2.80e-04). ETA=7:04:46, max mem: 11.4 GB 
[10/23 21:54:35 visual_prompt]: 	Training 200/553. train loss: 76.3868,	0.4848 s / batch. (data: 3.64e-04). ETA=7:11:47, max mem: 11.4 GB 
[10/23 21:56:06 visual_prompt]: 	Training 300/553. train loss: 7.9879,	2.1084 s / batch. (data: 1.62e+00). ETA=1 day, 7:14:25, max mem: 11.4 GB 
[10/23 21:57:33 visual_prompt]: 	Training 400/553. train loss: 41.2927,	2.0177 s / batch. (data: 1.53e+00). ETA=1 day, 5:50:21, max mem: 11.4 GB 
[10/23 21:59:05 visual_prompt]: 	Training 500/553. train loss: 0.4504,	3.4240 s / batch. (data: 2.94e+00). ETA=2 days, 2:32:33, max mem: 11.4 GB 
[10/23 21:59:52 visual_prompt]: Epoch 4 / 100: avg data time: 4.18e-01, avg batch time: 0.9095, average train loss: 51.3522
[10/23 22:00:45 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.1908, average loss: 35.8631
[10/23 22:00:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.10	
[10/23 22:00:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/23 22:02:18 visual_prompt]: 	Training 100/553. train loss: 48.9579,	0.5160 s / batch. (data: 2.74e-04). ETA=7:35:41, max mem: 11.4 GB 
[10/23 22:03:48 visual_prompt]: 	Training 200/553. train loss: 165.6014,	1.7541 s / batch. (data: 1.28e+00). ETA=1 day, 1:46:10, max mem: 11.4 GB 
[10/23 22:05:19 visual_prompt]: 	Training 300/553. train loss: 35.1674,	0.5120 s / batch. (data: 2.51e-04). ETA=7:30:28, max mem: 11.4 GB 
[10/23 22:06:55 visual_prompt]: 	Training 400/553. train loss: 60.2583,	0.4960 s / batch. (data: 2.57e-04). ETA=7:15:33, max mem: 11.4 GB 
[10/23 22:08:26 visual_prompt]: 	Training 500/553. train loss: 35.9558,	0.4980 s / batch. (data: 1.05e-02). ETA=7:16:28, max mem: 11.4 GB 
[10/23 22:09:13 visual_prompt]: Epoch 5 / 100: avg data time: 4.26e-01, avg batch time: 0.9176, average train loss: 71.2429
[10/23 22:10:08 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1910, average loss: 240.4862
[10/23 22:10:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.02	
[10/23 22:10:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/23 22:11:43 visual_prompt]: 	Training 100/553. train loss: 59.7895,	0.4960 s / batch. (data: 2.81e-04). ETA=7:13:27, max mem: 11.4 GB 
[10/23 22:13:13 visual_prompt]: 	Training 200/553. train loss: 242.4070,	0.5006 s / batch. (data: 7.96e-03). ETA=7:16:37, max mem: 11.4 GB 
[10/23 22:14:41 visual_prompt]: 	Training 300/553. train loss: 384.1437,	0.4856 s / batch. (data: 2.65e-04). ETA=7:02:47, max mem: 11.4 GB 
[10/23 22:16:16 visual_prompt]: 	Training 400/553. train loss: 9.5753,	1.0845 s / batch. (data: 5.70e-01). ETA=15:42:21, max mem: 11.4 GB 
[10/23 22:17:45 visual_prompt]: 	Training 500/553. train loss: 118.0835,	1.4280 s / batch. (data: 9.25e-01). ETA=20:38:25, max mem: 11.4 GB 
[10/23 22:18:31 visual_prompt]: Epoch 6 / 100: avg data time: 4.19e-01, avg batch time: 0.9094, average train loss: 100.4728
[10/23 22:19:25 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1926, average loss: 164.7630
[10/23 22:19:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.61	
[10/23 22:19:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/23 22:20:58 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4816 s / batch. (data: 2.79e-04). ETA=6:56:26, max mem: 11.4 GB 
[10/23 22:22:28 visual_prompt]: 	Training 200/553. train loss: 26.5846,	0.4970 s / batch. (data: 1.30e-02). ETA=7:08:56, max mem: 11.4 GB 
[10/23 22:24:03 visual_prompt]: 	Training 300/553. train loss: 32.6641,	2.6940 s / batch. (data: 2.21e+00). ETA=1 day, 14:40:31, max mem: 11.4 GB 
[10/23 22:25:33 visual_prompt]: 	Training 400/553. train loss: 41.3495,	2.3434 s / batch. (data: 1.86e+00). ETA=1 day, 9:34:38, max mem: 11.4 GB 
[10/23 22:27:01 visual_prompt]: 	Training 500/553. train loss: 192.6000,	0.4971 s / batch. (data: 3.04e-04). ETA=7:06:32, max mem: 11.4 GB 
[10/23 22:27:47 visual_prompt]: Epoch 7 / 100: avg data time: 4.17e-01, avg batch time: 0.9077, average train loss: 114.1238
[10/23 22:28:40 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1913, average loss: 4.1218
[10/23 22:28:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 48.35	
[10/23 22:28:40 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/23 22:30:11 visual_prompt]: 	Training 100/553. train loss: 208.6514,	0.5040 s / batch. (data: 2.84e-04). ETA=7:11:10, max mem: 11.4 GB 
[10/23 22:31:43 visual_prompt]: 	Training 200/553. train loss: 357.2567,	0.5087 s / batch. (data: 5.41e-03). ETA=7:14:20, max mem: 11.4 GB 
[10/23 22:33:15 visual_prompt]: 	Training 300/553. train loss: 32.3568,	0.4920 s / batch. (data: 2.70e-04). ETA=6:59:17, max mem: 11.4 GB 
[10/23 22:34:46 visual_prompt]: 	Training 400/553. train loss: 183.3047,	1.5843 s / batch. (data: 1.07e+00). ETA=22:27:23, max mem: 11.4 GB 
[10/23 22:36:17 visual_prompt]: 	Training 500/553. train loss: 13.9935,	2.0607 s / batch. (data: 1.55e+00). ETA=1 day, 5:09:09, max mem: 11.4 GB 
[10/23 22:37:03 visual_prompt]: Epoch 8 / 100: avg data time: 4.17e-01, avg batch time: 0.9087, average train loss: 115.3357
[10/23 22:37:56 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1925, average loss: 139.6530
[10/23 22:37:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.58	
[10/23 22:37:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/23 22:39:30 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.5000 s / batch. (data: 2.62e-04). ETA=7:03:07, max mem: 11.4 GB 
[10/23 22:41:00 visual_prompt]: 	Training 200/553. train loss: 308.5415,	0.4960 s / batch. (data: 2.97e-04). ETA=6:58:53, max mem: 11.4 GB 
[10/23 22:42:31 visual_prompt]: 	Training 300/553. train loss: 39.9280,	2.1120 s / batch. (data: 1.61e+00). ETA=1 day, 5:40:14, max mem: 11.4 GB 
[10/23 22:44:01 visual_prompt]: 	Training 400/553. train loss: 90.2759,	0.5037 s / batch. (data: 5.37e-03). ETA=7:03:42, max mem: 11.4 GB 
[10/23 22:45:32 visual_prompt]: 	Training 500/553. train loss: 36.4336,	0.5000 s / batch. (data: 2.79e-04). ETA=6:59:48, max mem: 11.4 GB 
[10/23 22:46:17 visual_prompt]: Epoch 9 / 100: avg data time: 4.15e-01, avg batch time: 0.9060, average train loss: 140.6996
[10/23 22:47:10 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.1913, average loss: 87.6665
[10/23 22:47:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.06	
[10/23 22:47:10 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/23 22:48:45 visual_prompt]: 	Training 100/553. train loss: 237.9947,	0.4943 s / batch. (data: 3.03e-04). ETA=6:53:45, max mem: 11.4 GB 
[10/23 22:50:14 visual_prompt]: 	Training 200/553. train loss: 29.7222,	0.4778 s / batch. (data: 2.65e-04). ETA=6:39:07, max mem: 11.4 GB 
[10/23 22:51:44 visual_prompt]: 	Training 300/553. train loss: 139.9897,	0.5042 s / batch. (data: 5.40e-03). ETA=7:00:23, max mem: 11.4 GB 
[10/23 22:53:12 visual_prompt]: 	Training 400/553. train loss: 145.4914,	0.9800 s / batch. (data: 4.76e-01). ETA=13:35:24, max mem: 11.4 GB 
[10/23 22:54:45 visual_prompt]: 	Training 500/553. train loss: 261.8989,	0.7094 s / batch. (data: 2.08e-01). ETA=9:49:04, max mem: 11.4 GB 
[10/23 22:55:31 visual_prompt]: Epoch 10 / 100: avg data time: 4.14e-01, avg batch time: 0.9050, average train loss: 153.4863
[10/23 22:56:24 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.1907, average loss: 140.1566
[10/23 22:56:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.84	
[10/23 22:56:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/23 22:58:01 visual_prompt]: 	Training 100/553. train loss: 222.7859,	0.5040 s / batch. (data: 2.93e-04). ETA=6:57:15, max mem: 11.4 GB 
[10/23 22:59:33 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.4760 s / batch. (data: 2.85e-04). ETA=6:33:14, max mem: 11.4 GB 
[10/23 23:01:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.7200 s / batch. (data: 2.21e+00). ETA=1 day, 13:22:38, max mem: 11.4 GB 
[10/23 23:02:36 visual_prompt]: 	Training 400/553. train loss: 93.8586,	0.5120 s / batch. (data: 2.57e-04). ETA=7:01:18, max mem: 11.4 GB 
[10/23 23:04:05 visual_prompt]: 	Training 500/553. train loss: 91.6328,	0.5240 s / batch. (data: 7.98e-03). ETA=7:10:18, max mem: 11.4 GB 
[10/23 23:04:51 visual_prompt]: Epoch 11 / 100: avg data time: 4.25e-01, avg batch time: 0.9158, average train loss: 150.9936
[10/23 23:05:44 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1900, average loss: 219.4165
[10/23 23:05:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.53	
[10/23 23:05:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/23 23:07:18 visual_prompt]: 	Training 100/553. train loss: 128.9657,	0.5301 s / batch. (data: 3.06e-04). ETA=7:13:58, max mem: 11.4 GB 
[10/23 23:08:48 visual_prompt]: 	Training 200/553. train loss: 276.4499,	0.5400 s / batch. (data: 2.61e-04). ETA=7:21:10, max mem: 11.4 GB 
[10/23 23:10:17 visual_prompt]: 	Training 300/553. train loss: 224.7011,	0.5182 s / batch. (data: 5.39e-03). ETA=7:02:27, max mem: 11.4 GB 
[10/23 23:11:47 visual_prompt]: 	Training 400/553. train loss: 74.6943,	0.4805 s / batch. (data: 2.82e-04). ETA=6:30:57, max mem: 11.4 GB 
[10/23 23:13:18 visual_prompt]: 	Training 500/553. train loss: 465.2473,	0.5241 s / batch. (data: 2.78e-04). ETA=7:05:31, max mem: 11.4 GB 
[10/23 23:14:02 visual_prompt]: Epoch 12 / 100: avg data time: 4.11e-01, avg batch time: 0.9015, average train loss: 171.4333
[10/23 23:14:55 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1908, average loss: 117.7547
[10/23 23:14:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.89	
[10/23 23:14:55 visual_prompt]: Best epoch 12: best metric: -117.755
[10/23 23:14:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/23 23:16:29 visual_prompt]: 	Training 100/553. train loss: 52.3742,	0.4960 s / batch. (data: 3.00e-04). ETA=6:41:26, max mem: 11.4 GB 
[10/23 23:17:57 visual_prompt]: 	Training 200/553. train loss: 61.4656,	0.4997 s / batch. (data: 2.62e-04). ETA=6:43:38, max mem: 11.4 GB 
[10/23 23:19:28 visual_prompt]: 	Training 300/553. train loss: 35.7074,	1.6440 s / batch. (data: 1.15e+00). ETA=22:05:08, max mem: 11.4 GB 
[10/23 23:20:57 visual_prompt]: 	Training 400/553. train loss: 339.1998,	0.4960 s / batch. (data: 3.18e-04). ETA=6:38:57, max mem: 11.4 GB 
[10/23 23:22:29 visual_prompt]: 	Training 500/553. train loss: 142.4548,	0.4745 s / batch. (data: 2.67e-04). ETA=6:20:52, max mem: 11.4 GB 
[10/23 23:23:15 visual_prompt]: Epoch 13 / 100: avg data time: 4.13e-01, avg batch time: 0.9040, average train loss: 171.7825
[10/23 23:24:09 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1899, average loss: 352.1291
[10/23 23:24:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.83	
[10/23 23:24:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/23 23:25:43 visual_prompt]: 	Training 100/553. train loss: 88.3381,	0.4938 s / batch. (data: 1.55e-02). ETA=6:35:05, max mem: 11.4 GB 
[10/23 23:27:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.6077 s / batch. (data: 1.13e+00). ETA=21:23:47, max mem: 11.4 GB 
[10/23 23:28:45 visual_prompt]: 	Training 300/553. train loss: 151.3968,	1.3971 s / batch. (data: 9.02e-01). ETA=18:33:16, max mem: 11.4 GB 
[10/23 23:30:13 visual_prompt]: 	Training 400/553. train loss: 74.7917,	0.5000 s / batch. (data: 2.74e-04). ETA=6:37:34, max mem: 11.4 GB 
[10/23 23:31:44 visual_prompt]: 	Training 500/553. train loss: 448.9344,	0.4999 s / batch. (data: 1.55e-02). ETA=6:36:40, max mem: 11.4 GB 
[10/23 23:32:29 visual_prompt]: Epoch 14 / 100: avg data time: 4.14e-01, avg batch time: 0.9037, average train loss: 174.3331
[10/23 23:33:22 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1913, average loss: 18.0623
[10/23 23:33:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.03	
[10/23 23:33:22 visual_prompt]: Best epoch 14: best metric: -18.062
[10/23 23:33:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/23 23:34:55 visual_prompt]: 	Training 100/553. train loss: 138.2129,	0.4960 s / batch. (data: 2.87e-04). ETA=6:32:18, max mem: 11.4 GB 
[10/23 23:36:23 visual_prompt]: 	Training 200/553. train loss: 475.9438,	0.5122 s / batch. (data: 1.55e-02). ETA=6:44:15, max mem: 11.4 GB 
[10/23 23:37:55 visual_prompt]: 	Training 300/553. train loss: 248.1727,	0.4880 s / batch. (data: 2.57e-04). ETA=6:24:22, max mem: 11.4 GB 
[10/23 23:39:23 visual_prompt]: 	Training 400/553. train loss: 83.6471,	0.4800 s / batch. (data: 2.62e-04). ETA=6:17:14, max mem: 11.4 GB 
[10/23 23:40:54 visual_prompt]: 	Training 500/553. train loss: 178.4578,	0.5240 s / batch. (data: 2.91e-04). ETA=6:50:59, max mem: 11.4 GB 
[10/23 23:41:41 visual_prompt]: Epoch 15 / 100: avg data time: 4.11e-01, avg batch time: 0.9025, average train loss: 156.8707
[10/23 23:42:35 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1922, average loss: 54.9077
[10/23 23:42:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.19	
[10/23 23:42:35 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/23 23:44:07 visual_prompt]: 	Training 100/553. train loss: 193.5837,	0.5136 s / batch. (data: 2.92e-04). ETA=6:41:29, max mem: 11.4 GB 
[10/23 23:45:37 visual_prompt]: 	Training 200/553. train loss: 125.8806,	0.4922 s / batch. (data: 3.98e-03). ETA=6:23:55, max mem: 11.4 GB 
[10/23 23:47:07 visual_prompt]: 	Training 300/553. train loss: 71.4460,	0.4800 s / batch. (data: 2.52e-04). ETA=6:13:39, max mem: 11.4 GB 
[10/23 23:48:37 visual_prompt]: 	Training 400/553. train loss: 106.2476,	0.4877 s / batch. (data: 1.39e-02). ETA=6:18:47, max mem: 11.4 GB 
[10/23 23:50:10 visual_prompt]: 	Training 500/553. train loss: 184.4701,	2.1004 s / batch. (data: 1.62e+00). ETA=1 day, 3:07:57, max mem: 11.4 GB 
[10/23 23:50:56 visual_prompt]: Epoch 16 / 100: avg data time: 4.15e-01, avg batch time: 0.9066, average train loss: 172.1332
[10/23 23:51:55 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1903, average loss: 7.0404
[10/23 23:51:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.33	
[10/23 23:51:55 visual_prompt]: Best epoch 16: best metric: -7.040
[10/23 23:51:55 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/23 23:53:26 visual_prompt]: 	Training 100/553. train loss: 57.4455,	0.5000 s / batch. (data: 2.80e-04). ETA=6:26:16, max mem: 11.4 GB 
[10/23 23:54:58 visual_prompt]: 	Training 200/553. train loss: 81.1776,	0.4981 s / batch. (data: 1.20e-02). ETA=6:23:58, max mem: 11.4 GB 
[10/23 23:56:28 visual_prompt]: 	Training 300/553. train loss: 436.6927,	0.5040 s / batch. (data: 2.66e-04). ETA=6:27:40, max mem: 11.4 GB 
[10/23 23:57:57 visual_prompt]: 	Training 400/553. train loss: 110.2957,	1.2880 s / batch. (data: 7.89e-01). ETA=16:28:36, max mem: 11.4 GB 
[10/23 23:59:27 visual_prompt]: 	Training 500/553. train loss: 50.2364,	2.3104 s / batch. (data: 1.83e+00). ETA=1 day, 5:29:28, max mem: 11.4 GB 
[10/24 00:00:13 visual_prompt]: Epoch 17 / 100: avg data time: 4.09e-01, avg batch time: 0.9005, average train loss: 159.9819
[10/24 00:01:06 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1926, average loss: 101.5697
[10/24 00:01:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.22	
[10/24 00:01:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/24 00:02:51 visual_prompt]: 	Training 100/553. train loss: 49.3088,	0.4927 s / batch. (data: 5.01e-03). ETA=6:16:05, max mem: 11.4 GB 
[10/24 00:04:34 visual_prompt]: 	Training 200/553. train loss: 80.0236,	0.4835 s / batch. (data: 3.02e-04). ETA=6:08:16, max mem: 11.4 GB 
[10/24 00:06:07 visual_prompt]: 	Training 300/553. train loss: 191.9007,	0.4832 s / batch. (data: 2.60e-04). ETA=6:07:12, max mem: 11.4 GB 
[10/24 00:07:40 visual_prompt]: 	Training 400/553. train loss: 16.6925,	0.4952 s / batch. (data: 2.86e-04). ETA=6:15:32, max mem: 11.4 GB 
[10/24 00:09:10 visual_prompt]: 	Training 500/553. train loss: 178.1935,	0.4880 s / batch. (data: 3.18e-04). ETA=6:09:16, max mem: 11.4 GB 
[10/24 00:09:56 visual_prompt]: Epoch 18 / 100: avg data time: 4.69e-01, avg batch time: 0.9585, average train loss: 172.7600
[10/24 00:10:49 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1896, average loss: 592.8258
[10/24 00:10:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.21	
[10/24 00:10:49 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[10/24 00:12:21 visual_prompt]: 	Training 100/553. train loss: 58.2503,	0.4820 s / batch. (data: 5.38e-03). ETA=6:03:26, max mem: 11.4 GB 
[10/24 00:13:52 visual_prompt]: 	Training 200/553. train loss: 112.5469,	0.5017 s / batch. (data: 1.08e-02). ETA=6:17:29, max mem: 11.4 GB 
[10/24 00:15:22 visual_prompt]: 	Training 300/553. train loss: 24.0729,	0.5016 s / batch. (data: 1.04e-02). ETA=6:16:33, max mem: 11.4 GB 
[10/24 00:16:52 visual_prompt]: 	Training 400/553. train loss: 105.9882,	0.4904 s / batch. (data: 2.84e-04). ETA=6:07:23, max mem: 11.4 GB 
[10/24 00:18:18 visual_prompt]: 	Training 500/553. train loss: 96.0216,	0.6640 s / batch. (data: 1.74e-01). ETA=8:16:19, max mem: 11.4 GB 
[10/24 00:19:05 visual_prompt]: Epoch 19 / 100: avg data time: 4.06e-01, avg batch time: 0.8966, average train loss: 179.4136
[10/24 00:19:58 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1907, average loss: 165.6166
[10/24 00:19:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.30	
[10/24 00:19:58 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[10/24 00:21:28 visual_prompt]: 	Training 100/553. train loss: 160.5914,	0.4990 s / batch. (data: 2.68e-04). ETA=6:11:43, max mem: 11.4 GB 
[10/24 00:23:00 visual_prompt]: 	Training 200/553. train loss: 386.9207,	0.4960 s / batch. (data: 3.06e-04). ETA=6:08:36, max mem: 11.4 GB 
[10/24 00:24:29 visual_prompt]: 	Training 300/553. train loss: 8.5658,	0.4881 s / batch. (data: 2.75e-04). ETA=6:01:57, max mem: 11.4 GB 
[10/24 00:25:59 visual_prompt]: 	Training 400/553. train loss: 164.3730,	0.5137 s / batch. (data: 2.06e-02). ETA=6:20:06, max mem: 11.4 GB 
[10/24 00:27:28 visual_prompt]: 	Training 500/553. train loss: 15.9617,	0.5080 s / batch. (data: 2.78e-04). ETA=6:14:59, max mem: 11.4 GB 
[10/24 00:28:15 visual_prompt]: Epoch 20 / 100: avg data time: 4.08e-01, avg batch time: 0.8991, average train loss: 180.9170
[10/24 00:29:08 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.1915, average loss: 42.6602
[10/24 00:29:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.91	
[10/24 00:29:08 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[10/24 00:30:43 visual_prompt]: 	Training 100/553. train loss: 142.8258,	0.8960 s / batch. (data: 4.02e-01). ETA=10:59:10, max mem: 11.4 GB 
[10/24 00:32:13 visual_prompt]: 	Training 200/553. train loss: 219.7512,	0.4781 s / batch. (data: 2.62e-04). ETA=5:50:56, max mem: 11.4 GB 
[10/24 00:33:43 visual_prompt]: 	Training 300/553. train loss: 328.9785,	0.4828 s / batch. (data: 7.98e-03). ETA=5:53:32, max mem: 11.4 GB 
[10/24 00:35:26 visual_prompt]: 	Training 400/553. train loss: 14.5155,	0.4771 s / batch. (data: 2.42e-04). ETA=5:48:34, max mem: 11.4 GB 
[10/24 00:37:04 visual_prompt]: 	Training 500/553. train loss: 182.4303,	0.4841 s / batch. (data: 2.62e-04). ETA=5:52:53, max mem: 11.4 GB 
[10/24 00:37:52 visual_prompt]: Epoch 21 / 100: avg data time: 4.57e-01, avg batch time: 0.9464, average train loss: 176.6675
[10/24 00:38:55 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1915, average loss: 244.1701
[10/24 00:38:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.34	
[10/24 00:38:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[10/24 00:40:34 visual_prompt]: 	Training 100/553. train loss: 5.2070,	0.4921 s / batch. (data: 7.96e-03). ETA=5:57:29, max mem: 11.4 GB 
[10/24 00:42:05 visual_prompt]: 	Training 200/553. train loss: 63.9021,	0.5040 s / batch. (data: 2.82e-04). ETA=6:05:15, max mem: 11.4 GB 
[10/24 00:43:34 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.5230 s / batch. (data: 5.36e-03). ETA=6:18:13, max mem: 11.4 GB 
[10/24 00:45:04 visual_prompt]: 	Training 400/553. train loss: 761.4287,	0.5020 s / batch. (data: 1.00e-02). ETA=6:02:11, max mem: 11.4 GB 
[10/24 00:46:34 visual_prompt]: 	Training 500/553. train loss: 123.0530,	0.4852 s / batch. (data: 2.76e-04). ETA=5:49:15, max mem: 11.4 GB 
[10/24 00:47:22 visual_prompt]: Epoch 22 / 100: avg data time: 4.27e-01, avg batch time: 0.9169, average train loss: 162.0059
[10/24 00:48:15 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1925, average loss: 294.1515
[10/24 00:48:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.45	
[10/24 00:48:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[10/24 00:49:49 visual_prompt]: 	Training 100/553. train loss: 75.8177,	0.4801 s / batch. (data: 2.55e-04). ETA=5:44:19, max mem: 11.4 GB 
[10/24 00:51:19 visual_prompt]: 	Training 200/553. train loss: 334.2861,	0.8022 s / batch. (data: 3.17e-01). ETA=9:34:02, max mem: 11.4 GB 
[10/24 00:52:50 visual_prompt]: 	Training 300/553. train loss: 60.4887,	0.5286 s / batch. (data: 1.55e-02). ETA=6:17:22, max mem: 11.4 GB 
[10/24 00:54:19 visual_prompt]: 	Training 400/553. train loss: 283.6967,	0.4783 s / batch. (data: 2.70e-04). ETA=5:40:41, max mem: 11.4 GB 
[10/24 00:55:46 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.5151 s / batch. (data: 1.07e-02). ETA=6:06:02, max mem: 11.4 GB 
[10/24 00:56:32 visual_prompt]: Epoch 23 / 100: avg data time: 4.08e-01, avg batch time: 0.8991, average train loss: 167.0501
[10/24 00:57:26 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1915, average loss: 56.4124
[10/24 00:57:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.95	
[10/24 00:57:26 visual_prompt]: Stopping early.
[10/24 00:57:26 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 00:57:26 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 00:57:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 00:57:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 00:57:26 visual_prompt]: Training with config:
[10/24 00:57:26 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr50.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 00:57:26 visual_prompt]: Loading training data...
[10/24 00:57:26 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 00:57:26 visual_prompt]: Loading validation data...
[10/24 00:57:26 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 00:57:26 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/24 00:57:29 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/24 00:57:29 visual_prompt]: tuned percent:0.529
[10/24 00:57:29 visual_prompt]: Device used for model: 0
[10/24 00:57:29 visual_prompt]: Setting up Evaluator...
[10/24 00:57:29 visual_prompt]: Setting up Trainer...
[10/24 00:57:29 visual_prompt]: 	Setting up the optimizer...
[10/24 00:57:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 00:59:02 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5042 s / batch. (data: 1.55e-02). ETA=7:43:50, max mem: 11.4 GB 
[10/24 01:00:30 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5080 s / batch. (data: 2.75e-04). ETA=7:46:31, max mem: 11.4 GB 
[10/24 01:02:03 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9170 s / batch. (data: 2.43e+00). ETA=1 day, 20:33:53, max mem: 11.4 GB 
[10/24 01:03:30 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4908 s / batch. (data: 2.85e-04). ETA=7:29:05, max mem: 11.4 GB 
[10/24 01:05:10 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5248 s / batch. (data: 1.05e-02). ETA=7:59:16, max mem: 11.4 GB 
[10/24 01:05:57 visual_prompt]: Epoch 1 / 100: avg data time: 4.25e-01, avg batch time: 0.9187, average train loss: 1.3966
[10/24 01:06:50 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1933, average loss: 1.3454
[10/24 01:06:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/24 01:06:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/24 01:08:23 visual_prompt]: 	Training 100/553. train loss: 10.0406,	0.4925 s / batch. (data: 1.24e-02). ETA=7:28:34, max mem: 11.4 GB 
[10/24 01:09:52 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.9080 s / batch. (data: 1.40e+00). ETA=1 day, 4:54:35, max mem: 11.4 GB 
[10/24 01:11:23 visual_prompt]: 	Training 300/553. train loss: 24.5283,	1.6240 s / batch. (data: 1.14e+00). ETA=1 day, 0:33:40, max mem: 11.4 GB 
[10/24 01:12:51 visual_prompt]: 	Training 400/553. train loss: 19.1852,	0.5030 s / batch. (data: 2.00e-04). ETA=7:35:35, max mem: 11.4 GB 
[10/24 01:14:23 visual_prompt]: 	Training 500/553. train loss: 8.1165,	0.5123 s / batch. (data: 1.05e-02). ETA=7:43:09, max mem: 11.4 GB 
[10/24 01:15:08 visual_prompt]: Epoch 2 / 100: avg data time: 4.08e-01, avg batch time: 0.9008, average train loss: 29.0130
[10/24 01:16:01 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1884, average loss: 15.9702
[10/24 01:16:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.22	
[10/24 01:16:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/24 01:17:32 visual_prompt]: 	Training 100/553. train loss: 28.1621,	0.6218 s / batch. (data: 1.30e-01). ETA=9:20:33, max mem: 11.4 GB 
[10/24 01:19:03 visual_prompt]: 	Training 200/553. train loss: 19.9880,	0.4800 s / batch. (data: 2.87e-04). ETA=7:11:56, max mem: 11.4 GB 
[10/24 01:20:31 visual_prompt]: 	Training 300/553. train loss: 13.1657,	0.5040 s / batch. (data: 2.93e-04). ETA=7:32:43, max mem: 11.4 GB 
[10/24 01:22:02 visual_prompt]: 	Training 400/553. train loss: 0.0002,	0.4893 s / batch. (data: 9.08e-03). ETA=7:18:40, max mem: 11.4 GB 
[10/24 01:23:33 visual_prompt]: 	Training 500/553. train loss: 8.1697,	1.8460 s / batch. (data: 1.36e+00). ETA=1 day, 3:32:00, max mem: 11.4 GB 
[10/24 01:24:17 visual_prompt]: Epoch 3 / 100: avg data time: 4.04e-01, avg batch time: 0.8967, average train loss: 32.0931
[10/24 01:25:10 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1923, average loss: 23.6176
[10/24 01:25:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.49	
[10/24 01:25:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/24 01:26:44 visual_prompt]: 	Training 100/553. train loss: 30.7057,	0.4909 s / batch. (data: 2.66e-04). ETA=7:18:02, max mem: 11.4 GB 
[10/24 01:28:13 visual_prompt]: 	Training 200/553. train loss: 225.6486,	0.5402 s / batch. (data: 4.02e-02). ETA=8:01:11, max mem: 11.4 GB 
[10/24 01:29:42 visual_prompt]: 	Training 300/553. train loss: 71.2100,	0.4880 s / batch. (data: 2.76e-04). ETA=7:13:51, max mem: 11.4 GB 
[10/24 01:31:07 visual_prompt]: 	Training 400/553. train loss: 140.0385,	0.4858 s / batch. (data: 2.74e-04). ETA=7:11:04, max mem: 11.4 GB 
[10/24 01:32:39 visual_prompt]: 	Training 500/553. train loss: 172.4364,	2.3880 s / batch. (data: 1.89e+00). ETA=1 day, 11:15:01, max mem: 11.4 GB 
[10/24 01:33:26 visual_prompt]: Epoch 4 / 100: avg data time: 4.05e-01, avg batch time: 0.8968, average train loss: 66.9126
[10/24 01:34:19 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1904, average loss: 116.2072
[10/24 01:34:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.94	
[10/24 01:34:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/24 01:35:51 visual_prompt]: 	Training 100/553. train loss: 177.0395,	0.4886 s / batch. (data: 2.71e-04). ETA=7:11:29, max mem: 11.4 GB 
[10/24 01:37:20 visual_prompt]: 	Training 200/553. train loss: 55.2763,	1.7786 s / batch. (data: 1.28e+00). ETA=1 day, 2:07:48, max mem: 11.4 GB 
[10/24 01:38:50 visual_prompt]: 	Training 300/553. train loss: 8.1846,	0.5236 s / batch. (data: 1.63e-02). ETA=7:40:42, max mem: 11.4 GB 
[10/24 01:40:18 visual_prompt]: 	Training 400/553. train loss: 121.1190,	0.4899 s / batch. (data: 2.64e-04). ETA=7:10:10, max mem: 11.4 GB 
[10/24 01:41:48 visual_prompt]: 	Training 500/553. train loss: 5.7840,	0.4832 s / batch. (data: 2.81e-04). ETA=7:03:30, max mem: 11.4 GB 
[10/24 01:42:35 visual_prompt]: Epoch 5 / 100: avg data time: 4.05e-01, avg batch time: 0.8968, average train loss: 58.0728
[10/24 01:43:28 visual_prompt]: Inference (val):avg data time: 1.81e-04, avg batch time: 0.1926, average loss: 73.6093
[10/24 01:43:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.10	
[10/24 01:43:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/24 01:45:02 visual_prompt]: 	Training 100/553. train loss: 67.0356,	0.4840 s / batch. (data: 2.65e-04). ETA=7:03:00, max mem: 11.4 GB 
[10/24 01:46:33 visual_prompt]: 	Training 200/553. train loss: 383.8297,	0.5040 s / batch. (data: 2.78e-04). ETA=7:19:36, max mem: 11.4 GB 
[10/24 01:48:02 visual_prompt]: 	Training 300/553. train loss: 55.4269,	0.5078 s / batch. (data: 1.17e-02). ETA=7:22:02, max mem: 11.4 GB 
[10/24 01:49:34 visual_prompt]: 	Training 400/553. train loss: 44.8993,	0.6320 s / batch. (data: 1.45e-01). ETA=9:09:08, max mem: 11.4 GB 
[10/24 01:51:04 visual_prompt]: 	Training 500/553. train loss: 218.2295,	1.9983 s / batch. (data: 1.50e+00). ETA=1 day, 4:53:02, max mem: 11.4 GB 
[10/24 01:51:49 visual_prompt]: Epoch 6 / 100: avg data time: 4.15e-01, avg batch time: 0.9060, average train loss: 94.4714
[10/24 01:52:42 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.1903, average loss: 57.7273
[10/24 01:52:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.88	
[10/24 01:52:42 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/24 01:54:13 visual_prompt]: 	Training 100/553. train loss: 358.7202,	0.5191 s / batch. (data: 1.19e-02). ETA=7:28:51, max mem: 11.4 GB 
[10/24 01:55:43 visual_prompt]: 	Training 200/553. train loss: 34.3776,	0.5004 s / batch. (data: 1.55e-02). ETA=7:11:52, max mem: 11.4 GB 
[10/24 01:57:16 visual_prompt]: 	Training 300/553. train loss: 130.2236,	2.4544 s / batch. (data: 1.98e+00). ETA=1 day, 11:14:05, max mem: 11.4 GB 
[10/24 01:58:45 visual_prompt]: 	Training 400/553. train loss: 77.4785,	2.4080 s / batch. (data: 1.92e+00). ETA=1 day, 10:30:08, max mem: 11.4 GB 
[10/24 02:00:11 visual_prompt]: 	Training 500/553. train loss: 17.2455,	0.4973 s / batch. (data: 2.78e-04). ETA=7:06:39, max mem: 11.4 GB 
[10/24 02:00:57 visual_prompt]: Epoch 7 / 100: avg data time: 4.04e-01, avg batch time: 0.8953, average train loss: 89.9690
[10/24 02:01:49 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1899, average loss: 46.7700
[10/24 02:01:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.31	
[10/24 02:01:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/24 02:03:20 visual_prompt]: 	Training 100/553. train loss: 40.1617,	1.1453 s / batch. (data: 6.65e-01). ETA=16:19:47, max mem: 11.4 GB 
[10/24 02:04:51 visual_prompt]: 	Training 200/553. train loss: 65.6964,	0.4935 s / batch. (data: 3.92e-04). ETA=7:01:21, max mem: 11.4 GB 
[10/24 02:06:21 visual_prompt]: 	Training 300/553. train loss: 7.4385,	0.4890 s / batch. (data: 2.60e-04). ETA=6:56:42, max mem: 11.4 GB 
[10/24 02:07:50 visual_prompt]: 	Training 400/553. train loss: 66.7455,	0.6200 s / batch. (data: 1.08e-01). ETA=8:47:18, max mem: 11.4 GB 
[10/24 02:09:20 visual_prompt]: 	Training 500/553. train loss: 337.2186,	1.4598 s / batch. (data: 9.80e-01). ETA=20:39:07, max mem: 11.4 GB 
[10/24 02:10:06 visual_prompt]: Epoch 8 / 100: avg data time: 4.06e-01, avg batch time: 0.8969, average train loss: 142.1268
[10/24 02:10:58 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1926, average loss: 31.5986
[10/24 02:10:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.29	
[10/24 02:10:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/24 02:12:31 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4926 s / batch. (data: 4.04e-04). ETA=6:56:51, max mem: 11.4 GB 
[10/24 02:14:00 visual_prompt]: 	Training 200/553. train loss: 240.5732,	0.4871 s / batch. (data: 2.55e-04). ETA=6:51:21, max mem: 11.4 GB 
[10/24 02:15:29 visual_prompt]: 	Training 300/553. train loss: 190.4000,	2.1266 s / batch. (data: 1.64e+00). ETA=1 day, 5:52:36, max mem: 11.4 GB 
[10/24 02:16:59 visual_prompt]: 	Training 400/553. train loss: 347.2908,	0.5080 s / batch. (data: 7.96e-03). ETA=7:07:19, max mem: 11.4 GB 
[10/24 02:18:29 visual_prompt]: 	Training 500/553. train loss: 32.9546,	1.0320 s / batch. (data: 5.48e-01). ETA=14:26:25, max mem: 11.4 GB 
[10/24 02:19:15 visual_prompt]: Epoch 9 / 100: avg data time: 4.06e-01, avg batch time: 0.8975, average train loss: 141.3530
[10/24 02:20:07 visual_prompt]: Inference (val):avg data time: 3.34e-04, avg batch time: 0.1905, average loss: 814.6346
[10/24 02:20:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.57	
[10/24 02:20:07 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/24 02:21:42 visual_prompt]: 	Training 100/553. train loss: 178.5143,	0.5023 s / batch. (data: 7.42e-04). ETA=7:00:28, max mem: 11.4 GB 
[10/24 02:23:10 visual_prompt]: 	Training 200/553. train loss: 25.8394,	0.5002 s / batch. (data: 1.22e-02). ETA=6:57:51, max mem: 11.4 GB 
[10/24 02:24:39 visual_prompt]: 	Training 300/553. train loss: 209.1846,	0.9500 s / batch. (data: 4.74e-01). ETA=13:12:01, max mem: 11.4 GB 
[10/24 02:26:06 visual_prompt]: 	Training 400/553. train loss: 123.0239,	1.2712 s / batch. (data: 7.87e-01). ETA=17:37:43, max mem: 11.4 GB 
[10/24 02:27:36 visual_prompt]: 	Training 500/553. train loss: 323.9196,	0.6632 s / batch. (data: 1.88e-01). ETA=9:10:43, max mem: 11.4 GB 
[10/24 02:28:22 visual_prompt]: Epoch 10 / 100: avg data time: 4.04e-01, avg batch time: 0.8944, average train loss: 156.4535
[10/24 02:29:15 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1921, average loss: 22.1079
[10/24 02:29:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.54	
[10/24 02:29:15 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/24 02:30:49 visual_prompt]: 	Training 100/553. train loss: 52.6923,	0.5120 s / batch. (data: 2.87e-04). ETA=7:03:53, max mem: 11.4 GB 
[10/24 02:32:20 visual_prompt]: 	Training 200/553. train loss: 492.8741,	0.4881 s / batch. (data: 4.33e-04). ETA=6:43:14, max mem: 11.4 GB 
[10/24 02:33:49 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.6629 s / batch. (data: 2.17e+00). ETA=1 day, 12:35:35, max mem: 11.4 GB 
[10/24 02:35:16 visual_prompt]: 	Training 400/553. train loss: 1.5338,	0.5282 s / batch. (data: 2.15e-02). ETA=7:14:36, max mem: 11.4 GB 
[10/24 02:36:45 visual_prompt]: 	Training 500/553. train loss: 302.3745,	0.4883 s / batch. (data: 7.88e-03). ETA=6:40:58, max mem: 11.4 GB 
[10/24 02:37:30 visual_prompt]: Epoch 11 / 100: avg data time: 4.03e-01, avg batch time: 0.8949, average train loss: 164.2890
[10/24 02:38:23 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1913, average loss: 52.9040
[10/24 02:38:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.57	
[10/24 02:38:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/24 02:39:56 visual_prompt]: 	Training 100/553. train loss: 116.9840,	0.4978 s / batch. (data: 1.01e-02). ETA=6:47:32, max mem: 11.4 GB 
[10/24 02:41:27 visual_prompt]: 	Training 200/553. train loss: 15.3220,	1.1675 s / batch. (data: 6.93e-01). ETA=15:53:49, max mem: 11.4 GB 
[10/24 02:42:54 visual_prompt]: 	Training 300/553. train loss: 109.0095,	0.4892 s / batch. (data: 2.79e-04). ETA=6:38:51, max mem: 11.4 GB 
[10/24 02:44:24 visual_prompt]: 	Training 400/553. train loss: 255.7421,	0.4762 s / batch. (data: 2.69e-04). ETA=6:27:26, max mem: 11.4 GB 
[10/24 02:45:53 visual_prompt]: 	Training 500/553. train loss: 436.1308,	0.5179 s / batch. (data: 1.05e-02). ETA=7:00:31, max mem: 11.4 GB 
[10/24 02:46:38 visual_prompt]: Epoch 12 / 100: avg data time: 4.04e-01, avg batch time: 0.8952, average train loss: 171.6386
[10/24 02:47:30 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1914, average loss: 215.9138
[10/24 02:47:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.51	
[10/24 02:47:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/24 02:49:04 visual_prompt]: 	Training 100/553. train loss: 73.4969,	0.5067 s / batch. (data: 2.68e-04). ETA=6:50:06, max mem: 11.4 GB 
[10/24 02:50:31 visual_prompt]: 	Training 200/553. train loss: 88.5233,	0.5045 s / batch. (data: 8.48e-03). ETA=6:47:31, max mem: 11.4 GB 
[10/24 02:52:01 visual_prompt]: 	Training 300/553. train loss: 108.2375,	0.9927 s / batch. (data: 5.21e-01). ETA=13:20:12, max mem: 11.4 GB 
[10/24 02:53:29 visual_prompt]: 	Training 400/553. train loss: 419.6348,	0.4941 s / batch. (data: 1.60e-02). ETA=6:37:29, max mem: 11.4 GB 
[10/24 02:55:00 visual_prompt]: 	Training 500/553. train loss: 269.2724,	0.4885 s / batch. (data: 1.05e-02). ETA=6:32:07, max mem: 11.4 GB 
[10/24 02:55:45 visual_prompt]: Epoch 13 / 100: avg data time: 4.03e-01, avg batch time: 0.8947, average train loss: 192.9376
[10/24 02:56:38 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1919, average loss: 96.6660
[10/24 02:56:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.50	
[10/24 02:56:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/24 02:58:11 visual_prompt]: 	Training 100/553. train loss: 12.1947,	0.4960 s / batch. (data: 2.76e-04). ETA=6:36:51, max mem: 11.4 GB 
[10/24 02:59:41 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.8520 s / batch. (data: 1.35e+00). ETA=1 day, 0:38:49, max mem: 11.4 GB 
[10/24 03:01:10 visual_prompt]: 	Training 300/553. train loss: 1.0919,	1.2520 s / batch. (data: 7.52e-01). ETA=16:37:38, max mem: 11.4 GB 
[10/24 03:02:40 visual_prompt]: 	Training 400/553. train loss: 85.1464,	0.4884 s / batch. (data: 8.34e-03). ETA=6:28:20, max mem: 11.4 GB 
[10/24 03:04:09 visual_prompt]: 	Training 500/553. train loss: 349.5356,	0.5240 s / batch. (data: 7.94e-03). ETA=6:55:47, max mem: 11.4 GB 
[10/24 03:04:54 visual_prompt]: Epoch 14 / 100: avg data time: 4.05e-01, avg batch time: 0.8963, average train loss: 183.5363
[10/24 03:05:47 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.1910, average loss: 39.9360
[10/24 03:05:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.50	
[10/24 03:05:47 visual_prompt]: Best epoch 14: best metric: -39.936
[10/24 03:05:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/24 03:07:19 visual_prompt]: 	Training 100/553. train loss: 41.1428,	0.5183 s / batch. (data: 1.40e-02). ETA=6:49:57, max mem: 11.4 GB 
[10/24 03:08:47 visual_prompt]: 	Training 200/553. train loss: 513.0481,	0.4786 s / batch. (data: 2.80e-04). ETA=6:17:45, max mem: 11.4 GB 
[10/24 03:10:17 visual_prompt]: 	Training 300/553. train loss: 239.1140,	0.5040 s / batch. (data: 2.62e-04). ETA=6:36:58, max mem: 11.4 GB 
[10/24 03:11:45 visual_prompt]: 	Training 400/553. train loss: 158.6899,	0.4980 s / batch. (data: 4.55e-04). ETA=6:31:25, max mem: 11.4 GB 
[10/24 03:13:15 visual_prompt]: 	Training 500/553. train loss: 105.2957,	0.4722 s / batch. (data: 2.74e-04). ETA=6:10:22, max mem: 11.4 GB 
[10/24 03:14:02 visual_prompt]: Epoch 15 / 100: avg data time: 4.05e-01, avg batch time: 0.8955, average train loss: 141.6358
[10/24 03:14:55 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1913, average loss: 127.5227
[10/24 03:14:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.23	
[10/24 03:14:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/24 03:16:26 visual_prompt]: 	Training 100/553. train loss: 76.2199,	0.4829 s / batch. (data: 2.70e-04). ETA=6:17:31, max mem: 11.4 GB 
[10/24 03:17:56 visual_prompt]: 	Training 200/553. train loss: 119.7144,	0.4994 s / batch. (data: 1.55e-02). ETA=6:29:35, max mem: 11.4 GB 
[10/24 03:19:26 visual_prompt]: 	Training 300/553. train loss: 22.9655,	0.5040 s / batch. (data: 2.85e-04). ETA=6:32:18, max mem: 11.4 GB 
[10/24 03:20:57 visual_prompt]: 	Training 400/553. train loss: 382.9413,	0.5128 s / batch. (data: 1.19e-02). ETA=6:38:17, max mem: 11.4 GB 
[10/24 03:22:26 visual_prompt]: 	Training 500/553. train loss: 337.6375,	2.1415 s / batch. (data: 1.66e+00). ETA=1 day, 3:39:48, max mem: 11.4 GB 
[10/24 03:23:12 visual_prompt]: Epoch 16 / 100: avg data time: 4.09e-01, avg batch time: 0.8997, average train loss: 173.5737
[10/24 03:24:06 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1899, average loss: 201.3102
[10/24 03:24:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.74	
[10/24 03:24:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/24 03:25:38 visual_prompt]: 	Training 100/553. train loss: 14.2168,	0.4898 s / batch. (data: 2.82e-04). ETA=6:18:25, max mem: 11.4 GB 
[10/24 03:27:09 visual_prompt]: 	Training 200/553. train loss: 55.8976,	0.4946 s / batch. (data: 2.54e-04). ETA=6:21:14, max mem: 11.4 GB 
[10/24 03:28:38 visual_prompt]: 	Training 300/553. train loss: 388.0796,	0.4960 s / batch. (data: 2.62e-04). ETA=6:21:32, max mem: 11.4 GB 
[10/24 03:30:07 visual_prompt]: 	Training 400/553. train loss: 127.1628,	1.1369 s / batch. (data: 6.41e-01). ETA=14:32:38, max mem: 11.4 GB 
[10/24 03:31:36 visual_prompt]: 	Training 500/553. train loss: 151.7552,	2.0121 s / batch. (data: 1.54e+00). ETA=1 day, 1:41:00, max mem: 11.4 GB 
[10/24 03:32:24 visual_prompt]: Epoch 17 / 100: avg data time: 4.10e-01, avg batch time: 0.9003, average train loss: 142.1107
[10/24 03:33:17 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1926, average loss: 33.9768
[10/24 03:33:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[10/24 03:33:17 visual_prompt]: Best epoch 17: best metric: -33.977
[10/24 03:33:17 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/24 03:34:51 visual_prompt]: 	Training 100/553. train loss: 210.4943,	0.4854 s / batch. (data: 5.37e-03). ETA=6:10:32, max mem: 11.4 GB 
[10/24 03:36:23 visual_prompt]: 	Training 200/553. train loss: 673.1316,	0.4960 s / batch. (data: 2.68e-04). ETA=6:17:45, max mem: 11.4 GB 
[10/24 03:37:53 visual_prompt]: 	Training 300/553. train loss: 140.7602,	0.5154 s / batch. (data: 5.40e-03). ETA=6:31:43, max mem: 11.4 GB 
[10/24 03:39:23 visual_prompt]: 	Training 400/553. train loss: 11.5327,	0.4960 s / batch. (data: 2.69e-04). ETA=6:16:06, max mem: 11.4 GB 
[10/24 03:40:51 visual_prompt]: 	Training 500/553. train loss: 48.1174,	0.5160 s / batch. (data: 2.47e-04). ETA=6:30:27, max mem: 11.4 GB 
[10/24 03:41:36 visual_prompt]: Epoch 18 / 100: avg data time: 4.11e-01, avg batch time: 0.9026, average train loss: 182.8842
[10/24 03:42:29 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.1904, average loss: 247.3397
[10/24 03:42:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.79	
[10/24 03:42:29 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[10/24 03:44:02 visual_prompt]: 	Training 100/553. train loss: 33.6165,	0.5520 s / batch. (data: 6.33e-02). ETA=6:56:14, max mem: 11.4 GB 
[10/24 03:45:31 visual_prompt]: 	Training 200/553. train loss: 92.2282,	0.4840 s / batch. (data: 2.64e-04). ETA=6:04:12, max mem: 11.4 GB 
[10/24 03:47:01 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.4777 s / batch. (data: 2.71e-04). ETA=5:58:36, max mem: 11.4 GB 
[10/24 03:48:31 visual_prompt]: 	Training 400/553. train loss: 234.8839,	0.5013 s / batch. (data: 6.69e-04). ETA=6:15:32, max mem: 11.4 GB 
[10/24 03:49:57 visual_prompt]: 	Training 500/553. train loss: 131.8353,	0.4920 s / batch. (data: 5.38e-03). ETA=6:07:43, max mem: 11.4 GB 
[10/24 03:50:43 visual_prompt]: Epoch 19 / 100: avg data time: 4.02e-01, avg batch time: 0.8933, average train loss: 182.0744
[10/24 03:51:36 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1897, average loss: 861.4836
[10/24 03:51:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.44	
[10/24 03:51:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[10/24 03:53:06 visual_prompt]: 	Training 100/553. train loss: 26.8481,	0.5040 s / batch. (data: 1.19e-02). ETA=6:15:24, max mem: 11.4 GB 
[10/24 03:54:37 visual_prompt]: 	Training 200/553. train loss: 202.9982,	0.4797 s / batch. (data: 6.56e-03). ETA=5:56:29, max mem: 11.4 GB 
[10/24 03:56:07 visual_prompt]: 	Training 300/553. train loss: 58.6431,	0.4800 s / batch. (data: 2.60e-04). ETA=5:55:57, max mem: 11.4 GB 
[10/24 03:57:35 visual_prompt]: 	Training 400/553. train loss: 59.0648,	0.5040 s / batch. (data: 3.95e-04). ETA=6:12:53, max mem: 11.4 GB 
[10/24 03:59:03 visual_prompt]: 	Training 500/553. train loss: 27.3313,	0.4917 s / batch. (data: 2.31e-04). ETA=6:03:00, max mem: 11.4 GB 
[10/24 03:59:54 visual_prompt]: Epoch 20 / 100: avg data time: 4.09e-01, avg batch time: 0.9005, average train loss: 167.9905
[10/24 04:00:47 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1908, average loss: 65.1740
[10/24 04:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.23	
[10/24 04:00:47 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[10/24 04:02:21 visual_prompt]: 	Training 100/553. train loss: 30.6378,	0.8640 s / batch. (data: 3.42e-01). ETA=10:35:36, max mem: 11.4 GB 
[10/24 04:03:50 visual_prompt]: 	Training 200/553. train loss: 287.9715,	0.4960 s / batch. (data: 2.57e-04). ETA=6:04:02, max mem: 11.4 GB 
[10/24 04:05:19 visual_prompt]: 	Training 300/553. train loss: 338.0889,	1.6932 s / batch. (data: 1.21e+00). ETA=20:39:57, max mem: 11.4 GB 
[10/24 04:06:47 visual_prompt]: 	Training 400/553. train loss: 410.1881,	0.4880 s / batch. (data: 2.68e-04). ETA=5:56:35, max mem: 11.4 GB 
[10/24 04:08:18 visual_prompt]: 	Training 500/553. train loss: 33.7655,	0.5109 s / batch. (data: 1.08e-02). ETA=6:12:25, max mem: 11.4 GB 
[10/24 04:09:02 visual_prompt]: Epoch 21 / 100: avg data time: 4.04e-01, avg batch time: 0.8945, average train loss: 146.1837
[10/24 04:09:55 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1900, average loss: 30.3013
[10/24 04:09:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.63	
[10/24 04:09:55 visual_prompt]: Best epoch 21: best metric: -30.301
[10/24 04:09:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[10/24 04:11:26 visual_prompt]: 	Training 100/553. train loss: 279.6943,	0.5407 s / batch. (data: 2.06e-02). ETA=6:32:48, max mem: 11.4 GB 
[10/24 04:12:56 visual_prompt]: 	Training 200/553. train loss: 302.7496,	0.5160 s / batch. (data: 1.19e-02). ETA=6:13:58, max mem: 11.4 GB 
[10/24 04:14:22 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.4800 s / batch. (data: 2.72e-04). ETA=5:47:05, max mem: 11.4 GB 
[10/24 04:15:52 visual_prompt]: 	Training 400/553. train loss: 551.1804,	0.4920 s / batch. (data: 2.68e-04). ETA=5:54:56, max mem: 11.4 GB 
[10/24 04:17:22 visual_prompt]: 	Training 500/553. train loss: 77.2845,	0.4840 s / batch. (data: 2.60e-04). ETA=5:48:22, max mem: 11.4 GB 
[10/24 04:18:09 visual_prompt]: Epoch 22 / 100: avg data time: 4.02e-01, avg batch time: 0.8932, average train loss: 160.0052
[10/24 04:19:01 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.1914, average loss: 73.3583
[10/24 04:19:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.50	
[10/24 04:19:01 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[10/24 04:20:34 visual_prompt]: 	Training 100/553. train loss: 216.1141,	0.7600 s / batch. (data: 2.84e-01). ETA=9:05:04, max mem: 11.4 GB 
[10/24 04:22:05 visual_prompt]: 	Training 200/553. train loss: 193.8305,	1.3733 s / batch. (data: 8.90e-01). ETA=16:22:39, max mem: 11.4 GB 
[10/24 04:23:35 visual_prompt]: 	Training 300/553. train loss: 102.2822,	0.5200 s / batch. (data: 1.20e-02). ETA=6:11:15, max mem: 11.4 GB 
[10/24 04:25:03 visual_prompt]: 	Training 400/553. train loss: 69.0033,	0.4790 s / batch. (data: 2.95e-04). ETA=5:41:08, max mem: 11.4 GB 
[10/24 04:26:29 visual_prompt]: 	Training 500/553. train loss: 124.8037,	0.5194 s / batch. (data: 2.50e-04). ETA=6:09:06, max mem: 11.4 GB 
[10/24 04:27:15 visual_prompt]: Epoch 23 / 100: avg data time: 4.02e-01, avg batch time: 0.8932, average train loss: 163.2835
[10/24 04:28:08 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1904, average loss: 131.7821
[10/24 04:28:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.55	
[10/24 04:28:08 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[10/24 04:29:38 visual_prompt]: 	Training 100/553. train loss: 39.8715,	0.4960 s / batch. (data: 1.20e-02). ETA=5:51:10, max mem: 11.4 GB 
[10/24 04:31:08 visual_prompt]: 	Training 200/553. train loss: 75.9271,	0.5023 s / batch. (data: 2.73e-04). ETA=5:54:47, max mem: 11.4 GB 
[10/24 04:32:38 visual_prompt]: 	Training 300/553. train loss: 39.6426,	1.5651 s / batch. (data: 1.09e+00). ETA=18:22:54, max mem: 11.4 GB 
[10/24 04:34:07 visual_prompt]: 	Training 400/553. train loss: 301.4420,	0.4840 s / batch. (data: 2.47e-04). ETA=5:40:17, max mem: 11.4 GB 
[10/24 04:35:38 visual_prompt]: 	Training 500/553. train loss: 154.5899,	0.7963 s / batch. (data: 3.10e-01). ETA=9:18:28, max mem: 11.4 GB 
[10/24 04:36:25 visual_prompt]: Epoch 24 / 100: avg data time: 4.06e-01, avg batch time: 0.8978, average train loss: 158.2168
[10/24 04:37:18 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1889, average loss: 68.6741
[10/24 04:37:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.56	
[10/24 04:37:18 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[10/24 04:38:54 visual_prompt]: 	Training 100/553. train loss: 376.2866,	0.4753 s / batch. (data: 2.36e-04). ETA=5:32:09, max mem: 11.4 GB 
[10/24 04:40:21 visual_prompt]: 	Training 200/553. train loss: 88.4962,	0.5148 s / batch. (data: 2.08e-02). ETA=5:58:52, max mem: 11.4 GB 
[10/24 04:41:51 visual_prompt]: 	Training 300/553. train loss: 207.2883,	0.5170 s / batch. (data: 1.05e-02). ETA=5:59:35, max mem: 11.4 GB 
[10/24 04:43:21 visual_prompt]: 	Training 400/553. train loss: 46.1691,	1.9160 s / batch. (data: 1.43e+00). ETA=22:09:18, max mem: 11.4 GB 
[10/24 04:44:51 visual_prompt]: 	Training 500/553. train loss: 97.5055,	1.9840 s / batch. (data: 1.47e+00). ETA=22:53:12, max mem: 11.4 GB 
[10/24 04:45:36 visual_prompt]: Epoch 25 / 100: avg data time: 4.10e-01, avg batch time: 0.9013, average train loss: 130.1747
[10/24 04:46:30 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1907, average loss: 244.6627
[10/24 04:46:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.59	
[10/24 04:46:30 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[10/24 04:48:02 visual_prompt]: 	Training 100/553. train loss: 24.6436,	0.7880 s / batch. (data: 3.13e-01). ETA=9:03:22, max mem: 11.4 GB 
[10/24 04:49:34 visual_prompt]: 	Training 200/553. train loss: 0.4645,	2.2551 s / batch. (data: 1.77e+00). ETA=1 day, 1:51:17, max mem: 11.4 GB 
[10/24 04:51:02 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.4800 s / batch. (data: 2.43e-04). ETA=5:29:24, max mem: 11.4 GB 
[10/24 04:52:31 visual_prompt]: 	Training 400/553. train loss: 5.5637,	0.4918 s / batch. (data: 2.55e-04). ETA=5:36:39, max mem: 11.4 GB 
[10/24 04:53:58 visual_prompt]: 	Training 500/553. train loss: 30.5845,	0.5091 s / batch. (data: 7.98e-03). ETA=5:47:40, max mem: 11.4 GB 
[10/24 04:54:44 visual_prompt]: Epoch 26 / 100: avg data time: 4.02e-01, avg batch time: 0.8940, average train loss: 156.9168
[10/24 04:55:37 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1908, average loss: 256.5742
[10/24 04:55:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.00	
[10/24 04:55:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[10/24 04:57:09 visual_prompt]: 	Training 100/553. train loss: 77.4686,	0.4853 s / batch. (data: 2.80e-04). ETA=5:30:12, max mem: 11.4 GB 
[10/24 04:58:38 visual_prompt]: 	Training 200/553. train loss: 678.2887,	2.2439 s / batch. (data: 1.74e+00). ETA=1 day, 1:22:58, max mem: 11.4 GB 
[10/24 05:00:07 visual_prompt]: 	Training 300/553. train loss: 67.2881,	1.2130 s / batch. (data: 7.10e-01). ETA=13:41:12, max mem: 11.4 GB 
[10/24 05:01:39 visual_prompt]: 	Training 400/553. train loss: 98.1417,	0.5015 s / batch. (data: 7.03e-04). ETA=5:38:42, max mem: 11.4 GB 
[10/24 05:03:09 visual_prompt]: 	Training 500/553. train loss: 153.5331,	0.4839 s / batch. (data: 2.78e-04). ETA=5:26:00, max mem: 11.4 GB 
[10/24 05:03:53 visual_prompt]: Epoch 27 / 100: avg data time: 4.07e-01, avg batch time: 0.8976, average train loss: 129.6721
[10/24 05:04:46 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1894, average loss: 327.6509
[10/24 05:04:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.33	
[10/24 05:04:46 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[10/24 05:06:17 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.5080 s / batch. (data: 2.94e-04). ETA=5:40:55, max mem: 11.4 GB 
[10/24 05:07:47 visual_prompt]: 	Training 200/553. train loss: 127.1688,	0.4941 s / batch. (data: 2.60e-04). ETA=5:30:45, max mem: 11.4 GB 
[10/24 05:09:17 visual_prompt]: 	Training 300/553. train loss: 133.7870,	1.5080 s / batch. (data: 1.02e+00). ETA=16:47:04, max mem: 11.4 GB 
[10/24 05:10:45 visual_prompt]: 	Training 400/553. train loss: 614.5444,	0.4820 s / batch. (data: 2.62e-04). ETA=5:21:03, max mem: 11.4 GB 
[10/24 05:12:12 visual_prompt]: 	Training 500/553. train loss: 334.2350,	0.5172 s / batch. (data: 9.12e-03). ETA=5:43:38, max mem: 11.4 GB 
[10/24 05:13:01 visual_prompt]: Epoch 28 / 100: avg data time: 4.05e-01, avg batch time: 0.8960, average train loss: 158.1543
[10/24 05:13:54 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1911, average loss: 28.9055
[10/24 05:13:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.37	
[10/24 05:13:54 visual_prompt]: Best epoch 28: best metric: -28.906
[10/24 05:13:54 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[10/24 05:15:32 visual_prompt]: 	Training 100/553. train loss: 49.4626,	0.4974 s / batch. (data: 2.51e-04). ETA=5:29:13, max mem: 11.4 GB 
[10/24 05:17:01 visual_prompt]: 	Training 200/553. train loss: 14.5246,	2.3023 s / batch. (data: 1.83e+00). ETA=1 day, 1:20:06, max mem: 11.4 GB 
[10/24 05:18:28 visual_prompt]: 	Training 300/553. train loss: 3.2849,	0.4840 s / batch. (data: 2.92e-04). ETA=5:18:45, max mem: 11.4 GB 
[10/24 05:19:52 visual_prompt]: 	Training 400/553. train loss: 89.3615,	0.6892 s / batch. (data: 2.05e-01). ETA=7:32:47, max mem: 11.4 GB 
[10/24 05:21:22 visual_prompt]: 	Training 500/553. train loss: 315.1914,	0.4940 s / batch. (data: 5.39e-03). ETA=5:23:42, max mem: 11.4 GB 
[10/24 05:22:08 visual_prompt]: Epoch 29 / 100: avg data time: 4.01e-01, avg batch time: 0.8928, average train loss: 139.0175
[10/24 05:23:00 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.1910, average loss: 243.6148
[10/24 05:23:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.75	
[10/24 05:23:00 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[10/24 05:24:31 visual_prompt]: 	Training 100/553. train loss: 76.8002,	0.4979 s / batch. (data: 2.06e-02). ETA=5:24:57, max mem: 11.4 GB 
[10/24 05:26:01 visual_prompt]: 	Training 200/553. train loss: 518.0726,	0.4866 s / batch. (data: 1.04e-02). ETA=5:16:46, max mem: 11.4 GB 
[10/24 05:27:29 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.5001 s / batch. (data: 2.66e-02). ETA=5:24:45, max mem: 11.4 GB 
[10/24 05:29:00 visual_prompt]: 	Training 400/553. train loss: 118.4066,	1.8145 s / batch. (data: 1.33e+00). ETA=19:35:17, max mem: 11.4 GB 
[10/24 05:30:28 visual_prompt]: 	Training 500/553. train loss: 99.6004,	2.0063 s / batch. (data: 1.53e+00). ETA=21:36:09, max mem: 11.4 GB 
[10/24 05:31:15 visual_prompt]: Epoch 30 / 100: avg data time: 4.03e-01, avg batch time: 0.8936, average train loss: 154.4865
[10/24 05:32:08 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1908, average loss: 39.7109
[10/24 05:32:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.06	
[10/24 05:32:08 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[10/24 05:33:41 visual_prompt]: 	Training 100/553. train loss: 40.6677,	0.4955 s / batch. (data: 1.10e-02). ETA=5:18:49, max mem: 11.4 GB 
[10/24 05:35:13 visual_prompt]: 	Training 200/553. train loss: 154.8555,	0.5108 s / batch. (data: 7.94e-03). ETA=5:27:50, max mem: 11.4 GB 
[10/24 05:36:38 visual_prompt]: 	Training 300/553. train loss: 325.2822,	0.4841 s / batch. (data: 2.87e-04). ETA=5:09:55, max mem: 11.4 GB 
[10/24 05:38:07 visual_prompt]: 	Training 400/553. train loss: 356.1335,	0.4773 s / batch. (data: 2.63e-04). ETA=5:04:45, max mem: 11.4 GB 
[10/24 05:39:36 visual_prompt]: 	Training 500/553. train loss: 174.0055,	0.4840 s / batch. (data: 2.73e-04). ETA=5:08:13, max mem: 11.4 GB 
[10/24 05:40:21 visual_prompt]: Epoch 31 / 100: avg data time: 4.00e-01, avg batch time: 0.8911, average train loss: 159.3276
[10/24 05:41:13 visual_prompt]: Inference (val):avg data time: 1.73e-04, avg batch time: 0.1900, average loss: 175.7819
[10/24 05:41:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.89	
[10/24 05:41:13 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[10/24 05:42:46 visual_prompt]: 	Training 100/553. train loss: 84.1847,	0.5442 s / batch. (data: 3.21e-02). ETA=5:45:10, max mem: 11.4 GB 
[10/24 05:44:15 visual_prompt]: 	Training 200/553. train loss: 133.8470,	0.4880 s / batch. (data: 2.93e-04). ETA=5:08:41, max mem: 11.4 GB 
[10/24 05:45:47 visual_prompt]: 	Training 300/553. train loss: 324.8206,	0.5040 s / batch. (data: 1.20e-02). ETA=5:18:00, max mem: 11.4 GB 
[10/24 05:47:17 visual_prompt]: 	Training 400/553. train loss: 381.5070,	0.5000 s / batch. (data: 1.19e-02). ETA=5:14:38, max mem: 11.4 GB 
[10/24 05:48:42 visual_prompt]: 	Training 500/553. train loss: 72.5670,	0.6760 s / batch. (data: 1.82e-01). ETA=7:04:17, max mem: 11.4 GB 
[10/24 05:49:27 visual_prompt]: Epoch 32 / 100: avg data time: 4.02e-01, avg batch time: 0.8932, average train loss: 158.2545
[10/24 05:50:20 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1909, average loss: 164.6402
[10/24 05:50:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.11	
[10/24 05:50:20 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[10/24 05:51:51 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.8657 s / batch. (data: 1.39e+00). ETA=19:26:10, max mem: 11.4 GB 
[10/24 05:53:22 visual_prompt]: 	Training 200/553. train loss: 343.1761,	2.4652 s / batch. (data: 1.98e+00). ETA=1 day, 1:36:48, max mem: 11.4 GB 
[10/24 05:54:50 visual_prompt]: 	Training 300/553. train loss: 63.5520,	0.5080 s / batch. (data: 2.83e-04). ETA=5:15:50, max mem: 11.4 GB 
[10/24 05:56:20 visual_prompt]: 	Training 400/553. train loss: 8.9091,	0.4925 s / batch. (data: 8.00e-03). ETA=5:05:23, max mem: 11.4 GB 
[10/24 05:57:48 visual_prompt]: 	Training 500/553. train loss: 13.2236,	1.0021 s / batch. (data: 5.09e-01). ETA=10:19:40, max mem: 11.4 GB 
[10/24 05:58:34 visual_prompt]: Epoch 33 / 100: avg data time: 4.02e-01, avg batch time: 0.8931, average train loss: 132.2952
[10/24 05:59:27 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1922, average loss: 133.5590
[10/24 05:59:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.27	
[10/24 05:59:27 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[10/24 06:01:00 visual_prompt]: 	Training 100/553. train loss: 23.7773,	1.3513 s / batch. (data: 8.67e-01). ETA=13:52:11, max mem: 11.4 GB 
[10/24 06:02:31 visual_prompt]: 	Training 200/553. train loss: 171.8454,	2.1640 s / batch. (data: 1.69e+00). ETA=22:09:04, max mem: 11.4 GB 
[10/24 06:03:59 visual_prompt]: 	Training 300/553. train loss: 32.7681,	1.2960 s / batch. (data: 7.94e-01). ETA=13:13:49, max mem: 11.4 GB 
[10/24 06:05:29 visual_prompt]: 	Training 400/553. train loss: 190.3567,	0.4909 s / batch. (data: 1.59e-02). ETA=4:59:53, max mem: 11.4 GB 
[10/24 06:06:58 visual_prompt]: 	Training 500/553. train loss: 6.8591,	2.0722 s / batch. (data: 1.60e+00). ETA=21:02:19, max mem: 11.4 GB 
[10/24 06:07:44 visual_prompt]: Epoch 34 / 100: avg data time: 4.08e-01, avg batch time: 0.8987, average train loss: 150.6206
[10/24 06:08:37 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1917, average loss: 139.8921
[10/24 06:08:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.58	
[10/24 06:08:37 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[10/24 06:10:10 visual_prompt]: 	Training 100/553. train loss: 114.7808,	0.5120 s / batch. (data: 2.77e-04). ETA=5:10:34, max mem: 11.4 GB 
[10/24 06:11:41 visual_prompt]: 	Training 200/553. train loss: 36.9601,	1.1925 s / batch. (data: 6.76e-01). ETA=12:01:26, max mem: 11.4 GB 
[10/24 06:13:08 visual_prompt]: 	Training 300/553. train loss: 10.4912,	0.4789 s / batch. (data: 2.47e-04). ETA=4:48:55, max mem: 11.4 GB 
[10/24 06:14:37 visual_prompt]: 	Training 400/553. train loss: 3.6088,	1.5440 s / batch. (data: 1.04e+00). ETA=15:28:53, max mem: 11.4 GB 
[10/24 06:16:04 visual_prompt]: 	Training 500/553. train loss: 95.7669,	0.9200 s / batch. (data: 4.18e-01). ETA=9:11:57, max mem: 11.4 GB 
[10/24 06:16:51 visual_prompt]: Epoch 35 / 100: avg data time: 4.02e-01, avg batch time: 0.8929, average train loss: 116.5104
[10/24 06:17:43 visual_prompt]: Inference (val):avg data time: 1.19e-04, avg batch time: 0.1912, average loss: 103.0388
[10/24 06:17:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.92	
[10/24 06:17:43 visual_prompt]: Stopping early.
[10/24 06:17:43 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 06:17:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 06:17:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 06:17:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 06:17:43 visual_prompt]: Training with config:
[10/24 06:17:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr50.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 06:17:43 visual_prompt]: Loading training data...
[10/24 06:17:43 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 06:17:43 visual_prompt]: Loading validation data...
[10/24 06:17:43 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 06:17:43 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/24 06:17:46 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/24 06:17:46 visual_prompt]: tuned percent:0.529
[10/24 06:17:46 visual_prompt]: Device used for model: 0
[10/24 06:17:46 visual_prompt]: Setting up Evaluator...
[10/24 06:17:46 visual_prompt]: Setting up Trainer...
[10/24 06:17:46 visual_prompt]: 	Setting up the optimizer...
[10/24 06:17:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 06:19:18 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5018 s / batch. (data: 5.39e-03). ETA=7:41:37, max mem: 11.4 GB 
[10/24 06:20:45 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5008 s / batch. (data: 8.73e-03). ETA=7:39:53, max mem: 11.4 GB 
[10/24 06:22:17 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.1263 s / batch. (data: 2.64e+00). ETA=1 day, 23:45:44, max mem: 11.4 GB 
[10/24 06:23:51 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4918 s / batch. (data: 3.12e-04). ETA=7:29:57, max mem: 11.4 GB 
[10/24 06:25:23 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5320 s / batch. (data: 2.77e-04). ETA=8:05:52, max mem: 11.4 GB 
[10/24 06:26:09 visual_prompt]: Epoch 1 / 100: avg data time: 4.16e-01, avg batch time: 0.9101, average train loss: 1.3966
[10/24 06:27:02 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1916, average loss: 1.3454
[10/24 06:27:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/24 06:27:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/24 06:28:33 visual_prompt]: 	Training 100/553. train loss: 39.2077,	0.8501 s / batch. (data: 3.60e-01). ETA=12:54:16, max mem: 11.4 GB 
[10/24 06:30:03 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.7840 s / batch. (data: 1.29e+00). ETA=1 day, 3:01:54, max mem: 11.4 GB 
[10/24 06:31:33 visual_prompt]: 	Training 300/553. train loss: 8.3003,	1.6400 s / batch. (data: 1.11e+00). ETA=1 day, 0:48:15, max mem: 11.4 GB 
[10/24 06:33:01 visual_prompt]: 	Training 400/553. train loss: 30.2630,	0.4842 s / batch. (data: 3.06e-04). ETA=7:18:35, max mem: 11.4 GB 
[10/24 06:34:32 visual_prompt]: 	Training 500/553. train loss: 12.5412,	0.5016 s / batch. (data: 1.04e-02). ETA=7:33:29, max mem: 11.4 GB 
[10/24 06:35:17 visual_prompt]: Epoch 2 / 100: avg data time: 4.02e-01, avg batch time: 0.8949, average train loss: 22.6686
[10/24 06:36:10 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1909, average loss: 23.6924
[10/24 06:36:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.39	
[10/24 06:36:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/24 06:37:41 visual_prompt]: 	Training 100/553. train loss: 2.5523,	0.4960 s / batch. (data: 7.98e-03). ETA=7:27:12, max mem: 11.4 GB 
[10/24 06:39:11 visual_prompt]: 	Training 200/553. train loss: 21.2676,	0.4774 s / batch. (data: 2.81e-04). ETA=7:09:36, max mem: 11.4 GB 
[10/24 06:40:43 visual_prompt]: 	Training 300/553. train loss: 7.7087,	0.5120 s / batch. (data: 5.40e-03). ETA=7:39:51, max mem: 11.4 GB 
[10/24 06:42:14 visual_prompt]: 	Training 400/553. train loss: 51.0897,	0.4900 s / batch. (data: 1.20e-02). ETA=7:19:19, max mem: 11.4 GB 
[10/24 06:43:44 visual_prompt]: 	Training 500/553. train loss: 32.3490,	1.8665 s / batch. (data: 1.39e+00). ETA=1 day, 3:50:20, max mem: 11.4 GB 
[10/24 06:44:29 visual_prompt]: Epoch 3 / 100: avg data time: 4.10e-01, avg batch time: 0.9022, average train loss: 36.9538
[10/24 06:45:21 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1910, average loss: 36.4656
[10/24 06:45:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.51	
[10/24 06:45:21 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/24 06:46:54 visual_prompt]: 	Training 100/553. train loss: 12.5067,	0.5061 s / batch. (data: 2.51e-04). ETA=7:31:34, max mem: 11.4 GB 
[10/24 06:48:24 visual_prompt]: 	Training 200/553. train loss: 13.6255,	0.5099 s / batch. (data: 3.15e-04). ETA=7:34:10, max mem: 11.4 GB 
[10/24 06:49:54 visual_prompt]: 	Training 300/553. train loss: 10.0874,	1.7948 s / batch. (data: 1.30e+00). ETA=1 day, 2:35:38, max mem: 11.4 GB 
[10/24 06:51:19 visual_prompt]: 	Training 400/553. train loss: 66.3782,	1.4701 s / batch. (data: 9.83e-01). ETA=21:44:30, max mem: 11.4 GB 
[10/24 06:52:50 visual_prompt]: 	Training 500/553. train loss: 2.3103,	3.8068 s / batch. (data: 3.33e+00). ETA=2 days, 8:11:38, max mem: 11.4 GB 
[10/24 06:53:36 visual_prompt]: Epoch 4 / 100: avg data time: 4.02e-01, avg batch time: 0.8940, average train loss: 44.0203
[10/24 06:54:28 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1903, average loss: 88.5470
[10/24 06:54:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.90	
[10/24 06:54:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/24 06:56:00 visual_prompt]: 	Training 100/553. train loss: 218.4113,	0.5080 s / batch. (data: 7.96e-03). ETA=7:28:38, max mem: 11.4 GB 
[10/24 06:57:30 visual_prompt]: 	Training 200/553. train loss: 57.7035,	1.9999 s / batch. (data: 1.49e+00). ETA=1 day, 5:22:52, max mem: 11.4 GB 
[10/24 06:59:00 visual_prompt]: 	Training 300/553. train loss: 4.1677,	0.4830 s / batch. (data: 5.40e-03). ETA=7:04:57, max mem: 11.4 GB 
[10/24 07:00:29 visual_prompt]: 	Training 400/553. train loss: 185.5824,	0.4927 s / batch. (data: 7.97e-03). ETA=7:12:37, max mem: 11.4 GB 
[10/24 07:01:59 visual_prompt]: 	Training 500/553. train loss: 1.2453,	0.4778 s / batch. (data: 2.66e-04). ETA=6:58:47, max mem: 11.4 GB 
[10/24 07:02:46 visual_prompt]: Epoch 5 / 100: avg data time: 4.09e-01, avg batch time: 0.9001, average train loss: 57.1808
[10/24 07:03:39 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.1918, average loss: 97.0105
[10/24 07:03:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.33	
[10/24 07:03:39 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/24 07:05:14 visual_prompt]: 	Training 100/553. train loss: 32.6024,	0.5040 s / batch. (data: 7.50e-04). ETA=7:20:25, max mem: 11.4 GB 
[10/24 07:06:43 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.5120 s / batch. (data: 1.60e-02). ETA=7:26:35, max mem: 11.4 GB 
[10/24 07:08:11 visual_prompt]: 	Training 300/553. train loss: 29.9138,	0.5033 s / batch. (data: 1.04e-02). ETA=7:18:10, max mem: 11.4 GB 
[10/24 07:09:43 visual_prompt]: 	Training 400/553. train loss: 30.6623,	0.8002 s / batch. (data: 2.95e-01). ETA=11:35:18, max mem: 11.4 GB 
[10/24 07:11:11 visual_prompt]: 	Training 500/553. train loss: 168.3242,	1.4099 s / batch. (data: 9.16e-01). ETA=20:22:42, max mem: 11.4 GB 
[10/24 07:11:56 visual_prompt]: Epoch 6 / 100: avg data time: 4.06e-01, avg batch time: 0.8980, average train loss: 72.4804
[10/24 07:12:49 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.1909, average loss: 74.0620
[10/24 07:12:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.55	
[10/24 07:12:49 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/24 07:14:20 visual_prompt]: 	Training 100/553. train loss: 39.0891,	0.4981 s / batch. (data: 7.93e-03). ETA=7:10:42, max mem: 11.4 GB 
[10/24 07:15:49 visual_prompt]: 	Training 200/553. train loss: 38.1030,	0.4801 s / batch. (data: 2.67e-04). ETA=6:54:19, max mem: 11.4 GB 
[10/24 07:17:21 visual_prompt]: 	Training 300/553. train loss: 118.9407,	2.0317 s / batch. (data: 1.55e+00). ETA=1 day, 5:10:01, max mem: 11.4 GB 
[10/24 07:18:50 visual_prompt]: 	Training 400/553. train loss: 0.7464,	2.5000 s / batch. (data: 1.99e+00). ETA=1 day, 11:49:15, max mem: 11.4 GB 
[10/24 07:20:17 visual_prompt]: 	Training 500/553. train loss: 267.8655,	0.7877 s / batch. (data: 3.02e-01). ETA=11:15:52, max mem: 11.4 GB 
[10/24 07:21:02 visual_prompt]: Epoch 7 / 100: avg data time: 4.01e-01, avg batch time: 0.8922, average train loss: 62.9934
[10/24 07:21:55 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1893, average loss: 36.4033
[10/24 07:21:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.05	
[10/24 07:21:55 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/24 07:23:26 visual_prompt]: 	Training 100/553. train loss: 40.4484,	1.2400 s / batch. (data: 7.22e-01). ETA=17:40:47, max mem: 11.4 GB 
[10/24 07:24:58 visual_prompt]: 	Training 200/553. train loss: 37.8188,	0.4761 s / batch. (data: 2.96e-04). ETA=6:46:28, max mem: 11.4 GB 
[10/24 07:26:28 visual_prompt]: 	Training 300/553. train loss: 240.0844,	0.4899 s / batch. (data: 3.70e-04). ETA=6:57:28, max mem: 11.4 GB 
[10/24 07:27:58 visual_prompt]: 	Training 400/553. train loss: 46.3868,	1.7439 s / batch. (data: 1.26e+00). ETA=1 day, 0:43:06, max mem: 11.4 GB 
[10/24 07:29:27 visual_prompt]: 	Training 500/553. train loss: 216.5097,	1.9360 s / batch. (data: 1.46e+00). ETA=1 day, 3:23:20, max mem: 11.4 GB 
[10/24 07:30:12 visual_prompt]: Epoch 8 / 100: avg data time: 4.08e-01, avg batch time: 0.8990, average train loss: 74.3661
[10/24 07:31:05 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1933, average loss: 41.1392
[10/24 07:31:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.99	
[10/24 07:31:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/24 07:32:37 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4993 s / batch. (data: 7.27e-03). ETA=7:02:33, max mem: 11.4 GB 
[10/24 07:34:06 visual_prompt]: 	Training 200/553. train loss: 28.4081,	0.4880 s / batch. (data: 2.73e-04). ETA=6:52:08, max mem: 11.4 GB 
[10/24 07:35:34 visual_prompt]: 	Training 300/553. train loss: 53.9226,	0.7440 s / batch. (data: 2.48e-01). ETA=10:27:09, max mem: 11.4 GB 
[10/24 07:37:06 visual_prompt]: 	Training 400/553. train loss: 82.9144,	0.4896 s / batch. (data: 3.06e-04). ETA=6:51:51, max mem: 11.4 GB 
[10/24 07:38:35 visual_prompt]: 	Training 500/553. train loss: 7.6539,	0.7437 s / batch. (data: 2.66e-01). ETA=10:24:22, max mem: 11.4 GB 
[10/24 07:39:26 visual_prompt]: Epoch 9 / 100: avg data time: 4.13e-01, avg batch time: 0.9049, average train loss: 74.9407
[10/24 07:40:18 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1904, average loss: 243.9292
[10/24 07:40:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.75	
[10/24 07:40:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/24 07:41:52 visual_prompt]: 	Training 100/553. train loss: 72.2803,	0.4880 s / batch. (data: 2.74e-04). ETA=6:48:29, max mem: 11.4 GB 
[10/24 07:43:21 visual_prompt]: 	Training 200/553. train loss: 102.5106,	0.5121 s / batch. (data: 1.21e-02). ETA=7:07:50, max mem: 11.4 GB 
[10/24 07:44:49 visual_prompt]: 	Training 300/553. train loss: 15.1294,	0.4849 s / batch. (data: 2.72e-04). ETA=6:44:15, max mem: 11.4 GB 
[10/24 07:46:16 visual_prompt]: 	Training 400/553. train loss: 5.2332,	0.5118 s / batch. (data: 7.76e-03). ETA=7:05:50, max mem: 11.4 GB 
[10/24 07:47:47 visual_prompt]: 	Training 500/553. train loss: 22.2380,	1.6267 s / batch. (data: 1.16e+00). ETA=22:30:45, max mem: 11.4 GB 
[10/24 07:48:33 visual_prompt]: Epoch 10 / 100: avg data time: 4.04e-01, avg batch time: 0.8943, average train loss: 105.0547
[10/24 07:49:26 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1929, average loss: 272.1315
[10/24 07:49:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.56	
[10/24 07:49:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/24 07:50:59 visual_prompt]: 	Training 100/553. train loss: 207.8326,	0.5040 s / batch. (data: 2.73e-04). ETA=6:57:12, max mem: 11.4 GB 
[10/24 07:52:31 visual_prompt]: 	Training 200/553. train loss: 99.4037,	0.4840 s / batch. (data: 3.29e-04). ETA=6:39:51, max mem: 11.4 GB 
[10/24 07:53:59 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.6597 s / batch. (data: 2.18e+00). ETA=1 day, 12:32:55, max mem: 11.4 GB 
[10/24 07:55:27 visual_prompt]: 	Training 400/553. train loss: 31.4478,	0.4787 s / batch. (data: 7.45e-04). ETA=6:33:55, max mem: 11.4 GB 
[10/24 07:56:55 visual_prompt]: 	Training 500/553. train loss: 154.3853,	0.5160 s / batch. (data: 1.19e-02). ETA=7:03:44, max mem: 11.4 GB 
[10/24 07:57:40 visual_prompt]: Epoch 11 / 100: avg data time: 4.03e-01, avg batch time: 0.8937, average train loss: 108.3145
[10/24 07:58:32 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1910, average loss: 131.8862
[10/24 07:58:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.02	
[10/24 07:58:32 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/24 08:00:06 visual_prompt]: 	Training 100/553. train loss: 60.7756,	0.8518 s / batch. (data: 3.61e-01). ETA=11:37:18, max mem: 11.4 GB 
[10/24 08:01:36 visual_prompt]: 	Training 200/553. train loss: 49.6700,	0.4873 s / batch. (data: 1.04e-02). ETA=6:38:07, max mem: 11.4 GB 
[10/24 08:03:04 visual_prompt]: 	Training 300/553. train loss: 26.1013,	0.5105 s / batch. (data: 1.17e-02). ETA=6:56:12, max mem: 11.4 GB 
[10/24 08:04:33 visual_prompt]: 	Training 400/553. train loss: 42.5828,	0.6558 s / batch. (data: 1.72e-01). ETA=8:53:35, max mem: 11.4 GB 
[10/24 08:06:02 visual_prompt]: 	Training 500/553. train loss: 24.9898,	0.5360 s / batch. (data: 7.21e-04). ETA=7:15:11, max mem: 11.4 GB 
[10/24 08:06:47 visual_prompt]: Epoch 12 / 100: avg data time: 4.03e-01, avg batch time: 0.8934, average train loss: 98.6418
[10/24 08:07:39 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.1908, average loss: 461.8121
[10/24 08:07:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.71	
[10/24 08:07:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/24 08:09:13 visual_prompt]: 	Training 100/553. train loss: 0.8605,	0.7633 s / batch. (data: 2.79e-01). ETA=10:17:47, max mem: 11.4 GB 
[10/24 08:10:40 visual_prompt]: 	Training 200/553. train loss: 138.5839,	0.5228 s / batch. (data: 2.67e-02). ETA=7:02:14, max mem: 11.4 GB 
[10/24 08:12:11 visual_prompt]: 	Training 300/553. train loss: 9.5106,	2.2705 s / batch. (data: 1.78e+00). ETA=1 day, 6:30:11, max mem: 11.4 GB 
[10/24 08:13:38 visual_prompt]: 	Training 400/553. train loss: 254.3375,	0.9680 s / batch. (data: 4.69e-01). ETA=12:58:41, max mem: 11.4 GB 
[10/24 08:15:08 visual_prompt]: 	Training 500/553. train loss: 56.8227,	0.5314 s / batch. (data: 3.22e-02). ETA=7:06:32, max mem: 11.4 GB 
[10/24 08:15:54 visual_prompt]: Epoch 13 / 100: avg data time: 4.05e-01, avg batch time: 0.8949, average train loss: 125.9322
[10/24 08:16:47 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1927, average loss: 59.5078
[10/24 08:16:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.87	
[10/24 08:16:47 visual_prompt]: Best epoch 13: best metric: -59.508
[10/24 08:16:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/24 08:18:20 visual_prompt]: 	Training 100/553. train loss: 63.5406,	0.4960 s / batch. (data: 2.53e-04). ETA=6:36:52, max mem: 11.4 GB 
[10/24 08:19:48 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.5600 s / batch. (data: 6.57e-02). ETA=7:27:10, max mem: 11.4 GB 
[10/24 08:21:18 visual_prompt]: 	Training 300/553. train loss: 213.1776,	1.3601 s / batch. (data: 8.79e-01). ETA=18:03:45, max mem: 11.4 GB 
[10/24 08:22:46 visual_prompt]: 	Training 400/553. train loss: 63.6546,	0.4954 s / batch. (data: 2.33e-02). ETA=6:33:56, max mem: 11.4 GB 
[10/24 08:24:17 visual_prompt]: 	Training 500/553. train loss: 108.1927,	0.4902 s / batch. (data: 2.51e-04). ETA=6:28:57, max mem: 11.4 GB 
[10/24 08:25:01 visual_prompt]: Epoch 14 / 100: avg data time: 4.04e-01, avg batch time: 0.8942, average train loss: 99.5735
[10/24 08:25:54 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.1898, average loss: 13.3545
[10/24 08:25:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.17	
[10/24 08:25:54 visual_prompt]: Best epoch 14: best metric: -13.354
[10/24 08:25:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/24 08:27:27 visual_prompt]: 	Training 100/553. train loss: 70.4011,	0.4881 s / batch. (data: 2.98e-04). ETA=6:26:04, max mem: 11.4 GB 
[10/24 08:28:55 visual_prompt]: 	Training 200/553. train loss: 138.9282,	0.4904 s / batch. (data: 2.65e-04). ETA=6:27:02, max mem: 11.4 GB 
[10/24 08:30:26 visual_prompt]: 	Training 300/553. train loss: 140.3184,	0.5148 s / batch. (data: 2.28e-02). ETA=6:45:29, max mem: 11.4 GB 
[10/24 08:31:53 visual_prompt]: 	Training 400/553. train loss: 19.6758,	0.5152 s / batch. (data: 1.56e-02). ETA=6:44:53, max mem: 11.4 GB 
[10/24 08:33:23 visual_prompt]: 	Training 500/553. train loss: 32.5361,	0.5088 s / batch. (data: 1.08e-02). ETA=6:39:01, max mem: 11.4 GB 
[10/24 08:34:09 visual_prompt]: Epoch 15 / 100: avg data time: 4.03e-01, avg batch time: 0.8950, average train loss: 125.2682
[10/24 08:35:02 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1899, average loss: 281.1143
[10/24 08:35:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.97	
[10/24 08:35:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/24 08:36:33 visual_prompt]: 	Training 100/553. train loss: 24.8303,	0.4903 s / batch. (data: 1.20e-02). ETA=6:23:15, max mem: 11.4 GB 
[10/24 08:38:02 visual_prompt]: 	Training 200/553. train loss: 11.5029,	0.4875 s / batch. (data: 1.05e-02). ETA=6:20:17, max mem: 11.4 GB 
[10/24 08:39:34 visual_prompt]: 	Training 300/553. train loss: 46.7475,	0.4792 s / batch. (data: 5.41e-03). ETA=6:13:02, max mem: 11.4 GB 
[10/24 08:41:03 visual_prompt]: 	Training 400/553. train loss: 163.5737,	0.4729 s / batch. (data: 2.68e-04). ETA=6:07:21, max mem: 11.4 GB 
[10/24 08:42:32 visual_prompt]: 	Training 500/553. train loss: 64.2236,	2.2001 s / batch. (data: 1.70e+00). ETA=1 day, 4:25:13, max mem: 11.4 GB 
[10/24 08:43:17 visual_prompt]: Epoch 16 / 100: avg data time: 4.05e-01, avg batch time: 0.8957, average train loss: 107.1834
[10/24 08:44:10 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1915, average loss: 67.7936
[10/24 08:44:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.25	
[10/24 08:44:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/24 08:45:42 visual_prompt]: 	Training 100/553. train loss: 42.2508,	0.4975 s / batch. (data: 5.38e-03). ETA=6:24:20, max mem: 11.4 GB 
[10/24 08:47:12 visual_prompt]: 	Training 200/553. train loss: 294.7244,	0.5014 s / batch. (data: 2.91e-04). ETA=6:26:29, max mem: 11.4 GB 
[10/24 08:48:41 visual_prompt]: 	Training 300/553. train loss: 249.2820,	0.5070 s / batch. (data: 5.38e-03). ETA=6:29:58, max mem: 11.4 GB 
[10/24 08:50:09 visual_prompt]: 	Training 400/553. train loss: 253.8910,	0.6167 s / batch. (data: 1.42e-01). ETA=7:53:18, max mem: 11.4 GB 
[10/24 08:51:38 visual_prompt]: 	Training 500/553. train loss: 164.9092,	2.1360 s / batch. (data: 1.65e+00). ETA=1 day, 3:15:54, max mem: 11.4 GB 
[10/24 08:52:25 visual_prompt]: Epoch 17 / 100: avg data time: 4.04e-01, avg batch time: 0.8942, average train loss: 111.5728
[10/24 08:53:17 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1914, average loss: 8.1119
[10/24 08:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.90	
[10/24 08:53:17 visual_prompt]: Best epoch 17: best metric: -8.112
[10/24 08:53:17 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/24 08:54:49 visual_prompt]: 	Training 100/553. train loss: 96.7139,	0.4960 s / batch. (data: 2.50e-04). ETA=6:18:35, max mem: 11.4 GB 
[10/24 08:56:22 visual_prompt]: 	Training 200/553. train loss: 37.8737,	0.4914 s / batch. (data: 5.82e-03). ETA=6:14:16, max mem: 11.4 GB 
[10/24 08:57:50 visual_prompt]: 	Training 300/553. train loss: 53.7618,	0.4960 s / batch. (data: 2.68e-04). ETA=6:16:57, max mem: 11.4 GB 
[10/24 08:59:19 visual_prompt]: 	Training 400/553. train loss: 138.5643,	0.5117 s / batch. (data: 1.55e-02). ETA=6:28:02, max mem: 11.4 GB 
[10/24 09:00:47 visual_prompt]: 	Training 500/553. train loss: 11.7348,	0.4780 s / batch. (data: 2.85e-04). ETA=6:01:42, max mem: 11.4 GB 
[10/24 09:01:32 visual_prompt]: Epoch 18 / 100: avg data time: 4.05e-01, avg batch time: 0.8941, average train loss: 120.2812
[10/24 09:02:25 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1886, average loss: 133.5487
[10/24 09:02:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.29	
[10/24 09:02:25 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[10/24 09:03:57 visual_prompt]: 	Training 100/553. train loss: 18.7030,	0.4805 s / batch. (data: 5.42e-03). ETA=6:02:22, max mem: 11.4 GB 
[10/24 09:05:26 visual_prompt]: 	Training 200/553. train loss: 129.1991,	0.4920 s / batch. (data: 2.66e-04). ETA=6:10:11, max mem: 11.4 GB 
[10/24 09:06:55 visual_prompt]: 	Training 300/553. train loss: 479.7955,	0.5120 s / batch. (data: 5.41e-03). ETA=6:24:22, max mem: 11.4 GB 
[10/24 09:08:25 visual_prompt]: 	Training 400/553. train loss: 56.2349,	0.5080 s / batch. (data: 7.14e-04). ETA=6:20:32, max mem: 11.4 GB 
[10/24 09:09:52 visual_prompt]: 	Training 500/553. train loss: 39.7890,	0.4968 s / batch. (data: 7.96e-03). ETA=6:11:18, max mem: 11.4 GB 
[10/24 09:10:39 visual_prompt]: Epoch 19 / 100: avg data time: 4.03e-01, avg batch time: 0.8940, average train loss: 87.4300
[10/24 09:11:32 visual_prompt]: Inference (val):avg data time: 3.97e-04, avg batch time: 0.1902, average loss: 275.4871
[10/24 09:11:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[10/24 09:11:32 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[10/24 09:13:06 visual_prompt]: 	Training 100/553. train loss: 22.9116,	0.5047 s / batch. (data: 1.27e-02). ETA=6:15:55, max mem: 11.4 GB 
[10/24 09:14:36 visual_prompt]: 	Training 200/553. train loss: 23.5475,	0.4915 s / batch. (data: 3.85e-04). ETA=6:05:16, max mem: 11.4 GB 
[10/24 09:16:07 visual_prompt]: 	Training 300/553. train loss: 381.2609,	0.4848 s / batch. (data: 5.39e-03). ETA=5:59:30, max mem: 11.4 GB 
[10/24 09:17:36 visual_prompt]: 	Training 400/553. train loss: 67.7467,	0.4935 s / batch. (data: 7.96e-03). ETA=6:05:06, max mem: 11.4 GB 
[10/24 09:19:05 visual_prompt]: 	Training 500/553. train loss: 45.9474,	0.5080 s / batch. (data: 2.50e-04). ETA=6:15:02, max mem: 11.4 GB 
[10/24 09:19:52 visual_prompt]: Epoch 20 / 100: avg data time: 4.13e-01, avg batch time: 0.9045, average train loss: 84.2297
[10/24 09:20:46 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1903, average loss: 6.2309
[10/24 09:20:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 53.71	
[10/24 09:20:46 visual_prompt]: Best epoch 20: best metric: -6.231
[10/24 09:20:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[10/24 09:22:21 visual_prompt]: 	Training 100/553. train loss: 106.1586,	0.5040 s / batch. (data: 2.64e-04). ETA=6:10:47, max mem: 11.4 GB 
[10/24 09:23:51 visual_prompt]: 	Training 200/553. train loss: 118.1395,	0.4879 s / batch. (data: 1.05e-02). ETA=5:58:05, max mem: 11.4 GB 
[10/24 09:25:20 visual_prompt]: 	Training 300/553. train loss: 57.7867,	0.6840 s / batch. (data: 2.03e-01). ETA=8:20:53, max mem: 11.4 GB 
[10/24 09:26:49 visual_prompt]: 	Training 400/553. train loss: 337.7622,	0.4961 s / batch. (data: 2.72e-04). ETA=6:02:30, max mem: 11.4 GB 
[10/24 09:28:20 visual_prompt]: 	Training 500/553. train loss: 35.0828,	0.5141 s / batch. (data: 2.06e-02). ETA=6:14:46, max mem: 11.4 GB 
[10/24 09:29:04 visual_prompt]: Epoch 21 / 100: avg data time: 4.11e-01, avg batch time: 0.9018, average train loss: 105.2187
[10/24 09:29:57 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1893, average loss: 36.4133
[10/24 09:29:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.84	
[10/24 09:29:57 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[10/24 09:31:29 visual_prompt]: 	Training 100/553. train loss: 139.1856,	0.4838 s / batch. (data: 4.27e-04). ETA=5:51:27, max mem: 11.4 GB 
[10/24 09:32:58 visual_prompt]: 	Training 200/553. train loss: 109.6138,	0.5029 s / batch. (data: 1.08e-02). ETA=6:04:29, max mem: 11.4 GB 
[10/24 09:34:28 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.4840 s / batch. (data: 2.76e-04). ETA=5:50:00, max mem: 11.4 GB 
[10/24 09:35:58 visual_prompt]: 	Training 400/553. train loss: 59.0174,	0.5080 s / batch. (data: 2.64e-04). ETA=6:06:30, max mem: 11.4 GB 
[10/24 09:37:28 visual_prompt]: 	Training 500/553. train loss: 3.3008,	0.4920 s / batch. (data: 2.77e-04). ETA=5:54:08, max mem: 11.4 GB 
[10/24 09:38:15 visual_prompt]: Epoch 22 / 100: avg data time: 4.09e-01, avg batch time: 0.8997, average train loss: 98.1015
[10/24 09:39:08 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1908, average loss: 54.8182
[10/24 09:39:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.07	
[10/24 09:39:08 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[10/24 09:40:41 visual_prompt]: 	Training 100/553. train loss: 21.3031,	0.5111 s / batch. (data: 2.93e-04). ETA=6:06:33, max mem: 11.4 GB 
[10/24 09:42:12 visual_prompt]: 	Training 200/553. train loss: 0.0015,	1.3958 s / batch. (data: 9.11e-01). ETA=16:38:46, max mem: 11.4 GB 
[10/24 09:43:41 visual_prompt]: 	Training 300/553. train loss: 282.4368,	0.5166 s / batch. (data: 1.04e-02). ETA=6:08:47, max mem: 11.4 GB 
[10/24 09:45:09 visual_prompt]: 	Training 400/553. train loss: 74.8636,	0.5160 s / batch. (data: 7.11e-04). ETA=6:07:30, max mem: 11.4 GB 
[10/24 09:46:35 visual_prompt]: 	Training 500/553. train loss: 400.7656,	0.5023 s / batch. (data: 1.21e-02). ETA=5:56:53, max mem: 11.4 GB 
[10/24 09:47:21 visual_prompt]: Epoch 23 / 100: avg data time: 4.01e-01, avg batch time: 0.8924, average train loss: 97.0382
[10/24 09:48:14 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1909, average loss: 20.8863
[10/24 09:48:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.94	
[10/24 09:48:14 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[10/24 09:49:44 visual_prompt]: 	Training 100/553. train loss: 72.7040,	0.5033 s / batch. (data: 7.94e-03). ETA=5:56:18, max mem: 11.4 GB 
[10/24 09:51:13 visual_prompt]: 	Training 200/553. train loss: 51.2639,	0.4893 s / batch. (data: 5.39e-03). ETA=5:45:37, max mem: 11.4 GB 
[10/24 09:52:42 visual_prompt]: 	Training 300/553. train loss: 95.1589,	1.7837 s / batch. (data: 1.31e+00). ETA=20:56:56, max mem: 11.4 GB 
[10/24 09:54:11 visual_prompt]: 	Training 400/553. train loss: 127.4380,	0.5040 s / batch. (data: 2.93e-04). ETA=5:54:18, max mem: 11.4 GB 
[10/24 09:55:41 visual_prompt]: 	Training 500/553. train loss: 159.0901,	0.8000 s / batch. (data: 3.02e-01). ETA=9:21:04, max mem: 11.4 GB 
[10/24 09:56:29 visual_prompt]: Epoch 24 / 100: avg data time: 4.04e-01, avg batch time: 0.8959, average train loss: 100.9585
[10/24 09:57:22 visual_prompt]: Inference (val):avg data time: 2.79e-04, avg batch time: 0.1920, average loss: 103.8710
[10/24 09:57:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.89	
[10/24 09:57:22 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[10/24 09:58:58 visual_prompt]: 	Training 100/553. train loss: 44.8884,	0.4760 s / batch. (data: 2.71e-04). ETA=5:32:37, max mem: 11.4 GB 
[10/24 10:00:24 visual_prompt]: 	Training 200/553. train loss: 97.8793,	0.4869 s / batch. (data: 2.68e-04). ETA=5:39:25, max mem: 11.4 GB 
[10/24 10:01:52 visual_prompt]: 	Training 300/553. train loss: 21.5888,	0.5960 s / batch. (data: 1.04e-01). ETA=6:54:31, max mem: 11.4 GB 
[10/24 10:03:22 visual_prompt]: 	Training 400/553. train loss: 4.3127,	1.9305 s / batch. (data: 1.45e+00). ETA=22:19:22, max mem: 11.4 GB 
[10/24 10:04:51 visual_prompt]: 	Training 500/553. train loss: 61.2003,	2.0062 s / batch. (data: 1.52e+00). ETA=23:08:34, max mem: 11.4 GB 
[10/24 10:05:36 visual_prompt]: Epoch 25 / 100: avg data time: 4.02e-01, avg batch time: 0.8931, average train loss: 91.2951
[10/24 10:06:29 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1902, average loss: 36.2083
[10/24 10:06:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.54	
[10/24 10:06:29 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[10/24 10:08:01 visual_prompt]: 	Training 100/553. train loss: 75.6529,	0.4851 s / batch. (data: 2.67e-04). ETA=5:34:30, max mem: 11.4 GB 
[10/24 10:09:30 visual_prompt]: 	Training 200/553. train loss: 75.4691,	1.4560 s / batch. (data: 9.57e-01). ETA=16:41:35, max mem: 11.4 GB 
[10/24 10:11:01 visual_prompt]: 	Training 300/553. train loss: 91.0218,	0.4945 s / batch. (data: 2.68e-04). ETA=5:39:22, max mem: 11.4 GB 
[10/24 10:12:28 visual_prompt]: 	Training 400/553. train loss: 20.4967,	0.5270 s / batch. (data: 1.05e-02). ETA=6:00:47, max mem: 11.4 GB 
[10/24 10:14:01 visual_prompt]: 	Training 500/553. train loss: 219.0303,	0.4856 s / batch. (data: 2.63e-04). ETA=5:31:38, max mem: 11.4 GB 
[10/24 10:14:47 visual_prompt]: Epoch 26 / 100: avg data time: 4.10e-01, avg batch time: 0.9012, average train loss: 105.7889
[10/24 10:15:41 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1886, average loss: 57.1493
[10/24 10:15:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.90	
[10/24 10:15:41 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[10/24 10:17:14 visual_prompt]: 	Training 100/553. train loss: 117.2340,	0.4984 s / batch. (data: 5.45e-03). ETA=5:39:07, max mem: 11.4 GB 
[10/24 10:18:43 visual_prompt]: 	Training 200/553. train loss: 41.3289,	2.2149 s / batch. (data: 1.74e+00). ETA=1 day, 1:03:16, max mem: 11.4 GB 
[10/24 10:20:12 visual_prompt]: 	Training 300/553. train loss: 51.1295,	1.1880 s / batch. (data: 6.81e-01). ETA=13:24:19, max mem: 11.4 GB 
[10/24 10:21:43 visual_prompt]: 	Training 400/553. train loss: 24.2917,	0.4933 s / batch. (data: 7.40e-04). ETA=5:33:10, max mem: 11.4 GB 
[10/24 10:23:13 visual_prompt]: 	Training 500/553. train loss: 63.5226,	0.5034 s / batch. (data: 1.12e-03). ETA=5:39:07, max mem: 11.4 GB 
[10/24 10:23:57 visual_prompt]: Epoch 27 / 100: avg data time: 4.06e-01, avg batch time: 0.8965, average train loss: 102.8620
[10/24 10:24:49 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1906, average loss: 116.0840
[10/24 10:24:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.14	
[10/24 10:24:49 visual_prompt]: Stopping early.
[10/24 10:24:49 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 10:24:49 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 10:24:49 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 10:24:49 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 10:24:49 visual_prompt]: Training with config:
[10/24 10:24:49 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr25.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 10:24:49 visual_prompt]: Loading training data...
[10/24 10:24:49 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 10:24:49 visual_prompt]: Loading validation data...
[10/24 10:24:49 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 10:24:49 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/24 10:24:52 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/24 10:24:52 visual_prompt]: tuned percent:0.529
[10/24 10:24:52 visual_prompt]: Device used for model: 0
[10/24 10:24:52 visual_prompt]: Setting up Evaluator...
[10/24 10:24:52 visual_prompt]: Setting up Trainer...
[10/24 10:24:52 visual_prompt]: 	Setting up the optimizer...
[10/24 10:24:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 10:26:25 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5238 s / batch. (data: 7.76e-03). ETA=8:01:54, max mem: 11.4 GB 
[10/24 10:27:52 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5079 s / batch. (data: 1.17e-02). ETA=7:46:23, max mem: 11.4 GB 
[10/24 10:29:24 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.0563 s / batch. (data: 2.55e+00). ETA=1 day, 22:41:36, max mem: 11.4 GB 
[10/24 10:30:51 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4805 s / batch. (data: 2.78e-04). ETA=7:19:39, max mem: 11.4 GB 
[10/24 10:32:22 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5080 s / batch. (data: 5.37e-03). ETA=7:43:59, max mem: 11.4 GB 
[10/24 10:33:09 visual_prompt]: Epoch 1 / 100: avg data time: 4.03e-01, avg batch time: 0.8974, average train loss: 1.3966
[10/24 10:34:01 visual_prompt]: Inference (val):avg data time: 3.76e-04, avg batch time: 0.1921, average loss: 1.3454
[10/24 10:34:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/24 10:34:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/24 10:35:34 visual_prompt]: 	Training 100/553. train loss: 1.4144,	1.1373 s / batch. (data: 6.42e-01). ETA=17:15:48, max mem: 11.4 GB 
[10/24 10:37:03 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.3835 s / batch. (data: 8.83e-01). ETA=20:57:46, max mem: 11.4 GB 
[10/24 10:38:34 visual_prompt]: 	Training 300/553. train loss: 21.6716,	1.6600 s / batch. (data: 1.16e+00). ETA=1 day, 1:06:21, max mem: 11.4 GB 
[10/24 10:40:01 visual_prompt]: 	Training 400/553. train loss: 9.8561,	0.4840 s / batch. (data: 5.43e-03). ETA=7:18:24, max mem: 11.4 GB 
[10/24 10:41:32 visual_prompt]: 	Training 500/553. train loss: 5.6534,	0.4800 s / batch. (data: 2.63e-04). ETA=7:13:59, max mem: 11.4 GB 
[10/24 10:42:17 visual_prompt]: Epoch 2 / 100: avg data time: 4.02e-01, avg batch time: 0.8959, average train loss: 9.2010
[10/24 10:43:10 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1923, average loss: 23.1535
[10/24 10:43:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.81	
[10/24 10:43:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/24 10:44:40 visual_prompt]: 	Training 100/553. train loss: 14.0727,	0.5082 s / batch. (data: 2.16e-02). ETA=7:38:09, max mem: 11.4 GB 
[10/24 10:46:10 visual_prompt]: 	Training 200/553. train loss: 53.4288,	0.4995 s / batch. (data: 1.17e-02). ETA=7:29:32, max mem: 11.4 GB 
[10/24 10:47:39 visual_prompt]: 	Training 300/553. train loss: 17.7029,	0.5122 s / batch. (data: 5.55e-03). ETA=7:40:04, max mem: 11.4 GB 
[10/24 10:49:09 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.5276 s / batch. (data: 3.72e-02). ETA=7:53:04, max mem: 11.4 GB 
[10/24 10:50:39 visual_prompt]: 	Training 500/553. train loss: 4.1368,	1.9000 s / batch. (data: 1.40e+00). ETA=1 day, 4:20:18, max mem: 11.4 GB 
[10/24 10:51:24 visual_prompt]: Epoch 3 / 100: avg data time: 4.01e-01, avg batch time: 0.8945, average train loss: 19.0569
[10/24 10:52:17 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1906, average loss: 32.8840
[10/24 10:52:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.77	
[10/24 10:52:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/24 10:53:51 visual_prompt]: 	Training 100/553. train loss: 19.0851,	0.4880 s / batch. (data: 2.74e-04). ETA=7:15:27, max mem: 11.4 GB 
[10/24 10:55:20 visual_prompt]: 	Training 200/553. train loss: 2.8598,	0.4881 s / batch. (data: 3.05e-04). ETA=7:14:46, max mem: 11.4 GB 
[10/24 10:56:50 visual_prompt]: 	Training 300/553. train loss: 0.9219,	1.7310 s / batch. (data: 1.26e+00). ETA=1 day, 1:38:53, max mem: 11.4 GB 
[10/24 10:58:15 visual_prompt]: 	Training 400/553. train loss: 4.7598,	0.8450 s / batch. (data: 3.49e-01). ETA=12:29:50, max mem: 11.4 GB 
[10/24 10:59:46 visual_prompt]: 	Training 500/553. train loss: 31.3253,	2.2678 s / batch. (data: 1.78e+00). ETA=1 day, 9:28:33, max mem: 11.4 GB 
[10/24 11:00:33 visual_prompt]: Epoch 4 / 100: avg data time: 4.03e-01, avg batch time: 0.8964, average train loss: 24.5239
[10/24 11:01:26 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1908, average loss: 6.3471
[10/24 11:01:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.85	
[10/24 11:01:26 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/24 11:02:57 visual_prompt]: 	Training 100/553. train loss: 215.5041,	0.5125 s / batch. (data: 1.05e-02). ETA=7:32:37, max mem: 11.4 GB 
[10/24 11:04:27 visual_prompt]: 	Training 200/553. train loss: 29.2992,	1.6714 s / batch. (data: 1.19e+00). ETA=1 day, 0:33:17, max mem: 11.4 GB 
[10/24 11:05:56 visual_prompt]: 	Training 300/553. train loss: 98.4022,	0.5327 s / batch. (data: 1.04e-02). ETA=7:48:39, max mem: 11.4 GB 
[10/24 11:07:24 visual_prompt]: 	Training 400/553. train loss: 65.7252,	0.5046 s / batch. (data: 1.05e-02). ETA=7:23:04, max mem: 11.4 GB 
[10/24 11:08:54 visual_prompt]: 	Training 500/553. train loss: 4.8875,	0.4891 s / batch. (data: 2.48e-04). ETA=7:08:40, max mem: 11.4 GB 
[10/24 11:09:49 visual_prompt]: Epoch 5 / 100: avg data time: 4.18e-01, avg batch time: 0.9105, average train loss: 35.4798
[10/24 11:10:42 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1924, average loss: 10.5287
[10/24 11:10:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.16	
[10/24 11:10:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/24 11:12:16 visual_prompt]: 	Training 100/553. train loss: 4.9048,	0.5079 s / batch. (data: 1.18e-03). ETA=7:23:51, max mem: 11.4 GB 
[10/24 11:13:44 visual_prompt]: 	Training 200/553. train loss: 0.0001,	0.5000 s / batch. (data: 2.62e-04). ETA=7:16:07, max mem: 11.4 GB 
[10/24 11:15:12 visual_prompt]: 	Training 300/553. train loss: 23.8084,	0.4830 s / batch. (data: 2.65e-04). ETA=7:00:28, max mem: 11.4 GB 
[10/24 11:16:45 visual_prompt]: 	Training 400/553. train loss: 43.0730,	0.6784 s / batch. (data: 2.06e-01). ETA=9:49:27, max mem: 11.4 GB 
[10/24 11:18:13 visual_prompt]: 	Training 500/553. train loss: 5.9760,	1.4396 s / batch. (data: 9.55e-01). ETA=20:48:30, max mem: 11.4 GB 
[10/24 11:18:58 visual_prompt]: Epoch 6 / 100: avg data time: 4.05e-01, avg batch time: 0.8974, average train loss: 45.6950
[10/24 11:19:51 visual_prompt]: Inference (val):avg data time: 1.74e-04, avg batch time: 0.1927, average loss: 28.8033
[10/24 11:19:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.67	
[10/24 11:19:51 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/24 11:21:22 visual_prompt]: 	Training 100/553. train loss: 102.2586,	0.4887 s / batch. (data: 2.94e-04). ETA=7:02:35, max mem: 11.4 GB 
[10/24 11:22:51 visual_prompt]: 	Training 200/553. train loss: 97.9314,	0.4935 s / batch. (data: 2.36e-04). ETA=7:05:55, max mem: 11.4 GB 
[10/24 11:24:23 visual_prompt]: 	Training 300/553. train loss: 133.9133,	2.3720 s / batch. (data: 1.86e+00). ETA=1 day, 10:03:10, max mem: 11.4 GB 
[10/24 11:25:53 visual_prompt]: 	Training 400/553. train loss: 25.3709,	2.2040 s / batch. (data: 1.70e+00). ETA=1 day, 7:34:45, max mem: 11.4 GB 
[10/24 11:27:20 visual_prompt]: 	Training 500/553. train loss: 59.0955,	0.5264 s / batch. (data: 1.05e-02). ETA=7:31:42, max mem: 11.4 GB 
[10/24 11:28:06 visual_prompt]: Epoch 7 / 100: avg data time: 4.03e-01, avg batch time: 0.8946, average train loss: 59.8691
[10/24 11:28:58 visual_prompt]: Inference (val):avg data time: 1.06e-04, avg batch time: 0.1921, average loss: 4.8135
[10/24 11:28:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.43	
[10/24 11:28:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/24 11:30:29 visual_prompt]: 	Training 100/553. train loss: 85.3434,	0.5157 s / batch. (data: 2.75e-04). ETA=7:21:09, max mem: 11.4 GB 
[10/24 11:32:00 visual_prompt]: 	Training 200/553. train loss: 20.0685,	0.5140 s / batch. (data: 2.82e-04). ETA=7:18:52, max mem: 11.4 GB 
[10/24 11:33:30 visual_prompt]: 	Training 300/553. train loss: 13.3705,	0.4919 s / batch. (data: 3.79e-04). ETA=6:59:12, max mem: 11.4 GB 
[10/24 11:34:59 visual_prompt]: 	Training 400/553. train loss: 99.9683,	0.5200 s / batch. (data: 7.98e-03). ETA=7:22:15, max mem: 11.4 GB 
[10/24 11:36:28 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.8156 s / batch. (data: 1.29e+00). ETA=1 day, 1:41:04, max mem: 11.4 GB 
[10/24 11:37:13 visual_prompt]: Epoch 8 / 100: avg data time: 4.03e-01, avg batch time: 0.8949, average train loss: 63.9999
[10/24 11:38:06 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1921, average loss: 32.7165
[10/24 11:38:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.81	
[10/24 11:38:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/24 11:39:38 visual_prompt]: 	Training 100/553. train loss: 0.0084,	0.5152 s / batch. (data: 1.55e-02). ETA=7:15:57, max mem: 11.4 GB 
[10/24 11:41:07 visual_prompt]: 	Training 200/553. train loss: 54.4279,	0.4726 s / batch. (data: 2.58e-04). ETA=6:39:09, max mem: 11.4 GB 
[10/24 11:42:37 visual_prompt]: 	Training 300/553. train loss: 529.9932,	2.4412 s / batch. (data: 1.93e+00). ETA=1 day, 10:17:45, max mem: 11.4 GB 
[10/24 11:44:06 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.4960 s / batch. (data: 7.96e-03). ETA=6:57:16, max mem: 11.4 GB 
[10/24 11:45:37 visual_prompt]: 	Training 500/553. train loss: 30.1462,	1.6166 s / batch. (data: 1.13e+00). ETA=22:37:16, max mem: 11.4 GB 
[10/24 11:46:21 visual_prompt]: Epoch 9 / 100: avg data time: 4.03e-01, avg batch time: 0.8944, average train loss: 77.0301
[10/24 11:47:14 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1927, average loss: 161.8617
[10/24 11:47:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.47	
[10/24 11:47:14 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/24 11:48:48 visual_prompt]: 	Training 100/553. train loss: 46.1793,	0.4747 s / batch. (data: 2.61e-04). ETA=6:37:18, max mem: 11.4 GB 
[10/24 11:50:15 visual_prompt]: 	Training 200/553. train loss: 26.2642,	0.5000 s / batch. (data: 7.96e-03). ETA=6:57:42, max mem: 11.4 GB 
[10/24 11:51:45 visual_prompt]: 	Training 300/553. train loss: 107.0382,	1.9596 s / batch. (data: 1.49e+00). ETA=1 day, 3:13:45, max mem: 11.4 GB 
[10/24 11:53:12 visual_prompt]: 	Training 400/553. train loss: 40.7335,	1.4034 s / batch. (data: 9.29e-01). ETA=19:27:40, max mem: 11.4 GB 
[10/24 11:54:42 visual_prompt]: 	Training 500/553. train loss: 84.9742,	1.5520 s / batch. (data: 1.07e+00). ETA=21:28:44, max mem: 11.4 GB 
[10/24 11:55:28 visual_prompt]: Epoch 10 / 100: avg data time: 4.02e-01, avg batch time: 0.8941, average train loss: 73.6814
[10/24 11:56:21 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1906, average loss: 54.6603
[10/24 11:56:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.49	
[10/24 11:56:21 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/24 11:57:55 visual_prompt]: 	Training 100/553. train loss: 27.3529,	0.5080 s / batch. (data: 2.51e-04). ETA=7:00:32, max mem: 11.4 GB 
[10/24 11:59:26 visual_prompt]: 	Training 200/553. train loss: 220.1405,	0.5139 s / batch. (data: 1.05e-02). ETA=7:04:32, max mem: 11.4 GB 
[10/24 12:00:54 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0460 s / batch. (data: 1.56e+00). ETA=1 day, 4:06:53, max mem: 11.4 GB 
[10/24 12:02:22 visual_prompt]: 	Training 400/553. train loss: 75.2390,	0.4795 s / batch. (data: 2.71e-04). ETA=6:34:31, max mem: 11.4 GB 
[10/24 12:03:51 visual_prompt]: 	Training 500/553. train loss: 129.3046,	0.4804 s / batch. (data: 2.57e-04). ETA=6:34:28, max mem: 11.4 GB 
[10/24 12:04:36 visual_prompt]: Epoch 11 / 100: avg data time: 4.05e-01, avg batch time: 0.8958, average train loss: 98.6327
[10/24 12:05:29 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1912, average loss: 122.1421
[10/24 12:05:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.52	
[10/24 12:05:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/24 12:07:02 visual_prompt]: 	Training 100/553. train loss: 47.3095,	0.5120 s / batch. (data: 2.72e-04). ETA=6:59:08, max mem: 11.4 GB 
[10/24 12:08:33 visual_prompt]: 	Training 200/553. train loss: 52.2904,	0.4963 s / batch. (data: 1.04e-02). ETA=6:45:26, max mem: 11.4 GB 
[10/24 12:10:01 visual_prompt]: 	Training 300/553. train loss: 43.5953,	0.5157 s / batch. (data: 2.60e-04). ETA=7:00:27, max mem: 11.4 GB 
[10/24 12:11:29 visual_prompt]: 	Training 400/553. train loss: 15.4026,	0.5031 s / batch. (data: 2.63e-04). ETA=6:49:20, max mem: 11.4 GB 
[10/24 12:12:59 visual_prompt]: 	Training 500/553. train loss: 258.5826,	0.5102 s / batch. (data: 2.64e-04). ETA=6:54:14, max mem: 11.4 GB 
[10/24 12:13:44 visual_prompt]: Epoch 12 / 100: avg data time: 4.02e-01, avg batch time: 0.8943, average train loss: 87.2183
[10/24 12:14:37 visual_prompt]: Inference (val):avg data time: 2.60e-04, avg batch time: 0.1909, average loss: 135.7415
[10/24 12:14:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.89	
[10/24 12:14:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/24 12:16:10 visual_prompt]: 	Training 100/553. train loss: 36.0140,	0.4892 s / batch. (data: 2.22e-04). ETA=6:35:56, max mem: 11.4 GB 
[10/24 12:17:36 visual_prompt]: 	Training 200/553. train loss: 295.5931,	0.7597 s / batch. (data: 2.86e-01). ETA=10:13:37, max mem: 11.4 GB 
[10/24 12:19:07 visual_prompt]: 	Training 300/553. train loss: 74.0532,	2.1600 s / batch. (data: 1.66e+00). ETA=1 day, 5:01:05, max mem: 11.4 GB 
[10/24 12:20:35 visual_prompt]: 	Training 400/553. train loss: 153.4325,	0.5005 s / batch. (data: 2.37e-04). ETA=6:42:38, max mem: 11.4 GB 
[10/24 12:22:06 visual_prompt]: 	Training 500/553. train loss: 634.8727,	0.5160 s / batch. (data: 2.47e-04). ETA=6:54:14, max mem: 11.4 GB 
[10/24 12:22:52 visual_prompt]: Epoch 13 / 100: avg data time: 4.03e-01, avg batch time: 0.8955, average train loss: 95.2155
[10/24 12:23:45 visual_prompt]: Inference (val):avg data time: 1.86e-04, avg batch time: 0.1894, average loss: 91.9932
[10/24 12:23:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.94	
[10/24 12:23:45 visual_prompt]: Best epoch 13: best metric: -91.993
[10/24 12:23:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/24 12:25:19 visual_prompt]: 	Training 100/553. train loss: 13.4421,	0.5080 s / batch. (data: 2.78e-04). ETA=6:46:30, max mem: 11.4 GB 
[10/24 12:26:49 visual_prompt]: 	Training 200/553. train loss: 17.4483,	1.9317 s / batch. (data: 1.44e+00). ETA=1 day, 1:42:30, max mem: 11.4 GB 
[10/24 12:28:18 visual_prompt]: 	Training 300/553. train loss: 135.1006,	1.3717 s / batch. (data: 8.91e-01). ETA=18:13:00, max mem: 11.4 GB 
[10/24 12:29:46 visual_prompt]: 	Training 400/553. train loss: 11.3717,	0.4920 s / batch. (data: 3.31e-04). ETA=6:31:14, max mem: 11.4 GB 
[10/24 12:31:15 visual_prompt]: 	Training 500/553. train loss: 104.9352,	0.4998 s / batch. (data: 1.18e-02). ETA=6:36:35, max mem: 11.4 GB 
[10/24 12:32:00 visual_prompt]: Epoch 14 / 100: avg data time: 4.04e-01, avg batch time: 0.8959, average train loss: 82.6294
[10/24 12:32:54 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1909, average loss: 43.7247
[10/24 12:32:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.26	
[10/24 12:32:54 visual_prompt]: Best epoch 14: best metric: -43.725
[10/24 12:32:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/24 12:34:26 visual_prompt]: 	Training 100/553. train loss: 181.6828,	0.6351 s / batch. (data: 1.54e-01). ETA=8:22:22, max mem: 11.4 GB 
[10/24 12:35:53 visual_prompt]: 	Training 200/553. train loss: 288.9369,	0.4960 s / batch. (data: 2.87e-04). ETA=6:31:29, max mem: 11.4 GB 
[10/24 12:37:24 visual_prompt]: 	Training 300/553. train loss: 201.8518,	0.4840 s / batch. (data: 2.70e-04). ETA=6:21:12, max mem: 11.4 GB 
[10/24 12:38:51 visual_prompt]: 	Training 400/553. train loss: 105.8384,	0.5040 s / batch. (data: 5.42e-03). ETA=6:36:07, max mem: 11.4 GB 
[10/24 12:40:22 visual_prompt]: 	Training 500/553. train loss: 147.6206,	0.4901 s / batch. (data: 5.38e-03). ETA=6:24:21, max mem: 11.4 GB 
[10/24 12:41:08 visual_prompt]: Epoch 15 / 100: avg data time: 4.03e-01, avg batch time: 0.8945, average train loss: 86.9840
[10/24 12:42:01 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1902, average loss: 28.0610
[10/24 12:42:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.91	
[10/24 12:42:01 visual_prompt]: Best epoch 15: best metric: -28.061
[10/24 12:42:01 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/24 12:43:33 visual_prompt]: 	Training 100/553. train loss: 18.4484,	0.5080 s / batch. (data: 2.78e-04). ETA=6:37:08, max mem: 11.4 GB 
[10/24 12:45:03 visual_prompt]: 	Training 200/553. train loss: 30.0773,	0.4960 s / batch. (data: 5.99e-04). ETA=6:26:54, max mem: 11.4 GB 
[10/24 12:46:34 visual_prompt]: 	Training 300/553. train loss: 10.4241,	0.4910 s / batch. (data: 2.64e-04). ETA=6:22:10, max mem: 11.4 GB 
[10/24 12:48:04 visual_prompt]: 	Training 400/553. train loss: 57.4855,	0.4790 s / batch. (data: 2.78e-04). ETA=6:12:02, max mem: 11.4 GB 
[10/24 12:49:34 visual_prompt]: 	Training 500/553. train loss: 155.0756,	1.9429 s / batch. (data: 1.46e+00). ETA=1 day, 1:05:52, max mem: 11.4 GB 
[10/24 12:50:19 visual_prompt]: Epoch 16 / 100: avg data time: 4.09e-01, avg batch time: 0.9011, average train loss: 91.0010
[10/24 12:51:13 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1925, average loss: 115.7068
[10/24 12:51:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.72	
[10/24 12:51:13 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/24 12:52:46 visual_prompt]: 	Training 100/553. train loss: 123.9052,	0.4880 s / batch. (data: 2.69e-04). ETA=6:16:58, max mem: 11.4 GB 
[10/24 12:54:16 visual_prompt]: 	Training 200/553. train loss: 46.0023,	0.4783 s / batch. (data: 2.63e-04). ETA=6:08:40, max mem: 11.4 GB 
[10/24 12:55:45 visual_prompt]: 	Training 300/553. train loss: 160.6408,	0.4991 s / batch. (data: 5.39e-03). ETA=6:23:55, max mem: 11.4 GB 
[10/24 12:57:14 visual_prompt]: 	Training 400/553. train loss: 190.0428,	1.6204 s / batch. (data: 1.15e+00). ETA=20:43:42, max mem: 11.4 GB 
[10/24 12:58:43 visual_prompt]: 	Training 500/553. train loss: 39.2821,	1.3315 s / batch. (data: 8.36e-01). ETA=16:59:43, max mem: 11.4 GB 
[10/24 12:59:30 visual_prompt]: Epoch 17 / 100: avg data time: 4.07e-01, avg batch time: 0.8982, average train loss: 94.7954
[10/24 13:00:23 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1909, average loss: 88.3109
[10/24 13:00:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.71	
[10/24 13:00:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/24 13:01:55 visual_prompt]: 	Training 100/553. train loss: 146.4106,	0.4968 s / batch. (data: 5.42e-03). ETA=6:19:14, max mem: 11.4 GB 
[10/24 13:03:28 visual_prompt]: 	Training 200/553. train loss: 20.6784,	0.5209 s / batch. (data: 1.60e-02). ETA=6:36:43, max mem: 11.4 GB 
[10/24 13:04:58 visual_prompt]: 	Training 300/553. train loss: 9.9667,	0.5256 s / batch. (data: 2.96e-04). ETA=6:39:25, max mem: 11.4 GB 
[10/24 13:06:27 visual_prompt]: 	Training 400/553. train loss: 8.8893,	0.4880 s / batch. (data: 2.73e-04). ETA=6:10:05, max mem: 11.4 GB 
[10/24 13:07:57 visual_prompt]: 	Training 500/553. train loss: 318.6508,	0.4997 s / batch. (data: 5.38e-03). ETA=6:18:05, max mem: 11.4 GB 
[10/24 13:08:41 visual_prompt]: Epoch 18 / 100: avg data time: 4.10e-01, avg batch time: 0.9009, average train loss: 77.2685
[10/24 13:09:35 visual_prompt]: Inference (val):avg data time: 1.97e-04, avg batch time: 0.1912, average loss: 35.1288
[10/24 13:09:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.17	
[10/24 13:09:35 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[10/24 13:11:08 visual_prompt]: 	Training 100/553. train loss: 161.0503,	1.7634 s / batch. (data: 1.26e+00). ETA=22:09:47, max mem: 11.4 GB 
[10/24 13:12:38 visual_prompt]: 	Training 200/553. train loss: 21.6254,	0.5065 s / batch. (data: 3.08e-04). ETA=6:21:05, max mem: 11.4 GB 
[10/24 13:14:08 visual_prompt]: 	Training 300/553. train loss: 167.5932,	0.4914 s / batch. (data: 2.76e-04). ETA=6:08:55, max mem: 11.4 GB 
[10/24 13:15:39 visual_prompt]: 	Training 400/553. train loss: 96.3501,	0.5023 s / batch. (data: 1.95e-02). ETA=6:16:17, max mem: 11.4 GB 
[10/24 13:17:04 visual_prompt]: 	Training 500/553. train loss: 191.1340,	0.4822 s / batch. (data: 2.82e-04). ETA=6:00:24, max mem: 11.4 GB 
[10/24 13:17:51 visual_prompt]: Epoch 19 / 100: avg data time: 4.05e-01, avg batch time: 0.8969, average train loss: 87.6934
[10/24 13:18:44 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1901, average loss: 106.8144
[10/24 13:18:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[10/24 13:18:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[10/24 13:20:14 visual_prompt]: 	Training 100/553. train loss: 26.9793,	0.5027 s / batch. (data: 1.20e-02). ETA=6:14:26, max mem: 11.4 GB 
[10/24 13:21:45 visual_prompt]: 	Training 200/553. train loss: 74.1415,	0.4786 s / batch. (data: 5.41e-03). ETA=5:55:40, max mem: 11.4 GB 
[10/24 13:23:14 visual_prompt]: 	Training 300/553. train loss: 112.5375,	0.4960 s / batch. (data: 2.98e-04). ETA=6:07:47, max mem: 11.4 GB 
[10/24 13:24:45 visual_prompt]: 	Training 400/553. train loss: 4.3639,	0.4880 s / batch. (data: 3.03e-04). ETA=6:01:04, max mem: 11.4 GB 
[10/24 13:26:13 visual_prompt]: 	Training 500/553. train loss: 260.8278,	0.4749 s / batch. (data: 2.64e-04). ETA=5:50:34, max mem: 11.4 GB 
[10/24 13:27:01 visual_prompt]: Epoch 20 / 100: avg data time: 4.08e-01, avg batch time: 0.9000, average train loss: 84.7102
[10/24 13:27:55 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1926, average loss: 122.9667
[10/24 13:27:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.18	
[10/24 13:27:55 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[10/24 13:29:29 visual_prompt]: 	Training 100/553. train loss: 4.8114,	0.4920 s / batch. (data: 2.74e-04). ETA=6:01:57, max mem: 11.4 GB 
[10/24 13:30:58 visual_prompt]: 	Training 200/553. train loss: 52.0689,	0.4920 s / batch. (data: 2.75e-04). ETA=6:01:07, max mem: 11.4 GB 
[10/24 13:32:26 visual_prompt]: 	Training 300/553. train loss: 127.5325,	1.3000 s / batch. (data: 8.06e-01). ETA=15:52:01, max mem: 11.4 GB 
[10/24 13:33:55 visual_prompt]: 	Training 400/553. train loss: 66.9447,	0.4840 s / batch. (data: 3.22e-04). ETA=5:53:38, max mem: 11.4 GB 
[10/24 13:35:24 visual_prompt]: 	Training 500/553. train loss: 71.3755,	0.5240 s / batch. (data: 7.94e-03). ETA=6:21:59, max mem: 11.4 GB 
[10/24 13:36:09 visual_prompt]: Epoch 21 / 100: avg data time: 4.03e-01, avg batch time: 0.8942, average train loss: 76.8273
[10/24 13:37:02 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1930, average loss: 27.7495
[10/24 13:37:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.42	
[10/24 13:37:02 visual_prompt]: Best epoch 21: best metric: -27.750
[10/24 13:37:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[10/24 13:38:33 visual_prompt]: 	Training 100/553. train loss: 92.8271,	0.4767 s / batch. (data: 2.76e-04). ETA=5:46:16, max mem: 11.4 GB 
[10/24 13:40:04 visual_prompt]: 	Training 200/553. train loss: 47.0318,	0.4920 s / batch. (data: 2.68e-04). ETA=5:56:36, max mem: 11.4 GB 
[10/24 13:41:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8144 s / batch. (data: 3.34e-01). ETA=9:48:52, max mem: 11.4 GB 
[10/24 13:43:00 visual_prompt]: 	Training 400/553. train loss: 104.2747,	0.4879 s / batch. (data: 5.38e-03). ETA=5:51:59, max mem: 11.4 GB 
[10/24 13:44:30 visual_prompt]: 	Training 500/553. train loss: 45.9301,	0.4893 s / batch. (data: 1.04e-02). ETA=5:52:09, max mem: 11.4 GB 
[10/24 13:45:17 visual_prompt]: Epoch 22 / 100: avg data time: 4.04e-01, avg batch time: 0.8951, average train loss: 80.4995
[10/24 13:46:10 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1920, average loss: 47.7685
[10/24 13:46:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.09	
[10/24 13:46:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[10/24 13:47:43 visual_prompt]: 	Training 100/553. train loss: 41.5792,	0.5000 s / batch. (data: 3.12e-04). ETA=5:58:37, max mem: 11.4 GB 
[10/24 13:49:13 visual_prompt]: 	Training 200/553. train loss: 9.0727,	1.4030 s / batch. (data: 9.24e-01). ETA=16:43:54, max mem: 11.4 GB 
[10/24 13:50:43 visual_prompt]: 	Training 300/553. train loss: 334.6997,	0.4774 s / batch. (data: 2.60e-04). ETA=5:40:50, max mem: 11.4 GB 
[10/24 13:52:11 visual_prompt]: 	Training 400/553. train loss: 22.5047,	0.5079 s / batch. (data: 4.56e-04). ETA=6:01:46, max mem: 11.4 GB 
[10/24 13:53:37 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.4842 s / batch. (data: 2.77e-04). ETA=5:44:02, max mem: 11.4 GB 
[10/24 13:54:24 visual_prompt]: Epoch 23 / 100: avg data time: 4.01e-01, avg batch time: 0.8933, average train loss: 87.1256
[10/24 13:55:17 visual_prompt]: Inference (val):avg data time: 3.68e-04, avg batch time: 0.1902, average loss: 47.8700
[10/24 13:55:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[10/24 13:55:17 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[10/24 13:56:47 visual_prompt]: 	Training 100/553. train loss: 123.2516,	0.4909 s / batch. (data: 2.80e-04). ETA=5:47:35, max mem: 11.4 GB 
[10/24 13:58:15 visual_prompt]: 	Training 200/553. train loss: 202.0950,	0.4994 s / batch. (data: 5.37e-03). ETA=5:52:43, max mem: 11.4 GB 
[10/24 13:59:45 visual_prompt]: 	Training 300/553. train loss: 78.4589,	1.4800 s / batch. (data: 9.77e-01). ETA=17:22:57, max mem: 11.4 GB 
[10/24 14:01:15 visual_prompt]: 	Training 400/553. train loss: 6.7210,	0.4920 s / batch. (data: 2.61e-04). ETA=5:45:54, max mem: 11.4 GB 
[10/24 14:02:46 visual_prompt]: 	Training 500/553. train loss: 40.8664,	0.8195 s / batch. (data: 3.17e-01). ETA=9:34:44, max mem: 11.4 GB 
[10/24 14:03:33 visual_prompt]: Epoch 24 / 100: avg data time: 4.05e-01, avg batch time: 0.8969, average train loss: 90.6044
[10/24 14:04:25 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1912, average loss: 311.8879
[10/24 14:04:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.77	
[10/24 14:04:25 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[10/24 14:06:01 visual_prompt]: 	Training 100/553. train loss: 20.7861,	0.4941 s / batch. (data: 2.54e-04). ETA=5:45:17, max mem: 11.4 GB 
[10/24 14:07:28 visual_prompt]: 	Training 200/553. train loss: 0.0010,	0.5260 s / batch. (data: 3.39e-02). ETA=6:06:40, max mem: 11.4 GB 
[10/24 14:08:56 visual_prompt]: 	Training 300/553. train loss: 57.1501,	0.4920 s / batch. (data: 2.74e-04). ETA=5:42:10, max mem: 11.4 GB 
[10/24 14:10:25 visual_prompt]: 	Training 400/553. train loss: 37.9154,	1.9610 s / batch. (data: 1.46e+00). ETA=22:40:34, max mem: 11.4 GB 
[10/24 14:11:55 visual_prompt]: 	Training 500/553. train loss: 25.8057,	2.1880 s / batch. (data: 1.70e+00). ETA=1 day, 1:14:22, max mem: 11.4 GB 
[10/24 14:12:40 visual_prompt]: Epoch 25 / 100: avg data time: 4.03e-01, avg batch time: 0.8949, average train loss: 73.3440
[10/24 14:13:33 visual_prompt]: Inference (val):avg data time: 2.11e-04, avg batch time: 0.1905, average loss: 149.3373
[10/24 14:13:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.56	
[10/24 14:13:33 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[10/24 14:15:05 visual_prompt]: 	Training 100/553. train loss: 39.0487,	0.5040 s / batch. (data: 7.97e-03). ETA=5:47:32, max mem: 11.4 GB 
[10/24 14:16:36 visual_prompt]: 	Training 200/553. train loss: 52.6907,	2.3577 s / batch. (data: 1.86e+00). ETA=1 day, 3:01:55, max mem: 11.4 GB 
[10/24 14:18:05 visual_prompt]: 	Training 300/553. train loss: 4.3148,	0.5159 s / batch. (data: 1.20e-02). ETA=5:54:04, max mem: 11.4 GB 
[10/24 14:19:33 visual_prompt]: 	Training 400/553. train loss: 119.0691,	0.4941 s / batch. (data: 5.39e-03). ETA=5:38:15, max mem: 11.4 GB 
[10/24 14:21:02 visual_prompt]: 	Training 500/553. train loss: 99.7272,	0.5120 s / batch. (data: 7.24e-04). ETA=5:49:39, max mem: 11.4 GB 
[10/24 14:21:49 visual_prompt]: Epoch 26 / 100: avg data time: 4.04e-01, avg batch time: 0.8959, average train loss: 66.6878
[10/24 14:22:41 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1917, average loss: 83.7482
[10/24 14:22:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.98	
[10/24 14:22:41 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[10/24 14:24:15 visual_prompt]: 	Training 100/553. train loss: 8.0442,	0.4960 s / batch. (data: 2.57e-04). ETA=5:37:27, max mem: 11.4 GB 
[10/24 14:25:44 visual_prompt]: 	Training 200/553. train loss: 24.4311,	2.2410 s / batch. (data: 1.77e+00). ETA=1 day, 1:20:58, max mem: 11.4 GB 
[10/24 14:27:14 visual_prompt]: 	Training 300/553. train loss: 127.9126,	1.2438 s / batch. (data: 7.57e-01). ETA=14:02:06, max mem: 11.4 GB 
[10/24 14:28:44 visual_prompt]: 	Training 400/553. train loss: 77.5644,	0.5161 s / batch. (data: 7.58e-04). ETA=5:48:31, max mem: 11.4 GB 
[10/24 14:30:14 visual_prompt]: 	Training 500/553. train loss: 6.1403,	0.5104 s / batch. (data: 7.53e-04). ETA=5:43:51, max mem: 11.4 GB 
[10/24 14:30:58 visual_prompt]: Epoch 27 / 100: avg data time: 4.07e-01, avg batch time: 0.8989, average train loss: 78.0643
[10/24 14:31:51 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1903, average loss: 102.3039
[10/24 14:31:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.37	
[10/24 14:31:51 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[10/24 14:33:22 visual_prompt]: 	Training 100/553. train loss: 24.5897,	0.5080 s / batch. (data: 9.11e-04). ETA=5:40:56, max mem: 11.4 GB 
[10/24 14:34:52 visual_prompt]: 	Training 200/553. train loss: 38.6389,	0.4920 s / batch. (data: 2.55e-04). ETA=5:29:23, max mem: 11.4 GB 
[10/24 14:36:22 visual_prompt]: 	Training 300/553. train loss: 109.1161,	1.5853 s / batch. (data: 1.10e+00). ETA=17:38:39, max mem: 11.4 GB 
[10/24 14:37:51 visual_prompt]: 	Training 400/553. train loss: 146.3588,	0.5279 s / batch. (data: 3.33e-04). ETA=5:51:39, max mem: 11.4 GB 
[10/24 14:39:18 visual_prompt]: 	Training 500/553. train loss: 2.5834,	0.4921 s / batch. (data: 1.04e-02). ETA=5:27:00, max mem: 11.4 GB 
[10/24 14:40:05 visual_prompt]: Epoch 28 / 100: avg data time: 4.02e-01, avg batch time: 0.8935, average train loss: 81.4525
[10/24 14:40:58 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1904, average loss: 18.4398
[10/24 14:40:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.98	
[10/24 14:40:58 visual_prompt]: Best epoch 28: best metric: -18.440
[10/24 14:40:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[10/24 14:42:36 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4800 s / batch. (data: 2.80e-04). ETA=5:17:43, max mem: 11.4 GB 
[10/24 14:44:05 visual_prompt]: 	Training 200/553. train loss: 218.7523,	2.3200 s / batch. (data: 1.84e+00). ETA=1 day, 1:31:50, max mem: 11.4 GB 
[10/24 14:45:31 visual_prompt]: 	Training 300/553. train loss: 161.3136,	0.4945 s / batch. (data: 2.96e-04). ETA=5:25:41, max mem: 11.4 GB 
[10/24 14:46:57 visual_prompt]: 	Training 400/553. train loss: 38.2146,	0.4880 s / batch. (data: 2.84e-04). ETA=5:20:35, max mem: 11.4 GB 
[10/24 14:48:27 visual_prompt]: 	Training 500/553. train loss: 74.8606,	0.4987 s / batch. (data: 2.58e-04). ETA=5:26:47, max mem: 11.4 GB 
[10/24 14:49:13 visual_prompt]: Epoch 29 / 100: avg data time: 4.02e-01, avg batch time: 0.8942, average train loss: 80.1111
[10/24 14:50:05 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1911, average loss: 78.3693
[10/24 14:50:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.62	
[10/24 14:50:05 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[10/24 14:51:37 visual_prompt]: 	Training 100/553. train loss: 11.9919,	0.4933 s / batch. (data: 5.36e-03). ETA=5:21:59, max mem: 11.4 GB 
[10/24 14:53:06 visual_prompt]: 	Training 200/553. train loss: 137.5582,	0.4861 s / batch. (data: 2.67e-04). ETA=5:16:29, max mem: 11.4 GB 
[10/24 14:54:34 visual_prompt]: 	Training 300/553. train loss: 149.0260,	0.8960 s / batch. (data: 3.90e-01). ETA=9:41:49, max mem: 11.4 GB 
[10/24 14:56:05 visual_prompt]: 	Training 400/553. train loss: 147.6129,	1.9668 s / batch. (data: 1.48e+00). ETA=21:13:57, max mem: 11.4 GB 
[10/24 14:57:34 visual_prompt]: 	Training 500/553. train loss: 24.3616,	1.9342 s / batch. (data: 1.46e+00). ETA=20:49:34, max mem: 11.4 GB 
[10/24 14:58:21 visual_prompt]: Epoch 30 / 100: avg data time: 4.05e-01, avg batch time: 0.8958, average train loss: 70.2831
[10/24 14:59:14 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1928, average loss: 21.3957
[10/24 14:59:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.89	
[10/24 14:59:14 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[10/24 15:00:47 visual_prompt]: 	Training 100/553. train loss: 88.2151,	0.4814 s / batch. (data: 3.80e-04). ETA=5:09:48, max mem: 11.4 GB 
[10/24 15:02:18 visual_prompt]: 	Training 200/553. train loss: 31.3271,	0.4880 s / batch. (data: 2.94e-04). ETA=5:13:12, max mem: 11.4 GB 
[10/24 15:03:45 visual_prompt]: 	Training 300/553. train loss: 126.6180,	0.5040 s / batch. (data: 2.66e-04). ETA=5:22:38, max mem: 11.4 GB 
[10/24 15:05:14 visual_prompt]: 	Training 400/553. train loss: 10.9146,	0.8641 s / batch. (data: 3.71e-01). ETA=9:11:44, max mem: 11.4 GB 
[10/24 15:06:43 visual_prompt]: 	Training 500/553. train loss: 69.2495,	0.5040 s / batch. (data: 2.63e-04). ETA=5:20:58, max mem: 11.4 GB 
[10/24 15:07:28 visual_prompt]: Epoch 31 / 100: avg data time: 4.01e-01, avg batch time: 0.8934, average train loss: 75.5102
[10/24 15:08:21 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1924, average loss: 228.6017
[10/24 15:08:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.53	
[10/24 15:08:21 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[10/24 15:09:54 visual_prompt]: 	Training 100/553. train loss: 11.5929,	0.5040 s / batch. (data: 2.66e-04). ETA=5:19:40, max mem: 11.4 GB 
[10/24 15:11:23 visual_prompt]: 	Training 200/553. train loss: 7.3268,	0.5041 s / batch. (data: 3.40e-04). ETA=5:18:53, max mem: 11.4 GB 
[10/24 15:12:56 visual_prompt]: 	Training 300/553. train loss: 26.0499,	0.4751 s / batch. (data: 2.68e-04). ETA=4:59:44, max mem: 11.4 GB 
[10/24 15:14:24 visual_prompt]: 	Training 400/553. train loss: 6.0523,	0.4960 s / batch. (data: 2.76e-04). ETA=5:12:06, max mem: 11.4 GB 
[10/24 15:15:50 visual_prompt]: 	Training 500/553. train loss: 112.6538,	0.4768 s / batch. (data: 2.69e-04). ETA=4:59:16, max mem: 11.4 GB 
[10/24 15:16:35 visual_prompt]: Epoch 32 / 100: avg data time: 4.02e-01, avg batch time: 0.8937, average train loss: 76.9500
[10/24 15:17:28 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1911, average loss: 61.4165
[10/24 15:17:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.51	
[10/24 15:17:28 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[10/24 15:18:59 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.9469 s / batch. (data: 4.52e-01). ETA=9:51:50, max mem: 11.4 GB 
[10/24 15:20:30 visual_prompt]: 	Training 200/553. train loss: 18.4207,	2.4572 s / batch. (data: 1.96e+00). ETA=1 day, 1:31:48, max mem: 11.4 GB 
[10/24 15:22:00 visual_prompt]: 	Training 300/553. train loss: 66.6846,	0.4768 s / batch. (data: 2.71e-04). ETA=4:56:25, max mem: 11.4 GB 
[10/24 15:23:30 visual_prompt]: 	Training 400/553. train loss: 42.1940,	0.4761 s / batch. (data: 2.76e-04). ETA=4:55:11, max mem: 11.4 GB 
[10/24 15:24:58 visual_prompt]: 	Training 500/553. train loss: 143.2521,	0.6101 s / batch. (data: 1.25e-01). ETA=6:17:17, max mem: 11.4 GB 
[10/24 15:25:44 visual_prompt]: Epoch 33 / 100: avg data time: 4.05e-01, avg batch time: 0.8971, average train loss: 91.9204
[10/24 15:26:37 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1905, average loss: 36.3803
[10/24 15:26:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.42	
[10/24 15:26:37 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[10/24 15:28:12 visual_prompt]: 	Training 100/553. train loss: 50.8696,	1.6019 s / batch. (data: 1.09e+00). ETA=16:26:33, max mem: 11.4 GB 
[10/24 15:29:39 visual_prompt]: 	Training 200/553. train loss: 94.2905,	0.9440 s / batch. (data: 4.50e-01). ETA=9:39:48, max mem: 11.4 GB 
[10/24 15:31:07 visual_prompt]: 	Training 300/553. train loss: 33.3775,	0.5520 s / batch. (data: 6.28e-02). ETA=5:38:07, max mem: 11.4 GB 
[10/24 15:32:37 visual_prompt]: 	Training 400/553. train loss: 19.4890,	0.5170 s / batch. (data: 2.50e-02). ETA=5:15:50, max mem: 11.4 GB 
[10/24 15:34:07 visual_prompt]: 	Training 500/553. train loss: 14.3941,	2.1440 s / batch. (data: 1.66e+00). ETA=21:46:04, max mem: 11.4 GB 
[10/24 15:34:53 visual_prompt]: Epoch 34 / 100: avg data time: 4.04e-01, avg batch time: 0.8959, average train loss: 77.8273
[10/24 15:35:46 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1920, average loss: 13.1825
[10/24 15:35:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.30	
[10/24 15:35:46 visual_prompt]: Best epoch 34: best metric: -13.183
[10/24 15:35:46 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[10/24 15:37:20 visual_prompt]: 	Training 100/553. train loss: 53.0963,	0.5320 s / batch. (data: 2.75e-04). ETA=5:22:43, max mem: 11.4 GB 
[10/24 15:38:50 visual_prompt]: 	Training 200/553. train loss: 15.8125,	1.1511 s / batch. (data: 6.64e-01). ETA=11:36:24, max mem: 11.4 GB 
[10/24 15:40:18 visual_prompt]: 	Training 300/553. train loss: 30.8947,	0.5000 s / batch. (data: 2.84e-04). ETA=5:01:38, max mem: 11.4 GB 
[10/24 15:41:46 visual_prompt]: 	Training 400/553. train loss: 145.9767,	1.3128 s / batch. (data: 8.26e-01). ETA=13:09:47, max mem: 11.4 GB 
[10/24 15:43:14 visual_prompt]: 	Training 500/553. train loss: 29.3083,	0.5040 s / batch. (data: 1.66e-02). ETA=5:02:23, max mem: 11.4 GB 
[10/24 15:44:02 visual_prompt]: Epoch 35 / 100: avg data time: 4.05e-01, avg batch time: 0.8966, average train loss: 69.4287
[10/24 15:44:55 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1931, average loss: 39.3369
[10/24 15:44:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.55	
[10/24 15:44:55 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[10/24 15:46:27 visual_prompt]: 	Training 100/553. train loss: 95.9259,	1.0823 s / batch. (data: 6.07e-01). ETA=10:46:36, max mem: 11.4 GB 
[10/24 15:48:00 visual_prompt]: 	Training 200/553. train loss: 91.6414,	0.4880 s / batch. (data: 2.60e-04). ETA=4:50:44, max mem: 11.4 GB 
[10/24 15:49:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.4805 s / batch. (data: 2.56e-04). ETA=4:45:27, max mem: 11.4 GB 
[10/24 15:51:01 visual_prompt]: 	Training 400/553. train loss: 65.1461,	0.4906 s / batch. (data: 5.40e-03). ETA=4:50:36, max mem: 11.4 GB 
[10/24 15:52:31 visual_prompt]: 	Training 500/553. train loss: 21.6512,	1.5855 s / batch. (data: 1.09e+00). ETA=15:36:37, max mem: 11.4 GB 
[10/24 15:53:14 visual_prompt]: Epoch 36 / 100: avg data time: 4.10e-01, avg batch time: 0.9017, average train loss: 66.1834
[10/24 15:54:07 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1916, average loss: 99.6432
[10/24 15:54:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.43	
[10/24 15:54:07 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[10/24 15:55:39 visual_prompt]: 	Training 100/553. train loss: 82.5378,	0.4851 s / batch. (data: 5.39e-03). ETA=4:45:20, max mem: 11.4 GB 
[10/24 15:57:08 visual_prompt]: 	Training 200/553. train loss: 12.7102,	0.5194 s / batch. (data: 1.20e-02). ETA=5:04:40, max mem: 11.4 GB 
[10/24 15:58:38 visual_prompt]: 	Training 300/553. train loss: 3.2744,	2.1558 s / batch. (data: 1.67e+00). ETA=21:00:50, max mem: 11.4 GB 
[10/24 16:00:09 visual_prompt]: 	Training 400/553. train loss: 68.0100,	2.5244 s / batch. (data: 2.05e+00). ETA=1 day, 0:32:13, max mem: 11.4 GB 
[10/24 16:01:36 visual_prompt]: 	Training 500/553. train loss: 17.1749,	1.6823 s / batch. (data: 1.19e+00). ETA=16:18:20, max mem: 11.4 GB 
[10/24 16:02:22 visual_prompt]: Epoch 37 / 100: avg data time: 4.03e-01, avg batch time: 0.8947, average train loss: 73.7053
[10/24 16:03:15 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1912, average loss: 57.6861
[10/24 16:03:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.55	
[10/24 16:03:15 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[10/24 16:04:45 visual_prompt]: 	Training 100/553. train loss: 61.0070,	1.0360 s / batch. (data: 5.33e-01). ETA=9:59:49, max mem: 11.4 GB 
[10/24 16:06:15 visual_prompt]: 	Training 200/553. train loss: 117.6867,	0.7881 s / batch. (data: 3.09e-01). ETA=7:34:57, max mem: 11.4 GB 
[10/24 16:07:46 visual_prompt]: 	Training 300/553. train loss: 263.7975,	0.4920 s / batch. (data: 2.75e-04). ETA=4:43:11, max mem: 11.4 GB 
[10/24 16:09:13 visual_prompt]: 	Training 400/553. train loss: 0.0000,	1.1922 s / batch. (data: 6.90e-01). ETA=11:24:17, max mem: 11.4 GB 
[10/24 16:10:45 visual_prompt]: 	Training 500/553. train loss: 62.2588,	0.5071 s / batch. (data: 7.40e-04). ETA=4:50:13, max mem: 11.4 GB 
[10/24 16:11:29 visual_prompt]: Epoch 38 / 100: avg data time: 4.02e-01, avg batch time: 0.8945, average train loss: 67.5955
[10/24 16:12:22 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1904, average loss: 43.2978
[10/24 16:12:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.29	
[10/24 16:12:22 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[10/24 16:13:54 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4736 s / batch. (data: 2.00e-04). ETA=4:29:51, max mem: 11.4 GB 
[10/24 16:15:28 visual_prompt]: 	Training 200/553. train loss: 47.3243,	0.4825 s / batch. (data: 2.82e-04). ETA=4:34:07, max mem: 11.4 GB 
[10/24 16:17:00 visual_prompt]: 	Training 300/553. train loss: 286.0663,	0.5114 s / batch. (data: 2.75e-02). ETA=4:49:41, max mem: 11.4 GB 
[10/24 16:18:28 visual_prompt]: 	Training 400/553. train loss: 30.6010,	0.4992 s / batch. (data: 2.99e-04). ETA=4:41:56, max mem: 11.4 GB 
[10/24 16:19:58 visual_prompt]: 	Training 500/553. train loss: 29.6834,	2.3223 s / batch. (data: 1.85e+00). ETA=21:47:40, max mem: 11.4 GB 
[10/24 16:20:40 visual_prompt]: Epoch 39 / 100: avg data time: 4.09e-01, avg batch time: 0.9011, average train loss: 67.6776
[10/24 16:21:33 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1915, average loss: 30.1976
[10/24 16:21:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.95	
[10/24 16:21:33 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[10/24 16:23:06 visual_prompt]: 	Training 100/553. train loss: 201.7407,	0.4920 s / batch. (data: 3.55e-04). ETA=4:35:46, max mem: 11.4 GB 
[10/24 16:24:35 visual_prompt]: 	Training 200/553. train loss: 27.5724,	0.5000 s / batch. (data: 2.87e-04). ETA=4:39:26, max mem: 11.4 GB 
[10/24 16:26:04 visual_prompt]: 	Training 300/553. train loss: 14.2423,	0.5048 s / batch. (data: 2.66e-04). ETA=4:41:17, max mem: 11.4 GB 
[10/24 16:27:34 visual_prompt]: 	Training 400/553. train loss: 27.6884,	0.4975 s / batch. (data: 1.05e-02). ETA=4:36:23, max mem: 11.4 GB 
[10/24 16:29:03 visual_prompt]: 	Training 500/553. train loss: 41.1520,	0.4905 s / batch. (data: 2.55e-04). ETA=4:31:41, max mem: 11.4 GB 
[10/24 16:29:50 visual_prompt]: Epoch 40 / 100: avg data time: 4.07e-01, avg batch time: 0.8980, average train loss: 60.7833
[10/24 16:30:43 visual_prompt]: Inference (val):avg data time: 1.83e-04, avg batch time: 0.1917, average loss: 31.9268
[10/24 16:30:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.93	
[10/24 16:30:43 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[10/24 16:32:19 visual_prompt]: 	Training 100/553. train loss: 41.1855,	0.4881 s / batch. (data: 2.27e-04). ETA=4:29:06, max mem: 11.4 GB 
[10/24 16:33:51 visual_prompt]: 	Training 200/553. train loss: 33.4603,	0.4741 s / batch. (data: 2.90e-04). ETA=4:20:36, max mem: 11.4 GB 
[10/24 16:35:19 visual_prompt]: 	Training 300/553. train loss: 241.7692,	0.4873 s / batch. (data: 2.55e-04). ETA=4:27:01, max mem: 11.4 GB 
[10/24 16:36:47 visual_prompt]: 	Training 400/553. train loss: 26.1111,	0.5025 s / batch. (data: 5.44e-03). ETA=4:34:31, max mem: 11.4 GB 
[10/24 16:38:14 visual_prompt]: 	Training 500/553. train loss: 23.8274,	0.5126 s / batch. (data: 5.98e-03). ETA=4:39:13, max mem: 11.4 GB 
[10/24 16:38:58 visual_prompt]: Epoch 41 / 100: avg data time: 4.04e-01, avg batch time: 0.8956, average train loss: 63.3080
[10/24 16:39:51 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1911, average loss: 95.8241
[10/24 16:39:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.60	
[10/24 16:39:51 visual_prompt]: Stopping early.
[10/24 16:39:51 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 16:39:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 16:39:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 16:39:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 16:39:51 visual_prompt]: Training with config:
[10/24 16:39:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr25.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 16:39:51 visual_prompt]: Loading training data...
[10/24 16:39:51 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 16:39:51 visual_prompt]: Loading validation data...
[10/24 16:39:51 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 16:39:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/24 16:39:54 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/24 16:39:54 visual_prompt]: tuned percent:0.529
[10/24 16:39:54 visual_prompt]: Device used for model: 0
[10/24 16:39:54 visual_prompt]: Setting up Evaluator...
[10/24 16:39:54 visual_prompt]: Setting up Trainer...
[10/24 16:39:54 visual_prompt]: 	Setting up the optimizer...
[10/24 16:39:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 16:41:27 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5107 s / batch. (data: 2.06e-02). ETA=7:49:48, max mem: 11.4 GB 
[10/24 16:42:55 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5160 s / batch. (data: 7.98e-03). ETA=7:53:51, max mem: 11.4 GB 
[10/24 16:44:29 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.0240 s / batch. (data: 2.53e+00). ETA=1 day, 22:12:01, max mem: 11.4 GB 
[10/24 16:45:55 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4871 s / batch. (data: 2.89e-04). ETA=7:25:39, max mem: 11.4 GB 
[10/24 16:47:28 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4877 s / batch. (data: 2.66e-04). ETA=7:25:26, max mem: 11.4 GB 
[10/24 16:48:14 visual_prompt]: Epoch 1 / 100: avg data time: 4.10e-01, avg batch time: 0.9047, average train loss: 1.3966
[10/24 16:49:07 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1919, average loss: 1.3454
[10/24 16:49:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/24 16:49:07 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/24 16:50:39 visual_prompt]: 	Training 100/553. train loss: 9.3639,	0.7533 s / batch. (data: 2.45e-01). ETA=11:26:06, max mem: 11.4 GB 
[10/24 16:52:08 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.4945 s / batch. (data: 1.01e+00). ETA=22:38:40, max mem: 11.4 GB 
[10/24 16:53:42 visual_prompt]: 	Training 300/553. train loss: 4.6903,	1.7038 s / batch. (data: 1.22e+00). ETA=1 day, 1:46:07, max mem: 11.4 GB 
[10/24 16:55:09 visual_prompt]: 	Training 400/553. train loss: 5.2198,	0.5040 s / batch. (data: 2.83e-04). ETA=7:36:28, max mem: 11.4 GB 
[10/24 16:56:43 visual_prompt]: 	Training 500/553. train loss: 1.7917,	0.5309 s / batch. (data: 2.63e-04). ETA=7:59:59, max mem: 11.4 GB 
[10/24 16:57:28 visual_prompt]: Epoch 2 / 100: avg data time: 4.10e-01, avg batch time: 0.9053, average train loss: 7.2045
[10/24 16:58:21 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1912, average loss: 1.9847
[10/24 16:58:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.79	
[10/24 16:58:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/24 16:59:52 visual_prompt]: 	Training 100/553. train loss: 50.5212,	0.4747 s / batch. (data: 2.67e-04). ETA=7:07:59, max mem: 11.4 GB 
[10/24 17:01:22 visual_prompt]: 	Training 200/553. train loss: 2.3044,	0.6640 s / batch. (data: 1.35e-01). ETA=9:57:32, max mem: 11.4 GB 
[10/24 17:02:51 visual_prompt]: 	Training 300/553. train loss: 27.1707,	0.5181 s / batch. (data: 1.01e-02). ETA=7:45:23, max mem: 11.4 GB 
[10/24 17:04:21 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.5217 s / batch. (data: 2.78e-04). ETA=7:47:43, max mem: 11.4 GB 
[10/24 17:05:51 visual_prompt]: 	Training 500/553. train loss: 2.3697,	1.8621 s / batch. (data: 1.39e+00). ETA=1 day, 3:46:24, max mem: 11.4 GB 
[10/24 17:06:36 visual_prompt]: Epoch 3 / 100: avg data time: 4.03e-01, avg batch time: 0.8954, average train loss: 19.1704
[10/24 17:07:29 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1908, average loss: 1.8866
[10/24 17:07:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.04	
[10/24 17:07:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/24 17:09:02 visual_prompt]: 	Training 100/553. train loss: 62.4385,	0.4900 s / batch. (data: 1.60e-02). ETA=7:17:13, max mem: 11.4 GB 
[10/24 17:10:34 visual_prompt]: 	Training 200/553. train loss: 6.1093,	0.4842 s / batch. (data: 2.74e-04). ETA=7:11:15, max mem: 11.4 GB 
[10/24 17:12:04 visual_prompt]: 	Training 300/553. train loss: 8.3027,	2.1440 s / batch. (data: 1.63e+00). ETA=1 day, 7:46:03, max mem: 11.4 GB 
[10/24 17:13:30 visual_prompt]: 	Training 400/553. train loss: 6.2670,	1.9400 s / batch. (data: 1.45e+00). ETA=1 day, 4:41:27, max mem: 11.4 GB 
[10/24 17:15:01 visual_prompt]: 	Training 500/553. train loss: 63.9398,	3.8109 s / batch. (data: 3.33e+00). ETA=2 days, 8:15:14, max mem: 11.4 GB 
[10/24 17:15:47 visual_prompt]: Epoch 4 / 100: avg data time: 4.06e-01, avg batch time: 0.8997, average train loss: 22.4203
[10/24 17:16:39 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1909, average loss: 35.4762
[10/24 17:16:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.41	
[10/24 17:16:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/24 17:18:11 visual_prompt]: 	Training 100/553. train loss: 4.3146,	0.4919 s / batch. (data: 2.84e-04). ETA=7:14:27, max mem: 11.4 GB 
[10/24 17:19:41 visual_prompt]: 	Training 200/553. train loss: 24.9217,	1.8720 s / batch. (data: 1.38e+00). ETA=1 day, 3:30:06, max mem: 11.4 GB 
[10/24 17:21:11 visual_prompt]: 	Training 300/553. train loss: 177.9636,	0.5146 s / batch. (data: 2.19e-02). ETA=7:32:44, max mem: 11.4 GB 
[10/24 17:22:40 visual_prompt]: 	Training 400/553. train loss: 27.6656,	0.5022 s / batch. (data: 6.21e-03). ETA=7:21:02, max mem: 11.4 GB 
[10/24 17:24:10 visual_prompt]: 	Training 500/553. train loss: 4.2376,	0.4793 s / batch. (data: 2.77e-04). ETA=7:00:03, max mem: 11.4 GB 
[10/24 17:24:57 visual_prompt]: Epoch 5 / 100: avg data time: 4.06e-01, avg batch time: 0.8991, average train loss: 33.8492
[10/24 17:25:50 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1927, average loss: 61.2850
[10/24 17:25:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.97	
[10/24 17:25:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/24 17:27:23 visual_prompt]: 	Training 100/553. train loss: 41.0749,	0.4864 s / batch. (data: 7.24e-04). ETA=7:05:04, max mem: 11.4 GB 
[10/24 17:28:52 visual_prompt]: 	Training 200/553. train loss: 2.0606,	0.4918 s / batch. (data: 2.79e-04). ETA=7:08:57, max mem: 11.4 GB 
[10/24 17:30:20 visual_prompt]: 	Training 300/553. train loss: 12.4434,	0.4960 s / batch. (data: 2.79e-04). ETA=7:11:48, max mem: 11.4 GB 
[10/24 17:31:54 visual_prompt]: 	Training 400/553. train loss: 37.3274,	1.2629 s / batch. (data: 7.78e-01). ETA=18:17:22, max mem: 11.4 GB 
[10/24 17:33:22 visual_prompt]: 	Training 500/553. train loss: 4.4406,	1.5432 s / batch. (data: 1.06e+00). ETA=22:18:22, max mem: 11.4 GB 
[10/24 17:34:14 visual_prompt]: Epoch 6 / 100: avg data time: 4.22e-01, avg batch time: 0.9129, average train loss: 43.2409
[10/24 17:35:09 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1905, average loss: 79.7655
[10/24 17:35:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.52	
[10/24 17:35:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/24 17:36:51 visual_prompt]: 	Training 100/553. train loss: 57.2845,	0.5000 s / batch. (data: 2.64e-04). ETA=7:12:22, max mem: 11.4 GB 
[10/24 17:38:25 visual_prompt]: 	Training 200/553. train loss: 8.0880,	2.2199 s / batch. (data: 1.73e+00). ETA=1 day, 7:55:53, max mem: 11.4 GB 
[10/24 17:39:58 visual_prompt]: 	Training 300/553. train loss: 19.6643,	2.6120 s / batch. (data: 2.12e+00). ETA=1 day, 13:29:53, max mem: 11.4 GB 
[10/24 17:41:27 visual_prompt]: 	Training 400/553. train loss: 58.2700,	2.5720 s / batch. (data: 2.08e+00). ETA=1 day, 12:51:08, max mem: 11.4 GB 
[10/24 17:42:55 visual_prompt]: 	Training 500/553. train loss: 62.0851,	0.5000 s / batch. (data: 2.76e-04). ETA=7:09:01, max mem: 11.4 GB 
[10/24 17:43:39 visual_prompt]: Epoch 7 / 100: avg data time: 4.30e-01, avg batch time: 0.9219, average train loss: 58.6833
[10/24 17:44:32 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1930, average loss: 83.4228
[10/24 17:44:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.97	
[10/24 17:44:32 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/24 17:46:02 visual_prompt]: 	Training 100/553. train loss: 144.9486,	1.1600 s / batch. (data: 6.67e-01). ETA=16:32:21, max mem: 11.4 GB 
[10/24 17:47:33 visual_prompt]: 	Training 200/553. train loss: 23.2396,	0.5035 s / batch. (data: 2.12e-02). ETA=7:09:55, max mem: 11.4 GB 
[10/24 17:49:03 visual_prompt]: 	Training 300/553. train loss: 45.2404,	0.4931 s / batch. (data: 2.80e-04). ETA=7:00:11, max mem: 11.4 GB 
[10/24 17:50:43 visual_prompt]: 	Training 400/553. train loss: 8.4046,	0.4920 s / batch. (data: 2.65e-04). ETA=6:58:24, max mem: 11.4 GB 
[10/24 17:52:18 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.5009 s / batch. (data: 2.37e-02). ETA=7:05:11, max mem: 11.4 GB 
[10/24 17:53:06 visual_prompt]: Epoch 8 / 100: avg data time: 4.40e-01, avg batch time: 0.9305, average train loss: 54.4156
[10/24 17:53:59 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1915, average loss: 22.3975
[10/24 17:53:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.32	
[10/24 17:53:59 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/24 17:55:32 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.5198 s / batch. (data: 1.18e-02). ETA=7:19:51, max mem: 11.4 GB 
[10/24 17:57:13 visual_prompt]: 	Training 200/553. train loss: 5.5189,	0.4941 s / batch. (data: 2.63e-04). ETA=6:57:18, max mem: 11.4 GB 
[10/24 17:58:48 visual_prompt]: 	Training 300/553. train loss: 7.4006,	1.8320 s / batch. (data: 1.35e+00). ETA=1 day, 1:44:16, max mem: 11.4 GB 
[10/24 18:00:24 visual_prompt]: 	Training 400/553. train loss: 10.4356,	7.2635 s / batch. (data: 6.76e+00). ETA=4 days, 5:50:32, max mem: 11.4 GB 
[10/24 18:01:56 visual_prompt]: 	Training 500/553. train loss: 15.3570,	1.2255 s / batch. (data: 7.28e-01). ETA=17:08:55, max mem: 11.4 GB 
[10/24 18:02:41 visual_prompt]: Epoch 9 / 100: avg data time: 4.52e-01, avg batch time: 0.9422, average train loss: 62.1580
[10/24 18:03:34 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1926, average loss: 58.8955
[10/24 18:03:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.29	
[10/24 18:03:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/24 18:05:10 visual_prompt]: 	Training 100/553. train loss: 124.1587,	0.4840 s / batch. (data: 2.90e-04). ETA=6:45:08, max mem: 11.4 GB 
[10/24 18:06:41 visual_prompt]: 	Training 200/553. train loss: 35.9999,	0.4920 s / batch. (data: 2.42e-04). ETA=6:51:02, max mem: 11.4 GB 
[10/24 18:08:10 visual_prompt]: 	Training 300/553. train loss: 555.3799,	1.8639 s / batch. (data: 1.36e+00). ETA=1 day, 1:53:59, max mem: 11.4 GB 
[10/24 18:09:37 visual_prompt]: 	Training 400/553. train loss: 51.7717,	1.1021 s / batch. (data: 6.09e-01). ETA=15:16:57, max mem: 11.4 GB 
[10/24 18:11:07 visual_prompt]: 	Training 500/553. train loss: 53.9833,	0.6400 s / batch. (data: 1.59e-01). ETA=8:51:27, max mem: 11.4 GB 
[10/24 18:11:53 visual_prompt]: Epoch 10 / 100: avg data time: 4.12e-01, avg batch time: 0.9031, average train loss: 82.3611
[10/24 18:12:46 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1919, average loss: 32.4544
[10/24 18:12:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.55	
[10/24 18:12:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/24 18:14:20 visual_prompt]: 	Training 100/553. train loss: 150.6198,	0.4744 s / batch. (data: 2.71e-04). ETA=6:32:44, max mem: 11.4 GB 
[10/24 18:15:51 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.4960 s / batch. (data: 2.41e-04). ETA=6:49:46, max mem: 11.4 GB 
[10/24 18:17:20 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.1600 s / batch. (data: 6.70e-01). ETA=15:56:23, max mem: 11.4 GB 
[10/24 18:18:52 visual_prompt]: 	Training 400/553. train loss: 7.9685,	0.5282 s / batch. (data: 1.05e-02). ETA=7:14:36, max mem: 11.4 GB 
[10/24 18:20:20 visual_prompt]: 	Training 500/553. train loss: 34.7250,	0.4936 s / batch. (data: 1.55e-02). ETA=6:45:18, max mem: 11.4 GB 
[10/24 18:21:04 visual_prompt]: Epoch 11 / 100: avg data time: 4.08e-01, avg batch time: 0.9005, average train loss: 89.1391
[10/24 18:21:57 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.1919, average loss: 53.6538
[10/24 18:21:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.35	
[10/24 18:21:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/24 18:23:31 visual_prompt]: 	Training 100/553. train loss: 100.1812,	0.4846 s / batch. (data: 5.41e-03). ETA=6:36:42, max mem: 11.4 GB 
[10/24 18:25:01 visual_prompt]: 	Training 200/553. train loss: 6.7712,	0.4996 s / batch. (data: 2.86e-04). ETA=6:48:07, max mem: 11.4 GB 
[10/24 18:26:29 visual_prompt]: 	Training 300/553. train loss: 13.1633,	0.4841 s / batch. (data: 2.71e-04). ETA=6:34:40, max mem: 11.4 GB 
[10/24 18:27:58 visual_prompt]: 	Training 400/553. train loss: 133.7484,	0.4765 s / batch. (data: 2.68e-04). ETA=6:27:38, max mem: 11.4 GB 
[10/24 18:29:28 visual_prompt]: 	Training 500/553. train loss: 571.1797,	0.4962 s / batch. (data: 5.40e-03). ETA=6:42:53, max mem: 11.4 GB 
[10/24 18:30:12 visual_prompt]: Epoch 12 / 100: avg data time: 4.05e-01, avg batch time: 0.8960, average train loss: 82.3673
[10/24 18:31:05 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1923, average loss: 91.9113
[10/24 18:31:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.01	
[10/24 18:31:05 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/24 18:32:39 visual_prompt]: 	Training 100/553. train loss: 92.1437,	0.5023 s / batch. (data: 7.95e-03). ETA=6:46:34, max mem: 11.4 GB 
[10/24 18:34:06 visual_prompt]: 	Training 200/553. train loss: 4.0119,	0.5080 s / batch. (data: 5.40e-03). ETA=6:50:17, max mem: 11.4 GB 
[10/24 18:35:35 visual_prompt]: 	Training 300/553. train loss: 84.5033,	1.8583 s / batch. (data: 1.39e+00). ETA=1 day, 0:57:56, max mem: 11.4 GB 
[10/24 18:37:12 visual_prompt]: 	Training 400/553. train loss: 215.8399,	0.6551 s / batch. (data: 1.67e-01). ETA=8:46:59, max mem: 11.4 GB 
[10/24 18:38:46 visual_prompt]: 	Training 500/553. train loss: 10.2980,	0.5107 s / batch. (data: 2.67e-04). ETA=6:49:56, max mem: 11.4 GB 
[10/24 18:39:32 visual_prompt]: Epoch 13 / 100: avg data time: 4.25e-01, avg batch time: 0.9160, average train loss: 99.2421
[10/24 18:40:25 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1905, average loss: 4.0916
[10/24 18:40:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.82	
[10/24 18:40:25 visual_prompt]: Best epoch 13: best metric: -4.092
[10/24 18:40:25 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/24 18:41:59 visual_prompt]: 	Training 100/553. train loss: 121.7436,	0.4746 s / batch. (data: 2.68e-04). ETA=6:19:46, max mem: 11.4 GB 
[10/24 18:43:29 visual_prompt]: 	Training 200/553. train loss: 391.6388,	1.9753 s / batch. (data: 1.49e+00). ETA=1 day, 2:17:20, max mem: 11.4 GB 
[10/24 18:44:59 visual_prompt]: 	Training 300/553. train loss: 8.2331,	1.2588 s / batch. (data: 7.70e-01). ETA=16:43:06, max mem: 11.4 GB 
[10/24 18:46:27 visual_prompt]: 	Training 400/553. train loss: 44.6492,	0.5060 s / batch. (data: 1.19e-02). ETA=6:42:19, max mem: 11.4 GB 
[10/24 18:47:57 visual_prompt]: 	Training 500/553. train loss: 41.5232,	0.4941 s / batch. (data: 7.98e-03). ETA=6:32:06, max mem: 11.4 GB 
[10/24 18:48:41 visual_prompt]: Epoch 14 / 100: avg data time: 4.04e-01, avg batch time: 0.8961, average train loss: 84.7249
[10/24 18:49:35 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1912, average loss: 246.1034
[10/24 18:49:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.93	
[10/24 18:49:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/24 18:51:07 visual_prompt]: 	Training 100/553. train loss: 112.0930,	1.0366 s / batch. (data: 5.44e-01). ETA=13:39:55, max mem: 11.4 GB 
[10/24 18:52:34 visual_prompt]: 	Training 200/553. train loss: 464.8499,	0.5320 s / batch. (data: 2.65e-04). ETA=6:59:55, max mem: 11.4 GB 
[10/24 18:54:05 visual_prompt]: 	Training 300/553. train loss: 171.9429,	0.5067 s / batch. (data: 2.30e-04). ETA=6:39:05, max mem: 11.4 GB 
[10/24 18:55:32 visual_prompt]: 	Training 400/553. train loss: 12.8731,	0.4954 s / batch. (data: 5.38e-03). ETA=6:29:19, max mem: 11.4 GB 
[10/24 18:57:03 visual_prompt]: 	Training 500/553. train loss: 22.6366,	0.4960 s / batch. (data: 3.19e-04). ETA=6:28:59, max mem: 11.4 GB 
[10/24 18:57:50 visual_prompt]: Epoch 15 / 100: avg data time: 4.04e-01, avg batch time: 0.8955, average train loss: 79.0621
[10/24 18:58:43 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1902, average loss: 21.0137
[10/24 18:58:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.74	
[10/24 18:58:43 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/24 19:00:14 visual_prompt]: 	Training 100/553. train loss: 89.3564,	0.4960 s / batch. (data: 7.96e-03). ETA=6:27:45, max mem: 11.4 GB 
[10/24 19:01:44 visual_prompt]: 	Training 200/553. train loss: 42.1749,	0.5014 s / batch. (data: 5.33e-03). ETA=6:31:06, max mem: 11.4 GB 
[10/24 19:03:14 visual_prompt]: 	Training 300/553. train loss: 14.1343,	0.4960 s / batch. (data: 2.70e-04). ETA=6:26:06, max mem: 11.4 GB 
[10/24 19:04:43 visual_prompt]: 	Training 400/553. train loss: 15.3246,	0.5040 s / batch. (data: 7.59e-04). ETA=6:31:28, max mem: 11.4 GB 
[10/24 19:06:12 visual_prompt]: 	Training 500/553. train loss: 42.7122,	2.0080 s / batch. (data: 1.53e+00). ETA=1 day, 1:56:20, max mem: 11.4 GB 
[10/24 19:06:57 visual_prompt]: Epoch 16 / 100: avg data time: 4.04e-01, avg batch time: 0.8945, average train loss: 84.7468
[10/24 19:07:50 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1909, average loss: 61.5796
[10/24 19:07:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.03	
[10/24 19:07:50 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/24 19:09:23 visual_prompt]: 	Training 100/553. train loss: 39.8008,	0.4880 s / batch. (data: 2.53e-04). ETA=6:17:00, max mem: 11.4 GB 
[10/24 19:10:53 visual_prompt]: 	Training 200/553. train loss: 62.7894,	0.4781 s / batch. (data: 2.75e-04). ETA=6:08:30, max mem: 11.4 GB 
[10/24 19:12:21 visual_prompt]: 	Training 300/553. train loss: 171.9126,	0.4960 s / batch. (data: 2.56e-04). ETA=6:21:31, max mem: 11.4 GB 
[10/24 19:13:51 visual_prompt]: 	Training 400/553. train loss: 174.4243,	0.4800 s / batch. (data: 2.72e-04). ETA=6:08:24, max mem: 11.4 GB 
[10/24 19:15:20 visual_prompt]: 	Training 500/553. train loss: 4.5313,	2.2730 s / batch. (data: 1.78e+00). ETA=1 day, 5:00:50, max mem: 11.4 GB 
[10/24 19:16:06 visual_prompt]: Epoch 17 / 100: avg data time: 4.05e-01, avg batch time: 0.8969, average train loss: 77.6880
[10/24 19:16:59 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1909, average loss: 60.4318
[10/24 19:16:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.68	
[10/24 19:16:59 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/24 19:18:32 visual_prompt]: 	Training 100/553. train loss: 38.8032,	0.4902 s / batch. (data: 7.32e-04). ETA=6:14:09, max mem: 11.4 GB 
[10/24 19:20:04 visual_prompt]: 	Training 200/553. train loss: 19.1458,	0.4960 s / batch. (data: 2.72e-04). ETA=6:17:47, max mem: 11.4 GB 
[10/24 19:21:32 visual_prompt]: 	Training 300/553. train loss: 3.0321,	0.4840 s / batch. (data: 2.70e-04). ETA=6:07:49, max mem: 11.4 GB 
[10/24 19:23:01 visual_prompt]: 	Training 400/553. train loss: 359.1249,	0.4920 s / batch. (data: 2.64e-04). ETA=6:13:07, max mem: 11.4 GB 
[10/24 19:24:34 visual_prompt]: 	Training 500/553. train loss: 42.2860,	1.1720 s / batch. (data: 6.73e-01). ETA=14:46:47, max mem: 11.4 GB 
[10/24 19:25:20 visual_prompt]: Epoch 18 / 100: avg data time: 4.14e-01, avg batch time: 0.9055, average train loss: 90.7684
[10/24 19:26:13 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.1921, average loss: 54.8292
[10/24 19:26:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.84	
[10/24 19:26:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[10/24 19:27:45 visual_prompt]: 	Training 100/553. train loss: 4.9434,	0.5159 s / batch. (data: 5.92e-03). ETA=6:29:04, max mem: 11.4 GB 
[10/24 19:29:15 visual_prompt]: 	Training 200/553. train loss: 23.8520,	0.5177 s / batch. (data: 7.72e-04). ETA=6:29:31, max mem: 11.4 GB 
[10/24 19:30:44 visual_prompt]: 	Training 300/553. train loss: 152.6721,	0.5120 s / batch. (data: 7.97e-03). ETA=6:24:22, max mem: 11.4 GB 
[10/24 19:32:14 visual_prompt]: 	Training 400/553. train loss: 13.0926,	0.5120 s / batch. (data: 7.03e-04). ETA=6:23:33, max mem: 11.4 GB 
[10/24 19:33:39 visual_prompt]: 	Training 500/553. train loss: 60.3658,	0.5001 s / batch. (data: 2.42e-04). ETA=6:13:45, max mem: 11.4 GB 
[10/24 19:34:26 visual_prompt]: Epoch 19 / 100: avg data time: 4.01e-01, avg batch time: 0.8929, average train loss: 90.6033
[10/24 19:35:19 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1918, average loss: 435.9791
[10/24 19:35:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.07	
[10/24 19:35:19 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[10/24 19:36:50 visual_prompt]: 	Training 100/553. train loss: 35.6997,	1.2600 s / batch. (data: 7.76e-01). ETA=15:38:33, max mem: 11.4 GB 
[10/24 19:38:20 visual_prompt]: 	Training 200/553. train loss: 39.6797,	0.4906 s / batch. (data: 7.98e-03). ETA=6:04:36, max mem: 11.4 GB 
[10/24 19:39:50 visual_prompt]: 	Training 300/553. train loss: 24.2558,	0.4880 s / batch. (data: 2.80e-04). ETA=6:01:51, max mem: 11.4 GB 
[10/24 19:41:19 visual_prompt]: 	Training 400/553. train loss: 26.2679,	0.5029 s / batch. (data: 5.42e-03). ETA=6:12:06, max mem: 11.4 GB 
[10/24 19:42:47 visual_prompt]: 	Training 500/553. train loss: 268.2239,	0.4946 s / batch. (data: 1.04e-02). ETA=6:05:07, max mem: 11.4 GB 
[10/24 19:43:35 visual_prompt]: Epoch 20 / 100: avg data time: 4.04e-01, avg batch time: 0.8956, average train loss: 91.5481
[10/24 19:44:27 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1895, average loss: 75.4645
[10/24 19:44:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.25	
[10/24 19:44:27 visual_prompt]: Stopping early.
[10/24 19:44:27 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 19:44:27 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 19:44:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 19:44:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 19:44:27 visual_prompt]: Training with config:
[10/24 19:44:27 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr25.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 19:44:27 visual_prompt]: Loading training data...
[10/24 19:44:27 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 19:44:28 visual_prompt]: Loading validation data...
[10/24 19:44:28 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 19:44:28 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/24 19:44:30 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/24 19:44:30 visual_prompt]: tuned percent:0.529
[10/24 19:44:30 visual_prompt]: Device used for model: 0
[10/24 19:44:30 visual_prompt]: Setting up Evaluator...
[10/24 19:44:30 visual_prompt]: Setting up Trainer...
[10/24 19:44:30 visual_prompt]: 	Setting up the optimizer...
[10/24 19:44:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 19:46:02 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5020 s / batch. (data: 1.31e-03). ETA=7:41:51, max mem: 11.4 GB 
[10/24 19:47:29 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5001 s / batch. (data: 7.95e-03). ETA=7:39:15, max mem: 11.4 GB 
[10/24 19:49:01 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9531 s / batch. (data: 2.46e+00). ETA=1 day, 21:07:00, max mem: 11.4 GB 
[10/24 19:50:28 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4784 s / batch. (data: 2.72e-04). ETA=7:17:42, max mem: 11.4 GB 
[10/24 19:51:59 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4920 s / batch. (data: 2.52e-04). ETA=7:29:20, max mem: 11.4 GB 
[10/24 19:52:46 visual_prompt]: Epoch 1 / 100: avg data time: 4.01e-01, avg batch time: 0.8957, average train loss: 1.3966
[10/24 19:53:38 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.1913, average loss: 1.3454
[10/24 19:53:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/24 19:53:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/24 19:55:10 visual_prompt]: 	Training 100/553. train loss: 9.0103,	0.5560 s / batch. (data: 6.81e-02). ETA=8:26:24, max mem: 11.4 GB 
[10/24 19:56:39 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.8399 s / batch. (data: 1.35e+00). ETA=1 day, 3:52:43, max mem: 11.4 GB 
[10/24 19:58:09 visual_prompt]: 	Training 300/553. train loss: 12.5811,	1.5278 s / batch. (data: 1.03e+00). ETA=23:06:21, max mem: 11.4 GB 
[10/24 19:59:36 visual_prompt]: 	Training 400/553. train loss: 3.7553,	0.4920 s / batch. (data: 2.69e-04). ETA=7:25:37, max mem: 11.4 GB 
[10/24 20:01:07 visual_prompt]: 	Training 500/553. train loss: 10.7637,	0.4995 s / batch. (data: 2.55e-04). ETA=7:31:35, max mem: 11.4 GB 
[10/24 20:01:52 visual_prompt]: Epoch 2 / 100: avg data time: 3.99e-01, avg batch time: 0.8925, average train loss: 11.3993
[10/24 20:02:44 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.1908, average loss: 11.8284
[10/24 20:02:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.72	
[10/24 20:02:44 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/24 20:04:15 visual_prompt]: 	Training 100/553. train loss: 10.0740,	0.5045 s / batch. (data: 5.43e-03). ETA=7:34:49, max mem: 11.4 GB 
[10/24 20:05:45 visual_prompt]: 	Training 200/553. train loss: 20.8273,	0.5070 s / batch. (data: 7.88e-03). ETA=7:36:15, max mem: 11.4 GB 
[10/24 20:07:13 visual_prompt]: 	Training 300/553. train loss: 16.8902,	0.4860 s / batch. (data: 7.98e-03). ETA=7:16:34, max mem: 11.4 GB 
[10/24 20:08:44 visual_prompt]: 	Training 400/553. train loss: 17.1502,	0.5009 s / batch. (data: 5.39e-03). ETA=7:29:04, max mem: 11.4 GB 
[10/24 20:10:14 visual_prompt]: 	Training 500/553. train loss: 12.8504,	1.7920 s / batch. (data: 1.30e+00). ETA=1 day, 2:43:39, max mem: 11.4 GB 
[10/24 20:10:58 visual_prompt]: Epoch 3 / 100: avg data time: 3.99e-01, avg batch time: 0.8934, average train loss: 14.9918
[10/24 20:11:51 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1895, average loss: 13.8956
[10/24 20:11:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.86	
[10/24 20:11:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/24 20:13:25 visual_prompt]: 	Training 100/553. train loss: 51.2008,	0.4960 s / batch. (data: 2.89e-04). ETA=7:22:37, max mem: 11.4 GB 
[10/24 20:14:54 visual_prompt]: 	Training 200/553. train loss: 1.8934,	0.5040 s / batch. (data: 2.83e-04). ETA=7:28:54, max mem: 11.4 GB 
[10/24 20:16:24 visual_prompt]: 	Training 300/553. train loss: 25.3609,	1.5450 s / batch. (data: 1.06e+00). ETA=22:53:33, max mem: 11.4 GB 
[10/24 20:17:49 visual_prompt]: 	Training 400/553. train loss: 49.0374,	1.7281 s / batch. (data: 1.26e+00). ETA=1 day, 1:33:26, max mem: 11.4 GB 
[10/24 20:19:20 visual_prompt]: 	Training 500/553. train loss: 79.8514,	3.7640 s / batch. (data: 3.27e+00). ETA=2 days, 7:33:42, max mem: 11.4 GB 
[10/24 20:20:06 visual_prompt]: Epoch 4 / 100: avg data time: 4.03e-01, avg batch time: 0.8953, average train loss: 28.2459
[10/24 20:20:59 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1947, average loss: 22.4325
[10/24 20:20:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.22	
[10/24 20:20:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/24 20:22:30 visual_prompt]: 	Training 100/553. train loss: 97.4638,	0.4983 s / batch. (data: 2.68e-04). ETA=7:20:05, max mem: 11.4 GB 
[10/24 20:24:00 visual_prompt]: 	Training 200/553. train loss: 2.0365,	1.9391 s / batch. (data: 1.44e+00). ETA=1 day, 4:29:13, max mem: 11.4 GB 
[10/24 20:25:29 visual_prompt]: 	Training 300/553. train loss: 220.1147,	0.4967 s / batch. (data: 2.55e-04). ETA=7:16:57, max mem: 11.4 GB 
[10/24 20:26:57 visual_prompt]: 	Training 400/553. train loss: 13.4308,	0.5000 s / batch. (data: 7.94e-03). ETA=7:19:03, max mem: 11.4 GB 
[10/24 20:28:28 visual_prompt]: 	Training 500/553. train loss: 43.5822,	0.4901 s / batch. (data: 5.39e-03). ETA=7:09:33, max mem: 11.4 GB 
[10/24 20:29:14 visual_prompt]: Epoch 5 / 100: avg data time: 4.03e-01, avg batch time: 0.8960, average train loss: 33.1010
[10/24 20:30:07 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1922, average loss: 38.3223
[10/24 20:30:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.86	
[10/24 20:30:07 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/24 20:31:41 visual_prompt]: 	Training 100/553. train loss: 65.7055,	0.5040 s / batch. (data: 7.23e-04). ETA=7:20:27, max mem: 11.4 GB 
[10/24 20:33:09 visual_prompt]: 	Training 200/553. train loss: 42.1882,	0.4928 s / batch. (data: 1.96e-03). ETA=7:09:48, max mem: 11.4 GB 
[10/24 20:34:36 visual_prompt]: 	Training 300/553. train loss: 10.1439,	0.5069 s / batch. (data: 2.93e-04). ETA=7:21:18, max mem: 11.4 GB 
[10/24 20:36:09 visual_prompt]: 	Training 400/553. train loss: 8.3231,	1.1690 s / batch. (data: 6.66e-01). ETA=16:55:44, max mem: 11.4 GB 
[10/24 20:37:37 visual_prompt]: 	Training 500/553. train loss: 49.7916,	1.4000 s / batch. (data: 9.09e-01). ETA=20:14:11, max mem: 11.4 GB 
[10/24 20:38:22 visual_prompt]: Epoch 6 / 100: avg data time: 4.02e-01, avg batch time: 0.8942, average train loss: 45.7718
[10/24 20:39:14 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1923, average loss: 93.7853
[10/24 20:39:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.41	
[10/24 20:39:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/24 20:40:45 visual_prompt]: 	Training 100/553. train loss: 83.9491,	0.4920 s / batch. (data: 2.75e-04). ETA=7:05:25, max mem: 11.4 GB 
[10/24 20:42:14 visual_prompt]: 	Training 200/553. train loss: 85.6121,	0.4840 s / batch. (data: 2.85e-04). ETA=6:57:40, max mem: 11.4 GB 
[10/24 20:43:46 visual_prompt]: 	Training 300/553. train loss: 9.4977,	2.4748 s / batch. (data: 1.98e+00). ETA=1 day, 11:31:42, max mem: 11.4 GB 
[10/24 20:45:17 visual_prompt]: 	Training 400/553. train loss: 54.9918,	2.2760 s / batch. (data: 1.79e+00). ETA=1 day, 8:36:41, max mem: 11.4 GB 
[10/24 20:46:44 visual_prompt]: 	Training 500/553. train loss: 24.9743,	0.4870 s / batch. (data: 2.88e-04). ETA=6:57:50, max mem: 11.4 GB 
[10/24 20:47:29 visual_prompt]: Epoch 7 / 100: avg data time: 4.03e-01, avg batch time: 0.8951, average train loss: 49.8319
[10/24 20:48:23 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1900, average loss: 17.9110
[10/24 20:48:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.87	
[10/24 20:48:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/24 20:49:54 visual_prompt]: 	Training 100/553. train loss: 125.3274,	0.4920 s / batch. (data: 2.84e-04). ETA=7:00:51, max mem: 11.4 GB 
[10/24 20:51:24 visual_prompt]: 	Training 200/553. train loss: 232.2232,	0.4895 s / batch. (data: 2.69e-04). ETA=6:57:55, max mem: 11.4 GB 
[10/24 20:52:54 visual_prompt]: 	Training 300/553. train loss: 62.6382,	0.5060 s / batch. (data: 1.20e-02). ETA=7:11:11, max mem: 11.4 GB 
[10/24 20:54:24 visual_prompt]: 	Training 400/553. train loss: 53.1418,	0.8775 s / batch. (data: 3.96e-01). ETA=12:26:19, max mem: 11.4 GB 
[10/24 20:55:53 visual_prompt]: 	Training 500/553. train loss: 162.0761,	2.0078 s / batch. (data: 1.53e+00). ETA=1 day, 4:24:13, max mem: 11.4 GB 
[10/24 20:56:39 visual_prompt]: Epoch 8 / 100: avg data time: 4.06e-01, avg batch time: 0.8980, average train loss: 55.6050
[10/24 20:57:32 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1915, average loss: 1.4624
[10/24 20:57:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.08	
[10/24 20:57:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/24 20:59:09 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.5000 s / batch. (data: 2.73e-04). ETA=7:03:09, max mem: 11.4 GB 
[10/24 21:00:40 visual_prompt]: 	Training 200/553. train loss: 138.0286,	0.5080 s / batch. (data: 7.97e-03). ETA=7:09:04, max mem: 11.4 GB 
[10/24 21:02:09 visual_prompt]: 	Training 300/553. train loss: 97.0234,	0.5080 s / batch. (data: 1.19e-02). ETA=7:08:10, max mem: 11.4 GB 
[10/24 21:03:40 visual_prompt]: 	Training 400/553. train loss: 77.7686,	0.4763 s / batch. (data: 2.35e-04). ETA=6:40:39, max mem: 11.4 GB 
[10/24 21:05:09 visual_prompt]: 	Training 500/553. train loss: 58.8085,	0.4925 s / batch. (data: 5.41e-03). ETA=6:53:28, max mem: 11.4 GB 
[10/24 21:05:55 visual_prompt]: Epoch 9 / 100: avg data time: 4.17e-01, avg batch time: 0.9091, average train loss: 54.2213
[10/24 21:06:48 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1935, average loss: 13.0776
[10/24 21:06:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.32	
[10/24 21:06:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/24 21:08:22 visual_prompt]: 	Training 100/553. train loss: 69.1838,	0.4878 s / batch. (data: 2.52e-04). ETA=6:48:16, max mem: 11.4 GB 
[10/24 21:10:00 visual_prompt]: 	Training 200/553. train loss: 4.7392,	0.4984 s / batch. (data: 9.46e-03). ETA=6:56:19, max mem: 11.4 GB 
[10/24 21:11:29 visual_prompt]: 	Training 300/553. train loss: 96.5657,	0.5800 s / batch. (data: 8.37e-02). ETA=8:03:34, max mem: 11.4 GB 
[10/24 21:13:00 visual_prompt]: 	Training 400/553. train loss: 66.7592,	1.3686 s / batch. (data: 8.95e-01). ETA=18:58:45, max mem: 11.4 GB 
[10/24 21:14:32 visual_prompt]: 	Training 500/553. train loss: 269.4683,	1.5868 s / batch. (data: 1.10e+00). ETA=21:57:40, max mem: 11.4 GB 
[10/24 21:15:17 visual_prompt]: Epoch 10 / 100: avg data time: 4.30e-01, avg batch time: 0.9207, average train loss: 81.7500
[10/24 21:16:10 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1920, average loss: 3.4992
[10/24 21:16:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.63	
[10/24 21:16:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/24 21:17:44 visual_prompt]: 	Training 100/553. train loss: 45.4537,	0.4775 s / batch. (data: 2.94e-04). ETA=6:35:19, max mem: 11.4 GB 
[10/24 21:19:14 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.4832 s / batch. (data: 2.54e-04). ETA=6:39:10, max mem: 11.4 GB 
[10/24 21:20:42 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.7240 s / batch. (data: 2.19e-01). ETA=9:56:55, max mem: 11.4 GB 
[10/24 21:22:11 visual_prompt]: 	Training 400/553. train loss: 47.3242,	0.4960 s / batch. (data: 5.40e-03). ETA=6:48:05, max mem: 11.4 GB 
[10/24 21:23:39 visual_prompt]: 	Training 500/553. train loss: 26.0427,	0.5000 s / batch. (data: 2.67e-04). ETA=6:50:35, max mem: 11.4 GB 
[10/24 21:24:23 visual_prompt]: Epoch 11 / 100: avg data time: 4.01e-01, avg batch time: 0.8926, average train loss: 80.9050
[10/24 21:25:16 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1919, average loss: 44.2710
[10/24 21:25:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.81	
[10/24 21:25:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/24 21:26:50 visual_prompt]: 	Training 100/553. train loss: 52.7459,	0.4840 s / batch. (data: 2.64e-04). ETA=6:36:12, max mem: 11.4 GB 
[10/24 21:28:20 visual_prompt]: 	Training 200/553. train loss: 7.5083,	1.1151 s / batch. (data: 6.08e-01). ETA=15:10:59, max mem: 11.4 GB 
[10/24 21:29:47 visual_prompt]: 	Training 300/553. train loss: 202.3550,	0.4961 s / batch. (data: 1.04e-02). ETA=6:44:29, max mem: 11.4 GB 
[10/24 21:31:16 visual_prompt]: 	Training 400/553. train loss: 113.7843,	0.4986 s / batch. (data: 3.04e-04). ETA=6:45:39, max mem: 11.4 GB 
[10/24 21:32:46 visual_prompt]: 	Training 500/553. train loss: 96.0161,	0.4977 s / batch. (data: 7.35e-04). ETA=6:44:06, max mem: 11.4 GB 
[10/24 21:33:30 visual_prompt]: Epoch 12 / 100: avg data time: 4.02e-01, avg batch time: 0.8930, average train loss: 85.7126
[10/24 21:34:23 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.1920, average loss: 175.8783
[10/24 21:34:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.07	
[10/24 21:34:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/24 21:35:56 visual_prompt]: 	Training 100/553. train loss: 68.0847,	0.4939 s / batch. (data: 1.19e-02). ETA=6:39:45, max mem: 11.4 GB 
[10/24 21:37:22 visual_prompt]: 	Training 200/553. train loss: 55.6533,	0.5080 s / batch. (data: 2.77e-04). ETA=6:50:20, max mem: 11.4 GB 
[10/24 21:38:53 visual_prompt]: 	Training 300/553. train loss: 1.6457,	1.7280 s / batch. (data: 1.24e+00). ETA=23:12:52, max mem: 11.4 GB 
[10/24 21:40:20 visual_prompt]: 	Training 400/553. train loss: 31.7823,	0.5008 s / batch. (data: 1.56e-02). ETA=6:42:51, max mem: 11.4 GB 
[10/24 21:41:50 visual_prompt]: 	Training 500/553. train loss: 146.3144,	0.5160 s / batch. (data: 2.71e-04). ETA=6:54:12, max mem: 11.4 GB 
[10/24 21:42:36 visual_prompt]: Epoch 13 / 100: avg data time: 4.00e-01, avg batch time: 0.8919, average train loss: 105.1251
[10/24 21:43:28 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1903, average loss: 15.0812
[10/24 21:43:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.79	
[10/24 21:43:28 visual_prompt]: Best epoch 13: best metric: -15.081
[10/24 21:43:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/24 21:45:01 visual_prompt]: 	Training 100/553. train loss: 154.5119,	0.4959 s / batch. (data: 5.42e-03). ETA=6:36:49, max mem: 11.4 GB 
[10/24 21:46:30 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.5600 s / batch. (data: 1.08e+00). ETA=20:45:41, max mem: 11.4 GB 
[10/24 21:48:00 visual_prompt]: 	Training 300/553. train loss: 19.5662,	1.5272 s / batch. (data: 1.02e+00). ETA=20:16:55, max mem: 11.4 GB 
[10/24 21:49:28 visual_prompt]: 	Training 400/553. train loss: 39.0272,	0.4949 s / batch. (data: 1.63e-02). ETA=6:33:32, max mem: 11.4 GB 
[10/24 21:50:58 visual_prompt]: 	Training 500/553. train loss: 171.1529,	0.5046 s / batch. (data: 2.44e-04). ETA=6:40:23, max mem: 11.4 GB 
[10/24 21:51:42 visual_prompt]: Epoch 14 / 100: avg data time: 4.01e-01, avg batch time: 0.8927, average train loss: 72.1235
[10/24 21:52:35 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.1919, average loss: 45.8067
[10/24 21:52:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.15	
[10/24 21:52:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/24 21:54:07 visual_prompt]: 	Training 100/553. train loss: 39.3021,	0.9657 s / batch. (data: 4.69e-01). ETA=12:43:50, max mem: 11.4 GB 
[10/24 21:55:35 visual_prompt]: 	Training 200/553. train loss: 516.2618,	0.5000 s / batch. (data: 2.62e-04). ETA=6:34:38, max mem: 11.4 GB 
[10/24 21:57:05 visual_prompt]: 	Training 300/553. train loss: 137.8376,	0.4800 s / batch. (data: 2.66e-04). ETA=6:18:04, max mem: 11.4 GB 
[10/24 21:58:32 visual_prompt]: 	Training 400/553. train loss: 166.7361,	0.8120 s / batch. (data: 2.92e-01). ETA=10:38:12, max mem: 11.4 GB 
[10/24 22:00:03 visual_prompt]: 	Training 500/553. train loss: 11.0820,	0.5321 s / batch. (data: 2.07e-02). ETA=6:57:21, max mem: 11.4 GB 
[10/24 22:00:50 visual_prompt]: Epoch 15 / 100: avg data time: 4.03e-01, avg batch time: 0.8949, average train loss: 82.0234
[10/24 22:01:42 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1914, average loss: 14.1891
[10/24 22:01:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.64	
[10/24 22:01:42 visual_prompt]: Best epoch 15: best metric: -14.189
[10/24 22:01:42 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/24 22:03:13 visual_prompt]: 	Training 100/553. train loss: 47.1388,	0.4756 s / batch. (data: 2.70e-04). ETA=6:11:49, max mem: 11.4 GB 
[10/24 22:04:43 visual_prompt]: 	Training 200/553. train loss: 46.7867,	0.4880 s / batch. (data: 2.52e-04). ETA=6:20:40, max mem: 11.4 GB 
[10/24 22:06:12 visual_prompt]: 	Training 300/553. train loss: 146.0635,	0.4757 s / batch. (data: 2.56e-04). ETA=6:10:17, max mem: 11.4 GB 
[10/24 22:07:41 visual_prompt]: 	Training 400/553. train loss: 102.7923,	0.5205 s / batch. (data: 5.81e-03). ETA=6:44:19, max mem: 11.4 GB 
[10/24 22:09:10 visual_prompt]: 	Training 500/553. train loss: 159.9487,	1.8855 s / batch. (data: 1.40e+00). ETA=1 day, 0:21:24, max mem: 11.4 GB 
[10/24 22:09:56 visual_prompt]: Epoch 16 / 100: avg data time: 4.01e-01, avg batch time: 0.8920, average train loss: 87.9846
[10/24 22:10:48 visual_prompt]: Inference (val):avg data time: 4.31e-04, avg batch time: 0.1912, average loss: 114.5401
[10/24 22:10:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.38	
[10/24 22:10:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/24 22:12:20 visual_prompt]: 	Training 100/553. train loss: 32.1960,	0.4988 s / batch. (data: 2.61e-04). ETA=6:25:21, max mem: 11.4 GB 
[10/24 22:13:51 visual_prompt]: 	Training 200/553. train loss: 62.6524,	0.5000 s / batch. (data: 2.71e-04). ETA=6:25:24, max mem: 11.4 GB 
[10/24 22:15:20 visual_prompt]: 	Training 300/553. train loss: 95.1371,	0.5000 s / batch. (data: 1.05e-02). ETA=6:24:37, max mem: 11.4 GB 
[10/24 22:16:48 visual_prompt]: 	Training 400/553. train loss: 130.3736,	1.8280 s / batch. (data: 1.33e+00). ETA=23:23:02, max mem: 11.4 GB 
[10/24 22:18:16 visual_prompt]: 	Training 500/553. train loss: 124.9786,	2.0880 s / batch. (data: 1.61e+00). ETA=1 day, 2:39:08, max mem: 11.4 GB 
[10/24 22:19:03 visual_prompt]: Epoch 17 / 100: avg data time: 4.02e-01, avg batch time: 0.8935, average train loss: 91.5236
[10/24 22:19:55 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1914, average loss: 61.1448
[10/24 22:19:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.19	
[10/24 22:19:55 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/24 22:21:28 visual_prompt]: 	Training 100/553. train loss: 49.1400,	0.5040 s / batch. (data: 2.76e-04). ETA=6:24:42, max mem: 11.4 GB 
[10/24 22:22:59 visual_prompt]: 	Training 200/553. train loss: 31.4104,	0.5080 s / batch. (data: 7.28e-04). ETA=6:26:54, max mem: 11.4 GB 
[10/24 22:24:28 visual_prompt]: 	Training 300/553. train loss: 13.3056,	0.4857 s / batch. (data: 7.97e-03). ETA=6:09:06, max mem: 11.4 GB 
[10/24 22:25:57 visual_prompt]: 	Training 400/553. train loss: 50.3320,	0.4956 s / batch. (data: 2.79e-04). ETA=6:15:49, max mem: 11.4 GB 
[10/24 22:27:29 visual_prompt]: 	Training 500/553. train loss: 19.6249,	0.4840 s / batch. (data: 2.73e-04). ETA=6:06:13, max mem: 11.4 GB 
[10/24 22:28:14 visual_prompt]: Epoch 18 / 100: avg data time: 4.10e-01, avg batch time: 0.9012, average train loss: 91.7078
[10/24 22:29:06 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1914, average loss: 109.5034
[10/24 22:29:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.65	
[10/24 22:29:06 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[10/24 22:30:40 visual_prompt]: 	Training 100/553. train loss: 31.0491,	0.4727 s / batch. (data: 2.35e-04). ETA=5:56:25, max mem: 11.4 GB 
[10/24 22:32:09 visual_prompt]: 	Training 200/553. train loss: 7.9009,	0.4908 s / batch. (data: 2.43e-04). ETA=6:09:17, max mem: 11.4 GB 
[10/24 22:33:38 visual_prompt]: 	Training 300/553. train loss: 121.4067,	0.5194 s / batch. (data: 1.05e-02). ETA=6:29:58, max mem: 11.4 GB 
[10/24 22:35:09 visual_prompt]: 	Training 400/553. train loss: 76.5881,	0.4731 s / batch. (data: 2.44e-04). ETA=5:54:21, max mem: 11.4 GB 
[10/24 22:36:33 visual_prompt]: 	Training 500/553. train loss: 13.1254,	0.5120 s / batch. (data: 1.19e-02). ETA=6:22:40, max mem: 11.4 GB 
[10/24 22:37:20 visual_prompt]: Epoch 19 / 100: avg data time: 4.01e-01, avg batch time: 0.8929, average train loss: 70.5217
[10/24 22:38:13 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1916, average loss: 226.0404
[10/24 22:38:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 39.57	
[10/24 22:38:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[10/24 22:39:43 visual_prompt]: 	Training 100/553. train loss: 76.8528,	0.5811 s / batch. (data: 8.76e-02). ETA=7:12:49, max mem: 11.4 GB 
[10/24 22:41:13 visual_prompt]: 	Training 200/553. train loss: 84.3374,	0.5000 s / batch. (data: 2.68e-04). ETA=6:11:37, max mem: 11.4 GB 
[10/24 22:42:42 visual_prompt]: 	Training 300/553. train loss: 40.7145,	0.4985 s / batch. (data: 2.71e-04). ETA=6:09:39, max mem: 11.4 GB 
[10/24 22:44:11 visual_prompt]: 	Training 400/553. train loss: 39.8028,	0.4799 s / batch. (data: 2.62e-04). ETA=5:55:03, max mem: 11.4 GB 
[10/24 22:45:39 visual_prompt]: 	Training 500/553. train loss: 5.6580,	0.4998 s / batch. (data: 2.73e-04). ETA=6:08:59, max mem: 11.4 GB 
[10/24 22:46:26 visual_prompt]: Epoch 20 / 100: avg data time: 4.00e-01, avg batch time: 0.8925, average train loss: 79.3143
[10/24 22:47:19 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1926, average loss: 9.8930
[10/24 22:47:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.75	
[10/24 22:47:19 visual_prompt]: Best epoch 20: best metric: -9.893
[10/24 22:47:19 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[10/24 22:48:53 visual_prompt]: 	Training 100/553. train loss: 8.9611,	0.5078 s / batch. (data: 7.95e-03). ETA=6:13:33, max mem: 11.4 GB 
[10/24 22:50:22 visual_prompt]: 	Training 200/553. train loss: 180.8225,	0.4969 s / batch. (data: 5.40e-03). ETA=6:04:45, max mem: 11.4 GB 
[10/24 22:51:51 visual_prompt]: 	Training 300/553. train loss: 316.4112,	1.6169 s / batch. (data: 1.11e+00). ETA=19:44:05, max mem: 11.4 GB 
[10/24 22:53:18 visual_prompt]: 	Training 400/553. train loss: 229.3523,	0.4767 s / batch. (data: 3.01e-04). ETA=5:48:20, max mem: 11.4 GB 
[10/24 22:54:48 visual_prompt]: 	Training 500/553. train loss: 21.5899,	0.4840 s / batch. (data: 2.50e-04). ETA=5:52:50, max mem: 11.4 GB 
[10/24 22:55:33 visual_prompt]: Epoch 21 / 100: avg data time: 4.00e-01, avg batch time: 0.8925, average train loss: 82.2036
[10/24 22:56:25 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1894, average loss: 23.6114
[10/24 22:56:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.39	
[10/24 22:56:25 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[10/24 22:57:56 visual_prompt]: 	Training 100/553. train loss: 102.5130,	0.5081 s / batch. (data: 8.04e-03). ETA=6:09:06, max mem: 11.4 GB 
[10/24 22:59:26 visual_prompt]: 	Training 200/553. train loss: 38.8265,	0.4999 s / batch. (data: 1.60e-02). ETA=6:02:17, max mem: 11.4 GB 
[10/24 23:00:54 visual_prompt]: 	Training 300/553. train loss: 49.3306,	0.5080 s / batch. (data: 2.56e-04). ETA=6:07:19, max mem: 11.4 GB 
[10/24 23:02:23 visual_prompt]: 	Training 400/553. train loss: 836.6269,	0.4855 s / batch. (data: 2.68e-04). ETA=5:50:15, max mem: 11.4 GB 
[10/24 23:03:53 visual_prompt]: 	Training 500/553. train loss: 5.2652,	0.5294 s / batch. (data: 2.14e-02). ETA=6:21:04, max mem: 11.4 GB 
[10/24 23:04:40 visual_prompt]: Epoch 22 / 100: avg data time: 4.04e-01, avg batch time: 0.8950, average train loss: 86.7464
[10/24 23:05:33 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1908, average loss: 46.2916
[10/24 23:05:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.28	
[10/24 23:05:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[10/24 23:07:06 visual_prompt]: 	Training 100/553. train loss: 168.6465,	1.7240 s / batch. (data: 1.23e+00). ETA=20:36:31, max mem: 11.4 GB 
[10/24 23:08:37 visual_prompt]: 	Training 200/553. train loss: 7.2942,	1.3665 s / batch. (data: 8.90e-01). ETA=16:17:51, max mem: 11.4 GB 
[10/24 23:10:06 visual_prompt]: 	Training 300/553. train loss: 101.3592,	0.4765 s / batch. (data: 2.58e-04). ETA=5:40:09, max mem: 11.4 GB 
[10/24 23:11:34 visual_prompt]: 	Training 400/553. train loss: 11.7250,	0.4789 s / batch. (data: 2.97e-04). ETA=5:41:03, max mem: 11.4 GB 
[10/24 23:13:00 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.4896 s / batch. (data: 6.08e-03). ETA=5:47:53, max mem: 11.4 GB 
[10/24 23:13:46 visual_prompt]: Epoch 23 / 100: avg data time: 4.01e-01, avg batch time: 0.8922, average train loss: 74.5747
[10/24 23:14:39 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1915, average loss: 87.8993
[10/24 23:14:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.82	
[10/24 23:14:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[10/24 23:16:08 visual_prompt]: 	Training 100/553. train loss: 16.8802,	0.5132 s / batch. (data: 9.32e-03). ETA=6:03:21, max mem: 11.4 GB 
[10/24 23:17:38 visual_prompt]: 	Training 200/553. train loss: 47.2989,	0.4725 s / batch. (data: 2.77e-04). ETA=5:33:43, max mem: 11.4 GB 
[10/24 23:19:07 visual_prompt]: 	Training 300/553. train loss: 107.3427,	1.4876 s / batch. (data: 1.00e+00). ETA=17:28:16, max mem: 11.4 GB 
[10/24 23:20:39 visual_prompt]: 	Training 400/553. train loss: 56.6063,	0.4858 s / batch. (data: 2.58e-04). ETA=5:41:32, max mem: 11.4 GB 
[10/24 23:22:10 visual_prompt]: 	Training 500/553. train loss: 105.4804,	0.9280 s / batch. (data: 4.37e-01). ETA=10:50:50, max mem: 11.4 GB 
[10/24 23:22:56 visual_prompt]: Epoch 24 / 100: avg data time: 4.07e-01, avg batch time: 0.8991, average train loss: 82.8377
[10/24 23:23:49 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.1913, average loss: 155.3265
[10/24 23:23:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.11	
[10/24 23:23:49 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[10/24 23:25:25 visual_prompt]: 	Training 100/553. train loss: 53.2982,	0.5018 s / batch. (data: 1.05e-02). ETA=5:50:40, max mem: 11.4 GB 
[10/24 23:26:51 visual_prompt]: 	Training 200/553. train loss: 59.0651,	0.5400 s / batch. (data: 6.40e-02). ETA=6:16:27, max mem: 11.4 GB 
[10/24 23:28:19 visual_prompt]: 	Training 300/553. train loss: 119.5137,	1.2000 s / batch. (data: 7.04e-01). ETA=13:54:31, max mem: 11.4 GB 
[10/24 23:29:48 visual_prompt]: 	Training 400/553. train loss: 12.5546,	2.0143 s / batch. (data: 1.53e+00). ETA=23:17:31, max mem: 11.4 GB 
[10/24 23:31:18 visual_prompt]: 	Training 500/553. train loss: 111.6115,	2.0842 s / batch. (data: 1.59e+00). ETA=1 day, 0:02:34, max mem: 11.4 GB 
[10/24 23:32:02 visual_prompt]: Epoch 25 / 100: avg data time: 4.00e-01, avg batch time: 0.8918, average train loss: 112.9158
[10/24 23:32:55 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1916, average loss: 128.8839
[10/24 23:32:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.17	
[10/24 23:32:55 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[10/24 23:34:30 visual_prompt]: 	Training 100/553. train loss: 28.0691,	0.5147 s / batch. (data: 1.31e-02). ETA=5:54:55, max mem: 11.4 GB 
[10/24 23:36:05 visual_prompt]: 	Training 200/553. train loss: 309.5794,	2.1926 s / batch. (data: 1.72e+00). ETA=1 day, 1:08:17, max mem: 11.4 GB 
[10/24 23:37:38 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.4880 s / batch. (data: 3.97e-03). ETA=5:34:53, max mem: 11.4 GB 
[10/24 23:39:25 visual_prompt]: 	Training 400/553. train loss: 75.7334,	0.4825 s / batch. (data: 2.72e-04). ETA=5:30:17, max mem: 11.4 GB 
[10/24 23:40:55 visual_prompt]: 	Training 500/553. train loss: 15.8009,	0.4793 s / batch. (data: 2.79e-04). ETA=5:27:20, max mem: 11.4 GB 
[10/24 23:41:42 visual_prompt]: Epoch 26 / 100: avg data time: 4.62e-01, avg batch time: 0.9531, average train loss: 79.9608
[10/24 23:42:38 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1914, average loss: 71.9884
[10/24 23:42:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.69	
[10/24 23:42:38 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[10/24 23:44:14 visual_prompt]: 	Training 100/553. train loss: 55.6681,	0.5089 s / batch. (data: 2.35e-04). ETA=5:46:12, max mem: 11.4 GB 
[10/24 23:45:46 visual_prompt]: 	Training 200/553. train loss: 324.6107,	2.2442 s / batch. (data: 1.76e+00). ETA=1 day, 1:23:07, max mem: 11.4 GB 
[10/24 23:47:19 visual_prompt]: 	Training 300/553. train loss: 15.8990,	1.4274 s / batch. (data: 9.51e-01). ETA=16:06:22, max mem: 11.4 GB 
[10/24 23:48:51 visual_prompt]: 	Training 400/553. train loss: 25.8014,	0.5266 s / batch. (data: 7.31e-04). ETA=5:55:40, max mem: 11.4 GB 
[10/24 23:50:24 visual_prompt]: 	Training 500/553. train loss: 36.7208,	0.5040 s / batch. (data: 1.20e-02). ETA=5:39:31, max mem: 11.4 GB 
[10/24 23:51:09 visual_prompt]: Epoch 27 / 100: avg data time: 4.31e-01, avg batch time: 0.9224, average train loss: 80.0769
[10/24 23:52:02 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1926, average loss: 3.9722
[10/24 23:52:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.01	
[10/24 23:52:02 visual_prompt]: Best epoch 27: best metric: -3.972
[10/24 23:52:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[10/24 23:53:34 visual_prompt]: 	Training 100/553. train loss: 46.3771,	0.5163 s / batch. (data: 1.05e-02). ETA=5:46:32, max mem: 11.4 GB 
[10/24 23:55:04 visual_prompt]: 	Training 200/553. train loss: 135.8868,	0.5200 s / batch. (data: 1.20e-02). ETA=5:48:08, max mem: 11.4 GB 
[10/24 23:56:33 visual_prompt]: 	Training 300/553. train loss: 28.6768,	1.6968 s / batch. (data: 1.21e+00). ETA=18:53:09, max mem: 11.4 GB 
[10/24 23:58:00 visual_prompt]: 	Training 400/553. train loss: 88.6259,	0.5056 s / batch. (data: 2.16e-02). ETA=5:36:47, max mem: 11.4 GB 
[10/24 23:59:27 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.4920 s / batch. (data: 2.72e-04). ETA=5:26:55, max mem: 11.4 GB 
[10/25 00:00:15 visual_prompt]: Epoch 28 / 100: avg data time: 3.98e-01, avg batch time: 0.8907, average train loss: 74.8044
[10/25 00:01:07 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1893, average loss: 30.1356
[10/25 00:01:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.32	
[10/25 00:01:07 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[10/25 00:02:50 visual_prompt]: 	Training 100/553. train loss: 0.0496,	0.4935 s / batch. (data: 1.04e-02). ETA=5:26:40, max mem: 11.4 GB 
[10/25 00:04:22 visual_prompt]: 	Training 200/553. train loss: 23.6766,	2.4000 s / batch. (data: 1.90e+00). ETA=1 day, 2:24:37, max mem: 11.4 GB 
[10/25 00:05:52 visual_prompt]: 	Training 300/553. train loss: 119.6051,	0.5338 s / batch. (data: 1.09e-02). ETA=5:51:34, max mem: 11.4 GB 
[10/25 00:07:21 visual_prompt]: 	Training 400/553. train loss: 135.9216,	2.0722 s / batch. (data: 1.60e+00). ETA=22:41:17, max mem: 11.4 GB 
[10/25 00:08:54 visual_prompt]: 	Training 500/553. train loss: 137.2926,	0.4996 s / batch. (data: 2.60e-04). ETA=5:27:21, max mem: 11.4 GB 
[10/25 00:09:42 visual_prompt]: Epoch 29 / 100: avg data time: 4.40e-01, avg batch time: 0.9311, average train loss: 85.7261
[10/25 00:10:40 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1898, average loss: 105.8434
[10/25 00:10:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/25 00:10:40 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[10/25 00:12:13 visual_prompt]: 	Training 100/553. train loss: 85.8801,	0.5004 s / batch. (data: 2.83e-04). ETA=5:26:36, max mem: 11.4 GB 
[10/25 00:13:44 visual_prompt]: 	Training 200/553. train loss: 167.7283,	0.4966 s / batch. (data: 2.74e-04). ETA=5:23:20, max mem: 11.4 GB 
[10/25 00:15:14 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.3010 s / batch. (data: 8.07e-01). ETA=14:04:52, max mem: 11.4 GB 
[10/25 00:16:46 visual_prompt]: 	Training 400/553. train loss: 62.9080,	1.9940 s / batch. (data: 1.51e+00). ETA=21:31:33, max mem: 11.4 GB 
[10/25 00:18:15 visual_prompt]: 	Training 500/553. train loss: 154.7803,	1.8480 s / batch. (data: 1.36e+00). ETA=19:53:54, max mem: 11.4 GB 
[10/25 00:19:04 visual_prompt]: Epoch 30 / 100: avg data time: 4.20e-01, avg batch time: 0.9117, average train loss: 86.7279
[10/25 00:20:00 visual_prompt]: Inference (val):avg data time: 3.71e-04, avg batch time: 0.1914, average loss: 182.5594
[10/25 00:20:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.67	
[10/25 00:20:00 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[10/25 00:21:38 visual_prompt]: 	Training 100/553. train loss: 37.8466,	0.4938 s / batch. (data: 2.71e-04). ETA=5:17:44, max mem: 11.4 GB 
[10/25 00:23:14 visual_prompt]: 	Training 200/553. train loss: 72.8132,	0.5040 s / batch. (data: 5.39e-03). ETA=5:23:28, max mem: 11.4 GB 
[10/25 00:24:44 visual_prompt]: 	Training 300/553. train loss: 65.5203,	0.4880 s / batch. (data: 2.89e-04). ETA=5:12:23, max mem: 11.4 GB 
[10/25 00:26:17 visual_prompt]: 	Training 400/553. train loss: 66.1868,	1.1583 s / batch. (data: 6.60e-01). ETA=12:19:33, max mem: 11.4 GB 
[10/25 00:27:50 visual_prompt]: 	Training 500/553. train loss: 24.7470,	0.4960 s / batch. (data: 2.82e-04). ETA=5:15:50, max mem: 11.4 GB 
[10/25 00:28:36 visual_prompt]: Epoch 31 / 100: avg data time: 4.41e-01, avg batch time: 0.9332, average train loss: 80.6510
[10/25 00:29:31 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1888, average loss: 110.5428
[10/25 00:29:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.63	
[10/25 00:29:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[10/25 00:31:07 visual_prompt]: 	Training 100/553. train loss: 22.4220,	0.4729 s / batch. (data: 2.91e-04). ETA=4:59:58, max mem: 11.4 GB 
[10/25 00:32:37 visual_prompt]: 	Training 200/553. train loss: 40.6682,	0.5373 s / batch. (data: 9.15e-03). ETA=5:39:52, max mem: 11.4 GB 
[10/25 00:34:13 visual_prompt]: 	Training 300/553. train loss: 132.8381,	0.4731 s / batch. (data: 2.96e-04). ETA=4:58:31, max mem: 11.4 GB 
[10/25 00:35:44 visual_prompt]: 	Training 400/553. train loss: 77.1219,	0.5000 s / batch. (data: 5.39e-03). ETA=5:14:38, max mem: 11.4 GB 
[10/25 00:37:12 visual_prompt]: 	Training 500/553. train loss: 96.4586,	0.5080 s / batch. (data: 2.83e-04). ETA=5:18:49, max mem: 11.4 GB 
[10/25 00:37:58 visual_prompt]: Epoch 32 / 100: avg data time: 4.25e-01, avg batch time: 0.9173, average train loss: 64.3726
[10/25 00:38:52 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1907, average loss: 73.4194
[10/25 00:38:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.29	
[10/25 00:38:52 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[10/25 00:40:26 visual_prompt]: 	Training 100/553. train loss: 20.6104,	1.9359 s / batch. (data: 1.42e+00). ETA=20:10:05, max mem: 11.4 GB 
[10/25 00:42:00 visual_prompt]: 	Training 200/553. train loss: 12.1811,	2.5240 s / batch. (data: 2.02e+00). ETA=1 day, 2:13:27, max mem: 11.4 GB 
[10/25 00:43:29 visual_prompt]: 	Training 300/553. train loss: 13.6113,	0.5153 s / batch. (data: 5.42e-03). ETA=5:20:21, max mem: 11.4 GB 
[10/25 00:45:02 visual_prompt]: 	Training 400/553. train loss: 1.6185,	0.4888 s / batch. (data: 2.70e-04). ETA=5:03:06, max mem: 11.4 GB 
[10/25 00:46:32 visual_prompt]: 	Training 500/553. train loss: 62.6210,	0.8921 s / batch. (data: 4.11e-01). ETA=9:11:39, max mem: 11.4 GB 
[10/25 00:47:20 visual_prompt]: Epoch 33 / 100: avg data time: 4.25e-01, avg batch time: 0.9175, average train loss: 69.8640
[10/25 00:48:14 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1894, average loss: 153.3017
[10/25 00:48:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.17	
[10/25 00:48:14 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[10/25 00:49:50 visual_prompt]: 	Training 100/553. train loss: 67.1878,	1.4905 s / batch. (data: 9.96e-01). ETA=15:17:56, max mem: 11.4 GB 
[10/25 00:51:20 visual_prompt]: 	Training 200/553. train loss: 9.9216,	1.7369 s / batch. (data: 1.23e+00). ETA=17:46:47, max mem: 11.4 GB 
[10/25 00:52:50 visual_prompt]: 	Training 300/553. train loss: 115.6078,	1.4600 s / batch. (data: 9.80e-01). ETA=14:54:14, max mem: 11.4 GB 
[10/25 00:54:22 visual_prompt]: 	Training 400/553. train loss: 144.7495,	0.4861 s / batch. (data: 2.70e-04). ETA=4:56:54, max mem: 11.4 GB 
[10/25 00:55:55 visual_prompt]: 	Training 500/553. train loss: 48.5139,	2.2230 s / batch. (data: 1.75e+00). ETA=22:34:11, max mem: 11.4 GB 
[10/25 00:56:41 visual_prompt]: Epoch 34 / 100: avg data time: 4.26e-01, avg batch time: 0.9177, average train loss: 76.4473
[10/25 00:57:35 visual_prompt]: Inference (val):avg data time: 5.66e-04, avg batch time: 0.1916, average loss: 7.5529
[10/25 00:57:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.67	
[10/25 00:57:35 visual_prompt]: Stopping early.
[10/25 00:57:36 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 00:57:36 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 00:57:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 00:57:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 00:57:36 visual_prompt]: Training with config:
[10/25 00:57:36 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr25.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 00:57:36 visual_prompt]: Loading training data...
[10/25 00:57:36 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 00:57:36 visual_prompt]: Loading validation data...
[10/25 00:57:36 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 00:57:36 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/25 00:57:39 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/25 00:57:39 visual_prompt]: tuned percent:0.529
[10/25 00:57:39 visual_prompt]: Device used for model: 0
[10/25 00:57:39 visual_prompt]: Setting up Evaluator...
[10/25 00:57:39 visual_prompt]: Setting up Trainer...
[10/25 00:57:39 visual_prompt]: 	Setting up the optimizer...
[10/25 00:57:39 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 00:59:14 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4834 s / batch. (data: 2.86e-04). ETA=7:24:45, max mem: 11.4 GB 
[10/25 01:00:43 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4908 s / batch. (data: 2.72e-04). ETA=7:30:40, max mem: 11.4 GB 
[10/25 01:02:18 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.2009 s / batch. (data: 2.71e+00). ETA=2 days, 0:54:08, max mem: 11.4 GB 
[10/25 01:03:47 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4923 s / batch. (data: 1.16e-02). ETA=7:30:25, max mem: 11.4 GB 
[10/25 01:05:21 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5078 s / batch. (data: 7.55e-04). ETA=7:43:48, max mem: 11.4 GB 
[10/25 01:06:09 visual_prompt]: Epoch 1 / 100: avg data time: 4.27e-01, avg batch time: 0.9220, average train loss: 1.3966
[10/25 01:07:03 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1907, average loss: 1.3454
[10/25 01:07:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/25 01:07:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/25 01:08:38 visual_prompt]: 	Training 100/553. train loss: 3.7574,	2.2465 s / batch. (data: 1.77e+00). ETA=1 day, 10:06:05, max mem: 11.4 GB 
[10/25 01:10:08 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.9480 s / batch. (data: 4.45e-01). ETA=14:21:51, max mem: 11.4 GB 
[10/25 01:11:42 visual_prompt]: 	Training 300/553. train loss: 2.9384,	1.8040 s / batch. (data: 1.31e+00). ETA=1 day, 3:17:03, max mem: 11.4 GB 
[10/25 01:13:12 visual_prompt]: 	Training 400/553. train loss: 21.5068,	0.5040 s / batch. (data: 2.80e-04). ETA=7:36:30, max mem: 11.4 GB 
[10/25 01:14:45 visual_prompt]: 	Training 500/553. train loss: 0.3375,	0.4880 s / batch. (data: 2.69e-04). ETA=7:21:11, max mem: 11.4 GB 
[10/25 01:15:31 visual_prompt]: Epoch 2 / 100: avg data time: 4.25e-01, avg batch time: 0.9195, average train loss: 11.9425
[10/25 01:16:26 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1887, average loss: 2.7998
[10/25 01:16:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.17	
[10/25 01:16:26 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/25 01:17:59 visual_prompt]: 	Training 100/553. train loss: 2.9109,	0.6322 s / batch. (data: 1.47e-01). ETA=9:30:00, max mem: 11.4 GB 
[10/25 01:19:32 visual_prompt]: 	Training 200/553. train loss: 0.6944,	0.5040 s / batch. (data: 3.07e-04). ETA=7:33:31, max mem: 11.4 GB 
[10/25 01:21:02 visual_prompt]: 	Training 300/553. train loss: 25.8844,	0.5026 s / batch. (data: 2.44e-02). ETA=7:31:29, max mem: 11.4 GB 
[10/25 01:22:39 visual_prompt]: 	Training 400/553. train loss: 34.0588,	0.5116 s / batch. (data: 2.79e-04). ETA=7:38:40, max mem: 11.4 GB 
[10/25 01:24:13 visual_prompt]: 	Training 500/553. train loss: 7.9929,	2.2893 s / batch. (data: 1.81e+00). ETA=1 day, 10:08:41, max mem: 11.4 GB 
[10/25 01:25:00 visual_prompt]: Epoch 3 / 100: avg data time: 4.36e-01, avg batch time: 0.9291, average train loss: 16.4348
[10/25 01:25:55 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1932, average loss: 15.1165
[10/25 01:25:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.23	
[10/25 01:25:55 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/25 01:27:31 visual_prompt]: 	Training 100/553. train loss: 81.9195,	0.4840 s / batch. (data: 2.74e-04). ETA=7:11:52, max mem: 11.4 GB 
[10/25 01:29:02 visual_prompt]: 	Training 200/553. train loss: 27.1787,	0.4865 s / batch. (data: 3.04e-04). ETA=7:13:20, max mem: 11.4 GB 
[10/25 01:30:35 visual_prompt]: 	Training 300/553. train loss: 54.8082,	2.0800 s / batch. (data: 1.59e+00). ETA=1 day, 6:49:11, max mem: 11.4 GB 
[10/25 01:32:03 visual_prompt]: 	Training 400/553. train loss: 53.6265,	1.8952 s / batch. (data: 1.37e+00). ETA=1 day, 4:01:44, max mem: 11.4 GB 
[10/25 01:33:36 visual_prompt]: 	Training 500/553. train loss: 19.9017,	3.9392 s / batch. (data: 3.46e+00). ETA=2 days, 10:08:54, max mem: 11.4 GB 
[10/25 01:34:29 visual_prompt]: Epoch 4 / 100: avg data time: 4.38e-01, avg batch time: 0.9299, average train loss: 38.2704
[10/25 01:35:23 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1915, average loss: 3.0733
[10/25 01:35:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.94	
[10/25 01:35:23 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/25 01:36:56 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4991 s / batch. (data: 2.78e-04). ETA=7:20:45, max mem: 11.4 GB 
[10/25 01:38:27 visual_prompt]: 	Training 200/553. train loss: 12.2122,	1.8676 s / batch. (data: 1.38e+00). ETA=1 day, 3:26:15, max mem: 11.4 GB 
[10/25 01:39:59 visual_prompt]: 	Training 300/553. train loss: 46.4532,	0.4841 s / batch. (data: 5.53e-03). ETA=7:05:53, max mem: 11.4 GB 
[10/25 01:41:34 visual_prompt]: 	Training 400/553. train loss: 56.1236,	0.4999 s / batch. (data: 3.29e-04). ETA=7:19:01, max mem: 11.4 GB 
[10/25 01:43:12 visual_prompt]: 	Training 500/553. train loss: 29.6080,	0.4879 s / batch. (data: 3.04e-04). ETA=7:07:40, max mem: 11.4 GB 
[10/25 01:44:02 visual_prompt]: Epoch 5 / 100: avg data time: 4.45e-01, avg batch time: 0.9385, average train loss: 33.7478
[10/25 01:44:57 visual_prompt]: Inference (val):avg data time: 2.06e-04, avg batch time: 0.1921, average loss: 54.1807
[10/25 01:44:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.70	
[10/25 01:44:57 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/25 01:46:39 visual_prompt]: 	Training 100/553. train loss: 63.5943,	0.4934 s / batch. (data: 3.41e-04). ETA=7:11:10, max mem: 11.4 GB 
[10/25 01:48:15 visual_prompt]: 	Training 200/553. train loss: 139.9753,	0.4840 s / batch. (data: 2.52e-04). ETA=7:02:09, max mem: 11.4 GB 
[10/25 01:49:49 visual_prompt]: 	Training 300/553. train loss: 2.6905,	0.5003 s / batch. (data: 5.38e-03). ETA=7:15:32, max mem: 11.4 GB 
[10/25 01:51:27 visual_prompt]: 	Training 400/553. train loss: 5.8374,	1.4200 s / batch. (data: 9.36e-01). ETA=20:33:53, max mem: 11.4 GB 
[10/25 01:53:00 visual_prompt]: 	Training 500/553. train loss: 17.0413,	1.7477 s / batch. (data: 1.27e+00). ETA=1 day, 1:15:44, max mem: 11.4 GB 
[10/25 01:53:48 visual_prompt]: Epoch 6 / 100: avg data time: 4.67e-01, avg batch time: 0.9595, average train loss: 30.3771
[10/25 01:54:43 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1915, average loss: 17.2483
[10/25 01:54:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.74	
[10/25 01:54:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/25 01:56:19 visual_prompt]: 	Training 100/553. train loss: 76.6286,	0.5007 s / batch. (data: 2.52e-04). ETA=7:12:56, max mem: 11.4 GB 
[10/25 01:57:51 visual_prompt]: 	Training 200/553. train loss: 20.7502,	0.5077 s / batch. (data: 5.38e-03). ETA=7:18:11, max mem: 11.4 GB 
[10/25 01:59:27 visual_prompt]: 	Training 300/553. train loss: 78.3374,	2.1600 s / batch. (data: 1.65e+00). ETA=1 day, 7:00:32, max mem: 11.4 GB 
[10/25 02:01:01 visual_prompt]: 	Training 400/553. train loss: 52.3393,	2.4742 s / batch. (data: 1.99e+00). ETA=1 day, 11:27:06, max mem: 11.4 GB 
[10/25 02:02:31 visual_prompt]: 	Training 500/553. train loss: 10.9412,	0.4982 s / batch. (data: 2.83e-04). ETA=7:07:27, max mem: 11.4 GB 
[10/25 02:03:19 visual_prompt]: Epoch 7 / 100: avg data time: 4.41e-01, avg batch time: 0.9326, average train loss: 45.2190
[10/25 02:04:15 visual_prompt]: Inference (val):avg data time: 1.93e-04, avg batch time: 0.1895, average loss: 46.4493
[10/25 02:04:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.91	
[10/25 02:04:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/25 02:05:48 visual_prompt]: 	Training 100/553. train loss: 77.2251,	0.4923 s / batch. (data: 1.19e-02). ETA=7:01:08, max mem: 11.4 GB 
[10/25 02:07:22 visual_prompt]: 	Training 200/553. train loss: 0.0009,	0.4966 s / batch. (data: 8.63e-03). ETA=7:04:02, max mem: 11.4 GB 
[10/25 02:08:54 visual_prompt]: 	Training 300/553. train loss: 8.0912,	0.4918 s / batch. (data: 3.25e-04). ETA=6:59:05, max mem: 11.4 GB 
[10/25 02:10:26 visual_prompt]: 	Training 400/553. train loss: 50.0991,	0.7802 s / batch. (data: 2.97e-01). ETA=11:03:35, max mem: 11.4 GB 
[10/25 02:12:02 visual_prompt]: 	Training 500/553. train loss: 83.1176,	1.6280 s / batch. (data: 1.13e+00). ETA=23:01:53, max mem: 11.4 GB 
[10/25 02:12:48 visual_prompt]: Epoch 8 / 100: avg data time: 4.34e-01, avg batch time: 0.9268, average train loss: 39.5306
[10/25 02:13:42 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1909, average loss: 1.3742
[10/25 02:13:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 48.03	
[10/25 02:13:42 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/25 02:15:16 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4895 s / batch. (data: 1.19e-02). ETA=6:54:14, max mem: 11.4 GB 
[10/25 02:16:46 visual_prompt]: 	Training 200/553. train loss: 20.4147,	0.4945 s / batch. (data: 2.71e-04). ETA=6:57:36, max mem: 11.4 GB 
[10/25 02:18:17 visual_prompt]: 	Training 300/553. train loss: 56.3881,	2.3280 s / batch. (data: 1.83e+00). ETA=1 day, 8:42:21, max mem: 11.4 GB 
[10/25 02:19:48 visual_prompt]: 	Training 400/553. train loss: 0.2477,	0.4948 s / batch. (data: 7.97e-03). ETA=6:56:15, max mem: 11.4 GB 
[10/25 02:21:23 visual_prompt]: 	Training 500/553. train loss: 0.6243,	1.4030 s / batch. (data: 9.14e-01). ETA=19:37:56, max mem: 11.4 GB 
[10/25 02:22:09 visual_prompt]: Epoch 9 / 100: avg data time: 4.24e-01, avg batch time: 0.9163, average train loss: 43.1275
[10/25 02:23:02 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.1910, average loss: 89.7237
[10/25 02:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.06	
[10/25 02:23:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/25 02:24:41 visual_prompt]: 	Training 100/553. train loss: 75.9300,	0.4917 s / batch. (data: 2.99e-04). ETA=6:51:34, max mem: 11.4 GB 
[10/25 02:26:12 visual_prompt]: 	Training 200/553. train loss: 10.7564,	0.4910 s / batch. (data: 2.83e-04). ETA=6:50:09, max mem: 11.4 GB 
[10/25 02:27:43 visual_prompt]: 	Training 300/553. train loss: 19.5143,	0.4840 s / batch. (data: 2.60e-04). ETA=6:43:30, max mem: 11.4 GB 
[10/25 02:29:12 visual_prompt]: 	Training 400/553. train loss: 9.9904,	0.4910 s / batch. (data: 3.14e-04). ETA=6:48:31, max mem: 11.4 GB 
[10/25 02:30:45 visual_prompt]: 	Training 500/553. train loss: 30.4320,	0.5123 s / batch. (data: 1.74e-02). ETA=7:05:26, max mem: 11.4 GB 
[10/25 02:31:31 visual_prompt]: Epoch 10 / 100: avg data time: 4.28e-01, avg batch time: 0.9201, average train loss: 60.1925
[10/25 02:32:25 visual_prompt]: Inference (val):avg data time: 3.90e-04, avg batch time: 0.1926, average loss: 13.5573
[10/25 02:32:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.45	
[10/25 02:32:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/25 02:34:01 visual_prompt]: 	Training 100/553. train loss: 63.0540,	0.4981 s / batch. (data: 1.05e-02). ETA=6:52:19, max mem: 11.4 GB 
[10/25 02:35:32 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.5322 s / batch. (data: 1.63e-02). ETA=7:19:38, max mem: 11.4 GB 
[10/25 02:37:01 visual_prompt]: 	Training 300/553. train loss: 0.0075,	1.7711 s / batch. (data: 1.30e+00). ETA=1 day, 0:20:18, max mem: 11.4 GB 
[10/25 02:38:28 visual_prompt]: 	Training 400/553. train loss: 30.4814,	0.4810 s / batch. (data: 2.67e-04). ETA=6:35:45, max mem: 11.4 GB 
[10/25 02:39:57 visual_prompt]: 	Training 500/553. train loss: 48.0603,	0.5160 s / batch. (data: 5.40e-03). ETA=7:03:41, max mem: 11.4 GB 
[10/25 02:40:41 visual_prompt]: Epoch 11 / 100: avg data time: 4.05e-01, avg batch time: 0.8969, average train loss: 49.9756
[10/25 02:41:33 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1918, average loss: 3.2441
[10/25 02:41:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.09	
[10/25 02:41:33 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/25 02:43:06 visual_prompt]: 	Training 100/553. train loss: 81.4591,	0.5199 s / batch. (data: 2.38e-02). ETA=7:05:34, max mem: 11.4 GB 
[10/25 02:44:35 visual_prompt]: 	Training 200/553. train loss: 45.0002,	0.9946 s / batch. (data: 5.04e-01). ETA=13:32:30, max mem: 11.4 GB 
[10/25 02:46:02 visual_prompt]: 	Training 300/553. train loss: 13.1564,	0.5051 s / batch. (data: 2.64e-04). ETA=6:51:50, max mem: 11.4 GB 
[10/25 02:47:31 visual_prompt]: 	Training 400/553. train loss: 64.6944,	0.5001 s / batch. (data: 1.20e-02). ETA=6:46:51, max mem: 11.4 GB 
[10/25 02:49:00 visual_prompt]: 	Training 500/553. train loss: 6.2830,	0.5040 s / batch. (data: 7.04e-04). ETA=6:49:12, max mem: 11.4 GB 
[10/25 02:49:44 visual_prompt]: Epoch 12 / 100: avg data time: 3.95e-01, avg batch time: 0.8869, average train loss: 60.8516
[10/25 02:50:36 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1912, average loss: 230.9326
[10/25 02:50:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.57	
[10/25 02:50:36 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/25 02:52:09 visual_prompt]: 	Training 100/553. train loss: 63.1137,	0.7720 s / batch. (data: 2.62e-01). ETA=10:24:51, max mem: 11.4 GB 
[10/25 02:53:35 visual_prompt]: 	Training 200/553. train loss: 22.5483,	0.4840 s / batch. (data: 2.59e-04). ETA=6:30:56, max mem: 11.4 GB 
[10/25 02:55:04 visual_prompt]: 	Training 300/553. train loss: 18.0534,	1.4920 s / batch. (data: 1.00e+00). ETA=20:02:37, max mem: 11.4 GB 
[10/25 02:56:31 visual_prompt]: 	Training 400/553. train loss: 7.3971,	0.4830 s / batch. (data: 2.86e-04). ETA=6:28:33, max mem: 11.4 GB 
[10/25 02:58:01 visual_prompt]: 	Training 500/553. train loss: 101.5449,	0.4923 s / batch. (data: 5.40e-03). ETA=6:35:11, max mem: 11.4 GB 
[10/25 02:58:47 visual_prompt]: Epoch 13 / 100: avg data time: 3.96e-01, avg batch time: 0.8880, average train loss: 60.4026
[10/25 02:59:40 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1921, average loss: 7.6414
[10/25 02:59:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.99	
[10/25 02:59:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/25 03:01:12 visual_prompt]: 	Training 100/553. train loss: 7.8102,	0.5010 s / batch. (data: 2.84e-04). ETA=6:40:51, max mem: 11.4 GB 
[10/25 03:02:41 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.4707 s / batch. (data: 9.63e-01). ETA=19:34:24, max mem: 11.4 GB 
[10/25 03:04:11 visual_prompt]: 	Training 300/553. train loss: 97.4467,	1.3572 s / batch. (data: 8.32e-01). ETA=18:01:27, max mem: 11.4 GB 
[10/25 03:05:38 visual_prompt]: 	Training 400/553. train loss: 13.8903,	0.4784 s / batch. (data: 2.92e-04). ETA=6:20:26, max mem: 11.4 GB 
[10/25 03:07:08 visual_prompt]: 	Training 500/553. train loss: 37.1675,	0.4960 s / batch. (data: 2.73e-04). ETA=6:33:35, max mem: 11.4 GB 
[10/25 03:07:53 visual_prompt]: Epoch 14 / 100: avg data time: 3.99e-01, avg batch time: 0.8915, average train loss: 50.0506
[10/25 03:08:46 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1904, average loss: 18.9847
[10/25 03:08:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.07	
[10/25 03:08:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/25 03:10:18 visual_prompt]: 	Training 100/553. train loss: 96.6485,	0.5021 s / batch. (data: 1.48e-02). ETA=6:37:06, max mem: 11.4 GB 
[10/25 03:11:46 visual_prompt]: 	Training 200/553. train loss: 140.3281,	0.4875 s / batch. (data: 2.65e-04). ETA=6:24:45, max mem: 11.4 GB 
[10/25 03:13:16 visual_prompt]: 	Training 300/553. train loss: 101.0369,	0.4845 s / batch. (data: 3.03e-04). ETA=6:21:35, max mem: 11.4 GB 
[10/25 03:14:43 visual_prompt]: 	Training 400/553. train loss: 37.4380,	0.4980 s / batch. (data: 3.30e-04). ETA=6:31:24, max mem: 11.4 GB 
[10/25 03:16:13 visual_prompt]: 	Training 500/553. train loss: 33.4586,	0.4835 s / batch. (data: 2.58e-04). ETA=6:19:11, max mem: 11.4 GB 
[10/25 03:16:59 visual_prompt]: Epoch 15 / 100: avg data time: 4.01e-01, avg batch time: 0.8925, average train loss: 72.4227
[10/25 03:17:52 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1900, average loss: 93.3064
[10/25 03:17:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.67	
[10/25 03:17:52 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/25 03:19:23 visual_prompt]: 	Training 100/553. train loss: 39.2655,	0.5056 s / batch. (data: 2.20e-02). ETA=6:35:14, max mem: 11.4 GB 
[10/25 03:20:52 visual_prompt]: 	Training 200/553. train loss: 65.0118,	0.5000 s / batch. (data: 2.61e-04). ETA=6:30:01, max mem: 11.4 GB 
[10/25 03:22:21 visual_prompt]: 	Training 300/553. train loss: 42.8137,	0.5127 s / batch. (data: 5.50e-03). ETA=6:39:05, max mem: 11.4 GB 
[10/25 03:23:50 visual_prompt]: 	Training 400/553. train loss: 75.8020,	0.4957 s / batch. (data: 7.27e-04). ETA=6:25:01, max mem: 11.4 GB 
[10/25 03:25:18 visual_prompt]: 	Training 500/553. train loss: 20.9195,	2.1440 s / batch. (data: 1.65e+00). ETA=1 day, 3:41:46, max mem: 11.4 GB 
[10/25 03:26:04 visual_prompt]: Epoch 16 / 100: avg data time: 3.98e-01, avg batch time: 0.8902, average train loss: 41.8540
[10/25 03:26:57 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1903, average loss: 55.1481
[10/25 03:26:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.54	
[10/25 03:26:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/25 03:28:28 visual_prompt]: 	Training 100/553. train loss: 9.8082,	0.5003 s / batch. (data: 1.20e-02). ETA=6:26:27, max mem: 11.4 GB 
[10/25 03:29:58 visual_prompt]: 	Training 200/553. train loss: 7.0255,	0.4894 s / batch. (data: 2.75e-04). ETA=6:17:15, max mem: 11.4 GB 
[10/25 03:31:27 visual_prompt]: 	Training 300/553. train loss: 13.7478,	0.4774 s / batch. (data: 2.80e-04). ETA=6:07:11, max mem: 11.4 GB 
[10/25 03:32:55 visual_prompt]: 	Training 400/553. train loss: 10.5069,	1.1993 s / batch. (data: 7.21e-01). ETA=15:20:30, max mem: 11.4 GB 
[10/25 03:34:24 visual_prompt]: 	Training 500/553. train loss: 88.6415,	2.0487 s / batch. (data: 1.58e+00). ETA=1 day, 2:09:01, max mem: 11.4 GB 
[10/25 03:35:10 visual_prompt]: Epoch 17 / 100: avg data time: 4.00e-01, avg batch time: 0.8919, average train loss: 54.3567
[10/25 03:36:02 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1904, average loss: 63.9384
[10/25 03:36:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.63	
[10/25 03:36:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/25 03:37:35 visual_prompt]: 	Training 100/553. train loss: 41.8655,	0.5171 s / batch. (data: 1.60e-02). ETA=6:34:44, max mem: 11.4 GB 
[10/25 03:39:06 visual_prompt]: 	Training 200/553. train loss: 117.0110,	0.4921 s / batch. (data: 4.12e-03). ETA=6:14:47, max mem: 11.4 GB 
[10/25 03:40:35 visual_prompt]: 	Training 300/553. train loss: 13.0699,	0.4920 s / batch. (data: 2.84e-04). ETA=6:13:56, max mem: 11.4 GB 
[10/25 03:42:03 visual_prompt]: 	Training 400/553. train loss: 14.9118,	0.4918 s / batch. (data: 2.42e-04). ETA=6:12:57, max mem: 11.4 GB 
[10/25 03:43:31 visual_prompt]: 	Training 500/553. train loss: 41.3943,	0.4801 s / batch. (data: 2.76e-04). ETA=6:03:13, max mem: 11.4 GB 
[10/25 03:44:15 visual_prompt]: Epoch 18 / 100: avg data time: 3.99e-01, avg batch time: 0.8912, average train loss: 48.7102
[10/25 03:45:08 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1918, average loss: 76.3634
[10/25 03:45:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.54	
[10/25 03:45:08 visual_prompt]: Stopping early.
[10/25 03:45:08 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 03:45:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 03:45:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 03:45:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 03:45:08 visual_prompt]: Training with config:
[10/25 03:45:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr10.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 03:45:08 visual_prompt]: Loading training data...
[10/25 03:45:08 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 03:45:08 visual_prompt]: Loading validation data...
[10/25 03:45:08 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 03:45:08 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/25 03:45:11 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/25 03:45:11 visual_prompt]: tuned percent:0.529
[10/25 03:45:11 visual_prompt]: Device used for model: 0
[10/25 03:45:11 visual_prompt]: Setting up Evaluator...
[10/25 03:45:11 visual_prompt]: Setting up Trainer...
[10/25 03:45:11 visual_prompt]: 	Setting up the optimizer...
[10/25 03:45:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 03:46:43 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5177 s / batch. (data: 2.56e-02). ETA=7:56:14, max mem: 11.4 GB 
[10/25 03:48:09 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5124 s / batch. (data: 1.07e-02). ETA=7:50:35, max mem: 11.4 GB 
[10/25 03:49:41 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9560 s / batch. (data: 2.46e+00). ETA=1 day, 21:09:41, max mem: 11.4 GB 
[10/25 03:51:08 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5080 s / batch. (data: 1.19e-02). ETA=7:44:47, max mem: 11.4 GB 
[10/25 03:52:39 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5240 s / batch. (data: 7.86e-04). ETA=7:58:34, max mem: 11.4 GB 
[10/25 03:53:25 visual_prompt]: Epoch 1 / 100: avg data time: 4.00e-01, avg batch time: 0.8945, average train loss: 1.3966
[10/25 03:54:18 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1919, average loss: 1.3454
[10/25 03:54:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/25 03:54:18 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/25 03:55:49 visual_prompt]: 	Training 100/553. train loss: 2.7195,	0.9592 s / batch. (data: 4.69e-01). ETA=14:33:37, max mem: 11.4 GB 
[10/25 03:57:17 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.5120 s / batch. (data: 2.84e-04). ETA=7:45:28, max mem: 11.4 GB 
[10/25 03:58:48 visual_prompt]: 	Training 300/553. train loss: 1.2449,	1.5840 s / batch. (data: 1.09e+00). ETA=23:57:22, max mem: 11.4 GB 
[10/25 04:00:16 visual_prompt]: 	Training 400/553. train loss: 6.2296,	0.4914 s / batch. (data: 2.60e-04). ETA=7:25:08, max mem: 11.4 GB 
[10/25 04:01:46 visual_prompt]: 	Training 500/553. train loss: 2.3339,	0.5040 s / batch. (data: 7.96e-03). ETA=7:35:41, max mem: 11.4 GB 
[10/25 04:02:30 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 0.8906, average train loss: 3.2781
[10/25 04:03:23 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1912, average loss: 0.7037
[10/25 04:03:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.89	
[10/25 04:03:23 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/25 04:04:53 visual_prompt]: 	Training 100/553. train loss: 10.6600,	0.4858 s / batch. (data: 7.91e-03). ETA=7:18:01, max mem: 11.4 GB 
[10/25 04:06:22 visual_prompt]: 	Training 200/553. train loss: 2.8611,	0.5880 s / batch. (data: 9.65e-02). ETA=8:49:08, max mem: 11.4 GB 
[10/25 04:07:49 visual_prompt]: 	Training 300/553. train loss: 0.6964,	0.5233 s / batch. (data: 2.89e-04). ETA=7:50:05, max mem: 11.4 GB 
[10/25 04:09:20 visual_prompt]: 	Training 400/553. train loss: 0.2515,	0.5412 s / batch. (data: 2.05e-02). ETA=8:05:14, max mem: 11.4 GB 
[10/25 04:10:49 visual_prompt]: 	Training 500/553. train loss: 24.8590,	1.8674 s / batch. (data: 1.38e+00). ETA=1 day, 3:51:09, max mem: 11.4 GB 
[10/25 04:11:33 visual_prompt]: Epoch 3 / 100: avg data time: 3.92e-01, avg batch time: 0.8859, average train loss: 6.8228
[10/25 04:12:25 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1914, average loss: 0.7124
[10/25 04:12:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 39.84	
[10/25 04:12:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/25 04:13:58 visual_prompt]: 	Training 100/553. train loss: 11.2021,	0.5160 s / batch. (data: 2.49e-04). ETA=7:40:25, max mem: 11.4 GB 
[10/25 04:15:27 visual_prompt]: 	Training 200/553. train loss: 5.3874,	0.5165 s / batch. (data: 1.25e-02). ETA=7:40:02, max mem: 11.4 GB 
[10/25 04:16:55 visual_prompt]: 	Training 300/553. train loss: 2.7356,	1.3284 s / batch. (data: 8.22e-01). ETA=19:41:00, max mem: 11.4 GB 
[10/25 04:18:20 visual_prompt]: 	Training 400/553. train loss: 2.1320,	1.3099 s / batch. (data: 8.21e-01). ETA=19:22:18, max mem: 11.4 GB 
[10/25 04:19:50 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.5680 s / batch. (data: 3.07e+00). ETA=2 days, 4:40:05, max mem: 11.4 GB 
[10/25 04:20:35 visual_prompt]: Epoch 4 / 100: avg data time: 3.92e-01, avg batch time: 0.8862, average train loss: 9.6659
[10/25 04:21:28 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1899, average loss: 16.6921
[10/25 04:21:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 55.81	
[10/25 04:21:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/25 04:22:58 visual_prompt]: 	Training 100/553. train loss: 9.6098,	0.4996 s / batch. (data: 5.39e-03). ETA=7:21:10, max mem: 11.4 GB 
[10/25 04:24:27 visual_prompt]: 	Training 200/553. train loss: 11.5952,	1.8755 s / batch. (data: 1.37e+00). ETA=1 day, 3:33:12, max mem: 11.4 GB 
[10/25 04:25:56 visual_prompt]: 	Training 300/553. train loss: 21.5849,	0.5199 s / batch. (data: 1.12e-03). ETA=7:37:26, max mem: 11.4 GB 
[10/25 04:27:25 visual_prompt]: 	Training 400/553. train loss: 30.3207,	0.5000 s / batch. (data: 2.69e-04). ETA=7:19:02, max mem: 11.4 GB 
[10/25 04:28:54 visual_prompt]: 	Training 500/553. train loss: 6.3147,	0.5000 s / batch. (data: 2.69e-04). ETA=7:18:14, max mem: 11.4 GB 
[10/25 04:29:40 visual_prompt]: Epoch 5 / 100: avg data time: 3.95e-01, avg batch time: 0.8898, average train loss: 13.5571
[10/25 04:30:32 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1896, average loss: 4.4246
[10/25 04:30:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.02	
[10/25 04:30:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/25 04:32:04 visual_prompt]: 	Training 100/553. train loss: 10.1100,	0.5080 s / batch. (data: 2.83e-04). ETA=7:23:57, max mem: 11.4 GB 
[10/25 04:33:32 visual_prompt]: 	Training 200/553. train loss: 85.6758,	0.5040 s / batch. (data: 2.78e-04). ETA=7:19:35, max mem: 11.4 GB 
[10/25 04:34:59 visual_prompt]: 	Training 300/553. train loss: 22.5875,	0.4811 s / batch. (data: 8.08e-03). ETA=6:58:50, max mem: 11.4 GB 
[10/25 04:36:31 visual_prompt]: 	Training 400/553. train loss: 5.4452,	0.7760 s / batch. (data: 2.79e-01). ETA=11:14:16, max mem: 11.4 GB 
[10/25 04:37:58 visual_prompt]: 	Training 500/553. train loss: 13.3914,	1.3929 s / batch. (data: 9.02e-01). ETA=20:08:01, max mem: 11.4 GB 
[10/25 04:38:43 visual_prompt]: Epoch 6 / 100: avg data time: 3.92e-01, avg batch time: 0.8869, average train loss: 18.2064
[10/25 04:39:35 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1904, average loss: 0.8635
[10/25 04:39:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.19	
[10/25 04:39:35 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/25 04:41:04 visual_prompt]: 	Training 100/553. train loss: 15.2095,	0.5099 s / batch. (data: 1.04e-02). ETA=7:20:54, max mem: 11.4 GB 
[10/25 04:42:33 visual_prompt]: 	Training 200/553. train loss: 8.1791,	0.4915 s / batch. (data: 2.94e-04). ETA=7:04:11, max mem: 11.4 GB 
[10/25 04:44:05 visual_prompt]: 	Training 300/553. train loss: 1.6486,	2.3622 s / batch. (data: 1.87e+00). ETA=1 day, 9:54:41, max mem: 11.4 GB 
[10/25 04:45:33 visual_prompt]: 	Training 400/553. train loss: 2.7331,	2.2952 s / batch. (data: 1.80e+00). ETA=1 day, 8:53:11, max mem: 11.4 GB 
[10/25 04:46:59 visual_prompt]: 	Training 500/553. train loss: 17.5581,	0.7645 s / batch. (data: 2.72e-01). ETA=10:55:59, max mem: 11.4 GB 
[10/25 04:47:44 visual_prompt]: Epoch 7 / 100: avg data time: 3.90e-01, avg batch time: 0.8841, average train loss: 21.9756
[10/25 04:48:36 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1919, average loss: 7.7348
[10/25 04:48:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.29	
[10/25 04:48:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/25 04:50:05 visual_prompt]: 	Training 100/553. train loss: 18.2809,	0.7504 s / batch. (data: 2.20e-01). ETA=10:41:57, max mem: 11.4 GB 
[10/25 04:51:35 visual_prompt]: 	Training 200/553. train loss: 61.2324,	0.5159 s / batch. (data: 1.19e-02). ETA=7:20:31, max mem: 11.4 GB 
[10/25 04:53:04 visual_prompt]: 	Training 300/553. train loss: 32.7313,	0.5288 s / batch. (data: 7.29e-04). ETA=7:30:34, max mem: 11.4 GB 
[10/25 04:54:33 visual_prompt]: 	Training 400/553. train loss: 6.3118,	1.6397 s / batch. (data: 1.13e+00). ETA=23:14:32, max mem: 11.4 GB 
[10/25 04:56:01 visual_prompt]: 	Training 500/553. train loss: 86.3733,	2.0029 s / batch. (data: 1.52e+00). ETA=1 day, 4:20:05, max mem: 11.4 GB 
[10/25 04:56:47 visual_prompt]: Epoch 8 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 22.9832
[10/25 04:57:39 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1909, average loss: 36.7645
[10/25 04:57:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.06	
[10/25 04:57:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/25 04:59:10 visual_prompt]: 	Training 100/553. train loss: 0.0030,	0.4920 s / batch. (data: 2.81e-04). ETA=6:56:20, max mem: 11.4 GB 
[10/25 05:00:38 visual_prompt]: 	Training 200/553. train loss: 6.4232,	0.5004 s / batch. (data: 2.83e-04). ETA=7:02:39, max mem: 11.4 GB 
[10/25 05:02:06 visual_prompt]: 	Training 300/553. train loss: 17.0703,	1.6446 s / batch. (data: 1.15e+00). ETA=23:06:19, max mem: 11.4 GB 
[10/25 05:03:35 visual_prompt]: 	Training 400/553. train loss: 30.1956,	0.4887 s / batch. (data: 2.87e-04). ETA=6:51:06, max mem: 11.4 GB 
[10/25 05:05:04 visual_prompt]: 	Training 500/553. train loss: 10.3365,	0.4800 s / batch. (data: 2.82e-04). ETA=6:43:00, max mem: 11.4 GB 
[10/25 05:05:49 visual_prompt]: Epoch 9 / 100: avg data time: 3.91e-01, avg batch time: 0.8854, average train loss: 25.9598
[10/25 05:06:41 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1911, average loss: 0.7760
[10/25 05:06:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 47.68	
[10/25 05:06:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/25 05:08:14 visual_prompt]: 	Training 100/553. train loss: 40.6147,	0.4779 s / batch. (data: 2.73e-04). ETA=6:40:03, max mem: 11.4 GB 
[10/25 05:09:42 visual_prompt]: 	Training 200/553. train loss: 13.5580,	0.5143 s / batch. (data: 1.02e-02). ETA=7:09:36, max mem: 11.4 GB 
[10/25 05:11:10 visual_prompt]: 	Training 300/553. train loss: 34.3723,	2.3494 s / batch. (data: 1.87e+00). ETA=1 day, 8:38:42, max mem: 11.4 GB 
[10/25 05:12:35 visual_prompt]: 	Training 400/553. train loss: 37.4602,	0.7631 s / batch. (data: 2.70e-01). ETA=10:34:56, max mem: 11.4 GB 
[10/25 05:14:06 visual_prompt]: 	Training 500/553. train loss: 28.7208,	1.6344 s / batch. (data: 1.15e+00). ETA=22:37:12, max mem: 11.4 GB 
[10/25 05:14:51 visual_prompt]: Epoch 10 / 100: avg data time: 3.91e-01, avg batch time: 0.8858, average train loss: 26.6320
[10/25 05:15:43 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1906, average loss: 36.9082
[10/25 05:15:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.08	
[10/25 05:15:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/25 05:17:16 visual_prompt]: 	Training 100/553. train loss: 4.8424,	0.4953 s / batch. (data: 5.40e-03). ETA=6:50:00, max mem: 11.4 GB 
[10/25 05:18:46 visual_prompt]: 	Training 200/553. train loss: 88.8136,	0.4906 s / batch. (data: 2.66e-04). ETA=6:45:19, max mem: 11.4 GB 
[10/25 05:20:13 visual_prompt]: 	Training 300/553. train loss: 17.2262,	1.4372 s / batch. (data: 9.45e-01). ETA=19:44:56, max mem: 11.4 GB 
[10/25 05:21:40 visual_prompt]: 	Training 400/553. train loss: 2.5076,	0.5066 s / batch. (data: 1.59e-02). ETA=6:56:48, max mem: 11.4 GB 
[10/25 05:23:08 visual_prompt]: 	Training 500/553. train loss: 33.9465,	0.4920 s / batch. (data: 2.75e-04). ETA=6:43:59, max mem: 11.4 GB 
[10/25 05:23:53 visual_prompt]: Epoch 11 / 100: avg data time: 3.93e-01, avg batch time: 0.8855, average train loss: 39.5771
[10/25 05:24:45 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1909, average loss: 4.9136
[10/25 05:24:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.63	
[10/25 05:24:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/25 05:26:18 visual_prompt]: 	Training 100/553. train loss: 5.1507,	0.7280 s / batch. (data: 2.33e-01). ETA=9:55:56, max mem: 11.4 GB 
[10/25 05:27:48 visual_prompt]: 	Training 200/553. train loss: 76.6972,	0.7840 s / batch. (data: 2.87e-01). ETA=10:40:30, max mem: 11.4 GB 
[10/25 05:29:16 visual_prompt]: 	Training 300/553. train loss: 25.2016,	0.5120 s / batch. (data: 2.71e-04). ETA=6:57:24, max mem: 11.4 GB 
[10/25 05:30:45 visual_prompt]: 	Training 400/553. train loss: 53.2529,	0.5042 s / batch. (data: 1.63e-02). ETA=6:50:15, max mem: 11.4 GB 
[10/25 05:32:15 visual_prompt]: 	Training 500/553. train loss: 45.2410,	0.4960 s / batch. (data: 2.51e-04). ETA=6:42:44, max mem: 11.4 GB 
[10/25 05:32:59 visual_prompt]: Epoch 12 / 100: avg data time: 3.99e-01, avg batch time: 0.8924, average train loss: 32.4371
[10/25 05:33:51 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1914, average loss: 4.8185
[10/25 05:33:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.26	
[10/25 05:33:51 visual_prompt]: Best epoch 12: best metric: -4.818
[10/25 05:33:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/25 05:35:25 visual_prompt]: 	Training 100/553. train loss: 7.3572,	0.5000 s / batch. (data: 2.85e-04). ETA=6:44:42, max mem: 11.4 GB 
[10/25 05:36:52 visual_prompt]: 	Training 200/553. train loss: 13.6279,	0.4888 s / batch. (data: 3.10e-04). ETA=6:34:48, max mem: 11.4 GB 
[10/25 05:38:21 visual_prompt]: 	Training 300/553. train loss: 30.6067,	2.1077 s / batch. (data: 1.62e+00). ETA=1 day, 4:18:57, max mem: 11.4 GB 
[10/25 05:39:48 visual_prompt]: 	Training 400/553. train loss: 22.8639,	0.6356 s / batch. (data: 1.27e-01). ETA=8:31:14, max mem: 11.4 GB 
[10/25 05:41:18 visual_prompt]: 	Training 500/553. train loss: 53.3459,	0.5080 s / batch. (data: 2.74e-04). ETA=6:47:48, max mem: 11.4 GB 
[10/25 05:42:04 visual_prompt]: Epoch 13 / 100: avg data time: 3.98e-01, avg batch time: 0.8913, average train loss: 33.3047
[10/25 05:42:57 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1899, average loss: 14.3807
[10/25 05:42:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.78	
[10/25 05:42:57 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/25 05:44:30 visual_prompt]: 	Training 100/553. train loss: 18.7732,	0.5040 s / batch. (data: 2.77e-04). ETA=6:43:17, max mem: 11.4 GB 
[10/25 05:45:59 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.9120 s / batch. (data: 1.40e+00). ETA=1 day, 1:26:45, max mem: 11.4 GB 
[10/25 05:47:27 visual_prompt]: 	Training 300/553. train loss: 17.1721,	1.3309 s / batch. (data: 8.36e-01). ETA=17:40:32, max mem: 11.4 GB 
[10/25 05:48:54 visual_prompt]: 	Training 400/553. train loss: 7.0507,	0.5239 s / batch. (data: 5.40e-03). ETA=6:56:35, max mem: 11.4 GB 
[10/25 05:50:23 visual_prompt]: 	Training 500/553. train loss: 19.7984,	0.5262 s / batch. (data: 2.59e-04). ETA=6:57:31, max mem: 11.4 GB 
[10/25 05:51:07 visual_prompt]: Epoch 14 / 100: avg data time: 3.93e-01, avg batch time: 0.8862, average train loss: 35.5265
[10/25 05:51:59 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1908, average loss: 6.5096
[10/25 05:51:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.14	
[10/25 05:51:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/25 05:53:31 visual_prompt]: 	Training 100/553. train loss: 6.4461,	0.4791 s / batch. (data: 3.03e-04). ETA=6:18:54, max mem: 11.4 GB 
[10/25 05:54:58 visual_prompt]: 	Training 200/553. train loss: 151.5186,	0.5040 s / batch. (data: 2.82e-04). ETA=6:37:47, max mem: 11.4 GB 
[10/25 05:56:27 visual_prompt]: 	Training 300/553. train loss: 192.0195,	0.4769 s / batch. (data: 2.71e-04). ETA=6:15:36, max mem: 11.4 GB 
[10/25 05:57:54 visual_prompt]: 	Training 400/553. train loss: 0.8242,	0.5159 s / batch. (data: 2.39e-02). ETA=6:45:29, max mem: 11.4 GB 
[10/25 05:59:22 visual_prompt]: 	Training 500/553. train loss: 12.7030,	0.5159 s / batch. (data: 1.05e-02). ETA=6:44:39, max mem: 11.4 GB 
[10/25 06:00:08 visual_prompt]: Epoch 15 / 100: avg data time: 3.91e-01, avg batch time: 0.8843, average train loss: 34.5118
[10/25 06:01:00 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1912, average loss: 104.8075
[10/25 06:01:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.06	
[10/25 06:01:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/25 06:02:31 visual_prompt]: 	Training 100/553. train loss: 29.4097,	0.4818 s / batch. (data: 2.48e-04). ETA=6:16:36, max mem: 11.4 GB 
[10/25 06:04:00 visual_prompt]: 	Training 200/553. train loss: 16.0769,	0.5271 s / batch. (data: 5.39e-03). ETA=6:51:09, max mem: 11.4 GB 
[10/25 06:05:28 visual_prompt]: 	Training 300/553. train loss: 73.2987,	0.4863 s / batch. (data: 3.26e-04). ETA=6:18:32, max mem: 11.4 GB 
[10/25 06:06:56 visual_prompt]: 	Training 400/553. train loss: 6.3079,	0.5125 s / batch. (data: 5.41e-03). ETA=6:38:04, max mem: 11.4 GB 
[10/25 06:08:24 visual_prompt]: 	Training 500/553. train loss: 1.5488,	1.9500 s / batch. (data: 1.47e+00). ETA=1 day, 1:11:22, max mem: 11.4 GB 
[10/25 06:09:09 visual_prompt]: Epoch 16 / 100: avg data time: 3.91e-01, avg batch time: 0.8842, average train loss: 35.7945
[10/25 06:10:02 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1903, average loss: 42.7699
[10/25 06:10:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.74	
[10/25 06:10:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/25 06:11:32 visual_prompt]: 	Training 100/553. train loss: 14.7368,	0.5000 s / batch. (data: 2.78e-04). ETA=6:26:16, max mem: 11.4 GB 
[10/25 06:13:02 visual_prompt]: 	Training 200/553. train loss: 47.8283,	0.4846 s / batch. (data: 4.37e-04). ETA=6:13:32, max mem: 11.4 GB 
[10/25 06:14:29 visual_prompt]: 	Training 300/553. train loss: 56.5925,	0.4933 s / batch. (data: 2.50e-04). ETA=6:19:27, max mem: 11.4 GB 
[10/25 06:15:57 visual_prompt]: 	Training 400/553. train loss: 82.7968,	1.5251 s / batch. (data: 1.03e+00). ETA=19:30:33, max mem: 11.4 GB 
[10/25 06:17:24 visual_prompt]: 	Training 500/553. train loss: 44.6672,	2.2115 s / batch. (data: 1.70e+00). ETA=1 day, 4:13:41, max mem: 11.4 GB 
[10/25 06:18:10 visual_prompt]: Epoch 17 / 100: avg data time: 3.91e-01, avg batch time: 0.8835, average train loss: 35.7164
[10/25 06:19:02 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1921, average loss: 11.2052
[10/25 06:19:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.73	
[10/25 06:19:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/25 06:20:34 visual_prompt]: 	Training 100/553. train loss: 41.8254,	0.4897 s / batch. (data: 2.63e-04). ETA=6:13:45, max mem: 11.4 GB 
[10/25 06:22:05 visual_prompt]: 	Training 200/553. train loss: 10.9531,	0.5000 s / batch. (data: 7.19e-04). ETA=6:20:50, max mem: 11.4 GB 
[10/25 06:23:33 visual_prompt]: 	Training 300/553. train loss: 21.3080,	0.4880 s / batch. (data: 2.62e-04). ETA=6:10:53, max mem: 11.4 GB 
[10/25 06:25:01 visual_prompt]: 	Training 400/553. train loss: 5.7513,	0.4840 s / batch. (data: 2.55e-04). ETA=6:07:00, max mem: 11.4 GB 
[10/25 06:26:29 visual_prompt]: 	Training 500/553. train loss: 41.4726,	0.4960 s / batch. (data: 5.38e-03). ETA=6:15:19, max mem: 11.4 GB 
[10/25 06:27:13 visual_prompt]: Epoch 18 / 100: avg data time: 3.93e-01, avg batch time: 0.8869, average train loss: 32.9152
[10/25 06:28:05 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1912, average loss: 62.2955
[10/25 06:28:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.76	
[10/25 06:28:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/25 06:29:36 visual_prompt]: 	Training 100/553. train loss: 54.1969,	0.4782 s / batch. (data: 2.88e-04). ETA=6:00:38, max mem: 11.4 GB 
[10/25 06:31:05 visual_prompt]: 	Training 200/553. train loss: 65.2348,	0.4768 s / batch. (data: 2.54e-04). ETA=5:58:47, max mem: 11.4 GB 
[10/25 06:32:33 visual_prompt]: 	Training 300/553. train loss: 40.8467,	0.4883 s / batch. (data: 2.74e-04). ETA=6:06:34, max mem: 11.4 GB 
[10/25 06:34:02 visual_prompt]: 	Training 400/553. train loss: 38.3633,	0.5064 s / batch. (data: 1.05e-02). ETA=6:19:20, max mem: 11.4 GB 
[10/25 06:35:27 visual_prompt]: 	Training 500/553. train loss: 26.2477,	0.5007 s / batch. (data: 2.67e-04). ETA=6:14:12, max mem: 11.4 GB 
[10/25 06:36:13 visual_prompt]: Epoch 19 / 100: avg data time: 3.90e-01, avg batch time: 0.8827, average train loss: 35.9858
[10/25 06:37:05 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1914, average loss: 26.7427
[10/25 06:37:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.11	
[10/25 06:37:05 visual_prompt]: Stopping early.
[10/25 06:37:05 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 06:37:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 06:37:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 06:37:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 06:37:05 visual_prompt]: Training with config:
[10/25 06:37:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr10.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 06:37:05 visual_prompt]: Loading training data...
[10/25 06:37:05 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 06:37:05 visual_prompt]: Loading validation data...
[10/25 06:37:05 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 06:37:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/25 06:37:08 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/25 06:37:08 visual_prompt]: tuned percent:0.529
[10/25 06:37:08 visual_prompt]: Device used for model: 0
[10/25 06:37:08 visual_prompt]: Setting up Evaluator...
[10/25 06:37:08 visual_prompt]: Setting up Trainer...
[10/25 06:37:08 visual_prompt]: 	Setting up the optimizer...
[10/25 06:37:08 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 06:38:40 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5093 s / batch. (data: 9.16e-03). ETA=7:48:31, max mem: 11.4 GB 
[10/25 06:40:06 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4913 s / batch. (data: 2.61e-04). ETA=7:31:12, max mem: 11.4 GB 
[10/25 06:41:38 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.0166 s / batch. (data: 2.52e+00). ETA=1 day, 22:05:12, max mem: 11.4 GB 
[10/25 06:43:03 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5000 s / batch. (data: 2.80e-04). ETA=7:37:29, max mem: 11.4 GB 
[10/25 06:44:34 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4921 s / batch. (data: 2.37e-04). ETA=7:29:24, max mem: 11.4 GB 
[10/25 06:45:20 visual_prompt]: Epoch 1 / 100: avg data time: 3.95e-01, avg batch time: 0.8896, average train loss: 1.3966
[10/25 06:46:12 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1907, average loss: 1.3454
[10/25 06:46:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/25 06:46:12 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/25 06:47:43 visual_prompt]: 	Training 100/553. train loss: 2.0834,	0.4883 s / batch. (data: 8.30e-03). ETA=7:24:46, max mem: 11.4 GB 
[10/25 06:49:11 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.4999 s / batch. (data: 3.55e-04). ETA=7:34:29, max mem: 11.4 GB 
[10/25 06:50:42 visual_prompt]: 	Training 300/553. train loss: 2.7537,	1.7240 s / batch. (data: 1.23e+00). ETA=1 day, 2:04:26, max mem: 11.4 GB 
[10/25 06:52:10 visual_prompt]: 	Training 400/553. train loss: 1.2766,	0.4826 s / batch. (data: 2.82e-04). ETA=7:17:08, max mem: 11.4 GB 
[10/25 06:53:40 visual_prompt]: 	Training 500/553. train loss: 2.8296,	0.4845 s / batch. (data: 2.87e-04). ETA=7:18:03, max mem: 11.4 GB 
[10/25 06:54:25 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 0.8907, average train loss: 4.3764
[10/25 06:55:17 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1903, average loss: 2.0508
[10/25 06:55:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.24	
[10/25 06:55:17 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/25 06:56:48 visual_prompt]: 	Training 100/553. train loss: 7.2884,	0.4960 s / batch. (data: 2.92e-04). ETA=7:27:13, max mem: 11.4 GB 
[10/25 06:58:18 visual_prompt]: 	Training 200/553. train loss: 1.3088,	1.1068 s / batch. (data: 6.14e-01). ETA=16:36:02, max mem: 11.4 GB 
[10/25 06:59:45 visual_prompt]: 	Training 300/553. train loss: 0.8338,	0.4846 s / batch. (data: 2.47e-04). ETA=7:15:16, max mem: 11.4 GB 
[10/25 07:01:15 visual_prompt]: 	Training 400/553. train loss: 4.6136,	0.4783 s / batch. (data: 2.68e-04). ETA=7:08:48, max mem: 11.4 GB 
[10/25 07:02:45 visual_prompt]: 	Training 500/553. train loss: 11.1137,	1.8544 s / batch. (data: 1.37e+00). ETA=1 day, 3:39:30, max mem: 11.4 GB 
[10/25 07:03:29 visual_prompt]: Epoch 3 / 100: avg data time: 3.93e-01, avg batch time: 0.8882, average train loss: 6.2173
[10/25 07:04:21 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1917, average loss: 16.2203
[10/25 07:04:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.40	
[10/25 07:04:21 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/25 07:05:53 visual_prompt]: 	Training 100/553. train loss: 8.2300,	0.4876 s / batch. (data: 5.38e-03). ETA=7:15:08, max mem: 11.4 GB 
[10/25 07:07:23 visual_prompt]: 	Training 200/553. train loss: 13.6493,	0.5041 s / batch. (data: 2.77e-04). ETA=7:28:58, max mem: 11.4 GB 
[10/25 07:08:52 visual_prompt]: 	Training 300/553. train loss: 2.5482,	1.6639 s / batch. (data: 1.18e+00). ETA=1 day, 0:39:12, max mem: 11.4 GB 
[10/25 07:10:16 visual_prompt]: 	Training 400/553. train loss: 7.6429,	1.4837 s / batch. (data: 1.01e+00). ETA=21:56:32, max mem: 11.4 GB 
[10/25 07:11:47 visual_prompt]: 	Training 500/553. train loss: 13.3656,	3.6520 s / batch. (data: 3.18e+00). ETA=2 days, 5:54:32, max mem: 11.4 GB 
[10/25 07:12:32 visual_prompt]: Epoch 4 / 100: avg data time: 3.93e-01, avg batch time: 0.8880, average train loss: 9.9654
[10/25 07:13:25 visual_prompt]: Inference (val):avg data time: 1.65e-04, avg batch time: 0.1921, average loss: 8.2708
[10/25 07:13:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.30	
[10/25 07:13:25 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/25 07:14:55 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4959 s / batch. (data: 2.91e-04). ETA=7:17:58, max mem: 11.4 GB 
[10/25 07:16:23 visual_prompt]: 	Training 200/553. train loss: 9.3205,	1.7400 s / batch. (data: 1.24e+00). ETA=1 day, 1:33:45, max mem: 11.4 GB 
[10/25 07:17:52 visual_prompt]: 	Training 300/553. train loss: 58.5300,	0.5040 s / batch. (data: 2.61e-04). ETA=7:23:24, max mem: 11.4 GB 
[10/25 07:19:20 visual_prompt]: 	Training 400/553. train loss: 3.4101,	0.4798 s / batch. (data: 2.74e-04). ETA=7:01:20, max mem: 11.4 GB 
[10/25 07:20:49 visual_prompt]: 	Training 500/553. train loss: 6.6801,	0.5120 s / batch. (data: 2.84e-04). ETA=7:28:42, max mem: 11.4 GB 
[10/25 07:21:35 visual_prompt]: Epoch 5 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 14.3649
[10/25 07:22:27 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1906, average loss: 24.3657
[10/25 07:22:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.16	
[10/25 07:22:27 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/25 07:24:00 visual_prompt]: 	Training 100/553. train loss: 5.8457,	0.4783 s / batch. (data: 2.48e-04). ETA=6:57:59, max mem: 11.4 GB 
[10/25 07:25:28 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.5018 s / batch. (data: 2.75e-04). ETA=7:17:42, max mem: 11.4 GB 
[10/25 07:26:55 visual_prompt]: 	Training 300/553. train loss: 14.5852,	0.4898 s / batch. (data: 2.81e-04). ETA=7:06:23, max mem: 11.4 GB 
[10/25 07:28:27 visual_prompt]: 	Training 400/553. train loss: 24.5116,	0.5002 s / batch. (data: 5.54e-03). ETA=7:14:35, max mem: 11.4 GB 
[10/25 07:29:54 visual_prompt]: 	Training 500/553. train loss: 31.3663,	1.3960 s / batch. (data: 9.02e-01). ETA=20:10:42, max mem: 11.4 GB 
[10/25 07:30:38 visual_prompt]: Epoch 6 / 100: avg data time: 3.94e-01, avg batch time: 0.8882, average train loss: 17.2785
[10/25 07:31:31 visual_prompt]: Inference (val):avg data time: 8.46e-05, avg batch time: 0.1911, average loss: 4.1387
[10/25 07:31:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.10	
[10/25 07:31:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/25 07:33:00 visual_prompt]: 	Training 100/553. train loss: 72.8699,	0.4880 s / batch. (data: 2.73e-04). ETA=7:01:57, max mem: 11.4 GB 
[10/25 07:34:29 visual_prompt]: 	Training 200/553. train loss: 12.5438,	0.4843 s / batch. (data: 2.41e-04). ETA=6:57:55, max mem: 11.4 GB 
[10/25 07:36:00 visual_prompt]: 	Training 300/553. train loss: 17.9239,	2.0200 s / batch. (data: 1.54e+00). ETA=1 day, 4:59:57, max mem: 11.4 GB 
[10/25 07:37:29 visual_prompt]: 	Training 400/553. train loss: 9.8064,	2.4108 s / batch. (data: 1.90e+00). ETA=1 day, 10:32:35, max mem: 11.4 GB 
[10/25 07:38:55 visual_prompt]: 	Training 500/553. train loss: 3.6440,	0.6320 s / batch. (data: 1.32e-01). ETA=9:02:17, max mem: 11.4 GB 
[10/25 07:39:40 visual_prompt]: Epoch 7 / 100: avg data time: 3.90e-01, avg batch time: 0.8842, average train loss: 21.2364
[10/25 07:40:32 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1912, average loss: 11.4780
[10/25 07:40:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.66	
[10/25 07:40:32 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/25 07:42:01 visual_prompt]: 	Training 100/553. train loss: 6.8728,	0.5127 s / batch. (data: 1.04e-02). ETA=7:18:36, max mem: 11.4 GB 
[10/25 07:43:31 visual_prompt]: 	Training 200/553. train loss: 62.1469,	0.4964 s / batch. (data: 2.94e-04). ETA=7:03:48, max mem: 11.4 GB 
[10/25 07:45:00 visual_prompt]: 	Training 300/553. train loss: 9.8223,	0.4857 s / batch. (data: 2.85e-04). ETA=6:53:52, max mem: 11.4 GB 
[10/25 07:46:29 visual_prompt]: 	Training 400/553. train loss: 68.8493,	1.5118 s / batch. (data: 1.03e+00). ETA=21:25:45, max mem: 11.4 GB 
[10/25 07:47:57 visual_prompt]: 	Training 500/553. train loss: 41.6014,	0.4995 s / batch. (data: 2.70e-04). ETA=7:04:00, max mem: 11.4 GB 
[10/25 07:48:43 visual_prompt]: Epoch 8 / 100: avg data time: 3.94e-01, avg batch time: 0.8876, average train loss: 22.6979
[10/25 07:49:35 visual_prompt]: Inference (val):avg data time: 1.66e-04, avg batch time: 0.1890, average loss: 23.5725
[10/25 07:49:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.79	
[10/25 07:49:35 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/25 07:51:06 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4884 s / batch. (data: 5.56e-03). ETA=6:53:20, max mem: 11.4 GB 
[10/25 07:52:34 visual_prompt]: 	Training 200/553. train loss: 7.6981,	0.5001 s / batch. (data: 5.41e-03). ETA=7:02:23, max mem: 11.4 GB 
[10/25 07:54:02 visual_prompt]: 	Training 300/553. train loss: 3.2139,	2.1356 s / batch. (data: 1.63e+00). ETA=1 day, 6:00:12, max mem: 11.4 GB 
[10/25 07:55:32 visual_prompt]: 	Training 400/553. train loss: 52.4959,	0.4877 s / batch. (data: 2.64e-04). ETA=6:50:17, max mem: 11.4 GB 
[10/25 07:57:01 visual_prompt]: 	Training 500/553. train loss: 45.5127,	0.7596 s / batch. (data: 2.71e-01). ETA=10:37:45, max mem: 11.4 GB 
[10/25 07:57:45 visual_prompt]: Epoch 9 / 100: avg data time: 3.92e-01, avg batch time: 0.8865, average train loss: 22.2537
[10/25 07:58:38 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1930, average loss: 34.3086
[10/25 07:58:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.51	
[10/25 07:58:38 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/25 08:00:11 visual_prompt]: 	Training 100/553. train loss: 5.4397,	0.5017 s / batch. (data: 1.34e-02). ETA=6:59:54, max mem: 11.4 GB 
[10/25 08:01:38 visual_prompt]: 	Training 200/553. train loss: 20.4420,	0.5045 s / batch. (data: 1.16e-02). ETA=7:01:28, max mem: 11.4 GB 
[10/25 08:03:06 visual_prompt]: 	Training 300/553. train loss: 41.6965,	0.5211 s / batch. (data: 3.31e-02). ETA=7:14:29, max mem: 11.4 GB 
[10/25 08:04:33 visual_prompt]: 	Training 400/553. train loss: 15.8068,	1.3680 s / batch. (data: 8.72e-01). ETA=18:58:13, max mem: 11.4 GB 
[10/25 08:06:02 visual_prompt]: 	Training 500/553. train loss: 16.8307,	0.4947 s / batch. (data: 2.76e-04). ETA=6:50:45, max mem: 11.4 GB 
[10/25 08:06:47 visual_prompt]: Epoch 10 / 100: avg data time: 3.92e-01, avg batch time: 0.8853, average train loss: 29.6774
[10/25 08:07:39 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1914, average loss: 25.8797
[10/25 08:07:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.58	
[10/25 08:07:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/25 08:09:13 visual_prompt]: 	Training 100/553. train loss: 4.5107,	0.4960 s / batch. (data: 7.97e-03). ETA=6:50:35, max mem: 11.4 GB 
[10/25 08:10:43 visual_prompt]: 	Training 200/553. train loss: 15.3534,	0.5033 s / batch. (data: 7.41e-04). ETA=6:55:46, max mem: 11.4 GB 
[10/25 08:12:11 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1560 s / batch. (data: 1.67e+00). ETA=1 day, 5:37:35, max mem: 11.4 GB 
[10/25 08:13:38 visual_prompt]: 	Training 400/553. train loss: 6.5713,	0.5040 s / batch. (data: 2.37e-04). ETA=6:54:44, max mem: 11.4 GB 
[10/25 08:15:07 visual_prompt]: 	Training 500/553. train loss: 28.6866,	0.4843 s / batch. (data: 5.41e-03). ETA=6:37:40, max mem: 11.4 GB 
[10/25 08:15:51 visual_prompt]: Epoch 11 / 100: avg data time: 3.95e-01, avg batch time: 0.8893, average train loss: 32.7429
[10/25 08:16:44 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1912, average loss: 30.1127
[10/25 08:16:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.41	
[10/25 08:16:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/25 08:18:17 visual_prompt]: 	Training 100/553. train loss: 1.3722,	1.2437 s / batch. (data: 7.52e-01). ETA=16:58:07, max mem: 11.4 GB 
[10/25 08:19:48 visual_prompt]: 	Training 200/553. train loss: 14.1676,	0.4919 s / batch. (data: 2.99e-04). ETA=6:41:53, max mem: 11.4 GB 
[10/25 08:21:15 visual_prompt]: 	Training 300/553. train loss: 5.8057,	0.5120 s / batch. (data: 2.78e-04). ETA=6:57:27, max mem: 11.4 GB 
[10/25 08:22:44 visual_prompt]: 	Training 400/553. train loss: 18.7444,	0.5070 s / batch. (data: 2.06e-02). ETA=6:52:32, max mem: 11.4 GB 
[10/25 08:24:14 visual_prompt]: 	Training 500/553. train loss: 10.3945,	0.4899 s / batch. (data: 1.04e-02). ETA=6:37:45, max mem: 11.4 GB 
[10/25 08:24:58 visual_prompt]: Epoch 12 / 100: avg data time: 4.01e-01, avg batch time: 0.8931, average train loss: 31.2986
[10/25 08:25:50 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1896, average loss: 29.5863
[10/25 08:25:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.95	
[10/25 08:25:50 visual_prompt]: Best epoch 12: best metric: -29.586
[10/25 08:25:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/25 08:27:23 visual_prompt]: 	Training 100/553. train loss: 64.4612,	0.9944 s / batch. (data: 5.04e-01). ETA=13:24:50, max mem: 11.4 GB 
[10/25 08:28:48 visual_prompt]: 	Training 200/553. train loss: 14.9184,	0.7992 s / batch. (data: 3.07e-01). ETA=10:45:34, max mem: 11.4 GB 
[10/25 08:30:18 visual_prompt]: 	Training 300/553. train loss: 13.6191,	1.8321 s / batch. (data: 1.34e+00). ETA=1 day, 0:36:46, max mem: 11.4 GB 
[10/25 08:31:46 visual_prompt]: 	Training 400/553. train loss: 81.1573,	0.4920 s / batch. (data: 2.72e-04). ETA=6:35:45, max mem: 11.4 GB 
[10/25 08:33:15 visual_prompt]: 	Training 500/553. train loss: 9.2370,	0.5040 s / batch. (data: 2.66e-04). ETA=6:44:34, max mem: 11.4 GB 
[10/25 08:34:01 visual_prompt]: Epoch 13 / 100: avg data time: 3.94e-01, avg batch time: 0.8875, average train loss: 33.8814
[10/25 08:34:53 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1905, average loss: 24.4338
[10/25 08:34:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.43	
[10/25 08:34:53 visual_prompt]: Best epoch 13: best metric: -24.434
[10/25 08:34:53 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/25 08:36:25 visual_prompt]: 	Training 100/553. train loss: 20.4180,	0.6985 s / batch. (data: 2.21e-01). ETA=9:18:54, max mem: 11.4 GB 
[10/25 08:37:56 visual_prompt]: 	Training 200/553. train loss: 9.3819,	1.9120 s / batch. (data: 1.42e+00). ETA=1 day, 1:26:47, max mem: 11.4 GB 
[10/25 08:39:23 visual_prompt]: 	Training 300/553. train loss: 27.5880,	1.1956 s / batch. (data: 7.00e-01). ETA=15:52:40, max mem: 11.4 GB 
[10/25 08:40:50 visual_prompt]: 	Training 400/553. train loss: 10.1997,	0.4864 s / batch. (data: 7.94e-03). ETA=6:26:48, max mem: 11.4 GB 
[10/25 08:42:19 visual_prompt]: 	Training 500/553. train loss: 15.6763,	0.5000 s / batch. (data: 2.56e-04). ETA=6:36:45, max mem: 11.4 GB 
[10/25 08:43:04 visual_prompt]: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8870, average train loss: 31.5401
[10/25 08:43:56 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1897, average loss: 5.4427
[10/25 08:43:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.44	
[10/25 08:43:56 visual_prompt]: Best epoch 14: best metric: -5.443
[10/25 08:43:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/25 08:45:28 visual_prompt]: 	Training 100/553. train loss: 5.1842,	0.6800 s / batch. (data: 1.82e-01). ETA=8:57:51, max mem: 11.4 GB 
[10/25 08:46:55 visual_prompt]: 	Training 200/553. train loss: 216.3350,	0.5079 s / batch. (data: 7.96e-03). ETA=6:40:54, max mem: 11.4 GB 
[10/25 08:48:26 visual_prompt]: 	Training 300/553. train loss: 97.8124,	0.5120 s / batch. (data: 2.35e-04). ETA=6:43:16, max mem: 11.4 GB 
[10/25 08:49:53 visual_prompt]: 	Training 400/553. train loss: 40.5482,	0.4783 s / batch. (data: 2.79e-04). ETA=6:15:55, max mem: 11.4 GB 
[10/25 08:51:23 visual_prompt]: 	Training 500/553. train loss: 33.5916,	0.5235 s / batch. (data: 1.59e-02). ETA=6:50:37, max mem: 11.4 GB 
[10/25 08:52:10 visual_prompt]: Epoch 15 / 100: avg data time: 3.99e-01, avg batch time: 0.8926, average train loss: 34.0745
[10/25 08:53:02 visual_prompt]: Inference (val):avg data time: 2.62e-04, avg batch time: 0.1923, average loss: 70.7752
[10/25 08:53:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.65	
[10/25 08:53:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/25 08:54:37 visual_prompt]: 	Training 100/553. train loss: 11.5465,	0.4987 s / batch. (data: 2.86e-04). ETA=6:29:51, max mem: 11.4 GB 
[10/25 08:56:12 visual_prompt]: 	Training 200/553. train loss: 20.8944,	0.5039 s / batch. (data: 2.81e-04). ETA=6:33:04, max mem: 11.4 GB 
[10/25 08:57:44 visual_prompt]: 	Training 300/553. train loss: 15.2109,	0.4764 s / batch. (data: 2.70e-04). ETA=6:10:49, max mem: 11.4 GB 
[10/25 08:59:17 visual_prompt]: 	Training 400/553. train loss: 10.8035,	0.5005 s / batch. (data: 2.88e-04). ETA=6:28:45, max mem: 11.4 GB 
[10/25 09:00:48 visual_prompt]: 	Training 500/553. train loss: 6.2283,	2.0115 s / batch. (data: 1.51e+00). ETA=1 day, 1:59:02, max mem: 11.4 GB 
[10/25 09:01:35 visual_prompt]: Epoch 16 / 100: avg data time: 4.34e-01, avg batch time: 0.9262, average train loss: 41.5263
[10/25 09:02:30 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1894, average loss: 19.5612
[10/25 09:02:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.95	
[10/25 09:02:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/25 09:04:02 visual_prompt]: 	Training 100/553. train loss: 19.3408,	0.4887 s / batch. (data: 2.85e-04). ETA=6:17:32, max mem: 11.4 GB 
[10/25 09:05:35 visual_prompt]: 	Training 200/553. train loss: 8.4499,	0.4998 s / batch. (data: 5.39e-03). ETA=6:25:15, max mem: 11.4 GB 
[10/25 09:07:05 visual_prompt]: 	Training 300/553. train loss: 5.0225,	0.5189 s / batch. (data: 1.05e-02). ETA=6:39:06, max mem: 11.4 GB 
[10/25 09:08:35 visual_prompt]: 	Training 400/553. train loss: 131.7819,	1.7976 s / batch. (data: 1.32e+00). ETA=22:59:41, max mem: 11.4 GB 
[10/25 09:10:05 visual_prompt]: 	Training 500/553. train loss: 58.8060,	2.2257 s / batch. (data: 1.73e+00). ETA=1 day, 4:24:36, max mem: 11.4 GB 
[10/25 09:10:51 visual_prompt]: Epoch 17 / 100: avg data time: 4.13e-01, avg batch time: 0.9068, average train loss: 35.5954
[10/25 09:11:45 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1907, average loss: 68.7518
[10/25 09:11:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.19	
[10/25 09:11:45 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/25 09:13:18 visual_prompt]: 	Training 100/553. train loss: 7.5340,	0.5372 s / batch. (data: 3.33e-02). ETA=6:50:02, max mem: 11.4 GB 
[10/25 09:14:52 visual_prompt]: 	Training 200/553. train loss: 13.8078,	0.4951 s / batch. (data: 1.15e-02). ETA=6:17:07, max mem: 11.4 GB 
[10/25 09:16:24 visual_prompt]: 	Training 300/553. train loss: 15.5424,	0.4956 s / batch. (data: 1.55e-02). ETA=6:16:39, max mem: 11.4 GB 
[10/25 09:18:02 visual_prompt]: 	Training 400/553. train loss: 25.7412,	0.4907 s / batch. (data: 2.64e-04). ETA=6:12:04, max mem: 11.4 GB 
[10/25 09:19:34 visual_prompt]: 	Training 500/553. train loss: 53.9353,	0.4872 s / batch. (data: 2.69e-04). ETA=6:08:38, max mem: 11.4 GB 
[10/25 09:20:20 visual_prompt]: Epoch 18 / 100: avg data time: 4.39e-01, avg batch time: 0.9312, average train loss: 38.9465
[10/25 09:21:14 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1909, average loss: 24.4347
[10/25 09:21:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.79	
[10/25 09:21:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/25 09:22:49 visual_prompt]: 	Training 100/553. train loss: 7.7362,	0.5204 s / batch. (data: 5.38e-03). ETA=6:32:24, max mem: 11.4 GB 
[10/25 09:24:21 visual_prompt]: 	Training 200/553. train loss: 39.0956,	0.5000 s / batch. (data: 2.93e-04). ETA=6:16:13, max mem: 11.4 GB 
[10/25 09:25:53 visual_prompt]: 	Training 300/553. train loss: 45.8461,	0.5209 s / batch. (data: 1.63e-02). ETA=6:31:05, max mem: 11.4 GB 
[10/25 09:27:27 visual_prompt]: 	Training 400/553. train loss: 14.8726,	0.4862 s / batch. (data: 2.88e-04). ETA=6:04:13, max mem: 11.4 GB 
[10/25 09:28:55 visual_prompt]: 	Training 500/553. train loss: 6.2373,	0.5120 s / batch. (data: 2.81e-04). ETA=6:22:41, max mem: 11.4 GB 
[10/25 09:29:42 visual_prompt]: Epoch 19 / 100: avg data time: 4.26e-01, avg batch time: 0.9190, average train loss: 34.7189
[10/25 09:30:36 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1901, average loss: 33.7331
[10/25 09:30:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.36	
[10/25 09:30:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[10/25 09:32:08 visual_prompt]: 	Training 100/553. train loss: 5.7694,	0.5554 s / batch. (data: 3.54e-02). ETA=6:53:43, max mem: 11.4 GB 
[10/25 09:33:40 visual_prompt]: 	Training 200/553. train loss: 47.3012,	0.4920 s / batch. (data: 1.19e-02). ETA=6:05:39, max mem: 11.4 GB 
[10/25 09:35:11 visual_prompt]: 	Training 300/553. train loss: 21.0664,	0.4962 s / batch. (data: 1.05e-02). ETA=6:07:57, max mem: 11.4 GB 
[10/25 09:36:40 visual_prompt]: 	Training 400/553. train loss: 25.8834,	0.5120 s / batch. (data: 7.98e-03). ETA=6:18:48, max mem: 11.4 GB 
[10/25 09:38:10 visual_prompt]: 	Training 500/553. train loss: 19.1408,	0.5016 s / batch. (data: 5.36e-03). ETA=6:10:17, max mem: 11.4 GB 
[10/25 09:38:58 visual_prompt]: Epoch 20 / 100: avg data time: 4.14e-01, avg batch time: 0.9075, average train loss: 28.5692
[10/25 09:39:51 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1913, average loss: 18.2520
[10/25 09:39:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.05	
[10/25 09:39:51 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[10/25 09:41:27 visual_prompt]: 	Training 100/553. train loss: 13.1408,	0.4772 s / batch. (data: 2.78e-04). ETA=5:51:01, max mem: 11.4 GB 
[10/25 09:42:54 visual_prompt]: 	Training 200/553. train loss: 82.9186,	0.4933 s / batch. (data: 2.84e-04). ETA=6:02:06, max mem: 11.4 GB 
[10/25 09:44:22 visual_prompt]: 	Training 300/553. train loss: 3.2471,	1.2045 s / batch. (data: 7.15e-01). ETA=14:42:04, max mem: 11.4 GB 
[10/25 09:45:49 visual_prompt]: 	Training 400/553. train loss: 11.2181,	0.4960 s / batch. (data: 2.76e-04). ETA=6:02:24, max mem: 11.4 GB 
[10/25 09:47:19 visual_prompt]: 	Training 500/553. train loss: 35.4925,	0.4785 s / batch. (data: 2.54e-04). ETA=5:48:51, max mem: 11.4 GB 
[10/25 09:48:04 visual_prompt]: Epoch 21 / 100: avg data time: 3.98e-01, avg batch time: 0.8908, average train loss: 32.7737
[10/25 09:48:56 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1912, average loss: 7.8991
[10/25 09:48:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.09	
[10/25 09:48:56 visual_prompt]: Stopping early.
[10/25 09:48:56 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 09:48:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 09:48:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 09:48:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 09:48:56 visual_prompt]: Training with config:
[10/25 09:48:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr10.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 09:48:56 visual_prompt]: Loading training data...
[10/25 09:48:56 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 09:48:57 visual_prompt]: Loading validation data...
[10/25 09:48:57 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 09:48:57 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/25 09:49:08 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/25 09:49:08 visual_prompt]: tuned percent:0.529
[10/25 09:49:08 visual_prompt]: Device used for model: 0
[10/25 09:49:08 visual_prompt]: Setting up Evaluator...
[10/25 09:49:08 visual_prompt]: Setting up Trainer...
[10/25 09:49:08 visual_prompt]: 	Setting up the optimizer...
[10/25 09:49:08 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 09:50:39 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5008 s / batch. (data: 2.84e-04). ETA=7:40:45, max mem: 11.4 GB 
[10/25 09:52:06 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5005 s / batch. (data: 8.41e-03). ETA=7:39:35, max mem: 11.4 GB 
[10/25 09:53:37 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.8675 s / batch. (data: 2.35e+00). ETA=1 day, 19:48:34, max mem: 11.4 GB 
[10/25 09:55:03 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5181 s / batch. (data: 3.19e-02). ETA=7:54:01, max mem: 11.4 GB 
[10/25 09:56:36 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4892 s / batch. (data: 1.16e-02). ETA=7:26:48, max mem: 11.4 GB 
[10/25 09:57:26 visual_prompt]: Epoch 1 / 100: avg data time: 4.07e-01, avg batch time: 0.9006, average train loss: 1.3966
[10/25 09:58:26 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1900, average loss: 1.3454
[10/25 09:58:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/25 09:58:26 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/25 10:00:04 visual_prompt]: 	Training 100/553. train loss: 2.2199,	0.4844 s / batch. (data: 2.73e-04). ETA=7:21:09, max mem: 11.4 GB 
[10/25 10:01:38 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.5120 s / batch. (data: 3.60e-04). ETA=7:45:27, max mem: 11.4 GB 
[10/25 10:03:13 visual_prompt]: 	Training 300/553. train loss: 5.0694,	1.9158 s / batch. (data: 1.43e+00). ETA=1 day, 4:58:31, max mem: 11.4 GB 
[10/25 10:04:45 visual_prompt]: 	Training 400/553. train loss: 6.3269,	0.4907 s / batch. (data: 2.63e-04). ETA=7:24:28, max mem: 11.4 GB 
[10/25 10:06:18 visual_prompt]: 	Training 500/553. train loss: 1.5629,	0.4920 s / batch. (data: 2.81e-04). ETA=7:24:47, max mem: 11.4 GB 
[10/25 10:07:04 visual_prompt]: Epoch 2 / 100: avg data time: 4.43e-01, avg batch time: 0.9361, average train loss: 4.0343
[10/25 10:07:58 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1910, average loss: 0.6896
[10/25 10:07:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 53.36	
[10/25 10:07:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/25 10:09:30 visual_prompt]: 	Training 100/553. train loss: 1.5190,	0.5119 s / batch. (data: 1.20e-02). ETA=7:41:31, max mem: 11.4 GB 
[10/25 10:11:01 visual_prompt]: 	Training 200/553. train loss: 1.3361,	0.5244 s / batch. (data: 2.06e-02). ETA=7:51:54, max mem: 11.4 GB 
[10/25 10:12:30 visual_prompt]: 	Training 300/553. train loss: 5.6767,	0.5029 s / batch. (data: 2.91e-04). ETA=7:31:43, max mem: 11.4 GB 
[10/25 10:14:02 visual_prompt]: 	Training 400/553. train loss: 27.5907,	0.4928 s / batch. (data: 1.55e-02). ETA=7:21:49, max mem: 11.4 GB 
[10/25 10:15:42 visual_prompt]: 	Training 500/553. train loss: 1.7009,	2.0560 s / batch. (data: 1.53e+00). ETA=1 day, 6:39:54, max mem: 11.4 GB 
[10/25 10:16:27 visual_prompt]: Epoch 3 / 100: avg data time: 4.26e-01, avg batch time: 0.9203, average train loss: 6.5065
[10/25 10:17:20 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1897, average loss: 12.1787
[10/25 10:17:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.56	
[10/25 10:17:20 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/25 10:18:54 visual_prompt]: 	Training 100/553. train loss: 0.6985,	0.5044 s / batch. (data: 1.55e-02). ETA=7:30:07, max mem: 11.4 GB 
[10/25 10:20:25 visual_prompt]: 	Training 200/553. train loss: 7.9729,	0.4916 s / batch. (data: 3.12e-04). ETA=7:17:50, max mem: 11.4 GB 
[10/25 10:21:56 visual_prompt]: 	Training 300/553. train loss: 2.8384,	1.5560 s / batch. (data: 1.07e+00). ETA=23:03:17, max mem: 11.4 GB 
[10/25 10:23:23 visual_prompt]: 	Training 400/553. train loss: 5.7769,	2.0800 s / batch. (data: 1.57e+00). ETA=1 day, 6:45:41, max mem: 11.4 GB 
[10/25 10:24:54 visual_prompt]: 	Training 500/553. train loss: 23.4113,	3.2127 s / batch. (data: 2.73e+00). ETA=1 day, 23:25:24, max mem: 11.4 GB 
[10/25 10:25:40 visual_prompt]: Epoch 4 / 100: avg data time: 4.10e-01, avg batch time: 0.9046, average train loss: 9.3412
[10/25 10:26:34 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1908, average loss: 2.2734
[10/25 10:26:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.87	
[10/25 10:26:34 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/25 10:28:06 visual_prompt]: 	Training 100/553. train loss: 28.7051,	0.4920 s / batch. (data: 2.74e-04). ETA=7:14:31, max mem: 11.4 GB 
[10/25 10:29:35 visual_prompt]: 	Training 200/553. train loss: 9.0778,	0.6680 s / batch. (data: 1.75e-01). ETA=9:48:50, max mem: 11.4 GB 
[10/25 10:31:07 visual_prompt]: 	Training 300/553. train loss: 54.4220,	0.4772 s / batch. (data: 2.53e-04). ETA=6:59:52, max mem: 11.4 GB 
[10/25 10:32:36 visual_prompt]: 	Training 400/553. train loss: 3.8051,	0.5057 s / batch. (data: 5.43e-03). ETA=7:24:05, max mem: 11.4 GB 
[10/25 10:34:07 visual_prompt]: 	Training 500/553. train loss: 3.2883,	0.4912 s / batch. (data: 3.62e-04). ETA=7:10:29, max mem: 11.4 GB 
[10/25 10:34:54 visual_prompt]: Epoch 5 / 100: avg data time: 4.12e-01, avg batch time: 0.9048, average train loss: 12.4714
[10/25 10:35:47 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1915, average loss: 14.5202
[10/25 10:35:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.79	
[10/25 10:35:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/25 10:37:22 visual_prompt]: 	Training 100/553. train loss: 0.4141,	0.4990 s / batch. (data: 6.99e-04). ETA=7:16:04, max mem: 11.4 GB 
[10/25 10:39:05 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.4782 s / batch. (data: 2.41e-04). ETA=6:57:05, max mem: 11.4 GB 
[10/25 10:40:51 visual_prompt]: 	Training 300/553. train loss: 86.3644,	0.4935 s / batch. (data: 2.98e-04). ETA=7:09:36, max mem: 11.4 GB 
[10/25 10:42:31 visual_prompt]: 	Training 400/553. train loss: 1.6520,	2.1833 s / batch. (data: 1.70e+00). ETA=1 day, 7:37:07, max mem: 11.4 GB 
[10/25 10:44:10 visual_prompt]: 	Training 500/553. train loss: 52.5349,	1.3196 s / batch. (data: 8.45e-01). ETA=19:04:27, max mem: 11.4 GB 
[10/25 10:45:02 visual_prompt]: Epoch 6 / 100: avg data time: 5.13e-01, avg batch time: 1.0022, average train loss: 18.2692
[10/25 10:45:56 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1915, average loss: 46.1044
[10/25 10:45:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.48	
[10/25 10:45:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/25 10:47:29 visual_prompt]: 	Training 100/553. train loss: 66.8460,	0.4955 s / batch. (data: 2.47e-04). ETA=7:08:25, max mem: 11.4 GB 
[10/25 10:49:13 visual_prompt]: 	Training 200/553. train loss: 7.2095,	0.4828 s / batch. (data: 2.98e-04). ETA=6:56:38, max mem: 11.4 GB 
[10/25 10:50:47 visual_prompt]: 	Training 300/553. train loss: 29.9643,	0.9600 s / batch. (data: 4.80e-01). ETA=13:46:54, max mem: 11.4 GB 
[10/25 10:52:18 visual_prompt]: 	Training 400/553. train loss: 1.4286,	1.8402 s / batch. (data: 1.32e+00). ETA=1 day, 2:22:03, max mem: 11.4 GB 
[10/25 10:53:48 visual_prompt]: 	Training 500/553. train loss: 5.5993,	0.4837 s / batch. (data: 6.41e-03). ETA=6:55:04, max mem: 11.4 GB 
[10/25 10:54:34 visual_prompt]: Epoch 7 / 100: avg data time: 4.44e-01, avg batch time: 0.9371, average train loss: 17.0101
[10/25 10:55:40 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1900, average loss: 4.9828
[10/25 10:55:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.18	
[10/25 10:55:40 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/25 10:57:11 visual_prompt]: 	Training 100/553. train loss: 38.9854,	1.0446 s / batch. (data: 5.57e-01). ETA=14:53:40, max mem: 11.4 GB 
[10/25 10:58:43 visual_prompt]: 	Training 200/553. train loss: 19.2402,	0.4920 s / batch. (data: 2.94e-04). ETA=7:00:05, max mem: 11.4 GB 
[10/25 11:00:15 visual_prompt]: 	Training 300/553. train loss: 4.4515,	0.4912 s / batch. (data: 3.18e-04). ETA=6:58:35, max mem: 11.4 GB 
[10/25 11:01:46 visual_prompt]: 	Training 400/553. train loss: 13.1689,	1.9525 s / batch. (data: 1.48e+00). ETA=1 day, 3:40:35, max mem: 11.4 GB 
[10/25 11:03:32 visual_prompt]: 	Training 500/553. train loss: 0.6211,	2.0799 s / batch. (data: 1.60e+00). ETA=1 day, 5:25:29, max mem: 11.4 GB 
[10/25 11:04:19 visual_prompt]: Epoch 8 / 100: avg data time: 4.46e-01, avg batch time: 0.9389, average train loss: 24.0014
[10/25 11:05:15 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1896, average loss: 2.6624
[10/25 11:05:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.17	
[10/25 11:05:15 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/25 11:06:53 visual_prompt]: 	Training 100/553. train loss: 11.9027,	0.5076 s / batch. (data: 5.39e-03). ETA=7:09:34, max mem: 11.4 GB 
[10/25 11:08:24 visual_prompt]: 	Training 200/553. train loss: 2.9969,	0.5056 s / batch. (data: 1.06e-02). ETA=7:07:03, max mem: 11.4 GB 
[10/25 11:09:57 visual_prompt]: 	Training 300/553. train loss: 6.1162,	2.7847 s / batch. (data: 2.29e+00). ETA=1 day, 15:07:18, max mem: 11.4 GB 
[10/25 11:11:35 visual_prompt]: 	Training 400/553. train loss: 21.5234,	2.0251 s / batch. (data: 1.54e+00). ETA=1 day, 4:23:39, max mem: 11.4 GB 
[10/25 11:13:12 visual_prompt]: 	Training 500/553. train loss: 32.0372,	1.0320 s / batch. (data: 5.37e-01). ETA=14:26:25, max mem: 11.4 GB 
[10/25 11:13:58 visual_prompt]: Epoch 9 / 100: avg data time: 4.55e-01, avg batch time: 0.9453, average train loss: 25.4862
[10/25 11:14:52 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1883, average loss: 3.0331
[10/25 11:14:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.60	
[10/25 11:14:52 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/25 11:16:33 visual_prompt]: 	Training 100/553. train loss: 22.9993,	0.5194 s / batch. (data: 7.30e-04). ETA=7:14:47, max mem: 11.4 GB 
[10/25 11:18:01 visual_prompt]: 	Training 200/553. train loss: 0.9821,	0.5000 s / batch. (data: 4.35e-04). ETA=6:57:39, max mem: 11.4 GB 
[10/25 11:19:32 visual_prompt]: 	Training 300/553. train loss: 1.1823,	0.4782 s / batch. (data: 4.08e-04). ETA=6:38:39, max mem: 11.4 GB 
[10/25 11:21:04 visual_prompt]: 	Training 400/553. train loss: 30.2770,	0.5960 s / batch. (data: 1.08e-01). ETA=8:15:52, max mem: 11.4 GB 
[10/25 11:22:36 visual_prompt]: 	Training 500/553. train loss: 12.9545,	1.6480 s / batch. (data: 1.15e+00). ETA=22:48:30, max mem: 11.4 GB 
[10/25 11:23:28 visual_prompt]: Epoch 10 / 100: avg data time: 4.40e-01, avg batch time: 0.9326, average train loss: 27.4897
[10/25 11:24:23 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1910, average loss: 5.8286
[10/25 11:24:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.15	
[10/25 11:24:23 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/25 11:26:06 visual_prompt]: 	Training 100/553. train loss: 1.8020,	0.4842 s / batch. (data: 5.40e-03). ETA=6:40:52, max mem: 11.4 GB 
[10/25 11:27:37 visual_prompt]: 	Training 200/553. train loss: 77.5346,	0.4845 s / batch. (data: 2.64e-04). ETA=6:40:16, max mem: 11.4 GB 
[10/25 11:29:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.3761 s / batch. (data: 8.90e-01). ETA=18:54:36, max mem: 11.4 GB 
[10/25 11:30:35 visual_prompt]: 	Training 400/553. train loss: 23.1846,	0.5040 s / batch. (data: 2.59e-04). ETA=6:54:40, max mem: 11.4 GB 
[10/25 11:32:04 visual_prompt]: 	Training 500/553. train loss: 38.6782,	0.5000 s / batch. (data: 2.79e-04). ETA=6:50:34, max mem: 11.4 GB 
[10/25 11:32:49 visual_prompt]: Epoch 11 / 100: avg data time: 4.23e-01, avg batch time: 0.9161, average train loss: 24.8287
[10/25 11:33:43 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1918, average loss: 12.0178
[10/25 11:33:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.17	
[10/25 11:33:43 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/25 11:35:18 visual_prompt]: 	Training 100/553. train loss: 14.6328,	0.4780 s / batch. (data: 2.89e-04). ETA=6:31:17, max mem: 11.4 GB 
[10/25 11:36:49 visual_prompt]: 	Training 200/553. train loss: 3.2111,	0.5003 s / batch. (data: 2.73e-04). ETA=6:48:41, max mem: 11.4 GB 
[10/25 11:38:18 visual_prompt]: 	Training 300/553. train loss: 26.8453,	0.5200 s / batch. (data: 5.40e-03). ETA=7:03:57, max mem: 11.4 GB 
[10/25 11:39:55 visual_prompt]: 	Training 400/553. train loss: 23.6135,	0.4999 s / batch. (data: 2.64e-04). ETA=6:46:42, max mem: 11.4 GB 
[10/25 11:41:28 visual_prompt]: 	Training 500/553. train loss: 9.1156,	0.4991 s / batch. (data: 1.05e-02). ETA=6:45:12, max mem: 11.4 GB 
[10/25 11:42:12 visual_prompt]: Epoch 12 / 100: avg data time: 4.28e-01, avg batch time: 0.9212, average train loss: 24.1209
[10/25 11:43:05 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1909, average loss: 156.4617
[10/25 11:43:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.80	
[10/25 11:43:05 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/25 11:44:40 visual_prompt]: 	Training 100/553. train loss: 0.9678,	0.8784 s / batch. (data: 4.01e-01). ETA=11:50:56, max mem: 11.4 GB 
[10/25 11:46:09 visual_prompt]: 	Training 200/553. train loss: 1.3247,	0.4896 s / batch. (data: 5.40e-03). ETA=6:35:28, max mem: 11.4 GB 
[10/25 11:47:50 visual_prompt]: 	Training 300/553. train loss: 15.2937,	0.7127 s / batch. (data: 2.32e-01). ETA=9:34:28, max mem: 11.4 GB 
[10/25 11:49:20 visual_prompt]: 	Training 400/553. train loss: 63.1122,	0.4770 s / batch. (data: 2.93e-04). ETA=6:23:43, max mem: 11.4 GB 
[10/25 11:50:53 visual_prompt]: 	Training 500/553. train loss: 21.8557,	0.4960 s / batch. (data: 2.77e-04). ETA=6:38:08, max mem: 11.4 GB 
[10/25 11:51:40 visual_prompt]: Epoch 13 / 100: avg data time: 4.38e-01, avg batch time: 0.9313, average train loss: 26.1270
[10/25 11:52:34 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1911, average loss: 32.2700
[10/25 11:52:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.52	
[10/25 11:52:34 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/25 11:54:09 visual_prompt]: 	Training 100/553. train loss: 21.7649,	0.5240 s / batch. (data: 2.75e-04). ETA=6:59:16, max mem: 11.4 GB 
[10/25 11:55:41 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.9394 s / batch. (data: 1.44e+00). ETA=1 day, 1:48:37, max mem: 11.4 GB 
[10/25 11:57:12 visual_prompt]: 	Training 300/553. train loss: 34.6655,	1.4720 s / batch. (data: 9.77e-01). ETA=19:32:56, max mem: 11.4 GB 
[10/25 11:58:56 visual_prompt]: 	Training 400/553. train loss: 21.9296,	1.2600 s / batch. (data: 7.48e-01). ETA=16:41:56, max mem: 11.4 GB 
[10/25 12:00:25 visual_prompt]: 	Training 500/553. train loss: 46.1958,	0.5796 s / batch. (data: 9.84e-02). ETA=7:39:54, max mem: 11.4 GB 
[10/25 12:01:11 visual_prompt]: Epoch 14 / 100: avg data time: 4.42e-01, avg batch time: 0.9350, average train loss: 26.1845
[10/25 12:02:04 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1905, average loss: 16.5371
[10/25 12:02:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.60	
[10/25 12:02:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/25 12:03:38 visual_prompt]: 	Training 100/553. train loss: 31.7815,	0.8361 s / batch. (data: 3.49e-01). ETA=11:01:18, max mem: 11.4 GB 
[10/25 12:05:08 visual_prompt]: 	Training 200/553. train loss: 20.2648,	0.4909 s / batch. (data: 3.00e-04). ETA=6:27:25, max mem: 11.4 GB 
[10/25 12:06:40 visual_prompt]: 	Training 300/553. train loss: 4.5865,	0.5080 s / batch. (data: 2.45e-04). ETA=6:40:05, max mem: 11.4 GB 
[10/25 12:08:08 visual_prompt]: 	Training 400/553. train loss: 5.5312,	0.4936 s / batch. (data: 2.72e-04). ETA=6:27:58, max mem: 11.4 GB 
[10/25 12:09:38 visual_prompt]: 	Training 500/553. train loss: 7.7796,	0.8355 s / batch. (data: 3.47e-01). ETA=10:55:15, max mem: 11.4 GB 
[10/25 12:10:26 visual_prompt]: Epoch 15 / 100: avg data time: 4.13e-01, avg batch time: 0.9062, average train loss: 31.0833
[10/25 12:11:19 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1912, average loss: 12.5149
[10/25 12:11:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.50	
[10/25 12:11:19 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/25 12:13:09 visual_prompt]: 	Training 100/553. train loss: 38.9610,	0.4784 s / batch. (data: 2.81e-04). ETA=6:14:01, max mem: 11.4 GB 
[10/25 12:14:40 visual_prompt]: 	Training 200/553. train loss: 3.0200,	0.4785 s / batch. (data: 2.60e-04). ETA=6:13:17, max mem: 11.4 GB 
[10/25 12:16:10 visual_prompt]: 	Training 300/553. train loss: 34.4683,	0.4888 s / batch. (data: 3.08e-04). ETA=6:20:28, max mem: 11.4 GB 
[10/25 12:17:40 visual_prompt]: 	Training 400/553. train loss: 13.3593,	0.4980 s / batch. (data: 5.37e-03). ETA=6:26:49, max mem: 11.4 GB 
[10/25 12:19:10 visual_prompt]: 	Training 500/553. train loss: 7.0707,	2.1111 s / batch. (data: 1.61e+00). ETA=1 day, 3:16:16, max mem: 11.4 GB 
[10/25 12:19:57 visual_prompt]: Epoch 16 / 100: avg data time: 4.43e-01, avg batch time: 0.9363, average train loss: 24.9332
[10/25 12:20:50 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1920, average loss: 85.9356
[10/25 12:20:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.33	
[10/25 12:20:50 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/25 12:22:23 visual_prompt]: 	Training 100/553. train loss: 68.7333,	0.4852 s / batch. (data: 2.55e-04). ETA=6:14:48, max mem: 11.4 GB 
[10/25 12:23:56 visual_prompt]: 	Training 200/553. train loss: 51.7481,	0.4840 s / batch. (data: 2.64e-04). ETA=6:13:05, max mem: 11.4 GB 
[10/25 12:25:25 visual_prompt]: 	Training 300/553. train loss: 45.8006,	0.4972 s / batch. (data: 2.03e-02). ETA=6:22:24, max mem: 11.4 GB 
[10/25 12:26:54 visual_prompt]: 	Training 400/553. train loss: 13.5469,	1.0400 s / batch. (data: 5.55e-01). ETA=13:18:14, max mem: 11.4 GB 
[10/25 12:28:24 visual_prompt]: 	Training 500/553. train loss: 10.2202,	2.3480 s / batch. (data: 1.86e+00). ETA=1 day, 5:58:15, max mem: 11.4 GB 
[10/25 12:29:10 visual_prompt]: Epoch 17 / 100: avg data time: 4.12e-01, avg batch time: 0.9040, average train loss: 42.7748
[10/25 12:30:08 visual_prompt]: Inference (val):avg data time: 4.09e-04, avg batch time: 0.1916, average loss: 25.9581
[10/25 12:30:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[10/25 12:30:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/25 12:31:42 visual_prompt]: 	Training 100/553. train loss: 7.0589,	0.4800 s / batch. (data: 2.65e-04). ETA=6:06:22, max mem: 11.4 GB 
[10/25 12:33:14 visual_prompt]: 	Training 200/553. train loss: 27.4098,	0.4920 s / batch. (data: 2.59e-04). ETA=6:14:45, max mem: 11.4 GB 
[10/25 12:34:43 visual_prompt]: 	Training 300/553. train loss: 38.9125,	0.5035 s / batch. (data: 7.97e-03). ETA=6:22:39, max mem: 11.4 GB 
[10/25 12:36:14 visual_prompt]: 	Training 400/553. train loss: 14.5951,	0.5006 s / batch. (data: 5.38e-03). ETA=6:19:36, max mem: 11.4 GB 
[10/25 12:37:43 visual_prompt]: 	Training 500/553. train loss: 4.8051,	0.4920 s / batch. (data: 2.76e-04). ETA=6:12:15, max mem: 11.4 GB 
[10/25 12:38:28 visual_prompt]: Epoch 18 / 100: avg data time: 4.10e-01, avg batch time: 0.9036, average train loss: 31.5561
[10/25 12:39:26 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1899, average loss: 20.5035
[10/25 12:39:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.03	
[10/25 12:39:26 visual_prompt]: Stopping early.
[10/25 12:39:27 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 12:39:27 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 12:39:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 12:39:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 12:39:27 visual_prompt]: Training with config:
[10/25 12:39:27 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr10.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 12:39:27 visual_prompt]: Loading training data...
[10/25 12:39:27 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 12:39:27 visual_prompt]: Loading validation data...
[10/25 12:39:27 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 12:39:27 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/25 12:39:30 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/25 12:39:30 visual_prompt]: tuned percent:0.529
[10/25 12:39:30 visual_prompt]: Device used for model: 0
[10/25 12:39:30 visual_prompt]: Setting up Evaluator...
[10/25 12:39:30 visual_prompt]: Setting up Trainer...
[10/25 12:39:30 visual_prompt]: 	Setting up the optimizer...
[10/25 12:39:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 12:41:03 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4958 s / batch. (data: 1.19e-02). ETA=7:36:06, max mem: 11.4 GB 
[10/25 12:42:31 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4840 s / batch. (data: 2.65e-04). ETA=7:24:28, max mem: 11.4 GB 
[10/25 12:44:04 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.0835 s / batch. (data: 2.59e+00). ETA=1 day, 23:06:30, max mem: 11.4 GB 
[10/25 12:45:31 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5199 s / batch. (data: 2.69e-04). ETA=7:55:41, max mem: 11.4 GB 
[10/25 12:47:03 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4781 s / batch. (data: 2.63e-04). ETA=7:16:38, max mem: 11.4 GB 
[10/25 12:47:51 visual_prompt]: Epoch 1 / 100: avg data time: 4.12e-01, avg batch time: 0.9054, average train loss: 1.3966
[10/25 12:48:44 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1915, average loss: 1.3454
[10/25 12:48:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/25 12:48:44 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/25 12:50:19 visual_prompt]: 	Training 100/553. train loss: 6.5593,	0.7477 s / batch. (data: 2.55e-01). ETA=11:20:57, max mem: 11.4 GB 
[10/25 12:51:49 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2400 s / batch. (data: 7.55e-01). ETA=18:47:16, max mem: 11.4 GB 
[10/25 12:53:22 visual_prompt]: 	Training 300/553. train loss: 8.5501,	1.6840 s / batch. (data: 1.20e+00). ETA=1 day, 1:28:09, max mem: 11.4 GB 
[10/25 12:54:50 visual_prompt]: 	Training 400/553. train loss: 3.5699,	0.6154 s / batch. (data: 1.20e-01). ETA=9:17:27, max mem: 11.4 GB 
[10/25 12:56:23 visual_prompt]: 	Training 500/553. train loss: 0.4025,	0.4989 s / batch. (data: 5.40e-03). ETA=7:31:03, max mem: 11.4 GB 
[10/25 12:57:08 visual_prompt]: Epoch 2 / 100: avg data time: 4.17e-01, avg batch time: 0.9108, average train loss: 4.8962
[10/25 12:58:02 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.1905, average loss: 8.7812
[10/25 12:58:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.35	
[10/25 12:58:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/25 12:59:33 visual_prompt]: 	Training 100/553. train loss: 10.8225,	1.1952 s / batch. (data: 6.89e-01). ETA=17:57:30, max mem: 11.4 GB 
[10/25 13:01:04 visual_prompt]: 	Training 200/553. train loss: 5.2945,	0.5000 s / batch. (data: 2.42e-04). ETA=7:29:58, max mem: 11.4 GB 
[10/25 13:02:33 visual_prompt]: 	Training 300/553. train loss: 0.7333,	0.5160 s / batch. (data: 2.40e-02). ETA=7:43:28, max mem: 11.4 GB 
[10/25 13:04:06 visual_prompt]: 	Training 400/553. train loss: 0.7712,	0.4942 s / batch. (data: 7.99e-03). ETA=7:23:02, max mem: 11.4 GB 
[10/25 13:05:38 visual_prompt]: 	Training 500/553. train loss: 6.3024,	1.8444 s / batch. (data: 1.34e+00). ETA=1 day, 3:30:34, max mem: 11.4 GB 
[10/25 13:06:23 visual_prompt]: Epoch 3 / 100: avg data time: 4.12e-01, avg batch time: 0.9071, average train loss: 7.5037
[10/25 13:07:17 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.1909, average loss: 8.1897
[10/25 13:07:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.88	
[10/25 13:07:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/25 13:08:51 visual_prompt]: 	Training 100/553. train loss: 8.0353,	0.4918 s / batch. (data: 5.41e-03). ETA=7:18:51, max mem: 11.4 GB 
[10/25 13:10:22 visual_prompt]: 	Training 200/553. train loss: 1.2946,	0.5008 s / batch. (data: 4.72e-03). ETA=7:26:02, max mem: 11.4 GB 
[10/25 13:11:52 visual_prompt]: 	Training 300/553. train loss: 6.5364,	1.4760 s / batch. (data: 9.64e-01). ETA=21:52:09, max mem: 11.4 GB 
[10/25 13:13:19 visual_prompt]: 	Training 400/553. train loss: 0.2788,	2.1120 s / batch. (data: 1.62e+00). ETA=1 day, 7:14:04, max mem: 11.4 GB 
[10/25 13:14:57 visual_prompt]: 	Training 500/553. train loss: 1.0410,	3.5600 s / batch. (data: 3.08e+00). ETA=2 days, 4:33:03, max mem: 11.4 GB 
[10/25 13:15:43 visual_prompt]: Epoch 4 / 100: avg data time: 4.21e-01, avg batch time: 0.9159, average train loss: 9.0508
[10/25 13:16:37 visual_prompt]: Inference (val):avg data time: 2.81e-04, avg batch time: 0.1929, average loss: 23.5472
[10/25 13:16:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.61	
[10/25 13:16:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/25 13:18:11 visual_prompt]: 	Training 100/553. train loss: 43.8693,	0.4932 s / batch. (data: 5.40e-03). ETA=7:15:35, max mem: 11.4 GB 
[10/25 13:19:45 visual_prompt]: 	Training 200/553. train loss: 2.4321,	0.9518 s / batch. (data: 4.58e-01). ETA=13:59:00, max mem: 11.4 GB 
[10/25 13:21:18 visual_prompt]: 	Training 300/553. train loss: 71.4032,	0.5203 s / batch. (data: 2.65e-02). ETA=7:37:46, max mem: 11.4 GB 
[10/25 13:22:47 visual_prompt]: 	Training 400/553. train loss: 3.2332,	0.4781 s / batch. (data: 2.83e-04). ETA=6:59:50, max mem: 11.4 GB 
[10/25 13:24:18 visual_prompt]: 	Training 500/553. train loss: 12.2993,	0.4920 s / batch. (data: 2.38e-04). ETA=7:11:13, max mem: 11.4 GB 
[10/25 13:25:05 visual_prompt]: Epoch 5 / 100: avg data time: 4.26e-01, avg batch time: 0.9193, average train loss: 13.8461
[10/25 13:25:58 visual_prompt]: Inference (val):avg data time: 2.75e-04, avg batch time: 0.1914, average loss: 17.0577
[10/25 13:25:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.98	
[10/25 13:25:58 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/25 13:27:33 visual_prompt]: 	Training 100/553. train loss: 3.1629,	0.5300 s / batch. (data: 1.84e-02). ETA=7:43:12, max mem: 11.4 GB 
[10/25 13:29:03 visual_prompt]: 	Training 200/553. train loss: 43.4837,	0.4780 s / batch. (data: 2.67e-04). ETA=6:56:58, max mem: 11.4 GB 
[10/25 13:30:51 visual_prompt]: 	Training 300/553. train loss: 17.4448,	0.4836 s / batch. (data: 2.48e-04). ETA=7:01:02, max mem: 11.4 GB 
[10/25 13:32:33 visual_prompt]: 	Training 400/553. train loss: 22.6689,	1.3696 s / batch. (data: 8.94e-01). ETA=19:50:01, max mem: 11.4 GB 
[10/25 13:34:12 visual_prompt]: 	Training 500/553. train loss: 36.1144,	1.3948 s / batch. (data: 9.11e-01). ETA=20:09:35, max mem: 11.4 GB 
[10/25 13:35:02 visual_prompt]: Epoch 6 / 100: avg data time: 4.91e-01, avg batch time: 0.9831, average train loss: 13.2479
[10/25 13:36:00 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1902, average loss: 15.4117
[10/25 13:36:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.11	
[10/25 13:36:00 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/25 13:37:34 visual_prompt]: 	Training 100/553. train loss: 21.6667,	0.5080 s / batch. (data: 2.99e-04). ETA=7:19:16, max mem: 11.4 GB 
[10/25 13:39:06 visual_prompt]: 	Training 200/553. train loss: 3.9238,	0.4878 s / batch. (data: 7.62e-03). ETA=7:01:01, max mem: 11.4 GB 
[10/25 13:40:40 visual_prompt]: 	Training 300/553. train loss: 19.3979,	1.5440 s / batch. (data: 1.04e+00). ETA=22:09:56, max mem: 11.4 GB 
[10/25 13:42:11 visual_prompt]: 	Training 400/553. train loss: 6.5611,	0.8160 s / batch. (data: 3.16e-01). ETA=11:41:29, max mem: 11.4 GB 
[10/25 13:43:42 visual_prompt]: 	Training 500/553. train loss: 4.3787,	0.4882 s / batch. (data: 2.42e-04). ETA=6:58:54, max mem: 11.4 GB 
[10/25 13:44:27 visual_prompt]: Epoch 7 / 100: avg data time: 4.23e-01, avg batch time: 0.9172, average train loss: 13.4094
[10/25 13:45:21 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1912, average loss: 19.7240
[10/25 13:45:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.09	
[10/25 13:45:21 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/25 13:46:53 visual_prompt]: 	Training 100/553. train loss: 32.0823,	1.6720 s / batch. (data: 1.19e+00). ETA=23:50:22, max mem: 11.4 GB 
[10/25 13:48:25 visual_prompt]: 	Training 200/553. train loss: 6.8636,	0.4882 s / batch. (data: 3.02e-04). ETA=6:56:48, max mem: 11.4 GB 
[10/25 13:49:56 visual_prompt]: 	Training 300/553. train loss: 1.8018,	0.5133 s / batch. (data: 7.36e-04). ETA=7:17:22, max mem: 11.4 GB 
[10/25 13:51:27 visual_prompt]: 	Training 400/553. train loss: 7.7565,	0.5160 s / batch. (data: 7.96e-03). ETA=7:18:51, max mem: 11.4 GB 
[10/25 13:52:57 visual_prompt]: 	Training 500/553. train loss: 81.0335,	1.9760 s / batch. (data: 1.50e+00). ETA=1 day, 3:57:15, max mem: 11.4 GB 
[10/25 13:53:43 visual_prompt]: Epoch 8 / 100: avg data time: 4.14e-01, avg batch time: 0.9072, average train loss: 18.3085
[10/25 13:54:35 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1906, average loss: 50.8058
[10/25 13:54:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.96	
[10/25 13:54:35 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/25 13:56:08 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.5093 s / batch. (data: 1.33e-02). ETA=7:11:01, max mem: 11.4 GB 
[10/25 13:57:37 visual_prompt]: 	Training 200/553. train loss: 2.4077,	0.4791 s / batch. (data: 2.71e-04). ETA=6:44:40, max mem: 11.4 GB 
[10/25 13:59:06 visual_prompt]: 	Training 300/553. train loss: 40.9394,	1.9835 s / batch. (data: 1.50e+00). ETA=1 day, 3:51:59, max mem: 11.4 GB 
[10/25 14:00:36 visual_prompt]: 	Training 400/553. train loss: 9.2310,	0.4840 s / batch. (data: 2.88e-04). ETA=6:47:09, max mem: 11.4 GB 
[10/25 14:02:06 visual_prompt]: 	Training 500/553. train loss: 3.1188,	0.4880 s / batch. (data: 2.79e-04). ETA=6:49:44, max mem: 11.4 GB 
[10/25 14:02:50 visual_prompt]: Epoch 9 / 100: avg data time: 4.01e-01, avg batch time: 0.8938, average train loss: 22.0006
[10/25 14:03:46 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1924, average loss: 2.9552
[10/25 14:03:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.05	
[10/25 14:03:46 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/25 14:05:22 visual_prompt]: 	Training 100/553. train loss: 30.3212,	1.3880 s / batch. (data: 8.81e-01). ETA=19:21:50, max mem: 11.4 GB 
[10/25 14:06:49 visual_prompt]: 	Training 200/553. train loss: 14.3108,	0.5120 s / batch. (data: 2.36e-04). ETA=7:07:44, max mem: 11.4 GB 
[10/25 14:08:18 visual_prompt]: 	Training 300/553. train loss: 5.7476,	1.1757 s / batch. (data: 6.99e-01). ETA=16:20:10, max mem: 11.4 GB 
[10/25 14:09:45 visual_prompt]: 	Training 400/553. train loss: 20.5457,	0.7637 s / batch. (data: 2.68e-01). ETA=10:35:26, max mem: 11.4 GB 
[10/25 14:11:15 visual_prompt]: 	Training 500/553. train loss: 8.9123,	0.4839 s / batch. (data: 2.75e-04). ETA=6:41:51, max mem: 11.4 GB 
[10/25 14:12:00 visual_prompt]: Epoch 10 / 100: avg data time: 4.02e-01, avg batch time: 0.8941, average train loss: 23.0698
[10/25 14:12:53 visual_prompt]: Inference (val):avg data time: 3.85e-04, avg batch time: 0.1916, average loss: 10.3406
[10/25 14:12:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[10/25 14:12:53 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/25 14:14:26 visual_prompt]: 	Training 100/553. train loss: 9.4288,	0.5094 s / batch. (data: 2.54e-04). ETA=7:01:40, max mem: 11.4 GB 
[10/25 14:15:56 visual_prompt]: 	Training 200/553. train loss: 35.1002,	0.5095 s / batch. (data: 5.43e-03). ETA=7:00:53, max mem: 11.4 GB 
[10/25 14:17:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.9381 s / batch. (data: 4.53e-01). ETA=12:53:27, max mem: 11.4 GB 
[10/25 14:18:53 visual_prompt]: 	Training 400/553. train loss: 17.3869,	0.4781 s / batch. (data: 3.07e-04). ETA=6:33:24, max mem: 11.4 GB 
[10/25 14:20:24 visual_prompt]: 	Training 500/553. train loss: 8.5567,	0.4990 s / batch. (data: 7.97e-03). ETA=6:49:46, max mem: 11.4 GB 
[10/25 14:21:09 visual_prompt]: Epoch 11 / 100: avg data time: 4.04e-01, avg batch time: 0.8980, average train loss: 14.6058
[10/25 14:22:04 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1909, average loss: 28.1553
[10/25 14:22:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.25	
[10/25 14:22:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/25 14:23:40 visual_prompt]: 	Training 100/553. train loss: 32.0554,	0.5124 s / batch. (data: 5.38e-03). ETA=6:59:29, max mem: 11.4 GB 
[10/25 14:25:12 visual_prompt]: 	Training 200/553. train loss: 60.7100,	0.4878 s / batch. (data: 7.95e-03). ETA=6:38:28, max mem: 11.4 GB 
[10/25 14:26:42 visual_prompt]: 	Training 300/553. train loss: 1.2949,	0.4959 s / batch. (data: 4.64e-04). ETA=6:44:17, max mem: 11.4 GB 
[10/25 14:28:14 visual_prompt]: 	Training 400/553. train loss: 4.5200,	0.4880 s / batch. (data: 2.65e-04). ETA=6:37:03, max mem: 11.4 GB 
[10/25 14:29:46 visual_prompt]: 	Training 500/553. train loss: 4.1445,	0.5205 s / batch. (data: 5.40e-03). ETA=7:02:36, max mem: 11.4 GB 
[10/25 14:30:33 visual_prompt]: Epoch 12 / 100: avg data time: 4.29e-01, avg batch time: 0.9203, average train loss: 20.2567
[10/25 14:31:25 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1906, average loss: 54.9723
[10/25 14:31:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.47	
[10/25 14:31:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/25 14:32:58 visual_prompt]: 	Training 100/553. train loss: 4.8666,	1.8120 s / batch. (data: 1.30e+00). ETA=1 day, 0:26:36, max mem: 11.4 GB 
[10/25 14:34:24 visual_prompt]: 	Training 200/553. train loss: 5.4969,	0.5000 s / batch. (data: 2.65e-04). ETA=6:43:51, max mem: 11.4 GB 
[10/25 14:35:54 visual_prompt]: 	Training 300/553. train loss: 4.7609,	2.4640 s / batch. (data: 1.95e+00). ETA=1 day, 9:06:07, max mem: 11.4 GB 
[10/25 14:37:24 visual_prompt]: 	Training 400/553. train loss: 2.2510,	0.4944 s / batch. (data: 2.67e-04). ETA=6:37:39, max mem: 11.4 GB 
[10/25 14:39:17 visual_prompt]: 	Training 500/553. train loss: 29.4086,	0.4774 s / batch. (data: 1.84e-04). ETA=6:23:12, max mem: 11.4 GB 
[10/25 14:40:09 visual_prompt]: Epoch 13 / 100: avg data time: 4.54e-01, avg batch time: 0.9468, average train loss: 18.8713
[10/25 14:41:07 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1910, average loss: 7.7764
[10/25 14:41:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.55	
[10/25 14:41:07 visual_prompt]: Best epoch 13: best metric: -7.776
[10/25 14:41:07 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/25 14:42:44 visual_prompt]: 	Training 100/553. train loss: 27.3158,	0.4825 s / batch. (data: 5.40e-03). ETA=6:26:03, max mem: 11.4 GB 
[10/25 14:44:16 visual_prompt]: 	Training 200/553. train loss: 0.2659,	1.8440 s / batch. (data: 1.35e+00). ETA=1 day, 0:32:28, max mem: 11.4 GB 
[10/25 14:45:45 visual_prompt]: 	Training 300/553. train loss: 6.7650,	1.4080 s / batch. (data: 9.18e-01). ETA=18:41:58, max mem: 11.4 GB 
[10/25 14:47:15 visual_prompt]: 	Training 400/553. train loss: 1.4383,	0.4966 s / batch. (data: 5.40e-03). ETA=6:34:50, max mem: 11.4 GB 
[10/25 14:48:44 visual_prompt]: 	Training 500/553. train loss: 21.9718,	0.5055 s / batch. (data: 2.57e-04). ETA=6:41:06, max mem: 11.4 GB 
[10/25 14:49:29 visual_prompt]: Epoch 14 / 100: avg data time: 4.14e-01, avg batch time: 0.9069, average train loss: 19.2662
[10/25 14:50:21 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1898, average loss: 9.0499
[10/25 14:50:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.55	
[10/25 14:50:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/25 14:51:53 visual_prompt]: 	Training 100/553. train loss: 17.5252,	0.4839 s / batch. (data: 2.99e-04). ETA=6:22:44, max mem: 11.4 GB 
[10/25 14:53:20 visual_prompt]: 	Training 200/553. train loss: 73.0554,	0.5120 s / batch. (data: 2.98e-04). ETA=6:44:06, max mem: 11.4 GB 
[10/25 14:54:49 visual_prompt]: 	Training 300/553. train loss: 31.9397,	0.5033 s / batch. (data: 2.79e-04). ETA=6:36:24, max mem: 11.4 GB 
[10/25 14:56:16 visual_prompt]: 	Training 400/553. train loss: 5.1875,	0.4960 s / batch. (data: 2.67e-04). ETA=6:29:50, max mem: 11.4 GB 
[10/25 14:57:46 visual_prompt]: 	Training 500/553. train loss: 17.9879,	0.5358 s / batch. (data: 2.38e-02). ETA=7:00:14, max mem: 11.4 GB 
[10/25 14:58:32 visual_prompt]: Epoch 15 / 100: avg data time: 3.94e-01, avg batch time: 0.8872, average train loss: 28.7956
[10/25 14:59:24 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1902, average loss: 27.9780
[10/25 14:59:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.23	
[10/25 14:59:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/25 15:00:55 visual_prompt]: 	Training 100/553. train loss: 27.4488,	0.4960 s / batch. (data: 2.56e-04). ETA=6:27:45, max mem: 11.4 GB 
[10/25 15:02:23 visual_prompt]: 	Training 200/553. train loss: 19.8402,	0.5120 s / batch. (data: 7.95e-03). ETA=6:39:24, max mem: 11.4 GB 
[10/25 15:03:53 visual_prompt]: 	Training 300/553. train loss: 8.1790,	0.5282 s / batch. (data: 2.43e-02). ETA=6:51:08, max mem: 11.4 GB 
[10/25 15:05:21 visual_prompt]: 	Training 400/553. train loss: 30.5717,	0.4933 s / batch. (data: 5.38e-03). ETA=6:23:08, max mem: 11.4 GB 
[10/25 15:06:49 visual_prompt]: 	Training 500/553. train loss: 33.4289,	2.1232 s / batch. (data: 1.63e+00). ETA=1 day, 3:25:38, max mem: 11.4 GB 
[10/25 15:07:35 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 0.8869, average train loss: 20.6539
[10/25 15:08:27 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.1921, average loss: 8.6879
[10/25 15:08:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.26	
[10/25 15:08:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/25 15:09:57 visual_prompt]: 	Training 100/553. train loss: 24.4087,	0.4917 s / batch. (data: 2.57e-04). ETA=6:19:49, max mem: 11.4 GB 
[10/25 15:11:28 visual_prompt]: 	Training 200/553. train loss: 67.9867,	0.4861 s / batch. (data: 1.04e-02). ETA=6:14:42, max mem: 11.4 GB 
[10/25 15:12:57 visual_prompt]: 	Training 300/553. train loss: 43.8830,	0.5013 s / batch. (data: 2.46e-04). ETA=6:25:35, max mem: 11.4 GB 
[10/25 15:14:24 visual_prompt]: 	Training 400/553. train loss: 10.6383,	1.6018 s / batch. (data: 1.12e+00). ETA=20:29:27, max mem: 11.4 GB 
[10/25 15:15:52 visual_prompt]: 	Training 500/553. train loss: 3.7817,	2.1640 s / batch. (data: 1.67e+00). ETA=1 day, 3:37:22, max mem: 11.4 GB 
[10/25 15:16:38 visual_prompt]: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8880, average train loss: 20.2306
[10/25 15:17:38 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1895, average loss: 2.8415
[10/25 15:17:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.17	
[10/25 15:17:38 visual_prompt]: Best epoch 17: best metric: -2.841
[10/25 15:17:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/25 15:19:17 visual_prompt]: 	Training 100/553. train loss: 18.0115,	0.4920 s / batch. (data: 2.73e-04). ETA=6:15:34, max mem: 11.4 GB 
[10/25 15:20:53 visual_prompt]: 	Training 200/553. train loss: 38.2478,	0.4857 s / batch. (data: 3.09e-04). ETA=6:09:55, max mem: 11.4 GB 
[10/25 15:22:24 visual_prompt]: 	Training 300/553. train loss: 24.1539,	0.4954 s / batch. (data: 5.40e-03). ETA=6:16:29, max mem: 11.4 GB 
[10/25 15:23:55 visual_prompt]: 	Training 400/553. train loss: 25.9192,	0.4880 s / batch. (data: 2.63e-04). ETA=6:10:05, max mem: 11.4 GB 
[10/25 15:25:24 visual_prompt]: 	Training 500/553. train loss: 13.2038,	0.4877 s / batch. (data: 2.73e-04). ETA=6:09:02, max mem: 11.4 GB 
[10/25 15:26:09 visual_prompt]: Epoch 18 / 100: avg data time: 4.34e-01, avg batch time: 0.9252, average train loss: 27.0578
[10/25 15:27:02 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.1915, average loss: 27.7778
[10/25 15:27:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.24	
[10/25 15:27:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/25 15:28:34 visual_prompt]: 	Training 100/553. train loss: 5.4877,	0.4881 s / batch. (data: 2.61e-04). ETA=6:08:04, max mem: 11.4 GB 
[10/25 15:30:03 visual_prompt]: 	Training 200/553. train loss: 13.0710,	0.4921 s / batch. (data: 2.70e-04). ETA=6:10:16, max mem: 11.4 GB 
[10/25 15:31:33 visual_prompt]: 	Training 300/553. train loss: 40.5538,	0.4994 s / batch. (data: 7.86e-03). ETA=6:14:55, max mem: 11.4 GB 
[10/25 15:33:05 visual_prompt]: 	Training 400/553. train loss: 19.3404,	0.5000 s / batch. (data: 1.20e-02). ETA=6:14:34, max mem: 11.4 GB 
[10/25 15:34:31 visual_prompt]: 	Training 500/553. train loss: 10.8735,	0.5442 s / batch. (data: 1.23e-02). ETA=6:46:46, max mem: 11.4 GB 
[10/25 15:35:17 visual_prompt]: Epoch 19 / 100: avg data time: 4.01e-01, avg batch time: 0.8961, average train loss: 17.0554
[10/25 15:36:10 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1914, average loss: 2.3476
[10/25 15:36:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.01	
[10/25 15:36:10 visual_prompt]: Best epoch 19: best metric: -2.348
[10/25 15:36:10 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[10/25 15:37:43 visual_prompt]: 	Training 100/553. train loss: 8.8807,	0.7153 s / batch. (data: 2.11e-01). ETA=8:52:50, max mem: 11.4 GB 
[10/25 15:39:21 visual_prompt]: 	Training 200/553. train loss: 10.2245,	0.8987 s / batch. (data: 4.16e-01). ETA=11:07:56, max mem: 11.4 GB 
[10/25 15:41:09 visual_prompt]: 	Training 300/553. train loss: 3.9762,	0.5207 s / batch. (data: 1.05e-02). ETA=6:26:06, max mem: 11.4 GB 
[10/25 15:42:48 visual_prompt]: 	Training 400/553. train loss: 20.1283,	0.4849 s / batch. (data: 2.77e-04). ETA=5:58:44, max mem: 11.4 GB 
[10/25 15:44:23 visual_prompt]: 	Training 500/553. train loss: 9.2693,	0.5087 s / batch. (data: 1.56e-02). ETA=6:15:33, max mem: 11.4 GB 
[10/25 15:45:12 visual_prompt]: Epoch 20 / 100: avg data time: 4.90e-01, avg batch time: 0.9811, average train loss: 17.6480
[10/25 15:46:06 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1905, average loss: 1.9208
[10/25 15:46:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 58.58	
[10/25 15:46:06 visual_prompt]: Best epoch 20: best metric: -1.921
[10/25 15:46:06 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[10/25 15:47:43 visual_prompt]: 	Training 100/553. train loss: 13.4399,	0.6882 s / batch. (data: 1.90e-01). ETA=8:26:18, max mem: 11.4 GB 
[10/25 15:49:12 visual_prompt]: 	Training 200/553. train loss: 16.6281,	0.5120 s / batch. (data: 2.78e-04). ETA=6:15:47, max mem: 11.4 GB 
[10/25 15:50:43 visual_prompt]: 	Training 300/553. train loss: 33.9035,	0.5000 s / batch. (data: 2.67e-04). ETA=6:06:10, max mem: 11.4 GB 
[10/25 15:52:13 visual_prompt]: 	Training 400/553. train loss: 61.8235,	0.4945 s / batch. (data: 1.19e-02). ETA=6:01:18, max mem: 11.4 GB 
[10/25 15:53:46 visual_prompt]: 	Training 500/553. train loss: 9.0838,	0.5016 s / batch. (data: 2.45e-04). ETA=6:05:38, max mem: 11.4 GB 
[10/25 15:54:32 visual_prompt]: Epoch 21 / 100: avg data time: 4.21e-01, avg batch time: 0.9144, average train loss: 18.8139
[10/25 15:55:27 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1908, average loss: 29.4499
[10/25 15:55:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.38	
[10/25 15:55:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[10/25 15:57:00 visual_prompt]: 	Training 100/553. train loss: 3.6557,	0.4966 s / batch. (data: 7.74e-04). ETA=6:00:47, max mem: 11.4 GB 
[10/25 15:58:30 visual_prompt]: 	Training 200/553. train loss: 24.8827,	0.4868 s / batch. (data: 2.78e-04). ETA=5:52:50, max mem: 11.4 GB 
[10/25 15:59:58 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.5036 s / batch. (data: 5.39e-03). ETA=6:04:09, max mem: 11.4 GB 
[10/25 16:01:28 visual_prompt]: 	Training 400/553. train loss: 9.6702,	0.4912 s / batch. (data: 2.62e-04). ETA=5:54:22, max mem: 11.4 GB 
[10/25 16:02:58 visual_prompt]: 	Training 500/553. train loss: 17.8480,	0.5330 s / batch. (data: 3.31e-02). ETA=6:23:38, max mem: 11.4 GB 
[10/25 16:03:45 visual_prompt]: Epoch 22 / 100: avg data time: 4.08e-01, avg batch time: 0.9006, average train loss: 19.9260
[10/25 16:04:38 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1908, average loss: 5.8901
[10/25 16:04:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.14	
[10/25 16:04:38 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[10/25 16:06:19 visual_prompt]: 	Training 100/553. train loss: 27.1268,	0.5040 s / batch. (data: 8.39e-03). ETA=6:01:27, max mem: 11.4 GB 
[10/25 16:07:55 visual_prompt]: 	Training 200/553. train loss: 0.2803,	0.5280 s / batch. (data: 5.39e-03). ETA=6:17:49, max mem: 11.4 GB 
[10/25 16:09:31 visual_prompt]: 	Training 300/553. train loss: 30.3169,	0.4922 s / batch. (data: 7.65e-04). ETA=5:51:24, max mem: 11.4 GB 
[10/25 16:11:03 visual_prompt]: 	Training 400/553. train loss: 4.9255,	0.4919 s / batch. (data: 2.68e-04). ETA=5:50:22, max mem: 11.4 GB 
[10/25 16:12:33 visual_prompt]: 	Training 500/553. train loss: 2.0019,	0.4937 s / batch. (data: 1.55e-02). ETA=5:50:47, max mem: 11.4 GB 
[10/25 16:13:20 visual_prompt]: Epoch 23 / 100: avg data time: 4.52e-01, avg batch time: 0.9439, average train loss: 17.9220
[10/25 16:14:16 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1905, average loss: 2.4323
[10/25 16:14:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.36	
[10/25 16:14:16 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[10/25 16:15:48 visual_prompt]: 	Training 100/553. train loss: 40.0275,	0.5000 s / batch. (data: 3.18e-04). ETA=5:53:59, max mem: 11.4 GB 
[10/25 16:17:18 visual_prompt]: 	Training 200/553. train loss: 15.3087,	0.5000 s / batch. (data: 6.95e-04). ETA=5:53:10, max mem: 11.4 GB 
[10/25 16:18:50 visual_prompt]: 	Training 300/553. train loss: 17.4195,	1.6683 s / batch. (data: 1.15e+00). ETA=19:35:36, max mem: 11.4 GB 
[10/25 16:20:20 visual_prompt]: 	Training 400/553. train loss: 0.1196,	0.4920 s / batch. (data: 2.87e-04). ETA=5:45:54, max mem: 11.4 GB 
[10/25 16:21:56 visual_prompt]: 	Training 500/553. train loss: 61.5785,	0.8382 s / batch. (data: 3.63e-01). ETA=9:47:50, max mem: 11.4 GB 
[10/25 16:22:43 visual_prompt]: Epoch 24 / 100: avg data time: 4.24e-01, avg batch time: 0.9175, average train loss: 21.2099
[10/25 16:23:36 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1920, average loss: 19.4753
[10/25 16:23:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.52	
[10/25 16:23:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[10/25 16:25:13 visual_prompt]: 	Training 100/553. train loss: 54.4213,	0.4809 s / batch. (data: 2.68e-04). ETA=5:36:02, max mem: 11.4 GB 
[10/25 16:26:40 visual_prompt]: 	Training 200/553. train loss: 8.0688,	0.5137 s / batch. (data: 8.76e-03). ETA=5:58:08, max mem: 11.4 GB 
[10/25 16:28:10 visual_prompt]: 	Training 300/553. train loss: 10.9026,	2.0436 s / batch. (data: 1.56e+00). ETA=23:41:14, max mem: 11.4 GB 
[10/25 16:29:42 visual_prompt]: 	Training 400/553. train loss: 34.9053,	1.9280 s / batch. (data: 1.43e+00). ETA=22:17:38, max mem: 11.4 GB 
[10/25 16:31:15 visual_prompt]: 	Training 500/553. train loss: 29.9444,	2.2641 s / batch. (data: 1.77e+00). ETA=1 day, 2:07:01, max mem: 11.4 GB 
[10/25 16:32:03 visual_prompt]: Epoch 25 / 100: avg data time: 4.21e-01, avg batch time: 0.9153, average train loss: 18.6986
[10/25 16:32:58 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1902, average loss: 11.6822
[10/25 16:32:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.16	
[10/25 16:32:58 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[10/25 16:34:33 visual_prompt]: 	Training 100/553. train loss: 25.6588,	0.5323 s / batch. (data: 1.61e-02). ETA=6:07:01, max mem: 11.4 GB 
[10/25 16:36:07 visual_prompt]: 	Training 200/553. train loss: 71.5235,	2.5081 s / batch. (data: 2.02e+00). ETA=1 day, 4:45:21, max mem: 11.4 GB 
[10/25 16:37:40 visual_prompt]: 	Training 300/553. train loss: 9.6458,	0.5080 s / batch. (data: 2.63e-04). ETA=5:48:38, max mem: 11.4 GB 
[10/25 16:39:11 visual_prompt]: 	Training 400/553. train loss: 49.1921,	0.4855 s / batch. (data: 2.72e-04). ETA=5:32:22, max mem: 11.4 GB 
[10/25 16:40:41 visual_prompt]: 	Training 500/553. train loss: 47.8002,	0.5120 s / batch. (data: 2.52e-04). ETA=5:49:40, max mem: 11.4 GB 
[10/25 16:41:28 visual_prompt]: Epoch 26 / 100: avg data time: 4.28e-01, avg batch time: 0.9214, average train loss: 22.9484
[10/25 16:42:21 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1896, average loss: 54.3274
[10/25 16:42:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.10	
[10/25 16:42:21 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[10/25 16:44:05 visual_prompt]: 	Training 100/553. train loss: 3.2324,	0.5200 s / batch. (data: 2.75e-04). ETA=5:53:45, max mem: 11.4 GB 
[10/25 16:45:36 visual_prompt]: 	Training 200/553. train loss: 17.8417,	2.3172 s / batch. (data: 1.84e+00). ETA=1 day, 2:12:42, max mem: 11.4 GB 
[10/25 16:47:07 visual_prompt]: 	Training 300/553. train loss: 13.8526,	1.1274 s / batch. (data: 6.25e-01). ETA=12:43:16, max mem: 11.4 GB 
[10/25 16:48:39 visual_prompt]: 	Training 400/553. train loss: 2.5081,	0.4874 s / batch. (data: 2.76e-04). ETA=5:29:10, max mem: 11.4 GB 
[10/25 16:50:13 visual_prompt]: 	Training 500/553. train loss: 22.5236,	0.4882 s / batch. (data: 2.62e-04). ETA=5:28:53, max mem: 11.4 GB 
[10/25 16:50:59 visual_prompt]: Epoch 27 / 100: avg data time: 4.42e-01, avg batch time: 0.9352, average train loss: 17.6784
[10/25 16:51:54 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1904, average loss: 60.7505
[10/25 16:51:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.99	
[10/25 16:51:54 visual_prompt]: Stopping early.
[10/25 16:51:54 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 16:51:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 16:51:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 16:51:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 16:51:54 visual_prompt]: Training with config:
[10/25 16:51:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr5.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 16:51:54 visual_prompt]: Loading training data...
[10/25 16:51:54 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 16:51:54 visual_prompt]: Loading validation data...
[10/25 16:51:54 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 16:51:54 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/25 16:51:57 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/25 16:51:57 visual_prompt]: tuned percent:0.529
[10/25 16:51:57 visual_prompt]: Device used for model: 0
[10/25 16:51:57 visual_prompt]: Setting up Evaluator...
[10/25 16:51:57 visual_prompt]: Setting up Trainer...
[10/25 16:51:57 visual_prompt]: 	Setting up the optimizer...
[10/25 16:51:57 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 16:53:38 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4767 s / batch. (data: 1.47e-04). ETA=7:18:35, max mem: 11.4 GB 
[10/25 16:55:14 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5239 s / batch. (data: 2.72e-04). ETA=8:01:09, max mem: 11.4 GB 
[10/25 16:56:53 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.0289 s / batch. (data: 2.55e+00). ETA=1 day, 22:16:29, max mem: 11.4 GB 
[10/25 16:58:21 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4960 s / batch. (data: 3.04e-04). ETA=7:33:49, max mem: 11.4 GB 
[10/25 16:59:55 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4997 s / batch. (data: 2.80e-04). ETA=7:36:23, max mem: 11.4 GB 
[10/25 17:00:42 visual_prompt]: Epoch 1 / 100: avg data time: 4.56e-01, avg batch time: 0.9498, average train loss: 1.3966
[10/25 17:01:36 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1894, average loss: 1.3454
[10/25 17:01:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/25 17:01:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/25 17:03:08 visual_prompt]: 	Training 100/553. train loss: 0.9065,	0.5628 s / batch. (data: 4.89e-02). ETA=8:32:34, max mem: 11.4 GB 
[10/25 17:04:38 visual_prompt]: 	Training 200/553. train loss: 0.0010,	1.1099 s / batch. (data: 6.28e-01). ETA=16:48:59, max mem: 11.4 GB 
[10/25 17:06:10 visual_prompt]: 	Training 300/553. train loss: 0.9532,	1.8152 s / batch. (data: 1.33e+00). ETA=1 day, 3:27:12, max mem: 11.4 GB 
[10/25 17:07:38 visual_prompt]: 	Training 400/553. train loss: 2.1679,	0.5115 s / batch. (data: 5.44e-03). ETA=7:43:18, max mem: 11.4 GB 
[10/25 17:09:10 visual_prompt]: 	Training 500/553. train loss: 1.8510,	0.4875 s / batch. (data: 9.60e-03). ETA=7:20:47, max mem: 11.4 GB 
[10/25 17:09:54 visual_prompt]: Epoch 2 / 100: avg data time: 4.06e-01, avg batch time: 0.9013, average train loss: 1.7747
[10/25 17:10:47 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.1918, average loss: 6.3718
[10/25 17:10:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.49	
[10/25 17:10:47 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/25 17:12:17 visual_prompt]: 	Training 100/553. train loss: 1.5914,	0.4841 s / batch. (data: 2.74e-04). ETA=7:16:24, max mem: 11.4 GB 
[10/25 17:13:48 visual_prompt]: 	Training 200/553. train loss: 1.0039,	0.5010 s / batch. (data: 5.40e-03). ETA=7:30:50, max mem: 11.4 GB 
[10/25 17:15:17 visual_prompt]: 	Training 300/553. train loss: 5.8669,	0.4791 s / batch. (data: 2.82e-04). ETA=7:10:17, max mem: 11.4 GB 
[10/25 17:16:49 visual_prompt]: 	Training 400/553. train loss: 0.0489,	0.4922 s / batch. (data: 2.84e-04). ETA=7:21:18, max mem: 11.4 GB 
[10/25 17:18:22 visual_prompt]: 	Training 500/553. train loss: 2.8812,	1.9892 s / batch. (data: 1.48e+00). ETA=1 day, 5:40:07, max mem: 11.4 GB 
[10/25 17:19:09 visual_prompt]: Epoch 3 / 100: avg data time: 4.13e-01, avg batch time: 0.9077, average train loss: 2.6084
[10/25 17:20:04 visual_prompt]: Inference (val):avg data time: 1.83e-04, avg batch time: 0.1922, average loss: 2.8892
[10/25 17:20:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.55	
[10/25 17:20:04 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/25 17:21:39 visual_prompt]: 	Training 100/553. train loss: 4.7377,	0.5041 s / batch. (data: 1.63e-02). ETA=7:29:52, max mem: 11.4 GB 
[10/25 17:23:13 visual_prompt]: 	Training 200/553. train loss: 2.8119,	0.5159 s / batch. (data: 2.02e-02). ETA=7:39:30, max mem: 11.4 GB 
[10/25 17:24:45 visual_prompt]: 	Training 300/553. train loss: 3.9637,	2.1875 s / batch. (data: 1.69e+00). ETA=1 day, 8:24:44, max mem: 11.4 GB 
[10/25 17:26:13 visual_prompt]: 	Training 400/553. train loss: 0.5640,	1.9620 s / batch. (data: 1.47e+00). ETA=1 day, 5:00:56, max mem: 11.4 GB 
[10/25 17:27:45 visual_prompt]: 	Training 500/553. train loss: 3.9540,	3.4280 s / batch. (data: 2.95e+00). ETA=2 days, 2:36:06, max mem: 11.4 GB 
[10/25 17:28:32 visual_prompt]: Epoch 4 / 100: avg data time: 4.25e-01, avg batch time: 0.9190, average train loss: 4.7205
[10/25 17:29:25 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1919, average loss: 1.5756
[10/25 17:29:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.22	
[10/25 17:29:25 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/25 17:30:58 visual_prompt]: 	Training 100/553. train loss: 2.6013,	0.5038 s / batch. (data: 7.69e-03). ETA=7:24:53, max mem: 11.4 GB 
[10/25 17:32:28 visual_prompt]: 	Training 200/553. train loss: 11.3852,	1.7690 s / batch. (data: 1.28e+00). ETA=1 day, 1:59:20, max mem: 11.4 GB 
[10/25 17:33:59 visual_prompt]: 	Training 300/553. train loss: 6.6760,	0.5041 s / batch. (data: 7.39e-04). ETA=7:23:31, max mem: 11.4 GB 
[10/25 17:35:28 visual_prompt]: 	Training 400/553. train loss: 1.6191,	0.5076 s / batch. (data: 2.51e-04). ETA=7:25:45, max mem: 11.4 GB 
[10/25 17:36:58 visual_prompt]: 	Training 500/553. train loss: 0.7236,	0.4881 s / batch. (data: 2.57e-04). ETA=7:07:48, max mem: 11.4 GB 
[10/25 17:37:44 visual_prompt]: Epoch 5 / 100: avg data time: 4.08e-01, avg batch time: 0.9016, average train loss: 6.3164
[10/25 17:38:36 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1930, average loss: 6.3522
[10/25 17:38:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.62	
[10/25 17:38:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/25 17:40:09 visual_prompt]: 	Training 100/553. train loss: 9.7328,	0.4800 s / batch. (data: 2.60e-04). ETA=6:59:30, max mem: 11.4 GB 
[10/25 17:41:41 visual_prompt]: 	Training 200/553. train loss: 4.6068,	0.5006 s / batch. (data: 2.97e-04). ETA=7:16:38, max mem: 11.4 GB 
[10/25 17:43:12 visual_prompt]: 	Training 300/553. train loss: 3.8882,	0.4891 s / batch. (data: 1.04e-02). ETA=7:05:48, max mem: 11.4 GB 
[10/25 17:44:54 visual_prompt]: 	Training 400/553. train loss: 4.5461,	0.9307 s / batch. (data: 4.45e-01). ETA=13:28:41, max mem: 11.4 GB 
[10/25 17:46:26 visual_prompt]: 	Training 500/553. train loss: 42.3274,	1.5285 s / batch. (data: 1.03e+00). ETA=22:05:37, max mem: 11.4 GB 
[10/25 17:47:12 visual_prompt]: Epoch 6 / 100: avg data time: 4.39e-01, avg batch time: 0.9328, average train loss: 9.4172
[10/25 17:48:06 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1904, average loss: 6.6783
[10/25 17:48:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.99	
[10/25 17:48:06 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/25 17:49:38 visual_prompt]: 	Training 100/553. train loss: 32.3347,	0.5174 s / batch. (data: 9.36e-03). ETA=7:27:25, max mem: 11.4 GB 
[10/25 17:51:08 visual_prompt]: 	Training 200/553. train loss: 6.3985,	0.5080 s / batch. (data: 7.94e-03). ETA=7:18:24, max mem: 11.4 GB 
[10/25 17:52:42 visual_prompt]: 	Training 300/553. train loss: 7.1146,	2.4000 s / batch. (data: 1.88e+00). ETA=1 day, 10:27:16, max mem: 11.4 GB 
[10/25 17:54:13 visual_prompt]: 	Training 400/553. train loss: 16.5686,	2.4560 s / batch. (data: 1.97e+00). ETA=1 day, 11:11:24, max mem: 11.4 GB 
[10/25 17:55:40 visual_prompt]: 	Training 500/553. train loss: 9.5569,	0.5120 s / batch. (data: 7.94e-03). ETA=7:19:19, max mem: 11.4 GB 
[10/25 17:56:25 visual_prompt]: Epoch 7 / 100: avg data time: 4.08e-01, avg batch time: 0.9032, average train loss: 9.2257
[10/25 17:57:18 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1908, average loss: 12.7857
[10/25 17:57:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.43	
[10/25 17:57:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/25 17:58:54 visual_prompt]: 	Training 100/553. train loss: 1.7061,	0.5240 s / batch. (data: 7.95e-03). ETA=7:28:16, max mem: 11.4 GB 
[10/25 18:00:24 visual_prompt]: 	Training 200/553. train loss: 3.2907,	0.4920 s / batch. (data: 2.67e-04). ETA=7:00:04, max mem: 11.4 GB 
[10/25 18:01:53 visual_prompt]: 	Training 300/553. train loss: 5.7511,	0.5070 s / batch. (data: 2.85e-02). ETA=7:12:02, max mem: 11.4 GB 
[10/25 18:03:25 visual_prompt]: 	Training 400/553. train loss: 17.4289,	0.5200 s / batch. (data: 5.37e-03). ETA=7:22:17, max mem: 11.4 GB 
[10/25 18:04:53 visual_prompt]: 	Training 500/553. train loss: 2.3076,	1.9268 s / batch. (data: 1.43e+00). ETA=1 day, 3:15:32, max mem: 11.4 GB 
[10/25 18:05:39 visual_prompt]: Epoch 8 / 100: avg data time: 4.12e-01, avg batch time: 0.9053, average train loss: 10.8746
[10/25 18:06:31 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1939, average loss: 13.1454
[10/25 18:06:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.99	
[10/25 18:06:31 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/25 18:08:03 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4859 s / batch. (data: 8.46e-03). ETA=6:51:14, max mem: 11.4 GB 
[10/25 18:09:32 visual_prompt]: 	Training 200/553. train loss: 8.6689,	0.5160 s / batch. (data: 2.71e-04). ETA=7:15:48, max mem: 11.4 GB 
[10/25 18:11:01 visual_prompt]: 	Training 300/553. train loss: 1.2190,	1.9160 s / batch. (data: 1.44e+00). ETA=1 day, 2:55:04, max mem: 11.4 GB 
[10/25 18:12:31 visual_prompt]: 	Training 400/553. train loss: 2.6023,	0.5022 s / batch. (data: 6.95e-03). ETA=7:02:27, max mem: 11.4 GB 
[10/25 18:14:04 visual_prompt]: 	Training 500/553. train loss: 5.5847,	0.5887 s / batch. (data: 1.06e-01). ETA=8:14:14, max mem: 11.4 GB 
[10/25 18:14:51 visual_prompt]: Epoch 9 / 100: avg data time: 4.08e-01, avg batch time: 0.9027, average train loss: 12.1472
[10/25 18:15:46 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1904, average loss: 19.1763
[10/25 18:15:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.77	
[10/25 18:15:46 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/25 18:17:23 visual_prompt]: 	Training 100/553. train loss: 47.6941,	0.5200 s / batch. (data: 7.97e-03). ETA=7:15:17, max mem: 11.4 GB 
[10/25 18:18:53 visual_prompt]: 	Training 200/553. train loss: 3.0058,	0.5000 s / batch. (data: 2.70e-04). ETA=6:57:42, max mem: 11.4 GB 
[10/25 18:20:24 visual_prompt]: 	Training 300/553. train loss: 6.2570,	0.5024 s / batch. (data: 1.01e-02). ETA=6:58:49, max mem: 11.4 GB 
[10/25 18:21:53 visual_prompt]: 	Training 400/553. train loss: 0.9910,	1.2754 s / batch. (data: 7.83e-01). ETA=17:41:11, max mem: 11.4 GB 
[10/25 18:23:24 visual_prompt]: 	Training 500/553. train loss: 11.9555,	1.6986 s / batch. (data: 1.21e+00). ETA=23:30:26, max mem: 11.4 GB 
[10/25 18:24:10 visual_prompt]: Epoch 10 / 100: avg data time: 4.17e-01, avg batch time: 0.9112, average train loss: 14.5087
[10/25 18:25:03 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1909, average loss: 14.2404
[10/25 18:25:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.05	
[10/25 18:25:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/25 18:26:38 visual_prompt]: 	Training 100/553. train loss: 15.3872,	0.5160 s / batch. (data: 7.96e-03). ETA=7:07:10, max mem: 11.4 GB 
[10/25 18:28:09 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.5130 s / batch. (data: 2.80e-04). ETA=7:03:50, max mem: 11.4 GB 
[10/25 18:29:37 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.6578 s / batch. (data: 1.15e+00). ETA=22:46:52, max mem: 11.4 GB 
[10/25 18:31:06 visual_prompt]: 	Training 400/553. train loss: 15.7907,	0.4957 s / batch. (data: 7.73e-04). ETA=6:47:51, max mem: 11.4 GB 
[10/25 18:32:34 visual_prompt]: 	Training 500/553. train loss: 19.1519,	0.4919 s / batch. (data: 2.60e-04). ETA=6:43:54, max mem: 11.4 GB 
[10/25 18:33:19 visual_prompt]: Epoch 11 / 100: avg data time: 4.01e-01, avg batch time: 0.8967, average train loss: 14.1827
[10/25 18:34:11 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1906, average loss: 7.6639
[10/25 18:34:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.71	
[10/25 18:34:11 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/25 18:35:44 visual_prompt]: 	Training 100/553. train loss: 17.8191,	0.4880 s / batch. (data: 2.37e-04). ETA=6:39:28, max mem: 11.4 GB 
[10/25 18:37:14 visual_prompt]: 	Training 200/553. train loss: 9.2448,	0.4989 s / batch. (data: 2.59e-04). ETA=6:47:35, max mem: 11.4 GB 
[10/25 18:38:41 visual_prompt]: 	Training 300/553. train loss: 7.5587,	0.5080 s / batch. (data: 2.77e-04). ETA=6:54:10, max mem: 11.4 GB 
[10/25 18:40:10 visual_prompt]: 	Training 400/553. train loss: 18.0851,	0.4960 s / batch. (data: 2.72e-04). ETA=6:43:32, max mem: 11.4 GB 
[10/25 18:41:39 visual_prompt]: 	Training 500/553. train loss: 3.4939,	0.5040 s / batch. (data: 2.22e-04). ETA=6:49:14, max mem: 11.4 GB 
[10/25 18:42:23 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8890, average train loss: 17.1432
[10/25 18:43:15 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1908, average loss: 10.7515
[10/25 18:43:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.03	
[10/25 18:43:15 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/25 18:44:48 visual_prompt]: 	Training 100/553. train loss: 21.6362,	0.4920 s / batch. (data: 2.92e-04). ETA=6:38:11, max mem: 11.4 GB 
[10/25 18:46:14 visual_prompt]: 	Training 200/553. train loss: 6.4349,	0.4955 s / batch. (data: 2.85e-04). ETA=6:40:13, max mem: 11.4 GB 
[10/25 18:47:43 visual_prompt]: 	Training 300/553. train loss: 7.4110,	1.3760 s / batch. (data: 8.85e-01). ETA=18:29:07, max mem: 11.4 GB 
[10/25 18:49:10 visual_prompt]: 	Training 400/553. train loss: 23.5988,	0.4895 s / batch. (data: 8.93e-03). ETA=6:33:46, max mem: 11.4 GB 
[10/25 18:50:40 visual_prompt]: 	Training 500/553. train loss: 46.4940,	0.4952 s / batch. (data: 2.19e-04). ETA=6:37:33, max mem: 11.4 GB 
[10/25 18:51:25 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8860, average train loss: 18.2380
[10/25 18:52:17 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1922, average loss: 23.4107
[10/25 18:52:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.14	
[10/25 18:52:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/25 18:53:49 visual_prompt]: 	Training 100/553. train loss: 1.6156,	0.5640 s / batch. (data: 4.31e-02). ETA=7:31:16, max mem: 11.4 GB 
[10/25 18:55:19 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.9150 s / batch. (data: 1.43e+00). ETA=1 day, 1:29:11, max mem: 11.4 GB 
[10/25 18:56:47 visual_prompt]: 	Training 300/553. train loss: 21.8375,	1.4471 s / batch. (data: 9.70e-01). ETA=19:13:07, max mem: 11.4 GB 
[10/25 18:58:15 visual_prompt]: 	Training 400/553. train loss: 8.7763,	0.5680 s / batch. (data: 8.24e-02). ETA=7:31:40, max mem: 11.4 GB 
[10/25 18:59:45 visual_prompt]: 	Training 500/553. train loss: 16.1122,	0.5000 s / batch. (data: 2.76e-04). ETA=6:36:46, max mem: 11.4 GB 
[10/25 19:00:29 visual_prompt]: Epoch 14 / 100: avg data time: 3.95e-01, avg batch time: 0.8891, average train loss: 16.3873
[10/25 19:01:22 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1907, average loss: 14.8015
[10/25 19:01:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.61	
[10/25 19:01:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/25 19:02:54 visual_prompt]: 	Training 100/553. train loss: 20.8542,	0.5320 s / batch. (data: 3.06e-04). ETA=7:00:47, max mem: 11.4 GB 
[10/25 19:04:21 visual_prompt]: 	Training 200/553. train loss: 77.9933,	0.5073 s / batch. (data: 5.39e-03). ETA=6:40:23, max mem: 11.4 GB 
[10/25 19:05:51 visual_prompt]: 	Training 300/553. train loss: 50.1198,	0.4916 s / batch. (data: 2.69e-04). ETA=6:27:11, max mem: 11.4 GB 
[10/25 19:07:18 visual_prompt]: 	Training 400/553. train loss: 12.7367,	0.4923 s / batch. (data: 5.45e-03). ETA=6:26:53, max mem: 11.4 GB 
[10/25 19:08:49 visual_prompt]: 	Training 500/553. train loss: 75.2143,	0.4890 s / batch. (data: 2.85e-04). ETA=6:23:30, max mem: 11.4 GB 
[10/25 19:09:35 visual_prompt]: Epoch 15 / 100: avg data time: 3.99e-01, avg batch time: 0.8928, average train loss: 20.1126
[10/25 19:10:28 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1912, average loss: 8.4491
[10/25 19:10:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.66	
[10/25 19:10:28 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/25 19:11:59 visual_prompt]: 	Training 100/553. train loss: 16.2103,	0.5026 s / batch. (data: 1.20e-02). ETA=6:32:54, max mem: 11.4 GB 
[10/25 19:13:29 visual_prompt]: 	Training 200/553. train loss: 0.9354,	0.5021 s / batch. (data: 1.01e-02). ETA=6:31:42, max mem: 11.4 GB 
[10/25 19:14:58 visual_prompt]: 	Training 300/553. train loss: 15.9852,	0.5200 s / batch. (data: 7.29e-04). ETA=6:44:44, max mem: 11.4 GB 
[10/25 19:16:27 visual_prompt]: 	Training 400/553. train loss: 1.4169,	0.5040 s / batch. (data: 2.69e-04). ETA=6:31:30, max mem: 11.4 GB 
[10/25 19:17:55 visual_prompt]: 	Training 500/553. train loss: 4.7628,	2.2002 s / batch. (data: 1.72e+00). ETA=1 day, 4:25:21, max mem: 11.4 GB 
[10/25 19:18:41 visual_prompt]: Epoch 16 / 100: avg data time: 3.96e-01, avg batch time: 0.8913, average train loss: 15.5826
[10/25 19:19:33 visual_prompt]: Inference (val):avg data time: 1.83e-04, avg batch time: 0.1919, average loss: 39.9440
[10/25 19:19:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.01	
[10/25 19:19:33 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/25 19:21:05 visual_prompt]: 	Training 100/553. train loss: 34.3456,	0.5200 s / batch. (data: 1.20e-02). ETA=6:41:43, max mem: 11.4 GB 
[10/25 19:22:40 visual_prompt]: 	Training 200/553. train loss: 13.6511,	0.5017 s / batch. (data: 2.23e-02). ETA=6:26:46, max mem: 11.4 GB 
[10/25 19:24:12 visual_prompt]: 	Training 300/553. train loss: 0.7477,	0.4935 s / batch. (data: 1.55e-02). ETA=6:19:38, max mem: 11.4 GB 
[10/25 19:25:43 visual_prompt]: 	Training 400/553. train loss: 10.8159,	1.7720 s / batch. (data: 1.25e+00). ETA=22:40:03, max mem: 11.4 GB 
[10/25 19:27:14 visual_prompt]: 	Training 500/553. train loss: 11.1733,	2.4037 s / batch. (data: 1.90e+00). ETA=1 day, 6:40:53, max mem: 11.4 GB 
[10/25 19:28:02 visual_prompt]: Epoch 17 / 100: avg data time: 4.25e-01, avg batch time: 0.9191, average train loss: 16.9719
[10/25 19:28:55 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1908, average loss: 2.9179
[10/25 19:28:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.87	
[10/25 19:28:55 visual_prompt]: Best epoch 17: best metric: -2.918
[10/25 19:28:55 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/25 19:30:30 visual_prompt]: 	Training 100/553. train loss: 5.3810,	0.5120 s / batch. (data: 2.65e-04). ETA=6:30:50, max mem: 11.4 GB 
[10/25 19:32:02 visual_prompt]: 	Training 200/553. train loss: 6.1555,	0.5120 s / batch. (data: 3.00e-04). ETA=6:29:57, max mem: 11.4 GB 
[10/25 19:33:31 visual_prompt]: 	Training 300/553. train loss: 9.5833,	0.5065 s / batch. (data: 7.94e-03). ETA=6:24:55, max mem: 11.4 GB 
[10/25 19:35:01 visual_prompt]: 	Training 400/553. train loss: 0.7426,	0.5164 s / batch. (data: 1.05e-02). ETA=6:31:37, max mem: 11.4 GB 
[10/25 19:36:30 visual_prompt]: 	Training 500/553. train loss: 17.1696,	0.4843 s / batch. (data: 2.75e-04). ETA=6:06:28, max mem: 11.4 GB 
[10/25 19:37:15 visual_prompt]: Epoch 18 / 100: avg data time: 4.08e-01, avg batch time: 0.9027, average train loss: 16.7256
[10/25 19:38:08 visual_prompt]: Inference (val):avg data time: 3.92e-04, avg batch time: 0.1912, average loss: 18.1588
[10/25 19:38:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.71	
[10/25 19:38:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/25 19:39:40 visual_prompt]: 	Training 100/553. train loss: 10.5189,	0.4912 s / batch. (data: 2.77e-04). ETA=6:10:23, max mem: 11.4 GB 
[10/25 19:41:10 visual_prompt]: 	Training 200/553. train loss: 1.3994,	0.5080 s / batch. (data: 1.19e-02). ETA=6:22:13, max mem: 11.4 GB 
[10/25 19:42:39 visual_prompt]: 	Training 300/553. train loss: 30.3402,	0.4886 s / batch. (data: 2.58e-04). ETA=6:06:51, max mem: 11.4 GB 
[10/25 19:44:09 visual_prompt]: 	Training 400/553. train loss: 3.7717,	0.4893 s / batch. (data: 2.70e-04). ETA=6:06:30, max mem: 11.4 GB 
[10/25 19:45:33 visual_prompt]: 	Training 500/553. train loss: 1.9785,	0.5000 s / batch. (data: 2.63e-04). ETA=6:13:42, max mem: 11.4 GB 
[10/25 19:46:19 visual_prompt]: Epoch 19 / 100: avg data time: 3.94e-01, avg batch time: 0.8890, average train loss: 17.3465
[10/25 19:47:11 visual_prompt]: Inference (val):avg data time: 1.83e-04, avg batch time: 0.1897, average loss: 5.2784
[10/25 19:47:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.40	
[10/25 19:47:11 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[10/25 19:48:42 visual_prompt]: 	Training 100/553. train loss: 11.8199,	0.9824 s / batch. (data: 4.66e-01). ETA=12:11:44, max mem: 11.4 GB 
[10/25 19:50:11 visual_prompt]: 	Training 200/553. train loss: 3.2815,	0.4960 s / batch. (data: 2.26e-04). ETA=6:08:39, max mem: 11.4 GB 
[10/25 19:51:40 visual_prompt]: 	Training 300/553. train loss: 11.5689,	0.5280 s / batch. (data: 2.29e-04). ETA=6:31:33, max mem: 11.4 GB 
[10/25 19:53:08 visual_prompt]: 	Training 400/553. train loss: 12.3975,	0.5040 s / batch. (data: 2.64e-04). ETA=6:12:53, max mem: 11.4 GB 
[10/25 19:54:35 visual_prompt]: 	Training 500/553. train loss: 21.1536,	0.5080 s / batch. (data: 2.62e-04). ETA=6:14:59, max mem: 11.4 GB 
[10/25 19:55:22 visual_prompt]: Epoch 20 / 100: avg data time: 3.93e-01, avg batch time: 0.8868, average train loss: 15.5465
[10/25 19:56:14 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1903, average loss: 10.0312
[10/25 19:56:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.51	
[10/25 19:56:14 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[10/25 19:57:48 visual_prompt]: 	Training 100/553. train loss: 24.5833,	0.4958 s / batch. (data: 2.77e-04). ETA=6:04:43, max mem: 11.4 GB 
[10/25 19:59:16 visual_prompt]: 	Training 200/553. train loss: 28.1524,	0.4788 s / batch. (data: 2.58e-04). ETA=5:51:24, max mem: 11.4 GB 
[10/25 20:00:44 visual_prompt]: 	Training 300/553. train loss: 158.5022,	1.5288 s / batch. (data: 1.01e+00). ETA=18:39:35, max mem: 11.4 GB 
[10/25 20:02:11 visual_prompt]: 	Training 400/553. train loss: 6.4873,	0.4920 s / batch. (data: 2.72e-04). ETA=5:59:27, max mem: 11.4 GB 
[10/25 20:03:40 visual_prompt]: 	Training 500/553. train loss: 12.9813,	0.4948 s / batch. (data: 2.59e-04). ETA=6:00:42, max mem: 11.4 GB 
[10/25 20:04:25 visual_prompt]: Epoch 21 / 100: avg data time: 3.92e-01, avg batch time: 0.8867, average train loss: 17.7782
[10/25 20:05:17 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1908, average loss: 0.9417
[10/25 20:05:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.38	
[10/25 20:05:17 visual_prompt]: Best epoch 21: best metric: -0.942
[10/25 20:05:17 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[10/25 20:06:47 visual_prompt]: 	Training 100/553. train loss: 21.0266,	0.5000 s / batch. (data: 7.97e-03). ETA=6:03:14, max mem: 11.4 GB 
[10/25 20:08:16 visual_prompt]: 	Training 200/553. train loss: 14.5018,	0.5169 s / batch. (data: 5.38e-03). ETA=6:14:39, max mem: 11.4 GB 
[10/25 20:09:43 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.4944 s / batch. (data: 2.85e-03). ETA=5:57:30, max mem: 11.4 GB 
[10/25 20:11:11 visual_prompt]: 	Training 400/553. train loss: 62.4422,	0.4940 s / batch. (data: 5.51e-03). ETA=5:56:23, max mem: 11.4 GB 
[10/25 20:12:40 visual_prompt]: 	Training 500/553. train loss: 9.4511,	0.4960 s / batch. (data: 2.88e-04). ETA=5:56:58, max mem: 11.4 GB 
[10/25 20:13:27 visual_prompt]: Epoch 22 / 100: avg data time: 3.91e-01, avg batch time: 0.8860, average train loss: 17.0790
[10/25 20:14:19 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1906, average loss: 15.7579
[10/25 20:14:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.38	
[10/25 20:14:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[10/25 20:15:51 visual_prompt]: 	Training 100/553. train loss: 44.2309,	0.5960 s / batch. (data: 1.03e-01). ETA=7:07:29, max mem: 11.4 GB 
[10/25 20:17:21 visual_prompt]: 	Training 200/553. train loss: 26.7967,	0.5000 s / batch. (data: 5.40e-03). ETA=5:57:47, max mem: 11.4 GB 
[10/25 20:18:50 visual_prompt]: 	Training 300/553. train loss: 1.7037,	0.5002 s / batch. (data: 1.16e-02). ETA=5:57:03, max mem: 11.4 GB 
[10/25 20:20:17 visual_prompt]: 	Training 400/553. train loss: 19.1338,	0.5220 s / batch. (data: 3.92e-03). ETA=6:11:45, max mem: 11.4 GB 
[10/25 20:21:44 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.4880 s / batch. (data: 2.75e-04). ETA=5:46:44, max mem: 11.4 GB 
[10/25 20:22:29 visual_prompt]: Epoch 23 / 100: avg data time: 3.92e-01, avg batch time: 0.8855, average train loss: 17.3599
[10/25 20:23:21 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1905, average loss: 14.9632
[10/25 20:23:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.81	
[10/25 20:23:21 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[10/25 20:24:50 visual_prompt]: 	Training 100/553. train loss: 48.9840,	0.5095 s / batch. (data: 2.06e-02). ETA=6:00:43, max mem: 11.4 GB 
[10/25 20:26:18 visual_prompt]: 	Training 200/553. train loss: 13.3653,	0.5230 s / batch. (data: 7.95e-03). ETA=6:09:24, max mem: 11.4 GB 
[10/25 20:27:48 visual_prompt]: 	Training 300/553. train loss: 22.4287,	1.5120 s / batch. (data: 1.01e+00). ETA=17:45:28, max mem: 11.4 GB 
[10/25 20:29:17 visual_prompt]: 	Training 400/553. train loss: 27.2356,	0.5112 s / batch. (data: 1.12e-02). ETA=5:59:24, max mem: 11.4 GB 
[10/25 20:30:48 visual_prompt]: 	Training 500/553. train loss: 7.2860,	0.8520 s / batch. (data: 3.56e-01). ETA=9:57:34, max mem: 11.4 GB 
[10/25 20:31:34 visual_prompt]: Epoch 24 / 100: avg data time: 3.95e-01, avg batch time: 0.8905, average train loss: 14.3937
[10/25 20:32:26 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1918, average loss: 3.8319
[10/25 20:32:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.96	
[10/25 20:32:26 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[10/25 20:34:02 visual_prompt]: 	Training 100/553. train loss: 25.4931,	0.5054 s / batch. (data: 1.01e-02). ETA=5:53:11, max mem: 11.4 GB 
[10/25 20:35:28 visual_prompt]: 	Training 200/553. train loss: 7.2713,	0.4916 s / batch. (data: 2.53e-04). ETA=5:42:41, max mem: 11.4 GB 
[10/25 20:36:55 visual_prompt]: 	Training 300/553. train loss: 15.8098,	0.5040 s / batch. (data: 2.58e-04). ETA=5:50:32, max mem: 11.4 GB 
[10/25 20:38:25 visual_prompt]: 	Training 400/553. train loss: 11.6793,	1.8528 s / batch. (data: 1.35e+00). ETA=21:25:28, max mem: 11.4 GB 
[10/25 20:39:55 visual_prompt]: 	Training 500/553. train loss: 0.5442,	2.0320 s / batch. (data: 1.54e+00). ETA=23:26:25, max mem: 11.4 GB 
[10/25 20:40:40 visual_prompt]: Epoch 25 / 100: avg data time: 3.98e-01, avg batch time: 0.8920, average train loss: 13.9462
[10/25 20:41:32 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1921, average loss: 5.6517
[10/25 20:41:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.51	
[10/25 20:41:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[10/25 20:43:03 visual_prompt]: 	Training 100/553. train loss: 80.3124,	0.5200 s / batch. (data: 1.19e-02). ETA=5:58:34, max mem: 11.4 GB 
[10/25 20:44:35 visual_prompt]: 	Training 200/553. train loss: 7.2835,	2.3440 s / batch. (data: 1.84e+00). ETA=1 day, 2:52:27, max mem: 11.4 GB 
[10/25 20:46:04 visual_prompt]: 	Training 300/553. train loss: 11.2962,	0.4840 s / batch. (data: 2.73e-04). ETA=5:32:08, max mem: 11.4 GB 
[10/25 20:47:32 visual_prompt]: 	Training 400/553. train loss: 31.3373,	0.5203 s / batch. (data: 8.26e-03). ETA=5:56:11, max mem: 11.4 GB 
[10/25 20:48:59 visual_prompt]: 	Training 500/553. train loss: 6.2830,	0.4884 s / batch. (data: 7.98e-03). ETA=5:33:33, max mem: 11.4 GB 
[10/25 20:49:45 visual_prompt]: Epoch 26 / 100: avg data time: 3.97e-01, avg batch time: 0.8910, average train loss: 14.6223
[10/25 20:50:38 visual_prompt]: Inference (val):avg data time: 1.24e-04, avg batch time: 0.1897, average loss: 6.3233
[10/25 20:50:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.45	
[10/25 20:50:38 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[10/25 20:52:10 visual_prompt]: 	Training 100/553. train loss: 37.5579,	0.4920 s / batch. (data: 7.96e-03). ETA=5:34:44, max mem: 11.4 GB 
[10/25 20:53:39 visual_prompt]: 	Training 200/553. train loss: 20.0499,	2.1447 s / batch. (data: 1.65e+00). ETA=1 day, 0:15:34, max mem: 11.4 GB 
[10/25 20:55:07 visual_prompt]: 	Training 300/553. train loss: 6.2096,	0.4998 s / batch. (data: 2.97e-04). ETA=5:38:24, max mem: 11.4 GB 
[10/25 20:56:38 visual_prompt]: 	Training 400/553. train loss: 0.8363,	0.4784 s / batch. (data: 2.53e-04). ETA=5:23:04, max mem: 11.4 GB 
[10/25 20:58:07 visual_prompt]: 	Training 500/553. train loss: 8.5207,	0.4782 s / batch. (data: 2.66e-04). ETA=5:22:10, max mem: 11.4 GB 
[10/25 20:58:51 visual_prompt]: Epoch 27 / 100: avg data time: 3.98e-01, avg batch time: 0.8923, average train loss: 17.9587
[10/25 20:59:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1912, average loss: 7.5026
[10/25 20:59:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.40	
[10/25 20:59:44 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[10/25 21:01:14 visual_prompt]: 	Training 100/553. train loss: 0.0453,	0.6240 s / batch. (data: 1.10e-01). ETA=6:58:49, max mem: 11.4 GB 
[10/25 21:02:44 visual_prompt]: 	Training 200/553. train loss: 12.0684,	0.5081 s / batch. (data: 2.78e-04). ETA=5:40:11, max mem: 11.4 GB 
[10/25 21:04:14 visual_prompt]: 	Training 300/553. train loss: 24.3599,	1.7676 s / batch. (data: 1.28e+00). ETA=19:40:25, max mem: 11.4 GB 
[10/25 21:05:42 visual_prompt]: 	Training 400/553. train loss: 26.8529,	0.4874 s / batch. (data: 2.61e-04). ETA=5:24:40, max mem: 11.4 GB 
[10/25 21:07:10 visual_prompt]: 	Training 500/553. train loss: 37.2582,	0.5080 s / batch. (data: 1.20e-02). ETA=5:37:33, max mem: 11.4 GB 
[10/25 21:07:56 visual_prompt]: Epoch 28 / 100: avg data time: 3.97e-01, avg batch time: 0.8907, average train loss: 16.2882
[10/25 21:08:49 visual_prompt]: Inference (val):avg data time: 6.30e-04, avg batch time: 0.1916, average loss: 3.6698
[10/25 21:08:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.90	
[10/25 21:08:49 visual_prompt]: Stopping early.
[10/25 21:08:49 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 21:08:49 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 21:08:49 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 21:08:49 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 21:08:49 visual_prompt]: Training with config:
[10/25 21:08:49 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr5.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 21:08:49 visual_prompt]: Loading training data...
[10/25 21:08:49 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 21:08:49 visual_prompt]: Loading validation data...
[10/25 21:08:49 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 21:08:49 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/25 21:08:56 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/25 21:08:56 visual_prompt]: tuned percent:0.529
[10/25 21:08:56 visual_prompt]: Device used for model: 0
[10/25 21:08:56 visual_prompt]: Setting up Evaluator...
[10/25 21:08:56 visual_prompt]: Setting up Trainer...
[10/25 21:08:56 visual_prompt]: 	Setting up the optimizer...
[10/25 21:08:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 21:10:28 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4867 s / batch. (data: 2.68e-04). ETA=7:27:47, max mem: 11.4 GB 
[10/25 21:11:54 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4782 s / batch. (data: 2.73e-04). ETA=7:19:08, max mem: 11.4 GB 
[10/25 21:13:26 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9041 s / batch. (data: 2.41e+00). ETA=1 day, 20:22:03, max mem: 11.4 GB 
[10/25 21:14:52 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4920 s / batch. (data: 2.62e-04). ETA=7:30:08, max mem: 11.4 GB 
[10/25 21:16:22 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5040 s / batch. (data: 7.98e-03). ETA=7:40:19, max mem: 11.4 GB 
[10/25 21:17:08 visual_prompt]: Epoch 1 / 100: avg data time: 3.95e-01, avg batch time: 0.8900, average train loss: 1.3966
[10/25 21:18:03 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1910, average loss: 1.3454
[10/25 21:18:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/25 21:18:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/25 21:19:51 visual_prompt]: 	Training 100/553. train loss: 2.8832,	0.4880 s / batch. (data: 2.90e-04). ETA=7:24:27, max mem: 11.4 GB 
[10/25 21:21:26 visual_prompt]: 	Training 200/553. train loss: 0.0048,	1.9197 s / batch. (data: 1.43e+00). ETA=1 day, 5:05:14, max mem: 11.4 GB 
[10/25 21:23:03 visual_prompt]: 	Training 300/553. train loss: 2.0924,	1.9599 s / batch. (data: 1.46e+00). ETA=1 day, 5:38:27, max mem: 11.4 GB 
[10/25 21:24:35 visual_prompt]: 	Training 400/553. train loss: 3.0068,	0.5280 s / batch. (data: 7.96e-03). ETA=7:58:15, max mem: 11.4 GB 
[10/25 21:26:11 visual_prompt]: 	Training 500/553. train loss: 0.7198,	0.4995 s / batch. (data: 5.40e-03). ETA=7:31:38, max mem: 11.4 GB 
[10/25 21:26:58 visual_prompt]: Epoch 2 / 100: avg data time: 4.73e-01, avg batch time: 0.9676, average train loss: 1.6880
[10/25 21:27:53 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.1914, average loss: 1.8392
[10/25 21:27:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.54	
[10/25 21:27:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/25 21:29:27 visual_prompt]: 	Training 100/553. train loss: 1.8549,	1.0231 s / batch. (data: 5.45e-01). ETA=15:22:24, max mem: 11.4 GB 
[10/25 21:31:01 visual_prompt]: 	Training 200/553. train loss: 0.6978,	0.4908 s / batch. (data: 2.93e-04). ETA=7:21:39, max mem: 11.4 GB 
[10/25 21:32:33 visual_prompt]: 	Training 300/553. train loss: 1.3957,	0.4960 s / batch. (data: 1.91e-04). ETA=7:25:32, max mem: 11.4 GB 
[10/25 21:34:07 visual_prompt]: 	Training 400/553. train loss: 2.0428,	0.5039 s / batch. (data: 7.96e-03). ETA=7:31:46, max mem: 11.4 GB 
[10/25 21:35:41 visual_prompt]: 	Training 500/553. train loss: 2.1735,	1.9759 s / batch. (data: 1.50e+00). ETA=1 day, 5:28:16, max mem: 11.4 GB 
[10/25 21:36:27 visual_prompt]: Epoch 3 / 100: avg data time: 4.36e-01, avg batch time: 0.9303, average train loss: 2.1920
[10/25 21:37:22 visual_prompt]: Inference (val):avg data time: 5.06e-04, avg batch time: 0.1921, average loss: 1.2439
[10/25 21:37:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.13	
[10/25 21:37:22 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/25 21:39:00 visual_prompt]: 	Training 100/553. train loss: 4.4006,	0.5117 s / batch. (data: 1.17e-02). ETA=7:36:38, max mem: 11.4 GB 
[10/25 21:40:34 visual_prompt]: 	Training 200/553. train loss: 4.0399,	0.5202 s / batch. (data: 2.44e-02). ETA=7:43:21, max mem: 11.4 GB 
[10/25 21:42:07 visual_prompt]: 	Training 300/553. train loss: 0.8053,	1.1267 s / batch. (data: 6.37e-01). ETA=16:41:40, max mem: 11.4 GB 
[10/25 21:43:35 visual_prompt]: 	Training 400/553. train loss: 1.6175,	0.5640 s / batch. (data: 3.95e-02). ETA=8:20:27, max mem: 11.4 GB 
[10/25 21:45:11 visual_prompt]: 	Training 500/553. train loss: 0.0024,	3.2560 s / batch. (data: 2.76e+00). ETA=2 days, 0:03:47, max mem: 11.4 GB 
[10/25 21:45:59 visual_prompt]: Epoch 4 / 100: avg data time: 4.41e-01, avg batch time: 0.9346, average train loss: 3.2892
[10/25 21:46:54 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1921, average loss: 2.4194
[10/25 21:46:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.74	
[10/25 21:46:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/25 21:48:29 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.5080 s / batch. (data: 2.53e-04). ETA=7:28:36, max mem: 11.4 GB 
[10/25 21:50:02 visual_prompt]: 	Training 200/553. train loss: 3.7656,	1.8095 s / batch. (data: 1.32e+00). ETA=1 day, 2:34:58, max mem: 11.4 GB 
[10/25 21:51:36 visual_prompt]: 	Training 300/553. train loss: 1.1641,	0.5000 s / batch. (data: 2.82e-04). ETA=7:19:53, max mem: 11.4 GB 
[10/25 21:53:08 visual_prompt]: 	Training 400/553. train loss: 12.6424,	0.5115 s / batch. (data: 7.96e-03). ETA=7:29:12, max mem: 11.4 GB 
[10/25 21:54:40 visual_prompt]: 	Training 500/553. train loss: 7.0672,	0.4996 s / batch. (data: 3.34e-03). ETA=7:17:51, max mem: 11.4 GB 
[10/25 21:55:28 visual_prompt]: Epoch 5 / 100: avg data time: 4.35e-01, avg batch time: 0.9291, average train loss: 5.5010
[10/25 21:56:23 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1911, average loss: 6.5423
[10/25 21:56:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.50	
[10/25 21:56:23 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/25 21:58:00 visual_prompt]: 	Training 100/553. train loss: 2.2749,	0.5041 s / batch. (data: 2.66e-04). ETA=7:20:30, max mem: 11.4 GB 
[10/25 21:59:32 visual_prompt]: 	Training 200/553. train loss: 4.3835,	0.5014 s / batch. (data: 9.32e-03). ETA=7:17:18, max mem: 11.4 GB 
[10/25 22:01:03 visual_prompt]: 	Training 300/553. train loss: 20.2025,	0.4921 s / batch. (data: 5.37e-03). ETA=7:08:23, max mem: 11.4 GB 
[10/25 22:02:39 visual_prompt]: 	Training 400/553. train loss: 6.5234,	0.6077 s / batch. (data: 1.06e-01). ETA=8:48:02, max mem: 11.4 GB 
[10/25 22:04:11 visual_prompt]: 	Training 500/553. train loss: 85.1704,	1.4840 s / batch. (data: 9.88e-01). ETA=21:26:59, max mem: 11.4 GB 
[10/25 22:04:58 visual_prompt]: Epoch 6 / 100: avg data time: 4.35e-01, avg batch time: 0.9304, average train loss: 7.9722
[10/25 22:05:53 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1909, average loss: 18.6243
[10/25 22:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.54	
[10/25 22:05:53 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/25 22:07:27 visual_prompt]: 	Training 100/553. train loss: 17.4502,	0.5051 s / batch. (data: 2.07e-02). ETA=7:16:44, max mem: 11.4 GB 
[10/25 22:08:59 visual_prompt]: 	Training 200/553. train loss: 13.2217,	0.4901 s / batch. (data: 1.20e-02). ETA=7:02:58, max mem: 11.4 GB 
[10/25 22:10:36 visual_prompt]: 	Training 300/553. train loss: 4.3430,	2.6761 s / batch. (data: 2.19e+00). ETA=1 day, 14:25:04, max mem: 11.4 GB 
[10/25 22:12:08 visual_prompt]: 	Training 400/553. train loss: 0.6222,	2.4841 s / batch. (data: 1.97e+00). ETA=1 day, 11:35:32, max mem: 11.4 GB 
[10/25 22:13:39 visual_prompt]: 	Training 500/553. train loss: 7.3432,	0.5360 s / batch. (data: 2.80e-04). ETA=7:39:54, max mem: 11.4 GB 
[10/25 22:14:26 visual_prompt]: Epoch 7 / 100: avg data time: 4.33e-01, avg batch time: 0.9275, average train loss: 9.2157
[10/25 22:15:21 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.1890, average loss: 2.2624
[10/25 22:15:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.78	
[10/25 22:15:21 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/25 22:16:55 visual_prompt]: 	Training 100/553. train loss: 18.8295,	0.5200 s / batch. (data: 7.97e-03). ETA=7:24:51, max mem: 11.4 GB 
[10/25 22:18:29 visual_prompt]: 	Training 200/553. train loss: 11.8682,	0.4841 s / batch. (data: 3.80e-04). ETA=6:53:19, max mem: 11.4 GB 
[10/25 22:20:02 visual_prompt]: 	Training 300/553. train loss: 30.6959,	0.4960 s / batch. (data: 7.95e-03). ETA=7:02:39, max mem: 11.4 GB 
[10/25 22:21:35 visual_prompt]: 	Training 400/553. train loss: 17.6882,	1.3409 s / batch. (data: 8.62e-01). ETA=19:00:22, max mem: 11.4 GB 
[10/25 22:23:09 visual_prompt]: 	Training 500/553. train loss: 53.0758,	2.1520 s / batch. (data: 1.64e+00). ETA=1 day, 6:26:39, max mem: 11.4 GB 
[10/25 22:23:55 visual_prompt]: Epoch 8 / 100: avg data time: 4.36e-01, avg batch time: 0.9303, average train loss: 11.3350
[10/25 22:24:50 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1908, average loss: 2.0924
[10/25 22:24:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.65	
[10/25 22:24:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/25 22:26:27 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4960 s / batch. (data: 2.81e-04). ETA=6:59:44, max mem: 11.4 GB 
[10/25 22:27:58 visual_prompt]: 	Training 200/553. train loss: 4.8309,	0.5045 s / batch. (data: 8.41e-03). ETA=7:06:03, max mem: 11.4 GB 
[10/25 22:29:30 visual_prompt]: 	Training 300/553. train loss: 5.7234,	1.0360 s / batch. (data: 5.37e-01). ETA=14:33:15, max mem: 11.4 GB 
[10/25 22:31:05 visual_prompt]: 	Training 400/553. train loss: 2.0224,	0.4997 s / batch. (data: 2.84e-04). ETA=7:00:25, max mem: 11.4 GB 
[10/25 22:32:38 visual_prompt]: 	Training 500/553. train loss: 4.1961,	1.5115 s / batch. (data: 1.02e+00). ETA=21:09:05, max mem: 11.4 GB 
[10/25 22:33:24 visual_prompt]: Epoch 9 / 100: avg data time: 4.34e-01, avg batch time: 0.9289, average train loss: 12.8463
[10/25 22:34:18 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1906, average loss: 5.6499
[10/25 22:34:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.41	
[10/25 22:34:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/25 22:35:57 visual_prompt]: 	Training 100/553. train loss: 23.3973,	0.5120 s / batch. (data: 2.68e-04). ETA=7:08:33, max mem: 11.4 GB 
[10/25 22:37:28 visual_prompt]: 	Training 200/553. train loss: 22.1949,	0.5043 s / batch. (data: 5.41e-03). ETA=7:01:18, max mem: 11.4 GB 
[10/25 22:39:00 visual_prompt]: 	Training 300/553. train loss: 10.3038,	1.3645 s / batch. (data: 8.62e-01). ETA=18:57:34, max mem: 11.4 GB 
[10/25 22:40:31 visual_prompt]: 	Training 400/553. train loss: 9.9133,	1.4042 s / batch. (data: 9.08e-01). ETA=19:28:21, max mem: 11.4 GB 
[10/25 22:42:04 visual_prompt]: 	Training 500/553. train loss: 11.0603,	0.6207 s / batch. (data: 1.37e-01). ETA=8:35:26, max mem: 11.4 GB 
[10/25 22:42:55 visual_prompt]: Epoch 10 / 100: avg data time: 4.40e-01, avg batch time: 0.9342, average train loss: 15.7784
[10/25 22:43:57 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1895, average loss: 9.9048
[10/25 22:43:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.19	
[10/25 22:43:57 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/25 22:45:41 visual_prompt]: 	Training 100/553. train loss: 15.2104,	0.5112 s / batch. (data: 1.59e-02). ETA=7:03:11, max mem: 11.4 GB 
[10/25 22:47:27 visual_prompt]: 	Training 200/553. train loss: 35.8780,	0.5320 s / batch. (data: 5.39e-03). ETA=7:19:30, max mem: 11.4 GB 
[10/25 22:48:57 visual_prompt]: 	Training 300/553. train loss: 20.8097,	0.5045 s / batch. (data: 6.58e-03). ETA=6:55:58, max mem: 11.4 GB 
[10/25 22:50:29 visual_prompt]: 	Training 400/553. train loss: 21.2620,	0.4880 s / batch. (data: 2.78e-04). ETA=6:41:33, max mem: 11.4 GB 
[10/25 22:51:58 visual_prompt]: 	Training 500/553. train loss: 8.3979,	0.4878 s / batch. (data: 2.45e-04). ETA=6:40:33, max mem: 11.4 GB 
[10/25 22:52:43 visual_prompt]: Epoch 11 / 100: avg data time: 4.59e-01, avg batch time: 0.9518, average train loss: 17.4600
[10/25 22:53:36 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1927, average loss: 10.8968
[10/25 22:53:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.55	
[10/25 22:53:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/25 22:55:09 visual_prompt]: 	Training 100/553. train loss: 1.4686,	0.4954 s / batch. (data: 5.39e-03). ETA=6:45:34, max mem: 11.4 GB 
[10/25 22:56:39 visual_prompt]: 	Training 200/553. train loss: 5.2975,	0.5012 s / batch. (data: 2.90e-04). ETA=6:49:27, max mem: 11.4 GB 
[10/25 22:58:06 visual_prompt]: 	Training 300/553. train loss: 18.4039,	0.4880 s / batch. (data: 2.66e-04). ETA=6:37:51, max mem: 11.4 GB 
[10/25 22:59:35 visual_prompt]: 	Training 400/553. train loss: 16.0352,	0.5096 s / batch. (data: 5.40e-03). ETA=6:54:36, max mem: 11.4 GB 
[10/25 23:01:05 visual_prompt]: 	Training 500/553. train loss: 10.1715,	0.5174 s / batch. (data: 7.02e-04). ETA=7:00:04, max mem: 11.4 GB 
[10/25 23:01:49 visual_prompt]: Epoch 12 / 100: avg data time: 3.97e-01, avg batch time: 0.8918, average train loss: 14.9352
[10/25 23:02:41 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1910, average loss: 9.3265
[10/25 23:02:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.10	
[10/25 23:02:41 visual_prompt]: Best epoch 12: best metric: -9.327
[10/25 23:02:41 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/25 23:04:14 visual_prompt]: 	Training 100/553. train loss: 12.0398,	0.7520 s / batch. (data: 2.57e-01). ETA=10:08:39, max mem: 11.4 GB 
[10/25 23:05:41 visual_prompt]: 	Training 200/553. train loss: 11.4044,	0.4920 s / batch. (data: 3.08e-04). ETA=6:37:22, max mem: 11.4 GB 
[10/25 23:07:11 visual_prompt]: 	Training 300/553. train loss: 2.6357,	2.3156 s / batch. (data: 1.83e+00). ETA=1 day, 7:06:33, max mem: 11.4 GB 
[10/25 23:08:38 visual_prompt]: 	Training 400/553. train loss: 5.9368,	0.5000 s / batch. (data: 2.92e-04). ETA=6:42:13, max mem: 11.4 GB 
[10/25 23:10:07 visual_prompt]: 	Training 500/553. train loss: 8.0586,	0.5078 s / batch. (data: 3.06e-04). ETA=6:47:38, max mem: 11.4 GB 
[10/25 23:10:53 visual_prompt]: Epoch 13 / 100: avg data time: 3.95e-01, avg batch time: 0.8892, average train loss: 16.7078
[10/25 23:11:46 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1905, average loss: 8.3612
[10/25 23:11:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.24	
[10/25 23:11:46 visual_prompt]: Best epoch 13: best metric: -8.361
[10/25 23:11:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/25 23:13:18 visual_prompt]: 	Training 100/553. train loss: 19.2734,	0.4908 s / batch. (data: 2.61e-04). ETA=6:32:41, max mem: 11.4 GB 
[10/25 23:14:47 visual_prompt]: 	Training 200/553. train loss: 1.7777,	1.6402 s / batch. (data: 1.16e+00). ETA=21:49:44, max mem: 11.4 GB 
[10/25 23:16:16 visual_prompt]: 	Training 300/553. train loss: 5.1446,	1.3634 s / batch. (data: 8.75e-01). ETA=18:06:25, max mem: 11.4 GB 
[10/25 23:17:44 visual_prompt]: 	Training 400/553. train loss: 7.3059,	1.1880 s / batch. (data: 7.02e-01). ETA=15:44:39, max mem: 11.4 GB 
[10/25 23:19:13 visual_prompt]: 	Training 500/553. train loss: 5.6333,	0.5080 s / batch. (data: 2.54e-04). ETA=6:43:06, max mem: 11.4 GB 
[10/25 23:19:57 visual_prompt]: Epoch 14 / 100: avg data time: 3.95e-01, avg batch time: 0.8885, average train loss: 13.2286
[10/25 23:20:50 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1917, average loss: 7.1881
[10/25 23:20:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.38	
[10/25 23:20:50 visual_prompt]: Best epoch 14: best metric: -7.188
[10/25 23:20:50 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/25 23:22:21 visual_prompt]: 	Training 100/553. train loss: 10.4136,	0.4873 s / batch. (data: 2.79e-04). ETA=6:25:25, max mem: 11.4 GB 
[10/25 23:23:49 visual_prompt]: 	Training 200/553. train loss: 45.0512,	0.5079 s / batch. (data: 7.80e-03). ETA=6:40:53, max mem: 11.4 GB 
[10/25 23:25:19 visual_prompt]: 	Training 300/553. train loss: 67.9314,	0.4826 s / batch. (data: 5.45e-03). ETA=6:20:08, max mem: 11.4 GB 
[10/25 23:26:46 visual_prompt]: 	Training 400/553. train loss: 23.7113,	0.5005 s / batch. (data: 2.47e-04). ETA=6:33:20, max mem: 11.4 GB 
[10/25 23:28:16 visual_prompt]: 	Training 500/553. train loss: 2.3329,	0.5163 s / batch. (data: 8.28e-03). ETA=6:44:56, max mem: 11.4 GB 
[10/25 23:29:02 visual_prompt]: Epoch 15 / 100: avg data time: 3.95e-01, avg batch time: 0.8910, average train loss: 14.3820
[10/25 23:29:55 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1913, average loss: 12.2620
[10/25 23:29:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.35	
[10/25 23:29:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/25 23:31:27 visual_prompt]: 	Training 100/553. train loss: 11.2105,	0.4960 s / batch. (data: 2.64e-04). ETA=6:27:43, max mem: 11.4 GB 
[10/25 23:32:54 visual_prompt]: 	Training 200/553. train loss: 8.5499,	0.5120 s / batch. (data: 2.45e-04). ETA=6:39:25, max mem: 11.4 GB 
[10/25 23:34:25 visual_prompt]: 	Training 300/553. train loss: 1.4251,	0.5120 s / batch. (data: 5.39e-03). ETA=6:38:32, max mem: 11.4 GB 
[10/25 23:35:53 visual_prompt]: 	Training 400/553. train loss: 0.8427,	0.5240 s / batch. (data: 1.20e-02). ETA=6:47:01, max mem: 11.4 GB 
[10/25 23:37:22 visual_prompt]: 	Training 500/553. train loss: 18.1223,	2.0443 s / batch. (data: 1.55e+00). ETA=1 day, 2:24:28, max mem: 11.4 GB 
[10/25 23:38:08 visual_prompt]: Epoch 16 / 100: avg data time: 3.98e-01, avg batch time: 0.8924, average train loss: 15.3711
[10/25 23:39:01 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1932, average loss: 10.0383
[10/25 23:39:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.50	
[10/25 23:39:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/25 23:40:33 visual_prompt]: 	Training 100/553. train loss: 11.0862,	0.4951 s / batch. (data: 2.52e-04). ETA=6:22:27, max mem: 11.4 GB 
[10/25 23:42:04 visual_prompt]: 	Training 200/553. train loss: 53.0533,	0.4840 s / batch. (data: 2.71e-04). ETA=6:13:05, max mem: 11.4 GB 
[10/25 23:43:32 visual_prompt]: 	Training 300/553. train loss: 30.9030,	0.5044 s / batch. (data: 8.33e-03). ETA=6:27:57, max mem: 11.4 GB 
[10/25 23:45:00 visual_prompt]: 	Training 400/553. train loss: 8.6213,	0.5055 s / batch. (data: 3.15e-04). ETA=6:28:01, max mem: 11.4 GB 
[10/25 23:46:29 visual_prompt]: 	Training 500/553. train loss: 3.6518,	2.4520 s / batch. (data: 1.94e+00). ETA=1 day, 7:17:52, max mem: 11.4 GB 
[10/25 23:47:16 visual_prompt]: Epoch 17 / 100: avg data time: 3.99e-01, avg batch time: 0.8941, average train loss: 13.0867
[10/25 23:48:08 visual_prompt]: Inference (val):avg data time: 1.25e-04, avg batch time: 0.1933, average loss: 3.6659
[10/25 23:48:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.98	
[10/25 23:48:08 visual_prompt]: Best epoch 17: best metric: -3.666
[10/25 23:48:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/25 23:49:41 visual_prompt]: 	Training 100/553. train loss: 17.0460,	0.4961 s / batch. (data: 1.17e-02). ETA=6:18:42, max mem: 11.4 GB 
[10/25 23:51:13 visual_prompt]: 	Training 200/553. train loss: 0.6990,	0.5005 s / batch. (data: 7.03e-04). ETA=6:21:13, max mem: 11.4 GB 
[10/25 23:52:42 visual_prompt]: 	Training 300/553. train loss: 9.8933,	0.4873 s / batch. (data: 2.67e-04). ETA=6:10:18, max mem: 11.4 GB 
[10/25 23:54:10 visual_prompt]: 	Training 400/553. train loss: 39.9172,	0.4912 s / batch. (data: 5.63e-03). ETA=6:12:28, max mem: 11.4 GB 
[10/25 23:55:38 visual_prompt]: 	Training 500/553. train loss: 16.4903,	0.5000 s / batch. (data: 2.71e-04). ETA=6:18:19, max mem: 11.4 GB 
[10/25 23:56:22 visual_prompt]: Epoch 18 / 100: avg data time: 3.98e-01, avg batch time: 0.8927, average train loss: 17.2017
[10/25 23:57:14 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1909, average loss: 27.6642
[10/25 23:57:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.64	
[10/25 23:57:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/25 23:58:45 visual_prompt]: 	Training 100/553. train loss: 11.3465,	0.9332 s / batch. (data: 4.31e-01). ETA=11:43:45, max mem: 11.4 GB 
[10/26 00:00:15 visual_prompt]: 	Training 200/553. train loss: 8.3811,	0.5000 s / batch. (data: 7.96e-03). ETA=6:16:12, max mem: 11.4 GB 
[10/26 00:01:44 visual_prompt]: 	Training 300/553. train loss: 0.0181,	0.4960 s / batch. (data: 2.98e-04). ETA=6:12:23, max mem: 11.4 GB 
[10/26 00:03:14 visual_prompt]: 	Training 400/553. train loss: 3.1088,	0.4894 s / batch. (data: 3.18e-04). ETA=6:06:37, max mem: 11.4 GB 
[10/26 00:04:39 visual_prompt]: 	Training 500/553. train loss: 38.9935,	0.4960 s / batch. (data: 2.71e-04). ETA=6:10:43, max mem: 11.4 GB 
[10/26 00:05:25 visual_prompt]: Epoch 19 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 17.1019
[10/26 00:06:18 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1904, average loss: 4.8427
[10/26 00:06:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.02	
[10/26 00:06:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[10/26 00:07:47 visual_prompt]: 	Training 100/553. train loss: 2.7637,	1.0033 s / batch. (data: 5.20e-01). ETA=12:27:19, max mem: 11.4 GB 
[10/26 00:09:18 visual_prompt]: 	Training 200/553. train loss: 5.5304,	0.4941 s / batch. (data: 5.40e-03). ETA=6:07:11, max mem: 11.4 GB 
[10/26 00:10:46 visual_prompt]: 	Training 300/553. train loss: 2.6034,	0.5160 s / batch. (data: 2.87e-04). ETA=6:22:37, max mem: 11.4 GB 
[10/26 00:12:15 visual_prompt]: 	Training 400/553. train loss: 14.2732,	0.4999 s / batch. (data: 4.73e-04). ETA=6:09:52, max mem: 11.4 GB 
[10/26 00:13:43 visual_prompt]: 	Training 500/553. train loss: 15.5653,	0.4854 s / batch. (data: 3.05e-04). ETA=5:58:19, max mem: 11.4 GB 
[10/26 00:14:30 visual_prompt]: Epoch 20 / 100: avg data time: 3.95e-01, avg batch time: 0.8896, average train loss: 13.5989
[10/26 00:15:22 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1908, average loss: 23.1206
[10/26 00:15:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.06	
[10/26 00:15:22 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[10/26 00:16:56 visual_prompt]: 	Training 100/553. train loss: 12.7444,	1.2405 s / batch. (data: 7.65e-01). ETA=15:12:33, max mem: 11.4 GB 
[10/26 00:18:24 visual_prompt]: 	Training 200/553. train loss: 19.3358,	0.5325 s / batch. (data: 3.91e-04). ETA=6:30:50, max mem: 11.4 GB 
[10/26 00:19:52 visual_prompt]: 	Training 300/553. train loss: 91.2992,	0.9160 s / batch. (data: 4.23e-01). ETA=11:10:47, max mem: 11.4 GB 
[10/26 00:21:20 visual_prompt]: 	Training 400/553. train loss: 24.2306,	0.4885 s / batch. (data: 2.59e-04). ETA=5:56:53, max mem: 11.4 GB 
[10/26 00:22:51 visual_prompt]: 	Training 500/553. train loss: 4.5272,	0.5161 s / batch. (data: 2.72e-04). ETA=6:16:15, max mem: 11.4 GB 
[10/26 00:23:35 visual_prompt]: Epoch 21 / 100: avg data time: 3.96e-01, avg batch time: 0.8908, average train loss: 18.6678
[10/26 00:24:27 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1904, average loss: 5.4679
[10/26 00:24:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.59	
[10/26 00:24:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[10/26 00:25:58 visual_prompt]: 	Training 100/553. train loss: 10.1515,	0.5310 s / batch. (data: 6.96e-03). ETA=6:25:44, max mem: 11.4 GB 
[10/26 00:27:27 visual_prompt]: 	Training 200/553. train loss: 2.7602,	0.5052 s / batch. (data: 1.05e-02). ETA=6:06:11, max mem: 11.4 GB 
[10/26 00:28:54 visual_prompt]: 	Training 300/553. train loss: 3.2788,	0.4920 s / batch. (data: 2.43e-04). ETA=5:55:45, max mem: 11.4 GB 
[10/26 00:30:23 visual_prompt]: 	Training 400/553. train loss: 57.5682,	0.4795 s / batch. (data: 2.62e-04). ETA=5:45:55, max mem: 11.4 GB 
[10/26 00:31:51 visual_prompt]: 	Training 500/553. train loss: 1.7176,	0.4950 s / batch. (data: 2.63e-04). ETA=5:56:17, max mem: 11.4 GB 
[10/26 00:32:39 visual_prompt]: Epoch 22 / 100: avg data time: 3.95e-01, avg batch time: 0.8890, average train loss: 13.6632
[10/26 00:33:31 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.1890, average loss: 12.2642
[10/26 00:33:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.27	
[10/26 00:33:31 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[10/26 00:35:05 visual_prompt]: 	Training 100/553. train loss: 17.7775,	0.5004 s / batch. (data: 1.19e-02). ETA=5:58:54, max mem: 11.4 GB 
[10/26 00:36:34 visual_prompt]: 	Training 200/553. train loss: 3.2660,	1.3026 s / batch. (data: 8.18e-01). ETA=15:32:06, max mem: 11.4 GB 
[10/26 00:38:04 visual_prompt]: 	Training 300/553. train loss: 30.8530,	0.4903 s / batch. (data: 2.56e-04). ETA=5:50:01, max mem: 11.4 GB 
[10/26 00:39:31 visual_prompt]: 	Training 400/553. train loss: 6.2153,	0.4920 s / batch. (data: 2.72e-04). ETA=5:50:25, max mem: 11.4 GB 
[10/26 00:40:58 visual_prompt]: 	Training 500/553. train loss: 7.6673,	0.4922 s / batch. (data: 2.83e-04). ETA=5:49:46, max mem: 11.4 GB 
[10/26 00:41:43 visual_prompt]: Epoch 23 / 100: avg data time: 3.95e-01, avg batch time: 0.8896, average train loss: 13.6134
[10/26 00:42:36 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1912, average loss: 12.1593
[10/26 00:42:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.12	
[10/26 00:42:36 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[10/26 00:44:05 visual_prompt]: 	Training 100/553. train loss: 27.6227,	0.4921 s / batch. (data: 2.88e-04). ETA=5:48:24, max mem: 11.4 GB 
[10/26 00:45:34 visual_prompt]: 	Training 200/553. train loss: 11.7779,	0.4842 s / batch. (data: 2.65e-04). ETA=5:42:00, max mem: 11.4 GB 
[10/26 00:47:03 visual_prompt]: 	Training 300/553. train loss: 9.0226,	1.4265 s / batch. (data: 9.35e-01). ETA=16:45:15, max mem: 11.4 GB 
[10/26 00:48:33 visual_prompt]: 	Training 400/553. train loss: 48.1988,	0.4909 s / batch. (data: 2.68e-04). ETA=5:45:07, max mem: 11.4 GB 
[10/26 00:50:02 visual_prompt]: 	Training 500/553. train loss: 3.3452,	0.8837 s / batch. (data: 3.61e-01). ETA=10:19:47, max mem: 11.4 GB 
[10/26 00:50:48 visual_prompt]: Epoch 24 / 100: avg data time: 3.96e-01, avg batch time: 0.8904, average train loss: 18.2163
[10/26 00:51:41 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.1906, average loss: 4.6389
[10/26 00:51:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.76	
[10/26 00:51:41 visual_prompt]: Stopping early.
[10/26 00:51:41 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 00:51:41 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 00:51:41 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 00:51:41 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 00:51:41 visual_prompt]: Training with config:
[10/26 00:51:41 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr5.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 00:51:41 visual_prompt]: Loading training data...
[10/26 00:51:41 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 00:51:41 visual_prompt]: Loading validation data...
[10/26 00:51:41 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 00:51:41 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/26 00:51:48 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/26 00:51:48 visual_prompt]: tuned percent:0.529
[10/26 00:51:48 visual_prompt]: Device used for model: 0
[10/26 00:51:48 visual_prompt]: Setting up Evaluator...
[10/26 00:51:48 visual_prompt]: Setting up Trainer...
[10/26 00:51:48 visual_prompt]: 	Setting up the optimizer...
[10/26 00:51:48 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 00:53:19 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4911 s / batch. (data: 2.61e-04). ETA=7:31:47, max mem: 11.4 GB 
[10/26 00:54:46 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5010 s / batch. (data: 5.40e-03). ETA=7:40:04, max mem: 11.4 GB 
[10/26 00:56:18 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9991 s / batch. (data: 2.50e+00). ETA=1 day, 21:49:10, max mem: 11.4 GB 
[10/26 00:57:44 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5045 s / batch. (data: 1.05e-02). ETA=7:41:36, max mem: 11.4 GB 
[10/26 00:59:15 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5309 s / batch. (data: 1.09e-02). ETA=8:04:53, max mem: 11.4 GB 
[10/26 01:00:01 visual_prompt]: Epoch 1 / 100: avg data time: 3.96e-01, avg batch time: 0.8919, average train loss: 1.3966
[10/26 01:00:53 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1911, average loss: 1.3454
[10/26 01:00:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/26 01:00:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/26 01:02:25 visual_prompt]: 	Training 100/553. train loss: 0.7287,	0.4986 s / batch. (data: 2.89e-04). ETA=7:34:09, max mem: 11.4 GB 
[10/26 01:03:53 visual_prompt]: 	Training 200/553. train loss: 0.0029,	0.6762 s / batch. (data: 1.90e-01). ETA=10:14:42, max mem: 11.4 GB 
[10/26 01:05:23 visual_prompt]: 	Training 300/553. train loss: 2.8828,	0.9439 s / batch. (data: 4.65e-01). ETA=14:16:33, max mem: 11.4 GB 
[10/26 01:06:51 visual_prompt]: 	Training 400/553. train loss: 0.6478,	0.5247 s / batch. (data: 2.08e-02). ETA=7:55:17, max mem: 11.4 GB 
[10/26 01:08:20 visual_prompt]: 	Training 500/553. train loss: 0.4853,	0.4789 s / batch. (data: 2.78e-04). ETA=7:12:58, max mem: 11.4 GB 
[10/26 01:09:06 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 0.8902, average train loss: 2.1185
[10/26 01:09:58 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1903, average loss: 6.1366
[10/26 01:09:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.25	
[10/26 01:09:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/26 01:11:28 visual_prompt]: 	Training 100/553. train loss: 7.4872,	1.0720 s / batch. (data: 5.62e-01). ETA=16:06:29, max mem: 11.4 GB 
[10/26 01:12:59 visual_prompt]: 	Training 200/553. train loss: 0.8288,	0.5004 s / batch. (data: 1.05e-02). ETA=7:30:19, max mem: 11.4 GB 
[10/26 01:14:25 visual_prompt]: 	Training 300/553. train loss: 3.7283,	0.4920 s / batch. (data: 3.36e-04). ETA=7:21:53, max mem: 11.4 GB 
[10/26 01:15:55 visual_prompt]: 	Training 400/553. train loss: 1.3106,	0.5010 s / batch. (data: 3.45e-04). ETA=7:29:09, max mem: 11.4 GB 
[10/26 01:17:26 visual_prompt]: 	Training 500/553. train loss: 1.4593,	1.8198 s / batch. (data: 1.34e+00). ETA=1 day, 3:08:30, max mem: 11.4 GB 
[10/26 01:18:10 visual_prompt]: Epoch 3 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 2.3632
[10/26 01:19:02 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1902, average loss: 3.7433
[10/26 01:19:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.82	
[10/26 01:19:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/26 01:20:36 visual_prompt]: 	Training 100/553. train loss: 2.9718,	0.4914 s / batch. (data: 2.50e-04). ETA=7:18:30, max mem: 11.4 GB 
[10/26 01:22:04 visual_prompt]: 	Training 200/553. train loss: 0.9883,	0.4920 s / batch. (data: 2.78e-04). ETA=7:18:12, max mem: 11.4 GB 
[10/26 01:23:34 visual_prompt]: 	Training 300/553. train loss: 1.2225,	2.0244 s / batch. (data: 1.53e+00). ETA=1 day, 5:59:45, max mem: 11.4 GB 
[10/26 01:24:59 visual_prompt]: 	Training 400/553. train loss: 0.7704,	1.9425 s / batch. (data: 1.46e+00). ETA=1 day, 4:43:42, max mem: 11.4 GB 
[10/26 01:26:29 visual_prompt]: 	Training 500/553. train loss: 4.2522,	3.1840 s / batch. (data: 2.69e+00). ETA=1 day, 23:00:00, max mem: 11.4 GB 
[10/26 01:27:15 visual_prompt]: Epoch 4 / 100: avg data time: 3.97e-01, avg batch time: 0.8909, average train loss: 2.4774
[10/26 01:28:07 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1904, average loss: 3.7360
[10/26 01:28:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.47	
[10/26 01:28:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/26 01:29:38 visual_prompt]: 	Training 100/553. train loss: 26.8719,	0.4911 s / batch. (data: 2.62e-04). ETA=7:13:44, max mem: 11.4 GB 
[10/26 01:31:07 visual_prompt]: 	Training 200/553. train loss: 2.2505,	1.6680 s / batch. (data: 1.18e+00). ETA=1 day, 0:30:18, max mem: 11.4 GB 
[10/26 01:32:37 visual_prompt]: 	Training 300/553. train loss: 1.6237,	0.4840 s / batch. (data: 2.56e-04). ETA=7:05:48, max mem: 11.4 GB 
[10/26 01:34:05 visual_prompt]: 	Training 400/553. train loss: 8.9198,	0.4999 s / batch. (data: 5.45e-03). ETA=7:18:58, max mem: 11.4 GB 
[10/26 01:35:34 visual_prompt]: 	Training 500/553. train loss: 3.7993,	0.5287 s / batch. (data: 2.07e-02). ETA=7:43:25, max mem: 11.4 GB 
[10/26 01:36:20 visual_prompt]: Epoch 5 / 100: avg data time: 3.95e-01, avg batch time: 0.8908, average train loss: 3.4754
[10/26 01:37:13 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1913, average loss: 0.7998
[10/26 01:37:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.00	
[10/26 01:37:13 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/26 01:38:46 visual_prompt]: 	Training 100/553. train loss: 3.3289,	0.5048 s / batch. (data: 2.39e-04). ETA=7:21:11, max mem: 11.4 GB 
[10/26 01:40:14 visual_prompt]: 	Training 200/553. train loss: 16.2253,	0.4999 s / batch. (data: 7.90e-03). ETA=7:16:03, max mem: 11.4 GB 
[10/26 01:41:41 visual_prompt]: 	Training 300/553. train loss: 6.0368,	0.4789 s / batch. (data: 2.79e-04). ETA=6:56:55, max mem: 11.4 GB 
[10/26 01:43:13 visual_prompt]: 	Training 400/553. train loss: 2.8529,	0.5115 s / batch. (data: 1.14e-02). ETA=7:24:25, max mem: 11.4 GB 
[10/26 01:44:41 visual_prompt]: 	Training 500/553. train loss: 10.0016,	1.2900 s / batch. (data: 8.13e-01). ETA=18:38:47, max mem: 11.4 GB 
[10/26 01:45:25 visual_prompt]: Epoch 6 / 100: avg data time: 3.96e-01, avg batch time: 0.8912, average train loss: 5.9108
[10/26 01:46:18 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1883, average loss: 8.2367
[10/26 01:46:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.86	
[10/26 01:46:18 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/26 01:47:49 visual_prompt]: 	Training 100/553. train loss: 9.5317,	0.5120 s / batch. (data: 7.44e-04). ETA=7:22:43, max mem: 11.4 GB 
[10/26 01:49:18 visual_prompt]: 	Training 200/553. train loss: 0.6301,	0.4957 s / batch. (data: 2.73e-04). ETA=7:07:49, max mem: 11.4 GB 
[10/26 01:50:52 visual_prompt]: 	Training 300/553. train loss: 4.4946,	2.8398 s / batch. (data: 2.36e+00). ETA=1 day, 16:46:06, max mem: 11.4 GB 
[10/26 01:52:20 visual_prompt]: 	Training 400/553. train loss: 6.7144,	2.3154 s / batch. (data: 1.81e+00). ETA=1 day, 9:10:33, max mem: 11.4 GB 
[10/26 01:53:46 visual_prompt]: 	Training 500/553. train loss: 12.4632,	0.4914 s / batch. (data: 2.73e-04). ETA=7:01:36, max mem: 11.4 GB 
[10/26 01:54:30 visual_prompt]: Epoch 7 / 100: avg data time: 3.95e-01, avg batch time: 0.8902, average train loss: 5.8479
[10/26 01:55:23 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1918, average loss: 5.4253
[10/26 01:55:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.63	
[10/26 01:55:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/26 01:56:53 visual_prompt]: 	Training 100/553. train loss: 7.1637,	0.5079 s / batch. (data: 7.96e-03). ETA=7:14:31, max mem: 11.4 GB 
[10/26 01:58:23 visual_prompt]: 	Training 200/553. train loss: 0.9091,	0.5070 s / batch. (data: 7.97e-03). ETA=7:12:50, max mem: 11.4 GB 
[10/26 01:59:51 visual_prompt]: 	Training 300/553. train loss: 6.6727,	0.5028 s / batch. (data: 9.69e-03). ETA=7:08:25, max mem: 11.4 GB 
[10/26 02:01:21 visual_prompt]: 	Training 400/553. train loss: 4.6011,	1.0804 s / batch. (data: 5.79e-01). ETA=15:18:50, max mem: 11.4 GB 
[10/26 02:02:50 visual_prompt]: 	Training 500/553. train loss: 40.1586,	2.0761 s / batch. (data: 1.57e+00). ETA=1 day, 5:22:12, max mem: 11.4 GB 
[10/26 02:03:35 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8898, average train loss: 10.9680
[10/26 02:04:27 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1908, average loss: 8.0847
[10/26 02:04:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.37	
[10/26 02:04:27 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/26 02:05:59 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4840 s / batch. (data: 2.37e-04). ETA=6:49:36, max mem: 11.4 GB 
[10/26 02:07:27 visual_prompt]: 	Training 200/553. train loss: 5.3104,	0.4995 s / batch. (data: 2.26e-04). ETA=7:01:51, max mem: 11.4 GB 
[10/26 02:08:56 visual_prompt]: 	Training 300/553. train loss: 1.4660,	2.3200 s / batch. (data: 1.83e+00). ETA=1 day, 8:35:35, max mem: 11.4 GB 
[10/26 02:10:25 visual_prompt]: 	Training 400/553. train loss: 14.9535,	0.4951 s / batch. (data: 7.98e-03). ETA=6:56:30, max mem: 11.4 GB 
[10/26 02:11:55 visual_prompt]: 	Training 500/553. train loss: 2.3561,	1.4998 s / batch. (data: 1.02e+00). ETA=20:59:14, max mem: 11.4 GB 
[10/26 02:12:39 visual_prompt]: Epoch 9 / 100: avg data time: 3.95e-01, avg batch time: 0.8893, average train loss: 9.5286
[10/26 02:13:32 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1912, average loss: 11.7752
[10/26 02:13:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.14	
[10/26 02:13:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/26 02:15:05 visual_prompt]: 	Training 100/553. train loss: 3.6801,	0.5101 s / batch. (data: 3.25e-04). ETA=7:06:58, max mem: 11.4 GB 
[10/26 02:16:32 visual_prompt]: 	Training 200/553. train loss: 16.6762,	0.4920 s / batch. (data: 2.81e-04). ETA=6:51:00, max mem: 11.4 GB 
[10/26 02:18:02 visual_prompt]: 	Training 300/553. train loss: 1.6767,	1.8607 s / batch. (data: 1.36e+00). ETA=1 day, 1:51:19, max mem: 11.4 GB 
[10/26 02:19:28 visual_prompt]: 	Training 400/553. train loss: 4.6693,	1.3973 s / batch. (data: 9.04e-01). ETA=19:22:35, max mem: 11.4 GB 
[10/26 02:20:58 visual_prompt]: 	Training 500/553. train loss: 1.6154,	1.4680 s / batch. (data: 9.70e-01). ETA=20:18:58, max mem: 11.4 GB 
[10/26 02:21:43 visual_prompt]: Epoch 10 / 100: avg data time: 3.93e-01, avg batch time: 0.8886, average train loss: 9.3432
[10/26 02:22:35 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1895, average loss: 5.4945
[10/26 02:22:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.83	
[10/26 02:22:35 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/26 02:24:09 visual_prompt]: 	Training 100/553. train loss: 10.0064,	0.5000 s / batch. (data: 1.20e-02). ETA=6:53:54, max mem: 11.4 GB 
[10/26 02:25:39 visual_prompt]: 	Training 200/553. train loss: 18.7238,	0.4805 s / batch. (data: 2.64e-04). ETA=6:36:59, max mem: 11.4 GB 
[10/26 02:27:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1678 s / batch. (data: 1.68e+00). ETA=1 day, 5:47:19, max mem: 11.4 GB 
[10/26 02:28:35 visual_prompt]: 	Training 400/553. train loss: 1.6231,	0.5320 s / batch. (data: 7.09e-04). ETA=7:17:45, max mem: 11.4 GB 
[10/26 02:30:02 visual_prompt]: 	Training 500/553. train loss: 9.7026,	0.4960 s / batch. (data: 2.76e-04). ETA=6:47:17, max mem: 11.4 GB 
[10/26 02:30:47 visual_prompt]: Epoch 11 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 10.0758
[10/26 02:31:40 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1908, average loss: 1.1789
[10/26 02:31:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 39.85	
[10/26 02:31:40 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/26 02:33:13 visual_prompt]: 	Training 100/553. train loss: 7.8945,	0.4880 s / batch. (data: 2.49e-04). ETA=6:39:28, max mem: 11.4 GB 
[10/26 02:34:42 visual_prompt]: 	Training 200/553. train loss: 1.6289,	0.4923 s / batch. (data: 2.51e-04). ETA=6:42:10, max mem: 11.4 GB 
[10/26 02:36:10 visual_prompt]: 	Training 300/553. train loss: 10.4888,	0.5105 s / batch. (data: 5.38e-03). ETA=6:56:13, max mem: 11.4 GB 
[10/26 02:37:39 visual_prompt]: 	Training 400/553. train loss: 6.8556,	0.5048 s / batch. (data: 2.77e-04). ETA=6:50:41, max mem: 11.4 GB 
[10/26 02:39:07 visual_prompt]: 	Training 500/553. train loss: 2.2146,	0.4932 s / batch. (data: 2.61e-04). ETA=6:40:25, max mem: 11.4 GB 
[10/26 02:39:51 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8893, average train loss: 8.6512
[10/26 02:40:44 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1906, average loss: 19.4005
[10/26 02:40:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.74	
[10/26 02:40:44 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/26 02:42:17 visual_prompt]: 	Training 100/553. train loss: 6.2042,	0.5173 s / batch. (data: 8.92e-03). ETA=6:58:44, max mem: 11.4 GB 
[10/26 02:43:43 visual_prompt]: 	Training 200/553. train loss: 17.2855,	0.4995 s / batch. (data: 2.76e-04). ETA=6:43:28, max mem: 11.4 GB 
[10/26 02:45:12 visual_prompt]: 	Training 300/553. train loss: 3.2223,	0.7153 s / batch. (data: 2.37e-01). ETA=9:36:36, max mem: 11.4 GB 
[10/26 02:46:39 visual_prompt]: 	Training 400/553. train loss: 2.3358,	0.4816 s / batch. (data: 2.73e-04). ETA=6:27:22, max mem: 11.4 GB 
[10/26 02:48:10 visual_prompt]: 	Training 500/553. train loss: 1.0418,	0.5247 s / batch. (data: 5.41e-03). ETA=7:01:12, max mem: 11.4 GB 
[10/26 02:48:55 visual_prompt]: Epoch 13 / 100: avg data time: 3.94e-01, avg batch time: 0.8886, average train loss: 7.9913
[10/26 02:49:48 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.1903, average loss: 1.1091
[10/26 02:49:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.19	
[10/26 02:49:48 visual_prompt]: Best epoch 13: best metric: -1.109
[10/26 02:49:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/26 02:51:20 visual_prompt]: 	Training 100/553. train loss: 62.5105,	0.4975 s / batch. (data: 2.69e-04). ETA=6:38:03, max mem: 11.4 GB 
[10/26 02:52:49 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.9040 s / batch. (data: 1.41e+00). ETA=1 day, 1:20:21, max mem: 11.4 GB 
[10/26 02:54:18 visual_prompt]: 	Training 300/553. train loss: 10.3488,	1.3736 s / batch. (data: 8.95e-01). ETA=18:14:34, max mem: 11.4 GB 
[10/26 02:55:45 visual_prompt]: 	Training 400/553. train loss: 1.5904,	0.4992 s / batch. (data: 5.47e-03). ETA=6:36:56, max mem: 11.4 GB 
[10/26 02:57:14 visual_prompt]: 	Training 500/553. train loss: 62.2967,	0.4840 s / batch. (data: 2.52e-04). ETA=6:24:06, max mem: 11.4 GB 
[10/26 02:57:59 visual_prompt]: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 12.3698
[10/26 02:58:51 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1914, average loss: 17.9445
[10/26 02:58:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.09	
[10/26 02:58:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/26 03:00:23 visual_prompt]: 	Training 100/553. train loss: 27.4773,	0.4907 s / batch. (data: 2.79e-04). ETA=6:28:06, max mem: 11.4 GB 
[10/26 03:01:51 visual_prompt]: 	Training 200/553. train loss: 67.8974,	0.4800 s / batch. (data: 2.75e-04). ETA=6:18:51, max mem: 11.4 GB 
[10/26 03:03:21 visual_prompt]: 	Training 300/553. train loss: 33.4701,	0.4836 s / batch. (data: 2.52e-04). ETA=6:20:51, max mem: 11.4 GB 
[10/26 03:04:47 visual_prompt]: 	Training 400/553. train loss: 3.2093,	0.4942 s / batch. (data: 2.71e-04). ETA=6:28:26, max mem: 11.4 GB 
[10/26 03:06:17 visual_prompt]: 	Training 500/553. train loss: 0.3904,	0.5233 s / batch. (data: 7.27e-03). ETA=6:50:26, max mem: 11.4 GB 
[10/26 03:07:04 visual_prompt]: Epoch 15 / 100: avg data time: 3.97e-01, avg batch time: 0.8907, average train loss: 22.3954
[10/26 03:07:56 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1920, average loss: 10.7320
[10/26 03:07:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.55	
[10/26 03:07:56 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/26 03:09:27 visual_prompt]: 	Training 100/553. train loss: 8.9266,	0.4781 s / batch. (data: 2.75e-04). ETA=6:13:45, max mem: 11.4 GB 
[10/26 03:10:56 visual_prompt]: 	Training 200/553. train loss: 1.7082,	0.4778 s / batch. (data: 2.80e-04). ETA=6:12:43, max mem: 11.4 GB 
[10/26 03:12:26 visual_prompt]: 	Training 300/553. train loss: 0.9033,	0.4805 s / batch. (data: 1.14e-03). ETA=6:14:01, max mem: 11.4 GB 
[10/26 03:13:54 visual_prompt]: 	Training 400/553. train loss: 31.5535,	0.4780 s / batch. (data: 2.77e-04). ETA=6:11:19, max mem: 11.4 GB 
[10/26 03:15:23 visual_prompt]: 	Training 500/553. train loss: 3.2071,	1.8838 s / batch. (data: 1.39e+00). ETA=1 day, 0:20:06, max mem: 11.4 GB 
[10/26 03:16:08 visual_prompt]: Epoch 16 / 100: avg data time: 3.96e-01, avg batch time: 0.8895, average train loss: 12.7780
[10/26 03:17:01 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1906, average loss: 2.8510
[10/26 03:17:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.35	
[10/26 03:17:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/26 03:18:32 visual_prompt]: 	Training 100/553. train loss: 7.1212,	0.5040 s / batch. (data: 4.54e-04). ETA=6:29:19, max mem: 11.4 GB 
[10/26 03:20:02 visual_prompt]: 	Training 200/553. train loss: 33.1376,	0.5003 s / batch. (data: 8.49e-03). ETA=6:25:41, max mem: 11.4 GB 
[10/26 03:21:30 visual_prompt]: 	Training 300/553. train loss: 5.7990,	0.5010 s / batch. (data: 2.48e-04). ETA=6:25:24, max mem: 11.4 GB 
[10/26 03:22:58 visual_prompt]: 	Training 400/553. train loss: 0.6266,	1.0560 s / batch. (data: 5.57e-01). ETA=13:30:32, max mem: 11.4 GB 
[10/26 03:24:25 visual_prompt]: 	Training 500/553. train loss: 10.5542,	0.5526 s / batch. (data: 4.74e-02). ETA=7:03:15, max mem: 11.4 GB 
[10/26 03:25:12 visual_prompt]: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 10.4214
[10/26 03:26:05 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1899, average loss: 8.6570
[10/26 03:26:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.79	
[10/26 03:26:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/26 03:27:36 visual_prompt]: 	Training 100/553. train loss: 7.6962,	0.4941 s / batch. (data: 1.04e-02). ETA=6:17:10, max mem: 11.4 GB 
[10/26 03:29:07 visual_prompt]: 	Training 200/553. train loss: 17.7688,	0.5280 s / batch. (data: 1.19e-02). ETA=6:42:07, max mem: 11.4 GB 
[10/26 03:30:36 visual_prompt]: 	Training 300/553. train loss: 10.3138,	0.4850 s / batch. (data: 5.40e-03). ETA=6:08:35, max mem: 11.4 GB 
[10/26 03:32:04 visual_prompt]: 	Training 400/553. train loss: 0.8387,	0.4917 s / batch. (data: 2.70e-04). ETA=6:12:51, max mem: 11.4 GB 
[10/26 03:33:32 visual_prompt]: 	Training 500/553. train loss: 2.2974,	0.4966 s / batch. (data: 5.40e-03). ETA=6:15:44, max mem: 11.4 GB 
[10/26 03:34:16 visual_prompt]: Epoch 18 / 100: avg data time: 3.95e-01, avg batch time: 0.8892, average train loss: 13.6318
[10/26 03:35:09 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1920, average loss: 3.8757
[10/26 03:35:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.35	
[10/26 03:35:09 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/26 03:36:40 visual_prompt]: 	Training 100/553. train loss: 8.3992,	0.5000 s / batch. (data: 2.81e-04). ETA=6:17:03, max mem: 11.4 GB 
[10/26 03:38:10 visual_prompt]: 	Training 200/553. train loss: 8.2049,	0.4965 s / batch. (data: 5.38e-03). ETA=6:13:35, max mem: 11.4 GB 
[10/26 03:39:39 visual_prompt]: 	Training 300/553. train loss: 0.1794,	0.4913 s / batch. (data: 2.67e-04). ETA=6:08:51, max mem: 11.4 GB 
[10/26 03:41:09 visual_prompt]: 	Training 400/553. train loss: 2.6432,	0.4880 s / batch. (data: 2.69e-04). ETA=6:05:34, max mem: 11.4 GB 
[10/26 03:42:33 visual_prompt]: 	Training 500/553. train loss: 4.7819,	0.5000 s / batch. (data: 2.79e-04). ETA=6:13:43, max mem: 11.4 GB 
[10/26 03:43:20 visual_prompt]: Epoch 19 / 100: avg data time: 3.95e-01, avg batch time: 0.8883, average train loss: 7.7568
[10/26 03:44:12 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1900, average loss: 16.5289
[10/26 03:44:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.20	
[10/26 03:44:12 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[10/26 03:45:42 visual_prompt]: 	Training 100/553. train loss: 5.7588,	0.5325 s / batch. (data: 2.75e-04). ETA=6:36:38, max mem: 11.4 GB 
[10/26 03:47:13 visual_prompt]: 	Training 200/553. train loss: 5.4121,	0.4900 s / batch. (data: 1.20e-02). ETA=6:04:11, max mem: 11.4 GB 
[10/26 03:48:41 visual_prompt]: 	Training 300/553. train loss: 2.7455,	0.5040 s / batch. (data: 7.96e-03). ETA=6:13:45, max mem: 11.4 GB 
[10/26 03:50:10 visual_prompt]: 	Training 400/553. train loss: 32.9596,	0.5243 s / batch. (data: 1.55e-02). ETA=6:27:56, max mem: 11.4 GB 
[10/26 03:51:38 visual_prompt]: 	Training 500/553. train loss: 8.5920,	0.4962 s / batch. (data: 3.12e-04). ETA=6:06:18, max mem: 11.4 GB 
[10/26 03:52:25 visual_prompt]: Epoch 20 / 100: avg data time: 3.97e-01, avg batch time: 0.8908, average train loss: 12.5349
[10/26 03:53:18 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1893, average loss: 4.5171
[10/26 03:53:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.13	
[10/26 03:53:18 visual_prompt]: Stopping early.
[10/26 03:53:18 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 03:53:18 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 03:53:18 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 03:53:18 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 03:53:18 visual_prompt]: Training with config:
[10/26 03:53:18 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr5.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 03:53:18 visual_prompt]: Loading training data...
[10/26 03:53:18 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 03:53:18 visual_prompt]: Loading validation data...
[10/26 03:53:18 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 03:53:18 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/26 03:53:20 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/26 03:53:20 visual_prompt]: tuned percent:0.529
[10/26 03:53:20 visual_prompt]: Device used for model: 0
[10/26 03:53:20 visual_prompt]: Setting up Evaluator...
[10/26 03:53:20 visual_prompt]: Setting up Trainer...
[10/26 03:53:20 visual_prompt]: 	Setting up the optimizer...
[10/26 03:53:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 03:54:52 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4918 s / batch. (data: 5.40e-03). ETA=7:32:27, max mem: 11.4 GB 
[10/26 03:56:19 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5157 s / batch. (data: 5.34e-03). ETA=7:53:32, max mem: 11.4 GB 
[10/26 03:57:51 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9537 s / batch. (data: 2.45e+00). ETA=1 day, 21:07:33, max mem: 11.4 GB 
[10/26 03:59:17 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5120 s / batch. (data: 2.56e-04). ETA=7:48:29, max mem: 11.4 GB 
[10/26 04:00:48 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5100 s / batch. (data: 5.84e-03). ETA=7:45:47, max mem: 11.4 GB 
[10/26 04:01:34 visual_prompt]: Epoch 1 / 100: avg data time: 3.98e-01, avg batch time: 0.8931, average train loss: 1.3966
[10/26 04:02:27 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1907, average loss: 1.3454
[10/26 04:02:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/26 04:02:27 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/26 04:03:58 visual_prompt]: 	Training 100/553. train loss: 3.2019,	0.4962 s / batch. (data: 7.97e-03). ETA=7:31:55, max mem: 11.4 GB 
[10/26 04:05:26 visual_prompt]: 	Training 200/553. train loss: 0.0047,	1.5523 s / batch. (data: 1.06e+00). ETA=23:31:11, max mem: 11.4 GB 
[10/26 04:06:57 visual_prompt]: 	Training 300/553. train loss: 2.8204,	1.5277 s / batch. (data: 1.04e+00). ETA=23:06:19, max mem: 11.4 GB 
[10/26 04:08:23 visual_prompt]: 	Training 400/553. train loss: 0.6047,	0.5121 s / batch. (data: 1.35e-02). ETA=7:43:49, max mem: 11.4 GB 
[10/26 04:09:54 visual_prompt]: 	Training 500/553. train loss: 0.6561,	0.4795 s / batch. (data: 2.71e-04). ETA=7:13:29, max mem: 11.4 GB 
[10/26 04:10:39 visual_prompt]: Epoch 2 / 100: avg data time: 3.95e-01, avg batch time: 0.8895, average train loss: 1.9986
[10/26 04:11:31 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.1901, average loss: 0.9400
[10/26 04:11:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.07	
[10/26 04:11:31 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/26 04:13:01 visual_prompt]: 	Training 100/553. train loss: 1.6439,	0.5080 s / batch. (data: 2.39e-04). ETA=7:38:00, max mem: 11.4 GB 
[10/26 04:14:31 visual_prompt]: 	Training 200/553. train loss: 1.0933,	0.5024 s / batch. (data: 1.16e-02). ETA=7:32:09, max mem: 11.4 GB 
[10/26 04:15:58 visual_prompt]: 	Training 300/553. train loss: 4.2792,	0.5006 s / batch. (data: 5.45e-03). ETA=7:29:39, max mem: 11.4 GB 
[10/26 04:17:28 visual_prompt]: 	Training 400/553. train loss: 0.9600,	0.5039 s / batch. (data: 2.68e-04). ETA=7:31:45, max mem: 11.4 GB 
[10/26 04:18:58 visual_prompt]: 	Training 500/553. train loss: 1.0864,	1.9151 s / batch. (data: 1.42e+00). ETA=1 day, 4:33:49, max mem: 11.4 GB 
[10/26 04:19:43 visual_prompt]: Epoch 3 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 2.1594
[10/26 04:20:35 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1907, average loss: 1.5693
[10/26 04:20:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.14	
[10/26 04:20:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/26 04:22:08 visual_prompt]: 	Training 100/553. train loss: 4.0321,	0.4874 s / batch. (data: 9.05e-03). ETA=7:14:58, max mem: 11.4 GB 
[10/26 04:23:37 visual_prompt]: 	Training 200/553. train loss: 2.3540,	0.4961 s / batch. (data: 2.54e-04). ETA=7:21:51, max mem: 11.4 GB 
[10/26 04:25:06 visual_prompt]: 	Training 300/553. train loss: 8.4845,	1.3033 s / batch. (data: 8.13e-01). ETA=19:18:37, max mem: 11.4 GB 
[10/26 04:26:31 visual_prompt]: 	Training 400/553. train loss: 3.4764,	1.8995 s / batch. (data: 1.41e+00). ETA=1 day, 4:05:29, max mem: 11.4 GB 
[10/26 04:28:01 visual_prompt]: 	Training 500/553. train loss: 1.7183,	3.7511 s / batch. (data: 3.26e+00). ETA=2 days, 7:22:15, max mem: 11.4 GB 
[10/26 04:28:47 visual_prompt]: Epoch 4 / 100: avg data time: 3.95e-01, avg batch time: 0.8901, average train loss: 3.5613
[10/26 04:29:40 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1910, average loss: 1.1414
[10/26 04:29:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.32	
[10/26 04:29:40 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/26 04:31:10 visual_prompt]: 	Training 100/553. train loss: 9.6816,	0.4902 s / batch. (data: 1.23e-02). ETA=7:12:55, max mem: 11.4 GB 
[10/26 04:32:40 visual_prompt]: 	Training 200/553. train loss: 1.4049,	1.7720 s / batch. (data: 1.26e+00). ETA=1 day, 2:01:57, max mem: 11.4 GB 
[10/26 04:34:09 visual_prompt]: 	Training 300/553. train loss: 7.1193,	0.4790 s / batch. (data: 2.80e-04). ETA=7:01:23, max mem: 11.4 GB 
[10/26 04:35:37 visual_prompt]: 	Training 400/553. train loss: 7.7065,	0.5138 s / batch. (data: 1.55e-02). ETA=7:31:12, max mem: 11.4 GB 
[10/26 04:37:06 visual_prompt]: 	Training 500/553. train loss: 3.9062,	0.4845 s / batch. (data: 2.73e-04). ETA=7:04:36, max mem: 11.4 GB 
[10/26 04:37:52 visual_prompt]: Epoch 5 / 100: avg data time: 3.96e-01, avg batch time: 0.8901, average train loss: 3.7100
[10/26 04:38:44 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1904, average loss: 19.8764
[10/26 04:38:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.06	
[10/26 04:38:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/26 04:40:17 visual_prompt]: 	Training 100/553. train loss: 11.7624,	0.4778 s / batch. (data: 2.77e-04). ETA=6:57:34, max mem: 11.4 GB 
[10/26 04:41:45 visual_prompt]: 	Training 200/553. train loss: 5.2101,	0.4919 s / batch. (data: 3.97e-04). ETA=7:09:01, max mem: 11.4 GB 
[10/26 04:43:13 visual_prompt]: 	Training 300/553. train loss: 0.4837,	0.5211 s / batch. (data: 1.55e-02). ETA=7:33:42, max mem: 11.4 GB 
[10/26 04:44:45 visual_prompt]: 	Training 400/553. train loss: 3.9729,	0.5552 s / batch. (data: 4.90e-02). ETA=8:02:25, max mem: 11.4 GB 
[10/26 04:46:13 visual_prompt]: 	Training 500/553. train loss: 11.3928,	1.5041 s / batch. (data: 1.00e+00). ETA=21:44:23, max mem: 11.4 GB 
[10/26 04:46:58 visual_prompt]: Epoch 6 / 100: avg data time: 3.98e-01, avg batch time: 0.8919, average train loss: 7.9543
[10/26 04:47:50 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1898, average loss: 0.6991
[10/26 04:47:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.48	
[10/26 04:47:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/26 04:49:21 visual_prompt]: 	Training 100/553. train loss: 3.7231,	0.5120 s / batch. (data: 2.73e-04). ETA=7:22:43, max mem: 11.4 GB 
[10/26 04:50:49 visual_prompt]: 	Training 200/553. train loss: 2.7318,	0.4838 s / batch. (data: 5.38e-03). ETA=6:57:32, max mem: 11.4 GB 
[10/26 04:52:21 visual_prompt]: 	Training 300/553. train loss: 0.7639,	2.5301 s / batch. (data: 2.04e+00). ETA=1 day, 12:19:18, max mem: 11.4 GB 
[10/26 04:53:50 visual_prompt]: 	Training 400/553. train loss: 1.7931,	2.0663 s / batch. (data: 1.58e+00). ETA=1 day, 5:36:25, max mem: 11.4 GB 
[10/26 04:55:17 visual_prompt]: 	Training 500/553. train loss: 2.2217,	0.7520 s / batch. (data: 2.55e-01). ETA=10:45:12, max mem: 11.4 GB 
[10/26 04:56:02 visual_prompt]: Epoch 7 / 100: avg data time: 3.94e-01, avg batch time: 0.8889, average train loss: 4.7802
[10/26 04:56:54 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1913, average loss: 4.2983
[10/26 04:56:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.56	
[10/26 04:56:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/26 04:58:24 visual_prompt]: 	Training 100/553. train loss: 11.7277,	0.5213 s / batch. (data: 5.41e-03). ETA=7:25:59, max mem: 11.4 GB 
[10/26 04:59:54 visual_prompt]: 	Training 200/553. train loss: 1.5039,	0.4920 s / batch. (data: 2.90e-04). ETA=7:00:04, max mem: 11.4 GB 
[10/26 05:01:24 visual_prompt]: 	Training 300/553. train loss: 3.2545,	0.5000 s / batch. (data: 2.59e-04). ETA=7:06:06, max mem: 11.4 GB 
[10/26 05:02:53 visual_prompt]: 	Training 400/553. train loss: 2.2357,	0.4893 s / batch. (data: 2.79e-04). ETA=6:56:06, max mem: 11.4 GB 
[10/26 05:04:22 visual_prompt]: 	Training 500/553. train loss: 17.1332,	1.7642 s / batch. (data: 1.27e+00). ETA=1 day, 0:57:26, max mem: 11.4 GB 
[10/26 05:05:07 visual_prompt]: Epoch 8 / 100: avg data time: 3.98e-01, avg batch time: 0.8919, average train loss: 5.4417
[10/26 05:06:00 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1905, average loss: 2.7877
[10/26 05:06:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.72	
[10/26 05:06:00 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/26 05:07:31 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.5186 s / batch. (data: 7.97e-03). ETA=7:18:53, max mem: 11.4 GB 
[10/26 05:08:59 visual_prompt]: 	Training 200/553. train loss: 0.6553,	0.4999 s / batch. (data: 4.02e-04). ETA=7:02:13, max mem: 11.4 GB 
[10/26 05:10:28 visual_prompt]: 	Training 300/553. train loss: 0.7652,	1.6364 s / batch. (data: 1.13e+00). ETA=22:59:23, max mem: 11.4 GB 
[10/26 05:11:57 visual_prompt]: 	Training 400/553. train loss: 10.1654,	0.4805 s / batch. (data: 2.79e-04). ETA=6:44:13, max mem: 11.4 GB 
[10/26 05:13:27 visual_prompt]: 	Training 500/553. train loss: 1.8611,	0.8942 s / batch. (data: 4.16e-01). ETA=12:30:45, max mem: 11.4 GB 
[10/26 05:14:11 visual_prompt]: Epoch 9 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 5.3813
[10/26 05:15:04 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1910, average loss: 17.0861
[10/26 05:15:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.10	
[10/26 05:15:04 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/26 05:16:38 visual_prompt]: 	Training 100/553. train loss: 16.4592,	0.4800 s / batch. (data: 2.45e-04). ETA=6:41:48, max mem: 11.4 GB 
[10/26 05:18:05 visual_prompt]: 	Training 200/553. train loss: 1.7122,	0.4960 s / batch. (data: 2.67e-04). ETA=6:54:20, max mem: 11.4 GB 
[10/26 05:19:33 visual_prompt]: 	Training 300/553. train loss: 6.4588,	0.5104 s / batch. (data: 2.76e-04). ETA=7:05:32, max mem: 11.4 GB 
[10/26 05:20:59 visual_prompt]: 	Training 400/553. train loss: 0.7902,	0.8791 s / batch. (data: 3.69e-01). ETA=12:11:29, max mem: 11.4 GB 
[10/26 05:22:29 visual_prompt]: 	Training 500/553. train loss: 4.3483,	0.4879 s / batch. (data: 2.70e-04). ETA=6:45:06, max mem: 11.4 GB 
[10/26 05:23:15 visual_prompt]: Epoch 10 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 8.6571
[10/26 05:24:07 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1896, average loss: 6.8557
[10/26 05:24:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.20	
[10/26 05:24:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/26 05:25:41 visual_prompt]: 	Training 100/553. train loss: 9.5890,	0.4892 s / batch. (data: 2.88e-04). ETA=6:45:01, max mem: 11.4 GB 
[10/26 05:27:11 visual_prompt]: 	Training 200/553. train loss: 0.0002,	0.5159 s / batch. (data: 2.86e-04). ETA=7:06:12, max mem: 11.4 GB 
[10/26 05:28:39 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2639 s / batch. (data: 1.76e+00). ETA=1 day, 7:06:37, max mem: 11.4 GB 
[10/26 05:30:06 visual_prompt]: 	Training 400/553. train loss: 4.9020,	0.4859 s / batch. (data: 2.89e-04). ETA=6:39:47, max mem: 11.4 GB 
[10/26 05:31:34 visual_prompt]: 	Training 500/553. train loss: 9.0528,	0.5162 s / batch. (data: 1.62e-02). ETA=7:03:54, max mem: 11.4 GB 
[10/26 05:32:19 visual_prompt]: Epoch 11 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 7.3039
[10/26 05:33:12 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1917, average loss: 10.0162
[10/26 05:33:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.66	
[10/26 05:33:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/26 05:34:46 visual_prompt]: 	Training 100/553. train loss: 2.9090,	0.5095 s / batch. (data: 2.88e-04). ETA=6:57:03, max mem: 11.4 GB 
[10/26 05:36:16 visual_prompt]: 	Training 200/553. train loss: 2.9459,	2.2320 s / batch. (data: 1.73e+00). ETA=1 day, 6:23:24, max mem: 11.4 GB 
[10/26 05:37:43 visual_prompt]: 	Training 300/553. train loss: 3.8896,	0.5483 s / batch. (data: 6.21e-02). ETA=7:27:00, max mem: 11.4 GB 
[10/26 05:39:12 visual_prompt]: 	Training 400/553. train loss: 0.7406,	0.5159 s / batch. (data: 2.83e-04). ETA=6:59:44, max mem: 11.4 GB 
[10/26 05:40:42 visual_prompt]: 	Training 500/553. train loss: 56.5929,	0.5120 s / batch. (data: 2.39e-04). ETA=6:55:43, max mem: 11.4 GB 
[10/26 05:41:27 visual_prompt]: Epoch 12 / 100: avg data time: 4.00e-01, avg batch time: 0.8945, average train loss: 7.2591
[10/26 05:42:19 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1901, average loss: 29.3135
[10/26 05:42:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.25	
[10/26 05:42:19 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/26 05:43:52 visual_prompt]: 	Training 100/553. train loss: 3.6861,	0.5322 s / batch. (data: 5.43e-03). ETA=7:10:43, max mem: 11.4 GB 
[10/26 05:45:19 visual_prompt]: 	Training 200/553. train loss: 7.7498,	0.4921 s / batch. (data: 2.95e-04). ETA=6:37:26, max mem: 11.4 GB 
[10/26 05:46:49 visual_prompt]: 	Training 300/553. train loss: 4.1355,	1.2960 s / batch. (data: 7.94e-01). ETA=17:24:39, max mem: 11.4 GB 
[10/26 05:48:16 visual_prompt]: 	Training 400/553. train loss: 37.1485,	0.4960 s / batch. (data: 2.74e-04). ETA=6:38:59, max mem: 11.4 GB 
[10/26 05:49:45 visual_prompt]: 	Training 500/553. train loss: 13.4464,	0.4960 s / batch. (data: 2.56e-04). ETA=6:38:08, max mem: 11.4 GB 
[10/26 05:50:32 visual_prompt]: Epoch 13 / 100: avg data time: 3.95e-01, avg batch time: 0.8905, average train loss: 8.1146
[10/26 05:51:24 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1903, average loss: 2.6656
[10/26 05:51:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.86	
[10/26 05:51:24 visual_prompt]: Best epoch 13: best metric: -2.666
[10/26 05:51:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/26 05:52:57 visual_prompt]: 	Training 100/553. train loss: 12.6793,	0.5042 s / batch. (data: 1.64e-02). ETA=6:43:25, max mem: 11.4 GB 
[10/26 05:54:26 visual_prompt]: 	Training 200/553. train loss: 0.0225,	1.6320 s / batch. (data: 1.14e+00). ETA=21:43:09, max mem: 11.4 GB 
[10/26 05:55:54 visual_prompt]: 	Training 300/553. train loss: 20.8670,	1.3447 s / batch. (data: 8.54e-01). ETA=17:51:32, max mem: 11.4 GB 
[10/26 05:57:23 visual_prompt]: 	Training 400/553. train loss: 16.4238,	0.4959 s / batch. (data: 2.91e-04). ETA=6:34:18, max mem: 11.4 GB 
[10/26 05:58:52 visual_prompt]: 	Training 500/553. train loss: 22.9328,	0.4879 s / batch. (data: 5.42e-03). ETA=6:27:08, max mem: 11.4 GB 
[10/26 05:59:36 visual_prompt]: Epoch 14 / 100: avg data time: 3.96e-01, avg batch time: 0.8898, average train loss: 7.7820
[10/26 06:00:29 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1900, average loss: 2.8244
[10/26 06:00:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.70	
[10/26 06:00:29 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/26 06:02:01 visual_prompt]: 	Training 100/553. train loss: 2.0603,	0.7400 s / batch. (data: 2.35e-01). ETA=9:45:18, max mem: 11.4 GB 
[10/26 06:03:30 visual_prompt]: 	Training 200/553. train loss: 22.2337,	0.5080 s / batch. (data: 2.51e-04). ETA=6:40:59, max mem: 11.4 GB 
[10/26 06:05:00 visual_prompt]: 	Training 300/553. train loss: 8.4374,	0.5040 s / batch. (data: 7.99e-03). ETA=6:36:58, max mem: 11.4 GB 
[10/26 06:06:27 visual_prompt]: 	Training 400/553. train loss: 2.9020,	0.5120 s / batch. (data: 3.12e-04). ETA=6:42:22, max mem: 11.4 GB 
[10/26 06:07:57 visual_prompt]: 	Training 500/553. train loss: 0.2060,	0.4842 s / batch. (data: 5.41e-03). ETA=6:19:47, max mem: 11.4 GB 
[10/26 06:08:44 visual_prompt]: Epoch 15 / 100: avg data time: 4.00e-01, avg batch time: 0.8949, average train loss: 8.9468
[10/26 06:09:37 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1930, average loss: 20.6211
[10/26 06:09:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.29	
[10/26 06:09:37 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/26 06:11:09 visual_prompt]: 	Training 100/553. train loss: 3.9764,	0.5080 s / batch. (data: 5.29e-03). ETA=6:37:06, max mem: 11.4 GB 
[10/26 06:12:37 visual_prompt]: 	Training 200/553. train loss: 25.0269,	0.4959 s / batch. (data: 2.62e-04). ETA=6:26:50, max mem: 11.4 GB 
[10/26 06:14:07 visual_prompt]: 	Training 300/553. train loss: 3.8790,	0.5040 s / batch. (data: 7.97e-03). ETA=6:32:20, max mem: 11.4 GB 
[10/26 06:15:37 visual_prompt]: 	Training 400/553. train loss: 8.8185,	0.5120 s / batch. (data: 7.95e-03). ETA=6:37:40, max mem: 11.4 GB 
[10/26 06:17:06 visual_prompt]: 	Training 500/553. train loss: 20.1333,	2.1114 s / batch. (data: 1.60e+00). ETA=1 day, 3:16:28, max mem: 11.4 GB 
[10/26 06:17:51 visual_prompt]: Epoch 16 / 100: avg data time: 3.99e-01, avg batch time: 0.8946, average train loss: 7.4585
[10/26 06:18:44 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1904, average loss: 1.4199
[10/26 06:18:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.08	
[10/26 06:18:44 visual_prompt]: Best epoch 16: best metric: -1.420
[10/26 06:18:44 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/26 06:20:16 visual_prompt]: 	Training 100/553. train loss: 1.0491,	0.4806 s / batch. (data: 2.83e-04). ETA=6:11:18, max mem: 11.4 GB 
[10/26 06:21:47 visual_prompt]: 	Training 200/553. train loss: 31.3392,	0.4915 s / batch. (data: 2.78e-04). ETA=6:18:55, max mem: 11.4 GB 
[10/26 06:23:15 visual_prompt]: 	Training 300/553. train loss: 23.5747,	0.5094 s / batch. (data: 9.10e-03). ETA=6:31:47, max mem: 11.4 GB 
[10/26 06:24:44 visual_prompt]: 	Training 400/553. train loss: 1.4846,	1.3268 s / batch. (data: 8.29e-01). ETA=16:58:22, max mem: 11.4 GB 
[10/26 06:26:13 visual_prompt]: 	Training 500/553. train loss: 3.5346,	2.1397 s / batch. (data: 1.64e+00). ETA=1 day, 3:18:43, max mem: 11.4 GB 
[10/26 06:26:59 visual_prompt]: Epoch 17 / 100: avg data time: 4.00e-01, avg batch time: 0.8951, average train loss: 7.3134
[10/26 06:27:52 visual_prompt]: Inference (val):avg data time: 2.71e-04, avg batch time: 0.1934, average loss: 1.0356
[10/26 06:27:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 57.74	
[10/26 06:27:52 visual_prompt]: Best epoch 17: best metric: -1.036
[10/26 06:27:52 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/26 06:29:25 visual_prompt]: 	Training 100/553. train loss: 9.4663,	0.5048 s / batch. (data: 2.50e-04). ETA=6:25:17, max mem: 11.4 GB 
[10/26 06:30:56 visual_prompt]: 	Training 200/553. train loss: 4.2255,	0.4960 s / batch. (data: 7.74e-04). ETA=6:17:46, max mem: 11.4 GB 
[10/26 06:32:25 visual_prompt]: 	Training 300/553. train loss: 2.0447,	0.5109 s / batch. (data: 7.95e-03). ETA=6:28:18, max mem: 11.4 GB 
[10/26 06:33:53 visual_prompt]: 	Training 400/553. train loss: 1.2824,	0.4805 s / batch. (data: 2.67e-04). ETA=6:04:21, max mem: 11.4 GB 
[10/26 06:35:22 visual_prompt]: 	Training 500/553. train loss: 6.2016,	0.5120 s / batch. (data: 7.96e-03). ETA=6:27:23, max mem: 11.4 GB 
[10/26 06:36:06 visual_prompt]: Epoch 18 / 100: avg data time: 3.99e-01, avg batch time: 0.8929, average train loss: 6.3219
[10/26 06:36:58 visual_prompt]: Inference (val):avg data time: 1.74e-04, avg batch time: 0.1931, average loss: 1.2306
[10/26 06:36:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.23	
[10/26 06:36:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/26 06:38:30 visual_prompt]: 	Training 100/553. train loss: 4.5688,	0.5000 s / batch. (data: 3.05e-04). ETA=6:17:02, max mem: 11.4 GB 
[10/26 06:39:59 visual_prompt]: 	Training 200/553. train loss: 1.4862,	0.5000 s / batch. (data: 2.79e-04). ETA=6:16:13, max mem: 11.4 GB 
[10/26 06:41:28 visual_prompt]: 	Training 300/553. train loss: 24.9951,	0.5041 s / batch. (data: 1.05e-02). ETA=6:18:27, max mem: 11.4 GB 
[10/26 06:42:57 visual_prompt]: 	Training 400/553. train loss: 2.7771,	0.4883 s / batch. (data: 7.96e-03). ETA=6:05:45, max mem: 11.4 GB 
[10/26 06:44:22 visual_prompt]: 	Training 500/553. train loss: 4.0265,	0.5029 s / batch. (data: 1.05e-02). ETA=6:15:53, max mem: 11.4 GB 
[10/26 06:45:09 visual_prompt]: Epoch 19 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 7.2694
[10/26 06:46:02 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1909, average loss: 14.8400
[10/26 06:46:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.05	
[10/26 06:46:02 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[10/26 06:47:31 visual_prompt]: 	Training 100/553. train loss: 2.6428,	0.4850 s / batch. (data: 2.84e-04). ETA=6:01:14, max mem: 11.4 GB 
[10/26 06:49:02 visual_prompt]: 	Training 200/553. train loss: 2.1571,	0.4960 s / batch. (data: 2.71e-04). ETA=6:08:38, max mem: 11.4 GB 
[10/26 06:50:31 visual_prompt]: 	Training 300/553. train loss: 3.5708,	0.4951 s / batch. (data: 2.63e-04). ETA=6:07:07, max mem: 11.4 GB 
[10/26 06:51:59 visual_prompt]: 	Training 400/553. train loss: 4.5585,	0.5358 s / batch. (data: 5.40e-03). ETA=6:36:24, max mem: 11.4 GB 
[10/26 06:53:27 visual_prompt]: 	Training 500/553. train loss: 3.9331,	0.4960 s / batch. (data: 2.74e-04). ETA=6:06:09, max mem: 11.4 GB 
[10/26 06:54:14 visual_prompt]: Epoch 20 / 100: avg data time: 3.96e-01, avg batch time: 0.8905, average train loss: 5.3302
[10/26 06:55:06 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1914, average loss: 1.5600
[10/26 06:55:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.26	
[10/26 06:55:06 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[10/26 06:56:40 visual_prompt]: 	Training 100/553. train loss: 2.6439,	0.4960 s / batch. (data: 2.60e-04). ETA=6:04:53, max mem: 11.4 GB 
[10/26 06:58:08 visual_prompt]: 	Training 200/553. train loss: 10.1054,	0.4965 s / batch. (data: 2.71e-04). ETA=6:04:23, max mem: 11.4 GB 
[10/26 06:59:37 visual_prompt]: 	Training 300/553. train loss: 5.4233,	1.2247 s / batch. (data: 7.36e-01). ETA=14:56:52, max mem: 11.4 GB 
[10/26 07:01:04 visual_prompt]: 	Training 400/553. train loss: 3.1274,	0.4880 s / batch. (data: 2.68e-04). ETA=5:56:32, max mem: 11.4 GB 
[10/26 07:02:34 visual_prompt]: 	Training 500/553. train loss: 1.1093,	0.4888 s / batch. (data: 2.78e-04). ETA=5:56:18, max mem: 11.4 GB 
[10/26 07:03:18 visual_prompt]: Epoch 21 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 7.1127
[10/26 07:04:11 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1906, average loss: 1.8126
[10/26 07:04:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.86	
[10/26 07:04:11 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[10/26 07:05:42 visual_prompt]: 	Training 100/553. train loss: 4.8249,	0.4960 s / batch. (data: 5.40e-03). ETA=6:00:19, max mem: 11.4 GB 
[10/26 07:07:11 visual_prompt]: 	Training 200/553. train loss: 0.0572,	0.4893 s / batch. (data: 8.88e-03). ETA=5:54:38, max mem: 11.4 GB 
[10/26 07:08:38 visual_prompt]: 	Training 300/553. train loss: 0.1948,	0.5200 s / batch. (data: 2.76e-04). ETA=6:16:01, max mem: 11.4 GB 
[10/26 07:10:06 visual_prompt]: 	Training 400/553. train loss: 11.4848,	0.5197 s / batch. (data: 1.16e-02). ETA=6:14:55, max mem: 11.4 GB 
[10/26 07:11:36 visual_prompt]: 	Training 500/553. train loss: 3.4223,	0.4999 s / batch. (data: 2.65e-04). ETA=5:59:50, max mem: 11.4 GB 
[10/26 07:12:23 visual_prompt]: Epoch 22 / 100: avg data time: 3.94e-01, avg batch time: 0.8889, average train loss: 5.6182
[10/26 07:13:15 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1933, average loss: 1.8248
[10/26 07:13:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.32	
[10/26 07:13:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[10/26 07:14:48 visual_prompt]: 	Training 100/553. train loss: 2.6460,	0.5020 s / batch. (data: 2.91e-04). ETA=6:00:03, max mem: 11.4 GB 
[10/26 07:16:18 visual_prompt]: 	Training 200/553. train loss: 0.5496,	1.5072 s / batch. (data: 1.02e+00). ETA=17:58:30, max mem: 11.4 GB 
[10/26 07:17:48 visual_prompt]: 	Training 300/553. train loss: 3.2850,	0.5198 s / batch. (data: 7.12e-04). ETA=6:11:03, max mem: 11.4 GB 
[10/26 07:19:15 visual_prompt]: 	Training 400/553. train loss: 3.0592,	0.4920 s / batch. (data: 2.68e-04). ETA=5:50:26, max mem: 11.4 GB 
[10/26 07:20:41 visual_prompt]: 	Training 500/553. train loss: 13.7895,	0.4800 s / batch. (data: 2.54e-04). ETA=5:41:05, max mem: 11.4 GB 
[10/26 07:21:27 visual_prompt]: Epoch 23 / 100: avg data time: 3.95e-01, avg batch time: 0.8895, average train loss: 6.2902
[10/26 07:22:19 visual_prompt]: Inference (val):avg data time: 3.13e-04, avg batch time: 0.1908, average loss: 6.5926
[10/26 07:22:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.73	
[10/26 07:22:19 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[10/26 07:23:48 visual_prompt]: 	Training 100/553. train loss: 8.7508,	0.5003 s / batch. (data: 1.05e-02). ETA=5:54:12, max mem: 11.4 GB 
[10/26 07:25:17 visual_prompt]: 	Training 200/553. train loss: 3.2920,	0.4909 s / batch. (data: 3.24e-04). ETA=5:46:45, max mem: 11.4 GB 
[10/26 07:26:46 visual_prompt]: 	Training 300/553. train loss: 3.0053,	1.6320 s / batch. (data: 1.11e+00). ETA=19:10:04, max mem: 11.4 GB 
[10/26 07:28:14 visual_prompt]: 	Training 400/553. train loss: 11.2126,	0.5037 s / batch. (data: 2.57e-02). ETA=5:54:06, max mem: 11.4 GB 
[10/26 07:29:45 visual_prompt]: 	Training 500/553. train loss: 4.3508,	0.7516 s / batch. (data: 2.69e-01). ETA=8:47:06, max mem: 11.4 GB 
[10/26 07:30:31 visual_prompt]: Epoch 24 / 100: avg data time: 3.94e-01, avg batch time: 0.8881, average train loss: 5.9796
[10/26 07:31:23 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1925, average loss: 1.3266
[10/26 07:31:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 56.14	
[10/26 07:31:23 visual_prompt]: Stopping early.
[10/26 07:31:23 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 07:31:23 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 07:31:23 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 07:31:23 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 07:31:23 visual_prompt]: Training with config:
[10/26 07:31:23 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr2.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 07:31:23 visual_prompt]: Loading training data...
[10/26 07:31:23 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 07:31:23 visual_prompt]: Loading validation data...
[10/26 07:31:23 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 07:31:23 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/26 07:31:26 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/26 07:31:26 visual_prompt]: tuned percent:0.529
[10/26 07:31:26 visual_prompt]: Device used for model: 0
[10/26 07:31:26 visual_prompt]: Setting up Evaluator...
[10/26 07:31:26 visual_prompt]: Setting up Trainer...
[10/26 07:31:26 visual_prompt]: 	Setting up the optimizer...
[10/26 07:31:26 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 07:32:57 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4840 s / batch. (data: 2.57e-04). ETA=7:25:19, max mem: 11.4 GB 
[10/26 07:34:25 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4921 s / batch. (data: 2.52e-04). ETA=7:31:52, max mem: 11.4 GB 
[10/26 07:35:56 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.8484 s / batch. (data: 2.35e+00). ETA=1 day, 19:31:00, max mem: 11.4 GB 
[10/26 07:37:22 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4963 s / batch. (data: 1.55e-02). ETA=7:34:05, max mem: 11.4 GB 
[10/26 07:38:53 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4916 s / batch. (data: 2.65e-04). ETA=7:28:59, max mem: 11.4 GB 
[10/26 07:39:39 visual_prompt]: Epoch 1 / 100: avg data time: 3.98e-01, avg batch time: 0.8919, average train loss: 1.3966
[10/26 07:40:31 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1887, average loss: 1.3454
[10/26 07:40:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/26 07:40:31 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/26 07:42:03 visual_prompt]: 	Training 100/553. train loss: 1.3108,	0.8220 s / batch. (data: 3.44e-01). ETA=12:28:40, max mem: 11.4 GB 
[10/26 07:43:32 visual_prompt]: 	Training 200/553. train loss: 0.0827,	2.6478 s / batch. (data: 2.17e+00). ETA=1 day, 16:07:09, max mem: 11.4 GB 
[10/26 07:45:02 visual_prompt]: 	Training 300/553. train loss: 0.7464,	1.6375 s / batch. (data: 1.16e+00). ETA=1 day, 0:45:59, max mem: 11.4 GB 
[10/26 07:46:28 visual_prompt]: 	Training 400/553. train loss: 1.4089,	0.5760 s / batch. (data: 8.40e-02). ETA=8:41:44, max mem: 11.4 GB 
[10/26 07:47:59 visual_prompt]: 	Training 500/553. train loss: 0.5819,	0.5087 s / batch. (data: 1.04e-02). ETA=7:39:54, max mem: 11.4 GB 
[10/26 07:48:44 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 0.8903, average train loss: 1.0832
[10/26 07:49:36 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1909, average loss: 2.1889
[10/26 07:49:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.91	
[10/26 07:49:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/26 07:51:07 visual_prompt]: 	Training 100/553. train loss: 1.0280,	0.5163 s / batch. (data: 8.22e-03). ETA=7:45:26, max mem: 11.4 GB 
[10/26 07:52:36 visual_prompt]: 	Training 200/553. train loss: 0.7103,	0.5040 s / batch. (data: 2.54e-04). ETA=7:33:33, max mem: 11.4 GB 
[10/26 07:54:04 visual_prompt]: 	Training 300/553. train loss: 1.0380,	0.4848 s / batch. (data: 2.53e-04). ETA=7:15:27, max mem: 11.4 GB 
[10/26 07:55:34 visual_prompt]: 	Training 400/553. train loss: 1.2087,	0.4960 s / batch. (data: 7.97e-03). ETA=7:24:44, max mem: 11.4 GB 
[10/26 07:57:04 visual_prompt]: 	Training 500/553. train loss: 0.7098,	1.8440 s / batch. (data: 1.35e+00). ETA=1 day, 3:30:10, max mem: 11.4 GB 
[10/26 07:57:48 visual_prompt]: Epoch 3 / 100: avg data time: 3.96e-01, avg batch time: 0.8895, average train loss: 1.4126
[10/26 07:58:41 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1930, average loss: 1.0902
[10/26 07:58:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.98	
[10/26 07:58:41 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/26 08:00:14 visual_prompt]: 	Training 100/553. train loss: 1.5569,	0.4880 s / batch. (data: 2.68e-04). ETA=7:15:27, max mem: 11.4 GB 
[10/26 08:01:43 visual_prompt]: 	Training 200/553. train loss: 0.5735,	0.5000 s / batch. (data: 2.65e-04). ETA=7:25:19, max mem: 11.4 GB 
[10/26 08:03:12 visual_prompt]: 	Training 300/553. train loss: 3.8015,	2.0836 s / batch. (data: 1.55e+00). ETA=1 day, 6:52:20, max mem: 11.4 GB 
[10/26 08:04:38 visual_prompt]: 	Training 400/553. train loss: 2.7453,	1.7800 s / batch. (data: 1.28e+00). ETA=1 day, 2:19:26, max mem: 11.4 GB 
[10/26 08:06:08 visual_prompt]: 	Training 500/553. train loss: 3.0722,	4.2110 s / batch. (data: 3.72e+00). ETA=2 days, 14:09:35, max mem: 11.4 GB 
[10/26 08:06:53 visual_prompt]: Epoch 4 / 100: avg data time: 3.97e-01, avg batch time: 0.8908, average train loss: 1.8262
[10/26 08:07:45 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1917, average loss: 1.0468
[10/26 08:07:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.95	
[10/26 08:07:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/26 08:09:16 visual_prompt]: 	Training 100/553. train loss: 0.0548,	0.5040 s / batch. (data: 2.78e-04). ETA=7:25:07, max mem: 11.4 GB 
[10/26 08:10:45 visual_prompt]: 	Training 200/553. train loss: 4.8232,	1.8404 s / batch. (data: 1.35e+00). ETA=1 day, 3:02:14, max mem: 11.4 GB 
[10/26 08:12:14 visual_prompt]: 	Training 300/553. train loss: 0.7816,	0.4920 s / batch. (data: 2.55e-04). ETA=7:12:51, max mem: 11.4 GB 
[10/26 08:13:42 visual_prompt]: 	Training 400/553. train loss: 1.1612,	0.4842 s / batch. (data: 5.41e-03). ETA=7:05:12, max mem: 11.4 GB 
[10/26 08:15:11 visual_prompt]: 	Training 500/553. train loss: 1.0019,	0.4965 s / batch. (data: 5.39e-03). ETA=7:15:08, max mem: 11.4 GB 
[10/26 08:15:57 visual_prompt]: Epoch 5 / 100: avg data time: 3.93e-01, avg batch time: 0.8889, average train loss: 2.2455
[10/26 08:16:50 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1910, average loss: 1.7697
[10/26 08:16:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.23	
[10/26 08:16:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/26 08:18:23 visual_prompt]: 	Training 100/553. train loss: 2.9715,	0.4963 s / batch. (data: 7.71e-04). ETA=7:13:41, max mem: 11.4 GB 
[10/26 08:19:51 visual_prompt]: 	Training 200/553. train loss: 5.2926,	0.5039 s / batch. (data: 3.32e-04). ETA=7:19:32, max mem: 11.4 GB 
[10/26 08:21:19 visual_prompt]: 	Training 300/553. train loss: 0.5843,	0.5079 s / batch. (data: 1.59e-02). ETA=7:22:10, max mem: 11.4 GB 
[10/26 08:22:50 visual_prompt]: 	Training 400/553. train loss: 4.3527,	0.5249 s / batch. (data: 1.54e-02). ETA=7:36:03, max mem: 11.4 GB 
[10/26 08:24:18 visual_prompt]: 	Training 500/553. train loss: 3.6534,	1.4040 s / batch. (data: 8.93e-01). ETA=20:17:36, max mem: 11.4 GB 
[10/26 08:25:03 visual_prompt]: Epoch 6 / 100: avg data time: 3.97e-01, avg batch time: 0.8920, average train loss: 3.8911
[10/26 08:25:55 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.1900, average loss: 0.9273
[10/26 08:25:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 41.13	
[10/26 08:25:55 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/26 08:27:26 visual_prompt]: 	Training 100/553. train loss: 7.1859,	0.4843 s / batch. (data: 2.58e-04). ETA=6:58:47, max mem: 11.4 GB 
[10/26 08:28:54 visual_prompt]: 	Training 200/553. train loss: 1.7234,	0.4903 s / batch. (data: 1.20e-02). ETA=7:03:10, max mem: 11.4 GB 
[10/26 08:30:27 visual_prompt]: 	Training 300/553. train loss: 26.6893,	2.3919 s / batch. (data: 1.90e+00). ETA=1 day, 10:20:20, max mem: 11.4 GB 
[10/26 08:31:56 visual_prompt]: 	Training 400/553. train loss: 2.0620,	2.3744 s / batch. (data: 1.88e+00). ETA=1 day, 10:01:15, max mem: 11.4 GB 
[10/26 08:33:22 visual_prompt]: 	Training 500/553. train loss: 7.7736,	0.9718 s / batch. (data: 4.72e-01). ETA=13:53:50, max mem: 11.4 GB 
[10/26 08:34:06 visual_prompt]: Epoch 7 / 100: avg data time: 3.94e-01, avg batch time: 0.8880, average train loss: 4.6596
[10/26 08:34:59 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1917, average loss: 1.7103
[10/26 08:34:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.23	
[10/26 08:34:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/26 08:36:28 visual_prompt]: 	Training 100/553. train loss: 0.9626,	0.5745 s / batch. (data: 9.26e-02). ETA=8:11:27, max mem: 11.4 GB 
[10/26 08:37:59 visual_prompt]: 	Training 200/553. train loss: 0.7456,	0.4921 s / batch. (data: 1.17e-03). ETA=7:00:11, max mem: 11.4 GB 
[10/26 08:39:28 visual_prompt]: 	Training 300/553. train loss: 1.9627,	0.4974 s / batch. (data: 2.71e-04). ETA=7:03:50, max mem: 11.4 GB 
[10/26 08:40:57 visual_prompt]: 	Training 400/553. train loss: 5.1736,	0.4800 s / batch. (data: 2.61e-04). ETA=6:48:16, max mem: 11.4 GB 
[10/26 08:42:26 visual_prompt]: 	Training 500/553. train loss: 18.8438,	1.9040 s / batch. (data: 1.42e+00). ETA=1 day, 2:56:10, max mem: 11.4 GB 
[10/26 08:43:11 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8894, average train loss: 4.9127
[10/26 08:44:03 visual_prompt]: Inference (val):avg data time: 3.76e-04, avg batch time: 0.1925, average loss: 7.4746
[10/26 08:44:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.65	
[10/26 08:44:03 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/26 08:45:35 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4890 s / batch. (data: 2.56e-04). ETA=6:53:48, max mem: 11.4 GB 
[10/26 08:47:03 visual_prompt]: 	Training 200/553. train loss: 4.8412,	0.4831 s / batch. (data: 4.75e-03). ETA=6:48:02, max mem: 11.4 GB 
[10/26 08:48:33 visual_prompt]: 	Training 300/553. train loss: 9.1992,	2.4440 s / batch. (data: 1.95e+00). ETA=1 day, 10:20:09, max mem: 11.4 GB 
[10/26 08:50:01 visual_prompt]: 	Training 400/553. train loss: 1.6134,	0.4840 s / batch. (data: 2.62e-04). ETA=6:47:10, max mem: 11.4 GB 
[10/26 08:51:30 visual_prompt]: 	Training 500/553. train loss: 0.6806,	0.8921 s / batch. (data: 4.14e-01). ETA=12:29:02, max mem: 11.4 GB 
[10/26 08:52:15 visual_prompt]: Epoch 9 / 100: avg data time: 3.95e-01, avg batch time: 0.8893, average train loss: 6.1733
[10/26 08:53:08 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1906, average loss: 5.6075
[10/26 08:53:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 52.27	
[10/26 08:53:08 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/26 08:54:42 visual_prompt]: 	Training 100/553. train loss: 13.9013,	0.4892 s / batch. (data: 1.05e-02). ETA=6:49:27, max mem: 11.4 GB 
[10/26 08:56:10 visual_prompt]: 	Training 200/553. train loss: 22.3663,	0.4789 s / batch. (data: 2.91e-04). ETA=6:40:01, max mem: 11.4 GB 
[10/26 08:57:39 visual_prompt]: 	Training 300/553. train loss: 19.1699,	0.4860 s / batch. (data: 7.96e-03). ETA=6:45:11, max mem: 11.4 GB 
[10/26 08:59:06 visual_prompt]: 	Training 400/553. train loss: 1.4272,	1.2151 s / batch. (data: 7.16e-01). ETA=16:51:01, max mem: 11.4 GB 
[10/26 09:00:36 visual_prompt]: 	Training 500/553. train loss: 1.6888,	1.5482 s / batch. (data: 1.02e+00). ETA=21:25:33, max mem: 11.4 GB 
[10/26 09:01:22 visual_prompt]: Epoch 10 / 100: avg data time: 3.98e-01, avg batch time: 0.8927, average train loss: 7.2269
[10/26 09:02:14 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.1909, average loss: 9.3257
[10/26 09:02:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.87	
[10/26 09:02:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/26 09:03:48 visual_prompt]: 	Training 100/553. train loss: 18.5091,	0.5249 s / batch. (data: 2.69e-04). ETA=7:14:31, max mem: 11.4 GB 
[10/26 09:05:18 visual_prompt]: 	Training 200/553. train loss: 15.5352,	0.4920 s / batch. (data: 7.97e-03). ETA=6:46:28, max mem: 11.4 GB 
[10/26 09:06:46 visual_prompt]: 	Training 300/553. train loss: 9.5833,	2.2800 s / batch. (data: 1.78e+00). ETA=1 day, 7:19:50, max mem: 11.4 GB 
[10/26 09:08:13 visual_prompt]: 	Training 400/553. train loss: 4.9555,	0.4897 s / batch. (data: 2.64e-04). ETA=6:42:55, max mem: 11.4 GB 
[10/26 09:09:41 visual_prompt]: 	Training 500/553. train loss: 5.5005,	0.4880 s / batch. (data: 2.89e-04). ETA=6:40:43, max mem: 11.4 GB 
[10/26 09:10:26 visual_prompt]: Epoch 11 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 8.5324
[10/26 09:11:18 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1946, average loss: 2.4615
[10/26 09:11:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.17	
[10/26 09:11:18 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/26 09:12:52 visual_prompt]: 	Training 100/553. train loss: 3.0686,	0.5234 s / batch. (data: 4.47e-02). ETA=7:08:26, max mem: 11.4 GB 
[10/26 09:14:21 visual_prompt]: 	Training 200/553. train loss: 7.5298,	0.4840 s / batch. (data: 2.27e-04). ETA=6:35:25, max mem: 11.4 GB 
[10/26 09:15:49 visual_prompt]: 	Training 300/553. train loss: 9.2868,	0.5040 s / batch. (data: 4.05e-04). ETA=6:50:52, max mem: 11.4 GB 
[10/26 09:17:17 visual_prompt]: 	Training 400/553. train loss: 4.2577,	0.5040 s / batch. (data: 2.42e-04). ETA=6:50:03, max mem: 11.4 GB 
[10/26 09:18:46 visual_prompt]: 	Training 500/553. train loss: 13.1523,	0.5163 s / batch. (data: 1.22e-02). ETA=6:59:10, max mem: 11.4 GB 
[10/26 09:19:30 visual_prompt]: Epoch 12 / 100: avg data time: 3.96e-01, avg batch time: 0.8903, average train loss: 8.8242
[10/26 09:20:23 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1903, average loss: 0.7064
[10/26 09:20:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.18	
[10/26 09:20:23 visual_prompt]: Best epoch 12: best metric: -0.706
[10/26 09:20:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/26 09:21:56 visual_prompt]: 	Training 100/553. train loss: 0.9207,	0.7308 s / batch. (data: 2.24e-01). ETA=9:51:30, max mem: 11.4 GB 
[10/26 09:23:21 visual_prompt]: 	Training 200/553. train loss: 12.1169,	0.8080 s / batch. (data: 3.22e-01). ETA=10:52:39, max mem: 11.4 GB 
[10/26 09:24:51 visual_prompt]: 	Training 300/553. train loss: 10.4854,	1.3924 s / batch. (data: 8.98e-01). ETA=18:42:22, max mem: 11.4 GB 
[10/26 09:26:19 visual_prompt]: 	Training 400/553. train loss: 24.4974,	0.5214 s / batch. (data: 2.14e-02). ETA=6:59:26, max mem: 11.4 GB 
[10/26 09:27:49 visual_prompt]: 	Training 500/553. train loss: 10.5142,	0.5365 s / batch. (data: 1.65e-02). ETA=7:10:40, max mem: 11.4 GB 
[10/26 09:28:34 visual_prompt]: Epoch 13 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 7.3220
[10/26 09:29:26 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1915, average loss: 1.0776
[10/26 09:29:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.31	
[10/26 09:29:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/26 09:30:59 visual_prompt]: 	Training 100/553. train loss: 1.2507,	0.5038 s / batch. (data: 8.64e-03). ETA=6:43:08, max mem: 11.4 GB 
[10/26 09:32:27 visual_prompt]: 	Training 200/553. train loss: 0.0113,	0.8119 s / batch. (data: 3.23e-01). ETA=10:48:17, max mem: 11.4 GB 
[10/26 09:33:56 visual_prompt]: 	Training 300/553. train loss: 10.4170,	0.9638 s / batch. (data: 4.66e-01). ETA=12:48:00, max mem: 11.4 GB 
[10/26 09:35:25 visual_prompt]: 	Training 400/553. train loss: 5.8681,	0.5079 s / batch. (data: 5.43e-03). ETA=6:43:54, max mem: 11.4 GB 
[10/26 09:36:53 visual_prompt]: 	Training 500/553. train loss: 5.3308,	0.9000 s / batch. (data: 4.16e-01). ETA=11:54:11, max mem: 11.4 GB 
[10/26 09:37:38 visual_prompt]: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 7.9697
[10/26 09:38:30 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1916, average loss: 12.6356
[10/26 09:38:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.30	
[10/26 09:38:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/26 09:40:02 visual_prompt]: 	Training 100/553. train loss: 1.3250,	0.5120 s / batch. (data: 2.84e-04). ETA=6:44:59, max mem: 11.4 GB 
[10/26 09:41:29 visual_prompt]: 	Training 200/553. train loss: 28.6789,	0.4842 s / batch. (data: 5.38e-03). ETA=6:22:12, max mem: 11.4 GB 
[10/26 09:42:59 visual_prompt]: 	Training 300/553. train loss: 0.6941,	0.5040 s / batch. (data: 2.79e-04). ETA=6:36:57, max mem: 11.4 GB 
[10/26 09:44:26 visual_prompt]: 	Training 400/553. train loss: 27.8322,	0.5004 s / batch. (data: 7.95e-03). ETA=6:33:16, max mem: 11.4 GB 
[10/26 09:45:55 visual_prompt]: 	Training 500/553. train loss: 7.9541,	0.5120 s / batch. (data: 3.16e-04). ETA=6:41:33, max mem: 11.4 GB 
[10/26 09:46:42 visual_prompt]: Epoch 15 / 100: avg data time: 3.95e-01, avg batch time: 0.8890, average train loss: 7.9133
[10/26 09:47:34 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1922, average loss: 1.5777
[10/26 09:47:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.80	
[10/26 09:47:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/26 09:49:05 visual_prompt]: 	Training 100/553. train loss: 44.4663,	0.5000 s / batch. (data: 2.64e-04). ETA=6:30:52, max mem: 11.4 GB 
[10/26 09:50:34 visual_prompt]: 	Training 200/553. train loss: 5.5277,	0.4921 s / batch. (data: 7.96e-03). ETA=6:23:51, max mem: 11.4 GB 
[10/26 09:52:03 visual_prompt]: 	Training 300/553. train loss: 9.1834,	0.5054 s / batch. (data: 6.79e-04). ETA=6:33:24, max mem: 11.4 GB 
[10/26 09:53:32 visual_prompt]: 	Training 400/553. train loss: 1.2699,	0.4846 s / batch. (data: 2.54e-04). ETA=6:16:22, max mem: 11.4 GB 
[10/26 09:55:00 visual_prompt]: 	Training 500/553. train loss: 0.6244,	1.7400 s / batch. (data: 1.25e+00). ETA=22:28:38, max mem: 11.4 GB 
[10/26 09:55:45 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 0.8880, average train loss: 7.5677
[10/26 09:56:38 visual_prompt]: Inference (val):avg data time: 3.83e-04, avg batch time: 0.1932, average loss: 1.0477
[10/26 09:56:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.30	
[10/26 09:56:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/26 09:58:09 visual_prompt]: 	Training 100/553. train loss: 3.0461,	0.4910 s / batch. (data: 2.34e-04). ETA=6:19:19, max mem: 11.4 GB 
[10/26 09:59:39 visual_prompt]: 	Training 200/553. train loss: 13.2987,	0.5000 s / batch. (data: 2.38e-04). ETA=6:25:26, max mem: 11.4 GB 
[10/26 10:01:07 visual_prompt]: 	Training 300/553. train loss: 7.8428,	0.5046 s / batch. (data: 1.04e-02). ETA=6:28:06, max mem: 11.4 GB 
[10/26 10:02:35 visual_prompt]: 	Training 400/553. train loss: 10.4371,	0.6120 s / batch. (data: 1.23e-01). ETA=7:49:43, max mem: 11.4 GB 
[10/26 10:04:04 visual_prompt]: 	Training 500/553. train loss: 1.3021,	2.1828 s / batch. (data: 1.67e+00). ETA=1 day, 3:51:43, max mem: 11.4 GB 
[10/26 10:04:50 visual_prompt]: Epoch 17 / 100: avg data time: 3.96e-01, avg batch time: 0.8906, average train loss: 7.9136
[10/26 10:05:43 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.1903, average loss: 46.9238
[10/26 10:05:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.25	
[10/26 10:05:43 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/26 10:07:15 visual_prompt]: 	Training 100/553. train loss: 3.0595,	0.4786 s / batch. (data: 2.74e-04). ETA=6:05:19, max mem: 11.4 GB 
[10/26 10:08:46 visual_prompt]: 	Training 200/553. train loss: 9.9311,	0.5442 s / batch. (data: 5.92e-03). ETA=6:54:28, max mem: 11.4 GB 
[10/26 10:10:15 visual_prompt]: 	Training 300/553. train loss: 21.3376,	0.5009 s / batch. (data: 2.54e-04). ETA=6:20:42, max mem: 11.4 GB 
[10/26 10:11:43 visual_prompt]: 	Training 400/553. train loss: 2.1421,	0.4920 s / batch. (data: 2.75e-04). ETA=6:13:05, max mem: 11.4 GB 
[10/26 10:13:11 visual_prompt]: 	Training 500/553. train loss: 1.2694,	0.6048 s / batch. (data: 1.27e-01). ETA=7:37:38, max mem: 11.4 GB 
[10/26 10:13:55 visual_prompt]: Epoch 18 / 100: avg data time: 3.95e-01, avg batch time: 0.8905, average train loss: 7.9391
[10/26 10:14:48 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1925, average loss: 0.8273
[10/26 10:14:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.25	
[10/26 10:14:48 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/26 10:16:19 visual_prompt]: 	Training 100/553. train loss: 6.5420,	0.4920 s / batch. (data: 7.96e-03). ETA=6:11:01, max mem: 11.4 GB 
[10/26 10:17:48 visual_prompt]: 	Training 200/553. train loss: 5.1704,	0.5037 s / batch. (data: 5.39e-03). ETA=6:19:00, max mem: 11.4 GB 
[10/26 10:19:17 visual_prompt]: 	Training 300/553. train loss: 1.7043,	0.6408 s / batch. (data: 1.39e-01). ETA=8:01:07, max mem: 11.4 GB 
[10/26 10:20:47 visual_prompt]: 	Training 400/553. train loss: 4.1694,	0.5483 s / batch. (data: 3.24e-02). ETA=6:50:43, max mem: 11.4 GB 
[10/26 10:22:12 visual_prompt]: 	Training 500/553. train loss: 3.5782,	0.4845 s / batch. (data: 5.37e-03). ETA=6:02:06, max mem: 11.4 GB 
[10/26 10:22:58 visual_prompt]: Epoch 19 / 100: avg data time: 3.91e-01, avg batch time: 0.8858, average train loss: 7.3843
[10/26 10:23:50 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1898, average loss: 16.2746
[10/26 10:23:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.93	
[10/26 10:23:50 visual_prompt]: Stopping early.
[10/26 10:23:50 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 10:23:50 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 10:23:50 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 10:23:50 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 10:23:50 visual_prompt]: Training with config:
[10/26 10:23:50 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr2.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 10:23:50 visual_prompt]: Loading training data...
[10/26 10:23:50 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 10:23:50 visual_prompt]: Loading validation data...
[10/26 10:23:50 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 10:23:50 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/26 10:23:53 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/26 10:23:53 visual_prompt]: tuned percent:0.529
[10/26 10:23:53 visual_prompt]: Device used for model: 0
[10/26 10:23:53 visual_prompt]: Setting up Evaluator...
[10/26 10:23:53 visual_prompt]: Setting up Trainer...
[10/26 10:23:53 visual_prompt]: 	Setting up the optimizer...
[10/26 10:23:53 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 10:25:24 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4842 s / batch. (data: 2.67e-04). ETA=7:25:25, max mem: 11.4 GB 
[10/26 10:26:51 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4784 s / batch. (data: 2.51e-04). ETA=7:19:18, max mem: 11.4 GB 
[10/26 10:28:23 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9485 s / batch. (data: 2.47e+00). ETA=1 day, 21:02:44, max mem: 11.4 GB 
[10/26 10:29:50 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4921 s / batch. (data: 2.72e-04). ETA=7:30:15, max mem: 11.4 GB 
[10/26 10:31:20 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5200 s / batch. (data: 5.94e-03). ETA=7:54:55, max mem: 11.4 GB 
[10/26 10:32:07 visual_prompt]: Epoch 1 / 100: avg data time: 3.98e-01, avg batch time: 0.8925, average train loss: 1.3966
[10/26 10:32:59 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1907, average loss: 1.3454
[10/26 10:32:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/26 10:32:59 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/26 10:34:30 visual_prompt]: 	Training 100/553. train loss: 1.6031,	0.5000 s / batch. (data: 7.98e-03). ETA=7:35:24, max mem: 11.4 GB 
[10/26 10:35:59 visual_prompt]: 	Training 200/553. train loss: 0.5786,	0.5080 s / batch. (data: 5.92e-03). ETA=7:41:50, max mem: 11.4 GB 
[10/26 10:37:30 visual_prompt]: 	Training 300/553. train loss: 1.2948,	1.7254 s / batch. (data: 1.25e+00). ETA=1 day, 2:05:41, max mem: 11.4 GB 
[10/26 10:38:57 visual_prompt]: 	Training 400/553. train loss: 0.5476,	0.4952 s / batch. (data: 4.85e-03). ETA=7:28:31, max mem: 11.4 GB 
[10/26 10:40:28 visual_prompt]: 	Training 500/553. train loss: 0.6693,	0.5040 s / batch. (data: 1.20e-02). ETA=7:35:37, max mem: 11.4 GB 
[10/26 10:41:13 visual_prompt]: Epoch 2 / 100: avg data time: 3.98e-01, avg batch time: 0.8923, average train loss: 1.2277
[10/26 10:42:06 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1900, average loss: 2.4743
[10/26 10:42:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.23	
[10/26 10:42:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/26 10:43:37 visual_prompt]: 	Training 100/553. train loss: 0.9705,	0.4840 s / batch. (data: 2.78e-04). ETA=7:16:20, max mem: 11.4 GB 
[10/26 10:45:07 visual_prompt]: 	Training 200/553. train loss: 0.7191,	0.4921 s / batch. (data: 7.97e-03). ETA=7:22:48, max mem: 11.4 GB 
[10/26 10:46:34 visual_prompt]: 	Training 300/553. train loss: 1.3549,	0.4919 s / batch. (data: 1.04e-02). ETA=7:21:50, max mem: 11.4 GB 
[10/26 10:48:05 visual_prompt]: 	Training 400/553. train loss: 0.3631,	0.5040 s / batch. (data: 2.72e-04). ETA=7:31:52, max mem: 11.4 GB 
[10/26 10:49:35 visual_prompt]: 	Training 500/553. train loss: 0.6062,	2.0080 s / batch. (data: 1.50e+00). ETA=1 day, 5:56:58, max mem: 11.4 GB 
[10/26 10:50:19 visual_prompt]: Epoch 3 / 100: avg data time: 3.97e-01, avg batch time: 0.8916, average train loss: 1.4150
[10/26 10:51:11 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1910, average loss: 2.5414
[10/26 10:51:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.31	
[10/26 10:51:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/26 10:52:44 visual_prompt]: 	Training 100/553. train loss: 2.1251,	0.4879 s / batch. (data: 2.78e-04). ETA=7:15:20, max mem: 11.4 GB 
[10/26 10:54:13 visual_prompt]: 	Training 200/553. train loss: 1.2496,	0.4936 s / batch. (data: 5.40e-03). ETA=7:19:38, max mem: 11.4 GB 
[10/26 10:55:42 visual_prompt]: 	Training 300/553. train loss: 1.0014,	1.9531 s / batch. (data: 1.47e+00). ETA=1 day, 4:56:20, max mem: 11.4 GB 
[10/26 10:57:08 visual_prompt]: 	Training 400/553. train loss: 1.6388,	1.9679 s / batch. (data: 1.49e+00). ETA=1 day, 5:06:10, max mem: 11.4 GB 
[10/26 10:58:38 visual_prompt]: 	Training 500/553. train loss: 2.7141,	3.7204 s / batch. (data: 3.24e+00). ETA=2 days, 6:55:07, max mem: 11.4 GB 
[10/26 10:59:23 visual_prompt]: Epoch 4 / 100: avg data time: 3.95e-01, avg batch time: 0.8896, average train loss: 1.5488
[10/26 11:00:16 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1907, average loss: 1.1650
[10/26 11:00:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.14	
[10/26 11:00:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/26 11:01:47 visual_prompt]: 	Training 100/553. train loss: 0.0013,	0.4977 s / batch. (data: 5.40e-03). ETA=7:19:33, max mem: 11.4 GB 
[10/26 11:03:16 visual_prompt]: 	Training 200/553. train loss: 0.8817,	1.8068 s / batch. (data: 1.33e+00). ETA=1 day, 2:32:38, max mem: 11.4 GB 
[10/26 11:04:45 visual_prompt]: 	Training 300/553. train loss: 9.8945,	0.4914 s / batch. (data: 2.66e-04). ETA=7:12:19, max mem: 11.4 GB 
[10/26 11:06:13 visual_prompt]: 	Training 400/553. train loss: 8.6966,	0.5083 s / batch. (data: 8.17e-03). ETA=7:26:18, max mem: 11.4 GB 
[10/26 11:07:42 visual_prompt]: 	Training 500/553. train loss: 0.9714,	0.5080 s / batch. (data: 7.96e-03). ETA=7:25:14, max mem: 11.4 GB 
[10/26 11:08:28 visual_prompt]: Epoch 5 / 100: avg data time: 3.96e-01, avg batch time: 0.8899, average train loss: 2.5512
[10/26 11:09:20 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1911, average loss: 1.1397
[10/26 11:09:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.36	
[10/26 11:09:20 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/26 11:10:53 visual_prompt]: 	Training 100/553. train loss: 4.7426,	0.4914 s / batch. (data: 6.95e-04). ETA=7:09:28, max mem: 11.4 GB 
[10/26 11:12:21 visual_prompt]: 	Training 200/553. train loss: 1.3344,	0.4988 s / batch. (data: 2.68e-04). ETA=7:15:02, max mem: 11.4 GB 
[10/26 11:13:49 visual_prompt]: 	Training 300/553. train loss: 3.1186,	0.5004 s / batch. (data: 2.56e-04). ETA=7:15:38, max mem: 11.4 GB 
[10/26 11:15:20 visual_prompt]: 	Training 400/553. train loss: 3.5189,	0.4960 s / batch. (data: 1.17e-02). ETA=7:10:59, max mem: 11.4 GB 
[10/26 11:16:48 visual_prompt]: 	Training 500/553. train loss: 3.0002,	1.4474 s / batch. (data: 9.71e-01). ETA=20:55:15, max mem: 11.4 GB 
[10/26 11:17:33 visual_prompt]: Epoch 6 / 100: avg data time: 3.97e-01, avg batch time: 0.8906, average train loss: 3.8572
[10/26 11:18:26 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1899, average loss: 5.3234
[10/26 11:18:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.09	
[10/26 11:18:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/26 11:19:56 visual_prompt]: 	Training 100/553. train loss: 13.0001,	0.5133 s / batch. (data: 2.55e-02). ETA=7:23:49, max mem: 11.4 GB 
[10/26 11:21:26 visual_prompt]: 	Training 200/553. train loss: 3.1703,	0.4784 s / batch. (data: 2.62e-04). ETA=6:52:50, max mem: 11.4 GB 
[10/26 11:22:58 visual_prompt]: 	Training 300/553. train loss: 0.7044,	2.3053 s / batch. (data: 1.83e+00). ETA=1 day, 9:05:44, max mem: 11.4 GB 
[10/26 11:24:27 visual_prompt]: 	Training 400/553. train loss: 0.6574,	2.4611 s / batch. (data: 1.96e+00). ETA=1 day, 11:15:48, max mem: 11.4 GB 
[10/26 11:25:54 visual_prompt]: 	Training 500/553. train loss: 4.9946,	0.4909 s / batch. (data: 2.95e-04). ETA=7:01:13, max mem: 11.4 GB 
[10/26 11:26:39 visual_prompt]: Epoch 7 / 100: avg data time: 3.98e-01, avg batch time: 0.8917, average train loss: 3.5662
[10/26 11:27:32 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1908, average loss: 2.4215
[10/26 11:27:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.40	
[10/26 11:27:32 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/26 11:29:01 visual_prompt]: 	Training 100/553. train loss: 3.4315,	0.5080 s / batch. (data: 3.09e-04). ETA=7:14:36, max mem: 11.4 GB 
[10/26 11:30:31 visual_prompt]: 	Training 200/553. train loss: 8.0953,	0.4997 s / batch. (data: 2.63e-04). ETA=7:06:36, max mem: 11.4 GB 
[10/26 11:32:01 visual_prompt]: 	Training 300/553. train loss: 4.1322,	0.4960 s / batch. (data: 7.96e-03). ETA=7:02:39, max mem: 11.4 GB 
[10/26 11:33:30 visual_prompt]: 	Training 400/553. train loss: 2.6836,	1.2920 s / batch. (data: 8.10e-01). ETA=18:18:51, max mem: 11.4 GB 
[10/26 11:34:59 visual_prompt]: 	Training 500/553. train loss: 27.8427,	1.9769 s / batch. (data: 1.48e+00). ETA=1 day, 3:58:02, max mem: 11.4 GB 
[10/26 11:35:44 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 4.8670
[10/26 11:36:36 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1915, average loss: 2.5112
[10/26 11:36:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.26	
[10/26 11:36:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/26 11:38:08 visual_prompt]: 	Training 100/553. train loss: 5.2965,	0.4909 s / batch. (data: 2.66e-04). ETA=6:55:25, max mem: 11.4 GB 
[10/26 11:39:36 visual_prompt]: 	Training 200/553. train loss: 1.7411,	0.4799 s / batch. (data: 2.76e-04). ETA=6:45:19, max mem: 11.4 GB 
[10/26 11:41:05 visual_prompt]: 	Training 300/553. train loss: 1.6305,	2.0276 s / batch. (data: 1.53e+00). ETA=1 day, 4:29:08, max mem: 11.4 GB 
[10/26 11:42:34 visual_prompt]: 	Training 400/553. train loss: 0.6364,	0.4963 s / batch. (data: 2.60e-04). ETA=6:57:30, max mem: 11.4 GB 
[10/26 11:44:04 visual_prompt]: 	Training 500/553. train loss: 2.4896,	1.4255 s / batch. (data: 9.27e-01). ETA=19:56:52, max mem: 11.4 GB 
[10/26 11:44:48 visual_prompt]: Epoch 9 / 100: avg data time: 3.94e-01, avg batch time: 0.8884, average train loss: 3.7744
[10/26 11:45:40 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1904, average loss: 7.0304
[10/26 11:45:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.40	
[10/26 11:45:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/26 11:47:14 visual_prompt]: 	Training 100/553. train loss: 5.8534,	0.4927 s / batch. (data: 5.37e-03). ETA=6:52:25, max mem: 11.4 GB 
[10/26 11:48:42 visual_prompt]: 	Training 200/553. train loss: 1.3331,	0.5097 s / batch. (data: 1.05e-02). ETA=7:05:47, max mem: 11.4 GB 
[10/26 11:50:11 visual_prompt]: 	Training 300/553. train loss: 1.7660,	1.1440 s / batch. (data: 6.50e-01). ETA=15:53:46, max mem: 11.4 GB 
[10/26 11:51:38 visual_prompt]: 	Training 400/553. train loss: 10.4030,	1.3507 s / batch. (data: 8.74e-01). ETA=18:43:51, max mem: 11.4 GB 
[10/26 11:53:09 visual_prompt]: 	Training 500/553. train loss: 0.7758,	1.7205 s / batch. (data: 1.22e+00). ETA=23:48:38, max mem: 11.4 GB 
[10/26 11:53:54 visual_prompt]: Epoch 10 / 100: avg data time: 4.00e-01, avg batch time: 0.8943, average train loss: 7.1393
[10/26 11:54:47 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1906, average loss: 4.1381
[10/26 11:54:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.61	
[10/26 11:54:47 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/26 11:56:22 visual_prompt]: 	Training 100/553. train loss: 3.2710,	0.4994 s / batch. (data: 2.89e-04). ETA=6:53:27, max mem: 11.4 GB 
[10/26 11:57:52 visual_prompt]: 	Training 200/553. train loss: 4.5540,	0.5200 s / batch. (data: 7.96e-03). ETA=7:09:36, max mem: 11.4 GB 
[10/26 11:59:22 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.0039 s / batch. (data: 5.09e-01). ETA=13:47:44, max mem: 11.4 GB 
[10/26 12:00:56 visual_prompt]: 	Training 400/553. train loss: 1.5607,	0.4951 s / batch. (data: 3.79e-03). ETA=6:47:23, max mem: 11.4 GB 
[10/26 12:02:26 visual_prompt]: 	Training 500/553. train loss: 11.1762,	2.4381 s / batch. (data: 1.95e+00). ETA=1 day, 9:22:05, max mem: 11.4 GB 
[10/26 12:03:20 visual_prompt]: Epoch 11 / 100: avg data time: 4.33e-01, avg batch time: 0.9268, average train loss: 5.2384
[10/26 12:04:19 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1896, average loss: 8.7784
[10/26 12:04:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.33	
[10/26 12:04:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/26 12:05:53 visual_prompt]: 	Training 100/553. train loss: 10.7005,	0.5136 s / batch. (data: 1.56e-02). ETA=7:00:27, max mem: 11.4 GB 
[10/26 12:07:23 visual_prompt]: 	Training 200/553. train loss: 8.0770,	0.4915 s / batch. (data: 1.15e-02). ETA=6:41:32, max mem: 11.4 GB 
[10/26 12:08:50 visual_prompt]: 	Training 300/553. train loss: 4.6919,	0.5104 s / batch. (data: 2.70e-04). ETA=6:56:09, max mem: 11.4 GB 
[10/26 12:10:19 visual_prompt]: 	Training 400/553. train loss: 18.3369,	0.5154 s / batch. (data: 3.63e-04). ETA=6:59:18, max mem: 11.4 GB 
[10/26 12:11:49 visual_prompt]: 	Training 500/553. train loss: 24.8271,	0.5065 s / batch. (data: 5.41e-03). ETA=6:51:15, max mem: 11.4 GB 
[10/26 12:12:33 visual_prompt]: Epoch 12 / 100: avg data time: 3.99e-01, avg batch time: 0.8921, average train loss: 9.2225
[10/26 12:13:25 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1908, average loss: 45.7897
[10/26 12:13:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.58	
[10/26 12:13:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/26 12:14:58 visual_prompt]: 	Training 100/553. train loss: 3.1674,	0.4961 s / batch. (data: 8.44e-03). ETA=6:41:31, max mem: 11.4 GB 
[10/26 12:16:24 visual_prompt]: 	Training 200/553. train loss: 10.9213,	0.4870 s / batch. (data: 2.71e-04). ETA=6:33:24, max mem: 11.4 GB 
[10/26 12:17:54 visual_prompt]: 	Training 300/553. train loss: 4.4136,	2.3839 s / batch. (data: 1.88e+00). ETA=1 day, 8:01:36, max mem: 11.4 GB 
[10/26 12:19:21 visual_prompt]: 	Training 400/553. train loss: 15.5523,	0.4798 s / batch. (data: 2.42e-04). ETA=6:25:56, max mem: 11.4 GB 
[10/26 12:20:51 visual_prompt]: 	Training 500/553. train loss: 12.4670,	0.5240 s / batch. (data: 7.97e-03). ETA=7:00:37, max mem: 11.4 GB 
[10/26 12:21:37 visual_prompt]: Epoch 13 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 7.5923
[10/26 12:22:29 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1910, average loss: 2.7581
[10/26 12:22:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.84	
[10/26 12:22:29 visual_prompt]: Best epoch 13: best metric: -2.758
[10/26 12:22:29 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/26 12:24:01 visual_prompt]: 	Training 100/553. train loss: 1.8725,	0.4960 s / batch. (data: 2.82e-04). ETA=6:36:54, max mem: 11.4 GB 
[10/26 12:25:31 visual_prompt]: 	Training 200/553. train loss: 12.4579,	1.9040 s / batch. (data: 1.40e+00). ETA=1 day, 1:20:22, max mem: 11.4 GB 
[10/26 12:26:59 visual_prompt]: 	Training 300/553. train loss: 4.6693,	1.4400 s / batch. (data: 9.57e-01). ETA=19:07:28, max mem: 11.4 GB 
[10/26 12:28:27 visual_prompt]: 	Training 400/553. train loss: 1.5424,	1.1485 s / batch. (data: 6.58e-01). ETA=15:13:17, max mem: 11.4 GB 
[10/26 12:29:56 visual_prompt]: 	Training 500/553. train loss: 0.9859,	1.7079 s / batch. (data: 1.21e+00). ETA=22:35:14, max mem: 11.4 GB 
[10/26 12:30:40 visual_prompt]: Epoch 14 / 100: avg data time: 3.93e-01, avg batch time: 0.8870, average train loss: 7.0901
[10/26 12:31:32 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1901, average loss: 7.6059
[10/26 12:31:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.90	
[10/26 12:31:32 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/26 12:33:04 visual_prompt]: 	Training 100/553. train loss: 0.6470,	0.4977 s / batch. (data: 5.39e-03). ETA=6:33:37, max mem: 11.4 GB 
[10/26 12:34:32 visual_prompt]: 	Training 200/553. train loss: 6.8782,	0.4924 s / batch. (data: 2.65e-04). ETA=6:28:38, max mem: 11.4 GB 
[10/26 12:36:01 visual_prompt]: 	Training 300/553. train loss: 3.8304,	0.4960 s / batch. (data: 2.68e-04). ETA=6:30:39, max mem: 11.4 GB 
[10/26 12:37:28 visual_prompt]: 	Training 400/553. train loss: 0.7193,	0.4821 s / batch. (data: 2.85e-04). ETA=6:18:52, max mem: 11.4 GB 
[10/26 12:38:58 visual_prompt]: 	Training 500/553. train loss: 0.7361,	0.5080 s / batch. (data: 7.94e-03). ETA=6:38:24, max mem: 11.4 GB 
[10/26 12:39:44 visual_prompt]: Epoch 15 / 100: avg data time: 3.96e-01, avg batch time: 0.8900, average train loss: 7.5467
[10/26 12:40:37 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1901, average loss: 2.7484
[10/26 12:40:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.61	
[10/26 12:40:37 visual_prompt]: Best epoch 15: best metric: -2.748
[10/26 12:40:37 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/26 12:42:07 visual_prompt]: 	Training 100/553. train loss: 7.7091,	0.4944 s / batch. (data: 2.69e-04). ETA=6:26:28, max mem: 11.4 GB 
[10/26 12:43:37 visual_prompt]: 	Training 200/553. train loss: 1.1596,	0.4867 s / batch. (data: 2.64e-04). ETA=6:19:39, max mem: 11.4 GB 
[10/26 12:45:06 visual_prompt]: 	Training 300/553. train loss: 0.5110,	0.4960 s / batch. (data: 2.77e-04). ETA=6:26:07, max mem: 11.4 GB 
[10/26 12:46:35 visual_prompt]: 	Training 400/553. train loss: 0.8581,	0.5052 s / batch. (data: 1.55e-02). ETA=6:32:26, max mem: 11.4 GB 
[10/26 12:48:04 visual_prompt]: 	Training 500/553. train loss: 1.6353,	2.0076 s / batch. (data: 1.53e+00). ETA=1 day, 1:56:05, max mem: 11.4 GB 
[10/26 12:48:50 visual_prompt]: Epoch 16 / 100: avg data time: 3.99e-01, avg batch time: 0.8922, average train loss: 6.2696
[10/26 12:49:43 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1923, average loss: 6.7182
[10/26 12:49:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.03	
[10/26 12:49:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/26 12:51:14 visual_prompt]: 	Training 100/553. train loss: 1.1295,	0.5009 s / batch. (data: 3.14e-04). ETA=6:26:59, max mem: 11.4 GB 
[10/26 12:52:45 visual_prompt]: 	Training 200/553. train loss: 20.1746,	0.4803 s / batch. (data: 2.52e-04). ETA=6:10:16, max mem: 11.4 GB 
[10/26 12:54:13 visual_prompt]: 	Training 300/553. train loss: 1.0634,	0.5072 s / batch. (data: 7.96e-03). ETA=6:30:09, max mem: 11.4 GB 
[10/26 12:55:41 visual_prompt]: 	Training 400/553. train loss: 0.8616,	0.4921 s / batch. (data: 2.67e-04). ETA=6:17:41, max mem: 11.4 GB 
[10/26 12:57:09 visual_prompt]: 	Training 500/553. train loss: 1.3910,	1.8118 s / batch. (data: 1.33e+00). ETA=23:07:37, max mem: 11.4 GB 
[10/26 12:57:55 visual_prompt]: Epoch 17 / 100: avg data time: 3.95e-01, avg batch time: 0.8908, average train loss: 6.5091
[10/26 12:58:48 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1908, average loss: 2.0199
[10/26 12:58:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.94	
[10/26 12:58:48 visual_prompt]: Best epoch 17: best metric: -2.020
[10/26 12:58:48 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/26 13:00:20 visual_prompt]: 	Training 100/553. train loss: 13.9041,	0.5010 s / batch. (data: 2.64e-04). ETA=6:22:26, max mem: 11.4 GB 
[10/26 13:01:51 visual_prompt]: 	Training 200/553. train loss: 5.7227,	0.5160 s / batch. (data: 2.70e-04). ETA=6:32:59, max mem: 11.4 GB 
[10/26 13:03:20 visual_prompt]: 	Training 300/553. train loss: 0.6380,	0.4779 s / batch. (data: 2.75e-04). ETA=6:03:10, max mem: 11.4 GB 
[10/26 13:04:48 visual_prompt]: 	Training 400/553. train loss: 7.4154,	0.4965 s / batch. (data: 9.41e-03). ETA=6:16:31, max mem: 11.4 GB 
[10/26 13:06:16 visual_prompt]: 	Training 500/553. train loss: 14.9058,	0.5124 s / batch. (data: 2.05e-02). ETA=6:27:40, max mem: 11.4 GB 
[10/26 13:07:00 visual_prompt]: Epoch 18 / 100: avg data time: 3.96e-01, avg batch time: 0.8898, average train loss: 7.2638
[10/26 13:07:52 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1897, average loss: 2.5984
[10/26 13:07:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.15	
[10/26 13:07:52 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/26 13:09:25 visual_prompt]: 	Training 100/553. train loss: 1.2330,	1.3355 s / batch. (data: 8.41e-01). ETA=16:47:07, max mem: 11.4 GB 
[10/26 13:10:54 visual_prompt]: 	Training 200/553. train loss: 4.4217,	0.5160 s / batch. (data: 2.77e-04). ETA=6:28:14, max mem: 11.4 GB 
[10/26 13:12:22 visual_prompt]: 	Training 300/553. train loss: 0.0053,	0.5920 s / batch. (data: 9.78e-02). ETA=7:24:29, max mem: 11.4 GB 
[10/26 13:13:52 visual_prompt]: 	Training 400/553. train loss: 2.8873,	0.4876 s / batch. (data: 2.64e-04). ETA=6:05:15, max mem: 11.4 GB 
[10/26 13:15:18 visual_prompt]: 	Training 500/553. train loss: 0.8346,	0.4960 s / batch. (data: 2.91e-04). ETA=6:10:42, max mem: 11.4 GB 
[10/26 13:16:05 visual_prompt]: Epoch 19 / 100: avg data time: 3.96e-01, avg batch time: 0.8906, average train loss: 6.4244
[10/26 13:16:57 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1924, average loss: 28.4681
[10/26 13:16:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.30	
[10/26 13:16:57 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[10/26 13:18:27 visual_prompt]: 	Training 100/553. train loss: 5.8433,	0.4960 s / batch. (data: 2.82e-04). ETA=6:09:29, max mem: 11.4 GB 
[10/26 13:19:57 visual_prompt]: 	Training 200/553. train loss: 1.5065,	0.5200 s / batch. (data: 7.94e-03). ETA=6:26:28, max mem: 11.4 GB 
[10/26 13:21:26 visual_prompt]: 	Training 300/553. train loss: 6.1680,	0.5131 s / batch. (data: 2.08e-02). ETA=6:20:29, max mem: 11.4 GB 
[10/26 13:22:54 visual_prompt]: 	Training 400/553. train loss: 2.2468,	0.5080 s / batch. (data: 2.82e-04). ETA=6:15:50, max mem: 11.4 GB 
[10/26 13:24:22 visual_prompt]: 	Training 500/553. train loss: 4.9295,	0.4953 s / batch. (data: 2.53e-04). ETA=6:05:37, max mem: 11.4 GB 
[10/26 13:25:09 visual_prompt]: Epoch 20 / 100: avg data time: 3.95e-01, avg batch time: 0.8895, average train loss: 6.1089
[10/26 13:26:02 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1905, average loss: 0.8215
[10/26 13:26:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.32	
[10/26 13:26:02 visual_prompt]: Best epoch 20: best metric: -0.822
[10/26 13:26:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[10/26 13:27:36 visual_prompt]: 	Training 100/553. train loss: 5.9459,	0.4960 s / batch. (data: 2.83e-04). ETA=6:04:53, max mem: 11.4 GB 
[10/26 13:29:04 visual_prompt]: 	Training 200/553. train loss: 4.5317,	0.4881 s / batch. (data: 5.36e-03). ETA=5:58:14, max mem: 11.4 GB 
[10/26 13:30:31 visual_prompt]: 	Training 300/553. train loss: 33.2589,	0.6770 s / batch. (data: 1.84e-01). ETA=8:15:46, max mem: 11.4 GB 
[10/26 13:31:59 visual_prompt]: 	Training 400/553. train loss: 16.5574,	0.5280 s / batch. (data: 2.89e-04). ETA=6:25:46, max mem: 11.4 GB 
[10/26 13:33:30 visual_prompt]: 	Training 500/553. train loss: 1.5533,	0.4884 s / batch. (data: 8.00e-03). ETA=5:56:02, max mem: 11.4 GB 
[10/26 13:34:14 visual_prompt]: Epoch 21 / 100: avg data time: 3.96e-01, avg batch time: 0.8899, average train loss: 5.9609
[10/26 13:35:06 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1913, average loss: 4.3375
[10/26 13:35:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.78	
[10/26 13:35:06 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[10/26 13:36:37 visual_prompt]: 	Training 100/553. train loss: 5.7389,	0.4880 s / batch. (data: 2.69e-04). ETA=5:54:29, max mem: 11.4 GB 
[10/26 13:38:06 visual_prompt]: 	Training 200/553. train loss: 11.7302,	0.5040 s / batch. (data: 2.57e-04). ETA=6:05:18, max mem: 11.4 GB 
[10/26 13:39:33 visual_prompt]: 	Training 300/553. train loss: 0.0004,	0.4776 s / batch. (data: 2.83e-04). ETA=5:45:22, max mem: 11.4 GB 
[10/26 13:41:01 visual_prompt]: 	Training 400/553. train loss: 19.2003,	0.4880 s / batch. (data: 2.58e-04). ETA=5:52:04, max mem: 11.4 GB 
[10/26 13:42:31 visual_prompt]: 	Training 500/553. train loss: 3.1471,	0.5120 s / batch. (data: 2.67e-04). ETA=6:08:32, max mem: 11.4 GB 
[10/26 13:43:18 visual_prompt]: Epoch 22 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 6.4805
[10/26 13:44:11 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1906, average loss: 3.1618
[10/26 13:44:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.95	
[10/26 13:44:11 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[10/26 13:45:45 visual_prompt]: 	Training 100/553. train loss: 17.9398,	0.9353 s / batch. (data: 4.36e-01). ETA=11:10:50, max mem: 11.4 GB 
[10/26 13:47:15 visual_prompt]: 	Training 200/553. train loss: 1.5598,	1.1600 s / batch. (data: 6.60e-01). ETA=13:50:03, max mem: 11.4 GB 
[10/26 13:48:45 visual_prompt]: 	Training 300/553. train loss: 2.5884,	0.4928 s / batch. (data: 3.59e-04). ETA=5:51:49, max mem: 11.4 GB 
[10/26 13:50:12 visual_prompt]: 	Training 400/553. train loss: 4.0665,	0.5159 s / batch. (data: 7.94e-04). ETA=6:07:28, max mem: 11.4 GB 
[10/26 13:51:39 visual_prompt]: 	Training 500/553. train loss: 4.9138,	0.5080 s / batch. (data: 2.65e-04). ETA=6:00:58, max mem: 11.4 GB 
[10/26 13:52:25 visual_prompt]: Epoch 23 / 100: avg data time: 4.00e-01, avg batch time: 0.8935, average train loss: 6.8762
[10/26 13:53:18 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.1905, average loss: 3.8412
[10/26 13:53:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.13	
[10/26 13:53:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[10/26 13:54:47 visual_prompt]: 	Training 100/553. train loss: 1.7864,	0.5160 s / batch. (data: 1.55e-02). ETA=6:05:18, max mem: 11.4 GB 
[10/26 13:56:16 visual_prompt]: 	Training 200/553. train loss: 1.1908,	0.4880 s / batch. (data: 2.68e-04). ETA=5:44:42, max mem: 11.4 GB 
[10/26 13:57:45 visual_prompt]: 	Training 300/553. train loss: 13.4590,	1.5440 s / batch. (data: 1.05e+00). ETA=18:08:01, max mem: 11.4 GB 
[10/26 13:59:14 visual_prompt]: 	Training 400/553. train loss: 1.8256,	0.4840 s / batch. (data: 2.77e-04). ETA=5:40:15, max mem: 11.4 GB 
[10/26 14:00:44 visual_prompt]: 	Training 500/553. train loss: 6.8042,	0.8696 s / batch. (data: 3.91e-01). ETA=10:09:55, max mem: 11.4 GB 
[10/26 14:01:30 visual_prompt]: Epoch 24 / 100: avg data time: 3.94e-01, avg batch time: 0.8896, average train loss: 5.9725
[10/26 14:02:22 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1906, average loss: 1.7833
[10/26 14:02:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.96	
[10/26 14:02:22 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[10/26 14:03:57 visual_prompt]: 	Training 100/553. train loss: 1.4282,	0.5087 s / batch. (data: 2.48e-02). ETA=5:55:28, max mem: 11.4 GB 
[10/26 14:05:23 visual_prompt]: 	Training 200/553. train loss: 14.8844,	0.5085 s / batch. (data: 5.42e-03). ETA=5:54:27, max mem: 11.4 GB 
[10/26 14:06:51 visual_prompt]: 	Training 300/553. train loss: 9.8377,	0.5093 s / batch. (data: 1.55e-02). ETA=5:54:11, max mem: 11.4 GB 
[10/26 14:08:20 visual_prompt]: 	Training 400/553. train loss: 3.5613,	1.8778 s / batch. (data: 1.39e+00). ETA=21:42:50, max mem: 11.4 GB 
[10/26 14:09:49 visual_prompt]: 	Training 500/553. train loss: 4.1276,	2.0407 s / batch. (data: 1.55e+00). ETA=23:32:24, max mem: 11.4 GB 
[10/26 14:10:34 visual_prompt]: Epoch 25 / 100: avg data time: 3.95e-01, avg batch time: 0.8889, average train loss: 8.1038
[10/26 14:11:26 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.1910, average loss: 15.1416
[10/26 14:11:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.94	
[10/26 14:11:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[10/26 14:12:58 visual_prompt]: 	Training 100/553. train loss: 0.7153,	0.4804 s / batch. (data: 2.58e-04). ETA=5:31:16, max mem: 11.4 GB 
[10/26 14:14:28 visual_prompt]: 	Training 200/553. train loss: 1.3057,	2.1677 s / batch. (data: 1.69e+00). ETA=1 day, 0:51:12, max mem: 11.4 GB 
[10/26 14:15:57 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.4805 s / batch. (data: 2.72e-04). ETA=5:29:46, max mem: 11.4 GB 
[10/26 14:17:25 visual_prompt]: 	Training 400/553. train loss: 1.5364,	0.4960 s / batch. (data: 2.77e-04). ETA=5:39:33, max mem: 11.4 GB 
[10/26 14:18:52 visual_prompt]: 	Training 500/553. train loss: 0.7077,	0.5012 s / batch. (data: 1.55e-02). ETA=5:42:15, max mem: 11.4 GB 
[10/26 14:19:37 visual_prompt]: Epoch 26 / 100: avg data time: 3.94e-01, avg batch time: 0.8880, average train loss: 5.9180
[10/26 14:20:30 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1911, average loss: 1.5768
[10/26 14:20:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.33	
[10/26 14:20:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[10/26 14:22:02 visual_prompt]: 	Training 100/553. train loss: 2.4149,	0.5159 s / batch. (data: 2.41e-02). ETA=5:51:00, max mem: 11.4 GB 
[10/26 14:23:31 visual_prompt]: 	Training 200/553. train loss: 13.9934,	2.1960 s / batch. (data: 1.69e+00). ETA=1 day, 0:50:26, max mem: 11.4 GB 
[10/26 14:24:59 visual_prompt]: 	Training 300/553. train loss: 10.8584,	1.2200 s / batch. (data: 7.31e-01). ETA=13:46:00, max mem: 11.4 GB 
[10/26 14:26:28 visual_prompt]: 	Training 400/553. train loss: 2.5404,	0.5480 s / batch. (data: 7.50e-04). ETA=6:10:05, max mem: 11.4 GB 
[10/26 14:27:58 visual_prompt]: 	Training 500/553. train loss: 0.6158,	0.4881 s / batch. (data: 1.08e-02). ETA=5:28:51, max mem: 11.4 GB 
[10/26 14:28:41 visual_prompt]: Epoch 27 / 100: avg data time: 3.94e-01, avg batch time: 0.8884, average train loss: 5.7204
[10/26 14:29:34 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1905, average loss: 10.2726
[10/26 14:29:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.02	
[10/26 14:29:34 visual_prompt]: Stopping early.
[10/26 14:29:34 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 14:29:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 14:29:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 14:29:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 14:29:34 visual_prompt]: Training with config:
[10/26 14:29:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr2.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 14:29:34 visual_prompt]: Loading training data...
[10/26 14:29:34 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 14:29:34 visual_prompt]: Loading validation data...
[10/26 14:29:34 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 14:29:34 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/26 14:29:36 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/26 14:29:36 visual_prompt]: tuned percent:0.529
[10/26 14:29:36 visual_prompt]: Device used for model: 0
[10/26 14:29:36 visual_prompt]: Setting up Evaluator...
[10/26 14:29:36 visual_prompt]: Setting up Trainer...
[10/26 14:29:36 visual_prompt]: 	Setting up the optimizer...
[10/26 14:29:36 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 14:31:08 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5200 s / batch. (data: 1.20e-02). ETA=7:58:23, max mem: 11.4 GB 
[10/26 14:32:36 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4784 s / batch. (data: 2.72e-04). ETA=7:19:20, max mem: 11.4 GB 
[10/26 14:34:07 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9160 s / batch. (data: 2.43e+00). ETA=1 day, 20:33:00, max mem: 11.4 GB 
[10/26 14:35:33 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4999 s / batch. (data: 2.86e-04). ETA=7:37:26, max mem: 11.4 GB 
[10/26 14:37:03 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4812 s / batch. (data: 2.51e-04). ETA=7:19:31, max mem: 11.4 GB 
[10/26 14:37:50 visual_prompt]: Epoch 1 / 100: avg data time: 3.96e-01, avg batch time: 0.8917, average train loss: 1.3966
[10/26 14:38:42 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.1923, average loss: 1.3454
[10/26 14:38:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/26 14:38:42 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/26 14:40:14 visual_prompt]: 	Training 100/553. train loss: 1.5214,	1.5480 s / batch. (data: 1.07e+00). ETA=23:29:54, max mem: 11.4 GB 
[10/26 14:41:43 visual_prompt]: 	Training 200/553. train loss: 0.6513,	0.5160 s / batch. (data: 2.97e-04). ETA=7:49:04, max mem: 11.4 GB 
[10/26 14:43:13 visual_prompt]: 	Training 300/553. train loss: 1.3172,	1.5680 s / batch. (data: 1.07e+00). ETA=23:42:52, max mem: 11.4 GB 
[10/26 14:44:40 visual_prompt]: 	Training 400/553. train loss: 0.5963,	0.4954 s / batch. (data: 2.70e-04). ETA=7:28:42, max mem: 11.4 GB 
[10/26 14:46:10 visual_prompt]: 	Training 500/553. train loss: 0.4813,	0.4848 s / batch. (data: 2.78e-04). ETA=7:18:21, max mem: 11.4 GB 
[10/26 14:46:55 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 0.8910, average train loss: 1.2933
[10/26 14:47:48 visual_prompt]: Inference (val):avg data time: 2.49e-04, avg batch time: 0.1900, average loss: 3.5743
[10/26 14:47:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.43	
[10/26 14:47:48 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/26 14:49:18 visual_prompt]: 	Training 100/553. train loss: 1.1667,	0.5000 s / batch. (data: 2.68e-04). ETA=7:30:44, max mem: 11.4 GB 
[10/26 14:50:47 visual_prompt]: 	Training 200/553. train loss: 0.8988,	0.4920 s / batch. (data: 2.66e-04). ETA=7:22:45, max mem: 11.4 GB 
[10/26 14:52:15 visual_prompt]: 	Training 300/553. train loss: 1.4804,	0.4879 s / batch. (data: 8.35e-03). ETA=7:18:15, max mem: 11.4 GB 
[10/26 14:53:44 visual_prompt]: 	Training 400/553. train loss: 4.4833,	0.4992 s / batch. (data: 2.40e-04). ETA=7:27:33, max mem: 11.4 GB 
[10/26 14:55:14 visual_prompt]: 	Training 500/553. train loss: 0.7986,	1.8549 s / batch. (data: 1.34e+00). ETA=1 day, 3:39:55, max mem: 11.4 GB 
[10/26 14:55:59 visual_prompt]: Epoch 3 / 100: avg data time: 3.92e-01, avg batch time: 0.8878, average train loss: 1.4476
[10/26 14:56:51 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1932, average loss: 1.9090
[10/26 14:56:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.35	
[10/26 14:56:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/26 14:58:24 visual_prompt]: 	Training 100/553. train loss: 1.7485,	0.4950 s / batch. (data: 2.76e-04). ETA=7:21:43, max mem: 11.4 GB 
[10/26 14:59:53 visual_prompt]: 	Training 200/553. train loss: 1.5691,	0.5120 s / batch. (data: 2.81e-04). ETA=7:36:00, max mem: 11.4 GB 
[10/26 15:01:22 visual_prompt]: 	Training 300/553. train loss: 1.0066,	1.7480 s / batch. (data: 1.24e+00). ETA=1 day, 1:53:59, max mem: 11.4 GB 
[10/26 15:02:47 visual_prompt]: 	Training 400/553. train loss: 2.1668,	1.4360 s / batch. (data: 9.59e-01). ETA=21:14:14, max mem: 11.4 GB 
[10/26 15:04:17 visual_prompt]: 	Training 500/553. train loss: 4.0393,	2.6010 s / batch. (data: 2.11e+00). ETA=1 day, 14:23:40, max mem: 11.4 GB 
[10/26 15:05:03 visual_prompt]: Epoch 4 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 1.6585
[10/26 15:05:55 visual_prompt]: Inference (val):avg data time: 1.82e-04, avg batch time: 0.1922, average loss: 0.8879
[10/26 15:05:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.18	
[10/26 15:05:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/26 15:07:26 visual_prompt]: 	Training 100/553. train loss: 4.9458,	0.4964 s / batch. (data: 5.41e-03). ETA=7:18:21, max mem: 11.4 GB 
[10/26 15:08:55 visual_prompt]: 	Training 200/553. train loss: 1.4338,	1.5279 s / batch. (data: 1.04e+00). ETA=22:26:45, max mem: 11.4 GB 
[10/26 15:10:25 visual_prompt]: 	Training 300/553. train loss: 0.6646,	0.4855 s / batch. (data: 2.82e-04). ETA=7:07:09, max mem: 11.4 GB 
[10/26 15:11:52 visual_prompt]: 	Training 400/553. train loss: 3.0951,	0.5000 s / batch. (data: 2.56e-04). ETA=7:19:05, max mem: 11.4 GB 
[10/26 15:13:21 visual_prompt]: 	Training 500/553. train loss: 1.5174,	0.4970 s / batch. (data: 2.62e-04). ETA=7:15:35, max mem: 11.4 GB 
[10/26 15:14:08 visual_prompt]: Epoch 5 / 100: avg data time: 3.95e-01, avg batch time: 0.8900, average train loss: 2.1502
[10/26 15:15:00 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1909, average loss: 4.1855
[10/26 15:15:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.94	
[10/26 15:15:00 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/26 15:16:34 visual_prompt]: 	Training 100/553. train loss: 17.3080,	0.4920 s / batch. (data: 2.58e-04). ETA=7:09:58, max mem: 11.4 GB 
[10/26 15:18:02 visual_prompt]: 	Training 200/553. train loss: 0.3624,	0.5160 s / batch. (data: 2.64e-04). ETA=7:30:04, max mem: 11.4 GB 
[10/26 15:19:30 visual_prompt]: 	Training 300/553. train loss: 4.7182,	0.5170 s / batch. (data: 5.42e-03). ETA=7:30:05, max mem: 11.4 GB 
[10/26 15:21:02 visual_prompt]: 	Training 400/553. train loss: 1.3569,	0.5280 s / batch. (data: 3.24e-02). ETA=7:38:47, max mem: 11.4 GB 
[10/26 15:22:31 visual_prompt]: 	Training 500/553. train loss: 4.3902,	1.5042 s / batch. (data: 1.01e+00). ETA=21:44:31, max mem: 11.4 GB 
[10/26 15:23:15 visual_prompt]: Epoch 6 / 100: avg data time: 4.01e-01, avg batch time: 0.8956, average train loss: 2.9877
[10/26 15:24:08 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1917, average loss: 2.1962
[10/26 15:24:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.19	
[10/26 15:24:08 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/26 15:25:38 visual_prompt]: 	Training 100/553. train loss: 2.6029,	0.4968 s / batch. (data: 2.56e-04). ETA=7:09:33, max mem: 11.4 GB 
[10/26 15:27:07 visual_prompt]: 	Training 200/553. train loss: 1.1847,	0.4959 s / batch. (data: 2.64e-04). ETA=7:07:59, max mem: 11.4 GB 
[10/26 15:28:39 visual_prompt]: 	Training 300/553. train loss: 1.1558,	2.3559 s / batch. (data: 1.86e+00). ETA=1 day, 9:49:15, max mem: 11.4 GB 
[10/26 15:30:08 visual_prompt]: 	Training 400/553. train loss: 0.6690,	2.0920 s / batch. (data: 1.61e+00). ETA=1 day, 5:58:30, max mem: 11.4 GB 
[10/26 15:31:34 visual_prompt]: 	Training 500/553. train loss: 18.3340,	0.4880 s / batch. (data: 2.92e-04). ETA=6:58:44, max mem: 11.4 GB 
[10/26 15:32:18 visual_prompt]: Epoch 7 / 100: avg data time: 3.92e-01, avg batch time: 0.8864, average train loss: 2.5398
[10/26 15:33:11 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1917, average loss: 1.3551
[10/26 15:33:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.28	
[10/26 15:33:11 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/26 15:34:40 visual_prompt]: 	Training 100/553. train loss: 5.9915,	0.5131 s / batch. (data: 9.26e-03). ETA=7:18:57, max mem: 11.4 GB 
[10/26 15:36:11 visual_prompt]: 	Training 200/553. train loss: 4.4277,	0.5040 s / batch. (data: 7.96e-03). ETA=7:10:19, max mem: 11.4 GB 
[10/26 15:37:40 visual_prompt]: 	Training 300/553. train loss: 2.2371,	0.4903 s / batch. (data: 2.89e-04). ETA=6:57:50, max mem: 11.4 GB 
[10/26 15:39:09 visual_prompt]: 	Training 400/553. train loss: 1.8457,	1.1639 s / batch. (data: 6.81e-01). ETA=16:29:53, max mem: 11.4 GB 
[10/26 15:40:37 visual_prompt]: 	Training 500/553. train loss: 4.6913,	1.2674 s / batch. (data: 7.87e-01). ETA=17:55:48, max mem: 11.4 GB 
[10/26 15:41:23 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8900, average train loss: 2.8313
[10/26 15:42:16 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1917, average loss: 0.9129
[10/26 15:42:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.20	
[10/26 15:42:16 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/26 15:43:47 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.5106 s / batch. (data: 2.69e-04). ETA=7:12:08, max mem: 11.4 GB 
[10/26 15:45:15 visual_prompt]: 	Training 200/553. train loss: 1.8505,	0.5281 s / batch. (data: 2.02e-02). ETA=7:26:02, max mem: 11.4 GB 
[10/26 15:46:44 visual_prompt]: 	Training 300/553. train loss: 0.6559,	2.3160 s / batch. (data: 1.83e+00). ETA=1 day, 8:32:14, max mem: 11.4 GB 
[10/26 15:48:13 visual_prompt]: 	Training 400/553. train loss: 1.8658,	0.4881 s / batch. (data: 5.40e-03). ETA=6:50:39, max mem: 11.4 GB 
[10/26 15:49:43 visual_prompt]: 	Training 500/553. train loss: 0.8344,	0.8710 s / batch. (data: 3.82e-01). ETA=12:11:16, max mem: 11.4 GB 
[10/26 15:50:27 visual_prompt]: Epoch 9 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 2.1005
[10/26 15:51:20 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1914, average loss: 1.2461
[10/26 15:51:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.59	
[10/26 15:51:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/26 15:52:54 visual_prompt]: 	Training 100/553. train loss: 10.9696,	0.5007 s / batch. (data: 1.05e-02). ETA=6:59:07, max mem: 11.4 GB 
[10/26 15:54:21 visual_prompt]: 	Training 200/553. train loss: 0.5838,	0.4880 s / batch. (data: 2.96e-04). ETA=6:47:42, max mem: 11.4 GB 
[10/26 15:55:50 visual_prompt]: 	Training 300/553. train loss: 5.2656,	0.5087 s / batch. (data: 8.64e-03). ETA=7:04:06, max mem: 11.4 GB 
[10/26 15:57:15 visual_prompt]: 	Training 400/553. train loss: 1.5021,	0.5052 s / batch. (data: 1.56e-02). ETA=7:00:22, max mem: 11.4 GB 
[10/26 15:58:45 visual_prompt]: 	Training 500/553. train loss: 0.7297,	0.4892 s / batch. (data: 2.58e-04). ETA=6:46:13, max mem: 11.4 GB 
[10/26 15:59:31 visual_prompt]: Epoch 10 / 100: avg data time: 3.94e-01, avg batch time: 0.8881, average train loss: 3.4474
[10/26 16:00:23 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1911, average loss: 1.2259
[10/26 16:00:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.68	
[10/26 16:00:23 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/26 16:01:56 visual_prompt]: 	Training 100/553. train loss: 1.2319,	0.5119 s / batch. (data: 1.19e-02). ETA=7:03:48, max mem: 11.4 GB 
[10/26 16:03:27 visual_prompt]: 	Training 200/553. train loss: 4.6683,	0.4960 s / batch. (data: 2.72e-04). ETA=6:49:45, max mem: 11.4 GB 
[10/26 16:04:55 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.3920 s / batch. (data: 1.89e+00). ETA=1 day, 8:52:10, max mem: 11.4 GB 
[10/26 16:06:22 visual_prompt]: 	Training 400/553. train loss: 5.1816,	0.4960 s / batch. (data: 2.71e-04). ETA=6:48:07, max mem: 11.4 GB 
[10/26 16:07:50 visual_prompt]: 	Training 500/553. train loss: 1.1014,	0.4958 s / batch. (data: 5.38e-03). ETA=6:47:08, max mem: 11.4 GB 
[10/26 16:08:34 visual_prompt]: Epoch 11 / 100: avg data time: 3.94e-01, avg batch time: 0.8884, average train loss: 2.8922
[10/26 16:09:27 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.1920, average loss: 2.7652
[10/26 16:09:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.18	
[10/26 16:09:27 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/26 16:11:00 visual_prompt]: 	Training 100/553. train loss: 2.2609,	0.4851 s / batch. (data: 2.87e-04). ETA=6:37:08, max mem: 11.4 GB 
[10/26 16:12:30 visual_prompt]: 	Training 200/553. train loss: 0.6700,	0.7920 s / batch. (data: 2.89e-01). ETA=10:47:02, max mem: 11.4 GB 
[10/26 16:13:57 visual_prompt]: 	Training 300/553. train loss: 0.7404,	0.5120 s / batch. (data: 2.47e-04). ETA=6:57:26, max mem: 11.4 GB 
[10/26 16:15:26 visual_prompt]: 	Training 400/553. train loss: 1.6251,	0.5091 s / batch. (data: 2.47e-02). ETA=6:54:11, max mem: 11.4 GB 
[10/26 16:16:55 visual_prompt]: 	Training 500/553. train loss: 5.7100,	0.5077 s / batch. (data: 6.96e-04). ETA=6:52:14, max mem: 11.4 GB 
[10/26 16:17:39 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8890, average train loss: 2.6045
[10/26 16:18:31 visual_prompt]: Inference (val):avg data time: 3.76e-04, avg batch time: 0.1925, average loss: 1.0026
[10/26 16:18:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.71	
[10/26 16:18:31 visual_prompt]: Best epoch 12: best metric: -1.003
[10/26 16:18:31 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/26 16:20:04 visual_prompt]: 	Training 100/553. train loss: 0.6546,	1.8440 s / batch. (data: 1.35e+00). ETA=1 day, 0:52:31, max mem: 11.4 GB 
[10/26 16:21:30 visual_prompt]: 	Training 200/553. train loss: 2.6798,	1.1080 s / batch. (data: 6.07e-01). ETA=14:54:58, max mem: 11.4 GB 
[10/26 16:22:59 visual_prompt]: 	Training 300/553. train loss: 1.2894,	1.1313 s / batch. (data: 6.31e-01). ETA=15:11:54, max mem: 11.4 GB 
[10/26 16:24:26 visual_prompt]: 	Training 400/553. train loss: 7.2068,	0.6205 s / batch. (data: 1.40e-01). ETA=8:19:08, max mem: 11.4 GB 
[10/26 16:25:57 visual_prompt]: 	Training 500/553. train loss: 6.1296,	0.4840 s / batch. (data: 2.55e-04). ETA=6:28:33, max mem: 11.4 GB 
[10/26 16:26:42 visual_prompt]: Epoch 13 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 4.5619
[10/26 16:27:34 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1910, average loss: 1.2006
[10/26 16:27:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.14	
[10/26 16:27:34 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/26 16:29:07 visual_prompt]: 	Training 100/553. train loss: 2.0201,	0.4781 s / batch. (data: 2.65e-04). ETA=6:22:33, max mem: 11.4 GB 
[10/26 16:30:36 visual_prompt]: 	Training 200/553. train loss: 0.0634,	1.1519 s / batch. (data: 6.48e-01). ETA=15:19:47, max mem: 11.4 GB 
[10/26 16:32:05 visual_prompt]: 	Training 300/553. train loss: 2.1493,	1.1811 s / batch. (data: 7.04e-01). ETA=15:41:07, max mem: 11.4 GB 
[10/26 16:33:33 visual_prompt]: 	Training 400/553. train loss: 1.5089,	0.5000 s / batch. (data: 2.45e-04). ETA=6:37:35, max mem: 11.4 GB 
[10/26 16:35:02 visual_prompt]: 	Training 500/553. train loss: 1.5354,	0.4962 s / batch. (data: 5.39e-03). ETA=6:33:45, max mem: 11.4 GB 
[10/26 16:35:46 visual_prompt]: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 2.5067
[10/26 16:36:38 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1932, average loss: 0.6898
[10/26 16:36:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.85	
[10/26 16:36:38 visual_prompt]: Best epoch 14: best metric: -0.690
[10/26 16:36:38 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/26 16:38:10 visual_prompt]: 	Training 100/553. train loss: 4.0929,	0.4781 s / batch. (data: 2.91e-04). ETA=6:18:10, max mem: 11.4 GB 
[10/26 16:39:37 visual_prompt]: 	Training 200/553. train loss: 6.8395,	0.4983 s / batch. (data: 7.98e-03). ETA=6:33:19, max mem: 11.4 GB 
[10/26 16:41:08 visual_prompt]: 	Training 300/553. train loss: 6.6551,	0.5000 s / batch. (data: 2.52e-04). ETA=6:33:48, max mem: 11.4 GB 
[10/26 16:42:35 visual_prompt]: 	Training 400/553. train loss: 2.6840,	0.7485 s / batch. (data: 2.49e-01). ETA=9:48:17, max mem: 11.4 GB 
[10/26 16:44:05 visual_prompt]: 	Training 500/553. train loss: 0.6171,	0.6920 s / batch. (data: 2.01e-01). ETA=9:02:44, max mem: 11.4 GB 
[10/26 16:44:52 visual_prompt]: Epoch 15 / 100: avg data time: 3.97e-01, avg batch time: 0.8924, average train loss: 3.4477
[10/26 16:45:44 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.1932, average loss: 4.8012
[10/26 16:45:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.47	
[10/26 16:45:44 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/26 16:47:16 visual_prompt]: 	Training 100/553. train loss: 0.7452,	0.5054 s / batch. (data: 9.36e-03). ETA=6:35:06, max mem: 11.4 GB 
[10/26 16:48:45 visual_prompt]: 	Training 200/553. train loss: 0.6312,	0.4887 s / batch. (data: 2.76e-04). ETA=6:21:15, max mem: 11.4 GB 
[10/26 16:50:14 visual_prompt]: 	Training 300/553. train loss: 0.9948,	0.4880 s / batch. (data: 7.97e-03). ETA=6:19:52, max mem: 11.4 GB 
[10/26 16:51:43 visual_prompt]: 	Training 400/553. train loss: 8.6996,	0.5358 s / batch. (data: 5.84e-03). ETA=6:56:08, max mem: 11.4 GB 
[10/26 16:53:10 visual_prompt]: 	Training 500/553. train loss: 0.6834,	2.0689 s / batch. (data: 1.59e+00). ETA=1 day, 2:43:36, max mem: 11.4 GB 
[10/26 16:53:56 visual_prompt]: Epoch 16 / 100: avg data time: 3.96e-01, avg batch time: 0.8899, average train loss: 3.0018
[10/26 16:54:49 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1921, average loss: 0.7271
[10/26 16:54:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.17	
[10/26 16:54:49 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/26 16:56:20 visual_prompt]: 	Training 100/553. train loss: 1.2867,	0.5440 s / batch. (data: 2.61e-04). ETA=7:00:15, max mem: 11.4 GB 
[10/26 16:57:50 visual_prompt]: 	Training 200/553. train loss: 10.2550,	0.4976 s / batch. (data: 2.58e-04). ETA=6:23:35, max mem: 11.4 GB 
[10/26 16:59:18 visual_prompt]: 	Training 300/553. train loss: 2.5118,	0.5117 s / batch. (data: 1.20e-02). ETA=6:33:36, max mem: 11.4 GB 
[10/26 17:00:46 visual_prompt]: 	Training 400/553. train loss: 0.7630,	1.0533 s / batch. (data: 5.51e-01). ETA=13:28:27, max mem: 11.4 GB 
[10/26 17:02:14 visual_prompt]: 	Training 500/553. train loss: 4.2285,	2.0720 s / batch. (data: 1.57e+00). ETA=1 day, 2:26:54, max mem: 11.4 GB 
[10/26 17:03:00 visual_prompt]: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 2.8501
[10/26 17:03:53 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1914, average loss: 1.9847
[10/26 17:03:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.16	
[10/26 17:03:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/26 17:05:25 visual_prompt]: 	Training 100/553. train loss: 5.4333,	0.4960 s / batch. (data: 2.56e-04). ETA=6:18:37, max mem: 11.4 GB 
[10/26 17:06:56 visual_prompt]: 	Training 200/553. train loss: 1.4178,	0.5697 s / batch. (data: 1.60e-02). ETA=7:13:55, max mem: 11.4 GB 
[10/26 17:08:25 visual_prompt]: 	Training 300/553. train loss: 2.3224,	0.5040 s / batch. (data: 2.62e-04). ETA=6:23:03, max mem: 11.4 GB 
[10/26 17:09:53 visual_prompt]: 	Training 400/553. train loss: 0.8586,	0.5178 s / batch. (data: 9.73e-03). ETA=6:32:38, max mem: 11.4 GB 
[10/26 17:11:21 visual_prompt]: 	Training 500/553. train loss: 1.8619,	0.4873 s / batch. (data: 2.62e-04). ETA=6:08:42, max mem: 11.4 GB 
[10/26 17:12:05 visual_prompt]: Epoch 18 / 100: avg data time: 3.96e-01, avg batch time: 0.8907, average train loss: 3.4671
[10/26 17:12:58 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1895, average loss: 1.6367
[10/26 17:12:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.00	
[10/26 17:12:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/26 17:14:29 visual_prompt]: 	Training 100/553. train loss: 0.6168,	0.5081 s / batch. (data: 1.05e-02). ETA=6:23:08, max mem: 11.4 GB 
[10/26 17:15:58 visual_prompt]: 	Training 200/553. train loss: 0.8813,	0.5080 s / batch. (data: 2.73e-04). ETA=6:22:13, max mem: 11.4 GB 
[10/26 17:17:27 visual_prompt]: 	Training 300/553. train loss: 0.0432,	0.4814 s / batch. (data: 2.37e-04). ETA=6:01:23, max mem: 11.4 GB 
[10/26 17:18:56 visual_prompt]: 	Training 400/553. train loss: 2.1460,	0.4959 s / batch. (data: 5.87e-03). ETA=6:11:29, max mem: 11.4 GB 
[10/26 17:20:21 visual_prompt]: 	Training 500/553. train loss: 0.5805,	0.5137 s / batch. (data: 1.34e-02). ETA=6:23:56, max mem: 11.4 GB 
[10/26 17:21:07 visual_prompt]: Epoch 19 / 100: avg data time: 3.93e-01, avg batch time: 0.8854, average train loss: 2.6183
[10/26 17:22:00 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1900, average loss: 5.9631
[10/26 17:22:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.60	
[10/26 17:22:00 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[10/26 17:23:30 visual_prompt]: 	Training 100/553. train loss: 2.2175,	1.0743 s / batch. (data: 5.97e-01). ETA=13:20:14, max mem: 11.4 GB 
[10/26 17:25:00 visual_prompt]: 	Training 200/553. train loss: 3.5397,	0.5078 s / batch. (data: 8.43e-03). ETA=6:17:25, max mem: 11.4 GB 
[10/26 17:26:29 visual_prompt]: 	Training 300/553. train loss: 0.9413,	0.5037 s / batch. (data: 7.98e-03). ETA=6:13:33, max mem: 11.4 GB 
[10/26 17:27:57 visual_prompt]: 	Training 400/553. train loss: 1.2513,	0.5212 s / batch. (data: 2.93e-02). ETA=6:25:39, max mem: 11.4 GB 
[10/26 17:29:25 visual_prompt]: 	Training 500/553. train loss: 2.2096,	0.5000 s / batch. (data: 2.94e-04). ETA=6:09:06, max mem: 11.4 GB 
[10/26 17:30:12 visual_prompt]: Epoch 20 / 100: avg data time: 3.97e-01, avg batch time: 0.8907, average train loss: 3.3969
[10/26 17:31:05 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1893, average loss: 1.3703
[10/26 17:31:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.66	
[10/26 17:31:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[10/26 17:32:39 visual_prompt]: 	Training 100/553. train loss: 7.3157,	0.5184 s / batch. (data: 2.78e-04). ETA=6:21:22, max mem: 11.4 GB 
[10/26 17:34:08 visual_prompt]: 	Training 200/553. train loss: 1.5637,	0.5121 s / batch. (data: 5.42e-03). ETA=6:15:55, max mem: 11.4 GB 
[10/26 17:35:36 visual_prompt]: 	Training 300/553. train loss: 8.2648,	1.6438 s / batch. (data: 1.16e+00). ETA=20:03:48, max mem: 11.4 GB 
[10/26 17:37:03 visual_prompt]: 	Training 400/553. train loss: 18.7807,	0.5004 s / batch. (data: 1.05e-02). ETA=6:05:35, max mem: 11.4 GB 
[10/26 17:38:33 visual_prompt]: 	Training 500/553. train loss: 3.7694,	0.5066 s / batch. (data: 2.75e-04). ETA=6:09:18, max mem: 11.4 GB 
[10/26 17:39:17 visual_prompt]: Epoch 21 / 100: avg data time: 3.97e-01, avg batch time: 0.8903, average train loss: 3.4146
[10/26 17:40:10 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1898, average loss: 2.9888
[10/26 17:40:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.09	
[10/26 17:40:10 visual_prompt]: Stopping early.
[10/26 17:40:10 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 17:40:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 17:40:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 17:40:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 17:40:10 visual_prompt]: Training with config:
[10/26 17:40:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr2.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 17:40:10 visual_prompt]: Loading training data...
[10/26 17:40:10 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 17:40:10 visual_prompt]: Loading validation data...
[10/26 17:40:10 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 17:40:10 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/26 17:40:12 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/26 17:40:12 visual_prompt]: tuned percent:0.529
[10/26 17:40:13 visual_prompt]: Device used for model: 0
[10/26 17:40:13 visual_prompt]: Setting up Evaluator...
[10/26 17:40:13 visual_prompt]: Setting up Trainer...
[10/26 17:40:13 visual_prompt]: 	Setting up the optimizer...
[10/26 17:40:13 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 17:41:44 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4783 s / batch. (data: 2.46e-04). ETA=7:20:02, max mem: 11.4 GB 
[10/26 17:43:10 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5007 s / batch. (data: 5.40e-03). ETA=7:39:48, max mem: 11.4 GB 
[10/26 17:44:42 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9203 s / batch. (data: 2.44e+00). ETA=1 day, 20:36:53, max mem: 11.4 GB 
[10/26 17:46:08 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5040 s / batch. (data: 3.00e-04). ETA=7:41:07, max mem: 11.4 GB 
[10/26 17:47:39 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4880 s / batch. (data: 2.73e-04). ETA=7:25:42, max mem: 11.4 GB 
[10/26 17:48:25 visual_prompt]: Epoch 1 / 100: avg data time: 3.95e-01, avg batch time: 0.8899, average train loss: 1.3966
[10/26 17:49:17 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1899, average loss: 1.3454
[10/26 17:49:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/26 17:49:17 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/26 17:50:49 visual_prompt]: 	Training 100/553. train loss: 1.5965,	0.5360 s / batch. (data: 3.15e-04). ETA=8:08:10, max mem: 11.4 GB 
[10/26 17:52:18 visual_prompt]: 	Training 200/553. train loss: 0.6470,	1.2413 s / batch. (data: 7.64e-01). ETA=18:48:30, max mem: 11.4 GB 
[10/26 17:53:48 visual_prompt]: 	Training 300/553. train loss: 1.3288,	1.6920 s / batch. (data: 1.19e+00). ETA=1 day, 1:35:24, max mem: 11.4 GB 
[10/26 17:55:15 visual_prompt]: 	Training 400/553. train loss: 0.6423,	0.5000 s / batch. (data: 2.88e-04). ETA=7:32:52, max mem: 11.4 GB 
[10/26 17:56:45 visual_prompt]: 	Training 500/553. train loss: 0.4763,	0.4942 s / batch. (data: 5.40e-03). ETA=7:26:46, max mem: 11.4 GB 
[10/26 17:57:30 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 0.8909, average train loss: 1.2790
[10/26 17:58:22 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1922, average loss: 2.2703
[10/26 17:58:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[10/26 17:58:22 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/26 17:59:53 visual_prompt]: 	Training 100/553. train loss: 3.0428,	1.0000 s / batch. (data: 5.18e-01). ETA=15:01:34, max mem: 11.4 GB 
[10/26 18:01:22 visual_prompt]: 	Training 200/553. train loss: 1.0757,	0.5086 s / batch. (data: 8.56e-03). ETA=7:37:40, max mem: 11.4 GB 
[10/26 18:02:50 visual_prompt]: 	Training 300/553. train loss: 1.4341,	0.4840 s / batch. (data: 2.80e-04). ETA=7:14:42, max mem: 11.4 GB 
[10/26 18:04:19 visual_prompt]: 	Training 400/553. train loss: 4.7927,	0.5373 s / batch. (data: 2.13e-02). ETA=8:01:44, max mem: 11.4 GB 
[10/26 18:05:49 visual_prompt]: 	Training 500/553. train loss: 0.8023,	1.8680 s / batch. (data: 1.38e+00). ETA=1 day, 3:51:40, max mem: 11.4 GB 
[10/26 18:06:33 visual_prompt]: Epoch 3 / 100: avg data time: 3.93e-01, avg batch time: 0.8876, average train loss: 1.6704
[10/26 18:07:26 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1930, average loss: 2.1507
[10/26 18:07:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.80	
[10/26 18:07:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/26 18:08:59 visual_prompt]: 	Training 100/553. train loss: 1.7607,	0.4971 s / batch. (data: 5.08e-03). ETA=7:23:35, max mem: 11.4 GB 
[10/26 18:10:28 visual_prompt]: 	Training 200/553. train loss: 1.3375,	0.5000 s / batch. (data: 7.96e-03). ETA=7:25:20, max mem: 11.4 GB 
[10/26 18:11:56 visual_prompt]: 	Training 300/553. train loss: 0.8326,	1.3712 s / batch. (data: 8.94e-01). ETA=20:19:02, max mem: 11.4 GB 
[10/26 18:13:22 visual_prompt]: 	Training 400/553. train loss: 0.8771,	2.0552 s / batch. (data: 1.56e+00). ETA=1 day, 6:23:41, max mem: 11.4 GB 
[10/26 18:14:52 visual_prompt]: 	Training 500/553. train loss: 1.5641,	3.5521 s / batch. (data: 3.06e+00). ETA=2 days, 4:26:00, max mem: 11.4 GB 
[10/26 18:15:38 visual_prompt]: Epoch 4 / 100: avg data time: 3.97e-01, avg batch time: 0.8912, average train loss: 1.7824
[10/26 18:16:31 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1893, average loss: 0.9729
[10/26 18:16:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.02	
[10/26 18:16:31 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/26 18:18:03 visual_prompt]: 	Training 100/553. train loss: 3.1976,	0.5080 s / batch. (data: 1.34e-02). ETA=7:28:38, max mem: 11.4 GB 
[10/26 18:19:32 visual_prompt]: 	Training 200/553. train loss: 2.6513,	1.8760 s / batch. (data: 1.37e+00). ETA=1 day, 3:33:37, max mem: 11.4 GB 
[10/26 18:21:02 visual_prompt]: 	Training 300/553. train loss: 7.3855,	0.4800 s / batch. (data: 2.71e-04). ETA=7:02:19, max mem: 11.4 GB 
[10/26 18:22:30 visual_prompt]: 	Training 400/553. train loss: 6.3355,	0.5040 s / batch. (data: 2.77e-04). ETA=7:22:34, max mem: 11.4 GB 
[10/26 18:23:59 visual_prompt]: 	Training 500/553. train loss: 1.3502,	0.4886 s / batch. (data: 5.36e-03). ETA=7:08:13, max mem: 11.4 GB 
[10/26 18:24:46 visual_prompt]: Epoch 5 / 100: avg data time: 3.99e-01, avg batch time: 0.8937, average train loss: 2.3528
[10/26 18:25:38 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1903, average loss: 3.9034
[10/26 18:25:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.23	
[10/26 18:25:38 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/26 18:27:11 visual_prompt]: 	Training 100/553. train loss: 0.8220,	0.5067 s / batch. (data: 2.68e-04). ETA=7:22:46, max mem: 11.4 GB 
[10/26 18:28:39 visual_prompt]: 	Training 200/553. train loss: 6.3834,	0.5028 s / batch. (data: 2.84e-04). ETA=7:18:34, max mem: 11.4 GB 
[10/26 18:30:06 visual_prompt]: 	Training 300/553. train loss: 0.6840,	0.5207 s / batch. (data: 4.65e-03). ETA=7:33:16, max mem: 11.4 GB 
[10/26 18:31:38 visual_prompt]: 	Training 400/553. train loss: 1.2502,	0.5120 s / batch. (data: 1.19e-02). ETA=7:24:51, max mem: 11.4 GB 
[10/26 18:33:06 visual_prompt]: 	Training 500/553. train loss: 7.5331,	1.3956 s / batch. (data: 9.19e-01). ETA=20:10:20, max mem: 11.4 GB 
[10/26 18:33:50 visual_prompt]: Epoch 6 / 100: avg data time: 3.95e-01, avg batch time: 0.8902, average train loss: 2.2502
[10/26 18:34:42 visual_prompt]: Inference (val):avg data time: 2.91e-04, avg batch time: 0.1908, average loss: 0.8104
[10/26 18:34:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.71	
[10/26 18:34:42 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/26 18:36:12 visual_prompt]: 	Training 100/553. train loss: 3.3987,	0.4993 s / batch. (data: 2.73e-04). ETA=7:11:46, max mem: 11.4 GB 
[10/26 18:37:42 visual_prompt]: 	Training 200/553. train loss: 0.9239,	0.5000 s / batch. (data: 2.52e-04). ETA=7:11:32, max mem: 11.4 GB 
[10/26 18:39:12 visual_prompt]: 	Training 300/553. train loss: 2.4975,	1.2431 s / batch. (data: 7.49e-01). ETA=17:50:47, max mem: 11.4 GB 
[10/26 18:40:40 visual_prompt]: 	Training 400/553. train loss: 2.2425,	0.5480 s / batch. (data: 5.25e-02). ETA=7:51:07, max mem: 11.4 GB 
[10/26 18:42:08 visual_prompt]: 	Training 500/553. train loss: 1.6499,	0.4920 s / batch. (data: 2.35e-04). ETA=7:02:10, max mem: 11.4 GB 
[10/26 18:42:52 visual_prompt]: Epoch 7 / 100: avg data time: 3.90e-01, avg batch time: 0.8856, average train loss: 2.1440
[10/26 18:43:45 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1901, average loss: 4.0009
[10/26 18:43:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.06	
[10/26 18:43:45 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/26 18:45:15 visual_prompt]: 	Training 100/553. train loss: 12.6454,	0.4956 s / batch. (data: 2.80e-04). ETA=7:03:57, max mem: 11.4 GB 
[10/26 18:46:45 visual_prompt]: 	Training 200/553. train loss: 0.4565,	0.5212 s / batch. (data: 2.86e-04). ETA=7:24:59, max mem: 11.4 GB 
[10/26 18:48:14 visual_prompt]: 	Training 300/553. train loss: 1.9225,	0.4960 s / batch. (data: 2.94e-04). ETA=7:02:38, max mem: 11.4 GB 
[10/26 18:49:44 visual_prompt]: 	Training 400/553. train loss: 1.5552,	1.5274 s / batch. (data: 1.03e+00). ETA=21:39:00, max mem: 11.4 GB 
[10/26 18:51:13 visual_prompt]: 	Training 500/553. train loss: 0.3872,	1.9611 s / batch. (data: 1.48e+00). ETA=1 day, 3:44:38, max mem: 11.4 GB 
[10/26 18:51:57 visual_prompt]: Epoch 8 / 100: avg data time: 3.96e-01, avg batch time: 0.8901, average train loss: 5.9818
[10/26 18:52:49 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1898, average loss: 2.3856
[10/26 18:52:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.00	
[10/26 18:52:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/26 18:54:21 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.4960 s / batch. (data: 7.96e-03). ETA=6:59:43, max mem: 11.4 GB 
[10/26 18:55:50 visual_prompt]: 	Training 200/553. train loss: 0.5036,	0.5000 s / batch. (data: 2.62e-04). ETA=7:02:18, max mem: 11.4 GB 
[10/26 18:57:18 visual_prompt]: 	Training 300/553. train loss: 1.6310,	1.5927 s / batch. (data: 1.10e+00). ETA=22:22:31, max mem: 11.4 GB 
[10/26 18:58:48 visual_prompt]: 	Training 400/553. train loss: 5.4151,	0.4886 s / batch. (data: 7.96e-03). ETA=6:51:02, max mem: 11.4 GB 
[10/26 19:00:17 visual_prompt]: 	Training 500/553. train loss: 1.9241,	1.4040 s / batch. (data: 8.96e-01). ETA=19:38:48, max mem: 11.4 GB 
[10/26 19:01:01 visual_prompt]: Epoch 9 / 100: avg data time: 3.95e-01, avg batch time: 0.8883, average train loss: 5.2725
[10/26 19:01:53 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1901, average loss: 4.3186
[10/26 19:01:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.65	
[10/26 19:01:53 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/26 19:03:27 visual_prompt]: 	Training 100/553. train loss: 16.7518,	0.5880 s / batch. (data: 9.87e-02). ETA=8:12:10, max mem: 11.4 GB 
[10/26 19:04:56 visual_prompt]: 	Training 200/553. train loss: 2.6327,	0.5078 s / batch. (data: 5.46e-03). ETA=7:04:14, max mem: 11.4 GB 
[10/26 19:06:25 visual_prompt]: 	Training 300/553. train loss: 3.6062,	2.4309 s / batch. (data: 1.95e+00). ETA=1 day, 9:46:38, max mem: 11.4 GB 
[10/26 19:07:52 visual_prompt]: 	Training 400/553. train loss: 8.7843,	1.3077 s / batch. (data: 8.11e-01). ETA=18:08:02, max mem: 11.4 GB 
[10/26 19:09:23 visual_prompt]: 	Training 500/553. train loss: 2.5871,	1.5953 s / batch. (data: 1.10e+00). ETA=22:04:41, max mem: 11.4 GB 
[10/26 19:10:09 visual_prompt]: Epoch 10 / 100: avg data time: 4.01e-01, avg batch time: 0.8955, average train loss: 6.4923
[10/26 19:11:01 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1916, average loss: 2.4429
[10/26 19:11:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.76	
[10/26 19:11:01 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/26 19:12:35 visual_prompt]: 	Training 100/553. train loss: 20.4373,	0.4988 s / batch. (data: 2.45e-04). ETA=6:52:55, max mem: 11.4 GB 
[10/26 19:14:06 visual_prompt]: 	Training 200/553. train loss: 1.6473,	0.5475 s / batch. (data: 1.15e-02). ETA=7:32:18, max mem: 11.4 GB 
[10/26 19:15:34 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.2569 s / batch. (data: 7.45e-01). ETA=17:16:20, max mem: 11.4 GB 
[10/26 19:17:01 visual_prompt]: 	Training 400/553. train loss: 6.8035,	0.5240 s / batch. (data: 7.54e-04). ETA=7:11:10, max mem: 11.4 GB 
[10/26 19:18:29 visual_prompt]: 	Training 500/553. train loss: 4.9223,	0.5032 s / batch. (data: 1.15e-02). ETA=6:53:12, max mem: 11.4 GB 
[10/26 19:19:14 visual_prompt]: Epoch 11 / 100: avg data time: 3.96e-01, avg batch time: 0.8904, average train loss: 7.1219
[10/26 19:20:06 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.1916, average loss: 1.7640
[10/26 19:20:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.85	
[10/26 19:20:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/26 19:21:39 visual_prompt]: 	Training 100/553. train loss: 1.7026,	0.5484 s / batch. (data: 4.50e-02). ETA=7:28:56, max mem: 11.4 GB 
[10/26 19:23:09 visual_prompt]: 	Training 200/553. train loss: 2.0880,	0.4784 s / batch. (data: 2.87e-04). ETA=6:30:47, max mem: 11.4 GB 
[10/26 19:24:36 visual_prompt]: 	Training 300/553. train loss: 2.2043,	0.4810 s / batch. (data: 2.61e-04). ETA=6:32:10, max mem: 11.4 GB 
[10/26 19:26:05 visual_prompt]: 	Training 400/553. train loss: 17.1961,	0.5120 s / batch. (data: 2.63e-04). ETA=6:56:33, max mem: 11.4 GB 
[10/26 19:27:34 visual_prompt]: 	Training 500/553. train loss: 1.4388,	0.5079 s / batch. (data: 2.68e-04). ETA=6:52:23, max mem: 11.4 GB 
[10/26 19:28:18 visual_prompt]: Epoch 12 / 100: avg data time: 3.95e-01, avg batch time: 0.8888, average train loss: 6.4858
[10/26 19:29:10 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1909, average loss: 10.2925
[10/26 19:29:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.57	
[10/26 19:29:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/26 19:30:42 visual_prompt]: 	Training 100/553. train loss: 3.0640,	0.4880 s / batch. (data: 5.60e-03). ETA=6:35:00, max mem: 11.4 GB 
[10/26 19:32:08 visual_prompt]: 	Training 200/553. train loss: 1.5026,	0.4841 s / batch. (data: 2.69e-04). ETA=6:31:02, max mem: 11.4 GB 
[10/26 19:33:38 visual_prompt]: 	Training 300/553. train loss: 5.1663,	2.1332 s / batch. (data: 1.66e+00). ETA=1 day, 4:39:27, max mem: 11.4 GB 
[10/26 19:35:05 visual_prompt]: 	Training 400/553. train loss: 24.0377,	0.4883 s / batch. (data: 5.41e-03). ETA=6:32:48, max mem: 11.4 GB 
[10/26 19:36:35 visual_prompt]: 	Training 500/553. train loss: 0.9895,	0.5295 s / batch. (data: 2.56e-02). ETA=7:05:04, max mem: 11.4 GB 
[10/26 19:37:20 visual_prompt]: Epoch 13 / 100: avg data time: 3.93e-01, avg batch time: 0.8869, average train loss: 5.7301
[10/26 19:38:13 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1915, average loss: 1.7001
[10/26 19:38:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[10/26 19:38:13 visual_prompt]: Best epoch 13: best metric: -1.700
[10/26 19:38:13 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/26 19:39:45 visual_prompt]: 	Training 100/553. train loss: 8.3392,	0.4913 s / batch. (data: 2.53e-04). ETA=6:33:08, max mem: 11.4 GB 
[10/26 19:41:14 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.8956 s / batch. (data: 1.41e+00). ETA=1 day, 1:13:42, max mem: 11.4 GB 
[10/26 19:42:42 visual_prompt]: 	Training 300/553. train loss: 0.3481,	1.1600 s / batch. (data: 6.68e-01). ETA=15:24:19, max mem: 11.4 GB 
[10/26 19:44:10 visual_prompt]: 	Training 400/553. train loss: 3.3540,	0.7400 s / batch. (data: 2.36e-01). ETA=9:48:25, max mem: 11.4 GB 
[10/26 19:45:39 visual_prompt]: 	Training 500/553. train loss: 6.3310,	0.5097 s / batch. (data: 2.56e-04). ETA=6:44:26, max mem: 11.4 GB 
[10/26 19:46:23 visual_prompt]: Epoch 14 / 100: avg data time: 3.91e-01, avg batch time: 0.8858, average train loss: 5.7099
[10/26 19:47:15 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1904, average loss: 9.5580
[10/26 19:47:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.35	
[10/26 19:47:15 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/26 19:48:47 visual_prompt]: 	Training 100/553. train loss: 4.9510,	0.5051 s / batch. (data: 1.55e-02). ETA=6:39:28, max mem: 11.4 GB 
[10/26 19:50:14 visual_prompt]: 	Training 200/553. train loss: 7.3209,	0.4840 s / batch. (data: 2.56e-04). ETA=6:22:02, max mem: 11.4 GB 
[10/26 19:51:44 visual_prompt]: 	Training 300/553. train loss: 1.4087,	0.5012 s / batch. (data: 2.97e-04). ETA=6:34:45, max mem: 11.4 GB 
[10/26 19:53:11 visual_prompt]: 	Training 400/553. train loss: 0.4336,	0.4883 s / batch. (data: 6.06e-03). ETA=6:23:46, max mem: 11.4 GB 
[10/26 19:54:40 visual_prompt]: 	Training 500/553. train loss: 3.8035,	0.5004 s / batch. (data: 2.60e-04). ETA=6:32:29, max mem: 11.4 GB 
[10/26 19:55:27 visual_prompt]: Epoch 15 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 6.6277
[10/26 19:56:19 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1911, average loss: 3.6001
[10/26 19:56:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.04	
[10/26 19:56:19 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/26 19:57:50 visual_prompt]: 	Training 100/553. train loss: 7.6596,	0.4999 s / batch. (data: 2.51e-04). ETA=6:30:46, max mem: 11.4 GB 
[10/26 19:59:19 visual_prompt]: 	Training 200/553. train loss: 2.2163,	0.4960 s / batch. (data: 2.58e-04). ETA=6:26:55, max mem: 11.4 GB 
[10/26 20:00:48 visual_prompt]: 	Training 300/553. train loss: 6.7848,	0.5040 s / batch. (data: 2.49e-04). ETA=6:32:20, max mem: 11.4 GB 
[10/26 20:02:16 visual_prompt]: 	Training 400/553. train loss: 11.9919,	0.5120 s / batch. (data: 6.47e-04). ETA=6:37:43, max mem: 11.4 GB 
[10/26 20:03:44 visual_prompt]: 	Training 500/553. train loss: 3.5479,	2.0837 s / batch. (data: 1.60e+00). ETA=1 day, 2:55:02, max mem: 11.4 GB 
[10/26 20:04:30 visual_prompt]: Epoch 16 / 100: avg data time: 3.92e-01, avg batch time: 0.8869, average train loss: 5.9262
[10/26 20:05:22 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1914, average loss: 1.0772
[10/26 20:05:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.49	
[10/26 20:05:22 visual_prompt]: Best epoch 16: best metric: -1.077
[10/26 20:05:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/26 20:06:52 visual_prompt]: 	Training 100/553. train loss: 0.0424,	0.5122 s / batch. (data: 1.63e-02). ETA=6:35:39, max mem: 11.4 GB 
[10/26 20:08:23 visual_prompt]: 	Training 200/553. train loss: 11.0443,	0.4927 s / batch. (data: 5.39e-03). ETA=6:19:48, max mem: 11.4 GB 
[10/26 20:09:51 visual_prompt]: 	Training 300/553. train loss: 24.4907,	0.5042 s / batch. (data: 1.04e-02). ETA=6:27:48, max mem: 11.4 GB 
[10/26 20:11:18 visual_prompt]: 	Training 400/553. train loss: 1.1508,	0.4880 s / batch. (data: 7.96e-03). ETA=6:14:32, max mem: 11.4 GB 
[10/26 20:12:47 visual_prompt]: 	Training 500/553. train loss: 1.4800,	2.1843 s / batch. (data: 1.70e+00). ETA=1 day, 3:52:53, max mem: 11.4 GB 
[10/26 20:13:33 visual_prompt]: Epoch 17 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 5.2000
[10/26 20:14:25 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1928, average loss: 2.2280
[10/26 20:14:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.08	
[10/26 20:14:25 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/26 20:15:58 visual_prompt]: 	Training 100/553. train loss: 1.9857,	0.5204 s / batch. (data: 9.30e-03). ETA=6:37:14, max mem: 11.4 GB 
[10/26 20:17:28 visual_prompt]: 	Training 200/553. train loss: 9.3762,	0.4913 s / batch. (data: 2.57e-04). ETA=6:14:12, max mem: 11.4 GB 
[10/26 20:18:57 visual_prompt]: 	Training 300/553. train loss: 0.5380,	0.5000 s / batch. (data: 2.67e-04). ETA=6:19:59, max mem: 11.4 GB 
[10/26 20:20:25 visual_prompt]: 	Training 400/553. train loss: 0.8163,	0.5123 s / batch. (data: 1.04e-02). ETA=6:28:31, max mem: 11.4 GB 
[10/26 20:21:53 visual_prompt]: 	Training 500/553. train loss: 4.5498,	0.5160 s / batch. (data: 2.72e-04). ETA=6:30:24, max mem: 11.4 GB 
[10/26 20:22:37 visual_prompt]: Epoch 18 / 100: avg data time: 3.95e-01, avg batch time: 0.8884, average train loss: 5.1056
[10/26 20:23:29 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1909, average loss: 1.0217
[10/26 20:23:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.49	
[10/26 20:23:29 visual_prompt]: Best epoch 18: best metric: -1.022
[10/26 20:23:29 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/26 20:25:01 visual_prompt]: 	Training 100/553. train loss: 6.6666,	0.5000 s / batch. (data: 2.83e-04). ETA=6:17:02, max mem: 11.4 GB 
[10/26 20:26:29 visual_prompt]: 	Training 200/553. train loss: 5.4647,	0.4840 s / batch. (data: 2.67e-04). ETA=6:04:10, max mem: 11.4 GB 
[10/26 20:27:58 visual_prompt]: 	Training 300/553. train loss: 0.0010,	1.1552 s / batch. (data: 6.63e-01). ETA=14:27:16, max mem: 11.4 GB 
[10/26 20:29:28 visual_prompt]: 	Training 400/553. train loss: 1.9409,	0.4997 s / batch. (data: 2.67e-04). ETA=6:14:17, max mem: 11.4 GB 
[10/26 20:30:52 visual_prompt]: 	Training 500/553. train loss: 1.7718,	0.5160 s / batch. (data: 7.97e-03). ETA=6:25:40, max mem: 11.4 GB 
[10/26 20:31:39 visual_prompt]: Epoch 19 / 100: avg data time: 3.91e-01, avg batch time: 0.8852, average train loss: 5.1869
[10/26 20:32:31 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.1905, average loss: 18.8231
[10/26 20:32:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.79	
[10/26 20:32:31 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[10/26 20:34:02 visual_prompt]: 	Training 100/553. train loss: 1.1407,	0.4885 s / batch. (data: 2.55e-04). ETA=6:03:50, max mem: 11.4 GB 
[10/26 20:35:32 visual_prompt]: 	Training 200/553. train loss: 2.1479,	0.5080 s / batch. (data: 7.95e-03). ETA=6:17:32, max mem: 11.4 GB 
[10/26 20:37:01 visual_prompt]: 	Training 300/553. train loss: 1.0877,	0.5120 s / batch. (data: 6.88e-04). ETA=6:19:40, max mem: 11.4 GB 
[10/26 20:38:30 visual_prompt]: 	Training 400/553. train loss: 12.4037,	0.4786 s / batch. (data: 3.04e-04). ETA=5:54:06, max mem: 11.4 GB 
[10/26 20:39:58 visual_prompt]: 	Training 500/553. train loss: 1.7724,	0.4927 s / batch. (data: 1.04e-02). ETA=6:03:44, max mem: 11.4 GB 
[10/26 20:40:45 visual_prompt]: Epoch 20 / 100: avg data time: 4.00e-01, avg batch time: 0.8936, average train loss: 6.2631
[10/26 20:41:38 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1913, average loss: 0.7667
[10/26 20:41:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.73	
[10/26 20:41:38 visual_prompt]: Best epoch 20: best metric: -0.767
[10/26 20:41:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[10/26 20:43:12 visual_prompt]: 	Training 100/553. train loss: 3.1162,	0.5080 s / batch. (data: 2.59e-04). ETA=6:13:44, max mem: 11.4 GB 
[10/26 20:44:41 visual_prompt]: 	Training 200/553. train loss: 15.3350,	0.5000 s / batch. (data: 2.13e-04). ETA=6:07:00, max mem: 11.4 GB 
[10/26 20:46:10 visual_prompt]: 	Training 300/553. train loss: 1.8649,	1.6503 s / batch. (data: 1.16e+00). ETA=20:08:32, max mem: 11.4 GB 
[10/26 20:47:37 visual_prompt]: 	Training 400/553. train loss: 0.2048,	0.4999 s / batch. (data: 4.47e-04). ETA=6:05:17, max mem: 11.4 GB 
[10/26 20:49:08 visual_prompt]: 	Training 500/553. train loss: 1.1634,	0.5040 s / batch. (data: 1.02e-02). ETA=6:07:26, max mem: 11.4 GB 
[10/26 20:49:52 visual_prompt]: Epoch 21 / 100: avg data time: 3.99e-01, avg batch time: 0.8937, average train loss: 5.6657
[10/26 20:50:45 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.1903, average loss: 1.5301
[10/26 20:50:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.43	
[10/26 20:50:45 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[10/26 20:52:16 visual_prompt]: 	Training 100/553. train loss: 2.2411,	0.4960 s / batch. (data: 2.69e-04). ETA=6:00:17, max mem: 11.4 GB 
[10/26 20:53:46 visual_prompt]: 	Training 200/553. train loss: 3.9229,	0.5034 s / batch. (data: 2.49e-04). ETA=6:04:51, max mem: 11.4 GB 
[10/26 20:55:12 visual_prompt]: 	Training 300/553. train loss: 0.0010,	0.4879 s / batch. (data: 3.62e-04). ETA=5:52:49, max mem: 11.4 GB 
[10/26 20:56:42 visual_prompt]: 	Training 400/553. train loss: 5.8634,	0.4908 s / batch. (data: 2.59e-04). ETA=5:54:06, max mem: 11.4 GB 
[10/26 20:58:11 visual_prompt]: 	Training 500/553. train loss: 1.9266,	0.5004 s / batch. (data: 7.99e-03). ETA=6:00:10, max mem: 11.4 GB 
[10/26 20:58:58 visual_prompt]: Epoch 22 / 100: avg data time: 3.98e-01, avg batch time: 0.8920, average train loss: 4.4686
[10/26 20:59:51 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1918, average loss: 3.2706
[10/26 20:59:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.10	
[10/26 20:59:51 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[10/26 21:01:24 visual_prompt]: 	Training 100/553. train loss: 0.6164,	0.5476 s / batch. (data: 4.35e-02). ETA=6:32:43, max mem: 11.4 GB 
[10/26 21:02:55 visual_prompt]: 	Training 200/553. train loss: 17.1310,	1.5605 s / batch. (data: 1.07e+00). ETA=18:36:37, max mem: 11.4 GB 
[10/26 21:04:25 visual_prompt]: 	Training 300/553. train loss: 3.6070,	0.5241 s / batch. (data: 1.68e-02). ETA=6:14:09, max mem: 11.4 GB 
[10/26 21:05:52 visual_prompt]: 	Training 400/553. train loss: 3.7909,	0.5001 s / batch. (data: 7.46e-04). ETA=5:56:10, max mem: 11.4 GB 
[10/26 21:07:19 visual_prompt]: 	Training 500/553. train loss: 0.5381,	0.5280 s / batch. (data: 2.65e-04). ETA=6:15:11, max mem: 11.4 GB 
[10/26 21:08:05 visual_prompt]: Epoch 23 / 100: avg data time: 3.99e-01, avg batch time: 0.8927, average train loss: 4.7311
[10/26 21:08:57 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1920, average loss: 4.4873
[10/26 21:08:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.31	
[10/26 21:08:57 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[10/26 21:10:27 visual_prompt]: 	Training 100/553. train loss: 14.3210,	0.4800 s / batch. (data: 2.72e-04). ETA=5:39:51, max mem: 11.4 GB 
[10/26 21:11:56 visual_prompt]: 	Training 200/553. train loss: 0.2510,	0.5039 s / batch. (data: 1.04e-03). ETA=5:55:57, max mem: 11.4 GB 
[10/26 21:13:26 visual_prompt]: 	Training 300/553. train loss: 2.9182,	1.4779 s / batch. (data: 9.90e-01). ETA=17:21:26, max mem: 11.4 GB 
[10/26 21:14:54 visual_prompt]: 	Training 400/553. train loss: 2.5248,	0.4881 s / batch. (data: 2.78e-04). ETA=5:43:10, max mem: 11.4 GB 
[10/26 21:16:25 visual_prompt]: 	Training 500/553. train loss: 6.5960,	0.7037 s / batch. (data: 2.06e-01). ETA=8:13:32, max mem: 11.4 GB 
[10/26 21:17:11 visual_prompt]: Epoch 24 / 100: avg data time: 3.98e-01, avg batch time: 0.8931, average train loss: 5.3354
[10/26 21:18:04 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1906, average loss: 1.0632
[10/26 21:18:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 53.20	
[10/26 21:18:04 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[10/26 21:19:39 visual_prompt]: 	Training 100/553. train loss: 0.0266,	0.4915 s / batch. (data: 2.57e-04). ETA=5:43:29, max mem: 11.4 GB 
[10/26 21:21:05 visual_prompt]: 	Training 200/553. train loss: 3.6534,	0.4960 s / batch. (data: 2.88e-04). ETA=5:45:45, max mem: 11.4 GB 
[10/26 21:22:33 visual_prompt]: 	Training 300/553. train loss: 5.9171,	0.4990 s / batch. (data: 2.54e-04). ETA=5:47:00, max mem: 11.4 GB 
[10/26 21:24:04 visual_prompt]: 	Training 400/553. train loss: 0.5744,	1.7711 s / batch. (data: 1.28e+00). ETA=20:28:49, max mem: 11.4 GB 
[10/26 21:25:33 visual_prompt]: 	Training 500/553. train loss: 4.6170,	2.0074 s / batch. (data: 1.52e+00). ETA=23:09:24, max mem: 11.4 GB 
[10/26 21:26:18 visual_prompt]: Epoch 25 / 100: avg data time: 3.98e-01, avg batch time: 0.8927, average train loss: 4.2081
[10/26 21:27:10 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1889, average loss: 9.0301
[10/26 21:27:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.37	
[10/26 21:27:10 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[10/26 21:28:42 visual_prompt]: 	Training 100/553. train loss: 3.9070,	0.5404 s / batch. (data: 2.43e-02). ETA=6:12:37, max mem: 11.4 GB 
[10/26 21:30:13 visual_prompt]: 	Training 200/553. train loss: 20.9396,	2.2040 s / batch. (data: 1.71e+00). ETA=1 day, 1:16:10, max mem: 11.4 GB 
[10/26 21:31:42 visual_prompt]: 	Training 300/553. train loss: 0.0004,	0.5069 s / batch. (data: 7.31e-04). ETA=5:47:52, max mem: 11.4 GB 
[10/26 21:33:09 visual_prompt]: 	Training 400/553. train loss: 1.7500,	0.4928 s / batch. (data: 2.73e-04). ETA=5:37:23, max mem: 11.4 GB 
[10/26 21:34:36 visual_prompt]: 	Training 500/553. train loss: 6.7239,	0.4787 s / batch. (data: 2.53e-04). ETA=5:26:53, max mem: 11.4 GB 
[10/26 21:35:22 visual_prompt]: Epoch 26 / 100: avg data time: 3.93e-01, avg batch time: 0.8880, average train loss: 5.1324
[10/26 21:36:14 visual_prompt]: Inference (val):avg data time: 2.52e-04, avg batch time: 0.1902, average loss: 2.7773
[10/26 21:36:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.50	
[10/26 21:36:14 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[10/26 21:37:46 visual_prompt]: 	Training 100/553. train loss: 2.2443,	0.5093 s / batch. (data: 1.67e-02). ETA=5:46:31, max mem: 11.4 GB 
[10/26 21:39:15 visual_prompt]: 	Training 200/553. train loss: 10.8247,	2.2545 s / batch. (data: 1.78e+00). ETA=1 day, 1:30:07, max mem: 11.4 GB 
[10/26 21:40:43 visual_prompt]: 	Training 300/553. train loss: 4.8428,	1.1483 s / batch. (data: 6.53e-01). ETA=12:57:24, max mem: 11.4 GB 
[10/26 21:42:12 visual_prompt]: 	Training 400/553. train loss: 6.2609,	0.5160 s / batch. (data: 7.07e-04). ETA=5:48:30, max mem: 11.4 GB 
[10/26 21:43:42 visual_prompt]: 	Training 500/553. train loss: 2.3257,	0.5004 s / batch. (data: 7.46e-04). ETA=5:37:07, max mem: 11.4 GB 
[10/26 21:44:26 visual_prompt]: Epoch 27 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 4.9289
[10/26 21:45:19 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1904, average loss: 3.4641
[10/26 21:45:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.38	
[10/26 21:45:19 visual_prompt]: Stopping early.
[10/26 21:45:19 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 21:45:19 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 21:45:19 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 21:45:19 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 21:45:19 visual_prompt]: Training with config:
[10/26 21:45:19 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr1.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 21:45:19 visual_prompt]: Loading training data...
[10/26 21:45:19 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 21:45:19 visual_prompt]: Loading validation data...
[10/26 21:45:19 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 21:45:19 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/26 21:45:21 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/26 21:45:21 visual_prompt]: tuned percent:0.529
[10/26 21:45:21 visual_prompt]: Device used for model: 0
[10/26 21:45:21 visual_prompt]: Setting up Evaluator...
[10/26 21:45:21 visual_prompt]: Setting up Trainer...
[10/26 21:45:21 visual_prompt]: 	Setting up the optimizer...
[10/26 21:45:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 21:46:53 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5040 s / batch. (data: 1.60e-02). ETA=7:43:43, max mem: 11.4 GB 
[10/26 21:48:20 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5000 s / batch. (data: 8.37e-03). ETA=7:39:08, max mem: 11.4 GB 
[10/26 21:49:53 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.0720 s / batch. (data: 2.58e+00). ETA=1 day, 22:56:00, max mem: 11.4 GB 
[10/26 21:51:19 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5000 s / batch. (data: 2.97e-04). ETA=7:37:29, max mem: 11.4 GB 
[10/26 21:52:50 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5193 s / batch. (data: 5.87e-03). ETA=7:54:16, max mem: 11.4 GB 
[10/26 21:53:36 visual_prompt]: Epoch 1 / 100: avg data time: 3.99e-01, avg batch time: 0.8943, average train loss: 1.3966
[10/26 21:54:28 visual_prompt]: Inference (val):avg data time: 3.89e-04, avg batch time: 0.1904, average loss: 1.3454
[10/26 21:54:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/26 21:54:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/26 21:55:59 visual_prompt]: 	Training 100/553. train loss: 0.8082,	0.5080 s / batch. (data: 3.20e-04). ETA=7:42:39, max mem: 11.4 GB 
[10/26 21:57:27 visual_prompt]: 	Training 200/553. train loss: 0.0487,	1.0520 s / batch. (data: 5.54e-01). ETA=15:56:24, max mem: 11.4 GB 
[10/26 21:58:58 visual_prompt]: 	Training 300/553. train loss: 0.8414,	1.5902 s / batch. (data: 1.11e+00). ETA=1 day, 0:03:01, max mem: 11.4 GB 
[10/26 22:00:24 visual_prompt]: 	Training 400/553. train loss: 1.1201,	0.8600 s / batch. (data: 3.61e-01). ETA=12:58:57, max mem: 11.4 GB 
[10/26 22:01:54 visual_prompt]: 	Training 500/553. train loss: 0.5814,	0.4863 s / batch. (data: 7.96e-03). ETA=7:19:38, max mem: 11.4 GB 
[10/26 22:02:39 visual_prompt]: Epoch 2 / 100: avg data time: 3.93e-01, avg batch time: 0.8870, average train loss: 0.8968
[10/26 22:03:31 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1908, average loss: 1.2630
[10/26 22:03:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.69	
[10/26 22:03:31 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/26 22:05:01 visual_prompt]: 	Training 100/553. train loss: 0.7759,	0.5000 s / batch. (data: 7.97e-03). ETA=7:30:47, max mem: 11.4 GB 
[10/26 22:06:31 visual_prompt]: 	Training 200/553. train loss: 0.9295,	0.4918 s / batch. (data: 2.86e-04). ETA=7:22:34, max mem: 11.4 GB 
[10/26 22:07:58 visual_prompt]: 	Training 300/553. train loss: 0.7642,	0.5240 s / batch. (data: 2.66e-04). ETA=7:50:39, max mem: 11.4 GB 
[10/26 22:09:28 visual_prompt]: 	Training 400/553. train loss: 0.4939,	0.4803 s / batch. (data: 2.59e-04). ETA=7:10:34, max mem: 11.4 GB 
[10/26 22:10:58 visual_prompt]: 	Training 500/553. train loss: 0.7018,	1.5680 s / batch. (data: 1.08e+00). ETA=23:23:12, max mem: 11.4 GB 
[10/26 22:11:42 visual_prompt]: Epoch 3 / 100: avg data time: 3.93e-01, avg batch time: 0.8880, average train loss: 0.9947
[10/26 22:12:34 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1905, average loss: 0.8391
[10/26 22:12:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.08	
[10/26 22:12:34 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/26 22:14:08 visual_prompt]: 	Training 100/553. train loss: 0.7077,	0.5328 s / batch. (data: 2.49e-02). ETA=7:55:27, max mem: 11.4 GB 
[10/26 22:15:37 visual_prompt]: 	Training 200/553. train loss: 0.9290,	0.5080 s / batch. (data: 2.61e-04). ETA=7:32:28, max mem: 11.4 GB 
[10/26 22:17:05 visual_prompt]: 	Training 300/553. train loss: 0.6868,	0.4910 s / batch. (data: 3.00e-04). ETA=7:16:29, max mem: 11.4 GB 
[10/26 22:18:29 visual_prompt]: 	Training 400/553. train loss: 0.6195,	0.4797 s / batch. (data: 2.90e-04). ETA=7:05:37, max mem: 11.4 GB 
[10/26 22:19:59 visual_prompt]: 	Training 500/553. train loss: 3.0063,	2.3360 s / batch. (data: 1.81e+00). ETA=1 day, 10:28:55, max mem: 11.4 GB 
[10/26 22:20:46 visual_prompt]: Epoch 4 / 100: avg data time: 3.94e-01, avg batch time: 0.8890, average train loss: 1.1220
[10/26 22:21:39 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1904, average loss: 0.6903
[10/26 22:21:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.75	
[10/26 22:21:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/26 22:23:09 visual_prompt]: 	Training 100/553. train loss: 2.1014,	0.4960 s / batch. (data: 7.95e-03). ETA=7:18:01, max mem: 11.4 GB 
[10/26 22:24:38 visual_prompt]: 	Training 200/553. train loss: 0.9740,	1.6514 s / batch. (data: 1.17e+00). ETA=1 day, 0:15:41, max mem: 11.4 GB 
[10/26 22:26:08 visual_prompt]: 	Training 300/553. train loss: 1.8681,	0.5000 s / batch. (data: 3.24e-04). ETA=7:19:51, max mem: 11.4 GB 
[10/26 22:27:35 visual_prompt]: 	Training 400/553. train loss: 1.8258,	0.5200 s / batch. (data: 7.97e-03). ETA=7:36:38, max mem: 11.4 GB 
[10/26 22:29:04 visual_prompt]: 	Training 500/553. train loss: 0.5172,	0.4920 s / batch. (data: 2.53e-04). ETA=7:11:13, max mem: 11.4 GB 
[10/26 22:29:50 visual_prompt]: Epoch 5 / 100: avg data time: 3.94e-01, avg batch time: 0.8887, average train loss: 1.2393
[10/26 22:30:42 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1926, average loss: 1.7541
[10/26 22:30:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[10/26 22:30:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/26 22:32:15 visual_prompt]: 	Training 100/553. train loss: 1.1599,	0.4929 s / batch. (data: 7.27e-04). ETA=7:10:43, max mem: 11.4 GB 
[10/26 22:33:44 visual_prompt]: 	Training 200/553. train loss: 2.5709,	0.5160 s / batch. (data: 2.79e-04). ETA=7:30:05, max mem: 11.4 GB 
[10/26 22:35:11 visual_prompt]: 	Training 300/553. train loss: 0.5509,	0.4840 s / batch. (data: 2.82e-04). ETA=7:01:21, max mem: 11.4 GB 
[10/26 22:36:42 visual_prompt]: 	Training 400/553. train loss: 1.0664,	0.5290 s / batch. (data: 4.56e-02). ETA=7:39:41, max mem: 11.4 GB 
[10/26 22:38:10 visual_prompt]: 	Training 500/553. train loss: 1.9321,	1.5475 s / batch. (data: 1.05e+00). ETA=22:22:04, max mem: 11.4 GB 
[10/26 22:38:54 visual_prompt]: Epoch 6 / 100: avg data time: 3.95e-01, avg batch time: 0.8894, average train loss: 1.4707
[10/26 22:39:47 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1891, average loss: 1.4529
[10/26 22:39:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.37	
[10/26 22:39:47 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/26 22:41:17 visual_prompt]: 	Training 100/553. train loss: 1.9750,	0.5120 s / batch. (data: 2.50e-04). ETA=7:22:45, max mem: 11.4 GB 
[10/26 22:42:46 visual_prompt]: 	Training 200/553. train loss: 0.9240,	0.4860 s / batch. (data: 7.97e-03). ETA=6:59:25, max mem: 11.4 GB 
[10/26 22:44:18 visual_prompt]: 	Training 300/553. train loss: 1.9148,	2.5337 s / batch. (data: 2.06e+00). ETA=1 day, 12:22:27, max mem: 11.4 GB 
[10/26 22:45:46 visual_prompt]: 	Training 400/553. train loss: 3.5495,	2.3280 s / batch. (data: 1.84e+00). ETA=1 day, 9:21:22, max mem: 11.4 GB 
[10/26 22:47:13 visual_prompt]: 	Training 500/553. train loss: 0.6319,	0.9280 s / batch. (data: 4.40e-01). ETA=13:16:16, max mem: 11.4 GB 
[10/26 22:47:57 visual_prompt]: Epoch 7 / 100: avg data time: 3.92e-01, avg batch time: 0.8865, average train loss: 1.7910
[10/26 22:48:49 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1909, average loss: 0.7795
[10/26 22:48:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.85	
[10/26 22:48:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/26 22:50:19 visual_prompt]: 	Training 100/553. train loss: 0.6735,	1.1914 s / batch. (data: 7.03e-01). ETA=16:59:13, max mem: 11.4 GB 
[10/26 22:51:49 visual_prompt]: 	Training 200/553. train loss: 0.6981,	0.5419 s / batch. (data: 2.07e-02). ETA=7:42:40, max mem: 11.4 GB 
[10/26 22:53:18 visual_prompt]: 	Training 300/553. train loss: 4.1897,	0.5029 s / batch. (data: 2.60e-04). ETA=7:08:32, max mem: 11.4 GB 
[10/26 22:54:47 visual_prompt]: 	Training 400/553. train loss: 1.0321,	0.5002 s / batch. (data: 1.05e-02). ETA=7:05:25, max mem: 11.4 GB 
[10/26 22:56:16 visual_prompt]: 	Training 500/553. train loss: 1.9940,	2.0206 s / batch. (data: 1.53e+00). ETA=1 day, 4:35:07, max mem: 11.4 GB 
[10/26 22:57:01 visual_prompt]: Epoch 8 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 2.1025
[10/26 22:57:53 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1908, average loss: 1.6207
[10/26 22:57:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.56	
[10/26 22:57:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/26 22:59:25 visual_prompt]: 	Training 100/553. train loss: 0.0112,	0.5040 s / batch. (data: 2.59e-04). ETA=7:06:31, max mem: 11.4 GB 
[10/26 23:00:52 visual_prompt]: 	Training 200/553. train loss: 0.6225,	0.4884 s / batch. (data: 2.52e-04). ETA=6:52:32, max mem: 11.4 GB 
[10/26 23:02:21 visual_prompt]: 	Training 300/553. train loss: 7.3433,	2.2760 s / batch. (data: 1.79e+00). ETA=1 day, 7:58:31, max mem: 11.4 GB 
[10/26 23:03:50 visual_prompt]: 	Training 400/553. train loss: 0.9497,	0.4803 s / batch. (data: 2.68e-04). ETA=6:44:04, max mem: 11.4 GB 
[10/26 23:05:20 visual_prompt]: 	Training 500/553. train loss: 5.2810,	1.5689 s / batch. (data: 1.09e+00). ETA=21:57:16, max mem: 11.4 GB 
[10/26 23:06:03 visual_prompt]: Epoch 9 / 100: avg data time: 3.93e-01, avg batch time: 0.8868, average train loss: 2.0200
[10/26 23:06:56 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1904, average loss: 1.5576
[10/26 23:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.26	
[10/26 23:06:56 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/26 23:08:30 visual_prompt]: 	Training 100/553. train loss: 1.2637,	0.5280 s / batch. (data: 7.97e-03). ETA=7:21:56, max mem: 11.4 GB 
[10/26 23:09:57 visual_prompt]: 	Training 200/553. train loss: 1.3963,	0.4789 s / batch. (data: 2.56e-04). ETA=6:40:05, max mem: 11.4 GB 
[10/26 23:11:25 visual_prompt]: 	Training 300/553. train loss: 4.5109,	0.5040 s / batch. (data: 7.98e-03). ETA=7:00:11, max mem: 11.4 GB 
[10/26 23:12:52 visual_prompt]: 	Training 400/553. train loss: 0.4302,	1.3698 s / batch. (data: 8.92e-01). ETA=18:59:45, max mem: 11.4 GB 
[10/26 23:14:21 visual_prompt]: 	Training 500/553. train loss: 2.6621,	1.5608 s / batch. (data: 1.08e+00). ETA=21:36:01, max mem: 11.4 GB 
[10/26 23:15:06 visual_prompt]: Epoch 10 / 100: avg data time: 3.93e-01, avg batch time: 0.8870, average train loss: 2.7314
[10/26 23:15:59 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1926, average loss: 0.6895
[10/26 23:15:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.41	
[10/26 23:15:59 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/26 23:17:32 visual_prompt]: 	Training 100/553. train loss: 2.4284,	0.4876 s / batch. (data: 2.70e-04). ETA=6:43:38, max mem: 11.4 GB 
[10/26 23:19:02 visual_prompt]: 	Training 200/553. train loss: 3.1160,	0.5040 s / batch. (data: 1.19e-02). ETA=6:56:21, max mem: 11.4 GB 
[10/26 23:20:31 visual_prompt]: 	Training 300/553. train loss: 0.0317,	2.2021 s / batch. (data: 1.71e+00). ETA=1 day, 6:15:39, max mem: 11.4 GB 
[10/26 23:21:58 visual_prompt]: 	Training 400/553. train loss: 1.1981,	0.5331 s / batch. (data: 8.02e-03). ETA=7:18:37, max mem: 11.4 GB 
[10/26 23:23:27 visual_prompt]: 	Training 500/553. train loss: 3.6657,	0.5280 s / batch. (data: 2.39e-02). ETA=7:13:32, max mem: 11.4 GB 
[10/26 23:24:11 visual_prompt]: Epoch 11 / 100: avg data time: 3.94e-01, avg batch time: 0.8900, average train loss: 2.6271
[10/26 23:25:03 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.1904, average loss: 0.6907
[10/26 23:25:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 51.77	
[10/26 23:25:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/26 23:26:36 visual_prompt]: 	Training 100/553. train loss: 2.4362,	0.5000 s / batch. (data: 7.97e-03). ETA=6:49:19, max mem: 11.4 GB 
[10/26 23:28:06 visual_prompt]: 	Training 200/553. train loss: 1.0005,	0.5204 s / batch. (data: 1.55e-02). ETA=7:05:10, max mem: 11.4 GB 
[10/26 23:29:33 visual_prompt]: 	Training 300/553. train loss: 0.7148,	0.5137 s / batch. (data: 9.05e-03). ETA=6:58:46, max mem: 11.4 GB 
[10/26 23:31:02 visual_prompt]: 	Training 400/553. train loss: 3.2733,	0.5030 s / batch. (data: 2.30e-02). ETA=6:49:16, max mem: 11.4 GB 
[10/26 23:32:31 visual_prompt]: 	Training 500/553. train loss: 1.3391,	0.4974 s / batch. (data: 1.03e-02). ETA=6:43:51, max mem: 11.4 GB 
[10/26 23:33:16 visual_prompt]: Epoch 12 / 100: avg data time: 3.96e-01, avg batch time: 0.8905, average train loss: 2.6639
[10/26 23:34:08 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1908, average loss: 2.1217
[10/26 23:34:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.40	
[10/26 23:34:08 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/26 23:35:40 visual_prompt]: 	Training 100/553. train loss: 1.6441,	0.8589 s / batch. (data: 3.77e-01). ETA=11:35:13, max mem: 11.4 GB 
[10/26 23:37:06 visual_prompt]: 	Training 200/553. train loss: 0.7418,	0.6137 s / batch. (data: 1.33e-01). ETA=8:15:40, max mem: 11.4 GB 
[10/26 23:38:36 visual_prompt]: 	Training 300/553. train loss: 1.0867,	1.6987 s / batch. (data: 1.20e+00). ETA=22:49:18, max mem: 11.4 GB 
[10/26 23:40:03 visual_prompt]: 	Training 400/553. train loss: 7.0001,	0.5240 s / batch. (data: 7.97e-03). ETA=7:01:30, max mem: 11.4 GB 
[10/26 23:41:33 visual_prompt]: 	Training 500/553. train loss: 2.9346,	0.4880 s / batch. (data: 2.53e-04). ETA=6:31:45, max mem: 11.4 GB 
[10/26 23:42:18 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8861, average train loss: 2.9897
[10/26 23:43:11 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1919, average loss: 0.9826
[10/26 23:43:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.76	
[10/26 23:43:11 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/26 23:44:43 visual_prompt]: 	Training 100/553. train loss: 1.0088,	0.4919 s / batch. (data: 1.05e-02). ETA=6:33:34, max mem: 11.4 GB 
[10/26 23:46:11 visual_prompt]: 	Training 200/553. train loss: 0.0040,	0.4840 s / batch. (data: 5.39e-03). ETA=6:26:27, max mem: 11.4 GB 
[10/26 23:47:40 visual_prompt]: 	Training 300/553. train loss: 0.6612,	1.3720 s / batch. (data: 8.64e-01). ETA=18:13:14, max mem: 11.4 GB 
[10/26 23:49:08 visual_prompt]: 	Training 400/553. train loss: 0.6725,	0.5006 s / batch. (data: 7.16e-03). ETA=6:38:02, max mem: 11.4 GB 
[10/26 23:50:38 visual_prompt]: 	Training 500/553. train loss: 0.6956,	0.5044 s / batch. (data: 1.05e-02). ETA=6:40:13, max mem: 11.4 GB 
[10/26 23:51:21 visual_prompt]: Epoch 14 / 100: avg data time: 3.91e-01, avg batch time: 0.8867, average train loss: 2.3852
[10/26 23:52:13 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1922, average loss: 1.5566
[10/26 23:52:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.18	
[10/26 23:52:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/26 23:53:45 visual_prompt]: 	Training 100/553. train loss: 2.3484,	0.4836 s / batch. (data: 2.83e-04). ETA=6:22:31, max mem: 11.4 GB 
[10/26 23:55:12 visual_prompt]: 	Training 200/553. train loss: 7.4340,	0.5214 s / batch. (data: 9.37e-03). ETA=6:51:32, max mem: 11.4 GB 
[10/26 23:56:42 visual_prompt]: 	Training 300/553. train loss: 13.9058,	0.5000 s / batch. (data: 2.72e-04). ETA=6:33:49, max mem: 11.4 GB 
[10/26 23:58:09 visual_prompt]: 	Training 400/553. train loss: 0.6012,	0.4927 s / batch. (data: 1.23e-02). ETA=6:27:13, max mem: 11.4 GB 
[10/26 23:59:38 visual_prompt]: 	Training 500/553. train loss: 0.5405,	0.5080 s / batch. (data: 5.38e-03). ETA=6:38:24, max mem: 11.4 GB 
[10/27 00:00:25 visual_prompt]: Epoch 15 / 100: avg data time: 3.93e-01, avg batch time: 0.8880, average train loss: 3.5940
[10/27 00:01:17 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1908, average loss: 5.6167
[10/27 00:01:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.82	
[10/27 00:01:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/27 00:02:47 visual_prompt]: 	Training 100/553. train loss: 6.4042,	0.4882 s / batch. (data: 2.78e-04). ETA=6:21:38, max mem: 11.4 GB 
[10/27 00:04:16 visual_prompt]: 	Training 200/553. train loss: 1.6837,	0.4960 s / batch. (data: 5.38e-03). ETA=6:26:55, max mem: 11.4 GB 
[10/27 00:05:45 visual_prompt]: 	Training 300/553. train loss: 0.5531,	0.5040 s / batch. (data: 2.46e-04). ETA=6:32:19, max mem: 11.4 GB 
[10/27 00:07:14 visual_prompt]: 	Training 400/553. train loss: 6.0491,	0.4985 s / batch. (data: 7.03e-04). ETA=6:27:10, max mem: 11.4 GB 
[10/27 00:08:42 visual_prompt]: 	Training 500/553. train loss: 1.2648,	1.7200 s / batch. (data: 1.21e+00). ETA=22:13:09, max mem: 11.4 GB 
[10/27 00:09:27 visual_prompt]: Epoch 16 / 100: avg data time: 3.92e-01, avg batch time: 0.8863, average train loss: 2.9072
[10/27 00:10:19 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1904, average loss: 1.0607
[10/27 00:10:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.12	
[10/27 00:10:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/27 00:11:50 visual_prompt]: 	Training 100/553. train loss: 1.1105,	0.4960 s / batch. (data: 2.61e-04). ETA=6:23:10, max mem: 11.4 GB 
[10/27 00:13:21 visual_prompt]: 	Training 200/553. train loss: 5.3381,	0.4917 s / batch. (data: 7.98e-03). ETA=6:19:00, max mem: 11.4 GB 
[10/27 00:14:49 visual_prompt]: 	Training 300/553. train loss: 2.8771,	0.5246 s / batch. (data: 3.24e-02). ETA=6:43:29, max mem: 11.4 GB 
[10/27 00:16:17 visual_prompt]: 	Training 400/553. train loss: 0.3425,	1.7960 s / batch. (data: 1.29e+00). ETA=22:58:28, max mem: 11.4 GB 
[10/27 00:17:45 visual_prompt]: 	Training 500/553. train loss: 2.6036,	2.2913 s / batch. (data: 1.81e+00). ETA=1 day, 5:14:48, max mem: 11.4 GB 
[10/27 00:18:30 visual_prompt]: Epoch 17 / 100: avg data time: 3.93e-01, avg batch time: 0.8878, average train loss: 2.9359
[10/27 00:19:23 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1910, average loss: 2.1445
[10/27 00:19:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.46	
[10/27 00:19:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/27 00:20:55 visual_prompt]: 	Training 100/553. train loss: 3.8399,	0.5000 s / batch. (data: 2.52e-04). ETA=6:21:38, max mem: 11.4 GB 
[10/27 00:22:26 visual_prompt]: 	Training 200/553. train loss: 1.1367,	0.4960 s / batch. (data: 2.73e-04). ETA=6:17:46, max mem: 11.4 GB 
[10/27 00:23:55 visual_prompt]: 	Training 300/553. train loss: 0.5283,	0.4916 s / batch. (data: 2.81e-04). ETA=6:13:37, max mem: 11.4 GB 
[10/27 00:25:23 visual_prompt]: 	Training 400/553. train loss: 1.2137,	0.5069 s / batch. (data: 7.97e-03). ETA=6:24:23, max mem: 11.4 GB 
[10/27 00:26:51 visual_prompt]: 	Training 500/553. train loss: 0.7524,	1.8800 s / batch. (data: 1.37e+00). ETA=23:42:29, max mem: 11.4 GB 
[10/27 00:27:35 visual_prompt]: Epoch 18 / 100: avg data time: 3.95e-01, avg batch time: 0.8890, average train loss: 2.6081
[10/27 00:28:27 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1908, average loss: 15.9698
[10/27 00:28:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.80	
[10/27 00:28:27 visual_prompt]: Stopping early.
[10/27 00:28:27 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 00:28:27 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 00:28:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 00:28:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 00:28:27 visual_prompt]: Training with config:
[10/27 00:28:27 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr1.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 00:28:27 visual_prompt]: Loading training data...
[10/27 00:28:27 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 00:28:27 visual_prompt]: Loading validation data...
[10/27 00:28:27 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 00:28:27 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/27 00:28:29 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/27 00:28:29 visual_prompt]: tuned percent:0.529
[10/27 00:28:30 visual_prompt]: Device used for model: 0
[10/27 00:28:30 visual_prompt]: Setting up Evaluator...
[10/27 00:28:30 visual_prompt]: Setting up Trainer...
[10/27 00:28:30 visual_prompt]: 	Setting up the optimizer...
[10/27 00:28:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 00:30:01 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5044 s / batch. (data: 8.30e-03). ETA=7:44:00, max mem: 11.4 GB 
[10/27 00:31:28 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4838 s / batch. (data: 5.39e-03). ETA=7:24:19, max mem: 11.4 GB 
[10/27 00:33:00 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9503 s / batch. (data: 2.45e+00). ETA=1 day, 21:04:25, max mem: 11.4 GB 
[10/27 00:34:26 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5000 s / batch. (data: 2.87e-04). ETA=7:37:30, max mem: 11.4 GB 
[10/27 00:35:57 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4911 s / batch. (data: 5.42e-03). ETA=7:28:34, max mem: 11.4 GB 
[10/27 00:36:43 visual_prompt]: Epoch 1 / 100: avg data time: 3.98e-01, avg batch time: 0.8930, average train loss: 1.3966
[10/27 00:37:36 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1908, average loss: 1.3454
[10/27 00:37:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/27 00:37:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/27 00:39:08 visual_prompt]: 	Training 100/553. train loss: 0.8890,	1.3818 s / batch. (data: 8.90e-01). ETA=20:58:29, max mem: 11.4 GB 
[10/27 00:40:37 visual_prompt]: 	Training 200/553. train loss: 0.0253,	1.4402 s / batch. (data: 9.47e-01). ETA=21:49:17, max mem: 11.4 GB 
[10/27 00:42:08 visual_prompt]: 	Training 300/553. train loss: 0.6565,	1.7560 s / batch. (data: 1.26e+00). ETA=1 day, 2:33:27, max mem: 11.4 GB 
[10/27 00:43:35 visual_prompt]: 	Training 400/553. train loss: 1.0473,	0.4960 s / batch. (data: 2.98e-04). ETA=7:29:16, max mem: 11.4 GB 
[10/27 00:45:05 visual_prompt]: 	Training 500/553. train loss: 0.6098,	0.5040 s / batch. (data: 2.51e-04). ETA=7:35:41, max mem: 11.4 GB 
[10/27 00:45:49 visual_prompt]: Epoch 2 / 100: avg data time: 3.97e-01, avg batch time: 0.8916, average train loss: 0.9690
[10/27 00:46:42 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1910, average loss: 1.2446
[10/27 00:46:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.55	
[10/27 00:46:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/27 00:48:12 visual_prompt]: 	Training 100/553. train loss: 1.2385,	0.4892 s / batch. (data: 2.66e-04). ETA=7:21:05, max mem: 11.4 GB 
[10/27 00:49:41 visual_prompt]: 	Training 200/553. train loss: 0.8409,	0.4920 s / batch. (data: 2.89e-04). ETA=7:22:43, max mem: 11.4 GB 
[10/27 00:51:08 visual_prompt]: 	Training 300/553. train loss: 0.7130,	0.4916 s / batch. (data: 2.89e-04). ETA=7:21:36, max mem: 11.4 GB 
[10/27 00:52:39 visual_prompt]: 	Training 400/553. train loss: 3.4049,	0.4972 s / batch. (data: 2.67e-04). ETA=7:25:44, max mem: 11.4 GB 
[10/27 00:54:08 visual_prompt]: 	Training 500/553. train loss: 0.7092,	1.7120 s / batch. (data: 1.23e+00). ETA=1 day, 1:32:02, max mem: 11.4 GB 
[10/27 00:54:52 visual_prompt]: Epoch 3 / 100: avg data time: 3.92e-01, avg batch time: 0.8870, average train loss: 0.9812
[10/27 00:55:45 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1911, average loss: 0.8214
[10/27 00:55:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.48	
[10/27 00:55:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/27 00:57:18 visual_prompt]: 	Training 100/553. train loss: 0.7062,	0.5000 s / batch. (data: 2.59e-04). ETA=7:26:09, max mem: 11.4 GB 
[10/27 00:58:47 visual_prompt]: 	Training 200/553. train loss: 0.6123,	0.5036 s / batch. (data: 2.68e-04). ETA=7:28:32, max mem: 11.4 GB 
[10/27 01:00:15 visual_prompt]: 	Training 300/553. train loss: 0.5751,	0.5042 s / batch. (data: 5.40e-03). ETA=7:28:17, max mem: 11.4 GB 
[10/27 01:01:40 visual_prompt]: 	Training 400/553. train loss: 1.3102,	1.2612 s / batch. (data: 7.75e-01). ETA=18:39:09, max mem: 11.4 GB 
[10/27 01:03:11 visual_prompt]: 	Training 500/553. train loss: 2.2759,	3.5120 s / batch. (data: 3.02e+00). ETA=2 days, 3:50:30, max mem: 11.4 GB 
[10/27 01:03:56 visual_prompt]: Epoch 4 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 1.0658
[10/27 01:04:49 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.1915, average loss: 2.2521
[10/27 01:04:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.09	
[10/27 01:04:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/27 01:06:20 visual_prompt]: 	Training 100/553. train loss: 3.4440,	0.5040 s / batch. (data: 7.65e-04). ETA=7:25:06, max mem: 11.4 GB 
[10/27 01:07:48 visual_prompt]: 	Training 200/553. train loss: 1.3251,	1.7113 s / batch. (data: 1.21e+00). ETA=1 day, 1:08:29, max mem: 11.4 GB 
[10/27 01:09:18 visual_prompt]: 	Training 300/553. train loss: 3.7286,	0.5156 s / batch. (data: 2.55e-04). ETA=7:33:35, max mem: 11.4 GB 
[10/27 01:10:45 visual_prompt]: 	Training 400/553. train loss: 2.3102,	0.5240 s / batch. (data: 1.19e-02). ETA=7:40:06, max mem: 11.4 GB 
[10/27 01:12:15 visual_prompt]: 	Training 500/553. train loss: 1.0222,	0.5200 s / batch. (data: 2.60e-04). ETA=7:35:45, max mem: 11.4 GB 
[10/27 01:13:00 visual_prompt]: Epoch 5 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 1.2780
[10/27 01:13:53 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1919, average loss: 2.7140
[10/27 01:13:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.94	
[10/27 01:13:53 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/27 01:15:26 visual_prompt]: 	Training 100/553. train loss: 0.6041,	0.5095 s / batch. (data: 1.05e-02). ETA=7:25:15, max mem: 11.4 GB 
[10/27 01:16:53 visual_prompt]: 	Training 200/553. train loss: 0.6452,	0.4919 s / batch. (data: 1.17e-02). ETA=7:09:01, max mem: 11.4 GB 
[10/27 01:18:21 visual_prompt]: 	Training 300/553. train loss: 0.6059,	0.5120 s / batch. (data: 5.37e-03). ETA=7:25:44, max mem: 11.4 GB 
[10/27 01:19:53 visual_prompt]: 	Training 400/553. train loss: 1.1379,	0.4786 s / batch. (data: 2.79e-04). ETA=6:55:53, max mem: 11.4 GB 
[10/27 01:21:20 visual_prompt]: 	Training 500/553. train loss: 4.7839,	1.3520 s / batch. (data: 8.51e-01). ETA=19:32:33, max mem: 11.4 GB 
[10/27 01:22:05 visual_prompt]: Epoch 6 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 1.4312
[10/27 01:22:57 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1917, average loss: 0.7051
[10/27 01:22:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.64	
[10/27 01:22:57 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/27 01:24:27 visual_prompt]: 	Training 100/553. train loss: 2.2735,	0.5004 s / batch. (data: 2.61e-04). ETA=7:12:40, max mem: 11.4 GB 
[10/27 01:25:56 visual_prompt]: 	Training 200/553. train loss: 1.0241,	0.5074 s / batch. (data: 7.96e-03). ETA=7:17:53, max mem: 11.4 GB 
[10/27 01:27:28 visual_prompt]: 	Training 300/553. train loss: 1.3398,	2.3840 s / batch. (data: 1.89e+00). ETA=1 day, 10:13:28, max mem: 11.4 GB 
[10/27 01:28:57 visual_prompt]: 	Training 400/553. train loss: 0.8546,	2.3584 s / batch. (data: 1.87e+00). ETA=1 day, 9:47:29, max mem: 11.4 GB 
[10/27 01:30:22 visual_prompt]: 	Training 500/553. train loss: 0.8384,	0.8062 s / batch. (data: 3.01e-01). ETA=11:31:46, max mem: 11.4 GB 
[10/27 01:31:07 visual_prompt]: Epoch 7 / 100: avg data time: 3.91e-01, avg batch time: 0.8859, average train loss: 1.3504
[10/27 01:32:00 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.1912, average loss: 0.7184
[10/27 01:32:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.09	
[10/27 01:32:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/27 01:33:29 visual_prompt]: 	Training 100/553. train loss: 1.1934,	0.4876 s / batch. (data: 8.71e-04). ETA=6:57:07, max mem: 11.4 GB 
[10/27 01:34:59 visual_prompt]: 	Training 200/553. train loss: 0.9501,	0.4960 s / batch. (data: 3.26e-04). ETA=7:03:29, max mem: 11.4 GB 
[10/27 01:36:28 visual_prompt]: 	Training 300/553. train loss: 3.8148,	0.4929 s / batch. (data: 2.68e-04). ETA=6:59:59, max mem: 11.4 GB 
[10/27 01:37:58 visual_prompt]: 	Training 400/553. train loss: 0.7289,	1.5280 s / batch. (data: 1.05e+00). ETA=21:39:32, max mem: 11.4 GB 
[10/27 01:39:27 visual_prompt]: 	Training 500/553. train loss: 0.6301,	1.9396 s / batch. (data: 1.46e+00). ETA=1 day, 3:26:21, max mem: 11.4 GB 
[10/27 01:40:12 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8896, average train loss: 1.7192
[10/27 01:41:04 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1917, average loss: 0.8140
[10/27 01:41:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.79	
[10/27 01:41:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/27 01:42:35 visual_prompt]: 	Training 100/553. train loss: 0.0007,	0.4892 s / batch. (data: 2.65e-04). ETA=6:54:01, max mem: 11.4 GB 
[10/27 01:44:04 visual_prompt]: 	Training 200/553. train loss: 0.5870,	0.5034 s / batch. (data: 3.27e-04). ETA=7:05:11, max mem: 11.4 GB 
[10/27 01:45:32 visual_prompt]: 	Training 300/553. train loss: 0.6121,	1.5835 s / batch. (data: 1.11e+00). ETA=22:14:46, max mem: 11.4 GB 
[10/27 01:47:01 visual_prompt]: 	Training 400/553. train loss: 0.7788,	0.5040 s / batch. (data: 2.52e-04). ETA=7:04:00, max mem: 11.4 GB 
[10/27 01:48:31 visual_prompt]: 	Training 500/553. train loss: 0.7784,	1.4040 s / batch. (data: 9.07e-01). ETA=19:38:47, max mem: 11.4 GB 
[10/27 01:49:14 visual_prompt]: Epoch 9 / 100: avg data time: 3.91e-01, avg batch time: 0.8860, average train loss: 1.5524
[10/27 01:50:06 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1907, average loss: 0.6883
[10/27 01:50:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.28	
[10/27 01:50:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/27 01:51:40 visual_prompt]: 	Training 100/553. train loss: 6.5076,	0.4918 s / batch. (data: 2.62e-04). ETA=6:51:41, max mem: 11.4 GB 
[10/27 01:53:07 visual_prompt]: 	Training 200/553. train loss: 0.6379,	0.5200 s / batch. (data: 7.97e-03). ETA=7:14:25, max mem: 11.4 GB 
[10/27 01:54:36 visual_prompt]: 	Training 300/553. train loss: 3.9486,	1.8803 s / batch. (data: 1.39e+00). ETA=1 day, 2:07:39, max mem: 11.4 GB 
[10/27 01:56:03 visual_prompt]: 	Training 400/553. train loss: 1.5665,	1.3917 s / batch. (data: 8.91e-01). ETA=19:17:59, max mem: 11.4 GB 
[10/27 01:57:32 visual_prompt]: 	Training 500/553. train loss: 0.9337,	1.5438 s / batch. (data: 1.07e+00). ETA=21:21:55, max mem: 11.4 GB 
[10/27 01:58:17 visual_prompt]: Epoch 10 / 100: avg data time: 3.94e-01, avg batch time: 0.8878, average train loss: 2.1492
[10/27 01:59:10 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1908, average loss: 0.7116
[10/27 01:59:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.48	
[10/27 01:59:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/27 02:00:43 visual_prompt]: 	Training 100/553. train loss: 1.3738,	0.4880 s / batch. (data: 2.61e-04). ETA=6:43:59, max mem: 11.4 GB 
[10/27 02:02:13 visual_prompt]: 	Training 200/553. train loss: 0.6922,	0.5120 s / batch. (data: 2.54e-04). ETA=7:03:00, max mem: 11.4 GB 
[10/27 02:03:42 visual_prompt]: 	Training 300/553. train loss: 0.0805,	2.5854 s / batch. (data: 2.10e+00). ETA=1 day, 11:31:38, max mem: 11.4 GB 
[10/27 02:05:09 visual_prompt]: 	Training 400/553. train loss: 0.5679,	0.5120 s / batch. (data: 2.78e-04). ETA=7:01:16, max mem: 11.4 GB 
[10/27 02:06:36 visual_prompt]: 	Training 500/553. train loss: 1.0256,	0.5086 s / batch. (data: 1.04e-02). ETA=6:57:38, max mem: 11.4 GB 
[10/27 02:07:21 visual_prompt]: Epoch 11 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 1.3334
[10/27 02:08:13 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1944, average loss: 0.6894
[10/27 02:08:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.31	
[10/27 02:08:13 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/27 02:09:47 visual_prompt]: 	Training 100/553. train loss: 0.9257,	1.3168 s / batch. (data: 8.40e-01). ETA=17:57:57, max mem: 11.4 GB 
[10/27 02:11:16 visual_prompt]: 	Training 200/553. train loss: 1.4529,	0.5168 s / batch. (data: 2.09e-02). ETA=7:02:12, max mem: 11.4 GB 
[10/27 02:12:43 visual_prompt]: 	Training 300/553. train loss: 4.1103,	0.5080 s / batch. (data: 7.95e-03). ETA=6:54:08, max mem: 11.4 GB 
[10/27 02:14:12 visual_prompt]: 	Training 400/553. train loss: 3.3238,	0.5310 s / batch. (data: 5.38e-03). ETA=7:12:01, max mem: 11.4 GB 
[10/27 02:15:41 visual_prompt]: 	Training 500/553. train loss: 1.7490,	0.5118 s / batch. (data: 5.37e-03). ETA=6:55:35, max mem: 11.4 GB 
[10/27 02:16:25 visual_prompt]: Epoch 12 / 100: avg data time: 3.93e-01, avg batch time: 0.8891, average train loss: 2.1733
[10/27 02:17:17 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1900, average loss: 0.7397
[10/27 02:17:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.72	
[10/27 02:17:17 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/27 02:18:49 visual_prompt]: 	Training 100/553. train loss: 1.3854,	0.5837 s / batch. (data: 9.24e-02). ETA=7:52:28, max mem: 11.4 GB 
[10/27 02:20:15 visual_prompt]: 	Training 200/553. train loss: 0.7921,	0.7613 s / batch. (data: 2.75e-01). ETA=10:14:53, max mem: 11.4 GB 
[10/27 02:21:45 visual_prompt]: 	Training 300/553. train loss: 0.4992,	1.9983 s / batch. (data: 1.52e+00). ETA=1 day, 2:50:46, max mem: 11.4 GB 
[10/27 02:23:12 visual_prompt]: 	Training 400/553. train loss: 0.4793,	1.1502 s / batch. (data: 6.53e-01). ETA=15:25:11, max mem: 11.4 GB 
[10/27 02:24:42 visual_prompt]: 	Training 500/553. train loss: 7.6401,	0.5096 s / batch. (data: 5.40e-03). ETA=6:49:03, max mem: 11.4 GB 
[10/27 02:25:27 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8861, average train loss: 2.7759
[10/27 02:26:19 visual_prompt]: Inference (val):avg data time: 3.36e-04, avg batch time: 0.1912, average loss: 1.1828
[10/27 02:26:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.50	
[10/27 02:26:19 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/27 02:27:52 visual_prompt]: 	Training 100/553. train loss: 1.8071,	0.4880 s / batch. (data: 2.76e-04). ETA=6:30:28, max mem: 11.4 GB 
[10/27 02:29:21 visual_prompt]: 	Training 200/553. train loss: 0.0230,	1.6443 s / batch. (data: 1.14e+00). ETA=21:52:58, max mem: 11.4 GB 
[10/27 02:30:49 visual_prompt]: 	Training 300/553. train loss: 0.6713,	1.2009 s / batch. (data: 6.86e-01). ETA=15:56:55, max mem: 11.4 GB 
[10/27 02:32:17 visual_prompt]: 	Training 400/553. train loss: 0.6550,	0.4839 s / batch. (data: 4.23e-04). ETA=6:24:47, max mem: 11.4 GB 
[10/27 02:33:46 visual_prompt]: 	Training 500/553. train loss: 0.6776,	0.4867 s / batch. (data: 2.92e-04). ETA=6:26:10, max mem: 11.4 GB 
[10/27 02:34:30 visual_prompt]: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8877, average train loss: 2.1620
[10/27 02:35:22 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1893, average loss: 2.4873
[10/27 02:35:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.65	
[10/27 02:35:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/27 02:36:54 visual_prompt]: 	Training 100/553. train loss: 0.6810,	0.5078 s / batch. (data: 1.17e-02). ETA=6:41:37, max mem: 11.4 GB 
[10/27 02:38:21 visual_prompt]: 	Training 200/553. train loss: 20.3426,	0.4812 s / batch. (data: 2.62e-04). ETA=6:19:47, max mem: 11.4 GB 
[10/27 02:39:51 visual_prompt]: 	Training 300/553. train loss: 1.8218,	0.4783 s / batch. (data: 2.62e-04). ETA=6:16:41, max mem: 11.4 GB 
[10/27 02:41:18 visual_prompt]: 	Training 400/553. train loss: 1.9726,	0.4787 s / batch. (data: 2.55e-04). ETA=6:16:14, max mem: 11.4 GB 
[10/27 02:42:47 visual_prompt]: 	Training 500/553. train loss: 0.4930,	0.5410 s / batch. (data: 4.30e-02). ETA=7:04:18, max mem: 11.4 GB 
[10/27 02:43:34 visual_prompt]: Epoch 15 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 3.5123
[10/27 02:44:26 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1922, average loss: 2.0144
[10/27 02:44:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.09	
[10/27 02:44:26 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/27 02:45:57 visual_prompt]: 	Training 100/553. train loss: 1.3928,	0.4808 s / batch. (data: 2.55e-04). ETA=6:15:52, max mem: 11.4 GB 
[10/27 02:47:26 visual_prompt]: 	Training 200/553. train loss: 2.1955,	0.4946 s / batch. (data: 2.74e-04). ETA=6:25:50, max mem: 11.4 GB 
[10/27 02:48:54 visual_prompt]: 	Training 300/553. train loss: 1.5951,	0.5010 s / batch. (data: 7.25e-04). ETA=6:30:00, max mem: 11.4 GB 
[10/27 02:50:22 visual_prompt]: 	Training 400/553. train loss: 2.0183,	0.5080 s / batch. (data: 7.97e-03). ETA=6:34:35, max mem: 11.4 GB 
[10/27 02:51:51 visual_prompt]: 	Training 500/553. train loss: 0.6304,	1.9809 s / batch. (data: 1.49e+00). ETA=1 day, 1:35:22, max mem: 11.4 GB 
[10/27 02:52:36 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 0.8865, average train loss: 1.7012
[10/27 02:53:29 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1911, average loss: 0.9455
[10/27 02:53:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.27	
[10/27 02:53:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/27 02:54:59 visual_prompt]: 	Training 100/553. train loss: 1.0466,	0.4999 s / batch. (data: 2.63e-04). ETA=6:26:13, max mem: 11.4 GB 
[10/27 02:56:30 visual_prompt]: 	Training 200/553. train loss: 5.8608,	0.5040 s / batch. (data: 2.60e-04). ETA=6:28:30, max mem: 11.4 GB 
[10/27 02:57:58 visual_prompt]: 	Training 300/553. train loss: 0.6215,	0.5081 s / batch. (data: 1.05e-02). ETA=6:30:47, max mem: 11.4 GB 
[10/27 02:59:25 visual_prompt]: 	Training 400/553. train loss: 0.7759,	0.4859 s / batch. (data: 2.91e-04). ETA=6:12:58, max mem: 11.4 GB 
[10/27 03:00:54 visual_prompt]: 	Training 500/553. train loss: 0.8710,	2.1228 s / batch. (data: 1.63e+00). ETA=1 day, 3:05:45, max mem: 11.4 GB 
[10/27 03:01:39 visual_prompt]: Epoch 17 / 100: avg data time: 3.93e-01, avg batch time: 0.8876, average train loss: 2.4519
[10/27 03:02:32 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1905, average loss: 2.9329
[10/27 03:02:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.66	
[10/27 03:02:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/27 03:04:04 visual_prompt]: 	Training 100/553. train loss: 1.8211,	0.5026 s / batch. (data: 2.62e-04). ETA=6:23:36, max mem: 11.4 GB 
[10/27 03:05:34 visual_prompt]: 	Training 200/553. train loss: 0.6931,	0.5000 s / batch. (data: 2.45e-04). ETA=6:20:48, max mem: 11.4 GB 
[10/27 03:07:03 visual_prompt]: 	Training 300/553. train loss: 0.8713,	0.5051 s / batch. (data: 9.14e-03). ETA=6:23:51, max mem: 11.4 GB 
[10/27 03:08:32 visual_prompt]: 	Training 400/553. train loss: 0.8424,	0.5103 s / batch. (data: 6.29e-03). ETA=6:27:00, max mem: 11.4 GB 
[10/27 03:10:00 visual_prompt]: 	Training 500/553. train loss: 0.9342,	0.5014 s / batch. (data: 1.32e-03). ETA=6:19:22, max mem: 11.4 GB 
[10/27 03:10:44 visual_prompt]: Epoch 18 / 100: avg data time: 3.95e-01, avg batch time: 0.8892, average train loss: 2.8838
[10/27 03:11:36 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1907, average loss: 1.3239
[10/27 03:11:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.73	
[10/27 03:11:36 visual_prompt]: Stopping early.
[10/27 03:11:36 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 03:11:36 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 03:11:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 03:11:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 03:11:36 visual_prompt]: Training with config:
[10/27 03:11:36 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr1.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 03:11:36 visual_prompt]: Loading training data...
[10/27 03:11:36 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 03:11:36 visual_prompt]: Loading validation data...
[10/27 03:11:36 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 03:11:36 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/27 03:11:38 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/27 03:11:38 visual_prompt]: tuned percent:0.529
[10/27 03:11:38 visual_prompt]: Device used for model: 0
[10/27 03:11:38 visual_prompt]: Setting up Evaluator...
[10/27 03:11:38 visual_prompt]: Setting up Trainer...
[10/27 03:11:38 visual_prompt]: 	Setting up the optimizer...
[10/27 03:11:38 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 03:13:10 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5162 s / batch. (data: 1.63e-02). ETA=7:54:54, max mem: 11.4 GB 
[10/27 03:14:36 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4786 s / batch. (data: 2.78e-04). ETA=7:19:31, max mem: 11.4 GB 
[10/27 03:16:08 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9567 s / batch. (data: 2.47e+00). ETA=1 day, 21:10:19, max mem: 11.4 GB 
[10/27 03:17:34 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4899 s / batch. (data: 1.19e-02). ETA=7:28:18, max mem: 11.4 GB 
[10/27 03:19:05 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4786 s / batch. (data: 2.09e-04). ETA=7:17:08, max mem: 11.4 GB 
[10/27 03:19:50 visual_prompt]: Epoch 1 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 1.3966
[10/27 03:20:43 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1916, average loss: 1.3454
[10/27 03:20:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/27 03:20:43 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/27 03:22:14 visual_prompt]: 	Training 100/553. train loss: 0.9065,	1.0671 s / batch. (data: 5.75e-01). ETA=16:11:51, max mem: 11.4 GB 
[10/27 03:23:42 visual_prompt]: 	Training 200/553. train loss: 0.0233,	1.2480 s / batch. (data: 7.50e-01). ETA=18:54:34, max mem: 11.4 GB 
[10/27 03:25:13 visual_prompt]: 	Training 300/553. train loss: 0.6511,	1.7237 s / batch. (data: 1.22e+00). ETA=1 day, 2:04:11, max mem: 11.4 GB 
[10/27 03:26:40 visual_prompt]: 	Training 400/553. train loss: 1.0747,	0.5000 s / batch. (data: 2.82e-04). ETA=7:32:55, max mem: 11.4 GB 
[10/27 03:28:10 visual_prompt]: 	Training 500/553. train loss: 0.6481,	0.5080 s / batch. (data: 7.97e-03). ETA=7:39:18, max mem: 11.4 GB 
[10/27 03:28:54 visual_prompt]: Epoch 2 / 100: avg data time: 3.94e-01, avg batch time: 0.8884, average train loss: 0.9838
[10/27 03:29:47 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1919, average loss: 1.2518
[10/27 03:29:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.79	
[10/27 03:29:47 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/27 03:31:16 visual_prompt]: 	Training 100/553. train loss: 1.4202,	0.4838 s / batch. (data: 5.39e-03). ETA=7:16:10, max mem: 11.4 GB 
[10/27 03:32:46 visual_prompt]: 	Training 200/553. train loss: 0.7591,	1.0720 s / batch. (data: 5.74e-01). ETA=16:04:42, max mem: 11.4 GB 
[10/27 03:34:13 visual_prompt]: 	Training 300/553. train loss: 0.5893,	0.4994 s / batch. (data: 1.04e-02). ETA=7:28:32, max mem: 11.4 GB 
[10/27 03:35:44 visual_prompt]: 	Training 400/553. train loss: 4.0390,	0.4930 s / batch. (data: 1.22e-02). ETA=7:22:01, max mem: 11.4 GB 
[10/27 03:37:14 visual_prompt]: 	Training 500/553. train loss: 0.6941,	1.9280 s / batch. (data: 1.44e+00). ETA=1 day, 4:45:22, max mem: 11.4 GB 
[10/27 03:37:58 visual_prompt]: Epoch 3 / 100: avg data time: 3.93e-01, avg batch time: 0.8884, average train loss: 1.0504
[10/27 03:38:50 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1904, average loss: 0.7498
[10/27 03:38:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.13	
[10/27 03:38:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/27 03:40:24 visual_prompt]: 	Training 100/553. train loss: 0.7763,	0.5163 s / batch. (data: 2.44e-02). ETA=7:40:40, max mem: 11.4 GB 
[10/27 03:41:52 visual_prompt]: 	Training 200/553. train loss: 0.6786,	0.5042 s / batch. (data: 7.95e-03). ETA=7:29:04, max mem: 11.4 GB 
[10/27 03:43:21 visual_prompt]: 	Training 300/553. train loss: 0.4870,	1.7876 s / batch. (data: 1.28e+00). ETA=1 day, 2:29:11, max mem: 11.4 GB 
[10/27 03:44:46 visual_prompt]: 	Training 400/553. train loss: 0.9724,	1.6312 s / batch. (data: 1.14e+00). ETA=1 day, 0:07:27, max mem: 11.4 GB 
[10/27 03:46:16 visual_prompt]: 	Training 500/553. train loss: 0.2775,	3.5830 s / batch. (data: 3.09e+00). ETA=2 days, 4:53:25, max mem: 11.4 GB 
[10/27 03:47:02 visual_prompt]: Epoch 4 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 1.1031
[10/27 03:47:55 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1913, average loss: 2.3304
[10/27 03:47:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.37	
[10/27 03:47:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/27 03:49:25 visual_prompt]: 	Training 100/553. train loss: 5.0933,	0.4878 s / batch. (data: 2.78e-04). ETA=7:10:48, max mem: 11.4 GB 
[10/27 03:50:54 visual_prompt]: 	Training 200/553. train loss: 1.5045,	1.7682 s / batch. (data: 1.29e+00). ETA=1 day, 1:58:38, max mem: 11.4 GB 
[10/27 03:52:24 visual_prompt]: 	Training 300/553. train loss: 2.6451,	0.5000 s / batch. (data: 2.82e-04). ETA=7:19:54, max mem: 11.4 GB 
[10/27 03:53:51 visual_prompt]: 	Training 400/553. train loss: 2.7261,	0.4931 s / batch. (data: 5.03e-03). ETA=7:12:58, max mem: 11.4 GB 
[10/27 03:55:20 visual_prompt]: 	Training 500/553. train loss: 1.1040,	0.4998 s / batch. (data: 1.12e-02). ETA=7:18:03, max mem: 11.4 GB 
[10/27 03:56:06 visual_prompt]: Epoch 5 / 100: avg data time: 3.94e-01, avg batch time: 0.8886, average train loss: 1.3446
[10/27 03:56:59 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1910, average loss: 0.9759
[10/27 03:56:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.85	
[10/27 03:56:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/27 03:58:31 visual_prompt]: 	Training 100/553. train loss: 0.6604,	0.5000 s / batch. (data: 2.92e-04). ETA=7:16:58, max mem: 11.4 GB 
[10/27 03:59:59 visual_prompt]: 	Training 200/553. train loss: 4.1923,	0.4789 s / batch. (data: 2.57e-04). ETA=6:57:42, max mem: 11.4 GB 
[10/27 04:01:26 visual_prompt]: 	Training 300/553. train loss: 1.9359,	0.4922 s / batch. (data: 1.14e-02). ETA=7:08:28, max mem: 11.4 GB 
[10/27 04:02:58 visual_prompt]: 	Training 400/553. train loss: 2.4990,	0.6258 s / batch. (data: 1.48e-01). ETA=9:03:48, max mem: 11.4 GB 
[10/27 04:04:26 visual_prompt]: 	Training 500/553. train loss: 2.5359,	1.4842 s / batch. (data: 1.01e+00). ETA=21:27:11, max mem: 11.4 GB 
[10/27 04:05:10 visual_prompt]: Epoch 6 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 1.3468
[10/27 04:06:03 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1910, average loss: 0.7761
[10/27 04:06:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.39	
[10/27 04:06:03 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/27 04:07:34 visual_prompt]: 	Training 100/553. train loss: 1.6386,	0.5004 s / batch. (data: 3.60e-04). ETA=7:12:40, max mem: 11.4 GB 
[10/27 04:09:01 visual_prompt]: 	Training 200/553. train loss: 0.5576,	0.5080 s / batch. (data: 2.87e-04). ETA=7:18:26, max mem: 11.4 GB 
[10/27 04:10:33 visual_prompt]: 	Training 300/553. train loss: 0.6192,	2.4425 s / batch. (data: 1.95e+00). ETA=1 day, 11:03:52, max mem: 11.4 GB 
[10/27 04:12:01 visual_prompt]: 	Training 400/553. train loss: 1.0124,	0.8809 s / batch. (data: 3.68e-01). ETA=12:37:16, max mem: 11.4 GB 
[10/27 04:13:29 visual_prompt]: 	Training 500/553. train loss: 0.8444,	0.4913 s / batch. (data: 1.05e-02). ETA=7:01:30, max mem: 11.4 GB 
[10/27 04:14:13 visual_prompt]: Epoch 7 / 100: avg data time: 3.91e-01, avg batch time: 0.8867, average train loss: 1.2265
[10/27 04:15:06 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1908, average loss: 0.7223
[10/27 04:15:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.05	
[10/27 04:15:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/27 04:16:37 visual_prompt]: 	Training 100/553. train loss: 2.4539,	1.6774 s / batch. (data: 1.20e+00). ETA=23:54:58, max mem: 11.4 GB 
[10/27 04:18:05 visual_prompt]: 	Training 200/553. train loss: 0.8683,	0.4882 s / batch. (data: 2.70e-04). ETA=6:56:48, max mem: 11.4 GB 
[10/27 04:19:34 visual_prompt]: 	Training 300/553. train loss: 0.6932,	0.4920 s / batch. (data: 4.63e-03). ETA=6:59:14, max mem: 11.4 GB 
[10/27 04:21:03 visual_prompt]: 	Training 400/553. train loss: 1.0156,	0.5121 s / batch. (data: 2.91e-04). ETA=7:15:29, max mem: 11.4 GB 
[10/27 04:22:32 visual_prompt]: 	Training 500/553. train loss: 5.0623,	1.9765 s / batch. (data: 1.50e+00). ETA=1 day, 3:57:38, max mem: 11.4 GB 
[10/27 04:23:17 visual_prompt]: Epoch 8 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 1.4785
[10/27 04:24:09 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1929, average loss: 1.0532
[10/27 04:24:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.59	
[10/27 04:24:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/27 04:25:41 visual_prompt]: 	Training 100/553. train loss: 2.6962,	0.4842 s / batch. (data: 2.78e-04). ETA=6:49:45, max mem: 11.4 GB 
[10/27 04:27:09 visual_prompt]: 	Training 200/553. train loss: 0.8256,	0.4841 s / batch. (data: 2.82e-04). ETA=6:48:54, max mem: 11.4 GB 
[10/27 04:28:37 visual_prompt]: 	Training 300/553. train loss: 1.5817,	1.7600 s / batch. (data: 1.26e+00). ETA=1 day, 0:43:33, max mem: 11.4 GB 
[10/27 04:30:06 visual_prompt]: 	Training 400/553. train loss: 1.4035,	0.5320 s / batch. (data: 1.62e-02). ETA=7:27:34, max mem: 11.4 GB 
[10/27 04:31:35 visual_prompt]: 	Training 500/553. train loss: 0.6144,	0.5059 s / batch. (data: 1.38e-02). ETA=7:04:46, max mem: 11.4 GB 
[10/27 04:32:20 visual_prompt]: Epoch 9 / 100: avg data time: 3.92e-01, avg batch time: 0.8868, average train loss: 1.3905
[10/27 04:33:12 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1897, average loss: 0.8243
[10/27 04:33:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.24	
[10/27 04:33:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/27 04:34:46 visual_prompt]: 	Training 100/553. train loss: 2.6734,	0.5200 s / batch. (data: 2.77e-04). ETA=7:15:15, max mem: 11.4 GB 
[10/27 04:36:13 visual_prompt]: 	Training 200/553. train loss: 1.8416,	0.4992 s / batch. (data: 7.98e-03). ETA=6:56:58, max mem: 11.4 GB 
[10/27 04:37:41 visual_prompt]: 	Training 300/553. train loss: 2.1513,	0.4880 s / batch. (data: 2.87e-04). ETA=6:46:50, max mem: 11.4 GB 
[10/27 04:39:09 visual_prompt]: 	Training 400/553. train loss: 2.9228,	1.3677 s / batch. (data: 8.66e-01). ETA=18:58:01, max mem: 11.4 GB 
[10/27 04:40:39 visual_prompt]: 	Training 500/553. train loss: 0.5722,	1.6680 s / batch. (data: 1.16e+00). ETA=23:05:03, max mem: 11.4 GB 
[10/27 04:41:24 visual_prompt]: Epoch 10 / 100: avg data time: 3.94e-01, avg batch time: 0.8890, average train loss: 1.7830
[10/27 04:42:16 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1906, average loss: 0.6860
[10/27 04:42:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.25	
[10/27 04:42:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/27 04:43:50 visual_prompt]: 	Training 100/553. train loss: 0.7808,	0.5440 s / batch. (data: 2.72e-04). ETA=7:30:19, max mem: 11.4 GB 
[10/27 04:45:20 visual_prompt]: 	Training 200/553. train loss: 0.5501,	0.5000 s / batch. (data: 7.96e-03). ETA=6:53:04, max mem: 11.4 GB 
[10/27 04:46:48 visual_prompt]: 	Training 300/553. train loss: 0.0288,	2.3554 s / batch. (data: 1.87e+00). ETA=1 day, 8:22:02, max mem: 11.4 GB 
[10/27 04:48:15 visual_prompt]: 	Training 400/553. train loss: 0.9481,	0.5370 s / batch. (data: 1.04e-02). ETA=7:21:53, max mem: 11.4 GB 
[10/27 04:49:43 visual_prompt]: 	Training 500/553. train loss: 1.3536,	0.5077 s / batch. (data: 7.99e-03). ETA=6:56:52, max mem: 11.4 GB 
[10/27 04:50:27 visual_prompt]: Epoch 11 / 100: avg data time: 3.93e-01, avg batch time: 0.8880, average train loss: 1.1524
[10/27 04:51:20 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1917, average loss: 0.7325
[10/27 04:51:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.97	
[10/27 04:51:20 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/27 04:52:53 visual_prompt]: 	Training 100/553. train loss: 0.8841,	0.4956 s / batch. (data: 2.89e-04). ETA=6:45:41, max mem: 11.4 GB 
[10/27 04:54:22 visual_prompt]: 	Training 200/553. train loss: 0.6117,	0.8971 s / batch. (data: 4.07e-01). ETA=12:12:51, max mem: 11.4 GB 
[10/27 04:55:50 visual_prompt]: 	Training 300/553. train loss: 1.4808,	0.5074 s / batch. (data: 2.40e-03). ETA=6:53:42, max mem: 11.4 GB 
[10/27 04:57:18 visual_prompt]: 	Training 400/553. train loss: 1.0003,	0.5287 s / batch. (data: 8.72e-03). ETA=7:10:11, max mem: 11.4 GB 
[10/27 04:58:47 visual_prompt]: 	Training 500/553. train loss: 6.0986,	0.4874 s / batch. (data: 2.83e-04). ETA=6:35:46, max mem: 11.4 GB 
[10/27 04:59:31 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8889, average train loss: 1.3395
[10/27 05:00:24 visual_prompt]: Inference (val):avg data time: 3.80e-04, avg batch time: 0.1902, average loss: 4.4327
[10/27 05:00:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.78	
[10/27 05:00:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/27 05:01:56 visual_prompt]: 	Training 100/553. train loss: 1.3904,	0.4965 s / batch. (data: 1.05e-02). ETA=6:41:52, max mem: 11.4 GB 
[10/27 05:03:22 visual_prompt]: 	Training 200/553. train loss: 0.7942,	0.4804 s / batch. (data: 2.82e-04). ETA=6:28:03, max mem: 11.4 GB 
[10/27 05:04:51 visual_prompt]: 	Training 300/553. train loss: 3.0590,	1.3075 s / batch. (data: 8.21e-01). ETA=17:33:56, max mem: 11.4 GB 
[10/27 05:06:19 visual_prompt]: 	Training 400/553. train loss: 10.0281,	0.4931 s / batch. (data: 5.39e-03). ETA=6:36:40, max mem: 11.4 GB 
[10/27 05:07:48 visual_prompt]: 	Training 500/553. train loss: 8.3884,	0.4961 s / batch. (data: 2.63e-04). ETA=6:38:11, max mem: 11.4 GB 
[10/27 05:08:34 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8863, average train loss: 3.5559
[10/27 05:09:26 visual_prompt]: Inference (val):avg data time: 1.74e-04, avg batch time: 0.1898, average loss: 0.6999
[10/27 05:09:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.07	
[10/27 05:09:26 visual_prompt]: Best epoch 13: best metric: -0.700
[10/27 05:09:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/27 05:10:58 visual_prompt]: 	Training 100/553. train loss: 3.9999,	0.4987 s / batch. (data: 8.02e-03). ETA=6:39:02, max mem: 11.4 GB 
[10/27 05:12:28 visual_prompt]: 	Training 200/553. train loss: 0.0190,	1.8280 s / batch. (data: 1.35e+00). ETA=1 day, 0:19:42, max mem: 11.4 GB 
[10/27 05:13:56 visual_prompt]: 	Training 300/553. train loss: 0.5893,	1.3400 s / batch. (data: 8.52e-01). ETA=17:47:46, max mem: 11.4 GB 
[10/27 05:15:24 visual_prompt]: 	Training 400/553. train loss: 1.0365,	0.5120 s / batch. (data: 3.09e-04). ETA=6:47:07, max mem: 11.4 GB 
[10/27 05:16:53 visual_prompt]: 	Training 500/553. train loss: 4.2662,	0.5240 s / batch. (data: 2.67e-04). ETA=6:55:48, max mem: 11.4 GB 
[10/27 05:17:37 visual_prompt]: Epoch 14 / 100: avg data time: 3.93e-01, avg batch time: 0.8871, average train loss: 2.9941
[10/27 05:18:29 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1903, average loss: 1.6815
[10/27 05:18:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.54	
[10/27 05:18:29 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/27 05:20:01 visual_prompt]: 	Training 100/553. train loss: 0.6404,	0.5080 s / batch. (data: 3.05e-04). ETA=6:41:48, max mem: 11.4 GB 
[10/27 05:21:29 visual_prompt]: 	Training 200/553. train loss: 0.1133,	0.4915 s / batch. (data: 2.55e-04). ETA=6:27:56, max mem: 11.4 GB 
[10/27 05:22:58 visual_prompt]: 	Training 300/553. train loss: 5.0764,	0.5080 s / batch. (data: 2.66e-04). ETA=6:40:08, max mem: 11.4 GB 
[10/27 05:24:25 visual_prompt]: 	Training 400/553. train loss: 0.6568,	0.4879 s / batch. (data: 3.07e-04). ETA=6:23:30, max mem: 11.4 GB 
[10/27 05:25:55 visual_prompt]: 	Training 500/553. train loss: 1.6972,	0.5160 s / batch. (data: 2.47e-04). ETA=6:44:42, max mem: 11.4 GB 
[10/27 05:26:41 visual_prompt]: Epoch 15 / 100: avg data time: 3.95e-01, avg batch time: 0.8885, average train loss: 3.1413
[10/27 05:27:33 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1895, average loss: 1.1991
[10/27 05:27:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.50	
[10/27 05:27:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/27 05:29:04 visual_prompt]: 	Training 100/553. train loss: 2.1335,	0.4840 s / batch. (data: 2.61e-04). ETA=6:18:20, max mem: 11.4 GB 
[10/27 05:30:32 visual_prompt]: 	Training 200/553. train loss: 0.5722,	0.5041 s / batch. (data: 1.20e-02). ETA=6:33:12, max mem: 11.4 GB 
[10/27 05:32:02 visual_prompt]: 	Training 300/553. train loss: 2.4912,	0.4880 s / batch. (data: 2.30e-04). ETA=6:19:52, max mem: 11.4 GB 
[10/27 05:33:30 visual_prompt]: 	Training 400/553. train loss: 2.3492,	0.5115 s / batch. (data: 7.57e-04). ETA=6:37:19, max mem: 11.4 GB 
[10/27 05:34:58 visual_prompt]: 	Training 500/553. train loss: 0.5888,	2.1579 s / batch. (data: 1.66e+00). ETA=1 day, 3:52:33, max mem: 11.4 GB 
[10/27 05:35:43 visual_prompt]: Epoch 16 / 100: avg data time: 3.94e-01, avg batch time: 0.8867, average train loss: 1.8843
[10/27 05:36:36 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1910, average loss: 1.2003
[10/27 05:36:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.16	
[10/27 05:36:36 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/27 05:38:07 visual_prompt]: 	Training 100/553. train loss: 2.8816,	0.5080 s / batch. (data: 2.77e-04). ETA=6:32:26, max mem: 11.4 GB 
[10/27 05:39:37 visual_prompt]: 	Training 200/553. train loss: 5.6943,	0.4770 s / batch. (data: 2.61e-04). ETA=6:07:40, max mem: 11.4 GB 
[10/27 05:41:05 visual_prompt]: 	Training 300/553. train loss: 2.7936,	0.5000 s / batch. (data: 7.96e-03). ETA=6:24:34, max mem: 11.4 GB 
[10/27 05:42:32 visual_prompt]: 	Training 400/553. train loss: 0.5676,	1.0690 s / batch. (data: 5.70e-01). ETA=13:40:28, max mem: 11.4 GB 
[10/27 05:44:00 visual_prompt]: 	Training 500/553. train loss: 0.7185,	1.8299 s / batch. (data: 1.33e+00). ETA=23:21:29, max mem: 11.4 GB 
[10/27 05:44:47 visual_prompt]: Epoch 17 / 100: avg data time: 3.95e-01, avg batch time: 0.8887, average train loss: 2.4120
[10/27 05:45:40 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1911, average loss: 1.9875
[10/27 05:45:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 39.81	
[10/27 05:45:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/27 05:47:11 visual_prompt]: 	Training 100/553. train loss: 0.9671,	0.5110 s / batch. (data: 1.61e-02). ETA=6:30:03, max mem: 11.4 GB 
[10/27 05:48:43 visual_prompt]: 	Training 200/553. train loss: 2.8619,	0.5116 s / batch. (data: 7.35e-04). ETA=6:29:41, max mem: 11.4 GB 
[10/27 05:50:12 visual_prompt]: 	Training 300/553. train loss: 1.2199,	0.4779 s / batch. (data: 2.53e-04). ETA=6:03:11, max mem: 11.4 GB 
[10/27 05:51:40 visual_prompt]: 	Training 400/553. train loss: 1.9150,	0.4960 s / batch. (data: 3.01e-04). ETA=6:16:06, max mem: 11.4 GB 
[10/27 05:53:08 visual_prompt]: 	Training 500/553. train loss: 0.7843,	0.4840 s / batch. (data: 2.52e-04). ETA=6:06:14, max mem: 11.4 GB 
[10/27 05:53:53 visual_prompt]: Epoch 18 / 100: avg data time: 3.98e-01, avg batch time: 0.8915, average train loss: 2.2610
[10/27 05:54:45 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1915, average loss: 2.3000
[10/27 05:54:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.38	
[10/27 05:54:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/27 05:56:18 visual_prompt]: 	Training 100/553. train loss: 0.6535,	1.4301 s / batch. (data: 9.50e-01). ETA=17:58:24, max mem: 11.4 GB 
[10/27 05:57:47 visual_prompt]: 	Training 200/553. train loss: 1.4517,	0.4785 s / batch. (data: 2.61e-04). ETA=6:00:02, max mem: 11.4 GB 
[10/27 05:59:17 visual_prompt]: 	Training 300/553. train loss: 0.0225,	0.9894 s / batch. (data: 4.98e-01). ETA=12:22:49, max mem: 11.4 GB 
[10/27 06:00:47 visual_prompt]: 	Training 400/553. train loss: 1.9057,	0.4840 s / batch. (data: 2.66e-04). ETA=6:02:32, max mem: 11.4 GB 
[10/27 06:02:12 visual_prompt]: 	Training 500/553. train loss: 0.7836,	0.5120 s / batch. (data: 2.60e-04). ETA=6:22:38, max mem: 11.4 GB 
[10/27 06:02:58 visual_prompt]: Epoch 19 / 100: avg data time: 3.98e-01, avg batch time: 0.8914, average train loss: 2.3686
[10/27 06:03:51 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1909, average loss: 1.1659
[10/27 06:03:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.52	
[10/27 06:03:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/27 06:05:22 visual_prompt]: 	Training 100/553. train loss: 0.5536,	0.5103 s / batch. (data: 2.68e-04). ETA=6:20:07, max mem: 11.4 GB 
[10/27 06:06:52 visual_prompt]: 	Training 200/553. train loss: 1.1474,	0.4918 s / batch. (data: 2.68e-04). ETA=6:05:32, max mem: 11.4 GB 
[10/27 06:08:21 visual_prompt]: 	Training 300/553. train loss: 0.6201,	0.4894 s / batch. (data: 2.81e-04). ETA=6:02:54, max mem: 11.4 GB 
[10/27 06:09:50 visual_prompt]: 	Training 400/553. train loss: 1.0629,	0.5128 s / batch. (data: 2.63e-04). ETA=6:19:24, max mem: 11.4 GB 
[10/27 06:11:18 visual_prompt]: 	Training 500/553. train loss: 0.8404,	0.4993 s / batch. (data: 5.40e-03). ETA=6:08:33, max mem: 11.4 GB 
[10/27 06:12:05 visual_prompt]: Epoch 20 / 100: avg data time: 3.98e-01, avg batch time: 0.8920, average train loss: 1.8826
[10/27 06:12:57 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1899, average loss: 1.1441
[10/27 06:12:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.75	
[10/27 06:12:57 visual_prompt]: Stopping early.
[10/27 06:12:57 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 06:12:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 06:12:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 06:12:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 06:12:57 visual_prompt]: Training with config:
[10/27 06:12:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 06:12:57 visual_prompt]: Loading training data...
[10/27 06:12:57 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 06:12:57 visual_prompt]: Loading validation data...
[10/27 06:12:57 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 06:12:57 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/27 06:13:00 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/27 06:13:00 visual_prompt]: tuned percent:0.529
[10/27 06:13:00 visual_prompt]: Device used for model: 0
[10/27 06:13:00 visual_prompt]: Setting up Evaluator...
[10/27 06:13:00 visual_prompt]: Setting up Trainer...
[10/27 06:13:00 visual_prompt]: 	Setting up the optimizer...
[10/27 06:13:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 06:14:31 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5120 s / batch. (data: 2.51e-04). ETA=7:51:03, max mem: 11.4 GB 
[10/27 06:15:57 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4806 s / batch. (data: 2.62e-04). ETA=7:21:19, max mem: 11.4 GB 
[10/27 06:17:29 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.8076 s / batch. (data: 2.31e+00). ETA=1 day, 18:53:40, max mem: 11.4 GB 
[10/27 06:18:55 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5200 s / batch. (data: 5.40e-03). ETA=7:55:46, max mem: 11.4 GB 
[10/27 06:20:26 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4881 s / batch. (data: 2.62e-04). ETA=7:25:47, max mem: 11.4 GB 
[10/27 06:21:11 visual_prompt]: Epoch 1 / 100: avg data time: 3.94e-01, avg batch time: 0.8891, average train loss: 1.3966
[10/27 06:22:04 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1916, average loss: 1.3454
[10/27 06:22:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/27 06:22:04 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/27 06:23:34 visual_prompt]: 	Training 100/553. train loss: 0.9057,	0.4996 s / batch. (data: 5.41e-03). ETA=7:35:00, max mem: 11.4 GB 
[10/27 06:25:03 visual_prompt]: 	Training 200/553. train loss: 0.0260,	1.6673 s / batch. (data: 1.18e+00). ETA=1 day, 1:15:47, max mem: 11.4 GB 
[10/27 06:26:33 visual_prompt]: 	Training 300/553. train loss: 0.6604,	1.6559 s / batch. (data: 1.15e+00). ETA=1 day, 1:02:38, max mem: 11.4 GB 
[10/27 06:27:59 visual_prompt]: 	Training 400/553. train loss: 1.0693,	0.5088 s / batch. (data: 3.03e-02). ETA=7:40:49, max mem: 11.4 GB 
[10/27 06:29:29 visual_prompt]: 	Training 500/553. train loss: 0.6497,	0.4986 s / batch. (data: 1.55e-02). ETA=7:30:48, max mem: 11.4 GB 
[10/27 06:30:14 visual_prompt]: Epoch 2 / 100: avg data time: 3.92e-01, avg batch time: 0.8867, average train loss: 0.9857
[10/27 06:31:06 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1920, average loss: 1.2281
[10/27 06:31:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.93	
[10/27 06:31:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/27 06:32:36 visual_prompt]: 	Training 100/553. train loss: 1.4293,	0.5037 s / batch. (data: 7.97e-03). ETA=7:34:08, max mem: 11.4 GB 
[10/27 06:34:06 visual_prompt]: 	Training 200/553. train loss: 0.7463,	0.8774 s / batch. (data: 3.80e-01). ETA=13:09:33, max mem: 11.4 GB 
[10/27 06:35:33 visual_prompt]: 	Training 300/553. train loss: 0.5847,	0.4843 s / batch. (data: 3.09e-04). ETA=7:15:02, max mem: 11.4 GB 
[10/27 06:37:03 visual_prompt]: 	Training 400/553. train loss: 4.1291,	0.5040 s / batch. (data: 2.52e-04). ETA=7:31:52, max mem: 11.4 GB 
[10/27 06:38:32 visual_prompt]: 	Training 500/553. train loss: 0.7245,	1.7440 s / batch. (data: 1.25e+00). ETA=1 day, 2:00:43, max mem: 11.4 GB 
[10/27 06:39:17 visual_prompt]: Epoch 3 / 100: avg data time: 3.92e-01, avg batch time: 0.8871, average train loss: 1.0701
[10/27 06:40:09 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1910, average loss: 0.7541
[10/27 06:40:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.99	
[10/27 06:40:09 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/27 06:41:41 visual_prompt]: 	Training 100/553. train loss: 0.7530,	0.5080 s / batch. (data: 2.40e-02). ETA=7:33:18, max mem: 11.4 GB 
[10/27 06:43:10 visual_prompt]: 	Training 200/553. train loss: 0.6898,	0.5278 s / batch. (data: 1.56e-02). ETA=7:50:04, max mem: 11.4 GB 
[10/27 06:44:40 visual_prompt]: 	Training 300/553. train loss: 0.4713,	2.0715 s / batch. (data: 1.59e+00). ETA=1 day, 6:41:35, max mem: 11.4 GB 
[10/27 06:46:05 visual_prompt]: 	Training 400/553. train loss: 0.8424,	1.9481 s / batch. (data: 1.46e+00). ETA=1 day, 4:48:41, max mem: 11.4 GB 
[10/27 06:47:35 visual_prompt]: 	Training 500/553. train loss: 0.3339,	3.7267 s / batch. (data: 3.25e+00). ETA=2 days, 7:00:42, max mem: 11.4 GB 
[10/27 06:48:20 visual_prompt]: Epoch 4 / 100: avg data time: 3.93e-01, avg batch time: 0.8872, average train loss: 1.1195
[10/27 06:49:12 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1919, average loss: 2.3973
[10/27 06:49:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.48	
[10/27 06:49:12 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/27 06:50:43 visual_prompt]: 	Training 100/553. train loss: 5.1152,	0.5000 s / batch. (data: 2.84e-04). ETA=7:21:33, max mem: 11.4 GB 
[10/27 06:52:12 visual_prompt]: 	Training 200/553. train loss: 1.6449,	1.8440 s / batch. (data: 1.34e+00). ETA=1 day, 3:05:24, max mem: 11.4 GB 
[10/27 06:53:42 visual_prompt]: 	Training 300/553. train loss: 0.8913,	0.5000 s / batch. (data: 2.71e-04). ETA=7:19:54, max mem: 11.4 GB 
[10/27 06:55:10 visual_prompt]: 	Training 400/553. train loss: 2.8845,	0.5030 s / batch. (data: 3.25e-04). ETA=7:21:43, max mem: 11.4 GB 
[10/27 06:56:39 visual_prompt]: 	Training 500/553. train loss: 1.1899,	0.5200 s / batch. (data: 2.68e-04). ETA=7:35:44, max mem: 11.4 GB 
[10/27 06:57:25 visual_prompt]: Epoch 5 / 100: avg data time: 3.97e-01, avg batch time: 0.8912, average train loss: 1.5351
[10/27 06:58:18 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1904, average loss: 4.1533
[10/27 06:58:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.49	
[10/27 06:58:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/27 06:59:52 visual_prompt]: 	Training 100/553. train loss: 0.9102,	0.5041 s / batch. (data: 7.66e-04). ETA=7:20:31, max mem: 11.4 GB 
[10/27 07:01:20 visual_prompt]: 	Training 200/553. train loss: 4.8737,	0.4800 s / batch. (data: 2.65e-04). ETA=6:58:40, max mem: 11.4 GB 
[10/27 07:02:47 visual_prompt]: 	Training 300/553. train loss: 0.6571,	0.5029 s / batch. (data: 4.14e-04). ETA=7:17:48, max mem: 11.4 GB 
[10/27 07:04:20 visual_prompt]: 	Training 400/553. train loss: 3.0294,	0.8007 s / batch. (data: 3.02e-01). ETA=11:35:43, max mem: 11.4 GB 
[10/27 07:05:48 visual_prompt]: 	Training 500/553. train loss: 3.8803,	1.4552 s / batch. (data: 9.66e-01). ETA=21:02:01, max mem: 11.4 GB 
[10/27 07:06:33 visual_prompt]: Epoch 6 / 100: avg data time: 4.00e-01, avg batch time: 0.8950, average train loss: 1.5446
[10/27 07:07:25 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1909, average loss: 0.9432
[10/27 07:07:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.77	
[10/27 07:07:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/27 07:08:56 visual_prompt]: 	Training 100/553. train loss: 2.5130,	0.4970 s / batch. (data: 1.55e-02). ETA=7:09:45, max mem: 11.4 GB 
[10/27 07:10:25 visual_prompt]: 	Training 200/553. train loss: 0.5376,	0.4888 s / batch. (data: 3.54e-04). ETA=7:01:52, max mem: 11.4 GB 
[10/27 07:11:57 visual_prompt]: 	Training 300/553. train loss: 0.6521,	2.0198 s / batch. (data: 1.51e+00). ETA=1 day, 4:59:48, max mem: 11.4 GB 
[10/27 07:13:26 visual_prompt]: 	Training 400/553. train loss: 1.0165,	2.5075 s / batch. (data: 2.02e+00). ETA=1 day, 11:55:40, max mem: 11.4 GB 
[10/27 07:14:53 visual_prompt]: 	Training 500/553. train loss: 1.0725,	0.4996 s / batch. (data: 5.38e-03). ETA=7:08:42, max mem: 11.4 GB 
[10/27 07:15:38 visual_prompt]: Epoch 7 / 100: avg data time: 3.97e-01, avg batch time: 0.8909, average train loss: 1.4830
[10/27 07:16:31 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1903, average loss: 0.6815
[10/27 07:16:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.92	
[10/27 07:16:31 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/27 07:18:01 visual_prompt]: 	Training 100/553. train loss: 3.5084,	0.4829 s / batch. (data: 3.42e-04). ETA=6:53:05, max mem: 11.4 GB 
[10/27 07:19:31 visual_prompt]: 	Training 200/553. train loss: 0.4947,	0.4796 s / batch. (data: 2.85e-04). ETA=6:49:30, max mem: 11.4 GB 
[10/27 07:21:01 visual_prompt]: 	Training 300/553. train loss: 4.5275,	0.4804 s / batch. (data: 2.63e-04). ETA=6:49:24, max mem: 11.4 GB 
[10/27 07:22:30 visual_prompt]: 	Training 400/553. train loss: 1.1102,	0.5000 s / batch. (data: 7.96e-03). ETA=7:05:14, max mem: 11.4 GB 
[10/27 07:23:59 visual_prompt]: 	Training 500/553. train loss: 1.5497,	1.9000 s / batch. (data: 1.41e+00). ETA=1 day, 2:52:45, max mem: 11.4 GB 
[10/27 07:24:44 visual_prompt]: Epoch 8 / 100: avg data time: 3.97e-01, avg batch time: 0.8913, average train loss: 1.7651
[10/27 07:25:36 visual_prompt]: Inference (val):avg data time: 9.83e-05, avg batch time: 0.1925, average loss: 0.7410
[10/27 07:25:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 58.13	
[10/27 07:25:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/27 07:27:08 visual_prompt]: 	Training 100/553. train loss: 0.0068,	0.5064 s / batch. (data: 1.59e-02). ETA=7:08:31, max mem: 11.4 GB 
[10/27 07:28:34 visual_prompt]: 	Training 200/553. train loss: 0.4976,	0.5079 s / batch. (data: 1.20e-02). ETA=7:09:00, max mem: 11.4 GB 
[10/27 07:30:02 visual_prompt]: 	Training 300/553. train loss: 1.2978,	1.4750 s / batch. (data: 9.98e-01). ETA=20:43:19, max mem: 11.4 GB 
[10/27 07:31:32 visual_prompt]: 	Training 400/553. train loss: 1.7800,	0.4812 s / batch. (data: 2.74e-04). ETA=6:44:48, max mem: 11.4 GB 
[10/27 07:33:02 visual_prompt]: 	Training 500/553. train loss: 0.5957,	1.3680 s / batch. (data: 8.87e-01). ETA=19:08:33, max mem: 11.4 GB 
[10/27 07:33:46 visual_prompt]: Epoch 9 / 100: avg data time: 3.92e-01, avg batch time: 0.8858, average train loss: 1.4315
[10/27 07:34:38 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1910, average loss: 0.6962
[10/27 07:34:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 59.05	
[10/27 07:34:38 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/27 07:36:13 visual_prompt]: 	Training 100/553. train loss: 4.9105,	0.4886 s / batch. (data: 2.56e-04). ETA=6:48:58, max mem: 11.4 GB 
[10/27 07:37:40 visual_prompt]: 	Training 200/553. train loss: 0.5672,	0.4921 s / batch. (data: 2.57e-04). ETA=6:51:07, max mem: 11.4 GB 
[10/27 07:39:08 visual_prompt]: 	Training 300/553. train loss: 2.5480,	0.5112 s / batch. (data: 1.11e-02). ETA=7:06:09, max mem: 11.4 GB 
[10/27 07:40:36 visual_prompt]: 	Training 400/553. train loss: 1.7509,	1.2998 s / batch. (data: 7.99e-01). ETA=18:01:30, max mem: 11.4 GB 
[10/27 07:42:06 visual_prompt]: 	Training 500/553. train loss: 0.5826,	1.1957 s / batch. (data: 7.02e-01). ETA=16:32:52, max mem: 11.4 GB 
[10/27 07:42:51 visual_prompt]: Epoch 10 / 100: avg data time: 3.95e-01, avg batch time: 0.8916, average train loss: 1.9910
[10/27 07:43:44 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1894, average loss: 0.7186
[10/27 07:43:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 56.22	
[10/27 07:43:44 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/27 07:45:17 visual_prompt]: 	Training 100/553. train loss: 0.6452,	0.5000 s / batch. (data: 2.77e-04). ETA=6:53:53, max mem: 11.4 GB 
[10/27 07:46:48 visual_prompt]: 	Training 200/553. train loss: 1.4340,	0.5032 s / batch. (data: 2.75e-04). ETA=6:55:41, max mem: 11.4 GB 
[10/27 07:48:17 visual_prompt]: 	Training 300/553. train loss: 0.0014,	1.2521 s / batch. (data: 7.49e-01). ETA=17:12:20, max mem: 11.4 GB 
[10/27 07:49:44 visual_prompt]: 	Training 400/553. train loss: 0.6794,	0.5189 s / batch. (data: 1.48e-02). ETA=7:06:56, max mem: 11.4 GB 
[10/27 07:51:11 visual_prompt]: 	Training 500/553. train loss: 0.6344,	0.5000 s / batch. (data: 2.82e-04). ETA=6:50:33, max mem: 11.4 GB 
[10/27 07:51:56 visual_prompt]: Epoch 11 / 100: avg data time: 3.94e-01, avg batch time: 0.8897, average train loss: 1.4090
[10/27 07:52:48 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1892, average loss: 1.2583
[10/27 07:52:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.14	
[10/27 07:52:48 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/27 07:54:21 visual_prompt]: 	Training 100/553. train loss: 0.8145,	0.6432 s / batch. (data: 1.52e-01). ETA=8:46:31, max mem: 11.4 GB 
[10/27 07:55:51 visual_prompt]: 	Training 200/553. train loss: 0.4534,	0.5000 s / batch. (data: 2.66e-04). ETA=6:48:28, max mem: 11.4 GB 
[10/27 07:57:18 visual_prompt]: 	Training 300/553. train loss: 2.1810,	0.5000 s / batch. (data: 2.54e-04). ETA=6:47:38, max mem: 11.4 GB 
[10/27 07:58:47 visual_prompt]: 	Training 400/553. train loss: 2.4444,	0.4852 s / batch. (data: 5.43e-03). ETA=6:34:43, max mem: 11.4 GB 
[10/27 08:00:16 visual_prompt]: 	Training 500/553. train loss: 6.2457,	0.4924 s / batch. (data: 7.34e-04). ETA=6:39:46, max mem: 11.4 GB 
[10/27 08:00:59 visual_prompt]: Epoch 12 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 1.6045
[10/27 08:01:51 visual_prompt]: Inference (val):avg data time: 1.96e-04, avg batch time: 0.1915, average loss: 7.5099
[10/27 08:01:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.74	
[10/27 08:01:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/27 08:03:24 visual_prompt]: 	Training 100/553. train loss: 0.5486,	1.1039 s / batch. (data: 5.95e-01). ETA=14:53:30, max mem: 11.4 GB 
[10/27 08:04:50 visual_prompt]: 	Training 200/553. train loss: 0.7789,	0.4885 s / batch. (data: 5.37e-03). ETA=6:34:35, max mem: 11.4 GB 
[10/27 08:06:20 visual_prompt]: 	Training 300/553. train loss: 1.7226,	2.3960 s / batch. (data: 1.89e+00). ETA=1 day, 8:11:20, max mem: 11.4 GB 
[10/27 08:07:46 visual_prompt]: 	Training 400/553. train loss: 7.0293,	1.1793 s / batch. (data: 6.98e-01). ETA=15:48:35, max mem: 11.4 GB 
[10/27 08:09:15 visual_prompt]: 	Training 500/553. train loss: 2.1535,	0.4920 s / batch. (data: 7.95e-03). ETA=6:34:55, max mem: 11.4 GB 
[10/27 08:10:01 visual_prompt]: Epoch 13 / 100: avg data time: 3.91e-01, avg batch time: 0.8856, average train loss: 1.8731
[10/27 08:10:54 visual_prompt]: Inference (val):avg data time: 2.53e-04, avg batch time: 0.1918, average loss: 0.8777
[10/27 08:10:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.75	
[10/27 08:10:54 visual_prompt]: Best epoch 13: best metric: -0.878
[10/27 08:10:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/27 08:12:26 visual_prompt]: 	Training 100/553. train loss: 0.8940,	0.5120 s / batch. (data: 2.46e-04). ETA=6:49:41, max mem: 11.4 GB 
[10/27 08:13:55 visual_prompt]: 	Training 200/553. train loss: 0.2490,	1.5201 s / batch. (data: 1.03e+00). ETA=20:13:47, max mem: 11.4 GB 
[10/27 08:15:23 visual_prompt]: 	Training 300/553. train loss: 0.2825,	1.3636 s / batch. (data: 8.75e-01). ETA=18:06:34, max mem: 11.4 GB 
[10/27 08:16:50 visual_prompt]: 	Training 400/553. train loss: 0.6143,	0.6476 s / batch. (data: 1.56e-01). ETA=8:34:56, max mem: 11.4 GB 
[10/27 08:18:20 visual_prompt]: 	Training 500/553. train loss: 1.8914,	0.4880 s / batch. (data: 2.18e-04). ETA=6:27:14, max mem: 11.4 GB 
[10/27 08:19:03 visual_prompt]: Epoch 14 / 100: avg data time: 3.90e-01, avg batch time: 0.8855, average train loss: 1.3456
[10/27 08:19:56 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1916, average loss: 0.9600
[10/27 08:19:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.60	
[10/27 08:19:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/27 08:21:27 visual_prompt]: 	Training 100/553. train loss: 1.0245,	0.5359 s / batch. (data: 1.78e-02). ETA=7:03:50, max mem: 11.4 GB 
[10/27 08:22:55 visual_prompt]: 	Training 200/553. train loss: 1.0615,	0.5037 s / batch. (data: 5.41e-03). ETA=6:37:34, max mem: 11.4 GB 
[10/27 08:24:24 visual_prompt]: 	Training 300/553. train loss: 2.2536,	0.5033 s / batch. (data: 2.72e-04). ETA=6:36:25, max mem: 11.4 GB 
[10/27 08:25:50 visual_prompt]: 	Training 400/553. train loss: 1.4707,	0.6376 s / batch. (data: 1.30e-01). ETA=8:21:06, max mem: 11.4 GB 
[10/27 08:27:20 visual_prompt]: 	Training 500/553. train loss: 0.6823,	0.4842 s / batch. (data: 2.43e-04). ETA=6:19:46, max mem: 11.4 GB 
[10/27 08:28:06 visual_prompt]: Epoch 15 / 100: avg data time: 3.92e-01, avg batch time: 0.8872, average train loss: 1.8973
[10/27 08:28:59 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1922, average loss: 1.7386
[10/27 08:28:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.15	
[10/27 08:28:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/27 08:30:29 visual_prompt]: 	Training 100/553. train loss: 0.3048,	0.4863 s / batch. (data: 7.98e-03). ETA=6:20:09, max mem: 11.4 GB 
[10/27 08:31:59 visual_prompt]: 	Training 200/553. train loss: 1.4333,	0.4874 s / batch. (data: 2.43e-04). ETA=6:20:13, max mem: 11.4 GB 
[10/27 08:33:27 visual_prompt]: 	Training 300/553. train loss: 1.2013,	0.5080 s / batch. (data: 2.50e-04). ETA=6:35:26, max mem: 11.4 GB 
[10/27 08:34:56 visual_prompt]: 	Training 400/553. train loss: 2.8039,	0.5200 s / batch. (data: 7.10e-04). ETA=6:43:54, max mem: 11.4 GB 
[10/27 08:36:24 visual_prompt]: 	Training 500/553. train loss: 0.6474,	1.9920 s / batch. (data: 1.49e+00). ETA=1 day, 1:43:57, max mem: 11.4 GB 
[10/27 08:37:09 visual_prompt]: Epoch 16 / 100: avg data time: 3.92e-01, avg batch time: 0.8862, average train loss: 1.5385
[10/27 08:38:01 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1918, average loss: 0.7162
[10/27 08:38:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 61.33	
[10/27 08:38:01 visual_prompt]: Best epoch 16: best metric: -0.716
[10/27 08:38:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/27 08:39:32 visual_prompt]: 	Training 100/553. train loss: 1.4822,	0.5277 s / batch. (data: 7.70e-03). ETA=6:47:41, max mem: 11.4 GB 
[10/27 08:41:02 visual_prompt]: 	Training 200/553. train loss: 2.4239,	0.4896 s / batch. (data: 2.51e-04). ETA=6:17:23, max mem: 11.4 GB 
[10/27 08:42:30 visual_prompt]: 	Training 300/553. train loss: 2.2727,	0.5040 s / batch. (data: 1.20e-02). ETA=6:27:39, max mem: 11.4 GB 
[10/27 08:43:58 visual_prompt]: 	Training 400/553. train loss: 0.7645,	0.5479 s / batch. (data: 4.74e-02). ETA=7:00:33, max mem: 11.4 GB 
[10/27 08:45:25 visual_prompt]: 	Training 500/553. train loss: 2.1498,	1.8399 s / batch. (data: 1.36e+00). ETA=23:29:06, max mem: 11.4 GB 
[10/27 08:46:12 visual_prompt]: Epoch 17 / 100: avg data time: 3.91e-01, avg batch time: 0.8868, average train loss: 1.7611
[10/27 08:47:04 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1914, average loss: 0.9406
[10/27 08:47:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.72	
[10/27 08:47:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/27 08:48:36 visual_prompt]: 	Training 100/553. train loss: 0.7777,	0.4902 s / batch. (data: 2.25e-04). ETA=6:14:10, max mem: 11.4 GB 
[10/27 08:50:07 visual_prompt]: 	Training 200/553. train loss: 0.9807,	0.5289 s / batch. (data: 1.05e-02). ETA=6:42:49, max mem: 11.4 GB 
[10/27 08:51:36 visual_prompt]: 	Training 300/553. train loss: 0.7625,	0.4961 s / batch. (data: 2.44e-04). ETA=6:17:00, max mem: 11.4 GB 
[10/27 08:53:04 visual_prompt]: 	Training 400/553. train loss: 1.1502,	0.5124 s / batch. (data: 2.75e-04). ETA=6:28:35, max mem: 11.4 GB 
[10/27 08:54:31 visual_prompt]: 	Training 500/553. train loss: 1.3760,	1.0543 s / batch. (data: 5.66e-01). ETA=13:17:42, max mem: 11.4 GB 
[10/27 08:55:15 visual_prompt]: Epoch 18 / 100: avg data time: 3.93e-01, avg batch time: 0.8883, average train loss: 1.5025
[10/27 08:56:08 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1920, average loss: 0.7540
[10/27 08:56:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.00	
[10/27 08:56:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/27 08:57:39 visual_prompt]: 	Training 100/553. train loss: 0.6910,	0.5473 s / batch. (data: 6.91e-02). ETA=6:52:42, max mem: 11.4 GB 
[10/27 08:59:08 visual_prompt]: 	Training 200/553. train loss: 0.8913,	0.4914 s / batch. (data: 2.53e-04). ETA=6:09:45, max mem: 11.4 GB 
[10/27 09:00:36 visual_prompt]: 	Training 300/553. train loss: 2.8486,	0.8080 s / batch. (data: 3.08e-01). ETA=10:06:39, max mem: 11.4 GB 
[10/27 09:02:06 visual_prompt]: 	Training 400/553. train loss: 0.4527,	0.4788 s / batch. (data: 2.96e-04). ETA=5:58:41, max mem: 11.4 GB 
[10/27 09:03:31 visual_prompt]: 	Training 500/553. train loss: 0.7508,	0.5276 s / batch. (data: 2.76e-02). ETA=6:34:21, max mem: 11.4 GB 
[10/27 09:04:17 visual_prompt]: Epoch 19 / 100: avg data time: 3.90e-01, avg batch time: 0.8848, average train loss: 1.3542
[10/27 09:05:09 visual_prompt]: Inference (val):avg data time: 3.02e-04, avg batch time: 0.1918, average loss: 3.0618
[10/27 09:05:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.13	
[10/27 09:05:09 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/27 09:06:39 visual_prompt]: 	Training 100/553. train loss: 1.0473,	0.4960 s / batch. (data: 2.64e-04). ETA=6:09:28, max mem: 11.4 GB 
[10/27 09:08:09 visual_prompt]: 	Training 200/553. train loss: 0.6562,	0.5160 s / batch. (data: 7.96e-03). ETA=6:23:30, max mem: 11.4 GB 
[10/27 09:09:38 visual_prompt]: 	Training 300/553. train loss: 2.9208,	0.5114 s / batch. (data: 2.88e-04). ETA=6:19:12, max mem: 11.4 GB 
[10/27 09:11:05 visual_prompt]: 	Training 400/553. train loss: 0.6316,	0.5171 s / batch. (data: 1.66e-02). ETA=6:22:35, max mem: 11.4 GB 
[10/27 09:12:33 visual_prompt]: 	Training 500/553. train loss: 0.7365,	0.4933 s / batch. (data: 2.70e-04). ETA=6:04:09, max mem: 11.4 GB 
[10/27 09:13:20 visual_prompt]: Epoch 20 / 100: avg data time: 3.93e-01, avg batch time: 0.8876, average train loss: 1.4254
[10/27 09:14:13 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1921, average loss: 0.7011
[10/27 09:14:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.12	
[10/27 09:14:13 visual_prompt]: Best epoch 20: best metric: -0.701
[10/27 09:14:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[10/27 09:15:46 visual_prompt]: 	Training 100/553. train loss: 1.6867,	0.4842 s / batch. (data: 2.63e-04). ETA=5:56:13, max mem: 11.4 GB 
[10/27 09:17:14 visual_prompt]: 	Training 200/553. train loss: 1.1582,	0.5240 s / batch. (data: 6.68e-04). ETA=6:24:37, max mem: 11.4 GB 
[10/27 09:18:42 visual_prompt]: 	Training 300/553. train loss: 2.2283,	1.0237 s / batch. (data: 5.14e-01). ETA=12:29:42, max mem: 11.4 GB 
[10/27 09:20:10 visual_prompt]: 	Training 400/553. train loss: 2.3304,	0.4840 s / batch. (data: 2.52e-04). ETA=5:53:40, max mem: 11.4 GB 
[10/27 09:21:40 visual_prompt]: 	Training 500/553. train loss: 1.0747,	0.4809 s / batch. (data: 2.99e-04). ETA=5:50:35, max mem: 11.4 GB 
[10/27 09:22:24 visual_prompt]: Epoch 21 / 100: avg data time: 3.93e-01, avg batch time: 0.8876, average train loss: 1.5282
[10/27 09:23:16 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1908, average loss: 1.3357
[10/27 09:23:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.84	
[10/27 09:23:16 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[10/27 09:24:46 visual_prompt]: 	Training 100/553. train loss: 0.9583,	0.5029 s / batch. (data: 1.11e-02). ETA=6:05:21, max mem: 11.4 GB 
[10/27 09:26:16 visual_prompt]: 	Training 200/553. train loss: 1.4408,	0.5256 s / batch. (data: 2.48e-04). ETA=6:20:57, max mem: 11.4 GB 
[10/27 09:27:42 visual_prompt]: 	Training 300/553. train loss: 0.3387,	0.4960 s / batch. (data: 2.59e-04). ETA=5:58:40, max mem: 11.4 GB 
[10/27 09:29:11 visual_prompt]: 	Training 400/553. train loss: 0.6176,	0.5036 s / batch. (data: 5.38e-03). ETA=6:03:18, max mem: 11.4 GB 
[10/27 09:30:40 visual_prompt]: 	Training 500/553. train loss: 0.7170,	0.4978 s / batch. (data: 5.36e-03). ETA=5:58:20, max mem: 11.4 GB 
[10/27 09:31:27 visual_prompt]: Epoch 22 / 100: avg data time: 3.93e-01, avg batch time: 0.8882, average train loss: 1.4756
[10/27 09:32:19 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1919, average loss: 0.7499
[10/27 09:32:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.74	
[10/27 09:32:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[10/27 09:33:52 visual_prompt]: 	Training 100/553. train loss: 0.9455,	0.7926 s / batch. (data: 3.13e-01). ETA=9:28:30, max mem: 11.4 GB 
[10/27 09:35:22 visual_prompt]: 	Training 200/553. train loss: 2.7180,	1.3504 s / batch. (data: 8.74e-01). ETA=16:06:20, max mem: 11.4 GB 
[10/27 09:36:51 visual_prompt]: 	Training 300/553. train loss: 0.6860,	0.4965 s / batch. (data: 1.55e-02). ETA=5:54:28, max mem: 11.4 GB 
[10/27 09:38:18 visual_prompt]: 	Training 400/553. train loss: 0.8200,	0.5006 s / batch. (data: 6.76e-04). ETA=5:56:32, max mem: 11.4 GB 
[10/27 09:39:44 visual_prompt]: 	Training 500/553. train loss: 0.9644,	0.5354 s / batch. (data: 3.55e-02). ETA=6:20:25, max mem: 11.4 GB 
[10/27 09:40:29 visual_prompt]: Epoch 23 / 100: avg data time: 3.91e-01, avg batch time: 0.8860, average train loss: 1.3087
[10/27 09:41:22 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1912, average loss: 0.7542
[10/27 09:41:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 57.50	
[10/27 09:41:22 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[10/27 09:42:51 visual_prompt]: 	Training 100/553. train loss: 1.4445,	0.5033 s / batch. (data: 1.05e-02). ETA=5:56:20, max mem: 11.4 GB 
[10/27 09:44:19 visual_prompt]: 	Training 200/553. train loss: 1.1852,	0.5040 s / batch. (data: 7.31e-04). ETA=5:55:59, max mem: 11.4 GB 
[10/27 09:45:48 visual_prompt]: 	Training 300/553. train loss: 0.4559,	1.4449 s / batch. (data: 9.68e-01). ETA=16:58:12, max mem: 11.4 GB 
[10/27 09:47:16 visual_prompt]: 	Training 400/553. train loss: 1.7081,	0.5111 s / batch. (data: 5.39e-03). ETA=5:59:20, max mem: 11.4 GB 
[10/27 09:48:46 visual_prompt]: 	Training 500/553. train loss: 0.8659,	0.8404 s / batch. (data: 3.44e-01). ETA=9:49:26, max mem: 11.4 GB 
[10/27 09:49:32 visual_prompt]: Epoch 24 / 100: avg data time: 3.92e-01, avg batch time: 0.8870, average train loss: 1.4623
[10/27 09:50:25 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.1908, average loss: 2.7815
[10/27 09:50:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.72	
[10/27 09:50:25 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[10/27 09:51:59 visual_prompt]: 	Training 100/553. train loss: 2.8104,	0.5130 s / batch. (data: 1.30e-02). ETA=5:58:30, max mem: 11.4 GB 
[10/27 09:53:25 visual_prompt]: 	Training 200/553. train loss: 0.5814,	0.4822 s / batch. (data: 2.60e-04). ETA=5:36:07, max mem: 11.4 GB 
[10/27 09:54:53 visual_prompt]: 	Training 300/553. train loss: 1.0593,	1.0240 s / batch. (data: 5.24e-01). ETA=11:52:10, max mem: 11.4 GB 
[10/27 09:56:21 visual_prompt]: 	Training 400/553. train loss: 0.3271,	1.4040 s / batch. (data: 9.20e-01). ETA=16:14:05, max mem: 11.4 GB 
[10/27 09:57:50 visual_prompt]: 	Training 500/553. train loss: 0.7178,	2.0352 s / batch. (data: 1.54e+00). ETA=23:28:35, max mem: 11.4 GB 
[10/27 09:58:35 visual_prompt]: Epoch 25 / 100: avg data time: 3.93e-01, avg batch time: 0.8871, average train loss: 1.4980
[10/27 09:59:28 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1910, average loss: 2.2566
[10/27 09:59:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.80	
[10/27 09:59:28 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[10/27 10:00:59 visual_prompt]: 	Training 100/553. train loss: 1.6284,	0.4844 s / batch. (data: 2.62e-04). ETA=5:34:00, max mem: 11.4 GB 
[10/27 10:02:28 visual_prompt]: 	Training 200/553. train loss: 3.7633,	2.2521 s / batch. (data: 1.76e+00). ETA=1 day, 1:49:13, max mem: 11.4 GB 
[10/27 10:03:58 visual_prompt]: 	Training 300/553. train loss: 0.1673,	0.4840 s / batch. (data: 2.55e-04). ETA=5:32:09, max mem: 11.4 GB 
[10/27 10:05:26 visual_prompt]: 	Training 400/553. train loss: 1.3989,	0.4800 s / batch. (data: 2.69e-04). ETA=5:28:35, max mem: 11.4 GB 
[10/27 10:06:52 visual_prompt]: 	Training 500/553. train loss: 0.3554,	0.5000 s / batch. (data: 2.51e-04). ETA=5:41:26, max mem: 11.4 GB 
[10/27 10:07:37 visual_prompt]: Epoch 26 / 100: avg data time: 3.90e-01, avg batch time: 0.8851, average train loss: 1.4473
[10/27 10:08:29 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1893, average loss: 0.7304
[10/27 10:08:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.92	
[10/27 10:08:29 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[10/27 10:10:02 visual_prompt]: 	Training 100/553. train loss: 0.5228,	0.5120 s / batch. (data: 7.98e-03). ETA=5:48:21, max mem: 11.4 GB 
[10/27 10:11:29 visual_prompt]: 	Training 200/553. train loss: 2.4695,	1.2955 s / batch. (data: 7.99e-01). ETA=14:39:14, max mem: 11.4 GB 
[10/27 10:12:58 visual_prompt]: 	Training 300/553. train loss: 1.9684,	1.1085 s / batch. (data: 6.22e-01). ETA=12:30:29, max mem: 11.4 GB 
[10/27 10:14:28 visual_prompt]: 	Training 400/553. train loss: 0.8205,	0.4921 s / batch. (data: 2.77e-04). ETA=5:32:19, max mem: 11.4 GB 
[10/27 10:15:57 visual_prompt]: 	Training 500/553. train loss: 1.3927,	0.4966 s / batch. (data: 7.44e-04). ETA=5:34:33, max mem: 11.4 GB 
[10/27 10:16:40 visual_prompt]: Epoch 27 / 100: avg data time: 3.94e-01, avg batch time: 0.8878, average train loss: 1.4390
[10/27 10:17:33 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.1917, average loss: 0.7579
[10/27 10:17:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 60.21	
[10/27 10:17:33 visual_prompt]: Stopping early.
[10/27 10:17:33 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 10:17:33 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 10:17:33 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 10:17:33 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 10:17:33 visual_prompt]: Training with config:
[10/27 10:17:33 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 10:17:33 visual_prompt]: Loading training data...
[10/27 10:17:33 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 10:17:33 visual_prompt]: Loading validation data...
[10/27 10:17:33 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 10:17:33 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/27 10:17:35 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/27 10:17:35 visual_prompt]: tuned percent:0.529
[10/27 10:17:35 visual_prompt]: Device used for model: 0
[10/27 10:17:35 visual_prompt]: Setting up Evaluator...
[10/27 10:17:35 visual_prompt]: Setting up Trainer...
[10/27 10:17:35 visual_prompt]: 	Setting up the optimizer...
[10/27 10:17:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 10:19:07 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4960 s / batch. (data: 2.63e-04). ETA=7:36:17, max mem: 11.4 GB 
[10/27 10:20:33 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4846 s / batch. (data: 2.70e-04). ETA=7:25:00, max mem: 11.4 GB 
[10/27 10:22:05 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9559 s / batch. (data: 2.44e+00). ETA=1 day, 21:09:34, max mem: 11.4 GB 
[10/27 10:23:31 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4919 s / batch. (data: 2.58e-04). ETA=7:30:06, max mem: 11.4 GB 
[10/27 10:25:01 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5202 s / batch. (data: 5.87e-03). ETA=7:55:06, max mem: 11.4 GB 
[10/27 10:25:47 visual_prompt]: Epoch 1 / 100: avg data time: 3.94e-01, avg batch time: 0.8898, average train loss: 1.3966
[10/27 10:26:40 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1929, average loss: 1.3454
[10/27 10:26:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/27 10:26:40 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/27 10:28:10 visual_prompt]: 	Training 100/553. train loss: 0.7508,	0.4780 s / batch. (data: 2.65e-04). ETA=7:15:22, max mem: 11.4 GB 
[10/27 10:29:39 visual_prompt]: 	Training 200/553. train loss: 0.1879,	0.5152 s / batch. (data: 2.30e-02). ETA=7:48:20, max mem: 11.4 GB 
[10/27 10:31:09 visual_prompt]: 	Training 300/553. train loss: 0.9955,	1.6320 s / batch. (data: 1.14e+00). ETA=1 day, 0:40:59, max mem: 11.4 GB 
[10/27 10:32:36 visual_prompt]: 	Training 400/553. train loss: 1.1424,	0.4797 s / batch. (data: 2.67e-04). ETA=7:14:32, max mem: 11.4 GB 
[10/27 10:34:07 visual_prompt]: 	Training 500/553. train loss: 0.6582,	0.5115 s / batch. (data: 8.85e-03). ETA=7:42:28, max mem: 11.4 GB 
[10/27 10:34:51 visual_prompt]: Epoch 2 / 100: avg data time: 3.94e-01, avg batch time: 0.8884, average train loss: 0.8480
[10/27 10:35:43 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1912, average loss: 0.7565
[10/27 10:35:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.42	
[10/27 10:35:43 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/27 10:37:14 visual_prompt]: 	Training 100/553. train loss: 0.8714,	0.5040 s / batch. (data: 2.62e-04). ETA=7:34:24, max mem: 11.4 GB 
[10/27 10:38:44 visual_prompt]: 	Training 200/553. train loss: 0.7112,	1.7853 s / batch. (data: 1.30e+00). ETA=1 day, 2:46:33, max mem: 11.4 GB 
[10/27 10:40:10 visual_prompt]: 	Training 300/553. train loss: 0.6641,	0.4808 s / batch. (data: 2.76e-04). ETA=7:11:51, max mem: 11.4 GB 
[10/27 10:41:40 visual_prompt]: 	Training 400/553. train loss: 1.0932,	0.5196 s / batch. (data: 5.41e-03). ETA=7:45:49, max mem: 11.4 GB 
[10/27 10:43:09 visual_prompt]: 	Training 500/553. train loss: 0.7286,	2.0120 s / batch. (data: 1.51e+00). ETA=1 day, 6:00:31, max mem: 11.4 GB 
[10/27 10:43:54 visual_prompt]: Epoch 3 / 100: avg data time: 3.92e-01, avg batch time: 0.8869, average train loss: 0.7700
[10/27 10:44:46 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.1904, average loss: 0.8002
[10/27 10:44:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.06	
[10/27 10:44:46 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/27 10:46:20 visual_prompt]: 	Training 100/553. train loss: 0.7929,	0.4920 s / batch. (data: 2.64e-04). ETA=7:18:59, max mem: 11.4 GB 
[10/27 10:47:48 visual_prompt]: 	Training 200/553. train loss: 0.5645,	0.5013 s / batch. (data: 3.07e-04). ETA=7:26:31, max mem: 11.4 GB 
[10/27 10:49:17 visual_prompt]: 	Training 300/553. train loss: 0.5945,	1.8040 s / batch. (data: 1.31e+00). ETA=1 day, 2:43:46, max mem: 11.4 GB 
[10/27 10:50:42 visual_prompt]: 	Training 400/553. train loss: 0.7087,	0.9240 s / batch. (data: 4.38e-01). ETA=13:39:55, max mem: 11.4 GB 
[10/27 10:52:12 visual_prompt]: 	Training 500/553. train loss: 0.2209,	3.7800 s / batch. (data: 3.28e+00). ETA=2 days, 7:47:51, max mem: 11.4 GB 
[10/27 10:52:58 visual_prompt]: Epoch 4 / 100: avg data time: 3.95e-01, avg batch time: 0.8894, average train loss: 0.8799
[10/27 10:53:51 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1894, average loss: 1.1873
[10/27 10:53:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.68	
[10/27 10:53:51 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/27 10:55:21 visual_prompt]: 	Training 100/553. train loss: 2.4432,	0.5262 s / batch. (data: 3.01e-02). ETA=7:44:40, max mem: 11.4 GB 
[10/27 10:56:50 visual_prompt]: 	Training 200/553. train loss: 0.6012,	1.6946 s / batch. (data: 1.22e+00). ETA=1 day, 0:53:41, max mem: 11.4 GB 
[10/27 10:58:19 visual_prompt]: 	Training 300/553. train loss: 2.2018,	0.5065 s / batch. (data: 1.14e-02). ETA=7:25:38, max mem: 11.4 GB 
[10/27 10:59:47 visual_prompt]: 	Training 400/553. train loss: 1.7868,	0.5086 s / batch. (data: 1.55e-02). ETA=7:26:37, max mem: 11.4 GB 
[10/27 11:01:16 visual_prompt]: 	Training 500/553. train loss: 0.5659,	0.4915 s / batch. (data: 3.52e-04). ETA=7:10:44, max mem: 11.4 GB 
[10/27 11:02:02 visual_prompt]: Epoch 5 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 0.9274
[10/27 11:02:54 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1918, average loss: 1.1144
[10/27 11:02:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.01	
[10/27 11:02:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/27 11:04:28 visual_prompt]: 	Training 100/553. train loss: 0.6109,	0.4865 s / batch. (data: 2.84e-04). ETA=7:05:10, max mem: 11.4 GB 
[10/27 11:05:56 visual_prompt]: 	Training 200/553. train loss: 0.7930,	0.5000 s / batch. (data: 2.82e-04). ETA=7:16:07, max mem: 11.4 GB 
[10/27 11:07:22 visual_prompt]: 	Training 300/553. train loss: 0.7573,	0.4960 s / batch. (data: 5.45e-03). ETA=7:11:47, max mem: 11.4 GB 
[10/27 11:08:54 visual_prompt]: 	Training 400/553. train loss: 0.6180,	0.7848 s / batch. (data: 2.85e-01). ETA=11:21:54, max mem: 11.4 GB 
[10/27 11:10:22 visual_prompt]: 	Training 500/553. train loss: 1.1546,	1.4096 s / batch. (data: 8.98e-01). ETA=20:22:26, max mem: 11.4 GB 
[10/27 11:11:07 visual_prompt]: Epoch 6 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 0.9929
[10/27 11:11:59 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1914, average loss: 0.8231
[10/27 11:11:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.72	
[10/27 11:11:59 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/27 11:13:30 visual_prompt]: 	Training 100/553. train loss: 1.9192,	0.5053 s / batch. (data: 3.35e-04). ETA=7:16:58, max mem: 11.4 GB 
[10/27 11:14:57 visual_prompt]: 	Training 200/553. train loss: 0.5787,	0.5051 s / batch. (data: 1.61e-02). ETA=7:15:55, max mem: 11.4 GB 
[10/27 11:16:30 visual_prompt]: 	Training 300/553. train loss: 1.2315,	1.9135 s / batch. (data: 1.43e+00). ETA=1 day, 3:28:12, max mem: 11.4 GB 
[10/27 11:17:58 visual_prompt]: 	Training 400/553. train loss: 0.7586,	2.3918 s / batch. (data: 1.91e+00). ETA=1 day, 10:16:13, max mem: 11.4 GB 
[10/27 11:19:25 visual_prompt]: 	Training 500/553. train loss: 1.1517,	0.4914 s / batch. (data: 2.72e-04). ETA=7:01:39, max mem: 11.4 GB 
[10/27 11:20:09 visual_prompt]: Epoch 7 / 100: avg data time: 3.92e-01, avg batch time: 0.8864, average train loss: 1.1448
[10/27 11:21:01 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1926, average loss: 0.8086
[10/27 11:21:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.13	
[10/27 11:21:01 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/27 11:22:31 visual_prompt]: 	Training 100/553. train loss: 0.7315,	1.5921 s / batch. (data: 1.10e+00). ETA=22:42:02, max mem: 11.4 GB 
[10/27 11:24:01 visual_prompt]: 	Training 200/553. train loss: 0.5630,	0.5354 s / batch. (data: 2.24e-02). ETA=7:37:08, max mem: 11.4 GB 
[10/27 11:25:30 visual_prompt]: 	Training 300/553. train loss: 3.8130,	0.4880 s / batch. (data: 2.70e-04). ETA=6:55:50, max mem: 11.4 GB 
[10/27 11:26:59 visual_prompt]: 	Training 400/553. train loss: 0.9625,	1.1840 s / batch. (data: 6.93e-01). ETA=16:46:58, max mem: 11.4 GB 
[10/27 11:28:28 visual_prompt]: 	Training 500/553. train loss: 5.9174,	2.0320 s / batch. (data: 1.53e+00). ETA=1 day, 4:44:46, max mem: 11.4 GB 
[10/27 11:29:13 visual_prompt]: Epoch 8 / 100: avg data time: 3.94e-01, avg batch time: 0.8880, average train loss: 1.3004
[10/27 11:30:05 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.1897, average loss: 0.8517
[10/27 11:30:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.18	
[10/27 11:30:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/27 11:31:37 visual_prompt]: 	Training 100/553. train loss: 0.1497,	0.4800 s / batch. (data: 2.77e-04). ETA=6:46:12, max mem: 11.4 GB 
[10/27 11:33:06 visual_prompt]: 	Training 200/553. train loss: 1.0899,	0.5040 s / batch. (data: 2.82e-04). ETA=7:05:38, max mem: 11.4 GB 
[10/27 11:34:35 visual_prompt]: 	Training 300/553. train loss: 0.8593,	1.0467 s / batch. (data: 5.55e-01). ETA=14:42:16, max mem: 11.4 GB 
[10/27 11:36:05 visual_prompt]: 	Training 400/553. train loss: 1.1645,	0.5120 s / batch. (data: 7.61e-04). ETA=7:10:42, max mem: 11.4 GB 
[10/27 11:37:34 visual_prompt]: 	Training 500/553. train loss: 0.7203,	0.4956 s / batch. (data: 2.88e-04). ETA=6:56:07, max mem: 11.4 GB 
[10/27 11:38:19 visual_prompt]: Epoch 9 / 100: avg data time: 3.98e-01, avg batch time: 0.8923, average train loss: 1.1137
[10/27 11:39:12 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1921, average loss: 0.7009
[10/27 11:39:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.32	
[10/27 11:39:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/27 11:40:46 visual_prompt]: 	Training 100/553. train loss: 1.8691,	0.5253 s / batch. (data: 2.13e-02). ETA=7:19:44, max mem: 11.4 GB 
[10/27 11:42:14 visual_prompt]: 	Training 200/553. train loss: 0.9774,	0.5199 s / batch. (data: 2.70e-04). ETA=7:14:21, max mem: 11.4 GB 
[10/27 11:43:41 visual_prompt]: 	Training 300/553. train loss: 3.9520,	0.7124 s / batch. (data: 2.32e-01). ETA=9:53:58, max mem: 11.4 GB 
[10/27 11:45:10 visual_prompt]: 	Training 400/553. train loss: 1.2181,	1.4600 s / batch. (data: 9.56e-01). ETA=20:14:47, max mem: 11.4 GB 
[10/27 11:46:40 visual_prompt]: 	Training 500/553. train loss: 0.5126,	1.6074 s / batch. (data: 1.13e+00). ETA=22:14:47, max mem: 11.4 GB 
[10/27 11:47:25 visual_prompt]: Epoch 10 / 100: avg data time: 3.97e-01, avg batch time: 0.8926, average train loss: 1.3772
[10/27 11:48:18 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.1905, average loss: 0.7019
[10/27 11:48:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[10/27 11:48:18 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/27 11:49:52 visual_prompt]: 	Training 100/553. train loss: 0.8154,	0.5292 s / batch. (data: 2.52e-02). ETA=7:18:06, max mem: 11.4 GB 
[10/27 11:51:23 visual_prompt]: 	Training 200/553. train loss: 0.2506,	0.4842 s / batch. (data: 5.40e-03). ETA=6:39:59, max mem: 11.4 GB 
[10/27 11:52:51 visual_prompt]: 	Training 300/553. train loss: 0.0017,	1.3993 s / batch. (data: 9.03e-01). ETA=19:13:43, max mem: 11.4 GB 
[10/27 11:54:19 visual_prompt]: 	Training 400/553. train loss: 0.8213,	0.5200 s / batch. (data: 2.66e-04). ETA=7:07:52, max mem: 11.4 GB 
[10/27 11:55:47 visual_prompt]: 	Training 500/553. train loss: 0.8161,	0.4952 s / batch. (data: 2.56e-04). ETA=6:46:37, max mem: 11.4 GB 
[10/27 11:56:31 visual_prompt]: Epoch 11 / 100: avg data time: 3.97e-01, avg batch time: 0.8918, average train loss: 1.3579
[10/27 11:57:24 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1918, average loss: 0.7974
[10/27 11:57:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.56	
[10/27 11:57:24 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/27 11:58:58 visual_prompt]: 	Training 100/553. train loss: 0.8539,	0.4923 s / batch. (data: 4.39e-04). ETA=6:43:01, max mem: 11.4 GB 
[10/27 12:00:27 visual_prompt]: 	Training 200/553. train loss: 1.6486,	0.5003 s / batch. (data: 8.32e-03). ETA=6:48:44, max mem: 11.4 GB 
[10/27 12:01:55 visual_prompt]: 	Training 300/553. train loss: 0.7707,	0.5040 s / batch. (data: 2.64e-04). ETA=6:50:55, max mem: 11.4 GB 
[10/27 12:03:25 visual_prompt]: 	Training 400/553. train loss: 0.6928,	0.5120 s / batch. (data: 2.60e-04). ETA=6:56:35, max mem: 11.4 GB 
[10/27 12:04:54 visual_prompt]: 	Training 500/553. train loss: 1.4851,	0.4999 s / batch. (data: 7.95e-03). ETA=6:45:52, max mem: 11.4 GB 
[10/27 12:05:38 visual_prompt]: Epoch 12 / 100: avg data time: 4.00e-01, avg batch time: 0.8939, average train loss: 1.4234
[10/27 12:06:31 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1917, average loss: 0.8023
[10/27 12:06:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.58	
[10/27 12:06:31 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/27 12:08:05 visual_prompt]: 	Training 100/553. train loss: 0.7611,	0.4782 s / batch. (data: 2.65e-04). ETA=6:27:02, max mem: 11.4 GB 
[10/27 12:09:31 visual_prompt]: 	Training 200/553. train loss: 1.0405,	0.4921 s / batch. (data: 2.54e-04). ETA=6:37:29, max mem: 11.4 GB 
[10/27 12:11:01 visual_prompt]: 	Training 300/553. train loss: 1.3457,	2.2805 s / batch. (data: 1.80e+00). ETA=1 day, 6:38:11, max mem: 11.4 GB 
[10/27 12:12:28 visual_prompt]: 	Training 400/553. train loss: 1.2225,	0.5120 s / batch. (data: 2.41e-02). ETA=6:51:50, max mem: 11.4 GB 
[10/27 12:13:59 visual_prompt]: 	Training 500/553. train loss: 1.2656,	0.4846 s / batch. (data: 5.40e-03). ETA=6:28:58, max mem: 11.4 GB 
[10/27 12:14:44 visual_prompt]: Epoch 13 / 100: avg data time: 3.97e-01, avg batch time: 0.8922, average train loss: 1.2442
[10/27 12:15:37 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1915, average loss: 0.7214
[10/27 12:15:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.74	
[10/27 12:15:37 visual_prompt]: Best epoch 13: best metric: -0.721
[10/27 12:15:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/27 12:17:10 visual_prompt]: 	Training 100/553. train loss: 0.7370,	0.5197 s / batch. (data: 2.01e-02). ETA=6:55:52, max mem: 11.4 GB 
[10/27 12:18:39 visual_prompt]: 	Training 200/553. train loss: 0.1668,	0.8040 s / batch. (data: 3.24e-01). ETA=10:42:00, max mem: 11.4 GB 
[10/27 12:20:09 visual_prompt]: 	Training 300/553. train loss: 0.6640,	1.3762 s / batch. (data: 8.99e-01). ETA=18:16:39, max mem: 11.4 GB 
[10/27 12:21:36 visual_prompt]: 	Training 400/553. train loss: 0.6021,	0.4891 s / batch. (data: 2.82e-04). ETA=6:28:56, max mem: 11.4 GB 
[10/27 12:23:06 visual_prompt]: 	Training 500/553. train loss: 2.3341,	0.4842 s / batch. (data: 2.50e-04). ETA=6:24:13, max mem: 11.4 GB 
[10/27 12:23:50 visual_prompt]: Epoch 14 / 100: avg data time: 3.96e-01, avg batch time: 0.8911, average train loss: 1.2387
[10/27 12:24:43 visual_prompt]: Inference (val):avg data time: 5.09e-04, avg batch time: 0.1922, average loss: 0.8051
[10/27 12:24:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.28	
[10/27 12:24:43 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/27 12:26:15 visual_prompt]: 	Training 100/553. train loss: 0.7414,	1.1259 s / batch. (data: 6.49e-01). ETA=14:50:30, max mem: 11.4 GB 
[10/27 12:27:44 visual_prompt]: 	Training 200/553. train loss: 7.5325,	0.4841 s / batch. (data: 2.57e-04). ETA=6:22:07, max mem: 11.4 GB 
[10/27 12:29:14 visual_prompt]: 	Training 300/553. train loss: 0.7067,	0.5071 s / batch. (data: 2.71e-04). ETA=6:39:22, max mem: 11.4 GB 
[10/27 12:30:41 visual_prompt]: 	Training 400/553. train loss: 0.9795,	0.4916 s / batch. (data: 2.67e-04). ETA=6:26:21, max mem: 11.4 GB 
[10/27 12:32:11 visual_prompt]: 	Training 500/553. train loss: 0.7347,	0.5000 s / batch. (data: 2.81e-04). ETA=6:32:08, max mem: 11.4 GB 
[10/27 12:32:58 visual_prompt]: Epoch 15 / 100: avg data time: 4.01e-01, avg batch time: 0.8957, average train loss: 1.5080
[10/27 12:33:51 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1928, average loss: 0.7667
[10/27 12:33:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.67	
[10/27 12:33:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/27 12:35:21 visual_prompt]: 	Training 100/553. train loss: 1.3692,	0.4842 s / batch. (data: 2.61e-04). ETA=6:18:31, max mem: 11.4 GB 
[10/27 12:36:51 visual_prompt]: 	Training 200/553. train loss: 3.4754,	0.5238 s / batch. (data: 2.52e-02). ETA=6:48:38, max mem: 11.4 GB 
[10/27 12:38:20 visual_prompt]: 	Training 300/553. train loss: 0.6700,	0.4942 s / batch. (data: 3.17e-03). ETA=6:24:39, max mem: 11.4 GB 
[10/27 12:39:49 visual_prompt]: 	Training 400/553. train loss: 0.8491,	0.5080 s / batch. (data: 2.56e-04). ETA=6:34:36, max mem: 11.4 GB 
[10/27 12:41:18 visual_prompt]: 	Training 500/553. train loss: 1.2242,	1.9787 s / batch. (data: 1.49e+00). ETA=1 day, 1:33:38, max mem: 11.4 GB 
[10/27 12:42:04 visual_prompt]: Epoch 16 / 100: avg data time: 3.98e-01, avg batch time: 0.8913, average train loss: 1.3936
[10/27 12:42:56 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1900, average loss: 3.1298
[10/27 12:42:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.13	
[10/27 12:42:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/27 12:44:28 visual_prompt]: 	Training 100/553. train loss: 0.7448,	0.5177 s / batch. (data: 1.37e-02). ETA=6:39:56, max mem: 11.4 GB 
[10/27 12:45:59 visual_prompt]: 	Training 200/553. train loss: 0.5694,	0.4834 s / batch. (data: 2.68e-04). ETA=6:12:38, max mem: 11.4 GB 
[10/27 12:47:27 visual_prompt]: 	Training 300/553. train loss: 1.0520,	0.4879 s / batch. (data: 2.63e-04). ETA=6:15:15, max mem: 11.4 GB 
[10/27 12:48:55 visual_prompt]: 	Training 400/553. train loss: 1.3167,	0.9176 s / batch. (data: 4.27e-01). ETA=11:44:17, max mem: 11.4 GB 
[10/27 12:50:24 visual_prompt]: 	Training 500/553. train loss: 1.2209,	2.0722 s / batch. (data: 1.60e+00). ETA=1 day, 2:27:01, max mem: 11.4 GB 
[10/27 12:51:10 visual_prompt]: Epoch 17 / 100: avg data time: 3.98e-01, avg batch time: 0.8929, average train loss: 1.8332
[10/27 12:52:03 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1941, average loss: 1.3666
[10/27 12:52:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.11	
[10/27 12:52:03 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/27 12:53:36 visual_prompt]: 	Training 100/553. train loss: 0.7108,	0.4869 s / batch. (data: 2.68e-04). ETA=6:11:40, max mem: 11.4 GB 
[10/27 12:55:07 visual_prompt]: 	Training 200/553. train loss: 1.3439,	0.4849 s / batch. (data: 5.03e-03). ETA=6:09:18, max mem: 11.4 GB 
[10/27 12:56:36 visual_prompt]: 	Training 300/553. train loss: 0.5764,	0.5054 s / batch. (data: 2.06e-02). ETA=6:24:04, max mem: 11.4 GB 
[10/27 12:58:05 visual_prompt]: 	Training 400/553. train loss: 0.6978,	0.5054 s / batch. (data: 2.64e-04). ETA=6:23:15, max mem: 11.4 GB 
[10/27 12:59:33 visual_prompt]: 	Training 500/553. train loss: 0.7412,	0.5288 s / batch. (data: 2.07e-02). ETA=6:40:04, max mem: 11.4 GB 
[10/27 13:00:17 visual_prompt]: Epoch 18 / 100: avg data time: 3.99e-01, avg batch time: 0.8938, average train loss: 1.3462
[10/27 13:01:10 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1908, average loss: 1.8738
[10/27 13:01:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.41	
[10/27 13:01:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/27 13:02:41 visual_prompt]: 	Training 100/553. train loss: 0.5937,	0.7840 s / batch. (data: 3.02e-01). ETA=9:51:14, max mem: 11.4 GB 
[10/27 13:04:11 visual_prompt]: 	Training 200/553. train loss: 0.7822,	0.5281 s / batch. (data: 2.43e-02). ETA=6:37:22, max mem: 11.4 GB 
[10/27 13:05:40 visual_prompt]: 	Training 300/553. train loss: 0.2625,	0.5120 s / batch. (data: 2.60e-04). ETA=6:24:24, max mem: 11.4 GB 
[10/27 13:07:10 visual_prompt]: 	Training 400/553. train loss: 0.8017,	0.5188 s / batch. (data: 7.73e-04). ETA=6:28:36, max mem: 11.4 GB 
[10/27 13:08:35 visual_prompt]: 	Training 500/553. train loss: 0.7907,	0.4960 s / batch. (data: 2.92e-04). ETA=6:10:42, max mem: 11.4 GB 
[10/27 13:09:22 visual_prompt]: Epoch 19 / 100: avg data time: 3.95e-01, avg batch time: 0.8898, average train loss: 1.3106
[10/27 13:10:14 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1904, average loss: 5.9720
[10/27 13:10:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.31	
[10/27 13:10:14 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[10/27 13:11:45 visual_prompt]: 	Training 100/553. train loss: 0.5700,	0.4841 s / batch. (data: 5.41e-03). ETA=6:00:34, max mem: 11.4 GB 
[10/27 13:13:15 visual_prompt]: 	Training 200/553. train loss: 0.7786,	0.4921 s / batch. (data: 2.78e-04). ETA=6:05:45, max mem: 11.4 GB 
[10/27 13:14:44 visual_prompt]: 	Training 300/553. train loss: 0.9738,	0.4989 s / batch. (data: 5.40e-03). ETA=6:09:57, max mem: 11.4 GB 
[10/27 13:16:13 visual_prompt]: 	Training 400/553. train loss: 0.8930,	0.4987 s / batch. (data: 2.60e-04). ETA=6:08:59, max mem: 11.4 GB 
[10/27 13:17:41 visual_prompt]: 	Training 500/553. train loss: 1.5884,	0.4856 s / batch. (data: 2.44e-04). ETA=5:58:28, max mem: 11.4 GB 
[10/27 13:18:28 visual_prompt]: Epoch 20 / 100: avg data time: 3.98e-01, avg batch time: 0.8929, average train loss: 1.2785
[10/27 13:19:21 visual_prompt]: Inference (val):avg data time: 3.04e-04, avg batch time: 0.1917, average loss: 0.7114
[10/27 13:19:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.22	
[10/27 13:19:21 visual_prompt]: Best epoch 20: best metric: -0.711
[10/27 13:19:21 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[10/27 13:20:55 visual_prompt]: 	Training 100/553. train loss: 3.2170,	0.5356 s / batch. (data: 1.86e-02). ETA=6:34:01, max mem: 11.4 GB 
[10/27 13:22:23 visual_prompt]: 	Training 200/553. train loss: 1.8813,	0.5125 s / batch. (data: 2.77e-02). ETA=6:16:08, max mem: 11.4 GB 
[10/27 13:23:52 visual_prompt]: 	Training 300/553. train loss: 2.0063,	0.5120 s / batch. (data: 1.60e-02). ETA=6:14:57, max mem: 11.4 GB 
[10/27 13:25:21 visual_prompt]: 	Training 400/553. train loss: 1.5634,	0.5080 s / batch. (data: 2.69e-04). ETA=6:11:09, max mem: 11.4 GB 
[10/27 13:26:50 visual_prompt]: 	Training 500/553. train loss: 0.7743,	0.5081 s / batch. (data: 2.31e-04). ETA=6:10:22, max mem: 11.4 GB 
[10/27 13:27:35 visual_prompt]: Epoch 21 / 100: avg data time: 3.97e-01, avg batch time: 0.8927, average train loss: 1.4627
[10/27 13:28:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1901, average loss: 1.9090
[10/27 13:28:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.46	
[10/27 13:28:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[10/27 13:29:58 visual_prompt]: 	Training 100/553. train loss: 1.9462,	0.4960 s / batch. (data: 2.98e-04). ETA=6:00:17, max mem: 11.4 GB 
[10/27 13:31:28 visual_prompt]: 	Training 200/553. train loss: 1.1154,	0.4920 s / batch. (data: 2.57e-04). ETA=5:56:33, max mem: 11.4 GB 
[10/27 13:32:55 visual_prompt]: 	Training 300/553. train loss: 0.2818,	0.5160 s / batch. (data: 2.60e-04). ETA=6:13:08, max mem: 11.4 GB 
[10/27 13:34:24 visual_prompt]: 	Training 400/553. train loss: 2.8655,	0.5073 s / batch. (data: 7.98e-03). ETA=6:05:58, max mem: 11.4 GB 
[10/27 13:35:53 visual_prompt]: 	Training 500/553. train loss: 0.7755,	0.4792 s / batch. (data: 2.65e-04). ETA=5:44:53, max mem: 11.4 GB 
[10/27 13:36:41 visual_prompt]: Epoch 22 / 100: avg data time: 3.98e-01, avg batch time: 0.8919, average train loss: 1.3423
[10/27 13:37:33 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1904, average loss: 1.5611
[10/27 13:37:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.44	
[10/27 13:37:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[10/27 13:39:06 visual_prompt]: 	Training 100/553. train loss: 0.9416,	0.5167 s / batch. (data: 2.06e-02). ETA=6:10:33, max mem: 11.4 GB 
[10/27 13:40:36 visual_prompt]: 	Training 200/553. train loss: 0.7077,	0.7760 s / batch. (data: 2.79e-01). ETA=9:15:17, max mem: 11.4 GB 
[10/27 13:42:06 visual_prompt]: 	Training 300/553. train loss: 0.9656,	0.4924 s / batch. (data: 2.84e-04). ETA=5:51:32, max mem: 11.4 GB 
[10/27 13:43:34 visual_prompt]: 	Training 400/553. train loss: 1.0682,	0.5076 s / batch. (data: 5.50e-03). ETA=6:01:30, max mem: 11.4 GB 
[10/27 13:45:01 visual_prompt]: 	Training 500/553. train loss: 0.0846,	0.5159 s / batch. (data: 1.18e-02). ETA=6:06:33, max mem: 11.4 GB 
[10/27 13:45:47 visual_prompt]: Epoch 23 / 100: avg data time: 3.96e-01, avg batch time: 0.8917, average train loss: 1.3497
[10/27 13:46:39 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1913, average loss: 1.3260
[10/27 13:46:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.13	
[10/27 13:46:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[10/27 13:48:08 visual_prompt]: 	Training 100/553. train loss: 1.3011,	0.4788 s / batch. (data: 2.78e-04). ETA=5:38:58, max mem: 11.4 GB 
[10/27 13:49:36 visual_prompt]: 	Training 200/553. train loss: 1.4170,	0.4900 s / batch. (data: 1.19e-02). ETA=5:46:05, max mem: 11.4 GB 
[10/27 13:51:05 visual_prompt]: 	Training 300/553. train loss: 1.0200,	1.5680 s / batch. (data: 1.07e+00). ETA=18:24:56, max mem: 11.4 GB 
[10/27 13:52:33 visual_prompt]: 	Training 400/553. train loss: 0.7066,	0.4971 s / batch. (data: 2.84e-04). ETA=5:49:27, max mem: 11.4 GB 
[10/27 13:54:03 visual_prompt]: 	Training 500/553. train loss: 0.8443,	0.8721 s / batch. (data: 3.72e-01). ETA=10:11:38, max mem: 11.4 GB 
[10/27 13:54:49 visual_prompt]: Epoch 24 / 100: avg data time: 3.92e-01, avg batch time: 0.8869, average train loss: 1.3989
[10/27 13:55:41 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1908, average loss: 3.2073
[10/27 13:55:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.06	
[10/27 13:55:41 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[10/27 13:57:16 visual_prompt]: 	Training 100/553. train loss: 0.7698,	0.4807 s / batch. (data: 2.47e-04). ETA=5:35:55, max mem: 11.4 GB 
[10/27 13:58:42 visual_prompt]: 	Training 200/553. train loss: 0.7792,	0.5040 s / batch. (data: 2.52e-04). ETA=5:51:21, max mem: 11.4 GB 
[10/27 14:00:10 visual_prompt]: 	Training 300/553. train loss: 0.7067,	0.4867 s / batch. (data: 5.40e-03). ETA=5:38:28, max mem: 11.4 GB 
[10/27 14:01:38 visual_prompt]: 	Training 400/553. train loss: 0.6587,	1.9483 s / batch. (data: 1.45e+00). ETA=22:31:44, max mem: 11.4 GB 
[10/27 14:03:07 visual_prompt]: 	Training 500/553. train loss: 0.6585,	2.0280 s / batch. (data: 1.55e+00). ETA=23:23:38, max mem: 11.4 GB 
[10/27 14:03:52 visual_prompt]: Epoch 25 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 1.3253
[10/27 14:04:45 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1914, average loss: 0.6974
[10/27 14:04:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.85	
[10/27 14:04:45 visual_prompt]: Best epoch 25: best metric: -0.697
[10/27 14:04:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[10/27 14:06:17 visual_prompt]: 	Training 100/553. train loss: 3.0042,	0.5030 s / batch. (data: 1.13e-02). ETA=5:46:50, max mem: 11.4 GB 
[10/27 14:07:46 visual_prompt]: 	Training 200/553. train loss: 0.8365,	0.8763 s / batch. (data: 3.78e-01). ETA=10:02:48, max mem: 11.4 GB 
[10/27 14:09:16 visual_prompt]: 	Training 300/553. train loss: 1.0444,	0.5042 s / batch. (data: 3.07e-04). ETA=5:45:58, max mem: 11.4 GB 
[10/27 14:10:45 visual_prompt]: 	Training 400/553. train loss: 0.8605,	0.5052 s / batch. (data: 1.55e-02). ETA=5:45:50, max mem: 11.4 GB 
[10/27 14:12:12 visual_prompt]: 	Training 500/553. train loss: 0.7240,	0.5126 s / batch. (data: 7.98e-03). ETA=5:50:03, max mem: 11.4 GB 
[10/27 14:12:58 visual_prompt]: Epoch 26 / 100: avg data time: 3.96e-01, avg batch time: 0.8908, average train loss: 1.2583
[10/27 14:13:51 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1918, average loss: 1.2564
[10/27 14:13:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.51	
[10/27 14:13:51 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[10/27 14:15:24 visual_prompt]: 	Training 100/553. train loss: 2.5816,	0.4929 s / batch. (data: 2.79e-04). ETA=5:35:20, max mem: 11.4 GB 
[10/27 14:16:53 visual_prompt]: 	Training 200/553. train loss: 1.9188,	2.3040 s / batch. (data: 1.79e+00). ETA=1 day, 2:03:43, max mem: 11.4 GB 
[10/27 14:18:21 visual_prompt]: 	Training 300/553. train loss: 1.2733,	1.2233 s / batch. (data: 7.46e-01). ETA=13:48:12, max mem: 11.4 GB 
[10/27 14:19:51 visual_prompt]: 	Training 400/553. train loss: 0.7424,	0.4976 s / batch. (data: 1.05e-02). ETA=5:36:04, max mem: 11.4 GB 
[10/27 14:21:21 visual_prompt]: 	Training 500/553. train loss: 0.5757,	0.5082 s / batch. (data: 7.26e-04). ETA=5:42:22, max mem: 11.4 GB 
[10/27 14:22:04 visual_prompt]: Epoch 27 / 100: avg data time: 3.98e-01, avg batch time: 0.8931, average train loss: 1.4154
[10/27 14:22:57 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1920, average loss: 0.9706
[10/27 14:22:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.16	
[10/27 14:22:57 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[10/27 14:24:29 visual_prompt]: 	Training 100/553. train loss: 0.0121,	0.5208 s / batch. (data: 3.28e-02). ETA=5:49:32, max mem: 11.4 GB 
[10/27 14:25:59 visual_prompt]: 	Training 200/553. train loss: 1.9885,	0.4917 s / batch. (data: 3.44e-04). ETA=5:29:12, max mem: 11.4 GB 
[10/27 14:27:28 visual_prompt]: 	Training 300/553. train loss: 0.9496,	2.2400 s / batch. (data: 1.74e+00). ETA=1 day, 0:55:53, max mem: 11.4 GB 
[10/27 14:28:56 visual_prompt]: 	Training 400/553. train loss: 1.2455,	0.5245 s / batch. (data: 1.55e-02). ETA=5:49:24, max mem: 11.4 GB 
[10/27 14:30:24 visual_prompt]: 	Training 500/553. train loss: 2.2565,	0.4886 s / batch. (data: 5.47e-03). ETA=5:24:41, max mem: 11.4 GB 
[10/27 14:31:11 visual_prompt]: Epoch 28 / 100: avg data time: 3.98e-01, avg batch time: 0.8926, average train loss: 1.5532
[10/27 14:32:04 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1910, average loss: 1.2214
[10/27 14:32:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 39.52	
[10/27 14:32:04 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[10/27 14:33:42 visual_prompt]: 	Training 100/553. train loss: 1.0555,	0.4960 s / batch. (data: 2.93e-04). ETA=5:28:19, max mem: 11.4 GB 
[10/27 14:35:11 visual_prompt]: 	Training 200/553. train loss: 0.8095,	2.3884 s / batch. (data: 1.89e+00). ETA=1 day, 2:17:00, max mem: 11.4 GB 
[10/27 14:36:38 visual_prompt]: 	Training 300/553. train loss: 0.9143,	0.5088 s / batch. (data: 7.35e-04). ETA=5:35:05, max mem: 11.4 GB 
[10/27 14:38:04 visual_prompt]: 	Training 400/553. train loss: 0.9483,	1.8528 s / batch. (data: 1.36e+00). ETA=20:17:10, max mem: 11.4 GB 
[10/27 14:39:33 visual_prompt]: 	Training 500/553. train loss: 0.8422,	1.2800 s / batch. (data: 7.88e-01). ETA=13:58:45, max mem: 11.4 GB 
[10/27 14:40:18 visual_prompt]: Epoch 29 / 100: avg data time: 3.99e-01, avg batch time: 0.8942, average train loss: 1.3598
[10/27 14:41:11 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1930, average loss: 1.2194
[10/27 14:41:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.48	
[10/27 14:41:11 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[10/27 14:42:41 visual_prompt]: 	Training 100/553. train loss: 0.9010,	0.4960 s / batch. (data: 2.66e-04). ETA=5:23:44, max mem: 11.4 GB 
[10/27 14:44:11 visual_prompt]: 	Training 200/553. train loss: 0.5771,	0.4960 s / batch. (data: 2.61e-04). ETA=5:22:54, max mem: 11.4 GB 
[10/27 14:45:39 visual_prompt]: 	Training 300/553. train loss: 0.9661,	0.8600 s / batch. (data: 3.80e-01). ETA=9:18:29, max mem: 11.4 GB 
[10/27 14:47:10 visual_prompt]: 	Training 400/553. train loss: 0.8305,	1.9349 s / batch. (data: 1.45e+00). ETA=20:53:16, max mem: 11.4 GB 
[10/27 14:48:38 visual_prompt]: 	Training 500/553. train loss: 1.5896,	2.0760 s / batch. (data: 1.57e+00). ETA=22:21:12, max mem: 11.4 GB 
[10/27 14:49:24 visual_prompt]: Epoch 30 / 100: avg data time: 3.98e-01, avg batch time: 0.8924, average train loss: 1.1778
[10/27 14:50:17 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1918, average loss: 0.6919
[10/27 14:50:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.29	
[10/27 14:50:17 visual_prompt]: Best epoch 30: best metric: -0.692
[10/27 14:50:17 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[10/27 14:51:50 visual_prompt]: 	Training 100/553. train loss: 0.5650,	0.4803 s / batch. (data: 2.77e-04). ETA=5:09:06, max mem: 11.4 GB 
[10/27 14:53:21 visual_prompt]: 	Training 200/553. train loss: 1.6314,	0.4926 s / batch. (data: 4.23e-04). ETA=5:16:11, max mem: 11.4 GB 
[10/27 14:54:47 visual_prompt]: 	Training 300/553. train loss: 2.6476,	0.5082 s / batch. (data: 5.38e-03). ETA=5:25:20, max mem: 11.4 GB 
[10/27 14:56:15 visual_prompt]: 	Training 400/553. train loss: 0.8845,	0.4993 s / batch. (data: 7.97e-03). ETA=5:18:48, max mem: 11.4 GB 
[10/27 14:57:44 visual_prompt]: 	Training 500/553. train loss: 0.9422,	0.5000 s / batch. (data: 2.70e-04). ETA=5:18:24, max mem: 11.4 GB 
[10/27 14:58:27 visual_prompt]: Epoch 31 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 1.3022
[10/27 14:59:20 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1925, average loss: 1.9101
[10/27 14:59:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.62	
[10/27 14:59:20 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[10/27 15:00:53 visual_prompt]: 	Training 100/553. train loss: 0.5619,	0.5015 s / batch. (data: 1.09e-02). ETA=5:18:04, max mem: 11.4 GB 
[10/27 15:02:20 visual_prompt]: 	Training 200/553. train loss: 1.4170,	0.4920 s / batch. (data: 3.08e-04). ETA=5:11:15, max mem: 11.4 GB 
[10/27 15:03:53 visual_prompt]: 	Training 300/553. train loss: 1.5430,	0.4779 s / batch. (data: 2.67e-04). ETA=5:01:32, max mem: 11.4 GB 
[10/27 15:05:21 visual_prompt]: 	Training 400/553. train loss: 0.7342,	0.5091 s / batch. (data: 1.04e-02). ETA=5:20:21, max mem: 11.4 GB 
[10/27 15:06:46 visual_prompt]: 	Training 500/553. train loss: 0.7999,	0.5320 s / batch. (data: 3.43e-02). ETA=5:33:54, max mem: 11.4 GB 
[10/27 15:07:30 visual_prompt]: Epoch 32 / 100: avg data time: 3.93e-01, avg batch time: 0.8868, average train loss: 1.1596
[10/27 15:08:23 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1919, average loss: 1.2380
[10/27 15:08:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.95	
[10/27 15:08:23 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[10/27 15:09:53 visual_prompt]: 	Training 100/553. train loss: 0.0892,	0.9208 s / batch. (data: 4.39e-01). ETA=9:35:33, max mem: 11.4 GB 
[10/27 15:11:24 visual_prompt]: 	Training 200/553. train loss: 1.7393,	2.4240 s / batch. (data: 1.92e+00). ETA=1 day, 1:11:06, max mem: 11.4 GB 
[10/27 15:12:51 visual_prompt]: 	Training 300/553. train loss: 0.5522,	0.5027 s / batch. (data: 2.58e-04). ETA=5:12:33, max mem: 11.4 GB 
[10/27 15:14:20 visual_prompt]: 	Training 400/553. train loss: 0.9413,	0.4880 s / batch. (data: 2.52e-04). ETA=5:02:36, max mem: 11.4 GB 
[10/27 15:15:47 visual_prompt]: 	Training 500/553. train loss: 1.0298,	0.5095 s / batch. (data: 2.63e-04). ETA=5:15:03, max mem: 11.4 GB 
[10/27 15:16:33 visual_prompt]: Epoch 33 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 1.2869
[10/27 15:17:25 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1897, average loss: 1.4090
[10/27 15:17:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.44	
[10/27 15:17:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[10/27 15:18:58 visual_prompt]: 	Training 100/553. train loss: 0.7045,	1.3477 s / batch. (data: 8.71e-01). ETA=13:49:59, max mem: 11.4 GB 
[10/27 15:20:24 visual_prompt]: 	Training 200/553. train loss: 1.3820,	0.6447 s / batch. (data: 1.65e-01). ETA=6:35:58, max mem: 11.4 GB 
[10/27 15:21:52 visual_prompt]: 	Training 300/553. train loss: 0.8091,	0.4796 s / batch. (data: 2.68e-04). ETA=4:53:44, max mem: 11.4 GB 
[10/27 15:23:21 visual_prompt]: 	Training 400/553. train loss: 1.4712,	0.5162 s / batch. (data: 1.04e-02). ETA=5:15:20, max mem: 11.4 GB 
[10/27 15:24:51 visual_prompt]: 	Training 500/553. train loss: 0.5831,	2.0192 s / batch. (data: 1.54e+00). ETA=20:30:03, max mem: 11.4 GB 
[10/27 15:25:35 visual_prompt]: Epoch 34 / 100: avg data time: 3.91e-01, avg batch time: 0.8854, average train loss: 1.2849
[10/27 15:26:27 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1902, average loss: 0.8168
[10/27 15:26:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.18	
[10/27 15:26:27 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[10/27 15:28:00 visual_prompt]: 	Training 100/553. train loss: 1.7408,	0.4819 s / batch. (data: 2.88e-04). ETA=4:52:21, max mem: 11.4 GB 
[10/27 15:29:30 visual_prompt]: 	Training 200/553. train loss: 1.2029,	1.0880 s / batch. (data: 5.67e-01). ETA=10:58:10, max mem: 11.4 GB 
[10/27 15:30:57 visual_prompt]: 	Training 300/553. train loss: 0.7261,	0.4880 s / batch. (data: 2.84e-04). ETA=4:54:23, max mem: 11.4 GB 
[10/27 15:32:25 visual_prompt]: 	Training 400/553. train loss: 1.4083,	0.5023 s / batch. (data: 1.09e-02). ETA=5:02:11, max mem: 11.4 GB 
[10/27 15:33:52 visual_prompt]: 	Training 500/553. train loss: 0.7186,	0.7684 s / batch. (data: 2.72e-01). ETA=7:40:59, max mem: 11.4 GB 
[10/27 15:34:37 visual_prompt]: Epoch 35 / 100: avg data time: 3.91e-01, avg batch time: 0.8855, average train loss: 1.2393
[10/27 15:35:29 visual_prompt]: Inference (val):avg data time: 1.99e-04, avg batch time: 0.1924, average loss: 1.3570
[10/27 15:35:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.81	
[10/27 15:35:29 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[10/27 15:37:00 visual_prompt]: 	Training 100/553. train loss: 0.5946,	1.2240 s / batch. (data: 7.45e-01). ETA=12:11:15, max mem: 11.4 GB 
[10/27 15:38:29 visual_prompt]: 	Training 200/553. train loss: 0.8660,	1.2956 s / batch. (data: 8.08e-01). ETA=12:51:52, max mem: 11.4 GB 
[10/27 15:40:00 visual_prompt]: 	Training 300/553. train loss: 0.0408,	0.5040 s / batch. (data: 2.72e-04). ETA=4:59:24, max mem: 11.4 GB 
[10/27 15:41:27 visual_prompt]: 	Training 400/553. train loss: 0.6426,	0.5024 s / batch. (data: 2.14e-02). ETA=4:57:38, max mem: 11.4 GB 
[10/27 15:42:57 visual_prompt]: 	Training 500/553. train loss: 0.9634,	1.7334 s / batch. (data: 1.23e+00). ETA=17:04:00, max mem: 11.4 GB 
[10/27 15:43:39 visual_prompt]: Epoch 36 / 100: avg data time: 3.91e-01, avg batch time: 0.8854, average train loss: 1.2201
[10/27 15:44:31 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1911, average loss: 2.1824
[10/27 15:44:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.41	
[10/27 15:44:31 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[10/27 15:46:04 visual_prompt]: 	Training 100/553. train loss: 0.5447,	0.5000 s / batch. (data: 2.85e-04). ETA=4:54:04, max mem: 11.4 GB 
[10/27 15:47:31 visual_prompt]: 	Training 200/553. train loss: 0.6660,	0.5001 s / batch. (data: 5.41e-03). ETA=4:53:19, max mem: 11.4 GB 
[10/27 15:49:00 visual_prompt]: 	Training 300/553. train loss: 2.7799,	2.1094 s / batch. (data: 1.63e+00). ETA=20:33:43, max mem: 11.4 GB 
[10/27 15:50:31 visual_prompt]: 	Training 400/553. train loss: 0.6247,	2.4313 s / batch. (data: 1.94e+00). ETA=23:37:55, max mem: 11.4 GB 
[10/27 15:51:56 visual_prompt]: 	Training 500/553. train loss: 0.9903,	1.5600 s / batch. (data: 1.07e+00). ETA=15:07:11, max mem: 11.4 GB 
[10/27 15:52:42 visual_prompt]: Epoch 37 / 100: avg data time: 3.92e-01, avg batch time: 0.8871, average train loss: 1.2886
[10/27 15:53:34 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1919, average loss: 0.7153
[10/27 15:53:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.66	
[10/27 15:53:34 visual_prompt]: Stopping early.
[10/27 15:53:34 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 15:53:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 15:53:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 15:53:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 15:53:34 visual_prompt]: Training with config:
[10/27 15:53:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 15:53:34 visual_prompt]: Loading training data...
[10/27 15:53:34 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 15:53:34 visual_prompt]: Loading validation data...
[10/27 15:53:34 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 15:53:34 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/27 15:53:37 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/27 15:53:37 visual_prompt]: tuned percent:0.529
[10/27 15:53:37 visual_prompt]: Device used for model: 0
[10/27 15:53:37 visual_prompt]: Setting up Evaluator...
[10/27 15:53:37 visual_prompt]: Setting up Trainer...
[10/27 15:53:37 visual_prompt]: 	Setting up the optimizer...
[10/27 15:53:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 15:55:09 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4917 s / batch. (data: 2.85e-04). ETA=7:32:24, max mem: 11.4 GB 
[10/27 15:56:35 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5115 s / batch. (data: 3.57e-03). ETA=7:49:42, max mem: 11.4 GB 
[10/27 15:58:07 visual_prompt]: 	Training 300/553. train loss: 1.5173,	3.0608 s / batch. (data: 2.57e+00). ETA=1 day, 22:45:45, max mem: 11.4 GB 
[10/27 15:59:32 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4868 s / batch. (data: 2.81e-04). ETA=7:25:26, max mem: 11.4 GB 
[10/27 16:01:03 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4947 s / batch. (data: 2.71e-04). ETA=7:31:49, max mem: 11.4 GB 
[10/27 16:01:48 visual_prompt]: Epoch 1 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 1.3966
[10/27 16:02:40 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.1915, average loss: 1.3454
[10/27 16:02:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/27 16:02:40 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/27 16:04:12 visual_prompt]: 	Training 100/553. train loss: 0.7849,	1.1520 s / batch. (data: 6.36e-01). ETA=17:29:12, max mem: 11.4 GB 
[10/27 16:05:39 visual_prompt]: 	Training 200/553. train loss: 0.0817,	0.7720 s / batch. (data: 2.76e-01). ETA=11:41:50, max mem: 11.4 GB 
[10/27 16:07:10 visual_prompt]: 	Training 300/553. train loss: 0.7955,	1.7160 s / batch. (data: 1.22e+00). ETA=1 day, 1:57:11, max mem: 11.4 GB 
[10/27 16:08:37 visual_prompt]: 	Training 400/553. train loss: 1.3678,	0.5120 s / batch. (data: 1.20e-02). ETA=7:43:44, max mem: 11.4 GB 
[10/27 16:10:06 visual_prompt]: 	Training 500/553. train loss: 0.5476,	0.4999 s / batch. (data: 2.63e-04). ETA=7:31:57, max mem: 11.4 GB 
[10/27 16:10:51 visual_prompt]: Epoch 2 / 100: avg data time: 3.92e-01, avg batch time: 0.8870, average train loss: 0.8942
[10/27 16:11:43 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1924, average loss: 1.2315
[10/27 16:11:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.55	
[10/27 16:11:43 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/27 16:13:13 visual_prompt]: 	Training 100/553. train loss: 0.7813,	0.7239 s / batch. (data: 2.39e-01). ETA=10:52:38, max mem: 11.4 GB 
[10/27 16:14:43 visual_prompt]: 	Training 200/553. train loss: 0.9577,	0.6961 s / batch. (data: 2.13e-01). ETA=10:26:26, max mem: 11.4 GB 
[10/27 16:16:09 visual_prompt]: 	Training 300/553. train loss: 0.6222,	0.5058 s / batch. (data: 3.24e-04). ETA=7:34:19, max mem: 11.4 GB 
[10/27 16:17:39 visual_prompt]: 	Training 400/553. train loss: 1.9766,	0.4819 s / batch. (data: 2.61e-04). ETA=7:12:02, max mem: 11.4 GB 
[10/27 16:19:09 visual_prompt]: 	Training 500/553. train loss: 0.8757,	1.7911 s / batch. (data: 1.30e+00). ETA=1 day, 2:42:52, max mem: 11.4 GB 
[10/27 16:19:53 visual_prompt]: Epoch 3 / 100: avg data time: 3.91e-01, avg batch time: 0.8858, average train loss: 0.8792
[10/27 16:20:45 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1904, average loss: 0.7605
[10/27 16:20:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.33	
[10/27 16:20:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/27 16:22:18 visual_prompt]: 	Training 100/553. train loss: 0.7222,	0.4879 s / batch. (data: 2.59e-04). ETA=7:15:22, max mem: 11.4 GB 
[10/27 16:23:47 visual_prompt]: 	Training 200/553. train loss: 1.1492,	0.5200 s / batch. (data: 1.20e-02). ETA=7:43:08, max mem: 11.4 GB 
[10/27 16:25:16 visual_prompt]: 	Training 300/553. train loss: 0.8207,	2.0281 s / batch. (data: 1.54e+00). ETA=1 day, 6:02:58, max mem: 11.4 GB 
[10/27 16:26:40 visual_prompt]: 	Training 400/553. train loss: 0.7945,	1.7680 s / batch. (data: 1.29e+00). ETA=1 day, 2:08:48, max mem: 11.4 GB 
[10/27 16:28:10 visual_prompt]: 	Training 500/553. train loss: 0.4768,	3.5240 s / batch. (data: 3.03e+00). ETA=2 days, 4:01:08, max mem: 11.4 GB 
[10/27 16:28:56 visual_prompt]: Epoch 4 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 0.9556
[10/27 16:29:48 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1897, average loss: 1.1235
[10/27 16:29:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.68	
[10/27 16:29:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/27 16:31:18 visual_prompt]: 	Training 100/553. train loss: 0.9331,	0.4907 s / batch. (data: 2.64e-04). ETA=7:13:22, max mem: 11.4 GB 
[10/27 16:32:47 visual_prompt]: 	Training 200/553. train loss: 0.5778,	1.7592 s / batch. (data: 1.28e+00). ETA=1 day, 1:50:39, max mem: 11.4 GB 
[10/27 16:34:16 visual_prompt]: 	Training 300/553. train loss: 2.1726,	0.5080 s / batch. (data: 8.25e-04). ETA=7:26:54, max mem: 11.4 GB 
[10/27 16:35:43 visual_prompt]: 	Training 400/553. train loss: 1.0378,	0.4783 s / batch. (data: 2.67e-04). ETA=6:59:59, max mem: 11.4 GB 
[10/27 16:37:12 visual_prompt]: 	Training 500/553. train loss: 0.5878,	0.5079 s / batch. (data: 1.04e-02). ETA=7:25:08, max mem: 11.4 GB 
[10/27 16:37:58 visual_prompt]: Epoch 5 / 100: avg data time: 3.92e-01, avg batch time: 0.8869, average train loss: 0.8851
[10/27 16:38:51 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1930, average loss: 1.2635
[10/27 16:38:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.05	
[10/27 16:38:51 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/27 16:40:24 visual_prompt]: 	Training 100/553. train loss: 0.5787,	0.4966 s / batch. (data: 7.61e-04). ETA=7:13:59, max mem: 11.4 GB 
[10/27 16:41:52 visual_prompt]: 	Training 200/553. train loss: 0.7370,	0.5038 s / batch. (data: 1.17e-02). ETA=7:19:25, max mem: 11.4 GB 
[10/27 16:43:18 visual_prompt]: 	Training 300/553. train loss: 0.5703,	0.5080 s / batch. (data: 2.70e-04). ETA=7:22:15, max mem: 11.4 GB 
[10/27 16:44:50 visual_prompt]: 	Training 400/553. train loss: 0.8446,	0.6594 s / batch. (data: 1.61e-01). ETA=9:32:57, max mem: 11.4 GB 
[10/27 16:46:17 visual_prompt]: 	Training 500/553. train loss: 1.0154,	1.3961 s / batch. (data: 9.04e-01). ETA=20:10:44, max mem: 11.4 GB 
[10/27 16:47:02 visual_prompt]: Epoch 6 / 100: avg data time: 3.93e-01, avg batch time: 0.8882, average train loss: 0.8991
[10/27 16:47:54 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1910, average loss: 0.8116
[10/27 16:47:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.12	
[10/27 16:47:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/27 16:49:23 visual_prompt]: 	Training 100/553. train loss: 1.0201,	0.5366 s / batch. (data: 2.07e-02). ETA=7:44:02, max mem: 11.4 GB 
[10/27 16:50:52 visual_prompt]: 	Training 200/553. train loss: 0.5961,	0.4916 s / batch. (data: 2.67e-04). ETA=7:04:17, max mem: 11.4 GB 
[10/27 16:52:24 visual_prompt]: 	Training 300/553. train loss: 0.5604,	2.4880 s / batch. (data: 2.00e+00). ETA=1 day, 11:43:05, max mem: 11.4 GB 
[10/27 16:53:53 visual_prompt]: 	Training 400/553. train loss: 0.5680,	2.3800 s / batch. (data: 1.90e+00). ETA=1 day, 10:06:05, max mem: 11.4 GB 
[10/27 16:55:18 visual_prompt]: 	Training 500/553. train loss: 1.3419,	0.4911 s / batch. (data: 2.92e-04). ETA=7:01:21, max mem: 11.4 GB 
[10/27 16:56:03 visual_prompt]: Epoch 7 / 100: avg data time: 3.88e-01, avg batch time: 0.8835, average train loss: 0.8437
[10/27 16:56:55 visual_prompt]: Inference (val):avg data time: 4.15e-04, avg batch time: 0.1930, average loss: 0.7096
[10/27 16:56:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.51	
[10/27 16:56:55 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/27 16:58:24 visual_prompt]: 	Training 100/553. train loss: 0.7378,	0.4920 s / batch. (data: 2.99e-04). ETA=7:00:53, max mem: 11.4 GB 
[10/27 16:59:55 visual_prompt]: 	Training 200/553. train loss: 1.4497,	0.4958 s / batch. (data: 2.92e-04). ETA=7:03:18, max mem: 11.4 GB 
[10/27 17:01:23 visual_prompt]: 	Training 300/553. train loss: 2.3054,	0.4889 s / batch. (data: 1.06e-02). ETA=6:56:36, max mem: 11.4 GB 
[10/27 17:02:53 visual_prompt]: 	Training 400/553. train loss: 0.6968,	1.5130 s / batch. (data: 1.01e+00). ETA=21:26:46, max mem: 11.4 GB 
[10/27 17:04:21 visual_prompt]: 	Training 500/553. train loss: 1.8070,	1.6139 s / batch. (data: 1.11e+00). ETA=22:49:52, max mem: 11.4 GB 
[10/27 17:05:06 visual_prompt]: Epoch 8 / 100: avg data time: 3.94e-01, avg batch time: 0.8879, average train loss: 1.1462
[10/27 17:05:58 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1923, average loss: 1.2314
[10/27 17:05:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.35	
[10/27 17:05:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/27 17:07:30 visual_prompt]: 	Training 100/553. train loss: 0.0786,	0.4959 s / batch. (data: 2.38e-04). ETA=6:59:41, max mem: 11.4 GB 
[10/27 17:08:58 visual_prompt]: 	Training 200/553. train loss: 0.6080,	0.4800 s / batch. (data: 2.73e-04). ETA=6:45:24, max mem: 11.4 GB 
[10/27 17:10:27 visual_prompt]: 	Training 300/553. train loss: 0.6227,	2.2280 s / batch. (data: 1.73e+00). ETA=1 day, 7:18:03, max mem: 11.4 GB 
[10/27 17:11:55 visual_prompt]: 	Training 400/553. train loss: 1.5931,	0.5037 s / batch. (data: 1.19e-02). ETA=7:03:45, max mem: 11.4 GB 
[10/27 17:13:25 visual_prompt]: 	Training 500/553. train loss: 0.7674,	1.2879 s / batch. (data: 8.02e-01). ETA=18:01:18, max mem: 11.4 GB 
[10/27 17:14:09 visual_prompt]: Epoch 9 / 100: avg data time: 3.92e-01, avg batch time: 0.8865, average train loss: 1.0142
[10/27 17:15:01 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1909, average loss: 1.3080
[10/27 17:15:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.94	
[10/27 17:15:01 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/27 17:16:34 visual_prompt]: 	Training 100/553. train loss: 1.5516,	0.5040 s / batch. (data: 5.39e-03). ETA=7:01:52, max mem: 11.4 GB 
[10/27 17:18:02 visual_prompt]: 	Training 200/553. train loss: 1.2630,	0.5054 s / batch. (data: 1.05e-02). ETA=7:02:13, max mem: 11.4 GB 
[10/27 17:19:29 visual_prompt]: 	Training 300/553. train loss: 0.5613,	0.4920 s / batch. (data: 2.70e-04). ETA=6:50:12, max mem: 11.4 GB 
[10/27 17:20:56 visual_prompt]: 	Training 400/553. train loss: 0.7199,	1.3240 s / batch. (data: 8.25e-01). ETA=18:21:38, max mem: 11.4 GB 
[10/27 17:22:25 visual_prompt]: 	Training 500/553. train loss: 0.5636,	0.8920 s / batch. (data: 3.90e-01). ETA=12:20:41, max mem: 11.4 GB 
[10/27 17:23:11 visual_prompt]: Epoch 10 / 100: avg data time: 3.90e-01, avg batch time: 0.8854, average train loss: 1.2713
[10/27 17:24:03 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1906, average loss: 0.7547
[10/27 17:24:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.61	
[10/27 17:24:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/27 17:25:36 visual_prompt]: 	Training 100/553. train loss: 1.0972,	0.5080 s / batch. (data: 2.65e-04). ETA=7:00:31, max mem: 11.4 GB 
[10/27 17:27:06 visual_prompt]: 	Training 200/553. train loss: 2.0765,	0.4885 s / batch. (data: 2.74e-04). ETA=6:43:35, max mem: 11.4 GB 
[10/27 17:28:34 visual_prompt]: 	Training 300/553. train loss: 0.0802,	2.4046 s / batch. (data: 1.92e+00). ETA=1 day, 9:02:33, max mem: 11.4 GB 
[10/27 17:30:00 visual_prompt]: 	Training 400/553. train loss: 0.9232,	0.4844 s / batch. (data: 2.59e-04). ETA=6:38:33, max mem: 11.4 GB 
[10/27 17:31:28 visual_prompt]: 	Training 500/553. train loss: 1.4194,	0.5164 s / batch. (data: 2.59e-04). ETA=7:04:03, max mem: 11.4 GB 
[10/27 17:32:13 visual_prompt]: Epoch 11 / 100: avg data time: 3.90e-01, avg batch time: 0.8857, average train loss: 1.0678
[10/27 17:33:05 visual_prompt]: Inference (val):avg data time: 3.38e-04, avg batch time: 0.1924, average loss: 0.6977
[10/27 17:33:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.22	
[10/27 17:33:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/27 17:34:38 visual_prompt]: 	Training 100/553. train loss: 0.8096,	0.5215 s / batch. (data: 1.35e-02). ETA=7:06:53, max mem: 11.4 GB 
[10/27 17:36:07 visual_prompt]: 	Training 200/553. train loss: 0.5722,	1.0273 s / batch. (data: 5.31e-01). ETA=13:59:13, max mem: 11.4 GB 
[10/27 17:37:34 visual_prompt]: 	Training 300/553. train loss: 0.8879,	0.5091 s / batch. (data: 1.05e-02). ETA=6:55:05, max mem: 11.4 GB 
[10/27 17:39:03 visual_prompt]: 	Training 400/553. train loss: 1.3653,	0.5000 s / batch. (data: 1.20e-02). ETA=6:46:49, max mem: 11.4 GB 
[10/27 17:40:32 visual_prompt]: 	Training 500/553. train loss: 4.1217,	0.5120 s / batch. (data: 2.76e-04). ETA=6:55:42, max mem: 11.4 GB 
[10/27 17:41:16 visual_prompt]: Epoch 12 / 100: avg data time: 3.93e-01, avg batch time: 0.8878, average train loss: 1.4926
[10/27 17:42:08 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1908, average loss: 3.3450
[10/27 17:42:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.01	
[10/27 17:42:08 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/27 17:43:41 visual_prompt]: 	Training 100/553. train loss: 0.8241,	0.6720 s / batch. (data: 1.65e-01). ETA=9:03:54, max mem: 11.4 GB 
[10/27 17:45:06 visual_prompt]: 	Training 200/553. train loss: 0.8612,	0.4958 s / batch. (data: 4.08e-04). ETA=6:40:30, max mem: 11.4 GB 
[10/27 17:46:36 visual_prompt]: 	Training 300/553. train loss: 1.2372,	1.6080 s / batch. (data: 1.10e+00). ETA=21:36:10, max mem: 11.4 GB 
[10/27 17:48:03 visual_prompt]: 	Training 400/553. train loss: 9.2050,	0.5120 s / batch. (data: 7.96e-03). ETA=6:51:51, max mem: 11.4 GB 
[10/27 17:49:32 visual_prompt]: 	Training 500/553. train loss: 1.6352,	0.4840 s / batch. (data: 2.66e-04). ETA=6:28:32, max mem: 11.4 GB 
[10/27 17:50:18 visual_prompt]: Epoch 13 / 100: avg data time: 3.91e-01, avg batch time: 0.8860, average train loss: 1.7058
[10/27 17:51:10 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1917, average loss: 0.8370
[10/27 17:51:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.84	
[10/27 17:51:10 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/27 17:52:43 visual_prompt]: 	Training 100/553. train loss: 0.5697,	0.5160 s / batch. (data: 7.96e-03). ETA=6:52:53, max mem: 11.4 GB 
[10/27 17:54:12 visual_prompt]: 	Training 200/553. train loss: 0.0415,	1.9430 s / batch. (data: 1.44e+00). ETA=1 day, 1:51:32, max mem: 11.4 GB 
[10/27 17:55:40 visual_prompt]: 	Training 300/553. train loss: 0.7266,	1.3911 s / batch. (data: 9.15e-01). ETA=18:28:30, max mem: 11.4 GB 
[10/27 17:57:08 visual_prompt]: 	Training 400/553. train loss: 1.0618,	0.4978 s / batch. (data: 5.37e-03). ETA=6:35:52, max mem: 11.4 GB 
[10/27 17:58:36 visual_prompt]: 	Training 500/553. train loss: 1.6016,	0.4961 s / batch. (data: 2.64e-04). ETA=6:33:39, max mem: 11.4 GB 
[10/27 17:59:21 visual_prompt]: Epoch 14 / 100: avg data time: 3.93e-01, avg batch time: 0.8866, average train loss: 1.3672
[10/27 18:00:13 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1894, average loss: 0.8363
[10/27 18:00:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.45	
[10/27 18:00:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/27 18:01:44 visual_prompt]: 	Training 100/553. train loss: 0.8651,	0.4857 s / batch. (data: 5.39e-03). ETA=6:24:10, max mem: 11.4 GB 
[10/27 18:03:12 visual_prompt]: 	Training 200/553. train loss: 7.4420,	0.5148 s / batch. (data: 2.47e-04). ETA=6:46:20, max mem: 11.4 GB 
[10/27 18:04:42 visual_prompt]: 	Training 300/553. train loss: 1.4422,	0.4964 s / batch. (data: 1.05e-02). ETA=6:30:59, max mem: 11.4 GB 
[10/27 18:06:08 visual_prompt]: 	Training 400/553. train loss: 0.7049,	0.4917 s / batch. (data: 2.58e-04). ETA=6:26:29, max mem: 11.4 GB 
[10/27 18:07:37 visual_prompt]: 	Training 500/553. train loss: 0.5574,	0.4970 s / batch. (data: 1.92e-02). ETA=6:29:48, max mem: 11.4 GB 
[10/27 18:08:24 visual_prompt]: Epoch 15 / 100: avg data time: 3.94e-01, avg batch time: 0.8874, average train loss: 1.2448
[10/27 18:09:16 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1898, average loss: 0.6983
[10/27 18:09:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.55	
[10/27 18:09:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/27 18:10:46 visual_prompt]: 	Training 100/553. train loss: 0.7021,	0.4913 s / batch. (data: 2.72e-04). ETA=6:24:04, max mem: 11.4 GB 
[10/27 18:12:15 visual_prompt]: 	Training 200/553. train loss: 1.5066,	0.4880 s / batch. (data: 2.58e-04). ETA=6:20:41, max mem: 11.4 GB 
[10/27 18:13:44 visual_prompt]: 	Training 300/553. train loss: 1.1086,	0.4950 s / batch. (data: 2.69e-04). ETA=6:25:19, max mem: 11.4 GB 
[10/27 18:15:12 visual_prompt]: 	Training 400/553. train loss: 0.7592,	0.5064 s / batch. (data: 7.98e-03). ETA=6:33:22, max mem: 11.4 GB 
[10/27 18:16:40 visual_prompt]: 	Training 500/553. train loss: 0.9825,	2.0671 s / batch. (data: 1.58e+00). ETA=1 day, 2:42:12, max mem: 11.4 GB 
[10/27 18:17:26 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 0.8859, average train loss: 1.1310
[10/27 18:18:18 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1907, average loss: 0.8358
[10/27 18:18:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.51	
[10/27 18:18:18 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/27 18:19:49 visual_prompt]: 	Training 100/553. train loss: 1.4485,	0.5167 s / batch. (data: 1.69e-02). ETA=6:39:10, max mem: 11.4 GB 
[10/27 18:21:19 visual_prompt]: 	Training 200/553. train loss: 3.5796,	0.4920 s / batch. (data: 2.68e-04). ETA=6:19:16, max mem: 11.4 GB 
[10/27 18:22:47 visual_prompt]: 	Training 300/553. train loss: 1.7648,	0.4862 s / batch. (data: 2.56e-04). ETA=6:13:58, max mem: 11.4 GB 
[10/27 18:24:14 visual_prompt]: 	Training 400/553. train loss: 0.6185,	1.0320 s / batch. (data: 5.39e-01). ETA=13:12:04, max mem: 11.4 GB 
[10/27 18:25:42 visual_prompt]: 	Training 500/553. train loss: 1.0635,	0.8517 s / batch. (data: 3.60e-01). ETA=10:52:17, max mem: 11.4 GB 
[10/27 18:26:29 visual_prompt]: Epoch 17 / 100: avg data time: 3.91e-01, avg batch time: 0.8871, average train loss: 1.2669
[10/27 18:27:21 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1919, average loss: 0.7793
[10/27 18:27:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.14	
[10/27 18:27:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/27 18:28:53 visual_prompt]: 	Training 100/553. train loss: 0.7913,	0.4785 s / batch. (data: 2.45e-04). ETA=6:05:13, max mem: 11.4 GB 
[10/27 18:30:24 visual_prompt]: 	Training 200/553. train loss: 0.6354,	0.5021 s / batch. (data: 5.39e-03). ETA=6:22:24, max mem: 11.4 GB 
[10/27 18:31:52 visual_prompt]: 	Training 300/553. train loss: 0.5532,	0.5160 s / batch. (data: 2.51e-04). ETA=6:32:09, max mem: 11.4 GB 
[10/27 18:33:20 visual_prompt]: 	Training 400/553. train loss: 1.0510,	0.5289 s / batch. (data: 2.78e-02). ETA=6:41:04, max mem: 11.4 GB 
[10/27 18:34:48 visual_prompt]: 	Training 500/553. train loss: 0.9966,	0.4968 s / batch. (data: 1.05e-02). ETA=6:15:53, max mem: 11.4 GB 
[10/27 18:35:32 visual_prompt]: Epoch 18 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 1.2314
[10/27 18:36:25 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1902, average loss: 0.8457
[10/27 18:36:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.75	
[10/27 18:36:25 visual_prompt]: Stopping early.
[10/27 18:36:25 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 18:36:25 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 18:36:25 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 18:36:25 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 18:36:25 visual_prompt]: Training with config:
[10/27 18:36:25 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 18:36:25 visual_prompt]: Loading training data...
[10/27 18:36:25 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 18:36:25 visual_prompt]: Loading validation data...
[10/27 18:36:25 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 18:36:25 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/27 18:36:27 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/27 18:36:27 visual_prompt]: tuned percent:0.529
[10/27 18:36:27 visual_prompt]: Device used for model: 0
[10/27 18:36:27 visual_prompt]: Setting up Evaluator...
[10/27 18:36:27 visual_prompt]: Setting up Trainer...
[10/27 18:36:27 visual_prompt]: 	Setting up the optimizer...
[10/27 18:36:27 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 18:37:59 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4920 s / batch. (data: 7.97e-03). ETA=7:32:39, max mem: 11.4 GB 
[10/27 18:39:25 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4788 s / batch. (data: 2.54e-04). ETA=7:19:43, max mem: 11.4 GB 
[10/27 18:40:58 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9200 s / batch. (data: 2.42e+00). ETA=1 day, 20:36:41, max mem: 11.4 GB 
[10/27 18:42:23 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4955 s / batch. (data: 7.47e-03). ETA=7:33:22, max mem: 11.4 GB 
[10/27 18:43:54 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4960 s / batch. (data: 2.88e-04). ETA=7:32:58, max mem: 11.4 GB 
[10/27 18:44:40 visual_prompt]: Epoch 1 / 100: avg data time: 3.95e-01, avg batch time: 0.8902, average train loss: 1.3966
[10/27 18:45:32 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1921, average loss: 1.3454
[10/27 18:45:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/27 18:45:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/27 18:47:03 visual_prompt]: 	Training 100/553. train loss: 0.7891,	1.0280 s / batch. (data: 5.33e-01). ETA=15:36:15, max mem: 11.4 GB 
[10/27 18:48:31 visual_prompt]: 	Training 200/553. train loss: 0.0766,	1.2186 s / batch. (data: 6.99e-01). ETA=18:27:52, max mem: 11.4 GB 
[10/27 18:50:02 visual_prompt]: 	Training 300/553. train loss: 0.7991,	1.6840 s / batch. (data: 1.18e+00). ETA=1 day, 1:28:07, max mem: 11.4 GB 
[10/27 18:51:30 visual_prompt]: 	Training 400/553. train loss: 1.3440,	0.5249 s / batch. (data: 6.40e-03). ETA=7:55:24, max mem: 11.4 GB 
[10/27 18:52:59 visual_prompt]: 	Training 500/553. train loss: 0.5504,	0.4920 s / batch. (data: 2.55e-04). ETA=7:24:49, max mem: 11.4 GB 
[10/27 18:53:43 visual_prompt]: Epoch 2 / 100: avg data time: 3.93e-01, avg batch time: 0.8884, average train loss: 0.8966
[10/27 18:54:36 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.1912, average loss: 1.3009
[10/27 18:54:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.43	
[10/27 18:54:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/27 18:56:06 visual_prompt]: 	Training 100/553. train loss: 0.8565,	0.5049 s / batch. (data: 2.65e-04). ETA=7:35:11, max mem: 11.4 GB 
[10/27 18:57:36 visual_prompt]: 	Training 200/553. train loss: 0.9810,	1.1440 s / batch. (data: 6.33e-01). ETA=17:09:30, max mem: 11.4 GB 
[10/27 18:59:02 visual_prompt]: 	Training 300/553. train loss: 0.6127,	0.5120 s / batch. (data: 2.62e-04). ETA=7:39:53, max mem: 11.4 GB 
[10/27 19:00:33 visual_prompt]: 	Training 400/553. train loss: 2.0015,	0.4960 s / batch. (data: 5.49e-03). ETA=7:24:43, max mem: 11.4 GB 
[10/27 19:02:03 visual_prompt]: 	Training 500/553. train loss: 0.9438,	1.8127 s / batch. (data: 1.32e+00). ETA=1 day, 3:02:11, max mem: 11.4 GB 
[10/27 19:02:48 visual_prompt]: Epoch 3 / 100: avg data time: 3.94e-01, avg batch time: 0.8894, average train loss: 0.9133
[10/27 19:03:40 visual_prompt]: Inference (val):avg data time: 9.08e-05, avg batch time: 0.1937, average loss: 0.7531
[10/27 19:03:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[10/27 19:03:40 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/27 19:05:14 visual_prompt]: 	Training 100/553. train loss: 0.8266,	0.5357 s / batch. (data: 5.39e-03). ETA=7:58:02, max mem: 11.4 GB 
[10/27 19:06:43 visual_prompt]: 	Training 200/553. train loss: 0.4386,	0.5081 s / batch. (data: 1.19e-02). ETA=7:32:31, max mem: 11.4 GB 
[10/27 19:08:13 visual_prompt]: 	Training 300/553. train loss: 0.9128,	1.8305 s / batch. (data: 1.33e+00). ETA=1 day, 3:07:19, max mem: 11.4 GB 
[10/27 19:09:38 visual_prompt]: 	Training 400/553. train loss: 1.2746,	1.9353 s / batch. (data: 1.45e+00). ETA=1 day, 4:37:17, max mem: 11.4 GB 
[10/27 19:11:09 visual_prompt]: 	Training 500/553. train loss: 0.2212,	3.6016 s / batch. (data: 3.12e+00). ETA=2 days, 5:09:53, max mem: 11.4 GB 
[10/27 19:11:55 visual_prompt]: Epoch 4 / 100: avg data time: 4.00e-01, avg batch time: 0.8942, average train loss: 1.0037
[10/27 19:12:48 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1897, average loss: 1.7492
[10/27 19:12:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.53	
[10/27 19:12:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/27 19:14:19 visual_prompt]: 	Training 100/553. train loss: 2.5729,	0.5082 s / batch. (data: 1.05e-02). ETA=7:28:46, max mem: 11.4 GB 
[10/27 19:15:48 visual_prompt]: 	Training 200/553. train loss: 0.8523,	1.8359 s / batch. (data: 1.35e+00). ETA=1 day, 2:58:15, max mem: 11.4 GB 
[10/27 19:17:18 visual_prompt]: 	Training 300/553. train loss: 1.5448,	0.5120 s / batch. (data: 2.84e-04). ETA=7:30:28, max mem: 11.4 GB 
[10/27 19:18:46 visual_prompt]: 	Training 400/553. train loss: 1.9887,	0.5000 s / batch. (data: 2.60e-04). ETA=7:19:05, max mem: 11.4 GB 
[10/27 19:20:15 visual_prompt]: 	Training 500/553. train loss: 0.5588,	0.5280 s / batch. (data: 3.97e-03). ETA=7:42:47, max mem: 11.4 GB 
[10/27 19:21:02 visual_prompt]: Epoch 5 / 100: avg data time: 3.98e-01, avg batch time: 0.8934, average train loss: 1.0179
[10/27 19:21:55 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1913, average loss: 1.8157
[10/27 19:21:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.13	
[10/27 19:21:55 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/27 19:23:28 visual_prompt]: 	Training 100/553. train loss: 0.8228,	0.5172 s / batch. (data: 5.73e-03). ETA=7:31:59, max mem: 11.4 GB 
[10/27 19:24:56 visual_prompt]: 	Training 200/553. train loss: 2.6638,	0.4840 s / batch. (data: 2.72e-04). ETA=7:02:10, max mem: 11.4 GB 
[10/27 19:26:24 visual_prompt]: 	Training 300/553. train loss: 0.5973,	0.4960 s / batch. (data: 2.75e-04). ETA=7:11:49, max mem: 11.4 GB 
[10/27 19:27:56 visual_prompt]: 	Training 400/553. train loss: 0.8031,	0.6120 s / batch. (data: 1.16e-01). ETA=8:51:47, max mem: 11.4 GB 
[10/27 19:29:25 visual_prompt]: 	Training 500/553. train loss: 1.7766,	1.4411 s / batch. (data: 9.61e-01). ETA=20:49:45, max mem: 11.4 GB 
[10/27 19:30:09 visual_prompt]: Epoch 6 / 100: avg data time: 3.99e-01, avg batch time: 0.8943, average train loss: 1.1451
[10/27 19:31:02 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1918, average loss: 1.3064
[10/27 19:31:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.30	
[10/27 19:31:02 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/27 19:32:33 visual_prompt]: 	Training 100/553. train loss: 2.8824,	0.5158 s / batch. (data: 5.83e-03). ETA=7:25:59, max mem: 11.4 GB 
[10/27 19:34:02 visual_prompt]: 	Training 200/553. train loss: 0.6122,	0.4842 s / batch. (data: 5.40e-03). ETA=6:57:51, max mem: 11.4 GB 
[10/27 19:35:35 visual_prompt]: 	Training 300/553. train loss: 0.7630,	2.4997 s / batch. (data: 2.02e+00). ETA=1 day, 11:53:07, max mem: 11.4 GB 
[10/27 19:37:03 visual_prompt]: 	Training 400/553. train loss: 0.5591,	1.8280 s / batch. (data: 1.35e+00). ETA=1 day, 2:11:30, max mem: 11.4 GB 
[10/27 19:38:30 visual_prompt]: 	Training 500/553. train loss: 1.2913,	0.4793 s / batch. (data: 2.50e-04). ETA=6:51:13, max mem: 11.4 GB 
[10/27 19:39:15 visual_prompt]: Epoch 7 / 100: avg data time: 3.96e-01, avg batch time: 0.8918, average train loss: 1.0157
[10/27 19:40:08 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1909, average loss: 0.7255
[10/27 19:40:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.21	
[10/27 19:40:08 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/27 19:41:38 visual_prompt]: 	Training 100/553. train loss: 1.0781,	1.0926 s / batch. (data: 6.05e-01). ETA=15:34:41, max mem: 11.4 GB 
[10/27 19:43:08 visual_prompt]: 	Training 200/553. train loss: 0.5249,	0.4842 s / batch. (data: 5.40e-03). ETA=6:53:23, max mem: 11.4 GB 
[10/27 19:44:38 visual_prompt]: 	Training 300/553. train loss: 2.1048,	0.5000 s / batch. (data: 2.70e-04). ETA=7:06:03, max mem: 11.4 GB 
[10/27 19:46:07 visual_prompt]: 	Training 400/553. train loss: 0.8543,	0.8360 s / batch. (data: 3.51e-01). ETA=11:51:00, max mem: 11.4 GB 
[10/27 19:47:36 visual_prompt]: 	Training 500/553. train loss: 1.8409,	2.1000 s / batch. (data: 1.59e+00). ETA=1 day, 5:42:32, max mem: 11.4 GB 
[10/27 19:48:20 visual_prompt]: Epoch 8 / 100: avg data time: 3.96e-01, avg batch time: 0.8906, average train loss: 1.1298
[10/27 19:49:13 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1919, average loss: 1.0439
[10/27 19:49:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.57	
[10/27 19:49:13 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/27 19:50:44 visual_prompt]: 	Training 100/553. train loss: 0.1056,	0.4878 s / batch. (data: 9.60e-03). ETA=6:52:49, max mem: 11.4 GB 
[10/27 19:52:12 visual_prompt]: 	Training 200/553. train loss: 0.9158,	0.4908 s / batch. (data: 1.21e-02). ETA=6:54:31, max mem: 11.4 GB 
[10/27 19:53:40 visual_prompt]: 	Training 300/553. train loss: 0.6081,	2.0080 s / batch. (data: 1.51e+00). ETA=1 day, 4:12:36, max mem: 11.4 GB 
[10/27 19:55:10 visual_prompt]: 	Training 400/553. train loss: 0.8540,	0.4992 s / batch. (data: 2.68e-04). ETA=6:59:55, max mem: 11.4 GB 
[10/27 19:56:39 visual_prompt]: 	Training 500/553. train loss: 1.0143,	1.3504 s / batch. (data: 8.73e-01). ETA=18:53:49, max mem: 11.4 GB 
[10/27 19:57:23 visual_prompt]: Epoch 9 / 100: avg data time: 3.92e-01, avg batch time: 0.8868, average train loss: 0.9284
[10/27 19:58:15 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1932, average loss: 1.4522
[10/27 19:58:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.16	
[10/27 19:58:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/27 19:59:49 visual_prompt]: 	Training 100/553. train loss: 1.9330,	0.4941 s / batch. (data: 1.20e-02). ETA=6:53:37, max mem: 11.4 GB 
[10/27 20:01:16 visual_prompt]: 	Training 200/553. train loss: 0.8991,	0.4882 s / batch. (data: 5.40e-03). ETA=6:47:48, max mem: 11.4 GB 
[10/27 20:02:45 visual_prompt]: 	Training 300/553. train loss: 0.7084,	1.4600 s / batch. (data: 9.65e-01). ETA=20:17:13, max mem: 11.4 GB 
[10/27 20:04:12 visual_prompt]: 	Training 400/553. train loss: 1.3526,	1.3042 s / batch. (data: 8.18e-01). ETA=18:05:10, max mem: 11.4 GB 
[10/27 20:05:41 visual_prompt]: 	Training 500/553. train loss: 0.6456,	1.5220 s / batch. (data: 1.04e+00). ETA=21:03:49, max mem: 11.4 GB 
[10/27 20:06:27 visual_prompt]: Epoch 10 / 100: avg data time: 3.93e-01, avg batch time: 0.8878, average train loss: 1.1051
[10/27 20:07:19 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1908, average loss: 0.7139
[10/27 20:07:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.07	
[10/27 20:07:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/27 20:08:53 visual_prompt]: 	Training 100/553. train loss: 0.6130,	0.5240 s / batch. (data: 7.98e-03). ETA=7:13:49, max mem: 11.4 GB 
[10/27 20:10:24 visual_prompt]: 	Training 200/553. train loss: 1.0468,	0.5158 s / batch. (data: 7.96e-03). ETA=7:06:10, max mem: 11.4 GB 
[10/27 20:11:52 visual_prompt]: 	Training 300/553. train loss: 0.0352,	2.5458 s / batch. (data: 2.05e+00). ETA=1 day, 10:59:02, max mem: 11.4 GB 
[10/27 20:13:19 visual_prompt]: 	Training 400/553. train loss: 0.6508,	0.4886 s / batch. (data: 2.76e-04). ETA=6:42:03, max mem: 11.4 GB 
[10/27 20:14:46 visual_prompt]: 	Training 500/553. train loss: 0.8888,	0.5000 s / batch. (data: 5.40e-03). ETA=6:50:34, max mem: 11.4 GB 
[10/27 20:15:31 visual_prompt]: Epoch 11 / 100: avg data time: 3.93e-01, avg batch time: 0.8884, average train loss: 0.9685
[10/27 20:16:23 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1906, average loss: 1.0599
[10/27 20:16:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.88	
[10/27 20:16:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/27 20:17:56 visual_prompt]: 	Training 100/553. train loss: 0.9867,	0.5400 s / batch. (data: 3.40e-02). ETA=7:22:03, max mem: 11.4 GB 
[10/27 20:19:26 visual_prompt]: 	Training 200/553. train loss: 0.8289,	1.2120 s / batch. (data: 7.23e-01). ETA=16:30:07, max mem: 11.4 GB 
[10/27 20:20:52 visual_prompt]: 	Training 300/553. train loss: 0.5928,	0.4874 s / batch. (data: 2.83e-04). ETA=6:37:21, max mem: 11.4 GB 
[10/27 20:22:21 visual_prompt]: 	Training 400/553. train loss: 0.9958,	0.5333 s / batch. (data: 9.22e-03). ETA=7:13:51, max mem: 11.4 GB 
[10/27 20:23:50 visual_prompt]: 	Training 500/553. train loss: 1.5598,	0.4791 s / batch. (data: 2.84e-04). ETA=6:28:58, max mem: 11.4 GB 
[10/27 20:24:34 visual_prompt]: Epoch 12 / 100: avg data time: 3.93e-01, avg batch time: 0.8875, average train loss: 1.0353
[10/27 20:25:26 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1929, average loss: 1.7338
[10/27 20:25:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.52	
[10/27 20:25:26 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/27 20:26:58 visual_prompt]: 	Training 100/553. train loss: 0.4935,	0.9717 s / batch. (data: 4.87e-01). ETA=13:06:27, max mem: 11.4 GB 
[10/27 20:28:25 visual_prompt]: 	Training 200/553. train loss: 0.7322,	0.5053 s / batch. (data: 1.55e-02). ETA=6:48:08, max mem: 11.4 GB 
[10/27 20:29:54 visual_prompt]: 	Training 300/553. train loss: 0.6502,	2.2264 s / batch. (data: 1.74e+00). ETA=1 day, 5:54:38, max mem: 11.4 GB 
[10/27 20:31:21 visual_prompt]: 	Training 400/553. train loss: 4.2979,	0.4959 s / batch. (data: 5.39e-03). ETA=6:38:55, max mem: 11.4 GB 
[10/27 20:32:50 visual_prompt]: 	Training 500/553. train loss: 0.9793,	0.5161 s / batch. (data: 2.80e-04). ETA=6:54:15, max mem: 11.4 GB 
[10/27 20:33:36 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8865, average train loss: 1.0679
[10/27 20:34:28 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1925, average loss: 0.9018
[10/27 20:34:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.56	
[10/27 20:34:28 visual_prompt]: Best epoch 13: best metric: -0.902
[10/27 20:34:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/27 20:36:01 visual_prompt]: 	Training 100/553. train loss: 0.5741,	0.5131 s / batch. (data: 1.55e-02). ETA=6:50:34, max mem: 11.4 GB 
[10/27 20:37:30 visual_prompt]: 	Training 200/553. train loss: 0.1554,	1.8716 s / batch. (data: 1.39e+00). ETA=1 day, 0:54:29, max mem: 11.4 GB 
[10/27 20:38:58 visual_prompt]: 	Training 300/553. train loss: 0.6708,	1.1601 s / batch. (data: 6.80e-01). ETA=15:24:26, max mem: 11.4 GB 
[10/27 20:40:26 visual_prompt]: 	Training 400/553. train loss: 0.7633,	0.4880 s / batch. (data: 2.45e-04). ETA=6:28:03, max mem: 11.4 GB 
[10/27 20:41:56 visual_prompt]: 	Training 500/553. train loss: 0.7374,	0.5125 s / batch. (data: 4.45e-03). ETA=6:46:39, max mem: 11.4 GB 
[10/27 20:42:39 visual_prompt]: Epoch 14 / 100: avg data time: 3.93e-01, avg batch time: 0.8872, average train loss: 1.3465
[10/27 20:43:32 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1896, average loss: 0.6976
[10/27 20:43:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.41	
[10/27 20:43:32 visual_prompt]: Best epoch 14: best metric: -0.698
[10/27 20:43:32 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/27 20:45:05 visual_prompt]: 	Training 100/553. train loss: 1.1456,	1.1540 s / batch. (data: 6.57e-01). ETA=15:12:44, max mem: 11.4 GB 
[10/27 20:46:32 visual_prompt]: 	Training 200/553. train loss: 0.6870,	0.4914 s / batch. (data: 2.63e-04). ETA=6:27:51, max mem: 11.4 GB 
[10/27 20:48:03 visual_prompt]: 	Training 300/553. train loss: 1.7931,	0.5183 s / batch. (data: 5.42e-03). ETA=6:48:12, max mem: 11.4 GB 
[10/27 20:49:30 visual_prompt]: 	Training 400/553. train loss: 0.5712,	0.5037 s / batch. (data: 7.98e-03). ETA=6:35:53, max mem: 11.4 GB 
[10/27 20:51:00 visual_prompt]: 	Training 500/553. train loss: 0.5794,	0.5039 s / batch. (data: 1.19e-02). ETA=6:35:11, max mem: 11.4 GB 
[10/27 20:51:47 visual_prompt]: Epoch 15 / 100: avg data time: 4.01e-01, avg batch time: 0.8947, average train loss: 1.5295
[10/27 20:52:40 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1901, average loss: 2.4272
[10/27 20:52:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.88	
[10/27 20:52:40 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/27 20:54:11 visual_prompt]: 	Training 100/553. train loss: 0.8225,	0.5200 s / batch. (data: 1.20e-02). ETA=6:46:30, max mem: 11.4 GB 
[10/27 20:55:40 visual_prompt]: 	Training 200/553. train loss: 1.9305,	0.4785 s / batch. (data: 2.66e-04). ETA=6:13:15, max mem: 11.4 GB 
[10/27 20:57:11 visual_prompt]: 	Training 300/553. train loss: 1.0690,	0.4935 s / batch. (data: 2.71e-04). ETA=6:24:09, max mem: 11.4 GB 
[10/27 20:58:39 visual_prompt]: 	Training 400/553. train loss: 1.0805,	0.5120 s / batch. (data: 7.41e-04). ETA=6:37:42, max mem: 11.4 GB 
[10/27 21:00:08 visual_prompt]: 	Training 500/553. train loss: 1.0064,	2.2144 s / batch. (data: 1.73e+00). ETA=1 day, 4:36:20, max mem: 11.4 GB 
[10/27 21:00:54 visual_prompt]: Epoch 16 / 100: avg data time: 3.98e-01, avg batch time: 0.8927, average train loss: 1.0199
[10/27 21:01:47 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1914, average loss: 0.6900
[10/27 21:01:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.45	
[10/27 21:01:47 visual_prompt]: Best epoch 16: best metric: -0.690
[10/27 21:01:47 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/27 21:03:19 visual_prompt]: 	Training 100/553. train loss: 0.7909,	0.5080 s / batch. (data: 1.20e-02). ETA=6:32:28, max mem: 11.4 GB 
[10/27 21:04:49 visual_prompt]: 	Training 200/553. train loss: 1.6670,	0.5000 s / batch. (data: 2.74e-04). ETA=6:25:25, max mem: 11.4 GB 
[10/27 21:06:18 visual_prompt]: 	Training 300/553. train loss: 1.2407,	0.4920 s / batch. (data: 7.98e-03). ETA=6:18:28, max mem: 11.4 GB 
[10/27 21:07:46 visual_prompt]: 	Training 400/553. train loss: 0.9229,	0.5201 s / batch. (data: 2.00e-02). ETA=6:39:13, max mem: 11.4 GB 
[10/27 21:09:14 visual_prompt]: 	Training 500/553. train loss: 1.0147,	1.7520 s / batch. (data: 1.23e+00). ETA=22:21:48, max mem: 11.4 GB 
[10/27 21:10:01 visual_prompt]: Epoch 17 / 100: avg data time: 3.99e-01, avg batch time: 0.8937, average train loss: 1.0409
[10/27 21:10:54 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1912, average loss: 0.6978
[10/27 21:10:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 59.28	
[10/27 21:10:54 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/27 21:12:27 visual_prompt]: 	Training 100/553. train loss: 0.8039,	0.5043 s / batch. (data: 1.05e-02). ETA=6:24:56, max mem: 11.4 GB 
[10/27 21:13:58 visual_prompt]: 	Training 200/553. train loss: 1.1996,	0.5197 s / batch. (data: 7.34e-04). ETA=6:35:49, max mem: 11.4 GB 
[10/27 21:15:27 visual_prompt]: 	Training 300/553. train loss: 0.5644,	0.5080 s / batch. (data: 2.27e-04). ETA=6:26:05, max mem: 11.4 GB 
[10/27 21:16:56 visual_prompt]: 	Training 400/553. train loss: 0.7405,	0.5156 s / batch. (data: 1.20e-02). ETA=6:30:57, max mem: 11.4 GB 
[10/27 21:18:24 visual_prompt]: 	Training 500/553. train loss: 1.3013,	0.5244 s / batch. (data: 3.25e-04). ETA=6:36:49, max mem: 11.4 GB 
[10/27 21:19:09 visual_prompt]: Epoch 18 / 100: avg data time: 3.98e-01, avg batch time: 0.8939, average train loss: 1.1639
[10/27 21:20:01 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1895, average loss: 0.8248
[10/27 21:20:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.12	
[10/27 21:20:01 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/27 21:21:34 visual_prompt]: 	Training 100/553. train loss: 0.6684,	0.5200 s / batch. (data: 7.96e-03). ETA=6:32:07, max mem: 11.4 GB 
[10/27 21:23:02 visual_prompt]: 	Training 200/553. train loss: 0.7811,	0.5080 s / batch. (data: 2.69e-04). ETA=6:22:14, max mem: 11.4 GB 
[10/27 21:24:32 visual_prompt]: 	Training 300/553. train loss: 3.3087,	0.4988 s / batch. (data: 2.72e-04). ETA=6:14:30, max mem: 11.4 GB 
[10/27 21:26:02 visual_prompt]: 	Training 400/553. train loss: 0.6804,	0.5400 s / batch. (data: 7.14e-04). ETA=6:44:31, max mem: 11.4 GB 
[10/27 21:27:26 visual_prompt]: 	Training 500/553. train loss: 0.6691,	0.4844 s / batch. (data: 2.47e-04). ETA=6:02:02, max mem: 11.4 GB 
[10/27 21:28:13 visual_prompt]: Epoch 19 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.9071
[10/27 21:29:06 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1905, average loss: 1.6016
[10/27 21:29:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.73	
[10/27 21:29:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[10/27 21:30:35 visual_prompt]: 	Training 100/553. train loss: 0.7957,	0.4880 s / batch. (data: 2.74e-04). ETA=6:03:30, max mem: 11.4 GB 
[10/27 21:32:06 visual_prompt]: 	Training 200/553. train loss: 0.4950,	0.5119 s / batch. (data: 2.92e-04). ETA=6:20:25, max mem: 11.4 GB 
[10/27 21:33:34 visual_prompt]: 	Training 300/553. train loss: 1.7569,	0.4942 s / batch. (data: 2.57e-04). ETA=6:06:28, max mem: 11.4 GB 
[10/27 21:35:02 visual_prompt]: 	Training 400/553. train loss: 0.5872,	0.5118 s / batch. (data: 3.30e-04). ETA=6:18:39, max mem: 11.4 GB 
[10/27 21:36:30 visual_prompt]: 	Training 500/553. train loss: 1.2568,	0.4787 s / batch. (data: 2.78e-04). ETA=5:53:22, max mem: 11.4 GB 
[10/27 21:37:17 visual_prompt]: Epoch 20 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 0.9863
[10/27 21:38:09 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1920, average loss: 0.8666
[10/27 21:38:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.40	
[10/27 21:38:09 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[10/27 21:39:43 visual_prompt]: 	Training 100/553. train loss: 0.8247,	0.5920 s / batch. (data: 1.03e-01). ETA=7:15:30, max mem: 11.4 GB 
[10/27 21:41:11 visual_prompt]: 	Training 200/553. train loss: 1.2051,	0.5097 s / batch. (data: 1.07e-02). ETA=6:14:05, max mem: 11.4 GB 
[10/27 21:42:40 visual_prompt]: 	Training 300/553. train loss: 2.6684,	1.6685 s / batch. (data: 1.16e+00). ETA=20:21:55, max mem: 11.4 GB 
[10/27 21:44:06 visual_prompt]: 	Training 400/553. train loss: 1.3162,	0.5038 s / batch. (data: 5.41e-03). ETA=6:08:08, max mem: 11.4 GB 
[10/27 21:45:35 visual_prompt]: 	Training 500/553. train loss: 0.6986,	0.4910 s / batch. (data: 2.45e-04). ETA=5:57:54, max mem: 11.4 GB 
[10/27 21:46:19 visual_prompt]: Epoch 21 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 0.9721
[10/27 21:47:12 visual_prompt]: Inference (val):avg data time: 5.06e-04, avg batch time: 0.1919, average loss: 0.6787
[10/27 21:47:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 59.04	
[10/27 21:47:12 visual_prompt]: Best epoch 21: best metric: -0.679
[10/27 21:47:12 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[10/27 21:48:43 visual_prompt]: 	Training 100/553. train loss: 1.2977,	0.4960 s / batch. (data: 2.53e-04). ETA=6:00:20, max mem: 11.4 GB 
[10/27 21:50:12 visual_prompt]: 	Training 200/553. train loss: 0.7353,	0.5242 s / batch. (data: 2.43e-02). ETA=6:19:54, max mem: 11.4 GB 
[10/27 21:51:38 visual_prompt]: 	Training 300/553. train loss: 0.3614,	0.5520 s / batch. (data: 6.63e-02). ETA=6:39:09, max mem: 11.4 GB 
[10/27 21:53:07 visual_prompt]: 	Training 400/553. train loss: 0.6027,	0.4805 s / batch. (data: 2.54e-04). ETA=5:46:39, max mem: 11.4 GB 
[10/27 21:54:36 visual_prompt]: 	Training 500/553. train loss: 0.5627,	0.4964 s / batch. (data: 1.55e-02). ETA=5:57:15, max mem: 11.4 GB 
[10/27 21:55:23 visual_prompt]: Epoch 22 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 0.9793
[10/27 21:56:15 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1908, average loss: 1.0108
[10/27 21:56:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.63	
[10/27 21:56:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[10/27 21:57:49 visual_prompt]: 	Training 100/553. train loss: 1.1862,	0.4960 s / batch. (data: 2.33e-04). ETA=5:55:47, max mem: 11.4 GB 
[10/27 21:59:18 visual_prompt]: 	Training 200/553. train loss: 1.3664,	1.2720 s / batch. (data: 7.70e-01). ETA=15:10:09, max mem: 11.4 GB 
[10/27 22:00:48 visual_prompt]: 	Training 300/553. train loss: 0.8777,	0.5400 s / batch. (data: 7.17e-04). ETA=6:25:30, max mem: 11.4 GB 
[10/27 22:02:14 visual_prompt]: 	Training 400/553. train loss: 0.5910,	0.4884 s / batch. (data: 2.70e-04). ETA=5:47:50, max mem: 11.4 GB 
[10/27 22:03:41 visual_prompt]: 	Training 500/553. train loss: 1.3555,	0.4919 s / batch. (data: 2.47e-04). ETA=5:49:30, max mem: 11.4 GB 
[10/27 22:04:26 visual_prompt]: Epoch 23 / 100: avg data time: 3.94e-01, avg batch time: 0.8880, average train loss: 0.9425
[10/27 22:05:19 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1916, average loss: 0.8482
[10/27 22:05:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.79	
[10/27 22:05:19 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[10/27 22:06:48 visual_prompt]: 	Training 100/553. train loss: 1.1844,	0.4974 s / batch. (data: 1.28e-02). ETA=5:52:07, max mem: 11.4 GB 
[10/27 22:08:16 visual_prompt]: 	Training 200/553. train loss: 0.7696,	0.5125 s / batch. (data: 1.05e-02). ETA=6:02:00, max mem: 11.4 GB 
[10/27 22:09:45 visual_prompt]: 	Training 300/553. train loss: 0.7723,	1.5344 s / batch. (data: 1.04e+00). ETA=18:01:17, max mem: 11.4 GB 
[10/27 22:11:14 visual_prompt]: 	Training 400/553. train loss: 0.8200,	0.4963 s / batch. (data: 2.84e-04). ETA=5:48:53, max mem: 11.4 GB 
[10/27 22:12:43 visual_prompt]: 	Training 500/553. train loss: 0.7005,	0.9282 s / batch. (data: 4.33e-01). ETA=10:50:58, max mem: 11.4 GB 
[10/27 22:13:30 visual_prompt]: Epoch 24 / 100: avg data time: 3.92e-01, avg batch time: 0.8874, average train loss: 0.9583
[10/27 22:14:22 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1925, average loss: 1.0279
[10/27 22:14:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.53	
[10/27 22:14:22 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[10/27 22:15:57 visual_prompt]: 	Training 100/553. train loss: 0.6263,	0.4910 s / batch. (data: 5.37e-03). ETA=5:43:04, max mem: 11.4 GB 
[10/27 22:17:22 visual_prompt]: 	Training 200/553. train loss: 1.9176,	0.4958 s / batch. (data: 2.52e-04). ETA=5:45:38, max mem: 11.4 GB 
[10/27 22:18:50 visual_prompt]: 	Training 300/553. train loss: 0.7795,	1.2841 s / batch. (data: 8.07e-01). ETA=14:53:02, max mem: 11.4 GB 
[10/27 22:20:19 visual_prompt]: 	Training 400/553. train loss: 0.6513,	1.8076 s / batch. (data: 1.31e+00). ETA=20:54:08, max mem: 11.4 GB 
[10/27 22:21:47 visual_prompt]: 	Training 500/553. train loss: 1.1312,	2.0429 s / batch. (data: 1.55e+00). ETA=23:33:56, max mem: 11.4 GB 
[10/27 22:22:32 visual_prompt]: Epoch 25 / 100: avg data time: 3.91e-01, avg batch time: 0.8867, average train loss: 1.0055
[10/27 22:23:25 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1909, average loss: 1.6364
[10/27 22:23:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.88	
[10/27 22:23:25 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[10/27 22:24:56 visual_prompt]: 	Training 100/553. train loss: 0.5253,	0.5070 s / batch. (data: 1.05e-02). ETA=5:49:35, max mem: 11.4 GB 
[10/27 22:26:26 visual_prompt]: 	Training 200/553. train loss: 2.0366,	2.2400 s / batch. (data: 1.73e+00). ETA=1 day, 1:40:55, max mem: 11.4 GB 
[10/27 22:27:55 visual_prompt]: 	Training 300/553. train loss: 0.3862,	0.4915 s / batch. (data: 2.53e-04). ETA=5:37:19, max mem: 11.4 GB 
[10/27 22:29:23 visual_prompt]: 	Training 400/553. train loss: 1.8544,	0.4806 s / batch. (data: 2.62e-04). ETA=5:29:01, max mem: 11.4 GB 
[10/27 22:30:50 visual_prompt]: 	Training 500/553. train loss: 0.5987,	0.4918 s / batch. (data: 2.51e-04). ETA=5:35:52, max mem: 11.4 GB 
[10/27 22:31:35 visual_prompt]: Epoch 26 / 100: avg data time: 3.91e-01, avg batch time: 0.8862, average train loss: 1.0053
[10/27 22:32:27 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.1906, average loss: 0.6781
[10/27 22:32:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.05	
[10/27 22:32:27 visual_prompt]: Best epoch 26: best metric: -0.678
[10/27 22:32:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[10/27 22:33:59 visual_prompt]: 	Training 100/553. train loss: 0.5245,	0.4789 s / batch. (data: 2.88e-04). ETA=5:25:49, max mem: 11.4 GB 
[10/27 22:35:27 visual_prompt]: 	Training 200/553. train loss: 1.2006,	1.7680 s / batch. (data: 1.27e+00). ETA=19:59:56, max mem: 11.4 GB 
[10/27 22:36:56 visual_prompt]: 	Training 300/553. train loss: 0.6498,	0.4811 s / batch. (data: 2.68e-04). ETA=5:25:41, max mem: 11.4 GB 
[10/27 22:38:26 visual_prompt]: 	Training 400/553. train loss: 0.8225,	0.5160 s / batch. (data: 7.54e-04). ETA=5:48:29, max mem: 11.4 GB 
[10/27 22:39:55 visual_prompt]: 	Training 500/553. train loss: 0.8757,	0.4840 s / batch. (data: 2.75e-04). ETA=5:26:05, max mem: 11.4 GB 
[10/27 22:40:39 visual_prompt]: Epoch 27 / 100: avg data time: 3.93e-01, avg batch time: 0.8889, average train loss: 0.9148
[10/27 22:41:31 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1927, average loss: 1.1595
[10/27 22:41:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.54	
[10/27 22:41:31 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[10/27 22:43:02 visual_prompt]: 	Training 100/553. train loss: 0.4319,	0.5003 s / batch. (data: 2.53e-04). ETA=5:35:47, max mem: 11.4 GB 
[10/27 22:44:31 visual_prompt]: 	Training 200/553. train loss: 1.9569,	0.5039 s / batch. (data: 2.28e-04). ETA=5:37:21, max mem: 11.4 GB 
[10/27 22:46:00 visual_prompt]: 	Training 300/553. train loss: 1.0865,	1.4757 s / batch. (data: 9.82e-01). ETA=16:25:29, max mem: 11.4 GB 
[10/27 22:47:28 visual_prompt]: 	Training 400/553. train loss: 0.6707,	0.4972 s / batch. (data: 1.89e-02). ETA=5:31:11, max mem: 11.4 GB 
[10/27 22:48:55 visual_prompt]: 	Training 500/553. train loss: 0.7380,	0.5120 s / batch. (data: 1.59e-02). ETA=5:40:12, max mem: 11.4 GB 
[10/27 22:49:42 visual_prompt]: Epoch 28 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 0.9441
[10/27 22:50:34 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1902, average loss: 1.0169
[10/27 22:50:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.15	
[10/27 22:50:34 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[10/27 22:52:12 visual_prompt]: 	Training 100/553. train loss: 0.9909,	0.5203 s / batch. (data: 5.41e-03). ETA=5:44:22, max mem: 11.4 GB 
[10/27 22:53:40 visual_prompt]: 	Training 200/553. train loss: 1.2092,	2.3621 s / batch. (data: 1.85e+00). ETA=1 day, 1:59:38, max mem: 11.4 GB 
[10/27 22:55:05 visual_prompt]: 	Training 300/553. train loss: 0.7080,	0.4840 s / batch. (data: 2.48e-04). ETA=5:18:45, max mem: 11.4 GB 
[10/27 22:56:30 visual_prompt]: 	Training 400/553. train loss: 1.0363,	0.4800 s / batch. (data: 3.77e-04). ETA=5:15:20, max mem: 11.4 GB 
[10/27 22:58:00 visual_prompt]: 	Training 500/553. train loss: 0.8368,	0.5043 s / batch. (data: 1.05e-02). ETA=5:30:28, max mem: 11.4 GB 
[10/27 22:58:45 visual_prompt]: Epoch 29 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 0.9993
[10/27 22:59:37 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1906, average loss: 1.1186
[10/27 22:59:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.40	
[10/27 22:59:37 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[10/27 23:01:08 visual_prompt]: 	Training 100/553. train loss: 0.6908,	0.4920 s / batch. (data: 2.69e-04). ETA=5:21:07, max mem: 11.4 GB 
[10/27 23:02:38 visual_prompt]: 	Training 200/553. train loss: 1.2905,	0.5040 s / batch. (data: 2.58e-04). ETA=5:28:08, max mem: 11.4 GB 
[10/27 23:04:07 visual_prompt]: 	Training 300/553. train loss: 0.1866,	0.4990 s / batch. (data: 2.56e-04). ETA=5:24:01, max mem: 11.4 GB 
[10/27 23:05:38 visual_prompt]: 	Training 400/553. train loss: 1.4865,	1.6881 s / batch. (data: 1.20e+00). ETA=18:13:23, max mem: 11.4 GB 
[10/27 23:07:06 visual_prompt]: 	Training 500/553. train loss: 0.8497,	2.0400 s / batch. (data: 1.54e+00). ETA=21:57:55, max mem: 11.4 GB 
[10/27 23:07:53 visual_prompt]: Epoch 30 / 100: avg data time: 4.00e-01, avg batch time: 0.8956, average train loss: 0.8728
[10/27 23:08:46 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1917, average loss: 0.7495
[10/27 23:08:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.55	
[10/27 23:08:46 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[10/27 23:10:19 visual_prompt]: 	Training 100/553. train loss: 0.5445,	0.5160 s / batch. (data: 2.93e-04). ETA=5:32:01, max mem: 11.4 GB 
[10/27 23:11:50 visual_prompt]: 	Training 200/553. train loss: 0.9287,	0.4995 s / batch. (data: 8.59e-03). ETA=5:20:35, max mem: 11.4 GB 
[10/27 23:13:17 visual_prompt]: 	Training 300/553. train loss: 0.9563,	0.4865 s / batch. (data: 7.96e-03). ETA=5:11:25, max mem: 11.4 GB 
[10/27 23:14:45 visual_prompt]: 	Training 400/553. train loss: 0.7191,	0.4968 s / batch. (data: 1.04e-02). ETA=5:17:11, max mem: 11.4 GB 
[10/27 23:16:13 visual_prompt]: 	Training 500/553. train loss: 0.5881,	0.4874 s / batch. (data: 2.62e-04). ETA=5:10:24, max mem: 11.4 GB 
[10/27 23:16:58 visual_prompt]: Epoch 31 / 100: avg data time: 3.94e-01, avg batch time: 0.8896, average train loss: 0.9159
[10/27 23:17:50 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1905, average loss: 0.8700
[10/27 23:17:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.57	
[10/27 23:17:50 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[10/27 23:19:23 visual_prompt]: 	Training 100/553. train loss: 0.5607,	0.4918 s / batch. (data: 7.77e-03). ETA=5:11:56, max mem: 11.4 GB 
[10/27 23:20:51 visual_prompt]: 	Training 200/553. train loss: 0.5467,	0.4852 s / batch. (data: 5.38e-03). ETA=5:06:56, max mem: 11.4 GB 
[10/27 23:22:23 visual_prompt]: 	Training 300/553. train loss: 0.9818,	0.4960 s / batch. (data: 2.52e-04). ETA=5:12:57, max mem: 11.4 GB 
[10/27 23:23:51 visual_prompt]: 	Training 400/553. train loss: 0.7841,	0.5112 s / batch. (data: 1.20e-02). ETA=5:21:39, max mem: 11.4 GB 
[10/27 23:25:16 visual_prompt]: 	Training 500/553. train loss: 0.7532,	0.4920 s / batch. (data: 2.64e-04). ETA=5:08:47, max mem: 11.4 GB 
[10/27 23:26:01 visual_prompt]: Epoch 32 / 100: avg data time: 3.93e-01, avg batch time: 0.8871, average train loss: 0.8340
[10/27 23:26:54 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1906, average loss: 0.6888
[10/27 23:26:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 59.28	
[10/27 23:26:54 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[10/27 23:28:24 visual_prompt]: 	Training 100/553. train loss: 1.2134,	0.5120 s / batch. (data: 2.75e-04). ETA=5:20:01, max mem: 11.4 GB 
[10/27 23:29:55 visual_prompt]: 	Training 200/553. train loss: 1.6819,	2.3880 s / batch. (data: 1.88e+00). ETA=1 day, 0:48:39, max mem: 11.4 GB 
[10/27 23:31:22 visual_prompt]: 	Training 300/553. train loss: 0.5219,	0.5160 s / batch. (data: 2.49e-04). ETA=5:20:49, max mem: 11.4 GB 
[10/27 23:32:51 visual_prompt]: 	Training 400/553. train loss: 1.0561,	0.5022 s / batch. (data: 5.37e-03). ETA=5:11:23, max mem: 11.4 GB 
[10/27 23:34:19 visual_prompt]: 	Training 500/553. train loss: 0.5132,	1.6600 s / batch. (data: 1.17e+00). ETA=17:06:33, max mem: 11.4 GB 
[10/27 23:35:04 visual_prompt]: Epoch 33 / 100: avg data time: 3.92e-01, avg batch time: 0.8873, average train loss: 1.1243
[10/27 23:35:57 visual_prompt]: Inference (val):avg data time: 6.17e-05, avg batch time: 0.1919, average loss: 1.0549
[10/27 23:35:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.40	
[10/27 23:35:57 visual_prompt]: Stopping early.
[10/27 23:35:57 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 23:35:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 23:35:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 23:35:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 23:35:57 visual_prompt]: Training with config:
[10/27 23:35:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 23:35:57 visual_prompt]: Loading training data...
[10/27 23:35:57 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 23:35:57 visual_prompt]: Loading validation data...
[10/27 23:35:57 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 23:35:57 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/27 23:35:59 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/27 23:35:59 visual_prompt]: tuned percent:0.529
[10/27 23:36:00 visual_prompt]: Device used for model: 0
[10/27 23:36:00 visual_prompt]: Setting up Evaluator...
[10/27 23:36:00 visual_prompt]: Setting up Trainer...
[10/27 23:36:00 visual_prompt]: 	Setting up the optimizer...
[10/27 23:36:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 23:37:31 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5237 s / batch. (data: 1.55e-02). ETA=8:01:49, max mem: 11.4 GB 
[10/27 23:38:58 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4920 s / batch. (data: 2.68e-04). ETA=7:31:47, max mem: 11.4 GB 
[10/27 23:40:30 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9834 s / batch. (data: 2.49e+00). ETA=1 day, 21:34:48, max mem: 11.4 GB 
[10/27 23:41:56 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4840 s / batch. (data: 2.77e-04). ETA=7:22:53, max mem: 11.4 GB 
[10/27 23:43:26 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4910 s / batch. (data: 5.41e-03). ETA=7:28:24, max mem: 11.4 GB 
[10/27 23:44:12 visual_prompt]: Epoch 1 / 100: avg data time: 3.96e-01, avg batch time: 0.8909, average train loss: 1.3966
[10/27 23:45:05 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.1924, average loss: 1.3454
[10/27 23:45:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/27 23:45:05 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/27 23:46:36 visual_prompt]: 	Training 100/553. train loss: 0.7893,	0.4989 s / batch. (data: 6.27e-03). ETA=7:34:23, max mem: 11.4 GB 
[10/27 23:48:03 visual_prompt]: 	Training 200/553. train loss: 0.0759,	0.5848 s / batch. (data: 1.05e-01). ETA=8:51:38, max mem: 11.4 GB 
[10/27 23:49:35 visual_prompt]: 	Training 300/553. train loss: 0.7953,	1.7543 s / batch. (data: 1.26e+00). ETA=1 day, 2:31:54, max mem: 11.4 GB 
[10/27 23:51:01 visual_prompt]: 	Training 400/553. train loss: 1.3411,	0.4958 s / batch. (data: 1.05e-02). ETA=7:29:03, max mem: 11.4 GB 
[10/27 23:52:31 visual_prompt]: 	Training 500/553. train loss: 0.5506,	0.4960 s / batch. (data: 2.32e-04). ETA=7:28:27, max mem: 11.4 GB 
[10/27 23:53:16 visual_prompt]: Epoch 2 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 0.8970
[10/27 23:54:08 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1915, average loss: 1.3062
[10/27 23:54:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.40	
[10/27 23:54:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/27 23:55:38 visual_prompt]: 	Training 100/553. train loss: 0.8667,	0.5283 s / batch. (data: 2.43e-02). ETA=7:56:16, max mem: 11.4 GB 
[10/27 23:57:08 visual_prompt]: 	Training 200/553. train loss: 0.9921,	0.5120 s / batch. (data: 2.65e-04). ETA=7:40:44, max mem: 11.4 GB 
[10/27 23:58:35 visual_prompt]: 	Training 300/553. train loss: 0.6105,	0.4960 s / batch. (data: 5.42e-03). ETA=7:25:30, max mem: 11.4 GB 
[10/28 00:00:04 visual_prompt]: 	Training 400/553. train loss: 2.0176,	0.5041 s / batch. (data: 1.20e-02). ETA=7:31:57, max mem: 11.4 GB 
[10/28 00:01:35 visual_prompt]: 	Training 500/553. train loss: 0.9544,	1.7200 s / batch. (data: 1.23e+00). ETA=1 day, 1:39:12, max mem: 11.4 GB 
[10/28 00:02:19 visual_prompt]: Epoch 3 / 100: avg data time: 3.94e-01, avg batch time: 0.8879, average train loss: 0.9171
[10/28 00:03:11 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1916, average loss: 0.7570
[10/28 00:03:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.43	
[10/28 00:03:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/28 00:04:44 visual_prompt]: 	Training 100/553. train loss: 0.8345,	0.4996 s / batch. (data: 7.97e-03). ETA=7:25:49, max mem: 11.4 GB 
[10/28 00:06:13 visual_prompt]: 	Training 200/553. train loss: 0.4429,	0.5125 s / batch. (data: 2.21e-02). ETA=7:36:28, max mem: 11.4 GB 
[10/28 00:07:42 visual_prompt]: 	Training 300/553. train loss: 0.9467,	1.7352 s / batch. (data: 1.25e+00). ETA=1 day, 1:42:37, max mem: 11.4 GB 
[10/28 00:09:07 visual_prompt]: 	Training 400/553. train loss: 1.3058,	1.8067 s / batch. (data: 1.32e+00). ETA=1 day, 2:43:10, max mem: 11.4 GB 
[10/28 00:10:37 visual_prompt]: 	Training 500/553. train loss: 0.2015,	3.9271 s / batch. (data: 3.44e+00). ETA=2 days, 9:58:08, max mem: 11.4 GB 
[10/28 00:11:23 visual_prompt]: Epoch 4 / 100: avg data time: 3.94e-01, avg batch time: 0.8880, average train loss: 1.0101
[10/28 00:12:15 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1906, average loss: 1.8125
[10/28 00:12:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.00	
[10/28 00:12:15 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/28 00:13:46 visual_prompt]: 	Training 100/553. train loss: 2.8067,	0.4998 s / batch. (data: 2.73e-04). ETA=7:21:22, max mem: 11.4 GB 
[10/28 00:15:14 visual_prompt]: 	Training 200/553. train loss: 0.9007,	1.8133 s / batch. (data: 1.30e+00). ETA=1 day, 2:38:19, max mem: 11.4 GB 
[10/28 00:16:43 visual_prompt]: 	Training 300/553. train loss: 1.8285,	0.5039 s / batch. (data: 5.38e-03). ETA=7:23:21, max mem: 11.4 GB 
[10/28 00:18:11 visual_prompt]: 	Training 400/553. train loss: 2.1104,	0.5199 s / batch. (data: 1.18e-02). ETA=7:36:29, max mem: 11.4 GB 
[10/28 00:19:40 visual_prompt]: 	Training 500/553. train loss: 0.5942,	0.5169 s / batch. (data: 2.67e-04). ETA=7:33:04, max mem: 11.4 GB 
[10/28 00:20:26 visual_prompt]: Epoch 5 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 1.0224
[10/28 00:21:19 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1896, average loss: 2.4722
[10/28 00:21:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.29	
[10/28 00:21:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/28 00:22:52 visual_prompt]: 	Training 100/553. train loss: 0.8663,	0.5068 s / batch. (data: 2.79e-04). ETA=7:22:54, max mem: 11.4 GB 
[10/28 00:24:19 visual_prompt]: 	Training 200/553. train loss: 2.9200,	0.5160 s / batch. (data: 5.39e-03). ETA=7:30:04, max mem: 11.4 GB 
[10/28 00:25:46 visual_prompt]: 	Training 300/553. train loss: 0.5873,	0.5290 s / batch. (data: 2.10e-02). ETA=7:40:30, max mem: 11.4 GB 
[10/28 00:27:18 visual_prompt]: 	Training 400/553. train loss: 0.7060,	0.6041 s / batch. (data: 1.26e-01). ETA=8:44:56, max mem: 11.4 GB 
[10/28 00:28:46 visual_prompt]: 	Training 500/553. train loss: 2.2144,	1.3390 s / batch. (data: 8.29e-01). ETA=19:21:16, max mem: 11.4 GB 
[10/28 00:29:30 visual_prompt]: Epoch 6 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 1.2073
[10/28 00:30:22 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1908, average loss: 1.3335
[10/28 00:30:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.51	
[10/28 00:30:22 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/28 00:31:52 visual_prompt]: 	Training 100/553. train loss: 2.4926,	0.4960 s / batch. (data: 7.96e-03). ETA=7:08:51, max mem: 11.4 GB 
[10/28 00:33:21 visual_prompt]: 	Training 200/553. train loss: 0.6071,	0.4844 s / batch. (data: 2.67e-04). ETA=6:58:01, max mem: 11.4 GB 
[10/28 00:34:53 visual_prompt]: 	Training 300/553. train loss: 0.7064,	2.5600 s / batch. (data: 2.06e+00). ETA=1 day, 12:45:05, max mem: 11.4 GB 
[10/28 00:36:22 visual_prompt]: 	Training 400/553. train loss: 0.5388,	2.4000 s / batch. (data: 1.91e+00). ETA=1 day, 10:23:14, max mem: 11.4 GB 
[10/28 00:37:48 visual_prompt]: 	Training 500/553. train loss: 1.3886,	0.4914 s / batch. (data: 2.97e-04). ETA=7:01:36, max mem: 11.4 GB 
[10/28 00:38:33 visual_prompt]: Epoch 7 / 100: avg data time: 3.91e-01, avg batch time: 0.8866, average train loss: 1.0805
[10/28 00:39:25 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1904, average loss: 0.8162
[10/28 00:39:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.47	
[10/28 00:39:25 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/28 00:40:54 visual_prompt]: 	Training 100/553. train loss: 1.0991,	0.5320 s / batch. (data: 3.64e-02). ETA=7:35:06, max mem: 11.4 GB 
[10/28 00:42:25 visual_prompt]: 	Training 200/553. train loss: 0.4491,	0.4960 s / batch. (data: 2.80e-04). ETA=7:03:29, max mem: 11.4 GB 
[10/28 00:43:54 visual_prompt]: 	Training 300/553. train loss: 2.3193,	0.5039 s / batch. (data: 1.17e-02). ETA=7:09:24, max mem: 11.4 GB 
[10/28 00:45:23 visual_prompt]: 	Training 400/553. train loss: 0.7523,	0.5080 s / batch. (data: 2.72e-04). ETA=7:12:01, max mem: 11.4 GB 
[10/28 00:46:52 visual_prompt]: 	Training 500/553. train loss: 2.3257,	2.1316 s / batch. (data: 1.65e+00). ETA=1 day, 6:09:21, max mem: 11.4 GB 
[10/28 00:47:37 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8892, average train loss: 1.2679
[10/28 00:48:29 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1936, average loss: 1.1915
[10/28 00:48:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.48	
[10/28 00:48:29 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/28 00:50:01 visual_prompt]: 	Training 100/553. train loss: 0.0551,	0.5041 s / batch. (data: 2.75e-04). ETA=7:06:34, max mem: 11.4 GB 
[10/28 00:51:29 visual_prompt]: 	Training 200/553. train loss: 0.8284,	0.4961 s / batch. (data: 2.48e-04). ETA=6:59:01, max mem: 11.4 GB 
[10/28 00:52:58 visual_prompt]: 	Training 300/553. train loss: 0.6215,	1.9108 s / batch. (data: 1.42e+00). ETA=1 day, 2:50:40, max mem: 11.4 GB 
[10/28 00:54:27 visual_prompt]: 	Training 400/553. train loss: 1.1293,	0.4880 s / batch. (data: 2.94e-04). ETA=6:50:32, max mem: 11.4 GB 
[10/28 00:55:56 visual_prompt]: 	Training 500/553. train loss: 1.3188,	1.3360 s / batch. (data: 8.57e-01). ETA=18:41:43, max mem: 11.4 GB 
[10/28 00:56:40 visual_prompt]: Epoch 9 / 100: avg data time: 3.94e-01, avg batch time: 0.8882, average train loss: 1.0145
[10/28 00:57:33 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1903, average loss: 2.1592
[10/28 00:57:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.03	
[10/28 00:57:33 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/28 00:59:07 visual_prompt]: 	Training 100/553. train loss: 2.3195,	0.5160 s / batch. (data: 3.36e-03). ETA=7:11:54, max mem: 11.4 GB 
[10/28 01:00:34 visual_prompt]: 	Training 200/553. train loss: 0.7495,	0.5120 s / batch. (data: 2.72e-04). ETA=7:07:42, max mem: 11.4 GB 
[10/28 01:02:01 visual_prompt]: 	Training 300/553. train loss: 0.6199,	0.4853 s / batch. (data: 7.96e-03). ETA=6:44:38, max mem: 11.4 GB 
[10/28 01:03:28 visual_prompt]: 	Training 400/553. train loss: 0.8609,	0.4884 s / batch. (data: 3.15e-04). ETA=6:46:23, max mem: 11.4 GB 
[10/28 01:04:58 visual_prompt]: 	Training 500/553. train loss: 0.5662,	0.4889 s / batch. (data: 1.08e-02). ETA=6:46:00, max mem: 11.4 GB 
[10/28 01:05:44 visual_prompt]: Epoch 10 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 1.3395
[10/28 01:06:36 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1925, average loss: 0.7091
[10/28 01:06:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.83	
[10/28 01:06:36 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/28 01:08:09 visual_prompt]: 	Training 100/553. train loss: 2.7740,	0.4988 s / batch. (data: 2.31e-04). ETA=6:52:55, max mem: 11.4 GB 
[10/28 01:09:40 visual_prompt]: 	Training 200/553. train loss: 1.4885,	0.5240 s / batch. (data: 2.31e-04). ETA=7:12:55, max mem: 11.4 GB 
[10/28 01:11:07 visual_prompt]: 	Training 300/553. train loss: 0.0198,	1.4834 s / batch. (data: 9.62e-01). ETA=20:23:01, max mem: 11.4 GB 
[10/28 01:12:35 visual_prompt]: 	Training 400/553. train loss: 0.6785,	0.4960 s / batch. (data: 2.57e-04). ETA=6:48:07, max mem: 11.4 GB 
[10/28 01:14:02 visual_prompt]: 	Training 500/553. train loss: 0.8898,	0.4962 s / batch. (data: 5.38e-03). ETA=6:47:29, max mem: 11.4 GB 
[10/28 01:14:47 visual_prompt]: Epoch 11 / 100: avg data time: 3.94e-01, avg batch time: 0.8876, average train loss: 1.0548
[10/28 01:15:39 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1911, average loss: 1.2068
[10/28 01:15:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.56	
[10/28 01:15:39 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/28 01:17:12 visual_prompt]: 	Training 100/553. train loss: 1.0895,	0.5010 s / batch. (data: 2.66e-04). ETA=6:50:07, max mem: 11.4 GB 
[10/28 01:18:41 visual_prompt]: 	Training 200/553. train loss: 0.6846,	0.5235 s / batch. (data: 2.01e-02). ETA=7:07:40, max mem: 11.4 GB 
[10/28 01:20:09 visual_prompt]: 	Training 300/553. train loss: 0.5001,	0.4960 s / batch. (data: 2.67e-04). ETA=6:44:24, max mem: 11.4 GB 
[10/28 01:21:37 visual_prompt]: 	Training 400/553. train loss: 1.0799,	0.4875 s / batch. (data: 7.98e-03). ETA=6:36:40, max mem: 11.4 GB 
[10/28 01:23:06 visual_prompt]: 	Training 500/553. train loss: 5.8215,	0.5040 s / batch. (data: 2.53e-04). ETA=6:49:12, max mem: 11.4 GB 
[10/28 01:23:50 visual_prompt]: Epoch 12 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 1.0995
[10/28 01:24:43 visual_prompt]: Inference (val):avg data time: 4.32e-04, avg batch time: 0.1918, average loss: 2.4043
[10/28 01:24:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.81	
[10/28 01:24:43 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/28 01:26:16 visual_prompt]: 	Training 100/553. train loss: 0.4596,	0.4880 s / batch. (data: 2.68e-04). ETA=6:34:57, max mem: 11.4 GB 
[10/28 01:27:41 visual_prompt]: 	Training 200/553. train loss: 0.6863,	0.5160 s / batch. (data: 2.60e-04). ETA=6:56:47, max mem: 11.4 GB 
[10/28 01:29:11 visual_prompt]: 	Training 300/553. train loss: 0.6555,	1.9380 s / batch. (data: 1.44e+00). ETA=1 day, 2:02:08, max mem: 11.4 GB 
[10/28 01:30:38 visual_prompt]: 	Training 400/553. train loss: 4.1979,	0.9809 s / batch. (data: 5.02e-01). ETA=13:09:02, max mem: 11.4 GB 
[10/28 01:32:08 visual_prompt]: 	Training 500/553. train loss: 1.1742,	0.5120 s / batch. (data: 1.20e-02). ETA=6:50:59, max mem: 11.4 GB 
[10/28 01:32:53 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8873, average train loss: 1.2034
[10/28 01:33:45 visual_prompt]: Inference (val):avg data time: 1.25e-04, avg batch time: 0.1919, average loss: 0.6660
[10/28 01:33:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 59.45	
[10/28 01:33:45 visual_prompt]: Best epoch 13: best metric: -0.666
[10/28 01:33:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/28 01:35:18 visual_prompt]: 	Training 100/553. train loss: 0.8327,	0.4879 s / batch. (data: 2.70e-04). ETA=6:30:26, max mem: 11.4 GB 
[10/28 01:36:47 visual_prompt]: 	Training 200/553. train loss: 0.1063,	1.7640 s / batch. (data: 1.28e+00). ETA=23:28:34, max mem: 11.4 GB 
[10/28 01:38:16 visual_prompt]: 	Training 300/553. train loss: 0.4367,	1.5000 s / batch. (data: 9.73e-01). ETA=19:55:15, max mem: 11.4 GB 
[10/28 01:39:43 visual_prompt]: 	Training 400/553. train loss: 0.5628,	0.9280 s / batch. (data: 4.34e-01). ETA=12:17:54, max mem: 11.4 GB 
[10/28 01:41:12 visual_prompt]: 	Training 500/553. train loss: 2.7653,	0.4940 s / batch. (data: 2.78e-04). ETA=6:32:02, max mem: 11.4 GB 
[10/28 01:41:55 visual_prompt]: Epoch 14 / 100: avg data time: 3.91e-01, avg batch time: 0.8857, average train loss: 1.0208
[10/28 01:42:48 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1902, average loss: 0.7605
[10/28 01:42:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 59.39	
[10/28 01:42:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/28 01:44:19 visual_prompt]: 	Training 100/553. train loss: 1.0625,	1.0416 s / batch. (data: 5.52e-01). ETA=13:43:51, max mem: 11.4 GB 
[10/28 01:45:46 visual_prompt]: 	Training 200/553. train loss: 0.6030,	0.5009 s / batch. (data: 5.39e-03). ETA=6:35:22, max mem: 11.4 GB 
[10/28 01:47:16 visual_prompt]: 	Training 300/553. train loss: 1.1247,	0.4994 s / batch. (data: 7.99e-03). ETA=6:33:21, max mem: 11.4 GB 
[10/28 01:48:42 visual_prompt]: 	Training 400/553. train loss: 0.3269,	0.5033 s / batch. (data: 1.60e-02). ETA=6:35:36, max mem: 11.4 GB 
[10/28 01:50:12 visual_prompt]: 	Training 500/553. train loss: 0.4201,	0.4881 s / batch. (data: 5.38e-03). ETA=6:22:48, max mem: 11.4 GB 
[10/28 01:50:59 visual_prompt]: Epoch 15 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 1.2399
[10/28 01:51:51 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1897, average loss: 2.2545
[10/28 01:51:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.37	
[10/28 01:51:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/28 01:53:22 visual_prompt]: 	Training 100/553. train loss: 0.6895,	0.5280 s / batch. (data: 1.20e-02). ETA=6:52:46, max mem: 11.4 GB 
[10/28 01:54:49 visual_prompt]: 	Training 200/553. train loss: 1.6739,	0.5240 s / batch. (data: 2.67e-04). ETA=6:48:46, max mem: 11.4 GB 
[10/28 01:56:20 visual_prompt]: 	Training 300/553. train loss: 1.5656,	0.4800 s / batch. (data: 2.81e-04). ETA=6:13:38, max mem: 11.4 GB 
[10/28 01:57:48 visual_prompt]: 	Training 400/553. train loss: 1.5345,	0.5133 s / batch. (data: 1.10e-02). ETA=6:38:43, max mem: 11.4 GB 
[10/28 01:59:16 visual_prompt]: 	Training 500/553. train loss: 0.9971,	2.1200 s / batch. (data: 1.64e+00). ETA=1 day, 3:23:11, max mem: 11.4 GB 
[10/28 02:00:02 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 0.8878, average train loss: 1.1329
[10/28 02:00:54 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1899, average loss: 0.6930
[10/28 02:00:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 55.67	
[10/28 02:00:54 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/28 02:02:25 visual_prompt]: 	Training 100/553. train loss: 0.4637,	0.4960 s / batch. (data: 7.97e-03). ETA=6:23:09, max mem: 11.4 GB 
[10/28 02:03:55 visual_prompt]: 	Training 200/553. train loss: 2.2197,	0.4847 s / batch. (data: 5.16e-04). ETA=6:13:38, max mem: 11.4 GB 
[10/28 02:05:23 visual_prompt]: 	Training 300/553. train loss: 1.2539,	0.5080 s / batch. (data: 2.57e-04). ETA=6:30:46, max mem: 11.4 GB 
[10/28 02:06:51 visual_prompt]: 	Training 400/553. train loss: 0.6579,	1.7800 s / batch. (data: 1.28e+00). ETA=22:46:11, max mem: 11.4 GB 
[10/28 02:08:19 visual_prompt]: 	Training 500/553. train loss: 1.2283,	2.1622 s / batch. (data: 1.68e+00). ETA=1 day, 3:35:55, max mem: 11.4 GB 
[10/28 02:09:05 visual_prompt]: Epoch 17 / 100: avg data time: 3.93e-01, avg batch time: 0.8875, average train loss: 1.0687
[10/28 02:09:57 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1909, average loss: 0.6693
[10/28 02:09:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 57.46	
[10/28 02:09:57 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/28 02:11:30 visual_prompt]: 	Training 100/553. train loss: 0.8292,	0.4920 s / batch. (data: 2.68e-04). ETA=6:15:33, max mem: 11.4 GB 
[10/28 02:13:00 visual_prompt]: 	Training 200/553. train loss: 0.6808,	0.5073 s / batch. (data: 2.68e-04). ETA=6:26:22, max mem: 11.4 GB 
[10/28 02:14:28 visual_prompt]: 	Training 300/553. train loss: 0.4821,	0.4886 s / batch. (data: 2.64e-04). ETA=6:11:19, max mem: 11.4 GB 
[10/28 02:15:56 visual_prompt]: 	Training 400/553. train loss: 0.6041,	0.4844 s / batch. (data: 2.60e-04). ETA=6:07:21, max mem: 11.4 GB 
[10/28 02:17:24 visual_prompt]: 	Training 500/553. train loss: 1.5189,	0.5400 s / batch. (data: 4.17e-02). ETA=6:48:35, max mem: 11.4 GB 
[10/28 02:18:08 visual_prompt]: Epoch 18 / 100: avg data time: 3.93e-01, avg batch time: 0.8878, average train loss: 1.1392
[10/28 02:19:01 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1915, average loss: 0.8058
[10/28 02:19:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 57.90	
[10/28 02:19:01 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/28 02:20:32 visual_prompt]: 	Training 100/553. train loss: 0.8085,	0.5130 s / batch. (data: 2.06e-02). ETA=6:26:52, max mem: 11.4 GB 
[10/28 02:22:00 visual_prompt]: 	Training 200/553. train loss: 0.7833,	0.5096 s / batch. (data: 9.52e-03). ETA=6:23:24, max mem: 11.4 GB 
[10/28 02:23:30 visual_prompt]: 	Training 300/553. train loss: 3.5966,	0.5142 s / batch. (data: 2.76e-04). ETA=6:26:03, max mem: 11.4 GB 
[10/28 02:24:59 visual_prompt]: 	Training 400/553. train loss: 0.3760,	0.5017 s / batch. (data: 5.54e-03). ETA=6:15:47, max mem: 11.4 GB 
[10/28 02:26:23 visual_prompt]: 	Training 500/553. train loss: 0.8367,	0.4880 s / batch. (data: 2.89e-04). ETA=6:04:42, max mem: 11.4 GB 
[10/28 02:27:10 visual_prompt]: Epoch 19 / 100: avg data time: 3.92e-01, avg batch time: 0.8854, average train loss: 0.9344
[10/28 02:28:03 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1923, average loss: 1.2951
[10/28 02:28:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.20	
[10/28 02:28:03 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[10/28 02:29:32 visual_prompt]: 	Training 100/553. train loss: 0.8940,	0.5006 s / batch. (data: 1.20e-02). ETA=6:12:53, max mem: 11.4 GB 
[10/28 02:31:02 visual_prompt]: 	Training 200/553. train loss: 0.3128,	0.5084 s / batch. (data: 2.40e-02). ETA=6:17:51, max mem: 11.4 GB 
[10/28 02:32:31 visual_prompt]: 	Training 300/553. train loss: 1.4720,	0.5011 s / batch. (data: 5.49e-03). ETA=6:11:37, max mem: 11.4 GB 
[10/28 02:33:59 visual_prompt]: 	Training 400/553. train loss: 0.5699,	0.4994 s / batch. (data: 7.95e-03). ETA=6:09:29, max mem: 11.4 GB 
[10/28 02:35:27 visual_prompt]: 	Training 500/553. train loss: 0.6871,	0.4926 s / batch. (data: 1.17e-02). ETA=6:03:39, max mem: 11.4 GB 
[10/28 02:36:14 visual_prompt]: Epoch 20 / 100: avg data time: 3.93e-01, avg batch time: 0.8882, average train loss: 1.0047
[10/28 02:37:06 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1907, average loss: 0.9354
[10/28 02:37:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.67	
[10/28 02:37:06 visual_prompt]: Stopping early.
[10/28 02:37:06 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 02:37:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 02:37:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 02:37:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 02:37:06 visual_prompt]: Training with config:
[10/28 02:37:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.25_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 02:37:06 visual_prompt]: Loading training data...
[10/28 02:37:06 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 02:37:06 visual_prompt]: Loading validation data...
[10/28 02:37:06 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 02:37:06 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/28 02:37:09 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/28 02:37:09 visual_prompt]: tuned percent:0.529
[10/28 02:37:09 visual_prompt]: Device used for model: 0
[10/28 02:37:09 visual_prompt]: Setting up Evaluator...
[10/28 02:37:09 visual_prompt]: Setting up Trainer...
[10/28 02:37:09 visual_prompt]: 	Setting up the optimizer...
[10/28 02:37:09 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 02:38:40 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5118 s / batch. (data: 7.95e-03). ETA=7:50:51, max mem: 11.4 GB 
[10/28 02:40:07 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4959 s / batch. (data: 5.38e-03). ETA=7:35:23, max mem: 11.4 GB 
[10/28 02:41:39 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9401 s / batch. (data: 2.45e+00). ETA=1 day, 20:55:03, max mem: 11.4 GB 
[10/28 02:43:05 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4806 s / batch. (data: 2.48e-04). ETA=7:19:43, max mem: 11.4 GB 
[10/28 02:44:35 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5280 s / batch. (data: 2.70e-04). ETA=8:02:12, max mem: 11.4 GB 
[10/28 02:45:21 visual_prompt]: Epoch 1 / 100: avg data time: 3.96e-01, avg batch time: 0.8907, average train loss: 1.3966
[10/28 02:46:14 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1912, average loss: 1.3454
[10/28 02:46:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/28 02:46:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/28 02:47:46 visual_prompt]: 	Training 100/553. train loss: 0.6658,	1.1486 s / batch. (data: 6.50e-01). ETA=17:26:09, max mem: 11.4 GB 
[10/28 02:49:13 visual_prompt]: 	Training 200/553. train loss: 0.2069,	0.5080 s / batch. (data: 2.71e-04). ETA=7:41:48, max mem: 11.4 GB 
[10/28 02:50:44 visual_prompt]: 	Training 300/553. train loss: 0.8691,	1.6827 s / batch. (data: 1.19e+00). ETA=1 day, 1:26:56, max mem: 11.4 GB 
[10/28 02:52:11 visual_prompt]: 	Training 400/553. train loss: 0.9633,	0.5040 s / batch. (data: 2.76e-04). ETA=7:36:30, max mem: 11.4 GB 
[10/28 02:53:41 visual_prompt]: 	Training 500/553. train loss: 0.6851,	0.5160 s / batch. (data: 2.73e-04). ETA=7:46:31, max mem: 11.4 GB 
[10/28 02:54:26 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 0.8902, average train loss: 0.8083
[10/28 02:55:19 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1917, average loss: 0.7331
[10/28 02:55:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.00	
[10/28 02:55:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/28 02:56:49 visual_prompt]: 	Training 100/553. train loss: 0.7502,	0.5120 s / batch. (data: 2.83e-04). ETA=7:41:36, max mem: 11.4 GB 
[10/28 02:58:20 visual_prompt]: 	Training 200/553. train loss: 0.7131,	0.5004 s / batch. (data: 2.85e-04). ETA=7:30:16, max mem: 11.4 GB 
[10/28 02:59:46 visual_prompt]: 	Training 300/553. train loss: 0.5859,	0.4911 s / batch. (data: 2.75e-04). ETA=7:21:09, max mem: 11.4 GB 
[10/28 03:01:16 visual_prompt]: 	Training 400/553. train loss: 0.5677,	0.5040 s / batch. (data: 2.70e-04). ETA=7:31:52, max mem: 11.4 GB 
[10/28 03:02:46 visual_prompt]: 	Training 500/553. train loss: 0.6983,	1.7360 s / batch. (data: 1.23e+00). ETA=1 day, 1:53:35, max mem: 11.4 GB 
[10/28 03:03:30 visual_prompt]: Epoch 3 / 100: avg data time: 3.92e-01, avg batch time: 0.8878, average train loss: 0.7368
[10/28 03:04:23 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.1930, average loss: 0.7477
[10/28 03:04:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.40	
[10/28 03:04:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/28 03:05:56 visual_prompt]: 	Training 100/553. train loss: 0.6899,	0.4928 s / batch. (data: 1.23e-02). ETA=7:19:44, max mem: 11.4 GB 
[10/28 03:07:25 visual_prompt]: 	Training 200/553. train loss: 0.7325,	0.4920 s / batch. (data: 2.95e-04). ETA=7:18:10, max mem: 11.4 GB 
[10/28 03:08:54 visual_prompt]: 	Training 300/553. train loss: 0.5732,	1.9758 s / batch. (data: 1.50e+00). ETA=1 day, 5:16:29, max mem: 11.4 GB 
[10/28 03:10:18 visual_prompt]: 	Training 400/553. train loss: 0.7320,	1.1473 s / batch. (data: 6.54e-01). ETA=16:58:05, max mem: 11.4 GB 
[10/28 03:11:49 visual_prompt]: 	Training 500/553. train loss: 0.6971,	3.7998 s / batch. (data: 3.32e+00). ETA=2 days, 8:05:24, max mem: 11.4 GB 
[10/28 03:12:34 visual_prompt]: Epoch 4 / 100: avg data time: 3.93e-01, avg batch time: 0.8880, average train loss: 0.7772
[10/28 03:13:26 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1905, average loss: 0.7470
[10/28 03:13:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.31	
[10/28 03:13:26 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/28 03:14:56 visual_prompt]: 	Training 100/553. train loss: 0.5019,	0.4960 s / batch. (data: 2.65e-04). ETA=7:18:00, max mem: 11.4 GB 
[10/28 03:16:25 visual_prompt]: 	Training 200/553. train loss: 0.5778,	1.0399 s / batch. (data: 5.03e-01). ETA=15:16:40, max mem: 11.4 GB 
[10/28 03:17:55 visual_prompt]: 	Training 300/553. train loss: 1.6900,	0.5057 s / batch. (data: 2.45e-04). ETA=7:24:57, max mem: 11.4 GB 
[10/28 03:19:22 visual_prompt]: 	Training 400/553. train loss: 1.1074,	0.4926 s / batch. (data: 7.94e-03). ETA=7:12:32, max mem: 11.4 GB 
[10/28 03:20:51 visual_prompt]: 	Training 500/553. train loss: 0.5587,	0.4787 s / batch. (data: 2.62e-04). ETA=6:59:36, max mem: 11.4 GB 
[10/28 03:21:37 visual_prompt]: Epoch 5 / 100: avg data time: 3.93e-01, avg batch time: 0.8878, average train loss: 0.8480
[10/28 03:22:30 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1921, average loss: 1.0544
[10/28 03:22:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.63	
[10/28 03:22:30 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/28 03:24:03 visual_prompt]: 	Training 100/553. train loss: 0.7043,	0.5000 s / batch. (data: 2.53e-04). ETA=7:16:57, max mem: 11.4 GB 
[10/28 03:25:31 visual_prompt]: 	Training 200/553. train loss: 0.5682,	0.4793 s / batch. (data: 2.63e-04). ETA=6:58:03, max mem: 11.4 GB 
[10/28 03:26:58 visual_prompt]: 	Training 300/553. train loss: 0.5540,	0.4846 s / batch. (data: 2.66e-04). ETA=7:01:52, max mem: 11.4 GB 
[10/28 03:28:29 visual_prompt]: 	Training 400/553. train loss: 0.6310,	0.6200 s / batch. (data: 1.18e-01). ETA=8:58:43, max mem: 11.4 GB 
[10/28 03:29:57 visual_prompt]: 	Training 500/553. train loss: 0.9908,	1.3880 s / batch. (data: 9.01e-01). ETA=20:03:44, max mem: 11.4 GB 
[10/28 03:30:41 visual_prompt]: Epoch 6 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.8062
[10/28 03:31:34 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1914, average loss: 0.7106
[10/28 03:31:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.05	
[10/28 03:31:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/28 03:33:04 visual_prompt]: 	Training 100/553. train loss: 0.8648,	0.5090 s / batch. (data: 5.39e-03). ETA=7:20:08, max mem: 11.4 GB 
[10/28 03:34:32 visual_prompt]: 	Training 200/553. train loss: 0.6128,	0.5080 s / batch. (data: 1.19e-02). ETA=7:18:24, max mem: 11.4 GB 
[10/28 03:36:05 visual_prompt]: 	Training 300/553. train loss: 0.6199,	2.6040 s / batch. (data: 2.12e+00). ETA=1 day, 13:23:00, max mem: 11.4 GB 
[10/28 03:37:33 visual_prompt]: 	Training 400/553. train loss: 0.5655,	2.3839 s / batch. (data: 1.88e+00). ETA=1 day, 10:09:28, max mem: 11.4 GB 
[10/28 03:39:00 visual_prompt]: 	Training 500/553. train loss: 1.3544,	0.4924 s / batch. (data: 7.97e-03). ETA=7:02:28, max mem: 11.4 GB 
[10/28 03:39:44 visual_prompt]: Epoch 7 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 0.8443
[10/28 03:40:36 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1929, average loss: 0.7075
[10/28 03:40:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.63	
[10/28 03:40:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/28 03:42:06 visual_prompt]: 	Training 100/553. train loss: 0.7288,	0.5000 s / batch. (data: 2.86e-04). ETA=7:07:46, max mem: 11.4 GB 
[10/28 03:43:36 visual_prompt]: 	Training 200/553. train loss: 0.8533,	0.4960 s / batch. (data: 3.03e-04). ETA=7:03:28, max mem: 11.4 GB 
[10/28 03:45:06 visual_prompt]: 	Training 300/553. train loss: 0.9143,	0.5164 s / batch. (data: 2.80e-04). ETA=7:20:01, max mem: 11.4 GB 
[10/28 03:46:34 visual_prompt]: 	Training 400/553. train loss: 0.7062,	0.8682 s / batch. (data: 3.81e-01). ETA=12:18:24, max mem: 11.4 GB 
[10/28 03:48:04 visual_prompt]: 	Training 500/553. train loss: 1.9430,	2.0240 s / batch. (data: 1.51e+00). ETA=1 day, 4:37:59, max mem: 11.4 GB 
[10/28 03:48:48 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8890, average train loss: 0.8843
[10/28 03:49:41 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1929, average loss: 0.6926
[10/28 03:49:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 55.76	
[10/28 03:49:41 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/28 03:51:13 visual_prompt]: 	Training 100/553. train loss: 0.2463,	0.4959 s / batch. (data: 5.39e-03). ETA=6:59:38, max mem: 11.4 GB 
[10/28 03:52:40 visual_prompt]: 	Training 200/553. train loss: 0.6630,	0.5201 s / batch. (data: 5.45e-03). ETA=7:19:19, max mem: 11.4 GB 
[10/28 03:54:08 visual_prompt]: 	Training 300/553. train loss: 0.6055,	2.3400 s / batch. (data: 1.86e+00). ETA=1 day, 8:52:27, max mem: 11.4 GB 
[10/28 03:55:38 visual_prompt]: 	Training 400/553. train loss: 0.6301,	0.4787 s / batch. (data: 2.68e-04). ETA=6:42:45, max mem: 11.4 GB 
[10/28 03:57:07 visual_prompt]: 	Training 500/553. train loss: 0.7500,	0.6373 s / batch. (data: 8.22e-02). ETA=8:55:03, max mem: 11.4 GB 
[10/28 03:57:51 visual_prompt]: Epoch 9 / 100: avg data time: 3.92e-01, avg batch time: 0.8863, average train loss: 0.8664
[10/28 03:58:43 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1918, average loss: 0.7374
[10/28 03:58:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.10	
[10/28 03:58:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/28 04:00:17 visual_prompt]: 	Training 100/553. train loss: 1.4348,	0.5077 s / batch. (data: 1.55e-02). ETA=7:05:00, max mem: 11.4 GB 
[10/28 04:01:44 visual_prompt]: 	Training 200/553. train loss: 0.7761,	0.5146 s / batch. (data: 1.06e-02). ETA=7:09:55, max mem: 11.4 GB 
[10/28 04:03:12 visual_prompt]: 	Training 300/553. train loss: 0.6619,	0.4844 s / batch. (data: 2.71e-04). ETA=6:43:52, max mem: 11.4 GB 
[10/28 04:04:39 visual_prompt]: 	Training 400/553. train loss: 0.7563,	1.2040 s / batch. (data: 7.08e-01). ETA=16:41:47, max mem: 11.4 GB 
[10/28 04:06:08 visual_prompt]: 	Training 500/553. train loss: 0.5860,	0.7360 s / batch. (data: 2.29e-01). ETA=10:11:08, max mem: 11.4 GB 
[10/28 04:06:54 visual_prompt]: Epoch 10 / 100: avg data time: 3.92e-01, avg batch time: 0.8868, average train loss: 0.9717
[10/28 04:07:46 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1911, average loss: 0.6984
[10/28 04:07:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.79	
[10/28 04:07:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/28 04:09:19 visual_prompt]: 	Training 100/553. train loss: 0.8577,	0.5040 s / batch. (data: 2.70e-04). ETA=6:57:14, max mem: 11.4 GB 
[10/28 04:10:49 visual_prompt]: 	Training 200/553. train loss: 1.4664,	0.5116 s / batch. (data: 7.96e-03). ETA=7:02:38, max mem: 11.4 GB 
[10/28 04:12:17 visual_prompt]: 	Training 300/553. train loss: 0.1111,	1.9929 s / batch. (data: 1.48e+00). ETA=1 day, 3:23:07, max mem: 11.4 GB 
[10/28 04:13:44 visual_prompt]: 	Training 400/553. train loss: 0.6017,	0.5236 s / batch. (data: 5.42e-03). ETA=7:10:49, max mem: 11.4 GB 
[10/28 04:15:12 visual_prompt]: 	Training 500/553. train loss: 0.8266,	0.5085 s / batch. (data: 5.38e-03). ETA=6:57:34, max mem: 11.4 GB 
[10/28 04:15:56 visual_prompt]: Epoch 11 / 100: avg data time: 3.91e-01, avg batch time: 0.8858, average train loss: 0.9006
[10/28 04:16:48 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1926, average loss: 0.8107
[10/28 04:16:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.56	
[10/28 04:16:48 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/28 04:18:22 visual_prompt]: 	Training 100/553. train loss: 1.1803,	0.4904 s / batch. (data: 5.42e-03). ETA=6:41:25, max mem: 11.4 GB 
[10/28 04:19:51 visual_prompt]: 	Training 200/553. train loss: 0.7578,	1.9044 s / batch. (data: 1.43e+00). ETA=1 day, 1:55:49, max mem: 11.4 GB 
[10/28 04:21:18 visual_prompt]: 	Training 300/553. train loss: 0.5742,	0.4999 s / batch. (data: 4.04e-04). ETA=6:47:35, max mem: 11.4 GB 
[10/28 04:22:46 visual_prompt]: 	Training 400/553. train loss: 0.8245,	0.5201 s / batch. (data: 2.40e-02). ETA=7:03:09, max mem: 11.4 GB 
[10/28 04:24:16 visual_prompt]: 	Training 500/553. train loss: 2.7402,	0.5008 s / batch. (data: 6.96e-04). ETA=6:46:39, max mem: 11.4 GB 
[10/28 04:25:00 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 0.9434
[10/28 04:25:52 visual_prompt]: Inference (val):avg data time: 3.93e-04, avg batch time: 0.1896, average loss: 2.9176
[10/28 04:25:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.83	
[10/28 04:25:52 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/28 04:27:25 visual_prompt]: 	Training 100/553. train loss: 0.5740,	0.6004 s / batch. (data: 1.01e-01). ETA=8:05:57, max mem: 11.4 GB 
[10/28 04:28:51 visual_prompt]: 	Training 200/553. train loss: 0.6916,	0.5080 s / batch. (data: 2.84e-04). ETA=6:50:20, max mem: 11.4 GB 
[10/28 04:30:21 visual_prompt]: 	Training 300/553. train loss: 0.6098,	2.2640 s / batch. (data: 1.76e+00). ETA=1 day, 6:24:56, max mem: 11.4 GB 
[10/28 04:31:47 visual_prompt]: 	Training 400/553. train loss: 4.9768,	0.5159 s / batch. (data: 2.91e-04). ETA=6:54:58, max mem: 11.4 GB 
[10/28 04:33:17 visual_prompt]: 	Training 500/553. train loss: 0.8637,	0.5162 s / batch. (data: 1.63e-02). ETA=6:54:22, max mem: 11.4 GB 
[10/28 04:34:02 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 1.1931
[10/28 04:34:55 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1902, average loss: 0.8260
[10/28 04:34:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.15	
[10/28 04:34:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/28 04:36:27 visual_prompt]: 	Training 100/553. train loss: 1.1490,	0.4922 s / batch. (data: 2.51e-04). ETA=6:33:49, max mem: 11.4 GB 
[10/28 04:37:56 visual_prompt]: 	Training 200/553. train loss: 0.0983,	1.4111 s / batch. (data: 9.22e-01). ETA=18:46:47, max mem: 11.4 GB 
[10/28 04:39:25 visual_prompt]: 	Training 300/553. train loss: 0.7110,	1.3066 s / batch. (data: 8.11e-01). ETA=17:21:07, max mem: 11.4 GB 
[10/28 04:40:53 visual_prompt]: 	Training 400/553. train loss: 0.5866,	0.4843 s / batch. (data: 5.42e-03). ETA=6:25:05, max mem: 11.4 GB 
[10/28 04:42:22 visual_prompt]: 	Training 500/553. train loss: 1.2014,	0.4970 s / batch. (data: 7.98e-03). ETA=6:34:24, max mem: 11.4 GB 
[10/28 04:43:06 visual_prompt]: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8881, average train loss: 0.9377
[10/28 04:43:58 visual_prompt]: Inference (val):avg data time: 1.63e-04, avg batch time: 0.1914, average loss: 0.6895
[10/28 04:43:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.77	
[10/28 04:43:58 visual_prompt]: Best epoch 14: best metric: -0.689
[10/28 04:43:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/28 04:45:30 visual_prompt]: 	Training 100/553. train loss: 0.8812,	0.5091 s / batch. (data: 2.70e-04). ETA=6:42:42, max mem: 11.4 GB 
[10/28 04:46:58 visual_prompt]: 	Training 200/553. train loss: 4.2019,	0.4884 s / batch. (data: 1.04e-02). ETA=6:25:27, max mem: 11.4 GB 
[10/28 04:48:27 visual_prompt]: 	Training 300/553. train loss: 4.3591,	0.4916 s / batch. (data: 2.81e-04). ETA=6:27:13, max mem: 11.4 GB 
[10/28 04:49:54 visual_prompt]: 	Training 400/553. train loss: 1.0182,	0.4918 s / batch. (data: 3.27e-04). ETA=6:26:31, max mem: 11.4 GB 
[10/28 04:51:24 visual_prompt]: 	Training 500/553. train loss: 0.5925,	0.5159 s / batch. (data: 3.15e-04). ETA=6:44:39, max mem: 11.4 GB 
[10/28 04:52:10 visual_prompt]: Epoch 15 / 100: avg data time: 3.95e-01, avg batch time: 0.8887, average train loss: 1.0825
[10/28 04:53:02 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1901, average loss: 0.8553
[10/28 04:53:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.57	
[10/28 04:53:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/28 04:54:33 visual_prompt]: 	Training 100/553. train loss: 0.5967,	0.4788 s / batch. (data: 2.74e-04). ETA=6:14:17, max mem: 11.4 GB 
[10/28 04:56:02 visual_prompt]: 	Training 200/553. train loss: 1.3569,	0.5040 s / batch. (data: 2.86e-04). ETA=6:33:09, max mem: 11.4 GB 
[10/28 04:57:30 visual_prompt]: 	Training 300/553. train loss: 0.9528,	0.5120 s / batch. (data: 2.78e-04). ETA=6:38:32, max mem: 11.4 GB 
[10/28 04:58:59 visual_prompt]: 	Training 400/553. train loss: 0.8420,	0.4907 s / batch. (data: 2.72e-04). ETA=6:21:07, max mem: 11.4 GB 
[10/28 05:00:27 visual_prompt]: 	Training 500/553. train loss: 0.7190,	1.9880 s / batch. (data: 1.50e+00). ETA=1 day, 1:40:50, max mem: 11.4 GB 
[10/28 05:01:13 visual_prompt]: Epoch 16 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 0.9551
[10/28 05:02:05 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1912, average loss: 0.7025
[10/28 05:02:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.42	
[10/28 05:02:05 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/28 05:03:37 visual_prompt]: 	Training 100/553. train loss: 0.8922,	0.4910 s / batch. (data: 2.65e-04). ETA=6:19:17, max mem: 11.4 GB 
[10/28 05:05:06 visual_prompt]: 	Training 200/553. train loss: 0.8662,	0.4783 s / batch. (data: 2.49e-04). ETA=6:08:42, max mem: 11.4 GB 
[10/28 05:06:34 visual_prompt]: 	Training 300/553. train loss: 0.9948,	0.4913 s / batch. (data: 2.68e-04). ETA=6:17:52, max mem: 11.4 GB 
[10/28 05:08:02 visual_prompt]: 	Training 400/553. train loss: 0.6573,	1.5685 s / batch. (data: 1.08e+00). ETA=20:03:51, max mem: 11.4 GB 
[10/28 05:09:30 visual_prompt]: 	Training 500/553. train loss: 0.9451,	2.1594 s / batch. (data: 1.66e+00). ETA=1 day, 3:33:50, max mem: 11.4 GB 
[10/28 05:10:16 visual_prompt]: Epoch 17 / 100: avg data time: 3.93e-01, avg batch time: 0.8875, average train loss: 0.9741
[10/28 05:11:09 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1925, average loss: 0.7615
[10/28 05:11:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.21	
[10/28 05:11:09 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/28 05:12:40 visual_prompt]: 	Training 100/553. train loss: 0.8560,	0.4918 s / batch. (data: 1.05e-02). ETA=6:15:22, max mem: 11.4 GB 
[10/28 05:14:11 visual_prompt]: 	Training 200/553. train loss: 0.9798,	0.4875 s / batch. (data: 2.74e-04). ETA=6:11:16, max mem: 11.4 GB 
[10/28 05:15:40 visual_prompt]: 	Training 300/553. train loss: 0.5614,	0.5023 s / batch. (data: 2.42e-02). ETA=6:21:46, max mem: 11.4 GB 
[10/28 05:17:08 visual_prompt]: 	Training 400/553. train loss: 0.9991,	0.4800 s / batch. (data: 2.73e-04). ETA=6:03:59, max mem: 11.4 GB 
[10/28 05:18:36 visual_prompt]: 	Training 500/553. train loss: 0.7214,	1.2720 s / batch. (data: 7.72e-01). ETA=16:02:27, max mem: 11.4 GB 
[10/28 05:19:20 visual_prompt]: Epoch 18 / 100: avg data time: 3.94e-01, avg batch time: 0.8891, average train loss: 1.0872
[10/28 05:20:13 visual_prompt]: Inference (val):avg data time: 1.83e-04, avg batch time: 0.1920, average loss: 1.0049
[10/28 05:20:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.15	
[10/28 05:20:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/28 05:21:44 visual_prompt]: 	Training 100/553. train loss: 0.6113,	0.4920 s / batch. (data: 2.74e-04). ETA=6:11:01, max mem: 11.4 GB 
[10/28 05:23:13 visual_prompt]: 	Training 200/553. train loss: 0.6320,	0.4917 s / batch. (data: 2.65e-04). ETA=6:10:00, max mem: 11.4 GB 
[10/28 05:24:42 visual_prompt]: 	Training 300/553. train loss: 0.9983,	1.5840 s / batch. (data: 1.07e+00). ETA=19:49:11, max mem: 11.4 GB 
[10/28 05:26:12 visual_prompt]: 	Training 400/553. train loss: 0.5864,	0.4998 s / batch. (data: 7.44e-04). ETA=6:14:22, max mem: 11.4 GB 
[10/28 05:27:36 visual_prompt]: 	Training 500/553. train loss: 1.1702,	0.5050 s / batch. (data: 7.28e-03). ETA=6:17:28, max mem: 11.4 GB 
[10/28 05:28:23 visual_prompt]: Epoch 19 / 100: avg data time: 3.92e-01, avg batch time: 0.8869, average train loss: 0.9441
[10/28 05:29:16 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1913, average loss: 1.4731
[10/28 05:29:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.84	
[10/28 05:29:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/28 05:30:45 visual_prompt]: 	Training 100/553. train loss: 0.6134,	0.6760 s / batch. (data: 1.81e-01). ETA=8:23:32, max mem: 11.4 GB 
[10/28 05:32:16 visual_prompt]: 	Training 200/553. train loss: 0.6434,	0.5123 s / batch. (data: 4.40e-03). ETA=6:20:44, max mem: 11.4 GB 
[10/28 05:33:45 visual_prompt]: 	Training 300/553. train loss: 0.8323,	0.4840 s / batch. (data: 2.81e-04). ETA=5:58:54, max mem: 11.4 GB 
[10/28 05:35:13 visual_prompt]: 	Training 400/553. train loss: 0.6234,	0.4841 s / batch. (data: 2.85e-04). ETA=5:58:09, max mem: 11.4 GB 
[10/28 05:36:41 visual_prompt]: 	Training 500/553. train loss: 0.7304,	0.4880 s / batch. (data: 2.71e-04). ETA=6:00:14, max mem: 11.4 GB 
[10/28 05:37:27 visual_prompt]: Epoch 20 / 100: avg data time: 3.94e-01, avg batch time: 0.8890, average train loss: 0.9117
[10/28 05:38:20 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1906, average loss: 0.8009
[10/28 05:38:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.97	
[10/28 05:38:20 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[10/28 05:39:54 visual_prompt]: 	Training 100/553. train loss: 1.2203,	1.0160 s / batch. (data: 5.02e-01). ETA=12:27:26, max mem: 11.4 GB 
[10/28 05:41:21 visual_prompt]: 	Training 200/553. train loss: 1.2670,	0.4881 s / batch. (data: 2.53e-04). ETA=5:58:17, max mem: 11.4 GB 
[10/28 05:42:49 visual_prompt]: 	Training 300/553. train loss: 2.8757,	0.8400 s / batch. (data: 3.33e-01). ETA=10:15:09, max mem: 11.4 GB 
[10/28 05:44:18 visual_prompt]: 	Training 400/553. train loss: 1.7923,	0.4866 s / batch. (data: 2.61e-04). ETA=5:55:33, max mem: 11.4 GB 
[10/28 05:45:47 visual_prompt]: 	Training 500/553. train loss: 0.6934,	0.4911 s / batch. (data: 2.80e-04). ETA=5:58:00, max mem: 11.4 GB 
[10/28 05:46:31 visual_prompt]: Epoch 21 / 100: avg data time: 3.94e-01, avg batch time: 0.8884, average train loss: 0.9198
[10/28 05:47:23 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1925, average loss: 0.7024
[10/28 05:47:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.78	
[10/28 05:47:23 visual_prompt]: Stopping early.
[10/28 05:47:23 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 05:47:23 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 05:47:23 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 05:47:23 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 05:47:23 visual_prompt]: Training with config:
[10/28 05:47:23 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 05:47:23 visual_prompt]: Loading training data...
[10/28 05:47:23 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 05:47:23 visual_prompt]: Loading validation data...
[10/28 05:47:23 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 05:47:23 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/28 05:47:26 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/28 05:47:26 visual_prompt]: tuned percent:0.529
[10/28 05:47:26 visual_prompt]: Device used for model: 0
[10/28 05:47:26 visual_prompt]: Setting up Evaluator...
[10/28 05:47:26 visual_prompt]: Setting up Trainer...
[10/28 05:47:26 visual_prompt]: 	Setting up the optimizer...
[10/28 05:47:26 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 05:48:57 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5024 s / batch. (data: 2.83e-04). ETA=7:42:09, max mem: 11.4 GB 
[10/28 05:50:24 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5160 s / batch. (data: 2.78e-04). ETA=7:53:51, max mem: 11.4 GB 
[10/28 05:51:56 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9133 s / batch. (data: 2.39e+00). ETA=1 day, 20:30:30, max mem: 11.4 GB 
[10/28 05:53:22 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4880 s / batch. (data: 2.27e-04). ETA=7:26:31, max mem: 11.4 GB 
[10/28 05:54:53 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5160 s / batch. (data: 2.70e-04). ETA=7:51:15, max mem: 11.4 GB 
[10/28 05:55:39 visual_prompt]: Epoch 1 / 100: avg data time: 3.95e-01, avg batch time: 0.8908, average train loss: 1.3966
[10/28 05:56:31 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1918, average loss: 1.3454
[10/28 05:56:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/28 05:56:31 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/28 05:58:02 visual_prompt]: 	Training 100/553. train loss: 0.6577,	0.5270 s / batch. (data: 1.91e-02). ETA=8:00:00, max mem: 11.4 GB 
[10/28 05:59:30 visual_prompt]: 	Training 200/553. train loss: 0.1724,	1.0193 s / batch. (data: 5.30e-01). ETA=15:26:37, max mem: 11.4 GB 
[10/28 06:01:00 visual_prompt]: 	Training 300/553. train loss: 0.9567,	1.4581 s / batch. (data: 9.80e-01). ETA=22:03:09, max mem: 11.4 GB 
[10/28 06:02:28 visual_prompt]: 	Training 400/553. train loss: 1.1840,	0.4891 s / batch. (data: 5.37e-03). ETA=7:23:03, max mem: 11.4 GB 
[10/28 06:03:58 visual_prompt]: 	Training 500/553. train loss: 0.6324,	0.4787 s / batch. (data: 2.49e-04). ETA=7:12:46, max mem: 11.4 GB 
[10/28 06:04:43 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 0.8906, average train loss: 0.8276
[10/28 06:05:36 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1903, average loss: 0.7433
[10/28 06:05:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.21	
[10/28 06:05:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/28 06:07:06 visual_prompt]: 	Training 100/553. train loss: 0.8026,	0.4961 s / batch. (data: 2.64e-04). ETA=7:27:15, max mem: 11.4 GB 
[10/28 06:08:37 visual_prompt]: 	Training 200/553. train loss: 0.7144,	0.4880 s / batch. (data: 2.75e-04). ETA=7:19:08, max mem: 11.4 GB 
[10/28 06:10:04 visual_prompt]: 	Training 300/553. train loss: 0.6032,	0.4996 s / batch. (data: 5.40e-03). ETA=7:28:47, max mem: 11.4 GB 
[10/28 06:11:35 visual_prompt]: 	Training 400/553. train loss: 0.7352,	0.5129 s / batch. (data: 1.04e-02). ETA=7:39:50, max mem: 11.4 GB 
[10/28 06:13:05 visual_prompt]: 	Training 500/553. train loss: 0.7270,	1.9440 s / batch. (data: 1.44e+00). ETA=1 day, 4:59:43, max mem: 11.4 GB 
[10/28 06:13:48 visual_prompt]: Epoch 3 / 100: avg data time: 3.96e-01, avg batch time: 0.8906, average train loss: 0.7716
[10/28 06:14:41 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1930, average loss: 0.7158
[10/28 06:14:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.92	
[10/28 06:14:41 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/28 06:16:14 visual_prompt]: 	Training 100/553. train loss: 0.7323,	0.4960 s / batch. (data: 2.52e-04). ETA=7:22:36, max mem: 11.4 GB 
[10/28 06:17:42 visual_prompt]: 	Training 200/553. train loss: 1.4443,	0.4960 s / batch. (data: 2.50e-04). ETA=7:21:47, max mem: 11.4 GB 
[10/28 06:19:11 visual_prompt]: 	Training 300/553. train loss: 0.6365,	0.5290 s / batch. (data: 2.06e-02). ETA=7:50:16, max mem: 11.4 GB 
[10/28 06:20:36 visual_prompt]: 	Training 400/553. train loss: 0.5907,	1.0601 s / batch. (data: 5.66e-01). ETA=15:40:41, max mem: 11.4 GB 
[10/28 06:22:05 visual_prompt]: 	Training 500/553. train loss: 0.7422,	3.2245 s / batch. (data: 2.74e+00). ETA=1 day, 23:35:51, max mem: 11.4 GB 
[10/28 06:22:52 visual_prompt]: Epoch 4 / 100: avg data time: 3.93e-01, avg batch time: 0.8878, average train loss: 0.8661
[10/28 06:23:44 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1929, average loss: 0.8707
[10/28 06:23:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.24	
[10/28 06:23:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/28 06:25:14 visual_prompt]: 	Training 100/553. train loss: 0.4970,	0.4937 s / batch. (data: 2.65e-04). ETA=7:16:02, max mem: 11.4 GB 
[10/28 06:26:43 visual_prompt]: 	Training 200/553. train loss: 0.5772,	1.7839 s / batch. (data: 1.30e+00). ETA=1 day, 2:12:29, max mem: 11.4 GB 
[10/28 06:28:12 visual_prompt]: 	Training 300/553. train loss: 1.3876,	0.4960 s / batch. (data: 5.34e-03). ETA=7:16:23, max mem: 11.4 GB 
[10/28 06:29:40 visual_prompt]: 	Training 400/553. train loss: 1.1977,	0.5000 s / batch. (data: 1.20e-02). ETA=7:19:03, max mem: 11.4 GB 
[10/28 06:31:09 visual_prompt]: 	Training 500/553. train loss: 0.5592,	0.4893 s / batch. (data: 8.77e-03). ETA=7:08:49, max mem: 11.4 GB 
[10/28 06:31:55 visual_prompt]: Epoch 5 / 100: avg data time: 3.93e-01, avg batch time: 0.8878, average train loss: 0.9102
[10/28 06:32:47 visual_prompt]: Inference (val):avg data time: 2.03e-04, avg batch time: 0.1928, average loss: 0.8118
[10/28 06:32:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.94	
[10/28 06:32:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/28 06:34:20 visual_prompt]: 	Training 100/553. train loss: 0.6349,	0.4800 s / batch. (data: 2.59e-04). ETA=6:59:28, max mem: 11.4 GB 
[10/28 06:35:48 visual_prompt]: 	Training 200/553. train loss: 0.6049,	0.4912 s / batch. (data: 2.94e-04). ETA=7:08:25, max mem: 11.4 GB 
[10/28 06:37:15 visual_prompt]: 	Training 300/553. train loss: 0.5702,	0.5098 s / batch. (data: 2.05e-02). ETA=7:23:50, max mem: 11.4 GB 
[10/28 06:38:46 visual_prompt]: 	Training 400/553. train loss: 0.5924,	0.6147 s / batch. (data: 1.27e-01). ETA=8:54:07, max mem: 11.4 GB 
[10/28 06:40:14 visual_prompt]: 	Training 500/553. train loss: 0.9529,	1.4560 s / batch. (data: 9.50e-01). ETA=21:02:41, max mem: 11.4 GB 
[10/28 06:40:58 visual_prompt]: Epoch 6 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 0.7765
[10/28 06:41:50 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1909, average loss: 0.6933
[10/28 06:41:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.10	
[10/28 06:41:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/28 06:43:20 visual_prompt]: 	Training 100/553. train loss: 0.7679,	0.5000 s / batch. (data: 2.71e-04). ETA=7:12:19, max mem: 11.4 GB 
[10/28 06:44:48 visual_prompt]: 	Training 200/553. train loss: 0.6106,	1.0080 s / batch. (data: 4.92e-01). ETA=14:29:56, max mem: 11.4 GB 
[10/28 06:46:20 visual_prompt]: 	Training 300/553. train loss: 0.5926,	1.6520 s / batch. (data: 1.14e+00). ETA=23:42:56, max mem: 11.4 GB 
[10/28 06:47:49 visual_prompt]: 	Training 400/553. train loss: 0.6216,	2.4009 s / batch. (data: 1.89e+00). ETA=1 day, 10:24:02, max mem: 11.4 GB 
[10/28 06:49:15 visual_prompt]: 	Training 500/553. train loss: 1.1040,	0.4964 s / batch. (data: 2.94e-04). ETA=7:05:56, max mem: 11.4 GB 
[10/28 06:50:00 visual_prompt]: Epoch 7 / 100: avg data time: 3.90e-01, avg batch time: 0.8850, average train loss: 0.7921
[10/28 06:50:52 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1909, average loss: 0.7091
[10/28 06:50:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.56	
[10/28 06:50:52 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/28 06:52:22 visual_prompt]: 	Training 100/553. train loss: 0.7071,	0.5005 s / batch. (data: 7.97e-03). ETA=7:08:11, max mem: 11.4 GB 
[10/28 06:53:51 visual_prompt]: 	Training 200/553. train loss: 1.2323,	0.4878 s / batch. (data: 2.79e-04). ETA=6:56:28, max mem: 11.4 GB 
[10/28 06:55:21 visual_prompt]: 	Training 300/553. train loss: 1.1297,	0.4972 s / batch. (data: 5.38e-03). ETA=7:03:41, max mem: 11.4 GB 
[10/28 06:56:50 visual_prompt]: 	Training 400/553. train loss: 0.7727,	0.5109 s / batch. (data: 1.58e-02). ETA=7:14:29, max mem: 11.4 GB 
[10/28 06:58:19 visual_prompt]: 	Training 500/553. train loss: 0.9599,	1.9512 s / batch. (data: 1.46e+00). ETA=1 day, 3:36:13, max mem: 11.4 GB 
[10/28 06:59:04 visual_prompt]: Epoch 8 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.7953
[10/28 06:59:56 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1902, average loss: 0.7653
[10/28 06:59:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.81	
[10/28 06:59:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/28 07:01:28 visual_prompt]: 	Training 100/553. train loss: 0.0363,	0.5001 s / batch. (data: 2.71e-04). ETA=7:03:15, max mem: 11.4 GB 
[10/28 07:02:56 visual_prompt]: 	Training 200/553. train loss: 0.8145,	0.5286 s / batch. (data: 2.85e-02). ETA=7:26:27, max mem: 11.4 GB 
[10/28 07:04:24 visual_prompt]: 	Training 300/553. train loss: 0.5867,	1.9994 s / batch. (data: 1.52e+00). ETA=1 day, 4:05:19, max mem: 11.4 GB 
[10/28 07:05:53 visual_prompt]: 	Training 400/553. train loss: 0.7351,	0.5253 s / batch. (data: 2.06e-02). ETA=7:21:53, max mem: 11.4 GB 
[10/28 07:07:22 visual_prompt]: 	Training 500/553. train loss: 0.7176,	0.6160 s / batch. (data: 1.17e-01). ETA=8:37:12, max mem: 11.4 GB 
[10/28 07:08:06 visual_prompt]: Epoch 9 / 100: avg data time: 3.91e-01, avg batch time: 0.8860, average train loss: 0.9507
[10/28 07:08:58 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1913, average loss: 0.6876
[10/28 07:08:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[10/28 07:08:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/28 07:10:32 visual_prompt]: 	Training 100/553. train loss: 1.4224,	0.4842 s / batch. (data: 2.72e-04). ETA=6:45:17, max mem: 11.4 GB 
[10/28 07:11:59 visual_prompt]: 	Training 200/553. train loss: 0.6250,	0.4960 s / batch. (data: 2.65e-04). ETA=6:54:21, max mem: 11.4 GB 
[10/28 07:13:27 visual_prompt]: 	Training 300/553. train loss: 0.6629,	0.4920 s / batch. (data: 2.85e-04). ETA=6:50:11, max mem: 11.4 GB 
[10/28 07:14:54 visual_prompt]: 	Training 400/553. train loss: 1.1318,	1.2960 s / batch. (data: 7.89e-01). ETA=17:58:17, max mem: 11.4 GB 
[10/28 07:16:24 visual_prompt]: 	Training 500/553. train loss: 0.8173,	1.5800 s / batch. (data: 1.09e+00). ETA=21:52:00, max mem: 11.4 GB 
[10/28 07:17:08 visual_prompt]: Epoch 10 / 100: avg data time: 3.92e-01, avg batch time: 0.8865, average train loss: 0.9896
[10/28 07:18:01 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1915, average loss: 0.7218
[10/28 07:18:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.38	
[10/28 07:18:01 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/28 07:19:34 visual_prompt]: 	Training 100/553. train loss: 1.5226,	0.4962 s / batch. (data: 1.20e-02). ETA=6:50:44, max mem: 11.4 GB 
[10/28 07:21:04 visual_prompt]: 	Training 200/553. train loss: 1.1051,	0.5186 s / batch. (data: 2.26e-02). ETA=7:08:28, max mem: 11.4 GB 
[10/28 07:22:32 visual_prompt]: 	Training 300/553. train loss: 0.2002,	2.6096 s / batch. (data: 2.10e+00). ETA=1 day, 11:51:39, max mem: 11.4 GB 
[10/28 07:23:59 visual_prompt]: 	Training 400/553. train loss: 0.6072,	0.5000 s / batch. (data: 2.66e-04). ETA=6:51:24, max mem: 11.4 GB 
[10/28 07:25:27 visual_prompt]: 	Training 500/553. train loss: 0.8212,	0.5279 s / batch. (data: 2.71e-02). ETA=7:13:27, max mem: 11.4 GB 
[10/28 07:26:11 visual_prompt]: Epoch 11 / 100: avg data time: 3.93e-01, avg batch time: 0.8873, average train loss: 0.9022
[10/28 07:27:04 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.1917, average loss: 0.6996
[10/28 07:27:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.20	
[10/28 07:27:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/28 07:28:37 visual_prompt]: 	Training 100/553. train loss: 0.8148,	0.4960 s / batch. (data: 2.62e-04). ETA=6:46:02, max mem: 11.4 GB 
[10/28 07:30:06 visual_prompt]: 	Training 200/553. train loss: 0.9216,	0.5120 s / batch. (data: 1.20e-02). ETA=6:58:16, max mem: 11.4 GB 
[10/28 07:31:33 visual_prompt]: 	Training 300/553. train loss: 0.7223,	0.5168 s / batch. (data: 7.92e-03). ETA=7:01:20, max mem: 11.4 GB 
[10/28 07:33:02 visual_prompt]: 	Training 400/553. train loss: 0.6996,	0.4962 s / batch. (data: 9.10e-03). ETA=6:43:42, max mem: 11.4 GB 
[10/28 07:34:31 visual_prompt]: 	Training 500/553. train loss: 3.1091,	0.4946 s / batch. (data: 2.51e-04). ETA=6:41:34, max mem: 11.4 GB 
[10/28 07:35:15 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8880, average train loss: 0.9328
[10/28 07:36:07 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1929, average loss: 2.5667
[10/28 07:36:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.31	
[10/28 07:36:07 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/28 07:37:40 visual_prompt]: 	Training 100/553. train loss: 0.5484,	0.7944 s / batch. (data: 2.96e-01). ETA=10:42:58, max mem: 11.4 GB 
[10/28 07:39:06 visual_prompt]: 	Training 200/553. train loss: 0.6685,	1.2358 s / batch. (data: 7.45e-01). ETA=16:38:11, max mem: 11.4 GB 
[10/28 07:40:35 visual_prompt]: 	Training 300/553. train loss: 0.5792,	2.2572 s / batch. (data: 1.76e+00). ETA=1 day, 6:19:25, max mem: 11.4 GB 
[10/28 07:42:02 visual_prompt]: 	Training 400/553. train loss: 3.5108,	1.4310 s / batch. (data: 9.52e-01). ETA=19:11:03, max mem: 11.4 GB 
[10/28 07:43:32 visual_prompt]: 	Training 500/553. train loss: 0.9525,	0.4920 s / batch. (data: 2.88e-04). ETA=6:34:57, max mem: 11.4 GB 
[10/28 07:44:17 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8863, average train loss: 1.1221
[10/28 07:45:10 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1911, average loss: 1.2269
[10/28 07:45:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.53	
[10/28 07:45:10 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/28 07:46:42 visual_prompt]: 	Training 100/553. train loss: 0.6181,	0.4783 s / batch. (data: 2.59e-04). ETA=6:22:45, max mem: 11.4 GB 
[10/28 07:48:11 visual_prompt]: 	Training 200/553. train loss: 0.4129,	1.9360 s / batch. (data: 1.43e+00). ETA=1 day, 1:45:54, max mem: 11.4 GB 
[10/28 07:49:39 visual_prompt]: 	Training 300/553. train loss: 0.6907,	1.2400 s / batch. (data: 7.54e-01). ETA=16:28:05, max mem: 11.4 GB 
[10/28 07:51:07 visual_prompt]: 	Training 400/553. train loss: 0.8204,	0.5081 s / batch. (data: 2.78e-04). ETA=6:44:04, max mem: 11.4 GB 
[10/28 07:52:35 visual_prompt]: 	Training 500/553. train loss: 1.3922,	0.5125 s / batch. (data: 3.10e-04). ETA=6:46:41, max mem: 11.4 GB 
[10/28 07:53:20 visual_prompt]: Epoch 14 / 100: avg data time: 3.93e-01, avg batch time: 0.8870, average train loss: 0.8496
[10/28 07:54:12 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1908, average loss: 0.6891
[10/28 07:54:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.04	
[10/28 07:54:12 visual_prompt]: Best epoch 14: best metric: -0.689
[10/28 07:54:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/28 07:55:44 visual_prompt]: 	Training 100/553. train loss: 0.6944,	0.5083 s / batch. (data: 8.19e-03). ETA=6:42:00, max mem: 11.4 GB 
[10/28 07:57:11 visual_prompt]: 	Training 200/553. train loss: 0.9755,	0.4960 s / batch. (data: 2.80e-04). ETA=6:31:28, max mem: 11.4 GB 
[10/28 07:58:41 visual_prompt]: 	Training 300/553. train loss: 0.8836,	0.5040 s / batch. (data: 2.65e-04). ETA=6:36:57, max mem: 11.4 GB 
[10/28 08:00:08 visual_prompt]: 	Training 400/553. train loss: 0.7995,	0.4955 s / batch. (data: 3.09e-04). ETA=6:29:26, max mem: 11.4 GB 
[10/28 08:01:38 visual_prompt]: 	Training 500/553. train loss: 0.5873,	0.5125 s / batch. (data: 2.04e-02). ETA=6:41:55, max mem: 11.4 GB 
[10/28 08:02:24 visual_prompt]: Epoch 15 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 0.9203
[10/28 08:03:16 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1906, average loss: 1.1806
[10/28 08:03:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.28	
[10/28 08:03:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/28 08:04:47 visual_prompt]: 	Training 100/553. train loss: 0.5987,	0.5007 s / batch. (data: 1.04e-02). ETA=6:31:24, max mem: 11.4 GB 
[10/28 08:06:15 visual_prompt]: 	Training 200/553. train loss: 1.1923,	0.5199 s / batch. (data: 1.20e-02). ETA=6:45:35, max mem: 11.4 GB 
[10/28 08:07:45 visual_prompt]: 	Training 300/553. train loss: 0.8523,	0.5186 s / batch. (data: 3.21e-02). ETA=6:43:42, max mem: 11.4 GB 
[10/28 08:09:13 visual_prompt]: 	Training 400/553. train loss: 0.6985,	0.4788 s / batch. (data: 2.56e-04). ETA=6:11:52, max mem: 11.4 GB 
[10/28 08:10:41 visual_prompt]: 	Training 500/553. train loss: 0.5574,	1.7320 s / batch. (data: 1.23e+00). ETA=22:22:26, max mem: 11.4 GB 
[10/28 08:11:27 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 0.8870, average train loss: 0.8668
[10/28 08:12:19 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1928, average loss: 0.7386
[10/28 08:12:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.13	
[10/28 08:12:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/28 08:13:49 visual_prompt]: 	Training 100/553. train loss: 0.8593,	0.4987 s / batch. (data: 2.34e-04). ETA=6:25:15, max mem: 11.4 GB 
[10/28 08:15:19 visual_prompt]: 	Training 200/553. train loss: 1.5798,	0.4880 s / batch. (data: 2.86e-04). ETA=6:16:11, max mem: 11.4 GB 
[10/28 08:16:48 visual_prompt]: 	Training 300/553. train loss: 1.1929,	0.5000 s / batch. (data: 2.82e-04). ETA=6:24:35, max mem: 11.4 GB 
[10/28 08:18:15 visual_prompt]: 	Training 400/553. train loss: 1.1602,	0.5077 s / batch. (data: 2.23e-02). ETA=6:29:38, max mem: 11.4 GB 
[10/28 08:19:43 visual_prompt]: 	Training 500/553. train loss: 0.8799,	1.8160 s / batch. (data: 1.32e+00). ETA=23:10:48, max mem: 11.4 GB 
[10/28 08:20:29 visual_prompt]: Epoch 17 / 100: avg data time: 3.93e-01, avg batch time: 0.8864, average train loss: 0.9853
[10/28 08:21:22 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1907, average loss: 0.7457
[10/28 08:21:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.29	
[10/28 08:21:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/28 08:22:54 visual_prompt]: 	Training 100/553. train loss: 0.8469,	0.4880 s / batch. (data: 2.45e-04). ETA=6:12:28, max mem: 11.4 GB 
[10/28 08:24:25 visual_prompt]: 	Training 200/553. train loss: 0.6719,	0.4886 s / batch. (data: 1.05e-02). ETA=6:12:07, max mem: 11.4 GB 
[10/28 08:25:55 visual_prompt]: 	Training 300/553. train loss: 0.5890,	0.4976 s / batch. (data: 1.36e-02). ETA=6:18:09, max mem: 11.4 GB 
[10/28 08:27:24 visual_prompt]: 	Training 400/553. train loss: 0.7407,	0.4782 s / batch. (data: 2.64e-04). ETA=6:02:38, max mem: 11.4 GB 
[10/28 08:28:52 visual_prompt]: 	Training 500/553. train loss: 0.9516,	0.4842 s / batch. (data: 2.68e-04). ETA=6:06:21, max mem: 11.4 GB 
[10/28 08:29:36 visual_prompt]: Epoch 18 / 100: avg data time: 3.99e-01, avg batch time: 0.8936, average train loss: 1.0370
[10/28 08:30:28 visual_prompt]: Inference (val):avg data time: 2.05e-04, avg batch time: 0.1900, average loss: 0.7055
[10/28 08:30:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.71	
[10/28 08:30:28 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/28 08:32:01 visual_prompt]: 	Training 100/553. train loss: 0.5616,	0.5111 s / batch. (data: 2.97e-04). ETA=6:25:24, max mem: 11.4 GB 
[10/28 08:33:30 visual_prompt]: 	Training 200/553. train loss: 0.5678,	0.5159 s / batch. (data: 1.05e-02). ETA=6:28:09, max mem: 11.4 GB 
[10/28 08:35:00 visual_prompt]: 	Training 300/553. train loss: 1.4644,	0.5133 s / batch. (data: 2.53e-04). ETA=6:25:24, max mem: 11.4 GB 
[10/28 08:36:30 visual_prompt]: 	Training 400/553. train loss: 0.6074,	0.5000 s / batch. (data: 2.84e-04). ETA=6:14:32, max mem: 11.4 GB 
[10/28 08:37:54 visual_prompt]: 	Training 500/553. train loss: 0.7857,	0.4890 s / batch. (data: 5.38e-03). ETA=6:05:27, max mem: 11.4 GB 
[10/28 08:38:41 visual_prompt]: Epoch 19 / 100: avg data time: 3.96e-01, avg batch time: 0.8911, average train loss: 0.8189
[10/28 08:39:34 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1916, average loss: 1.1766
[10/28 08:39:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.79	
[10/28 08:39:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/28 08:41:04 visual_prompt]: 	Training 100/553. train loss: 0.5898,	0.7963 s / batch. (data: 2.82e-01). ETA=9:53:07, max mem: 11.4 GB 
[10/28 08:42:34 visual_prompt]: 	Training 200/553. train loss: 0.5796,	0.4985 s / batch. (data: 5.46e-03). ETA=6:10:28, max mem: 11.4 GB 
[10/28 08:44:03 visual_prompt]: 	Training 300/553. train loss: 1.0050,	0.5090 s / batch. (data: 2.49e-02). ETA=6:17:25, max mem: 11.4 GB 
[10/28 08:45:32 visual_prompt]: 	Training 400/553. train loss: 0.7562,	0.4995 s / batch. (data: 5.41e-03). ETA=6:09:33, max mem: 11.4 GB 
[10/28 08:47:00 visual_prompt]: 	Training 500/553. train loss: 1.4040,	0.4846 s / batch. (data: 6.85e-03). ETA=5:57:44, max mem: 11.4 GB 
[10/28 08:47:47 visual_prompt]: Epoch 20 / 100: avg data time: 3.97e-01, avg batch time: 0.8921, average train loss: 1.0540
[10/28 08:48:40 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1906, average loss: 0.8959
[10/28 08:48:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.78	
[10/28 08:48:40 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[10/28 08:50:14 visual_prompt]: 	Training 100/553. train loss: 0.8698,	0.4779 s / batch. (data: 2.37e-04). ETA=5:51:35, max mem: 11.4 GB 
[10/28 08:51:42 visual_prompt]: 	Training 200/553. train loss: 1.1159,	0.4881 s / batch. (data: 2.61e-04). ETA=5:58:15, max mem: 11.4 GB 
[10/28 08:53:10 visual_prompt]: 	Training 300/553. train loss: 1.4099,	1.4543 s / batch. (data: 9.78e-01). ETA=17:45:02, max mem: 11.4 GB 
[10/28 08:54:38 visual_prompt]: 	Training 400/553. train loss: 1.5039,	0.5078 s / batch. (data: 5.38e-03). ETA=6:11:00, max mem: 11.4 GB 
[10/28 08:56:07 visual_prompt]: 	Training 500/553. train loss: 0.7153,	0.4920 s / batch. (data: 2.84e-04). ETA=5:58:37, max mem: 11.4 GB 
[10/28 08:56:51 visual_prompt]: Epoch 21 / 100: avg data time: 3.95e-01, avg batch time: 0.8883, average train loss: 0.9404
[10/28 08:57:43 visual_prompt]: Inference (val):avg data time: 7.59e-04, avg batch time: 0.1912, average loss: 0.7117
[10/28 08:57:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.33	
[10/28 08:57:43 visual_prompt]: Stopping early.
[10/28 08:57:43 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 08:57:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 08:57:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 08:57:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 08:57:43 visual_prompt]: Training with config:
[10/28 08:57:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.25_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 08:57:43 visual_prompt]: Loading training data...
[10/28 08:57:43 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 08:57:43 visual_prompt]: Loading validation data...
[10/28 08:57:43 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 08:57:43 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/28 08:57:46 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/28 08:57:46 visual_prompt]: tuned percent:0.529
[10/28 08:57:46 visual_prompt]: Device used for model: 0
[10/28 08:57:46 visual_prompt]: Setting up Evaluator...
[10/28 08:57:46 visual_prompt]: Setting up Trainer...
[10/28 08:57:46 visual_prompt]: 	Setting up the optimizer...
[10/28 08:57:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 08:59:17 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5160 s / batch. (data: 2.64e-04). ETA=7:54:43, max mem: 11.4 GB 
[10/28 09:00:44 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4961 s / batch. (data: 5.39e-03). ETA=7:35:35, max mem: 11.4 GB 
[10/28 09:02:16 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.7007 s / batch. (data: 2.21e+00). ETA=1 day, 17:15:40, max mem: 11.4 GB 
[10/28 09:03:42 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4883 s / batch. (data: 2.87e-04). ETA=7:26:49, max mem: 11.4 GB 
[10/28 09:05:13 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4984 s / batch. (data: 7.02e-04). ETA=7:35:10, max mem: 11.4 GB 
[10/28 09:05:59 visual_prompt]: Epoch 1 / 100: avg data time: 3.96e-01, avg batch time: 0.8911, average train loss: 1.3966
[10/28 09:06:51 visual_prompt]: Inference (val):avg data time: 1.85e-04, avg batch time: 0.1903, average loss: 1.3454
[10/28 09:06:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/28 09:06:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/28 09:08:22 visual_prompt]: 	Training 100/553. train loss: 0.6565,	1.2162 s / batch. (data: 7.05e-01). ETA=18:27:40, max mem: 11.4 GB 
[10/28 09:09:50 visual_prompt]: 	Training 200/553. train loss: 0.1703,	0.6791 s / batch. (data: 1.93e-01). ETA=10:17:24, max mem: 11.4 GB 
[10/28 09:11:21 visual_prompt]: 	Training 300/553. train loss: 0.9646,	1.5186 s / batch. (data: 1.04e+00). ETA=22:58:02, max mem: 11.4 GB 
[10/28 09:12:47 visual_prompt]: 	Training 400/553. train loss: 1.2121,	0.4960 s / batch. (data: 7.96e-03). ETA=7:29:16, max mem: 11.4 GB 
[10/28 09:14:17 visual_prompt]: 	Training 500/553. train loss: 0.6295,	0.4907 s / batch. (data: 5.37e-03). ETA=7:23:40, max mem: 11.4 GB 
[10/28 09:15:02 visual_prompt]: Epoch 2 / 100: avg data time: 3.93e-01, avg batch time: 0.8874, average train loss: 0.8304
[10/28 09:15:54 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1897, average loss: 0.7505
[10/28 09:15:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.44	
[10/28 09:15:54 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/28 09:17:24 visual_prompt]: 	Training 100/553. train loss: 0.8078,	0.5440 s / batch. (data: 4.76e-02). ETA=8:10:28, max mem: 11.4 GB 
[10/28 09:18:54 visual_prompt]: 	Training 200/553. train loss: 0.7288,	0.5050 s / batch. (data: 1.14e-02). ETA=7:34:27, max mem: 11.4 GB 
[10/28 09:20:21 visual_prompt]: 	Training 300/553. train loss: 0.5854,	0.5103 s / batch. (data: 1.10e-02). ETA=7:38:19, max mem: 11.4 GB 
[10/28 09:21:50 visual_prompt]: 	Training 400/553. train loss: 0.7303,	0.4960 s / batch. (data: 2.87e-04). ETA=7:24:40, max mem: 11.4 GB 
[10/28 09:23:21 visual_prompt]: 	Training 500/553. train loss: 0.7167,	1.7993 s / batch. (data: 1.31e+00). ETA=1 day, 2:50:10, max mem: 11.4 GB 
[10/28 09:24:05 visual_prompt]: Epoch 3 / 100: avg data time: 3.92e-01, avg batch time: 0.8865, average train loss: 0.7797
[10/28 09:24:57 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1905, average loss: 0.7138
[10/28 09:24:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.06	
[10/28 09:24:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/28 09:26:30 visual_prompt]: 	Training 100/553. train loss: 0.7546,	0.4880 s / batch. (data: 2.72e-04). ETA=7:15:29, max mem: 11.4 GB 
[10/28 09:27:58 visual_prompt]: 	Training 200/553. train loss: 1.6475,	0.4808 s / batch. (data: 2.70e-04). ETA=7:08:16, max mem: 11.4 GB 
[10/28 09:29:27 visual_prompt]: 	Training 300/553. train loss: 0.6518,	1.7232 s / batch. (data: 1.23e+00). ETA=1 day, 1:31:59, max mem: 11.4 GB 
[10/28 09:30:51 visual_prompt]: 	Training 400/553. train loss: 0.6071,	0.4994 s / batch. (data: 2.71e-04). ETA=7:23:07, max mem: 11.4 GB 
[10/28 09:32:23 visual_prompt]: 	Training 500/553. train loss: 0.7409,	3.3178 s / batch. (data: 2.84e+00). ETA=2 days, 0:58:29, max mem: 11.4 GB 
[10/28 09:33:08 visual_prompt]: Epoch 4 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.8928
[10/28 09:34:00 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1911, average loss: 0.9589
[10/28 09:34:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.94	
[10/28 09:34:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/28 09:35:30 visual_prompt]: 	Training 100/553. train loss: 0.4821,	0.4966 s / batch. (data: 1.51e-02). ETA=7:18:33, max mem: 11.4 GB 
[10/28 09:36:59 visual_prompt]: 	Training 200/553. train loss: 0.6268,	1.7040 s / batch. (data: 1.21e+00). ETA=1 day, 1:02:00, max mem: 11.4 GB 
[10/28 09:38:28 visual_prompt]: 	Training 300/553. train loss: 1.6569,	0.4959 s / batch. (data: 2.42e-04). ETA=7:16:16, max mem: 11.4 GB 
[10/28 09:39:56 visual_prompt]: 	Training 400/553. train loss: 1.1513,	0.5160 s / batch. (data: 2.41e-04). ETA=7:33:08, max mem: 11.4 GB 
[10/28 09:41:25 visual_prompt]: 	Training 500/553. train loss: 0.5046,	0.5175 s / batch. (data: 3.28e-02). ETA=7:33:34, max mem: 11.4 GB 
[10/28 09:42:11 visual_prompt]: Epoch 5 / 100: avg data time: 3.92e-01, avg batch time: 0.8877, average train loss: 0.9905
[10/28 09:43:04 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1908, average loss: 0.8601
[10/28 09:43:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.22	
[10/28 09:43:04 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/28 09:44:37 visual_prompt]: 	Training 100/553. train loss: 0.5824,	0.5080 s / batch. (data: 7.71e-04). ETA=7:23:54, max mem: 11.4 GB 
[10/28 09:46:05 visual_prompt]: 	Training 200/553. train loss: 0.8551,	0.4880 s / batch. (data: 7.95e-03). ETA=7:05:39, max mem: 11.4 GB 
[10/28 09:47:32 visual_prompt]: 	Training 300/553. train loss: 0.5732,	0.5120 s / batch. (data: 1.20e-02). ETA=7:25:44, max mem: 11.4 GB 
[10/28 09:49:04 visual_prompt]: 	Training 400/553. train loss: 0.6036,	0.8960 s / batch. (data: 4.10e-01). ETA=12:58:32, max mem: 11.4 GB 
[10/28 09:50:31 visual_prompt]: 	Training 500/553. train loss: 1.0857,	1.4394 s / batch. (data: 9.33e-01). ETA=20:48:18, max mem: 11.4 GB 
[10/28 09:51:16 visual_prompt]: Epoch 6 / 100: avg data time: 3.95e-01, avg batch time: 0.8894, average train loss: 0.8539
[10/28 09:52:08 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1902, average loss: 0.7966
[10/28 09:52:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.86	
[10/28 09:52:08 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/28 09:53:39 visual_prompt]: 	Training 100/553. train loss: 1.0104,	0.4966 s / batch. (data: 7.18e-04). ETA=7:09:26, max mem: 11.4 GB 
[10/28 09:55:07 visual_prompt]: 	Training 200/553. train loss: 0.5376,	0.5080 s / batch. (data: 2.75e-04). ETA=7:18:23, max mem: 11.4 GB 
[10/28 09:56:37 visual_prompt]: 	Training 300/553. train loss: 0.6001,	0.5041 s / batch. (data: 1.05e-02). ETA=7:14:15, max mem: 11.4 GB 
[10/28 09:58:05 visual_prompt]: 	Training 400/553. train loss: 0.5928,	0.8800 s / batch. (data: 4.00e-01). ETA=12:36:32, max mem: 11.4 GB 
[10/28 09:59:33 visual_prompt]: 	Training 500/553. train loss: 1.2689,	0.4969 s / batch. (data: 1.05e-02). ETA=7:06:21, max mem: 11.4 GB 
[10/28 10:00:18 visual_prompt]: Epoch 7 / 100: avg data time: 3.91e-01, avg batch time: 0.8859, average train loss: 0.8567
[10/28 10:01:10 visual_prompt]: Inference (val):avg data time: 2.86e-04, avg batch time: 0.1916, average loss: 0.7585
[10/28 10:01:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.63	
[10/28 10:01:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/28 10:02:40 visual_prompt]: 	Training 100/553. train loss: 0.8450,	0.4958 s / batch. (data: 2.54e-04). ETA=7:04:09, max mem: 11.4 GB 
[10/28 10:04:10 visual_prompt]: 	Training 200/553. train loss: 1.5341,	0.4858 s / batch. (data: 5.39e-03). ETA=6:54:45, max mem: 11.4 GB 
[10/28 10:05:39 visual_prompt]: 	Training 300/553. train loss: 1.0538,	0.4920 s / batch. (data: 2.88e-04). ETA=6:59:15, max mem: 11.4 GB 
[10/28 10:07:08 visual_prompt]: 	Training 400/553. train loss: 0.7418,	0.4889 s / batch. (data: 1.00e-02). ETA=6:55:45, max mem: 11.4 GB 
[10/28 10:08:37 visual_prompt]: 	Training 500/553. train loss: 1.1893,	1.9679 s / batch. (data: 1.47e+00). ETA=1 day, 3:50:23, max mem: 11.4 GB 
[10/28 10:09:22 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8886, average train loss: 0.9517
[10/28 10:10:14 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1912, average loss: 1.2860
[10/28 10:10:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.87	
[10/28 10:10:14 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/28 10:11:46 visual_prompt]: 	Training 100/553. train loss: 0.3952,	0.5068 s / batch. (data: 1.20e-02). ETA=7:08:55, max mem: 11.4 GB 
[10/28 10:13:14 visual_prompt]: 	Training 200/553. train loss: 0.6730,	0.4847 s / batch. (data: 5.40e-03). ETA=6:49:23, max mem: 11.4 GB 
[10/28 10:14:42 visual_prompt]: 	Training 300/553. train loss: 0.6648,	2.0395 s / batch. (data: 1.54e+00). ETA=1 day, 4:39:07, max mem: 11.4 GB 
[10/28 10:16:11 visual_prompt]: 	Training 400/553. train loss: 0.6344,	0.4966 s / batch. (data: 5.40e-03). ETA=6:57:48, max mem: 11.4 GB 
[10/28 10:17:41 visual_prompt]: 	Training 500/553. train loss: 0.6567,	1.2160 s / batch. (data: 7.19e-01). ETA=17:00:57, max mem: 11.4 GB 
[10/28 10:18:25 visual_prompt]: Epoch 9 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 0.8650
[10/28 10:19:18 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1890, average loss: 1.1003
[10/28 10:19:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.82	
[10/28 10:19:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/28 10:20:52 visual_prompt]: 	Training 100/553. train loss: 1.1636,	0.5084 s / batch. (data: 2.06e-02). ETA=7:05:33, max mem: 11.4 GB 
[10/28 10:22:20 visual_prompt]: 	Training 200/553. train loss: 0.5975,	0.4926 s / batch. (data: 5.38e-03). ETA=6:51:32, max mem: 11.4 GB 
[10/28 10:23:48 visual_prompt]: 	Training 300/553. train loss: 0.7290,	0.7370 s / batch. (data: 2.60e-01). ETA=10:14:27, max mem: 11.4 GB 
[10/28 10:25:16 visual_prompt]: 	Training 400/553. train loss: 0.9802,	1.2880 s / batch. (data: 7.86e-01). ETA=17:51:40, max mem: 11.4 GB 
[10/28 10:26:46 visual_prompt]: 	Training 500/553. train loss: 1.9685,	1.5941 s / batch. (data: 1.12e+00). ETA=22:03:41, max mem: 11.4 GB 
[10/28 10:27:31 visual_prompt]: Epoch 10 / 100: avg data time: 3.97e-01, avg batch time: 0.8915, average train loss: 0.9873
[10/28 10:28:24 visual_prompt]: Inference (val):avg data time: 3.59e-04, avg batch time: 0.1916, average loss: 0.7052
[10/28 10:28:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.94	
[10/28 10:28:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/28 10:29:57 visual_prompt]: 	Training 100/553. train loss: 1.4108,	0.4914 s / batch. (data: 2.51e-04). ETA=6:46:46, max mem: 11.4 GB 
[10/28 10:31:28 visual_prompt]: 	Training 200/553. train loss: 1.0399,	0.4964 s / batch. (data: 1.20e-02). ETA=6:50:05, max mem: 11.4 GB 
[10/28 10:32:57 visual_prompt]: 	Training 300/553. train loss: 0.1415,	2.7475 s / batch. (data: 2.24e+00). ETA=1 day, 13:45:19, max mem: 11.4 GB 
[10/28 10:34:25 visual_prompt]: 	Training 400/553. train loss: 0.6734,	0.5000 s / batch. (data: 6.78e-04). ETA=6:51:23, max mem: 11.4 GB 
[10/28 10:35:53 visual_prompt]: 	Training 500/553. train loss: 0.8127,	0.5120 s / batch. (data: 2.58e-04). ETA=7:00:25, max mem: 11.4 GB 
[10/28 10:36:38 visual_prompt]: Epoch 11 / 100: avg data time: 3.98e-01, avg batch time: 0.8933, average train loss: 0.9356
[10/28 10:37:30 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1904, average loss: 0.9351
[10/28 10:37:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.27	
[10/28 10:37:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/28 10:39:04 visual_prompt]: 	Training 100/553. train loss: 0.6266,	1.0834 s / batch. (data: 5.91e-01). ETA=14:46:51, max mem: 11.4 GB 
[10/28 10:40:34 visual_prompt]: 	Training 200/553. train loss: 0.8905,	0.7752 s / batch. (data: 2.84e-01). ETA=10:33:16, max mem: 11.4 GB 
[10/28 10:42:01 visual_prompt]: 	Training 300/553. train loss: 0.5817,	0.5116 s / batch. (data: 1.16e-02). ETA=6:57:06, max mem: 11.4 GB 
[10/28 10:43:30 visual_prompt]: 	Training 400/553. train loss: 0.6598,	0.4942 s / batch. (data: 3.64e-04). ETA=6:42:05, max mem: 11.4 GB 
[10/28 10:45:00 visual_prompt]: 	Training 500/553. train loss: 2.3827,	0.5041 s / batch. (data: 7.60e-04). ETA=6:49:19, max mem: 11.4 GB 
[10/28 10:45:44 visual_prompt]: Epoch 12 / 100: avg data time: 3.98e-01, avg batch time: 0.8927, average train loss: 0.8586
[10/28 10:46:37 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1925, average loss: 1.9768
[10/28 10:46:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.39	
[10/28 10:46:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/28 10:48:11 visual_prompt]: 	Training 100/553. train loss: 0.7887,	2.3636 s / batch. (data: 1.89e+00). ETA=1 day, 7:53:08, max mem: 11.4 GB 
[10/28 10:49:36 visual_prompt]: 	Training 200/553. train loss: 0.7311,	0.4920 s / batch. (data: 2.84e-04). ETA=6:37:22, max mem: 11.4 GB 
[10/28 10:51:06 visual_prompt]: 	Training 300/553. train loss: 0.6320,	1.5058 s / batch. (data: 1.02e+00). ETA=20:13:45, max mem: 11.4 GB 
[10/28 10:52:34 visual_prompt]: 	Training 400/553. train loss: 1.7611,	0.4989 s / batch. (data: 2.77e-04). ETA=6:41:21, max mem: 11.4 GB 
[10/28 10:54:04 visual_prompt]: 	Training 500/553. train loss: 0.6819,	0.4785 s / batch. (data: 2.31e-04). ETA=6:24:07, max mem: 11.4 GB 
[10/28 10:54:50 visual_prompt]: Epoch 13 / 100: avg data time: 3.97e-01, avg batch time: 0.8916, average train loss: 0.8532
[10/28 10:55:42 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1896, average loss: 0.8049
[10/28 10:55:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.50	
[10/28 10:55:42 visual_prompt]: Best epoch 13: best metric: -0.805
[10/28 10:55:42 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/28 10:57:15 visual_prompt]: 	Training 100/553. train loss: 0.6677,	0.4880 s / batch. (data: 2.99e-04). ETA=6:30:28, max mem: 11.4 GB 
[10/28 10:58:44 visual_prompt]: 	Training 200/553. train loss: 0.7951,	0.7028 s / batch. (data: 2.12e-01). ETA=9:21:10, max mem: 11.4 GB 
[10/28 11:00:13 visual_prompt]: 	Training 300/553. train loss: 0.9969,	1.2278 s / batch. (data: 7.47e-01). ETA=16:18:21, max mem: 11.4 GB 
[10/28 11:01:42 visual_prompt]: 	Training 400/553. train loss: 0.8308,	0.5000 s / batch. (data: 7.96e-03). ETA=6:37:34, max mem: 11.4 GB 
[10/28 11:03:11 visual_prompt]: 	Training 500/553. train loss: 1.7512,	0.4943 s / batch. (data: 5.39e-03). ETA=6:32:14, max mem: 11.4 GB 
[10/28 11:03:56 visual_prompt]: Epoch 14 / 100: avg data time: 3.97e-01, avg batch time: 0.8921, average train loss: 0.8428
[10/28 11:04:48 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1917, average loss: 0.6814
[10/28 11:04:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.14	
[10/28 11:04:48 visual_prompt]: Best epoch 14: best metric: -0.681
[10/28 11:04:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/28 11:06:20 visual_prompt]: 	Training 100/553. train loss: 0.7164,	0.5085 s / batch. (data: 1.63e-02). ETA=6:42:14, max mem: 11.4 GB 
[10/28 11:07:48 visual_prompt]: 	Training 200/553. train loss: 1.6317,	0.4803 s / batch. (data: 2.85e-04). ETA=6:19:06, max mem: 11.4 GB 
[10/28 11:09:18 visual_prompt]: 	Training 300/553. train loss: 0.5621,	0.5035 s / batch. (data: 2.48e-04). ETA=6:36:35, max mem: 11.4 GB 
[10/28 11:10:46 visual_prompt]: 	Training 400/553. train loss: 0.5648,	0.5195 s / batch. (data: 2.78e-04). ETA=6:48:19, max mem: 11.4 GB 
[10/28 11:12:16 visual_prompt]: 	Training 500/553. train loss: 0.7607,	0.4905 s / batch. (data: 3.01e-04). ETA=6:24:42, max mem: 11.4 GB 
[10/28 11:13:03 visual_prompt]: Epoch 15 / 100: avg data time: 3.99e-01, avg batch time: 0.8935, average train loss: 0.8765
[10/28 11:13:55 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1916, average loss: 0.8302
[10/28 11:13:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.97	
[10/28 11:13:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/28 11:15:26 visual_prompt]: 	Training 100/553. train loss: 0.4890,	0.5251 s / batch. (data: 2.62e-02). ETA=6:50:27, max mem: 11.4 GB 
[10/28 11:16:56 visual_prompt]: 	Training 200/553. train loss: 1.0049,	0.4825 s / batch. (data: 2.65e-04). ETA=6:16:25, max mem: 11.4 GB 
[10/28 11:18:25 visual_prompt]: 	Training 300/553. train loss: 0.7792,	0.4831 s / batch. (data: 5.38e-03). ETA=6:16:04, max mem: 11.4 GB 
[10/28 11:19:54 visual_prompt]: 	Training 400/553. train loss: 0.7195,	0.5118 s / batch. (data: 1.24e-02). ETA=6:37:34, max mem: 11.4 GB 
[10/28 11:21:23 visual_prompt]: 	Training 500/553. train loss: 1.0800,	2.1279 s / batch. (data: 1.62e+00). ETA=1 day, 3:29:19, max mem: 11.4 GB 
[10/28 11:22:09 visual_prompt]: Epoch 16 / 100: avg data time: 3.97e-01, avg batch time: 0.8924, average train loss: 0.8994
[10/28 11:23:01 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1890, average loss: 0.6914
[10/28 11:23:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.53	
[10/28 11:23:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/28 11:24:33 visual_prompt]: 	Training 100/553. train loss: 0.4945,	0.4960 s / batch. (data: 2.58e-04). ETA=6:23:10, max mem: 11.4 GB 
[10/28 11:26:03 visual_prompt]: 	Training 200/553. train loss: 1.0948,	0.4930 s / batch. (data: 1.05e-02). ETA=6:20:04, max mem: 11.4 GB 
[10/28 11:27:32 visual_prompt]: 	Training 300/553. train loss: 1.2403,	0.5047 s / batch. (data: 2.44e-02). ETA=6:28:12, max mem: 11.4 GB 
[10/28 11:29:00 visual_prompt]: 	Training 400/553. train loss: 1.2021,	1.3690 s / batch. (data: 8.74e-01). ETA=17:30:46, max mem: 11.4 GB 
[10/28 11:30:28 visual_prompt]: 	Training 500/553. train loss: 0.8690,	2.1705 s / batch. (data: 1.68e+00). ETA=1 day, 3:42:19, max mem: 11.4 GB 
[10/28 11:31:14 visual_prompt]: Epoch 17 / 100: avg data time: 3.95e-01, avg batch time: 0.8915, average train loss: 0.8508
[10/28 11:32:07 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1925, average loss: 0.7313
[10/28 11:32:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.77	
[10/28 11:32:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/28 11:33:40 visual_prompt]: 	Training 100/553. train loss: 0.7458,	0.4840 s / batch. (data: 2.49e-04). ETA=6:09:27, max mem: 11.4 GB 
[10/28 11:35:11 visual_prompt]: 	Training 200/553. train loss: 0.5865,	0.5117 s / batch. (data: 7.46e-04). ETA=6:29:45, max mem: 11.4 GB 
[10/28 11:36:40 visual_prompt]: 	Training 300/553. train loss: 0.6555,	0.4786 s / batch. (data: 2.64e-04). ETA=6:03:41, max mem: 11.4 GB 
[10/28 11:38:09 visual_prompt]: 	Training 400/553. train loss: 0.7650,	0.5199 s / batch. (data: 5.40e-03). ETA=6:34:15, max mem: 11.4 GB 
[10/28 11:39:37 visual_prompt]: 	Training 500/553. train loss: 0.7288,	0.5135 s / batch. (data: 2.05e-02). ETA=6:28:32, max mem: 11.4 GB 
[10/28 11:40:21 visual_prompt]: Epoch 18 / 100: avg data time: 3.99e-01, avg batch time: 0.8929, average train loss: 0.8702
[10/28 11:41:14 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1908, average loss: 0.6904
[10/28 11:41:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.49	
[10/28 11:41:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/28 11:42:46 visual_prompt]: 	Training 100/553. train loss: 0.6422,	0.4784 s / batch. (data: 2.54e-04). ETA=6:00:46, max mem: 11.4 GB 
[10/28 11:44:15 visual_prompt]: 	Training 200/553. train loss: 0.5801,	0.4878 s / batch. (data: 5.34e-03). ETA=6:07:00, max mem: 11.4 GB 
[10/28 11:45:44 visual_prompt]: 	Training 300/553. train loss: 0.8551,	0.4840 s / batch. (data: 2.60e-04). ETA=6:03:22, max mem: 11.4 GB 
[10/28 11:47:15 visual_prompt]: 	Training 400/553. train loss: 0.5702,	0.5285 s / batch. (data: 4.05e-02). ETA=6:35:54, max mem: 11.4 GB 
[10/28 11:48:39 visual_prompt]: 	Training 500/553. train loss: 1.0989,	0.5000 s / batch. (data: 4.00e-03). ETA=6:13:42, max mem: 11.4 GB 
[10/28 11:49:26 visual_prompt]: Epoch 19 / 100: avg data time: 3.97e-01, avg batch time: 0.8910, average train loss: 0.8075
[10/28 11:50:19 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.1920, average loss: 0.9422
[10/28 11:50:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.72	
[10/28 11:50:19 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/28 11:51:49 visual_prompt]: 	Training 100/553. train loss: 0.7715,	1.1528 s / batch. (data: 6.66e-01). ETA=14:18:43, max mem: 11.4 GB 
[10/28 11:53:20 visual_prompt]: 	Training 200/553. train loss: 0.5141,	0.5245 s / batch. (data: 1.05e-02). ETA=6:29:47, max mem: 11.4 GB 
[10/28 11:54:49 visual_prompt]: 	Training 300/553. train loss: 0.7848,	0.4963 s / batch. (data: 2.59e-04). ETA=6:08:01, max mem: 11.4 GB 
[10/28 11:56:17 visual_prompt]: 	Training 400/553. train loss: 0.6419,	0.5044 s / batch. (data: 1.19e-02). ETA=6:13:10, max mem: 11.4 GB 
[10/28 11:57:45 visual_prompt]: 	Training 500/553. train loss: 1.6532,	0.5000 s / batch. (data: 7.94e-03). ETA=6:09:06, max mem: 11.4 GB 
[10/28 11:58:32 visual_prompt]: Epoch 20 / 100: avg data time: 3.97e-01, avg batch time: 0.8915, average train loss: 0.8203
[10/28 11:59:25 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1902, average loss: 0.8108
[10/28 11:59:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.92	
[10/28 11:59:25 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[10/28 12:00:59 visual_prompt]: 	Training 100/553. train loss: 0.5803,	1.6200 s / batch. (data: 1.14e+00). ETA=19:51:47, max mem: 11.4 GB 
[10/28 12:02:27 visual_prompt]: 	Training 200/553. train loss: 0.5788,	0.4840 s / batch. (data: 2.79e-04). ETA=5:55:16, max mem: 11.4 GB 
[10/28 12:03:56 visual_prompt]: 	Training 300/553. train loss: 0.8864,	1.5279 s / batch. (data: 1.01e+00). ETA=18:38:56, max mem: 11.4 GB 
[10/28 12:05:25 visual_prompt]: 	Training 400/553. train loss: 0.9870,	0.5296 s / batch. (data: 2.85e-02). ETA=6:26:59, max mem: 11.4 GB 
[10/28 12:06:54 visual_prompt]: 	Training 500/553. train loss: 0.7107,	0.4960 s / batch. (data: 2.66e-04). ETA=6:01:33, max mem: 11.4 GB 
[10/28 12:07:38 visual_prompt]: Epoch 21 / 100: avg data time: 3.98e-01, avg batch time: 0.8929, average train loss: 0.8118
[10/28 12:08:31 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1933, average loss: 0.6955
[10/28 12:08:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 58.23	
[10/28 12:08:31 visual_prompt]: Stopping early.
[10/28 12:08:31 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 12:08:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 12:08:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 12:08:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 12:08:31 visual_prompt]: Training with config:
[10/28 12:08:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 12:08:31 visual_prompt]: Loading training data...
[10/28 12:08:31 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 12:08:31 visual_prompt]: Loading validation data...
[10/28 12:08:31 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 12:08:31 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/28 12:08:34 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/28 12:08:34 visual_prompt]: tuned percent:0.529
[10/28 12:08:34 visual_prompt]: Device used for model: 0
[10/28 12:08:34 visual_prompt]: Setting up Evaluator...
[10/28 12:08:34 visual_prompt]: Setting up Trainer...
[10/28 12:08:34 visual_prompt]: 	Setting up the optimizer...
[10/28 12:08:34 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 12:10:06 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4968 s / batch. (data: 2.83e-04). ETA=7:37:03, max mem: 11.4 GB 
[10/28 12:11:33 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5028 s / batch. (data: 5.37e-03). ETA=7:41:42, max mem: 11.4 GB 
[10/28 12:13:05 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9675 s / batch. (data: 2.49e+00). ETA=1 day, 21:20:09, max mem: 11.4 GB 
[10/28 12:14:32 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5071 s / batch. (data: 2.63e-04). ETA=7:44:02, max mem: 11.4 GB 
[10/28 12:16:04 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4972 s / batch. (data: 5.41e-03). ETA=7:34:07, max mem: 11.4 GB 
[10/28 12:16:50 visual_prompt]: Epoch 1 / 100: avg data time: 4.02e-01, avg batch time: 0.8969, average train loss: 1.3966
[10/28 12:17:43 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1917, average loss: 1.3454
[10/28 12:17:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/28 12:17:43 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/28 12:19:15 visual_prompt]: 	Training 100/553. train loss: 0.6564,	0.4928 s / batch. (data: 2.42e-04). ETA=7:28:49, max mem: 11.4 GB 
[10/28 12:20:43 visual_prompt]: 	Training 200/553. train loss: 0.1701,	0.7086 s / batch. (data: 2.18e-01). ETA=10:44:12, max mem: 11.4 GB 
[10/28 12:22:15 visual_prompt]: 	Training 300/553. train loss: 0.9653,	1.7509 s / batch. (data: 1.27e+00). ETA=1 day, 2:28:51, max mem: 11.4 GB 
[10/28 12:23:42 visual_prompt]: 	Training 400/553. train loss: 1.2155,	0.5040 s / batch. (data: 2.72e-04). ETA=7:36:30, max mem: 11.4 GB 
[10/28 12:25:12 visual_prompt]: 	Training 500/553. train loss: 0.6291,	0.5267 s / batch. (data: 2.26e-02). ETA=7:56:09, max mem: 11.4 GB 
[10/28 12:25:57 visual_prompt]: Epoch 2 / 100: avg data time: 4.00e-01, avg batch time: 0.8945, average train loss: 0.8307
[10/28 12:26:50 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1924, average loss: 0.7514
[10/28 12:26:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.50	
[10/28 12:26:50 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/28 12:28:21 visual_prompt]: 	Training 100/553. train loss: 0.8093,	0.5201 s / batch. (data: 2.92e-02). ETA=7:48:53, max mem: 11.4 GB 
[10/28 12:29:52 visual_prompt]: 	Training 200/553. train loss: 0.7192,	0.5040 s / batch. (data: 2.73e-04). ETA=7:33:33, max mem: 11.4 GB 
[10/28 12:31:19 visual_prompt]: 	Training 300/553. train loss: 0.6006,	0.5041 s / batch. (data: 2.52e-04). ETA=7:32:45, max mem: 11.4 GB 
[10/28 12:32:49 visual_prompt]: 	Training 400/553. train loss: 0.7205,	0.4920 s / batch. (data: 2.58e-04). ETA=7:21:07, max mem: 11.4 GB 
[10/28 12:34:20 visual_prompt]: 	Training 500/553. train loss: 0.7148,	1.8480 s / batch. (data: 1.35e+00). ETA=1 day, 3:33:46, max mem: 11.4 GB 
[10/28 12:35:04 visual_prompt]: Epoch 3 / 100: avg data time: 3.99e-01, avg batch time: 0.8936, average train loss: 0.7859
[10/28 12:35:57 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1907, average loss: 0.7147
[10/28 12:35:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.47	
[10/28 12:35:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/28 12:37:31 visual_prompt]: 	Training 100/553. train loss: 0.7485,	0.4879 s / batch. (data: 2.69e-04). ETA=7:15:25, max mem: 11.4 GB 
[10/28 12:39:00 visual_prompt]: 	Training 200/553. train loss: 1.6391,	0.4871 s / batch. (data: 3.89e-03). ETA=7:13:53, max mem: 11.4 GB 
[10/28 12:40:30 visual_prompt]: 	Training 300/553. train loss: 0.6465,	2.1240 s / batch. (data: 1.63e+00). ETA=1 day, 7:28:15, max mem: 11.4 GB 
[10/28 12:41:55 visual_prompt]: 	Training 400/553. train loss: 0.6134,	1.9320 s / batch. (data: 1.45e+00). ETA=1 day, 4:34:22, max mem: 11.4 GB 
[10/28 12:43:26 visual_prompt]: 	Training 500/553. train loss: 0.7315,	3.7962 s / batch. (data: 3.32e+00). ETA=2 days, 8:02:11, max mem: 11.4 GB 
[10/28 12:44:12 visual_prompt]: Epoch 4 / 100: avg data time: 4.01e-01, avg batch time: 0.8951, average train loss: 0.8940
[10/28 12:45:05 visual_prompt]: Inference (val):avg data time: 1.35e-04, avg batch time: 0.1925, average loss: 0.9671
[10/28 12:45:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.18	
[10/28 12:45:05 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/28 12:46:36 visual_prompt]: 	Training 100/553. train loss: 0.4892,	0.4840 s / batch. (data: 2.56e-04). ETA=7:07:26, max mem: 11.4 GB 
[10/28 12:48:05 visual_prompt]: 	Training 200/553. train loss: 0.6404,	1.8144 s / batch. (data: 1.31e+00). ETA=1 day, 2:39:20, max mem: 11.4 GB 
[10/28 12:49:35 visual_prompt]: 	Training 300/553. train loss: 1.6575,	0.5090 s / batch. (data: 2.33e-04). ETA=7:27:50, max mem: 11.4 GB 
[10/28 12:51:02 visual_prompt]: 	Training 400/553. train loss: 1.1495,	0.4801 s / batch. (data: 2.49e-04). ETA=7:01:33, max mem: 11.4 GB 
[10/28 12:52:31 visual_prompt]: 	Training 500/553. train loss: 0.5013,	0.4800 s / batch. (data: 2.56e-04). ETA=7:00:41, max mem: 11.4 GB 
[10/28 12:53:17 visual_prompt]: Epoch 5 / 100: avg data time: 3.97e-01, avg batch time: 0.8904, average train loss: 0.9977
[10/28 12:54:10 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1906, average loss: 0.8694
[10/28 12:54:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.92	
[10/28 12:54:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/28 12:55:43 visual_prompt]: 	Training 100/553. train loss: 0.5793,	0.5057 s / batch. (data: 7.10e-04). ETA=7:21:54, max mem: 11.4 GB 
[10/28 12:57:11 visual_prompt]: 	Training 200/553. train loss: 0.9041,	0.4957 s / batch. (data: 1.04e-02). ETA=7:12:22, max mem: 11.4 GB 
[10/28 12:58:38 visual_prompt]: 	Training 300/553. train loss: 0.5742,	0.4960 s / batch. (data: 7.96e-03). ETA=7:11:47, max mem: 11.4 GB 
[10/28 13:00:10 visual_prompt]: 	Training 400/553. train loss: 0.6122,	0.4897 s / batch. (data: 1.05e-02). ETA=7:05:32, max mem: 11.4 GB 
[10/28 13:01:37 visual_prompt]: 	Training 500/553. train loss: 1.0647,	1.3834 s / batch. (data: 8.87e-01). ETA=19:59:45, max mem: 11.4 GB 
[10/28 13:02:22 visual_prompt]: Epoch 6 / 100: avg data time: 3.95e-01, avg batch time: 0.8898, average train loss: 0.8667
[10/28 13:03:14 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1919, average loss: 0.7876
[10/28 13:03:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.17	
[10/28 13:03:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/28 13:04:45 visual_prompt]: 	Training 100/553. train loss: 1.0863,	0.4994 s / batch. (data: 5.39e-03). ETA=7:11:52, max mem: 11.4 GB 
[10/28 13:06:14 visual_prompt]: 	Training 200/553. train loss: 0.5224,	0.5010 s / batch. (data: 8.96e-03). ETA=7:12:21, max mem: 11.4 GB 
[10/28 13:07:45 visual_prompt]: 	Training 300/553. train loss: 0.6253,	1.8891 s / batch. (data: 1.39e+00). ETA=1 day, 3:07:14, max mem: 11.4 GB 
[10/28 13:09:14 visual_prompt]: 	Training 400/553. train loss: 0.6152,	2.4643 s / batch. (data: 1.97e+00). ETA=1 day, 11:18:31, max mem: 11.4 GB 
[10/28 13:10:41 visual_prompt]: 	Training 500/553. train loss: 1.3691,	0.4974 s / batch. (data: 2.53e-04). ETA=7:06:47, max mem: 11.4 GB 
[10/28 13:11:25 visual_prompt]: Epoch 7 / 100: avg data time: 3.93e-01, avg batch time: 0.8876, average train loss: 0.8654
[10/28 13:12:18 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1916, average loss: 0.7659
[10/28 13:12:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.65	
[10/28 13:12:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/28 13:13:47 visual_prompt]: 	Training 100/553. train loss: 0.8767,	0.5039 s / batch. (data: 1.04e-02). ETA=7:11:04, max mem: 11.4 GB 
[10/28 13:15:18 visual_prompt]: 	Training 200/553. train loss: 1.3968,	0.4912 s / batch. (data: 2.90e-04). ETA=6:59:25, max mem: 11.4 GB 
[10/28 13:16:47 visual_prompt]: 	Training 300/553. train loss: 1.1168,	0.5071 s / batch. (data: 5.39e-03). ETA=7:12:07, max mem: 11.4 GB 
[10/28 13:18:16 visual_prompt]: 	Training 400/553. train loss: 0.7460,	1.3360 s / batch. (data: 8.33e-01). ETA=18:56:13, max mem: 11.4 GB 
[10/28 13:19:45 visual_prompt]: 	Training 500/553. train loss: 1.2996,	1.9773 s / batch. (data: 1.48e+00). ETA=1 day, 3:58:21, max mem: 11.4 GB 
[10/28 13:20:30 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8898, average train loss: 0.9801
[10/28 13:21:22 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1918, average loss: 1.2894
[10/28 13:21:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.40	
[10/28 13:21:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/28 13:22:54 visual_prompt]: 	Training 100/553. train loss: 0.2652,	0.4881 s / batch. (data: 7.96e-03). ETA=6:53:02, max mem: 11.4 GB 
[10/28 13:24:22 visual_prompt]: 	Training 200/553. train loss: 0.7263,	0.5080 s / batch. (data: 2.87e-04). ETA=7:09:02, max mem: 11.4 GB 
[10/28 13:25:52 visual_prompt]: 	Training 300/553. train loss: 0.5716,	2.3353 s / batch. (data: 1.83e+00). ETA=1 day, 8:48:28, max mem: 11.4 GB 
[10/28 13:27:20 visual_prompt]: 	Training 400/553. train loss: 0.6289,	0.9360 s / batch. (data: 4.46e-01). ETA=13:07:27, max mem: 11.4 GB 
[10/28 13:28:50 visual_prompt]: 	Training 500/553. train loss: 0.6341,	0.5000 s / batch. (data: 2.92e-04). ETA=6:59:47, max mem: 11.4 GB 
[10/28 13:29:34 visual_prompt]: Epoch 9 / 100: avg data time: 3.95e-01, avg batch time: 0.8894, average train loss: 0.8904
[10/28 13:30:27 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1904, average loss: 1.2370
[10/28 13:30:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.56	
[10/28 13:30:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/28 13:32:01 visual_prompt]: 	Training 100/553. train loss: 1.5457,	0.5042 s / batch. (data: 1.05e-02). ETA=7:02:01, max mem: 11.4 GB 
[10/28 13:33:28 visual_prompt]: 	Training 200/553. train loss: 0.6959,	0.5119 s / batch. (data: 5.41e-03). ETA=7:07:36, max mem: 11.4 GB 
[10/28 13:34:57 visual_prompt]: 	Training 300/553. train loss: 0.7510,	0.5200 s / batch. (data: 7.96e-03). ETA=7:13:29, max mem: 11.4 GB 
[10/28 13:36:23 visual_prompt]: 	Training 400/553. train loss: 0.7888,	0.9833 s / batch. (data: 4.79e-01). ETA=13:38:09, max mem: 11.4 GB 
[10/28 13:37:54 visual_prompt]: 	Training 500/553. train loss: 1.0945,	1.8128 s / batch. (data: 1.32e+00). ETA=1 day, 1:05:17, max mem: 11.4 GB 
[10/28 13:38:39 visual_prompt]: Epoch 10 / 100: avg data time: 3.95e-01, avg batch time: 0.8902, average train loss: 1.0452
[10/28 13:39:31 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1914, average loss: 0.7252
[10/28 13:39:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.22	
[10/28 13:39:31 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/28 13:41:05 visual_prompt]: 	Training 100/553. train loss: 1.3957,	0.5004 s / batch. (data: 1.20e-02). ETA=6:54:16, max mem: 11.4 GB 
[10/28 13:42:35 visual_prompt]: 	Training 200/553. train loss: 1.3680,	0.5040 s / batch. (data: 2.68e-04). ETA=6:56:22, max mem: 11.4 GB 
[10/28 13:44:03 visual_prompt]: 	Training 300/553. train loss: 0.1721,	1.2879 s / batch. (data: 8.11e-01). ETA=17:41:52, max mem: 11.4 GB 
[10/28 13:45:30 visual_prompt]: 	Training 400/553. train loss: 0.7696,	0.4840 s / batch. (data: 2.93e-04). ETA=6:38:14, max mem: 11.4 GB 
[10/28 13:46:58 visual_prompt]: 	Training 500/553. train loss: 0.9083,	0.4880 s / batch. (data: 6.18e-03). ETA=6:40:42, max mem: 11.4 GB 
[10/28 13:47:43 visual_prompt]: Epoch 11 / 100: avg data time: 3.92e-01, avg batch time: 0.8890, average train loss: 1.0056
[10/28 13:48:36 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1918, average loss: 0.8287
[10/28 13:48:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.20	
[10/28 13:48:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/28 13:50:09 visual_prompt]: 	Training 100/553. train loss: 0.5987,	1.1473 s / batch. (data: 6.62e-01). ETA=15:39:09, max mem: 11.4 GB 
[10/28 13:51:38 visual_prompt]: 	Training 200/553. train loss: 1.0305,	0.5049 s / batch. (data: 1.29e-02). ETA=6:52:30, max mem: 11.4 GB 
[10/28 13:53:06 visual_prompt]: 	Training 300/553. train loss: 0.5740,	0.4965 s / batch. (data: 1.20e-02). ETA=6:44:49, max mem: 11.4 GB 
[10/28 13:54:34 visual_prompt]: 	Training 400/553. train loss: 0.6578,	0.4886 s / batch. (data: 2.82e-04). ETA=6:37:31, max mem: 11.4 GB 
[10/28 13:56:04 visual_prompt]: 	Training 500/553. train loss: 3.4496,	0.5373 s / batch. (data: 5.96e-03). ETA=7:16:16, max mem: 11.4 GB 
[10/28 13:56:48 visual_prompt]: Epoch 12 / 100: avg data time: 3.95e-01, avg batch time: 0.8892, average train loss: 0.9384
[10/28 13:57:40 visual_prompt]: Inference (val):avg data time: 1.96e-04, avg batch time: 0.1907, average loss: 2.2359
[10/28 13:57:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.49	
[10/28 13:57:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/28 13:59:13 visual_prompt]: 	Training 100/553. train loss: 0.5069,	0.5080 s / batch. (data: 2.68e-04). ETA=6:51:11, max mem: 11.4 GB 
[10/28 14:00:38 visual_prompt]: 	Training 200/553. train loss: 0.6386,	0.5160 s / batch. (data: 2.82e-04). ETA=6:56:47, max mem: 11.4 GB 
[10/28 14:02:09 visual_prompt]: 	Training 300/553. train loss: 0.6238,	2.0520 s / batch. (data: 1.55e+00). ETA=1 day, 3:34:02, max mem: 11.4 GB 
[10/28 14:03:35 visual_prompt]: 	Training 400/553. train loss: 3.0482,	0.4848 s / batch. (data: 2.85e-04). ETA=6:29:58, max mem: 11.4 GB 
[10/28 14:05:06 visual_prompt]: 	Training 500/553. train loss: 1.0212,	0.5218 s / batch. (data: 5.42e-03). ETA=6:58:51, max mem: 11.4 GB 
[10/28 14:05:51 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8874, average train loss: 1.0091
[10/28 14:06:43 visual_prompt]: Inference (val):avg data time: 1.93e-04, avg batch time: 0.1920, average loss: 1.0674
[10/28 14:06:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.28	
[10/28 14:06:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/28 14:08:16 visual_prompt]: 	Training 100/553. train loss: 0.6988,	0.4919 s / batch. (data: 5.40e-03). ETA=6:33:36, max mem: 11.4 GB 
[10/28 14:09:44 visual_prompt]: 	Training 200/553. train loss: 1.4245,	0.6440 s / batch. (data: 1.42e-01). ETA=8:34:13, max mem: 11.4 GB 
[10/28 14:11:14 visual_prompt]: 	Training 300/553. train loss: 0.5142,	1.4156 s / batch. (data: 9.33e-01). ETA=18:48:01, max mem: 11.4 GB 
[10/28 14:12:42 visual_prompt]: 	Training 400/553. train loss: 0.9103,	0.5282 s / batch. (data: 2.41e-02). ETA=6:59:59, max mem: 11.4 GB 
[10/28 14:14:11 visual_prompt]: 	Training 500/553. train loss: 2.1249,	0.5122 s / batch. (data: 1.05e-02). ETA=6:46:24, max mem: 11.4 GB 
[10/28 14:14:55 visual_prompt]: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 0.8956
[10/28 14:15:47 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1921, average loss: 0.7019
[10/28 14:15:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 60.93	
[10/28 14:15:47 visual_prompt]: Best epoch 14: best metric: -0.702
[10/28 14:15:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/28 14:17:19 visual_prompt]: 	Training 100/553. train loss: 0.7702,	0.7090 s / batch. (data: 2.00e-01). ETA=9:20:45, max mem: 11.4 GB 
[10/28 14:18:46 visual_prompt]: 	Training 200/553. train loss: 0.5853,	0.5041 s / batch. (data: 5.42e-03). ETA=6:37:53, max mem: 11.4 GB 
[10/28 14:20:16 visual_prompt]: 	Training 300/553. train loss: 0.7137,	0.4946 s / batch. (data: 2.66e-04). ETA=6:29:31, max mem: 11.4 GB 
[10/28 14:21:43 visual_prompt]: 	Training 400/553. train loss: 0.4182,	0.5105 s / batch. (data: 2.94e-04). ETA=6:41:12, max mem: 11.4 GB 
[10/28 14:23:12 visual_prompt]: 	Training 500/553. train loss: 0.5351,	0.4912 s / batch. (data: 2.96e-04). ETA=6:25:14, max mem: 11.4 GB 
[10/28 14:23:59 visual_prompt]: Epoch 15 / 100: avg data time: 3.94e-01, avg batch time: 0.8889, average train loss: 0.9729
[10/28 14:24:51 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1894, average loss: 1.2437
[10/28 14:24:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.35	
[10/28 14:24:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/28 14:26:21 visual_prompt]: 	Training 100/553. train loss: 0.3461,	0.4950 s / batch. (data: 2.71e-04). ETA=6:26:57, max mem: 11.4 GB 
[10/28 14:27:51 visual_prompt]: 	Training 200/553. train loss: 0.8664,	0.5160 s / batch. (data: 2.69e-04). ETA=6:42:31, max mem: 11.4 GB 
[10/28 14:29:19 visual_prompt]: 	Training 300/553. train loss: 0.9608,	0.5171 s / batch. (data: 2.12e-02). ETA=6:42:29, max mem: 11.4 GB 
[10/28 14:30:47 visual_prompt]: 	Training 400/553. train loss: 0.6390,	0.4960 s / batch. (data: 3.18e-04). ETA=6:25:14, max mem: 11.4 GB 
[10/28 14:32:16 visual_prompt]: 	Training 500/553. train loss: 0.7700,	1.9280 s / batch. (data: 1.44e+00). ETA=1 day, 0:54:19, max mem: 11.4 GB 
[10/28 14:33:01 visual_prompt]: Epoch 16 / 100: avg data time: 3.92e-01, avg batch time: 0.8870, average train loss: 0.8608
[10/28 14:33:54 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1913, average loss: 0.6668
[10/28 14:33:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 61.08	
[10/28 14:33:54 visual_prompt]: Best epoch 16: best metric: -0.667
[10/28 14:33:54 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/28 14:35:24 visual_prompt]: 	Training 100/553. train loss: 0.4233,	0.5000 s / batch. (data: 7.95e-03). ETA=6:26:16, max mem: 11.4 GB 
[10/28 14:36:55 visual_prompt]: 	Training 200/553. train loss: 1.0285,	0.4835 s / batch. (data: 5.43e-03). ETA=6:12:44, max mem: 11.4 GB 
[10/28 14:38:24 visual_prompt]: 	Training 300/553. train loss: 1.5760,	0.5000 s / batch. (data: 2.57e-04). ETA=6:24:34, max mem: 11.4 GB 
[10/28 14:39:51 visual_prompt]: 	Training 400/553. train loss: 0.9464,	1.2360 s / batch. (data: 7.58e-01). ETA=15:48:40, max mem: 11.4 GB 
[10/28 14:41:19 visual_prompt]: 	Training 500/553. train loss: 1.0170,	1.9132 s / batch. (data: 1.40e+00). ETA=1 day, 0:25:16, max mem: 11.4 GB 
[10/28 14:42:06 visual_prompt]: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8893, average train loss: 0.8559
[10/28 14:42:58 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1914, average loss: 0.6648
[10/28 14:42:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 61.22	
[10/28 14:42:58 visual_prompt]: Best epoch 17: best metric: -0.665
[10/28 14:42:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/28 14:44:30 visual_prompt]: 	Training 100/553. train loss: 0.8332,	0.4948 s / batch. (data: 3.01e-04). ETA=6:17:41, max mem: 11.4 GB 
[10/28 14:46:01 visual_prompt]: 	Training 200/553. train loss: 0.4453,	0.4866 s / batch. (data: 2.54e-04). ETA=6:10:39, max mem: 11.4 GB 
[10/28 14:47:30 visual_prompt]: 	Training 300/553. train loss: 0.5111,	0.4868 s / batch. (data: 2.87e-04). ETA=6:09:57, max mem: 11.4 GB 
[10/28 14:48:58 visual_prompt]: 	Training 400/553. train loss: 0.7156,	0.4789 s / batch. (data: 2.51e-04). ETA=6:03:08, max mem: 11.4 GB 
[10/28 14:50:26 visual_prompt]: 	Training 500/553. train loss: 1.0939,	0.4915 s / batch. (data: 2.33e-04). ETA=6:11:53, max mem: 11.4 GB 
[10/28 14:51:10 visual_prompt]: Epoch 18 / 100: avg data time: 3.94e-01, avg batch time: 0.8895, average train loss: 0.9268
[10/28 14:52:03 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1917, average loss: 0.6804
[10/28 14:52:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 58.98	
[10/28 14:52:03 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/28 14:53:36 visual_prompt]: 	Training 100/553. train loss: 0.6206,	2.4001 s / batch. (data: 1.91e+00). ETA=1 day, 6:09:53, max mem: 11.4 GB 
[10/28 14:55:04 visual_prompt]: 	Training 200/553. train loss: 0.4957,	0.5098 s / batch. (data: 2.05e-02). ETA=6:23:33, max mem: 11.4 GB 
[10/28 14:56:32 visual_prompt]: 	Training 300/553. train loss: 1.6911,	0.5115 s / batch. (data: 2.73e-04). ETA=6:23:58, max mem: 11.4 GB 
[10/28 14:58:01 visual_prompt]: 	Training 400/553. train loss: 0.5387,	0.5003 s / batch. (data: 8.25e-03). ETA=6:14:46, max mem: 11.4 GB 
[10/28 14:59:26 visual_prompt]: 	Training 500/553. train loss: 1.6879,	0.5170 s / batch. (data: 2.06e-02). ETA=6:26:23, max mem: 11.4 GB 
[10/28 15:00:13 visual_prompt]: Epoch 19 / 100: avg data time: 3.91e-01, avg batch time: 0.8862, average train loss: 0.8201
[10/28 15:01:06 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.1902, average loss: 0.9106
[10/28 15:01:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.02	
[10/28 15:01:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/28 15:02:36 visual_prompt]: 	Training 100/553. train loss: 0.5531,	0.4961 s / batch. (data: 5.40e-03). ETA=6:09:31, max mem: 11.4 GB 
[10/28 15:04:06 visual_prompt]: 	Training 200/553. train loss: 0.4550,	0.5000 s / batch. (data: 2.84e-04). ETA=6:11:35, max mem: 11.4 GB 
[10/28 15:05:34 visual_prompt]: 	Training 300/553. train loss: 0.9424,	0.4848 s / batch. (data: 5.39e-03). ETA=5:59:31, max mem: 11.4 GB 
[10/28 15:07:03 visual_prompt]: 	Training 400/553. train loss: 0.4622,	0.5000 s / batch. (data: 2.88e-04). ETA=6:09:57, max mem: 11.4 GB 
[10/28 15:08:31 visual_prompt]: 	Training 500/553. train loss: 1.0337,	0.4910 s / batch. (data: 2.82e-04). ETA=6:02:27, max mem: 11.4 GB 
[10/28 15:09:18 visual_prompt]: Epoch 20 / 100: avg data time: 3.96e-01, avg batch time: 0.8901, average train loss: 0.8390
[10/28 15:10:10 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1925, average loss: 0.9768
[10/28 15:10:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.76	
[10/28 15:10:10 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[10/28 15:11:45 visual_prompt]: 	Training 100/553. train loss: 0.5176,	1.7120 s / batch. (data: 1.22e+00). ETA=20:59:26, max mem: 11.4 GB 
[10/28 15:13:12 visual_prompt]: 	Training 200/553. train loss: 0.5676,	0.4855 s / batch. (data: 2.11e-04). ETA=5:56:19, max mem: 11.4 GB 
[10/28 15:14:40 visual_prompt]: 	Training 300/553. train loss: 1.5001,	1.2680 s / batch. (data: 7.52e-01). ETA=15:28:36, max mem: 11.4 GB 
[10/28 15:16:08 visual_prompt]: 	Training 400/553. train loss: 0.8132,	0.5120 s / batch. (data: 2.42e-04). ETA=6:14:07, max mem: 11.4 GB 
[10/28 15:17:37 visual_prompt]: 	Training 500/553. train loss: 0.8888,	0.4880 s / batch. (data: 7.99e-03). ETA=5:55:46, max mem: 11.4 GB 
[10/28 15:18:22 visual_prompt]: Epoch 21 / 100: avg data time: 3.93e-01, avg batch time: 0.8882, average train loss: 0.9148
[10/28 15:19:14 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1905, average loss: 0.6858
[10/28 15:19:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.47	
[10/28 15:19:14 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[10/28 15:20:45 visual_prompt]: 	Training 100/553. train loss: 0.6059,	0.5001 s / batch. (data: 1.63e-02). ETA=6:03:19, max mem: 11.4 GB 
[10/28 15:22:14 visual_prompt]: 	Training 200/553. train loss: 0.6141,	0.4966 s / batch. (data: 2.61e-04). ETA=5:59:56, max mem: 11.4 GB 
[10/28 15:23:40 visual_prompt]: 	Training 300/553. train loss: 0.4212,	0.4924 s / batch. (data: 5.42e-03). ETA=5:56:04, max mem: 11.4 GB 
[10/28 15:25:10 visual_prompt]: 	Training 400/553. train loss: 0.5131,	0.5000 s / batch. (data: 2.57e-04). ETA=6:00:42, max mem: 11.4 GB 
[10/28 15:26:39 visual_prompt]: 	Training 500/553. train loss: 0.7181,	0.5080 s / batch. (data: 8.00e-03). ETA=6:05:39, max mem: 11.4 GB 
[10/28 15:27:26 visual_prompt]: Epoch 22 / 100: avg data time: 3.94e-01, avg batch time: 0.8889, average train loss: 0.8116
[10/28 15:28:18 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1905, average loss: 0.7116
[10/28 15:28:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 62.35	
[10/28 15:28:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[10/28 15:29:51 visual_prompt]: 	Training 100/553. train loss: 1.1739,	1.4173 s / batch. (data: 9.28e-01). ETA=16:56:31, max mem: 11.4 GB 
[10/28 15:31:21 visual_prompt]: 	Training 200/553. train loss: 1.0260,	1.0040 s / batch. (data: 4.99e-01). ETA=11:58:26, max mem: 11.4 GB 
[10/28 15:32:51 visual_prompt]: 	Training 300/553. train loss: 1.0002,	0.5160 s / batch. (data: 7.19e-04). ETA=6:08:21, max mem: 11.4 GB 
[10/28 15:34:17 visual_prompt]: 	Training 400/553. train loss: 0.5556,	0.4800 s / batch. (data: 2.77e-04). ETA=5:41:51, max mem: 11.4 GB 
[10/28 15:35:43 visual_prompt]: 	Training 500/553. train loss: 1.1220,	0.5113 s / batch. (data: 1.05e-02). ETA=6:03:18, max mem: 11.4 GB 
[10/28 15:36:30 visual_prompt]: Epoch 23 / 100: avg data time: 3.94e-01, avg batch time: 0.8881, average train loss: 0.8448
[10/28 15:37:22 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1904, average loss: 0.7156
[10/28 15:37:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.40	
[10/28 15:37:22 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[10/28 15:38:51 visual_prompt]: 	Training 100/553. train loss: 0.8880,	0.5451 s / batch. (data: 9.20e-03). ETA=6:25:56, max mem: 11.4 GB 
[10/28 15:40:20 visual_prompt]: 	Training 200/553. train loss: 0.7511,	0.5000 s / batch. (data: 7.61e-04). ETA=5:53:09, max mem: 11.4 GB 
[10/28 15:41:49 visual_prompt]: 	Training 300/553. train loss: 0.7071,	1.6960 s / batch. (data: 1.20e+00). ETA=19:55:07, max mem: 11.4 GB 
[10/28 15:43:18 visual_prompt]: 	Training 400/553. train loss: 0.5993,	0.5183 s / batch. (data: 5.42e-03). ETA=6:04:21, max mem: 11.4 GB 
[10/28 15:44:48 visual_prompt]: 	Training 500/553. train loss: 0.5889,	0.7475 s / batch. (data: 2.46e-01). ETA=8:44:14, max mem: 11.4 GB 
[10/28 15:45:35 visual_prompt]: Epoch 24 / 100: avg data time: 3.96e-01, avg batch time: 0.8910, average train loss: 0.8107
[10/28 15:46:28 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1908, average loss: 0.8193
[10/28 15:46:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 60.54	
[10/28 15:46:28 visual_prompt]: Stopping early.
[10/28 15:46:28 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 15:46:28 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 15:46:28 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 15:46:28 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 15:46:28 visual_prompt]: Training with config:
[10/28 15:46:28 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.1_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 15:46:28 visual_prompt]: Loading training data...
[10/28 15:46:28 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 15:46:28 visual_prompt]: Loading validation data...
[10/28 15:46:28 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 15:46:28 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/28 15:46:30 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/28 15:46:30 visual_prompt]: tuned percent:0.529
[10/28 15:46:30 visual_prompt]: Device used for model: 0
[10/28 15:46:30 visual_prompt]: Setting up Evaluator...
[10/28 15:46:30 visual_prompt]: Setting up Trainer...
[10/28 15:46:30 visual_prompt]: 	Setting up the optimizer...
[10/28 15:46:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 15:48:02 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4901 s / batch. (data: 2.71e-04). ETA=7:30:53, max mem: 11.4 GB 
[10/28 15:49:29 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4880 s / batch. (data: 2.57e-04). ETA=7:28:08, max mem: 11.4 GB 
[10/28 15:51:02 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9026 s / batch. (data: 2.43e+00). ETA=1 day, 20:20:44, max mem: 11.4 GB 
[10/28 15:52:28 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5156 s / batch. (data: 2.79e-04). ETA=7:51:46, max mem: 11.4 GB 
[10/28 15:54:00 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4950 s / batch. (data: 3.69e-04). ETA=7:32:07, max mem: 11.4 GB 
[10/28 15:54:46 visual_prompt]: Epoch 1 / 100: avg data time: 4.01e-01, avg batch time: 0.8961, average train loss: 1.3966
[10/28 15:55:38 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1918, average loss: 1.3454
[10/28 15:55:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/28 15:55:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/28 15:57:10 visual_prompt]: 	Training 100/553. train loss: 0.6600,	0.5036 s / batch. (data: 1.19e-02). ETA=7:38:40, max mem: 11.4 GB 
[10/28 15:58:38 visual_prompt]: 	Training 200/553. train loss: 0.2485,	1.3547 s / batch. (data: 8.58e-01). ETA=20:31:36, max mem: 11.4 GB 
[10/28 16:00:10 visual_prompt]: 	Training 300/553. train loss: 0.7184,	1.6719 s / batch. (data: 1.16e+00). ETA=1 day, 1:17:12, max mem: 11.4 GB 
[10/28 16:01:37 visual_prompt]: 	Training 400/553. train loss: 0.8895,	0.5201 s / batch. (data: 2.81e-02). ETA=7:51:07, max mem: 11.4 GB 
[10/28 16:03:08 visual_prompt]: 	Training 500/553. train loss: 0.6350,	0.5410 s / batch. (data: 8.96e-03). ETA=8:09:07, max mem: 11.4 GB 
[10/28 16:03:52 visual_prompt]: Epoch 2 / 100: avg data time: 3.99e-01, avg batch time: 0.8935, average train loss: 0.7682
[10/28 16:04:45 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1909, average loss: 0.7330
[10/28 16:04:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.29	
[10/28 16:04:45 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/28 16:06:16 visual_prompt]: 	Training 100/553. train loss: 0.7353,	0.5136 s / batch. (data: 2.73e-04). ETA=7:43:02, max mem: 11.4 GB 
[10/28 16:07:47 visual_prompt]: 	Training 200/553. train loss: 0.7086,	0.5047 s / batch. (data: 9.65e-03). ETA=7:34:12, max mem: 11.4 GB 
[10/28 16:09:14 visual_prompt]: 	Training 300/553. train loss: 0.5833,	0.4878 s / batch. (data: 8.38e-03). ETA=7:18:08, max mem: 11.4 GB 
[10/28 16:10:45 visual_prompt]: 	Training 400/553. train loss: 0.6362,	0.4785 s / batch. (data: 2.69e-04). ETA=7:09:00, max mem: 11.4 GB 
[10/28 16:12:15 visual_prompt]: 	Training 500/553. train loss: 0.7274,	1.8151 s / batch. (data: 1.33e+00). ETA=1 day, 3:04:22, max mem: 11.4 GB 
[10/28 16:13:00 visual_prompt]: Epoch 3 / 100: avg data time: 3.99e-01, avg batch time: 0.8946, average train loss: 0.7384
[10/28 16:13:53 visual_prompt]: Inference (val):avg data time: 3.84e-04, avg batch time: 0.1903, average loss: 0.7241
[10/28 16:13:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.73	
[10/28 16:13:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/28 16:15:26 visual_prompt]: 	Training 100/553. train loss: 0.7225,	0.5040 s / batch. (data: 4.37e-04). ETA=7:29:42, max mem: 11.4 GB 
[10/28 16:16:56 visual_prompt]: 	Training 200/553. train loss: 0.6150,	0.4989 s / batch. (data: 6.08e-03). ETA=7:24:19, max mem: 11.4 GB 
[10/28 16:18:25 visual_prompt]: 	Training 300/553. train loss: 0.6632,	1.6555 s / batch. (data: 1.17e+00). ETA=1 day, 0:31:43, max mem: 11.4 GB 
[10/28 16:19:50 visual_prompt]: 	Training 400/553. train loss: 0.7837,	1.3360 s / batch. (data: 8.43e-01). ETA=19:45:29, max mem: 11.4 GB 
[10/28 16:21:20 visual_prompt]: 	Training 500/553. train loss: 0.3779,	3.9560 s / batch. (data: 3.45e+00). ETA=2 days, 10:23:45, max mem: 11.4 GB 
[10/28 16:22:05 visual_prompt]: Epoch 4 / 100: avg data time: 3.96e-01, avg batch time: 0.8910, average train loss: 0.7262
[10/28 16:22:58 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1900, average loss: 0.7174
[10/28 16:22:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[10/28 16:22:58 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/28 16:24:28 visual_prompt]: 	Training 100/553. train loss: 0.5422,	0.4784 s / batch. (data: 2.58e-04). ETA=7:02:27, max mem: 11.4 GB 
[10/28 16:25:57 visual_prompt]: 	Training 200/553. train loss: 0.5682,	1.6752 s / batch. (data: 1.19e+00). ETA=1 day, 0:36:38, max mem: 11.4 GB 
[10/28 16:27:26 visual_prompt]: 	Training 300/553. train loss: 0.9348,	0.5040 s / batch. (data: 2.65e-04). ETA=7:23:27, max mem: 11.4 GB 
[10/28 16:28:53 visual_prompt]: 	Training 400/553. train loss: 0.5768,	0.5073 s / batch. (data: 1.95e-02). ETA=7:25:31, max mem: 11.4 GB 
[10/28 16:30:22 visual_prompt]: 	Training 500/553. train loss: 0.6529,	0.4807 s / batch. (data: 2.75e-04). ETA=7:01:17, max mem: 11.4 GB 
[10/28 16:31:09 visual_prompt]: Epoch 5 / 100: avg data time: 3.93e-01, avg batch time: 0.8875, average train loss: 0.7205
[10/28 16:32:01 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1915, average loss: 0.6949
[10/28 16:32:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.10	
[10/28 16:32:01 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/28 16:33:34 visual_prompt]: 	Training 100/553. train loss: 0.6574,	0.5070 s / batch. (data: 7.20e-04). ETA=7:23:02, max mem: 11.4 GB 
[10/28 16:35:02 visual_prompt]: 	Training 200/553. train loss: 0.6330,	0.4862 s / batch. (data: 7.94e-03). ETA=7:04:06, max mem: 11.4 GB 
[10/28 16:36:29 visual_prompt]: 	Training 300/553. train loss: 0.5643,	0.5031 s / batch. (data: 2.73e-04). ETA=7:17:58, max mem: 11.4 GB 
[10/28 16:38:01 visual_prompt]: 	Training 400/553. train loss: 0.6981,	0.7200 s / batch. (data: 2.35e-01). ETA=10:25:38, max mem: 11.4 GB 
[10/28 16:39:28 visual_prompt]: 	Training 500/553. train loss: 0.7028,	1.4600 s / batch. (data: 9.55e-01). ETA=21:06:11, max mem: 11.4 GB 
[10/28 16:40:12 visual_prompt]: Epoch 6 / 100: avg data time: 3.94e-01, avg batch time: 0.8886, average train loss: 0.7268
[10/28 16:41:05 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1907, average loss: 0.7519
[10/28 16:41:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/28 16:41:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/28 16:42:36 visual_prompt]: 	Training 100/553. train loss: 0.6349,	0.5000 s / batch. (data: 2.71e-04). ETA=7:12:19, max mem: 11.4 GB 
[10/28 16:44:04 visual_prompt]: 	Training 200/553. train loss: 0.5870,	0.4790 s / batch. (data: 2.70e-04). ETA=6:53:21, max mem: 11.4 GB 
[10/28 16:45:35 visual_prompt]: 	Training 300/553. train loss: 0.7772,	2.4087 s / batch. (data: 1.92e+00). ETA=1 day, 10:34:45, max mem: 11.4 GB 
[10/28 16:47:04 visual_prompt]: 	Training 400/553. train loss: 0.5952,	2.3320 s / batch. (data: 1.83e+00). ETA=1 day, 9:24:50, max mem: 11.4 GB 
[10/28 16:48:30 visual_prompt]: 	Training 500/553. train loss: 0.8810,	0.9403 s / batch. (data: 4.36e-01). ETA=13:26:47, max mem: 11.4 GB 
[10/28 16:49:14 visual_prompt]: Epoch 7 / 100: avg data time: 3.90e-01, avg batch time: 0.8853, average train loss: 0.7417
[10/28 16:50:07 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.1936, average loss: 0.8050
[10/28 16:50:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.40	
[10/28 16:50:07 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/28 16:51:36 visual_prompt]: 	Training 100/553. train loss: 0.6947,	0.5000 s / batch. (data: 2.78e-04). ETA=7:07:42, max mem: 11.4 GB 
[10/28 16:53:07 visual_prompt]: 	Training 200/553. train loss: 1.3240,	0.5135 s / batch. (data: 2.95e-04). ETA=7:18:24, max mem: 11.4 GB 
[10/28 16:54:36 visual_prompt]: 	Training 300/553. train loss: 0.7482,	0.5077 s / batch. (data: 5.42e-03). ETA=7:12:40, max mem: 11.4 GB 
[10/28 16:56:05 visual_prompt]: 	Training 400/553. train loss: 0.6901,	0.8795 s / batch. (data: 3.88e-01). ETA=12:27:59, max mem: 11.4 GB 
[10/28 16:57:34 visual_prompt]: 	Training 500/553. train loss: 1.0438,	1.9082 s / batch. (data: 1.41e+00). ETA=1 day, 2:59:44, max mem: 11.4 GB 
[10/28 16:58:19 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8893, average train loss: 0.7467
[10/28 16:59:11 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1910, average loss: 0.6898
[10/28 16:59:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.56	
[10/28 16:59:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/28 17:00:43 visual_prompt]: 	Training 100/553. train loss: 0.4168,	0.5040 s / batch. (data: 2.52e-04). ETA=7:06:31, max mem: 11.4 GB 
[10/28 17:02:10 visual_prompt]: 	Training 200/553. train loss: 0.7157,	0.4949 s / batch. (data: 1.63e-02). ETA=6:58:00, max mem: 11.4 GB 
[10/28 17:03:38 visual_prompt]: 	Training 300/553. train loss: 0.6590,	1.7674 s / batch. (data: 1.28e+00). ETA=1 day, 0:49:49, max mem: 11.4 GB 
[10/28 17:05:08 visual_prompt]: 	Training 400/553. train loss: 0.5715,	0.5376 s / batch. (data: 2.55e-02). ETA=7:32:13, max mem: 11.4 GB 
[10/28 17:06:37 visual_prompt]: 	Training 500/553. train loss: 0.7688,	0.9918 s / batch. (data: 5.06e-01). ETA=13:52:42, max mem: 11.4 GB 
[10/28 17:07:21 visual_prompt]: Epoch 9 / 100: avg data time: 3.91e-01, avg batch time: 0.8861, average train loss: 0.7437
[10/28 17:08:13 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1916, average loss: 0.7488
[10/28 17:08:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.78	
[10/28 17:08:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/28 17:09:47 visual_prompt]: 	Training 100/553. train loss: 0.6946,	0.4878 s / batch. (data: 2.67e-04). ETA=6:48:20, max mem: 11.4 GB 
[10/28 17:11:14 visual_prompt]: 	Training 200/553. train loss: 0.6746,	1.1512 s / batch. (data: 6.62e-01). ETA=16:01:40, max mem: 11.4 GB 
[10/28 17:12:43 visual_prompt]: 	Training 300/553. train loss: 0.5772,	2.4723 s / batch. (data: 1.99e+00). ETA=1 day, 10:21:14, max mem: 11.4 GB 
[10/28 17:14:08 visual_prompt]: 	Training 400/553. train loss: 0.7443,	1.1781 s / batch. (data: 7.01e-01). ETA=16:20:13, max mem: 11.4 GB 
[10/28 17:15:38 visual_prompt]: 	Training 500/553. train loss: 1.0208,	0.5130 s / batch. (data: 1.05e-02). ETA=7:05:57, max mem: 11.4 GB 
[10/28 17:16:24 visual_prompt]: Epoch 10 / 100: avg data time: 3.92e-01, avg batch time: 0.8865, average train loss: 0.7816
[10/28 17:17:16 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1882, average loss: 0.7065
[10/28 17:17:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[10/28 17:17:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/28 17:18:49 visual_prompt]: 	Training 100/553. train loss: 0.8909,	0.5320 s / batch. (data: 2.01e-02). ETA=7:20:25, max mem: 11.4 GB 
[10/28 17:20:20 visual_prompt]: 	Training 200/553. train loss: 1.1422,	0.4923 s / batch. (data: 2.30e-04). ETA=6:46:43, max mem: 11.4 GB 
[10/28 17:21:48 visual_prompt]: 	Training 300/553. train loss: 0.5869,	2.0760 s / batch. (data: 1.58e+00). ETA=1 day, 4:31:40, max mem: 11.4 GB 
[10/28 17:23:15 visual_prompt]: 	Training 400/553. train loss: 0.5660,	0.5020 s / batch. (data: 1.09e-02). ETA=6:53:03, max mem: 11.4 GB 
[10/28 17:24:42 visual_prompt]: 	Training 500/553. train loss: 0.8377,	0.5040 s / batch. (data: 5.38e-03). ETA=6:53:52, max mem: 11.4 GB 
[10/28 17:25:27 visual_prompt]: Epoch 11 / 100: avg data time: 3.92e-01, avg batch time: 0.8875, average train loss: 0.8343
[10/28 17:26:19 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1890, average loss: 0.7891
[10/28 17:26:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.14	
[10/28 17:26:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/28 17:27:52 visual_prompt]: 	Training 100/553. train loss: 0.8844,	0.5080 s / batch. (data: 2.79e-04). ETA=6:55:51, max mem: 11.4 GB 
[10/28 17:29:22 visual_prompt]: 	Training 200/553. train loss: 0.7399,	0.4840 s / batch. (data: 2.75e-04). ETA=6:35:26, max mem: 11.4 GB 
[10/28 17:30:49 visual_prompt]: 	Training 300/553. train loss: 0.6644,	0.4795 s / batch. (data: 3.54e-04). ETA=6:30:53, max mem: 11.4 GB 
[10/28 17:32:17 visual_prompt]: 	Training 400/553. train loss: 0.7711,	0.4925 s / batch. (data: 2.80e-04). ETA=6:40:43, max mem: 11.4 GB 
[10/28 17:33:46 visual_prompt]: 	Training 500/553. train loss: 1.9058,	0.5058 s / batch. (data: 1.05e-02). ETA=6:50:40, max mem: 11.4 GB 
[10/28 17:34:30 visual_prompt]: Epoch 12 / 100: avg data time: 3.93e-01, avg batch time: 0.8873, average train loss: 0.7933
[10/28 17:35:22 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.1914, average loss: 0.8529
[10/28 17:35:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.68	
[10/28 17:35:22 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/28 17:36:56 visual_prompt]: 	Training 100/553. train loss: 0.7426,	1.5687 s / batch. (data: 1.07e+00). ETA=21:09:44, max mem: 11.4 GB 
[10/28 17:38:21 visual_prompt]: 	Training 200/553. train loss: 0.7008,	0.9604 s / batch. (data: 4.73e-01). ETA=12:55:43, max mem: 11.4 GB 
[10/28 17:39:51 visual_prompt]: 	Training 300/553. train loss: 0.7697,	2.1727 s / batch. (data: 1.69e+00). ETA=1 day, 5:11:22, max mem: 11.4 GB 
[10/28 17:41:18 visual_prompt]: 	Training 400/553. train loss: 1.9720,	0.4920 s / batch. (data: 5.41e-03). ETA=6:35:44, max mem: 11.4 GB 
[10/28 17:42:48 visual_prompt]: 	Training 500/553. train loss: 0.7407,	0.4960 s / batch. (data: 2.53e-04). ETA=6:38:09, max mem: 11.4 GB 
[10/28 17:43:33 visual_prompt]: Epoch 13 / 100: avg data time: 3.93e-01, avg batch time: 0.8874, average train loss: 0.8004
[10/28 17:44:25 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1906, average loss: 0.7296
[10/28 17:44:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.63	
[10/28 17:44:25 visual_prompt]: Best epoch 13: best metric: -0.730
[10/28 17:44:25 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/28 17:45:57 visual_prompt]: 	Training 100/553. train loss: 0.6205,	0.4971 s / batch. (data: 5.39e-03). ETA=6:37:48, max mem: 11.4 GB 
[10/28 17:47:25 visual_prompt]: 	Training 200/553. train loss: 0.6559,	0.6415 s / batch. (data: 1.36e-01). ETA=8:32:15, max mem: 11.4 GB 
[10/28 17:48:54 visual_prompt]: 	Training 300/553. train loss: 0.7012,	1.3268 s / batch. (data: 8.38e-01). ETA=17:37:17, max mem: 11.4 GB 
[10/28 17:50:22 visual_prompt]: 	Training 400/553. train loss: 0.6926,	0.5046 s / batch. (data: 1.04e-02). ETA=6:41:13, max mem: 11.4 GB 
[10/28 17:51:51 visual_prompt]: 	Training 500/553. train loss: 1.1294,	0.4905 s / batch. (data: 1.25e-02). ETA=6:29:15, max mem: 11.4 GB 
[10/28 17:52:35 visual_prompt]: Epoch 14 / 100: avg data time: 3.92e-01, avg batch time: 0.8863, average train loss: 0.7475
[10/28 17:53:28 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1903, average loss: 0.7177
[10/28 17:53:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.06	
[10/28 17:53:28 visual_prompt]: Best epoch 14: best metric: -0.718
[10/28 17:53:28 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/28 17:55:00 visual_prompt]: 	Training 100/553. train loss: 0.7710,	0.5153 s / batch. (data: 7.81e-03). ETA=6:47:35, max mem: 11.4 GB 
[10/28 17:56:27 visual_prompt]: 	Training 200/553. train loss: 1.0706,	0.5080 s / batch. (data: 2.54e-04). ETA=6:40:58, max mem: 11.4 GB 
[10/28 17:57:56 visual_prompt]: 	Training 300/553. train loss: 0.6188,	0.5006 s / batch. (data: 5.41e-03). ETA=6:34:16, max mem: 11.4 GB 
[10/28 17:59:23 visual_prompt]: 	Training 400/553. train loss: 0.5664,	0.4986 s / batch. (data: 2.75e-04). ETA=6:31:54, max mem: 11.4 GB 
[10/28 18:00:53 visual_prompt]: 	Training 500/553. train loss: 0.6298,	0.5213 s / batch. (data: 1.18e-02). ETA=6:48:53, max mem: 11.4 GB 
[10/28 18:01:39 visual_prompt]: Epoch 15 / 100: avg data time: 3.93e-01, avg batch time: 0.8882, average train loss: 0.7728
[10/28 18:02:32 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1902, average loss: 0.7580
[10/28 18:02:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.06	
[10/28 18:02:32 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/28 18:04:03 visual_prompt]: 	Training 100/553. train loss: 0.5686,	0.4825 s / batch. (data: 2.52e-04). ETA=6:17:10, max mem: 11.4 GB 
[10/28 18:05:32 visual_prompt]: 	Training 200/553. train loss: 1.0570,	0.5159 s / batch. (data: 1.55e-02). ETA=6:42:25, max mem: 11.4 GB 
[10/28 18:07:01 visual_prompt]: 	Training 300/553. train loss: 0.8784,	0.4968 s / batch. (data: 5.39e-03). ETA=6:26:44, max mem: 11.4 GB 
[10/28 18:08:29 visual_prompt]: 	Training 400/553. train loss: 0.9043,	0.4969 s / batch. (data: 6.95e-04). ETA=6:25:56, max mem: 11.4 GB 
[10/28 18:09:58 visual_prompt]: 	Training 500/553. train loss: 1.0802,	2.0205 s / batch. (data: 1.52e+00). ETA=1 day, 2:06:04, max mem: 11.4 GB 
[10/28 18:10:43 visual_prompt]: Epoch 16 / 100: avg data time: 3.95e-01, avg batch time: 0.8884, average train loss: 0.8139
[10/28 18:11:35 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1891, average loss: 0.6891
[10/28 18:11:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.24	
[10/28 18:11:35 visual_prompt]: Best epoch 16: best metric: -0.689
[10/28 18:11:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/28 18:13:06 visual_prompt]: 	Training 100/553. train loss: 0.5780,	0.5041 s / batch. (data: 5.39e-03). ETA=6:29:23, max mem: 11.4 GB 
[10/28 18:14:36 visual_prompt]: 	Training 200/553. train loss: 1.4327,	0.5080 s / batch. (data: 2.50e-04). ETA=6:31:36, max mem: 11.4 GB 
[10/28 18:16:04 visual_prompt]: 	Training 300/553. train loss: 1.3078,	0.4884 s / batch. (data: 2.73e-04). ETA=6:15:41, max mem: 11.4 GB 
[10/28 18:17:32 visual_prompt]: 	Training 400/553. train loss: 0.7890,	1.4683 s / batch. (data: 9.69e-01). ETA=18:46:59, max mem: 11.4 GB 
[10/28 18:18:59 visual_prompt]: 	Training 500/553. train loss: 0.6358,	2.1912 s / batch. (data: 1.69e+00). ETA=1 day, 3:58:10, max mem: 11.4 GB 
[10/28 18:19:45 visual_prompt]: Epoch 17 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 0.7746
[10/28 18:20:38 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1908, average loss: 0.7264
[10/28 18:20:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.62	
[10/28 18:20:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/28 18:22:10 visual_prompt]: 	Training 100/553. train loss: 0.7288,	0.5040 s / batch. (data: 2.30e-04). ETA=6:24:42, max mem: 11.4 GB 
[10/28 18:23:41 visual_prompt]: 	Training 200/553. train loss: 0.6697,	0.5042 s / batch. (data: 8.00e-04). ETA=6:24:00, max mem: 11.4 GB 
[10/28 18:25:10 visual_prompt]: 	Training 300/553. train loss: 0.7003,	0.4952 s / batch. (data: 2.76e-04). ETA=6:16:21, max mem: 11.4 GB 
[10/28 18:26:38 visual_prompt]: 	Training 400/553. train loss: 0.7544,	0.4876 s / batch. (data: 2.99e-04). ETA=6:09:43, max mem: 11.4 GB 
[10/28 18:28:05 visual_prompt]: 	Training 500/553. train loss: 0.7299,	0.9312 s / batch. (data: 4.44e-01). ETA=11:44:37, max mem: 11.4 GB 
[10/28 18:28:49 visual_prompt]: Epoch 18 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 0.8485
[10/28 18:29:41 visual_prompt]: Inference (val):avg data time: 1.95e-04, avg batch time: 0.1913, average loss: 0.6948
[10/28 18:29:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.01	
[10/28 18:29:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/28 18:31:13 visual_prompt]: 	Training 100/553. train loss: 0.6637,	0.5160 s / batch. (data: 2.89e-04). ETA=6:29:06, max mem: 11.4 GB 
[10/28 18:32:42 visual_prompt]: 	Training 200/553. train loss: 0.8101,	0.5080 s / batch. (data: 1.59e-03). ETA=6:22:12, max mem: 11.4 GB 
[10/28 18:34:10 visual_prompt]: 	Training 300/553. train loss: 0.7359,	0.4840 s / batch. (data: 2.70e-04). ETA=6:03:21, max mem: 11.4 GB 
[10/28 18:35:40 visual_prompt]: 	Training 400/553. train loss: 0.5661,	0.5160 s / batch. (data: 7.43e-04). ETA=6:26:31, max mem: 11.4 GB 
[10/28 18:37:04 visual_prompt]: 	Training 500/553. train loss: 0.8998,	0.5038 s / batch. (data: 2.73e-04). ETA=6:16:34, max mem: 11.4 GB 
[10/28 18:37:51 visual_prompt]: Epoch 19 / 100: avg data time: 3.91e-01, avg batch time: 0.8843, average train loss: 0.7758
[10/28 18:38:43 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1915, average loss: 0.7440
[10/28 18:38:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.48	
[10/28 18:38:43 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/28 18:40:13 visual_prompt]: 	Training 100/553. train loss: 0.5817,	0.5068 s / batch. (data: 2.94e-02). ETA=6:17:32, max mem: 11.4 GB 
[10/28 18:41:42 visual_prompt]: 	Training 200/553. train loss: 0.5682,	0.4920 s / batch. (data: 2.81e-04). ETA=6:05:39, max mem: 11.4 GB 
[10/28 18:43:11 visual_prompt]: 	Training 300/553. train loss: 0.8432,	0.4880 s / batch. (data: 2.73e-04). ETA=6:01:51, max mem: 11.4 GB 
[10/28 18:44:39 visual_prompt]: 	Training 400/553. train loss: 0.6497,	0.5000 s / batch. (data: 2.50e-04). ETA=6:09:57, max mem: 11.4 GB 
[10/28 18:46:06 visual_prompt]: 	Training 500/553. train loss: 1.3595,	0.5079 s / batch. (data: 2.69e-04). ETA=6:14:58, max mem: 11.4 GB 
[10/28 18:46:53 visual_prompt]: Epoch 20 / 100: avg data time: 3.92e-01, avg batch time: 0.8865, average train loss: 0.8053
[10/28 18:47:46 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1905, average loss: 0.7811
[10/28 18:47:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.33	
[10/28 18:47:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/28 18:49:20 visual_prompt]: 	Training 100/553. train loss: 0.5644,	1.3240 s / batch. (data: 8.46e-01). ETA=16:14:02, max mem: 11.4 GB 
[10/28 18:50:46 visual_prompt]: 	Training 200/553. train loss: 0.7070,	0.5088 s / batch. (data: 2.91e-04). ETA=6:13:27, max mem: 11.4 GB 
[10/28 18:52:17 visual_prompt]: 	Training 300/553. train loss: 0.9706,	1.6439 s / batch. (data: 1.15e+00). ETA=20:03:52, max mem: 11.4 GB 
[10/28 18:53:43 visual_prompt]: 	Training 400/553. train loss: 0.6921,	0.5034 s / batch. (data: 1.63e-02). ETA=6:07:47, max mem: 11.4 GB 
[10/28 18:55:13 visual_prompt]: 	Training 500/553. train loss: 0.7285,	0.4960 s / batch. (data: 1.60e-02). ETA=6:01:32, max mem: 11.4 GB 
[10/28 18:55:57 visual_prompt]: Epoch 21 / 100: avg data time: 3.94e-01, avg batch time: 0.8877, average train loss: 0.7653
[10/28 18:56:49 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1911, average loss: 0.7094
[10/28 18:56:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.51	
[10/28 18:56:49 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/28 18:58:20 visual_prompt]: 	Training 100/553. train loss: 0.7456,	0.5161 s / batch. (data: 2.89e-02). ETA=6:14:54, max mem: 11.4 GB 
[10/28 18:59:48 visual_prompt]: 	Training 200/553. train loss: 0.5679,	0.5035 s / batch. (data: 2.58e-04). ETA=6:04:56, max mem: 11.4 GB 
[10/28 19:01:15 visual_prompt]: 	Training 300/553. train loss: 0.6679,	0.4785 s / batch. (data: 2.80e-04). ETA=5:45:58, max mem: 11.4 GB 
[10/28 19:02:43 visual_prompt]: 	Training 400/553. train loss: 0.8835,	0.4840 s / batch. (data: 2.51e-04). ETA=5:49:11, max mem: 11.4 GB 
[10/28 19:04:12 visual_prompt]: 	Training 500/553. train loss: 0.7278,	0.5289 s / batch. (data: 1.92e-02). ETA=6:20:40, max mem: 11.4 GB 
[10/28 19:04:59 visual_prompt]: Epoch 22 / 100: avg data time: 3.93e-01, avg batch time: 0.8864, average train loss: 0.7768
[10/28 19:05:52 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1893, average loss: 0.7192
[10/28 19:05:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.86	
[10/28 19:05:52 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/28 19:07:24 visual_prompt]: 	Training 100/553. train loss: 1.0403,	0.4978 s / batch. (data: 5.43e-03). ETA=5:57:04, max mem: 11.4 GB 
[10/28 19:08:54 visual_prompt]: 	Training 200/553. train loss: 0.6991,	1.4000 s / batch. (data: 9.22e-01). ETA=16:41:48, max mem: 11.4 GB 
[10/28 19:10:24 visual_prompt]: 	Training 300/553. train loss: 1.2591,	0.4782 s / batch. (data: 2.48e-04). ETA=5:41:25, max mem: 11.4 GB 
[10/28 19:11:50 visual_prompt]: 	Training 400/553. train loss: 0.5634,	0.4920 s / batch. (data: 7.98e-03). ETA=5:50:25, max mem: 11.4 GB 
[10/28 19:13:16 visual_prompt]: 	Training 500/553. train loss: 1.3473,	0.4863 s / batch. (data: 9.14e-03). ETA=5:45:34, max mem: 11.4 GB 
[10/28 19:14:02 visual_prompt]: Epoch 23 / 100: avg data time: 3.93e-01, avg batch time: 0.8864, average train loss: 0.7763
[10/28 19:14:54 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1918, average loss: 0.6895
[10/28 19:14:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.55	
[10/28 19:14:54 visual_prompt]: Stopping early.
[10/28 19:14:54 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 19:14:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 19:14:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 19:14:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 19:14:54 visual_prompt]: Training with config:
[10/28 19:14:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.1_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 19:14:54 visual_prompt]: Loading training data...
[10/28 19:14:54 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 19:14:54 visual_prompt]: Loading validation data...
[10/28 19:14:54 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 19:14:54 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/28 19:14:57 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/28 19:14:57 visual_prompt]: tuned percent:0.529
[10/28 19:14:57 visual_prompt]: Device used for model: 0
[10/28 19:14:57 visual_prompt]: Setting up Evaluator...
[10/28 19:14:57 visual_prompt]: Setting up Trainer...
[10/28 19:14:57 visual_prompt]: 	Setting up the optimizer...
[10/28 19:14:57 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 19:16:28 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5280 s / batch. (data: 2.75e-04). ETA=8:05:46, max mem: 11.4 GB 
[10/28 19:17:55 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4785 s / batch. (data: 2.64e-04). ETA=7:19:24, max mem: 11.4 GB 
[10/28 19:19:26 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.7992 s / batch. (data: 2.30e+00). ETA=1 day, 18:45:55, max mem: 11.4 GB 
[10/28 19:20:52 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4800 s / batch. (data: 2.57e-04). ETA=7:19:12, max mem: 11.4 GB 
[10/28 19:22:22 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5148 s / batch. (data: 1.08e-02). ETA=7:50:10, max mem: 11.4 GB 
[10/28 19:23:08 visual_prompt]: Epoch 1 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 1.3966
[10/28 19:24:00 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1905, average loss: 1.3454
[10/28 19:24:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/28 19:24:00 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/28 19:25:31 visual_prompt]: 	Training 100/553. train loss: 0.6492,	0.6791 s / batch. (data: 1.84e-01). ETA=10:18:29, max mem: 11.4 GB 
[10/28 19:26:59 visual_prompt]: 	Training 200/553. train loss: 0.2324,	0.6840 s / batch. (data: 1.66e-01). ETA=10:21:48, max mem: 11.4 GB 
[10/28 19:28:30 visual_prompt]: 	Training 300/553. train loss: 0.7312,	1.5940 s / batch. (data: 1.10e+00). ETA=1 day, 0:06:29, max mem: 11.4 GB 
[10/28 19:29:58 visual_prompt]: 	Training 400/553. train loss: 0.9358,	0.4998 s / batch. (data: 2.73e-04). ETA=7:32:42, max mem: 11.4 GB 
[10/28 19:31:28 visual_prompt]: 	Training 500/553. train loss: 0.6426,	0.4800 s / batch. (data: 2.75e-04). ETA=7:13:57, max mem: 11.4 GB 
[10/28 19:32:13 visual_prompt]: Epoch 2 / 100: avg data time: 3.95e-01, avg batch time: 0.8903, average train loss: 0.7749
[10/28 19:33:06 visual_prompt]: Inference (val):avg data time: 3.37e-04, avg batch time: 0.1915, average loss: 0.7224
[10/28 19:33:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.83	
[10/28 19:33:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/28 19:34:37 visual_prompt]: 	Training 100/553. train loss: 0.7337,	1.0880 s / batch. (data: 5.90e-01). ETA=16:20:55, max mem: 11.4 GB 
[10/28 19:36:06 visual_prompt]: 	Training 200/553. train loss: 0.7159,	0.5679 s / batch. (data: 5.88e-02). ETA=8:31:04, max mem: 11.4 GB 
[10/28 19:37:34 visual_prompt]: 	Training 300/553. train loss: 0.5977,	0.5000 s / batch. (data: 2.62e-04). ETA=7:29:08, max mem: 11.4 GB 
[10/28 19:39:04 visual_prompt]: 	Training 400/553. train loss: 0.5777,	0.4884 s / batch. (data: 2.40e-04). ETA=7:17:55, max mem: 11.4 GB 
[10/28 19:40:33 visual_prompt]: 	Training 500/553. train loss: 0.7091,	1.7780 s / batch. (data: 1.29e+00). ETA=1 day, 2:31:06, max mem: 11.4 GB 
[10/28 19:41:17 visual_prompt]: Epoch 3 / 100: avg data time: 3.94e-01, avg batch time: 0.8887, average train loss: 0.7532
[10/28 19:42:10 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1916, average loss: 0.7640
[10/28 19:42:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.55	
[10/28 19:42:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/28 19:43:42 visual_prompt]: 	Training 100/553. train loss: 0.7741,	0.4841 s / batch. (data: 5.40e-03). ETA=7:11:56, max mem: 11.4 GB 
[10/28 19:45:12 visual_prompt]: 	Training 200/553. train loss: 0.7432,	0.4941 s / batch. (data: 1.55e-02). ETA=7:20:04, max mem: 11.4 GB 
[10/28 19:46:40 visual_prompt]: 	Training 300/553. train loss: 0.5807,	1.5124 s / batch. (data: 1.03e+00). ETA=22:24:33, max mem: 11.4 GB 
[10/28 19:48:05 visual_prompt]: 	Training 400/553. train loss: 0.6147,	1.8960 s / batch. (data: 1.40e+00). ETA=1 day, 4:02:24, max mem: 11.4 GB 
[10/28 19:49:35 visual_prompt]: 	Training 500/553. train loss: 0.6959,	3.1633 s / batch. (data: 2.67e+00). ETA=1 day, 22:41:43, max mem: 11.4 GB 
[10/28 19:50:21 visual_prompt]: Epoch 4 / 100: avg data time: 3.95e-01, avg batch time: 0.8888, average train loss: 0.7759
[10/28 19:51:14 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1914, average loss: 0.6930
[10/28 19:51:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.83	
[10/28 19:51:14 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/28 19:52:44 visual_prompt]: 	Training 100/553. train loss: 0.4961,	0.4787 s / batch. (data: 2.88e-04). ETA=7:02:46, max mem: 11.4 GB 
[10/28 19:54:13 visual_prompt]: 	Training 200/553. train loss: 0.7465,	1.7559 s / batch. (data: 1.27e+00). ETA=1 day, 1:47:45, max mem: 11.4 GB 
[10/28 19:55:43 visual_prompt]: 	Training 300/553. train loss: 0.9614,	0.5277 s / batch. (data: 1.09e-02). ETA=7:44:17, max mem: 11.4 GB 
[10/28 19:57:10 visual_prompt]: 	Training 400/553. train loss: 0.6824,	0.5001 s / batch. (data: 2.76e-04). ETA=7:19:06, max mem: 11.4 GB 
[10/28 19:58:39 visual_prompt]: 	Training 500/553. train loss: 0.6227,	0.4805 s / batch. (data: 2.62e-04). ETA=7:01:10, max mem: 11.4 GB 
[10/28 19:59:25 visual_prompt]: Epoch 5 / 100: avg data time: 3.94e-01, avg batch time: 0.8886, average train loss: 0.7784
[10/28 20:00:17 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1901, average loss: 0.7344
[10/28 20:00:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.21	
[10/28 20:00:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/28 20:01:50 visual_prompt]: 	Training 100/553. train loss: 0.6970,	0.5291 s / batch. (data: 7.90e-04). ETA=7:42:23, max mem: 11.4 GB 
[10/28 20:03:18 visual_prompt]: 	Training 200/553. train loss: 0.5674,	0.5120 s / batch. (data: 2.71e-04). ETA=7:26:34, max mem: 11.4 GB 
[10/28 20:04:45 visual_prompt]: 	Training 300/553. train loss: 0.5507,	0.4884 s / batch. (data: 2.63e-04). ETA=7:05:13, max mem: 11.4 GB 
[10/28 20:06:17 visual_prompt]: 	Training 400/553. train loss: 0.5774,	0.6080 s / batch. (data: 1.08e-01). ETA=8:48:18, max mem: 11.4 GB 
[10/28 20:07:45 visual_prompt]: 	Training 500/553. train loss: 0.8609,	1.4040 s / batch. (data: 9.02e-01). ETA=20:17:36, max mem: 11.4 GB 
[10/28 20:08:29 visual_prompt]: Epoch 6 / 100: avg data time: 3.93e-01, avg batch time: 0.8892, average train loss: 0.7562
[10/28 20:09:22 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1899, average loss: 0.6824
[10/28 20:09:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.99	
[10/28 20:09:22 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/28 20:10:52 visual_prompt]: 	Training 100/553. train loss: 0.6393,	0.5250 s / batch. (data: 2.49e-02). ETA=7:33:55, max mem: 11.4 GB 
[10/28 20:12:20 visual_prompt]: 	Training 200/553. train loss: 0.5335,	0.5322 s / batch. (data: 3.63e-02). ETA=7:39:17, max mem: 11.4 GB 
[10/28 20:13:52 visual_prompt]: 	Training 300/553. train loss: 0.7837,	2.4600 s / batch. (data: 1.95e+00). ETA=1 day, 11:18:57, max mem: 11.4 GB 
[10/28 20:15:21 visual_prompt]: 	Training 400/553. train loss: 0.5687,	2.4080 s / batch. (data: 1.91e+00). ETA=1 day, 10:30:09, max mem: 11.4 GB 
[10/28 20:16:47 visual_prompt]: 	Training 500/553. train loss: 0.9142,	1.0078 s / batch. (data: 5.14e-01). ETA=14:24:43, max mem: 11.4 GB 
[10/28 20:17:32 visual_prompt]: Epoch 7 / 100: avg data time: 3.92e-01, avg batch time: 0.8867, average train loss: 0.7646
[10/28 20:18:24 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1895, average loss: 0.8153
[10/28 20:18:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.34	
[10/28 20:18:24 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/28 20:19:54 visual_prompt]: 	Training 100/553. train loss: 0.6919,	0.9433 s / batch. (data: 4.45e-01). ETA=13:26:58, max mem: 11.4 GB 
[10/28 20:21:24 visual_prompt]: 	Training 200/553. train loss: 1.2016,	0.4865 s / batch. (data: 7.96e-03). ETA=6:55:22, max mem: 11.4 GB 
[10/28 20:22:54 visual_prompt]: 	Training 300/553. train loss: 0.7832,	0.4997 s / batch. (data: 7.13e-04). ETA=7:05:47, max mem: 11.4 GB 
[10/28 20:24:22 visual_prompt]: 	Training 400/553. train loss: 0.7775,	1.5000 s / batch. (data: 1.01e+00). ETA=21:15:43, max mem: 11.4 GB 
[10/28 20:25:51 visual_prompt]: 	Training 500/553. train loss: 0.8841,	2.0200 s / batch. (data: 1.54e+00). ETA=1 day, 4:34:37, max mem: 11.4 GB 
[10/28 20:26:36 visual_prompt]: Epoch 8 / 100: avg data time: 3.93e-01, avg batch time: 0.8880, average train loss: 0.7713
[10/28 20:27:28 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1897, average loss: 0.6918
[10/28 20:27:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.34	
[10/28 20:27:28 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/28 20:28:59 visual_prompt]: 	Training 100/553. train loss: 0.4198,	0.5016 s / batch. (data: 2.78e-04). ETA=7:04:30, max mem: 11.4 GB 
[10/28 20:30:27 visual_prompt]: 	Training 200/553. train loss: 0.6697,	0.4843 s / batch. (data: 5.44e-03). ETA=6:49:04, max mem: 11.4 GB 
[10/28 20:31:56 visual_prompt]: 	Training 300/553. train loss: 0.7503,	2.1800 s / batch. (data: 1.68e+00). ETA=1 day, 6:37:35, max mem: 11.4 GB 
[10/28 20:33:25 visual_prompt]: 	Training 400/553. train loss: 0.5879,	0.4944 s / batch. (data: 2.84e-04). ETA=6:55:53, max mem: 11.4 GB 
[10/28 20:34:54 visual_prompt]: 	Training 500/553. train loss: 0.7182,	0.4883 s / batch. (data: 2.78e-04). ETA=6:49:58, max mem: 11.4 GB 
[10/28 20:35:38 visual_prompt]: Epoch 9 / 100: avg data time: 3.91e-01, avg batch time: 0.8864, average train loss: 0.7525
[10/28 20:36:30 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1912, average loss: 0.7276
[10/28 20:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.95	
[10/28 20:36:30 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/28 20:38:04 visual_prompt]: 	Training 100/553. train loss: 0.6930,	0.5240 s / batch. (data: 2.63e-04). ETA=7:18:37, max mem: 11.4 GB 
[10/28 20:39:32 visual_prompt]: 	Training 200/553. train loss: 0.6780,	0.5120 s / batch. (data: 2.43e-04). ETA=7:07:44, max mem: 11.4 GB 
[10/28 20:41:00 visual_prompt]: 	Training 300/553. train loss: 0.6192,	2.2757 s / batch. (data: 1.77e+00). ETA=1 day, 7:37:15, max mem: 11.4 GB 
[10/28 20:42:27 visual_prompt]: 	Training 400/553. train loss: 0.7441,	1.3197 s / batch. (data: 8.29e-01). ETA=18:18:01, max mem: 11.4 GB 
[10/28 20:43:55 visual_prompt]: 	Training 500/553. train loss: 1.0162,	1.0466 s / batch. (data: 5.58e-01). ETA=14:29:06, max mem: 11.4 GB 
[10/28 20:44:41 visual_prompt]: Epoch 10 / 100: avg data time: 3.92e-01, avg batch time: 0.8868, average train loss: 0.7793
[10/28 20:45:33 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1904, average loss: 0.9306
[10/28 20:45:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.61	
[10/28 20:45:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/28 20:47:07 visual_prompt]: 	Training 100/553. train loss: 0.7020,	0.5210 s / batch. (data: 2.38e-04). ETA=7:11:19, max mem: 11.4 GB 
[10/28 20:48:37 visual_prompt]: 	Training 200/553. train loss: 0.9795,	0.4960 s / batch. (data: 2.75e-04). ETA=6:49:46, max mem: 11.4 GB 
[10/28 20:50:05 visual_prompt]: 	Training 300/553. train loss: 0.4725,	1.8161 s / batch. (data: 1.33e+00). ETA=1 day, 0:57:24, max mem: 11.4 GB 
[10/28 20:51:31 visual_prompt]: 	Training 400/553. train loss: 0.7409,	0.4959 s / batch. (data: 2.68e-04). ETA=6:48:00, max mem: 11.4 GB 
[10/28 20:52:59 visual_prompt]: 	Training 500/553. train loss: 0.6936,	0.4874 s / batch. (data: 2.55e-04). ETA=6:40:13, max mem: 11.4 GB 
[10/28 20:53:44 visual_prompt]: Epoch 11 / 100: avg data time: 3.93e-01, avg batch time: 0.8876, average train loss: 0.7411
[10/28 20:54:37 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1906, average loss: 0.7001
[10/28 20:54:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.33	
[10/28 20:54:37 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/28 20:56:10 visual_prompt]: 	Training 100/553. train loss: 0.9557,	0.4847 s / batch. (data: 5.38e-03). ETA=6:36:47, max mem: 11.4 GB 
[10/28 20:57:39 visual_prompt]: 	Training 200/553. train loss: 0.6741,	1.1760 s / batch. (data: 6.40e-01). ETA=16:00:41, max mem: 11.4 GB 
[10/28 20:59:07 visual_prompt]: 	Training 300/553. train loss: 0.6196,	0.5118 s / batch. (data: 2.88e-04). ETA=6:57:17, max mem: 11.4 GB 
[10/28 21:00:35 visual_prompt]: 	Training 400/553. train loss: 1.1319,	0.5160 s / batch. (data: 7.96e-03). ETA=6:59:48, max mem: 11.4 GB 
[10/28 21:02:04 visual_prompt]: 	Training 500/553. train loss: 1.7865,	0.5120 s / batch. (data: 5.39e-03). ETA=6:55:42, max mem: 11.4 GB 
[10/28 21:02:49 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8898, average train loss: 0.7769
[10/28 21:03:41 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1917, average loss: 1.1128
[10/28 21:03:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.71	
[10/28 21:03:41 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/28 21:05:14 visual_prompt]: 	Training 100/553. train loss: 0.6353,	0.4786 s / batch. (data: 2.66e-04). ETA=6:27:23, max mem: 11.4 GB 
[10/28 21:06:39 visual_prompt]: 	Training 200/553. train loss: 0.7345,	0.4960 s / batch. (data: 2.99e-04). ETA=6:40:38, max mem: 11.4 GB 
[10/28 21:08:10 visual_prompt]: 	Training 300/553. train loss: 0.6670,	2.2728 s / batch. (data: 1.79e+00). ETA=1 day, 6:32:00, max mem: 11.4 GB 
[10/28 21:09:36 visual_prompt]: 	Training 400/553. train loss: 1.2901,	0.6160 s / batch. (data: 1.23e-01). ETA=8:15:32, max mem: 11.4 GB 
[10/28 21:11:05 visual_prompt]: 	Training 500/553. train loss: 0.7068,	0.5028 s / batch. (data: 2.73e-04). ETA=6:43:38, max mem: 11.4 GB 
[10/28 21:11:51 visual_prompt]: Epoch 13 / 100: avg data time: 3.91e-01, avg batch time: 0.8862, average train loss: 0.7801
[10/28 21:12:44 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1917, average loss: 0.7029
[10/28 21:12:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.52	
[10/28 21:12:44 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/28 21:14:17 visual_prompt]: 	Training 100/553. train loss: 0.6359,	0.5090 s / batch. (data: 5.38e-03). ETA=6:47:19, max mem: 11.4 GB 
[10/28 21:15:46 visual_prompt]: 	Training 200/553. train loss: 0.6303,	0.5598 s / batch. (data: 7.69e-02). ETA=7:26:59, max mem: 11.4 GB 
[10/28 21:17:15 visual_prompt]: 	Training 300/553. train loss: 0.6994,	1.4560 s / batch. (data: 9.56e-01). ETA=19:20:13, max mem: 11.4 GB 
[10/28 21:18:43 visual_prompt]: 	Training 400/553. train loss: 0.6200,	0.4960 s / batch. (data: 2.77e-04). ETA=6:34:24, max mem: 11.4 GB 
[10/28 21:20:13 visual_prompt]: 	Training 500/553. train loss: 1.0078,	0.4918 s / batch. (data: 1.04e-02). ETA=6:30:13, max mem: 11.4 GB 
[10/28 21:20:57 visual_prompt]: Epoch 14 / 100: avg data time: 3.96e-01, avg batch time: 0.8912, average train loss: 0.7449
[10/28 21:21:49 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1915, average loss: 0.7216
[10/28 21:21:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.86	
[10/28 21:21:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/28 21:23:21 visual_prompt]: 	Training 100/553. train loss: 0.7587,	0.5120 s / batch. (data: 2.70e-04). ETA=6:44:58, max mem: 11.4 GB 
[10/28 21:24:49 visual_prompt]: 	Training 200/553. train loss: 0.6664,	0.5109 s / batch. (data: 2.74e-04). ETA=6:43:13, max mem: 11.4 GB 
[10/28 21:26:18 visual_prompt]: 	Training 300/553. train loss: 0.7540,	0.5114 s / batch. (data: 5.38e-03). ETA=6:42:47, max mem: 11.4 GB 
[10/28 21:27:45 visual_prompt]: 	Training 400/553. train loss: 0.6084,	0.5035 s / batch. (data: 5.41e-03). ETA=6:35:43, max mem: 11.4 GB 
[10/28 21:29:14 visual_prompt]: 	Training 500/553. train loss: 0.9062,	0.5127 s / batch. (data: 3.32e-04). ETA=6:42:08, max mem: 11.4 GB 
[10/28 21:30:01 visual_prompt]: Epoch 15 / 100: avg data time: 3.93e-01, avg batch time: 0.8884, average train loss: 0.7475
[10/28 21:30:53 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1917, average loss: 0.7143
[10/28 21:30:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.04	
[10/28 21:30:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/28 21:32:24 visual_prompt]: 	Training 100/553. train loss: 0.5635,	0.4921 s / batch. (data: 3.01e-04). ETA=6:24:41, max mem: 11.4 GB 
[10/28 21:33:53 visual_prompt]: 	Training 200/553. train loss: 0.7201,	0.4955 s / batch. (data: 5.39e-03). ETA=6:26:30, max mem: 11.4 GB 
[10/28 21:35:21 visual_prompt]: 	Training 300/553. train loss: 1.1128,	0.5240 s / batch. (data: 5.38e-03). ETA=6:47:53, max mem: 11.4 GB 
[10/28 21:36:50 visual_prompt]: 	Training 400/553. train loss: 0.7491,	0.4920 s / batch. (data: 3.26e-04). ETA=6:22:07, max mem: 11.4 GB 
[10/28 21:38:18 visual_prompt]: 	Training 500/553. train loss: 1.1506,	1.9271 s / batch. (data: 1.45e+00). ETA=1 day, 0:53:37, max mem: 11.4 GB 
[10/28 21:39:04 visual_prompt]: Epoch 16 / 100: avg data time: 3.92e-01, avg batch time: 0.8869, average train loss: 0.7282
[10/28 21:39:56 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1951, average loss: 0.6988
[10/28 21:39:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.72	
[10/28 21:39:56 visual_prompt]: Best epoch 16: best metric: -0.699
[10/28 21:39:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/28 21:41:27 visual_prompt]: 	Training 100/553. train loss: 0.5613,	0.4920 s / batch. (data: 7.95e-03). ETA=6:20:04, max mem: 11.4 GB 
[10/28 21:42:57 visual_prompt]: 	Training 200/553. train loss: 0.8139,	0.5000 s / batch. (data: 2.70e-04). ETA=6:25:26, max mem: 11.4 GB 
[10/28 21:44:25 visual_prompt]: 	Training 300/553. train loss: 1.0045,	0.4920 s / batch. (data: 2.68e-04). ETA=6:18:28, max mem: 11.4 GB 
[10/28 21:45:52 visual_prompt]: 	Training 400/553. train loss: 0.8070,	1.5219 s / batch. (data: 1.05e+00). ETA=19:28:08, max mem: 11.4 GB 
[10/28 21:47:21 visual_prompt]: 	Training 500/553. train loss: 0.6029,	1.9827 s / batch. (data: 1.49e+00). ETA=1 day, 1:18:27, max mem: 11.4 GB 
[10/28 21:48:06 visual_prompt]: Epoch 17 / 100: avg data time: 3.92e-01, avg batch time: 0.8864, average train loss: 0.7395
[10/28 21:48:58 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1910, average loss: 0.7093
[10/28 21:48:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.12	
[10/28 21:48:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/28 21:50:30 visual_prompt]: 	Training 100/553. train loss: 0.7287,	0.4934 s / batch. (data: 1.04e-02). ETA=6:16:37, max mem: 11.4 GB 
[10/28 21:52:01 visual_prompt]: 	Training 200/553. train loss: 0.7421,	0.5114 s / batch. (data: 5.80e-03). ETA=6:29:31, max mem: 11.4 GB 
[10/28 21:53:29 visual_prompt]: 	Training 300/553. train loss: 0.6771,	0.5000 s / batch. (data: 7.96e-03). ETA=6:20:00, max mem: 11.4 GB 
[10/28 21:54:57 visual_prompt]: 	Training 400/553. train loss: 0.6863,	0.4960 s / batch. (data: 2.81e-04). ETA=6:16:06, max mem: 11.4 GB 
[10/28 21:56:25 visual_prompt]: 	Training 500/553. train loss: 0.6973,	0.5080 s / batch. (data: 2.55e-04). ETA=6:24:23, max mem: 11.4 GB 
[10/28 21:57:09 visual_prompt]: Epoch 18 / 100: avg data time: 3.94e-01, avg batch time: 0.8873, average train loss: 0.7230
[10/28 21:58:02 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.1934, average loss: 0.7481
[10/28 21:58:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.85	
[10/28 21:58:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/28 21:59:34 visual_prompt]: 	Training 100/553. train loss: 1.0526,	0.5071 s / batch. (data: 1.19e-02). ETA=6:22:22, max mem: 11.4 GB 
[10/28 22:01:02 visual_prompt]: 	Training 200/553. train loss: 0.8016,	0.5133 s / batch. (data: 1.05e-02). ETA=6:26:14, max mem: 11.4 GB 
[10/28 22:02:32 visual_prompt]: 	Training 300/553. train loss: 1.0726,	0.4797 s / batch. (data: 2.71e-04). ETA=6:00:10, max mem: 11.4 GB 
[10/28 22:04:01 visual_prompt]: 	Training 400/553. train loss: 0.5696,	0.5216 s / batch. (data: 1.09e-02). ETA=6:30:42, max mem: 11.4 GB 
[10/28 22:05:26 visual_prompt]: 	Training 500/553. train loss: 0.9710,	0.4847 s / batch. (data: 2.95e-04). ETA=6:02:17, max mem: 11.4 GB 
[10/28 22:06:13 visual_prompt]: Epoch 19 / 100: avg data time: 3.95e-01, avg batch time: 0.8884, average train loss: 0.7289
[10/28 22:07:06 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1916, average loss: 0.6950
[10/28 22:07:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.15	
[10/28 22:07:06 visual_prompt]: Best epoch 19: best metric: -0.695
[10/28 22:07:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/28 22:08:36 visual_prompt]: 	Training 100/553. train loss: 0.7686,	0.4798 s / batch. (data: 3.60e-04). ETA=5:57:24, max mem: 11.4 GB 
[10/28 22:10:06 visual_prompt]: 	Training 200/553. train loss: 0.5904,	0.5370 s / batch. (data: 3.31e-02). ETA=6:39:06, max mem: 11.4 GB 
[10/28 22:11:35 visual_prompt]: 	Training 300/553. train loss: 0.7119,	0.5418 s / batch. (data: 3.37e-02). ETA=6:41:44, max mem: 11.4 GB 
[10/28 22:13:04 visual_prompt]: 	Training 400/553. train loss: 0.5733,	0.4945 s / batch. (data: 1.55e-02). ETA=6:05:50, max mem: 11.4 GB 
[10/28 22:14:31 visual_prompt]: 	Training 500/553. train loss: 0.7782,	0.5396 s / batch. (data: 1.55e-02). ETA=6:38:21, max mem: 11.4 GB 
[10/28 22:15:18 visual_prompt]: Epoch 20 / 100: avg data time: 3.96e-01, avg batch time: 0.8906, average train loss: 0.7473
[10/28 22:16:10 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1905, average loss: 0.7772
[10/28 22:16:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.03	
[10/28 22:16:10 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/28 22:17:44 visual_prompt]: 	Training 100/553. train loss: 0.5661,	0.5040 s / batch. (data: 2.89e-04). ETA=6:10:46, max mem: 11.4 GB 
[10/28 22:19:12 visual_prompt]: 	Training 200/553. train loss: 0.7017,	0.4920 s / batch. (data: 2.67e-04). ETA=6:01:07, max mem: 11.4 GB 
[10/28 22:20:41 visual_prompt]: 	Training 300/553. train loss: 0.8976,	1.6840 s / batch. (data: 1.20e+00). ETA=20:33:17, max mem: 11.4 GB 
[10/28 22:22:07 visual_prompt]: 	Training 400/553. train loss: 0.6239,	0.4958 s / batch. (data: 1.05e-02). ETA=6:02:16, max mem: 11.4 GB 
[10/28 22:23:38 visual_prompt]: 	Training 500/553. train loss: 0.7009,	0.4923 s / batch. (data: 7.82e-03). ETA=5:58:52, max mem: 11.4 GB 
[10/28 22:24:22 visual_prompt]: Epoch 21 / 100: avg data time: 3.95e-01, avg batch time: 0.8890, average train loss: 0.7312
[10/28 22:25:15 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1922, average loss: 0.8381
[10/28 22:25:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.85	
[10/28 22:25:15 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/28 22:26:46 visual_prompt]: 	Training 100/553. train loss: 0.7084,	0.4917 s / batch. (data: 2.62e-04). ETA=5:57:13, max mem: 11.4 GB 
[10/28 22:28:15 visual_prompt]: 	Training 200/553. train loss: 0.5736,	0.4907 s / batch. (data: 2.76e-04). ETA=5:55:39, max mem: 11.4 GB 
[10/28 22:29:42 visual_prompt]: 	Training 300/553. train loss: 0.4279,	0.4979 s / batch. (data: 8.72e-03). ETA=6:00:02, max mem: 11.4 GB 
[10/28 22:31:12 visual_prompt]: 	Training 400/553. train loss: 0.9886,	0.5000 s / batch. (data: 2.52e-04). ETA=6:00:42, max mem: 11.4 GB 
[10/28 22:32:41 visual_prompt]: 	Training 500/553. train loss: 0.7406,	0.4841 s / batch. (data: 2.64e-04). ETA=5:48:27, max mem: 11.4 GB 
[10/28 22:33:28 visual_prompt]: Epoch 22 / 100: avg data time: 3.98e-01, avg batch time: 0.8919, average train loss: 0.7746
[10/28 22:34:20 visual_prompt]: Inference (val):avg data time: 3.81e-04, avg batch time: 0.1916, average loss: 0.7023
[10/28 22:34:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.01	
[10/28 22:34:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/28 22:35:53 visual_prompt]: 	Training 100/553. train loss: 0.6867,	0.4831 s / batch. (data: 5.37e-03). ETA=5:46:31, max mem: 11.4 GB 
[10/28 22:37:23 visual_prompt]: 	Training 200/553. train loss: 0.5947,	1.3363 s / batch. (data: 8.46e-01). ETA=15:56:12, max mem: 11.4 GB 
[10/28 22:38:52 visual_prompt]: 	Training 300/553. train loss: 1.2600,	0.5032 s / batch. (data: 7.35e-04). ETA=5:59:15, max mem: 11.4 GB 
[10/28 22:40:19 visual_prompt]: 	Training 400/553. train loss: 0.5616,	0.5085 s / batch. (data: 3.93e-04). ETA=6:02:11, max mem: 11.4 GB 
[10/28 22:41:45 visual_prompt]: 	Training 500/553. train loss: 0.9461,	0.5080 s / batch. (data: 2.79e-04). ETA=6:00:57, max mem: 11.4 GB 
[10/28 22:42:31 visual_prompt]: Epoch 23 / 100: avg data time: 3.93e-01, avg batch time: 0.8869, average train loss: 0.7349
[10/28 22:43:23 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1943, average loss: 0.6886
[10/28 22:43:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.24	
[10/28 22:43:23 visual_prompt]: Best epoch 23: best metric: -0.689
[10/28 22:43:23 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/28 22:44:52 visual_prompt]: 	Training 100/553. train loss: 0.7220,	0.4920 s / batch. (data: 7.96e-03). ETA=5:48:21, max mem: 11.4 GB 
[10/28 22:46:21 visual_prompt]: 	Training 200/553. train loss: 0.6895,	0.5097 s / batch. (data: 5.82e-03). ETA=6:00:01, max mem: 11.4 GB 
[10/28 22:47:50 visual_prompt]: 	Training 300/553. train loss: 0.7244,	1.4798 s / batch. (data: 9.80e-01). ETA=17:22:49, max mem: 11.4 GB 
[10/28 22:49:19 visual_prompt]: 	Training 400/553. train loss: 0.6113,	0.5143 s / batch. (data: 3.10e-04). ETA=6:01:35, max mem: 11.4 GB 
[10/28 22:50:49 visual_prompt]: 	Training 500/553. train loss: 0.8161,	0.7913 s / batch. (data: 3.12e-01). ETA=9:14:57, max mem: 11.4 GB 
[10/28 22:51:36 visual_prompt]: Epoch 24 / 100: avg data time: 3.95e-01, avg batch time: 0.8899, average train loss: 0.7356
[10/28 22:52:28 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1906, average loss: 0.6891
[10/28 22:52:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.71	
[10/28 22:52:28 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/28 22:54:04 visual_prompt]: 	Training 100/553. train loss: 0.6330,	0.4771 s / batch. (data: 2.66e-04). ETA=5:33:24, max mem: 11.4 GB 
[10/28 22:55:31 visual_prompt]: 	Training 200/553. train loss: 0.7301,	0.5261 s / batch. (data: 1.02e-02). ETA=6:06:43, max mem: 11.4 GB 
[10/28 22:56:59 visual_prompt]: 	Training 300/553. train loss: 0.6969,	0.4840 s / batch. (data: 2.83e-04). ETA=5:36:36, max mem: 11.4 GB 
[10/28 22:58:27 visual_prompt]: 	Training 400/553. train loss: 0.6533,	1.6711 s / batch. (data: 1.18e+00). ETA=19:19:25, max mem: 11.4 GB 
[10/28 22:59:56 visual_prompt]: 	Training 500/553. train loss: 0.8251,	1.9914 s / batch. (data: 1.51e+00). ETA=22:58:17, max mem: 11.4 GB 
[10/28 23:00:41 visual_prompt]: Epoch 25 / 100: avg data time: 3.97e-01, avg batch time: 0.8913, average train loss: 0.7301
[10/28 23:01:34 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1917, average loss: 0.7333
[10/28 23:01:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.07	
[10/28 23:01:34 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[10/28 23:03:06 visual_prompt]: 	Training 100/553. train loss: 0.6122,	0.5364 s / batch. (data: 2.04e-02). ETA=6:09:52, max mem: 11.4 GB 
[10/28 23:04:36 visual_prompt]: 	Training 200/553. train loss: 0.7032,	2.1680 s / batch. (data: 1.68e+00). ETA=1 day, 0:51:24, max mem: 11.4 GB 
[10/28 23:06:05 visual_prompt]: 	Training 300/553. train loss: 0.4920,	0.5044 s / batch. (data: 7.60e-03). ETA=5:46:08, max mem: 11.4 GB 
[10/28 23:07:33 visual_prompt]: 	Training 400/553. train loss: 0.5732,	0.4965 s / batch. (data: 2.61e-04). ETA=5:39:54, max mem: 11.4 GB 
[10/28 23:09:00 visual_prompt]: 	Training 500/553. train loss: 0.6860,	0.4932 s / batch. (data: 2.52e-04). ETA=5:36:47, max mem: 11.4 GB 
[10/28 23:09:45 visual_prompt]: Epoch 26 / 100: avg data time: 3.94e-01, avg batch time: 0.8880, average train loss: 0.7418
[10/28 23:10:37 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1913, average loss: 0.7723
[10/28 23:10:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.05	
[10/28 23:10:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[10/28 23:12:09 visual_prompt]: 	Training 100/553. train loss: 0.5762,	0.4878 s / batch. (data: 1.04e-02). ETA=5:31:54, max mem: 11.4 GB 
[10/28 23:13:37 visual_prompt]: 	Training 200/553. train loss: 0.7588,	2.1440 s / batch. (data: 1.67e+00). ETA=1 day, 0:15:09, max mem: 11.4 GB 
[10/28 23:15:07 visual_prompt]: 	Training 300/553. train loss: 0.6188,	1.2042 s / batch. (data: 7.21e-01). ETA=13:35:18, max mem: 11.4 GB 
[10/28 23:16:35 visual_prompt]: 	Training 400/553. train loss: 0.7773,	0.4858 s / batch. (data: 5.32e-03). ETA=5:28:05, max mem: 11.4 GB 
[10/28 23:18:05 visual_prompt]: 	Training 500/553. train loss: 0.9906,	0.5228 s / batch. (data: 5.80e-03). ETA=5:52:11, max mem: 11.4 GB 
[10/28 23:18:48 visual_prompt]: Epoch 27 / 100: avg data time: 3.94e-01, avg batch time: 0.8877, average train loss: 0.7448
[10/28 23:19:41 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1917, average loss: 0.7122
[10/28 23:19:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.77	
[10/28 23:19:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[10/28 23:21:12 visual_prompt]: 	Training 100/553. train loss: 0.4192,	0.4925 s / batch. (data: 2.65e-04). ETA=5:30:31, max mem: 11.4 GB 
[10/28 23:22:41 visual_prompt]: 	Training 200/553. train loss: 0.6175,	0.4769 s / batch. (data: 2.56e-04). ETA=5:19:14, max mem: 11.4 GB 
[10/28 23:24:10 visual_prompt]: 	Training 300/553. train loss: 0.6075,	1.9555 s / batch. (data: 1.48e+00). ETA=21:45:56, max mem: 11.4 GB 
[10/28 23:25:37 visual_prompt]: 	Training 400/553. train loss: 0.7108,	0.5055 s / batch. (data: 1.04e-02). ETA=5:36:46, max mem: 11.4 GB 
[10/28 23:27:04 visual_prompt]: 	Training 500/553. train loss: 0.4510,	0.5040 s / batch. (data: 2.90e-04). ETA=5:34:53, max mem: 11.4 GB 
[10/28 23:27:51 visual_prompt]: Epoch 28 / 100: avg data time: 3.93e-01, avg batch time: 0.8860, average train loss: 0.7238
[10/28 23:28:43 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1906, average loss: 0.7692
[10/28 23:28:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.14	
[10/28 23:28:43 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[10/28 23:30:21 visual_prompt]: 	Training 100/553. train loss: 0.5239,	0.5049 s / batch. (data: 5.87e-03). ETA=5:34:11, max mem: 11.4 GB 
[10/28 23:31:49 visual_prompt]: 	Training 200/553. train loss: 0.7101,	2.3539 s / batch. (data: 1.86e+00). ETA=1 day, 1:54:12, max mem: 11.4 GB 
[10/28 23:33:15 visual_prompt]: 	Training 300/553. train loss: 0.7162,	0.4840 s / batch. (data: 2.16e-04). ETA=5:18:45, max mem: 11.4 GB 
[10/28 23:34:40 visual_prompt]: 	Training 400/553. train loss: 0.5804,	2.1539 s / batch. (data: 1.66e+00). ETA=23:34:57, max mem: 11.4 GB 
[10/28 23:36:08 visual_prompt]: 	Training 500/553. train loss: 0.6964,	0.7554 s / batch. (data: 2.63e-01). ETA=8:14:59, max mem: 11.4 GB 
[10/28 23:36:54 visual_prompt]: Epoch 29 / 100: avg data time: 3.94e-01, avg batch time: 0.8870, average train loss: 0.7342
[10/28 23:37:46 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1908, average loss: 0.6984
[10/28 23:37:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.16	
[10/28 23:37:46 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[10/28 23:39:17 visual_prompt]: 	Training 100/553. train loss: 0.7333,	0.4905 s / batch. (data: 2.53e-04). ETA=5:20:09, max mem: 11.4 GB 
[10/28 23:40:45 visual_prompt]: 	Training 200/553. train loss: 0.7558,	0.4921 s / batch. (data: 2.70e-04). ETA=5:20:24, max mem: 11.4 GB 
[10/28 23:42:13 visual_prompt]: 	Training 300/553. train loss: 0.4592,	1.3055 s / batch. (data: 8.24e-01). ETA=14:07:48, max mem: 11.4 GB 
[10/28 23:43:44 visual_prompt]: 	Training 400/553. train loss: 0.7039,	1.8080 s / batch. (data: 1.29e+00). ETA=19:31:04, max mem: 11.4 GB 
[10/28 23:45:11 visual_prompt]: 	Training 500/553. train loss: 0.6153,	2.0080 s / batch. (data: 1.52e+00). ETA=21:37:16, max mem: 11.4 GB 
[10/28 23:45:57 visual_prompt]: Epoch 30 / 100: avg data time: 3.94e-01, avg batch time: 0.8875, average train loss: 0.7453
[10/28 23:46:50 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1918, average loss: 0.7035
[10/28 23:46:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.02	
[10/28 23:46:50 visual_prompt]: Stopping early.
[10/28 23:46:50 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 23:46:50 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 23:46:50 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 23:46:50 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 23:46:50 visual_prompt]: Training with config:
[10/28 23:46:50 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 23:46:50 visual_prompt]: Loading training data...
[10/28 23:46:50 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 23:46:50 visual_prompt]: Loading validation data...
[10/28 23:46:50 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 23:46:50 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/28 23:46:52 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/28 23:46:52 visual_prompt]: tuned percent:0.529
[10/28 23:46:52 visual_prompt]: Device used for model: 0
[10/28 23:46:52 visual_prompt]: Setting up Evaluator...
[10/28 23:46:52 visual_prompt]: Setting up Trainer...
[10/28 23:46:52 visual_prompt]: 	Setting up the optimizer...
[10/28 23:46:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 23:48:24 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5055 s / batch. (data: 5.40e-03). ETA=7:45:02, max mem: 11.4 GB 
[10/28 23:49:50 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4896 s / batch. (data: 2.78e-04). ETA=7:29:34, max mem: 11.4 GB 
[10/28 23:51:22 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.8920 s / batch. (data: 2.39e+00). ETA=1 day, 20:10:59, max mem: 11.4 GB 
[10/28 23:52:47 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5519 s / batch. (data: 5.93e-02). ETA=8:25:00, max mem: 11.4 GB 
[10/28 23:54:18 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4937 s / batch. (data: 2.56e-04). ETA=7:30:54, max mem: 11.4 GB 
[10/28 23:55:04 visual_prompt]: Epoch 1 / 100: avg data time: 3.95e-01, avg batch time: 0.8888, average train loss: 1.3966
[10/28 23:55:56 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1915, average loss: 1.3454
[10/28 23:55:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/28 23:55:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/28 23:57:28 visual_prompt]: 	Training 100/553. train loss: 0.6474,	1.2455 s / batch. (data: 7.31e-01). ETA=18:54:24, max mem: 11.4 GB 
[10/28 23:58:56 visual_prompt]: 	Training 200/553. train loss: 0.2329,	1.1442 s / batch. (data: 6.48e-01). ETA=17:20:11, max mem: 11.4 GB 
[10/29 00:00:26 visual_prompt]: 	Training 300/553. train loss: 0.7310,	1.6926 s / batch. (data: 1.19e+00). ETA=1 day, 1:35:55, max mem: 11.4 GB 
[10/29 00:01:52 visual_prompt]: 	Training 400/553. train loss: 0.9559,	0.6800 s / batch. (data: 1.83e-01). ETA=10:15:55, max mem: 11.4 GB 
[10/29 00:03:23 visual_prompt]: 	Training 500/553. train loss: 0.6164,	0.4844 s / batch. (data: 5.44e-03). ETA=7:17:56, max mem: 11.4 GB 
[10/29 00:04:08 visual_prompt]: Epoch 2 / 100: avg data time: 3.93e-01, avg batch time: 0.8885, average train loss: 0.7766
[10/29 00:05:00 visual_prompt]: Inference (val):avg data time: 4.13e-04, avg batch time: 0.1921, average loss: 0.7213
[10/29 00:05:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.35	
[10/29 00:05:00 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/29 00:06:30 visual_prompt]: 	Training 100/553. train loss: 0.7372,	0.4911 s / batch. (data: 2.93e-04). ETA=7:22:46, max mem: 11.4 GB 
[10/29 00:08:00 visual_prompt]: 	Training 200/553. train loss: 0.7217,	0.4981 s / batch. (data: 3.76e-04). ETA=7:28:12, max mem: 11.4 GB 
[10/29 00:09:27 visual_prompt]: 	Training 300/553. train loss: 0.6024,	0.5120 s / batch. (data: 2.73e-04). ETA=7:39:54, max mem: 11.4 GB 
[10/29 00:10:57 visual_prompt]: 	Training 400/553. train loss: 0.5457,	0.5014 s / batch. (data: 1.34e-02). ETA=7:29:31, max mem: 11.4 GB 
[10/29 00:12:27 visual_prompt]: 	Training 500/553. train loss: 0.7150,	1.7870 s / batch. (data: 1.28e+00). ETA=1 day, 2:39:08, max mem: 11.4 GB 
[10/29 00:13:11 visual_prompt]: Epoch 3 / 100: avg data time: 3.92e-01, avg batch time: 0.8881, average train loss: 0.7580
[10/29 00:14:04 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1898, average loss: 0.7801
[10/29 00:14:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.05	
[10/29 00:14:04 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/29 00:15:36 visual_prompt]: 	Training 100/553. train loss: 0.7383,	0.4919 s / batch. (data: 2.73e-04). ETA=7:18:57, max mem: 11.4 GB 
[10/29 00:17:06 visual_prompt]: 	Training 200/553. train loss: 0.8426,	0.4937 s / batch. (data: 2.89e-04). ETA=7:19:43, max mem: 11.4 GB 
[10/29 00:18:34 visual_prompt]: 	Training 300/553. train loss: 0.5948,	1.2804 s / batch. (data: 7.86e-01). ETA=18:58:19, max mem: 11.4 GB 
[10/29 00:19:59 visual_prompt]: 	Training 400/553. train loss: 0.6190,	1.1262 s / batch. (data: 6.48e-01). ETA=16:39:22, max mem: 11.4 GB 
[10/29 00:21:29 visual_prompt]: 	Training 500/553. train loss: 0.7330,	4.0205 s / batch. (data: 3.52e+00). ETA=2 days, 11:20:54, max mem: 11.4 GB 
[10/29 00:22:15 visual_prompt]: Epoch 4 / 100: avg data time: 3.96e-01, avg batch time: 0.8893, average train loss: 0.7880
[10/29 00:23:08 visual_prompt]: Inference (val):avg data time: 5.36e-05, avg batch time: 0.1906, average loss: 0.6928
[10/29 00:23:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.83	
[10/29 00:23:08 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/29 00:24:38 visual_prompt]: 	Training 100/553. train loss: 0.6195,	0.4920 s / batch. (data: 2.76e-04). ETA=7:14:31, max mem: 11.4 GB 
[10/29 00:26:07 visual_prompt]: 	Training 200/553. train loss: 0.8007,	1.6160 s / batch. (data: 1.14e+00). ETA=23:44:25, max mem: 11.4 GB 
[10/29 00:27:36 visual_prompt]: 	Training 300/553. train loss: 0.9658,	0.4884 s / batch. (data: 5.39e-03). ETA=7:09:41, max mem: 11.4 GB 
[10/29 00:29:04 visual_prompt]: 	Training 400/553. train loss: 0.7755,	0.5040 s / batch. (data: 5.42e-03). ETA=7:22:33, max mem: 11.4 GB 
[10/29 00:30:33 visual_prompt]: 	Training 500/553. train loss: 0.6312,	0.4845 s / batch. (data: 2.74e-04). ETA=7:04:39, max mem: 11.4 GB 
[10/29 00:31:19 visual_prompt]: Epoch 5 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 0.8068
[10/29 00:32:11 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1896, average loss: 0.7650
[10/29 00:32:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.59	
[10/29 00:32:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/29 00:33:44 visual_prompt]: 	Training 100/553. train loss: 0.6325,	0.4840 s / batch. (data: 2.74e-04). ETA=7:02:59, max mem: 11.4 GB 
[10/29 00:35:12 visual_prompt]: 	Training 200/553. train loss: 0.5579,	0.4880 s / batch. (data: 2.66e-04). ETA=7:05:39, max mem: 11.4 GB 
[10/29 00:36:39 visual_prompt]: 	Training 300/553. train loss: 0.5548,	0.5000 s / batch. (data: 7.97e-03). ETA=7:15:16, max mem: 11.4 GB 
[10/29 00:38:11 visual_prompt]: 	Training 400/553. train loss: 0.5823,	0.4786 s / batch. (data: 3.19e-04). ETA=6:55:49, max mem: 11.4 GB 
[10/29 00:39:39 visual_prompt]: 	Training 500/553. train loss: 0.9574,	1.5080 s / batch. (data: 1.01e+00). ETA=21:47:48, max mem: 11.4 GB 
[10/29 00:40:23 visual_prompt]: Epoch 6 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 0.7674
[10/29 00:41:16 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1918, average loss: 0.6819
[10/29 00:41:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.73	
[10/29 00:41:16 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/29 00:42:47 visual_prompt]: 	Training 100/553. train loss: 0.6956,	0.4790 s / batch. (data: 2.50e-04). ETA=6:54:10, max mem: 11.4 GB 
[10/29 00:44:15 visual_prompt]: 	Training 200/553. train loss: 0.5178,	0.4789 s / batch. (data: 2.41e-04). ETA=6:53:20, max mem: 11.4 GB 
[10/29 00:45:46 visual_prompt]: 	Training 300/553. train loss: 0.7898,	1.9237 s / batch. (data: 1.44e+00). ETA=1 day, 3:36:58, max mem: 11.4 GB 
[10/29 00:47:15 visual_prompt]: 	Training 400/553. train loss: 0.5944,	2.4208 s / batch. (data: 1.93e+00). ETA=1 day, 10:41:07, max mem: 11.4 GB 
[10/29 00:48:41 visual_prompt]: 	Training 500/553. train loss: 1.0525,	0.5301 s / batch. (data: 5.17e-02). ETA=7:34:49, max mem: 11.4 GB 
[10/29 00:49:26 visual_prompt]: Epoch 7 / 100: avg data time: 3.91e-01, avg batch time: 0.8864, average train loss: 0.7814
[10/29 00:50:19 visual_prompt]: Inference (val):avg data time: 1.82e-04, avg batch time: 0.1925, average loss: 0.7764
[10/29 00:50:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.82	
[10/29 00:50:19 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/29 00:51:49 visual_prompt]: 	Training 100/553. train loss: 0.7052,	1.3960 s / batch. (data: 8.99e-01). ETA=19:54:15, max mem: 11.4 GB 
[10/29 00:53:18 visual_prompt]: 	Training 200/553. train loss: 1.3532,	0.5040 s / batch. (data: 7.99e-03). ETA=7:10:18, max mem: 11.4 GB 
[10/29 00:54:48 visual_prompt]: 	Training 300/553. train loss: 0.8492,	0.4931 s / batch. (data: 2.63e-04). ETA=7:00:12, max mem: 11.4 GB 
[10/29 00:56:16 visual_prompt]: 	Training 400/553. train loss: 0.7410,	0.7938 s / batch. (data: 3.10e-01). ETA=11:15:07, max mem: 11.4 GB 
[10/29 00:57:46 visual_prompt]: 	Training 500/553. train loss: 1.1030,	2.0199 s / batch. (data: 1.53e+00). ETA=1 day, 4:34:33, max mem: 11.4 GB 
[10/29 00:58:30 visual_prompt]: Epoch 8 / 100: avg data time: 3.94e-01, avg batch time: 0.8889, average train loss: 0.7817
[10/29 00:59:23 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1909, average loss: 0.8581
[10/29 00:59:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.78	
[10/29 00:59:23 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/29 01:00:54 visual_prompt]: 	Training 100/553. train loss: 0.6491,	0.4989 s / batch. (data: 1.05e-02). ETA=7:02:12, max mem: 11.4 GB 
[10/29 01:02:22 visual_prompt]: 	Training 200/553. train loss: 0.7934,	0.5119 s / batch. (data: 2.79e-04). ETA=7:12:23, max mem: 11.4 GB 
[10/29 01:03:51 visual_prompt]: 	Training 300/553. train loss: 0.5166,	2.2800 s / batch. (data: 1.78e+00). ETA=1 day, 8:01:52, max mem: 11.4 GB 
[10/29 01:05:21 visual_prompt]: 	Training 400/553. train loss: 0.6478,	0.4788 s / batch. (data: 2.76e-04). ETA=6:42:46, max mem: 11.4 GB 
[10/29 01:06:50 visual_prompt]: 	Training 500/553. train loss: 1.0292,	0.7391 s / batch. (data: 2.51e-01). ETA=10:20:34, max mem: 11.4 GB 
[10/29 01:07:34 visual_prompt]: Epoch 9 / 100: avg data time: 3.94e-01, avg batch time: 0.8890, average train loss: 0.7846
[10/29 01:08:27 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1890, average loss: 0.7947
[10/29 01:08:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.93	
[10/29 01:08:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/29 01:10:00 visual_prompt]: 	Training 100/553. train loss: 0.6445,	0.4889 s / batch. (data: 2.80e-04). ETA=6:49:11, max mem: 11.4 GB 
[10/29 01:11:28 visual_prompt]: 	Training 200/553. train loss: 0.5955,	0.5118 s / batch. (data: 2.51e-04). ETA=7:07:32, max mem: 11.4 GB 
[10/29 01:12:56 visual_prompt]: 	Training 300/553. train loss: 0.6050,	0.4918 s / batch. (data: 6.16e-03). ETA=6:50:02, max mem: 11.4 GB 
[10/29 01:14:22 visual_prompt]: 	Training 400/553. train loss: 0.7479,	1.3749 s / batch. (data: 8.74e-01). ETA=19:03:57, max mem: 11.4 GB 
[10/29 01:15:51 visual_prompt]: 	Training 500/553. train loss: 1.6482,	0.7640 s / batch. (data: 2.70e-01). ETA=10:34:24, max mem: 11.4 GB 
[10/29 01:16:37 visual_prompt]: Epoch 10 / 100: avg data time: 3.92e-01, avg batch time: 0.8868, average train loss: 0.8032
[10/29 01:17:30 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.1934, average loss: 1.0692
[10/29 01:17:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.61	
[10/29 01:17:30 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/29 01:19:03 visual_prompt]: 	Training 100/553. train loss: 0.5997,	0.4880 s / batch. (data: 2.67e-04). ETA=6:43:59, max mem: 11.4 GB 
[10/29 01:20:34 visual_prompt]: 	Training 200/553. train loss: 1.7100,	0.5004 s / batch. (data: 5.39e-03). ETA=6:53:23, max mem: 11.4 GB 
[10/29 01:22:02 visual_prompt]: 	Training 300/553. train loss: 0.6418,	2.1560 s / batch. (data: 1.67e+00). ETA=1 day, 5:37:36, max mem: 11.4 GB 
[10/29 01:23:30 visual_prompt]: 	Training 400/553. train loss: 0.7521,	0.4880 s / batch. (data: 2.77e-04). ETA=6:41:33, max mem: 11.4 GB 
[10/29 01:24:58 visual_prompt]: 	Training 500/553. train loss: 0.7251,	0.5041 s / batch. (data: 2.43e-02). ETA=6:53:58, max mem: 11.4 GB 
[10/29 01:25:42 visual_prompt]: Epoch 11 / 100: avg data time: 3.97e-01, avg batch time: 0.8911, average train loss: 0.8178
[10/29 01:26:35 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1924, average loss: 0.7829
[10/29 01:26:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.53	
[10/29 01:26:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/29 01:28:09 visual_prompt]: 	Training 100/553. train loss: 0.9369,	0.5138 s / batch. (data: 2.66e-04). ETA=7:00:34, max mem: 11.4 GB 
[10/29 01:29:38 visual_prompt]: 	Training 200/553. train loss: 0.6822,	0.5039 s / batch. (data: 1.22e-02). ETA=6:51:40, max mem: 11.4 GB 
[10/29 01:31:05 visual_prompt]: 	Training 300/553. train loss: 0.6545,	0.4788 s / batch. (data: 2.63e-04). ETA=6:30:19, max mem: 11.4 GB 
[10/29 01:32:33 visual_prompt]: 	Training 400/553. train loss: 1.0319,	0.4929 s / batch. (data: 2.49e-04). ETA=6:41:00, max mem: 11.4 GB 
[10/29 01:34:02 visual_prompt]: 	Training 500/553. train loss: 1.7416,	0.5026 s / batch. (data: 2.74e-04). ETA=6:48:06, max mem: 11.4 GB 
[10/29 01:34:46 visual_prompt]: Epoch 12 / 100: avg data time: 3.92e-01, avg batch time: 0.8877, average train loss: 0.7962
[10/29 01:35:39 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1920, average loss: 1.3268
[10/29 01:35:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.17	
[10/29 01:35:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/29 01:37:12 visual_prompt]: 	Training 100/553. train loss: 0.7571,	0.7360 s / batch. (data: 2.56e-01). ETA=9:55:42, max mem: 11.4 GB 
[10/29 01:38:38 visual_prompt]: 	Training 200/553. train loss: 0.7145,	1.1120 s / batch. (data: 6.16e-01). ETA=14:58:11, max mem: 11.4 GB 
[10/29 01:40:07 visual_prompt]: 	Training 300/553. train loss: 0.8553,	2.3560 s / batch. (data: 1.86e+00). ETA=1 day, 7:39:05, max mem: 11.4 GB 
[10/29 01:41:34 visual_prompt]: 	Training 400/553. train loss: 0.9103,	0.4960 s / batch. (data: 2.67e-04). ETA=6:38:58, max mem: 11.4 GB 
[10/29 01:43:03 visual_prompt]: 	Training 500/553. train loss: 0.6679,	0.5188 s / batch. (data: 2.87e-04). ETA=6:56:26, max mem: 11.4 GB 
[10/29 01:43:49 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8874, average train loss: 0.7916
[10/29 01:44:42 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1905, average loss: 0.6945
[10/29 01:44:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.84	
[10/29 01:44:42 visual_prompt]: Best epoch 13: best metric: -0.694
[10/29 01:44:42 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/29 01:46:15 visual_prompt]: 	Training 100/553. train loss: 0.7112,	0.5073 s / batch. (data: 5.60e-03). ETA=6:45:55, max mem: 11.4 GB 
[10/29 01:47:45 visual_prompt]: 	Training 200/553. train loss: 0.6354,	1.9760 s / batch. (data: 1.46e+00). ETA=1 day, 2:17:51, max mem: 11.4 GB 
[10/29 01:49:14 visual_prompt]: 	Training 300/553. train loss: 0.6677,	1.3909 s / batch. (data: 9.13e-01). ETA=18:28:18, max mem: 11.4 GB 
[10/29 01:50:43 visual_prompt]: 	Training 400/553. train loss: 0.7180,	0.5026 s / batch. (data: 2.41e-02). ETA=6:39:39, max mem: 11.4 GB 
[10/29 01:52:11 visual_prompt]: 	Training 500/553. train loss: 0.9826,	0.4782 s / batch. (data: 2.81e-04). ETA=6:19:27, max mem: 11.4 GB 
[10/29 01:52:56 visual_prompt]: Epoch 14 / 100: avg data time: 3.98e-01, avg batch time: 0.8931, average train loss: 0.7608
[10/29 01:53:49 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1920, average loss: 0.7106
[10/29 01:53:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 61.16	
[10/29 01:53:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/29 01:55:21 visual_prompt]: 	Training 100/553. train loss: 0.8377,	0.4840 s / batch. (data: 3.02e-04). ETA=6:22:50, max mem: 11.4 GB 
[10/29 01:56:49 visual_prompt]: 	Training 200/553. train loss: 1.0465,	0.4920 s / batch. (data: 2.44e-04). ETA=6:28:21, max mem: 11.4 GB 
[10/29 01:58:20 visual_prompt]: 	Training 300/553. train loss: 0.4617,	0.5000 s / batch. (data: 2.69e-04). ETA=6:33:49, max mem: 11.4 GB 
[10/29 01:59:47 visual_prompt]: 	Training 400/553. train loss: 0.5276,	0.5000 s / batch. (data: 2.65e-04). ETA=6:33:01, max mem: 11.4 GB 
[10/29 02:01:17 visual_prompt]: 	Training 500/553. train loss: 1.0997,	0.4920 s / batch. (data: 2.87e-04). ETA=6:25:53, max mem: 11.4 GB 
[10/29 02:02:04 visual_prompt]: Epoch 15 / 100: avg data time: 3.99e-01, avg batch time: 0.8943, average train loss: 0.7919
[10/29 02:02:57 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1904, average loss: 0.7907
[10/29 02:02:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.14	
[10/29 02:02:57 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/29 02:04:28 visual_prompt]: 	Training 100/553. train loss: 0.4597,	0.5041 s / batch. (data: 1.04e-02). ETA=6:34:02, max mem: 11.4 GB 
[10/29 02:05:57 visual_prompt]: 	Training 200/553. train loss: 1.0395,	0.4860 s / batch. (data: 7.96e-03). ETA=6:19:08, max mem: 11.4 GB 
[10/29 02:07:26 visual_prompt]: 	Training 300/553. train loss: 0.8358,	0.4908 s / batch. (data: 2.39e-04). ETA=6:22:01, max mem: 11.4 GB 
[10/29 02:08:56 visual_prompt]: 	Training 400/553. train loss: 0.7321,	0.4920 s / batch. (data: 2.61e-04). ETA=6:22:10, max mem: 11.4 GB 
[10/29 02:10:24 visual_prompt]: 	Training 500/553. train loss: 1.2053,	2.0720 s / batch. (data: 1.58e+00). ETA=1 day, 2:45:57, max mem: 11.4 GB 
[10/29 02:11:09 visual_prompt]: Epoch 16 / 100: avg data time: 3.96e-01, avg batch time: 0.8905, average train loss: 0.7583
[10/29 02:12:02 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1891, average loss: 0.6802
[10/29 02:12:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.84	
[10/29 02:12:02 visual_prompt]: Best epoch 16: best metric: -0.680
[10/29 02:12:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/29 02:13:33 visual_prompt]: 	Training 100/553. train loss: 0.4227,	0.5119 s / batch. (data: 4.38e-03). ETA=6:35:25, max mem: 11.4 GB 
[10/29 02:15:03 visual_prompt]: 	Training 200/553. train loss: 0.8255,	0.5000 s / batch. (data: 2.65e-04). ETA=6:25:26, max mem: 11.4 GB 
[10/29 02:16:31 visual_prompt]: 	Training 300/553. train loss: 1.6130,	0.4840 s / batch. (data: 3.34e-04). ETA=6:12:16, max mem: 11.4 GB 
[10/29 02:17:59 visual_prompt]: 	Training 400/553. train loss: 0.6253,	0.5164 s / batch. (data: 3.30e-02). ETA=6:36:21, max mem: 11.4 GB 
[10/29 02:19:26 visual_prompt]: 	Training 500/553. train loss: 0.5106,	1.0275 s / batch. (data: 5.38e-01). ETA=13:06:57, max mem: 11.4 GB 
[10/29 02:20:13 visual_prompt]: Epoch 17 / 100: avg data time: 3.93e-01, avg batch time: 0.8884, average train loss: 0.7343
[10/29 02:21:06 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1909, average loss: 0.8207
[10/29 02:21:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.57	
[10/29 02:21:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/29 02:22:37 visual_prompt]: 	Training 100/553. train loss: 0.8491,	0.4960 s / batch. (data: 2.62e-04). ETA=6:18:36, max mem: 11.4 GB 
[10/29 02:24:09 visual_prompt]: 	Training 200/553. train loss: 0.6995,	0.5363 s / batch. (data: 1.09e-02). ETA=6:48:28, max mem: 11.4 GB 
[10/29 02:25:37 visual_prompt]: 	Training 300/553. train loss: 0.5118,	0.4904 s / batch. (data: 2.55e-04). ETA=6:12:43, max mem: 11.4 GB 
[10/29 02:27:05 visual_prompt]: 	Training 400/553. train loss: 0.7344,	0.4916 s / batch. (data: 2.65e-04). ETA=6:12:45, max mem: 11.4 GB 
[10/29 02:28:33 visual_prompt]: 	Training 500/553. train loss: 0.7121,	0.5058 s / batch. (data: 5.37e-03). ETA=6:22:42, max mem: 11.4 GB 
[10/29 02:29:17 visual_prompt]: Epoch 18 / 100: avg data time: 3.94e-01, avg batch time: 0.8894, average train loss: 0.7576
[10/29 02:30:10 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1917, average loss: 0.6701
[10/29 02:30:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 61.80	
[10/29 02:30:10 visual_prompt]: Best epoch 18: best metric: -0.670
[10/29 02:30:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/29 02:31:41 visual_prompt]: 	Training 100/553. train loss: 0.8989,	0.5171 s / batch. (data: 5.40e-03). ETA=6:29:56, max mem: 11.4 GB 
[10/29 02:33:10 visual_prompt]: 	Training 200/553. train loss: 0.7215,	0.4846 s / batch. (data: 2.52e-04). ETA=6:04:38, max mem: 11.4 GB 
[10/29 02:34:40 visual_prompt]: 	Training 300/553. train loss: 0.9771,	0.5040 s / batch. (data: 2.67e-04). ETA=6:18:22, max mem: 11.4 GB 
[10/29 02:36:10 visual_prompt]: 	Training 400/553. train loss: 0.4966,	0.5200 s / batch. (data: 7.41e-04). ETA=6:29:32, max mem: 11.4 GB 
[10/29 02:37:35 visual_prompt]: 	Training 500/553. train loss: 0.7351,	0.5082 s / batch. (data: 2.69e-04). ETA=6:19:49, max mem: 11.4 GB 
[10/29 02:38:22 visual_prompt]: Epoch 19 / 100: avg data time: 3.95e-01, avg batch time: 0.8902, average train loss: 0.7467
[10/29 02:39:15 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1921, average loss: 0.7115
[10/29 02:39:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 61.59	
[10/29 02:39:15 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/29 02:40:45 visual_prompt]: 	Training 100/553. train loss: 1.6037,	0.5575 s / batch. (data: 5.61e-02). ETA=6:55:14, max mem: 11.4 GB 
[10/29 02:42:15 visual_prompt]: 	Training 200/553. train loss: 0.3812,	0.5068 s / batch. (data: 1.18e-02). ETA=6:16:39, max mem: 11.4 GB 
[10/29 02:43:44 visual_prompt]: 	Training 300/553. train loss: 0.5784,	0.5157 s / batch. (data: 5.85e-03). ETA=6:22:25, max mem: 11.4 GB 
[10/29 02:45:12 visual_prompt]: 	Training 400/553. train loss: 0.5143,	0.4925 s / batch. (data: 7.97e-03). ETA=6:04:24, max mem: 11.4 GB 
[10/29 02:46:39 visual_prompt]: 	Training 500/553. train loss: 1.1153,	0.5159 s / batch. (data: 4.70e-04). ETA=6:20:52, max mem: 11.4 GB 
[10/29 02:47:26 visual_prompt]: Epoch 20 / 100: avg data time: 3.94e-01, avg batch time: 0.8886, average train loss: 0.7661
[10/29 02:48:19 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1917, average loss: 0.8306
[10/29 02:48:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.17	
[10/29 02:48:19 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/29 02:49:53 visual_prompt]: 	Training 100/553. train loss: 0.6189,	0.5000 s / batch. (data: 2.69e-04). ETA=6:07:49, max mem: 11.4 GB 
[10/29 02:51:21 visual_prompt]: 	Training 200/553. train loss: 0.5717,	0.5000 s / batch. (data: 2.36e-04). ETA=6:07:01, max mem: 11.4 GB 
[10/29 02:52:50 visual_prompt]: 	Training 300/553. train loss: 1.1753,	1.6699 s / batch. (data: 1.15e+00). ETA=20:22:57, max mem: 11.4 GB 
[10/29 02:54:17 visual_prompt]: 	Training 400/553. train loss: 0.5016,	0.4889 s / batch. (data: 2.98e-04). ETA=5:57:15, max mem: 11.4 GB 
[10/29 02:55:46 visual_prompt]: 	Training 500/553. train loss: 0.7039,	0.4920 s / batch. (data: 2.69e-04). ETA=5:58:39, max mem: 11.4 GB 
[10/29 02:56:30 visual_prompt]: Epoch 21 / 100: avg data time: 3.94e-01, avg batch time: 0.8882, average train loss: 0.7389
[10/29 02:57:23 visual_prompt]: Inference (val):avg data time: 3.91e-04, avg batch time: 0.1919, average loss: 0.7134
[10/29 02:57:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 60.92	
[10/29 02:57:23 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/29 02:58:54 visual_prompt]: 	Training 100/553. train loss: 0.6564,	0.5120 s / batch. (data: 2.74e-04). ETA=6:11:56, max mem: 11.4 GB 
[10/29 02:00:23 visual_prompt]: 	Training 200/553. train loss: 0.5058,	0.5068 s / batch. (data: 1.04e-02). ETA=6:07:18, max mem: 11.4 GB 
[10/29 02:01:49 visual_prompt]: 	Training 300/553. train loss: 0.2757,	0.5110 s / batch. (data: 2.00e-02). ETA=6:09:32, max mem: 11.4 GB 
[10/29 02:03:18 visual_prompt]: 	Training 400/553. train loss: 0.6565,	0.5199 s / batch. (data: 4.50e-03). ETA=6:15:05, max mem: 11.4 GB 
[10/29 02:04:47 visual_prompt]: 	Training 500/553. train loss: 0.7485,	0.4901 s / batch. (data: 5.39e-03). ETA=5:52:47, max mem: 11.4 GB 
[10/29 02:05:34 visual_prompt]: Epoch 22 / 100: avg data time: 3.93e-01, avg batch time: 0.8882, average train loss: 0.7564
[10/29 02:06:26 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1916, average loss: 0.7852
[10/29 02:06:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.86	
[10/29 02:06:26 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/29 02:07:59 visual_prompt]: 	Training 100/553. train loss: 0.7912,	0.6919 s / batch. (data: 1.96e-01). ETA=8:16:13, max mem: 11.4 GB 
[10/29 02:09:29 visual_prompt]: 	Training 200/553. train loss: 0.6402,	1.3720 s / batch. (data: 8.83e-01). ETA=16:21:44, max mem: 11.4 GB 
[10/29 02:10:58 visual_prompt]: 	Training 300/553. train loss: 1.3872,	0.4924 s / batch. (data: 2.35e-04). ETA=5:51:32, max mem: 11.4 GB 
[10/29 02:12:25 visual_prompt]: 	Training 400/553. train loss: 0.5961,	0.4921 s / batch. (data: 5.39e-03). ETA=5:50:27, max mem: 11.4 GB 
[10/29 02:13:52 visual_prompt]: 	Training 500/553. train loss: 0.7983,	0.4877 s / batch. (data: 8.92e-03). ETA=5:46:32, max mem: 11.4 GB 
[10/29 02:14:37 visual_prompt]: Epoch 23 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 0.7324
[10/29 02:15:29 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1923, average loss: 0.7679
[10/29 02:15:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.48	
[10/29 02:15:29 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/29 02:16:58 visual_prompt]: 	Training 100/553. train loss: 0.9761,	0.4880 s / batch. (data: 5.42e-03). ETA=5:45:31, max mem: 11.4 GB 
[10/29 02:18:27 visual_prompt]: 	Training 200/553. train loss: 0.7766,	0.5134 s / batch. (data: 5.82e-03). ETA=6:02:36, max mem: 11.4 GB 
[10/29 02:19:56 visual_prompt]: 	Training 300/553. train loss: 0.8037,	1.4037 s / batch. (data: 9.27e-01). ETA=16:29:10, max mem: 11.4 GB 
[10/29 02:21:24 visual_prompt]: 	Training 400/553. train loss: 0.6905,	0.4791 s / batch. (data: 2.64e-04). ETA=5:36:46, max mem: 11.4 GB 
[10/29 02:22:55 visual_prompt]: 	Training 500/553. train loss: 0.6645,	0.8919 s / batch. (data: 4.02e-01). ETA=10:25:32, max mem: 11.4 GB 
[10/29 02:23:41 visual_prompt]: Epoch 24 / 100: avg data time: 3.95e-01, avg batch time: 0.8889, average train loss: 0.7462
[10/29 02:24:33 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1904, average loss: 0.6763
[10/29 02:24:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 61.48	
[10/29 02:24:33 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/29 02:26:08 visual_prompt]: 	Training 100/553. train loss: 0.6292,	0.4928 s / batch. (data: 5.41e-03). ETA=5:44:22, max mem: 11.4 GB 
[10/29 02:27:34 visual_prompt]: 	Training 200/553. train loss: 0.8529,	0.5445 s / batch. (data: 6.36e-02). ETA=6:19:33, max mem: 11.4 GB 
[10/29 02:29:01 visual_prompt]: 	Training 300/553. train loss: 0.8143,	0.4922 s / batch. (data: 2.80e-04). ETA=5:42:19, max mem: 11.4 GB 
[10/29 02:30:30 visual_prompt]: 	Training 400/553. train loss: 0.5535,	1.8606 s / batch. (data: 1.38e+00). ETA=21:30:51, max mem: 11.4 GB 
[10/29 02:31:59 visual_prompt]: 	Training 500/553. train loss: 0.8132,	1.7440 s / batch. (data: 1.24e+00). ETA=20:07:03, max mem: 11.4 GB 
[10/29 02:32:44 visual_prompt]: Epoch 25 / 100: avg data time: 3.93e-01, avg batch time: 0.8875, average train loss: 0.7327
[10/29 02:33:37 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1915, average loss: 0.8112
[10/29 02:33:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.85	
[10/29 02:33:37 visual_prompt]: Stopping early.
[10/29 02:33:37 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 02:33:37 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 02:33:37 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 02:33:37 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 02:33:37 visual_prompt]: Training with config:
[10/29 02:33:37 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.1_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 02:33:37 visual_prompt]: Loading training data...
[10/29 02:33:37 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 02:33:37 visual_prompt]: Loading validation data...
[10/29 02:33:37 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 02:33:37 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/29 02:33:40 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/29 02:33:40 visual_prompt]: tuned percent:0.529
[10/29 02:33:40 visual_prompt]: Device used for model: 0
[10/29 02:33:40 visual_prompt]: Setting up Evaluator...
[10/29 02:33:40 visual_prompt]: Setting up Trainer...
[10/29 02:33:40 visual_prompt]: 	Setting up the optimizer...
[10/29 02:33:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 02:35:13 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5186 s / batch. (data: 1.55e-02). ETA=7:57:04, max mem: 11.4 GB 
[10/29 02:36:40 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4880 s / batch. (data: 2.55e-04). ETA=7:28:07, max mem: 11.4 GB 
[10/29 02:38:11 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9439 s / batch. (data: 2.45e+00). ETA=1 day, 20:58:37, max mem: 11.4 GB 
[10/29 02:39:37 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4899 s / batch. (data: 1.05e-02). ETA=7:28:13, max mem: 11.4 GB 
[10/29 02:41:08 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4939 s / batch. (data: 1.55e-02). ETA=7:31:03, max mem: 11.4 GB 
[10/29 02:41:54 visual_prompt]: Epoch 1 / 100: avg data time: 3.98e-01, avg batch time: 0.8929, average train loss: 1.3966
[10/29 02:42:46 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1930, average loss: 1.3454
[10/29 02:42:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/29 02:42:46 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/29 02:44:18 visual_prompt]: 	Training 100/553. train loss: 0.6472,	0.4948 s / batch. (data: 2.70e-04). ETA=7:30:40, max mem: 11.4 GB 
[10/29 02:45:46 visual_prompt]: 	Training 200/553. train loss: 0.2328,	1.2043 s / batch. (data: 7.23e-01). ETA=18:14:49, max mem: 11.4 GB 
[10/29 02:47:17 visual_prompt]: 	Training 300/553. train loss: 0.7310,	1.6438 s / batch. (data: 1.16e+00). ETA=1 day, 0:51:38, max mem: 11.4 GB 
[10/29 02:48:44 visual_prompt]: 	Training 400/553. train loss: 0.9562,	0.5278 s / batch. (data: 1.04e-02). ETA=7:58:01, max mem: 11.4 GB 
[10/29 02:50:14 visual_prompt]: 	Training 500/553. train loss: 0.6165,	0.5094 s / batch. (data: 5.38e-03). ETA=7:40:31, max mem: 11.4 GB 
[10/29 02:50:58 visual_prompt]: Epoch 2 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 0.7766
[10/29 02:51:51 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1922, average loss: 0.7211
[10/29 02:51:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.36	
[10/29 02:51:51 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/29 02:53:21 visual_prompt]: 	Training 100/553. train loss: 0.7376,	0.5080 s / batch. (data: 2.87e-04). ETA=7:38:00, max mem: 11.4 GB 
[10/29 02:54:51 visual_prompt]: 	Training 200/553. train loss: 0.7221,	0.5065 s / batch. (data: 1.04e-02). ETA=7:35:48, max mem: 11.4 GB 
[10/29 02:56:18 visual_prompt]: 	Training 300/553. train loss: 0.6046,	0.4999 s / batch. (data: 7.91e-03). ETA=7:29:03, max mem: 11.4 GB 
[10/29 02:57:48 visual_prompt]: 	Training 400/553. train loss: 0.5678,	0.5040 s / batch. (data: 2.52e-04). ETA=7:31:53, max mem: 11.4 GB 
[10/29 02:59:18 visual_prompt]: 	Training 500/553. train loss: 0.7380,	1.8985 s / batch. (data: 1.42e+00). ETA=1 day, 4:18:57, max mem: 11.4 GB 
[10/29 03:00:02 visual_prompt]: Epoch 3 / 100: avg data time: 3.93e-01, avg batch time: 0.8883, average train loss: 0.7551
[10/29 03:00:55 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1941, average loss: 0.7819
[10/29 03:00:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.41	
[10/29 03:00:55 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/29 03:02:27 visual_prompt]: 	Training 100/553. train loss: 0.7900,	0.4959 s / batch. (data: 1.04e-02). ETA=7:22:28, max mem: 11.4 GB 
[10/29 03:03:57 visual_prompt]: 	Training 200/553. train loss: 0.8321,	0.4957 s / batch. (data: 7.82e-03). ETA=7:21:33, max mem: 11.4 GB 
[10/29 03:05:26 visual_prompt]: 	Training 300/553. train loss: 0.5937,	1.9360 s / batch. (data: 1.43e+00). ETA=1 day, 4:41:07, max mem: 11.4 GB 
[10/29 03:06:51 visual_prompt]: 	Training 400/553. train loss: 0.6077,	1.7089 s / batch. (data: 1.23e+00). ETA=1 day, 1:16:22, max mem: 11.4 GB 
[10/29 03:08:20 visual_prompt]: 	Training 500/553. train loss: 0.7939,	3.4477 s / batch. (data: 2.96e+00). ETA=2 days, 2:53:33, max mem: 11.4 GB 
[10/29 03:09:07 visual_prompt]: Epoch 4 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 0.7916
[10/29 03:09:59 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1937, average loss: 0.6950
[10/29 03:09:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.88	
[10/29 03:09:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/29 03:11:30 visual_prompt]: 	Training 100/553. train loss: 0.5619,	0.5320 s / batch. (data: 1.59e-02). ETA=7:49:47, max mem: 11.4 GB 
[10/29 03:12:58 visual_prompt]: 	Training 200/553. train loss: 0.7953,	1.3452 s / batch. (data: 8.36e-01). ETA=19:45:44, max mem: 11.4 GB 
[10/29 03:14:27 visual_prompt]: 	Training 300/553. train loss: 0.9387,	0.5168 s / batch. (data: 5.76e-03). ETA=7:34:41, max mem: 11.4 GB 
[10/29 03:15:55 visual_prompt]: 	Training 400/553. train loss: 0.7651,	0.5116 s / batch. (data: 2.36e-02). ETA=7:29:17, max mem: 11.4 GB 
[10/29 03:17:24 visual_prompt]: 	Training 500/553. train loss: 0.6270,	0.4876 s / batch. (data: 2.68e-04). ETA=7:07:20, max mem: 11.4 GB 
[10/29 03:18:10 visual_prompt]: Epoch 5 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 0.8098
[10/29 03:19:02 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1911, average loss: 0.7685
[10/29 03:19:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.97	
[10/29 03:19:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/29 03:20:35 visual_prompt]: 	Training 100/553. train loss: 0.6498,	0.5117 s / batch. (data: 1.04e-02). ETA=7:27:09, max mem: 11.4 GB 
[10/29 03:22:03 visual_prompt]: 	Training 200/553. train loss: 0.5458,	0.4991 s / batch. (data: 2.68e-04). ETA=7:15:21, max mem: 11.4 GB 
[10/29 03:23:31 visual_prompt]: 	Training 300/553. train loss: 0.5536,	0.4883 s / batch. (data: 9.65e-03). ETA=7:05:08, max mem: 11.4 GB 
[10/29 03:25:03 visual_prompt]: 	Training 400/553. train loss: 0.5762,	0.5960 s / batch. (data: 9.64e-02). ETA=8:37:53, max mem: 11.4 GB 
[10/29 03:26:31 visual_prompt]: 	Training 500/553. train loss: 0.9807,	1.4560 s / batch. (data: 9.66e-01). ETA=21:02:41, max mem: 11.4 GB 
[10/29 03:27:16 visual_prompt]: Epoch 6 / 100: avg data time: 3.98e-01, avg batch time: 0.8926, average train loss: 0.7676
[10/29 03:28:09 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1918, average loss: 0.6825
[10/29 03:28:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.12	
[10/29 03:28:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/29 03:29:40 visual_prompt]: 	Training 100/553. train loss: 0.7026,	0.4992 s / batch. (data: 2.69e-04). ETA=7:11:37, max mem: 11.4 GB 
[10/29 03:31:09 visual_prompt]: 	Training 200/553. train loss: 0.5137,	0.4840 s / batch. (data: 2.70e-04). ETA=6:57:42, max mem: 11.4 GB 
[10/29 03:32:42 visual_prompt]: 	Training 300/553. train loss: 0.8071,	2.5839 s / batch. (data: 2.09e+00). ETA=1 day, 13:05:39, max mem: 11.4 GB 
[10/29 03:34:12 visual_prompt]: 	Training 400/553. train loss: 0.5797,	2.5880 s / batch. (data: 2.08e+00). ETA=1 day, 13:04:54, max mem: 11.4 GB 
[10/29 03:35:39 visual_prompt]: 	Training 500/553. train loss: 1.0526,	0.4799 s / batch. (data: 2.83e-04). ETA=6:51:45, max mem: 11.4 GB 
[10/29 03:36:24 visual_prompt]: Epoch 7 / 100: avg data time: 4.00e-01, avg batch time: 0.8942, average train loss: 0.7837
[10/29 03:37:16 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1913, average loss: 0.7799
[10/29 03:37:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.97	
[10/29 03:37:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/29 03:38:46 visual_prompt]: 	Training 100/553. train loss: 0.7096,	0.5003 s / batch. (data: 1.05e-02). ETA=7:08:01, max mem: 11.4 GB 
[10/29 03:40:16 visual_prompt]: 	Training 200/553. train loss: 1.3640,	0.4919 s / batch. (data: 2.54e-04). ETA=6:59:57, max mem: 11.4 GB 
[10/29 03:41:45 visual_prompt]: 	Training 300/553. train loss: 0.8303,	0.5057 s / batch. (data: 2.68e-04). ETA=7:10:57, max mem: 11.4 GB 
[10/29 03:43:14 visual_prompt]: 	Training 400/553. train loss: 0.7428,	0.5053 s / batch. (data: 1.05e-02). ETA=7:09:45, max mem: 11.4 GB 
[10/29 03:44:43 visual_prompt]: 	Training 500/553. train loss: 1.0787,	1.7531 s / batch. (data: 1.27e+00). ETA=1 day, 0:48:03, max mem: 11.4 GB 
[10/29 03:45:28 visual_prompt]: Epoch 8 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 0.7840
[10/29 03:46:20 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1920, average loss: 0.8924
[10/29 03:46:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.84	
[10/29 03:46:20 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/29 03:47:52 visual_prompt]: 	Training 100/553. train loss: 0.7043,	0.5160 s / batch. (data: 7.97e-03). ETA=7:16:40, max mem: 11.4 GB 
[10/29 03:49:20 visual_prompt]: 	Training 200/553. train loss: 0.7636,	0.5020 s / batch. (data: 5.42e-03). ETA=7:03:57, max mem: 11.4 GB 
[10/29 03:50:48 visual_prompt]: 	Training 300/553. train loss: 0.4830,	1.7601 s / batch. (data: 1.26e+00). ETA=1 day, 0:43:36, max mem: 11.4 GB 
[10/29 03:52:18 visual_prompt]: 	Training 400/553. train loss: 0.6371,	0.4896 s / batch. (data: 5.40e-03). ETA=6:51:50, max mem: 11.4 GB 
[10/29 03:53:47 visual_prompt]: 	Training 500/553. train loss: 1.0644,	1.1960 s / batch. (data: 6.94e-01). ETA=16:44:09, max mem: 11.4 GB 
[10/29 03:54:32 visual_prompt]: Epoch 9 / 100: avg data time: 3.94e-01, avg batch time: 0.8884, average train loss: 0.7807
[10/29 03:55:24 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.1907, average loss: 0.7924
[10/29 03:55:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.64	
[10/29 03:55:24 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/29 03:56:58 visual_prompt]: 	Training 100/553. train loss: 0.6445,	0.5201 s / batch. (data: 3.18e-02). ETA=7:15:20, max mem: 11.4 GB 
[10/29 03:58:25 visual_prompt]: 	Training 200/553. train loss: 0.5838,	0.4980 s / batch. (data: 9.51e-03). ETA=6:56:03, max mem: 11.4 GB 
[10/29 03:59:53 visual_prompt]: 	Training 300/553. train loss: 0.6068,	0.4960 s / batch. (data: 7.95e-03). ETA=6:53:29, max mem: 11.4 GB 
[10/29 04:01:20 visual_prompt]: 	Training 400/553. train loss: 0.7585,	1.4359 s / batch. (data: 9.58e-01). ETA=19:54:45, max mem: 11.4 GB 
[10/29 04:02:49 visual_prompt]: 	Training 500/553. train loss: 1.6692,	1.6040 s / batch. (data: 1.11e+00). ETA=22:11:55, max mem: 11.4 GB 
[10/29 04:03:35 visual_prompt]: Epoch 10 / 100: avg data time: 3.93e-01, avg batch time: 0.8872, average train loss: 0.8074
[10/29 04:04:27 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1893, average loss: 1.0249
[10/29 04:04:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.33	
[10/29 04:04:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/29 04:06:01 visual_prompt]: 	Training 100/553. train loss: 0.5739,	0.5073 s / batch. (data: 1.17e-02). ETA=6:59:56, max mem: 11.4 GB 
[10/29 04:07:31 visual_prompt]: 	Training 200/553. train loss: 1.9463,	0.4920 s / batch. (data: 2.74e-04). ETA=6:46:28, max mem: 11.4 GB 
[10/29 04:08:59 visual_prompt]: 	Training 300/553. train loss: 0.5917,	1.7237 s / batch. (data: 1.21e+00). ETA=23:41:09, max mem: 11.4 GB 
[10/29 04:10:26 visual_prompt]: 	Training 400/553. train loss: 0.7230,	0.4817 s / batch. (data: 2.75e-04). ETA=6:36:23, max mem: 11.4 GB 
[10/29 04:11:53 visual_prompt]: 	Training 500/553. train loss: 0.6616,	0.4914 s / batch. (data: 2.94e-04). ETA=6:43:30, max mem: 11.4 GB 
[10/29 04:12:38 visual_prompt]: Epoch 11 / 100: avg data time: 3.93e-01, avg batch time: 0.8875, average train loss: 0.8299
[10/29 04:13:30 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1930, average loss: 0.8582
[10/29 04:13:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.93	
[10/29 04:13:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/29 04:15:04 visual_prompt]: 	Training 100/553. train loss: 0.9070,	0.7354 s / batch. (data: 2.45e-01). ETA=10:02:00, max mem: 11.4 GB 
[10/29 04:16:34 visual_prompt]: 	Training 200/553. train loss: 0.7314,	0.4913 s / batch. (data: 2.76e-04). ETA=6:41:21, max mem: 11.4 GB 
[10/29 04:18:01 visual_prompt]: 	Training 300/553. train loss: 0.6487,	0.4920 s / batch. (data: 1.20e-02). ETA=6:41:07, max mem: 11.4 GB 
[10/29 04:19:29 visual_prompt]: 	Training 400/553. train loss: 0.9534,	0.4920 s / batch. (data: 2.83e-04). ETA=6:40:16, max mem: 11.4 GB 
[10/29 04:20:58 visual_prompt]: 	Training 500/553. train loss: 1.6549,	0.4847 s / batch. (data: 2.64e-04). ETA=6:33:33, max mem: 11.4 GB 
[10/29 04:21:42 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 0.8121
[10/29 04:22:35 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1895, average loss: 1.5058
[10/29 04:22:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.43	
[10/29 04:22:35 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/29 04:24:08 visual_prompt]: 	Training 100/553. train loss: 0.7920,	0.5725 s / batch. (data: 9.56e-02). ETA=7:43:22, max mem: 11.4 GB 
[10/29 04:25:33 visual_prompt]: 	Training 200/553. train loss: 0.7049,	0.5292 s / batch. (data: 5.02e-02). ETA=7:07:26, max mem: 11.4 GB 
[10/29 04:27:04 visual_prompt]: 	Training 300/553. train loss: 0.8141,	2.1947 s / batch. (data: 1.71e+00). ETA=1 day, 5:29:06, max mem: 11.4 GB 
[10/29 04:28:30 visual_prompt]: 	Training 400/553. train loss: 1.0838,	0.5120 s / batch. (data: 5.42e-03). ETA=6:51:52, max mem: 11.4 GB 
[10/29 04:30:01 visual_prompt]: 	Training 500/553. train loss: 0.5789,	0.4983 s / batch. (data: 1.05e-02). ETA=6:39:59, max mem: 11.4 GB 
[10/29 04:30:46 visual_prompt]: Epoch 13 / 100: avg data time: 3.94e-01, avg batch time: 0.8880, average train loss: 0.8011
[10/29 04:31:39 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1912, average loss: 0.7202
[10/29 04:31:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.10	
[10/29 04:31:39 visual_prompt]: Best epoch 13: best metric: -0.720
[10/29 04:31:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/29 04:33:11 visual_prompt]: 	Training 100/553. train loss: 0.7678,	0.5107 s / batch. (data: 1.04e-02). ETA=6:48:39, max mem: 11.4 GB 
[10/29 04:34:39 visual_prompt]: 	Training 200/553. train loss: 0.7834,	0.7720 s / batch. (data: 2.69e-01). ETA=10:16:25, max mem: 11.4 GB 
[10/29 04:36:08 visual_prompt]: 	Training 300/553. train loss: 0.7010,	1.0950 s / batch. (data: 5.96e-01). ETA=14:32:33, max mem: 11.4 GB 
[10/29 04:37:37 visual_prompt]: 	Training 400/553. train loss: 0.7844,	0.5080 s / batch. (data: 2.60e-04). ETA=6:43:57, max mem: 11.4 GB 
[10/29 04:39:05 visual_prompt]: 	Training 500/553. train loss: 0.9715,	0.5124 s / batch. (data: 2.80e-02). ETA=6:46:38, max mem: 11.4 GB 
[10/29 04:39:49 visual_prompt]: Epoch 14 / 100: avg data time: 3.93e-01, avg batch time: 0.8866, average train loss: 0.7704
[10/29 04:40:41 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1918, average loss: 0.7521
[10/29 04:40:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 60.99	
[10/29 04:40:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/29 04:42:13 visual_prompt]: 	Training 100/553. train loss: 0.8433,	0.4924 s / batch. (data: 3.12e-04). ETA=6:29:27, max mem: 11.4 GB 
[10/29 04:43:41 visual_prompt]: 	Training 200/553. train loss: 1.3322,	0.4962 s / batch. (data: 2.52e-04). ETA=6:31:36, max mem: 11.4 GB 
[10/29 04:45:10 visual_prompt]: 	Training 300/553. train loss: 0.4400,	0.4884 s / batch. (data: 2.64e-04). ETA=6:24:42, max mem: 11.4 GB 
[10/29 04:46:36 visual_prompt]: 	Training 400/553. train loss: 0.4615,	0.4959 s / batch. (data: 2.74e-04). ETA=6:29:44, max mem: 11.4 GB 
[10/29 04:48:06 visual_prompt]: 	Training 500/553. train loss: 0.9041,	0.5000 s / batch. (data: 2.47e-04). ETA=6:32:08, max mem: 11.4 GB 
[10/29 04:48:53 visual_prompt]: Epoch 15 / 100: avg data time: 3.92e-01, avg batch time: 0.8878, average train loss: 0.8071
[10/29 04:49:45 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1910, average loss: 0.7994
[10/29 04:49:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.23	
[10/29 04:49:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/29 04:51:15 visual_prompt]: 	Training 100/553. train loss: 0.4396,	0.4788 s / batch. (data: 2.65e-04). ETA=6:14:19, max mem: 11.4 GB 
[10/29 04:52:45 visual_prompt]: 	Training 200/553. train loss: 0.8760,	0.4850 s / batch. (data: 2.56e-04). ETA=6:18:18, max mem: 11.4 GB 
[10/29 04:54:14 visual_prompt]: 	Training 300/553. train loss: 0.9989,	0.4800 s / batch. (data: 2.89e-04). ETA=6:13:39, max mem: 11.4 GB 
[10/29 04:55:42 visual_prompt]: 	Training 400/553. train loss: 0.7405,	0.5000 s / batch. (data: 7.28e-04). ETA=6:28:23, max mem: 11.4 GB 
[10/29 04:57:10 visual_prompt]: 	Training 500/553. train loss: 1.2469,	2.1203 s / batch. (data: 1.64e+00). ETA=1 day, 3:23:22, max mem: 11.4 GB 
[10/29 04:57:56 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 0.8882, average train loss: 0.7740
[10/29 04:58:49 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1919, average loss: 0.7084
[10/29 04:58:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 62.01	
[10/29 04:58:49 visual_prompt]: Best epoch 16: best metric: -0.708
[10/29 04:58:49 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/29 05:00:20 visual_prompt]: 	Training 100/553. train loss: 0.3799,	0.4960 s / batch. (data: 2.33e-04). ETA=6:23:12, max mem: 11.4 GB 
[10/29 05:01:50 visual_prompt]: 	Training 200/553. train loss: 0.7702,	0.4960 s / batch. (data: 2.78e-04). ETA=6:22:23, max mem: 11.4 GB 
[10/29 05:03:18 visual_prompt]: 	Training 300/553. train loss: 1.6322,	0.4986 s / batch. (data: 2.69e-04). ETA=6:23:32, max mem: 11.4 GB 
[10/29 05:04:46 visual_prompt]: 	Training 400/553. train loss: 0.6202,	0.8200 s / batch. (data: 3.12e-01). ETA=10:29:22, max mem: 11.4 GB 
[10/29 05:06:14 visual_prompt]: 	Training 500/553. train loss: 0.6412,	1.9760 s / batch. (data: 1.48e+00). ETA=1 day, 1:13:21, max mem: 11.4 GB 
[10/29 05:07:01 visual_prompt]: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8889, average train loss: 0.7411
[10/29 05:07:53 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1911, average loss: 0.7342
[10/29 05:07:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.92	
[10/29 05:07:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/29 05:09:25 visual_prompt]: 	Training 100/553. train loss: 0.7906,	0.5043 s / batch. (data: 7.96e-03). ETA=6:24:54, max mem: 11.4 GB 
[10/29 05:10:56 visual_prompt]: 	Training 200/553. train loss: 0.6755,	0.4972 s / batch. (data: 2.66e-04). ETA=6:18:42, max mem: 11.4 GB 
[10/29 05:12:25 visual_prompt]: 	Training 300/553. train loss: 0.5209,	0.5120 s / batch. (data: 2.74e-04). ETA=6:29:06, max mem: 11.4 GB 
[10/29 05:13:54 visual_prompt]: 	Training 400/553. train loss: 0.7614,	0.5000 s / batch. (data: 7.96e-03). ETA=6:19:09, max mem: 11.4 GB 
[10/29 05:15:21 visual_prompt]: 	Training 500/553. train loss: 0.6695,	0.5081 s / batch. (data: 7.96e-03). ETA=6:24:25, max mem: 11.4 GB 
[10/29 05:16:05 visual_prompt]: Epoch 18 / 100: avg data time: 3.95e-01, avg batch time: 0.8898, average train loss: 0.7777
[10/29 05:16:58 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1896, average loss: 0.6735
[10/29 05:16:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.65	
[10/29 05:16:58 visual_prompt]: Best epoch 18: best metric: -0.673
[10/29 05:16:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/29 05:18:29 visual_prompt]: 	Training 100/553. train loss: 0.7991,	0.5079 s / batch. (data: 3.48e-04). ETA=6:23:01, max mem: 11.4 GB 
[10/29 05:19:58 visual_prompt]: 	Training 200/553. train loss: 0.6970,	0.4962 s / batch. (data: 7.94e-03). ETA=6:13:23, max mem: 11.4 GB 
[10/29 05:21:27 visual_prompt]: 	Training 300/553. train loss: 0.6772,	0.5170 s / batch. (data: 1.05e-02). ETA=6:28:07, max mem: 11.4 GB 
[10/29 05:22:57 visual_prompt]: 	Training 400/553. train loss: 0.4530,	0.5000 s / batch. (data: 3.05e-04). ETA=6:14:32, max mem: 11.4 GB 
[10/29 05:24:21 visual_prompt]: 	Training 500/553. train loss: 0.7304,	0.5194 s / batch. (data: 2.97e-04). ETA=6:28:13, max mem: 11.4 GB 
[10/29 05:25:08 visual_prompt]: Epoch 19 / 100: avg data time: 3.92e-01, avg batch time: 0.8872, average train loss: 0.7644
[10/29 05:26:01 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1909, average loss: 0.7106
[10/29 05:26:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 62.85	
[10/29 05:26:01 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/29 05:27:32 visual_prompt]: 	Training 100/553. train loss: 1.4981,	0.4919 s / batch. (data: 1.04e-02). ETA=6:06:24, max mem: 11.4 GB 
[10/29 05:29:02 visual_prompt]: 	Training 200/553. train loss: 0.3353,	0.5040 s / batch. (data: 2.82e-04). ETA=6:14:36, max mem: 11.4 GB 
[10/29 05:30:32 visual_prompt]: 	Training 300/553. train loss: 0.7497,	0.5290 s / batch. (data: 4.10e-02). ETA=6:32:15, max mem: 11.4 GB 
[10/29 05:32:00 visual_prompt]: 	Training 400/553. train loss: 0.5848,	0.5118 s / batch. (data: 1.18e-02). ETA=6:18:39, max mem: 11.4 GB 
[10/29 05:33:29 visual_prompt]: 	Training 500/553. train loss: 1.3545,	0.4955 s / batch. (data: 5.39e-03). ETA=6:05:48, max mem: 11.4 GB 
[10/29 05:34:16 visual_prompt]: Epoch 20 / 100: avg data time: 4.00e-01, avg batch time: 0.8947, average train loss: 0.7624
[10/29 05:35:09 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1928, average loss: 0.7319
[10/29 05:35:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 63.12	
[10/29 05:35:09 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/29 05:36:43 visual_prompt]: 	Training 100/553. train loss: 0.6251,	0.4960 s / batch. (data: 2.76e-04). ETA=6:04:54, max mem: 11.4 GB 
[10/29 05:38:12 visual_prompt]: 	Training 200/553. train loss: 0.6035,	0.4950 s / batch. (data: 2.65e-04). ETA=6:03:18, max mem: 11.4 GB 
[10/29 05:39:41 visual_prompt]: 	Training 300/553. train loss: 1.0755,	1.6080 s / batch. (data: 1.11e+00). ETA=19:37:33, max mem: 11.4 GB 
[10/29 05:41:08 visual_prompt]: 	Training 400/553. train loss: 0.6667,	0.5000 s / batch. (data: 2.78e-04). ETA=6:05:21, max mem: 11.4 GB 
[10/29 05:42:39 visual_prompt]: 	Training 500/553. train loss: 0.7885,	0.4974 s / batch. (data: 2.82e-04). ETA=6:02:35, max mem: 11.4 GB 
[10/29 05:43:23 visual_prompt]: Epoch 21 / 100: avg data time: 3.98e-01, avg batch time: 0.8938, average train loss: 0.7597
[10/29 05:44:16 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1920, average loss: 0.7423
[10/29 05:44:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 61.22	
[10/29 05:44:16 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/29 05:45:48 visual_prompt]: 	Training 100/553. train loss: 0.6977,	0.5397 s / batch. (data: 5.38e-03). ETA=6:32:04, max mem: 11.4 GB 
[10/29 05:47:17 visual_prompt]: 	Training 200/553. train loss: 0.4982,	0.5120 s / batch. (data: 7.98e-03). ETA=6:11:06, max mem: 11.4 GB 
[10/29 05:48:44 visual_prompt]: 	Training 300/553. train loss: 0.3658,	0.6480 s / batch. (data: 1.50e-01). ETA=7:48:34, max mem: 11.4 GB 
[10/29 05:50:13 visual_prompt]: 	Training 400/553. train loss: 0.4970,	0.4856 s / batch. (data: 2.56e-04). ETA=5:50:19, max mem: 11.4 GB 
[10/29 05:51:42 visual_prompt]: 	Training 500/553. train loss: 0.6446,	0.5087 s / batch. (data: 5.39e-03). ETA=6:06:07, max mem: 11.4 GB 
[10/29 05:52:29 visual_prompt]: Epoch 22 / 100: avg data time: 3.96e-01, avg batch time: 0.8915, average train loss: 0.7448
[10/29 05:53:22 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1904, average loss: 0.6588
[10/29 05:53:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 63.90	
[10/29 05:53:22 visual_prompt]: Best epoch 22: best metric: -0.659
[10/29 05:53:22 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/29 05:54:55 visual_prompt]: 	Training 100/553. train loss: 0.9682,	0.4800 s / batch. (data: 2.97e-04). ETA=5:44:15, max mem: 11.4 GB 
[10/29 05:56:26 visual_prompt]: 	Training 200/553. train loss: 0.9211,	1.4800 s / batch. (data: 9.99e-01). ETA=17:39:02, max mem: 11.4 GB 
[10/29 05:57:57 visual_prompt]: 	Training 300/553. train loss: 1.4152,	0.5064 s / batch. (data: 7.27e-04). ETA=6:01:29, max mem: 11.4 GB 
[10/29 05:59:23 visual_prompt]: 	Training 400/553. train loss: 0.5711,	0.4923 s / batch. (data: 4.84e-03). ETA=5:50:38, max mem: 11.4 GB 
[10/29 06:00:49 visual_prompt]: 	Training 500/553. train loss: 0.6797,	0.4887 s / batch. (data: 2.92e-04). ETA=5:47:16, max mem: 11.4 GB 
[10/29 06:01:35 visual_prompt]: Epoch 23 / 100: avg data time: 3.97e-01, avg batch time: 0.8922, average train loss: 0.7500
[10/29 06:02:28 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.1917, average loss: 0.6745
[10/29 06:02:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 63.41	
[10/29 06:02:28 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/29 06:03:58 visual_prompt]: 	Training 100/553. train loss: 0.9383,	0.5240 s / batch. (data: 2.74e-04). ETA=6:10:59, max mem: 11.4 GB 
[10/29 06:05:26 visual_prompt]: 	Training 200/553. train loss: 0.8099,	0.5285 s / batch. (data: 5.86e-03). ETA=6:13:18, max mem: 11.4 GB 
[10/29 06:06:56 visual_prompt]: 	Training 300/553. train loss: 0.7620,	1.5240 s / batch. (data: 1.01e+00). ETA=17:53:56, max mem: 11.4 GB 
[10/29 06:08:24 visual_prompt]: 	Training 400/553. train loss: 0.6396,	0.5040 s / batch. (data: 2.88e-04). ETA=5:54:19, max mem: 11.4 GB 
[10/29 06:09:54 visual_prompt]: 	Training 500/553. train loss: 0.6781,	0.8809 s / batch. (data: 3.86e-01). ETA=10:17:48, max mem: 11.4 GB 
[10/29 06:10:40 visual_prompt]: Epoch 24 / 100: avg data time: 3.94e-01, avg batch time: 0.8895, average train loss: 0.7637
[10/29 06:11:33 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1912, average loss: 0.6989
[10/29 06:11:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 62.31	
[10/29 06:11:33 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/29 06:13:08 visual_prompt]: 	Training 100/553. train loss: 0.5544,	0.5252 s / batch. (data: 4.11e-02). ETA=6:06:59, max mem: 11.4 GB 
[10/29 06:14:33 visual_prompt]: 	Training 200/553. train loss: 0.9502,	0.5079 s / batch. (data: 7.98e-03). ETA=5:54:02, max mem: 11.4 GB 
[10/29 06:16:01 visual_prompt]: 	Training 300/553. train loss: 0.8887,	0.5032 s / batch. (data: 2.78e-04). ETA=5:49:55, max mem: 11.4 GB 
[10/29 06:17:30 visual_prompt]: 	Training 400/553. train loss: 0.5470,	1.8425 s / batch. (data: 1.34e+00). ETA=21:18:19, max mem: 11.4 GB 
[10/29 06:18:59 visual_prompt]: 	Training 500/553. train loss: 0.6922,	1.9576 s / batch. (data: 1.48e+00). ETA=22:34:57, max mem: 11.4 GB 
[10/29 06:19:44 visual_prompt]: Epoch 25 / 100: avg data time: 3.93e-01, avg batch time: 0.8884, average train loss: 0.7502
[10/29 06:20:37 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1911, average loss: 1.0276
[10/29 06:20:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.82	
[10/29 06:20:37 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[10/29 06:22:08 visual_prompt]: 	Training 100/553. train loss: 0.2319,	0.4786 s / batch. (data: 2.40e-04). ETA=5:30:03, max mem: 11.4 GB 
[10/29 06:23:37 visual_prompt]: 	Training 200/553. train loss: 0.8734,	1.5925 s / batch. (data: 1.09e+00). ETA=18:15:32, max mem: 11.4 GB 
[10/29 06:25:07 visual_prompt]: 	Training 300/553. train loss: 1.0229,	0.4920 s / batch. (data: 2.64e-04). ETA=5:37:38, max mem: 11.4 GB 
[10/29 06:26:34 visual_prompt]: 	Training 400/553. train loss: 0.4922,	0.4886 s / batch. (data: 2.51e-04). ETA=5:34:31, max mem: 11.4 GB 
[10/29 06:28:01 visual_prompt]: 	Training 500/553. train loss: 0.5286,	0.4869 s / batch. (data: 7.97e-03). ETA=5:32:32, max mem: 11.4 GB 
[10/29 06:28:47 visual_prompt]: Epoch 26 / 100: avg data time: 3.91e-01, avg batch time: 0.8860, average train loss: 0.7821
[10/29 06:29:39 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1914, average loss: 0.6682
[10/29 06:29:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 63.05	
[10/29 06:29:39 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[10/29 06:31:13 visual_prompt]: 	Training 100/553. train loss: 0.6733,	0.4960 s / batch. (data: 2.81e-04). ETA=5:37:26, max mem: 11.4 GB 
[10/29 06:32:41 visual_prompt]: 	Training 200/553. train loss: 0.8356,	2.3320 s / batch. (data: 1.83e+00). ETA=1 day, 2:22:45, max mem: 11.4 GB 
[10/29 06:34:11 visual_prompt]: 	Training 300/553. train loss: 0.5853,	1.1203 s / batch. (data: 6.43e-01). ETA=12:38:29, max mem: 11.4 GB 
[10/29 06:35:39 visual_prompt]: 	Training 400/553. train loss: 0.8325,	0.4909 s / batch. (data: 2.40e-04). ETA=5:31:33, max mem: 11.4 GB 
[10/29 06:37:09 visual_prompt]: 	Training 500/553. train loss: 0.6494,	0.4782 s / batch. (data: 2.64e-04). ETA=5:22:10, max mem: 11.4 GB 
[10/29 06:37:52 visual_prompt]: Epoch 27 / 100: avg data time: 3.95e-01, avg batch time: 0.8908, average train loss: 0.7653
[10/29 06:38:45 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1905, average loss: 0.6923
[10/29 06:38:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 60.66	
[10/29 06:38:45 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[10/29 06:40:15 visual_prompt]: 	Training 100/553. train loss: 0.2371,	0.5716 s / batch. (data: 5.58e-02). ETA=6:23:37, max mem: 11.4 GB 
[10/29 06:41:45 visual_prompt]: 	Training 200/553. train loss: 0.3979,	0.4865 s / batch. (data: 7.98e-03). ETA=5:25:42, max mem: 11.4 GB 
[10/29 06:43:14 visual_prompt]: 	Training 300/553. train loss: 0.6317,	1.9083 s / batch. (data: 1.43e+00). ETA=21:14:25, max mem: 11.4 GB 
[10/29 06:44:42 visual_prompt]: 	Training 400/553. train loss: 0.7698,	0.5120 s / batch. (data: 2.66e-04). ETA=5:41:04, max mem: 11.4 GB 
[10/29 06:46:09 visual_prompt]: 	Training 500/553. train loss: 0.1994,	0.4819 s / batch. (data: 2.71e-04). ETA=5:20:13, max mem: 11.4 GB 
[10/29 06:46:55 visual_prompt]: Epoch 28 / 100: avg data time: 3.93e-01, avg batch time: 0.8869, average train loss: 0.7465
[10/29 06:47:48 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1896, average loss: 0.7771
[10/29 06:47:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 63.37	
[10/29 06:47:48 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[10/29 06:49:25 visual_prompt]: 	Training 100/553. train loss: 0.6183,	0.5280 s / batch. (data: 6.98e-04). ETA=5:49:28, max mem: 11.4 GB 
[10/29 06:50:53 visual_prompt]: 	Training 200/553. train loss: 0.6224,	2.0951 s / batch. (data: 1.61e+00). ETA=23:03:19, max mem: 11.4 GB 
[10/29 06:52:18 visual_prompt]: 	Training 300/553. train loss: 0.6622,	0.5101 s / batch. (data: 9.75e-03). ETA=5:35:56, max mem: 11.4 GB 
[10/29 06:53:44 visual_prompt]: 	Training 400/553. train loss: 0.5145,	0.4825 s / batch. (data: 2.77e-04). ETA=5:16:57, max mem: 11.4 GB 
[10/29 06:55:13 visual_prompt]: 	Training 500/553. train loss: 0.7287,	0.5227 s / batch. (data: 2.70e-04). ETA=5:42:29, max mem: 11.4 GB 
[10/29 06:55:58 visual_prompt]: Epoch 29 / 100: avg data time: 3.93e-01, avg batch time: 0.8870, average train loss: 0.7648
[10/29 06:56:51 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1920, average loss: 0.8057
[10/29 06:56:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 63.39	
[10/29 06:56:51 visual_prompt]: Stopping early.
[10/29 06:56:51 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 06:56:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 06:56:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 06:56:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 06:56:51 visual_prompt]: Training with config:
[10/29 06:56:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.05_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 06:56:51 visual_prompt]: Loading training data...
[10/29 06:56:51 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 06:56:51 visual_prompt]: Loading validation data...
[10/29 06:56:51 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 06:56:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/29 06:56:53 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/29 06:56:53 visual_prompt]: tuned percent:0.529
[10/29 06:56:53 visual_prompt]: Device used for model: 0
[10/29 06:56:53 visual_prompt]: Setting up Evaluator...
[10/29 06:56:53 visual_prompt]: Setting up Trainer...
[10/29 06:56:53 visual_prompt]: 	Setting up the optimizer...
[10/29 06:56:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 06:58:25 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5160 s / batch. (data: 2.72e-04). ETA=7:54:45, max mem: 11.4 GB 
[10/29 06:59:52 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5200 s / batch. (data: 2.50e-04). ETA=7:57:33, max mem: 11.4 GB 
[10/29 07:01:24 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9048 s / batch. (data: 2.43e+00). ETA=1 day, 20:22:42, max mem: 11.4 GB 
[10/29 07:02:49 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5079 s / batch. (data: 5.41e-03). ETA=7:44:43, max mem: 11.4 GB 
[10/29 07:04:21 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5113 s / batch. (data: 7.26e-04). ETA=7:46:59, max mem: 11.4 GB 
[10/29 07:05:07 visual_prompt]: Epoch 1 / 100: avg data time: 3.97e-01, avg batch time: 0.8914, average train loss: 1.3966
[10/29 07:05:59 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1914, average loss: 1.3454
[10/29 07:05:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/29 07:05:59 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/29 07:07:30 visual_prompt]: 	Training 100/553. train loss: 0.6966,	1.2640 s / batch. (data: 7.75e-01). ETA=19:11:14, max mem: 11.4 GB 
[10/29 07:08:58 visual_prompt]: 	Training 200/553. train loss: 0.3809,	0.5495 s / batch. (data: 5.34e-02). ETA=8:19:33, max mem: 11.4 GB 
[10/29 07:10:29 visual_prompt]: 	Training 300/553. train loss: 0.6489,	1.6160 s / batch. (data: 1.13e+00). ETA=1 day, 0:26:26, max mem: 11.4 GB 
[10/29 07:11:56 visual_prompt]: 	Training 400/553. train loss: 0.7770,	0.4840 s / batch. (data: 2.66e-04). ETA=7:18:23, max mem: 11.4 GB 
[10/29 07:13:26 visual_prompt]: 	Training 500/553. train loss: 0.6588,	0.5160 s / batch. (data: 2.81e-04). ETA=7:46:31, max mem: 11.4 GB 
[10/29 07:14:10 visual_prompt]: Epoch 2 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 0.7477
[10/29 07:15:03 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.1917, average loss: 0.7293
[10/29 07:15:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.12	
[10/29 07:15:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/29 07:16:33 visual_prompt]: 	Training 100/553. train loss: 0.7463,	0.4843 s / batch. (data: 5.40e-03). ETA=7:16:40, max mem: 11.4 GB 
[10/29 07:18:04 visual_prompt]: 	Training 200/553. train loss: 0.7569,	0.5160 s / batch. (data: 2.59e-04). ETA=7:44:21, max mem: 11.4 GB 
[10/29 07:19:31 visual_prompt]: 	Training 300/553. train loss: 0.5688,	0.5086 s / batch. (data: 2.06e-02). ETA=7:36:48, max mem: 11.4 GB 
[10/29 07:21:01 visual_prompt]: 	Training 400/553. train loss: 0.6251,	0.5122 s / batch. (data: 2.57e-04). ETA=7:39:15, max mem: 11.4 GB 
[10/29 07:22:30 visual_prompt]: 	Training 500/553. train loss: 0.7282,	1.8002 s / batch. (data: 1.30e+00). ETA=1 day, 2:50:57, max mem: 11.4 GB 
[10/29 07:23:15 visual_prompt]: Epoch 3 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 0.7314
[10/29 07:24:08 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1919, average loss: 0.7282
[10/29 07:24:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.99	
[10/29 07:24:08 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/29 07:25:40 visual_prompt]: 	Training 100/553. train loss: 0.7141,	0.4920 s / batch. (data: 2.93e-04). ETA=7:19:01, max mem: 11.4 GB 
[10/29 07:27:09 visual_prompt]: 	Training 200/553. train loss: 0.5769,	0.4872 s / batch. (data: 7.96e-03). ETA=7:13:55, max mem: 11.4 GB 
[10/29 07:28:38 visual_prompt]: 	Training 300/553. train loss: 0.5637,	1.5280 s / batch. (data: 1.04e+00). ETA=22:38:26, max mem: 11.4 GB 
[10/29 07:30:03 visual_prompt]: 	Training 400/553. train loss: 0.7945,	1.4106 s / batch. (data: 9.07e-01). ETA=20:51:41, max mem: 11.4 GB 
[10/29 07:31:33 visual_prompt]: 	Training 500/553. train loss: 0.3783,	3.8401 s / batch. (data: 3.34e+00). ETA=2 days, 8:41:05, max mem: 11.4 GB 
[10/29 07:32:19 visual_prompt]: Epoch 4 / 100: avg data time: 3.95e-01, avg batch time: 0.8890, average train loss: 0.7269
[10/29 07:33:12 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1926, average loss: 0.7159
[10/29 07:33:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.33	
[10/29 07:33:12 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/29 07:34:43 visual_prompt]: 	Training 100/553. train loss: 0.5959,	0.5000 s / batch. (data: 2.63e-04). ETA=7:21:32, max mem: 11.4 GB 
[10/29 07:36:11 visual_prompt]: 	Training 200/553. train loss: 0.5651,	1.3320 s / batch. (data: 8.15e-01). ETA=19:34:06, max mem: 11.4 GB 
[10/29 07:37:41 visual_prompt]: 	Training 300/553. train loss: 0.7150,	0.5000 s / batch. (data: 2.71e-04). ETA=7:19:52, max mem: 11.4 GB 
[10/29 07:39:08 visual_prompt]: 	Training 400/553. train loss: 0.5700,	0.4886 s / batch. (data: 2.58e-04). ETA=7:09:05, max mem: 11.4 GB 
[10/29 07:40:38 visual_prompt]: 	Training 500/553. train loss: 0.6936,	0.5054 s / batch. (data: 2.78e-04). ETA=7:23:00, max mem: 11.4 GB 
[10/29 07:41:24 visual_prompt]: Epoch 5 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 0.7147
[10/29 07:42:16 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1920, average loss: 0.6963
[10/29 07:42:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.87	
[10/29 07:42:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/29 07:43:49 visual_prompt]: 	Training 100/553. train loss: 0.9843,	0.4998 s / batch. (data: 1.04e-02). ETA=7:16:46, max mem: 11.4 GB 
[10/29 07:45:17 visual_prompt]: 	Training 200/553. train loss: 0.5572,	0.5009 s / batch. (data: 1.04e-02). ETA=7:16:57, max mem: 11.4 GB 
[10/29 07:46:44 visual_prompt]: 	Training 300/553. train loss: 0.6023,	0.5111 s / batch. (data: 2.70e-04). ETA=7:24:58, max mem: 11.4 GB 
[10/29 07:48:16 visual_prompt]: 	Training 400/553. train loss: 0.6392,	0.5557 s / batch. (data: 5.28e-02). ETA=8:02:51, max mem: 11.4 GB 
[10/29 07:49:44 visual_prompt]: 	Training 500/553. train loss: 0.6947,	1.4407 s / batch. (data: 9.36e-01). ETA=20:49:25, max mem: 11.4 GB 
[10/29 07:50:29 visual_prompt]: Epoch 6 / 100: avg data time: 3.95e-01, avg batch time: 0.8900, average train loss: 0.7149
[10/29 07:51:21 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1905, average loss: 0.7419
[10/29 07:51:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.02	
[10/29 07:51:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/29 07:52:52 visual_prompt]: 	Training 100/553. train loss: 0.4255,	0.5000 s / batch. (data: 2.71e-04). ETA=7:12:19, max mem: 11.4 GB 
[10/29 07:54:20 visual_prompt]: 	Training 200/553. train loss: 0.6104,	0.5007 s / batch. (data: 3.03e-04). ETA=7:12:08, max mem: 11.4 GB 
[10/29 07:55:53 visual_prompt]: 	Training 300/553. train loss: 0.6393,	2.4371 s / batch. (data: 1.95e+00). ETA=1 day, 10:59:15, max mem: 11.4 GB 
[10/29 07:57:22 visual_prompt]: 	Training 400/553. train loss: 0.6493,	2.3440 s / batch. (data: 1.83e+00). ETA=1 day, 9:35:10, max mem: 11.4 GB 
[10/29 07:58:48 visual_prompt]: 	Training 500/553. train loss: 0.6926,	1.8160 s / batch. (data: 1.32e+00). ETA=1 day, 1:58:09, max mem: 11.4 GB 
[10/29 07:59:33 visual_prompt]: Epoch 7 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.7045
[10/29 08:00:25 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1899, average loss: 0.7695
[10/29 08:00:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.24	
[10/29 08:00:25 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/29 08:01:55 visual_prompt]: 	Training 100/553. train loss: 0.7170,	0.7592 s / batch. (data: 2.45e-01). ETA=10:49:27, max mem: 11.4 GB 
[10/29 08:03:25 visual_prompt]: 	Training 200/553. train loss: 0.9803,	0.4977 s / batch. (data: 1.05e-02). ETA=7:04:57, max mem: 11.4 GB 
[10/29 08:04:55 visual_prompt]: 	Training 300/553. train loss: 0.6354,	0.5213 s / batch. (data: 9.34e-03). ETA=7:24:15, max mem: 11.4 GB 
[10/29 08:06:24 visual_prompt]: 	Training 400/553. train loss: 0.7049,	1.1804 s / batch. (data: 6.93e-01). ETA=16:43:56, max mem: 11.4 GB 
[10/29 08:07:52 visual_prompt]: 	Training 500/553. train loss: 0.9333,	1.9633 s / batch. (data: 1.47e+00). ETA=1 day, 3:46:27, max mem: 11.4 GB 
[10/29 08:08:37 visual_prompt]: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8896, average train loss: 0.7211
[10/29 08:09:30 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1903, average loss: 0.7729
[10/29 08:09:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.48	
[10/29 08:09:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/29 08:11:03 visual_prompt]: 	Training 100/553. train loss: 0.7415,	0.5172 s / batch. (data: 2.06e-02). ETA=7:17:42, max mem: 11.4 GB 
[10/29 08:12:31 visual_prompt]: 	Training 200/553. train loss: 0.6105,	0.5160 s / batch. (data: 5.41e-03). ETA=7:15:48, max mem: 11.4 GB 
[10/29 08:13:59 visual_prompt]: 	Training 300/553. train loss: 0.6326,	0.5061 s / batch. (data: 5.36e-03). ETA=7:06:34, max mem: 11.4 GB 
[10/29 08:15:29 visual_prompt]: 	Training 400/553. train loss: 0.6770,	0.5133 s / batch. (data: 2.44e-04). ETA=7:11:51, max mem: 11.4 GB 
[10/29 08:16:57 visual_prompt]: 	Training 500/553. train loss: 0.5622,	0.5084 s / batch. (data: 1.99e-02). ETA=7:06:51, max mem: 11.4 GB 
[10/29 08:17:42 visual_prompt]: Epoch 9 / 100: avg data time: 3.95e-01, avg batch time: 0.8887, average train loss: 0.7275
[10/29 08:18:34 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1897, average loss: 0.6904
[10/29 08:18:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.53	
[10/29 08:18:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/29 08:20:08 visual_prompt]: 	Training 100/553. train loss: 0.7013,	0.7280 s / batch. (data: 2.32e-01). ETA=10:09:22, max mem: 11.4 GB 
[10/29 08:21:36 visual_prompt]: 	Training 200/553. train loss: 0.6626,	1.3840 s / batch. (data: 8.87e-01). ETA=19:16:08, max mem: 11.4 GB 
[10/29 08:23:04 visual_prompt]: 	Training 300/553. train loss: 0.6803,	1.5240 s / batch. (data: 1.02e+00). ETA=21:10:34, max mem: 11.4 GB 
[10/29 08:24:31 visual_prompt]: 	Training 400/553. train loss: 0.8082,	1.2291 s / batch. (data: 7.51e-01). ETA=17:02:39, max mem: 11.4 GB 
[10/29 08:26:00 visual_prompt]: 	Training 500/553. train loss: 0.8801,	0.5042 s / batch. (data: 7.41e-03). ETA=6:58:38, max mem: 11.4 GB 
[10/29 08:26:45 visual_prompt]: Epoch 10 / 100: avg data time: 3.93e-01, avg batch time: 0.8878, average train loss: 0.7305
[10/29 08:27:38 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1906, average loss: 0.7350
[10/29 08:27:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.56	
[10/29 08:27:38 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/29 08:29:11 visual_prompt]: 	Training 100/553. train loss: 0.7037,	0.5088 s / batch. (data: 2.60e-04). ETA=7:01:10, max mem: 11.4 GB 
[10/29 08:30:42 visual_prompt]: 	Training 200/553. train loss: 1.0198,	0.5239 s / batch. (data: 3.04e-04). ETA=7:12:47, max mem: 11.4 GB 
[10/29 08:32:09 visual_prompt]: 	Training 300/553. train loss: 0.4708,	0.8200 s / batch. (data: 3.14e-01). ETA=11:16:04, max mem: 11.4 GB 
[10/29 08:33:37 visual_prompt]: 	Training 400/553. train loss: 0.7642,	0.4937 s / batch. (data: 5.39e-03). ETA=6:46:14, max mem: 11.4 GB 
[10/29 08:35:05 visual_prompt]: 	Training 500/553. train loss: 0.6981,	0.5120 s / batch. (data: 2.56e-04). ETA=7:00:24, max mem: 11.4 GB 
[10/29 08:35:50 visual_prompt]: Epoch 11 / 100: avg data time: 3.94e-01, avg batch time: 0.8891, average train loss: 0.7318
[10/29 08:36:42 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1906, average loss: 0.6884
[10/29 08:36:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.04	
[10/29 08:36:42 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/29 08:38:15 visual_prompt]: 	Training 100/553. train loss: 0.7955,	0.5000 s / batch. (data: 2.83e-04). ETA=6:49:18, max mem: 11.4 GB 
[10/29 08:39:45 visual_prompt]: 	Training 200/553. train loss: 0.5637,	0.5008 s / batch. (data: 5.45e-03). ETA=6:49:09, max mem: 11.4 GB 
[10/29 08:41:12 visual_prompt]: 	Training 300/553. train loss: 0.8540,	0.5004 s / batch. (data: 8.39e-03). ETA=6:48:00, max mem: 11.4 GB 
[10/29 08:42:41 visual_prompt]: 	Training 400/553. train loss: 0.7220,	0.5010 s / batch. (data: 1.05e-02). ETA=6:47:38, max mem: 11.4 GB 
[10/29 08:44:10 visual_prompt]: 	Training 500/553. train loss: 1.4666,	0.5026 s / batch. (data: 1.20e-02). ETA=6:48:04, max mem: 11.4 GB 
[10/29 08:44:54 visual_prompt]: Epoch 12 / 100: avg data time: 3.95e-01, avg batch time: 0.8894, average train loss: 0.7304
[10/29 08:45:46 visual_prompt]: Inference (val):avg data time: 2.31e-04, avg batch time: 0.1920, average loss: 0.8177
[10/29 08:45:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.12	
[10/29 08:45:46 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/29 08:47:20 visual_prompt]: 	Training 100/553. train loss: 0.5973,	0.5153 s / batch. (data: 2.61e-04). ETA=6:57:03, max mem: 11.4 GB 
[10/29 08:48:45 visual_prompt]: 	Training 200/553. train loss: 0.7841,	0.5200 s / batch. (data: 7.97e-03). ETA=7:00:00, max mem: 11.4 GB 
[10/29 08:50:15 visual_prompt]: 	Training 300/553. train loss: 0.6030,	2.1920 s / batch. (data: 1.70e+00). ETA=1 day, 5:26:51, max mem: 11.4 GB 
[10/29 08:51:43 visual_prompt]: 	Training 400/553. train loss: 0.8790,	0.5200 s / batch. (data: 7.95e-03). ETA=6:58:16, max mem: 11.4 GB 
[10/29 08:53:12 visual_prompt]: 	Training 500/553. train loss: 0.7091,	0.4960 s / batch. (data: 2.62e-04). ETA=6:38:08, max mem: 11.4 GB 
[10/29 08:53:58 visual_prompt]: Epoch 13 / 100: avg data time: 3.93e-01, avg batch time: 0.8883, average train loss: 0.7376
[10/29 08:54:50 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1910, average loss: 0.6904
[10/29 08:54:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.08	
[10/29 08:54:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/29 08:56:23 visual_prompt]: 	Training 100/553. train loss: 0.8168,	0.4838 s / batch. (data: 2.76e-04). ETA=6:27:07, max mem: 11.4 GB 
[10/29 08:57:51 visual_prompt]: 	Training 200/553. train loss: 0.7186,	1.0089 s / batch. (data: 5.04e-01). ETA=13:25:39, max mem: 11.4 GB 
[10/29 08:59:20 visual_prompt]: 	Training 300/553. train loss: 0.7022,	0.9640 s / batch. (data: 4.63e-01). ETA=12:48:10, max mem: 11.4 GB 
[10/29 09:00:48 visual_prompt]: 	Training 400/553. train loss: 0.6009,	0.5032 s / batch. (data: 1.52e-02). ETA=6:40:07, max mem: 11.4 GB 
[10/29 09:02:17 visual_prompt]: 	Training 500/553. train loss: 0.9286,	0.5107 s / batch. (data: 2.47e-04). ETA=6:45:15, max mem: 11.4 GB 
[10/29 09:03:01 visual_prompt]: Epoch 14 / 100: avg data time: 3.93e-01, avg batch time: 0.8880, average train loss: 0.7225
[10/29 09:03:54 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1916, average loss: 0.7731
[10/29 09:03:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.52	
[10/29 09:03:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/29 09:05:26 visual_prompt]: 	Training 100/553. train loss: 0.7056,	0.5040 s / batch. (data: 7.97e-03). ETA=6:38:38, max mem: 11.4 GB 
[10/29 09:06:53 visual_prompt]: 	Training 200/553. train loss: 0.7123,	0.4914 s / batch. (data: 2.57e-04). ETA=6:27:52, max mem: 11.4 GB 
[10/29 09:08:23 visual_prompt]: 	Training 300/553. train loss: 0.6757,	0.5194 s / batch. (data: 5.35e-03). ETA=6:49:07, max mem: 11.4 GB 
[10/29 09:09:50 visual_prompt]: 	Training 400/553. train loss: 0.6358,	0.5000 s / batch. (data: 2.74e-04). ETA=6:32:57, max mem: 11.4 GB 
[10/29 09:11:19 visual_prompt]: 	Training 500/553. train loss: 0.8137,	0.4789 s / batch. (data: 2.62e-04). ETA=6:15:37, max mem: 11.4 GB 
[10/29 09:12:06 visual_prompt]: Epoch 15 / 100: avg data time: 3.94e-01, avg batch time: 0.8887, average train loss: 0.7283
[10/29 09:12:58 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.1904, average loss: 0.6982
[10/29 09:12:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.17	
[10/29 09:12:58 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/29 09:14:29 visual_prompt]: 	Training 100/553. train loss: 0.5701,	0.4885 s / batch. (data: 1.05e-02). ETA=6:21:53, max mem: 11.4 GB 
[10/29 09:15:58 visual_prompt]: 	Training 200/553. train loss: 0.7143,	0.4960 s / batch. (data: 2.65e-04). ETA=6:26:54, max mem: 11.4 GB 
[10/29 09:17:27 visual_prompt]: 	Training 300/553. train loss: 0.9723,	0.4960 s / batch. (data: 5.36e-03). ETA=6:26:04, max mem: 11.4 GB 
[10/29 09:18:56 visual_prompt]: 	Training 400/553. train loss: 0.7126,	0.4847 s / batch. (data: 2.40e-04). ETA=6:16:31, max mem: 11.4 GB 
[10/29 09:20:24 visual_prompt]: 	Training 500/553. train loss: 0.7066,	1.6911 s / batch. (data: 1.21e+00). ETA=21:50:44, max mem: 11.4 GB 
[10/29 09:21:10 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 0.8883, average train loss: 0.7168
[10/29 09:22:02 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1913, average loss: 0.7747
[10/29 09:22:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.66	
[10/29 09:22:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/29 09:23:33 visual_prompt]: 	Training 100/553. train loss: 0.5644,	0.4907 s / batch. (data: 3.29e-04). ETA=6:19:05, max mem: 11.4 GB 
[10/29 09:25:03 visual_prompt]: 	Training 200/553. train loss: 0.9856,	0.4953 s / batch. (data: 5.42e-03). ETA=6:21:47, max mem: 11.4 GB 
[10/29 09:26:31 visual_prompt]: 	Training 300/553. train loss: 0.9806,	0.4800 s / batch. (data: 2.73e-04). ETA=6:09:13, max mem: 11.4 GB 
[10/29 09:27:59 visual_prompt]: 	Training 400/553. train loss: 0.6512,	0.8299 s / batch. (data: 3.46e-01). ETA=10:36:58, max mem: 11.4 GB 
[10/29 09:29:28 visual_prompt]: 	Training 500/553. train loss: 0.6417,	2.1201 s / batch. (data: 1.64e+00). ETA=1 day, 3:03:41, max mem: 11.4 GB 
[10/29 09:30:14 visual_prompt]: Epoch 17 / 100: avg data time: 3.95e-01, avg batch time: 0.8888, average train loss: 0.7311
[10/29 09:31:06 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1929, average loss: 0.7090
[10/29 09:31:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.19	
[10/29 09:31:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/29 09:32:39 visual_prompt]: 	Training 100/553. train loss: 0.7207,	0.4880 s / batch. (data: 6.56e-03). ETA=6:12:28, max mem: 11.4 GB 
[10/29 09:34:10 visual_prompt]: 	Training 200/553. train loss: 0.7615,	0.5162 s / batch. (data: 7.48e-04). ETA=6:33:11, max mem: 11.4 GB 
[10/29 09:35:38 visual_prompt]: 	Training 300/553. train loss: 0.6470,	0.4917 s / batch. (data: 2.74e-04). ETA=6:13:39, max mem: 11.4 GB 
[10/29 09:37:07 visual_prompt]: 	Training 400/553. train loss: 0.6916,	0.5275 s / batch. (data: 2.71e-04). ETA=6:39:59, max mem: 11.4 GB 
[10/29 09:38:35 visual_prompt]: 	Training 500/553. train loss: 0.6999,	0.4847 s / batch. (data: 5.36e-03). ETA=6:06:42, max mem: 11.4 GB 
[10/29 09:39:19 visual_prompt]: Epoch 18 / 100: avg data time: 3.96e-01, avg batch time: 0.8911, average train loss: 0.7190
[10/29 09:40:12 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1926, average loss: 0.7633
[10/29 09:40:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.81	
[10/29 09:40:12 visual_prompt]: Stopping early.
[10/29 09:40:12 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 09:40:12 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 09:40:12 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 09:40:12 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 09:40:12 visual_prompt]: Training with config:
[10/29 09:40:12 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.05_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 09:40:12 visual_prompt]: Loading training data...
[10/29 09:40:12 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 09:40:12 visual_prompt]: Loading validation data...
[10/29 09:40:12 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 09:40:12 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/29 09:40:14 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/29 09:40:14 visual_prompt]: tuned percent:0.529
[10/29 09:40:15 visual_prompt]: Device used for model: 0
[10/29 09:40:15 visual_prompt]: Setting up Evaluator...
[10/29 09:40:15 visual_prompt]: Setting up Trainer...
[10/29 09:40:15 visual_prompt]: 	Setting up the optimizer...
[10/29 09:40:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 09:41:46 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5121 s / batch. (data: 1.20e-02). ETA=7:51:08, max mem: 11.4 GB 
[10/29 09:43:13 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.4923 s / batch. (data: 2.63e-04). ETA=7:32:03, max mem: 11.4 GB 
[10/29 09:44:44 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.9272 s / batch. (data: 2.45e+00). ETA=1 day, 20:43:14, max mem: 11.4 GB 
[10/29 09:46:11 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5131 s / batch. (data: 9.05e-03). ETA=7:49:28, max mem: 11.4 GB 
[10/29 09:47:42 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.4960 s / batch. (data: 2.56e-04). ETA=7:33:00, max mem: 11.4 GB 
[10/29 09:48:27 visual_prompt]: Epoch 1 / 100: avg data time: 3.96e-01, avg batch time: 0.8909, average train loss: 1.3966
[10/29 09:49:20 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1906, average loss: 1.3454
[10/29 09:49:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/29 09:49:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/29 09:50:51 visual_prompt]: 	Training 100/553. train loss: 0.6972,	1.0201 s / batch. (data: 5.34e-01). ETA=15:29:05, max mem: 11.4 GB 
[10/29 09:52:19 visual_prompt]: 	Training 200/553. train loss: 0.3721,	1.9400 s / batch. (data: 1.45e+00). ETA=1 day, 5:23:42, max mem: 11.4 GB 
[10/29 09:53:49 visual_prompt]: 	Training 300/553. train loss: 0.6404,	1.6360 s / batch. (data: 1.12e+00). ETA=1 day, 0:44:34, max mem: 11.4 GB 
[10/29 09:55:16 visual_prompt]: 	Training 400/553. train loss: 0.7997,	0.5160 s / batch. (data: 2.82e-04). ETA=7:47:22, max mem: 11.4 GB 
[10/29 09:56:46 visual_prompt]: 	Training 500/553. train loss: 0.6541,	0.4886 s / batch. (data: 2.75e-04). ETA=7:21:45, max mem: 11.4 GB 
[10/29 09:57:31 visual_prompt]: Epoch 2 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 0.7501
[10/29 09:58:23 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1908, average loss: 0.7281
[10/29 09:58:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.55	
[10/29 09:58:23 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/29 09:59:54 visual_prompt]: 	Training 100/553. train loss: 0.7582,	0.4847 s / batch. (data: 5.37e-03). ETA=7:16:59, max mem: 11.4 GB 
[10/29 10:01:24 visual_prompt]: 	Training 200/553. train loss: 0.7944,	1.6344 s / batch. (data: 1.16e+00). ETA=1 day, 0:30:47, max mem: 11.4 GB 
[10/29 10:02:50 visual_prompt]: 	Training 300/553. train loss: 0.5395,	0.5159 s / batch. (data: 7.85e-03). ETA=7:43:25, max mem: 11.4 GB 
[10/29 10:04:20 visual_prompt]: 	Training 400/553. train loss: 0.6360,	0.4960 s / batch. (data: 2.57e-04). ETA=7:24:41, max mem: 11.4 GB 
[10/29 10:05:50 visual_prompt]: 	Training 500/553. train loss: 0.7208,	1.9033 s / batch. (data: 1.41e+00). ETA=1 day, 4:23:13, max mem: 11.4 GB 
[10/29 10:06:34 visual_prompt]: Epoch 3 / 100: avg data time: 3.92e-01, avg batch time: 0.8871, average train loss: 0.7409
[10/29 10:07:26 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1910, average loss: 0.7305
[10/29 10:07:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.83	
[10/29 10:07:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/29 10:09:00 visual_prompt]: 	Training 100/553. train loss: 0.7351,	0.5080 s / batch. (data: 7.98e-03). ETA=7:33:17, max mem: 11.4 GB 
[10/29 10:10:29 visual_prompt]: 	Training 200/553. train loss: 0.5611,	0.4788 s / batch. (data: 2.74e-04). ETA=7:06:29, max mem: 11.4 GB 
[10/29 10:11:57 visual_prompt]: 	Training 300/553. train loss: 0.6160,	1.3426 s / batch. (data: 8.57e-01). ETA=19:53:34, max mem: 11.4 GB 
[10/29 10:13:23 visual_prompt]: 	Training 400/553. train loss: 0.7723,	1.9451 s / batch. (data: 1.47e+00). ETA=1 day, 4:46:01, max mem: 11.4 GB 
[10/29 10:14:53 visual_prompt]: 	Training 500/553. train loss: 0.4610,	3.6800 s / batch. (data: 3.20e+00). ETA=2 days, 6:19:19, max mem: 11.4 GB 
[10/29 10:15:38 visual_prompt]: Epoch 4 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.7425
[10/29 10:16:31 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1917, average loss: 0.6851
[10/29 10:16:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[10/29 10:16:31 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/29 10:18:01 visual_prompt]: 	Training 100/553. train loss: 0.4832,	0.4800 s / batch. (data: 3.08e-04). ETA=7:03:51, max mem: 11.4 GB 
[10/29 10:19:31 visual_prompt]: 	Training 200/553. train loss: 0.6412,	1.9080 s / batch. (data: 1.42e+00). ETA=1 day, 4:01:50, max mem: 11.4 GB 
[10/29 10:21:00 visual_prompt]: 	Training 300/553. train loss: 0.8802,	0.4792 s / batch. (data: 2.53e-04). ETA=7:01:38, max mem: 11.4 GB 
[10/29 10:22:27 visual_prompt]: 	Training 400/553. train loss: 0.5523,	0.4926 s / batch. (data: 1.20e-02). ETA=7:12:32, max mem: 11.4 GB 
[10/29 10:23:57 visual_prompt]: 	Training 500/553. train loss: 0.5851,	0.5120 s / batch. (data: 2.52e-04). ETA=7:28:45, max mem: 11.4 GB 
[10/29 10:24:44 visual_prompt]: Epoch 5 / 100: avg data time: 3.97e-01, avg batch time: 0.8915, average train loss: 0.7433
[10/29 10:25:36 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.1908, average loss: 0.6887
[10/29 10:25:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.85	
[10/29 10:25:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/29 10:27:10 visual_prompt]: 	Training 100/553. train loss: 0.5821,	0.5131 s / batch. (data: 5.83e-03). ETA=7:28:26, max mem: 11.4 GB 
[10/29 10:28:39 visual_prompt]: 	Training 200/553. train loss: 0.7939,	0.5000 s / batch. (data: 1.20e-02). ETA=7:16:05, max mem: 11.4 GB 
[10/29 10:30:06 visual_prompt]: 	Training 300/553. train loss: 0.5456,	0.4953 s / batch. (data: 2.87e-04). ETA=7:11:10, max mem: 11.4 GB 
[10/29 10:31:40 visual_prompt]: 	Training 400/553. train loss: 0.6289,	1.2249 s / batch. (data: 7.22e-01). ETA=17:44:20, max mem: 11.4 GB 
[10/29 10:33:08 visual_prompt]: 	Training 500/553. train loss: 0.7905,	1.4242 s / batch. (data: 9.37e-01). ETA=20:35:06, max mem: 11.4 GB 
[10/29 10:33:53 visual_prompt]: Epoch 6 / 100: avg data time: 4.02e-01, avg batch time: 0.8972, average train loss: 0.7461
[10/29 10:34:46 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1902, average loss: 0.6924
[10/29 10:34:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.94	
[10/29 10:34:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/29 10:36:17 visual_prompt]: 	Training 100/553. train loss: 0.4529,	0.5080 s / batch. (data: 2.73e-04). ETA=7:19:16, max mem: 11.4 GB 
[10/29 10:37:46 visual_prompt]: 	Training 200/553. train loss: 0.5302,	1.6760 s / batch. (data: 1.20e+00). ETA=1 day, 0:06:26, max mem: 11.4 GB 
[10/29 10:39:19 visual_prompt]: 	Training 300/553. train loss: 0.9286,	2.4962 s / batch. (data: 2.02e+00). ETA=1 day, 11:50:06, max mem: 11.4 GB 
[10/29 10:40:48 visual_prompt]: 	Training 400/553. train loss: 0.6694,	2.3467 s / batch. (data: 1.87e+00). ETA=1 day, 9:37:24, max mem: 11.4 GB 
[10/29 10:42:14 visual_prompt]: 	Training 500/553. train loss: 0.7590,	0.4923 s / batch. (data: 2.75e-04). ETA=7:02:22, max mem: 11.4 GB 
[10/29 10:42:59 visual_prompt]: Epoch 7 / 100: avg data time: 3.97e-01, avg batch time: 0.8921, average train loss: 0.7393
[10/29 10:43:52 visual_prompt]: Inference (val):avg data time: 1.85e-04, avg batch time: 0.1920, average loss: 0.7732
[10/29 10:43:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.29	
[10/29 10:43:52 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/29 10:45:23 visual_prompt]: 	Training 100/553. train loss: 0.7081,	0.9673 s / batch. (data: 4.87e-01). ETA=13:47:30, max mem: 11.4 GB 
[10/29 10:46:53 visual_prompt]: 	Training 200/553. train loss: 1.1810,	0.4794 s / batch. (data: 2.43e-04). ETA=6:49:17, max mem: 11.4 GB 
[10/29 10:48:23 visual_prompt]: 	Training 300/553. train loss: 0.7646,	0.5026 s / batch. (data: 5.39e-03). ETA=7:08:16, max mem: 11.4 GB 
[10/29 10:49:53 visual_prompt]: 	Training 400/553. train loss: 0.7255,	1.4315 s / batch. (data: 9.43e-01). ETA=20:17:29, max mem: 11.4 GB 
[10/29 10:51:23 visual_prompt]: 	Training 500/553. train loss: 0.9127,	2.3103 s / batch. (data: 1.83e+00). ETA=1 day, 8:41:01, max mem: 11.4 GB 
[10/29 10:52:08 visual_prompt]: Epoch 8 / 100: avg data time: 4.02e-01, avg batch time: 0.8962, average train loss: 0.7551
[10/29 10:53:01 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1892, average loss: 0.6962
[10/29 10:53:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.18	
[10/29 10:53:01 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/29 10:54:33 visual_prompt]: 	Training 100/553. train loss: 0.4322,	0.4923 s / batch. (data: 7.96e-03). ETA=6:56:36, max mem: 11.4 GB 
[10/29 10:56:02 visual_prompt]: 	Training 200/553. train loss: 0.5753,	0.5000 s / batch. (data: 2.76e-04). ETA=7:02:19, max mem: 11.4 GB 
[10/29 10:57:31 visual_prompt]: 	Training 300/553. train loss: 0.5802,	2.3542 s / batch. (data: 1.86e+00). ETA=1 day, 9:04:26, max mem: 11.4 GB 
[10/29 10:59:01 visual_prompt]: 	Training 400/553. train loss: 0.6194,	0.5120 s / batch. (data: 7.97e-03). ETA=7:10:43, max mem: 11.4 GB 
[10/29 11:00:31 visual_prompt]: 	Training 500/553. train loss: 0.5945,	1.4070 s / batch. (data: 9.12e-01). ETA=19:41:16, max mem: 11.4 GB 
[10/29 11:01:15 visual_prompt]: Epoch 9 / 100: avg data time: 3.98e-01, avg batch time: 0.8930, average train loss: 0.7512
[10/29 11:02:08 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1910, average loss: 0.7002
[10/29 11:02:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.10	
[10/29 11:02:08 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/29 11:03:42 visual_prompt]: 	Training 100/553. train loss: 0.6827,	0.5075 s / batch. (data: 5.38e-03). ETA=7:04:48, max mem: 11.4 GB 
[10/29 11:05:10 visual_prompt]: 	Training 200/553. train loss: 0.6950,	0.4887 s / batch. (data: 2.58e-04). ETA=6:48:16, max mem: 11.4 GB 
[10/29 11:06:38 visual_prompt]: 	Training 300/553. train loss: 0.7623,	1.6743 s / batch. (data: 1.17e+00). ETA=23:15:53, max mem: 11.4 GB 
[10/29 11:08:04 visual_prompt]: 	Training 400/553. train loss: 0.9120,	0.5722 s / batch. (data: 7.55e-02). ETA=7:56:08, max mem: 11.4 GB 
[10/29 11:09:34 visual_prompt]: 	Training 500/553. train loss: 0.8730,	0.4792 s / batch. (data: 2.73e-04). ETA=6:37:54, max mem: 11.4 GB 
[10/29 11:10:20 visual_prompt]: Epoch 10 / 100: avg data time: 3.94e-01, avg batch time: 0.8894, average train loss: 0.7498
[10/29 11:11:12 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1923, average loss: 0.8428
[10/29 11:11:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.47	
[10/29 11:11:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/29 11:12:46 visual_prompt]: 	Training 100/553. train loss: 0.6822,	0.4973 s / batch. (data: 5.38e-03). ETA=6:51:39, max mem: 11.4 GB 
[10/29 11:14:16 visual_prompt]: 	Training 200/553. train loss: 1.0283,	0.5080 s / batch. (data: 2.77e-04). ETA=6:59:41, max mem: 11.4 GB 
[10/29 11:15:43 visual_prompt]: 	Training 300/553. train loss: 0.4260,	2.0883 s / batch. (data: 1.59e+00). ETA=1 day, 4:41:49, max mem: 11.4 GB 
[10/29 11:17:11 visual_prompt]: 	Training 400/553. train loss: 0.7427,	0.4919 s / batch. (data: 7.55e-04). ETA=6:44:44, max mem: 11.4 GB 
[10/29 11:18:39 visual_prompt]: 	Training 500/553. train loss: 0.6990,	0.4868 s / batch. (data: 8.81e-03). ETA=6:39:45, max mem: 11.4 GB 
[10/29 11:19:24 visual_prompt]: Epoch 11 / 100: avg data time: 3.93e-01, avg batch time: 0.8886, average train loss: 0.7409
[10/29 11:20:16 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1904, average loss: 0.7037
[10/29 11:20:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.14	
[10/29 11:20:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/29 11:21:49 visual_prompt]: 	Training 100/553. train loss: 0.9702,	0.4893 s / batch. (data: 8.30e-03). ETA=6:40:31, max mem: 11.4 GB 
[10/29 11:23:19 visual_prompt]: 	Training 200/553. train loss: 0.5719,	0.4920 s / batch. (data: 5.41e-03). ETA=6:41:56, max mem: 11.4 GB 
[10/29 11:24:45 visual_prompt]: 	Training 300/553. train loss: 0.7246,	0.4915 s / batch. (data: 2.72e-04). ETA=6:40:43, max mem: 11.4 GB 
[10/29 11:26:15 visual_prompt]: 	Training 400/553. train loss: 0.6917,	0.5017 s / batch. (data: 2.73e-04). ETA=6:48:13, max mem: 11.4 GB 
[10/29 11:27:44 visual_prompt]: 	Training 500/553. train loss: 1.4247,	0.4831 s / batch. (data: 5.39e-03). ETA=6:32:12, max mem: 11.4 GB 
[10/29 11:28:28 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8896, average train loss: 0.7462
[10/29 11:29:21 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1912, average loss: 0.8303
[10/29 11:29:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.81	
[10/29 11:29:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/29 11:30:54 visual_prompt]: 	Training 100/553. train loss: 0.5744,	1.2438 s / batch. (data: 7.43e-01). ETA=16:46:43, max mem: 11.4 GB 
[10/29 11:32:20 visual_prompt]: 	Training 200/553. train loss: 0.7672,	1.3954 s / batch. (data: 8.83e-01). ETA=18:47:06, max mem: 11.4 GB 
[10/29 11:33:50 visual_prompt]: 	Training 300/553. train loss: 0.5941,	2.1440 s / batch. (data: 1.65e+00). ETA=1 day, 4:48:11, max mem: 11.4 GB 
[10/29 11:35:17 visual_prompt]: 	Training 400/553. train loss: 0.6977,	0.4955 s / batch. (data: 7.96e-03). ETA=6:38:33, max mem: 11.4 GB 
[10/29 11:36:46 visual_prompt]: 	Training 500/553. train loss: 0.6939,	0.5125 s / batch. (data: 1.05e-02). ETA=6:51:23, max mem: 11.4 GB 
[10/29 11:37:32 visual_prompt]: Epoch 13 / 100: avg data time: 3.93e-01, avg batch time: 0.8884, average train loss: 0.7419
[10/29 11:38:24 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1893, average loss: 0.6922
[10/29 11:38:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.61	
[10/29 11:38:24 visual_prompt]: Best epoch 13: best metric: -0.692
[10/29 11:38:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/29 11:39:57 visual_prompt]: 	Training 100/553. train loss: 0.8477,	0.5080 s / batch. (data: 7.94e-03). ETA=6:46:29, max mem: 11.4 GB 
[10/29 11:41:27 visual_prompt]: 	Training 200/553. train loss: 0.6724,	1.8149 s / batch. (data: 1.33e+00). ETA=1 day, 0:09:13, max mem: 11.4 GB 
[10/29 11:42:55 visual_prompt]: 	Training 300/553. train loss: 0.6762,	1.3359 s / batch. (data: 8.54e-01). ETA=17:44:30, max mem: 11.4 GB 
[10/29 11:44:22 visual_prompt]: 	Training 400/553. train loss: 0.6416,	0.5215 s / batch. (data: 2.06e-02). ETA=6:54:39, max mem: 11.4 GB 
[10/29 11:45:51 visual_prompt]: 	Training 500/553. train loss: 0.9632,	0.4901 s / batch. (data: 2.29e-04). ETA=6:28:53, max mem: 11.4 GB 
[10/29 11:46:36 visual_prompt]: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.7278
[10/29 11:47:29 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1915, average loss: 0.7771
[10/29 11:47:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.77	
[10/29 11:47:29 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/29 11:49:00 visual_prompt]: 	Training 100/553. train loss: 0.6976,	0.5201 s / batch. (data: 2.65e-04). ETA=6:51:24, max mem: 11.4 GB 
[10/29 11:50:28 visual_prompt]: 	Training 200/553. train loss: 0.6563,	0.5048 s / batch. (data: 2.81e-04). ETA=6:38:26, max mem: 11.4 GB 
[10/29 11:51:57 visual_prompt]: 	Training 300/553. train loss: 0.7683,	0.5190 s / batch. (data: 1.04e-02). ETA=6:48:47, max mem: 11.4 GB 
[10/29 11:53:24 visual_prompt]: 	Training 400/553. train loss: 0.6147,	0.5925 s / batch. (data: 8.09e-02). ETA=7:45:39, max mem: 11.4 GB 
[10/29 11:54:54 visual_prompt]: 	Training 500/553. train loss: 0.8391,	0.4787 s / batch. (data: 2.87e-04). ETA=6:15:26, max mem: 11.4 GB 
[10/29 11:55:40 visual_prompt]: Epoch 15 / 100: avg data time: 3.93e-01, avg batch time: 0.8891, average train loss: 0.7285
[10/29 11:56:33 visual_prompt]: Inference (val):avg data time: 1.99e-04, avg batch time: 0.1900, average loss: 0.7075
[10/29 11:56:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.48	
[10/29 11:56:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/29 11:58:03 visual_prompt]: 	Training 100/553. train loss: 0.5637,	0.4960 s / batch. (data: 2.62e-04). ETA=6:27:44, max mem: 11.4 GB 
[10/29 11:59:32 visual_prompt]: 	Training 200/553. train loss: 0.8473,	0.5006 s / batch. (data: 1.60e-02). ETA=6:30:31, max mem: 11.4 GB 
[10/29 12:01:01 visual_prompt]: 	Training 300/553. train loss: 1.1255,	0.4785 s / batch. (data: 2.94e-04). ETA=6:12:29, max mem: 11.4 GB 
[10/29 12:02:30 visual_prompt]: 	Training 400/553. train loss: 0.7138,	0.4996 s / batch. (data: 2.06e-02). ETA=6:28:05, max mem: 11.4 GB 
[10/29 12:03:58 visual_prompt]: 	Training 500/553. train loss: 0.7026,	1.9160 s / batch. (data: 1.42e+00). ETA=1 day, 0:45:02, max mem: 11.4 GB 
[10/29 12:04:44 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 0.7237
[10/29 12:05:37 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1918, average loss: 0.7588
[10/29 12:05:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.23	
[10/29 12:05:37 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/29 12:07:07 visual_prompt]: 	Training 100/553. train loss: 0.5557,	0.4791 s / batch. (data: 2.65e-04). ETA=6:10:05, max mem: 11.4 GB 
[10/29 12:08:38 visual_prompt]: 	Training 200/553. train loss: 0.6224,	0.5040 s / batch. (data: 2.65e-04). ETA=6:28:30, max mem: 11.4 GB 
[10/29 12:10:06 visual_prompt]: 	Training 300/553. train loss: 0.9705,	0.4908 s / batch. (data: 2.58e-04). ETA=6:17:32, max mem: 11.4 GB 
[10/29 12:11:34 visual_prompt]: 	Training 400/553. train loss: 0.5705,	1.3866 s / batch. (data: 8.99e-01). ETA=17:44:14, max mem: 11.4 GB 
[10/29 12:13:02 visual_prompt]: 	Training 500/553. train loss: 0.6609,	2.2080 s / batch. (data: 1.71e+00). ETA=1 day, 4:11:02, max mem: 11.4 GB 
[10/29 12:13:48 visual_prompt]: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.7161
[10/29 12:14:41 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1911, average loss: 0.7185
[10/29 12:14:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.41	
[10/29 12:14:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/29 12:16:13 visual_prompt]: 	Training 100/553. train loss: 0.7142,	0.4960 s / batch. (data: 2.57e-04). ETA=6:18:36, max mem: 11.4 GB 
[10/29 12:17:44 visual_prompt]: 	Training 200/553. train loss: 0.7519,	0.5322 s / batch. (data: 2.61e-02). ETA=6:45:21, max mem: 11.4 GB 
[10/29 12:19:13 visual_prompt]: 	Training 300/553. train loss: 0.6267,	0.5284 s / batch. (data: 2.04e-02). ETA=6:41:34, max mem: 11.4 GB 
[10/29 12:20:40 visual_prompt]: 	Training 400/553. train loss: 0.7104,	0.4846 s / batch. (data: 2.73e-04). ETA=6:07:28, max mem: 11.4 GB 
[10/29 12:22:08 visual_prompt]: 	Training 500/553. train loss: 0.6983,	0.6360 s / batch. (data: 1.41e-01). ETA=8:01:12, max mem: 11.4 GB 
[10/29 12:22:53 visual_prompt]: Epoch 18 / 100: avg data time: 3.94e-01, avg batch time: 0.8890, average train loss: 0.7208
[10/29 12:23:45 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1924, average loss: 0.7070
[10/29 12:23:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.96	
[10/29 12:23:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[10/29 12:25:17 visual_prompt]: 	Training 100/553. train loss: 0.9295,	0.4948 s / batch. (data: 2.74e-04). ETA=6:13:08, max mem: 11.4 GB 
[10/29 12:26:46 visual_prompt]: 	Training 200/553. train loss: 0.7734,	0.4976 s / batch. (data: 3.10e-04). ETA=6:14:23, max mem: 11.4 GB 
[10/29 12:28:14 visual_prompt]: 	Training 300/553. train loss: 1.1660,	0.5000 s / batch. (data: 2.70e-04). ETA=6:15:22, max mem: 11.4 GB 
[10/29 12:29:45 visual_prompt]: 	Training 400/553. train loss: 0.5715,	0.5002 s / batch. (data: 7.42e-04). ETA=6:14:43, max mem: 11.4 GB 
[10/29 12:31:09 visual_prompt]: 	Training 500/553. train loss: 0.7393,	0.4883 s / batch. (data: 2.99e-04). ETA=6:05:00, max mem: 11.4 GB 
[10/29 12:31:55 visual_prompt]: Epoch 19 / 100: avg data time: 3.90e-01, avg batch time: 0.8861, average train loss: 0.7112
[10/29 12:32:48 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1920, average loss: 0.6877
[10/29 12:32:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.26	
[10/29 12:32:48 visual_prompt]: Best epoch 19: best metric: -0.688
[10/29 12:32:48 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[10/29 12:34:18 visual_prompt]: 	Training 100/553. train loss: 0.7806,	0.4920 s / batch. (data: 7.96e-03). ETA=6:06:30, max mem: 11.4 GB 
[10/29 12:35:48 visual_prompt]: 	Training 200/553. train loss: 0.6497,	0.4920 s / batch. (data: 2.92e-04). ETA=6:05:39, max mem: 11.4 GB 
[10/29 12:37:17 visual_prompt]: 	Training 300/553. train loss: 0.6701,	0.4921 s / batch. (data: 2.82e-04). ETA=6:04:54, max mem: 11.4 GB 
[10/29 12:38:45 visual_prompt]: 	Training 400/553. train loss: 0.5960,	0.5040 s / batch. (data: 2.60e-04). ETA=6:12:53, max mem: 11.4 GB 
[10/29 12:40:13 visual_prompt]: 	Training 500/553. train loss: 0.7665,	0.4920 s / batch. (data: 2.66e-04). ETA=6:03:13, max mem: 11.4 GB 
[10/29 12:41:00 visual_prompt]: Epoch 20 / 100: avg data time: 3.94e-01, avg batch time: 0.8894, average train loss: 0.7287
[10/29 12:41:52 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1915, average loss: 0.7485
[10/29 12:41:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.56	
[10/29 12:41:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[10/29 12:43:26 visual_prompt]: 	Training 100/553. train loss: 0.6148,	0.6676 s / batch. (data: 1.90e-01). ETA=8:11:09, max mem: 11.4 GB 
[10/29 12:44:53 visual_prompt]: 	Training 200/553. train loss: 0.6961,	0.5120 s / batch. (data: 7.97e-03). ETA=6:15:49, max mem: 11.4 GB 
[10/29 12:46:23 visual_prompt]: 	Training 300/553. train loss: 0.8802,	1.4731 s / batch. (data: 9.62e-01). ETA=17:58:48, max mem: 11.4 GB 
[10/29 12:47:49 visual_prompt]: 	Training 400/553. train loss: 0.6213,	0.5360 s / batch. (data: 4.81e-02). ETA=6:31:37, max mem: 11.4 GB 
[10/29 12:49:19 visual_prompt]: 	Training 500/553. train loss: 0.7084,	0.5075 s / batch. (data: 3.09e-04). ETA=6:09:59, max mem: 11.4 GB 
[10/29 12:50:03 visual_prompt]: Epoch 21 / 100: avg data time: 3.94e-01, avg batch time: 0.8878, average train loss: 0.7228
[10/29 12:50:56 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.1904, average loss: 0.7615
[10/29 12:50:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.99	
[10/29 12:50:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[10/29 12:52:27 visual_prompt]: 	Training 100/553. train loss: 0.7085,	0.4913 s / batch. (data: 2.79e-04). ETA=5:56:53, max mem: 11.4 GB 
[10/29 12:53:56 visual_prompt]: 	Training 200/553. train loss: 0.6094,	0.5000 s / batch. (data: 2.56e-04). ETA=6:02:23, max mem: 11.4 GB 
[10/29 12:55:22 visual_prompt]: 	Training 300/553. train loss: 0.5022,	0.5443 s / batch. (data: 4.26e-02). ETA=6:33:36, max mem: 11.4 GB 
[10/29 12:56:51 visual_prompt]: 	Training 400/553. train loss: 0.7057,	0.4983 s / batch. (data: 2.70e-04). ETA=5:59:31, max mem: 11.4 GB 
[10/29 12:58:20 visual_prompt]: 	Training 500/553. train loss: 0.6227,	0.5038 s / batch. (data: 1.04e-02). ETA=6:02:35, max mem: 11.4 GB 
[10/29 12:59:07 visual_prompt]: Epoch 22 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.7235
[10/29 13:00:00 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1909, average loss: 0.7204
[10/29 13:00:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.74	
[10/29 13:00:00 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[10/29 13:01:33 visual_prompt]: 	Training 100/553. train loss: 0.6923,	0.4913 s / batch. (data: 2.62e-04). ETA=5:52:22, max mem: 11.4 GB 
[10/29 13:03:02 visual_prompt]: 	Training 200/553. train loss: 0.6374,	1.4948 s / batch. (data: 1.01e+00). ETA=17:49:37, max mem: 11.4 GB 
[10/29 13:04:32 visual_prompt]: 	Training 300/553. train loss: 0.7860,	0.4912 s / batch. (data: 7.12e-04). ETA=5:50:40, max mem: 11.4 GB 
[10/29 13:05:59 visual_prompt]: 	Training 400/553. train loss: 0.5748,	0.5076 s / batch. (data: 5.83e-03). ETA=6:01:31, max mem: 11.4 GB 
[10/29 13:07:24 visual_prompt]: 	Training 500/553. train loss: 0.9828,	0.5443 s / batch. (data: 8.23e-03). ETA=6:26:43, max mem: 11.4 GB 
[10/29 13:08:10 visual_prompt]: Epoch 23 / 100: avg data time: 3.93e-01, avg batch time: 0.8873, average train loss: 0.7234
[10/29 13:09:03 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.1911, average loss: 0.6916
[10/29 13:09:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.15	
[10/29 13:09:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[10/29 13:10:32 visual_prompt]: 	Training 100/553. train loss: 0.7899,	0.5028 s / batch. (data: 2.67e-04). ETA=5:56:00, max mem: 11.4 GB 
[10/29 13:12:00 visual_prompt]: 	Training 200/553. train loss: 0.6643,	0.5013 s / batch. (data: 1.55e-02). ETA=5:54:05, max mem: 11.4 GB 
[10/29 13:13:29 visual_prompt]: 	Training 300/553. train loss: 0.7070,	1.5999 s / batch. (data: 1.12e+00). ETA=18:47:24, max mem: 11.4 GB 
[10/29 13:14:59 visual_prompt]: 	Training 400/553. train loss: 0.7029,	0.5040 s / batch. (data: 1.19e-02). ETA=5:54:17, max mem: 11.4 GB 
[10/29 13:16:29 visual_prompt]: 	Training 500/553. train loss: 0.8630,	0.9554 s / batch. (data: 4.52e-01). ETA=11:10:05, max mem: 11.4 GB 
[10/29 13:17:15 visual_prompt]: Epoch 24 / 100: avg data time: 3.95e-01, avg batch time: 0.8902, average train loss: 0.7279
[10/29 13:18:08 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1919, average loss: 0.6936
[10/29 13:18:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.69	
[10/29 13:18:08 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[10/29 13:19:44 visual_prompt]: 	Training 100/553. train loss: 0.5967,	0.5162 s / batch. (data: 1.20e-02). ETA=6:00:43, max mem: 11.4 GB 
[10/29 13:21:11 visual_prompt]: 	Training 200/553. train loss: 0.7751,	0.4966 s / batch. (data: 1.05e-02). ETA=5:46:12, max mem: 11.4 GB 
[10/29 13:22:39 visual_prompt]: 	Training 300/553. train loss: 0.6999,	1.1078 s / batch. (data: 6.16e-01). ETA=12:50:26, max mem: 11.4 GB 
[10/29 13:24:08 visual_prompt]: 	Training 400/553. train loss: 0.7335,	1.6600 s / batch. (data: 1.15e+00). ETA=19:11:41, max mem: 11.4 GB 
[10/29 13:25:37 visual_prompt]: 	Training 500/553. train loss: 0.6985,	2.1534 s / batch. (data: 1.65e+00). ETA=1 day, 0:50:26, max mem: 11.4 GB 
[10/29 13:26:22 visual_prompt]: Epoch 25 / 100: avg data time: 3.98e-01, avg batch time: 0.8933, average train loss: 0.7108
[10/29 13:27:15 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1907, average loss: 0.6919
[10/29 13:27:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.98	
[10/29 13:27:15 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[10/29 13:28:46 visual_prompt]: 	Training 100/553. train loss: 0.7106,	0.5000 s / batch. (data: 7.98e-03). ETA=5:44:47, max mem: 11.4 GB 
[10/29 13:30:15 visual_prompt]: 	Training 200/553. train loss: 0.6190,	1.6280 s / batch. (data: 1.12e+00). ETA=18:39:55, max mem: 11.4 GB 
[10/29 13:31:46 visual_prompt]: 	Training 300/553. train loss: 0.4986,	0.4803 s / batch. (data: 2.70e-04). ETA=5:29:36, max mem: 11.4 GB 
[10/29 13:33:13 visual_prompt]: 	Training 400/553. train loss: 0.5682,	0.4902 s / batch. (data: 2.58e-04). ETA=5:35:33, max mem: 11.4 GB 
[10/29 13:34:40 visual_prompt]: 	Training 500/553. train loss: 0.7560,	0.5042 s / batch. (data: 2.45e-04). ETA=5:44:19, max mem: 11.4 GB 
[10/29 13:35:25 visual_prompt]: Epoch 26 / 100: avg data time: 3.93e-01, avg batch time: 0.8866, average train loss: 0.7106
[10/29 13:36:18 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1912, average loss: 0.7463
[10/29 13:36:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.01	
[10/29 13:36:18 visual_prompt]: Stopping early.
[10/29 13:36:18 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 13:36:18 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 13:36:18 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 13:36:18 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 13:36:18 visual_prompt]: Training with config:
[10/29 13:36:18 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.05_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 13:36:18 visual_prompt]: Loading training data...
[10/29 13:36:18 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 13:36:18 visual_prompt]: Loading validation data...
[10/29 13:36:18 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 13:36:18 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/29 13:36:20 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/29 13:36:20 visual_prompt]: tuned percent:0.529
[10/29 13:36:20 visual_prompt]: Device used for model: 0
[10/29 13:36:20 visual_prompt]: Setting up Evaluator...
[10/29 13:36:20 visual_prompt]: Setting up Trainer...
[10/29 13:36:20 visual_prompt]: 	Setting up the optimizer...
[10/29 13:36:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 13:37:52 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.4880 s / batch. (data: 2.64e-04). ETA=7:28:57, max mem: 11.4 GB 
[10/29 13:39:19 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5393 s / batch. (data: 9.58e-04). ETA=8:15:18, max mem: 11.4 GB 
[10/29 13:40:50 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.8668 s / batch. (data: 2.38e+00). ETA=1 day, 19:47:51, max mem: 11.4 GB 
[10/29 13:42:16 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.4920 s / batch. (data: 2.49e-04). ETA=7:30:12, max mem: 11.4 GB 
[10/29 13:43:47 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5420 s / batch. (data: 2.43e-02). ETA=8:15:04, max mem: 11.4 GB 
[10/29 13:44:33 visual_prompt]: Epoch 1 / 100: avg data time: 3.94e-01, avg batch time: 0.8900, average train loss: 1.3966
[10/29 13:45:25 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1910, average loss: 1.3454
[10/29 13:45:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/29 13:45:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/29 13:46:57 visual_prompt]: 	Training 100/553. train loss: 0.6973,	2.1000 s / batch. (data: 1.61e+00). ETA=1 day, 7:52:40, max mem: 11.4 GB 
[10/29 13:48:24 visual_prompt]: 	Training 200/553. train loss: 0.3712,	1.0160 s / batch. (data: 5.07e-01). ETA=15:23:39, max mem: 11.4 GB 
[10/29 13:49:55 visual_prompt]: 	Training 300/553. train loss: 0.6396,	1.7503 s / batch. (data: 1.26e+00). ETA=1 day, 2:28:21, max mem: 11.4 GB 
[10/29 13:51:22 visual_prompt]: 	Training 400/553. train loss: 0.8021,	0.5600 s / batch. (data: 4.72e-02). ETA=8:27:13, max mem: 11.4 GB 
[10/29 13:52:52 visual_prompt]: 	Training 500/553. train loss: 0.6534,	0.5161 s / batch. (data: 2.58e-04). ETA=7:46:34, max mem: 11.4 GB 
[10/29 13:53:36 visual_prompt]: Epoch 2 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 0.7504
[10/29 13:54:29 visual_prompt]: Inference (val):avg data time: 1.82e-04, avg batch time: 0.1911, average loss: 0.7280
[10/29 13:54:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.65	
[10/29 13:54:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/29 13:55:59 visual_prompt]: 	Training 100/553. train loss: 0.7585,	0.5068 s / batch. (data: 4.17e-04). ETA=7:36:55, max mem: 11.4 GB 
[10/29 13:57:29 visual_prompt]: 	Training 200/553. train loss: 0.7985,	1.1640 s / batch. (data: 6.71e-01). ETA=17:27:28, max mem: 11.4 GB 
[10/29 13:58:57 visual_prompt]: 	Training 300/553. train loss: 0.5383,	0.5069 s / batch. (data: 2.98e-04). ETA=7:35:21, max mem: 11.4 GB 
[10/29 14:00:28 visual_prompt]: 	Training 400/553. train loss: 0.6304,	0.5004 s / batch. (data: 8.27e-03). ETA=7:28:36, max mem: 11.4 GB 
[10/29 14:01:58 visual_prompt]: 	Training 500/553. train loss: 0.7210,	1.7760 s / batch. (data: 1.28e+00). ETA=1 day, 2:29:18, max mem: 11.4 GB 
[10/29 14:02:42 visual_prompt]: Epoch 3 / 100: avg data time: 3.97e-01, avg batch time: 0.8921, average train loss: 0.7416
[10/29 14:03:35 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1914, average loss: 0.7315
[10/29 14:03:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.86	
[10/29 14:03:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/29 14:05:08 visual_prompt]: 	Training 100/553. train loss: 0.7500,	0.4935 s / batch. (data: 2.68e-04). ETA=7:20:23, max mem: 11.4 GB 
[10/29 14:06:38 visual_prompt]: 	Training 200/553. train loss: 0.5644,	0.4927 s / batch. (data: 5.39e-03). ETA=7:18:49, max mem: 11.4 GB 
[10/29 14:08:07 visual_prompt]: 	Training 300/553. train loss: 0.6101,	1.5472 s / batch. (data: 1.06e+00). ETA=22:55:28, max mem: 11.4 GB 
[10/29 14:09:32 visual_prompt]: 	Training 400/553. train loss: 0.7542,	0.4911 s / batch. (data: 2.66e-04). ETA=7:15:49, max mem: 11.4 GB 
[10/29 14:11:03 visual_prompt]: 	Training 500/553. train loss: 0.4861,	3.7840 s / batch. (data: 3.30e+00). ETA=2 days, 7:51:26, max mem: 11.4 GB 
[10/29 14:11:48 visual_prompt]: Epoch 4 / 100: avg data time: 3.97e-01, avg batch time: 0.8919, average train loss: 0.7450
[10/29 14:12:41 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1906, average loss: 0.6835
[10/29 14:12:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.29	
[10/29 14:12:41 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/29 14:14:10 visual_prompt]: 	Training 100/553. train loss: 0.4775,	0.5120 s / batch. (data: 1.20e-02). ETA=7:32:07, max mem: 11.4 GB 
[10/29 14:15:40 visual_prompt]: 	Training 200/553. train loss: 0.6579,	1.8760 s / batch. (data: 1.39e+00). ETA=1 day, 3:33:36, max mem: 11.4 GB 
[10/29 14:17:09 visual_prompt]: 	Training 300/553. train loss: 0.8829,	0.5153 s / batch. (data: 7.29e-04). ETA=7:33:21, max mem: 11.4 GB 
[10/29 14:18:36 visual_prompt]: 	Training 400/553. train loss: 0.5526,	0.5000 s / batch. (data: 2.62e-04). ETA=7:19:05, max mem: 11.4 GB 
[10/29 14:20:05 visual_prompt]: 	Training 500/553. train loss: 0.5940,	0.5160 s / batch. (data: 7.96e-03). ETA=7:32:14, max mem: 11.4 GB 
[10/29 14:20:51 visual_prompt]: Epoch 5 / 100: avg data time: 3.92e-01, avg batch time: 0.8868, average train loss: 0.7482
[10/29 14:21:43 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1908, average loss: 0.6877
[10/29 14:21:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.58	
[10/29 14:21:43 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/29 14:23:16 visual_prompt]: 	Training 100/553. train loss: 0.5917,	0.5281 s / batch. (data: 3.55e-04). ETA=7:41:32, max mem: 11.4 GB 
[10/29 14:24:44 visual_prompt]: 	Training 200/553. train loss: 0.7763,	0.4905 s / batch. (data: 1.20e-02). ETA=7:07:47, max mem: 11.4 GB 
[10/29 14:26:11 visual_prompt]: 	Training 300/553. train loss: 0.5226,	0.5040 s / batch. (data: 2.70e-04). ETA=7:18:44, max mem: 11.4 GB 
[10/29 14:27:43 visual_prompt]: 	Training 400/553. train loss: 0.6181,	0.6465 s / batch. (data: 1.54e-01). ETA=9:21:45, max mem: 11.4 GB 
[10/29 14:29:10 visual_prompt]: 	Training 500/553. train loss: 0.7956,	1.2719 s / batch. (data: 7.87e-01). ETA=18:23:05, max mem: 11.4 GB 
[10/29 14:29:55 visual_prompt]: Epoch 6 / 100: avg data time: 3.94e-01, avg batch time: 0.8884, average train loss: 0.7444
[10/29 14:30:47 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1903, average loss: 0.6814
[10/29 14:30:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.88	
[10/29 14:30:47 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/29 14:32:17 visual_prompt]: 	Training 100/553. train loss: 0.4760,	0.4795 s / batch. (data: 2.71e-04). ETA=6:54:35, max mem: 11.4 GB 
[10/29 14:33:45 visual_prompt]: 	Training 200/553. train loss: 0.5196,	0.4864 s / batch. (data: 3.13e-04). ETA=6:59:45, max mem: 11.4 GB 
[10/29 14:35:18 visual_prompt]: 	Training 300/553. train loss: 0.9527,	2.3798 s / batch. (data: 1.90e+00). ETA=1 day, 10:09:52, max mem: 11.4 GB 
[10/29 14:36:46 visual_prompt]: 	Training 400/553. train loss: 0.6252,	2.6277 s / batch. (data: 2.12e+00). ETA=1 day, 13:39:00, max mem: 11.4 GB 
[10/29 14:38:12 visual_prompt]: 	Training 500/553. train loss: 0.7833,	1.1521 s / batch. (data: 6.61e-01). ETA=16:28:32, max mem: 11.4 GB 
[10/29 14:38:56 visual_prompt]: Epoch 7 / 100: avg data time: 3.91e-01, avg batch time: 0.8848, average train loss: 0.7473
[10/29 14:39:49 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1923, average loss: 0.7978
[10/29 14:39:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.18	
[10/29 14:39:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/29 14:41:18 visual_prompt]: 	Training 100/553. train loss: 0.6912,	1.0716 s / batch. (data: 5.50e-01). ETA=15:16:42, max mem: 11.4 GB 
[10/29 14:42:48 visual_prompt]: 	Training 200/553. train loss: 1.5044,	0.5034 s / batch. (data: 1.15e-02). ETA=7:09:50, max mem: 11.4 GB 
[10/29 14:44:18 visual_prompt]: 	Training 300/553. train loss: 0.6457,	0.5147 s / batch. (data: 7.43e-04). ETA=7:18:37, max mem: 11.4 GB 
[10/29 14:45:46 visual_prompt]: 	Training 400/553. train loss: 0.7207,	0.5800 s / batch. (data: 9.20e-02). ETA=8:13:16, max mem: 11.4 GB 
[10/29 14:47:16 visual_prompt]: 	Training 500/553. train loss: 0.8853,	2.0560 s / batch. (data: 1.58e+00). ETA=1 day, 5:05:07, max mem: 11.4 GB 
[10/29 14:48:00 visual_prompt]: Epoch 8 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 0.7617
[10/29 14:48:52 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1900, average loss: 0.7415
[10/29 14:48:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.29	
[10/29 14:48:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/29 14:50:24 visual_prompt]: 	Training 100/553. train loss: 0.5251,	0.5042 s / batch. (data: 1.04e-02). ETA=7:06:39, max mem: 11.4 GB 
[10/29 14:51:52 visual_prompt]: 	Training 200/553. train loss: 0.6225,	0.4840 s / batch. (data: 2.54e-04). ETA=6:48:47, max mem: 11.4 GB 
[10/29 14:53:21 visual_prompt]: 	Training 300/553. train loss: 0.5784,	2.4038 s / batch. (data: 1.91e+00). ETA=1 day, 9:46:15, max mem: 11.4 GB 
[10/29 14:54:50 visual_prompt]: 	Training 400/553. train loss: 0.5870,	0.4962 s / batch. (data: 2.88e-04). ETA=6:57:25, max mem: 11.4 GB 
[10/29 14:56:19 visual_prompt]: 	Training 500/553. train loss: 0.7073,	1.3757 s / batch. (data: 8.63e-01). ETA=19:15:03, max mem: 11.4 GB 
[10/29 14:57:03 visual_prompt]: Epoch 9 / 100: avg data time: 3.93e-01, avg batch time: 0.8872, average train loss: 0.7485
[10/29 14:57:55 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1923, average loss: 0.7106
[10/29 14:57:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 57.75	
[10/29 14:57:55 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/29 14:59:29 visual_prompt]: 	Training 100/553. train loss: 0.6413,	0.8236 s / batch. (data: 3.17e-01). ETA=11:29:25, max mem: 11.4 GB 
[10/29 15:00:56 visual_prompt]: 	Training 200/553. train loss: 0.6702,	0.5193 s / batch. (data: 5.40e-03). ETA=7:13:51, max mem: 11.4 GB 
[10/29 15:02:25 visual_prompt]: 	Training 300/553. train loss: 0.6412,	0.4915 s / batch. (data: 2.80e-04). ETA=6:49:46, max mem: 11.4 GB 
[10/29 15:03:52 visual_prompt]: 	Training 400/553. train loss: 0.8596,	1.0053 s / batch. (data: 5.10e-01). ETA=13:56:26, max mem: 11.4 GB 
[10/29 15:05:21 visual_prompt]: 	Training 500/553. train loss: 0.9527,	1.7084 s / batch. (data: 1.23e+00). ETA=23:38:37, max mem: 11.4 GB 
[10/29 15:06:07 visual_prompt]: Epoch 10 / 100: avg data time: 3.94e-01, avg batch time: 0.8882, average train loss: 0.7628
[10/29 15:06:59 visual_prompt]: Inference (val):avg data time: 3.48e-04, avg batch time: 0.1945, average loss: 1.0120
[10/29 15:06:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.97	
[10/29 15:06:59 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/29 15:08:32 visual_prompt]: 	Training 100/553. train loss: 0.6133,	0.5120 s / batch. (data: 2.68e-04). ETA=7:03:51, max mem: 11.4 GB 
[10/29 15:10:03 visual_prompt]: 	Training 200/553. train loss: 1.0401,	0.5039 s / batch. (data: 3.74e-04). ETA=6:56:16, max mem: 11.4 GB 
[10/29 15:11:31 visual_prompt]: 	Training 300/553. train loss: 0.6376,	1.9689 s / batch. (data: 1.49e+00). ETA=1 day, 3:03:20, max mem: 11.4 GB 
[10/29 15:12:58 visual_prompt]: 	Training 400/553. train loss: 0.7111,	0.4960 s / batch. (data: 2.89e-04). ETA=6:48:05, max mem: 11.4 GB 
[10/29 15:14:26 visual_prompt]: 	Training 500/553. train loss: 0.8120,	0.4999 s / batch. (data: 4.16e-04). ETA=6:50:31, max mem: 11.4 GB 
[10/29 15:15:10 visual_prompt]: Epoch 11 / 100: avg data time: 3.94e-01, avg batch time: 0.8881, average train loss: 0.7490
[10/29 15:16:03 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1902, average loss: 0.7031
[10/29 15:16:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 58.69	
[10/29 15:16:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/29 15:17:36 visual_prompt]: 	Training 100/553. train loss: 0.9650,	0.5158 s / batch. (data: 1.17e-02). ETA=7:02:14, max mem: 11.4 GB 
[10/29 15:19:06 visual_prompt]: 	Training 200/553. train loss: 0.6505,	1.4163 s / batch. (data: 9.24e-01). ETA=19:17:01, max mem: 11.4 GB 
[10/29 15:20:32 visual_prompt]: 	Training 300/553. train loss: 0.6930,	0.5080 s / batch. (data: 1.20e-02). ETA=6:54:09, max mem: 11.4 GB 
[10/29 15:22:02 visual_prompt]: 	Training 400/553. train loss: 0.7986,	0.5200 s / batch. (data: 3.00e-04). ETA=7:03:03, max mem: 11.4 GB 
[10/29 15:23:30 visual_prompt]: 	Training 500/553. train loss: 1.6872,	0.5177 s / batch. (data: 5.40e-03). ETA=7:00:18, max mem: 11.4 GB 
[10/29 15:24:14 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8893, average train loss: 0.7602
[10/29 15:25:07 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1928, average loss: 0.8197
[10/29 15:25:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.86	
[10/29 15:25:07 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/29 15:26:39 visual_prompt]: 	Training 100/553. train loss: 0.5583,	0.5040 s / batch. (data: 2.88e-04). ETA=6:47:55, max mem: 11.4 GB 
[10/29 15:28:07 visual_prompt]: 	Training 200/553. train loss: 0.7242,	0.4789 s / batch. (data: 2.55e-04). ETA=6:26:49, max mem: 11.4 GB 
[10/29 15:29:36 visual_prompt]: 	Training 300/553. train loss: 0.6661,	2.3435 s / batch. (data: 1.87e+00). ETA=1 day, 7:28:59, max mem: 11.4 GB 
[10/29 15:31:03 visual_prompt]: 	Training 400/553. train loss: 0.8730,	0.5206 s / batch. (data: 2.57e-02). ETA=6:58:47, max mem: 11.4 GB 
[10/29 15:32:33 visual_prompt]: 	Training 500/553. train loss: 0.6393,	0.4997 s / batch. (data: 2.62e-04). ETA=6:41:06, max mem: 11.4 GB 
[10/29 15:33:19 visual_prompt]: Epoch 13 / 100: avg data time: 3.94e-01, avg batch time: 0.8900, average train loss: 0.7507
[10/29 15:34:12 visual_prompt]: Inference (val):avg data time: 5.05e-04, avg batch time: 0.1905, average loss: 0.6755
[10/29 15:34:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 59.37	
[10/29 15:34:12 visual_prompt]: Best epoch 13: best metric: -0.675
[10/29 15:34:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/29 15:35:45 visual_prompt]: 	Training 100/553. train loss: 0.8464,	0.5117 s / batch. (data: 1.17e-02). ETA=6:49:27, max mem: 11.4 GB 
[10/29 15:37:13 visual_prompt]: 	Training 200/553. train loss: 0.5223,	1.1064 s / batch. (data: 6.18e-01). ETA=14:43:26, max mem: 11.4 GB 
[10/29 15:38:43 visual_prompt]: 	Training 300/553. train loss: 0.5806,	1.3856 s / batch. (data: 9.08e-01). ETA=18:24:08, max mem: 11.4 GB 
[10/29 15:40:11 visual_prompt]: 	Training 400/553. train loss: 0.6777,	0.4839 s / batch. (data: 2.88e-04). ETA=6:24:45, max mem: 11.4 GB 
[10/29 15:41:40 visual_prompt]: 	Training 500/553. train loss: 1.1164,	0.4960 s / batch. (data: 2.41e-04). ETA=6:33:36, max mem: 11.4 GB 
[10/29 15:42:25 visual_prompt]: Epoch 14 / 100: avg data time: 3.96e-01, avg batch time: 0.8910, average train loss: 0.7358
[10/29 15:43:17 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1919, average loss: 0.6821
[10/29 15:43:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.28	
[10/29 15:43:17 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/29 15:44:49 visual_prompt]: 	Training 100/553. train loss: 0.8832,	0.5122 s / batch. (data: 5.39e-03). ETA=6:45:08, max mem: 11.4 GB 
[10/29 15:46:16 visual_prompt]: 	Training 200/553. train loss: 0.5478,	0.4877 s / batch. (data: 2.83e-04). ETA=6:24:58, max mem: 11.4 GB 
[10/29 15:47:46 visual_prompt]: 	Training 300/553. train loss: 0.5222,	0.4885 s / batch. (data: 5.39e-03). ETA=6:24:43, max mem: 11.4 GB 
[10/29 15:49:13 visual_prompt]: 	Training 400/553. train loss: 0.4773,	0.4914 s / batch. (data: 2.63e-04). ETA=6:26:14, max mem: 11.4 GB 
[10/29 15:50:42 visual_prompt]: 	Training 500/553. train loss: 1.0181,	0.5037 s / batch. (data: 5.42e-03). ETA=6:35:04, max mem: 11.4 GB 
[10/29 15:51:29 visual_prompt]: Epoch 15 / 100: avg data time: 3.92e-01, avg batch time: 0.8882, average train loss: 0.7550
[10/29 15:52:21 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1918, average loss: 0.6921
[10/29 15:52:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 59.31	
[10/29 15:52:21 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/29 15:53:51 visual_prompt]: 	Training 100/553. train loss: 0.4366,	0.4846 s / batch. (data: 2.48e-04). ETA=6:18:50, max mem: 11.4 GB 
[10/29 15:55:20 visual_prompt]: 	Training 200/553. train loss: 0.8948,	0.5034 s / batch. (data: 2.59e-04). ETA=6:32:41, max mem: 11.4 GB 
[10/29 15:56:49 visual_prompt]: 	Training 300/553. train loss: 1.1888,	0.4883 s / batch. (data: 1.04e-02). ETA=6:20:08, max mem: 11.4 GB 
[10/29 15:58:17 visual_prompt]: 	Training 400/553. train loss: 0.7768,	0.5053 s / batch. (data: 1.64e-02). ETA=6:32:31, max mem: 11.4 GB 
[10/29 15:59:45 visual_prompt]: 	Training 500/553. train loss: 0.9474,	1.8345 s / batch. (data: 1.34e+00). ETA=23:41:54, max mem: 11.4 GB 
[10/29 16:00:31 visual_prompt]: Epoch 16 / 100: avg data time: 3.91e-01, avg batch time: 0.8862, average train loss: 0.7261
[10/29 16:01:24 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1916, average loss: 0.6739
[10/29 16:01:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 59.51	
[10/29 16:01:24 visual_prompt]: Best epoch 16: best metric: -0.674
[10/29 16:01:24 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/29 16:02:54 visual_prompt]: 	Training 100/553. train loss: 0.3817,	0.5000 s / batch. (data: 2.70e-04). ETA=6:26:16, max mem: 11.4 GB 
[10/29 16:04:25 visual_prompt]: 	Training 200/553. train loss: 0.7866,	0.4917 s / batch. (data: 2.17e-04). ETA=6:19:00, max mem: 11.4 GB 
[10/29 16:05:54 visual_prompt]: 	Training 300/553. train loss: 1.1298,	0.4918 s / batch. (data: 2.83e-04). ETA=6:18:16, max mem: 11.4 GB 
[10/29 16:07:22 visual_prompt]: 	Training 400/553. train loss: 0.5779,	1.0280 s / batch. (data: 5.30e-01). ETA=13:09:01, max mem: 11.4 GB 
[10/29 16:08:51 visual_prompt]: 	Training 500/553. train loss: 0.6534,	2.2280 s / batch. (data: 1.73e+00). ETA=1 day, 4:26:21, max mem: 11.4 GB 
[10/29 16:09:37 visual_prompt]: Epoch 17 / 100: avg data time: 3.98e-01, avg batch time: 0.8927, average train loss: 0.7234
[10/29 16:10:30 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1926, average loss: 0.7092
[10/29 16:10:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.31	
[10/29 16:10:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/29 16:12:03 visual_prompt]: 	Training 100/553. train loss: 0.8488,	0.4989 s / batch. (data: 2.06e-02). ETA=6:20:51, max mem: 11.4 GB 
[10/29 16:13:34 visual_prompt]: 	Training 200/553. train loss: 0.7360,	0.5120 s / batch. (data: 6.95e-04). ETA=6:29:57, max mem: 11.4 GB 
[10/29 16:15:03 visual_prompt]: 	Training 300/553. train loss: 0.5546,	0.4885 s / batch. (data: 1.05e-02). ETA=6:11:17, max mem: 11.4 GB 
[10/29 16:16:32 visual_prompt]: 	Training 400/553. train loss: 0.7532,	0.4961 s / batch. (data: 2.75e-04). ETA=6:16:10, max mem: 11.4 GB 
[10/29 16:18:00 visual_prompt]: 	Training 500/553. train loss: 0.6143,	0.5213 s / batch. (data: 5.50e-03). ETA=6:34:26, max mem: 11.4 GB 
[10/29 16:18:44 visual_prompt]: Epoch 18 / 100: avg data time: 3.98e-01, avg batch time: 0.8935, average train loss: 0.7192
[10/29 16:19:37 visual_prompt]: Inference (val):avg data time: 1.35e-04, avg batch time: 0.1928, average loss: 0.7340
[10/29 16:19:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 61.41	
[10/29 16:19:37 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[10/29 16:21:09 visual_prompt]: 	Training 100/553. train loss: 1.0990,	0.7601 s / batch. (data: 2.58e-01). ETA=9:33:10, max mem: 11.4 GB 
[10/29 16:22:39 visual_prompt]: 	Training 200/553. train loss: 0.8007,	0.4846 s / batch. (data: 4.08e-04). ETA=6:04:38, max mem: 11.4 GB 
[10/29 16:24:08 visual_prompt]: 	Training 300/553. train loss: 1.0028,	0.5073 s / batch. (data: 5.42e-03). ETA=6:20:49, max mem: 11.4 GB 
[10/29 16:25:38 visual_prompt]: 	Training 400/553. train loss: 0.5027,	0.5009 s / batch. (data: 6.96e-04). ETA=6:15:12, max mem: 11.4 GB 
[10/29 16:27:03 visual_prompt]: 	Training 500/553. train loss: 0.9197,	0.4880 s / batch. (data: 2.52e-04). ETA=6:04:43, max mem: 11.4 GB 
[10/29 16:27:49 visual_prompt]: Epoch 19 / 100: avg data time: 3.96e-01, avg batch time: 0.8901, average train loss: 0.7254
[10/29 16:28:42 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1915, average loss: 0.6774
[10/29 16:28:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 59.77	
[10/29 16:28:42 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[10/29 16:30:13 visual_prompt]: 	Training 100/553. train loss: 1.7657,	0.4943 s / batch. (data: 1.60e-02). ETA=6:08:10, max mem: 11.4 GB 
[10/29 16:31:42 visual_prompt]: 	Training 200/553. train loss: 0.4522,	0.6840 s / batch. (data: 2.04e-01). ETA=8:28:22, max mem: 11.4 GB 
[10/29 16:33:12 visual_prompt]: 	Training 300/553. train loss: 0.7156,	0.4965 s / batch. (data: 1.19e-02). ETA=6:08:12, max mem: 11.4 GB 
[10/29 16:34:41 visual_prompt]: 	Training 400/553. train loss: 0.5214,	0.5155 s / batch. (data: 1.15e-02). ETA=6:21:24, max mem: 11.4 GB 
[10/29 16:36:09 visual_prompt]: 	Training 500/553. train loss: 0.8209,	0.4925 s / batch. (data: 2.50e-04). ETA=6:03:35, max mem: 11.4 GB 
[10/29 16:36:56 visual_prompt]: Epoch 20 / 100: avg data time: 3.98e-01, avg batch time: 0.8932, average train loss: 0.7346
[10/29 16:37:49 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1917, average loss: 0.9466
[10/29 16:37:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.03	
[10/29 16:37:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[10/29 16:39:24 visual_prompt]: 	Training 100/553. train loss: 0.5971,	1.1472 s / batch. (data: 6.57e-01). ETA=14:03:56, max mem: 11.4 GB 
[10/29 16:40:52 visual_prompt]: 	Training 200/553. train loss: 0.6354,	0.5205 s / batch. (data: 5.36e-03). ETA=6:22:04, max mem: 11.4 GB 
[10/29 16:42:20 visual_prompt]: 	Training 300/553. train loss: 0.9963,	0.5120 s / batch. (data: 2.72e-04). ETA=6:14:56, max mem: 11.4 GB 
[10/29 16:43:49 visual_prompt]: 	Training 400/553. train loss: 0.5293,	0.4834 s / batch. (data: 5.40e-03). ETA=5:53:11, max mem: 11.4 GB 
[10/29 16:45:19 visual_prompt]: 	Training 500/553. train loss: 0.6890,	0.4892 s / batch. (data: 1.04e-02). ETA=5:56:38, max mem: 11.4 GB 
[10/29 16:46:04 visual_prompt]: Epoch 21 / 100: avg data time: 4.00e-01, avg batch time: 0.8946, average train loss: 0.7225
[10/29 16:46:56 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1900, average loss: 0.7899
[10/29 16:46:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.21	
[10/29 16:46:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[10/29 16:48:28 visual_prompt]: 	Training 100/553. train loss: 0.7013,	0.4969 s / batch. (data: 1.05e-02). ETA=6:00:58, max mem: 11.4 GB 
[10/29 16:49:57 visual_prompt]: 	Training 200/553. train loss: 0.5943,	0.5120 s / batch. (data: 2.74e-04). ETA=6:11:04, max mem: 11.4 GB 
[10/29 16:51:23 visual_prompt]: 	Training 300/553. train loss: 0.3982,	0.5076 s / batch. (data: 1.04e-02). ETA=6:07:03, max mem: 11.4 GB 
[10/29 16:52:51 visual_prompt]: 	Training 400/553. train loss: 0.6473,	0.5007 s / batch. (data: 2.68e-04). ETA=6:01:14, max mem: 11.4 GB 
[10/29 16:54:21 visual_prompt]: 	Training 500/553. train loss: 0.9031,	0.5200 s / batch. (data: 7.98e-03). ETA=6:14:17, max mem: 11.4 GB 
[10/29 16:55:07 visual_prompt]: Epoch 22 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 0.7544
[10/29 16:56:00 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1905, average loss: 0.8498
[10/29 16:56:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.77	
[10/29 16:56:00 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[10/29 16:57:32 visual_prompt]: 	Training 100/553. train loss: 0.7157,	0.4801 s / batch. (data: 2.75e-04). ETA=5:44:18, max mem: 11.4 GB 
[10/29 16:59:02 visual_prompt]: 	Training 200/553. train loss: 0.5313,	0.5026 s / batch. (data: 6.57e-03). ETA=5:59:38, max mem: 11.4 GB 
[10/29 17:00:32 visual_prompt]: 	Training 300/553. train loss: 1.4706,	0.4877 s / batch. (data: 2.67e-04). ETA=5:48:11, max mem: 11.4 GB 
[10/29 17:01:58 visual_prompt]: 	Training 400/553. train loss: 0.5713,	0.5000 s / batch. (data: 2.74e-04). ETA=5:56:07, max mem: 11.4 GB 
[10/29 17:03:25 visual_prompt]: 	Training 500/553. train loss: 0.8137,	0.5194 s / batch. (data: 1.05e-02). ETA=6:09:03, max mem: 11.4 GB 
[10/29 17:04:10 visual_prompt]: Epoch 23 / 100: avg data time: 3.92e-01, avg batch time: 0.8864, average train loss: 0.7194
[10/29 17:05:03 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1917, average loss: 0.6970
[10/29 17:05:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 61.78	
[10/29 17:05:03 visual_prompt]: Stopping early.
[10/29 17:05:03 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 17:05:03 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 17:05:03 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 17:05:03 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 17:05:03 visual_prompt]: Training with config:
[10/29 17:05:03 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.05_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 17:05:03 visual_prompt]: Loading training data...
[10/29 17:05:03 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 17:05:03 visual_prompt]: Loading validation data...
[10/29 17:05:03 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 17:05:03 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/29 17:05:05 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/29 17:05:05 visual_prompt]: tuned percent:0.529
[10/29 17:05:05 visual_prompt]: Device used for model: 0
[10/29 17:05:05 visual_prompt]: Setting up Evaluator...
[10/29 17:05:05 visual_prompt]: Setting up Trainer...
[10/29 17:05:05 visual_prompt]: 	Setting up the optimizer...
[10/29 17:05:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 17:06:37 visual_prompt]: 	Training 100/553. train loss: 1.8535,	0.5081 s / batch. (data: 2.67e-04). ETA=7:47:25, max mem: 11.4 GB 
[10/29 17:08:04 visual_prompt]: 	Training 200/553. train loss: 1.4692,	0.5160 s / batch. (data: 2.67e-04). ETA=7:53:51, max mem: 11.4 GB 
[10/29 17:09:35 visual_prompt]: 	Training 300/553. train loss: 1.5173,	2.7074 s / batch. (data: 2.21e+00). ETA=1 day, 17:21:46, max mem: 11.4 GB 
[10/29 17:11:02 visual_prompt]: 	Training 400/553. train loss: 0.0673,	0.5322 s / batch. (data: 1.63e-02). ETA=8:06:56, max mem: 11.4 GB 
[10/29 17:12:32 visual_prompt]: 	Training 500/553. train loss: 0.8113,	0.5160 s / batch. (data: 7.41e-04). ETA=7:51:14, max mem: 11.4 GB 
[10/29 17:13:18 visual_prompt]: Epoch 1 / 100: avg data time: 3.96e-01, avg batch time: 0.8909, average train loss: 1.3966
[10/29 17:14:10 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.1921, average loss: 1.3454
[10/29 17:14:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/29 17:14:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/29 17:15:41 visual_prompt]: 	Training 100/553. train loss: 0.6973,	0.5084 s / batch. (data: 2.01e-02). ETA=7:42:59, max mem: 11.4 GB 
[10/29 17:17:10 visual_prompt]: 	Training 200/553. train loss: 0.3711,	1.4760 s / batch. (data: 9.75e-01). ETA=22:21:51, max mem: 11.4 GB 
[10/29 17:18:39 visual_prompt]: 	Training 300/553. train loss: 0.6395,	1.6841 s / batch. (data: 1.18e+00). ETA=1 day, 1:28:12, max mem: 11.4 GB 
[10/29 17:20:06 visual_prompt]: 	Training 400/553. train loss: 0.8023,	0.4965 s / batch. (data: 2.64e-04). ETA=7:29:44, max mem: 11.4 GB 
[10/29 17:21:36 visual_prompt]: 	Training 500/553. train loss: 0.6533,	0.5109 s / batch. (data: 6.86e-03). ETA=7:41:54, max mem: 11.4 GB 
[10/29 17:22:21 visual_prompt]: Epoch 2 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 0.7504
[10/29 17:23:14 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1916, average loss: 0.7279
[10/29 17:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.67	
[10/29 17:23:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/29 17:24:44 visual_prompt]: 	Training 100/553. train loss: 0.7585,	0.5000 s / batch. (data: 2.72e-04). ETA=7:30:47, max mem: 11.4 GB 
[10/29 17:26:15 visual_prompt]: 	Training 200/553. train loss: 0.7990,	0.5000 s / batch. (data: 7.96e-03). ETA=7:29:57, max mem: 11.4 GB 
[10/29 17:27:42 visual_prompt]: 	Training 300/553. train loss: 0.5372,	0.5160 s / batch. (data: 7.98e-03). ETA=7:43:29, max mem: 11.4 GB 
[10/29 17:29:13 visual_prompt]: 	Training 400/553. train loss: 0.6304,	0.5040 s / batch. (data: 7.97e-03). ETA=7:31:52, max mem: 11.4 GB 
[10/29 17:30:43 visual_prompt]: 	Training 500/553. train loss: 0.7210,	1.7826 s / batch. (data: 1.29e+00). ETA=1 day, 2:35:13, max mem: 11.4 GB 
[10/29 17:31:28 visual_prompt]: Epoch 3 / 100: avg data time: 3.98e-01, avg batch time: 0.8933, average train loss: 0.7418
[10/29 17:32:20 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1900, average loss: 0.7316
[10/29 17:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.88	
[10/29 17:32:20 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/29 17:33:53 visual_prompt]: 	Training 100/553. train loss: 0.7507,	0.4840 s / batch. (data: 3.00e-04). ETA=7:11:53, max mem: 11.4 GB 
[10/29 17:35:23 visual_prompt]: 	Training 200/553. train loss: 0.5646,	0.5098 s / batch. (data: 5.39e-03). ETA=7:34:06, max mem: 11.4 GB 
[10/29 17:36:52 visual_prompt]: 	Training 300/553. train loss: 0.6099,	1.3516 s / batch. (data: 8.52e-01). ETA=20:01:36, max mem: 11.4 GB 
[10/29 17:38:18 visual_prompt]: 	Training 400/553. train loss: 0.7537,	1.8960 s / batch. (data: 1.39e+00). ETA=1 day, 4:02:26, max mem: 11.4 GB 
[10/29 17:39:49 visual_prompt]: 	Training 500/553. train loss: 0.4893,	3.5772 s / batch. (data: 3.09e+00). ETA=2 days, 4:48:14, max mem: 11.4 GB 
[10/29 17:40:34 visual_prompt]: Epoch 4 / 100: avg data time: 3.99e-01, avg batch time: 0.8935, average train loss: 0.7454
[10/29 17:41:27 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1930, average loss: 0.6834
[10/29 17:41:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.30	
[10/29 17:41:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/29 17:42:58 visual_prompt]: 	Training 100/553. train loss: 0.4784,	0.4958 s / batch. (data: 2.97e-04). ETA=7:17:53, max mem: 11.4 GB 
[10/29 17:44:27 visual_prompt]: 	Training 200/553. train loss: 0.6608,	1.6796 s / batch. (data: 1.20e+00). ETA=1 day, 0:40:33, max mem: 11.4 GB 
[10/29 17:45:57 visual_prompt]: 	Training 300/553. train loss: 0.8831,	0.4962 s / batch. (data: 2.83e-04). ETA=7:16:31, max mem: 11.4 GB 
[10/29 17:47:25 visual_prompt]: 	Training 400/553. train loss: 0.5526,	0.4885 s / batch. (data: 2.72e-04). ETA=7:08:59, max mem: 11.4 GB 
[10/29 17:48:55 visual_prompt]: 	Training 500/553. train loss: 0.5960,	0.4916 s / batch. (data: 2.52e-04). ETA=7:10:54, max mem: 11.4 GB 
[10/29 17:49:41 visual_prompt]: Epoch 5 / 100: avg data time: 3.99e-01, avg batch time: 0.8938, average train loss: 0.7490
[10/29 17:50:34 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.1900, average loss: 0.6883
[10/29 17:50:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.62	
[10/29 17:50:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/29 17:52:08 visual_prompt]: 	Training 100/553. train loss: 0.5908,	0.5160 s / batch. (data: 7.19e-04). ETA=7:30:55, max mem: 11.4 GB 
[10/29 17:53:36 visual_prompt]: 	Training 200/553. train loss: 0.7837,	0.4789 s / batch. (data: 2.59e-04). ETA=6:57:41, max mem: 11.4 GB 
[10/29 17:55:04 visual_prompt]: 	Training 300/553. train loss: 0.5204,	0.4791 s / batch. (data: 2.47e-04). ETA=6:57:06, max mem: 11.4 GB 
[10/29 17:56:36 visual_prompt]: 	Training 400/553. train loss: 0.6148,	0.6280 s / batch. (data: 1.45e-01). ETA=9:05:41, max mem: 11.4 GB 
[10/29 17:58:04 visual_prompt]: 	Training 500/553. train loss: 0.7930,	1.4961 s / batch. (data: 1.01e+00). ETA=21:37:27, max mem: 11.4 GB 
[10/29 17:58:49 visual_prompt]: Epoch 6 / 100: avg data time: 4.00e-01, avg batch time: 0.8953, average train loss: 0.7448
[10/29 17:59:42 visual_prompt]: Inference (val):avg data time: 3.62e-04, avg batch time: 0.1900, average loss: 0.6804
[10/29 17:59:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.91	
[10/29 17:59:42 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/29 18:01:12 visual_prompt]: 	Training 100/553. train loss: 0.4762,	0.5170 s / batch. (data: 1.05e-02). ETA=7:27:02, max mem: 11.4 GB 
[10/29 18:02:42 visual_prompt]: 	Training 200/553. train loss: 0.5204,	0.5205 s / batch. (data: 2.06e-02). ETA=7:29:10, max mem: 11.4 GB 
[10/29 18:04:13 visual_prompt]: 	Training 300/553. train loss: 0.9491,	1.3480 s / batch. (data: 8.65e-01). ETA=19:21:06, max mem: 11.4 GB 
[10/29 18:05:43 visual_prompt]: 	Training 400/553. train loss: 0.6214,	2.3234 s / batch. (data: 1.84e+00). ETA=1 day, 9:17:24, max mem: 11.4 GB 
[10/29 18:07:10 visual_prompt]: 	Training 500/553. train loss: 0.7854,	0.5080 s / batch. (data: 2.45e-04). ETA=7:15:53, max mem: 11.4 GB 
[10/29 18:07:55 visual_prompt]: Epoch 7 / 100: avg data time: 3.97e-01, avg batch time: 0.8917, average train loss: 0.7489
[10/29 18:08:48 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1913, average loss: 0.8008
[10/29 18:08:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.15	
[10/29 18:08:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/29 18:10:17 visual_prompt]: 	Training 100/553. train loss: 0.6869,	0.5080 s / batch. (data: 2.58e-04). ETA=7:14:34, max mem: 11.4 GB 
[10/29 18:11:48 visual_prompt]: 	Training 200/553. train loss: 1.5166,	0.4920 s / batch. (data: 2.69e-04). ETA=7:00:03, max mem: 11.4 GB 
[10/29 18:13:18 visual_prompt]: 	Training 300/553. train loss: 0.6472,	0.5359 s / batch. (data: 1.20e-02). ETA=7:36:42, max mem: 11.4 GB 
[10/29 18:14:48 visual_prompt]: 	Training 400/553. train loss: 0.7226,	0.9558 s / batch. (data: 4.66e-01). ETA=13:32:52, max mem: 11.4 GB 
[10/29 18:16:17 visual_prompt]: 	Training 500/553. train loss: 0.8986,	2.0007 s / batch. (data: 1.50e+00). ETA=1 day, 4:18:12, max mem: 11.4 GB 
[10/29 18:17:02 visual_prompt]: Epoch 8 / 100: avg data time: 3.99e-01, avg batch time: 0.8940, average train loss: 0.7625
[10/29 18:17:55 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1897, average loss: 0.7339
[10/29 18:17:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.48	
[10/29 18:17:55 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/29 18:19:27 visual_prompt]: 	Training 100/553. train loss: 0.5168,	0.4960 s / batch. (data: 2.90e-04). ETA=6:59:45, max mem: 11.4 GB 
[10/29 18:20:56 visual_prompt]: 	Training 200/553. train loss: 0.6018,	0.5084 s / batch. (data: 1.55e-02). ETA=7:09:25, max mem: 11.4 GB 
[10/29 18:22:25 visual_prompt]: 	Training 300/553. train loss: 0.5708,	2.2009 s / batch. (data: 1.71e+00). ETA=1 day, 6:55:12, max mem: 11.4 GB 
[10/29 18:23:55 visual_prompt]: 	Training 400/553. train loss: 0.5822,	0.5200 s / batch. (data: 2.88e-04). ETA=7:17:27, max mem: 11.4 GB 
[10/29 18:25:25 visual_prompt]: 	Training 500/553. train loss: 0.7203,	1.2635 s / batch. (data: 7.86e-01). ETA=17:40:50, max mem: 11.4 GB 
[10/29 18:26:09 visual_prompt]: Epoch 9 / 100: avg data time: 3.98e-01, avg batch time: 0.8940, average train loss: 0.7474
[10/29 18:27:02 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1906, average loss: 0.7197
[10/29 18:27:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 57.56	
[10/29 18:27:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/29 18:28:37 visual_prompt]: 	Training 100/553. train loss: 0.6344,	0.5075 s / batch. (data: 8.20e-04). ETA=7:04:48, max mem: 11.4 GB 
[10/29 18:30:04 visual_prompt]: 	Training 200/553. train loss: 0.6556,	0.5128 s / batch. (data: 5.37e-03). ETA=7:08:22, max mem: 11.4 GB 
[10/29 18:31:32 visual_prompt]: 	Training 300/553. train loss: 0.6252,	0.6309 s / batch. (data: 1.43e-01). ETA=8:45:58, max mem: 11.4 GB 
[10/29 18:32:58 visual_prompt]: 	Training 400/553. train loss: 0.8571,	1.3474 s / batch. (data: 8.70e-01). ETA=18:41:08, max mem: 11.4 GB 
[10/29 18:34:28 visual_prompt]: 	Training 500/553. train loss: 0.9385,	1.4840 s / batch. (data: 9.84e-01). ETA=20:32:16, max mem: 11.4 GB 
[10/29 18:35:13 visual_prompt]: Epoch 10 / 100: avg data time: 3.95e-01, avg batch time: 0.8887, average train loss: 0.7649
[10/29 18:36:06 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1912, average loss: 1.0245
[10/29 18:36:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.05	
[10/29 18:36:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/29 18:37:39 visual_prompt]: 	Training 100/553. train loss: 0.6101,	0.5203 s / batch. (data: 2.63e-04). ETA=7:10:44, max mem: 11.4 GB 
[10/29 18:39:09 visual_prompt]: 	Training 200/553. train loss: 1.0460,	0.5240 s / batch. (data: 1.20e-02). ETA=7:12:54, max mem: 11.4 GB 
[10/29 18:40:37 visual_prompt]: 	Training 300/553. train loss: 0.6654,	1.6779 s / batch. (data: 1.18e+00). ETA=23:03:26, max mem: 11.4 GB 
[10/29 18:42:05 visual_prompt]: 	Training 400/553. train loss: 0.7144,	0.5040 s / batch. (data: 7.23e-04). ETA=6:54:42, max mem: 11.4 GB 
[10/29 18:43:32 visual_prompt]: 	Training 500/553. train loss: 0.7131,	0.5240 s / batch. (data: 7.96e-03). ETA=7:10:19, max mem: 11.4 GB 
[10/29 18:44:16 visual_prompt]: Epoch 11 / 100: avg data time: 3.92e-01, avg batch time: 0.8875, average train loss: 0.7594
[10/29 18:45:09 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1905, average loss: 0.7127
[10/29 18:45:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 57.78	
[10/29 18:45:09 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/29 18:46:42 visual_prompt]: 	Training 100/553. train loss: 0.9071,	0.4918 s / batch. (data: 2.71e-04). ETA=6:42:36, max mem: 11.4 GB 
[10/29 18:48:12 visual_prompt]: 	Training 200/553. train loss: 0.6629,	2.1840 s / batch. (data: 1.69e+00). ETA=1 day, 5:44:12, max mem: 11.4 GB 
[10/29 18:49:38 visual_prompt]: 	Training 300/553. train loss: 0.6947,	0.5040 s / batch. (data: 1.19e-02). ETA=6:50:52, max mem: 11.4 GB 
[10/29 18:51:07 visual_prompt]: 	Training 400/553. train loss: 0.8417,	0.4796 s / batch. (data: 2.55e-04). ETA=6:30:11, max mem: 11.4 GB 
[10/29 18:52:36 visual_prompt]: 	Training 500/553. train loss: 1.6918,	0.4938 s / batch. (data: 2.74e-04). ETA=6:40:57, max mem: 11.4 GB 
[10/29 18:53:20 visual_prompt]: Epoch 12 / 100: avg data time: 3.93e-01, avg batch time: 0.8885, average train loss: 0.7617
[10/29 18:54:12 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1914, average loss: 0.8258
[10/29 18:54:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.64	
[10/29 18:54:12 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/29 18:55:45 visual_prompt]: 	Training 100/553. train loss: 0.5661,	0.4920 s / batch. (data: 2.70e-04). ETA=6:38:14, max mem: 11.4 GB 
[10/29 18:57:11 visual_prompt]: 	Training 200/553. train loss: 0.7079,	0.5160 s / batch. (data: 2.74e-04). ETA=6:56:48, max mem: 11.4 GB 
[10/29 18:58:41 visual_prompt]: 	Training 300/553. train loss: 0.6650,	2.3065 s / batch. (data: 1.83e+00). ETA=1 day, 6:59:10, max mem: 11.4 GB 
[10/29 19:00:07 visual_prompt]: 	Training 400/553. train loss: 0.8445,	0.5005 s / batch. (data: 2.20e-02). ETA=6:42:33, max mem: 11.4 GB 
[10/29 19:01:37 visual_prompt]: 	Training 500/553. train loss: 0.6268,	0.5040 s / batch. (data: 5.40e-03). ETA=6:44:32, max mem: 11.4 GB 
[10/29 19:02:23 visual_prompt]: Epoch 13 / 100: avg data time: 3.91e-01, avg batch time: 0.8861, average train loss: 0.7525
[10/29 19:03:15 visual_prompt]: Inference (val):avg data time: 4.48e-04, avg batch time: 0.1907, average loss: 0.6742
[10/29 19:03:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.68	
[10/29 19:03:15 visual_prompt]: Best epoch 13: best metric: -0.674
[10/29 19:03:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/29 19:04:47 visual_prompt]: 	Training 100/553. train loss: 0.8751,	0.4994 s / batch. (data: 2.86e-04). ETA=6:39:35, max mem: 11.4 GB 
[10/29 19:06:17 visual_prompt]: 	Training 200/553. train loss: 0.5081,	2.0716 s / batch. (data: 1.58e+00). ETA=1 day, 3:34:11, max mem: 11.4 GB 
[10/29 19:07:46 visual_prompt]: 	Training 300/553. train loss: 0.5721,	1.3556 s / batch. (data: 8.65e-01). ETA=18:00:12, max mem: 11.4 GB 
[10/29 19:09:13 visual_prompt]: 	Training 400/553. train loss: 0.7109,	0.4960 s / batch. (data: 2.83e-04). ETA=6:34:23, max mem: 11.4 GB 
[10/29 19:10:41 visual_prompt]: 	Training 500/553. train loss: 1.1347,	0.5167 s / batch. (data: 2.06e-02). ETA=6:50:00, max mem: 11.4 GB 
[10/29 19:11:26 visual_prompt]: Epoch 14 / 100: avg data time: 3.93e-01, avg batch time: 0.8875, average train loss: 0.7377
[10/29 19:12:18 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1921, average loss: 0.6789
[10/29 19:12:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.44	
[10/29 19:12:18 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/29 19:13:50 visual_prompt]: 	Training 100/553. train loss: 0.9324,	0.5146 s / batch. (data: 5.44e-03). ETA=6:47:00, max mem: 11.4 GB 
[10/29 19:15:17 visual_prompt]: 	Training 200/553. train loss: 0.5592,	0.5086 s / batch. (data: 1.05e-02). ETA=6:41:27, max mem: 11.4 GB 
[10/29 19:16:47 visual_prompt]: 	Training 300/553. train loss: 0.5017,	0.5148 s / batch. (data: 1.55e-02). ETA=6:45:29, max mem: 11.4 GB 
[10/29 19:18:14 visual_prompt]: 	Training 400/553. train loss: 0.4838,	0.5076 s / batch. (data: 1.16e-02). ETA=6:38:58, max mem: 11.4 GB 
[10/29 19:19:44 visual_prompt]: 	Training 500/553. train loss: 1.0179,	0.4912 s / batch. (data: 2.55e-04). ETA=6:25:14, max mem: 11.4 GB 
[10/29 19:20:30 visual_prompt]: Epoch 15 / 100: avg data time: 3.95e-01, avg batch time: 0.8896, average train loss: 0.7574
[10/29 19:21:22 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1926, average loss: 0.6941
[10/29 19:21:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 60.17	
[10/29 19:21:22 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/29 19:22:53 visual_prompt]: 	Training 100/553. train loss: 0.4395,	0.5004 s / batch. (data: 2.55e-04). ETA=6:31:11, max mem: 11.4 GB 
[10/29 19:24:22 visual_prompt]: 	Training 200/553. train loss: 0.9364,	0.4916 s / batch. (data: 1.16e-02). ETA=6:23:29, max mem: 11.4 GB 
[10/29 19:25:51 visual_prompt]: 	Training 300/553. train loss: 1.1798,	0.4994 s / batch. (data: 4.03e-04). ETA=6:28:42, max mem: 11.4 GB 
[10/29 19:27:19 visual_prompt]: 	Training 400/553. train loss: 0.7832,	0.4806 s / batch. (data: 2.73e-04). ETA=6:13:16, max mem: 11.4 GB 
[10/29 19:28:47 visual_prompt]: 	Training 500/553. train loss: 0.9466,	1.9538 s / batch. (data: 1.48e+00). ETA=1 day, 1:14:22, max mem: 11.4 GB 
[10/29 19:29:33 visual_prompt]: Epoch 16 / 100: avg data time: 3.92e-01, avg batch time: 0.8866, average train loss: 0.7296
[10/29 19:30:25 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1911, average loss: 0.6730
[10/29 19:30:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.64	
[10/29 19:30:25 visual_prompt]: Best epoch 16: best metric: -0.673
[10/29 19:30:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/29 19:31:56 visual_prompt]: 	Training 100/553. train loss: 0.3578,	0.5045 s / batch. (data: 1.04e-02). ETA=6:29:43, max mem: 11.4 GB 
[10/29 19:33:26 visual_prompt]: 	Training 200/553. train loss: 0.8185,	0.5082 s / batch. (data: 1.64e-02). ETA=6:31:43, max mem: 11.4 GB 
[10/29 19:34:55 visual_prompt]: 	Training 300/553. train loss: 1.1858,	0.4787 s / batch. (data: 2.64e-04). ETA=6:08:11, max mem: 11.4 GB 
[10/29 19:36:22 visual_prompt]: 	Training 400/553. train loss: 0.6785,	0.9961 s / batch. (data: 4.83e-01). ETA=12:44:30, max mem: 11.4 GB 
[10/29 19:37:51 visual_prompt]: 	Training 500/553. train loss: 0.6311,	2.3160 s / batch. (data: 1.81e+00). ETA=1 day, 5:33:43, max mem: 11.4 GB 
[10/29 19:38:37 visual_prompt]: Epoch 17 / 100: avg data time: 3.93e-01, avg batch time: 0.8883, average train loss: 0.7262
[10/29 19:39:29 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1903, average loss: 0.7150
[10/29 19:39:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 61.20	
[10/29 19:39:29 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/29 19:41:01 visual_prompt]: 	Training 100/553. train loss: 0.8355,	0.5061 s / batch. (data: 1.04e-02). ETA=6:26:20, max mem: 11.4 GB 
[10/29 19:42:32 visual_prompt]: 	Training 200/553. train loss: 0.7742,	0.4987 s / batch. (data: 2.40e-04). ETA=6:19:48, max mem: 11.4 GB 
[10/29 19:44:00 visual_prompt]: 	Training 300/553. train loss: 0.5581,	0.4781 s / batch. (data: 2.74e-04). ETA=6:03:19, max mem: 11.4 GB 
[10/29 19:45:29 visual_prompt]: 	Training 400/553. train loss: 0.7694,	0.4800 s / batch. (data: 2.68e-04). ETA=6:03:58, max mem: 11.4 GB 
[10/29 19:46:56 visual_prompt]: 	Training 500/553. train loss: 0.5825,	0.4960 s / batch. (data: 2.64e-04). ETA=6:15:19, max mem: 11.4 GB 
[10/29 19:47:40 visual_prompt]: Epoch 18 / 100: avg data time: 3.94e-01, avg batch time: 0.8887, average train loss: 0.7230
[10/29 19:48:32 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1930, average loss: 0.7275
[10/29 19:48:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 61.45	
[10/29 19:48:32 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[10/29 19:50:04 visual_prompt]: 	Training 100/553. train loss: 1.0938,	0.4782 s / batch. (data: 2.51e-04). ETA=6:00:36, max mem: 11.4 GB 
[10/29 19:51:33 visual_prompt]: 	Training 200/553. train loss: 0.8200,	0.4990 s / batch. (data: 5.46e-03). ETA=6:15:27, max mem: 11.4 GB 
[10/29 19:53:01 visual_prompt]: 	Training 300/553. train loss: 1.0252,	0.6862 s / batch. (data: 1.97e-01). ETA=8:35:09, max mem: 11.4 GB 
[10/29 19:54:30 visual_prompt]: 	Training 400/553. train loss: 0.5087,	0.5221 s / batch. (data: 5.37e-03). ETA=6:31:07, max mem: 11.4 GB 
[10/29 19:55:56 visual_prompt]: 	Training 500/553. train loss: 0.8944,	0.5120 s / batch. (data: 2.73e-04). ETA=6:22:40, max mem: 11.4 GB 
[10/29 19:56:42 visual_prompt]: Epoch 19 / 100: avg data time: 3.89e-01, avg batch time: 0.8848, average train loss: 0.7297
[10/29 19:57:34 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1908, average loss: 0.6776
[10/29 19:57:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.04	
[10/29 19:57:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[10/29 19:59:03 visual_prompt]: 	Training 100/553. train loss: 1.8207,	0.4918 s / batch. (data: 2.40e-04). ETA=6:06:21, max mem: 11.4 GB 
[10/29 20:00:34 visual_prompt]: 	Training 200/553. train loss: 0.4389,	0.4935 s / batch. (data: 8.96e-03). ETA=6:06:46, max mem: 11.4 GB 
[10/29 20:02:03 visual_prompt]: 	Training 300/553. train loss: 0.6755,	0.4883 s / batch. (data: 5.40e-03). ETA=6:02:05, max mem: 11.4 GB 
[10/29 20:03:31 visual_prompt]: 	Training 400/553. train loss: 0.5106,	0.4850 s / batch. (data: 5.43e-03). ETA=5:58:49, max mem: 11.4 GB 
[10/29 20:04:59 visual_prompt]: 	Training 500/553. train loss: 0.8348,	0.5038 s / batch. (data: 2.63e-04). ETA=6:11:55, max mem: 11.4 GB 
[10/29 20:05:46 visual_prompt]: Epoch 20 / 100: avg data time: 3.93e-01, avg batch time: 0.8893, average train loss: 0.7384
[10/29 20:06:38 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1920, average loss: 0.9986
[10/29 20:06:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.87	
[10/29 20:06:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[10/29 20:08:12 visual_prompt]: 	Training 100/553. train loss: 0.6040,	0.8960 s / batch. (data: 3.89e-01). ETA=10:59:10, max mem: 11.4 GB 
[10/29 20:09:39 visual_prompt]: 	Training 200/553. train loss: 0.6280,	0.5118 s / batch. (data: 1.05e-02). ETA=6:15:38, max mem: 11.4 GB 
[10/29 20:11:09 visual_prompt]: 	Training 300/553. train loss: 0.9933,	1.7471 s / batch. (data: 1.22e+00). ETA=21:19:26, max mem: 11.4 GB 
[10/29 20:12:35 visual_prompt]: 	Training 400/553. train loss: 0.4639,	0.5124 s / batch. (data: 2.04e-02). ETA=6:14:24, max mem: 11.4 GB 
[10/29 20:14:05 visual_prompt]: 	Training 500/553. train loss: 0.7196,	0.5160 s / batch. (data: 2.76e-04). ETA=6:16:08, max mem: 11.4 GB 
[10/29 20:14:49 visual_prompt]: Epoch 21 / 100: avg data time: 3.92e-01, avg batch time: 0.8875, average train loss: 0.7217
[10/29 20:15:41 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1912, average loss: 0.8067
[10/29 20:15:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 60.68	
[10/29 20:15:41 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[10/29 20:17:12 visual_prompt]: 	Training 100/553. train loss: 0.6685,	0.4957 s / batch. (data: 2.64e-04). ETA=6:00:07, max mem: 11.4 GB 
[10/29 20:18:41 visual_prompt]: 	Training 200/553. train loss: 0.5677,	0.4966 s / batch. (data: 1.04e-02). ETA=5:59:57, max mem: 11.4 GB 
[10/29 20:20:07 visual_prompt]: 	Training 300/553. train loss: 0.4189,	0.7360 s / batch. (data: 2.43e-01). ETA=8:52:11, max mem: 11.4 GB 
[10/29 20:21:36 visual_prompt]: 	Training 400/553. train loss: 0.6123,	0.4920 s / batch. (data: 2.66e-04). ETA=5:54:57, max mem: 11.4 GB 
[10/29 20:23:05 visual_prompt]: 	Training 500/553. train loss: 0.8729,	0.4807 s / batch. (data: 2.51e-04). ETA=5:46:00, max mem: 11.4 GB 
[10/29 20:23:52 visual_prompt]: Epoch 22 / 100: avg data time: 3.93e-01, avg batch time: 0.8873, average train loss: 0.7489
[10/29 20:24:44 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1904, average loss: 0.8530
[10/29 20:24:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.55	
[10/29 20:24:44 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[10/29 20:26:17 visual_prompt]: 	Training 100/553. train loss: 0.7933,	0.4844 s / batch. (data: 2.62e-04). ETA=5:47:27, max mem: 11.4 GB 
[10/29 20:27:46 visual_prompt]: 	Training 200/553. train loss: 0.5425,	1.1400 s / batch. (data: 6.62e-01). ETA=13:35:44, max mem: 11.4 GB 
[10/29 20:29:17 visual_prompt]: 	Training 300/553. train loss: 1.4967,	0.4924 s / batch. (data: 2.69e-04). ETA=5:51:33, max mem: 11.4 GB 
[10/29 20:30:44 visual_prompt]: 	Training 400/553. train loss: 0.5781,	0.4959 s / batch. (data: 1.14e-03). ETA=5:53:12, max mem: 11.4 GB 
[10/29 20:32:09 visual_prompt]: 	Training 500/553. train loss: 0.7422,	0.4794 s / batch. (data: 3.18e-04). ETA=5:40:40, max mem: 11.4 GB 
[10/29 20:32:56 visual_prompt]: Epoch 23 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 0.7210
[10/29 20:33:48 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1909, average loss: 0.7154
[10/29 20:33:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 61.72	
[10/29 20:33:48 visual_prompt]: Stopping early.
[10/29 20:33:49 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 20:33:49 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 20:33:49 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 20:33:49 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 20:33:49 visual_prompt]: Training with config:
[10/29 20:33:49 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/test/seed9805/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 9805, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 20:33:49 visual_prompt]: Loading training data...
[10/29 20:33:49 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 20:33:49 visual_prompt]: Loading validation data...
[10/29 20:33:49 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 20:33:49 visual_prompt]: Loading test data...
[10/29 20:33:49 visual_prompt]: Constructing mammo-cbis dataset test...
[10/29 20:33:49 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/29 20:33:51 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/29 20:33:51 visual_prompt]: tuned percent:0.529
[10/29 20:33:51 visual_prompt]: Device used for model: 0
[10/29 20:33:51 visual_prompt]: Setting up Evaluator...
[10/29 20:33:51 visual_prompt]: Setting up Trainer...
[10/29 20:33:51 visual_prompt]: 	Setting up the optimizer...
[10/29 20:33:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 20:35:23 visual_prompt]: 	Training 100/553. train loss: 1.0889,	1.6171 s / batch. (data: 1.10e+00). ETA=1 day, 0:47:41, max mem: 11.4 GB 
[10/29 20:36:53 visual_prompt]: 	Training 200/553. train loss: 1.0954,	2.3153 s / batch. (data: 1.82e+00). ETA=1 day, 11:26:13, max mem: 11.4 GB 
[10/29 20:38:22 visual_prompt]: 	Training 300/553. train loss: 1.4634,	1.6197 s / batch. (data: 1.13e+00). ETA=1 day, 0:44:42, max mem: 11.4 GB 
[10/29 20:39:50 visual_prompt]: 	Training 400/553. train loss: 0.7579,	0.4844 s / batch. (data: 5.36e-03). ETA=7:23:15, max mem: 11.4 GB 
[10/29 20:41:20 visual_prompt]: 	Training 500/553. train loss: 0.6822,	1.9960 s / batch. (data: 1.49e+00). ETA=1 day, 6:22:59, max mem: 11.4 GB 
[10/29 20:42:05 visual_prompt]: Epoch 1 / 100: avg data time: 3.97e-01, avg batch time: 0.8928, average train loss: 0.9696
[10/29 20:42:57 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1916, average loss: 0.9997
[10/29 20:42:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 45.45	
[10/29 20:44:36 visual_prompt]: 	Test 100/162. loss: 1.128, 0.1851 s / batch. (data: 3.12e-05)max mem: 11.41573 GB 
[10/29 20:45:35 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.1920, average loss: 1.0220
[10/29 20:45:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.55	rocauc: 47.56	
[10/29 20:45:35 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/29 20:47:08 visual_prompt]: 	Training 100/553. train loss: 0.6112,	2.5619 s / batch. (data: 2.06e+00). ETA=1 day, 14:53:22, max mem: 11.4 GB 
[10/29 20:48:36 visual_prompt]: 	Training 200/553. train loss: 0.6565,	0.5076 s / batch. (data: 2.57e-04). ETA=7:41:29, max mem: 11.4 GB 
[10/29 20:50:04 visual_prompt]: 	Training 300/553. train loss: 1.2892,	0.5080 s / batch. (data: 7.97e-03). ETA=7:40:58, max mem: 11.4 GB 
[10/29 20:51:32 visual_prompt]: 	Training 400/553. train loss: 0.6569,	0.4960 s / batch. (data: 5.39e-03). ETA=7:29:16, max mem: 11.4 GB 
[10/29 20:52:59 visual_prompt]: 	Training 500/553. train loss: 0.8599,	1.1400 s / batch. (data: 6.56e-01). ETA=17:10:42, max mem: 11.4 GB 
[10/29 20:53:46 visual_prompt]: Epoch 2 / 100: avg data time: 3.93e-01, avg batch time: 0.8873, average train loss: 0.9526
[10/29 20:54:38 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1924, average loss: 0.6912
[10/29 20:54:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 48.74	
[10/29 20:56:05 visual_prompt]: 	Test 100/162. loss: 0.680, 0.1885 s / batch. (data: 3.10e-05)max mem: 11.41573 GB 
[10/29 20:56:53 visual_prompt]: Inference (test):avg data time: 1.08e-04, avg batch time: 0.1926, average loss: 0.6796
[10/29 20:56:53 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 52.09	
[10/29 20:56:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/29 20:58:26 visual_prompt]: 	Training 100/553. train loss: 0.5209,	0.4880 s / batch. (data: 2.64e-04). ETA=7:19:59, max mem: 11.4 GB 
[10/29 20:59:55 visual_prompt]: 	Training 200/553. train loss: 0.9921,	0.4774 s / batch. (data: 2.86e-04). ETA=7:09:38, max mem: 11.4 GB 
[10/29 21:01:21 visual_prompt]: 	Training 300/553. train loss: 0.6139,	0.5000 s / batch. (data: 7.97e-03). ETA=7:29:08, max mem: 11.4 GB 
[10/29 21:02:51 visual_prompt]: 	Training 400/553. train loss: 0.9634,	0.5163 s / batch. (data: 2.82e-04). ETA=7:42:55, max mem: 11.4 GB 
[10/29 21:04:20 visual_prompt]: 	Training 500/553. train loss: 1.0704,	2.3600 s / batch. (data: 1.88e+00). ETA=1 day, 11:11:57, max mem: 11.4 GB 
[10/29 21:05:05 visual_prompt]: Epoch 3 / 100: avg data time: 3.95e-01, avg batch time: 0.8904, average train loss: 0.9930
[10/29 21:05:58 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1923, average loss: 0.8102
[10/29 21:05:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.27	
[10/29 21:07:24 visual_prompt]: 	Test 100/162. loss: 0.826, 0.1855 s / batch. (data: 3.70e-05)max mem: 11.41573 GB 
[10/29 21:08:13 visual_prompt]: Inference (test):avg data time: 1.18e-04, avg batch time: 0.1923, average loss: 0.7553
[10/29 21:08:13 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.28	
[10/29 21:08:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/29 21:09:44 visual_prompt]: 	Training 100/553. train loss: 0.8435,	0.4837 s / batch. (data: 5.43e-03). ETA=7:11:35, max mem: 11.4 GB 
[10/29 21:11:14 visual_prompt]: 	Training 200/553. train loss: 0.8774,	0.5240 s / batch. (data: 2.33e-04). ETA=7:46:42, max mem: 11.4 GB 
[10/29 21:12:44 visual_prompt]: 	Training 300/553. train loss: 3.1789,	1.4960 s / batch. (data: 1.02e+00). ETA=22:09:55, max mem: 11.4 GB 
[10/29 21:14:14 visual_prompt]: 	Training 400/553. train loss: 1.1221,	3.2391 s / batch. (data: 2.75e+00). ETA=1 day, 23:54:11, max mem: 11.4 GB 
[10/29 21:15:42 visual_prompt]: 	Training 500/553. train loss: 1.5302,	1.5212 s / batch. (data: 1.02e+00). ETA=22:27:20, max mem: 11.4 GB 
[10/29 21:16:27 visual_prompt]: Epoch 4 / 100: avg data time: 4.00e-01, avg batch time: 0.8940, average train loss: 1.2391
[10/29 21:17:20 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.1916, average loss: 0.6876
[10/29 21:17:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 56.13	
[10/29 21:18:45 visual_prompt]: 	Test 100/162. loss: 0.641, 0.1975 s / batch. (data: 3.62e-05)max mem: 11.41573 GB 
[10/29 21:19:34 visual_prompt]: Inference (test):avg data time: 7.88e-05, avg batch time: 0.1926, average loss: 0.6691
[10/29 21:19:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.91	rocauc: 57.89	
[10/29 21:19:34 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/29 21:21:06 visual_prompt]: 	Training 100/553. train loss: 0.7435,	0.4880 s / batch. (data: 2.58e-04). ETA=7:10:58, max mem: 11.4 GB 
[10/29 21:22:35 visual_prompt]: 	Training 200/553. train loss: 1.9769,	0.5293 s / batch. (data: 5.36e-03). ETA=7:46:31, max mem: 11.4 GB 
[10/29 21:24:05 visual_prompt]: 	Training 300/553. train loss: 1.4037,	0.9400 s / batch. (data: 4.52e-01). ETA=13:46:59, max mem: 11.4 GB 
[10/29 21:25:33 visual_prompt]: 	Training 400/553. train loss: 0.0030,	0.5135 s / batch. (data: 5.38e-03). ETA=7:30:52, max mem: 11.4 GB 
[10/29 21:26:59 visual_prompt]: 	Training 500/553. train loss: 0.5965,	0.5552 s / batch. (data: 4.30e-02). ETA=8:06:37, max mem: 11.4 GB 
[10/29 21:27:44 visual_prompt]: Epoch 5 / 100: avg data time: 3.92e-01, avg batch time: 0.8867, average train loss: 1.4552
[10/29 21:28:36 visual_prompt]: Inference (val):avg data time: 2.02e-04, avg batch time: 0.1914, average loss: 1.9198
[10/29 21:28:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.76	
[10/29 21:30:02 visual_prompt]: 	Test 100/162. loss: 2.056, 0.1997 s / batch. (data: 3.19e-05)max mem: 11.41573 GB 
[10/29 21:30:51 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.1927, average loss: 1.7362
[10/29 21:30:51 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 58.30	
[10/29 21:30:51 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/29 21:32:24 visual_prompt]: 	Training 100/553. train loss: 1.1667,	0.4924 s / batch. (data: 2.54e-04). ETA=7:10:17, max mem: 11.4 GB 
[10/29 21:33:51 visual_prompt]: 	Training 200/553. train loss: 0.7806,	0.5067 s / batch. (data: 7.18e-04). ETA=7:22:00, max mem: 11.4 GB 
[10/29 21:35:19 visual_prompt]: 	Training 300/553. train loss: 0.8117,	0.4954 s / batch. (data: 1.04e-02). ETA=7:11:15, max mem: 11.4 GB 
[10/29 21:36:50 visual_prompt]: 	Training 400/553. train loss: 0.8028,	0.5400 s / batch. (data: 7.47e-04). ETA=7:49:13, max mem: 11.4 GB 
[10/29 21:38:16 visual_prompt]: 	Training 500/553. train loss: 2.3276,	1.6407 s / batch. (data: 1.14e+00). ETA=23:42:54, max mem: 11.4 GB 
[10/29 21:39:02 visual_prompt]: Epoch 6 / 100: avg data time: 3.94e-01, avg batch time: 0.8878, average train loss: 1.3181
[10/29 21:39:55 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1920, average loss: 1.2873
[10/29 21:39:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.25	
[10/29 21:41:20 visual_prompt]: 	Test 100/162. loss: 1.123, 0.1894 s / batch. (data: 3.08e-05)max mem: 11.41573 GB 
[10/29 21:42:09 visual_prompt]: Inference (test):avg data time: 3.32e-05, avg batch time: 0.1908, average loss: 1.3904
[10/29 21:42:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 58.15	
[10/29 21:42:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/29 21:43:40 visual_prompt]: 	Training 100/553. train loss: 1.0499,	0.4784 s / batch. (data: 2.80e-04). ETA=6:53:39, max mem: 11.4 GB 
[10/29 21:45:08 visual_prompt]: 	Training 200/553. train loss: 2.1166,	0.4813 s / batch. (data: 2.70e-04). ETA=6:55:24, max mem: 11.4 GB 
[10/29 21:46:38 visual_prompt]: 	Training 300/553. train loss: 0.5223,	0.5077 s / batch. (data: 5.84e-03). ETA=7:17:19, max mem: 11.4 GB 
[10/29 21:48:07 visual_prompt]: 	Training 400/553. train loss: 1.0528,	0.4917 s / batch. (data: 4.89e-03). ETA=7:02:44, max mem: 11.4 GB 
[10/29 21:49:37 visual_prompt]: 	Training 500/553. train loss: 0.7396,	0.5120 s / batch. (data: 2.73e-04). ETA=7:19:18, max mem: 11.4 GB 
[10/29 21:50:22 visual_prompt]: Epoch 7 / 100: avg data time: 3.98e-01, avg batch time: 0.8927, average train loss: 1.2147
[10/29 21:51:15 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1926, average loss: 2.1357
[10/29 21:51:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.42	
[10/29 21:52:42 visual_prompt]: 	Test 100/162. loss: 1.862, 0.1973 s / batch. (data: 2.91e-05)max mem: 11.41573 GB 
[10/29 21:53:30 visual_prompt]: Inference (test):avg data time: 7.76e-05, avg batch time: 0.1928, average loss: 2.3189
[10/29 21:53:30 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.63	
[10/29 21:53:30 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/29 21:55:03 visual_prompt]: 	Training 100/553. train loss: 1.0491,	0.4920 s / batch. (data: 1.20e-02). ETA=7:00:51, max mem: 11.4 GB 
[10/29 21:56:33 visual_prompt]: 	Training 200/553. train loss: 1.9086,	0.5203 s / batch. (data: 5.37e-03). ETA=7:24:12, max mem: 11.4 GB 
[10/29 21:58:04 visual_prompt]: 	Training 300/553. train loss: 0.0178,	0.5041 s / batch. (data: 1.04e-02). ETA=7:09:34, max mem: 11.4 GB 
[10/29 21:59:33 visual_prompt]: 	Training 400/553. train loss: 0.5961,	0.5052 s / batch. (data: 1.30e-02). ETA=7:09:37, max mem: 11.4 GB 
[10/29 22:01:00 visual_prompt]: 	Training 500/553. train loss: 1.2364,	1.4521 s / batch. (data: 9.54e-01). ETA=20:32:32, max mem: 11.4 GB 
[10/29 22:01:44 visual_prompt]: Epoch 8 / 100: avg data time: 3.98e-01, avg batch time: 0.8929, average train loss: 1.1981
[10/29 22:02:37 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1909, average loss: 1.9682
[10/29 22:02:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.37	
[10/29 22:04:04 visual_prompt]: 	Test 100/162. loss: 2.073, 0.1915 s / batch. (data: 3.86e-05)max mem: 11.41573 GB 
[10/29 22:04:52 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.1944, average loss: 1.7792
[10/29 22:04:52 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.82	
[10/29 22:04:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/29 22:06:25 visual_prompt]: 	Training 100/553. train loss: 3.1184,	1.6555 s / batch. (data: 1.14e+00). ETA=23:21:00, max mem: 11.4 GB 
[10/29 22:07:52 visual_prompt]: 	Training 200/553. train loss: 0.5700,	1.4078 s / batch. (data: 9.12e-01). ETA=19:49:03, max mem: 11.4 GB 
[10/29 22:09:21 visual_prompt]: 	Training 300/553. train loss: 0.5823,	0.5201 s / batch. (data: 1.20e-02). ETA=7:18:23, max mem: 11.4 GB 
[10/29 22:10:51 visual_prompt]: 	Training 400/553. train loss: 2.2540,	0.5001 s / batch. (data: 7.97e-03). ETA=7:00:41, max mem: 11.4 GB 
[10/29 22:12:19 visual_prompt]: 	Training 500/553. train loss: 0.6689,	0.4928 s / batch. (data: 2.60e-04). ETA=6:53:45, max mem: 11.4 GB 
[10/29 22:13:02 visual_prompt]: Epoch 9 / 100: avg data time: 3.91e-01, avg batch time: 0.8871, average train loss: 1.3976
[10/29 22:13:54 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1903, average loss: 1.5543
[10/29 22:13:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.41	
[10/29 22:15:21 visual_prompt]: 	Test 100/162. loss: 1.730, 0.1948 s / batch. (data: 3.12e-05)max mem: 11.41573 GB 
[10/29 22:16:09 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.1926, average loss: 1.4240
[10/29 22:16:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 58.29	
[10/29 22:16:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/29 22:17:42 visual_prompt]: 	Training 100/553. train loss: 1.0461,	0.4840 s / batch. (data: 2.69e-04). ETA=6:45:06, max mem: 11.4 GB 
[10/29 22:19:12 visual_prompt]: 	Training 200/553. train loss: 3.1905,	0.4799 s / batch. (data: 2.53e-04). ETA=6:40:55, max mem: 11.4 GB 
[10/29 22:20:40 visual_prompt]: 	Training 300/553. train loss: 1.0667,	0.5006 s / batch. (data: 2.38e-04). ETA=6:57:19, max mem: 11.4 GB 
[10/29 22:22:05 visual_prompt]: 	Training 400/553. train loss: 2.8774,	2.3039 s / batch. (data: 1.81e+00). ETA=1 day, 7:56:57, max mem: 11.4 GB 
[10/29 22:23:35 visual_prompt]: 	Training 500/553. train loss: 0.8213,	0.5128 s / batch. (data: 1.09e-02). ETA=7:05:47, max mem: 11.4 GB 
[10/29 22:24:20 visual_prompt]: Epoch 10 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 1.3393
[10/29 22:25:13 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1909, average loss: 0.7590
[10/29 22:25:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 61.58	
[10/29 22:26:39 visual_prompt]: 	Test 100/162. loss: 0.513, 0.1857 s / batch. (data: 3.27e-05)max mem: 11.41573 GB 
[10/29 22:27:27 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.1933, average loss: 0.8170
[10/29 22:27:27 visual_prompt]: Classification results with test_mammo-cbis: top1: 44.03	rocauc: 56.90	
[10/29 22:27:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/29 22:28:58 visual_prompt]: 	Training 100/553. train loss: 1.1168,	1.9713 s / batch. (data: 1.49e+00). ETA=1 day, 3:11:56, max mem: 11.4 GB 
[10/29 22:30:27 visual_prompt]: 	Training 200/553. train loss: 1.7846,	1.3029 s / batch. (data: 8.11e-01). ETA=17:56:25, max mem: 11.4 GB 
[10/29 22:31:57 visual_prompt]: 	Training 300/553. train loss: 0.4868,	1.0531 s / batch. (data: 5.76e-01). ETA=14:28:17, max mem: 11.4 GB 
[10/29 22:33:26 visual_prompt]: 	Training 400/553. train loss: 1.4601,	0.9040 s / batch. (data: 4.09e-01). ETA=12:23:51, max mem: 11.4 GB 
[10/29 22:34:53 visual_prompt]: 	Training 500/553. train loss: 0.8993,	1.5117 s / batch. (data: 1.02e+00). ETA=20:41:19, max mem: 11.4 GB 
[10/29 22:35:37 visual_prompt]: Epoch 11 / 100: avg data time: 3.91e-01, avg batch time: 0.8862, average train loss: 1.6893
[10/29 22:36:30 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1905, average loss: 3.7567
[10/29 22:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.45	
[10/29 22:37:56 visual_prompt]: 	Test 100/162. loss: 3.180, 0.1901 s / batch. (data: 3.03e-05)max mem: 11.41573 GB 
[10/29 22:38:44 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.1918, average loss: 4.0896
[10/29 22:38:44 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.72	
[10/29 22:38:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/29 22:40:15 visual_prompt]: 	Training 100/553. train loss: 6.2034,	1.0319 s / batch. (data: 5.39e-01). ETA=14:04:43, max mem: 11.4 GB 
[10/29 22:41:43 visual_prompt]: 	Training 200/553. train loss: 0.7054,	0.5402 s / batch. (data: 2.83e-02). ETA=7:21:20, max mem: 11.4 GB 
[10/29 22:43:11 visual_prompt]: 	Training 300/553. train loss: 0.4920,	0.5063 s / batch. (data: 1.55e-02). ETA=6:52:46, max mem: 11.4 GB 
[10/29 22:44:42 visual_prompt]: 	Training 400/553. train loss: 0.6944,	0.5000 s / batch. (data: 1.60e-02). ETA=6:46:48, max mem: 11.4 GB 
[10/29 22:46:09 visual_prompt]: 	Training 500/553. train loss: 0.7683,	1.2836 s / batch. (data: 7.93e-01). ETA=17:22:12, max mem: 11.4 GB 
[10/29 22:46:54 visual_prompt]: Epoch 12 / 100: avg data time: 3.91e-01, avg batch time: 0.8858, average train loss: 1.5785
[10/29 22:47:46 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1900, average loss: 4.7708
[10/29 22:47:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.84	
[10/29 22:49:12 visual_prompt]: 	Test 100/162. loss: 5.101, 0.1997 s / batch. (data: 3.50e-05)max mem: 11.41573 GB 
[10/29 22:50:01 visual_prompt]: Inference (test):avg data time: 1.69e-04, avg batch time: 0.1925, average loss: 4.3098
[10/29 22:50:01 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.99	
[10/29 22:50:01 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/29 22:51:33 visual_prompt]: 	Training 100/553. train loss: 1.6521,	0.5095 s / batch. (data: 1.73e-02). ETA=6:52:23, max mem: 11.4 GB 
[10/29 22:53:02 visual_prompt]: 	Training 200/553. train loss: 1.6675,	0.5120 s / batch. (data: 2.53e-04). ETA=6:53:35, max mem: 11.4 GB 
[10/29 22:54:31 visual_prompt]: 	Training 300/553. train loss: 0.4540,	0.5160 s / batch. (data: 2.95e-04). ETA=6:55:54, max mem: 11.4 GB 
[10/29 22:56:01 visual_prompt]: 	Training 400/553. train loss: 0.7096,	0.5280 s / batch. (data: 7.23e-04). ETA=7:04:44, max mem: 11.4 GB 
[10/29 22:57:28 visual_prompt]: 	Training 500/553. train loss: 2.9772,	1.5011 s / batch. (data: 9.95e-01). ETA=20:04:59, max mem: 11.4 GB 
[10/29 22:58:12 visual_prompt]: Epoch 13 / 100: avg data time: 3.93e-01, avg batch time: 0.8876, average train loss: 1.5781
[10/29 22:59:04 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1908, average loss: 0.8184
[10/29 22:59:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.57	
[10/29 23:00:30 visual_prompt]: 	Test 100/162. loss: 0.738, 0.1937 s / batch. (data: 3.43e-05)max mem: 11.41573 GB 
[10/29 23:01:18 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.1929, average loss: 0.7584
[10/29 23:01:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 58.53	
[10/29 23:01:18 visual_prompt]: Best epoch 13: best metric: -0.818
[10/29 23:01:18 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/29 23:02:49 visual_prompt]: 	Training 100/553. train loss: 2.3473,	0.5080 s / batch. (data: 2.73e-04). ETA=6:46:28, max mem: 11.4 GB 
[10/29 23:04:18 visual_prompt]: 	Training 200/553. train loss: 2.3773,	2.3557 s / batch. (data: 1.88e+00). ETA=1 day, 7:21:03, max mem: 11.4 GB 
[10/29 23:05:49 visual_prompt]: 	Training 300/553. train loss: 0.9586,	2.1640 s / batch. (data: 1.68e+00). ETA=1 day, 4:44:24, max mem: 11.4 GB 
[10/29 23:07:16 visual_prompt]: 	Training 400/553. train loss: 1.2273,	2.3139 s / batch. (data: 1.81e+00). ETA=1 day, 6:39:58, max mem: 11.4 GB 
[10/29 23:08:43 visual_prompt]: 	Training 500/553. train loss: 1.2645,	0.5050 s / batch. (data: 1.04e-02). ETA=6:40:45, max mem: 11.4 GB 
[10/29 23:09:29 visual_prompt]: Epoch 14 / 100: avg data time: 3.92e-01, avg batch time: 0.8873, average train loss: 1.5279
[10/29 23:10:21 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1916, average loss: 0.9176
[10/29 23:10:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.32	
[10/29 23:11:48 visual_prompt]: 	Test 100/162. loss: 0.656, 0.1853 s / batch. (data: 3.08e-05)max mem: 11.41573 GB 
[10/29 23:12:36 visual_prompt]: Inference (test):avg data time: 3.17e-05, avg batch time: 0.1928, average loss: 0.8403
[10/29 23:12:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.09	rocauc: 60.37	
[10/29 23:12:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/29 23:14:05 visual_prompt]: 	Training 100/553. train loss: 0.8274,	0.6987 s / batch. (data: 2.21e-01). ETA=9:12:39, max mem: 11.4 GB 
[10/29 23:15:37 visual_prompt]: 	Training 200/553. train loss: 0.6088,	0.5006 s / batch. (data: 1.20e-02). ETA=6:35:07, max mem: 11.4 GB 
[10/29 23:17:05 visual_prompt]: 	Training 300/553. train loss: 0.5138,	0.5114 s / batch. (data: 1.16e-02). ETA=6:42:45, max mem: 11.4 GB 
[10/29 23:18:31 visual_prompt]: 	Training 400/553. train loss: 0.4966,	0.4934 s / batch. (data: 2.94e-04). ETA=6:27:48, max mem: 11.4 GB 
[10/29 23:20:00 visual_prompt]: 	Training 500/553. train loss: 0.9006,	0.5041 s / batch. (data: 1.19e-02). ETA=6:35:22, max mem: 11.4 GB 
[10/29 23:20:47 visual_prompt]: Epoch 15 / 100: avg data time: 3.94e-01, avg batch time: 0.8880, average train loss: 1.6334
[10/29 23:21:39 visual_prompt]: Inference (val):avg data time: 1.90e-04, avg batch time: 0.1897, average loss: 0.6960
[10/29 23:21:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 60.08	
[10/29 23:23:06 visual_prompt]: 	Test 100/162. loss: 0.454, 0.1855 s / batch. (data: 6.01e-05)max mem: 11.41573 GB 
[10/29 23:23:53 visual_prompt]: Inference (test):avg data time: 3.46e-05, avg batch time: 0.1916, average loss: 0.7198
[10/29 23:23:53 visual_prompt]: Classification results with test_mammo-cbis: top1: 53.02	rocauc: 58.59	
[10/29 23:23:53 visual_prompt]: Best epoch 15: best metric: -0.696
[10/29 23:23:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/29 23:25:23 visual_prompt]: 	Training 100/553. train loss: 0.5178,	0.4967 s / batch. (data: 7.96e-03). ETA=6:28:19, max mem: 11.4 GB 
[10/29 23:26:52 visual_prompt]: 	Training 200/553. train loss: 3.8222,	0.4916 s / batch. (data: 2.77e-04). ETA=6:23:27, max mem: 11.4 GB 
[10/29 23:28:21 visual_prompt]: 	Training 300/553. train loss: 2.1514,	0.5040 s / batch. (data: 2.78e-04). ETA=6:32:19, max mem: 11.4 GB 
[10/29 23:29:49 visual_prompt]: 	Training 400/553. train loss: 2.4929,	0.5000 s / batch. (data: 2.76e-04). ETA=6:28:22, max mem: 11.4 GB 
[10/29 23:31:17 visual_prompt]: 	Training 500/553. train loss: 0.7537,	0.4963 s / batch. (data: 2.35e-04). ETA=6:24:40, max mem: 11.4 GB 
[10/29 23:32:03 visual_prompt]: Epoch 16 / 100: avg data time: 3.90e-01, avg batch time: 0.8853, average train loss: 1.5633
[10/29 23:32:55 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1902, average loss: 1.3627
[10/29 23:32:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.56	
[10/29 23:34:20 visual_prompt]: 	Test 100/162. loss: 0.946, 0.1857 s / batch. (data: 3.91e-05)max mem: 11.41573 GB 
[10/29 23:35:10 visual_prompt]: Inference (test):avg data time: 3.25e-04, avg batch time: 0.1930, average loss: 1.5200
[10/29 23:35:10 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 59.76	
[10/29 23:35:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/29 23:36:40 visual_prompt]: 	Training 100/553. train loss: 0.8514,	0.4921 s / batch. (data: 2.67e-04). ETA=6:20:08, max mem: 11.4 GB 
[10/29 23:38:09 visual_prompt]: 	Training 200/553. train loss: 0.5557,	0.5240 s / batch. (data: 1.20e-02). ETA=6:43:56, max mem: 11.4 GB 
[10/29 23:39:34 visual_prompt]: 	Training 300/553. train loss: 0.9791,	0.4980 s / batch. (data: 1.72e-02). ETA=6:23:05, max mem: 11.4 GB 
[10/29 23:41:06 visual_prompt]: 	Training 400/553. train loss: 1.3986,	0.4785 s / batch. (data: 2.57e-04). ETA=6:07:14, max mem: 11.4 GB 
[10/29 23:42:36 visual_prompt]: 	Training 500/553. train loss: 0.0518,	0.4835 s / batch. (data: 2.74e-04). ETA=6:10:17, max mem: 11.4 GB 
[10/29 23:43:21 visual_prompt]: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8886, average train loss: 1.3431
[10/29 23:44:14 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1899, average loss: 0.9837
[10/29 23:44:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.93	
[10/29 23:45:40 visual_prompt]: 	Test 100/162. loss: 0.702, 0.1857 s / batch. (data: 4.03e-05)max mem: 11.41573 GB 
[10/29 23:46:28 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.1924, average loss: 0.8914
[10/29 23:46:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 59.62	
[10/29 23:46:28 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/29 23:48:03 visual_prompt]: 	Training 100/553. train loss: 0.6742,	0.5320 s / batch. (data: 6.75e-04). ETA=6:46:05, max mem: 11.4 GB 
[10/29 23:49:30 visual_prompt]: 	Training 200/553. train loss: 2.4417,	0.5000 s / batch. (data: 4.12e-04). ETA=6:20:48, max mem: 11.4 GB 
[10/29 23:50:57 visual_prompt]: 	Training 300/553. train loss: 0.8684,	0.4952 s / batch. (data: 2.23e-04). ETA=6:16:22, max mem: 11.4 GB 
[10/29 23:52:24 visual_prompt]: 	Training 400/553. train loss: 0.7803,	0.5001 s / batch. (data: 7.96e-03). ETA=6:19:15, max mem: 11.4 GB 
[10/29 23:53:53 visual_prompt]: 	Training 500/553. train loss: 0.8904,	0.4909 s / batch. (data: 2.63e-04). ETA=6:11:28, max mem: 11.4 GB 
[10/29 23:54:38 visual_prompt]: Epoch 18 / 100: avg data time: 3.91e-01, avg batch time: 0.8858, average train loss: 1.4032
[10/29 23:55:30 visual_prompt]: Inference (val):avg data time: 3.75e-04, avg batch time: 0.1922, average loss: 0.7212
[10/29 23:55:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 60.34	
[10/29 23:56:55 visual_prompt]: 	Test 100/162. loss: 0.556, 0.1857 s / batch. (data: 3.62e-05)max mem: 11.41573 GB 
[10/29 23:57:44 visual_prompt]: Inference (test):avg data time: 1.50e-04, avg batch time: 0.1922, average loss: 0.6741
[10/29 23:57:44 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.78	rocauc: 61.95	
[10/29 23:57:44 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/29 23:59:16 visual_prompt]: 	Training 100/553. train loss: 0.7433,	1.8720 s / batch. (data: 1.38e+00). ETA=23:31:40, max mem: 11.4 GB 
[10/30 00:00:44 visual_prompt]: 	Training 200/553. train loss: 1.5255,	1.6715 s / batch. (data: 1.18e+00). ETA=20:57:43, max mem: 11.4 GB 
[10/30 00:02:13 visual_prompt]: 	Training 300/553. train loss: 0.9570,	0.5052 s / batch. (data: 5.43e-03). ETA=6:19:17, max mem: 11.4 GB 
[10/30 00:03:41 visual_prompt]: 	Training 400/553. train loss: 0.6108,	0.5200 s / batch. (data: 3.45e-04). ETA=6:29:30, max mem: 11.4 GB 
[10/30 00:05:08 visual_prompt]: 	Training 500/553. train loss: 0.1830,	1.0920 s / batch. (data: 5.92e-01). ETA=13:36:11, max mem: 11.4 GB 
[10/30 00:05:54 visual_prompt]: Epoch 19 / 100: avg data time: 3.91e-01, avg batch time: 0.8859, average train loss: 1.3725
[10/30 00:06:47 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1922, average loss: 1.0325
[10/30 00:06:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 58.93	
[10/30 00:08:13 visual_prompt]: 	Test 100/162. loss: 0.733, 0.1856 s / batch. (data: 4.98e-05)max mem: 11.41573 GB 
[10/30 00:09:01 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.1934, average loss: 0.9201
[10/30 00:09:01 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.91	rocauc: 60.70	
[10/30 00:09:01 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/30 00:10:32 visual_prompt]: 	Training 100/553. train loss: 0.6566,	1.1603 s / batch. (data: 6.59e-01). ETA=14:24:18, max mem: 11.4 GB 
[10/30 00:11:58 visual_prompt]: 	Training 200/553. train loss: 5.2992,	1.2937 s / batch. (data: 7.93e-01). ETA=16:01:29, max mem: 11.4 GB 
[10/30 00:13:28 visual_prompt]: 	Training 300/553. train loss: 0.8961,	0.4898 s / batch. (data: 1.12e-02). ETA=6:03:12, max mem: 11.4 GB 
[10/30 00:14:57 visual_prompt]: 	Training 400/553. train loss: 0.9494,	0.4814 s / batch. (data: 2.78e-04). ETA=5:56:12, max mem: 11.4 GB 
[10/30 00:16:26 visual_prompt]: 	Training 500/553. train loss: 3.9045,	0.5035 s / batch. (data: 7.25e-04). ETA=6:11:41, max mem: 11.4 GB 
[10/30 00:17:12 visual_prompt]: Epoch 20 / 100: avg data time: 3.92e-01, avg batch time: 0.8875, average train loss: 1.4016
[10/30 00:18:04 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1915, average loss: 0.9718
[10/30 00:18:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.77	
[10/30 00:19:30 visual_prompt]: 	Test 100/162. loss: 0.655, 0.1854 s / batch. (data: 3.27e-05)max mem: 11.41573 GB 
[10/30 00:20:18 visual_prompt]: Inference (test):avg data time: 3.33e-05, avg batch time: 0.1919, average loss: 0.8739
[10/30 00:20:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.16	rocauc: 59.44	
[10/30 00:20:18 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[10/30 00:21:49 visual_prompt]: 	Training 100/553. train loss: 3.5591,	0.5080 s / batch. (data: 2.68e-04). ETA=6:13:43, max mem: 11.4 GB 
[10/30 00:23:14 visual_prompt]: 	Training 200/553. train loss: 0.5290,	0.4829 s / batch. (data: 4.64e-03). ETA=5:54:26, max mem: 11.4 GB 
[10/30 00:24:44 visual_prompt]: 	Training 300/553. train loss: 0.6672,	0.4919 s / batch. (data: 7.56e-03). ETA=6:00:13, max mem: 11.4 GB 
[10/30 00:26:13 visual_prompt]: 	Training 400/553. train loss: 3.1764,	0.5080 s / batch. (data: 1.20e-02). ETA=6:11:09, max mem: 11.4 GB 
[10/30 00:27:43 visual_prompt]: 	Training 500/553. train loss: 0.5487,	0.4923 s / batch. (data: 2.83e-04). ETA=5:58:51, max mem: 11.4 GB 
[10/30 00:28:29 visual_prompt]: Epoch 21 / 100: avg data time: 3.92e-01, avg batch time: 0.8867, average train loss: 1.5840
[10/30 00:29:21 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1909, average loss: 0.7440
[10/30 00:29:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 60.92	
[10/30 00:30:47 visual_prompt]: 	Test 100/162. loss: 0.529, 0.1857 s / batch. (data: 3.55e-05)max mem: 11.41573 GB 
[10/30 00:31:35 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.1930, average loss: 0.6966
[10/30 00:31:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.31	rocauc: 61.91	
[10/30 00:31:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[10/30 00:33:08 visual_prompt]: 	Training 100/553. train loss: 1.1703,	0.4960 s / batch. (data: 3.12e-04). ETA=6:00:17, max mem: 11.4 GB 
[10/30 00:34:35 visual_prompt]: 	Training 200/553. train loss: 1.0573,	0.5087 s / batch. (data: 5.39e-03). ETA=6:08:41, max mem: 11.4 GB 
[10/30 00:36:01 visual_prompt]: 	Training 300/553. train loss: 0.2898,	1.2399 s / batch. (data: 7.40e-01). ETA=14:56:35, max mem: 11.4 GB 
[10/30 00:37:30 visual_prompt]: 	Training 400/553. train loss: 1.3914,	1.7766 s / batch. (data: 1.28e+00). ETA=21:21:42, max mem: 11.4 GB 
[10/30 00:38:59 visual_prompt]: 	Training 500/553. train loss: 0.7133,	0.4869 s / batch. (data: 7.94e-03). ETA=5:50:27, max mem: 11.4 GB 
[10/30 00:39:46 visual_prompt]: Epoch 22 / 100: avg data time: 3.91e-01, avg batch time: 0.8864, average train loss: 1.5087
[10/30 00:40:38 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1905, average loss: 0.7373
[10/30 00:40:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.97	
[10/30 00:42:04 visual_prompt]: 	Test 100/162. loss: 0.432, 0.1876 s / batch. (data: 5.36e-05)max mem: 11.41573 GB 
[10/30 00:42:53 visual_prompt]: Inference (test):avg data time: 3.44e-05, avg batch time: 0.1924, average loss: 0.6854
[10/30 00:42:53 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.78	rocauc: 59.61	
[10/30 00:42:53 visual_prompt]: Stopping early.
[10/30 00:42:53 visual_prompt]: Rank of current process: 0. World size: 1
[10/30 00:42:53 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/30 00:42:53 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/30 00:42:53 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/30 00:42:53 visual_prompt]: Training with config:
[10/30 00:42:53 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/test/seed875/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 875, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/30 00:42:53 visual_prompt]: Loading training data...
[10/30 00:42:53 visual_prompt]: Constructing mammo-cbis dataset train...
[10/30 00:42:53 visual_prompt]: Loading validation data...
[10/30 00:42:53 visual_prompt]: Constructing mammo-cbis dataset val...
[10/30 00:42:53 visual_prompt]: Loading test data...
[10/30 00:42:53 visual_prompt]: Constructing mammo-cbis dataset test...
[10/30 00:42:53 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/30 00:42:56 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/30 00:42:56 visual_prompt]: tuned percent:0.529
[10/30 00:42:56 visual_prompt]: Device used for model: 0
[10/30 00:42:56 visual_prompt]: Setting up Evaluator...
[10/30 00:42:56 visual_prompt]: Setting up Trainer...
[10/30 00:42:56 visual_prompt]: 	Setting up the optimizer...
[10/30 00:42:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/30 00:44:29 visual_prompt]: 	Training 100/553. train loss: 0.9638,	0.5077 s / batch. (data: 1.20e-02). ETA=7:47:02, max mem: 11.4 GB 
[10/30 00:45:57 visual_prompt]: 	Training 200/553. train loss: 0.5349,	0.4960 s / batch. (data: 2.93e-04). ETA=7:35:29, max mem: 11.4 GB 
[10/30 00:47:28 visual_prompt]: 	Training 300/553. train loss: 0.4504,	0.4902 s / batch. (data: 2.59e-04). ETA=7:29:21, max mem: 11.4 GB 
[10/30 00:48:55 visual_prompt]: 	Training 400/553. train loss: 0.5397,	0.5079 s / batch. (data: 1.05e-02). ETA=7:44:44, max mem: 11.4 GB 
[10/30 00:50:26 visual_prompt]: 	Training 500/553. train loss: 0.7269,	0.5160 s / batch. (data: 2.74e-04). ETA=7:51:15, max mem: 11.4 GB 
[10/30 00:51:11 visual_prompt]: Epoch 1 / 100: avg data time: 4.00e-01, avg batch time: 0.8945, average train loss: 0.8085
[10/30 00:52:03 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1915, average loss: 0.7116
[10/30 00:52:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 54.49	
[10/30 00:53:29 visual_prompt]: 	Test 100/162. loss: 0.539, 0.1968 s / batch. (data: 3.31e-05)max mem: 11.41573 GB 
[10/30 00:54:19 visual_prompt]: Inference (test):avg data time: 7.90e-05, avg batch time: 0.1920, average loss: 0.7121
[10/30 00:54:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.45	rocauc: 51.96	
[10/30 00:54:19 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/30 00:55:53 visual_prompt]: 	Training 100/553. train loss: 1.0995,	0.4920 s / batch. (data: 2.61e-04). ETA=7:28:06, max mem: 11.4 GB 
[10/30 00:57:22 visual_prompt]: 	Training 200/553. train loss: 0.8581,	0.4957 s / batch. (data: 2.72e-04). ETA=7:30:41, max mem: 11.4 GB 
[10/30 00:58:49 visual_prompt]: 	Training 300/553. train loss: 0.6156,	0.5000 s / batch. (data: 2.87e-04). ETA=7:33:45, max mem: 11.4 GB 
[10/30 01:00:18 visual_prompt]: 	Training 400/553. train loss: 0.2692,	0.5080 s / batch. (data: 7.96e-03). ETA=7:40:08, max mem: 11.4 GB 
[10/30 01:01:47 visual_prompt]: 	Training 500/553. train loss: 0.6090,	0.4880 s / batch. (data: 5.39e-03). ETA=7:21:12, max mem: 11.4 GB 
[10/30 01:02:32 visual_prompt]: Epoch 2 / 100: avg data time: 3.97e-01, avg batch time: 0.8922, average train loss: 0.9917
[10/30 01:03:25 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.1915, average loss: 0.9311
[10/30 01:03:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.28	
[10/30 01:04:52 visual_prompt]: 	Test 100/162. loss: 0.932, 0.1926 s / batch. (data: 3.17e-05)max mem: 11.41573 GB 
[10/30 01:05:41 visual_prompt]: Inference (test):avg data time: 8.03e-05, avg batch time: 0.1922, average loss: 0.8597
[10/30 01:05:41 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.72	
[10/30 01:05:41 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/30 01:07:12 visual_prompt]: 	Training 100/553. train loss: 1.3199,	0.4784 s / batch. (data: 2.63e-04). ETA=7:11:18, max mem: 11.4 GB 
[10/30 01:08:42 visual_prompt]: 	Training 200/553. train loss: 0.6368,	1.1199 s / batch. (data: 6.26e-01). ETA=16:47:48, max mem: 11.4 GB 
[10/30 01:10:11 visual_prompt]: 	Training 300/553. train loss: 0.9971,	0.5081 s / batch. (data: 2.58e-04). ETA=7:36:22, max mem: 11.4 GB 
[10/30 01:11:42 visual_prompt]: 	Training 400/553. train loss: 0.0258,	0.4954 s / batch. (data: 2.76e-04). ETA=7:24:11, max mem: 11.4 GB 
[10/30 01:13:08 visual_prompt]: 	Training 500/553. train loss: 0.5855,	0.4887 s / batch. (data: 2.61e-04). ETA=7:17:19, max mem: 11.4 GB 
[10/30 01:13:55 visual_prompt]: Epoch 3 / 100: avg data time: 3.98e-01, avg batch time: 0.8933, average train loss: 1.0490
[10/30 01:14:47 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1915, average loss: 1.3278
[10/30 01:14:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.23	
[10/30 01:16:15 visual_prompt]: 	Test 100/162. loss: 1.195, 0.1853 s / batch. (data: 3.10e-05)max mem: 11.41573 GB 
[10/30 01:17:03 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.1918, average loss: 1.4287
[10/30 01:17:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.49	
[10/30 01:17:03 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/30 01:18:36 visual_prompt]: 	Training 100/553. train loss: 0.6964,	0.5026 s / batch. (data: 1.20e-02). ETA=7:28:30, max mem: 11.4 GB 
[10/30 01:20:04 visual_prompt]: 	Training 200/553. train loss: 0.7194,	0.4840 s / batch. (data: 2.74e-04). ETA=7:11:04, max mem: 11.4 GB 
[10/30 01:21:34 visual_prompt]: 	Training 300/553. train loss: 0.6405,	1.2960 s / batch. (data: 7.94e-01). ETA=19:12:09, max mem: 11.4 GB 
[10/30 01:23:02 visual_prompt]: 	Training 400/553. train loss: 2.3063,	0.4796 s / batch. (data: 2.83e-04). ETA=7:05:33, max mem: 11.4 GB 
[10/30 01:24:33 visual_prompt]: 	Training 500/553. train loss: 0.8267,	0.5122 s / batch. (data: 3.49e-04). ETA=7:33:40, max mem: 11.4 GB 
[10/30 01:25:18 visual_prompt]: Epoch 4 / 100: avg data time: 3.99e-01, avg batch time: 0.8943, average train loss: 1.1717
[10/30 01:26:10 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1902, average loss: 0.7563
[10/30 01:26:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.57	
[10/30 01:27:37 visual_prompt]: 	Test 100/162. loss: 0.649, 0.1853 s / batch. (data: 3.10e-05)max mem: 11.41573 GB 
[10/30 01:28:26 visual_prompt]: Inference (test):avg data time: 3.33e-05, avg batch time: 0.1936, average loss: 0.7816
[10/30 01:28:26 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 58.29	
[10/30 01:28:26 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/30 01:29:59 visual_prompt]: 	Training 100/553. train loss: 1.4664,	0.5073 s / batch. (data: 1.60e-02). ETA=7:27:58, max mem: 11.4 GB 
[10/30 01:31:29 visual_prompt]: 	Training 200/553. train loss: 1.7995,	0.4804 s / batch. (data: 2.56e-04). ETA=7:03:26, max mem: 11.4 GB 
[10/30 01:32:57 visual_prompt]: 	Training 300/553. train loss: 1.6961,	1.6079 s / batch. (data: 1.12e+00). ETA=23:34:39, max mem: 11.4 GB 
[10/30 01:34:28 visual_prompt]: 	Training 400/553. train loss: 1.0353,	1.5868 s / batch. (data: 1.11e+00). ETA=23:13:24, max mem: 11.4 GB 
[10/30 01:35:57 visual_prompt]: 	Training 500/553. train loss: 0.5083,	1.8597 s / batch. (data: 1.38e+00). ETA=1 day, 3:09:58, max mem: 11.4 GB 
[10/30 01:36:40 visual_prompt]: Epoch 5 / 100: avg data time: 3.98e-01, avg batch time: 0.8930, average train loss: 1.0581
[10/30 01:37:33 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1925, average loss: 1.6655
[10/30 01:37:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.87	
[10/30 01:39:00 visual_prompt]: 	Test 100/162. loss: 1.725, 0.1855 s / batch. (data: 3.12e-05)max mem: 11.41573 GB 
[10/30 01:39:48 visual_prompt]: Inference (test):avg data time: 3.52e-05, avg batch time: 0.1926, average loss: 1.5060
[10/30 01:39:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.43	
[10/30 01:39:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/30 01:41:20 visual_prompt]: 	Training 100/553. train loss: 0.6837,	0.4958 s / batch. (data: 2.92e-04). ETA=7:13:18, max mem: 11.4 GB 
[10/30 01:42:48 visual_prompt]: 	Training 200/553. train loss: 1.0511,	0.5318 s / batch. (data: 4.20e-02). ETA=7:43:52, max mem: 11.4 GB 
[10/30 01:44:19 visual_prompt]: 	Training 300/553. train loss: 0.6538,	1.0658 s / batch. (data: 5.65e-01). ETA=15:27:54, max mem: 11.4 GB 
[10/30 01:45:47 visual_prompt]: 	Training 400/553. train loss: 1.3957,	0.5160 s / batch. (data: 7.98e-03). ETA=7:28:22, max mem: 11.4 GB 
[10/30 01:47:16 visual_prompt]: 	Training 500/553. train loss: 1.2883,	0.5078 s / batch. (data: 1.05e-02). ETA=7:20:22, max mem: 11.4 GB 
[10/30 01:48:01 visual_prompt]: Epoch 6 / 100: avg data time: 3.97e-01, avg batch time: 0.8914, average train loss: 1.2205
[10/30 01:48:54 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.1921, average loss: 1.5335
[10/30 01:48:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.99	
[10/30 01:50:21 visual_prompt]: 	Test 100/162. loss: 1.364, 0.1854 s / batch. (data: 3.17e-05)max mem: 11.41573 GB 
[10/30 01:51:10 visual_prompt]: Inference (test):avg data time: 1.28e-04, avg batch time: 0.1933, average loss: 1.6562
[10/30 01:51:10 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 55.61	
[10/30 01:51:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/30 01:52:43 visual_prompt]: 	Training 100/553. train loss: 0.7761,	0.5200 s / batch. (data: 2.85e-04). ETA=7:29:37, max mem: 11.4 GB 
[10/30 01:54:12 visual_prompt]: 	Training 200/553. train loss: 2.2792,	1.4240 s / batch. (data: 9.38e-01). ETA=20:28:57, max mem: 11.4 GB 
[10/30 01:55:39 visual_prompt]: 	Training 300/553. train loss: 1.0318,	1.6884 s / batch. (data: 1.20e+00). ETA=1 day, 0:14:22, max mem: 11.4 GB 
[10/30 01:57:06 visual_prompt]: 	Training 400/553. train loss: 0.7468,	0.5017 s / batch. (data: 1.04e-02). ETA=7:11:20, max mem: 11.4 GB 
[10/30 01:58:35 visual_prompt]: 	Training 500/553. train loss: 3.4300,	0.7720 s / batch. (data: 2.83e-01). ETA=11:02:26, max mem: 11.4 GB 
[10/30 01:59:22 visual_prompt]: Epoch 7 / 100: avg data time: 3.95e-01, avg batch time: 0.8905, average train loss: 1.2194
[10/30 02:00:15 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1913, average loss: 2.7450
[10/30 02:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.58	
[10/30 02:01:41 visual_prompt]: 	Test 100/162. loss: 2.929, 0.1856 s / batch. (data: 3.19e-05)max mem: 11.41573 GB 
[10/30 02:02:29 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.1937, average loss: 2.4832
[10/30 02:02:29 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 58.42	
[10/30 02:02:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/30 02:03:59 visual_prompt]: 	Training 100/553. train loss: 1.1555,	0.7478 s / batch. (data: 2.55e-01). ETA=10:39:41, max mem: 11.4 GB 
[10/30 02:05:30 visual_prompt]: 	Training 200/553. train loss: 1.8048,	1.3840 s / batch. (data: 8.92e-01). ETA=19:41:40, max mem: 11.4 GB 
[10/30 02:06:58 visual_prompt]: 	Training 300/553. train loss: 1.0362,	0.4960 s / batch. (data: 2.88e-04). ETA=7:02:39, max mem: 11.4 GB 
[10/30 02:08:28 visual_prompt]: 	Training 400/553. train loss: 0.7841,	0.5049 s / batch. (data: 1.20e-03). ETA=7:09:26, max mem: 11.4 GB 
[10/30 02:09:53 visual_prompt]: 	Training 500/553. train loss: 0.6850,	0.5053 s / batch. (data: 1.05e-02). ETA=7:08:53, max mem: 11.4 GB 
[10/30 02:10:40 visual_prompt]: Epoch 8 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 1.0927
[10/30 02:11:33 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1911, average loss: 1.1156
[10/30 02:11:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.99	
[10/30 02:12:57 visual_prompt]: 	Test 100/162. loss: 0.918, 0.1860 s / batch. (data: 2.69e-05)max mem: 11.41573 GB 
[10/30 02:13:47 visual_prompt]: Inference (test):avg data time: 7.96e-05, avg batch time: 0.1913, average loss: 1.2031
[10/30 02:13:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.71	
[10/30 02:13:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/30 02:15:19 visual_prompt]: 	Training 100/553. train loss: 1.3511,	0.5032 s / batch. (data: 2.66e-04). ETA=7:05:48, max mem: 11.4 GB 
[10/30 02:16:47 visual_prompt]: 	Training 200/553. train loss: 0.9764,	0.4884 s / batch. (data: 2.51e-04). ETA=6:52:28, max mem: 11.4 GB 
[10/30 02:18:14 visual_prompt]: 	Training 300/553. train loss: 0.1995,	0.4790 s / batch. (data: 3.07e-04). ETA=6:43:47, max mem: 11.4 GB 
[10/30 02:19:43 visual_prompt]: 	Training 400/553. train loss: 1.3772,	0.4960 s / batch. (data: 2.78e-04). ETA=6:57:14, max mem: 11.4 GB 
[10/30 02:21:15 visual_prompt]: 	Training 500/553. train loss: 0.7180,	2.2943 s / batch. (data: 1.80e+00). ETA=1 day, 8:06:19, max mem: 11.4 GB 
[10/30 02:21:58 visual_prompt]: Epoch 9 / 100: avg data time: 3.93e-01, avg batch time: 0.8873, average train loss: 1.4464
[10/30 02:22:50 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1913, average loss: 1.5762
[10/30 02:22:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.58	
[10/30 02:24:18 visual_prompt]: 	Test 100/162. loss: 1.317, 0.1983 s / batch. (data: 5.03e-05)max mem: 11.41573 GB 
[10/30 02:25:06 visual_prompt]: Inference (test):avg data time: 8.33e-05, avg batch time: 0.1942, average loss: 1.7054
[10/30 02:25:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 58.28	
[10/30 02:25:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/30 02:26:37 visual_prompt]: 	Training 100/553. train loss: 1.9680,	1.0515 s / batch. (data: 5.48e-01). ETA=14:40:11, max mem: 11.4 GB 
[10/30 02:28:08 visual_prompt]: 	Training 200/553. train loss: 1.5950,	0.4920 s / batch. (data: 5.38e-03). ETA=6:50:58, max mem: 11.4 GB 
[10/30 02:29:38 visual_prompt]: 	Training 300/553. train loss: 0.5136,	0.4887 s / batch. (data: 2.65e-04). ETA=6:47:27, max mem: 11.4 GB 
[10/30 02:31:07 visual_prompt]: 	Training 400/553. train loss: 0.8802,	0.5120 s / batch. (data: 2.81e-04). ETA=7:06:00, max mem: 11.4 GB 
[10/30 02:32:35 visual_prompt]: 	Training 500/553. train loss: 0.9037,	0.4921 s / batch. (data: 2.59e-04). ETA=6:48:37, max mem: 11.4 GB 
[10/30 02:33:21 visual_prompt]: Epoch 10 / 100: avg data time: 4.00e-01, avg batch time: 0.8943, average train loss: 1.3565
[10/30 02:34:13 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1915, average loss: 2.9978
[10/30 02:34:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.68	
[10/30 02:35:40 visual_prompt]: 	Test 100/162. loss: 3.245, 0.2019 s / batch. (data: 3.43e-05)max mem: 11.41573 GB 
[10/30 02:36:28 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.1926, average loss: 2.7153
[10/30 02:36:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.93	
[10/30 02:36:28 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/30 02:38:00 visual_prompt]: 	Training 100/553. train loss: 0.7356,	0.4880 s / batch. (data: 2.73e-04). ETA=6:43:58, max mem: 11.4 GB 
[10/30 02:39:27 visual_prompt]: 	Training 200/553. train loss: 1.3718,	1.4440 s / batch. (data: 9.66e-01). ETA=19:52:58, max mem: 11.4 GB 
[10/30 02:40:53 visual_prompt]: 	Training 300/553. train loss: 0.0270,	0.6520 s / batch. (data: 1.55e-01). ETA=8:57:33, max mem: 11.4 GB 
[10/30 02:42:22 visual_prompt]: 	Training 400/553. train loss: 0.9827,	1.2133 s / batch. (data: 7.14e-01). ETA=16:38:20, max mem: 11.4 GB 
[10/30 02:43:54 visual_prompt]: 	Training 500/553. train loss: 2.0704,	0.9122 s / batch. (data: 4.33e-01). ETA=12:29:01, max mem: 11.4 GB 
[10/30 02:44:38 visual_prompt]: Epoch 11 / 100: avg data time: 3.90e-01, avg batch time: 0.8857, average train loss: 1.6297
[10/30 02:45:31 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.1908, average loss: 2.6168
[10/30 02:45:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.37	
[10/30 02:46:57 visual_prompt]: 	Test 100/162. loss: 2.282, 0.1855 s / batch. (data: 3.05e-05)max mem: 11.41573 GB 
[10/30 02:47:45 visual_prompt]: Inference (test):avg data time: 3.60e-05, avg batch time: 0.1917, average loss: 2.8493
[10/30 02:47:45 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.23	
[10/30 02:47:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/30 02:49:19 visual_prompt]: 	Training 100/553. train loss: 2.8323,	0.5483 s / batch. (data: 6.28e-02). ETA=7:28:49, max mem: 11.4 GB 
[10/30 02:50:47 visual_prompt]: 	Training 200/553. train loss: 1.0791,	0.4880 s / batch. (data: 2.67e-04). ETA=6:38:40, max mem: 11.4 GB 
[10/30 02:52:15 visual_prompt]: 	Training 300/553. train loss: 1.3491,	0.5191 s / batch. (data: 2.56e-02). ETA=7:03:10, max mem: 11.4 GB 
[10/30 02:53:43 visual_prompt]: 	Training 400/553. train loss: 1.2814,	0.5279 s / batch. (data: 2.39e-02). ETA=7:09:32, max mem: 11.4 GB 
[10/30 02:55:14 visual_prompt]: 	Training 500/553. train loss: 0.8501,	0.5037 s / batch. (data: 2.80e-04). ETA=6:49:00, max mem: 11.4 GB 
[10/30 02:55:58 visual_prompt]: Epoch 12 / 100: avg data time: 3.97e-01, avg batch time: 0.8914, average train loss: 1.3963
[10/30 02:56:51 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1912, average loss: 1.0465
[10/30 02:56:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[10/30 02:58:18 visual_prompt]: 	Test 100/162. loss: 0.735, 0.1892 s / batch. (data: 3.22e-05)max mem: 11.41573 GB 
[10/30 02:59:06 visual_prompt]: Inference (test):avg data time: 3.74e-05, avg batch time: 0.1919, average loss: 1.1518
[10/30 02:59:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 58.05	
[10/30 02:59:06 visual_prompt]: Best epoch 12: best metric: -1.047
[10/30 02:59:06 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/30 03:00:37 visual_prompt]: 	Training 100/553. train loss: 1.5965,	0.5165 s / batch. (data: 1.05e-02). ETA=6:58:01, max mem: 11.4 GB 
[10/30 03:02:05 visual_prompt]: 	Training 200/553. train loss: 0.1380,	0.4793 s / batch. (data: 2.66e-04). ETA=6:27:09, max mem: 11.4 GB 
[10/30 03:03:34 visual_prompt]: 	Training 300/553. train loss: 3.1689,	1.1240 s / batch. (data: 6.43e-01). ETA=15:06:01, max mem: 11.4 GB 
[10/30 03:05:04 visual_prompt]: 	Training 400/553. train loss: 0.8445,	0.5280 s / batch. (data: 7.99e-03). ETA=7:04:44, max mem: 11.4 GB 
[10/30 03:06:32 visual_prompt]: 	Training 500/553. train loss: 0.6960,	2.4171 s / batch. (data: 1.92e+00). ETA=1 day, 8:20:15, max mem: 11.4 GB 
[10/30 03:07:17 visual_prompt]: Epoch 13 / 100: avg data time: 3.93e-01, avg batch time: 0.8872, average train loss: 1.5082
[10/30 03:08:09 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1905, average loss: 0.9822
[10/30 03:08:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.28	
[10/30 03:09:36 visual_prompt]: 	Test 100/162. loss: 0.923, 0.1854 s / batch. (data: 3.77e-05)max mem: 11.41573 GB 
[10/30 03:10:24 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.1930, average loss: 0.8933
[10/30 03:10:24 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 61.19	
[10/30 03:10:24 visual_prompt]: Best epoch 13: best metric: -0.982
[10/30 03:10:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/30 03:11:58 visual_prompt]: 	Training 100/553. train loss: 1.3853,	0.4800 s / batch. (data: 2.59e-04). ETA=6:24:05, max mem: 11.4 GB 
[10/30 03:13:25 visual_prompt]: 	Training 200/553. train loss: 1.7196,	1.7000 s / batch. (data: 1.21e+00). ETA=22:37:30, max mem: 11.4 GB 
[10/30 03:14:54 visual_prompt]: 	Training 300/553. train loss: 1.6434,	1.1788 s / batch. (data: 6.97e-01). ETA=15:39:21, max mem: 11.4 GB 
[10/30 03:16:24 visual_prompt]: 	Training 400/553. train loss: 1.5784,	2.2914 s / batch. (data: 1.79e+00). ETA=1 day, 6:22:03, max mem: 11.4 GB 
[10/30 03:17:50 visual_prompt]: 	Training 500/553. train loss: 0.6165,	2.6494 s / batch. (data: 2.17e+00). ETA=1 day, 11:02:18, max mem: 11.4 GB 
[10/30 03:18:34 visual_prompt]: Epoch 14 / 100: avg data time: 3.91e-01, avg batch time: 0.8863, average train loss: 1.5089
[10/30 03:19:27 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1914, average loss: 3.6294
[10/30 03:19:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.12	
[10/30 03:20:53 visual_prompt]: 	Test 100/162. loss: 3.810, 0.1949 s / batch. (data: 2.91e-05)max mem: 11.41573 GB 
[10/30 03:21:41 visual_prompt]: Inference (test):avg data time: 3.31e-05, avg batch time: 0.1927, average loss: 3.2799
[10/30 03:21:41 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 58.34	
[10/30 03:21:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/30 03:23:12 visual_prompt]: 	Training 100/553. train loss: 0.8387,	0.5088 s / batch. (data: 8.72e-03). ETA=6:42:25, max mem: 11.4 GB 
[10/30 03:24:42 visual_prompt]: 	Training 200/553. train loss: 2.4654,	1.3640 s / batch. (data: 8.86e-01). ETA=17:56:36, max mem: 11.4 GB 
[10/30 03:26:11 visual_prompt]: 	Training 300/553. train loss: 1.1876,	0.5049 s / batch. (data: 1.04e-02). ETA=6:37:41, max mem: 11.4 GB 
[10/30 03:27:40 visual_prompt]: 	Training 400/553. train loss: 1.6124,	0.4925 s / batch. (data: 2.55e-04). ETA=6:27:06, max mem: 11.4 GB 
[10/30 03:29:08 visual_prompt]: 	Training 500/553. train loss: 0.5599,	0.9267 s / batch. (data: 4.27e-01). ETA=12:06:50, max mem: 11.4 GB 
[10/30 03:29:52 visual_prompt]: Epoch 15 / 100: avg data time: 3.92e-01, avg batch time: 0.8872, average train loss: 1.5273
[10/30 03:30:44 visual_prompt]: Inference (val):avg data time: 2.80e-04, avg batch time: 0.1920, average loss: 3.2376
[10/30 03:30:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.74	
[10/30 03:32:10 visual_prompt]: 	Test 100/162. loss: 2.670, 0.1913 s / batch. (data: 3.34e-05)max mem: 11.41573 GB 
[10/30 03:32:59 visual_prompt]: Inference (test):avg data time: 1.66e-04, avg batch time: 0.1922, average loss: 3.5197
[10/30 03:32:59 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 61.18	
[10/30 03:32:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/30 03:34:31 visual_prompt]: 	Training 100/553. train loss: 0.7812,	0.5090 s / batch. (data: 2.78e-04). ETA=6:37:56, max mem: 11.4 GB 
[10/30 03:35:56 visual_prompt]: 	Training 200/553. train loss: 0.6593,	0.5160 s / batch. (data: 2.76e-04). ETA=6:42:31, max mem: 11.4 GB 
[10/30 03:37:25 visual_prompt]: 	Training 300/553. train loss: 0.8577,	1.9364 s / batch. (data: 1.45e+00). ETA=1 day, 1:07:21, max mem: 11.4 GB 
[10/30 03:38:56 visual_prompt]: 	Training 400/553. train loss: 0.4132,	0.5230 s / batch. (data: 1.04e-02). ETA=6:46:16, max mem: 11.4 GB 
[10/30 03:40:26 visual_prompt]: 	Training 500/553. train loss: 4.0159,	0.4930 s / batch. (data: 6.97e-04). ETA=6:22:05, max mem: 11.4 GB 
[10/30 03:41:10 visual_prompt]: Epoch 16 / 100: avg data time: 3.95e-01, avg batch time: 0.8886, average train loss: 1.6827
[10/30 03:42:03 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1913, average loss: 0.8479
[10/30 03:42:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.83	
[10/30 03:43:29 visual_prompt]: 	Test 100/162. loss: 0.773, 0.1899 s / batch. (data: 1.21e-04)max mem: 11.41573 GB 
[10/30 03:44:17 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.1929, average loss: 0.7929
[10/30 03:44:17 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 60.32	
[10/30 03:44:17 visual_prompt]: Best epoch 16: best metric: -0.848
[10/30 03:44:17 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/30 03:45:51 visual_prompt]: 	Training 100/553. train loss: 1.1253,	0.5069 s / batch. (data: 2.96e-04). ETA=6:31:33, max mem: 11.4 GB 
[10/30 03:47:18 visual_prompt]: 	Training 200/553. train loss: 0.9976,	0.4794 s / batch. (data: 2.45e-04). ETA=6:09:32, max mem: 11.4 GB 
[10/30 03:48:47 visual_prompt]: 	Training 300/553. train loss: 0.4685,	0.4960 s / batch. (data: 7.99e-03). ETA=6:21:30, max mem: 11.4 GB 
[10/30 03:50:16 visual_prompt]: 	Training 400/553. train loss: 0.3524,	0.4927 s / batch. (data: 1.20e-02). ETA=6:18:11, max mem: 11.4 GB 
[10/30 03:51:44 visual_prompt]: 	Training 500/553. train loss: 0.8997,	0.5240 s / batch. (data: 7.12e-04). ETA=6:41:18, max mem: 11.4 GB 
[10/30 03:52:29 visual_prompt]: Epoch 17 / 100: avg data time: 3.95e-01, avg batch time: 0.8894, average train loss: 1.4726
[10/30 03:53:21 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1930, average loss: 1.3366
[10/30 03:53:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.47	
[10/30 03:54:48 visual_prompt]: 	Test 100/162. loss: 1.089, 0.1854 s / batch. (data: 3.43e-05)max mem: 11.41573 GB 
[10/30 03:55:36 visual_prompt]: Inference (test):avg data time: 2.38e-04, avg batch time: 0.1930, average loss: 1.1869
[10/30 03:55:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 60.33	
[10/30 03:55:36 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/30 03:57:09 visual_prompt]: 	Training 100/553. train loss: 0.6515,	0.5039 s / batch. (data: 5.37e-03). ETA=6:24:36, max mem: 11.4 GB 
[10/30 03:58:39 visual_prompt]: 	Training 200/553. train loss: 0.7463,	0.5080 s / batch. (data: 2.66e-04). ETA=6:26:53, max mem: 11.4 GB 
[10/30 04:00:05 visual_prompt]: 	Training 300/553. train loss: 0.9825,	0.4846 s / batch. (data: 2.71e-04). ETA=6:08:18, max mem: 11.4 GB 
[10/30 04:01:33 visual_prompt]: 	Training 400/553. train loss: 1.3475,	2.2173 s / batch. (data: 1.72e+00). ETA=1 day, 4:01:22, max mem: 11.4 GB 
[10/30 04:03:06 visual_prompt]: 	Training 500/553. train loss: 0.9564,	2.7125 s / batch. (data: 2.24e+00). ETA=1 day, 10:12:24, max mem: 11.4 GB 
[10/30 04:03:49 visual_prompt]: Epoch 18 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 1.5418
[10/30 04:04:41 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1915, average loss: 0.9815
[10/30 04:04:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.57	
[10/30 04:06:07 visual_prompt]: 	Test 100/162. loss: 0.712, 0.1913 s / batch. (data: 5.08e-05)max mem: 11.41573 GB 
[10/30 04:06:55 visual_prompt]: Inference (test):avg data time: 8.12e-05, avg batch time: 0.1924, average loss: 1.0804
[10/30 04:06:55 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 62.12	
[10/30 04:06:55 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/30 04:08:28 visual_prompt]: 	Training 100/553. train loss: 2.0698,	0.5160 s / batch. (data: 7.96e-03). ETA=6:29:05, max mem: 11.4 GB 
[10/30 04:10:00 visual_prompt]: 	Training 200/553. train loss: 3.9998,	0.5082 s / batch. (data: 2.72e-02). ETA=6:22:22, max mem: 11.4 GB 
[10/30 04:11:29 visual_prompt]: 	Training 300/553. train loss: 1.3160,	0.4960 s / batch. (data: 2.70e-04). ETA=6:12:22, max mem: 11.4 GB 
[10/30 04:12:53 visual_prompt]: 	Training 400/553. train loss: 0.4630,	0.4789 s / batch. (data: 2.77e-04). ETA=5:58:42, max mem: 11.4 GB 
[10/30 04:14:21 visual_prompt]: 	Training 500/553. train loss: 3.5458,	0.5120 s / batch. (data: 2.92e-04). ETA=6:22:41, max mem: 11.4 GB 
[10/30 04:15:07 visual_prompt]: Epoch 19 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 1.5573
[10/30 04:15:59 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1910, average loss: 1.2006
[10/30 04:15:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.02	
[10/30 04:17:26 visual_prompt]: 	Test 100/162. loss: 0.914, 0.1855 s / batch. (data: 3.08e-05)max mem: 11.41573 GB 
[10/30 04:18:13 visual_prompt]: Inference (test):avg data time: 7.76e-05, avg batch time: 0.1924, average loss: 1.3398
[10/30 04:18:13 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 60.27	
[10/30 04:18:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/30 04:19:46 visual_prompt]: 	Training 100/553. train loss: 1.2673,	0.5393 s / batch. (data: 4.30e-02). ETA=6:41:44, max mem: 11.4 GB 
[10/30 04:21:12 visual_prompt]: 	Training 200/553. train loss: 0.9455,	0.4920 s / batch. (data: 2.66e-04). ETA=6:05:39, max mem: 11.4 GB 
[10/30 04:22:41 visual_prompt]: 	Training 300/553. train loss: 3.0542,	1.8773 s / batch. (data: 1.39e+00). ETA=23:12:06, max mem: 11.4 GB 
[10/30 04:24:11 visual_prompt]: 	Training 400/553. train loss: 2.6342,	0.5046 s / batch. (data: 1.55e-02). ETA=6:13:22, max mem: 11.4 GB 
[10/30 04:25:40 visual_prompt]: 	Training 500/553. train loss: 0.8810,	0.4842 s / batch. (data: 2.77e-04). ETA=5:57:26, max mem: 11.4 GB 
[10/30 04:26:24 visual_prompt]: Epoch 20 / 100: avg data time: 3.92e-01, avg batch time: 0.8872, average train loss: 1.3697
[10/30 04:27:17 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1911, average loss: 0.7256
[10/30 04:27:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 60.76	
[10/30 04:28:43 visual_prompt]: 	Test 100/162. loss: 0.382, 0.1957 s / batch. (data: 4.82e-05)max mem: 11.41573 GB 
[10/30 04:29:32 visual_prompt]: Inference (test):avg data time: 3.62e-05, avg batch time: 0.1914, average loss: 0.6895
[10/30 04:29:32 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.00	rocauc: 61.63	
[10/30 04:29:32 visual_prompt]: Best epoch 20: best metric: -0.726
[10/30 04:29:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[10/30 04:31:04 visual_prompt]: 	Training 100/553. train loss: 1.9605,	1.0560 s / batch. (data: 5.68e-01). ETA=12:56:50, max mem: 11.4 GB 
[10/30 04:32:30 visual_prompt]: 	Training 200/553. train loss: 1.0960,	0.4920 s / batch. (data: 2.70e-04). ETA=6:01:07, max mem: 11.4 GB 
[10/30 04:33:58 visual_prompt]: 	Training 300/553. train loss: 0.5783,	0.5120 s / batch. (data: 1.20e-02). ETA=6:14:59, max mem: 11.4 GB 
[10/30 04:35:27 visual_prompt]: 	Training 400/553. train loss: 0.4933,	0.4911 s / batch. (data: 2.72e-04). ETA=5:58:51, max mem: 11.4 GB 
[10/30 04:36:59 visual_prompt]: 	Training 500/553. train loss: 2.4973,	0.4911 s / batch. (data: 2.48e-04). ETA=5:57:58, max mem: 11.4 GB 
[10/30 04:37:45 visual_prompt]: Epoch 21 / 100: avg data time: 3.98e-01, avg batch time: 0.8922, average train loss: 1.3528
[10/30 04:38:38 visual_prompt]: Inference (val):avg data time: 1.67e-04, avg batch time: 0.1927, average loss: 1.2517
[10/30 04:38:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.76	
[10/30 04:40:06 visual_prompt]: 	Test 100/162. loss: 0.916, 0.1947 s / batch. (data: 3.39e-05)max mem: 11.41573 GB 
[10/30 04:40:54 visual_prompt]: Inference (test):avg data time: 3.32e-05, avg batch time: 0.1931, average loss: 1.3662
[10/30 04:40:54 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 59.86	
[10/30 04:40:54 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[10/30 04:42:26 visual_prompt]: 	Training 100/553. train loss: 0.1555,	0.5042 s / batch. (data: 2.38e-02). ETA=6:06:15, max mem: 11.4 GB 
[10/30 04:43:54 visual_prompt]: 	Training 200/553. train loss: 0.6940,	0.4840 s / batch. (data: 2.49e-04). ETA=5:50:48, max mem: 11.4 GB 
[10/30 04:45:26 visual_prompt]: 	Training 300/553. train loss: 1.3887,	0.4890 s / batch. (data: 2.60e-04). ETA=5:53:34, max mem: 11.4 GB 
[10/30 04:46:54 visual_prompt]: 	Training 400/553. train loss: 0.5261,	0.5039 s / batch. (data: 6.51e-04). ETA=6:03:34, max mem: 11.4 GB 
[10/30 04:48:22 visual_prompt]: 	Training 500/553. train loss: 9.0321,	0.5000 s / batch. (data: 3.53e-04). ETA=5:59:52, max mem: 11.4 GB 
[10/30 04:49:08 visual_prompt]: Epoch 22 / 100: avg data time: 4.00e-01, avg batch time: 0.8944, average train loss: 1.5932
[10/30 04:50:01 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1912, average loss: 0.8174
[10/30 04:50:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.67	
[10/30 04:51:27 visual_prompt]: 	Test 100/162. loss: 0.631, 0.1857 s / batch. (data: 3.67e-05)max mem: 11.41573 GB 
[10/30 04:52:15 visual_prompt]: Inference (test):avg data time: 2.33e-04, avg batch time: 0.1926, average loss: 0.7546
[10/30 04:52:15 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 60.40	
[10/30 04:52:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[10/30 04:53:45 visual_prompt]: 	Training 100/553. train loss: 1.2778,	0.4953 s / batch. (data: 4.50e-03). ETA=5:55:16, max mem: 11.4 GB 
[10/30 04:55:16 visual_prompt]: 	Training 200/553. train loss: 0.8424,	2.1913 s / batch. (data: 1.71e+00). ETA=1 day, 2:08:00, max mem: 11.4 GB 
[10/30 04:56:43 visual_prompt]: 	Training 300/553. train loss: 0.9789,	0.5169 s / batch. (data: 5.93e-03). ETA=6:09:01, max mem: 11.4 GB 
[10/30 04:58:13 visual_prompt]: 	Training 400/553. train loss: 0.7435,	0.4848 s / batch. (data: 3.02e-04). ETA=5:45:19, max mem: 11.4 GB 
[10/30 04:59:42 visual_prompt]: 	Training 500/553. train loss: 0.6923,	1.9899 s / batch. (data: 1.51e+00). ETA=23:33:59, max mem: 11.4 GB 
[10/30 05:00:26 visual_prompt]: Epoch 23 / 100: avg data time: 3.92e-01, avg batch time: 0.8873, average train loss: 1.1862
[10/30 05:01:18 visual_prompt]: Inference (val):avg data time: 2.92e-04, avg batch time: 0.1916, average loss: 0.8753
[10/30 05:01:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 63.27	
[10/30 05:02:44 visual_prompt]: 	Test 100/162. loss: 0.513, 0.1956 s / batch. (data: 2.69e-05)max mem: 11.41573 GB 
[10/30 05:03:33 visual_prompt]: Inference (test):avg data time: 1.56e-04, avg batch time: 0.1914, average loss: 0.9753
[10/30 05:03:33 visual_prompt]: Classification results with test_mammo-cbis: top1: 45.89	rocauc: 61.43	
[10/30 05:03:33 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[10/30 05:05:07 visual_prompt]: 	Training 100/553. train loss: 0.6139,	0.4840 s / batch. (data: 2.86e-04). ETA=5:42:40, max mem: 11.4 GB 
[10/30 05:06:36 visual_prompt]: 	Training 200/553. train loss: 1.4779,	0.4923 s / batch. (data: 2.70e-04). ETA=5:47:45, max mem: 11.4 GB 
[10/30 05:08:05 visual_prompt]: 	Training 300/553. train loss: 2.0859,	0.4964 s / batch. (data: 1.04e-02). ETA=5:49:47, max mem: 11.4 GB 
[10/30 05:09:35 visual_prompt]: 	Training 400/553. train loss: 2.9514,	0.5202 s / batch. (data: 2.42e-02). ETA=6:05:41, max mem: 11.4 GB 
[10/30 05:11:01 visual_prompt]: 	Training 500/553. train loss: 0.2176,	0.4911 s / batch. (data: 2.72e-04). ETA=5:44:28, max mem: 11.4 GB 
[10/30 05:11:46 visual_prompt]: Epoch 24 / 100: avg data time: 3.98e-01, avg batch time: 0.8925, average train loss: 1.4568
[10/30 05:12:39 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1913, average loss: 1.0978
[10/30 05:12:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.66	
[10/30 05:14:06 visual_prompt]: 	Test 100/162. loss: 0.848, 0.2017 s / batch. (data: 3.58e-05)max mem: 11.41573 GB 
[10/30 05:14:54 visual_prompt]: Inference (test):avg data time: 1.03e-04, avg batch time: 0.1923, average loss: 1.1861
[10/30 05:14:54 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 62.82	
[10/30 05:14:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[10/30 05:16:29 visual_prompt]: 	Training 100/553. train loss: 0.6009,	0.5320 s / batch. (data: 3.02e-02). ETA=6:11:44, max mem: 11.4 GB 
[10/30 05:17:57 visual_prompt]: 	Training 200/553. train loss: 2.9475,	1.5771 s / batch. (data: 1.08e+00). ETA=18:19:25, max mem: 11.4 GB 
[10/30 05:19:27 visual_prompt]: 	Training 300/553. train loss: 1.3628,	0.5113 s / batch. (data: 5.34e-03). ETA=5:55:34, max mem: 11.4 GB 
[10/30 05:20:53 visual_prompt]: 	Training 400/553. train loss: 0.7051,	0.4960 s / batch. (data: 2.83e-04). ETA=5:44:06, max mem: 11.4 GB 
[10/30 05:22:23 visual_prompt]: 	Training 500/553. train loss: 1.6762,	0.5047 s / batch. (data: 2.65e-04). ETA=5:49:20, max mem: 11.4 GB 
[10/30 05:23:08 visual_prompt]: Epoch 25 / 100: avg data time: 3.97e-01, avg batch time: 0.8925, average train loss: 1.3541
[10/30 05:24:01 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1905, average loss: 0.9901
[10/30 05:24:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 60.29	
[10/30 05:25:28 visual_prompt]: 	Test 100/162. loss: 0.676, 0.1855 s / batch. (data: 3.43e-05)max mem: 11.41573 GB 
[10/30 05:26:16 visual_prompt]: Inference (test):avg data time: 3.15e-05, avg batch time: 0.1926, average loss: 0.8953
[10/30 05:26:16 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 60.09	
[10/30 05:26:16 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[10/30 05:27:50 visual_prompt]: 	Training 100/553. train loss: 0.6129,	2.6321 s / batch. (data: 2.14e+00). ETA=1 day, 6:15:01, max mem: 11.4 GB 
[10/30 05:29:18 visual_prompt]: 	Training 200/553. train loss: 1.7804,	0.9880 s / batch. (data: 5.01e-01). ETA=11:19:39, max mem: 11.4 GB 
[10/30 05:30:47 visual_prompt]: 	Training 300/553. train loss: 0.7053,	0.4791 s / batch. (data: 2.65e-04). ETA=5:28:45, max mem: 11.4 GB 
[10/30 05:32:15 visual_prompt]: 	Training 400/553. train loss: 0.7528,	1.9634 s / batch. (data: 1.45e+00). ETA=22:24:08, max mem: 11.4 GB 
[10/30 05:33:44 visual_prompt]: 	Training 500/553. train loss: 1.4278,	1.5157 s / batch. (data: 1.03e+00). ETA=17:15:07, max mem: 11.4 GB 
[10/30 05:34:30 visual_prompt]: Epoch 26 / 100: avg data time: 3.98e-01, avg batch time: 0.8921, average train loss: 1.1344
[10/30 05:35:22 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.1914, average loss: 0.6885
[10/30 05:35:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.88	
[10/30 05:36:50 visual_prompt]: 	Test 100/162. loss: 0.416, 0.1996 s / batch. (data: 4.94e-05)max mem: 11.41573 GB 
[10/30 05:37:38 visual_prompt]: Inference (test):avg data time: 3.30e-05, avg batch time: 0.1929, average loss: 0.6696
[10/30 05:37:38 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 60.50	
[10/30 05:37:38 visual_prompt]: Best epoch 26: best metric: -0.688
[10/30 05:37:38 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[10/30 05:39:11 visual_prompt]: 	Training 100/553. train loss: 3.3269,	1.0663 s / batch. (data: 5.67e-01). ETA=12:05:26, max mem: 11.4 GB 
[10/30 05:40:43 visual_prompt]: 	Training 200/553. train loss: 0.8477,	0.5360 s / batch. (data: 7.24e-04). ETA=6:03:46, max mem: 11.4 GB 
[10/30 05:42:11 visual_prompt]: 	Training 300/553. train loss: 1.2114,	0.5188 s / batch. (data: 7.98e-03). ETA=5:51:15, max mem: 11.4 GB 
[10/30 05:43:39 visual_prompt]: 	Training 400/553. train loss: 0.4988,	0.4880 s / batch. (data: 2.62e-04). ETA=5:29:33, max mem: 11.4 GB 
[10/30 05:45:06 visual_prompt]: 	Training 500/553. train loss: 0.4877,	1.0365 s / batch. (data: 5.28e-01). ETA=11:38:15, max mem: 11.4 GB 
[10/30 05:45:53 visual_prompt]: Epoch 27 / 100: avg data time: 4.00e-01, avg batch time: 0.8944, average train loss: 1.6960
[10/30 05:46:46 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1914, average loss: 1.8192
[10/30 05:46:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.20	
[10/30 05:48:12 visual_prompt]: 	Test 100/162. loss: 1.738, 0.1905 s / batch. (data: 3.48e-05)max mem: 11.41573 GB 
[10/30 05:49:01 visual_prompt]: Inference (test):avg data time: 3.20e-05, avg batch time: 0.1920, average loss: 1.6362
[10/30 05:49:01 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.60	
[10/30 05:49:01 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[10/30 05:50:34 visual_prompt]: 	Training 100/553. train loss: 1.4331,	2.0764 s / batch. (data: 1.59e+00). ETA=23:13:35, max mem: 11.4 GB 
[10/30 05:52:04 visual_prompt]: 	Training 200/553. train loss: 1.4888,	0.5040 s / batch. (data: 7.30e-04). ETA=5:37:25, max mem: 11.4 GB 
[10/30 05:53:31 visual_prompt]: 	Training 300/553. train loss: 1.3028,	0.4880 s / batch. (data: 7.96e-03). ETA=5:25:52, max mem: 11.4 GB 
[10/30 05:54:59 visual_prompt]: 	Training 400/553. train loss: 3.2510,	0.5085 s / batch. (data: 2.59e-04). ETA=5:38:46, max mem: 11.4 GB 
[10/30 05:56:27 visual_prompt]: 	Training 500/553. train loss: 3.0596,	0.5141 s / batch. (data: 2.63e-04). ETA=5:41:36, max mem: 11.4 GB 
[10/30 05:57:15 visual_prompt]: Epoch 28 / 100: avg data time: 3.98e-01, avg batch time: 0.8927, average train loss: 1.4798
[10/30 05:58:07 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1904, average loss: 0.7228
[10/30 05:58:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 59.12	
[10/30 05:59:34 visual_prompt]: 	Test 100/162. loss: 0.445, 0.1858 s / batch. (data: 3.43e-05)max mem: 11.41573 GB 
[10/30 06:00:23 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.1924, average loss: 0.6840
[10/30 06:00:23 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.00	rocauc: 61.22	
[10/30 06:00:23 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[10/30 06:01:54 visual_prompt]: 	Training 100/553. train loss: 0.1167,	1.6080 s / batch. (data: 1.11e+00). ETA=17:44:23, max mem: 11.4 GB 
[10/30 06:03:26 visual_prompt]: 	Training 200/553. train loss: 1.8167,	0.5040 s / batch. (data: 5.38e-03). ETA=5:32:47, max mem: 11.4 GB 
[10/30 06:04:54 visual_prompt]: 	Training 300/553. train loss: 0.5082,	0.4840 s / batch. (data: 2.69e-04). ETA=5:18:46, max mem: 11.4 GB 
[10/30 06:06:19 visual_prompt]: 	Training 400/553. train loss: 2.9348,	0.4931 s / batch. (data: 1.04e-02). ETA=5:23:57, max mem: 11.4 GB 
[10/30 06:07:49 visual_prompt]: 	Training 500/553. train loss: 2.8212,	1.8795 s / batch. (data: 1.39e+00). ETA=20:31:34, max mem: 11.4 GB 
[10/30 06:08:35 visual_prompt]: Epoch 29 / 100: avg data time: 3.97e-01, avg batch time: 0.8906, average train loss: 1.3082
[10/30 06:09:28 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.1901, average loss: 2.0332
[10/30 06:09:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.02	
[10/30 06:10:54 visual_prompt]: 	Test 100/162. loss: 1.563, 0.2198 s / batch. (data: 3.86e-05)max mem: 11.41573 GB 
[10/30 06:11:43 visual_prompt]: Inference (test):avg data time: 3.72e-05, avg batch time: 0.1922, average loss: 2.3151
[10/30 06:11:43 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 60.66	
[10/30 06:11:43 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[10/30 06:13:17 visual_prompt]: 	Training 100/553. train loss: 0.9006,	0.4799 s / batch. (data: 2.53e-04). ETA=5:13:12, max mem: 11.4 GB 
[10/30 06:14:42 visual_prompt]: 	Training 200/553. train loss: 2.6382,	1.1594 s / batch. (data: 6.65e-01). ETA=12:34:50, max mem: 11.4 GB 
[10/30 06:16:11 visual_prompt]: 	Training 300/553. train loss: 1.3515,	0.9760 s / batch. (data: 4.91e-01). ETA=10:33:47, max mem: 11.4 GB 
[10/30 06:17:38 visual_prompt]: 	Training 400/553. train loss: 4.3795,	1.4155 s / batch. (data: 9.20e-01). ETA=15:16:49, max mem: 11.4 GB 
[10/30 06:19:09 visual_prompt]: 	Training 500/553. train loss: 0.1377,	0.5043 s / batch. (data: 1.22e-02). ETA=5:25:47, max mem: 11.4 GB 
[10/30 06:19:54 visual_prompt]: Epoch 30 / 100: avg data time: 3.93e-01, avg batch time: 0.8876, average train loss: 1.3687
[10/30 06:20:46 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.1922, average loss: 2.3544
[10/30 06:20:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.67	
[10/30 06:22:11 visual_prompt]: 	Test 100/162. loss: 2.330, 0.1861 s / batch. (data: 3.58e-05)max mem: 11.41573 GB 
[10/30 06:23:00 visual_prompt]: Inference (test):avg data time: 3.82e-05, avg batch time: 0.1928, average loss: 2.1312
[10/30 06:23:00 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 60.44	
[10/30 06:23:00 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[10/30 06:24:33 visual_prompt]: 	Training 100/553. train loss: 0.6581,	0.5031 s / batch. (data: 2.66e-04). ETA=5:23:45, max mem: 11.4 GB 
[10/30 06:26:00 visual_prompt]: 	Training 200/553. train loss: 0.7324,	0.4790 s / batch. (data: 2.64e-04). ETA=5:07:25, max mem: 11.4 GB 
[10/30 06:27:30 visual_prompt]: 	Training 300/553. train loss: 1.8019,	1.5067 s / batch. (data: 1.02e+00). ETA=16:04:33, max mem: 11.4 GB 
[10/30 06:29:00 visual_prompt]: 	Training 400/553. train loss: 0.6768,	2.1480 s / batch. (data: 1.67e+00). ETA=22:51:29, max mem: 11.4 GB 
[10/30 06:30:28 visual_prompt]: 	Training 500/553. train loss: 1.0993,	0.4928 s / batch. (data: 5.36e-03). ETA=5:13:51, max mem: 11.4 GB 
[10/30 06:31:12 visual_prompt]: Epoch 31 / 100: avg data time: 3.94e-01, avg batch time: 0.8890, average train loss: 1.1873
[10/30 06:32:05 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1909, average loss: 1.0098
[10/30 06:32:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.18	
[10/30 06:33:31 visual_prompt]: 	Test 100/162. loss: 0.870, 0.1851 s / batch. (data: 3.00e-05)max mem: 11.41573 GB 
[10/30 06:34:19 visual_prompt]: Inference (test):avg data time: 3.51e-05, avg batch time: 0.1918, average loss: 1.0792
[10/30 06:34:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 58.03	
[10/30 06:34:19 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[10/30 06:35:52 visual_prompt]: 	Training 100/553. train loss: 1.0445,	0.5080 s / batch. (data: 2.63e-04). ETA=5:22:13, max mem: 11.4 GB 
[10/30 06:37:23 visual_prompt]: 	Training 200/553. train loss: 1.9476,	0.5308 s / batch. (data: 6.48e-04). ETA=5:35:47, max mem: 11.4 GB 
[10/30 06:38:49 visual_prompt]: 	Training 300/553. train loss: 0.8108,	0.4960 s / batch. (data: 2.88e-04). ETA=5:12:56, max mem: 11.4 GB 
[10/30 06:40:19 visual_prompt]: 	Training 400/553. train loss: 0.8744,	0.5000 s / batch. (data: 7.96e-03). ETA=5:14:38, max mem: 11.4 GB 
[10/30 06:41:47 visual_prompt]: 	Training 500/553. train loss: 0.9312,	2.0398 s / batch. (data: 1.56e+00). ETA=21:20:14, max mem: 11.4 GB 
[10/30 06:42:31 visual_prompt]: Epoch 32 / 100: avg data time: 3.95e-01, avg batch time: 0.8891, average train loss: 1.3026
[10/30 06:43:23 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1906, average loss: 0.7383
[10/30 06:43:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.14	
[10/30 06:44:49 visual_prompt]: 	Test 100/162. loss: 0.348, 0.1855 s / batch. (data: 3.27e-05)max mem: 11.41573 GB 
[10/30 06:45:38 visual_prompt]: Inference (test):avg data time: 3.63e-05, avg batch time: 0.1919, average loss: 0.7159
[10/30 06:45:38 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.00	rocauc: 60.81	
[10/30 06:45:38 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[10/30 06:47:10 visual_prompt]: 	Training 100/553. train loss: 0.6812,	2.5760 s / batch. (data: 2.05e+00). ETA=1 day, 2:50:09, max mem: 11.4 GB 
[10/30 06:48:39 visual_prompt]: 	Training 200/553. train loss: 2.2884,	0.6600 s / batch. (data: 1.62e-01). ETA=6:51:26, max mem: 11.4 GB 
[10/30 06:50:09 visual_prompt]: 	Training 300/553. train loss: 1.1203,	0.5281 s / batch. (data: 1.05e-02). ETA=5:28:20, max mem: 11.4 GB 
[10/30 06:51:36 visual_prompt]: 	Training 400/553. train loss: 0.0003,	0.4962 s / batch. (data: 5.41e-03). ETA=5:07:42, max mem: 11.4 GB 
[10/30 06:53:05 visual_prompt]: 	Training 500/553. train loss: 0.6262,	2.6754 s / batch. (data: 2.19e+00). ETA=1 day, 3:34:26, max mem: 11.4 GB 
[10/30 06:53:49 visual_prompt]: Epoch 33 / 100: avg data time: 3.94e-01, avg batch time: 0.8882, average train loss: 1.1268
[10/30 06:54:41 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1906, average loss: 0.6828
[10/30 06:54:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 58.86	
[10/30 06:56:08 visual_prompt]: 	Test 100/162. loss: 0.522, 0.1921 s / batch. (data: 3.19e-05)max mem: 11.41573 GB 
[10/30 06:56:56 visual_prompt]: Inference (test):avg data time: 3.58e-05, avg batch time: 0.1921, average loss: 0.7049
[10/30 06:56:56 visual_prompt]: Classification results with test_mammo-cbis: top1: 55.19	rocauc: 55.77	
[10/30 06:56:56 visual_prompt]: Best epoch 33: best metric: -0.683
[10/30 06:56:56 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[10/30 06:58:27 visual_prompt]: 	Training 100/553. train loss: 0.8592,	0.4800 s / batch. (data: 2.51e-04). ETA=4:55:37, max mem: 11.4 GB 
[10/30 06:59:56 visual_prompt]: 	Training 200/553. train loss: 0.8568,	0.4806 s / batch. (data: 2.48e-04). ETA=4:55:09, max mem: 11.4 GB 
[10/30 07:01:24 visual_prompt]: 	Training 300/553. train loss: 3.2132,	0.5044 s / batch. (data: 5.41e-03). ETA=5:08:57, max mem: 11.4 GB 
[10/30 07:02:54 visual_prompt]: 	Training 400/553. train loss: 0.0101,	0.5175 s / batch. (data: 1.55e-02). ETA=5:16:06, max mem: 11.4 GB 
[10/30 07:04:22 visual_prompt]: 	Training 500/553. train loss: 2.4649,	0.5170 s / batch. (data: 2.53e-02). ETA=5:14:57, max mem: 11.4 GB 
[10/30 07:05:07 visual_prompt]: Epoch 34 / 100: avg data time: 3.92e-01, avg batch time: 0.8867, average train loss: 1.3430
[10/30 07:05:59 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1909, average loss: 1.3734
[10/30 07:05:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.98	
[10/30 07:07:25 visual_prompt]: 	Test 100/162. loss: 1.148, 0.1855 s / batch. (data: 4.89e-05)max mem: 11.41573 GB 
[10/30 07:08:13 visual_prompt]: Inference (test):avg data time: 3.37e-05, avg batch time: 0.1932, average loss: 1.5062
[10/30 07:08:13 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 60.76	
[10/30 07:08:13 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[10/30 07:09:45 visual_prompt]: 	Training 100/553. train loss: 0.6457,	0.7800 s / batch. (data: 2.88e-01). ETA=7:53:11, max mem: 11.4 GB 
[10/30 07:11:15 visual_prompt]: 	Training 200/553. train loss: 1.6976,	0.5170 s / batch. (data: 7.98e-03). ETA=5:12:44, max mem: 11.4 GB 
[10/30 07:12:43 visual_prompt]: 	Training 300/553. train loss: 0.1311,	0.4995 s / batch. (data: 1.04e-02). ETA=5:01:20, max mem: 11.4 GB 
[10/30 07:14:11 visual_prompt]: 	Training 400/553. train loss: 0.8774,	0.4787 s / batch. (data: 3.02e-04). ETA=4:48:00, max mem: 11.4 GB 
[10/30 07:15:39 visual_prompt]: 	Training 500/553. train loss: 1.5881,	0.5087 s / batch. (data: 8.65e-03). ETA=5:05:12, max mem: 11.4 GB 
[10/30 07:16:24 visual_prompt]: Epoch 35 / 100: avg data time: 3.93e-01, avg batch time: 0.8869, average train loss: 1.1446
[10/30 07:17:16 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1914, average loss: 1.9823
[10/30 07:17:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.46	
[10/30 07:18:43 visual_prompt]: 	Test 100/162. loss: 1.616, 0.1915 s / batch. (data: 3.08e-05)max mem: 11.41573 GB 
[10/30 07:19:31 visual_prompt]: Inference (test):avg data time: 3.44e-05, avg batch time: 0.1934, average loss: 1.7575
[10/30 07:19:31 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 61.95	
[10/30 07:19:31 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[10/30 07:21:04 visual_prompt]: 	Training 100/553. train loss: 0.9214,	0.5040 s / batch. (data: 4.11e-04). ETA=5:01:06, max mem: 11.4 GB 
[10/30 07:22:32 visual_prompt]: 	Training 200/553. train loss: 2.7324,	0.5093 s / batch. (data: 2.82e-04). ETA=5:03:24, max mem: 11.4 GB 
[10/30 07:24:03 visual_prompt]: 	Training 300/553. train loss: 1.0211,	0.6681 s / batch. (data: 1.82e-01). ETA=6:36:52, max mem: 11.4 GB 
[10/30 07:25:32 visual_prompt]: 	Training 400/553. train loss: 3.9796,	0.5000 s / batch. (data: 7.53e-04). ETA=4:56:10, max mem: 11.4 GB 
[10/30 07:26:58 visual_prompt]: 	Training 500/553. train loss: 0.4051,	0.4965 s / batch. (data: 2.61e-04). ETA=4:53:18, max mem: 11.4 GB 
[10/30 07:27:43 visual_prompt]: Epoch 36 / 100: avg data time: 3.95e-01, avg batch time: 0.8903, average train loss: 1.1844
[10/30 07:28:36 visual_prompt]: Inference (val):avg data time: 1.72e-04, avg batch time: 0.1923, average loss: 0.6608
[10/30 07:28:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 63.79	
[10/30 07:30:02 visual_prompt]: 	Test 100/162. loss: 0.429, 0.1854 s / batch. (data: 3.24e-05)max mem: 11.41573 GB 
[10/30 07:30:50 visual_prompt]: Inference (test):avg data time: 1.31e-04, avg batch time: 0.1925, average loss: 0.6668
[10/30 07:30:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.45	rocauc: 61.49	
[10/30 07:30:50 visual_prompt]: Best epoch 36: best metric: -0.661
[10/30 07:30:50 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[10/30 07:32:21 visual_prompt]: 	Training 100/553. train loss: 0.0297,	0.5159 s / batch. (data: 1.19e-02). ETA=5:03:27, max mem: 11.4 GB 
[10/30 07:33:49 visual_prompt]: 	Training 200/553. train loss: 0.6681,	0.5040 s / batch. (data: 5.42e-03). ETA=4:55:35, max mem: 11.4 GB 
[10/30 07:35:18 visual_prompt]: 	Training 300/553. train loss: 1.9748,	2.0677 s / batch. (data: 1.57e+00). ETA=20:09:18, max mem: 11.4 GB 
[10/30 07:36:46 visual_prompt]: 	Training 400/553. train loss: 0.9972,	0.5027 s / batch. (data: 6.43e-03). ETA=4:53:09, max mem: 11.4 GB 
[10/30 07:38:16 visual_prompt]: 	Training 500/553. train loss: 0.6559,	0.5008 s / batch. (data: 2.28e-04). ETA=4:51:14, max mem: 11.4 GB 
[10/30 07:39:01 visual_prompt]: Epoch 37 / 100: avg data time: 3.92e-01, avg batch time: 0.8863, average train loss: 1.1328
[10/30 07:39:53 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1897, average loss: 1.4633
[10/30 07:39:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.95	
[10/30 07:41:19 visual_prompt]: 	Test 100/162. loss: 0.978, 0.1857 s / batch. (data: 4.94e-05)max mem: 11.41573 GB 
[10/30 07:42:07 visual_prompt]: Inference (test):avg data time: 3.36e-05, avg batch time: 0.1916, average loss: 1.6523
[10/30 07:42:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 64.44	
[10/30 07:42:07 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[10/30 07:43:37 visual_prompt]: 	Training 100/553. train loss: 0.7781,	0.4799 s / batch. (data: 2.96e-04). ETA=4:37:50, max mem: 11.4 GB 
[10/30 07:45:09 visual_prompt]: 	Training 200/553. train loss: 2.2686,	0.4967 s / batch. (data: 2.42e-04). ETA=4:46:43, max mem: 11.4 GB 
[10/30 07:46:36 visual_prompt]: 	Training 300/553. train loss: 0.6016,	0.5040 s / batch. (data: 2.69e-04). ETA=4:50:08, max mem: 11.4 GB 
[10/30 07:48:04 visual_prompt]: 	Training 400/553. train loss: 0.6174,	0.4933 s / batch. (data: 4.87e-04). ETA=4:43:09, max mem: 11.4 GB 
[10/30 07:49:34 visual_prompt]: 	Training 500/553. train loss: 0.5592,	0.4809 s / batch. (data: 2.80e-04). ETA=4:35:12, max mem: 11.4 GB 
[10/30 07:50:18 visual_prompt]: Epoch 38 / 100: avg data time: 3.93e-01, avg batch time: 0.8874, average train loss: 1.1021
[10/30 07:51:10 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1918, average loss: 0.8447
[10/30 07:51:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 63.68	
[10/30 07:52:36 visual_prompt]: 	Test 100/162. loss: 0.569, 0.2096 s / batch. (data: 3.12e-05)max mem: 11.41573 GB 
[10/30 07:53:25 visual_prompt]: Inference (test):avg data time: 3.25e-05, avg batch time: 0.1927, average loss: 0.9129
[10/30 07:53:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 52.87	rocauc: 63.39	
[10/30 07:53:25 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[10/30 07:54:59 visual_prompt]: 	Training 100/553. train loss: 1.1656,	0.5240 s / batch. (data: 7.02e-04). ETA=4:58:34, max mem: 11.4 GB 
[10/30 07:56:25 visual_prompt]: 	Training 200/553. train loss: 0.7266,	0.4960 s / batch. (data: 2.96e-04). ETA=4:41:45, max mem: 11.4 GB 
[10/30 07:57:56 visual_prompt]: 	Training 300/553. train loss: 2.1052,	1.0709 s / batch. (data: 5.94e-01). ETA=10:06:35, max mem: 11.4 GB 
[10/30 07:59:26 visual_prompt]: 	Training 400/553. train loss: 0.7028,	0.5240 s / batch. (data: 1.08e-03). ETA=4:55:54, max mem: 11.4 GB 
[10/30 08:00:52 visual_prompt]: 	Training 500/553. train loss: 0.6355,	0.5186 s / batch. (data: 5.39e-03). ETA=4:51:59, max mem: 11.4 GB 
[10/30 08:01:36 visual_prompt]: Epoch 39 / 100: avg data time: 3.93e-01, avg batch time: 0.8876, average train loss: 1.1350
[10/30 08:02:28 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1902, average loss: 0.7029
[10/30 08:02:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 62.36	
[10/30 08:03:55 visual_prompt]: 	Test 100/162. loss: 0.582, 0.1854 s / batch. (data: 4.01e-05)max mem: 11.41573 GB 
[10/30 08:04:44 visual_prompt]: Inference (test):avg data time: 1.02e-04, avg batch time: 0.1933, average loss: 0.6531
[10/30 08:04:44 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.93	rocauc: 64.82	
[10/30 08:04:44 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[10/30 08:06:16 visual_prompt]: 	Training 100/553. train loss: 0.9390,	0.5114 s / batch. (data: 5.43e-03). ETA=4:46:40, max mem: 11.4 GB 
[10/30 08:07:47 visual_prompt]: 	Training 200/553. train loss: 0.4890,	0.5000 s / batch. (data: 2.76e-04). ETA=4:39:27, max mem: 11.4 GB 
[10/30 08:09:15 visual_prompt]: 	Training 300/553. train loss: 2.1835,	0.5010 s / batch. (data: 2.84e-04). ETA=4:39:10, max mem: 11.4 GB 
[10/30 08:10:44 visual_prompt]: 	Training 400/553. train loss: 1.2804,	0.4920 s / batch. (data: 2.84e-04). ETA=4:33:19, max mem: 11.4 GB 
[10/30 08:12:13 visual_prompt]: 	Training 500/553. train loss: 0.5712,	0.5229 s / batch. (data: 7.02e-04). ETA=4:49:38, max mem: 11.4 GB 
[10/30 08:12:58 visual_prompt]: Epoch 40 / 100: avg data time: 3.99e-01, avg batch time: 0.8935, average train loss: 1.0908
[10/30 08:13:51 visual_prompt]: Inference (val):avg data time: 2.47e-04, avg batch time: 0.1916, average loss: 0.7774
[10/30 08:13:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.69	
[10/30 08:15:19 visual_prompt]: 	Test 100/162. loss: 0.661, 0.1978 s / batch. (data: 3.19e-05)max mem: 11.41573 GB 
[10/30 08:16:06 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.1923, average loss: 0.8104
[10/30 08:16:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.36	rocauc: 61.52	
[10/30 08:16:06 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[10/30 08:17:38 visual_prompt]: 	Training 100/553. train loss: 0.0212,	2.0924 s / batch. (data: 1.60e+00). ETA=19:13:35, max mem: 11.4 GB 
[10/30 08:19:06 visual_prompt]: 	Training 200/553. train loss: 0.0072,	0.4917 s / batch. (data: 2.63e-04). ETA=4:30:15, max mem: 11.4 GB 
[10/30 08:20:35 visual_prompt]: 	Training 300/553. train loss: 1.4384,	0.5077 s / batch. (data: 2.73e-02). ETA=4:38:13, max mem: 11.4 GB 
[10/30 08:22:04 visual_prompt]: 	Training 400/553. train loss: 0.8436,	1.6243 s / batch. (data: 1.15e+00). ETA=14:47:25, max mem: 11.4 GB 
[10/30 08:23:35 visual_prompt]: 	Training 500/553. train loss: 0.8137,	1.3880 s / batch. (data: 8.83e-01). ETA=12:35:59, max mem: 11.4 GB 
[10/30 08:24:18 visual_prompt]: Epoch 41 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 1.0319
[10/30 08:25:10 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1916, average loss: 1.2114
[10/30 08:25:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.97	
[10/30 08:26:40 visual_prompt]: 	Test 100/162. loss: 1.024, 0.1935 s / batch. (data: 4.98e-05)max mem: 11.41573 GB 
[10/30 08:27:34 visual_prompt]: Inference (test):avg data time: 3.39e-05, avg batch time: 0.1910, average loss: 1.3223
[10/30 08:27:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 62.17	
[10/30 08:27:34 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[10/30 08:29:10 visual_prompt]: 	Training 100/553. train loss: 0.6689,	0.4806 s / batch. (data: 2.66e-04). ETA=4:20:31, max mem: 11.4 GB 
[10/30 08:30:45 visual_prompt]: 	Training 200/553. train loss: 0.9343,	0.4783 s / batch. (data: 2.83e-04). ETA=4:18:31, max mem: 11.4 GB 
[10/30 08:32:19 visual_prompt]: 	Training 300/553. train loss: 1.2474,	0.5000 s / batch. (data: 3.81e-04). ETA=4:29:23, max mem: 11.4 GB 
[10/30 08:33:54 visual_prompt]: 	Training 400/553. train loss: 0.6799,	0.4963 s / batch. (data: 5.39e-03). ETA=4:26:35, max mem: 11.4 GB 
[10/30 08:35:28 visual_prompt]: 	Training 500/553. train loss: 0.3726,	0.4913 s / batch. (data: 2.99e-04). ETA=4:23:04, max mem: 11.4 GB 
[10/30 08:36:15 visual_prompt]: Epoch 42 / 100: avg data time: 4.48e-01, avg batch time: 0.9417, average train loss: 1.1347
[10/30 08:37:10 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1913, average loss: 0.7240
[10/30 08:37:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 62.34	
[10/30 08:38:42 visual_prompt]: 	Test 100/162. loss: 0.312, 0.1925 s / batch. (data: 5.34e-05)max mem: 11.41573 GB 
[10/30 08:39:34 visual_prompt]: Inference (test):avg data time: 3.29e-05, avg batch time: 0.1912, average loss: 0.7011
[10/30 08:39:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.55	rocauc: 62.59	
[10/30 08:39:34 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[10/30 08:41:11 visual_prompt]: 	Training 100/553. train loss: 0.5264,	0.5040 s / batch. (data: 6.96e-04). ETA=4:28:34, max mem: 11.4 GB 
[10/30 08:42:46 visual_prompt]: 	Training 200/553. train loss: 1.0774,	0.4917 s / batch. (data: 2.91e-04). ETA=4:21:11, max mem: 11.4 GB 
[10/30 08:44:25 visual_prompt]: 	Training 300/553. train loss: 0.4767,	1.6119 s / batch. (data: 1.13e+00). ETA=14:13:35, max mem: 11.4 GB 
[10/30 08:45:58 visual_prompt]: 	Training 400/553. train loss: 0.6599,	1.3695 s / batch. (data: 8.92e-01). ETA=12:02:57, max mem: 11.4 GB 
[10/30 08:47:33 visual_prompt]: 	Training 500/553. train loss: 0.4444,	0.4914 s / batch. (data: 3.92e-04). ETA=4:18:35, max mem: 11.4 GB 
[10/30 08:48:19 visual_prompt]: Epoch 43 / 100: avg data time: 4.55e-01, avg batch time: 0.9498, average train loss: 1.1630
[10/30 08:49:14 visual_prompt]: Inference (val):avg data time: 5.81e-04, avg batch time: 0.1914, average loss: 2.7884
[10/30 08:49:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.56	
[10/30 08:50:45 visual_prompt]: 	Test 100/162. loss: 2.416, 0.1856 s / batch. (data: 3.74e-05)max mem: 11.41573 GB 
[10/30 08:51:36 visual_prompt]: Inference (test):avg data time: 3.38e-05, avg batch time: 0.1915, average loss: 3.0838
[10/30 08:51:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 61.62	
[10/30 08:51:36 visual_prompt]: Stopping early.
[10/30 08:51:36 visual_prompt]: Rank of current process: 0. World size: 1
[10/30 08:51:36 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/30 08:51:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/30 08:51:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/30 08:51:36 visual_prompt]: Training with config:
[10/30 08:51:36 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/test/seed4536/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 4536, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/30 08:51:36 visual_prompt]: Loading training data...
[10/30 08:51:36 visual_prompt]: Constructing mammo-cbis dataset train...
[10/30 08:51:36 visual_prompt]: Loading validation data...
[10/30 08:51:36 visual_prompt]: Constructing mammo-cbis dataset val...
[10/30 08:51:36 visual_prompt]: Loading test data...
[10/30 08:51:36 visual_prompt]: Constructing mammo-cbis dataset test...
[10/30 08:51:36 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/30 08:51:39 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/30 08:51:39 visual_prompt]: tuned percent:0.529
[10/30 08:51:39 visual_prompt]: Device used for model: 0
[10/30 08:51:39 visual_prompt]: Setting up Evaluator...
[10/30 08:51:39 visual_prompt]: Setting up Trainer...
[10/30 08:51:39 visual_prompt]: 	Setting up the optimizer...
[10/30 08:51:39 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/30 08:53:20 visual_prompt]: 	Training 100/553. train loss: 1.4371,	0.4932 s / batch. (data: 2.81e-04). ETA=7:33:45, max mem: 11.4 GB 
[10/30 08:54:59 visual_prompt]: 	Training 200/553. train loss: 1.3406,	1.8720 s / batch. (data: 1.39e+00). ETA=1 day, 4:39:06, max mem: 11.4 GB 
[10/30 08:56:36 visual_prompt]: 	Training 300/553. train loss: 0.6616,	2.0319 s / batch. (data: 1.55e+00). ETA=1 day, 7:02:36, max mem: 11.4 GB 
[10/30 08:58:11 visual_prompt]: 	Training 400/553. train loss: 1.7395,	0.5040 s / batch. (data: 2.77e-04). ETA=7:41:09, max mem: 11.4 GB 
[10/30 08:59:45 visual_prompt]: 	Training 500/553. train loss: 0.9298,	0.4844 s / batch. (data: 2.79e-04). ETA=7:22:23, max mem: 11.4 GB 
[10/30 09:00:31 visual_prompt]: Epoch 1 / 100: avg data time: 4.69e-01, avg batch time: 0.9631, average train loss: 1.4869
[10/30 09:01:29 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1915, average loss: 1.4553
[10/30 09:01:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.26	
[10/30 09:03:03 visual_prompt]: 	Test 100/162. loss: 1.514, 0.2008 s / batch. (data: 3.15e-05)max mem: 11.41573 GB 
[10/30 09:03:57 visual_prompt]: Inference (test):avg data time: 8.38e-05, avg batch time: 0.1903, average loss: 1.3284
[10/30 09:03:57 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 47.49	
[10/30 09:03:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/30 09:05:35 visual_prompt]: 	Training 100/553. train loss: 1.7356,	0.5255 s / batch. (data: 8.89e-04). ETA=7:58:35, max mem: 11.4 GB 
[10/30 09:07:12 visual_prompt]: 	Training 200/553. train loss: 0.8204,	0.5170 s / batch. (data: 9.00e-03). ETA=7:50:02, max mem: 11.4 GB 
[10/30 09:08:49 visual_prompt]: 	Training 300/553. train loss: 0.9058,	0.4801 s / batch. (data: 2.63e-04). ETA=7:15:38, max mem: 11.4 GB 
[10/30 09:10:27 visual_prompt]: 	Training 400/553. train loss: 0.8255,	0.4901 s / batch. (data: 7.50e-04). ETA=7:23:55, max mem: 11.4 GB 
[10/30 09:11:59 visual_prompt]: 	Training 500/553. train loss: 0.7216,	0.4856 s / batch. (data: 7.96e-03). ETA=7:19:01, max mem: 11.4 GB 
[10/30 09:12:47 visual_prompt]: Epoch 2 / 100: avg data time: 4.65e-01, avg batch time: 0.9595, average train loss: 1.0685
[10/30 09:13:45 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1880, average loss: 0.7218
[10/30 09:13:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.21	
[10/30 09:15:19 visual_prompt]: 	Test 100/162. loss: 0.732, 0.2100 s / batch. (data: 3.48e-05)max mem: 11.41573 GB 
[10/30 09:16:12 visual_prompt]: Inference (test):avg data time: 3.48e-05, avg batch time: 0.1910, average loss: 0.6901
[10/30 09:16:12 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.93	
[10/30 09:16:12 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/30 09:17:49 visual_prompt]: 	Training 100/553. train loss: 0.8202,	2.1979 s / batch. (data: 1.71e+00). ETA=1 day, 9:01:35, max mem: 11.4 GB 
[10/30 09:19:19 visual_prompt]: 	Training 200/553. train loss: 0.8207,	1.8080 s / batch. (data: 1.30e+00). ETA=1 day, 3:06:59, max mem: 11.4 GB 
[10/30 09:20:49 visual_prompt]: 	Training 300/553. train loss: 0.6062,	1.1640 s / batch. (data: 6.44e-01). ETA=17:25:32, max mem: 11.4 GB 
[10/30 09:22:20 visual_prompt]: 	Training 400/553. train loss: 0.7098,	0.5043 s / batch. (data: 1.24e-02). ETA=7:32:05, max mem: 11.4 GB 
[10/30 09:23:52 visual_prompt]: 	Training 500/553. train loss: 2.7159,	1.1121 s / batch. (data: 6.25e-01). ETA=16:35:13, max mem: 11.4 GB 
[10/30 09:24:37 visual_prompt]: Epoch 3 / 100: avg data time: 4.20e-01, avg batch time: 0.9142, average train loss: 0.9999
[10/30 09:25:30 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1902, average loss: 0.7325
[10/30 09:25:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.89	
[10/30 09:26:59 visual_prompt]: 	Test 100/162. loss: 0.750, 0.1853 s / batch. (data: 3.65e-05)max mem: 11.41573 GB 
[10/30 09:27:46 visual_prompt]: Inference (test):avg data time: 1.27e-04, avg batch time: 0.1917, average loss: 0.6966
[10/30 09:27:46 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.54	
[10/30 09:27:46 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/30 09:29:22 visual_prompt]: 	Training 100/553. train loss: 3.0860,	0.5040 s / batch. (data: 3.68e-04). ETA=7:29:42, max mem: 11.4 GB 
[10/30 09:30:48 visual_prompt]: 	Training 200/553. train loss: 3.6022,	0.5038 s / batch. (data: 1.24e-02). ETA=7:28:42, max mem: 11.4 GB 
[10/30 09:32:18 visual_prompt]: 	Training 300/553. train loss: 1.8135,	0.4920 s / batch. (data: 2.59e-04). ETA=7:17:24, max mem: 11.4 GB 
[10/30 09:33:46 visual_prompt]: 	Training 400/553. train loss: 0.6846,	0.5238 s / batch. (data: 1.60e-02). ETA=7:44:48, max mem: 11.4 GB 
[10/30 09:35:13 visual_prompt]: 	Training 500/553. train loss: 1.5158,	0.4811 s / batch. (data: 2.32e-04). ETA=7:06:03, max mem: 11.4 GB 
[10/30 09:36:00 visual_prompt]: Epoch 4 / 100: avg data time: 3.98e-01, avg batch time: 0.8923, average train loss: 1.2422
[10/30 09:36:52 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1914, average loss: 0.8353
[10/30 09:36:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[10/30 09:38:18 visual_prompt]: 	Test 100/162. loss: 0.865, 0.1996 s / batch. (data: 2.93e-05)max mem: 11.41573 GB 
[10/30 09:39:07 visual_prompt]: Inference (test):avg data time: 9.08e-05, avg batch time: 0.1919, average loss: 0.7767
[10/30 09:39:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 58.45	
[10/30 09:39:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/30 09:40:41 visual_prompt]: 	Training 100/553. train loss: 2.6907,	0.5000 s / batch. (data: 2.63e-04). ETA=7:21:34, max mem: 11.4 GB 
[10/30 09:42:10 visual_prompt]: 	Training 200/553. train loss: 0.0185,	0.5280 s / batch. (data: 1.09e-02). ETA=7:45:25, max mem: 11.4 GB 
[10/30 09:43:38 visual_prompt]: 	Training 300/553. train loss: 0.3728,	2.2701 s / batch. (data: 1.76e+00). ETA=1 day, 9:17:13, max mem: 11.4 GB 
[10/30 09:45:07 visual_prompt]: 	Training 400/553. train loss: 1.2087,	0.4910 s / batch. (data: 2.71e-04). ETA=7:11:08, max mem: 11.4 GB 
[10/30 09:46:34 visual_prompt]: 	Training 500/553. train loss: 0.8371,	0.5044 s / batch. (data: 7.96e-03). ETA=7:22:07, max mem: 11.4 GB 
[10/30 09:47:19 visual_prompt]: Epoch 5 / 100: avg data time: 3.94e-01, avg batch time: 0.8891, average train loss: 1.2751
[10/30 09:48:12 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1901, average loss: 1.0087
[10/30 09:48:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.05	
[10/30 09:49:38 visual_prompt]: 	Test 100/162. loss: 1.079, 0.1858 s / batch. (data: 3.03e-05)max mem: 11.41573 GB 
[10/30 09:50:27 visual_prompt]: Inference (test):avg data time: 3.30e-05, avg batch time: 0.1933, average loss: 0.9272
[10/30 09:50:27 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 53.76	
[10/30 09:50:27 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/30 09:52:00 visual_prompt]: 	Training 100/553. train loss: 0.9795,	0.4898 s / batch. (data: 5.44e-03). ETA=7:08:04, max mem: 11.4 GB 
[10/30 09:53:28 visual_prompt]: 	Training 200/553. train loss: 1.8635,	1.3400 s / batch. (data: 8.44e-01). ETA=19:28:47, max mem: 11.4 GB 
[10/30 09:54:57 visual_prompt]: 	Training 300/553. train loss: 0.9629,	1.1520 s / batch. (data: 6.47e-01). ETA=16:42:55, max mem: 11.4 GB 
[10/30 09:56:27 visual_prompt]: 	Training 400/553. train loss: 0.6841,	0.5040 s / batch. (data: 7.26e-04). ETA=7:17:56, max mem: 11.4 GB 
[10/30 09:57:55 visual_prompt]: 	Training 500/553. train loss: 0.6158,	0.5259 s / batch. (data: 3.91e-02). ETA=7:36:06, max mem: 11.4 GB 
[10/30 09:58:41 visual_prompt]: Epoch 6 / 100: avg data time: 3.99e-01, avg batch time: 0.8936, average train loss: 1.3838
[10/30 09:59:33 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1907, average loss: 1.5696
[10/30 09:59:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.33	
[10/30 10:00:59 visual_prompt]: 	Test 100/162. loss: 1.392, 0.1928 s / batch. (data: 3.84e-05)max mem: 11.41573 GB 
[10/30 10:01:48 visual_prompt]: Inference (test):avg data time: 3.35e-05, avg batch time: 0.1918, average loss: 1.6890
[10/30 10:01:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.01	
[10/30 10:01:48 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/30 10:03:22 visual_prompt]: 	Training 100/553. train loss: 1.1516,	1.7120 s / batch. (data: 1.21e+00). ETA=1 day, 0:40:20, max mem: 11.4 GB 
[10/30 10:04:53 visual_prompt]: 	Training 200/553. train loss: 0.5084,	0.5118 s / batch. (data: 7.17e-04). ETA=7:21:41, max mem: 11.4 GB 
[10/30 10:06:18 visual_prompt]: 	Training 300/553. train loss: 1.2017,	0.4905 s / batch. (data: 2.52e-04). ETA=7:02:31, max mem: 11.4 GB 
[10/30 10:07:47 visual_prompt]: 	Training 400/553. train loss: 1.1203,	1.8434 s / batch. (data: 1.35e+00). ETA=1 day, 2:24:45, max mem: 11.4 GB 
[10/30 10:09:16 visual_prompt]: 	Training 500/553. train loss: 0.8167,	1.5766 s / batch. (data: 1.09e+00). ETA=22:32:46, max mem: 11.4 GB 
[10/30 10:09:59 visual_prompt]: Epoch 7 / 100: avg data time: 3.94e-01, avg batch time: 0.8879, average train loss: 1.3230
[10/30 10:10:52 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1909, average loss: 0.9623
[10/30 10:10:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.41	
[10/30 10:12:18 visual_prompt]: 	Test 100/162. loss: 0.907, 0.1900 s / batch. (data: 3.00e-05)max mem: 11.41573 GB 
[10/30 10:13:07 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.1915, average loss: 0.8880
[10/30 10:13:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.03	
[10/30 10:13:07 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/30 10:14:35 visual_prompt]: 	Training 100/553. train loss: 0.8851,	0.4914 s / batch. (data: 2.65e-04). ETA=7:00:22, max mem: 11.4 GB 
[10/30 10:16:05 visual_prompt]: 	Training 200/553. train loss: 1.1242,	1.4660 s / batch. (data: 9.73e-01). ETA=20:51:42, max mem: 11.4 GB 
[10/30 10:17:35 visual_prompt]: 	Training 300/553. train loss: 1.2341,	1.5196 s / batch. (data: 1.01e+00). ETA=21:34:56, max mem: 11.4 GB 
[10/30 10:19:05 visual_prompt]: 	Training 400/553. train loss: 0.6367,	1.6880 s / batch. (data: 1.20e+00). ETA=23:55:38, max mem: 11.4 GB 
[10/30 10:20:35 visual_prompt]: 	Training 500/553. train loss: 0.9640,	1.8480 s / batch. (data: 1.36e+00). ETA=1 day, 2:08:35, max mem: 11.4 GB 
[10/30 10:21:20 visual_prompt]: Epoch 8 / 100: avg data time: 3.97e-01, avg batch time: 0.8916, average train loss: 1.3323
[10/30 10:22:13 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1907, average loss: 1.2922
[10/30 10:22:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.96	
[10/30 10:23:40 visual_prompt]: 	Test 100/162. loss: 0.944, 0.1974 s / batch. (data: 3.08e-05)max mem: 11.41573 GB 
[10/30 10:24:28 visual_prompt]: Inference (test):avg data time: 3.55e-05, avg batch time: 0.1925, average loss: 1.3653
[10/30 10:24:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.24	
[10/30 10:24:28 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/30 10:26:02 visual_prompt]: 	Training 100/553. train loss: 1.1281,	0.4856 s / batch. (data: 8.65e-03). ETA=6:50:56, max mem: 11.4 GB 
[10/30 10:27:30 visual_prompt]: 	Training 200/553. train loss: 2.7313,	0.5197 s / batch. (data: 7.58e-04). ETA=7:18:58, max mem: 11.4 GB 
[10/30 10:28:58 visual_prompt]: 	Training 300/553. train loss: 0.0092,	0.5200 s / batch. (data: 2.95e-04). ETA=7:18:19, max mem: 11.4 GB 
[10/30 10:30:23 visual_prompt]: 	Training 400/553. train loss: 0.6668,	0.4876 s / batch. (data: 2.58e-04). ETA=6:50:12, max mem: 11.4 GB 
[10/30 10:31:55 visual_prompt]: 	Training 500/553. train loss: 0.5633,	0.5093 s / batch. (data: 5.80e-03). ETA=7:07:38, max mem: 11.4 GB 
[10/30 10:32:40 visual_prompt]: Epoch 9 / 100: avg data time: 3.95e-01, avg batch time: 0.8892, average train loss: 1.8781
[10/30 10:33:32 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.1918, average loss: 1.1281
[10/30 10:33:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.74	
[10/30 10:35:00 visual_prompt]: 	Test 100/162. loss: 1.220, 0.1920 s / batch. (data: 3.17e-05)max mem: 11.41573 GB 
[10/30 10:35:47 visual_prompt]: Inference (test):avg data time: 3.49e-05, avg batch time: 0.1918, average loss: 1.0295
[10/30 10:35:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 51.53	
[10/30 10:35:47 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/30 10:37:21 visual_prompt]: 	Training 100/553. train loss: 1.1210,	0.5007 s / batch. (data: 8.64e-03). ETA=6:59:04, max mem: 11.4 GB 
[10/30 10:38:50 visual_prompt]: 	Training 200/553. train loss: 1.6614,	1.1802 s / batch. (data: 7.03e-01). ETA=16:25:56, max mem: 11.4 GB 
[10/30 10:40:17 visual_prompt]: 	Training 300/553. train loss: 0.7132,	0.5080 s / batch. (data: 7.96e-03). ETA=7:03:31, max mem: 11.4 GB 
[10/30 10:41:46 visual_prompt]: 	Training 400/553. train loss: 0.7588,	0.5200 s / batch. (data: 1.20e-02). ETA=7:12:38, max mem: 11.4 GB 
[10/30 10:43:14 visual_prompt]: 	Training 500/553. train loss: 0.9059,	0.4960 s / batch. (data: 2.69e-04). ETA=6:51:52, max mem: 11.4 GB 
[10/30 10:43:59 visual_prompt]: Epoch 10 / 100: avg data time: 3.95e-01, avg batch time: 0.8888, average train loss: 1.6842
[10/30 10:44:51 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1914, average loss: 0.9161
[10/30 10:44:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.95	
[10/30 10:46:18 visual_prompt]: 	Test 100/162. loss: 0.802, 0.1936 s / batch. (data: 5.27e-05)max mem: 11.41573 GB 
[10/30 10:47:07 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.1908, average loss: 0.9683
[10/30 10:47:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 55.99	
[10/30 10:47:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/30 10:48:41 visual_prompt]: 	Training 100/553. train loss: 2.8703,	0.4960 s / batch. (data: 7.96e-03). ETA=6:50:35, max mem: 11.4 GB 
[10/30 10:50:10 visual_prompt]: 	Training 200/553. train loss: 0.4511,	0.4960 s / batch. (data: 2.65e-04). ETA=6:49:47, max mem: 11.4 GB 
[10/30 10:51:40 visual_prompt]: 	Training 300/553. train loss: 4.2086,	2.0594 s / batch. (data: 1.57e+00). ETA=1 day, 4:17:57, max mem: 11.4 GB 
[10/30 10:53:10 visual_prompt]: 	Training 400/553. train loss: 1.6411,	2.0520 s / batch. (data: 1.55e+00). ETA=1 day, 4:08:27, max mem: 11.4 GB 
[10/30 10:54:38 visual_prompt]: 	Training 500/553. train loss: 1.6270,	2.0514 s / batch. (data: 1.55e+00). ETA=1 day, 4:04:30, max mem: 11.4 GB 
[10/30 10:55:22 visual_prompt]: Epoch 11 / 100: avg data time: 4.01e-01, avg batch time: 0.8948, average train loss: 1.6319
[10/30 10:56:14 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1907, average loss: 1.7326
[10/30 10:56:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.90	
[10/30 10:57:42 visual_prompt]: 	Test 100/162. loss: 1.503, 0.1851 s / batch. (data: 3.29e-05)max mem: 11.41573 GB 
[10/30 10:58:30 visual_prompt]: Inference (test):avg data time: 1.15e-04, avg batch time: 0.1930, average loss: 1.8602
[10/30 10:58:30 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.30	
[10/30 10:58:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/30 11:00:04 visual_prompt]: 	Training 100/553. train loss: 2.8869,	0.5124 s / batch. (data: 1.24e-02). ETA=6:59:26, max mem: 11.4 GB 
[10/30 11:01:32 visual_prompt]: 	Training 200/553. train loss: 2.8563,	0.5120 s / batch. (data: 7.39e-04). ETA=6:58:18, max mem: 11.4 GB 
[10/30 11:02:59 visual_prompt]: 	Training 300/553. train loss: 1.5629,	0.5241 s / batch. (data: 2.61e-04). ETA=7:07:15, max mem: 11.4 GB 
[10/30 11:04:29 visual_prompt]: 	Training 400/553. train loss: 0.7287,	0.4800 s / batch. (data: 2.78e-04). ETA=6:30:31, max mem: 11.4 GB 
[10/30 11:05:59 visual_prompt]: 	Training 500/553. train loss: 0.8927,	0.5682 s / batch. (data: 7.83e-02). ETA=7:41:18, max mem: 11.4 GB 
[10/30 11:06:46 visual_prompt]: Epoch 12 / 100: avg data time: 4.02e-01, avg batch time: 0.8972, average train loss: 1.5534
[10/30 11:07:39 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1887, average loss: 0.7935
[10/30 11:07:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.92	
[10/30 11:09:06 visual_prompt]: 	Test 100/162. loss: 0.850, 0.2043 s / batch. (data: 3.34e-05)max mem: 11.41573 GB 
[10/30 11:09:55 visual_prompt]: Inference (test):avg data time: 8.51e-05, avg batch time: 0.1927, average loss: 0.7456
[10/30 11:09:55 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 50.56	
[10/30 11:09:55 visual_prompt]: Best epoch 12: best metric: -0.793
[10/30 11:09:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/30 11:11:26 visual_prompt]: 	Training 100/553. train loss: 0.5821,	0.4918 s / batch. (data: 3.21e-04). ETA=6:38:04, max mem: 11.4 GB 
[10/30 11:12:55 visual_prompt]: 	Training 200/553. train loss: 2.9288,	0.5136 s / batch. (data: 9.54e-03). ETA=6:54:48, max mem: 11.4 GB 
[10/30 11:14:25 visual_prompt]: 	Training 300/553. train loss: 0.6207,	2.0680 s / batch. (data: 1.58e+00). ETA=1 day, 3:46:54, max mem: 11.4 GB 
[10/30 11:15:57 visual_prompt]: 	Training 400/553. train loss: 2.3999,	3.2865 s / batch. (data: 2.81e+00). ETA=1 day, 20:03:39, max mem: 11.4 GB 
[10/30 11:17:26 visual_prompt]: 	Training 500/553. train loss: 5.9132,	1.2393 s / batch. (data: 7.43e-01). ETA=16:34:47, max mem: 11.4 GB 
[10/30 11:18:09 visual_prompt]: Epoch 13 / 100: avg data time: 4.00e-01, avg batch time: 0.8940, average train loss: 1.8847
[10/30 11:19:02 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1902, average loss: 1.7502
[10/30 11:19:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.66	
[10/30 11:20:29 visual_prompt]: 	Test 100/162. loss: 1.577, 0.1880 s / batch. (data: 3.84e-05)max mem: 11.41573 GB 
[10/30 11:21:18 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.1922, average loss: 1.8889
[10/30 11:21:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 52.63	
[10/30 11:21:18 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/30 11:22:52 visual_prompt]: 	Training 100/553. train loss: 0.8992,	1.8119 s / batch. (data: 1.33e+00). ETA=1 day, 0:09:51, max mem: 11.4 GB 
[10/30 11:24:22 visual_prompt]: 	Training 200/553. train loss: 3.0260,	1.6680 s / batch. (data: 1.18e+00). ETA=22:11:53, max mem: 11.4 GB 
[10/30 11:25:49 visual_prompt]: 	Training 300/553. train loss: 0.6590,	0.4960 s / batch. (data: 2.71e-04). ETA=6:35:12, max mem: 11.4 GB 
[10/30 11:27:19 visual_prompt]: 	Training 400/553. train loss: 3.3878,	1.6960 s / batch. (data: 1.19e+00). ETA=22:28:38, max mem: 11.4 GB 
[10/30 11:28:47 visual_prompt]: 	Training 500/553. train loss: 0.1131,	0.5085 s / batch. (data: 9.37e-03). ETA=6:43:31, max mem: 11.4 GB 
[10/30 11:29:34 visual_prompt]: Epoch 14 / 100: avg data time: 4.01e-01, avg batch time: 0.8955, average train loss: 1.8731
[10/30 11:30:27 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1914, average loss: 0.9686
[10/30 11:30:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.75	
[10/30 11:31:53 visual_prompt]: 	Test 100/162. loss: 0.908, 0.1854 s / batch. (data: 2.88e-05)max mem: 11.41573 GB 
[10/30 11:32:42 visual_prompt]: Inference (test):avg data time: 3.38e-05, avg batch time: 0.1918, average loss: 0.8910
[10/30 11:32:42 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.08	
[10/30 11:32:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/30 11:34:13 visual_prompt]: 	Training 100/553. train loss: 6.7268,	0.4920 s / batch. (data: 2.68e-04). ETA=6:29:08, max mem: 11.4 GB 
[10/30 11:35:45 visual_prompt]: 	Training 200/553. train loss: 1.4506,	0.4960 s / batch. (data: 2.72e-04). ETA=6:31:27, max mem: 11.4 GB 
[10/30 11:37:14 visual_prompt]: 	Training 300/553. train loss: 0.5335,	0.5160 s / batch. (data: 1.20e-02). ETA=6:46:24, max mem: 11.4 GB 
[10/30 11:38:43 visual_prompt]: 	Training 400/553. train loss: 0.5850,	0.5010 s / batch. (data: 5.44e-03). ETA=6:33:48, max mem: 11.4 GB 
[10/30 11:40:12 visual_prompt]: 	Training 500/553. train loss: 5.4442,	0.5080 s / batch. (data: 2.72e-04). ETA=6:38:25, max mem: 11.4 GB 
[10/30 11:40:58 visual_prompt]: Epoch 15 / 100: avg data time: 4.01e-01, avg batch time: 0.8959, average train loss: 1.5521
[10/30 11:41:51 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1919, average loss: 2.2593
[10/30 11:41:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.22	
[10/30 11:43:18 visual_prompt]: 	Test 100/162. loss: 2.337, 0.1941 s / batch. (data: 5.15e-05)max mem: 11.41573 GB 
[10/30 11:44:07 visual_prompt]: Inference (test):avg data time: 1.10e-04, avg batch time: 0.1926, average loss: 2.0395
[10/30 11:44:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.13	
[10/30 11:44:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/30 11:45:40 visual_prompt]: 	Training 100/553. train loss: 1.3253,	0.5044 s / batch. (data: 2.84e-04). ETA=6:34:18, max mem: 11.4 GB 
[10/30 11:47:07 visual_prompt]: 	Training 200/553. train loss: 5.6254,	0.4920 s / batch. (data: 2.93e-04). ETA=6:23:48, max mem: 11.4 GB 
[10/30 11:48:41 visual_prompt]: 	Training 300/553. train loss: 1.1782,	0.5000 s / batch. (data: 2.90e-04). ETA=6:29:12, max mem: 11.4 GB 
[10/30 11:50:09 visual_prompt]: 	Training 400/553. train loss: 0.9625,	0.5203 s / batch. (data: 1.05e-02). ETA=6:44:10, max mem: 11.4 GB 
[10/30 11:51:37 visual_prompt]: 	Training 500/553. train loss: 4.0760,	0.4920 s / batch. (data: 2.68e-04). ETA=6:21:20, max mem: 11.4 GB 
[10/30 11:52:22 visual_prompt]: Epoch 16 / 100: avg data time: 4.02e-01, avg batch time: 0.8953, average train loss: 1.6227
[10/30 11:53:15 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.1888, average loss: 0.6993
[10/30 11:53:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.69	
[10/30 11:54:42 visual_prompt]: 	Test 100/162. loss: 0.615, 0.1851 s / batch. (data: 5.20e-05)max mem: 11.41573 GB 
[10/30 11:55:30 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.1920, average loss: 0.6709
[10/30 11:55:30 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.83	
[10/30 11:55:30 visual_prompt]: Best epoch 16: best metric: -0.699
[10/30 11:55:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/30 11:57:05 visual_prompt]: 	Training 100/553. train loss: 1.2996,	0.4921 s / batch. (data: 7.96e-03). ETA=6:20:09, max mem: 11.4 GB 
[10/30 11:58:34 visual_prompt]: 	Training 200/553. train loss: 0.5564,	1.8640 s / batch. (data: 1.38e+00). ETA=23:56:55, max mem: 11.4 GB 
[10/30 12:00:01 visual_prompt]: 	Training 300/553. train loss: 1.8698,	0.5118 s / batch. (data: 1.18e-02). ETA=6:33:41, max mem: 11.4 GB 
[10/30 12:01:27 visual_prompt]: 	Training 400/553. train loss: 0.0313,	0.4799 s / batch. (data: 2.74e-04). ETA=6:08:22, max mem: 11.4 GB 
[10/30 12:02:59 visual_prompt]: 	Training 500/553. train loss: 1.7001,	0.4845 s / batch. (data: 2.87e-04). ETA=6:11:04, max mem: 11.4 GB 
[10/30 12:03:45 visual_prompt]: Epoch 17 / 100: avg data time: 4.00e-01, avg batch time: 0.8937, average train loss: 1.4637
[10/30 12:04:38 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1899, average loss: 1.4332
[10/30 12:04:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.28	
[10/30 12:06:03 visual_prompt]: 	Test 100/162. loss: 1.189, 0.1857 s / batch. (data: 3.10e-05)max mem: 11.41573 GB 
[10/30 12:06:53 visual_prompt]: Inference (test):avg data time: 3.31e-05, avg batch time: 0.1918, average loss: 1.5498
[10/30 12:06:53 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 55.47	
[10/30 12:06:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/30 12:08:25 visual_prompt]: 	Training 100/553. train loss: 2.8765,	0.5120 s / batch. (data: 7.96e-03). ETA=6:30:47, max mem: 11.4 GB 
[10/30 12:09:53 visual_prompt]: 	Training 200/553. train loss: 3.6117,	0.4876 s / batch. (data: 4.50e-03). ETA=6:11:24, max mem: 11.4 GB 
[10/30 12:11:23 visual_prompt]: 	Training 300/553. train loss: 1.0105,	0.4920 s / batch. (data: 2.91e-04). ETA=6:13:55, max mem: 11.4 GB 
[10/30 12:12:55 visual_prompt]: 	Training 400/553. train loss: 1.5350,	2.0879 s / batch. (data: 1.61e+00). ETA=1 day, 2:23:15, max mem: 11.4 GB 
[10/30 12:14:24 visual_prompt]: 	Training 500/553. train loss: 0.6866,	0.5093 s / batch. (data: 5.39e-03). ETA=6:25:23, max mem: 11.4 GB 
[10/30 12:15:07 visual_prompt]: Epoch 18 / 100: avg data time: 3.98e-01, avg batch time: 0.8930, average train loss: 1.5144
[10/30 12:16:00 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1912, average loss: 0.7128
[10/30 12:16:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.29	
[10/30 12:17:28 visual_prompt]: 	Test 100/162. loss: 0.630, 0.1931 s / batch. (data: 3.53e-05)max mem: 11.41573 GB 
[10/30 12:18:16 visual_prompt]: Inference (test):avg data time: 3.25e-05, avg batch time: 0.1925, average loss: 0.6879
[10/30 12:18:16 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.91	rocauc: 54.12	
[10/30 12:18:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/30 12:19:49 visual_prompt]: 	Training 100/553. train loss: 0.8798,	0.5040 s / batch. (data: 7.56e-04). ETA=6:20:05, max mem: 11.4 GB 
[10/30 12:21:19 visual_prompt]: 	Training 200/553. train loss: 0.7491,	0.5005 s / batch. (data: 8.02e-04). ETA=6:16:33, max mem: 11.4 GB 
[10/30 12:22:47 visual_prompt]: 	Training 300/553. train loss: 1.4097,	0.4920 s / batch. (data: 7.95e-03). ETA=6:09:21, max mem: 11.4 GB 
[10/30 12:24:17 visual_prompt]: 	Training 400/553. train loss: 1.1475,	1.6760 s / batch. (data: 1.19e+00). ETA=20:55:28, max mem: 11.4 GB 
[10/30 12:25:47 visual_prompt]: 	Training 500/553. train loss: 0.5052,	0.5000 s / batch. (data: 4.45e-04). ETA=6:13:41, max mem: 11.4 GB 
[10/30 12:26:32 visual_prompt]: Epoch 19 / 100: avg data time: 4.02e-01, avg batch time: 0.8968, average train loss: 1.4507
[10/30 12:27:25 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1902, average loss: 0.6917
[10/30 12:27:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 57.14	
[10/30 12:28:52 visual_prompt]: 	Test 100/162. loss: 0.493, 0.2061 s / batch. (data: 2.88e-05)max mem: 11.41573 GB 
[10/30 12:29:41 visual_prompt]: Inference (test):avg data time: 1.99e-04, avg batch time: 0.1932, average loss: 0.6806
[10/30 12:29:41 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.14	rocauc: 57.56	
[10/30 12:29:41 visual_prompt]: Best epoch 19: best metric: -0.692
[10/30 12:29:41 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/30 12:31:14 visual_prompt]: 	Training 100/553. train loss: 1.5847,	0.4781 s / batch. (data: 2.62e-04). ETA=5:56:07, max mem: 11.4 GB 
[10/30 12:32:46 visual_prompt]: 	Training 200/553. train loss: 0.0418,	0.5174 s / batch. (data: 1.33e-02). ETA=6:24:31, max mem: 11.4 GB 
[10/30 12:34:14 visual_prompt]: 	Training 300/553. train loss: 0.4232,	0.5200 s / batch. (data: 2.92e-04). ETA=6:25:36, max mem: 11.4 GB 
[10/30 12:35:44 visual_prompt]: 	Training 400/553. train loss: 0.0156,	1.6480 s / batch. (data: 1.16e+00). ETA=20:19:19, max mem: 11.4 GB 
[10/30 12:37:13 visual_prompt]: 	Training 500/553. train loss: 0.8329,	2.0908 s / batch. (data: 1.61e+00). ETA=1 day, 1:43:29, max mem: 11.4 GB 
[10/30 12:37:57 visual_prompt]: Epoch 20 / 100: avg data time: 4.01e-01, avg batch time: 0.8959, average train loss: 1.4294
[10/30 12:38:49 visual_prompt]: Inference (val):avg data time: 1.60e-04, avg batch time: 0.1890, average loss: 0.8302
[10/30 12:38:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.41	rocauc: 56.07	
[10/30 12:40:18 visual_prompt]: 	Test 100/162. loss: 0.508, 0.1966 s / batch. (data: 3.27e-05)max mem: 11.41573 GB 
[10/30 12:41:06 visual_prompt]: Inference (test):avg data time: 3.93e-05, avg batch time: 0.1928, average loss: 0.8621
[10/30 12:41:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 47.44	rocauc: 57.26	
[10/30 12:41:06 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[10/30 12:42:39 visual_prompt]: 	Training 100/553. train loss: 1.1542,	2.4919 s / batch. (data: 2.01e+00). ETA=1 day, 6:33:14, max mem: 11.4 GB 
[10/30 12:44:08 visual_prompt]: 	Training 200/553. train loss: 5.5618,	0.9480 s / batch. (data: 4.40e-01). ETA=11:35:49, max mem: 11.4 GB 
[10/30 12:45:36 visual_prompt]: 	Training 300/553. train loss: 1.0716,	0.4909 s / batch. (data: 4.26e-04). ETA=5:59:29, max mem: 11.4 GB 
[10/30 12:47:04 visual_prompt]: 	Training 400/553. train loss: 0.5715,	0.9556 s / batch. (data: 4.50e-01). ETA=11:38:11, max mem: 11.4 GB 
[10/30 12:48:33 visual_prompt]: 	Training 500/553. train loss: 3.2055,	0.4893 s / batch. (data: 8.70e-03). ETA=5:56:40, max mem: 11.4 GB 
[10/30 12:49:18 visual_prompt]: Epoch 21 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 1.7768
[10/30 12:50:10 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1888, average loss: 1.9418
[10/30 12:50:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.05	
[10/30 12:51:37 visual_prompt]: 	Test 100/162. loss: 1.943, 0.1852 s / batch. (data: 2.96e-05)max mem: 11.41573 GB 
[10/30 12:52:25 visual_prompt]: Inference (test):avg data time: 3.35e-05, avg batch time: 0.1917, average loss: 1.7515
[10/30 12:52:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.04	
[10/30 12:52:25 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[10/30 12:54:01 visual_prompt]: 	Training 100/553. train loss: 1.0842,	2.0944 s / batch. (data: 1.61e+00). ETA=1 day, 1:21:27, max mem: 11.4 GB 
[10/30 12:55:28 visual_prompt]: 	Training 200/553. train loss: 1.4854,	1.6512 s / batch. (data: 1.15e+00). ETA=19:56:47, max mem: 11.4 GB 
[10/30 12:56:54 visual_prompt]: 	Training 300/553. train loss: 0.4679,	0.5007 s / batch. (data: 8.59e-03). ETA=6:02:03, max mem: 11.4 GB 
[10/30 12:58:23 visual_prompt]: 	Training 400/553. train loss: 3.0535,	0.4787 s / batch. (data: 3.05e-04). ETA=5:45:19, max mem: 11.4 GB 
[10/30 12:59:51 visual_prompt]: 	Training 500/553. train loss: 1.9270,	0.4844 s / batch. (data: 2.73e-04). ETA=5:48:41, max mem: 11.4 GB 
[10/30 13:00:37 visual_prompt]: Epoch 22 / 100: avg data time: 3.96e-01, avg batch time: 0.8899, average train loss: 1.5065
[10/30 13:01:30 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1903, average loss: 0.7827
[10/30 13:01:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 56.42	
[10/30 13:02:58 visual_prompt]: 	Test 100/162. loss: 0.583, 0.1852 s / batch. (data: 3.41e-05)max mem: 11.41573 GB 
[10/30 13:03:45 visual_prompt]: Inference (test):avg data time: 3.42e-05, avg batch time: 0.1914, average loss: 0.8128
[10/30 13:03:45 visual_prompt]: Classification results with test_mammo-cbis: top1: 48.53	rocauc: 56.10	
[10/30 13:03:45 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[10/30 13:05:19 visual_prompt]: 	Training 100/553. train loss: 1.1955,	1.6551 s / batch. (data: 1.18e+00). ETA=19:47:03, max mem: 11.4 GB 
[10/30 13:06:49 visual_prompt]: 	Training 200/553. train loss: 0.8959,	0.4920 s / batch. (data: 2.64e-04). ETA=5:52:03, max mem: 11.4 GB 
[10/30 13:08:17 visual_prompt]: 	Training 300/553. train loss: 0.6105,	0.5280 s / batch. (data: 3.01e-04). ETA=6:16:55, max mem: 11.4 GB 
[10/30 13:09:44 visual_prompt]: 	Training 400/553. train loss: 0.6495,	0.5160 s / batch. (data: 7.96e-03). ETA=6:07:31, max mem: 11.4 GB 
[10/30 13:11:14 visual_prompt]: 	Training 500/553. train loss: 0.9390,	0.7211 s / batch. (data: 2.44e-01). ETA=8:32:23, max mem: 11.4 GB 
[10/30 13:11:57 visual_prompt]: Epoch 23 / 100: avg data time: 3.95e-01, avg batch time: 0.8894, average train loss: 1.6014
[10/30 13:12:50 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1930, average loss: 0.9074
[10/30 13:12:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.43	
[10/30 13:14:16 visual_prompt]: 	Test 100/162. loss: 0.806, 0.2025 s / batch. (data: 3.10e-05)max mem: 11.41573 GB 
[10/30 13:15:05 visual_prompt]: Inference (test):avg data time: 7.20e-05, avg batch time: 0.1923, average loss: 0.8424
[10/30 13:15:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.18	
[10/30 13:15:05 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[10/30 13:16:37 visual_prompt]: 	Training 100/553. train loss: 5.0937,	0.5040 s / batch. (data: 8.00e-03). ETA=5:56:52, max mem: 11.4 GB 
[10/30 13:18:07 visual_prompt]: 	Training 200/553. train loss: 1.6072,	0.5039 s / batch. (data: 8.24e-03). ETA=5:55:55, max mem: 11.4 GB 
[10/30 13:19:36 visual_prompt]: 	Training 300/553. train loss: 0.5588,	2.1560 s / batch. (data: 1.66e+00). ETA=1 day, 1:19:17, max mem: 11.4 GB 
[10/30 13:21:02 visual_prompt]: 	Training 400/553. train loss: 5.1244,	0.4876 s / batch. (data: 2.36e-04). ETA=5:42:47, max mem: 11.4 GB 
[10/30 13:22:33 visual_prompt]: 	Training 500/553. train loss: 4.6968,	0.5082 s / batch. (data: 5.79e-03). ETA=5:56:27, max mem: 11.4 GB 
[10/30 13:23:17 visual_prompt]: Epoch 24 / 100: avg data time: 3.96e-01, avg batch time: 0.8901, average train loss: 1.6805
[10/30 13:24:10 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1933, average loss: 3.3060
[10/30 13:24:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.41	
[10/30 13:25:37 visual_prompt]: 	Test 100/162. loss: 2.891, 0.1854 s / batch. (data: 3.10e-05)max mem: 11.41573 GB 
[10/30 13:26:25 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.1923, average loss: 3.5855
[10/30 13:26:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.82	
[10/30 13:26:25 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[10/30 13:27:56 visual_prompt]: 	Training 100/553. train loss: 0.7598,	1.5931 s / batch. (data: 1.12e+00). ETA=18:33:15, max mem: 11.4 GB 
[10/30 13:29:26 visual_prompt]: 	Training 200/553. train loss: 1.2979,	1.8036 s / batch. (data: 1.30e+00). ETA=20:57:18, max mem: 11.4 GB 
[10/30 13:30:54 visual_prompt]: 	Training 300/553. train loss: 1.4295,	1.0174 s / batch. (data: 5.22e-01). ETA=11:47:33, max mem: 11.4 GB 
[10/30 13:32:21 visual_prompt]: 	Training 400/553. train loss: 0.7702,	1.3755 s / batch. (data: 8.82e-01). ETA=15:54:18, max mem: 11.4 GB 
[10/30 13:33:52 visual_prompt]: 	Training 500/553. train loss: 2.0975,	0.5035 s / batch. (data: 2.61e-04). ETA=5:48:31, max mem: 11.4 GB 
[10/30 13:34:36 visual_prompt]: Epoch 25 / 100: avg data time: 3.94e-01, avg batch time: 0.8889, average train loss: 1.5869
[10/30 13:35:29 visual_prompt]: Inference (val):avg data time: 5.81e-05, avg batch time: 0.1907, average loss: 0.7346
[10/30 13:35:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 56.12	
[10/30 13:36:56 visual_prompt]: 	Test 100/162. loss: 0.538, 0.1964 s / batch. (data: 3.93e-05)max mem: 11.41573 GB 
[10/30 13:37:44 visual_prompt]: Inference (test):avg data time: 3.72e-05, avg batch time: 0.1924, average loss: 0.7504
[10/30 13:37:44 visual_prompt]: Classification results with test_mammo-cbis: top1: 50.08	rocauc: 56.12	
[10/30 13:37:44 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[10/30 13:39:17 visual_prompt]: 	Training 100/553. train loss: 0.7080,	0.5120 s / batch. (data: 6.94e-04). ETA=5:53:04, max mem: 11.4 GB 
[10/30 13:40:43 visual_prompt]: 	Training 200/553. train loss: 0.6773,	0.5092 s / batch. (data: 9.21e-03). ETA=5:50:18, max mem: 11.4 GB 
[10/30 13:42:12 visual_prompt]: 	Training 300/553. train loss: 0.7981,	0.5240 s / batch. (data: 5.40e-03). ETA=5:59:35, max mem: 11.4 GB 
[10/30 13:43:41 visual_prompt]: 	Training 400/553. train loss: 0.9049,	0.5216 s / batch. (data: 2.74e-02). ETA=5:57:03, max mem: 11.4 GB 
[10/30 13:45:10 visual_prompt]: 	Training 500/553. train loss: 1.9186,	2.4126 s / batch. (data: 1.92e+00). ETA=1 day, 3:27:35, max mem: 11.4 GB 
[10/30 13:45:55 visual_prompt]: Epoch 26 / 100: avg data time: 3.95e-01, avg batch time: 0.8888, average train loss: 1.3474
[10/30 13:46:48 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1902, average loss: 1.4086
[10/30 13:46:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.65	
[10/30 13:48:14 visual_prompt]: 	Test 100/162. loss: 1.129, 0.1853 s / batch. (data: 3.39e-05)max mem: 11.41573 GB 
[10/30 13:49:03 visual_prompt]: Inference (test):avg data time: 3.37e-05, avg batch time: 0.1911, average loss: 1.5312
[10/30 13:49:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.82	
[10/30 13:49:03 visual_prompt]: Stopping early.
[10/30 13:49:03 visual_prompt]: Rank of current process: 0. World size: 1
[10/30 13:49:03 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/30 13:49:03 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/30 13:49:03 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/30 13:49:03 visual_prompt]: Training with config:
[10/30 13:49:03 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/test/seed3172/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 3172, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/30 13:49:03 visual_prompt]: Loading training data...
[10/30 13:49:03 visual_prompt]: Constructing mammo-cbis dataset train...
[10/30 13:49:05 visual_prompt]: Loading validation data...
[10/30 13:49:05 visual_prompt]: Constructing mammo-cbis dataset val...
[10/30 13:49:05 visual_prompt]: Loading test data...
[10/30 13:49:05 visual_prompt]: Constructing mammo-cbis dataset test...
[10/30 13:49:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/30 13:49:11 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/30 13:49:11 visual_prompt]: tuned percent:0.529
[10/30 13:49:11 visual_prompt]: Device used for model: 0
[10/30 13:49:11 visual_prompt]: Setting up Evaluator...
[10/30 13:49:11 visual_prompt]: Setting up Trainer...
[10/30 13:49:11 visual_prompt]: 	Setting up the optimizer...
[10/30 13:49:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/30 13:50:43 visual_prompt]: 	Training 100/553. train loss: 1.7317,	0.5000 s / batch. (data: 2.12e-04). ETA=7:39:58, max mem: 11.4 GB 
[10/30 13:52:13 visual_prompt]: 	Training 200/553. train loss: 0.3802,	0.5120 s / batch. (data: 2.72e-04). ETA=7:50:10, max mem: 11.4 GB 
[10/30 13:53:44 visual_prompt]: 	Training 300/553. train loss: 0.7714,	0.4836 s / batch. (data: 2.55e-04). ETA=7:23:19, max mem: 11.4 GB 
[10/30 13:55:11 visual_prompt]: 	Training 400/553. train loss: 1.1985,	1.0082 s / batch. (data: 5.13e-01). ETA=15:22:32, max mem: 11.4 GB 
[10/30 13:56:37 visual_prompt]: 	Training 500/553. train loss: 0.4257,	1.1560 s / batch. (data: 6.58e-01). ETA=17:35:47, max mem: 11.4 GB 
[10/30 13:57:25 visual_prompt]: Epoch 1 / 100: avg data time: 3.97e-01, avg batch time: 0.8920, average train loss: 1.0003
[10/30 13:58:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1927, average loss: 0.9496
[10/30 13:58:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.50	
[10/30 13:59:44 visual_prompt]: 	Test 100/162. loss: 1.108, 0.1924 s / batch. (data: 3.15e-05)max mem: 11.41573 GB 
[10/30 14:00:32 visual_prompt]: Inference (test):avg data time: 3.33e-05, avg batch time: 0.1920, average loss: 0.9881
[10/30 14:00:32 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.86	rocauc: 51.78	
[10/30 14:00:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/30 14:02:04 visual_prompt]: 	Training 100/553. train loss: 0.9127,	0.5032 s / batch. (data: 1.05e-02). ETA=7:38:16, max mem: 11.4 GB 
[10/30 14:03:33 visual_prompt]: 	Training 200/553. train loss: 0.5825,	1.0854 s / batch. (data: 5.85e-01). ETA=16:26:45, max mem: 11.4 GB 
[10/30 14:05:02 visual_prompt]: 	Training 300/553. train loss: 1.1599,	1.4440 s / batch. (data: 9.31e-01). ETA=21:50:19, max mem: 11.4 GB 
[10/30 14:06:32 visual_prompt]: 	Training 400/553. train loss: 0.6446,	1.5219 s / batch. (data: 1.04e+00). ETA=22:58:28, max mem: 11.4 GB 
[10/30 14:07:57 visual_prompt]: 	Training 500/553. train loss: 0.6585,	0.7205 s / batch. (data: 2.35e-01). ETA=10:51:26, max mem: 11.4 GB 
[10/30 14:08:44 visual_prompt]: Epoch 2 / 100: avg data time: 3.95e-01, avg batch time: 0.8892, average train loss: 0.9173
[10/30 14:09:36 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1896, average loss: 1.0560
[10/30 14:09:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.36	
[10/30 14:11:03 visual_prompt]: 	Test 100/162. loss: 0.950, 0.2071 s / batch. (data: 3.03e-05)max mem: 11.41573 GB 
[10/30 14:11:51 visual_prompt]: Inference (test):avg data time: 1.03e-04, avg batch time: 0.1939, average loss: 1.1233
[10/30 14:11:51 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.53	
[10/30 14:11:51 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/30 14:13:25 visual_prompt]: 	Training 100/553. train loss: 0.8222,	0.5160 s / batch. (data: 2.50e-04). ETA=7:45:12, max mem: 11.4 GB 
[10/30 14:14:53 visual_prompt]: 	Training 200/553. train loss: 0.9445,	1.0013 s / batch. (data: 5.01e-01). ETA=15:01:04, max mem: 11.4 GB 
[10/30 14:16:25 visual_prompt]: 	Training 300/553. train loss: 0.0317,	0.5120 s / batch. (data: 7.59e-04). ETA=7:39:52, max mem: 11.4 GB 
[10/30 14:17:50 visual_prompt]: 	Training 400/553. train loss: 0.6680,	0.5162 s / batch. (data: 1.63e-02). ETA=7:42:49, max mem: 11.4 GB 
[10/30 14:19:17 visual_prompt]: 	Training 500/553. train loss: 0.7010,	0.4847 s / batch. (data: 5.39e-03). ETA=7:13:46, max mem: 11.4 GB 
[10/30 14:20:03 visual_prompt]: Epoch 3 / 100: avg data time: 3.96e-01, avg batch time: 0.8899, average train loss: 1.0577
[10/30 14:20:56 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1922, average loss: 0.7393
[10/30 14:20:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.57	
[10/30 14:22:21 visual_prompt]: 	Test 100/162. loss: 0.717, 0.1854 s / batch. (data: 3.22e-05)max mem: 11.41573 GB 
[10/30 14:23:10 visual_prompt]: Inference (test):avg data time: 2.82e-04, avg batch time: 0.1923, average loss: 0.7010
[10/30 14:23:10 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.22	
[10/30 14:23:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/30 14:24:42 visual_prompt]: 	Training 100/553. train loss: 0.8090,	1.0473 s / batch. (data: 5.54e-01). ETA=15:34:33, max mem: 11.4 GB 
[10/30 14:26:12 visual_prompt]: 	Training 200/553. train loss: 1.0096,	0.5040 s / batch. (data: 7.97e-03). ETA=7:28:55, max mem: 11.4 GB 
[10/30 14:27:41 visual_prompt]: 	Training 300/553. train loss: 1.7239,	1.6080 s / batch. (data: 1.10e+00). ETA=23:49:32, max mem: 11.4 GB 
[10/30 14:29:11 visual_prompt]: 	Training 400/553. train loss: 0.7537,	0.4786 s / batch. (data: 2.80e-04). ETA=7:04:39, max mem: 11.4 GB 
[10/30 14:30:39 visual_prompt]: 	Training 500/553. train loss: 1.5811,	1.5400 s / batch. (data: 9.96e-01). ETA=22:43:58, max mem: 11.4 GB 
[10/30 14:31:22 visual_prompt]: Epoch 4 / 100: avg data time: 3.95e-01, avg batch time: 0.8898, average train loss: 1.1110
[10/30 14:32:15 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1923, average loss: 0.9371
[10/30 14:32:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.92	
[10/30 14:33:40 visual_prompt]: 	Test 100/162. loss: 0.965, 0.1986 s / batch. (data: 3.31e-05)max mem: 11.41573 GB 
[10/30 14:34:29 visual_prompt]: Inference (test):avg data time: 3.42e-05, avg batch time: 0.1929, average loss: 0.8625
[10/30 14:34:29 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.66	
[10/30 14:34:29 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/30 14:36:01 visual_prompt]: 	Training 100/553. train loss: 0.5629,	0.6956 s / batch. (data: 2.14e-01). ETA=10:14:18, max mem: 11.4 GB 
[10/30 14:37:31 visual_prompt]: 	Training 200/553. train loss: 0.6453,	2.1600 s / batch. (data: 1.66e+00). ETA=1 day, 7:43:57, max mem: 11.4 GB 
[10/30 14:39:00 visual_prompt]: 	Training 300/553. train loss: 2.2737,	1.4241 s / batch. (data: 9.16e-01). ETA=20:52:55, max mem: 11.4 GB 
[10/30 14:40:27 visual_prompt]: 	Training 400/553. train loss: 2.8598,	0.5121 s / batch. (data: 5.39e-03). ETA=7:29:43, max mem: 11.4 GB 
[10/30 14:41:55 visual_prompt]: 	Training 500/553. train loss: 2.0649,	0.4842 s / batch. (data: 2.39e-04). ETA=7:04:21, max mem: 11.4 GB 
[10/30 14:42:40 visual_prompt]: Epoch 5 / 100: avg data time: 3.93e-01, avg batch time: 0.8870, average train loss: 1.3056
[10/30 14:43:32 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1905, average loss: 1.3330
[10/30 14:43:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.61	
[10/30 14:44:59 visual_prompt]: 	Test 100/162. loss: 1.163, 0.2059 s / batch. (data: 2.91e-05)max mem: 11.41573 GB 
[10/30 14:45:47 visual_prompt]: Inference (test):avg data time: 3.34e-05, avg batch time: 0.1922, average loss: 1.4375
[10/30 14:45:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.64	
[10/30 14:45:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/30 14:47:22 visual_prompt]: 	Training 100/553. train loss: 0.7143,	0.4807 s / batch. (data: 2.75e-04). ETA=7:00:06, max mem: 11.4 GB 
[10/30 14:48:49 visual_prompt]: 	Training 200/553. train loss: 0.5881,	0.4926 s / batch. (data: 3.27e-04). ETA=7:09:39, max mem: 11.4 GB 
[10/30 14:50:18 visual_prompt]: 	Training 300/553. train loss: 0.7768,	0.4972 s / batch. (data: 2.91e-04). ETA=7:12:53, max mem: 11.4 GB 
[10/30 14:51:46 visual_prompt]: 	Training 400/553. train loss: 1.5542,	0.4784 s / batch. (data: 2.74e-04). ETA=6:55:43, max mem: 11.4 GB 
[10/30 14:53:16 visual_prompt]: 	Training 500/553. train loss: 1.0227,	0.5322 s / batch. (data: 2.44e-02). ETA=7:41:33, max mem: 11.4 GB 
[10/30 14:54:02 visual_prompt]: Epoch 6 / 100: avg data time: 3.99e-01, avg batch time: 0.8935, average train loss: 1.4494
[10/30 14:54:54 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1920, average loss: 0.8814
[10/30 14:54:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.56	
[10/30 14:56:21 visual_prompt]: 	Test 100/162. loss: 0.734, 0.2036 s / batch. (data: 3.41e-05)max mem: 11.41573 GB 
[10/30 14:57:10 visual_prompt]: Inference (test):avg data time: 1.03e-04, avg batch time: 0.1927, average loss: 0.9374
[10/30 14:57:10 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.45	
[10/30 14:57:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/30 14:58:43 visual_prompt]: 	Training 100/553. train loss: 0.8154,	0.4778 s / batch. (data: 2.81e-04). ETA=6:53:08, max mem: 11.4 GB 
[10/30 15:00:13 visual_prompt]: 	Training 200/553. train loss: 1.1637,	0.5000 s / batch. (data: 7.97e-03). ETA=7:11:30, max mem: 11.4 GB 
[10/30 15:01:39 visual_prompt]: 	Training 300/553. train loss: 1.5493,	0.4790 s / batch. (data: 2.57e-04). ETA=6:52:33, max mem: 11.4 GB 
[10/30 15:03:08 visual_prompt]: 	Training 400/553. train loss: 1.1038,	0.4808 s / batch. (data: 2.59e-04). ETA=6:53:18, max mem: 11.4 GB 
[10/30 15:04:35 visual_prompt]: 	Training 500/553. train loss: 0.9341,	0.5358 s / batch. (data: 5.83e-03). ETA=7:39:45, max mem: 11.4 GB 
[10/30 15:05:21 visual_prompt]: Epoch 7 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 1.3626
[10/30 15:06:13 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1925, average loss: 0.6930
[10/30 15:06:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 53.98	
[10/30 15:07:40 visual_prompt]: 	Test 100/162. loss: 0.620, 0.2019 s / batch. (data: 4.77e-05)max mem: 11.41573 GB 
[10/30 15:08:28 visual_prompt]: Inference (test):avg data time: 3.36e-05, avg batch time: 0.1923, average loss: 0.6985
[10/30 15:08:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 47.75	rocauc: 54.84	
[10/30 15:08:28 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/30 15:10:01 visual_prompt]: 	Training 100/553. train loss: 3.3163,	0.4843 s / batch. (data: 2.73e-04). ETA=6:54:17, max mem: 11.4 GB 
[10/30 15:11:28 visual_prompt]: 	Training 200/553. train loss: 1.9011,	0.5004 s / batch. (data: 5.42e-03). ETA=7:07:16, max mem: 11.4 GB 
[10/30 15:12:58 visual_prompt]: 	Training 300/553. train loss: 1.3255,	0.5000 s / batch. (data: 2.77e-04). ETA=7:06:04, max mem: 11.4 GB 
[10/30 15:14:27 visual_prompt]: 	Training 400/553. train loss: 1.0628,	0.5160 s / batch. (data: 1.62e-03). ETA=7:18:51, max mem: 11.4 GB 
[10/30 15:15:53 visual_prompt]: 	Training 500/553. train loss: 3.5677,	0.5120 s / batch. (data: 2.62e-04). ETA=7:14:34, max mem: 11.4 GB 
[10/30 15:16:39 visual_prompt]: Epoch 8 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 1.5682
[10/30 15:17:32 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1920, average loss: 1.9839
[10/30 15:17:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.40	
[10/30 15:18:59 visual_prompt]: 	Test 100/162. loss: 2.095, 0.1895 s / batch. (data: 3.41e-05)max mem: 11.41573 GB 
[10/30 15:19:47 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.1930, average loss: 1.7989
[10/30 15:19:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.61	
[10/30 15:19:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/30 15:21:22 visual_prompt]: 	Training 100/553. train loss: 1.0393,	0.5012 s / batch. (data: 9.50e-03). ETA=7:04:08, max mem: 11.4 GB 
[10/30 15:22:53 visual_prompt]: 	Training 200/553. train loss: 1.8534,	0.4880 s / batch. (data: 2.80e-04). ETA=6:52:08, max mem: 11.4 GB 
[10/30 15:24:23 visual_prompt]: 	Training 300/553. train loss: 0.6464,	0.5378 s / batch. (data: 3.78e-02). ETA=7:33:17, max mem: 11.4 GB 
[10/30 15:25:50 visual_prompt]: 	Training 400/553. train loss: 0.9733,	0.6135 s / batch. (data: 1.30e-01). ETA=8:36:05, max mem: 11.4 GB 
[10/30 15:27:16 visual_prompt]: 	Training 500/553. train loss: 1.5212,	0.5114 s / batch. (data: 1.05e-02). ETA=7:09:22, max mem: 11.4 GB 
[10/30 15:28:01 visual_prompt]: Epoch 9 / 100: avg data time: 4.00e-01, avg batch time: 0.8936, average train loss: 1.2634
[10/30 15:28:53 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1914, average loss: 0.7212
[10/30 15:28:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 52.94	
[10/30 15:30:20 visual_prompt]: 	Test 100/162. loss: 0.594, 0.1855 s / batch. (data: 3.34e-05)max mem: 11.41573 GB 
[10/30 15:31:08 visual_prompt]: Inference (test):avg data time: 2.33e-04, avg batch time: 0.1940, average loss: 0.7519
[10/30 15:31:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 42.33	rocauc: 52.16	
[10/30 15:31:08 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/30 15:32:41 visual_prompt]: 	Training 100/553. train loss: 1.1213,	0.5121 s / batch. (data: 7.37e-04). ETA=7:08:36, max mem: 11.4 GB 
[10/30 15:34:07 visual_prompt]: 	Training 200/553. train loss: 0.6259,	0.5038 s / batch. (data: 5.40e-03). ETA=7:00:51, max mem: 11.4 GB 
[10/30 15:35:36 visual_prompt]: 	Training 300/553. train loss: 1.9013,	1.1599 s / batch. (data: 6.54e-01). ETA=16:07:03, max mem: 11.4 GB 
[10/30 15:37:04 visual_prompt]: 	Training 400/553. train loss: 2.7713,	0.4791 s / batch. (data: 2.67e-04). ETA=6:38:37, max mem: 11.4 GB 
[10/30 15:38:33 visual_prompt]: 	Training 500/553. train loss: 1.1322,	0.9244 s / batch. (data: 4.33e-01). ETA=12:47:37, max mem: 11.4 GB 
[10/30 15:39:21 visual_prompt]: Epoch 10 / 100: avg data time: 3.96e-01, avg batch time: 0.8906, average train loss: 1.5891
[10/30 15:40:13 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1900, average loss: 2.0120
[10/30 15:40:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.23	
[10/30 15:41:40 visual_prompt]: 	Test 100/162. loss: 1.690, 0.2076 s / batch. (data: 3.17e-05)max mem: 11.41573 GB 
[10/30 15:42:28 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.1946, average loss: 2.2163
[10/30 15:42:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.04	
[10/30 15:42:28 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/30 15:44:02 visual_prompt]: 	Training 100/553. train loss: 2.2509,	0.5120 s / batch. (data: 2.74e-04). ETA=7:03:50, max mem: 11.4 GB 
[10/30 15:45:29 visual_prompt]: 	Training 200/553. train loss: 0.5895,	0.4960 s / batch. (data: 2.68e-04). ETA=6:49:47, max mem: 11.4 GB 
[10/30 15:46:57 visual_prompt]: 	Training 300/553. train loss: 1.2198,	0.5043 s / batch. (data: 7.98e-03). ETA=6:55:48, max mem: 11.4 GB 
[10/30 15:48:26 visual_prompt]: 	Training 400/553. train loss: 0.0306,	0.4836 s / batch. (data: 5.40e-03). ETA=6:37:53, max mem: 11.4 GB 
[10/30 15:49:57 visual_prompt]: 	Training 500/553. train loss: 1.4884,	0.4877 s / batch. (data: 2.33e-04). ETA=6:40:30, max mem: 11.4 GB 
[10/30 15:50:40 visual_prompt]: Epoch 11 / 100: avg data time: 3.95e-01, avg batch time: 0.8901, average train loss: 1.5143
[10/30 15:51:33 visual_prompt]: Inference (val):avg data time: 2.02e-04, avg batch time: 0.1915, average loss: 1.1293
[10/30 15:51:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.13	
[10/30 15:52:59 visual_prompt]: 	Test 100/162. loss: 0.901, 0.1856 s / batch. (data: 3.50e-05)max mem: 11.41573 GB 
[10/30 15:53:48 visual_prompt]: Inference (test):avg data time: 3.36e-05, avg batch time: 0.1926, average loss: 1.0296
[10/30 15:53:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 58.56	
[10/30 15:53:48 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/30 15:55:22 visual_prompt]: 	Training 100/553. train loss: 1.0482,	0.5069 s / batch. (data: 7.15e-04). ETA=6:54:55, max mem: 11.4 GB 
[10/30 15:56:50 visual_prompt]: 	Training 200/553. train loss: 4.0867,	0.4794 s / batch. (data: 2.17e-04). ETA=6:31:40, max mem: 11.4 GB 
[10/30 15:58:17 visual_prompt]: 	Training 300/553. train loss: 1.7108,	0.4960 s / batch. (data: 2.69e-04). ETA=6:44:22, max mem: 11.4 GB 
[10/30 15:59:47 visual_prompt]: 	Training 400/553. train loss: 1.6490,	0.5021 s / batch. (data: 7.64e-04). ETA=6:48:30, max mem: 11.4 GB 
[10/30 16:01:15 visual_prompt]: 	Training 500/553. train loss: 1.6664,	0.4960 s / batch. (data: 2.76e-04). ETA=6:42:43, max mem: 11.4 GB 
[10/30 16:02:00 visual_prompt]: Epoch 12 / 100: avg data time: 3.96e-01, avg batch time: 0.8908, average train loss: 1.5868
[10/30 16:02:53 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1918, average loss: 2.2753
[10/30 16:02:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.21	
[10/30 16:04:19 visual_prompt]: 	Test 100/162. loss: 2.227, 0.2007 s / batch. (data: 3.34e-05)max mem: 11.41573 GB 
[10/30 16:05:07 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.1930, average loss: 2.0441
[10/30 16:05:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.06	
[10/30 16:05:07 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/30 16:06:41 visual_prompt]: 	Training 100/553. train loss: 3.1637,	0.4920 s / batch. (data: 2.72e-04). ETA=6:38:14, max mem: 11.4 GB 
[10/30 16:08:09 visual_prompt]: 	Training 200/553. train loss: 0.6879,	0.4960 s / batch. (data: 2.82e-04). ETA=6:40:37, max mem: 11.4 GB 
[10/30 16:09:36 visual_prompt]: 	Training 300/553. train loss: 0.8064,	0.4959 s / batch. (data: 5.37e-03). ETA=6:39:43, max mem: 11.4 GB 
[10/30 16:11:05 visual_prompt]: 	Training 400/553. train loss: 3.3420,	0.4909 s / batch. (data: 3.95e-04). ETA=6:34:50, max mem: 11.4 GB 
[10/30 16:12:33 visual_prompt]: 	Training 500/553. train loss: 3.3268,	0.4960 s / batch. (data: 2.75e-04). ETA=6:38:10, max mem: 11.4 GB 
[10/30 16:13:20 visual_prompt]: Epoch 13 / 100: avg data time: 3.96e-01, avg batch time: 0.8907, average train loss: 1.7626
[10/30 16:14:13 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1911, average loss: 1.7448
[10/30 16:14:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.91	
[10/30 16:15:39 visual_prompt]: 	Test 100/162. loss: 1.533, 0.1948 s / batch. (data: 3.77e-05)max mem: 11.41573 GB 
[10/30 16:16:28 visual_prompt]: Inference (test):avg data time: 3.31e-05, avg batch time: 0.1917, average loss: 1.5593
[10/30 16:16:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 58.15	
[10/30 16:16:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/30 16:17:59 visual_prompt]: 	Training 100/553. train loss: 3.3059,	0.4800 s / batch. (data: 2.81e-04). ETA=6:24:04, max mem: 11.4 GB 
[10/30 16:19:29 visual_prompt]: 	Training 200/553. train loss: 3.2699,	0.5154 s / batch. (data: 2.34e-02). ETA=6:51:34, max mem: 11.4 GB 
[10/30 16:20:59 visual_prompt]: 	Training 300/553. train loss: 8.1587,	0.5080 s / batch. (data: 7.97e-03). ETA=6:44:48, max mem: 11.4 GB 
[10/30 16:22:25 visual_prompt]: 	Training 400/553. train loss: 0.6728,	1.4793 s / batch. (data: 1.00e+00). ETA=19:36:19, max mem: 11.4 GB 
[10/30 16:23:56 visual_prompt]: 	Training 500/553. train loss: 2.6596,	1.8000 s / batch. (data: 1.30e+00). ETA=23:48:18, max mem: 11.4 GB 
[10/30 16:24:39 visual_prompt]: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 1.8880
[10/30 16:25:31 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1916, average loss: 0.6941
[10/30 16:25:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 59.59	
[10/30 16:26:57 visual_prompt]: 	Test 100/162. loss: 0.534, 0.1895 s / batch. (data: 3.15e-05)max mem: 11.41573 GB 
[10/30 16:27:46 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.1922, average loss: 0.6653
[10/30 16:27:46 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 60.29	
[10/30 16:27:46 visual_prompt]: Best epoch 14: best metric: -0.694
[10/30 16:27:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/30 16:29:18 visual_prompt]: 	Training 100/553. train loss: 0.3678,	0.4931 s / batch. (data: 2.67e-04). ETA=6:29:59, max mem: 11.4 GB 
[10/30 16:30:47 visual_prompt]: 	Training 200/553. train loss: 0.5327,	0.4809 s / batch. (data: 2.93e-04). ETA=6:19:33, max mem: 11.4 GB 
[10/30 16:32:18 visual_prompt]: 	Training 300/553. train loss: 3.0757,	0.7937 s / batch. (data: 2.98e-01). ETA=10:25:06, max mem: 11.4 GB 
[10/30 16:33:47 visual_prompt]: 	Training 400/553. train loss: 1.0188,	2.3663 s / batch. (data: 1.87e+00). ETA=1 day, 6:59:50, max mem: 11.4 GB 
[10/30 16:35:12 visual_prompt]: 	Training 500/553. train loss: 1.8848,	0.4920 s / batch. (data: 3.12e-04). ETA=6:25:53, max mem: 11.4 GB 
[10/30 16:35:58 visual_prompt]: Epoch 15 / 100: avg data time: 3.95e-01, avg batch time: 0.8884, average train loss: 1.5685
[10/30 16:36:50 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1909, average loss: 0.9354
[10/30 16:36:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 60.90	
[10/30 16:38:16 visual_prompt]: 	Test 100/162. loss: 0.544, 0.2115 s / batch. (data: 5.03e-05)max mem: 11.41573 GB 
[10/30 16:39:05 visual_prompt]: Inference (test):avg data time: 3.35e-05, avg batch time: 0.1928, average loss: 1.0405
[10/30 16:39:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 43.10	rocauc: 58.70	
[10/30 16:39:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/30 16:40:37 visual_prompt]: 	Training 100/553. train loss: 2.3114,	0.5000 s / batch. (data: 2.51e-04). ETA=6:30:52, max mem: 11.4 GB 
[10/30 16:42:07 visual_prompt]: 	Training 200/553. train loss: 1.5897,	0.4925 s / batch. (data: 2.67e-04). ETA=6:24:12, max mem: 11.4 GB 
[10/30 16:43:35 visual_prompt]: 	Training 300/553. train loss: 0.0221,	0.6120 s / batch. (data: 9.18e-02). ETA=7:56:23, max mem: 11.4 GB 
[10/30 16:45:06 visual_prompt]: 	Training 400/553. train loss: 0.0090,	1.8000 s / batch. (data: 1.30e+00). ETA=23:18:07, max mem: 11.4 GB 
[10/30 16:46:32 visual_prompt]: 	Training 500/553. train loss: 1.9320,	0.5042 s / batch. (data: 4.52e-03). ETA=6:30:47, max mem: 11.4 GB 
[10/30 16:47:17 visual_prompt]: Epoch 16 / 100: avg data time: 3.94e-01, avg batch time: 0.8891, average train loss: 1.7234
[10/30 16:48:09 visual_prompt]: Inference (val):avg data time: 2.82e-04, avg batch time: 0.1908, average loss: 1.8573
[10/30 16:48:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.55	
[10/30 16:49:35 visual_prompt]: 	Test 100/162. loss: 1.384, 0.1897 s / batch. (data: 2.81e-05)max mem: 11.41573 GB 
[10/30 16:50:24 visual_prompt]: Inference (test):avg data time: 3.37e-05, avg batch time: 0.1930, average loss: 2.0271
[10/30 16:50:24 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 60.14	
[10/30 16:50:24 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/30 16:51:55 visual_prompt]: 	Training 100/553. train loss: 2.3705,	0.4782 s / batch. (data: 2.63e-04). ETA=6:09:27, max mem: 11.4 GB 
[10/30 16:53:26 visual_prompt]: 	Training 200/553. train loss: 1.2801,	0.4800 s / batch. (data: 2.51e-04). ETA=6:10:01, max mem: 11.4 GB 
[10/30 16:54:54 visual_prompt]: 	Training 300/553. train loss: 0.9010,	2.1770 s / batch. (data: 1.69e+00). ETA=1 day, 3:54:33, max mem: 11.4 GB 
[10/30 16:56:22 visual_prompt]: 	Training 400/553. train loss: 1.5567,	1.4521 s / batch. (data: 9.62e-01). ETA=18:34:30, max mem: 11.4 GB 
[10/30 16:57:50 visual_prompt]: 	Training 500/553. train loss: 0.5196,	0.4958 s / batch. (data: 1.05e-02). ETA=6:19:44, max mem: 11.4 GB 
[10/30 16:58:35 visual_prompt]: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8886, average train loss: 1.4845
[10/30 16:59:28 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1903, average loss: 0.7273
[10/30 16:59:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 57.90	
[10/30 17:00:54 visual_prompt]: 	Test 100/162. loss: 0.476, 0.1856 s / batch. (data: 3.03e-05)max mem: 11.41573 GB 
[10/30 17:01:43 visual_prompt]: Inference (test):avg data time: 3.24e-05, avg batch time: 0.1915, average loss: 0.7058
[10/30 17:01:43 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.53	rocauc: 60.52	
[10/30 17:01:43 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/30 17:03:14 visual_prompt]: 	Training 100/553. train loss: 0.2423,	0.4831 s / batch. (data: 5.40e-03). ETA=6:08:45, max mem: 11.4 GB 
[10/30 17:04:42 visual_prompt]: 	Training 200/553. train loss: 1.1092,	0.5120 s / batch. (data: 2.80e-04). ETA=6:29:58, max mem: 11.4 GB 
[10/30 17:06:09 visual_prompt]: 	Training 300/553. train loss: 1.0749,	0.4999 s / batch. (data: 5.40e-03). ETA=6:19:57, max mem: 11.4 GB 
[10/30 17:07:40 visual_prompt]: 	Training 400/553. train loss: 0.9300,	0.7680 s / batch. (data: 2.64e-01). ETA=9:42:21, max mem: 11.4 GB 
[10/30 17:09:09 visual_prompt]: 	Training 500/553. train loss: 1.5743,	1.6915 s / batch. (data: 1.19e+00). ETA=21:19:52, max mem: 11.4 GB 
[10/30 17:09:54 visual_prompt]: Epoch 18 / 100: avg data time: 3.94e-01, avg batch time: 0.8882, average train loss: 1.8928
[10/30 17:10:46 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1924, average loss: 0.6940
[10/30 17:10:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.44	
[10/30 17:12:13 visual_prompt]: 	Test 100/162. loss: 0.456, 0.1889 s / batch. (data: 4.51e-05)max mem: 11.41573 GB 
[10/30 17:13:02 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.1926, average loss: 0.7139
[10/30 17:13:02 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.43	rocauc: 59.70	
[10/30 17:13:02 visual_prompt]: Best epoch 18: best metric: -0.694
[10/30 17:13:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/30 17:14:34 visual_prompt]: 	Training 100/553. train loss: 3.7011,	0.4960 s / batch. (data: 2.59e-04). ETA=6:14:00, max mem: 11.4 GB 
[10/30 17:16:05 visual_prompt]: 	Training 200/553. train loss: 1.0571,	0.5367 s / batch. (data: 5.74e-02). ETA=6:43:51, max mem: 11.4 GB 
[10/30 17:17:35 visual_prompt]: 	Training 300/553. train loss: 0.8560,	1.9762 s / batch. (data: 1.49e+00). ETA=1 day, 0:43:39, max mem: 11.4 GB 
[10/30 17:19:03 visual_prompt]: 	Training 400/553. train loss: 1.1192,	0.4867 s / batch. (data: 2.75e-04). ETA=6:04:33, max mem: 11.4 GB 
[10/30 17:20:31 visual_prompt]: 	Training 500/553. train loss: 5.1113,	0.4846 s / batch. (data: 2.78e-04). ETA=6:02:12, max mem: 11.4 GB 
[10/30 17:21:16 visual_prompt]: Epoch 19 / 100: avg data time: 3.98e-01, avg batch time: 0.8931, average train loss: 1.5206
[10/30 17:22:09 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1914, average loss: 1.7185
[10/30 17:22:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.45	
[10/30 17:23:36 visual_prompt]: 	Test 100/162. loss: 1.348, 0.1853 s / batch. (data: 3.91e-05)max mem: 11.41573 GB 
[10/30 17:24:25 visual_prompt]: Inference (test):avg data time: 7.47e-05, avg batch time: 0.1926, average loss: 1.5286
[10/30 17:24:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 60.76	
[10/30 17:24:25 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/30 17:25:58 visual_prompt]: 	Training 100/553. train loss: 2.7640,	0.5063 s / batch. (data: 2.06e-02). ETA=6:17:05, max mem: 11.4 GB 
[10/30 17:27:27 visual_prompt]: 	Training 200/553. train loss: 0.6117,	0.8916 s / batch. (data: 3.95e-01). ETA=11:02:38, max mem: 11.4 GB 
[10/30 17:28:57 visual_prompt]: 	Training 300/553. train loss: 1.1361,	0.5320 s / batch. (data: 7.46e-04). ETA=6:34:30, max mem: 11.4 GB 
[10/30 17:30:23 visual_prompt]: 	Training 400/553. train loss: 3.3112,	0.4920 s / batch. (data: 2.60e-04). ETA=6:04:02, max mem: 11.4 GB 
[10/30 17:31:53 visual_prompt]: 	Training 500/553. train loss: 1.1120,	0.5243 s / batch. (data: 3.28e-02). ETA=6:27:02, max mem: 11.4 GB 
[10/30 17:32:39 visual_prompt]: Epoch 20 / 100: avg data time: 3.99e-01, avg batch time: 0.8936, average train loss: 1.5209
[10/30 17:33:32 visual_prompt]: Inference (val):avg data time: 4.49e-04, avg batch time: 0.1907, average loss: 1.1864
[10/30 17:33:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.07	
[10/30 17:34:58 visual_prompt]: 	Test 100/162. loss: 0.838, 0.1906 s / batch. (data: 3.89e-05)max mem: 11.41573 GB 
[10/30 17:35:47 visual_prompt]: Inference (test):avg data time: 3.60e-05, avg batch time: 0.1911, average loss: 1.2977
[10/30 17:35:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.09	rocauc: 60.47	
[10/30 17:35:47 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[10/30 17:37:21 visual_prompt]: 	Training 100/553. train loss: 0.7951,	0.4961 s / batch. (data: 3.55e-04). ETA=6:04:55, max mem: 11.4 GB 
[10/30 17:38:52 visual_prompt]: 	Training 200/553. train loss: 0.7520,	1.8400 s / batch. (data: 1.36e+00). ETA=22:30:35, max mem: 11.4 GB 
[10/30 17:40:22 visual_prompt]: 	Training 300/553. train loss: 5.1718,	1.8712 s / batch. (data: 1.37e+00). ETA=22:50:22, max mem: 11.4 GB 
[10/30 17:41:52 visual_prompt]: 	Training 400/553. train loss: 1.4131,	0.4934 s / batch. (data: 8.36e-04). ETA=6:00:29, max mem: 11.4 GB 
[10/30 17:43:19 visual_prompt]: 	Training 500/553. train loss: 1.3302,	0.4884 s / batch. (data: 2.67e-04). ETA=5:56:04, max mem: 11.4 GB 
[10/30 17:44:03 visual_prompt]: Epoch 21 / 100: avg data time: 4.01e-01, avg batch time: 0.8964, average train loss: 1.4507
[10/30 17:44:56 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1905, average loss: 1.3074
[10/30 17:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.71	
[10/30 17:46:23 visual_prompt]: 	Test 100/162. loss: 0.886, 0.1854 s / batch. (data: 2.93e-05)max mem: 11.41573 GB 
[10/30 17:47:12 visual_prompt]: Inference (test):avg data time: 2.38e-04, avg batch time: 0.1930, average loss: 1.4095
[10/30 17:47:12 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 62.08	
[10/30 17:47:12 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[10/30 17:48:46 visual_prompt]: 	Training 100/553. train loss: 1.7819,	1.5840 s / batch. (data: 1.09e+00). ETA=19:10:42, max mem: 11.4 GB 
[10/30 17:50:14 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.4910 s / batch. (data: 2.55e-04). ETA=5:55:50, max mem: 11.4 GB 
[10/30 17:51:40 visual_prompt]: 	Training 300/553. train loss: 0.8443,	0.5243 s / batch. (data: 8.29e-03). ETA=6:19:07, max mem: 11.4 GB 
[10/30 17:53:10 visual_prompt]: 	Training 400/553. train loss: 1.2869,	1.6113 s / batch. (data: 1.13e+00). ETA=19:22:26, max mem: 11.4 GB 
[10/30 17:54:39 visual_prompt]: 	Training 500/553. train loss: 0.0002,	0.4969 s / batch. (data: 5.41e-03). ETA=5:57:37, max mem: 11.4 GB 
[10/30 17:55:25 visual_prompt]: Epoch 22 / 100: avg data time: 3.97e-01, avg batch time: 0.8924, average train loss: 1.6757
[10/30 17:56:18 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1915, average loss: 1.0275
[10/30 17:56:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 60.73	
[10/30 17:57:45 visual_prompt]: 	Test 100/162. loss: 0.454, 0.1977 s / batch. (data: 3.91e-05)max mem: 11.41573 GB 
[10/30 17:58:34 visual_prompt]: Inference (test):avg data time: 3.08e-05, avg batch time: 0.1935, average loss: 0.9172
[10/30 17:58:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 60.17	
[10/30 17:58:34 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[10/30 18:00:06 visual_prompt]: 	Training 100/553. train loss: 1.0087,	0.4882 s / batch. (data: 2.83e-04). ETA=5:50:07, max mem: 11.4 GB 
[10/30 18:01:34 visual_prompt]: 	Training 200/553. train loss: 0.9527,	0.4960 s / batch. (data: 2.97e-04). ETA=5:54:54, max mem: 11.4 GB 
[10/30 18:03:02 visual_prompt]: 	Training 300/553. train loss: 0.8130,	0.4912 s / batch. (data: 2.49e-04). ETA=5:50:38, max mem: 11.4 GB 
[10/30 18:04:30 visual_prompt]: 	Training 400/553. train loss: 1.7208,	1.4087 s / batch. (data: 9.32e-01). ETA=16:43:19, max mem: 11.4 GB 
[10/30 18:06:03 visual_prompt]: 	Training 500/553. train loss: 2.3552,	2.1677 s / batch. (data: 1.67e+00). ETA=1 day, 1:40:15, max mem: 11.4 GB 
[10/30 18:06:49 visual_prompt]: Epoch 23 / 100: avg data time: 4.00e-01, avg batch time: 0.8952, average train loss: 1.3904
[10/30 18:07:42 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1916, average loss: 3.6119
[10/30 18:07:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.84	
[10/30 18:09:08 visual_prompt]: 	Test 100/162. loss: 3.115, 0.1857 s / batch. (data: 6.72e-05)max mem: 11.41573 GB 
[10/30 18:09:58 visual_prompt]: Inference (test):avg data time: 3.29e-05, avg batch time: 0.1927, average loss: 3.9328
[10/30 18:09:58 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 60.89	
[10/30 18:09:58 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[10/30 18:11:32 visual_prompt]: 	Training 100/553. train loss: 1.2767,	1.7160 s / batch. (data: 1.18e+00). ETA=20:14:58, max mem: 11.4 GB 
[10/30 18:13:02 visual_prompt]: 	Training 200/553. train loss: 0.4394,	0.5000 s / batch. (data: 2.73e-04). ETA=5:53:09, max mem: 11.4 GB 
[10/30 18:14:32 visual_prompt]: 	Training 300/553. train loss: 1.5183,	1.1597 s / batch. (data: 6.49e-01). ETA=13:37:14, max mem: 11.4 GB 
[10/30 18:15:59 visual_prompt]: 	Training 400/553. train loss: 5.4899,	0.4925 s / batch. (data: 1.17e-02). ETA=5:46:13, max mem: 11.4 GB 
[10/30 18:17:27 visual_prompt]: 	Training 500/553. train loss: 0.4815,	0.5000 s / batch. (data: 2.54e-04). ETA=5:50:42, max mem: 11.4 GB 
[10/30 18:18:12 visual_prompt]: Epoch 24 / 100: avg data time: 3.99e-01, avg batch time: 0.8943, average train loss: 1.5364
[10/30 18:19:05 visual_prompt]: Inference (val):avg data time: 3.34e-04, avg batch time: 0.1916, average loss: 1.2294
[10/30 18:19:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.17	
[10/30 18:20:31 visual_prompt]: 	Test 100/162. loss: 0.950, 0.1875 s / batch. (data: 4.03e-05)max mem: 11.41573 GB 
[10/30 18:21:21 visual_prompt]: Inference (test):avg data time: 3.47e-05, avg batch time: 0.1918, average loss: 1.3481
[10/30 18:21:21 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 58.34	
[10/30 18:21:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[10/30 18:22:53 visual_prompt]: 	Training 100/553. train loss: 0.8597,	0.5015 s / batch. (data: 3.33e-04). ETA=5:50:25, max mem: 11.4 GB 
[10/30 18:24:23 visual_prompt]: 	Training 200/553. train loss: 2.3046,	0.5120 s / batch. (data: 7.97e-03). ETA=5:56:55, max mem: 11.4 GB 
[10/30 18:25:51 visual_prompt]: 	Training 300/553. train loss: 2.0987,	0.4954 s / batch. (data: 3.92e-04). ETA=5:44:31, max mem: 11.4 GB 
[10/30 18:27:19 visual_prompt]: 	Training 400/553. train loss: 1.9767,	0.4800 s / batch. (data: 2.74e-04). ETA=5:33:01, max mem: 11.4 GB 
[10/30 18:28:48 visual_prompt]: 	Training 500/553. train loss: 0.8697,	0.5120 s / batch. (data: 1.60e-02). ETA=5:54:22, max mem: 11.4 GB 
[10/30 18:29:35 visual_prompt]: Epoch 25 / 100: avg data time: 4.00e-01, avg batch time: 0.8944, average train loss: 1.5109
[10/30 18:30:28 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1904, average loss: 2.1204
[10/30 18:30:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.87	
[10/30 18:31:55 visual_prompt]: 	Test 100/162. loss: 2.146, 0.1857 s / batch. (data: 5.17e-05)max mem: 11.41573 GB 
[10/30 18:32:43 visual_prompt]: Inference (test):avg data time: 1.05e-04, avg batch time: 0.1924, average loss: 1.8778
[10/30 18:32:43 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 61.22	
[10/30 18:32:43 visual_prompt]: Stopping early.
[10/30 18:32:43 visual_prompt]: Rank of current process: 0. World size: 1
[10/30 18:32:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/30 18:32:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/30 18:32:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/30 18:32:43 visual_prompt]: Training with config:
[10/30 18:32:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/test/seed8393/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 8393, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/30 18:32:43 visual_prompt]: Loading training data...
[10/30 18:32:43 visual_prompt]: Constructing mammo-cbis dataset train...
[10/30 18:32:43 visual_prompt]: Loading validation data...
[10/30 18:32:43 visual_prompt]: Constructing mammo-cbis dataset val...
[10/30 18:32:43 visual_prompt]: Loading test data...
[10/30 18:32:43 visual_prompt]: Constructing mammo-cbis dataset test...
[10/30 18:32:43 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 1765, 768])
load_pretrained: grid-size from 14 to 42
[10/30 18:32:46 visual_prompt]: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/30 18:32:46 visual_prompt]: tuned percent:0.529
[10/30 18:32:46 visual_prompt]: Device used for model: 0
[10/30 18:32:46 visual_prompt]: Setting up Evaluator...
[10/30 18:32:46 visual_prompt]: Setting up Trainer...
[10/30 18:32:46 visual_prompt]: 	Setting up the optimizer...
[10/30 18:32:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/30 18:34:18 visual_prompt]: 	Training 100/553. train loss: 1.7873,	0.5161 s / batch. (data: 5.87e-03). ETA=7:54:50, max mem: 11.4 GB 
[10/30 18:35:46 visual_prompt]: 	Training 200/553. train loss: 2.6571,	1.2871 s / batch. (data: 8.05e-01). ETA=19:42:01, max mem: 11.4 GB 
[10/30 18:37:15 visual_prompt]: 	Training 300/553. train loss: 0.5013,	0.5867 s / batch. (data: 1.08e-01). ETA=8:57:50, max mem: 11.4 GB 
[10/30 18:38:45 visual_prompt]: 	Training 400/553. train loss: 1.5368,	0.5255 s / batch. (data: 2.56e-02). ETA=8:00:52, max mem: 11.4 GB 
[10/30 18:40:13 visual_prompt]: 	Training 500/553. train loss: 0.2311,	0.4859 s / batch. (data: 5.39e-03). ETA=7:23:47, max mem: 11.4 GB 
[10/30 18:40:58 visual_prompt]: Epoch 1 / 100: avg data time: 3.95e-01, avg batch time: 0.8891, average train loss: 1.3150
[10/30 18:41:50 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1917, average loss: 1.1105
[10/30 18:41:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.68	
[10/30 18:43:17 visual_prompt]: 	Test 100/162. loss: 1.258, 0.1925 s / batch. (data: 3.22e-05)max mem: 11.41573 GB 
[10/30 18:44:05 visual_prompt]: Inference (test):avg data time: 7.81e-05, avg batch time: 0.1934, average loss: 1.0377
[10/30 18:44:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 48.44	
[10/30 18:44:05 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/30 18:45:36 visual_prompt]: 	Training 100/553. train loss: 1.1486,	1.9154 s / batch. (data: 1.43e+00). ETA=1 day, 5:04:30, max mem: 11.4 GB 
[10/30 18:47:05 visual_prompt]: 	Training 200/553. train loss: 0.6997,	0.5160 s / batch. (data: 2.67e-04). ETA=7:49:06, max mem: 11.4 GB 
[10/30 18:48:31 visual_prompt]: 	Training 300/553. train loss: 0.6338,	0.6339 s / batch. (data: 1.16e-01). ETA=9:35:15, max mem: 11.4 GB 
[10/30 18:50:00 visual_prompt]: 	Training 400/553. train loss: 0.7799,	1.4800 s / batch. (data: 9.71e-01). ETA=22:20:33, max mem: 11.4 GB 
[10/30 18:51:30 visual_prompt]: 	Training 500/553. train loss: 0.7864,	0.4977 s / batch. (data: 2.76e-04). ETA=7:29:57, max mem: 11.4 GB 
[10/30 18:52:17 visual_prompt]: Epoch 2 / 100: avg data time: 3.95e-01, avg batch time: 0.8888, average train loss: 0.9529
[10/30 18:53:09 visual_prompt]: Inference (val):avg data time: 1.60e-04, avg batch time: 0.1904, average loss: 0.8714
[10/30 18:53:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.82	
[10/30 18:54:36 visual_prompt]: 	Test 100/162. loss: 0.778, 0.2121 s / batch. (data: 1.17e-02)max mem: 11.41573 GB 
[10/30 18:55:24 visual_prompt]: Inference (test):avg data time: 1.03e-04, avg batch time: 0.1918, average loss: 0.9256
[10/30 18:55:24 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 50.30	
[10/30 18:55:24 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/30 18:56:57 visual_prompt]: 	Training 100/553. train loss: 0.5148,	0.5228 s / batch. (data: 2.83e-02). ETA=7:51:21, max mem: 11.4 GB 
[10/30 18:58:24 visual_prompt]: 	Training 200/553. train loss: 0.6812,	0.5000 s / batch. (data: 5.37e-03). ETA=7:29:58, max mem: 11.4 GB 
[10/30 18:59:52 visual_prompt]: 	Training 300/553. train loss: 0.8141,	0.4913 s / batch. (data: 2.71e-04). ETA=7:21:19, max mem: 11.4 GB 
[10/30 19:01:19 visual_prompt]: 	Training 400/553. train loss: 0.6871,	0.4844 s / batch. (data: 5.42e-03). ETA=7:14:17, max mem: 11.4 GB 
[10/30 19:02:49 visual_prompt]: 	Training 500/553. train loss: 0.6584,	0.5093 s / batch. (data: 1.55e-02). ETA=7:35:45, max mem: 11.4 GB 
[10/30 19:03:36 visual_prompt]: Epoch 3 / 100: avg data time: 3.95e-01, avg batch time: 0.8896, average train loss: 1.2265
[10/30 19:04:28 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1925, average loss: 1.2589
[10/30 19:04:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.59	
[10/30 19:05:55 visual_prompt]: 	Test 100/162. loss: 1.109, 0.2010 s / batch. (data: 3.31e-05)max mem: 11.41573 GB 
[10/30 19:06:43 visual_prompt]: Inference (test):avg data time: 9.79e-05, avg batch time: 0.1918, average loss: 1.3542
[10/30 19:06:43 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 55.45	
[10/30 19:06:43 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/30 19:08:16 visual_prompt]: 	Training 100/553. train loss: 0.6953,	1.0090 s / batch. (data: 5.31e-01). ETA=15:00:24, max mem: 11.4 GB 
[10/30 19:09:45 visual_prompt]: 	Training 200/553. train loss: 1.4433,	0.5038 s / batch. (data: 7.96e-03). ETA=7:28:44, max mem: 11.4 GB 
[10/30 19:11:14 visual_prompt]: 	Training 300/553. train loss: 1.1559,	0.5160 s / batch. (data: 2.67e-04). ETA=7:38:43, max mem: 11.4 GB 
[10/30 19:12:42 visual_prompt]: 	Training 400/553. train loss: 0.8241,	0.4922 s / batch. (data: 7.99e-03). ETA=7:16:44, max mem: 11.4 GB 
[10/30 19:14:10 visual_prompt]: 	Training 500/553. train loss: 0.7202,	0.4844 s / batch. (data: 2.87e-04). ETA=7:09:01, max mem: 11.4 GB 
[10/30 19:14:57 visual_prompt]: Epoch 4 / 100: avg data time: 3.97e-01, avg batch time: 0.8918, average train loss: 1.1490
[10/30 19:15:49 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1918, average loss: 0.8790
[10/30 19:15:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.01	
[10/30 19:17:15 visual_prompt]: 	Test 100/162. loss: 0.724, 0.1917 s / batch. (data: 3.22e-05)max mem: 11.41573 GB 
[10/30 19:18:04 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.1928, average loss: 0.9307
[10/30 19:18:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 58.46	
[10/30 19:18:04 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/30 19:19:34 visual_prompt]: 	Training 100/553. train loss: 1.1767,	0.5000 s / batch. (data: 2.63e-04). ETA=7:21:34, max mem: 11.4 GB 
[10/30 19:21:00 visual_prompt]: 	Training 200/553. train loss: 0.7521,	0.4964 s / batch. (data: 1.05e-02). ETA=7:17:35, max mem: 11.4 GB 
[10/30 19:22:30 visual_prompt]: 	Training 300/553. train loss: 1.9251,	0.5003 s / batch. (data: 5.40e-03). ETA=7:20:12, max mem: 11.4 GB 
[10/30 19:24:00 visual_prompt]: 	Training 400/553. train loss: 1.6180,	0.5000 s / batch. (data: 3.01e-04). ETA=7:19:03, max mem: 11.4 GB 
[10/30 19:25:29 visual_prompt]: 	Training 500/553. train loss: 0.5161,	2.0919 s / batch. (data: 1.60e+00). ETA=1 day, 6:33:28, max mem: 11.4 GB 
[10/30 19:26:15 visual_prompt]: Epoch 5 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 1.1865
[10/30 19:27:08 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1901, average loss: 1.2133
[10/30 19:27:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.91	
[10/30 19:28:34 visual_prompt]: 	Test 100/162. loss: 1.243, 0.1924 s / batch. (data: 3.72e-05)max mem: 11.41573 GB 
[10/30 19:29:22 visual_prompt]: Inference (test):avg data time: 7.87e-05, avg batch time: 0.1923, average loss: 1.1072
[10/30 19:29:22 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.61	
[10/30 19:29:22 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/30 19:30:56 visual_prompt]: 	Training 100/553. train loss: 3.1105,	0.5842 s / batch. (data: 7.86e-02). ETA=8:30:30, max mem: 11.4 GB 
[10/30 19:32:24 visual_prompt]: 	Training 200/553. train loss: 0.7183,	1.6080 s / batch. (data: 1.13e+00). ETA=23:22:34, max mem: 11.4 GB 
[10/30 19:33:52 visual_prompt]: 	Training 300/553. train loss: 0.4374,	0.5441 s / batch. (data: 2.95e-04). ETA=7:53:39, max mem: 11.4 GB 
[10/30 19:35:20 visual_prompt]: 	Training 400/553. train loss: 0.9050,	0.5000 s / batch. (data: 2.85e-04). ETA=7:14:26, max mem: 11.4 GB 
[10/30 19:36:48 visual_prompt]: 	Training 500/553. train loss: 3.2598,	0.5080 s / batch. (data: 1.19e-02). ETA=7:20:31, max mem: 11.4 GB 
[10/30 19:37:34 visual_prompt]: Epoch 6 / 100: avg data time: 3.95e-01, avg batch time: 0.8893, average train loss: 1.2922
[10/30 19:38:27 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1928, average loss: 0.6719
[10/30 19:38:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.84	
[10/30 19:39:53 visual_prompt]: 	Test 100/162. loss: 0.587, 0.1987 s / batch. (data: 3.15e-05)max mem: 11.41573 GB 
[10/30 19:40:41 visual_prompt]: Inference (test):avg data time: 3.30e-05, avg batch time: 0.1926, average loss: 0.6709
[10/30 19:40:41 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.91	rocauc: 57.18	
[10/30 19:40:41 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/30 19:42:13 visual_prompt]: 	Training 100/553. train loss: 2.9249,	0.8600 s / batch. (data: 3.78e-01). ETA=12:23:39, max mem: 11.4 GB 
[10/30 19:43:43 visual_prompt]: 	Training 200/553. train loss: 2.4909,	0.5177 s / batch. (data: 1.55e-02). ETA=7:26:48, max mem: 11.4 GB 
[10/30 19:45:10 visual_prompt]: 	Training 300/553. train loss: 1.1223,	0.4920 s / batch. (data: 5.39e-03). ETA=7:03:46, max mem: 11.4 GB 
[10/30 19:46:40 visual_prompt]: 	Training 400/553. train loss: 0.9688,	0.5279 s / batch. (data: 5.46e-03). ETA=7:33:51, max mem: 11.4 GB 
[10/30 19:48:07 visual_prompt]: 	Training 500/553. train loss: 0.7632,	0.5120 s / batch. (data: 2.62e-04). ETA=7:19:18, max mem: 11.4 GB 
[10/30 19:48:55 visual_prompt]: Epoch 7 / 100: avg data time: 3.98e-01, avg batch time: 0.8919, average train loss: 1.4714
[10/30 19:49:47 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1905, average loss: 0.9359
[10/30 19:49:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.41	
[10/30 19:51:14 visual_prompt]: 	Test 100/162. loss: 0.719, 0.1953 s / batch. (data: 3.19e-05)max mem: 11.41573 GB 
[10/30 19:52:02 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.1918, average loss: 1.0044
[10/30 19:52:02 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 59.33	
[10/30 19:52:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/30 19:53:36 visual_prompt]: 	Training 100/553. train loss: 1.0401,	0.4880 s / batch. (data: 2.60e-04). ETA=6:57:28, max mem: 11.4 GB 
[10/30 19:55:04 visual_prompt]: 	Training 200/553. train loss: 0.8174,	0.4907 s / batch. (data: 2.84e-04). ETA=6:58:57, max mem: 11.4 GB 
[10/30 19:56:28 visual_prompt]: 	Training 300/553. train loss: 5.8486,	0.5000 s / batch. (data: 2.38e-04). ETA=7:06:06, max mem: 11.4 GB 
[10/30 19:57:56 visual_prompt]: 	Training 400/553. train loss: 3.1575,	0.5080 s / batch. (data: 2.63e-04). ETA=7:12:01, max mem: 11.4 GB 
[10/30 19:59:27 visual_prompt]: 	Training 500/553. train loss: 0.6822,	0.5238 s / batch. (data: 1.55e-02). ETA=7:24:36, max mem: 11.4 GB 
[10/30 20:00:13 visual_prompt]: Epoch 8 / 100: avg data time: 3.94e-01, avg batch time: 0.8882, average train loss: 1.7533
[10/30 20:01:06 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1926, average loss: 0.7656
[10/30 20:01:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 59.33	
[10/30 20:02:31 visual_prompt]: 	Test 100/162. loss: 0.448, 0.1907 s / batch. (data: 3.60e-05)max mem: 11.41573 GB 
[10/30 20:03:20 visual_prompt]: Inference (test):avg data time: 7.76e-05, avg batch time: 0.1915, average loss: 0.8083
[10/30 20:03:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 48.84	rocauc: 58.18	
[10/30 20:03:20 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/30 20:04:54 visual_prompt]: 	Training 100/553. train loss: 6.5665,	2.3208 s / batch. (data: 1.84e+00). ETA=1 day, 8:44:00, max mem: 11.4 GB 
[10/30 20:06:23 visual_prompt]: 	Training 200/553. train loss: 1.8671,	0.5080 s / batch. (data: 2.74e-04). ETA=7:09:03, max mem: 11.4 GB 
[10/30 20:07:51 visual_prompt]: 	Training 300/553. train loss: 1.2186,	1.4231 s / batch. (data: 9.23e-01). ETA=19:59:37, max mem: 11.4 GB 
[10/30 20:09:18 visual_prompt]: 	Training 400/553. train loss: 5.4080,	0.5040 s / batch. (data: 1.20e-02). ETA=7:03:59, max mem: 11.4 GB 
[10/30 20:10:46 visual_prompt]: 	Training 500/553. train loss: 2.9336,	0.5040 s / batch. (data: 1.19e-02). ETA=7:03:08, max mem: 11.4 GB 
[10/30 20:11:32 visual_prompt]: Epoch 9 / 100: avg data time: 3.93e-01, avg batch time: 0.8883, average train loss: 1.6621
[10/30 20:12:24 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1912, average loss: 2.4187
[10/30 20:12:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.28	
[10/30 20:13:51 visual_prompt]: 	Test 100/162. loss: 1.916, 0.1854 s / batch. (data: 3.03e-05)max mem: 11.41573 GB 
[10/30 20:14:39 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.1924, average loss: 2.6271
[10/30 20:14:39 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 59.95	
[10/30 20:14:39 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/30 20:16:11 visual_prompt]: 	Training 100/553. train loss: 0.7395,	0.5041 s / batch. (data: 2.57e-04). ETA=7:01:55, max mem: 11.4 GB 
[10/30 20:17:41 visual_prompt]: 	Training 200/553. train loss: 0.6941,	0.5080 s / batch. (data: 2.69e-04). ETA=7:04:22, max mem: 11.4 GB 
[10/30 20:19:09 visual_prompt]: 	Training 300/553. train loss: 2.3820,	0.4914 s / batch. (data: 2.77e-04). ETA=6:49:39, max mem: 11.4 GB 
[10/30 20:20:37 visual_prompt]: 	Training 400/553. train loss: 1.5538,	0.5040 s / batch. (data: 1.30e-02). ETA=6:59:23, max mem: 11.4 GB 
[10/30 20:22:06 visual_prompt]: 	Training 500/553. train loss: 0.4852,	0.4960 s / batch. (data: 2.58e-04). ETA=6:51:49, max mem: 11.4 GB 
[10/30 20:22:52 visual_prompt]: Epoch 10 / 100: avg data time: 3.97e-01, avg batch time: 0.8926, average train loss: 1.8774
[10/30 20:23:45 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1909, average loss: 0.6896
[10/30 20:23:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 58.15	
[10/30 20:25:12 visual_prompt]: 	Test 100/162. loss: 0.450, 0.1951 s / batch. (data: 3.27e-05)max mem: 11.41573 GB 
[10/30 20:26:01 visual_prompt]: Inference (test):avg data time: 3.33e-05, avg batch time: 0.1926, average loss: 0.6903
[10/30 20:26:01 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.98	rocauc: 57.45	
[10/30 20:26:01 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/30 20:27:32 visual_prompt]: 	Training 100/553. train loss: 2.2517,	0.5293 s / batch. (data: 1.60e-02). ETA=7:18:09, max mem: 11.4 GB 
[10/30 20:29:00 visual_prompt]: 	Training 200/553. train loss: 5.2648,	1.3916 s / batch. (data: 8.88e-01). ETA=19:09:42, max mem: 11.4 GB 
[10/30 20:30:32 visual_prompt]: 	Training 300/553. train loss: 0.9349,	0.4960 s / batch. (data: 2.71e-04). ETA=6:48:56, max mem: 11.4 GB 
[10/30 20:32:01 visual_prompt]: 	Training 400/553. train loss: 1.8563,	2.6320 s / batch. (data: 2.14e+00). ETA=1 day, 12:05:40, max mem: 11.4 GB 
[10/30 20:33:29 visual_prompt]: 	Training 500/553. train loss: 0.4785,	0.4842 s / batch. (data: 6.88e-03). ETA=6:37:38, max mem: 11.4 GB 
[10/30 20:34:15 visual_prompt]: Epoch 11 / 100: avg data time: 3.99e-01, avg batch time: 0.8941, average train loss: 2.0413
[10/30 20:35:08 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.1911, average loss: 2.0004
[10/30 20:35:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.59	
[10/30 20:36:36 visual_prompt]: 	Test 100/162. loss: 1.584, 0.1981 s / batch. (data: 3.27e-05)max mem: 11.41573 GB 
[10/30 20:37:24 visual_prompt]: Inference (test):avg data time: 1.02e-04, avg batch time: 0.1923, average loss: 2.1774
[10/30 20:37:24 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 57.87	
[10/30 20:37:24 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/30 20:38:57 visual_prompt]: 	Training 100/553. train loss: 1.9291,	0.4962 s / batch. (data: 2.59e-04). ETA=6:46:11, max mem: 11.4 GB 
[10/30 20:40:27 visual_prompt]: 	Training 200/553. train loss: 1.2511,	1.2942 s / batch. (data: 8.02e-01). ETA=17:37:17, max mem: 11.4 GB 
[10/30 20:41:54 visual_prompt]: 	Training 300/553. train loss: 0.6110,	0.4786 s / batch. (data: 2.70e-04). ETA=6:30:11, max mem: 11.4 GB 
[10/30 20:43:24 visual_prompt]: 	Training 400/553. train loss: 0.0002,	0.5000 s / batch. (data: 2.47e-04). ETA=6:46:48, max mem: 11.4 GB 
[10/30 20:44:52 visual_prompt]: 	Training 500/553. train loss: 0.0127,	0.4880 s / batch. (data: 2.37e-04). ETA=6:36:14, max mem: 11.4 GB 
[10/30 20:45:39 visual_prompt]: Epoch 12 / 100: avg data time: 3.99e-01, avg batch time: 0.8942, average train loss: 2.2992
[10/30 20:46:31 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1902, average loss: 1.0765
[10/30 20:46:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.97	
[10/30 20:47:58 visual_prompt]: 	Test 100/162. loss: 1.040, 0.1953 s / batch. (data: 3.41e-05)max mem: 11.41573 GB 
[10/30 20:48:46 visual_prompt]: Inference (test):avg data time: 8.09e-05, avg batch time: 0.1916, average loss: 0.9860
[10/30 20:48:46 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.58	
[10/30 20:48:46 visual_prompt]: Best epoch 12: best metric: -1.076
[10/30 20:48:46 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/30 20:50:17 visual_prompt]: 	Training 100/553. train loss: 0.6399,	0.5120 s / batch. (data: 2.74e-04). ETA=6:54:25, max mem: 11.4 GB 
[10/30 20:51:46 visual_prompt]: 	Training 200/553. train loss: 3.0622,	0.7160 s / batch. (data: 2.11e-01). ETA=9:38:20, max mem: 11.4 GB 
[10/30 20:53:14 visual_prompt]: 	Training 300/553. train loss: 1.5397,	0.5040 s / batch. (data: 2.76e-04). ETA=6:46:16, max mem: 11.4 GB 
[10/30 20:54:43 visual_prompt]: 	Training 400/553. train loss: 0.6096,	0.5040 s / batch. (data: 3.00e-04). ETA=6:45:25, max mem: 11.4 GB 
[10/30 20:56:12 visual_prompt]: 	Training 500/553. train loss: 1.1909,	1.6384 s / batch. (data: 1.14e+00). ETA=21:55:10, max mem: 11.4 GB 
[10/30 20:56:56 visual_prompt]: Epoch 13 / 100: avg data time: 3.92e-01, avg batch time: 0.8864, average train loss: 1.7764
[10/30 20:57:49 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1901, average loss: 1.5598
[10/30 20:57:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.90	
[10/30 20:59:15 visual_prompt]: 	Test 100/162. loss: 1.643, 0.1854 s / batch. (data: 3.10e-05)max mem: 11.41573 GB 
[10/30 21:00:03 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.1923, average loss: 1.4138
[10/30 21:00:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.62	
[10/30 21:00:03 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/30 21:01:36 visual_prompt]: 	Training 100/553. train loss: 0.5553,	0.5120 s / batch. (data: 2.70e-04). ETA=6:49:40, max mem: 11.4 GB 
[10/30 21:03:05 visual_prompt]: 	Training 200/553. train loss: 0.7887,	0.5400 s / batch. (data: 7.98e-03). ETA=7:11:11, max mem: 11.4 GB 
[10/30 21:04:31 visual_prompt]: 	Training 300/553. train loss: 3.7432,	0.5259 s / batch. (data: 2.57e-02). ETA=6:59:04, max mem: 11.4 GB 
[10/30 21:06:02 visual_prompt]: 	Training 400/553. train loss: 5.1842,	2.8437 s / batch. (data: 2.34e+00). ETA=1 day, 13:41:15, max mem: 11.4 GB 
[10/30 21:07:29 visual_prompt]: 	Training 500/553. train loss: 0.8988,	1.4640 s / batch. (data: 9.62e-01). ETA=19:21:42, max mem: 11.4 GB 
[10/30 21:08:15 visual_prompt]: Epoch 14 / 100: avg data time: 3.95e-01, avg batch time: 0.8889, average train loss: 2.0017
[10/30 21:09:08 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1898, average loss: 2.0272
[10/30 21:09:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.21	
[10/30 21:10:35 visual_prompt]: 	Test 100/162. loss: 2.080, 0.1856 s / batch. (data: 3.19e-05)max mem: 11.41573 GB 
[10/30 21:11:22 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.1927, average loss: 1.8247
[10/30 21:11:22 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 60.39	
[10/30 21:11:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/30 21:12:55 visual_prompt]: 	Training 100/553. train loss: 1.5476,	0.4803 s / batch. (data: 2.69e-04). ETA=6:19:55, max mem: 11.4 GB 
[10/30 21:14:23 visual_prompt]: 	Training 200/553. train loss: 3.0803,	0.5040 s / batch. (data: 2.81e-04). ETA=6:37:48, max mem: 11.4 GB 
[10/30 21:15:53 visual_prompt]: 	Training 300/553. train loss: 0.8414,	3.2280 s / batch. (data: 2.74e+00). ETA=1 day, 18:22:30, max mem: 11.4 GB 
[10/30 21:17:21 visual_prompt]: 	Training 400/553. train loss: 2.0301,	0.5126 s / batch. (data: 2.06e-02). ETA=6:42:54, max mem: 11.4 GB 
[10/30 21:18:51 visual_prompt]: 	Training 500/553. train loss: 2.0586,	0.4840 s / batch. (data: 2.60e-04). ETA=6:19:37, max mem: 11.4 GB 
[10/30 21:19:38 visual_prompt]: Epoch 15 / 100: avg data time: 4.01e-01, avg batch time: 0.8959, average train loss: 1.4975
[10/30 21:20:31 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.1913, average loss: 0.6869
[10/30 21:20:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 59.53	
[10/30 21:21:58 visual_prompt]: 	Test 100/162. loss: 0.506, 0.1856 s / batch. (data: 2.98e-05)max mem: 11.41573 GB 
[10/30 21:22:46 visual_prompt]: Inference (test):avg data time: 2.73e-04, avg batch time: 0.1926, average loss: 0.6992
[10/30 21:22:46 visual_prompt]: Classification results with test_mammo-cbis: top1: 55.97	rocauc: 58.99	
[10/30 21:22:46 visual_prompt]: Best epoch 15: best metric: -0.687
[10/30 21:22:46 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/30 21:24:20 visual_prompt]: 	Training 100/553. train loss: 1.1935,	1.3400 s / batch. (data: 8.48e-01). ETA=17:27:30, max mem: 11.4 GB 
[10/30 21:25:49 visual_prompt]: 	Training 200/553. train loss: 0.6289,	0.4840 s / batch. (data: 2.61e-04). ETA=6:17:34, max mem: 11.4 GB 
[10/30 21:27:15 visual_prompt]: 	Training 300/553. train loss: 3.5135,	0.5050 s / batch. (data: 1.29e-02). ETA=6:33:04, max mem: 11.4 GB 
[10/30 21:28:45 visual_prompt]: 	Training 400/553. train loss: 0.0029,	2.4597 s / batch. (data: 1.94e+00). ETA=1 day, 7:50:32, max mem: 11.4 GB 
[10/30 21:30:12 visual_prompt]: 	Training 500/553. train loss: 3.2658,	0.7482 s / batch. (data: 2.49e-01). ETA=9:39:54, max mem: 11.4 GB 
[10/30 21:30:59 visual_prompt]: Epoch 16 / 100: avg data time: 3.96e-01, avg batch time: 0.8912, average train loss: 1.5884
[10/30 21:31:52 visual_prompt]: Inference (val):avg data time: 1.75e-04, avg batch time: 0.1907, average loss: 0.6782
[10/30 21:31:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 61.15	
[10/30 21:33:18 visual_prompt]: 	Test 100/162. loss: 0.563, 0.1854 s / batch. (data: 3.03e-05)max mem: 11.41573 GB 
[10/30 21:34:07 visual_prompt]: Inference (test):avg data time: 3.39e-05, avg batch time: 0.1932, average loss: 0.6734
[10/30 21:34:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.71	rocauc: 59.84	
[10/30 21:34:07 visual_prompt]: Best epoch 16: best metric: -0.678
[10/30 21:34:07 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/30 21:35:38 visual_prompt]: 	Training 100/553. train loss: 2.5446,	2.0160 s / batch. (data: 1.52e+00). ETA=1 day, 1:57:25, max mem: 11.4 GB 
[10/30 21:37:06 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2160 s / batch. (data: 7.38e-01). ETA=15:37:21, max mem: 11.4 GB 
[10/30 21:38:36 visual_prompt]: 	Training 300/553. train loss: 1.2377,	0.4960 s / batch. (data: 2.57e-04). ETA=6:21:32, max mem: 11.4 GB 
[10/30 21:40:04 visual_prompt]: 	Training 400/553. train loss: 0.9191,	0.5000 s / batch. (data: 2.66e-04). ETA=6:23:44, max mem: 11.4 GB 
[10/30 21:41:31 visual_prompt]: 	Training 500/553. train loss: 1.1514,	0.4818 s / batch. (data: 2.86e-04). ETA=6:09:00, max mem: 11.4 GB 
[10/30 21:42:18 visual_prompt]: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 1.7716
[10/30 21:43:11 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1902, average loss: 1.1946
[10/30 21:43:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.87	
[10/30 21:44:37 visual_prompt]: 	Test 100/162. loss: 1.080, 0.1971 s / batch. (data: 5.05e-05)max mem: 11.41573 GB 
[10/30 21:45:26 visual_prompt]: Inference (test):avg data time: 3.41e-05, avg batch time: 0.1915, average loss: 1.0854
[10/30 21:45:26 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.22	
[10/30 21:45:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/30 21:46:58 visual_prompt]: 	Training 100/553. train loss: 0.3359,	0.4868 s / batch. (data: 2.50e-04). ETA=6:11:36, max mem: 11.4 GB 
[10/30 21:48:26 visual_prompt]: 	Training 200/553. train loss: 1.0581,	0.5320 s / batch. (data: 7.56e-04). ETA=6:45:12, max mem: 11.4 GB 
[10/30 21:49:55 visual_prompt]: 	Training 300/553. train loss: 1.3170,	1.0316 s / batch. (data: 5.40e-01). ETA=13:03:58, max mem: 11.4 GB 
[10/30 21:51:25 visual_prompt]: 	Training 400/553. train loss: 1.9790,	0.4979 s / batch. (data: 2.64e-04). ETA=6:17:35, max mem: 11.4 GB 
[10/30 21:52:54 visual_prompt]: 	Training 500/553. train loss: 0.6848,	0.5054 s / batch. (data: 1.05e-02). ETA=6:22:24, max mem: 11.4 GB 
[10/30 21:53:39 visual_prompt]: Epoch 18 / 100: avg data time: 3.97e-01, avg batch time: 0.8919, average train loss: 1.7761
[10/30 21:54:32 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1910, average loss: 0.9219
[10/30 21:54:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 60.63	
[10/30 21:55:58 visual_prompt]: 	Test 100/162. loss: 0.254, 0.1900 s / batch. (data: 3.55e-05)max mem: 11.41573 GB 
[10/30 21:56:46 visual_prompt]: Inference (test):avg data time: 3.18e-05, avg batch time: 0.1924, average loss: 0.9879
[10/30 21:56:46 visual_prompt]: Classification results with test_mammo-cbis: top1: 53.64	rocauc: 60.38	
[10/30 21:56:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/30 21:58:20 visual_prompt]: 	Training 100/553. train loss: 3.1199,	0.4965 s / batch. (data: 1.05e-02). ETA=6:14:24, max mem: 11.4 GB 
[10/30 21:59:51 visual_prompt]: 	Training 200/553. train loss: 1.0958,	0.5146 s / batch. (data: 1.08e-02). ETA=6:27:10, max mem: 11.4 GB 
[10/30 22:01:19 visual_prompt]: 	Training 300/553. train loss: 0.4271,	0.4881 s / batch. (data: 2.68e-04). ETA=6:06:28, max mem: 11.4 GB 
[10/30 22:02:46 visual_prompt]: 	Training 400/553. train loss: 0.6963,	0.5044 s / batch. (data: 5.39e-03). ETA=6:17:51, max mem: 11.4 GB 
[10/30 22:04:12 visual_prompt]: 	Training 500/553. train loss: 2.1544,	0.4956 s / batch. (data: 2.59e-04). ETA=6:10:24, max mem: 11.4 GB 
[10/30 22:04:59 visual_prompt]: Epoch 19 / 100: avg data time: 3.96e-01, avg batch time: 0.8902, average train loss: 1.4513
[10/30 22:05:51 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1908, average loss: 3.0359
[10/30 22:05:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.85	
[10/30 22:07:18 visual_prompt]: 	Test 100/162. loss: 2.853, 0.1854 s / batch. (data: 5.10e-05)max mem: 11.41573 GB 
[10/30 22:08:06 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.1921, average loss: 2.7187
[10/30 22:08:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.64	
[10/30 22:08:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/30 22:09:38 visual_prompt]: 	Training 100/553. train loss: 2.0620,	0.5079 s / batch. (data: 2.41e-02). ETA=6:18:17, max mem: 11.4 GB 
[10/30 22:11:08 visual_prompt]: 	Training 200/553. train loss: 1.2649,	0.5040 s / batch. (data: 2.64e-04). ETA=6:14:34, max mem: 11.4 GB 
[10/30 22:12:37 visual_prompt]: 	Training 300/553. train loss: 1.0881,	0.5218 s / batch. (data: 9.74e-03). ETA=6:26:55, max mem: 11.4 GB 
[10/30 22:14:04 visual_prompt]: 	Training 400/553. train loss: 0.5868,	0.5000 s / batch. (data: 2.67e-04). ETA=6:09:56, max mem: 11.4 GB 
[10/30 22:15:30 visual_prompt]: 	Training 500/553. train loss: 1.6402,	0.5109 s / batch. (data: 2.76e-04). ETA=6:17:09, max mem: 11.4 GB 
[10/30 22:16:18 visual_prompt]: Epoch 20 / 100: avg data time: 3.93e-01, avg batch time: 0.8887, average train loss: 1.3044
[10/30 22:17:10 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1910, average loss: 0.7863
[10/30 22:17:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.53	
[10/30 22:18:37 visual_prompt]: 	Test 100/162. loss: 0.418, 0.1878 s / batch. (data: 3.17e-05)max mem: 11.41573 GB 
[10/30 22:19:25 visual_prompt]: Inference (test):avg data time: 3.20e-05, avg batch time: 0.1929, average loss: 0.8226
[10/30 22:19:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 49.15	rocauc: 60.60	
[10/30 22:19:25 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[10/30 22:20:56 visual_prompt]: 	Training 100/553. train loss: 0.4765,	0.4899 s / batch. (data: 1.16e-02). ETA=6:00:24, max mem: 11.4 GB 
[10/30 22:22:25 visual_prompt]: 	Training 200/553. train loss: 1.9471,	1.7200 s / batch. (data: 1.22e+00). ETA=21:02:28, max mem: 11.4 GB 
[10/30 22:23:54 visual_prompt]: 	Training 300/553. train loss: 2.2616,	0.4788 s / batch. (data: 2.73e-04). ETA=5:50:38, max mem: 11.4 GB 
[10/30 22:25:23 visual_prompt]: 	Training 400/553. train loss: 1.8110,	0.4791 s / batch. (data: 2.52e-04). ETA=5:50:05, max mem: 11.4 GB 
[10/30 22:26:52 visual_prompt]: 	Training 500/553. train loss: 0.8422,	0.4909 s / batch. (data: 2.93e-04). ETA=5:57:50, max mem: 11.4 GB 
[10/30 22:27:37 visual_prompt]: Epoch 21 / 100: avg data time: 3.94e-01, avg batch time: 0.8894, average train loss: 1.4275
[10/30 22:28:29 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1925, average loss: 0.7969
[10/30 22:28:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 62.64	
[10/30 22:29:56 visual_prompt]: 	Test 100/162. loss: 0.508, 0.1854 s / batch. (data: 3.10e-05)max mem: 11.41573 GB 
[10/30 22:30:44 visual_prompt]: Inference (test):avg data time: 1.75e-04, avg batch time: 0.1928, average loss: 0.7525
[10/30 22:30:44 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.55	rocauc: 60.46	
[10/30 22:30:44 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[10/30 22:32:13 visual_prompt]: 	Training 100/553. train loss: 1.2591,	0.4882 s / batch. (data: 2.77e-04). ETA=5:54:41, max mem: 11.4 GB 
[10/30 22:33:44 visual_prompt]: 	Training 200/553. train loss: 2.0726,	2.3080 s / batch. (data: 1.82e+00). ETA=1 day, 3:52:48, max mem: 11.4 GB 
[10/30 22:35:13 visual_prompt]: 	Training 300/553. train loss: 0.8347,	0.4880 s / batch. (data: 2.81e-04). ETA=5:52:51, max mem: 11.4 GB 
[10/30 22:36:43 visual_prompt]: 	Training 400/553. train loss: 0.3696,	0.4914 s / batch. (data: 2.56e-04). ETA=5:54:31, max mem: 11.4 GB 
[10/30 22:38:13 visual_prompt]: 	Training 500/553. train loss: 1.3968,	0.4887 s / batch. (data: 2.58e-04). ETA=5:51:45, max mem: 11.4 GB 
[10/30 22:38:59 visual_prompt]: Epoch 22 / 100: avg data time: 3.98e-01, avg batch time: 0.8940, average train loss: 1.3334
[10/30 22:39:52 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1917, average loss: 1.1975
[10/30 22:39:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.61	
[10/30 22:41:19 visual_prompt]: 	Test 100/162. loss: 1.172, 0.1951 s / batch. (data: 2.91e-05)max mem: 11.41573 GB 
[10/30 22:42:07 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.1911, average loss: 1.0759
[10/30 22:42:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.95	
[10/30 22:42:07 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[10/30 22:43:39 visual_prompt]: 	Training 100/553. train loss: 1.6992,	0.5040 s / batch. (data: 1.59e-02). ETA=6:01:28, max mem: 11.4 GB 
[10/30 22:45:11 visual_prompt]: 	Training 200/553. train loss: 2.5997,	0.5159 s / batch. (data: 1.19e-02). ETA=6:09:10, max mem: 11.4 GB 
[10/30 22:46:40 visual_prompt]: 	Training 300/553. train loss: 0.6215,	0.5160 s / batch. (data: 7.96e-03). ETA=6:08:22, max mem: 11.4 GB 
[10/30 22:48:06 visual_prompt]: 	Training 400/553. train loss: 2.0428,	0.5240 s / batch. (data: 2.81e-04). ETA=6:13:12, max mem: 11.4 GB 
[10/30 22:49:37 visual_prompt]: 	Training 500/553. train loss: 1.1072,	0.5083 s / batch. (data: 2.79e-04). ETA=6:01:12, max mem: 11.4 GB 
[10/30 22:50:22 visual_prompt]: Epoch 23 / 100: avg data time: 4.01e-01, avg batch time: 0.8953, average train loss: 1.4227
[10/30 22:51:15 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.1923, average loss: 3.2454
[10/30 22:51:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.64	
[10/30 22:52:42 visual_prompt]: 	Test 100/162. loss: 3.299, 0.1854 s / batch. (data: 3.10e-05)max mem: 11.41573 GB 
[10/30 22:53:31 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.1924, average loss: 2.9220
[10/30 22:53:31 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.82	
[10/30 22:53:31 visual_prompt]: Stopping early.
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

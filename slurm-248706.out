/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/20 11:50:11 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 11:50:12 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 11:50:12 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/20 11:50:12 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 11:50:12 visual_prompt]: Training with config:
[11/20 11:50:12 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.005_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 11:50:12 visual_prompt]: Loading training data...
[11/20 11:50:12 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 11:50:12 visual_prompt]: Loading validation data...
[11/20 11:50:12 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 11:50:12 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 11:50:16 visual_prompt]: Enable all parameters update during training
[11/20 11:50:16 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 11:50:16 visual_prompt]: tuned percent:100.000
[11/20 11:50:17 visual_prompt]: Device used for model: 0
[11/20 11:50:17 visual_prompt]: Setting up Evaluator...
[11/20 11:50:17 visual_prompt]: Setting up Trainer...
[11/20 11:50:17 visual_prompt]: 	Setting up the optimizer...
[11/20 11:50:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 11:52:07 visual_prompt]: 	Training 100/553. train loss: 10.8600,	1.8831 s / batch. (data: 9.44e-01). ETA=1 day, 4:52:28, max mem: 23.5 GB 
[11/20 11:53:51 visual_prompt]: 	Training 200/553. train loss: 9.4496,	2.6973 s / batch. (data: 1.77e+00). ETA=1 day, 17:17:00, max mem: 23.5 GB 
[11/20 11:55:34 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9408 s / batch. (data: 5.39e-03). ETA=14:22:22, max mem: 23.5 GB 
[11/20 11:57:12 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9856 s / batch. (data: 3.69e-02). ETA=15:01:48, max mem: 23.5 GB 
[11/20 11:58:57 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9124 s / batch. (data: 2.92e-04). ETA=13:53:17, max mem: 23.5 GB 
[11/20 12:00:02 visual_prompt]: Epoch 1 / 100: avg data time: 1.27e-01, avg batch time: 1.0588, average train loss: 7.6130
[11/20 12:01:30 visual_prompt]: Inference (val):avg data time: 2.90e-04, avg batch time: 0.3028, average loss: 6.9126
[11/20 12:01:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 12:01:30 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 12:03:20 visual_prompt]: 	Training 100/553. train loss: 0.9452,	0.9360 s / batch. (data: 8.35e-04). ETA=14:12:28, max mem: 23.5 GB 
[11/20 12:04:56 visual_prompt]: 	Training 200/553. train loss: 0.6642,	2.6760 s / batch. (data: 1.74e+00). ETA=1 day, 16:32:47, max mem: 23.5 GB 
[11/20 12:06:35 visual_prompt]: 	Training 300/553. train loss: 0.9095,	0.9200 s / batch. (data: 2.65e-04). ETA=13:54:51, max mem: 23.5 GB 
[11/20 12:08:15 visual_prompt]: 	Training 400/553. train loss: 0.6858,	0.9390 s / batch. (data: 2.92e-04). ETA=14:10:30, max mem: 23.5 GB 
[11/20 12:09:53 visual_prompt]: 	Training 500/553. train loss: 1.2723,	0.9402 s / batch. (data: 3.94e-03). ETA=14:10:01, max mem: 23.5 GB 
[11/20 12:10:43 visual_prompt]: Epoch 2 / 100: avg data time: 6.90e-02, avg batch time: 0.9991, average train loss: 1.2956
[11/20 12:11:41 visual_prompt]: Inference (val):avg data time: 2.63e-04, avg batch time: 0.3061, average loss: 0.8355
[11/20 12:11:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.97	
[11/20 12:11:41 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 12:13:29 visual_prompt]: 	Training 100/553. train loss: 0.6008,	0.9340 s / batch. (data: 7.45e-04). ETA=14:02:01, max mem: 23.5 GB 
[11/20 12:15:09 visual_prompt]: 	Training 200/553. train loss: 3.0885,	0.9160 s / batch. (data: 2.93e-04). ETA=13:44:17, max mem: 23.5 GB 
[11/20 12:16:47 visual_prompt]: 	Training 300/553. train loss: 0.6137,	0.9105 s / batch. (data: 2.64e-04). ETA=13:37:51, max mem: 23.5 GB 
[11/20 12:18:25 visual_prompt]: 	Training 400/553. train loss: 0.3420,	0.9430 s / batch. (data: 5.85e-03). ETA=14:05:26, max mem: 23.5 GB 
[11/20 12:20:00 visual_prompt]: 	Training 500/553. train loss: 1.9176,	0.9400 s / batch. (data: 2.60e-04). ETA=14:01:12, max mem: 23.5 GB 
[11/20 12:20:53 visual_prompt]: Epoch 3 / 100: avg data time: 6.45e-02, avg batch time: 0.9965, average train loss: 0.8792
[11/20 12:21:51 visual_prompt]: Inference (val):avg data time: 8.33e-05, avg batch time: 0.3051, average loss: 1.0922
[11/20 12:21:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.48	
[11/20 12:21:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 12:23:38 visual_prompt]: 	Training 100/553. train loss: 1.3720,	0.9234 s / batch. (data: 2.56e-04). ETA=13:44:00, max mem: 23.5 GB 
[11/20 12:25:17 visual_prompt]: 	Training 200/553. train loss: 0.9382,	1.7865 s / batch. (data: 8.48e-01). ETA=1 day, 2:31:12, max mem: 23.5 GB 
[11/20 12:26:53 visual_prompt]: 	Training 300/553. train loss: 1.3174,	0.9549 s / batch. (data: 1.05e-02). ETA=14:08:53, max mem: 23.5 GB 
[11/20 12:28:32 visual_prompt]: 	Training 400/553. train loss: 0.5539,	0.9073 s / batch. (data: 2.73e-04). ETA=13:25:04, max mem: 23.5 GB 
[11/20 12:30:13 visual_prompt]: 	Training 500/553. train loss: 0.6447,	0.9646 s / batch. (data: 2.77e-04). ETA=14:14:18, max mem: 23.5 GB 
[11/20 12:31:04 visual_prompt]: Epoch 4 / 100: avg data time: 6.90e-02, avg batch time: 1.0005, average train loss: 0.8800
[11/20 12:32:03 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3045, average loss: 0.6958
[11/20 12:32:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.68	
[11/20 12:32:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 12:33:50 visual_prompt]: 	Training 100/553. train loss: 0.5594,	0.9360 s / batch. (data: 2.89e-04). ETA=13:46:36, max mem: 23.5 GB 
[11/20 12:35:31 visual_prompt]: 	Training 200/553. train loss: 0.6114,	0.9908 s / batch. (data: 7.97e-03). ETA=14:33:19, max mem: 23.5 GB 
[11/20 12:37:09 visual_prompt]: 	Training 300/553. train loss: 1.4400,	0.9541 s / batch. (data: 1.04e-02). ETA=13:59:24, max mem: 23.5 GB 
[11/20 12:38:49 visual_prompt]: 	Training 400/553. train loss: 0.7134,	3.7898 s / batch. (data: 2.88e+00). ETA=2 days, 7:27:57, max mem: 23.5 GB 
[11/20 12:40:29 visual_prompt]: 	Training 500/553. train loss: 0.6110,	0.9400 s / batch. (data: 2.80e-04). ETA=13:43:52, max mem: 23.5 GB 
[11/20 12:41:18 visual_prompt]: Epoch 5 / 100: avg data time: 7.37e-02, avg batch time: 1.0037, average train loss: 0.8402
[11/20 12:42:17 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3034, average loss: 0.7450
[11/20 12:42:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.07	
[11/20 12:42:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 12:44:06 visual_prompt]: 	Training 100/553. train loss: 0.7235,	0.9680 s / batch. (data: 3.43e-04). ETA=14:05:55, max mem: 23.5 GB 
[11/20 12:45:42 visual_prompt]: 	Training 200/553. train loss: 1.0460,	0.9401 s / batch. (data: 2.86e-04). ETA=13:39:58, max mem: 23.5 GB 
[11/20 12:47:20 visual_prompt]: 	Training 300/553. train loss: 0.8026,	0.9513 s / batch. (data: 1.53e-02). ETA=13:48:13, max mem: 23.5 GB 
[11/20 12:48:59 visual_prompt]: 	Training 400/553. train loss: 0.7359,	3.0600 s / batch. (data: 2.14e+00). ETA=1 day, 20:18:53, max mem: 23.5 GB 
[11/20 12:50:40 visual_prompt]: 	Training 500/553. train loss: 1.0744,	0.9488 s / batch. (data: 2.57e-04). ETA=13:42:50, max mem: 23.5 GB 
[11/20 12:51:29 visual_prompt]: Epoch 6 / 100: avg data time: 6.79e-02, avg batch time: 0.9995, average train loss: 0.8559
[11/20 12:52:28 visual_prompt]: Inference (val):avg data time: 2.57e-04, avg batch time: 0.3047, average loss: 0.6949
[11/20 12:52:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.84	
[11/20 12:52:28 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 12:54:21 visual_prompt]: 	Training 100/553. train loss: 0.8347,	0.9440 s / batch. (data: 2.65e-04). ETA=13:36:17, max mem: 23.5 GB 
[11/20 12:55:58 visual_prompt]: 	Training 200/553. train loss: 0.6483,	0.9311 s / batch. (data: 1.93e-03). ETA=13:23:33, max mem: 23.5 GB 
[11/20 12:57:34 visual_prompt]: 	Training 300/553. train loss: 0.7766,	0.9551 s / batch. (data: 2.52e-04). ETA=13:42:44, max mem: 23.5 GB 
[11/20 12:59:10 visual_prompt]: 	Training 400/553. train loss: 0.6395,	0.9440 s / batch. (data: 5.39e-03). ETA=13:31:33, max mem: 23.5 GB 
[11/20 13:00:49 visual_prompt]: 	Training 500/553. train loss: 0.6784,	0.9320 s / batch. (data: 3.98e-03). ETA=13:19:40, max mem: 23.5 GB 
[11/20 13:01:40 visual_prompt]: Epoch 7 / 100: avg data time: 6.62e-02, avg batch time: 0.9977, average train loss: 0.8175
[11/20 13:02:38 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3042, average loss: 0.6872
[11/20 13:02:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 55.90	
[11/20 13:02:38 visual_prompt]: Best epoch 7: best metric: -0.687
[11/20 13:02:38 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 13:04:26 visual_prompt]: 	Training 100/553. train loss: 0.6043,	0.9600 s / batch. (data: 1.55e-02). ETA=13:41:14, max mem: 23.5 GB 
[11/20 13:06:05 visual_prompt]: 	Training 200/553. train loss: 0.8574,	0.9286 s / batch. (data: 5.37e-03). ETA=13:12:52, max mem: 23.5 GB 
[11/20 13:07:44 visual_prompt]: 	Training 300/553. train loss: 0.7348,	0.9254 s / batch. (data: 1.04e-02). ETA=13:08:36, max mem: 23.5 GB 
[11/20 13:09:24 visual_prompt]: 	Training 400/553. train loss: 0.6876,	0.9215 s / batch. (data: 5.39e-03). ETA=13:03:43, max mem: 23.5 GB 
[11/20 13:11:03 visual_prompt]: 	Training 500/553. train loss: 0.6065,	0.9226 s / batch. (data: 7.98e-03). ETA=13:03:08, max mem: 23.5 GB 
[11/20 13:11:53 visual_prompt]: Epoch 8 / 100: avg data time: 7.05e-02, avg batch time: 1.0027, average train loss: 0.7913
[11/20 13:12:52 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3058, average loss: 0.7066
[11/20 13:12:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.22	
[11/20 13:12:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 13:14:43 visual_prompt]: 	Training 100/553. train loss: 0.7685,	0.9208 s / batch. (data: 7.99e-03). ETA=12:59:13, max mem: 23.5 GB 
[11/20 13:16:22 visual_prompt]: 	Training 200/553. train loss: 1.3163,	0.9630 s / batch. (data: 5.40e-03). ETA=13:33:19, max mem: 23.5 GB 
[11/20 13:17:58 visual_prompt]: 	Training 300/553. train loss: 0.6712,	0.9217 s / batch. (data: 2.50e-04). ETA=12:56:56, max mem: 23.5 GB 
[11/20 13:19:37 visual_prompt]: 	Training 400/553. train loss: 0.6897,	0.9385 s / batch. (data: 5.84e-03). ETA=13:09:29, max mem: 23.5 GB 
[11/20 13:21:11 visual_prompt]: 	Training 500/553. train loss: 0.7533,	0.9218 s / batch. (data: 2.67e-04). ETA=12:53:55, max mem: 23.5 GB 
[11/20 13:22:04 visual_prompt]: Epoch 9 / 100: avg data time: 6.63e-02, avg batch time: 0.9978, average train loss: 0.7723
[11/20 13:23:02 visual_prompt]: Inference (val):avg data time: 1.41e-04, avg batch time: 0.3054, average loss: 0.7004
[11/20 13:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.99	
[11/20 13:23:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 13:24:52 visual_prompt]: 	Training 100/553. train loss: 0.9533,	0.9400 s / batch. (data: 6.52e-03). ETA=13:06:50, max mem: 23.5 GB 
[11/20 13:26:32 visual_prompt]: 	Training 200/553. train loss: 0.7712,	0.9480 s / batch. (data: 2.78e-04). ETA=13:11:56, max mem: 23.5 GB 
[11/20 13:28:07 visual_prompt]: 	Training 300/553. train loss: 0.7950,	0.9501 s / batch. (data: 5.85e-03). ETA=13:12:08, max mem: 23.5 GB 
[11/20 13:29:45 visual_prompt]: 	Training 400/553. train loss: 0.6543,	0.9657 s / batch. (data: 5.38e-03). ETA=13:23:29, max mem: 23.5 GB 
[11/20 13:31:23 visual_prompt]: 	Training 500/553. train loss: 0.7171,	1.6931 s / batch. (data: 7.83e-01). ETA=23:25:57, max mem: 23.5 GB 
[11/20 13:32:16 visual_prompt]: Epoch 10 / 100: avg data time: 6.93e-02, avg batch time: 1.0014, average train loss: 0.7251
[11/20 13:33:14 visual_prompt]: Inference (val):avg data time: 7.24e-04, avg batch time: 0.3055, average loss: 0.7069
[11/20 13:33:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.33	
[11/20 13:33:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 13:35:05 visual_prompt]: 	Training 100/553. train loss: 0.7719,	0.9607 s / batch. (data: 1.05e-02). ETA=13:15:16, max mem: 23.5 GB 
[11/20 13:36:42 visual_prompt]: 	Training 200/553. train loss: 0.7517,	0.9254 s / batch. (data: 5.47e-03). ETA=12:44:31, max mem: 23.5 GB 
[11/20 13:38:19 visual_prompt]: 	Training 300/553. train loss: 0.5713,	0.9440 s / batch. (data: 2.67e-04). ETA=12:58:19, max mem: 23.5 GB 
[11/20 13:39:57 visual_prompt]: 	Training 400/553. train loss: 0.6939,	0.9320 s / batch. (data: 2.80e-04). ETA=12:46:53, max mem: 23.5 GB 
[11/20 13:41:34 visual_prompt]: 	Training 500/553. train loss: 0.7759,	0.9422 s / batch. (data: 3.05e-04). ETA=12:53:41, max mem: 23.5 GB 
[11/20 13:42:24 visual_prompt]: Epoch 11 / 100: avg data time: 6.27e-02, avg batch time: 0.9938, average train loss: 0.7173
[11/20 13:43:23 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3039, average loss: 0.7248
[11/20 13:43:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.56	
[11/20 13:43:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 13:45:08 visual_prompt]: 	Training 100/553. train loss: 0.5654,	0.9333 s / batch. (data: 2.79e-04). ETA=12:43:58, max mem: 23.5 GB 
[11/20 13:46:49 visual_prompt]: 	Training 200/553. train loss: 0.6514,	0.9253 s / batch. (data: 5.36e-03). ETA=12:35:54, max mem: 23.5 GB 
[11/20 13:48:26 visual_prompt]: 	Training 300/553. train loss: 0.7770,	0.9246 s / batch. (data: 5.83e-03). ETA=12:33:49, max mem: 23.5 GB 
[11/20 13:50:04 visual_prompt]: 	Training 400/553. train loss: 0.7927,	0.9059 s / batch. (data: 2.68e-04). ETA=12:17:02, max mem: 23.5 GB 
[11/20 13:51:43 visual_prompt]: 	Training 500/553. train loss: 0.7470,	0.9161 s / batch. (data: 2.66e-04). ETA=12:23:49, max mem: 23.5 GB 
[11/20 13:52:34 visual_prompt]: Epoch 12 / 100: avg data time: 6.59e-02, avg batch time: 0.9959, average train loss: 0.7193
[11/20 13:53:32 visual_prompt]: Inference (val):avg data time: 2.94e-04, avg batch time: 0.3046, average loss: 0.8856
[11/20 13:53:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.01	
[11/20 13:53:32 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 13:55:21 visual_prompt]: 	Training 100/553. train loss: 0.7335,	0.9200 s / batch. (data: 3.07e-04). ETA=12:24:36, max mem: 23.5 GB 
[11/20 13:56:57 visual_prompt]: 	Training 200/553. train loss: 0.6151,	0.9160 s / batch. (data: 2.65e-04). ETA=12:19:54, max mem: 23.5 GB 
[11/20 13:58:36 visual_prompt]: 	Training 300/553. train loss: 0.5480,	0.9193 s / batch. (data: 7.95e-03). ETA=12:21:03, max mem: 23.5 GB 
[11/20 14:00:17 visual_prompt]: 	Training 400/553. train loss: 0.6916,	0.9420 s / batch. (data: 5.36e-03). ETA=12:37:46, max mem: 23.5 GB 
[11/20 14:01:54 visual_prompt]: 	Training 500/553. train loss: 0.6251,	0.9575 s / batch. (data: 5.82e-03). ETA=12:48:38, max mem: 23.5 GB 
[11/20 14:02:44 visual_prompt]: Epoch 13 / 100: avg data time: 6.58e-02, avg batch time: 0.9975, average train loss: 0.7058
[11/20 14:03:42 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3063, average loss: 0.6886
[11/20 14:03:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.99	
[11/20 14:03:42 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 14:05:32 visual_prompt]: 	Training 100/553. train loss: 0.6928,	0.9213 s / batch. (data: 4.11e-03). ETA=12:17:11, max mem: 23.5 GB 
[11/20 14:07:12 visual_prompt]: 	Training 200/553. train loss: 0.6326,	0.9311 s / batch. (data: 4.62e-04). ETA=12:23:31, max mem: 23.5 GB 
[11/20 14:08:50 visual_prompt]: 	Training 300/553. train loss: 0.7323,	0.9292 s / batch. (data: 2.29e-02). ETA=12:20:28, max mem: 23.5 GB 
[11/20 14:10:27 visual_prompt]: 	Training 400/553. train loss: 0.7059,	0.9506 s / batch. (data: 8.09e-03). ETA=12:35:54, max mem: 23.5 GB 
[11/20 14:12:03 visual_prompt]: 	Training 500/553. train loss: 0.6986,	0.9649 s / batch. (data: 4.09e-02). ETA=12:45:38, max mem: 23.5 GB 
[11/20 14:12:55 visual_prompt]: Epoch 14 / 100: avg data time: 6.97e-02, avg batch time: 1.0000, average train loss: 0.6957
[11/20 14:13:53 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3044, average loss: 0.6897
[11/20 14:13:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/20 14:13:53 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 14:15:42 visual_prompt]: 	Training 100/553. train loss: 0.6171,	0.9541 s / batch. (data: 8.23e-03). ETA=12:34:40, max mem: 23.5 GB 
[11/20 14:17:22 visual_prompt]: 	Training 200/553. train loss: 0.6982,	0.9532 s / batch. (data: 2.12e-02). ETA=12:32:20, max mem: 23.5 GB 
[11/20 14:18:59 visual_prompt]: 	Training 300/553. train loss: 0.7662,	0.9560 s / batch. (data: 5.39e-03). ETA=12:32:56, max mem: 23.5 GB 
[11/20 14:20:37 visual_prompt]: 	Training 400/553. train loss: 0.7340,	0.9280 s / batch. (data: 3.08e-04). ETA=12:09:22, max mem: 23.5 GB 
[11/20 14:22:13 visual_prompt]: 	Training 500/553. train loss: 0.8565,	0.9512 s / batch. (data: 1.04e-02). ETA=12:26:00, max mem: 23.5 GB 
[11/20 14:23:02 visual_prompt]: Epoch 15 / 100: avg data time: 6.18e-02, avg batch time: 0.9923, average train loss: 0.6944
[11/20 14:24:00 visual_prompt]: Inference (val):avg data time: 2.50e-04, avg batch time: 0.3055, average loss: 0.6991
[11/20 14:24:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/20 14:24:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 14:25:47 visual_prompt]: 	Training 100/553. train loss: 0.6995,	0.9216 s / batch. (data: 5.80e-03). ETA=12:00:25, max mem: 23.5 GB 
[11/20 14:27:22 visual_prompt]: 	Training 200/553. train loss: 0.7639,	1.5443 s / batch. (data: 6.25e-01). ETA=20:04:42, max mem: 23.5 GB 
[11/20 14:29:06 visual_prompt]: 	Training 300/553. train loss: 0.6950,	0.9320 s / batch. (data: 2.87e-04). ETA=12:05:28, max mem: 23.5 GB 
[11/20 14:30:43 visual_prompt]: 	Training 400/553. train loss: 0.7448,	0.9360 s / batch. (data: 3.19e-04). ETA=12:07:00, max mem: 23.5 GB 
[11/20 14:32:20 visual_prompt]: 	Training 500/553. train loss: 0.7248,	0.9308 s / batch. (data: 1.55e-02). ETA=12:01:27, max mem: 23.5 GB 
[11/20 14:33:11 visual_prompt]: Epoch 16 / 100: avg data time: 6.38e-02, avg batch time: 0.9952, average train loss: 0.6950
[11/20 14:34:09 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3033, average loss: 0.6918
[11/20 14:34:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.03	
[11/20 14:34:09 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 14:35:54 visual_prompt]: 	Training 100/553. train loss: 0.7147,	0.9660 s / batch. (data: 7.50e-03). ETA=12:26:17, max mem: 23.5 GB 
[11/20 14:37:35 visual_prompt]: 	Training 200/553. train loss: 0.7040,	0.9349 s / batch. (data: 1.05e-02). ETA=12:00:39, max mem: 23.5 GB 
[11/20 14:39:09 visual_prompt]: 	Training 300/553. train loss: 0.7053,	0.9566 s / batch. (data: 1.04e-02). ETA=12:15:50, max mem: 23.5 GB 
[11/20 14:40:50 visual_prompt]: 	Training 400/553. train loss: 0.7005,	0.9320 s / batch. (data: 3.61e-04). ETA=11:55:20, max mem: 23.5 GB 
[11/20 14:42:28 visual_prompt]: 	Training 500/553. train loss: 0.8084,	0.9134 s / batch. (data: 2.58e-04). ETA=11:39:32, max mem: 23.5 GB 
[11/20 14:43:19 visual_prompt]: Epoch 17 / 100: avg data time: 6.21e-02, avg batch time: 0.9945, average train loss: 0.6956
[11/20 14:44:18 visual_prompt]: Inference (val):avg data time: 1.40e-04, avg batch time: 0.3064, average loss: 0.6884
[11/20 14:44:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/20 14:44:18 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 14:46:09 visual_prompt]: 	Training 100/553. train loss: 0.6959,	0.9240 s / batch. (data: 8.01e-03). ETA=11:45:19, max mem: 23.5 GB 
[11/20 14:47:47 visual_prompt]: 	Training 200/553. train loss: 0.6983,	0.9234 s / batch. (data: 2.89e-04). ETA=11:43:17, max mem: 23.5 GB 
[11/20 14:49:22 visual_prompt]: 	Training 300/553. train loss: 0.7109,	0.9097 s / batch. (data: 2.76e-04). ETA=11:31:22, max mem: 23.5 GB 
[11/20 14:50:59 visual_prompt]: 	Training 400/553. train loss: 0.6371,	0.9201 s / batch. (data: 5.37e-03). ETA=11:37:42, max mem: 23.5 GB 
[11/20 14:52:40 visual_prompt]: 	Training 500/553. train loss: 0.7605,	0.9140 s / batch. (data: 2.54e-04). ETA=11:31:32, max mem: 23.5 GB 
[11/20 14:53:29 visual_prompt]: Epoch 18 / 100: avg data time: 6.58e-02, avg batch time: 0.9964, average train loss: 0.6961
[11/20 14:54:28 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3054, average loss: 0.6908
[11/20 14:54:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/20 14:54:28 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 14:56:16 visual_prompt]: 	Training 100/553. train loss: 0.6469,	0.9440 s / batch. (data: 5.44e-03). ETA=11:51:52, max mem: 23.5 GB 
[11/20 14:57:56 visual_prompt]: 	Training 200/553. train loss: 0.7055,	3.5121 s / batch. (data: 2.58e+00). ETA=1 day, 20:02:38, max mem: 23.5 GB 
[11/20 14:59:33 visual_prompt]: 	Training 300/553. train loss: 0.7023,	0.9134 s / batch. (data: 3.97e-03). ETA=11:25:42, max mem: 23.5 GB 
[11/20 15:01:10 visual_prompt]: 	Training 400/553. train loss: 0.6715,	0.9605 s / batch. (data: 2.92e-04). ETA=11:59:29, max mem: 23.5 GB 
[11/20 15:02:48 visual_prompt]: 	Training 500/553. train loss: 0.6001,	0.9400 s / batch. (data: 2.67e-04). ETA=11:42:35, max mem: 23.5 GB 
[11/20 15:03:38 visual_prompt]: Epoch 19 / 100: avg data time: 6.37e-02, avg batch time: 0.9946, average train loss: 0.6922
[11/20 15:04:36 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3047, average loss: 0.7847
[11/20 15:04:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.48	
[11/20 15:04:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 15:06:27 visual_prompt]: 	Training 100/553. train loss: 0.6883,	0.9207 s / batch. (data: 2.89e-04). ETA=11:25:50, max mem: 23.5 GB 
[11/20 15:08:01 visual_prompt]: 	Training 200/553. train loss: 0.7023,	0.9296 s / batch. (data: 1.05e-02). ETA=11:30:55, max mem: 23.5 GB 
[11/20 15:09:40 visual_prompt]: 	Training 300/553. train loss: 0.7031,	0.9388 s / batch. (data: 5.37e-03). ETA=11:36:07, max mem: 23.5 GB 
[11/20 15:11:19 visual_prompt]: 	Training 400/553. train loss: 0.8295,	0.9406 s / batch. (data: 7.46e-04). ETA=11:35:56, max mem: 23.5 GB 
[11/20 15:12:56 visual_prompt]: 	Training 500/553. train loss: 0.6990,	1.4879 s / batch. (data: 5.57e-01). ETA=18:18:21, max mem: 23.5 GB 
[11/20 15:13:48 visual_prompt]: Epoch 20 / 100: avg data time: 6.79e-02, avg batch time: 0.9984, average train loss: 0.6923
[11/20 15:14:47 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3059, average loss: 0.6934
[11/20 15:14:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.78	
[11/20 15:14:47 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 15:16:38 visual_prompt]: 	Training 100/553. train loss: 0.7061,	0.9178 s / batch. (data: 7.86e-04). ETA=11:15:10, max mem: 23.5 GB 
[11/20 15:18:15 visual_prompt]: 	Training 200/553. train loss: 0.6571,	0.9226 s / batch. (data: 2.51e-04). ETA=11:17:09, max mem: 23.5 GB 
[11/20 15:19:52 visual_prompt]: 	Training 300/553. train loss: 0.6977,	0.9151 s / batch. (data: 2.62e-04). ETA=11:10:08, max mem: 23.5 GB 
[11/20 15:21:30 visual_prompt]: 	Training 400/553. train loss: 0.7891,	0.9157 s / batch. (data: 2.79e-04). ETA=11:09:02, max mem: 23.5 GB 
[11/20 15:23:06 visual_prompt]: 	Training 500/553. train loss: 0.7367,	0.9170 s / batch. (data: 3.09e-04). ETA=11:08:29, max mem: 23.5 GB 
[11/20 15:23:57 visual_prompt]: Epoch 21 / 100: avg data time: 6.39e-02, avg batch time: 0.9945, average train loss: 0.6909
[11/20 15:24:55 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3040, average loss: 0.6886
[11/20 15:24:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.85	
[11/20 15:24:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 15:26:41 visual_prompt]: 	Training 100/553. train loss: 0.6961,	0.9200 s / batch. (data: 2.52e-04). ETA=11:08:19, max mem: 23.5 GB 
[11/20 15:28:22 visual_prompt]: 	Training 200/553. train loss: 0.6998,	0.9579 s / batch. (data: 5.85e-03). ETA=11:34:14, max mem: 23.5 GB 
[11/20 15:30:02 visual_prompt]: 	Training 300/553. train loss: 0.7607,	0.9088 s / batch. (data: 2.57e-04). ETA=10:57:07, max mem: 23.5 GB 
[11/20 15:31:36 visual_prompt]: 	Training 400/553. train loss: 0.8677,	0.9245 s / batch. (data: 4.79e-03). ETA=11:06:59, max mem: 23.5 GB 
[11/20 15:33:16 visual_prompt]: 	Training 500/553. train loss: 0.6961,	0.9414 s / batch. (data: 5.39e-03). ETA=11:17:34, max mem: 23.5 GB 
[11/20 15:34:05 visual_prompt]: Epoch 22 / 100: avg data time: 6.40e-02, avg batch time: 0.9952, average train loss: 0.6957
[11/20 15:35:04 visual_prompt]: Inference (val):avg data time: 2.28e-04, avg batch time: 0.3064, average loss: 0.6898
[11/20 15:35:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.99	
[11/20 15:35:04 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 15:36:55 visual_prompt]: 	Training 100/553. train loss: 0.6949,	0.9640 s / batch. (data: 8.04e-04). ETA=11:31:25, max mem: 23.5 GB 
[11/20 15:38:32 visual_prompt]: 	Training 200/553. train loss: 0.6357,	0.9531 s / batch. (data: 7.45e-04). ETA=11:22:02, max mem: 23.5 GB 
[11/20 15:40:07 visual_prompt]: 	Training 300/553. train loss: 0.6128,	0.9438 s / batch. (data: 7.85e-03). ETA=11:13:48, max mem: 23.5 GB 
[11/20 15:41:45 visual_prompt]: 	Training 400/553. train loss: 0.7007,	0.9568 s / batch. (data: 1.04e-02). ETA=11:21:27, max mem: 23.5 GB 
[11/20 15:43:21 visual_prompt]: 	Training 500/553. train loss: 0.6839,	0.9565 s / batch. (data: 7.95e-03). ETA=11:19:39, max mem: 23.5 GB 
[11/20 15:44:14 visual_prompt]: Epoch 23 / 100: avg data time: 6.35e-02, avg batch time: 0.9943, average train loss: 0.6981
[11/20 15:45:12 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3044, average loss: 0.6965
[11/20 15:45:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.27	
[11/20 15:45:12 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 15:46:59 visual_prompt]: 	Training 100/553. train loss: 0.6949,	0.9074 s / batch. (data: 2.73e-04). ETA=10:42:25, max mem: 23.5 GB 
[11/20 15:48:38 visual_prompt]: 	Training 200/553. train loss: 0.7031,	0.9320 s / batch. (data: 5.35e-03). ETA=10:58:18, max mem: 23.5 GB 
[11/20 15:50:20 visual_prompt]: 	Training 300/553. train loss: 0.7055,	0.9385 s / batch. (data: 1.05e-02). ETA=11:01:19, max mem: 23.5 GB 
[11/20 15:51:57 visual_prompt]: 	Training 400/553. train loss: 0.6973,	0.9400 s / batch. (data: 2.82e-04). ETA=11:00:50, max mem: 23.5 GB 
[11/20 15:53:36 visual_prompt]: 	Training 500/553. train loss: 0.6927,	0.9554 s / batch. (data: 1.59e-02). ETA=11:10:02, max mem: 23.5 GB 
[11/20 15:54:25 visual_prompt]: Epoch 24 / 100: avg data time: 6.88e-02, avg batch time: 0.9997, average train loss: 0.6933
[11/20 15:55:24 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3030, average loss: 0.6885
[11/20 15:55:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.93	
[11/20 15:55:24 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/20 15:57:09 visual_prompt]: 	Training 100/553. train loss: 0.6409,	0.9459 s / batch. (data: 1.04e-02). ETA=11:01:00, max mem: 23.5 GB 
[11/20 15:58:50 visual_prompt]: 	Training 200/553. train loss: 0.7566,	0.9378 s / batch. (data: 5.38e-03). ETA=10:53:47, max mem: 23.5 GB 
[11/20 16:00:29 visual_prompt]: 	Training 300/553. train loss: 0.6965,	0.9327 s / batch. (data: 5.48e-03). ETA=10:48:39, max mem: 23.5 GB 
[11/20 16:02:08 visual_prompt]: 	Training 400/553. train loss: 0.7076,	0.9507 s / batch. (data: 7.97e-03). ETA=10:59:34, max mem: 23.5 GB 
[11/20 16:03:47 visual_prompt]: 	Training 500/553. train loss: 0.6326,	0.9403 s / batch. (data: 5.55e-03). ETA=10:50:48, max mem: 23.5 GB 
[11/20 16:04:39 visual_prompt]: Epoch 25 / 100: avg data time: 7.43e-02, avg batch time: 1.0039, average train loss: 0.6931
[11/20 16:05:37 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.3048, average loss: 0.6915
[11/20 16:05:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/20 16:05:37 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/20 16:07:25 visual_prompt]: 	Training 100/553. train loss: 0.6673,	0.9560 s / batch. (data: 7.59e-04). ETA=10:59:14, max mem: 23.5 GB 
[11/20 16:09:04 visual_prompt]: 	Training 200/553. train loss: 0.5924,	0.9320 s / batch. (data: 2.75e-04). ETA=10:41:08, max mem: 23.5 GB 
[11/20 16:10:45 visual_prompt]: 	Training 300/553. train loss: 0.7515,	0.9602 s / batch. (data: 7.16e-04). ETA=10:58:55, max mem: 23.5 GB 
[11/20 16:12:20 visual_prompt]: 	Training 400/553. train loss: 0.6934,	0.9722 s / batch. (data: 1.62e-02). ETA=11:05:33, max mem: 23.5 GB 
[11/20 16:14:00 visual_prompt]: 	Training 500/553. train loss: 0.6298,	0.9480 s / batch. (data: 6.92e-04). ETA=10:47:24, max mem: 23.5 GB 
[11/20 16:14:50 visual_prompt]: Epoch 26 / 100: avg data time: 6.80e-02, avg batch time: 0.9995, average train loss: 0.6926
[11/20 16:15:49 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3053, average loss: 0.6960
[11/20 16:15:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.20	
[11/20 16:15:49 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/20 16:17:35 visual_prompt]: 	Training 100/553. train loss: 0.7486,	0.9360 s / batch. (data: 2.47e-04). ETA=10:36:49, max mem: 23.5 GB 
[11/20 16:19:15 visual_prompt]: 	Training 200/553. train loss: 0.6995,	0.9426 s / batch. (data: 7.97e-03). ETA=10:39:44, max mem: 23.5 GB 
[11/20 16:20:49 visual_prompt]: 	Training 300/553. train loss: 0.7345,	0.9320 s / batch. (data: 3.10e-04). ETA=10:30:58, max mem: 23.5 GB 
[11/20 16:22:29 visual_prompt]: 	Training 400/553. train loss: 0.7856,	0.9259 s / batch. (data: 7.14e-04). ETA=10:25:18, max mem: 23.5 GB 
[11/20 16:24:10 visual_prompt]: 	Training 500/553. train loss: 0.7501,	0.9777 s / batch. (data: 2.17e-02). ETA=10:58:40, max mem: 23.5 GB 
[11/20 16:25:01 visual_prompt]: Epoch 27 / 100: avg data time: 6.74e-02, avg batch time: 0.9981, average train loss: 0.6962
[11/20 16:26:00 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3049, average loss: 0.6903
[11/20 16:26:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.96	
[11/20 16:26:00 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/20 16:27:45 visual_prompt]: 	Training 100/553. train loss: 0.7348,	0.9467 s / batch. (data: 2.96e-04). ETA=10:35:24, max mem: 23.5 GB 
[11/20 16:29:24 visual_prompt]: 	Training 200/553. train loss: 0.6581,	0.9316 s / batch. (data: 7.57e-03). ETA=10:23:40, max mem: 23.5 GB 
[11/20 16:31:03 visual_prompt]: 	Training 300/553. train loss: 0.6910,	0.9397 s / batch. (data: 2.46e-02). ETA=10:27:33, max mem: 23.5 GB 
[11/20 16:32:43 visual_prompt]: 	Training 400/553. train loss: 0.6944,	1.1307 s / batch. (data: 2.07e-01). ETA=12:33:12, max mem: 23.5 GB 
[11/20 16:34:20 visual_prompt]: 	Training 500/553. train loss: 0.8030,	0.9272 s / batch. (data: 5.85e-03). ETA=10:16:08, max mem: 23.5 GB 
[11/20 16:35:11 visual_prompt]: Epoch 28 / 100: avg data time: 6.56e-02, avg batch time: 0.9969, average train loss: 0.6961
[11/20 16:36:09 visual_prompt]: Inference (val):avg data time: 3.01e-04, avg batch time: 0.3054, average loss: 0.6898
[11/20 16:36:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/20 16:36:09 visual_prompt]: Stopping early.
[11/20 16:36:10 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 16:36:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 16:36:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/20 16:36:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 16:36:10 visual_prompt]: Training with config:
[11/20 16:36:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.005_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 16:36:10 visual_prompt]: Loading training data...
[11/20 16:36:10 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 16:36:10 visual_prompt]: Loading validation data...
[11/20 16:36:10 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 16:36:10 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 16:36:11 visual_prompt]: Enable all parameters update during training
[11/20 16:36:11 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 16:36:11 visual_prompt]: tuned percent:100.000
[11/20 16:36:11 visual_prompt]: Device used for model: 0
[11/20 16:36:11 visual_prompt]: Setting up Evaluator...
[11/20 16:36:11 visual_prompt]: Setting up Trainer...
[11/20 16:36:11 visual_prompt]: 	Setting up the optimizer...
[11/20 16:36:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 16:37:55 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9285 s / batch. (data: 3.99e-03). ETA=14:14:11, max mem: 24.8 GB 
[11/20 16:39:36 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9312 s / batch. (data: 2.81e-04). ETA=14:15:06, max mem: 24.8 GB 
[11/20 16:41:13 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9328 s / batch. (data: 2.45e-04). ETA=14:15:04, max mem: 24.8 GB 
[11/20 16:42:47 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9280 s / batch. (data: 2.79e-04). ETA=14:09:08, max mem: 24.8 GB 
[11/20 16:44:23 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9248 s / batch. (data: 2.38e-04). ETA=14:04:38, max mem: 24.8 GB 
[11/20 16:45:13 visual_prompt]: Epoch 1 / 100: avg data time: 4.59e-02, avg batch time: 0.9797, average train loss: 7.6130
[11/20 16:46:11 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3068, average loss: 6.9126
[11/20 16:46:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 16:46:11 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 16:47:58 visual_prompt]: 	Training 100/553. train loss: 0.9419,	0.9307 s / batch. (data: 5.82e-03). ETA=14:07:38, max mem: 24.8 GB 
[11/20 16:49:33 visual_prompt]: 	Training 200/553. train loss: 0.6641,	2.0956 s / batch. (data: 1.18e+00). ETA=1 day, 7:45:10, max mem: 24.8 GB 
[11/20 16:51:10 visual_prompt]: 	Training 300/553. train loss: 0.9274,	0.9431 s / batch. (data: 2.53e-04). ETA=14:15:49, max mem: 24.8 GB 
[11/20 16:52:47 visual_prompt]: 	Training 400/553. train loss: 0.6835,	0.9353 s / batch. (data: 5.38e-03). ETA=14:07:13, max mem: 24.8 GB 
[11/20 16:54:24 visual_prompt]: 	Training 500/553. train loss: 1.2724,	0.9526 s / batch. (data: 5.35e-03). ETA=14:21:13, max mem: 24.8 GB 
[11/20 16:55:13 visual_prompt]: Epoch 2 / 100: avg data time: 4.72e-02, avg batch time: 0.9797, average train loss: 1.2955
[11/20 16:56:11 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3052, average loss: 0.8354
[11/20 16:56:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.95	
[11/20 16:56:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 16:57:56 visual_prompt]: 	Training 100/553. train loss: 0.6031,	0.9106 s / batch. (data: 2.55e-04). ETA=13:40:57, max mem: 24.8 GB 
[11/20 16:59:33 visual_prompt]: 	Training 200/553. train loss: 3.1021,	0.9680 s / batch. (data: 2.65e-04). ETA=14:31:04, max mem: 24.8 GB 
[11/20 17:01:10 visual_prompt]: 	Training 300/553. train loss: 0.6149,	0.9258 s / batch. (data: 2.60e-04). ETA=13:51:32, max mem: 24.8 GB 
[11/20 17:02:44 visual_prompt]: 	Training 400/553. train loss: 0.3450,	0.9489 s / batch. (data: 5.35e-03). ETA=14:10:43, max mem: 24.8 GB 
[11/20 17:04:19 visual_prompt]: 	Training 500/553. train loss: 1.9257,	0.9414 s / batch. (data: 6.28e-03). ETA=14:02:27, max mem: 24.8 GB 
[11/20 17:05:09 visual_prompt]: Epoch 3 / 100: avg data time: 4.15e-02, avg batch time: 0.9740, average train loss: 0.8786
[11/20 17:06:07 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3051, average loss: 1.0993
[11/20 17:06:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.52	
[11/20 17:06:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 17:07:53 visual_prompt]: 	Training 100/553. train loss: 1.3790,	0.9672 s / batch. (data: 3.26e-04). ETA=14:23:04, max mem: 24.8 GB 
[11/20 17:09:28 visual_prompt]: 	Training 200/553. train loss: 0.9360,	0.9851 s / batch. (data: 7.59e-02). ETA=14:37:25, max mem: 24.8 GB 
[11/20 17:11:04 visual_prompt]: 	Training 300/553. train loss: 1.3390,	0.9301 s / batch. (data: 2.91e-04). ETA=13:46:50, max mem: 24.8 GB 
[11/20 17:12:40 visual_prompt]: 	Training 400/553. train loss: 0.5701,	0.9584 s / batch. (data: 2.56e-04). ETA=14:10:23, max mem: 24.8 GB 
[11/20 17:14:16 visual_prompt]: 	Training 500/553. train loss: 0.6535,	0.9520 s / batch. (data: 7.99e-03). ETA=14:03:10, max mem: 24.8 GB 
[11/20 17:15:07 visual_prompt]: Epoch 4 / 100: avg data time: 4.55e-02, avg batch time: 0.9771, average train loss: 0.8807
[11/20 17:16:05 visual_prompt]: Inference (val):avg data time: 1.27e-04, avg batch time: 0.3045, average loss: 0.6926
[11/20 17:16:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.86	
[11/20 17:16:05 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 17:17:50 visual_prompt]: 	Training 100/553. train loss: 0.5609,	0.9239 s / batch. (data: 4.82e-04). ETA=13:35:54, max mem: 24.8 GB 
[11/20 17:19:27 visual_prompt]: 	Training 200/553. train loss: 0.6042,	0.9486 s / batch. (data: 1.05e-02). ETA=13:56:07, max mem: 24.8 GB 
[11/20 17:21:02 visual_prompt]: 	Training 300/553. train loss: 1.4403,	0.9385 s / batch. (data: 5.37e-03). ETA=13:45:39, max mem: 24.8 GB 
[11/20 17:22:40 visual_prompt]: 	Training 400/553. train loss: 0.7130,	4.0189 s / batch. (data: 3.12e+00). ETA=2 days, 10:49:07, max mem: 24.8 GB 
[11/20 17:24:19 visual_prompt]: 	Training 500/553. train loss: 0.6001,	0.9090 s / batch. (data: 6.77e-04). ETA=13:16:42, max mem: 24.8 GB 
[11/20 17:25:08 visual_prompt]: Epoch 5 / 100: avg data time: 5.14e-02, avg batch time: 0.9824, average train loss: 0.8416
[11/20 17:26:06 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3043, average loss: 0.7386
[11/20 17:26:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.36	
[11/20 17:26:06 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 17:27:56 visual_prompt]: 	Training 100/553. train loss: 0.7543,	0.9266 s / batch. (data: 3.17e-04). ETA=13:29:44, max mem: 24.8 GB 
[11/20 17:29:30 visual_prompt]: 	Training 200/553. train loss: 1.0364,	0.9142 s / batch. (data: 3.02e-04). ETA=13:17:26, max mem: 24.8 GB 
[11/20 17:31:08 visual_prompt]: 	Training 300/553. train loss: 0.8410,	0.9519 s / batch. (data: 1.63e-02). ETA=13:48:44, max mem: 24.8 GB 
[11/20 17:32:43 visual_prompt]: 	Training 400/553. train loss: 0.7177,	1.6456 s / batch. (data: 7.30e-01). ETA=23:49:54, max mem: 24.8 GB 
[11/20 17:34:22 visual_prompt]: 	Training 500/553. train loss: 1.0181,	0.9168 s / batch. (data: 7.22e-04). ETA=13:15:07, max mem: 24.8 GB 
[11/20 17:35:12 visual_prompt]: Epoch 6 / 100: avg data time: 5.34e-02, avg batch time: 0.9863, average train loss: 0.8547
[11/20 17:36:09 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3056, average loss: 0.6960
[11/20 17:36:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.96	
[11/20 17:36:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 17:38:01 visual_prompt]: 	Training 100/553. train loss: 0.8119,	0.9240 s / batch. (data: 2.54e-04). ETA=13:18:59, max mem: 24.8 GB 
[11/20 17:39:35 visual_prompt]: 	Training 200/553. train loss: 0.6645,	0.9270 s / batch. (data: 7.59e-04). ETA=13:20:03, max mem: 24.8 GB 
[11/20 17:41:09 visual_prompt]: 	Training 300/553. train loss: 0.7345,	0.9336 s / batch. (data: 5.36e-03). ETA=13:24:11, max mem: 24.8 GB 
[11/20 17:42:43 visual_prompt]: 	Training 400/553. train loss: 0.6267,	0.9221 s / batch. (data: 3.00e-04). ETA=13:12:45, max mem: 24.8 GB 
[11/20 17:44:19 visual_prompt]: 	Training 500/553. train loss: 0.6703,	0.9305 s / batch. (data: 2.49e-04). ETA=13:18:25, max mem: 24.8 GB 
[11/20 17:45:09 visual_prompt]: Epoch 7 / 100: avg data time: 4.40e-02, avg batch time: 0.9759, average train loss: 0.8175
[11/20 17:46:07 visual_prompt]: Inference (val):avg data time: 8.52e-05, avg batch time: 0.3046, average loss: 0.6913
[11/20 17:46:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 55.54	
[11/20 17:46:07 visual_prompt]: Best epoch 7: best metric: -0.691
[11/20 17:46:07 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 17:47:53 visual_prompt]: 	Training 100/553. train loss: 0.6094,	0.9708 s / batch. (data: 6.57e-03). ETA=13:50:32, max mem: 24.8 GB 
[11/20 17:49:31 visual_prompt]: 	Training 200/553. train loss: 0.8292,	0.9111 s / batch. (data: 2.76e-04). ETA=12:57:54, max mem: 24.8 GB 
[11/20 17:51:06 visual_prompt]: 	Training 300/553. train loss: 0.7380,	0.9532 s / batch. (data: 6.96e-04). ETA=13:32:14, max mem: 24.8 GB 
[11/20 17:52:42 visual_prompt]: 	Training 400/553. train loss: 0.6902,	0.9155 s / batch. (data: 2.48e-04). ETA=12:58:36, max mem: 24.8 GB 
[11/20 17:54:19 visual_prompt]: 	Training 500/553. train loss: 0.5900,	0.9560 s / batch. (data: 7.96e-03). ETA=13:31:27, max mem: 24.8 GB 
[11/20 17:55:08 visual_prompt]: Epoch 8 / 100: avg data time: 4.75e-02, avg batch time: 0.9781, average train loss: 0.7929
[11/20 17:56:06 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3031, average loss: 0.7041
[11/20 17:56:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.91	
[11/20 17:56:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 17:57:55 visual_prompt]: 	Training 100/553. train loss: 0.7689,	0.9608 s / batch. (data: 1.68e-02). ETA=13:33:03, max mem: 24.8 GB 
[11/20 17:59:31 visual_prompt]: 	Training 200/553. train loss: 1.3069,	0.9481 s / batch. (data: 7.98e-03). ETA=13:20:44, max mem: 24.8 GB 
[11/20 18:01:05 visual_prompt]: 	Training 300/553. train loss: 0.6321,	0.9094 s / batch. (data: 2.51e-04). ETA=12:46:34, max mem: 24.8 GB 
[11/20 18:02:39 visual_prompt]: 	Training 400/553. train loss: 0.6812,	0.9261 s / batch. (data: 7.48e-04). ETA=12:59:05, max mem: 24.8 GB 
[11/20 18:04:13 visual_prompt]: 	Training 500/553. train loss: 0.7632,	0.9640 s / batch. (data: 2.86e-04). ETA=13:29:24, max mem: 24.8 GB 
[11/20 18:05:04 visual_prompt]: Epoch 9 / 100: avg data time: 4.08e-02, avg batch time: 0.9724, average train loss: 0.7711
[11/20 18:06:01 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3039, average loss: 0.7027
[11/20 18:06:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.59	
[11/20 18:06:01 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 18:07:48 visual_prompt]: 	Training 100/553. train loss: 0.9652,	0.9096 s / batch. (data: 3.97e-03). ETA=12:41:21, max mem: 24.8 GB 
[11/20 18:09:26 visual_prompt]: 	Training 200/553. train loss: 1.8504,	0.9449 s / batch. (data: 1.04e-02). ETA=13:09:18, max mem: 24.8 GB 
[11/20 18:11:00 visual_prompt]: 	Training 300/553. train loss: 0.7933,	0.9480 s / batch. (data: 7.99e-03). ETA=13:10:19, max mem: 24.8 GB 
[11/20 18:12:34 visual_prompt]: 	Training 400/553. train loss: 0.6548,	0.9328 s / batch. (data: 5.77e-03). ETA=12:56:08, max mem: 24.8 GB 
[11/20 18:14:10 visual_prompt]: 	Training 500/553. train loss: 0.7185,	0.9332 s / batch. (data: 5.46e-03). ETA=12:54:57, max mem: 24.8 GB 
[11/20 18:15:01 visual_prompt]: Epoch 10 / 100: avg data time: 4.42e-02, avg batch time: 0.9761, average train loss: 0.7284
[11/20 18:15:59 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3077, average loss: 0.7079
[11/20 18:15:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.40	
[11/20 18:15:59 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 18:17:49 visual_prompt]: 	Training 100/553. train loss: 0.7596,	0.9410 s / batch. (data: 2.59e-04). ETA=12:58:58, max mem: 24.8 GB 
[11/20 18:19:24 visual_prompt]: 	Training 200/553. train loss: 0.7280,	0.9378 s / batch. (data: 3.44e-04). ETA=12:54:44, max mem: 24.8 GB 
[11/20 18:20:58 visual_prompt]: 	Training 300/553. train loss: 0.5768,	0.9219 s / batch. (data: 5.38e-03). ETA=12:40:06, max mem: 24.8 GB 
[11/20 18:22:34 visual_prompt]: 	Training 400/553. train loss: 0.6880,	0.9320 s / batch. (data: 2.60e-04). ETA=12:46:52, max mem: 24.8 GB 
[11/20 18:24:10 visual_prompt]: 	Training 500/553. train loss: 0.7832,	0.9264 s / batch. (data: 2.06e-02). ETA=12:40:45, max mem: 24.8 GB 
[11/20 18:25:00 visual_prompt]: Epoch 11 / 100: avg data time: 4.43e-02, avg batch time: 0.9778, average train loss: 0.7197
[11/20 18:25:57 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3057, average loss: 0.7194
[11/20 18:25:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.24	
[11/20 18:25:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 18:27:40 visual_prompt]: 	Training 100/553. train loss: 0.5679,	0.9530 s / batch. (data: 5.36e-03). ETA=13:00:07, max mem: 24.8 GB 
[11/20 18:29:20 visual_prompt]: 	Training 200/553. train loss: 0.6638,	0.9405 s / batch. (data: 5.45e-03). ETA=12:48:22, max mem: 24.8 GB 
[11/20 18:30:55 visual_prompt]: 	Training 300/553. train loss: 0.7059,	0.9295 s / batch. (data: 2.52e-04). ETA=12:37:48, max mem: 24.8 GB 
[11/20 18:32:31 visual_prompt]: 	Training 400/553. train loss: 0.8318,	0.9529 s / batch. (data: 7.26e-04). ETA=12:55:19, max mem: 24.8 GB 
[11/20 18:34:06 visual_prompt]: 	Training 500/553. train loss: 0.7342,	0.9636 s / batch. (data: 8.79e-03). ETA=13:02:22, max mem: 24.8 GB 
[11/20 18:34:57 visual_prompt]: Epoch 12 / 100: avg data time: 4.43e-02, avg batch time: 0.9759, average train loss: 0.7072
[11/20 18:35:55 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3031, average loss: 0.7452
[11/20 18:35:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.71	
[11/20 18:35:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 18:37:43 visual_prompt]: 	Training 100/553. train loss: 0.6686,	0.9489 s / batch. (data: 5.82e-03). ETA=12:48:01, max mem: 24.8 GB 
[11/20 18:39:17 visual_prompt]: 	Training 200/553. train loss: 0.7229,	0.9456 s / batch. (data: 1.66e-02). ETA=12:43:49, max mem: 24.8 GB 
[11/20 18:40:54 visual_prompt]: 	Training 300/553. train loss: 0.8969,	0.9186 s / batch. (data: 2.99e-04). ETA=12:20:29, max mem: 24.8 GB 
[11/20 18:42:31 visual_prompt]: 	Training 400/553. train loss: 0.6962,	0.9360 s / batch. (data: 2.53e-04). ETA=12:32:55, max mem: 24.8 GB 
[11/20 18:44:06 visual_prompt]: 	Training 500/553. train loss: 0.6120,	0.9494 s / batch. (data: 2.52e-04). ETA=12:42:08, max mem: 24.8 GB 
[11/20 18:44:56 visual_prompt]: Epoch 13 / 100: avg data time: 4.68e-02, avg batch time: 0.9779, average train loss: 0.7218
[11/20 18:45:53 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3038, average loss: 0.6893
[11/20 18:45:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.71	
[11/20 18:45:53 visual_prompt]: Best epoch 13: best metric: -0.689
[11/20 18:45:53 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 18:47:42 visual_prompt]: 	Training 100/553. train loss: 0.6937,	0.9503 s / batch. (data: 1.04e-02). ETA=12:40:24, max mem: 24.8 GB 
[11/20 18:49:21 visual_prompt]: 	Training 200/553. train loss: 0.6285,	0.9045 s / batch. (data: 2.60e-04). ETA=12:02:17, max mem: 24.8 GB 
[11/20 18:50:57 visual_prompt]: 	Training 300/553. train loss: 0.7460,	0.9232 s / batch. (data: 2.85e-04). ETA=12:15:40, max mem: 24.8 GB 
[11/20 18:52:31 visual_prompt]: 	Training 400/553. train loss: 0.7047,	0.9191 s / batch. (data: 5.40e-03). ETA=12:10:51, max mem: 24.8 GB 
[11/20 18:54:07 visual_prompt]: 	Training 500/553. train loss: 0.7003,	0.9360 s / batch. (data: 2.73e-04). ETA=12:22:43, max mem: 24.8 GB 
[11/20 18:54:56 visual_prompt]: Epoch 14 / 100: avg data time: 5.07e-02, avg batch time: 0.9814, average train loss: 0.6947
[11/20 18:55:54 visual_prompt]: Inference (val):avg data time: 3.66e-04, avg batch time: 0.3041, average loss: 0.6899
[11/20 18:55:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.16	
[11/20 18:55:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 18:57:42 visual_prompt]: 	Training 100/553. train loss: 0.6083,	0.9440 s / batch. (data: 7.99e-03). ETA=12:26:40, max mem: 24.8 GB 
[11/20 18:59:19 visual_prompt]: 	Training 200/553. train loss: 0.6969,	0.9280 s / batch. (data: 3.39e-04). ETA=12:12:27, max mem: 24.8 GB 
[11/20 19:00:54 visual_prompt]: 	Training 300/553. train loss: 0.7597,	0.9582 s / batch. (data: 2.53e-04). ETA=12:34:41, max mem: 24.8 GB 
[11/20 19:02:29 visual_prompt]: 	Training 400/553. train loss: 0.7301,	0.9244 s / batch. (data: 1.04e-02). ETA=12:06:34, max mem: 24.8 GB 
[11/20 19:04:04 visual_prompt]: 	Training 500/553. train loss: 0.8647,	0.9453 s / batch. (data: 5.41e-03). ETA=12:21:24, max mem: 24.8 GB 
[11/20 19:04:54 visual_prompt]: Epoch 15 / 100: avg data time: 4.41e-02, avg batch time: 0.9761, average train loss: 0.6936
[11/20 19:05:51 visual_prompt]: Inference (val):avg data time: 5.34e-04, avg batch time: 0.3045, average loss: 0.6975
[11/20 19:05:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.93	
[11/20 19:05:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 19:07:36 visual_prompt]: 	Training 100/553. train loss: 0.7097,	0.9364 s / batch. (data: 5.37e-03). ETA=12:12:03, max mem: 24.8 GB 
[11/20 19:09:10 visual_prompt]: 	Training 200/553. train loss: 0.7654,	0.9362 s / batch. (data: 3.10e-04). ETA=12:10:18, max mem: 24.8 GB 
[11/20 19:10:49 visual_prompt]: 	Training 300/553. train loss: 0.6962,	0.9393 s / batch. (data: 5.35e-03). ETA=12:11:08, max mem: 24.8 GB 
[11/20 19:12:26 visual_prompt]: 	Training 400/553. train loss: 0.7395,	0.9333 s / batch. (data: 5.39e-03). ETA=12:04:54, max mem: 24.8 GB 
[11/20 19:13:59 visual_prompt]: 	Training 500/553. train loss: 0.7262,	0.9508 s / batch. (data: 5.38e-03). ETA=12:16:55, max mem: 24.8 GB 
[11/20 19:14:49 visual_prompt]: Epoch 16 / 100: avg data time: 4.11e-02, avg batch time: 0.9731, average train loss: 0.6947
[11/20 19:15:47 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3056, average loss: 0.6908
[11/20 19:15:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.32	
[11/20 19:15:47 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 19:17:30 visual_prompt]: 	Training 100/553. train loss: 0.7147,	0.9400 s / batch. (data: 2.74e-04). ETA=12:06:12, max mem: 24.8 GB 
[11/20 19:19:10 visual_prompt]: 	Training 200/553. train loss: 0.7081,	0.9441 s / batch. (data: 3.99e-03). ETA=12:07:45, max mem: 24.8 GB 
[11/20 19:20:43 visual_prompt]: 	Training 300/553. train loss: 0.7002,	0.9356 s / batch. (data: 5.39e-03). ETA=11:59:38, max mem: 24.8 GB 
[11/20 19:22:21 visual_prompt]: 	Training 400/553. train loss: 0.7030,	0.9695 s / batch. (data: 5.39e-03). ETA=12:24:05, max mem: 24.8 GB 
[11/20 19:23:58 visual_prompt]: 	Training 500/553. train loss: 0.8001,	0.9440 s / batch. (data: 2.66e-04). ETA=12:03:00, max mem: 24.8 GB 
[11/20 19:24:48 visual_prompt]: Epoch 17 / 100: avg data time: 4.71e-02, avg batch time: 0.9776, average train loss: 0.6947
[11/20 19:25:45 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3047, average loss: 0.6886
[11/20 19:25:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.15	
[11/20 19:25:45 visual_prompt]: Best epoch 17: best metric: -0.689
[11/20 19:25:45 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 19:27:35 visual_prompt]: 	Training 100/553. train loss: 0.6984,	0.9285 s / batch. (data: 7.04e-04). ETA=11:48:42, max mem: 24.8 GB 
[11/20 19:29:12 visual_prompt]: 	Training 200/553. train loss: 0.6977,	0.9322 s / batch. (data: 2.97e-04). ETA=11:50:00, max mem: 24.8 GB 
[11/20 19:30:47 visual_prompt]: 	Training 300/553. train loss: 0.7049,	0.9339 s / batch. (data: 3.98e-03). ETA=11:49:42, max mem: 24.8 GB 
[11/20 19:32:21 visual_prompt]: 	Training 400/553. train loss: 0.6627,	0.9259 s / batch. (data: 1.04e-02). ETA=11:42:07, max mem: 24.8 GB 
[11/20 19:33:59 visual_prompt]: 	Training 500/553. train loss: 0.7958,	0.9249 s / batch. (data: 7.26e-04). ETA=11:39:51, max mem: 24.8 GB 
[11/20 19:34:49 visual_prompt]: Epoch 18 / 100: avg data time: 4.93e-02, avg batch time: 0.9826, average train loss: 0.6966
[11/20 19:35:46 visual_prompt]: Inference (val):avg data time: 1.67e-04, avg batch time: 0.3054, average loss: 0.6907
[11/20 19:35:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.12	
[11/20 19:35:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 19:37:33 visual_prompt]: 	Training 100/553. train loss: 0.6541,	0.9520 s / batch. (data: 8.17e-04). ETA=11:57:55, max mem: 24.8 GB 
[11/20 19:39:10 visual_prompt]: 	Training 200/553. train loss: 0.6992,	2.2523 s / batch. (data: 1.33e+00). ETA=1 day, 4:14:43, max mem: 24.8 GB 
[11/20 19:40:45 visual_prompt]: 	Training 300/553. train loss: 0.7026,	0.9513 s / batch. (data: 1.05e-02). ETA=11:54:12, max mem: 24.8 GB 
[11/20 19:42:21 visual_prompt]: 	Training 400/553. train loss: 0.6721,	0.9180 s / batch. (data: 2.71e-04). ETA=11:27:41, max mem: 24.8 GB 
[11/20 19:43:57 visual_prompt]: 	Training 500/553. train loss: 0.6013,	0.9353 s / batch. (data: 7.89e-04). ETA=11:39:06, max mem: 24.8 GB 
[11/20 19:44:46 visual_prompt]: Epoch 19 / 100: avg data time: 4.52e-02, avg batch time: 0.9763, average train loss: 0.6925
[11/20 19:45:44 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3059, average loss: 0.7328
[11/20 19:45:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.89	
[11/20 19:45:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 19:47:33 visual_prompt]: 	Training 100/553. train loss: 0.6929,	0.9511 s / batch. (data: 1.04e-02). ETA=11:48:28, max mem: 24.8 GB 
[11/20 19:49:06 visual_prompt]: 	Training 200/553. train loss: 0.7010,	0.9311 s / batch. (data: 7.99e-03). ETA=11:31:59, max mem: 24.8 GB 
[11/20 19:50:42 visual_prompt]: 	Training 300/553. train loss: 0.7024,	0.9201 s / batch. (data: 2.46e-04). ETA=11:22:16, max mem: 24.8 GB 
[11/20 19:52:19 visual_prompt]: 	Training 400/553. train loss: 0.7864,	0.9627 s / batch. (data: 2.48e-02). ETA=11:52:15, max mem: 24.8 GB 
[11/20 19:53:53 visual_prompt]: 	Training 500/553. train loss: 0.7008,	0.9190 s / batch. (data: 7.95e-03). ETA=11:18:26, max mem: 24.8 GB 
[11/20 19:54:44 visual_prompt]: Epoch 20 / 100: avg data time: 4.67e-02, avg batch time: 0.9769, average train loss: 0.6928
[11/20 19:55:42 visual_prompt]: Inference (val):avg data time: 2.63e-04, avg batch time: 0.3039, average loss: 0.6955
[11/20 19:55:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.93	
[11/20 19:55:42 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 19:57:31 visual_prompt]: 	Training 100/553. train loss: 0.7057,	0.9530 s / batch. (data: 5.83e-03). ETA=11:41:06, max mem: 24.8 GB 
[11/20 19:59:08 visual_prompt]: 	Training 200/553. train loss: 0.6649,	0.9237 s / batch. (data: 2.54e-04). ETA=11:17:58, max mem: 24.8 GB 
[11/20 20:00:44 visual_prompt]: 	Training 300/553. train loss: 0.6986,	0.9121 s / batch. (data: 5.39e-03). ETA=11:07:58, max mem: 24.8 GB 
[11/20 20:02:20 visual_prompt]: 	Training 400/553. train loss: 0.7918,	0.9256 s / batch. (data: 5.35e-03). ETA=11:16:19, max mem: 24.8 GB 
[11/20 20:03:54 visual_prompt]: 	Training 500/553. train loss: 0.7299,	0.9222 s / batch. (data: 5.37e-03). ETA=11:12:18, max mem: 24.8 GB 
[11/20 20:04:44 visual_prompt]: Epoch 21 / 100: avg data time: 4.75e-02, avg batch time: 0.9782, average train loss: 0.6915
[11/20 20:05:41 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3045, average loss: 0.6890
[11/20 20:05:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.94	
[11/20 20:05:41 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 20:07:26 visual_prompt]: 	Training 100/553. train loss: 0.6983,	0.9302 s / batch. (data: 2.46e-04). ETA=11:15:46, max mem: 24.8 GB 
[11/20 20:09:04 visual_prompt]: 	Training 200/553. train loss: 0.7030,	0.9477 s / batch. (data: 7.62e-04). ETA=11:26:53, max mem: 24.8 GB 
[11/20 20:10:41 visual_prompt]: 	Training 300/553. train loss: 0.7502,	0.9400 s / batch. (data: 7.45e-04). ETA=11:19:44, max mem: 24.8 GB 
[11/20 20:12:15 visual_prompt]: 	Training 400/553. train loss: 0.8898,	0.9452 s / batch. (data: 5.42e-03). ETA=11:21:54, max mem: 24.8 GB 
[11/20 20:13:49 visual_prompt]: 	Training 500/553. train loss: 0.6982,	0.9440 s / batch. (data: 1.04e-02). ETA=11:19:28, max mem: 24.8 GB 
[11/20 20:14:39 visual_prompt]: Epoch 22 / 100: avg data time: 4.18e-02, avg batch time: 0.9730, average train loss: 0.6955
[11/20 20:15:37 visual_prompt]: Inference (val):avg data time: 8.12e-05, avg batch time: 0.3032, average loss: 0.6914
[11/20 20:15:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.06	
[11/20 20:15:37 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 20:17:25 visual_prompt]: 	Training 100/553. train loss: 0.6899,	0.9092 s / batch. (data: 2.59e-04). ETA=10:52:07, max mem: 24.8 GB 
[11/20 20:19:01 visual_prompt]: 	Training 200/553. train loss: 0.6349,	0.9461 s / batch. (data: 5.36e-03). ETA=11:17:01, max mem: 24.8 GB 
[11/20 20:20:35 visual_prompt]: 	Training 300/553. train loss: 0.6100,	0.9160 s / batch. (data: 3.08e-04). ETA=10:53:54, max mem: 24.8 GB 
[11/20 20:22:09 visual_prompt]: 	Training 400/553. train loss: 0.6990,	0.9235 s / batch. (data: 2.71e-04). ETA=10:57:44, max mem: 24.8 GB 
[11/20 20:23:44 visual_prompt]: 	Training 500/553. train loss: 0.6752,	0.9479 s / batch. (data: 2.56e-04). ETA=11:13:34, max mem: 24.8 GB 
[11/20 20:24:35 visual_prompt]: Epoch 23 / 100: avg data time: 4.31e-02, avg batch time: 0.9736, average train loss: 0.6986
[11/20 20:25:33 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3076, average loss: 0.6990
[11/20 20:25:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.06	
[11/20 20:25:33 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 20:27:18 visual_prompt]: 	Training 100/553. train loss: 0.6977,	0.9239 s / batch. (data: 7.91e-04). ETA=10:54:09, max mem: 24.8 GB 
[11/20 20:28:54 visual_prompt]: 	Training 200/553. train loss: 0.7066,	0.9480 s / batch. (data: 7.99e-03). ETA=11:09:37, max mem: 24.8 GB 
[11/20 20:30:33 visual_prompt]: 	Training 300/553. train loss: 0.7047,	0.9278 s / batch. (data: 1.04e-02). ETA=10:53:47, max mem: 24.8 GB 
[11/20 20:32:07 visual_prompt]: 	Training 400/553. train loss: 0.6956,	0.9440 s / batch. (data: 4.01e-03). ETA=11:03:38, max mem: 24.8 GB 
[11/20 20:33:43 visual_prompt]: 	Training 500/553. train loss: 0.6906,	0.9432 s / batch. (data: 7.15e-03). ETA=11:01:30, max mem: 24.8 GB 
[11/20 20:34:32 visual_prompt]: Epoch 24 / 100: avg data time: 4.45e-02, avg batch time: 0.9752, average train loss: 0.6953
[11/20 20:35:30 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3052, average loss: 0.6900
[11/20 20:35:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.03	
[11/20 20:35:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/20 20:37:13 visual_prompt]: 	Training 100/553. train loss: 0.6341,	0.9320 s / batch. (data: 3.98e-03). ETA=10:51:16, max mem: 24.8 GB 
[11/20 20:38:53 visual_prompt]: 	Training 200/553. train loss: 0.7488,	0.9359 s / batch. (data: 2.60e-04). ETA=10:52:27, max mem: 24.8 GB 
[11/20 20:40:28 visual_prompt]: 	Training 300/553. train loss: 0.6982,	0.9446 s / batch. (data: 1.32e-02). ETA=10:56:57, max mem: 24.8 GB 
[11/20 20:42:04 visual_prompt]: 	Training 400/553. train loss: 0.7085,	0.9535 s / batch. (data: 1.04e-02). ETA=11:01:31, max mem: 24.8 GB 
[11/20 20:43:40 visual_prompt]: 	Training 500/553. train loss: 0.6289,	0.9576 s / batch. (data: 2.52e-04). ETA=11:02:48, max mem: 24.8 GB 
[11/20 20:44:32 visual_prompt]: Epoch 25 / 100: avg data time: 4.87e-02, avg batch time: 0.9799, average train loss: 0.6934
[11/20 20:45:30 visual_prompt]: Inference (val):avg data time: 4.95e-04, avg batch time: 0.3062, average loss: 0.6903
[11/20 20:45:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.06	
[11/20 20:45:30 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/20 20:47:16 visual_prompt]: 	Training 100/553. train loss: 0.6550,	0.9448 s / batch. (data: 1.04e-02). ETA=10:51:32, max mem: 24.8 GB 
[11/20 20:48:59 visual_prompt]: 	Training 200/553. train loss: 0.5876,	0.9141 s / batch. (data: 7.90e-03). ETA=10:28:49, max mem: 24.8 GB 
[11/20 20:50:38 visual_prompt]: 	Training 300/553. train loss: 0.7435,	0.9551 s / batch. (data: 1.05e-02). ETA=10:55:27, max mem: 24.8 GB 
[11/20 20:52:13 visual_prompt]: 	Training 400/553. train loss: 0.6948,	0.9262 s / batch. (data: 2.68e-04). ETA=10:34:03, max mem: 24.8 GB 
[11/20 20:53:51 visual_prompt]: 	Training 500/553. train loss: 0.6353,	0.9226 s / batch. (data: 7.36e-04). ETA=10:30:01, max mem: 24.8 GB 
[11/20 20:54:42 visual_prompt]: Epoch 26 / 100: avg data time: 7.00e-02, avg batch time: 0.9984, average train loss: 0.6925
[11/20 20:55:39 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3055, average loss: 0.6884
[11/20 20:55:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.29	
[11/20 20:55:39 visual_prompt]: Best epoch 26: best metric: -0.688
[11/20 20:55:39 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/20 20:57:25 visual_prompt]: 	Training 100/553. train loss: 0.7394,	0.9115 s / batch. (data: 2.19e-04). ETA=10:20:07, max mem: 24.8 GB 
[11/20 20:59:04 visual_prompt]: 	Training 200/553. train loss: 0.6986,	0.9455 s / batch. (data: 1.34e-02). ETA=10:41:41, max mem: 24.8 GB 
[11/20 21:00:38 visual_prompt]: 	Training 300/553. train loss: 0.7340,	0.9640 s / batch. (data: 7.79e-04). ETA=10:52:37, max mem: 24.8 GB 
[11/20 21:02:13 visual_prompt]: 	Training 400/553. train loss: 0.7807,	0.9160 s / batch. (data: 2.85e-04). ETA=10:18:36, max mem: 24.8 GB 
[11/20 21:03:50 visual_prompt]: 	Training 500/553. train loss: 0.7488,	0.9261 s / batch. (data: 5.37e-03). ETA=10:23:56, max mem: 24.8 GB 
[11/20 21:04:41 visual_prompt]: Epoch 27 / 100: avg data time: 4.72e-02, avg batch time: 0.9794, average train loss: 0.6987
[11/20 21:05:39 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3032, average loss: 0.6899
[11/20 21:05:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/20 21:05:39 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/20 21:07:23 visual_prompt]: 	Training 100/553. train loss: 0.7162,	0.9280 s / batch. (data: 2.76e-04). ETA=10:22:49, max mem: 24.8 GB 
[11/20 21:09:00 visual_prompt]: 	Training 200/553. train loss: 0.6330,	0.9424 s / batch. (data: 2.52e-04). ETA=10:30:57, max mem: 24.8 GB 
[11/20 21:10:37 visual_prompt]: 	Training 300/553. train loss: 0.6856,	0.9299 s / batch. (data: 7.98e-03). ETA=10:21:00, max mem: 24.8 GB 
[11/20 21:12:14 visual_prompt]: 	Training 400/553. train loss: 0.6928,	1.3369 s / batch. (data: 4.17e-01). ETA=14:50:34, max mem: 24.8 GB 
[11/20 21:13:49 visual_prompt]: 	Training 500/553. train loss: 0.8141,	0.9480 s / batch. (data: 8.03e-04). ETA=10:29:54, max mem: 24.8 GB 
[11/20 21:14:40 visual_prompt]: Epoch 28 / 100: avg data time: 4.76e-02, avg batch time: 0.9779, average train loss: 0.6992
[11/20 21:15:38 visual_prompt]: Inference (val):avg data time: 7.76e-04, avg batch time: 0.3072, average loss: 0.6926
[11/20 21:15:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.97	
[11/20 21:15:38 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/20 21:17:25 visual_prompt]: 	Training 100/553. train loss: 0.7434,	0.9274 s / batch. (data: 7.27e-04). ETA=10:13:53, max mem: 24.8 GB 
[11/20 21:19:03 visual_prompt]: 	Training 200/553. train loss: 0.7089,	0.9280 s / batch. (data: 2.55e-04). ETA=10:12:44, max mem: 24.8 GB 
[11/20 21:20:42 visual_prompt]: 	Training 300/553. train loss: 0.6987,	0.9212 s / batch. (data: 3.98e-03). ETA=10:06:42, max mem: 24.8 GB 
[11/20 21:22:20 visual_prompt]: 	Training 400/553. train loss: 0.6419,	0.9395 s / batch. (data: 5.35e-03). ETA=10:17:10, max mem: 24.8 GB 
[11/20 21:23:53 visual_prompt]: 	Training 500/553. train loss: 0.7097,	0.9223 s / batch. (data: 2.54e-04). ETA=10:04:22, max mem: 24.8 GB 
[11/20 21:24:43 visual_prompt]: Epoch 29 / 100: avg data time: 5.60e-02, avg batch time: 0.9855, average train loss: 0.6917
[11/20 21:25:40 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3035, average loss: 0.6969
[11/20 21:25:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.02	
[11/20 21:25:40 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/20 21:27:26 visual_prompt]: 	Training 100/553. train loss: 0.6404,	0.9240 s / batch. (data: 2.55e-04). ETA=10:03:06, max mem: 24.8 GB 
[11/20 21:29:02 visual_prompt]: 	Training 200/553. train loss: 0.5987,	0.9560 s / batch. (data: 7.98e-03). ETA=10:22:24, max mem: 24.8 GB 
[11/20 21:30:39 visual_prompt]: 	Training 300/553. train loss: 0.6316,	0.9153 s / batch. (data: 5.49e-03). ETA=9:54:24, max mem: 24.8 GB 
[11/20 21:32:14 visual_prompt]: 	Training 400/553. train loss: 0.6334,	0.9440 s / batch. (data: 8.47e-04). ETA=10:11:27, max mem: 24.8 GB 
[11/20 21:33:49 visual_prompt]: 	Training 500/553. train loss: 0.6983,	0.9400 s / batch. (data: 3.02e-04). ETA=10:07:16, max mem: 24.8 GB 
[11/20 21:34:38 visual_prompt]: Epoch 30 / 100: avg data time: 4.26e-02, avg batch time: 0.9729, average train loss: 0.6954
[11/20 21:35:36 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3030, average loss: 0.7083
[11/20 21:35:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.29	
[11/20 21:35:36 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/20 21:37:26 visual_prompt]: 	Training 100/553. train loss: 0.6625,	0.9252 s / batch. (data: 5.40e-03). ETA=9:55:22, max mem: 24.8 GB 
[11/20 21:39:00 visual_prompt]: 	Training 200/553. train loss: 0.6396,	0.9096 s / batch. (data: 5.37e-03). ETA=9:43:47, max mem: 24.8 GB 
[11/20 21:40:36 visual_prompt]: 	Training 300/553. train loss: 0.6025,	0.9312 s / batch. (data: 2.51e-04). ETA=9:56:06, max mem: 24.8 GB 
[11/20 21:42:12 visual_prompt]: 	Training 400/553. train loss: 0.6940,	0.9600 s / batch. (data: 2.64e-04). ETA=10:12:57, max mem: 24.8 GB 
[11/20 21:43:46 visual_prompt]: 	Training 500/553. train loss: 0.9248,	0.9560 s / batch. (data: 2.55e-04). ETA=10:08:48, max mem: 24.8 GB 
[11/20 21:44:37 visual_prompt]: Epoch 31 / 100: avg data time: 4.64e-02, avg batch time: 0.9782, average train loss: 0.6928
[11/20 21:45:35 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.3046, average loss: 0.6884
[11/20 21:45:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.00	
[11/20 21:45:35 visual_prompt]: Best epoch 31: best metric: -0.688
[11/20 21:45:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/20 21:47:18 visual_prompt]: 	Training 100/553. train loss: 0.6989,	0.9200 s / batch. (data: 2.44e-04). ETA=9:43:32, max mem: 24.8 GB 
[11/20 21:48:55 visual_prompt]: 	Training 200/553. train loss: 0.6886,	0.9223 s / batch. (data: 2.55e-04). ETA=9:43:26, max mem: 24.8 GB 
[11/20 21:50:31 visual_prompt]: 	Training 300/553. train loss: 0.6601,	0.9501 s / batch. (data: 3.01e-04). ETA=9:59:29, max mem: 24.8 GB 
[11/20 21:52:08 visual_prompt]: 	Training 400/553. train loss: 0.6630,	0.9440 s / batch. (data: 2.63e-04). ETA=9:54:02, max mem: 24.8 GB 
[11/20 21:53:46 visual_prompt]: 	Training 500/553. train loss: 0.7529,	0.9189 s / batch. (data: 3.33e-04). ETA=9:36:41, max mem: 24.8 GB 
[11/20 21:54:36 visual_prompt]: Epoch 32 / 100: avg data time: 4.78e-02, avg batch time: 0.9777, average train loss: 0.6922
[11/20 21:55:33 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3064, average loss: 0.6884
[11/20 21:55:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.99	
[11/20 21:55:33 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/20 21:57:19 visual_prompt]: 	Training 100/553. train loss: 0.7006,	0.9601 s / batch. (data: 2.15e-02). ETA=10:00:07, max mem: 24.8 GB 
[11/20 21:58:55 visual_prompt]: 	Training 200/553. train loss: 0.7165,	0.9155 s / batch. (data: 2.55e-04). ETA=9:30:44, max mem: 24.8 GB 
[11/20 22:00:32 visual_prompt]: 	Training 300/553. train loss: 0.6935,	0.9524 s / batch. (data: 1.04e-02). ETA=9:52:08, max mem: 24.8 GB 
[11/20 22:02:08 visual_prompt]: 	Training 400/553. train loss: 0.7641,	0.9235 s / batch. (data: 5.36e-03). ETA=9:32:39, max mem: 24.8 GB 
[11/20 22:03:43 visual_prompt]: 	Training 500/553. train loss: 0.8660,	0.9176 s / batch. (data: 5.36e-03). ETA=9:27:28, max mem: 24.8 GB 
[11/20 22:04:33 visual_prompt]: Epoch 33 / 100: avg data time: 4.66e-02, avg batch time: 0.9764, average train loss: 0.6968
[11/20 22:05:31 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3059, average loss: 0.6887
[11/20 22:05:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/20 22:05:31 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/20 22:07:17 visual_prompt]: 	Training 100/553. train loss: 0.7377,	0.9080 s / batch. (data: 2.59e-04). ETA=9:19:11, max mem: 24.8 GB 
[11/20 22:08:52 visual_prompt]: 	Training 200/553. train loss: 0.6986,	0.9240 s / batch. (data: 3.97e-03). ETA=9:27:29, max mem: 24.8 GB 
[11/20 22:10:30 visual_prompt]: 	Training 300/553. train loss: 0.7122,	0.9008 s / batch. (data: 2.73e-04). ETA=9:11:45, max mem: 24.8 GB 
[11/20 22:12:05 visual_prompt]: 	Training 400/553. train loss: 1.6157,	0.9760 s / batch. (data: 7.32e-04). ETA=9:56:10, max mem: 24.8 GB 
[11/20 22:13:39 visual_prompt]: 	Training 500/553. train loss: 0.7281,	0.9533 s / batch. (data: 2.80e-04). ETA=9:40:43, max mem: 24.8 GB 
[11/20 22:14:31 visual_prompt]: Epoch 34 / 100: avg data time: 4.62e-02, avg batch time: 0.9763, average train loss: 0.7001
[11/20 22:15:28 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3047, average loss: 0.6885
[11/20 22:15:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.20	
[11/20 22:15:28 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/20 22:17:14 visual_prompt]: 	Training 100/553. train loss: 0.7116,	0.9312 s / batch. (data: 2.58e-04). ETA=9:24:52, max mem: 24.8 GB 
[11/20 22:18:49 visual_prompt]: 	Training 200/553. train loss: 0.6550,	0.9097 s / batch. (data: 2.49e-04). ETA=9:10:19, max mem: 24.8 GB 
[11/20 22:20:23 visual_prompt]: 	Training 300/553. train loss: 0.6948,	0.9516 s / batch. (data: 6.54e-03). ETA=9:34:04, max mem: 24.8 GB 
[11/20 22:21:57 visual_prompt]: 	Training 400/553. train loss: 0.6561,	0.9400 s / batch. (data: 2.64e-04). ETA=9:25:33, max mem: 24.8 GB 
[11/20 22:23:37 visual_prompt]: 	Training 500/553. train loss: 0.7423,	0.9294 s / batch. (data: 5.36e-03). ETA=9:17:37, max mem: 24.8 GB 
[11/20 22:24:27 visual_prompt]: Epoch 35 / 100: avg data time: 4.14e-02, avg batch time: 0.9731, average train loss: 0.6911
[11/20 22:25:24 visual_prompt]: Inference (val):avg data time: 1.32e-04, avg batch time: 0.3051, average loss: 0.6931
[11/20 22:25:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.00	
[11/20 22:25:24 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.003867370395306068
[11/20 22:27:08 visual_prompt]: 	Training 100/553. train loss: 0.7763,	0.9360 s / batch. (data: 2.69e-04). ETA=9:19:11, max mem: 24.8 GB 
[11/20 22:28:45 visual_prompt]: 	Training 200/553. train loss: 0.7043,	0.9136 s / batch. (data: 5.36e-03). ETA=9:04:17, max mem: 24.8 GB 
[11/20 22:30:20 visual_prompt]: 	Training 300/553. train loss: 0.6930,	0.9116 s / batch. (data: 2.55e-04). ETA=9:01:35, max mem: 24.8 GB 
[11/20 22:31:54 visual_prompt]: 	Training 400/553. train loss: 0.8240,	0.9612 s / batch. (data: 1.56e-02). ETA=9:29:27, max mem: 24.8 GB 
[11/20 22:33:32 visual_prompt]: 	Training 500/553. train loss: 0.6939,	0.9308 s / batch. (data: 2.71e-04). ETA=9:09:51, max mem: 24.8 GB 
[11/20 22:34:22 visual_prompt]: Epoch 36 / 100: avg data time: 4.16e-02, avg batch time: 0.9721, average train loss: 0.6918
[11/20 22:35:19 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.3063, average loss: 0.6885
[11/20 22:35:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.99	
[11/20 22:35:19 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.0037974239344530382
[11/20 22:37:08 visual_prompt]: 	Training 100/553. train loss: 0.6938,	0.9432 s / batch. (data: 1.54e-02). ETA=9:14:49, max mem: 24.8 GB 
[11/20 22:38:44 visual_prompt]: 	Training 200/553. train loss: 0.6323,	0.9256 s / batch. (data: 2.74e-04). ETA=9:02:53, max mem: 24.8 GB 
[11/20 22:40:22 visual_prompt]: 	Training 300/553. train loss: 0.6841,	0.9599 s / batch. (data: 2.79e-02). ETA=9:21:25, max mem: 24.8 GB 
[11/20 22:41:57 visual_prompt]: 	Training 400/553. train loss: 0.6084,	0.9352 s / batch. (data: 2.50e-04). ETA=9:05:23, max mem: 24.8 GB 
[11/20 22:43:31 visual_prompt]: 	Training 500/553. train loss: 0.7234,	0.9338 s / batch. (data: 8.08e-03). ETA=9:03:01, max mem: 24.8 GB 
[11/20 22:44:21 visual_prompt]: Epoch 37 / 100: avg data time: 4.86e-02, avg batch time: 0.9784, average train loss: 0.6960
[11/20 22:45:19 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3048, average loss: 0.6886
[11/20 22:45:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/20 22:45:19 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.0037260587595762708
[11/20 22:47:05 visual_prompt]: 	Training 100/553. train loss: 0.7349,	0.9600 s / batch. (data: 8.08e-03). ETA=9:15:49, max mem: 24.8 GB 
[11/20 22:48:43 visual_prompt]: 	Training 200/553. train loss: 0.7152,	0.9482 s / batch. (data: 1.09e-02). ETA=9:07:23, max mem: 24.8 GB 
[11/20 22:50:19 visual_prompt]: 	Training 300/553. train loss: 0.6889,	0.9518 s / batch. (data: 2.52e-04). ETA=9:07:54, max mem: 24.8 GB 
[11/20 22:51:56 visual_prompt]: 	Training 400/553. train loss: 0.6407,	0.9407 s / batch. (data: 2.80e-04). ETA=8:59:58, max mem: 24.8 GB 
[11/20 22:53:30 visual_prompt]: 	Training 500/553. train loss: 0.7413,	0.9406 s / batch. (data: 1.64e-02). ETA=8:58:19, max mem: 24.8 GB 
[11/20 22:54:19 visual_prompt]: Epoch 38 / 100: avg data time: 5.00e-02, avg batch time: 0.9776, average train loss: 0.6920
[11/20 22:55:17 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3055, average loss: 0.7115
[11/20 22:55:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/20 22:55:17 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.0036533529074467197
[11/20 22:57:03 visual_prompt]: 	Training 100/553. train loss: 0.6975,	0.9212 s / batch. (data: 1.04e-02). ETA=8:44:50, max mem: 24.8 GB 
[11/20 22:58:38 visual_prompt]: 	Training 200/553. train loss: 0.6452,	0.9320 s / batch. (data: 2.87e-04). ETA=8:49:28, max mem: 24.8 GB 
[11/20 23:00:13 visual_prompt]: 	Training 300/553. train loss: 0.6548,	0.9185 s / batch. (data: 2.73e-04). ETA=8:40:16, max mem: 24.8 GB 
[11/20 23:01:50 visual_prompt]: 	Training 400/553. train loss: 0.6497,	0.9360 s / batch. (data: 1.19e-02). ETA=8:48:36, max mem: 24.8 GB 
[11/20 23:03:26 visual_prompt]: 	Training 500/553. train loss: 0.8282,	0.9666 s / batch. (data: 2.10e-02). ETA=9:04:17, max mem: 24.8 GB 
[11/20 23:04:16 visual_prompt]: Epoch 39 / 100: avg data time: 4.44e-02, avg batch time: 0.9749, average train loss: 0.6910
[11/20 23:05:14 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3047, average loss: 0.6889
[11/20 23:05:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.96	
[11/20 23:05:14 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.003579385880846232
[11/20 23:06:58 visual_prompt]: 	Training 100/553. train loss: 0.6557,	2.4282 s / batch. (data: 1.50e+00). ETA=22:41:09, max mem: 24.8 GB 
[11/20 23:08:38 visual_prompt]: 	Training 200/553. train loss: 0.6966,	0.9482 s / batch. (data: 8.31e-03). ETA=8:49:57, max mem: 24.8 GB 
[11/20 23:10:12 visual_prompt]: 	Training 300/553. train loss: 0.8469,	0.9596 s / batch. (data: 5.91e-03). ETA=8:54:43, max mem: 24.8 GB 
[11/20 23:11:50 visual_prompt]: 	Training 400/553. train loss: 0.6693,	0.9076 s / batch. (data: 3.82e-03). ETA=8:24:12, max mem: 24.8 GB 
[11/20 23:13:24 visual_prompt]: 	Training 500/553. train loss: 0.7631,	0.9520 s / batch. (data: 3.18e-04). ETA=8:47:16, max mem: 24.8 GB 
[11/20 23:14:15 visual_prompt]: Epoch 40 / 100: avg data time: 4.87e-02, avg batch time: 0.9792, average train loss: 0.6903
[11/20 23:15:13 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.3057, average loss: 0.6928
[11/20 23:15:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/20 23:15:13 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.003504238561632424
[11/20 23:17:01 visual_prompt]: 	Training 100/553. train loss: 0.6429,	0.9562 s / batch. (data: 3.02e-04). ETA=8:47:12, max mem: 24.8 GB 
[11/20 23:18:39 visual_prompt]: 	Training 200/553. train loss: 0.6683,	0.9400 s / batch. (data: 2.87e-04). ETA=8:36:40, max mem: 24.8 GB 
[11/20 23:20:15 visual_prompt]: 	Training 300/553. train loss: 0.6719,	0.9270 s / batch. (data: 2.40e-04). ETA=8:28:00, max mem: 24.8 GB 
[11/20 23:21:48 visual_prompt]: 	Training 400/553. train loss: 0.6821,	0.9218 s / batch. (data: 2.58e-04). ETA=8:23:36, max mem: 24.8 GB 
[11/20 23:23:23 visual_prompt]: 	Training 500/553. train loss: 0.6902,	0.9200 s / batch. (data: 2.91e-04). ETA=8:21:06, max mem: 24.8 GB 
[11/20 23:24:13 visual_prompt]: Epoch 41 / 100: avg data time: 4.81e-02, avg batch time: 0.9773, average train loss: 0.6924
[11/20 23:25:11 visual_prompt]: Inference (val):avg data time: 1.30e-04, avg batch time: 0.3043, average loss: 0.6923
[11/20 23:25:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[11/20 23:25:11 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.003427993122295552
[11/20 23:26:58 visual_prompt]: 	Training 100/553. train loss: 0.6951,	0.9311 s / batch. (data: 7.97e-03). ETA=8:24:46, max mem: 24.8 GB 
[11/20 23:28:33 visual_prompt]: 	Training 200/553. train loss: 0.8952,	0.9242 s / batch. (data: 3.15e-04). ETA=8:19:29, max mem: 24.8 GB 
[11/20 23:30:10 visual_prompt]: 	Training 300/553. train loss: 0.6611,	0.9151 s / batch. (data: 1.04e-02). ETA=8:13:03, max mem: 24.8 GB 
[11/20 23:31:44 visual_prompt]: 	Training 400/553. train loss: 0.6335,	0.9480 s / batch. (data: 3.99e-03). ETA=8:29:12, max mem: 24.8 GB 
[11/20 23:33:24 visual_prompt]: 	Training 500/553. train loss: 0.7260,	0.9329 s / batch. (data: 2.31e-04). ETA=8:19:31, max mem: 24.8 GB 
[11/20 23:34:13 visual_prompt]: Epoch 42 / 100: avg data time: 5.09e-02, avg batch time: 0.9802, average train loss: 0.6920
[11/20 23:35:11 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3061, average loss: 0.6890
[11/20 23:35:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.94	
[11/20 23:35:11 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.003350732936104108
[11/20 23:36:57 visual_prompt]: 	Training 100/553. train loss: 0.6909,	0.9200 s / batch. (data: 7.97e-03). ETA=8:10:17, max mem: 24.8 GB 
[11/20 23:38:31 visual_prompt]: 	Training 200/553. train loss: 0.7263,	0.9453 s / batch. (data: 2.77e-04). ETA=8:22:10, max mem: 24.8 GB 
[11/20 23:40:09 visual_prompt]: 	Training 300/553. train loss: 0.8538,	0.9385 s / batch. (data: 6.20e-03). ETA=8:16:58, max mem: 24.8 GB 
[11/20 23:41:45 visual_prompt]: 	Training 400/553. train loss: 0.6754,	0.9387 s / batch. (data: 5.38e-03). ETA=8:15:31, max mem: 24.8 GB 
[11/20 23:43:19 visual_prompt]: 	Training 500/553. train loss: 0.7182,	0.9240 s / batch. (data: 7.96e-03). ETA=8:06:13, max mem: 24.8 GB 
[11/20 23:44:10 visual_prompt]: Epoch 43 / 100: avg data time: 4.34e-02, avg batch time: 0.9736, average train loss: 0.6945
[11/20 23:45:08 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3044, average loss: 0.7080
[11/20 23:45:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.57	
[11/20 23:45:08 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.0032725424859373687
[11/20 23:46:55 visual_prompt]: 	Training 100/553. train loss: 0.6975,	0.9752 s / batch. (data: 5.35e-02). ETA=8:30:40, max mem: 24.8 GB 
[11/20 23:48:33 visual_prompt]: 	Training 200/553. train loss: 0.6432,	0.9360 s / batch. (data: 2.83e-04). ETA=8:08:36, max mem: 24.8 GB 
[11/20 23:50:09 visual_prompt]: 	Training 300/553. train loss: 0.6949,	0.9227 s / batch. (data: 2.90e-04). ETA=8:00:06, max mem: 24.8 GB 
[11/20 23:51:45 visual_prompt]: 	Training 400/553. train loss: 0.6305,	0.9400 s / batch. (data: 2.91e-04). ETA=8:07:33, max mem: 24.8 GB 
[11/20 23:53:21 visual_prompt]: 	Training 500/553. train loss: 0.7045,	0.9154 s / batch. (data: 2.72e-04). ETA=7:53:16, max mem: 24.8 GB 
[11/20 23:54:10 visual_prompt]: Epoch 44 / 100: avg data time: 5.18e-02, avg batch time: 0.9806, average train loss: 0.6948
[11/20 23:55:08 visual_prompt]: Inference (val):avg data time: 4.05e-04, avg batch time: 0.3035, average loss: 0.6888
[11/20 23:55:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/20 23:55:08 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0031935072719046116
[11/20 23:56:52 visual_prompt]: 	Training 100/553. train loss: 0.7096,	0.9024 s / batch. (data: 2.69e-04). ETA=7:44:15, max mem: 24.8 GB 
[11/20 23:58:31 visual_prompt]: 	Training 200/553. train loss: 0.6399,	0.9278 s / batch. (data: 2.90e-04). ETA=7:55:45, max mem: 24.8 GB 
[11/21 00:00:06 visual_prompt]: 	Training 300/553. train loss: 0.6597,	0.9604 s / batch. (data: 1.64e-02). ETA=8:10:54, max mem: 24.8 GB 
[11/21 00:01:42 visual_prompt]: 	Training 400/553. train loss: 0.6951,	0.9600 s / batch. (data: 2.86e-04). ETA=8:09:04, max mem: 24.8 GB 
[11/21 00:03:18 visual_prompt]: 	Training 500/553. train loss: 0.6474,	0.9129 s / batch. (data: 7.97e-03). ETA=7:43:35, max mem: 24.8 GB 
[11/21 00:04:10 visual_prompt]: Epoch 45 / 100: avg data time: 4.91e-02, avg batch time: 0.9798, average train loss: 0.6929
[11/21 00:05:07 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3046, average loss: 0.6884
[11/21 00:05:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/21 00:05:07 visual_prompt]: Best epoch 45: best metric: -0.688
[11/21 00:05:07 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.003113713717851998
[11/21 00:06:52 visual_prompt]: 	Training 100/553. train loss: 0.7060,	0.9645 s / batch. (data: 3.24e-02). ETA=8:07:18, max mem: 24.8 GB 
[11/21 00:08:28 visual_prompt]: 	Training 200/553. train loss: 0.7643,	0.9082 s / batch. (data: 2.81e-04). ETA=7:37:22, max mem: 24.8 GB 
[11/21 00:10:03 visual_prompt]: 	Training 300/553. train loss: 0.7293,	0.9194 s / batch. (data: 2.62e-04). ETA=7:41:29, max mem: 24.8 GB 
[11/21 00:11:40 visual_prompt]: 	Training 400/553. train loss: 0.6984,	0.9197 s / batch. (data: 2.64e-04). ETA=7:40:03, max mem: 24.8 GB 
[11/21 00:13:14 visual_prompt]: 	Training 500/553. train loss: 0.6978,	0.9212 s / batch. (data: 2.47e-04). ETA=7:39:17, max mem: 24.8 GB 
[11/21 00:14:05 visual_prompt]: Epoch 46 / 100: avg data time: 4.39e-02, avg batch time: 0.9726, average train loss: 0.6909
[11/21 00:15:03 visual_prompt]: Inference (val):avg data time: 2.73e-04, avg batch time: 0.3061, average loss: 0.6884
[11/21 00:15:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/21 00:15:03 visual_prompt]: Best epoch 46: best metric: -0.688
[11/21 00:15:03 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.0030332490768593674
[11/21 00:16:50 visual_prompt]: 	Training 100/553. train loss: 0.6662,	0.9131 s / batch. (data: 2.69e-04). ETA=7:32:55, max mem: 24.8 GB 
[11/21 00:18:30 visual_prompt]: 	Training 200/553. train loss: 0.4950,	0.9600 s / batch. (data: 7.90e-04). ETA=7:54:36, max mem: 24.8 GB 
[11/21 00:20:05 visual_prompt]: 	Training 300/553. train loss: 0.6938,	0.9546 s / batch. (data: 6.60e-03). ETA=7:50:21, max mem: 24.8 GB 
[11/21 00:21:40 visual_prompt]: 	Training 400/553. train loss: 0.6199,	0.9401 s / batch. (data: 2.49e-04). ETA=7:41:38, max mem: 24.8 GB 
[11/21 00:23:15 visual_prompt]: 	Training 500/553. train loss: 0.7309,	0.9184 s / batch. (data: 3.25e-04). ETA=7:29:26, max mem: 24.8 GB 
[11/21 00:24:04 visual_prompt]: Epoch 47 / 100: avg data time: 4.76e-02, avg batch time: 0.9777, average train loss: 0.6933
[11/21 00:25:01 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3034, average loss: 0.6898
[11/21 00:25:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/21 00:25:01 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.0029522013358302753
[11/21 00:26:45 visual_prompt]: 	Training 100/553. train loss: 0.8286,	0.9236 s / batch. (data: 2.64e-04). ETA=7:29:37, max mem: 24.8 GB 
[11/21 00:28:22 visual_prompt]: 	Training 200/553. train loss: 0.7580,	0.9020 s / batch. (data: 2.52e-04). ETA=7:17:35, max mem: 24.8 GB 
[11/21 00:29:59 visual_prompt]: 	Training 300/553. train loss: 0.7033,	0.9053 s / batch. (data: 2.85e-04). ETA=7:17:42, max mem: 24.8 GB 
[11/21 00:31:33 visual_prompt]: 	Training 400/553. train loss: 0.6425,	0.9440 s / batch. (data: 2.50e-04). ETA=7:34:50, max mem: 24.8 GB 
[11/21 00:33:11 visual_prompt]: 	Training 500/553. train loss: 0.7000,	0.9382 s / batch. (data: 5.39e-03). ETA=7:30:27, max mem: 24.8 GB 
[11/21 00:34:00 visual_prompt]: Epoch 48 / 100: avg data time: 4.32e-02, avg batch time: 0.9743, average train loss: 0.6918
[11/21 00:34:58 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3046, average loss: 0.6885
[11/21 00:34:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.00	
[11/21 00:34:58 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.002870659119279605
[11/21 00:36:43 visual_prompt]: 	Training 100/553. train loss: 0.6929,	0.9244 s / batch. (data: 5.37e-03). ETA=7:21:28, max mem: 24.8 GB 
[11/21 00:38:20 visual_prompt]: 	Training 200/553. train loss: 0.6861,	0.9244 s / batch. (data: 2.73e-04). ETA=7:19:57, max mem: 24.8 GB 
[11/21 00:39:57 visual_prompt]: 	Training 300/553. train loss: 0.7392,	0.9469 s / batch. (data: 1.56e-02). ETA=7:29:05, max mem: 24.8 GB 
[11/21 00:41:34 visual_prompt]: 	Training 400/553. train loss: 0.7800,	0.9229 s / batch. (data: 2.19e-04). ETA=7:16:10, max mem: 24.8 GB 
[11/21 00:43:09 visual_prompt]: 	Training 500/553. train loss: 0.7095,	0.9319 s / batch. (data: 1.04e-02). ETA=7:18:52, max mem: 24.8 GB 
[11/21 00:44:00 visual_prompt]: Epoch 49 / 100: avg data time: 4.91e-02, avg batch time: 0.9796, average train loss: 0.6902
[11/21 00:44:58 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3038, average loss: 0.6971
[11/21 00:44:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.77	
[11/21 00:44:58 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.002788711592423966
[11/21 00:46:45 visual_prompt]: 	Training 100/553. train loss: 0.7004,	0.9120 s / batch. (data: 2.51e-04). ETA=7:07:10, max mem: 24.8 GB 
[11/21 00:48:21 visual_prompt]: 	Training 200/553. train loss: 0.6650,	0.9512 s / batch. (data: 2.66e-04). ETA=7:23:57, max mem: 24.8 GB 
[11/21 00:49:55 visual_prompt]: 	Training 300/553. train loss: 0.6913,	0.9615 s / batch. (data: 7.31e-04). ETA=7:27:09, max mem: 24.8 GB 
[11/21 00:51:31 visual_prompt]: 	Training 400/553. train loss: 0.8397,	0.9320 s / batch. (data: 2.67e-04). ETA=7:11:52, max mem: 24.8 GB 
[11/21 00:53:05 visual_prompt]: 	Training 500/553. train loss: 0.6927,	0.9460 s / batch. (data: 6.17e-03). ETA=7:16:47, max mem: 24.8 GB 
[11/21 00:53:56 visual_prompt]: Epoch 50 / 100: avg data time: 4.35e-02, avg batch time: 0.9735, average train loss: 0.6973
[11/21 00:54:54 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.3036, average loss: 0.6885
[11/21 00:54:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.86	
[11/21 00:54:54 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.0027064483636808313
[11/21 00:56:36 visual_prompt]: 	Training 100/553. train loss: 0.6254,	0.9315 s / batch. (data: 3.96e-03). ETA=7:07:41, max mem: 24.8 GB 
[11/21 00:58:14 visual_prompt]: 	Training 200/553. train loss: 0.5557,	0.9370 s / batch. (data: 2.19e-02). ETA=7:08:39, max mem: 24.8 GB 
[11/21 00:59:48 visual_prompt]: 	Training 300/553. train loss: 0.8064,	0.9320 s / batch. (data: 3.15e-04). ETA=7:04:50, max mem: 24.8 GB 
[11/21 01:01:24 visual_prompt]: 	Training 400/553. train loss: 0.6945,	0.9393 s / batch. (data: 5.36e-03). ETA=7:06:35, max mem: 24.8 GB 
[11/21 01:03:02 visual_prompt]: 	Training 500/553. train loss: 0.6966,	0.9491 s / batch. (data: 2.07e-02). ETA=7:09:28, max mem: 24.8 GB 
[11/21 01:03:52 visual_prompt]: Epoch 51 / 100: avg data time: 4.21e-02, avg batch time: 0.9730, average train loss: 0.6926
[11/21 01:04:49 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3037, average loss: 0.6884
[11/21 01:04:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.10	
[11/21 01:04:49 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.0026239593866830557
[11/21 01:06:37 visual_prompt]: 	Training 100/553. train loss: 0.6116,	0.9240 s / batch. (data: 3.98e-03). ETA=6:55:45, max mem: 24.8 GB 
[11/21 01:08:11 visual_prompt]: 	Training 200/553. train loss: 0.6988,	0.9591 s / batch. (data: 6.59e-03). ETA=7:09:55, max mem: 24.8 GB 
[11/21 01:09:50 visual_prompt]: 	Training 300/553. train loss: 0.6572,	0.9409 s / batch. (data: 6.98e-04). ETA=7:00:13, max mem: 24.8 GB 
[11/21 01:11:25 visual_prompt]: 	Training 400/553. train loss: 0.6460,	0.9480 s / batch. (data: 3.32e-03). ETA=7:01:48, max mem: 24.8 GB 
[11/21 01:13:00 visual_prompt]: 	Training 500/553. train loss: 0.6505,	0.9440 s / batch. (data: 2.58e-04). ETA=6:58:27, max mem: 24.8 GB 
[11/21 01:13:49 visual_prompt]: Epoch 52 / 100: avg data time: 4.66e-02, avg batch time: 0.9762, average train loss: 0.6897
[11/21 01:14:47 visual_prompt]: Inference (val):avg data time: 2.58e-04, avg batch time: 0.3037, average loss: 0.6886
[11/21 01:14:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.11	
[11/21 01:14:47 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.0025413348619158964
[11/21 01:16:32 visual_prompt]: 	Training 100/553. train loss: 0.5925,	0.9360 s / batch. (data: 2.65e-04). ETA=6:52:31, max mem: 24.8 GB 
[11/21 01:18:08 visual_prompt]: 	Training 200/553. train loss: 0.6930,	0.9477 s / batch. (data: 7.20e-04). ETA=6:56:06, max mem: 24.8 GB 
[11/21 01:19:48 visual_prompt]: 	Training 300/553. train loss: 0.6958,	0.9283 s / batch. (data: 5.81e-03). ETA=6:46:02, max mem: 24.8 GB 
[11/21 01:21:24 visual_prompt]: 	Training 400/553. train loss: 0.6634,	0.9762 s / batch. (data: 1.63e-02). ETA=7:05:22, max mem: 24.8 GB 
[11/21 01:22:58 visual_prompt]: 	Training 500/553. train loss: 0.7020,	0.9456 s / batch. (data: 7.93e-03). ETA=6:50:27, max mem: 24.8 GB 
[11/21 01:23:48 visual_prompt]: Epoch 53 / 100: avg data time: 4.73e-02, avg batch time: 0.9767, average train loss: 0.6906
[11/21 01:24:46 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3032, average loss: 0.6884
[11/21 01:24:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.13	
[11/21 01:24:46 visual_prompt]: Best epoch 53: best metric: -0.688
[11/21 01:24:46 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.0024586651380841037
[11/21 01:26:33 visual_prompt]: 	Training 100/553. train loss: 0.6996,	0.9398 s / batch. (data: 3.99e-03). ETA=6:45:32, max mem: 24.8 GB 
[11/21 01:28:08 visual_prompt]: 	Training 200/553. train loss: 0.5967,	0.9401 s / batch. (data: 2.71e-04). ETA=6:44:06, max mem: 24.8 GB 
[11/21 01:29:45 visual_prompt]: 	Training 300/553. train loss: 0.7420,	0.9520 s / batch. (data: 2.90e-04). ETA=6:47:38, max mem: 24.8 GB 
[11/21 01:31:19 visual_prompt]: 	Training 400/553. train loss: 0.6489,	0.9160 s / batch. (data: 3.09e-04). ETA=6:30:41, max mem: 24.8 GB 
[11/21 01:32:52 visual_prompt]: 	Training 500/553. train loss: 0.6585,	0.9288 s / batch. (data: 5.42e-03). ETA=6:34:35, max mem: 24.8 GB 
[11/21 01:33:45 visual_prompt]: Epoch 54 / 100: avg data time: 4.32e-02, avg batch time: 0.9740, average train loss: 0.6903
[11/21 01:34:43 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3030, average loss: 0.6956
[11/21 01:34:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.85	
[11/21 01:34:43 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.002376040613316944
[11/21 01:36:32 visual_prompt]: 	Training 100/553. train loss: 0.6481,	0.9107 s / batch. (data: 2.70e-04). ETA=6:24:35, max mem: 24.8 GB 
[11/21 01:38:05 visual_prompt]: 	Training 200/553. train loss: 0.6965,	0.9225 s / batch. (data: 2.88e-04). ETA=6:28:03, max mem: 24.8 GB 
[11/21 01:39:42 visual_prompt]: 	Training 300/553. train loss: 0.6047,	0.9347 s / batch. (data: 2.33e-04). ETA=6:31:35, max mem: 24.8 GB 
[11/21 01:41:18 visual_prompt]: 	Training 400/553. train loss: 0.6941,	0.9849 s / batch. (data: 2.74e-04). ETA=6:51:00, max mem: 24.8 GB 
[11/21 01:42:54 visual_prompt]: 	Training 500/553. train loss: 0.6432,	0.9188 s / batch. (data: 8.00e-03). ETA=6:21:53, max mem: 24.8 GB 
[11/21 01:43:43 visual_prompt]: Epoch 55 / 100: avg data time: 4.71e-02, avg batch time: 0.9770, average train loss: 0.6935
[11/21 01:44:41 visual_prompt]: Inference (val):avg data time: 2.58e-04, avg batch time: 0.3011, average loss: 0.6884
[11/21 01:44:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/21 01:44:41 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.0022935516363191692
[11/21 01:46:29 visual_prompt]: 	Training 100/553. train loss: 0.6075,	0.9142 s / batch. (data: 6.91e-03). ETA=6:17:38, max mem: 24.8 GB 
[11/21 01:48:03 visual_prompt]: 	Training 200/553. train loss: 0.7019,	0.9325 s / batch. (data: 1.55e-02). ETA=6:23:38, max mem: 24.8 GB 
[11/21 01:49:41 visual_prompt]: 	Training 300/553. train loss: 0.6450,	0.9263 s / batch. (data: 7.51e-04). ETA=6:19:32, max mem: 24.8 GB 
[11/21 01:51:18 visual_prompt]: 	Training 400/553. train loss: 0.6977,	0.9298 s / batch. (data: 7.24e-04). ETA=6:19:25, max mem: 24.8 GB 
[11/21 01:52:53 visual_prompt]: 	Training 500/553. train loss: 0.7672,	0.9411 s / batch. (data: 2.47e-04). ETA=6:22:29, max mem: 24.8 GB 
[11/21 01:53:43 visual_prompt]: Epoch 56 / 100: avg data time: 5.20e-02, avg batch time: 0.9808, average train loss: 0.6900
[11/21 01:54:41 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.3040, average loss: 0.6891
[11/21 01:54:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/21 01:54:41 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.0022112884075760346
[11/21 01:56:26 visual_prompt]: 	Training 100/553. train loss: 0.7846,	0.9483 s / batch. (data: 1.04e-02). ETA=6:22:58, max mem: 24.8 GB 
[11/21 01:58:03 visual_prompt]: 	Training 200/553. train loss: 0.6605,	0.9368 s / batch. (data: 8.72e-03). ETA=6:16:46, max mem: 24.8 GB 
[11/21 01:59:38 visual_prompt]: 	Training 300/553. train loss: 0.6975,	0.9257 s / batch. (data: 3.33e-04). ETA=6:10:47, max mem: 24.8 GB 
[11/21 02:01:14 visual_prompt]: 	Training 400/553. train loss: 0.6574,	0.9280 s / batch. (data: 2.85e-04). ETA=6:10:08, max mem: 24.8 GB 
[11/21 02:02:50 visual_prompt]: 	Training 500/553. train loss: 0.6972,	0.9389 s / batch. (data: 2.80e-04). ETA=6:12:56, max mem: 24.8 GB 
[11/21 02:03:40 visual_prompt]: Epoch 57 / 100: avg data time: 4.36e-02, avg batch time: 0.9737, average train loss: 0.6915
[11/21 02:04:37 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3037, average loss: 0.6884
[11/21 02:04:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.08	
[11/21 02:04:37 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.002129340880720395
[11/21 02:06:23 visual_prompt]: 	Training 100/553. train loss: 0.7379,	0.9240 s / batch. (data: 3.99e-03). ETA=6:04:40, max mem: 24.8 GB 
[11/21 02:08:01 visual_prompt]: 	Training 200/553. train loss: 0.6121,	0.9449 s / batch. (data: 2.49e-04). ETA=6:11:20, max mem: 24.8 GB 
[11/21 02:09:36 visual_prompt]: 	Training 300/553. train loss: 0.6978,	0.9320 s / batch. (data: 2.99e-04). ETA=6:04:42, max mem: 24.8 GB 
[11/21 02:11:10 visual_prompt]: 	Training 400/553. train loss: 0.7697,	0.9328 s / batch. (data: 5.37e-03). ETA=6:03:28, max mem: 24.8 GB 
[11/21 02:12:46 visual_prompt]: 	Training 500/553. train loss: 0.6558,	0.9422 s / batch. (data: 1.05e-02). ETA=6:05:34, max mem: 24.8 GB 
[11/21 02:13:37 visual_prompt]: Epoch 58 / 100: avg data time: 4.46e-02, avg batch time: 0.9751, average train loss: 0.6897
[11/21 02:14:34 visual_prompt]: Inference (val):avg data time: 8.54e-05, avg batch time: 0.3038, average loss: 0.6884
[11/21 02:14:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.00	
[11/21 02:14:34 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.002047798664169726
[11/21 02:16:20 visual_prompt]: 	Training 100/553. train loss: 0.6957,	0.9153 s / batch. (data: 2.58e-04). ETA=5:52:46, max mem: 24.8 GB 
[11/21 02:17:56 visual_prompt]: 	Training 200/553. train loss: 0.5837,	0.9508 s / batch. (data: 5.56e-03). ETA=6:04:53, max mem: 24.8 GB 
[11/21 02:19:33 visual_prompt]: 	Training 300/553. train loss: 0.8063,	0.9186 s / batch. (data: 5.39e-03). ETA=5:50:59, max mem: 24.8 GB 
[11/21 02:21:08 visual_prompt]: 	Training 400/553. train loss: 0.6980,	0.9404 s / batch. (data: 7.99e-03). ETA=5:57:45, max mem: 24.8 GB 
[11/21 02:22:45 visual_prompt]: 	Training 500/553. train loss: 0.7028,	0.9440 s / batch. (data: 2.63e-04). ETA=5:57:33, max mem: 24.8 GB 
[11/21 02:23:35 visual_prompt]: Epoch 59 / 100: avg data time: 4.83e-02, avg batch time: 0.9783, average train loss: 0.6904
[11/21 02:24:33 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3053, average loss: 0.6892
[11/21 02:24:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/21 02:24:33 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.001966750923140633
[11/21 02:26:19 visual_prompt]: 	Training 100/553. train loss: 0.7169,	0.9377 s / batch. (data: 5.88e-03). ETA=5:52:46, max mem: 24.8 GB 
[11/21 02:27:55 visual_prompt]: 	Training 200/553. train loss: 0.6711,	0.9400 s / batch. (data: 2.70e-04). ETA=5:52:05, max mem: 24.8 GB 
[11/21 02:29:31 visual_prompt]: 	Training 300/553. train loss: 0.6594,	0.9446 s / batch. (data: 1.64e-02). ETA=5:52:14, max mem: 24.8 GB 
[11/21 02:31:07 visual_prompt]: 	Training 400/553. train loss: 0.6579,	0.9435 s / batch. (data: 5.37e-03). ETA=5:50:14, max mem: 24.8 GB 
[11/21 02:32:45 visual_prompt]: 	Training 500/553. train loss: 0.7669,	0.9625 s / batch. (data: 1.04e-02). ETA=5:55:41, max mem: 24.8 GB 
[11/21 02:33:37 visual_prompt]: Epoch 60 / 100: avg data time: 5.32e-02, avg batch time: 0.9827, average train loss: 0.6937
[11/21 02:34:34 visual_prompt]: Inference (val):avg data time: 1.22e-04, avg batch time: 0.3047, average loss: 0.6900
[11/21 02:34:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.65	
[11/21 02:34:34 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.0018862862821480023
[11/21 02:36:25 visual_prompt]: 	Training 100/553. train loss: 0.5878,	0.9480 s / batch. (data: 7.80e-04). ETA=5:47:55, max mem: 24.8 GB 
[11/21 02:37:58 visual_prompt]: 	Training 200/553. train loss: 0.6129,	0.9202 s / batch. (data: 2.53e-04). ETA=5:36:11, max mem: 24.8 GB 
[11/21 02:39:33 visual_prompt]: 	Training 300/553. train loss: 0.6708,	0.9287 s / batch. (data: 1.05e-02). ETA=5:37:44, max mem: 24.8 GB 
[11/21 02:41:08 visual_prompt]: 	Training 400/553. train loss: 0.8036,	0.9279 s / batch. (data: 4.81e-03). ETA=5:35:53, max mem: 24.8 GB 
[11/21 02:42:44 visual_prompt]: 	Training 500/553. train loss: 0.6988,	0.9720 s / batch. (data: 7.99e-03). ETA=5:50:15, max mem: 24.8 GB 
[11/21 02:43:36 visual_prompt]: Epoch 61 / 100: avg data time: 4.84e-02, avg batch time: 0.9781, average train loss: 0.6897
[11/21 02:44:33 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3019, average loss: 0.6886
[11/21 02:44:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.02	
[11/21 02:44:33 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0018064927280953891
[11/21 02:46:21 visual_prompt]: 	Training 100/553. train loss: 0.6990,	0.9414 s / batch. (data: 5.37e-03). ETA=5:36:48, max mem: 24.8 GB 
[11/21 02:47:55 visual_prompt]: 	Training 200/553. train loss: 0.6990,	0.9652 s / batch. (data: 7.69e-04). ETA=5:43:43, max mem: 24.8 GB 
[11/21 02:49:33 visual_prompt]: 	Training 300/553. train loss: 0.6984,	0.9326 s / batch. (data: 5.37e-03). ETA=5:30:33, max mem: 24.8 GB 
[11/21 02:51:08 visual_prompt]: 	Training 400/553. train loss: 0.6963,	0.9328 s / batch. (data: 6.80e-04). ETA=5:29:04, max mem: 24.8 GB 
[11/21 02:52:42 visual_prompt]: 	Training 500/553. train loss: 0.6009,	0.9237 s / batch. (data: 5.27e-03). ETA=5:24:20, max mem: 24.8 GB 
[11/21 02:53:32 visual_prompt]: Epoch 62 / 100: avg data time: 4.50e-02, avg batch time: 0.9735, average train loss: 0.6893
[11/21 02:54:29 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3044, average loss: 0.6921
[11/21 02:54:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.93	
[11/21 02:54:29 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.0017274575140626316
[11/21 02:56:15 visual_prompt]: 	Training 100/553. train loss: 0.8543,	0.9240 s / batch. (data: 3.19e-04). ETA=5:22:04, max mem: 24.8 GB 
[11/21 02:57:54 visual_prompt]: 	Training 200/553. train loss: 0.6983,	0.9369 s / batch. (data: 3.94e-03). ETA=5:24:59, max mem: 24.8 GB 
[11/21 02:59:31 visual_prompt]: 	Training 300/553. train loss: 0.6976,	0.9195 s / batch. (data: 2.60e-04). ETA=5:17:26, max mem: 24.8 GB 
[11/21 03:01:05 visual_prompt]: 	Training 400/553. train loss: 0.6972,	0.9309 s / batch. (data: 8.03e-03). ETA=5:19:49, max mem: 24.8 GB 
[11/21 03:02:43 visual_prompt]: 	Training 500/553. train loss: 0.7285,	0.9116 s / batch. (data: 3.28e-04). ETA=5:11:40, max mem: 24.8 GB 
[11/21 03:03:32 visual_prompt]: Epoch 63 / 100: avg data time: 5.21e-02, avg batch time: 0.9814, average train loss: 0.6900
[11/21 03:04:30 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3042, average loss: 0.6884
[11/21 03:04:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/21 03:04:30 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.0016492670638958923
[11/21 03:06:17 visual_prompt]: 	Training 100/553. train loss: 0.6468,	0.9440 s / batch. (data: 2.45e-04). ETA=5:20:21, max mem: 24.8 GB 
[11/21 03:07:53 visual_prompt]: 	Training 200/553. train loss: 0.7485,	0.9313 s / batch. (data: 7.36e-04). ETA=5:14:29, max mem: 24.8 GB 
[11/21 03:09:27 visual_prompt]: 	Training 300/553. train loss: 0.7673,	0.9280 s / batch. (data: 4.07e-04). ETA=5:11:50, max mem: 24.8 GB 
[11/21 03:11:03 visual_prompt]: 	Training 400/553. train loss: 0.6975,	0.9320 s / batch. (data: 2.68e-04). ETA=5:11:37, max mem: 24.8 GB 
[11/21 03:12:38 visual_prompt]: 	Training 500/553. train loss: 0.7260,	0.9347 s / batch. (data: 1.04e-02). ETA=5:10:57, max mem: 24.8 GB 
[11/21 03:13:28 visual_prompt]: Epoch 64 / 100: avg data time: 4.32e-02, avg batch time: 0.9727, average train loss: 0.6893
[11/21 03:14:26 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3041, average loss: 0.6888
[11/21 03:14:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.54	
[11/21 03:14:26 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.0015720068777044477
[11/21 03:16:14 visual_prompt]: 	Training 100/553. train loss: 0.6433,	0.9222 s / batch. (data: 2.70e-04). ETA=5:04:26, max mem: 24.8 GB 
[11/21 03:17:50 visual_prompt]: 	Training 200/553. train loss: 0.6042,	0.9160 s / batch. (data: 2.84e-04). ETA=5:00:52, max mem: 24.8 GB 
[11/21 03:19:24 visual_prompt]: 	Training 300/553. train loss: 0.7421,	0.9655 s / batch. (data: 5.38e-03). ETA=5:15:31, max mem: 24.8 GB 
[11/21 03:20:59 visual_prompt]: 	Training 400/553. train loss: 0.7084,	0.9266 s / batch. (data: 1.04e-02). ETA=5:01:16, max mem: 24.8 GB 
[11/21 03:22:34 visual_prompt]: 	Training 500/553. train loss: 0.7022,	0.9371 s / batch. (data: 2.56e-02). ETA=5:03:07, max mem: 24.8 GB 
[11/21 03:23:28 visual_prompt]: Epoch 65 / 100: avg data time: 5.11e-02, avg batch time: 0.9805, average train loss: 0.6900
[11/21 03:24:26 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3047, average loss: 0.6886
[11/21 03:24:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.32	
[11/21 03:24:26 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.0014957614383675768
[11/21 03:26:13 visual_prompt]: 	Training 100/553. train loss: 0.6966,	0.9385 s / batch. (data: 2.93e-04). ETA=5:01:09, max mem: 24.8 GB 
[11/21 03:27:47 visual_prompt]: 	Training 200/553. train loss: 0.7553,	0.9338 s / batch. (data: 2.77e-04). ETA=4:58:06, max mem: 24.8 GB 
[11/21 03:29:23 visual_prompt]: 	Training 300/553. train loss: 0.6421,	0.9320 s / batch. (data: 2.87e-04). ETA=4:55:58, max mem: 24.8 GB 
[11/21 03:31:01 visual_prompt]: 	Training 400/553. train loss: 0.7026,	0.9534 s / batch. (data: 7.97e-03). ETA=5:01:10, max mem: 24.8 GB 
[11/21 03:32:36 visual_prompt]: 	Training 500/553. train loss: 0.6987,	0.9359 s / batch. (data: 2.83e-04). ETA=4:54:05, max mem: 24.8 GB 
[11/21 03:33:26 visual_prompt]: Epoch 66 / 100: avg data time: 4.59e-02, avg batch time: 0.9762, average train loss: 0.6897
[11/21 03:34:23 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3035, average loss: 0.6884
[11/21 03:34:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.18	
[11/21 03:34:23 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.001420614119153768
[11/21 03:36:10 visual_prompt]: 	Training 100/553. train loss: 0.6972,	0.9399 s / batch. (data: 7.72e-04). ETA=4:52:58, max mem: 24.8 GB 
[11/21 03:37:47 visual_prompt]: 	Training 200/553. train loss: 0.7553,	0.9400 s / batch. (data: 7.15e-04). ETA=4:51:25, max mem: 24.8 GB 
[11/21 03:39:25 visual_prompt]: 	Training 300/553. train loss: 0.7405,	0.9320 s / batch. (data: 2.48e-04). ETA=4:47:24, max mem: 24.8 GB 
[11/21 03:41:02 visual_prompt]: 	Training 400/553. train loss: 0.6661,	0.9317 s / batch. (data: 2.47e-04). ETA=4:45:45, max mem: 24.8 GB 
[11/21 03:42:36 visual_prompt]: 	Training 500/553. train loss: 0.8157,	0.9315 s / batch. (data: 3.05e-04). ETA=4:44:09, max mem: 24.8 GB 
[11/21 03:43:26 visual_prompt]: Epoch 67 / 100: avg data time: 5.18e-02, avg batch time: 0.9810, average train loss: 0.6900
[11/21 03:44:24 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.3027, average loss: 0.6930
[11/21 03:44:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/21 03:44:24 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.0013466470925532809
[11/21 03:46:10 visual_prompt]: 	Training 100/553. train loss: 0.6478,	0.9240 s / batch. (data: 7.17e-03). ETA=4:39:29, max mem: 24.8 GB 
[11/21 03:47:44 visual_prompt]: 	Training 200/553. train loss: 0.6961,	0.9240 s / batch. (data: 2.83e-04). ETA=4:37:57, max mem: 24.8 GB 
[11/21 03:49:20 visual_prompt]: 	Training 300/553. train loss: 0.6518,	0.9469 s / batch. (data: 3.62e-02). ETA=4:43:16, max mem: 24.8 GB 
[11/21 03:50:55 visual_prompt]: 	Training 400/553. train loss: 0.6504,	0.9203 s / batch. (data: 2.51e-04). ETA=4:33:46, max mem: 24.8 GB 
[11/21 03:52:32 visual_prompt]: 	Training 500/553. train loss: 0.7495,	2.1597 s / batch. (data: 1.23e+00). ETA=10:38:53, max mem: 24.8 GB 
[11/21 03:53:25 visual_prompt]: Epoch 68 / 100: avg data time: 4.81e-02, avg batch time: 0.9784, average train loss: 0.6896
[11/21 03:54:23 visual_prompt]: Inference (val):avg data time: 8.89e-05, avg batch time: 0.3038, average loss: 0.6887
[11/21 03:54:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.62	
[11/21 03:54:23 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.0012739412404237304
[11/21 03:56:09 visual_prompt]: 	Training 100/553. train loss: 0.6984,	2.1676 s / batch. (data: 1.25e+00). ETA=10:35:40, max mem: 24.8 GB 
[11/21 03:57:45 visual_prompt]: 	Training 200/553. train loss: 0.7963,	0.9226 s / batch. (data: 5.47e-03). ETA=4:29:01, max mem: 24.8 GB 
[11/21 03:59:20 visual_prompt]: 	Training 300/553. train loss: 0.7586,	0.9248 s / batch. (data: 3.14e-04). ETA=4:28:07, max mem: 24.8 GB 
[11/21 04:00:58 visual_prompt]: 	Training 400/553. train loss: 0.6964,	0.9394 s / batch. (data: 2.90e-04). ETA=4:30:47, max mem: 24.8 GB 
[11/21 04:02:35 visual_prompt]: 	Training 500/553. train loss: 0.6958,	0.9314 s / batch. (data: 1.70e-02). ETA=4:26:56, max mem: 24.8 GB 
[11/21 04:03:24 visual_prompt]: Epoch 69 / 100: avg data time: 4.99e-02, avg batch time: 0.9786, average train loss: 0.6893
[11/21 04:04:22 visual_prompt]: Inference (val):avg data time: 8.27e-05, avg batch time: 0.3036, average loss: 0.6886
[11/21 04:04:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.51	
[11/21 04:04:22 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.0012025760655469627
[11/21 04:06:09 visual_prompt]: 	Training 100/553. train loss: 0.7059,	0.9212 s / batch. (data: 2.60e-04). ETA=4:21:39, max mem: 24.8 GB 
[11/21 04:07:42 visual_prompt]: 	Training 200/553. train loss: 0.6962,	0.9648 s / batch. (data: 1.05e-02). ETA=4:32:26, max mem: 24.8 GB 
[11/21 04:09:16 visual_prompt]: 	Training 300/553. train loss: 0.6430,	0.9471 s / batch. (data: 2.35e-02). ETA=4:25:52, max mem: 24.8 GB 
[11/21 04:10:54 visual_prompt]: 	Training 400/553. train loss: 0.6413,	0.9253 s / batch. (data: 2.56e-04). ETA=4:18:12, max mem: 24.8 GB 
[11/21 04:12:30 visual_prompt]: 	Training 500/553. train loss: 0.7518,	0.9440 s / batch. (data: 2.62e-04). ETA=4:21:51, max mem: 24.8 GB 
[11/21 04:13:22 visual_prompt]: Epoch 70 / 100: avg data time: 4.64e-02, avg batch time: 0.9762, average train loss: 0.6902
[11/21 04:14:20 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3035, average loss: 0.6887
[11/21 04:14:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.84	
[11/21 04:14:20 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.0011326296046939332
[11/21 04:16:06 visual_prompt]: 	Training 100/553. train loss: 0.6962,	0.9454 s / batch. (data: 1.09e-02). ETA=4:19:49, max mem: 24.8 GB 
[11/21 04:17:41 visual_prompt]: 	Training 200/553. train loss: 0.6264,	0.9444 s / batch. (data: 2.91e-04). ETA=4:17:58, max mem: 24.8 GB 
[11/21 04:19:15 visual_prompt]: 	Training 300/553. train loss: 0.7521,	0.9161 s / batch. (data: 2.58e-04). ETA=4:08:42, max mem: 24.8 GB 
[11/21 04:20:51 visual_prompt]: 	Training 400/553. train loss: 0.7354,	0.9360 s / batch. (data: 2.44e-04). ETA=4:12:33, max mem: 24.8 GB 
[11/21 04:22:27 visual_prompt]: 	Training 500/553. train loss: 0.7574,	0.9517 s / batch. (data: 2.63e-02). ETA=4:15:12, max mem: 24.8 GB 
[11/21 04:23:17 visual_prompt]: Epoch 71 / 100: avg data time: 4.35e-02, avg batch time: 0.9716, average train loss: 0.6888
[11/21 04:24:15 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3030, average loss: 0.6886
[11/21 04:24:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.40	
[11/21 04:24:15 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.001064178343292641
[11/21 04:26:01 visual_prompt]: 	Training 100/553. train loss: 0.6000,	0.8992 s / batch. (data: 2.67e-04). ETA=3:58:51, max mem: 24.8 GB 
[11/21 04:27:38 visual_prompt]: 	Training 200/553. train loss: 0.6467,	0.9630 s / batch. (data: 5.81e-03). ETA=4:14:10, max mem: 24.8 GB 
[11/21 04:29:16 visual_prompt]: 	Training 300/553. train loss: 0.6973,	0.9445 s / batch. (data: 5.35e-03). ETA=4:07:43, max mem: 24.8 GB 
[11/21 04:30:51 visual_prompt]: 	Training 400/553. train loss: 0.5473,	0.9137 s / batch. (data: 2.65e-04). ETA=3:58:08, max mem: 24.8 GB 
[11/21 04:32:25 visual_prompt]: 	Training 500/553. train loss: 0.5784,	0.9560 s / batch. (data: 2.92e-04). ETA=4:07:33, max mem: 24.8 GB 
[11/21 04:33:15 visual_prompt]: Epoch 72 / 100: avg data time: 4.71e-02, avg batch time: 0.9765, average train loss: 0.6911
[11/21 04:34:13 visual_prompt]: Inference (val):avg data time: 5.27e-04, avg batch time: 0.3040, average loss: 0.6885
[11/21 04:34:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.93	
[11/21 04:34:13 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.0009972971317924373
[11/21 04:36:00 visual_prompt]: 	Training 100/553. train loss: 0.6097,	0.9320 s / batch. (data: 7.98e-03). ETA=3:58:57, max mem: 24.8 GB 
[11/21 04:37:34 visual_prompt]: 	Training 200/553. train loss: 0.6442,	0.9199 s / batch. (data: 2.65e-04). ETA=3:54:19, max mem: 24.8 GB 
[11/21 04:39:11 visual_prompt]: 	Training 300/553. train loss: 0.6504,	0.9360 s / batch. (data: 2.77e-04). ETA=3:56:51, max mem: 24.8 GB 
[11/21 04:40:44 visual_prompt]: 	Training 400/553. train loss: 0.6493,	0.9417 s / batch. (data: 2.78e-04). ETA=3:56:44, max mem: 24.8 GB 
[11/21 04:42:20 visual_prompt]: 	Training 500/553. train loss: 0.6987,	0.9352 s / batch. (data: 5.37e-03). ETA=3:53:33, max mem: 24.8 GB 
[11/21 04:43:10 visual_prompt]: Epoch 73 / 100: avg data time: 4.38e-02, avg batch time: 0.9718, average train loss: 0.6890
[11/21 04:44:08 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3040, average loss: 0.6884
[11/21 04:44:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.17	
[11/21 04:44:08 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.0009320591038161574
[11/21 04:45:53 visual_prompt]: 	Training 100/553. train loss: 0.6459,	0.9356 s / batch. (data: 2.83e-04). ETA=3:51:15, max mem: 24.8 GB 
[11/21 04:47:31 visual_prompt]: 	Training 200/553. train loss: 0.6989,	0.9750 s / batch. (data: 1.11e-03). ETA=3:59:22, max mem: 24.8 GB 
[11/21 04:49:07 visual_prompt]: 	Training 300/553. train loss: 0.6712,	0.9640 s / batch. (data: 2.83e-04). ETA=3:55:04, max mem: 24.8 GB 
[11/21 04:50:42 visual_prompt]: 	Training 400/553. train loss: 0.7372,	0.9335 s / batch. (data: 1.55e-02). ETA=3:46:05, max mem: 24.8 GB 
[11/21 04:52:18 visual_prompt]: 	Training 500/553. train loss: 0.6975,	0.9280 s / batch. (data: 2.78e-04). ETA=3:43:11, max mem: 24.8 GB 
[11/21 04:53:07 visual_prompt]: Epoch 74 / 100: avg data time: 4.59e-02, avg batch time: 0.9755, average train loss: 0.6890
[11/21 04:54:05 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.3047, average loss: 0.6892
[11/21 04:54:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.38	
[11/21 04:54:05 visual_prompt]: Stopping early.
[11/21 04:54:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 04:54:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 04:54:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/21 04:54:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 04:54:05 visual_prompt]: Training with config:
[11/21 04:54:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.005_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 04:54:05 visual_prompt]: Loading training data...
[11/21 04:54:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 04:54:05 visual_prompt]: Loading validation data...
[11/21 04:54:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 04:54:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 04:54:07 visual_prompt]: Enable all parameters update during training
[11/21 04:54:07 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 04:54:07 visual_prompt]: tuned percent:100.000
[11/21 04:54:07 visual_prompt]: Device used for model: 0
[11/21 04:54:07 visual_prompt]: Setting up Evaluator...
[11/21 04:54:07 visual_prompt]: Setting up Trainer...
[11/21 04:54:07 visual_prompt]: 	Setting up the optimizer...
[11/21 04:54:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 04:55:51 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9440 s / batch. (data: 3.02e-04). ETA=14:28:29, max mem: 26.1 GB 
[11/21 04:57:30 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9645 s / batch. (data: 2.45e-02). ETA=14:45:46, max mem: 26.1 GB 
[11/21 04:59:06 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9385 s / batch. (data: 5.38e-03). ETA=14:20:16, max mem: 26.1 GB 
[11/21 05:00:40 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9400 s / batch. (data: 5.41e-03). ETA=14:20:05, max mem: 26.1 GB 
[11/21 05:02:16 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9302 s / batch. (data: 1.55e-02). ETA=14:09:36, max mem: 26.1 GB 
[11/21 05:03:06 visual_prompt]: Epoch 1 / 100: avg data time: 4.21e-02, avg batch time: 0.9748, average train loss: 7.6130
[11/21 05:04:04 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3059, average loss: 6.9126
[11/21 05:04:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 05:04:04 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/21 05:05:50 visual_prompt]: 	Training 100/553. train loss: 0.9501,	0.9235 s / batch. (data: 2.79e-04). ETA=14:01:04, max mem: 26.1 GB 
[11/21 05:07:24 visual_prompt]: 	Training 200/553. train loss: 0.6622,	1.3880 s / batch. (data: 4.57e-01). ETA=21:01:49, max mem: 26.1 GB 
[11/21 05:09:01 visual_prompt]: 	Training 300/553. train loss: 0.8984,	0.9160 s / batch. (data: 2.53e-04). ETA=13:51:14, max mem: 26.1 GB 
[11/21 05:10:38 visual_prompt]: 	Training 400/553. train loss: 0.6841,	0.9182 s / batch. (data: 2.63e-04). ETA=13:51:40, max mem: 26.1 GB 
[11/21 05:12:15 visual_prompt]: 	Training 500/553. train loss: 1.2699,	0.9210 s / batch. (data: 4.14e-03). ETA=13:52:40, max mem: 26.1 GB 
[11/21 05:13:04 visual_prompt]: Epoch 2 / 100: avg data time: 4.48e-02, avg batch time: 0.9774, average train loss: 1.2961
[11/21 05:14:02 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3026, average loss: 0.8350
[11/21 05:14:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.97	
[11/21 05:14:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/21 05:15:48 visual_prompt]: 	Training 100/553. train loss: 0.5967,	0.9491 s / batch. (data: 7.16e-04). ETA=14:15:38, max mem: 26.1 GB 
[11/21 05:17:25 visual_prompt]: 	Training 200/553. train loss: 3.0689,	0.9226 s / batch. (data: 2.72e-04). ETA=13:50:13, max mem: 26.1 GB 
[11/21 05:19:01 visual_prompt]: 	Training 300/553. train loss: 0.6372,	0.9549 s / batch. (data: 1.09e-02). ETA=14:17:42, max mem: 26.1 GB 
[11/21 05:20:35 visual_prompt]: 	Training 400/553. train loss: 0.3423,	0.9832 s / batch. (data: 7.16e-03). ETA=14:41:30, max mem: 26.1 GB 
[11/21 05:22:09 visual_prompt]: 	Training 500/553. train loss: 1.9293,	0.9360 s / batch. (data: 2.62e-04). ETA=13:57:37, max mem: 26.1 GB 
[11/21 05:22:59 visual_prompt]: Epoch 3 / 100: avg data time: 4.06e-02, avg batch time: 0.9705, average train loss: 0.8801
[11/21 05:23:57 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3063, average loss: 1.0978
[11/21 05:23:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.51	
[11/21 05:23:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/21 05:25:43 visual_prompt]: 	Training 100/553. train loss: 1.2474,	0.9295 s / batch. (data: 1.36e-02). ETA=13:49:28, max mem: 26.1 GB 
[11/21 05:27:18 visual_prompt]: 	Training 200/553. train loss: 0.9357,	1.1480 s / batch. (data: 2.24e-01). ETA=17:02:30, max mem: 26.1 GB 
[11/21 05:28:54 visual_prompt]: 	Training 300/553. train loss: 1.3655,	0.9400 s / batch. (data: 3.95e-03). ETA=13:55:38, max mem: 26.1 GB 
[11/21 05:30:28 visual_prompt]: 	Training 400/553. train loss: 0.5637,	0.9479 s / batch. (data: 4.35e-04). ETA=14:01:09, max mem: 26.1 GB 
[11/21 05:32:05 visual_prompt]: 	Training 500/553. train loss: 0.6613,	0.9360 s / batch. (data: 2.84e-04). ETA=13:48:59, max mem: 26.1 GB 
[11/21 05:32:56 visual_prompt]: Epoch 4 / 100: avg data time: 4.29e-02, avg batch time: 0.9745, average train loss: 0.8757
[11/21 05:33:54 visual_prompt]: Inference (val):avg data time: 5.11e-04, avg batch time: 0.3021, average loss: 0.6924
[11/21 05:33:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.85	
[11/21 05:33:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/21 05:35:39 visual_prompt]: 	Training 100/553. train loss: 0.5634,	0.9320 s / batch. (data: 2.44e-04). ETA=13:43:04, max mem: 26.1 GB 
[11/21 05:37:16 visual_prompt]: 	Training 200/553. train loss: 0.5963,	0.9338 s / batch. (data: 2.71e-04). ETA=13:43:04, max mem: 26.1 GB 
[11/21 05:38:50 visual_prompt]: 	Training 300/553. train loss: 1.4508,	0.9803 s / batch. (data: 1.05e-02). ETA=14:22:27, max mem: 26.1 GB 
[11/21 05:40:27 visual_prompt]: 	Training 400/553. train loss: 0.7032,	2.9813 s / batch. (data: 2.08e+00). ETA=1 day, 19:37:58, max mem: 26.1 GB 
[11/21 05:42:05 visual_prompt]: 	Training 500/553. train loss: 0.6028,	0.9400 s / batch. (data: 4.01e-03). ETA=13:43:53, max mem: 26.1 GB 
[11/21 05:42:54 visual_prompt]: Epoch 5 / 100: avg data time: 4.49e-02, avg batch time: 0.9763, average train loss: 0.8406
[11/21 05:43:52 visual_prompt]: Inference (val):avg data time: 8.24e-05, avg batch time: 0.3038, average loss: 0.7385
[11/21 05:43:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.72	
[11/21 05:43:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/21 05:45:40 visual_prompt]: 	Training 100/553. train loss: 0.7379,	0.9378 s / batch. (data: 2.85e-04). ETA=13:39:33, max mem: 26.1 GB 
[11/21 05:47:15 visual_prompt]: 	Training 200/553. train loss: 0.9842,	0.9376 s / batch. (data: 2.65e-04). ETA=13:37:48, max mem: 26.1 GB 
[11/21 05:48:51 visual_prompt]: 	Training 300/553. train loss: 0.8380,	0.9412 s / batch. (data: 1.05e-02). ETA=13:39:25, max mem: 26.1 GB 
[11/21 05:50:26 visual_prompt]: 	Training 400/553. train loss: 0.7362,	1.2144 s / batch. (data: 2.97e-01). ETA=17:35:10, max mem: 26.1 GB 
[11/21 05:52:05 visual_prompt]: 	Training 500/553. train loss: 1.0814,	0.9339 s / batch. (data: 5.36e-03). ETA=13:29:55, max mem: 26.1 GB 
[11/21 05:52:55 visual_prompt]: Epoch 6 / 100: avg data time: 4.90e-02, avg batch time: 0.9807, average train loss: 0.8601
[11/21 05:53:53 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3064, average loss: 0.6918
[11/21 05:53:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 56.44	
[11/21 05:53:53 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/21 05:55:43 visual_prompt]: 	Training 100/553. train loss: 0.8776,	0.9342 s / batch. (data: 2.72e-04). ETA=13:27:49, max mem: 26.1 GB 
[11/21 05:57:18 visual_prompt]: 	Training 200/553. train loss: 0.6498,	0.9485 s / batch. (data: 2.19e-02). ETA=13:38:35, max mem: 26.1 GB 
[11/21 05:58:52 visual_prompt]: 	Training 300/553. train loss: 0.7785,	0.9236 s / batch. (data: 5.36e-03). ETA=13:15:34, max mem: 26.1 GB 
[11/21 06:00:26 visual_prompt]: 	Training 400/553. train loss: 0.6673,	0.9620 s / batch. (data: 2.76e-04). ETA=13:47:01, max mem: 26.1 GB 
[11/21 06:02:01 visual_prompt]: 	Training 500/553. train loss: 0.6658,	0.9450 s / batch. (data: 3.75e-03). ETA=13:30:48, max mem: 26.1 GB 
[11/21 06:02:51 visual_prompt]: Epoch 7 / 100: avg data time: 4.15e-02, avg batch time: 0.9730, average train loss: 0.8220
[11/21 06:03:49 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3055, average loss: 0.6851
[11/21 06:03:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 55.84	
[11/21 06:03:49 visual_prompt]: Best epoch 7: best metric: -0.685
[11/21 06:03:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/21 06:05:36 visual_prompt]: 	Training 100/553. train loss: 0.6027,	0.9200 s / batch. (data: 2.50e-04). ETA=13:07:01, max mem: 26.1 GB 
[11/21 06:07:13 visual_prompt]: 	Training 200/553. train loss: 0.8529,	0.9362 s / batch. (data: 2.72e-04). ETA=13:19:21, max mem: 26.1 GB 
[11/21 06:08:49 visual_prompt]: 	Training 300/553. train loss: 0.7427,	0.9314 s / batch. (data: 7.42e-03). ETA=13:13:42, max mem: 26.1 GB 
[11/21 06:10:25 visual_prompt]: 	Training 400/553. train loss: 0.6873,	0.9360 s / batch. (data: 2.60e-04). ETA=13:16:03, max mem: 26.1 GB 
[11/21 06:12:02 visual_prompt]: 	Training 500/553. train loss: 0.6013,	0.9621 s / batch. (data: 3.01e-02). ETA=13:36:37, max mem: 26.1 GB 
[11/21 06:12:52 visual_prompt]: Epoch 8 / 100: avg data time: 5.04e-02, avg batch time: 0.9816, average train loss: 0.7892
[11/21 06:13:49 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3056, average loss: 0.7060
[11/21 06:13:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.90	
[11/21 06:13:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/21 06:15:39 visual_prompt]: 	Training 100/553. train loss: 0.7687,	0.9429 s / batch. (data: 2.78e-04). ETA=13:17:55, max mem: 26.1 GB 
[11/21 06:17:15 visual_prompt]: 	Training 200/553. train loss: 1.3077,	0.9592 s / batch. (data: 7.17e-03). ETA=13:30:07, max mem: 26.1 GB 
[11/21 06:18:50 visual_prompt]: 	Training 300/553. train loss: 0.6617,	0.9282 s / batch. (data: 2.74e-04). ETA=13:02:23, max mem: 26.1 GB 
[11/21 06:20:24 visual_prompt]: 	Training 400/553. train loss: 0.6873,	0.9386 s / batch. (data: 7.63e-04). ETA=13:09:35, max mem: 26.1 GB 
[11/21 06:21:58 visual_prompt]: 	Training 500/553. train loss: 0.7575,	0.9338 s / batch. (data: 5.70e-03). ETA=13:03:58, max mem: 26.1 GB 
[11/21 06:22:50 visual_prompt]: Epoch 9 / 100: avg data time: 4.58e-02, avg batch time: 0.9775, average train loss: 0.7718
[11/21 06:23:48 visual_prompt]: Inference (val):avg data time: 2.51e-04, avg batch time: 0.3039, average loss: 0.7000
[11/21 06:23:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.72	
[11/21 06:23:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/21 06:25:34 visual_prompt]: 	Training 100/553. train loss: 0.9542,	0.9262 s / batch. (data: 5.37e-03). ETA=12:55:14, max mem: 26.1 GB 
[11/21 06:27:12 visual_prompt]: 	Training 200/553. train loss: 0.6991,	0.9217 s / batch. (data: 5.68e-03). ETA=12:50:00, max mem: 26.1 GB 
[11/21 06:28:46 visual_prompt]: 	Training 300/553. train loss: 0.7796,	0.9302 s / batch. (data: 5.80e-03). ETA=12:55:31, max mem: 26.1 GB 
[11/21 06:30:21 visual_prompt]: 	Training 400/553. train loss: 0.6351,	0.9504 s / batch. (data: 1.26e-02). ETA=13:10:45, max mem: 26.1 GB 
[11/21 06:31:57 visual_prompt]: 	Training 500/553. train loss: 0.7146,	0.9474 s / batch. (data: 5.41e-03). ETA=13:06:43, max mem: 26.1 GB 
[11/21 06:32:48 visual_prompt]: Epoch 10 / 100: avg data time: 4.54e-02, avg batch time: 0.9773, average train loss: 0.7250
[11/21 06:33:46 visual_prompt]: Inference (val):avg data time: 7.85e-05, avg batch time: 0.3054, average loss: 0.7094
[11/21 06:33:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.52	
[11/21 06:33:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/21 06:35:35 visual_prompt]: 	Training 100/553. train loss: 0.7386,	0.9160 s / batch. (data: 2.59e-04). ETA=12:38:18, max mem: 26.1 GB 
[11/21 06:37:11 visual_prompt]: 	Training 200/553. train loss: 0.6957,	0.9280 s / batch. (data: 5.39e-03). ETA=12:46:41, max mem: 26.1 GB 
[11/21 06:38:45 visual_prompt]: 	Training 300/553. train loss: 0.6119,	0.9404 s / batch. (data: 5.83e-03). ETA=12:55:22, max mem: 26.1 GB 
[11/21 06:40:20 visual_prompt]: 	Training 400/553. train loss: 0.8948,	0.9537 s / batch. (data: 5.37e-03). ETA=13:04:42, max mem: 26.1 GB 
[11/21 06:41:57 visual_prompt]: 	Training 500/553. train loss: 0.8002,	0.9307 s / batch. (data: 2.85e-04). ETA=12:44:15, max mem: 26.1 GB 
[11/21 06:42:47 visual_prompt]: Epoch 11 / 100: avg data time: 4.60e-02, avg batch time: 0.9774, average train loss: 0.7268
[11/21 06:43:44 visual_prompt]: Inference (val):avg data time: 2.65e-04, avg batch time: 0.3043, average loss: 0.7185
[11/21 06:43:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.09	
[11/21 06:43:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/21 06:45:27 visual_prompt]: 	Training 100/553. train loss: 0.5634,	0.9297 s / batch. (data: 2.69e-04). ETA=12:41:03, max mem: 26.1 GB 
[11/21 06:47:09 visual_prompt]: 	Training 200/553. train loss: 0.7285,	0.9263 s / batch. (data: 4.93e-03). ETA=12:36:43, max mem: 26.1 GB 
[11/21 06:48:44 visual_prompt]: 	Training 300/553. train loss: 0.7004,	0.9213 s / batch. (data: 6.46e-03). ETA=12:31:09, max mem: 26.1 GB 
[11/21 06:50:21 visual_prompt]: 	Training 400/553. train loss: 0.8326,	0.9640 s / batch. (data: 2.68e-04). ETA=13:04:20, max mem: 26.1 GB 
[11/21 06:51:57 visual_prompt]: 	Training 500/553. train loss: 0.7340,	0.9425 s / batch. (data: 5.81e-03). ETA=12:45:14, max mem: 26.1 GB 
[11/21 06:52:48 visual_prompt]: Epoch 12 / 100: avg data time: 5.14e-02, avg batch time: 0.9826, average train loss: 0.7105
[11/21 06:53:45 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3034, average loss: 0.7421
[11/21 06:53:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.22	
[11/21 06:53:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/21 06:55:33 visual_prompt]: 	Training 100/553. train loss: 0.6919,	0.9416 s / batch. (data: 1.36e-02). ETA=12:42:07, max mem: 26.1 GB 
[11/21 06:57:08 visual_prompt]: 	Training 200/553. train loss: 0.6094,	0.9534 s / batch. (data: 2.53e-04). ETA=12:50:07, max mem: 26.1 GB 
[11/21 06:58:45 visual_prompt]: 	Training 300/553. train loss: 0.5717,	0.9415 s / batch. (data: 1.35e-02). ETA=12:38:53, max mem: 26.1 GB 
[11/21 07:00:23 visual_prompt]: 	Training 400/553. train loss: 0.6960,	0.9475 s / batch. (data: 2.72e-04). ETA=12:42:12, max mem: 26.1 GB 
[11/21 07:01:58 visual_prompt]: 	Training 500/553. train loss: 0.6232,	0.9280 s / batch. (data: 2.59e-04). ETA=12:24:56, max mem: 26.1 GB 
[11/21 07:02:48 visual_prompt]: Epoch 13 / 100: avg data time: 5.00e-02, avg batch time: 0.9801, average train loss: 0.7025
[11/21 07:03:45 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3053, average loss: 0.6882
[11/21 07:03:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.72	
[11/21 07:03:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/21 07:05:33 visual_prompt]: 	Training 100/553. train loss: 0.6922,	0.9456 s / batch. (data: 5.37e-03). ETA=12:36:41, max mem: 26.1 GB 
[11/21 07:07:12 visual_prompt]: 	Training 200/553. train loss: 0.6247,	0.9422 s / batch. (data: 5.36e-03). ETA=12:32:22, max mem: 26.1 GB 
[11/21 07:08:47 visual_prompt]: 	Training 300/553. train loss: 0.5828,	0.9393 s / batch. (data: 7.98e-03). ETA=12:28:30, max mem: 26.1 GB 
[11/21 07:10:21 visual_prompt]: 	Training 400/553. train loss: 0.7100,	0.9153 s / batch. (data: 3.07e-04). ETA=12:07:51, max mem: 26.1 GB 
[11/21 07:11:57 visual_prompt]: 	Training 500/553. train loss: 0.7008,	0.9401 s / batch. (data: 2.91e-04). ETA=12:25:57, max mem: 26.1 GB 
[11/21 07:12:46 visual_prompt]: Epoch 14 / 100: avg data time: 4.89e-02, avg batch time: 0.9783, average train loss: 0.6967
[11/21 07:13:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3013, average loss: 0.6907
[11/21 07:13:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.96	
[11/21 07:13:44 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/21 07:15:32 visual_prompt]: 	Training 100/553. train loss: 0.6086,	0.9287 s / batch. (data: 1.04e-02). ETA=12:14:36, max mem: 26.1 GB 
[11/21 07:17:10 visual_prompt]: 	Training 200/553. train loss: 0.6987,	0.9165 s / batch. (data: 6.74e-04). ETA=12:03:23, max mem: 26.1 GB 
[11/21 07:18:44 visual_prompt]: 	Training 300/553. train loss: 0.7582,	0.9319 s / batch. (data: 2.52e-04). ETA=12:13:59, max mem: 26.1 GB 
[11/21 07:20:20 visual_prompt]: 	Training 400/553. train loss: 0.7470,	0.9710 s / batch. (data: 6.86e-03). ETA=12:43:08, max mem: 26.1 GB 
[11/21 07:21:55 visual_prompt]: 	Training 500/553. train loss: 0.8463,	0.9299 s / batch. (data: 3.96e-03). ETA=12:09:18, max mem: 26.1 GB 
[11/21 07:22:44 visual_prompt]: Epoch 15 / 100: avg data time: 4.65e-02, avg batch time: 0.9767, average train loss: 0.6932
[11/21 07:23:42 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3046, average loss: 0.6940
[11/21 07:23:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/21 07:23:42 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/21 07:25:28 visual_prompt]: 	Training 100/553. train loss: 0.6979,	0.9410 s / batch. (data: 3.98e-03). ETA=12:15:36, max mem: 26.1 GB 
[11/21 07:27:02 visual_prompt]: 	Training 200/553. train loss: 0.7349,	0.9117 s / batch. (data: 2.75e-04). ETA=11:51:10, max mem: 26.1 GB 
[11/21 07:28:41 visual_prompt]: 	Training 300/553. train loss: 0.6935,	0.9360 s / batch. (data: 2.58e-04). ETA=12:08:36, max mem: 26.1 GB 
[11/21 07:30:17 visual_prompt]: 	Training 400/553. train loss: 0.7458,	0.9406 s / batch. (data: 5.36e-03). ETA=12:10:36, max mem: 26.1 GB 
[11/21 07:31:51 visual_prompt]: 	Training 500/553. train loss: 0.7374,	0.9615 s / batch. (data: 1.09e-02). ETA=12:25:15, max mem: 26.1 GB 
[11/21 07:32:42 visual_prompt]: Epoch 16 / 100: avg data time: 4.32e-02, avg batch time: 0.9749, average train loss: 0.6914
[11/21 07:33:40 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3035, average loss: 0.6895
[11/21 07:33:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.13	
[11/21 07:33:40 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/21 07:35:23 visual_prompt]: 	Training 100/553. train loss: 0.7041,	0.9105 s / batch. (data: 5.37e-03). ETA=11:43:23, max mem: 26.1 GB 
[11/21 07:37:02 visual_prompt]: 	Training 200/553. train loss: 0.7189,	0.9268 s / batch. (data: 5.39e-03). ETA=11:54:24, max mem: 26.1 GB 
[11/21 07:38:36 visual_prompt]: 	Training 300/553. train loss: 0.6956,	0.9373 s / batch. (data: 5.39e-03). ETA=12:00:59, max mem: 26.1 GB 
[11/21 07:40:13 visual_prompt]: 	Training 400/553. train loss: 0.7072,	0.9354 s / batch. (data: 7.44e-03). ETA=11:57:57, max mem: 26.1 GB 
[11/21 07:41:51 visual_prompt]: 	Training 500/553. train loss: 0.8083,	0.9321 s / batch. (data: 5.40e-03). ETA=11:53:51, max mem: 26.1 GB 
[11/21 07:42:41 visual_prompt]: Epoch 17 / 100: avg data time: 4.84e-02, avg batch time: 0.9783, average train loss: 0.6933
[11/21 07:43:38 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3018, average loss: 0.6884
[11/21 07:43:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.06	
[11/21 07:43:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/21 07:45:28 visual_prompt]: 	Training 100/553. train loss: 0.6976,	0.9350 s / batch. (data: 5.36e-03). ETA=11:53:41, max mem: 26.1 GB 
[11/21 07:47:06 visual_prompt]: 	Training 200/553. train loss: 0.6941,	0.9253 s / batch. (data: 1.05e-02). ETA=11:44:45, max mem: 26.1 GB 
[11/21 07:48:41 visual_prompt]: 	Training 300/553. train loss: 0.7025,	0.9320 s / batch. (data: 3.23e-04). ETA=11:48:16, max mem: 26.1 GB 
[11/21 07:50:15 visual_prompt]: 	Training 400/553. train loss: 0.6688,	0.9463 s / batch. (data: 5.35e-03). ETA=11:57:35, max mem: 26.1 GB 
[11/21 07:51:54 visual_prompt]: 	Training 500/553. train loss: 0.7546,	0.9282 s / batch. (data: 5.38e-03). ETA=11:42:17, max mem: 26.1 GB 
[11/21 07:52:43 visual_prompt]: Epoch 18 / 100: avg data time: 5.58e-02, avg batch time: 0.9849, average train loss: 0.6937
[11/21 07:53:42 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3027, average loss: 0.6894
[11/21 07:53:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/21 07:53:42 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/21 07:55:28 visual_prompt]: 	Training 100/553. train loss: 0.6430,	0.9226 s / batch. (data: 5.38e-03). ETA=11:35:42, max mem: 26.1 GB 
[11/21 07:57:06 visual_prompt]: 	Training 200/553. train loss: 0.6983,	2.1120 s / batch. (data: 1.17e+00). ETA=1 day, 2:29:10, max mem: 26.1 GB 
[11/21 07:58:40 visual_prompt]: 	Training 300/553. train loss: 0.7061,	0.9240 s / batch. (data: 5.38e-03). ETA=11:33:43, max mem: 26.1 GB 
[11/21 08:00:17 visual_prompt]: 	Training 400/553. train loss: 0.6715,	0.9360 s / batch. (data: 2.66e-04). ETA=11:41:09, max mem: 26.1 GB 
[11/21 08:01:54 visual_prompt]: 	Training 500/553. train loss: 0.5961,	0.9320 s / batch. (data: 7.86e-04). ETA=11:36:35, max mem: 26.1 GB 
[11/21 08:02:43 visual_prompt]: Epoch 19 / 100: avg data time: 4.94e-02, avg batch time: 0.9794, average train loss: 0.6911
[11/21 08:03:41 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3047, average loss: 0.6897
[11/21 08:03:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.99	
[11/21 08:03:41 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/21 08:05:32 visual_prompt]: 	Training 100/553. train loss: 0.6934,	0.9348 s / batch. (data: 1.05e-02). ETA=11:36:19, max mem: 26.1 GB 
[11/21 08:07:06 visual_prompt]: 	Training 200/553. train loss: 0.7010,	0.9316 s / batch. (data: 5.55e-03). ETA=11:32:24, max mem: 26.1 GB 
[11/21 08:08:42 visual_prompt]: 	Training 300/553. train loss: 0.7056,	0.9339 s / batch. (data: 5.36e-03). ETA=11:32:30, max mem: 26.1 GB 
[11/21 08:10:18 visual_prompt]: 	Training 400/553. train loss: 0.8665,	0.9512 s / batch. (data: 6.98e-04). ETA=11:43:45, max mem: 26.1 GB 
[11/21 08:11:53 visual_prompt]: 	Training 500/553. train loss: 0.6988,	0.9485 s / batch. (data: 5.37e-03). ETA=11:40:12, max mem: 26.1 GB 
[11/21 08:12:45 visual_prompt]: Epoch 20 / 100: avg data time: 5.56e-02, avg batch time: 0.9838, average train loss: 0.6914
[11/21 08:13:43 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3034, average loss: 0.6952
[11/21 08:13:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.08	
[11/21 08:13:43 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/21 08:15:32 visual_prompt]: 	Training 100/553. train loss: 0.7027,	0.9451 s / batch. (data: 1.04e-02). ETA=11:35:18, max mem: 26.1 GB 
[11/21 08:17:08 visual_prompt]: 	Training 200/553. train loss: 0.6542,	0.9567 s / batch. (data: 5.94e-03). ETA=11:42:12, max mem: 26.1 GB 
[11/21 08:18:45 visual_prompt]: 	Training 300/553. train loss: 0.6934,	0.9285 s / batch. (data: 2.90e-04). ETA=11:19:57, max mem: 26.1 GB 
[11/21 08:20:22 visual_prompt]: 	Training 400/553. train loss: 0.8078,	0.9253 s / batch. (data: 3.01e-04). ETA=11:16:05, max mem: 26.1 GB 
[11/21 08:21:55 visual_prompt]: 	Training 500/553. train loss: 0.7397,	0.9397 s / batch. (data: 5.39e-03). ETA=11:25:03, max mem: 26.1 GB 
[11/21 08:22:45 visual_prompt]: Epoch 21 / 100: avg data time: 5.01e-02, avg batch time: 0.9796, average train loss: 0.6905
[11/21 08:23:43 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3051, average loss: 0.6887
[11/21 08:23:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.32	
[11/21 08:23:43 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/21 08:25:28 visual_prompt]: 	Training 100/553. train loss: 0.6954,	0.9154 s / batch. (data: 8.09e-03). ETA=11:04:59, max mem: 26.1 GB 
[11/21 08:27:07 visual_prompt]: 	Training 200/553. train loss: 0.6980,	0.9520 s / batch. (data: 7.46e-04). ETA=11:29:59, max mem: 26.1 GB 
[11/21 08:28:43 visual_prompt]: 	Training 300/553. train loss: 0.7678,	0.9215 s / batch. (data: 5.43e-03). ETA=11:06:19, max mem: 26.1 GB 
[11/21 08:30:17 visual_prompt]: 	Training 400/553. train loss: 0.8607,	0.9643 s / batch. (data: 5.37e-03). ETA=11:35:43, max mem: 26.1 GB 
[11/21 08:31:52 visual_prompt]: 	Training 500/553. train loss: 0.6948,	0.9302 s / batch. (data: 2.57e-04). ETA=11:09:33, max mem: 26.1 GB 
[11/21 08:32:42 visual_prompt]: Epoch 22 / 100: avg data time: 4.68e-02, avg batch time: 0.9754, average train loss: 0.6953
[11/21 08:33:40 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3037, average loss: 0.6896
[11/21 08:33:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.72	
[11/21 08:33:40 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/21 08:35:29 visual_prompt]: 	Training 100/553. train loss: 0.6980,	0.9582 s / batch. (data: 2.06e-02). ETA=11:27:15, max mem: 26.1 GB 
[11/21 08:37:05 visual_prompt]: 	Training 200/553. train loss: 0.6121,	0.9200 s / batch. (data: 2.79e-04). ETA=10:58:18, max mem: 26.1 GB 
[11/21 08:38:39 visual_prompt]: 	Training 300/553. train loss: 0.6254,	0.9413 s / batch. (data: 5.39e-03). ETA=11:12:01, max mem: 26.1 GB 
[11/21 08:40:15 visual_prompt]: 	Training 400/553. train loss: 0.7018,	0.9412 s / batch. (data: 5.38e-03). ETA=11:10:20, max mem: 26.1 GB 
[11/21 08:41:48 visual_prompt]: 	Training 500/553. train loss: 0.7643,	0.9332 s / batch. (data: 5.41e-03). ETA=11:03:04, max mem: 26.1 GB 
[11/21 08:42:41 visual_prompt]: Epoch 23 / 100: avg data time: 4.78e-02, avg batch time: 0.9773, average train loss: 0.6923
[11/21 08:43:40 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3044, average loss: 0.6900
[11/21 08:43:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/21 08:43:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/21 08:45:27 visual_prompt]: 	Training 100/553. train loss: 0.6969,	0.9421 s / batch. (data: 2.06e-02). ETA=11:06:59, max mem: 26.1 GB 
[11/21 08:47:02 visual_prompt]: 	Training 200/553. train loss: 0.7060,	0.9302 s / batch. (data: 7.99e-03). ETA=10:57:03, max mem: 26.1 GB 
[11/21 08:48:40 visual_prompt]: 	Training 300/553. train loss: 0.7074,	0.9571 s / batch. (data: 2.76e-04). ETA=11:14:28, max mem: 26.1 GB 
[11/21 08:50:15 visual_prompt]: 	Training 400/553. train loss: 0.6981,	0.9494 s / batch. (data: 7.96e-03). ETA=11:07:26, max mem: 26.1 GB 
[11/21 08:51:54 visual_prompt]: 	Training 500/553. train loss: 0.6941,	0.9343 s / batch. (data: 1.60e-02). ETA=10:55:15, max mem: 26.1 GB 
[11/21 08:52:43 visual_prompt]: Epoch 24 / 100: avg data time: 5.32e-02, avg batch time: 0.9823, average train loss: 0.6900
[11/21 08:53:41 visual_prompt]: Inference (val):avg data time: 8.72e-05, avg batch time: 0.3042, average loss: 0.6889
[11/21 08:53:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/21 08:53:41 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/21 08:55:24 visual_prompt]: 	Training 100/553. train loss: 0.6520,	0.9175 s / batch. (data: 5.50e-03). ETA=10:41:10, max mem: 26.1 GB 
[11/21 08:57:05 visual_prompt]: 	Training 200/553. train loss: 0.7539,	0.9330 s / batch. (data: 1.05e-02). ETA=10:50:23, max mem: 26.1 GB 
[11/21 08:58:40 visual_prompt]: 	Training 300/553. train loss: 0.6948,	0.9200 s / batch. (data: 2.52e-04). ETA=10:39:50, max mem: 26.1 GB 
[11/21 09:00:17 visual_prompt]: 	Training 400/553. train loss: 0.7103,	0.9369 s / batch. (data: 2.76e-04). ETA=10:50:01, max mem: 26.1 GB 
[11/21 09:01:53 visual_prompt]: 	Training 500/553. train loss: 0.6422,	0.9923 s / batch. (data: 1.56e-02). ETA=11:26:46, max mem: 26.1 GB 
[11/21 09:02:44 visual_prompt]: Epoch 25 / 100: avg data time: 5.21e-02, avg batch time: 0.9816, average train loss: 0.6928
[11/21 09:03:42 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3031, average loss: 0.6972
[11/21 09:03:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.96	
[11/21 09:03:42 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/21 09:05:30 visual_prompt]: 	Training 100/553. train loss: 0.6137,	0.9138 s / batch. (data: 2.50e-04). ETA=10:30:06, max mem: 26.1 GB 
[11/21 09:07:07 visual_prompt]: 	Training 200/553. train loss: 0.5890,	0.9270 s / batch. (data: 3.98e-03). ETA=10:37:42, max mem: 26.1 GB 
[11/21 09:08:45 visual_prompt]: 	Training 300/553. train loss: 0.7288,	0.9212 s / batch. (data: 2.11e-04). ETA=10:32:10, max mem: 26.1 GB 
[11/21 09:10:19 visual_prompt]: 	Training 400/553. train loss: 0.6967,	0.9280 s / batch. (data: 5.39e-03). ETA=10:35:16, max mem: 26.1 GB 
[11/21 09:11:56 visual_prompt]: 	Training 500/553. train loss: 0.6315,	0.9402 s / batch. (data: 1.09e-02). ETA=10:42:02, max mem: 26.1 GB 
[11/21 09:12:46 visual_prompt]: Epoch 26 / 100: avg data time: 5.30e-02, avg batch time: 0.9823, average train loss: 0.6936
[11/21 09:13:44 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3046, average loss: 0.6899
[11/21 09:13:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.64	
[11/21 09:13:44 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/21 09:15:28 visual_prompt]: 	Training 100/553. train loss: 0.5731,	0.9296 s / batch. (data: 7.95e-03). ETA=10:32:28, max mem: 26.1 GB 
[11/21 09:17:06 visual_prompt]: 	Training 200/553. train loss: 0.6995,	0.9310 s / batch. (data: 2.11e-02). ETA=10:31:53, max mem: 26.1 GB 
[11/21 09:18:39 visual_prompt]: 	Training 300/553. train loss: 0.7408,	0.9425 s / batch. (data: 2.89e-04). ETA=10:38:07, max mem: 26.1 GB 
[11/21 09:20:14 visual_prompt]: 	Training 400/553. train loss: 0.8054,	0.9502 s / batch. (data: 7.17e-04). ETA=10:41:45, max mem: 26.1 GB 
[11/21 09:21:52 visual_prompt]: 	Training 500/553. train loss: 0.7227,	0.9455 s / batch. (data: 5.42e-03). ETA=10:36:58, max mem: 26.1 GB 
[11/21 09:22:42 visual_prompt]: Epoch 27 / 100: avg data time: 4.70e-02, avg batch time: 0.9737, average train loss: 0.7037
[11/21 09:23:40 visual_prompt]: Inference (val):avg data time: 8.50e-05, avg batch time: 0.3033, average loss: 0.6897
[11/21 09:23:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.00	
[11/21 09:23:40 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/21 09:25:25 visual_prompt]: 	Training 100/553. train loss: 0.6876,	0.9432 s / batch. (data: 7.96e-03). ETA=10:33:03, max mem: 26.1 GB 
[11/21 09:27:02 visual_prompt]: 	Training 200/553. train loss: 0.6511,	0.9309 s / batch. (data: 2.72e-04). ETA=10:23:14, max mem: 26.1 GB 
[11/21 09:28:37 visual_prompt]: 	Training 300/553. train loss: 0.7443,	0.9479 s / batch. (data: 2.66e-02). ETA=10:33:00, max mem: 26.1 GB 
[11/21 09:30:15 visual_prompt]: 	Training 400/553. train loss: 0.6950,	1.1852 s / batch. (data: 2.44e-01). ETA=13:09:29, max mem: 26.1 GB 
[11/21 09:31:51 visual_prompt]: 	Training 500/553. train loss: 0.7581,	0.9214 s / batch. (data: 2.50e-04). ETA=10:12:14, max mem: 26.1 GB 
[11/21 09:32:41 visual_prompt]: Epoch 28 / 100: avg data time: 4.88e-02, avg batch time: 0.9769, average train loss: 0.6925
[11/21 09:33:39 visual_prompt]: Inference (val):avg data time: 1.74e-04, avg batch time: 0.3037, average loss: 0.6905
[11/21 09:33:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.02	
[11/21 09:33:39 visual_prompt]: Stopping early.
[11/21 09:33:39 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 09:33:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 09:33:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/21 09:33:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 09:33:39 visual_prompt]: Training with config:
[11/21 09:33:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.005_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 09:33:39 visual_prompt]: Loading training data...
[11/21 09:33:39 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 09:33:39 visual_prompt]: Loading validation data...
[11/21 09:33:39 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 09:33:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 09:33:40 visual_prompt]: Enable all parameters update during training
[11/21 09:33:40 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 09:33:40 visual_prompt]: tuned percent:100.000
[11/21 09:33:40 visual_prompt]: Device used for model: 0
[11/21 09:33:40 visual_prompt]: Setting up Evaluator...
[11/21 09:33:40 visual_prompt]: Setting up Trainer...
[11/21 09:33:40 visual_prompt]: 	Setting up the optimizer...
[11/21 09:33:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 09:35:24 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9164 s / batch. (data: 5.43e-03). ETA=14:03:02, max mem: 26.1 GB 
[11/21 09:37:03 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9680 s / batch. (data: 3.06e-04). ETA=14:48:57, max mem: 26.1 GB 
[11/21 09:38:40 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9320 s / batch. (data: 3.11e-04). ETA=14:14:19, max mem: 26.1 GB 
[11/21 09:40:14 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9280 s / batch. (data: 3.05e-04). ETA=14:09:07, max mem: 26.1 GB 
[11/21 09:41:50 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9360 s / batch. (data: 2.85e-04). ETA=14:14:52, max mem: 26.1 GB 
[11/21 09:42:40 visual_prompt]: Epoch 1 / 100: avg data time: 4.75e-02, avg batch time: 0.9752, average train loss: 7.6130
[11/21 09:43:38 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3067, average loss: 6.9126
[11/21 09:43:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 09:43:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/21 09:45:24 visual_prompt]: 	Training 100/553. train loss: 1.4247,	0.9112 s / batch. (data: 7.96e-03). ETA=13:49:55, max mem: 26.1 GB 
[11/21 09:46:58 visual_prompt]: 	Training 200/553. train loss: 1.9070,	1.2160 s / batch. (data: 2.99e-01). ETA=18:25:29, max mem: 26.1 GB 
[11/21 09:48:34 visual_prompt]: 	Training 300/553. train loss: 1.7284,	0.9566 s / batch. (data: 1.56e-02). ETA=14:28:06, max mem: 26.1 GB 
[11/21 09:50:12 visual_prompt]: 	Training 400/553. train loss: 1.5336,	0.9360 s / batch. (data: 2.93e-04). ETA=14:07:48, max mem: 26.1 GB 
[11/21 09:51:49 visual_prompt]: 	Training 500/553. train loss: 0.5991,	0.9243 s / batch. (data: 3.10e-04). ETA=13:55:37, max mem: 26.1 GB 
[11/21 09:52:38 visual_prompt]: Epoch 2 / 100: avg data time: 5.14e-02, avg batch time: 0.9778, average train loss: 2.0394
[11/21 09:53:37 visual_prompt]: Inference (val):avg data time: 1.33e-04, avg batch time: 0.3072, average loss: 1.4962
[11/21 09:53:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.12	
[11/21 09:53:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/21 09:55:22 visual_prompt]: 	Training 100/553. train loss: 0.7896,	0.9154 s / batch. (data: 1.05e-02). ETA=13:45:17, max mem: 26.1 GB 
[11/21 09:57:00 visual_prompt]: 	Training 200/553. train loss: 5.1451,	0.9120 s / batch. (data: 2.78e-04). ETA=13:40:42, max mem: 26.1 GB 
[11/21 09:58:36 visual_prompt]: 	Training 300/553. train loss: 0.3358,	0.9149 s / batch. (data: 2.99e-04). ETA=13:41:45, max mem: 26.1 GB 
[11/21 10:00:13 visual_prompt]: 	Training 400/553. train loss: 0.6944,	0.9474 s / batch. (data: 7.68e-04). ETA=14:09:24, max mem: 26.1 GB 
[11/21 10:01:47 visual_prompt]: 	Training 500/553. train loss: 0.7584,	0.9335 s / batch. (data: 2.56e-02). ETA=13:55:24, max mem: 26.1 GB 
[11/21 10:02:37 visual_prompt]: Epoch 3 / 100: avg data time: 5.11e-02, avg batch time: 0.9770, average train loss: 1.5089
[11/21 10:03:35 visual_prompt]: Inference (val):avg data time: 3.19e-04, avg batch time: 0.3041, average loss: 4.1810
[11/21 10:03:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.28	
[11/21 10:03:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/21 10:05:21 visual_prompt]: 	Training 100/553. train loss: 1.1651,	0.9343 s / batch. (data: 5.48e-03). ETA=13:53:44, max mem: 26.1 GB 
[11/21 10:06:57 visual_prompt]: 	Training 200/553. train loss: 1.3040,	1.3320 s / batch. (data: 4.14e-01). ETA=19:46:22, max mem: 26.1 GB 
[11/21 10:08:32 visual_prompt]: 	Training 300/553. train loss: 2.1400,	0.9180 s / batch. (data: 3.08e-04). ETA=13:36:07, max mem: 26.1 GB 
[11/21 10:10:07 visual_prompt]: 	Training 400/553. train loss: 0.7194,	0.9355 s / batch. (data: 2.91e-04). ETA=13:50:08, max mem: 26.1 GB 
[11/21 10:11:44 visual_prompt]: 	Training 500/553. train loss: 1.0244,	0.9360 s / batch. (data: 3.02e-04). ETA=13:48:58, max mem: 26.1 GB 
[11/21 10:12:35 visual_prompt]: Epoch 4 / 100: avg data time: 5.07e-02, avg batch time: 0.9769, average train loss: 2.1861
[11/21 10:13:33 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3058, average loss: 1.3190
[11/21 10:13:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.08	
[11/21 10:13:33 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/21 10:15:18 visual_prompt]: 	Training 100/553. train loss: 2.9849,	0.9360 s / batch. (data: 2.85e-04). ETA=13:46:36, max mem: 26.1 GB 
[11/21 10:16:55 visual_prompt]: 	Training 200/553. train loss: 2.6551,	0.9480 s / batch. (data: 3.05e-04). ETA=13:55:36, max mem: 26.1 GB 
[11/21 10:18:29 visual_prompt]: 	Training 300/553. train loss: 6.7512,	0.9330 s / batch. (data: 1.05e-02). ETA=13:40:53, max mem: 26.1 GB 
[11/21 10:20:06 visual_prompt]: 	Training 400/553. train loss: 0.6051,	3.2160 s / batch. (data: 2.30e+00). ETA=1 day, 23:04:04, max mem: 26.1 GB 
[11/21 10:21:44 visual_prompt]: 	Training 500/553. train loss: 0.7543,	0.9168 s / batch. (data: 8.17e-04). ETA=13:23:32, max mem: 26.1 GB 
[11/21 10:22:33 visual_prompt]: Epoch 5 / 100: avg data time: 4.99e-02, avg batch time: 0.9767, average train loss: 1.9878
[11/21 10:23:31 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3048, average loss: 1.1516
[11/21 10:23:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.17	
[11/21 10:23:31 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/21 10:25:18 visual_prompt]: 	Training 100/553. train loss: 0.6223,	0.9201 s / batch. (data: 2.03e-02). ETA=13:24:07, max mem: 26.1 GB 
[11/21 10:26:53 visual_prompt]: 	Training 200/553. train loss: 7.9312,	0.9317 s / batch. (data: 7.97e-03). ETA=13:32:39, max mem: 26.1 GB 
[11/21 10:28:29 visual_prompt]: 	Training 300/553. train loss: 0.6063,	0.9389 s / batch. (data: 1.56e-02). ETA=13:37:23, max mem: 26.1 GB 
[11/21 10:30:04 visual_prompt]: 	Training 400/553. train loss: 1.2825,	1.8440 s / batch. (data: 9.13e-01). ETA=1 day, 2:42:17, max mem: 26.1 GB 
[11/21 10:31:43 visual_prompt]: 	Training 500/553. train loss: 2.4552,	0.9424 s / batch. (data: 5.84e-03). ETA=13:37:17, max mem: 26.1 GB 
[11/21 10:32:32 visual_prompt]: Epoch 6 / 100: avg data time: 5.26e-02, avg batch time: 0.9788, average train loss: 2.4920
[11/21 10:33:30 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3040, average loss: 0.9287
[11/21 10:33:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.65	
[11/21 10:33:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/21 10:35:20 visual_prompt]: 	Training 100/553. train loss: 1.7369,	0.9199 s / batch. (data: 1.55e-02). ETA=13:15:27, max mem: 26.1 GB 
[11/21 10:36:55 visual_prompt]: 	Training 200/553. train loss: 1.5202,	0.9320 s / batch. (data: 2.99e-04). ETA=13:24:19, max mem: 26.1 GB 
[11/21 10:38:29 visual_prompt]: 	Training 300/553. train loss: 0.7774,	0.9479 s / batch. (data: 1.17e-03). ETA=13:36:30, max mem: 26.1 GB 
[11/21 10:40:03 visual_prompt]: 	Training 400/553. train loss: 1.4662,	0.9560 s / batch. (data: 5.44e-03). ETA=13:41:53, max mem: 26.1 GB 
[11/21 10:41:39 visual_prompt]: 	Training 500/553. train loss: 0.3932,	0.9144 s / batch. (data: 2.77e-04). ETA=13:04:36, max mem: 26.1 GB 
[11/21 10:42:30 visual_prompt]: Epoch 7 / 100: avg data time: 4.87e-02, avg batch time: 0.9762, average train loss: 1.7798
[11/21 10:43:27 visual_prompt]: Inference (val):avg data time: 5.56e-04, avg batch time: 0.3055, average loss: 1.0446
[11/21 10:43:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.33	
[11/21 10:43:27 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/21 10:45:13 visual_prompt]: 	Training 100/553. train loss: 0.5033,	0.9281 s / batch. (data: 8.01e-03). ETA=13:13:56, max mem: 26.1 GB 
[11/21 10:46:51 visual_prompt]: 	Training 200/553. train loss: 3.0592,	0.9399 s / batch. (data: 2.86e-04). ETA=13:22:32, max mem: 26.1 GB 
[11/21 10:48:26 visual_prompt]: 	Training 300/553. train loss: 2.2657,	0.9267 s / batch. (data: 5.42e-03). ETA=13:09:40, max mem: 26.1 GB 
[11/21 10:50:03 visual_prompt]: 	Training 400/553. train loss: 0.5574,	0.9240 s / batch. (data: 2.81e-04). ETA=13:05:52, max mem: 26.1 GB 
[11/21 10:51:40 visual_prompt]: 	Training 500/553. train loss: 0.8671,	0.9400 s / batch. (data: 5.39e-03). ETA=13:17:53, max mem: 26.1 GB 
[11/21 10:52:29 visual_prompt]: Epoch 8 / 100: avg data time: 5.18e-02, avg batch time: 0.9788, average train loss: 1.4429
[11/21 10:53:26 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3062, average loss: 0.6871
[11/21 10:53:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.24	
[11/21 10:53:26 visual_prompt]: Best epoch 8: best metric: -0.687
[11/21 10:53:26 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/21 10:55:16 visual_prompt]: 	Training 100/553. train loss: 0.5359,	0.9440 s / batch. (data: 5.42e-03). ETA=13:18:53, max mem: 26.1 GB 
[11/21 10:56:52 visual_prompt]: 	Training 200/553. train loss: 0.5565,	0.9153 s / batch. (data: 3.14e-04). ETA=12:53:04, max mem: 26.1 GB 
[11/21 10:58:26 visual_prompt]: 	Training 300/553. train loss: 3.9039,	0.9200 s / batch. (data: 2.86e-04). ETA=12:55:28, max mem: 26.1 GB 
[11/21 11:00:00 visual_prompt]: 	Training 400/553. train loss: 1.6155,	0.9200 s / batch. (data: 3.26e-04). ETA=12:53:57, max mem: 26.1 GB 
[11/21 11:01:34 visual_prompt]: 	Training 500/553. train loss: 1.3830,	0.9120 s / batch. (data: 3.09e-04). ETA=12:45:43, max mem: 26.1 GB 
[11/21 11:02:24 visual_prompt]: Epoch 9 / 100: avg data time: 4.65e-02, avg batch time: 0.9726, average train loss: 1.3509
[11/21 11:03:22 visual_prompt]: Inference (val):avg data time: 8.45e-05, avg batch time: 0.3067, average loss: 1.4583
[11/21 11:03:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.03	
[11/21 11:03:22 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/21 11:05:08 visual_prompt]: 	Training 100/553. train loss: 0.9705,	0.9200 s / batch. (data: 2.70e-04). ETA=12:50:04, max mem: 26.1 GB 
[11/21 11:06:46 visual_prompt]: 	Training 200/553. train loss: 1.0458,	0.9280 s / batch. (data: 3.10e-04). ETA=12:55:14, max mem: 26.1 GB 
[11/21 11:08:20 visual_prompt]: 	Training 300/553. train loss: 4.9483,	0.9520 s / batch. (data: 2.69e-04). ETA=13:13:43, max mem: 26.1 GB 
[11/21 11:09:54 visual_prompt]: 	Training 400/553. train loss: 0.8726,	0.9190 s / batch. (data: 7.00e-03). ETA=12:44:39, max mem: 26.1 GB 
[11/21 11:11:30 visual_prompt]: 	Training 500/553. train loss: 0.8944,	0.9280 s / batch. (data: 3.23e-04). ETA=12:50:34, max mem: 26.1 GB 
[11/21 11:12:21 visual_prompt]: Epoch 10 / 100: avg data time: 4.94e-02, avg batch time: 0.9747, average train loss: 1.3137
[11/21 11:13:19 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.3058, average loss: 1.0604
[11/21 11:13:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.77	
[11/21 11:13:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/21 11:15:09 visual_prompt]: 	Training 100/553. train loss: 1.3651,	0.9087 s / batch. (data: 2.40e-04). ETA=12:32:14, max mem: 26.1 GB 
[11/21 11:16:43 visual_prompt]: 	Training 200/553. train loss: 1.0848,	0.9120 s / batch. (data: 7.96e-03). ETA=12:33:26, max mem: 26.1 GB 
[11/21 11:18:17 visual_prompt]: 	Training 300/553. train loss: 1.8450,	0.9395 s / batch. (data: 5.41e-03). ETA=12:54:35, max mem: 26.1 GB 
[11/21 11:19:53 visual_prompt]: 	Training 400/553. train loss: 0.3595,	0.9240 s / batch. (data: 2.92e-04). ETA=12:40:17, max mem: 26.1 GB 
[11/21 11:21:28 visual_prompt]: 	Training 500/553. train loss: 0.5248,	0.9087 s / batch. (data: 3.08e-04). ETA=12:26:12, max mem: 26.1 GB 
[11/21 11:22:17 visual_prompt]: Epoch 11 / 100: avg data time: 4.77e-02, avg batch time: 0.9738, average train loss: 1.0908
[11/21 11:23:15 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3070, average loss: 0.9445
[11/21 11:23:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.70	
[11/21 11:23:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/21 11:24:58 visual_prompt]: 	Training 100/553. train loss: 0.9070,	0.9200 s / batch. (data: 2.82e-04). ETA=12:33:06, max mem: 26.1 GB 
[11/21 11:26:37 visual_prompt]: 	Training 200/553. train loss: 0.7172,	0.9560 s / batch. (data: 2.84e-04). ETA=13:01:00, max mem: 26.1 GB 
[11/21 11:28:12 visual_prompt]: 	Training 300/553. train loss: 1.3493,	0.9440 s / batch. (data: 3.09e-04). ETA=12:49:37, max mem: 26.1 GB 
[11/21 11:29:48 visual_prompt]: 	Training 400/553. train loss: 1.2170,	0.9883 s / batch. (data: 2.43e-02). ETA=13:24:05, max mem: 26.1 GB 
[11/21 11:31:24 visual_prompt]: 	Training 500/553. train loss: 0.7329,	0.9005 s / batch. (data: 2.98e-04). ETA=12:11:08, max mem: 26.1 GB 
[11/21 11:32:15 visual_prompt]: Epoch 12 / 100: avg data time: 4.92e-02, avg batch time: 0.9751, average train loss: 1.0618
[11/21 11:33:12 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3041, average loss: 0.6740
[11/21 11:33:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 58.51	
[11/21 11:33:12 visual_prompt]: Best epoch 12: best metric: -0.674
[11/21 11:33:12 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/21 11:34:59 visual_prompt]: 	Training 100/553. train loss: 0.8093,	0.9280 s / batch. (data: 2.90e-04). ETA=12:31:08, max mem: 26.1 GB 
[11/21 11:36:33 visual_prompt]: 	Training 200/553. train loss: 1.5780,	0.9101 s / batch. (data: 2.75e-04). ETA=12:15:04, max mem: 26.1 GB 
[11/21 11:38:11 visual_prompt]: 	Training 300/553. train loss: 2.9963,	0.9416 s / batch. (data: 3.98e-03). ETA=12:38:57, max mem: 26.1 GB 
[11/21 11:39:49 visual_prompt]: 	Training 400/553. train loss: 0.9872,	0.9280 s / batch. (data: 5.57e-03). ETA=12:26:27, max mem: 26.1 GB 
[11/21 11:41:24 visual_prompt]: 	Training 500/553. train loss: 0.7138,	0.9205 s / batch. (data: 5.42e-03). ETA=12:18:54, max mem: 26.1 GB 
[11/21 11:42:13 visual_prompt]: Epoch 13 / 100: avg data time: 5.01e-02, avg batch time: 0.9777, average train loss: 0.8841
[11/21 11:43:10 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3052, average loss: 0.6946
[11/21 11:43:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 54.45	
[11/21 11:43:10 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/21 11:44:58 visual_prompt]: 	Training 100/553. train loss: 1.0760,	0.9511 s / batch. (data: 1.55e-02). ETA=12:41:02, max mem: 26.1 GB 
[11/21 11:46:38 visual_prompt]: 	Training 200/553. train loss: 0.4208,	0.9456 s / batch. (data: 1.05e-02). ETA=12:35:05, max mem: 26.1 GB 
[11/21 11:48:13 visual_prompt]: 	Training 300/553. train loss: 0.7685,	0.9297 s / batch. (data: 3.34e-04). ETA=12:20:50, max mem: 26.1 GB 
[11/21 11:49:49 visual_prompt]: 	Training 400/553. train loss: 1.1280,	0.9160 s / batch. (data: 3.15e-04). ETA=12:08:23, max mem: 26.1 GB 
[11/21 11:51:25 visual_prompt]: 	Training 500/553. train loss: 0.7168,	0.9400 s / batch. (data: 3.14e-04). ETA=12:25:53, max mem: 26.1 GB 
[11/21 11:52:15 visual_prompt]: Epoch 14 / 100: avg data time: 5.78e-02, avg batch time: 0.9840, average train loss: 0.8587
[11/21 11:53:12 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3042, average loss: 0.8357
[11/21 11:53:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.39	
[11/21 11:53:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/21 11:55:01 visual_prompt]: 	Training 100/553. train loss: 0.6012,	0.9724 s / batch. (data: 2.84e-04). ETA=12:49:08, max mem: 26.1 GB 
[11/21 11:56:38 visual_prompt]: 	Training 200/553. train loss: 0.7494,	0.9240 s / batch. (data: 2.89e-04). ETA=12:09:18, max mem: 26.1 GB 
[11/21 11:58:14 visual_prompt]: 	Training 300/553. train loss: 0.5559,	0.9080 s / batch. (data: 2.92e-04). ETA=11:55:09, max mem: 26.1 GB 
[11/21 11:59:48 visual_prompt]: 	Training 400/553. train loss: 0.5820,	0.9280 s / batch. (data: 2.99e-04). ETA=12:09:22, max mem: 26.1 GB 
[11/21 12:01:23 visual_prompt]: 	Training 500/553. train loss: 0.5310,	0.9231 s / batch. (data: 7.08e-03). ETA=12:04:00, max mem: 26.1 GB 
[11/21 12:02:13 visual_prompt]: Epoch 15 / 100: avg data time: 5.03e-02, avg batch time: 0.9767, average train loss: 0.8706
[11/21 12:03:10 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3023, average loss: 0.8532
[11/21 12:03:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.00	
[11/21 12:03:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/21 12:04:56 visual_prompt]: 	Training 100/553. train loss: 1.0945,	0.9526 s / batch. (data: 8.59e-03). ETA=12:24:42, max mem: 26.1 GB 
[11/21 12:06:29 visual_prompt]: 	Training 200/553. train loss: 0.5328,	0.9567 s / batch. (data: 2.06e-02). ETA=12:26:17, max mem: 26.1 GB 
[11/21 12:08:10 visual_prompt]: 	Training 300/553. train loss: 0.6223,	0.9560 s / batch. (data: 2.86e-04). ETA=12:24:10, max mem: 26.1 GB 
[11/21 12:09:46 visual_prompt]: 	Training 400/553. train loss: 0.4783,	0.9218 s / batch. (data: 1.44e-03). ETA=11:56:02, max mem: 26.1 GB 
[11/21 12:11:20 visual_prompt]: 	Training 500/553. train loss: 1.1763,	0.9600 s / batch. (data: 7.76e-04). ETA=12:24:03, max mem: 26.1 GB 
[11/21 12:12:10 visual_prompt]: Epoch 16 / 100: avg data time: 4.91e-02, avg batch time: 0.9761, average train loss: 0.8148
[11/21 12:13:08 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3043, average loss: 0.6894
[11/21 12:13:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 54.69	
[11/21 12:13:08 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/21 12:14:51 visual_prompt]: 	Training 100/553. train loss: 0.6604,	0.9252 s / batch. (data: 2.96e-04). ETA=11:54:45, max mem: 26.1 GB 
[11/21 12:16:31 visual_prompt]: 	Training 200/553. train loss: 1.3822,	0.9361 s / batch. (data: 5.79e-03). ETA=12:01:37, max mem: 26.1 GB 
[11/21 12:18:04 visual_prompt]: 	Training 300/553. train loss: 1.3786,	0.9412 s / batch. (data: 2.07e-02). ETA=12:03:59, max mem: 26.1 GB 
[11/21 12:19:41 visual_prompt]: 	Training 400/553. train loss: 0.5780,	0.9331 s / batch. (data: 5.41e-03). ETA=11:56:12, max mem: 26.1 GB 
[11/21 12:21:17 visual_prompt]: 	Training 500/553. train loss: 0.5902,	0.9080 s / batch. (data: 3.40e-04). ETA=11:35:23, max mem: 26.1 GB 
[11/21 12:22:07 visual_prompt]: Epoch 17 / 100: avg data time: 4.91e-02, avg batch time: 0.9751, average train loss: 0.7816
[11/21 12:23:06 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3027, average loss: 0.9451
[11/21 12:23:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.50	
[11/21 12:23:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/21 12:24:55 visual_prompt]: 	Training 100/553. train loss: 0.5209,	0.8957 s / batch. (data: 2.83e-04). ETA=11:23:41, max mem: 26.1 GB 
[11/21 12:26:32 visual_prompt]: 	Training 200/553. train loss: 0.6412,	0.9320 s / batch. (data: 7.92e-03). ETA=11:49:51, max mem: 26.1 GB 
[11/21 12:28:05 visual_prompt]: 	Training 300/553. train loss: 0.8375,	0.9120 s / batch. (data: 3.14e-04). ETA=11:33:06, max mem: 26.1 GB 
[11/21 12:29:40 visual_prompt]: 	Training 400/553. train loss: 0.6729,	0.9680 s / batch. (data: 7.95e-03). ETA=12:14:04, max mem: 26.1 GB 
[11/21 12:31:18 visual_prompt]: 	Training 500/553. train loss: 0.4456,	0.9240 s / batch. (data: 3.07e-04). ETA=11:39:08, max mem: 26.1 GB 
[11/21 12:32:07 visual_prompt]: Epoch 18 / 100: avg data time: 5.29e-02, avg batch time: 0.9783, average train loss: 0.7650
[11/21 12:33:04 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3056, average loss: 0.7300
[11/21 12:33:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 55.53	
[11/21 12:33:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/21 12:34:50 visual_prompt]: 	Training 100/553. train loss: 0.5641,	0.8975 s / batch. (data: 2.83e-04). ETA=11:16:46, max mem: 26.1 GB 
[11/21 12:36:29 visual_prompt]: 	Training 200/553. train loss: 0.6937,	2.7240 s / batch. (data: 1.83e+00). ETA=1 day, 10:09:37, max mem: 26.1 GB 
[11/21 12:38:06 visual_prompt]: 	Training 300/553. train loss: 0.6737,	0.9560 s / batch. (data: 3.11e-04). ETA=11:57:43, max mem: 26.1 GB 
[11/21 12:39:41 visual_prompt]: 	Training 400/553. train loss: 0.6837,	0.9400 s / batch. (data: 2.80e-04). ETA=11:44:10, max mem: 26.1 GB 
[11/21 12:41:17 visual_prompt]: 	Training 500/553. train loss: 0.3063,	0.9276 s / batch. (data: 3.02e-04). ETA=11:33:19, max mem: 26.1 GB 
[11/21 12:42:07 visual_prompt]: Epoch 19 / 100: avg data time: 5.52e-02, avg batch time: 0.9812, average train loss: 0.7199
[11/21 12:43:05 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3034, average loss: 0.8324
[11/21 12:43:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.87	
[11/21 12:43:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/21 12:44:54 visual_prompt]: 	Training 100/553. train loss: 0.7152,	0.9279 s / batch. (data: 7.33e-03). ETA=11:31:09, max mem: 26.1 GB 
[11/21 12:46:27 visual_prompt]: 	Training 200/553. train loss: 0.8440,	0.9358 s / batch. (data: 1.60e-02). ETA=11:35:29, max mem: 26.1 GB 
[11/21 12:48:04 visual_prompt]: 	Training 300/553. train loss: 0.5085,	0.9564 s / batch. (data: 2.57e-02). ETA=11:49:13, max mem: 26.1 GB 
[11/21 12:49:40 visual_prompt]: 	Training 400/553. train loss: 0.5328,	0.9286 s / batch. (data: 7.97e-03). ETA=11:27:05, max mem: 26.1 GB 
[11/21 12:51:13 visual_prompt]: 	Training 500/553. train loss: 0.6588,	1.0240 s / batch. (data: 9.42e-02). ETA=12:35:56, max mem: 26.1 GB 
[11/21 12:52:06 visual_prompt]: Epoch 20 / 100: avg data time: 5.21e-02, avg batch time: 0.9778, average train loss: 0.7230
[11/21 12:53:03 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3056, average loss: 0.7657
[11/21 12:53:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.41	rocauc: 57.56	
[11/21 12:53:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/21 12:54:54 visual_prompt]: 	Training 100/553. train loss: 0.8145,	0.9318 s / batch. (data: 2.84e-04). ETA=11:25:30, max mem: 26.1 GB 
[11/21 12:56:31 visual_prompt]: 	Training 200/553. train loss: 0.9234,	0.9360 s / batch. (data: 5.41e-03). ETA=11:27:01, max mem: 26.1 GB 
[11/21 12:58:07 visual_prompt]: 	Training 300/553. train loss: 0.6873,	0.9261 s / batch. (data: 7.98e-03). ETA=11:18:13, max mem: 26.1 GB 
[11/21 12:59:43 visual_prompt]: 	Training 400/553. train loss: 0.9933,	0.9280 s / batch. (data: 4.24e-04). ETA=11:18:02, max mem: 26.1 GB 
[11/21 13:01:18 visual_prompt]: 	Training 500/553. train loss: 0.6344,	0.9360 s / batch. (data: 2.89e-04). ETA=11:22:19, max mem: 26.1 GB 
[11/21 13:02:07 visual_prompt]: Epoch 21 / 100: avg data time: 5.73e-02, avg batch time: 0.9841, average train loss: 0.7069
[11/21 13:03:05 visual_prompt]: Inference (val):avg data time: 8.26e-05, avg batch time: 0.3049, average loss: 0.6862
[11/21 13:03:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.93	
[11/21 13:03:05 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/21 13:04:50 visual_prompt]: 	Training 100/553. train loss: 0.6172,	0.9400 s / batch. (data: 4.08e-04). ETA=11:22:50, max mem: 26.1 GB 
[11/21 13:06:29 visual_prompt]: 	Training 200/553. train loss: 0.9731,	0.9360 s / batch. (data: 7.80e-04). ETA=11:18:22, max mem: 26.1 GB 
[11/21 13:08:05 visual_prompt]: 	Training 300/553. train loss: 0.4800,	0.9356 s / batch. (data: 7.42e-04). ETA=11:16:34, max mem: 26.1 GB 
[11/21 13:09:38 visual_prompt]: 	Training 400/553. train loss: 1.0471,	0.9266 s / batch. (data: 7.97e-03). ETA=11:08:28, max mem: 26.1 GB 
[11/21 13:11:15 visual_prompt]: 	Training 500/553. train loss: 0.7977,	0.9360 s / batch. (data: 2.78e-04). ETA=11:13:42, max mem: 26.1 GB 
[11/21 13:12:04 visual_prompt]: Epoch 22 / 100: avg data time: 4.90e-02, avg batch time: 0.9748, average train loss: 0.6979
[11/21 13:13:02 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3072, average loss: 0.7079
[11/21 13:13:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 57.54	
[11/21 13:13:02 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/21 13:14:50 visual_prompt]: 	Training 100/553. train loss: 0.5784,	0.9440 s / batch. (data: 8.28e-04). ETA=11:17:04, max mem: 26.1 GB 
[11/21 13:16:26 visual_prompt]: 	Training 200/553. train loss: 0.6687,	0.9320 s / batch. (data: 7.04e-04). ETA=11:06:54, max mem: 26.1 GB 
[11/21 13:18:00 visual_prompt]: 	Training 300/553. train loss: 0.2720,	0.9480 s / batch. (data: 7.96e-03). ETA=11:16:47, max mem: 26.1 GB 
[11/21 13:19:36 visual_prompt]: 	Training 400/553. train loss: 0.7471,	0.9440 s / batch. (data: 5.42e-03). ETA=11:12:20, max mem: 26.1 GB 
[11/21 13:21:09 visual_prompt]: 	Training 500/553. train loss: 0.1725,	0.9349 s / batch. (data: 2.70e-04). ETA=11:04:16, max mem: 26.1 GB 
[11/21 13:22:01 visual_prompt]: Epoch 23 / 100: avg data time: 4.84e-02, avg batch time: 0.9744, average train loss: 0.6915
[11/21 13:22:59 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3065, average loss: 0.8739
[11/21 13:22:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.40	
[11/21 13:22:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/21 13:24:44 visual_prompt]: 	Training 100/553. train loss: 0.7935,	0.9004 s / batch. (data: 2.77e-04). ETA=10:37:31, max mem: 26.1 GB 
[11/21 13:26:21 visual_prompt]: 	Training 200/553. train loss: 0.4992,	0.9572 s / batch. (data: 2.18e-02). ETA=11:16:08, max mem: 26.1 GB 
[11/21 13:27:59 visual_prompt]: 	Training 300/553. train loss: 0.6142,	0.9480 s / batch. (data: 3.36e-04). ETA=11:08:01, max mem: 26.1 GB 
[11/21 13:29:35 visual_prompt]: 	Training 400/553. train loss: 0.7865,	0.9280 s / batch. (data: 2.80e-04). ETA=10:52:23, max mem: 26.1 GB 
[11/21 13:31:14 visual_prompt]: 	Training 500/553. train loss: 0.7062,	0.9360 s / batch. (data: 7.72e-04). ETA=10:56:27, max mem: 26.1 GB 
[11/21 13:32:03 visual_prompt]: Epoch 24 / 100: avg data time: 5.70e-02, avg batch time: 0.9828, average train loss: 0.6941
[11/21 13:33:01 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3049, average loss: 0.7391
[11/21 13:33:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.47	
[11/21 13:33:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/21 13:34:44 visual_prompt]: 	Training 100/553. train loss: 0.8961,	0.9334 s / batch. (data: 3.05e-04). ETA=10:52:13, max mem: 26.1 GB 
[11/21 13:36:24 visual_prompt]: 	Training 200/553. train loss: 0.4227,	0.9281 s / batch. (data: 2.86e-04). ETA=10:46:59, max mem: 26.1 GB 
[11/21 13:37:59 visual_prompt]: 	Training 300/553. train loss: 0.6595,	0.9400 s / batch. (data: 3.09e-04). ETA=10:53:43, max mem: 26.1 GB 
[11/21 13:39:36 visual_prompt]: 	Training 400/553. train loss: 0.6722,	0.9160 s / batch. (data: 3.96e-03). ETA=10:35:31, max mem: 26.1 GB 
[11/21 13:41:12 visual_prompt]: 	Training 500/553. train loss: 0.5489,	0.9600 s / batch. (data: 7.95e-03). ETA=11:04:26, max mem: 26.1 GB 
[11/21 13:42:03 visual_prompt]: Epoch 25 / 100: avg data time: 5.50e-02, avg batch time: 0.9801, average train loss: 0.6857
[11/21 13:43:01 visual_prompt]: Inference (val):avg data time: 8.35e-05, avg batch time: 0.3063, average loss: 0.7132
[11/21 13:43:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.56	
[11/21 13:43:01 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/21 13:44:46 visual_prompt]: 	Training 100/553. train loss: 0.4820,	0.9280 s / batch. (data: 7.80e-04). ETA=10:39:57, max mem: 26.1 GB 
[11/21 13:46:23 visual_prompt]: 	Training 200/553. train loss: 0.4461,	0.9440 s / batch. (data: 7.96e-03). ETA=10:49:23, max mem: 26.1 GB 
[11/21 13:48:01 visual_prompt]: 	Training 300/553. train loss: 0.6212,	0.9400 s / batch. (data: 3.17e-04). ETA=10:45:04, max mem: 26.1 GB 
[11/21 13:49:41 visual_prompt]: 	Training 400/553. train loss: 0.7062,	0.9200 s / batch. (data: 3.12e-04). ETA=10:29:48, max mem: 26.1 GB 
[11/21 13:51:36 visual_prompt]: 	Training 500/553. train loss: 0.5692,	0.9200 s / batch. (data: 3.12e-04). ETA=10:28:17, max mem: 26.1 GB 
[11/21 13:52:28 visual_prompt]: Epoch 26 / 100: avg data time: 1.04e-01, avg batch time: 1.0263, average train loss: 0.7119
[11/21 13:53:30 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3051, average loss: 0.8444
[11/21 13:53:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.78	
[11/21 13:53:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/21 13:55:31 visual_prompt]: 	Training 100/553. train loss: 0.6843,	0.9245 s / batch. (data: 2.07e-02). ETA=10:29:01, max mem: 26.1 GB 
[11/21 13:57:10 visual_prompt]: 	Training 200/553. train loss: 0.6722,	0.9480 s / batch. (data: 2.93e-04). ETA=10:43:23, max mem: 26.1 GB 
[11/21 13:58:43 visual_prompt]: 	Training 300/553. train loss: 0.7370,	0.9376 s / batch. (data: 5.41e-03). ETA=10:34:47, max mem: 26.1 GB 
[11/21 14:00:52 visual_prompt]: 	Training 400/553. train loss: 1.0423,	0.8960 s / batch. (data: 2.89e-04). ETA=10:05:06, max mem: 26.1 GB 
[11/21 14:02:37 visual_prompt]: 	Training 500/553. train loss: 0.9136,	0.9120 s / batch. (data: 3.11e-04). ETA=10:14:26, max mem: 26.1 GB 
[11/21 14:03:29 visual_prompt]: Epoch 27 / 100: avg data time: 1.63e-01, avg batch time: 1.0831, average train loss: 0.6991
[11/21 14:04:27 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3055, average loss: 0.6973
[11/21 14:04:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 55.31	
[11/21 14:04:27 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/21 14:06:12 visual_prompt]: 	Training 100/553. train loss: 0.7760,	0.9122 s / batch. (data: 7.97e-03). ETA=10:12:12, max mem: 26.1 GB 
[11/21 14:07:50 visual_prompt]: 	Training 200/553. train loss: 0.5653,	0.9280 s / batch. (data: 2.76e-04). ETA=10:21:16, max mem: 26.1 GB 
[11/21 14:09:27 visual_prompt]: 	Training 300/553. train loss: 1.0459,	0.9200 s / batch. (data: 2.81e-04). ETA=10:14:23, max mem: 26.1 GB 
[11/21 14:11:04 visual_prompt]: 	Training 400/553. train loss: 0.6283,	0.9122 s / batch. (data: 1.05e-02). ETA=10:07:40, max mem: 26.1 GB 
[11/21 14:12:41 visual_prompt]: 	Training 500/553. train loss: 0.9453,	0.9309 s / batch. (data: 7.44e-04). ETA=10:18:35, max mem: 26.1 GB 
[11/21 14:13:30 visual_prompt]: Epoch 28 / 100: avg data time: 5.66e-02, avg batch time: 0.9820, average train loss: 0.6986
[11/21 14:14:28 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3034, average loss: 0.6970
[11/21 14:14:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.80	
[11/21 14:14:28 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/21 14:16:16 visual_prompt]: 	Training 100/553. train loss: 0.7620,	0.9353 s / batch. (data: 1.05e-02). ETA=10:19:08, max mem: 26.1 GB 
[11/21 14:17:54 visual_prompt]: 	Training 200/553. train loss: 0.6212,	0.9280 s / batch. (data: 7.96e-03). ETA=10:12:43, max mem: 26.1 GB 
[11/21 14:19:38 visual_prompt]: 	Training 300/553. train loss: 0.7351,	1.1080 s / batch. (data: 1.44e-01). ETA=12:09:43, max mem: 26.1 GB 
[11/21 14:21:22 visual_prompt]: 	Training 400/553. train loss: 0.6625,	0.9440 s / batch. (data: 7.50e-04). ETA=10:20:08, max mem: 26.1 GB 
[11/21 14:22:56 visual_prompt]: 	Training 500/553. train loss: 0.8023,	0.9341 s / batch. (data: 7.02e-04). ETA=10:12:04, max mem: 26.1 GB 
[11/21 14:23:45 visual_prompt]: Epoch 29 / 100: avg data time: 8.26e-02, avg batch time: 1.0066, average train loss: 0.6884
[11/21 14:24:43 visual_prompt]: Inference (val):avg data time: 2.74e-04, avg batch time: 0.3035, average loss: 0.7287
[11/21 14:24:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 56.44	
[11/21 14:24:43 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/21 14:26:28 visual_prompt]: 	Training 100/553. train loss: 0.5773,	0.9139 s / batch. (data: 3.39e-04). ETA=9:56:32, max mem: 26.1 GB 
[11/21 14:28:09 visual_prompt]: 	Training 200/553. train loss: 0.5371,	0.9146 s / batch. (data: 6.08e-04). ETA=9:55:28, max mem: 26.1 GB 
[11/21 14:29:46 visual_prompt]: 	Training 300/553. train loss: 0.6029,	0.9395 s / batch. (data: 2.39e-02). ETA=10:10:04, max mem: 26.1 GB 
[11/21 14:31:21 visual_prompt]: 	Training 400/553. train loss: 0.5813,	0.9200 s / batch. (data: 5.40e-03). ETA=9:55:53, max mem: 26.1 GB 
[11/21 14:32:57 visual_prompt]: 	Training 500/553. train loss: 0.8251,	0.9397 s / batch. (data: 3.07e-04). ETA=10:07:06, max mem: 26.1 GB 
[11/21 14:33:47 visual_prompt]: Epoch 30 / 100: avg data time: 5.75e-02, avg batch time: 0.9833, average train loss: 0.6903
[11/21 14:34:45 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3079, average loss: 0.7515
[11/21 14:34:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 54.47	
[11/21 14:34:45 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/21 14:36:35 visual_prompt]: 	Training 100/553. train loss: 0.6778,	0.9240 s / batch. (data: 3.78e-04). ETA=9:54:35, max mem: 26.1 GB 
[11/21 14:38:09 visual_prompt]: 	Training 200/553. train loss: 0.5993,	0.9160 s / batch. (data: 3.20e-04). ETA=9:47:55, max mem: 26.1 GB 
[11/21 14:39:47 visual_prompt]: 	Training 300/553. train loss: 0.7533,	0.9248 s / batch. (data: 5.40e-03). ETA=9:52:00, max mem: 26.1 GB 
[11/21 14:41:23 visual_prompt]: 	Training 400/553. train loss: 0.6334,	0.9143 s / batch. (data: 3.07e-04). ETA=9:43:46, max mem: 26.1 GB 
[11/21 14:42:57 visual_prompt]: 	Training 500/553. train loss: 1.1411,	0.9760 s / batch. (data: 6.03e-02). ETA=10:21:32, max mem: 26.1 GB 
[11/21 14:43:48 visual_prompt]: Epoch 31 / 100: avg data time: 5.66e-02, avg batch time: 0.9822, average train loss: 0.6844
[11/21 14:44:46 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3038, average loss: 0.6861
[11/21 14:44:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.00	
[11/21 14:44:46 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/21 14:46:32 visual_prompt]: 	Training 100/553. train loss: 0.5935,	0.9222 s / batch. (data: 1.55e-02). ETA=9:44:56, max mem: 26.1 GB 
[11/21 14:48:09 visual_prompt]: 	Training 200/553. train loss: 0.7077,	0.9289 s / batch. (data: 5.40e-03). ETA=9:47:37, max mem: 26.1 GB 
[11/21 14:49:45 visual_prompt]: 	Training 300/553. train loss: 0.4939,	0.9120 s / batch. (data: 3.45e-04). ETA=9:35:25, max mem: 26.1 GB 
[11/21 14:51:23 visual_prompt]: 	Training 400/553. train loss: 0.7132,	0.9160 s / batch. (data: 2.80e-04). ETA=9:36:25, max mem: 26.1 GB 
[11/21 14:53:02 visual_prompt]: 	Training 500/553. train loss: 1.1047,	0.9035 s / batch. (data: 2.89e-04). ETA=9:27:04, max mem: 26.1 GB 
[11/21 14:53:51 visual_prompt]: Epoch 32 / 100: avg data time: 5.95e-02, avg batch time: 0.9852, average train loss: 0.6859
[11/21 14:54:50 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3065, average loss: 0.6806
[11/21 14:54:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.17	
[11/21 14:54:50 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/21 14:56:37 visual_prompt]: 	Training 100/553. train loss: 0.7002,	0.9280 s / batch. (data: 7.95e-03). ETA=9:40:03, max mem: 26.1 GB 
[11/21 14:58:14 visual_prompt]: 	Training 200/553. train loss: 0.6114,	0.9360 s / batch. (data: 3.25e-04). ETA=9:43:29, max mem: 26.1 GB 
[11/21 14:59:50 visual_prompt]: 	Training 300/553. train loss: 0.6628,	0.9400 s / batch. (data: 3.97e-03). ETA=9:44:25, max mem: 26.1 GB 
[11/21 15:01:27 visual_prompt]: 	Training 400/553. train loss: 0.7941,	0.9720 s / batch. (data: 1.05e-02). ETA=10:02:42, max mem: 26.1 GB 
[11/21 15:03:02 visual_prompt]: 	Training 500/553. train loss: 0.5260,	0.9400 s / batch. (data: 7.95e-03). ETA=9:41:17, max mem: 26.1 GB 
[11/21 15:03:52 visual_prompt]: Epoch 33 / 100: avg data time: 5.35e-02, avg batch time: 0.9805, average train loss: 0.6832
[11/21 15:04:51 visual_prompt]: Inference (val):avg data time: 8.05e-04, avg batch time: 0.3068, average loss: 0.6915
[11/21 15:04:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.24	
[11/21 15:04:51 visual_prompt]: Stopping early.
[11/21 15:04:51 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 15:04:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 15:04:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/21 15:04:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 15:04:51 visual_prompt]: Training with config:
[11/21 15:04:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.001_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 15:04:51 visual_prompt]: Loading training data...
[11/21 15:04:51 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 15:04:51 visual_prompt]: Loading validation data...
[11/21 15:04:51 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 15:04:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 15:04:55 visual_prompt]: Enable all parameters update during training
[11/21 15:04:55 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 15:04:55 visual_prompt]: tuned percent:100.000
[11/21 15:04:56 visual_prompt]: Device used for model: 0
[11/21 15:04:56 visual_prompt]: Setting up Evaluator...
[11/21 15:04:56 visual_prompt]: Setting up Trainer...
[11/21 15:04:56 visual_prompt]: 	Setting up the optimizer...
[11/21 15:04:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 15:06:40 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9251 s / batch. (data: 2.90e-04). ETA=14:11:04, max mem: 26.1 GB 
[11/21 15:08:21 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9109 s / batch. (data: 2.67e-04). ETA=13:56:28, max mem: 26.1 GB 
[11/21 15:09:58 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9200 s / batch. (data: 2.65e-04). ETA=14:03:20, max mem: 26.1 GB 
[11/21 15:11:32 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9472 s / batch. (data: 7.54e-03). ETA=14:26:39, max mem: 26.1 GB 
[11/21 15:13:10 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9346 s / batch. (data: 6.64e-03). ETA=14:13:37, max mem: 26.1 GB 
[11/21 15:14:00 visual_prompt]: Epoch 1 / 100: avg data time: 4.96e-02, avg batch time: 0.9832, average train loss: 7.6130
[11/21 15:14:57 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3047, average loss: 6.9126
[11/21 15:14:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 15:14:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/21 15:16:44 visual_prompt]: 	Training 100/553. train loss: 1.3811,	0.9565 s / batch. (data: 1.56e-02). ETA=14:31:09, max mem: 26.1 GB 
[11/21 15:18:19 visual_prompt]: 	Training 200/553. train loss: 0.9806,	1.6881 s / batch. (data: 7.66e-01). ETA=1 day, 1:34:39, max mem: 26.1 GB 
[11/21 15:19:56 visual_prompt]: 	Training 300/553. train loss: 0.7313,	0.9241 s / batch. (data: 5.37e-03). ETA=13:58:33, max mem: 26.1 GB 
[11/21 15:21:34 visual_prompt]: 	Training 400/553. train loss: 1.0503,	0.9680 s / batch. (data: 2.72e-04). ETA=14:36:47, max mem: 26.1 GB 
[11/21 15:23:12 visual_prompt]: 	Training 500/553. train loss: 1.1025,	0.9533 s / batch. (data: 3.01e-04). ETA=14:21:55, max mem: 26.1 GB 
[11/21 15:24:02 visual_prompt]: Epoch 2 / 100: avg data time: 5.31e-02, avg batch time: 0.9843, average train loss: 1.0187
[11/21 15:25:00 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3036, average loss: 0.8974
[11/21 15:25:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.21	
[11/21 15:25:00 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/21 15:26:47 visual_prompt]: 	Training 100/553. train loss: 0.7051,	0.9383 s / batch. (data: 8.04e-04). ETA=14:05:56, max mem: 26.1 GB 
[11/21 15:28:25 visual_prompt]: 	Training 200/553. train loss: 3.2898,	0.9238 s / batch. (data: 2.47e-04). ETA=13:51:22, max mem: 26.1 GB 
[11/21 15:30:01 visual_prompt]: 	Training 300/553. train loss: 0.6686,	0.9412 s / batch. (data: 5.85e-03). ETA=14:05:22, max mem: 26.1 GB 
[11/21 15:31:38 visual_prompt]: 	Training 400/553. train loss: 0.9099,	0.9330 s / batch. (data: 7.33e-04). ETA=13:56:29, max mem: 26.1 GB 
[11/21 15:33:14 visual_prompt]: 	Training 500/553. train loss: 1.2285,	0.9410 s / batch. (data: 5.37e-03). ETA=14:02:08, max mem: 26.1 GB 
[11/21 15:34:07 visual_prompt]: Epoch 3 / 100: avg data time: 5.96e-02, avg batch time: 0.9896, average train loss: 0.8710
[11/21 15:35:05 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3038, average loss: 0.7660
[11/21 15:35:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.16	
[11/21 15:35:05 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/21 15:36:52 visual_prompt]: 	Training 100/553. train loss: 0.8723,	0.9400 s / batch. (data: 2.92e-04). ETA=13:58:47, max mem: 26.1 GB 
[11/21 15:38:29 visual_prompt]: 	Training 200/553. train loss: 1.0055,	1.2112 s / batch. (data: 2.65e-01). ETA=17:58:45, max mem: 26.1 GB 
[11/21 15:40:05 visual_prompt]: 	Training 300/553. train loss: 1.4327,	0.9439 s / batch. (data: 5.37e-03). ETA=13:59:07, max mem: 26.1 GB 
[11/21 15:41:42 visual_prompt]: 	Training 400/553. train loss: 0.4517,	0.9412 s / batch. (data: 1.04e-02). ETA=13:55:09, max mem: 26.1 GB 
[11/21 15:43:19 visual_prompt]: 	Training 500/553. train loss: 0.9736,	0.9219 s / batch. (data: 5.39e-03). ETA=13:36:33, max mem: 26.1 GB 
[11/21 15:44:11 visual_prompt]: Epoch 4 / 100: avg data time: 5.42e-02, avg batch time: 0.9862, average train loss: 0.8388
[11/21 15:45:09 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3067, average loss: 0.7355
[11/21 15:45:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 61.02	
[11/21 15:45:09 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/21 15:46:56 visual_prompt]: 	Training 100/553. train loss: 0.5977,	0.9274 s / batch. (data: 2.88e-04). ETA=13:38:59, max mem: 26.1 GB 
[11/21 15:48:33 visual_prompt]: 	Training 200/553. train loss: 0.5090,	0.9439 s / batch. (data: 3.21e-04). ETA=13:52:03, max mem: 26.1 GB 
[11/21 15:50:07 visual_prompt]: 	Training 300/553. train loss: 1.0795,	0.9228 s / batch. (data: 5.37e-03). ETA=13:31:53, max mem: 26.1 GB 
[11/21 15:51:45 visual_prompt]: 	Training 400/553. train loss: 1.3712,	3.1983 s / batch. (data: 2.27e+00). ETA=1 day, 22:48:33, max mem: 26.1 GB 
[11/21 15:53:23 visual_prompt]: 	Training 500/553. train loss: 0.7056,	0.9606 s / batch. (data: 1.66e-02). ETA=14:01:53, max mem: 26.1 GB 
[11/21 15:54:12 visual_prompt]: Epoch 5 / 100: avg data time: 5.03e-02, avg batch time: 0.9819, average train loss: 0.8059
[11/21 15:55:10 visual_prompt]: Inference (val):avg data time: 2.26e-04, avg batch time: 0.3059, average loss: 0.6884
[11/21 15:55:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 60.88	
[11/21 15:55:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/21 15:56:58 visual_prompt]: 	Training 100/553. train loss: 0.6809,	0.9360 s / batch. (data: 2.81e-04). ETA=13:37:59, max mem: 26.1 GB 
[11/21 15:58:32 visual_prompt]: 	Training 200/553. train loss: 0.8200,	0.9237 s / batch. (data: 3.28e-04). ETA=13:25:40, max mem: 26.1 GB 
[11/21 16:00:09 visual_prompt]: 	Training 300/553. train loss: 1.0878,	0.9472 s / batch. (data: 2.07e-02). ETA=13:44:34, max mem: 26.1 GB 
[11/21 16:01:45 visual_prompt]: 	Training 400/553. train loss: 0.7240,	1.9600 s / batch. (data: 1.04e+00). ETA=1 day, 4:23:03, max mem: 26.1 GB 
[11/21 16:03:24 visual_prompt]: 	Training 500/553. train loss: 0.8336,	0.9321 s / batch. (data: 1.09e-02). ETA=13:28:22, max mem: 26.1 GB 
[11/21 16:04:13 visual_prompt]: Epoch 6 / 100: avg data time: 4.95e-02, avg batch time: 0.9816, average train loss: 0.8084
[11/21 16:05:12 visual_prompt]: Inference (val):avg data time: 3.77e-04, avg batch time: 0.3048, average loss: 0.6746
[11/21 16:05:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 59.67	
[11/21 16:05:12 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/21 16:07:03 visual_prompt]: 	Training 100/553. train loss: 0.7405,	0.9164 s / batch. (data: 4.55e-04). ETA=13:12:24, max mem: 26.1 GB 
[11/21 16:08:39 visual_prompt]: 	Training 200/553. train loss: 0.4985,	0.9561 s / batch. (data: 2.46e-04). ETA=13:45:06, max mem: 26.1 GB 
[11/21 16:10:14 visual_prompt]: 	Training 300/553. train loss: 0.7450,	0.9340 s / batch. (data: 5.39e-03). ETA=13:24:31, max mem: 26.1 GB 
[11/21 16:11:49 visual_prompt]: 	Training 400/553. train loss: 0.6797,	0.9657 s / batch. (data: 2.83e-04). ETA=13:50:11, max mem: 26.1 GB 
[11/21 16:13:25 visual_prompt]: 	Training 500/553. train loss: 0.5659,	0.9310 s / batch. (data: 5.36e-03). ETA=13:18:48, max mem: 26.1 GB 
[11/21 16:14:15 visual_prompt]: Epoch 7 / 100: avg data time: 4.96e-02, avg batch time: 0.9812, average train loss: 0.7779
[11/21 16:15:12 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3047, average loss: 0.6812
[11/21 16:15:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.47	
[11/21 16:15:12 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/21 16:16:59 visual_prompt]: 	Training 100/553. train loss: 0.5917,	0.9392 s / batch. (data: 1.52e-02). ETA=13:23:30, max mem: 26.1 GB 
[11/21 16:18:36 visual_prompt]: 	Training 200/553. train loss: 0.9095,	0.9318 s / batch. (data: 2.71e-04). ETA=13:15:34, max mem: 26.1 GB 
[11/21 16:20:14 visual_prompt]: 	Training 300/553. train loss: 0.8412,	0.9411 s / batch. (data: 1.14e-02). ETA=13:21:56, max mem: 26.1 GB 
[11/21 16:21:52 visual_prompt]: 	Training 400/553. train loss: 0.6306,	0.9320 s / batch. (data: 2.69e-04). ETA=13:12:37, max mem: 26.1 GB 
[11/21 16:23:28 visual_prompt]: 	Training 500/553. train loss: 0.5578,	0.9290 s / batch. (data: 2.50e-04). ETA=13:08:32, max mem: 26.1 GB 
[11/21 16:24:18 visual_prompt]: Epoch 8 / 100: avg data time: 5.44e-02, avg batch time: 0.9860, average train loss: 0.7921
[11/21 16:25:15 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3059, average loss: 0.6922
[11/21 16:25:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.69	
[11/21 16:25:15 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/21 16:27:05 visual_prompt]: 	Training 100/553. train loss: 0.8306,	0.9160 s / batch. (data: 2.57e-04). ETA=12:55:11, max mem: 26.1 GB 
[11/21 16:28:42 visual_prompt]: 	Training 200/553. train loss: 1.2101,	0.9178 s / batch. (data: 3.17e-04). ETA=12:55:12, max mem: 26.1 GB 
[11/21 16:30:16 visual_prompt]: 	Training 300/553. train loss: 0.7033,	0.9281 s / batch. (data: 2.73e-04). ETA=13:02:19, max mem: 26.1 GB 
[11/21 16:31:51 visual_prompt]: 	Training 400/553. train loss: 0.8009,	0.9310 s / batch. (data: 2.96e-04). ETA=13:03:11, max mem: 26.1 GB 
[11/21 16:33:25 visual_prompt]: 	Training 500/553. train loss: 0.6448,	0.9448 s / batch. (data: 1.55e-02). ETA=13:13:17, max mem: 26.1 GB 
[11/21 16:34:16 visual_prompt]: Epoch 9 / 100: avg data time: 4.55e-02, avg batch time: 0.9774, average train loss: 0.7883
[11/21 16:35:14 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3032, average loss: 0.7025
[11/21 16:35:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 57.99	
[11/21 16:35:14 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/21 16:37:01 visual_prompt]: 	Training 100/553. train loss: 0.8561,	0.9360 s / batch. (data: 2.77e-04). ETA=13:03:28, max mem: 26.1 GB 
[11/21 16:38:39 visual_prompt]: 	Training 200/553. train loss: 0.7142,	0.9387 s / batch. (data: 1.05e-02). ETA=13:04:11, max mem: 26.1 GB 
[11/21 16:40:14 visual_prompt]: 	Training 300/553. train loss: 0.7298,	0.9238 s / batch. (data: 2.66e-04). ETA=12:50:09, max mem: 26.1 GB 
[11/21 16:41:48 visual_prompt]: 	Training 400/553. train loss: 0.6147,	0.9519 s / batch. (data: 3.96e-03). ETA=13:12:03, max mem: 26.1 GB 
[11/21 16:43:25 visual_prompt]: 	Training 500/553. train loss: 0.6698,	0.9360 s / batch. (data: 3.15e-04). ETA=12:57:15, max mem: 26.1 GB 
[11/21 16:44:16 visual_prompt]: Epoch 10 / 100: avg data time: 4.77e-02, avg batch time: 0.9798, average train loss: 0.7526
[11/21 16:45:14 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3046, average loss: 0.7684
[11/21 16:45:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.76	
[11/21 16:45:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/21 16:47:04 visual_prompt]: 	Training 100/553. train loss: 0.9897,	0.9491 s / batch. (data: 2.15e-02). ETA=13:05:40, max mem: 26.1 GB 
[11/21 16:48:39 visual_prompt]: 	Training 200/553. train loss: 0.8338,	0.9303 s / batch. (data: 5.39e-03). ETA=12:48:34, max mem: 26.1 GB 
[11/21 16:50:13 visual_prompt]: 	Training 300/553. train loss: 0.4827,	0.9320 s / batch. (data: 2.47e-04). ETA=12:48:26, max mem: 26.1 GB 
[11/21 16:51:49 visual_prompt]: 	Training 400/553. train loss: 0.9774,	0.9441 s / batch. (data: 2.59e-04). ETA=12:56:47, max mem: 26.1 GB 
[11/21 16:53:24 visual_prompt]: 	Training 500/553. train loss: 0.5547,	0.9390 s / batch. (data: 1.55e-02). ETA=12:51:03, max mem: 26.1 GB 
[11/21 16:54:14 visual_prompt]: Epoch 11 / 100: avg data time: 4.38e-02, avg batch time: 0.9763, average train loss: 0.7400
[11/21 16:55:12 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3047, average loss: 0.6941
[11/21 16:55:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.41	rocauc: 55.35	
[11/21 16:55:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/21 16:56:55 visual_prompt]: 	Training 100/553. train loss: 0.5623,	0.9320 s / batch. (data: 2.52e-04). ETA=12:42:58, max mem: 26.1 GB 
[11/21 16:58:36 visual_prompt]: 	Training 200/553. train loss: 0.8473,	0.9349 s / batch. (data: 3.89e-04). ETA=12:43:46, max mem: 26.1 GB 
[11/21 17:00:11 visual_prompt]: 	Training 300/553. train loss: 0.8645,	0.9336 s / batch. (data: 3.88e-03). ETA=12:41:10, max mem: 26.1 GB 
[11/21 17:01:47 visual_prompt]: 	Training 400/553. train loss: 1.0744,	0.9494 s / batch. (data: 7.55e-04). ETA=12:52:24, max mem: 26.1 GB 
[11/21 17:03:24 visual_prompt]: 	Training 500/553. train loss: 0.6659,	0.9560 s / batch. (data: 7.34e-04). ETA=12:56:12, max mem: 26.1 GB 
[11/21 17:04:15 visual_prompt]: Epoch 12 / 100: avg data time: 4.90e-02, avg batch time: 0.9808, average train loss: 0.7675
[11/21 17:05:12 visual_prompt]: Inference (val):avg data time: 3.98e-04, avg batch time: 0.3035, average loss: 1.0217
[11/21 17:05:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.28	
[11/21 17:05:12 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/21 17:07:00 visual_prompt]: 	Training 100/553. train loss: 0.6570,	0.9490 s / batch. (data: 2.50e-04). ETA=12:48:07, max mem: 26.1 GB 
[11/21 17:08:36 visual_prompt]: 	Training 200/553. train loss: 0.8170,	0.9159 s / batch. (data: 1.04e-02). ETA=12:19:47, max mem: 26.1 GB 
[11/21 17:10:13 visual_prompt]: 	Training 300/553. train loss: 0.6601,	0.9562 s / batch. (data: 2.65e-04). ETA=12:50:46, max mem: 26.1 GB 
[11/21 17:11:51 visual_prompt]: 	Training 400/553. train loss: 0.6985,	0.9495 s / batch. (data: 4.01e-04). ETA=12:43:45, max mem: 26.1 GB 
[11/21 17:13:26 visual_prompt]: 	Training 500/553. train loss: 0.8042,	0.9400 s / batch. (data: 2.57e-04). ETA=12:34:32, max mem: 26.1 GB 
[11/21 17:14:16 visual_prompt]: Epoch 13 / 100: avg data time: 5.20e-02, avg batch time: 0.9817, average train loss: 0.7440
[11/21 17:15:14 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3052, average loss: 0.7179
[11/21 17:15:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.32	
[11/21 17:15:14 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/21 17:17:02 visual_prompt]: 	Training 100/553. train loss: 0.6693,	0.9042 s / batch. (data: 3.04e-04). ETA=12:03:29, max mem: 26.1 GB 
[11/21 17:18:41 visual_prompt]: 	Training 200/553. train loss: 0.3790,	0.9113 s / batch. (data: 7.41e-04). ETA=12:07:40, max mem: 26.1 GB 
[11/21 17:20:17 visual_prompt]: 	Training 300/553. train loss: 0.7460,	0.9240 s / batch. (data: 3.02e-04). ETA=12:16:17, max mem: 26.1 GB 
[11/21 17:21:51 visual_prompt]: 	Training 400/553. train loss: 0.9635,	0.9520 s / batch. (data: 4.34e-03). ETA=12:36:58, max mem: 26.1 GB 
[11/21 17:23:27 visual_prompt]: 	Training 500/553. train loss: 0.7120,	0.9224 s / batch. (data: 5.41e-03). ETA=12:11:57, max mem: 26.1 GB 
[11/21 17:24:17 visual_prompt]: Epoch 14 / 100: avg data time: 5.10e-02, avg batch time: 0.9815, average train loss: 0.7402
[11/21 17:25:15 visual_prompt]: Inference (val):avg data time: 8.51e-05, avg batch time: 0.3065, average loss: 0.6992
[11/21 17:25:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.00	
[11/21 17:25:15 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/21 17:27:05 visual_prompt]: 	Training 100/553. train loss: 0.7573,	0.9058 s / batch. (data: 2.64e-04). ETA=11:56:28, max mem: 26.1 GB 
[11/21 17:28:41 visual_prompt]: 	Training 200/553. train loss: 0.6727,	0.9437 s / batch. (data: 7.25e-04). ETA=12:24:51, max mem: 26.1 GB 
[11/21 17:30:17 visual_prompt]: 	Training 300/553. train loss: 0.9559,	0.9568 s / batch. (data: 2.77e-04). ETA=12:33:38, max mem: 26.1 GB 
[11/21 17:31:53 visual_prompt]: 	Training 400/553. train loss: 0.5846,	0.9366 s / batch. (data: 2.57e-04). ETA=12:16:10, max mem: 26.1 GB 
[11/21 17:33:29 visual_prompt]: 	Training 500/553. train loss: 0.8937,	0.9241 s / batch. (data: 5.37e-03). ETA=12:04:44, max mem: 26.1 GB 
[11/21 17:34:19 visual_prompt]: Epoch 15 / 100: avg data time: 5.13e-02, avg batch time: 0.9821, average train loss: 0.7305
[11/21 17:35:16 visual_prompt]: Inference (val):avg data time: 2.83e-04, avg batch time: 0.3047, average loss: 0.7404
[11/21 17:35:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.63	
[11/21 17:35:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/21 17:37:02 visual_prompt]: 	Training 100/553. train loss: 0.7449,	0.9120 s / batch. (data: 2.66e-04). ETA=11:52:58, max mem: 26.1 GB 
[11/21 17:38:36 visual_prompt]: 	Training 200/553. train loss: 0.6490,	0.9217 s / batch. (data: 5.69e-03). ETA=11:59:00, max mem: 26.1 GB 
[11/21 17:40:17 visual_prompt]: 	Training 300/553. train loss: 0.9165,	0.9391 s / batch. (data: 1.04e-02). ETA=12:11:02, max mem: 26.1 GB 
[11/21 17:41:53 visual_prompt]: 	Training 400/553. train loss: 0.6079,	0.9305 s / batch. (data: 4.01e-03). ETA=12:02:45, max mem: 26.1 GB 
[11/21 17:43:29 visual_prompt]: 	Training 500/553. train loss: 0.6887,	0.9596 s / batch. (data: 2.24e-02). ETA=12:23:46, max mem: 26.1 GB 
[11/21 17:44:19 visual_prompt]: Epoch 16 / 100: avg data time: 5.02e-02, avg batch time: 0.9819, average train loss: 0.7209
[11/21 17:45:17 visual_prompt]: Inference (val):avg data time: 1.17e-04, avg batch time: 0.3034, average loss: 0.7895
[11/21 17:45:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.49	
[11/21 17:45:17 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/21 17:47:00 visual_prompt]: 	Training 100/553. train loss: 0.8028,	0.9479 s / batch. (data: 1.04e-02). ETA=12:12:17, max mem: 26.1 GB 
[11/21 17:48:41 visual_prompt]: 	Training 200/553. train loss: 0.7898,	1.0008 s / batch. (data: 1.63e-02). ETA=12:51:30, max mem: 26.1 GB 
[11/21 17:50:15 visual_prompt]: 	Training 300/553. train loss: 1.2146,	0.9149 s / batch. (data: 2.90e-04). ETA=11:43:44, max mem: 26.1 GB 
[11/21 17:51:52 visual_prompt]: 	Training 400/553. train loss: 0.6952,	0.9600 s / batch. (data: 7.99e-03). ETA=12:16:47, max mem: 26.1 GB 
[11/21 17:53:28 visual_prompt]: 	Training 500/553. train loss: 0.7378,	0.9120 s / batch. (data: 2.54e-04). ETA=11:38:28, max mem: 26.1 GB 
[11/21 17:54:18 visual_prompt]: Epoch 17 / 100: avg data time: 4.62e-02, avg batch time: 0.9778, average train loss: 0.7214
[11/21 17:55:17 visual_prompt]: Inference (val):avg data time: 2.40e-04, avg batch time: 0.3051, average loss: 0.6793
[11/21 17:55:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 57.68	
[11/21 17:55:17 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/21 17:57:07 visual_prompt]: 	Training 100/553. train loss: 0.6786,	0.9270 s / batch. (data: 2.50e-04). ETA=11:47:37, max mem: 26.1 GB 
[11/21 17:58:43 visual_prompt]: 	Training 200/553. train loss: 0.7041,	0.9210 s / batch. (data: 1.04e-02). ETA=11:41:29, max mem: 26.1 GB 
[11/21 18:00:17 visual_prompt]: 	Training 300/553. train loss: 0.7593,	0.9263 s / batch. (data: 3.24e-04). ETA=11:43:58, max mem: 26.1 GB 
[11/21 18:01:52 visual_prompt]: 	Training 400/553. train loss: 0.7287,	0.9600 s / batch. (data: 2.75e-04). ETA=12:07:58, max mem: 26.1 GB 
[11/21 18:03:29 visual_prompt]: 	Training 500/553. train loss: 0.7040,	0.9560 s / batch. (data: 2.52e-04). ETA=12:03:21, max mem: 26.1 GB 
[11/21 18:04:19 visual_prompt]: Epoch 18 / 100: avg data time: 4.86e-02, avg batch time: 0.9799, average train loss: 0.7189
[11/21 18:05:16 visual_prompt]: Inference (val):avg data time: 1.39e-04, avg batch time: 0.3037, average loss: 0.7570
[11/21 18:05:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.29	
[11/21 18:05:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/21 18:07:03 visual_prompt]: 	Training 100/553. train loss: 0.6978,	0.9320 s / batch. (data: 8.00e-03). ETA=11:42:48, max mem: 26.1 GB 
[11/21 18:08:41 visual_prompt]: 	Training 200/553. train loss: 0.9509,	2.3963 s / batch. (data: 1.46e+00). ETA=1 day, 6:03:01, max mem: 26.1 GB 
[11/21 18:10:17 visual_prompt]: 	Training 300/553. train loss: 0.7024,	0.9232 s / batch. (data: 2.79e-04). ETA=11:33:06, max mem: 26.1 GB 
[11/21 18:11:53 visual_prompt]: 	Training 400/553. train loss: 0.6942,	0.9360 s / batch. (data: 2.97e-04). ETA=11:41:10, max mem: 26.1 GB 
[11/21 18:13:30 visual_prompt]: 	Training 500/553. train loss: 0.4323,	0.9524 s / batch. (data: 5.81e-03). ETA=11:51:50, max mem: 26.1 GB 
[11/21 18:14:20 visual_prompt]: Epoch 19 / 100: avg data time: 4.99e-02, avg batch time: 0.9827, average train loss: 0.7150
[11/21 18:15:18 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3044, average loss: 0.8877
[11/21 18:15:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.18	
[11/21 18:15:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/21 18:17:08 visual_prompt]: 	Training 100/553. train loss: 0.6600,	0.9046 s / batch. (data: 5.39e-03). ETA=11:13:49, max mem: 26.1 GB 
[11/21 18:18:41 visual_prompt]: 	Training 200/553. train loss: 0.6995,	0.9237 s / batch. (data: 7.97e-03). ETA=11:26:29, max mem: 26.1 GB 
[11/21 18:20:18 visual_prompt]: 	Training 300/553. train loss: 0.6521,	0.9360 s / batch. (data: 2.48e-04). ETA=11:34:05, max mem: 26.1 GB 
[11/21 18:21:55 visual_prompt]: 	Training 400/553. train loss: 0.7315,	0.9519 s / batch. (data: 7.45e-04). ETA=11:44:19, max mem: 26.1 GB 
[11/21 18:23:30 visual_prompt]: 	Training 500/553. train loss: 0.7553,	0.9560 s / batch. (data: 7.94e-03). ETA=11:45:44, max mem: 26.1 GB 
[11/21 18:24:23 visual_prompt]: Epoch 20 / 100: avg data time: 5.36e-02, avg batch time: 0.9851, average train loss: 0.7215
[11/21 18:25:21 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3046, average loss: 0.6894
[11/21 18:25:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.92	
[11/21 18:25:21 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/21 18:27:10 visual_prompt]: 	Training 100/553. train loss: 0.6937,	0.9188 s / batch. (data: 1.04e-02). ETA=11:15:56, max mem: 26.1 GB 
[11/21 18:28:49 visual_prompt]: 	Training 200/553. train loss: 0.6770,	0.9800 s / batch. (data: 7.73e-04). ETA=11:59:18, max mem: 26.1 GB 
[11/21 18:30:25 visual_prompt]: 	Training 300/553. train loss: 0.6732,	0.9276 s / batch. (data: 2.76e-04). ETA=11:19:19, max mem: 26.1 GB 
[11/21 18:32:01 visual_prompt]: 	Training 400/553. train loss: 1.0009,	0.9308 s / batch. (data: 2.26e-04). ETA=11:20:04, max mem: 26.1 GB 
[11/21 18:33:35 visual_prompt]: 	Training 500/553. train loss: 0.6297,	0.9360 s / batch. (data: 2.69e-04). ETA=11:22:19, max mem: 26.1 GB 
[11/21 18:34:24 visual_prompt]: Epoch 21 / 100: avg data time: 5.22e-02, avg batch time: 0.9822, average train loss: 0.7052
[11/21 18:35:22 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3019, average loss: 0.6849
[11/21 18:35:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 56.11	
[11/21 18:35:22 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/21 18:37:08 visual_prompt]: 	Training 100/553. train loss: 0.6818,	0.9302 s / batch. (data: 5.37e-03). ETA=11:15:43, max mem: 26.1 GB 
[11/21 18:38:47 visual_prompt]: 	Training 200/553. train loss: 0.8930,	0.9268 s / batch. (data: 2.92e-04). ETA=11:11:43, max mem: 26.1 GB 
[11/21 18:40:24 visual_prompt]: 	Training 300/553. train loss: 0.5973,	0.9385 s / batch. (data: 1.09e-02). ETA=11:18:39, max mem: 26.1 GB 
[11/21 18:41:58 visual_prompt]: 	Training 400/553. train loss: 0.8479,	0.9560 s / batch. (data: 3.07e-04). ETA=11:29:40, max mem: 26.1 GB 
[11/21 18:43:33 visual_prompt]: 	Training 500/553. train loss: 0.7730,	0.9280 s / batch. (data: 2.68e-04). ETA=11:07:58, max mem: 26.1 GB 
[11/21 18:44:25 visual_prompt]: Epoch 22 / 100: avg data time: 4.94e-02, avg batch time: 0.9804, average train loss: 0.7098
[11/21 18:45:23 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.3052, average loss: 0.7303
[11/21 18:45:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.68	
[11/21 18:45:23 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/21 18:47:13 visual_prompt]: 	Training 100/553. train loss: 0.6959,	0.9048 s / batch. (data: 2.50e-04). ETA=10:48:56, max mem: 26.1 GB 
[11/21 18:48:49 visual_prompt]: 	Training 200/553. train loss: 0.6556,	0.9364 s / batch. (data: 1.36e-02). ETA=11:10:05, max mem: 26.1 GB 
[11/21 18:50:25 visual_prompt]: 	Training 300/553. train loss: 0.3672,	0.9490 s / batch. (data: 1.05e-02). ETA=11:17:29, max mem: 26.1 GB 
[11/21 18:52:02 visual_prompt]: 	Training 400/553. train loss: 0.8594,	0.9335 s / batch. (data: 2.56e-04). ETA=11:04:51, max mem: 26.1 GB 
[11/21 18:53:37 visual_prompt]: 	Training 500/553. train loss: 0.4517,	0.9224 s / batch. (data: 5.40e-03). ETA=10:55:24, max mem: 26.1 GB 
[11/21 18:54:30 visual_prompt]: Epoch 23 / 100: avg data time: 5.89e-02, avg batch time: 0.9902, average train loss: 0.7093
[11/21 18:55:29 visual_prompt]: Inference (val):avg data time: 2.34e-04, avg batch time: 0.3039, average loss: 0.8868
[11/21 18:55:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.14	
[11/21 18:55:29 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/21 18:57:14 visual_prompt]: 	Training 100/553. train loss: 0.6879,	0.9381 s / batch. (data: 1.04e-02). ETA=11:04:10, max mem: 26.1 GB 
[11/21 18:58:51 visual_prompt]: 	Training 200/553. train loss: 0.6946,	0.9600 s / batch. (data: 2.56e-04). ETA=11:18:07, max mem: 26.1 GB 
[11/21 19:00:29 visual_prompt]: 	Training 300/553. train loss: 0.7028,	0.9588 s / batch. (data: 1.56e-02). ETA=11:15:38, max mem: 26.1 GB 
[11/21 19:02:05 visual_prompt]: 	Training 400/553. train loss: 0.6998,	0.9334 s / batch. (data: 5.39e-03). ETA=10:56:12, max mem: 26.1 GB 
[11/21 19:03:43 visual_prompt]: 	Training 500/553. train loss: 0.7033,	0.9268 s / batch. (data: 1.20e-02). ETA=10:50:02, max mem: 26.1 GB 
[11/21 19:04:33 visual_prompt]: Epoch 24 / 100: avg data time: 5.27e-02, avg batch time: 0.9835, average train loss: 0.7286
[11/21 19:05:30 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3049, average loss: 0.7308
[11/21 19:05:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.03	
[11/21 19:05:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/21 19:07:13 visual_prompt]: 	Training 100/553. train loss: 0.6032,	0.9640 s / batch. (data: 3.97e-03). ETA=11:13:39, max mem: 26.1 GB 
[11/21 19:08:54 visual_prompt]: 	Training 200/553. train loss: 0.4546,	0.9720 s / batch. (data: 3.95e-03). ETA=11:17:34, max mem: 26.1 GB 
[11/21 19:10:29 visual_prompt]: 	Training 300/553. train loss: 0.6869,	0.9229 s / batch. (data: 5.43e-03). ETA=10:41:49, max mem: 26.1 GB 
[11/21 19:12:06 visual_prompt]: 	Training 400/553. train loss: 0.7170,	0.9440 s / batch. (data: 2.61e-04). ETA=10:54:56, max mem: 26.1 GB 
[11/21 19:13:42 visual_prompt]: 	Training 500/553. train loss: 0.6693,	0.9233 s / batch. (data: 2.77e-04). ETA=10:39:03, max mem: 26.1 GB 
[11/21 19:14:34 visual_prompt]: Epoch 25 / 100: avg data time: 5.17e-02, avg batch time: 0.9830, average train loss: 0.7091
[11/21 19:15:32 visual_prompt]: Inference (val):avg data time: 1.38e-04, avg batch time: 0.3032, average loss: 0.7013
[11/21 19:15:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 53.62	
[11/21 19:15:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/21 19:17:19 visual_prompt]: 	Training 100/553. train loss: 0.5317,	0.9579 s / batch. (data: 5.84e-03). ETA=11:00:33, max mem: 26.1 GB 
[11/21 19:18:57 visual_prompt]: 	Training 200/553. train loss: 0.4054,	0.9400 s / batch. (data: 3.39e-04). ETA=10:46:40, max mem: 26.1 GB 
[11/21 19:20:35 visual_prompt]: 	Training 300/553. train loss: 0.6346,	0.9451 s / batch. (data: 1.55e-02). ETA=10:48:34, max mem: 26.1 GB 
[11/21 19:22:08 visual_prompt]: 	Training 400/553. train loss: 0.6825,	0.9532 s / batch. (data: 2.92e-02). ETA=10:52:33, max mem: 26.1 GB 
[11/21 19:23:44 visual_prompt]: 	Training 500/553. train loss: 0.5631,	0.9440 s / batch. (data: 2.86e-04). ETA=10:44:40, max mem: 26.1 GB 
[11/21 19:24:35 visual_prompt]: Epoch 26 / 100: avg data time: 5.07e-02, avg batch time: 0.9819, average train loss: 0.7118
[11/21 19:25:33 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3057, average loss: 0.8272
[11/21 19:25:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[11/21 19:25:33 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/21 19:27:19 visual_prompt]: 	Training 100/553. train loss: 0.7238,	0.9256 s / batch. (data: 7.90e-03). ETA=10:29:44, max mem: 26.1 GB 
[11/21 19:28:58 visual_prompt]: 	Training 200/553. train loss: 0.6948,	0.9338 s / batch. (data: 1.05e-02). ETA=10:33:45, max mem: 26.1 GB 
[11/21 19:30:32 visual_prompt]: 	Training 300/553. train loss: 0.7581,	0.9140 s / batch. (data: 2.62e-04). ETA=10:18:47, max mem: 26.1 GB 
[11/21 19:32:07 visual_prompt]: 	Training 400/553. train loss: 0.9818,	0.9607 s / batch. (data: 5.38e-03). ETA=10:48:49, max mem: 26.1 GB 
[11/21 19:33:46 visual_prompt]: 	Training 500/553. train loss: 0.8241,	0.9601 s / batch. (data: 8.09e-03). ETA=10:46:47, max mem: 26.1 GB 
[11/21 19:34:37 visual_prompt]: Epoch 27 / 100: avg data time: 5.04e-02, avg batch time: 0.9829, average train loss: 0.7070
[11/21 19:35:35 visual_prompt]: Inference (val):avg data time: 1.32e-04, avg batch time: 0.3048, average loss: 0.6984
[11/21 19:35:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 50.88	
[11/21 19:35:35 visual_prompt]: Stopping early.
[11/21 19:35:35 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 19:35:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 19:35:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/21 19:35:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 19:35:35 visual_prompt]: Training with config:
[11/21 19:35:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.001_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 19:35:35 visual_prompt]: Loading training data...
[11/21 19:35:35 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 19:35:35 visual_prompt]: Loading validation data...
[11/21 19:35:35 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 19:35:36 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 19:35:37 visual_prompt]: Enable all parameters update during training
[11/21 19:35:37 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 19:35:37 visual_prompt]: tuned percent:100.000
[11/21 19:35:37 visual_prompt]: Device used for model: 0
[11/21 19:35:37 visual_prompt]: Setting up Evaluator...
[11/21 19:35:37 visual_prompt]: Setting up Trainer...
[11/21 19:35:37 visual_prompt]: 	Setting up the optimizer...
[11/21 19:35:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 19:37:22 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9643 s / batch. (data: 1.05e-02). ETA=14:47:08, max mem: 27.1 GB 
[11/21 19:39:03 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9521 s / batch. (data: 2.44e-02). ETA=14:34:21, max mem: 27.1 GB 
[11/21 19:40:39 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9444 s / batch. (data: 5.36e-03). ETA=14:25:43, max mem: 27.1 GB 
[11/21 19:42:13 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9400 s / batch. (data: 2.77e-04). ETA=14:20:05, max mem: 27.1 GB 
[11/21 19:43:51 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9234 s / batch. (data: 2.54e-04). ETA=14:03:19, max mem: 27.1 GB 
[11/21 19:44:40 visual_prompt]: Epoch 1 / 100: avg data time: 4.85e-02, avg batch time: 0.9811, average train loss: 7.6130
[11/21 19:45:38 visual_prompt]: Inference (val):avg data time: 1.38e-04, avg batch time: 0.3049, average loss: 6.9126
[11/21 19:45:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 19:45:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/21 19:47:26 visual_prompt]: 	Training 100/553. train loss: 1.3821,	0.9452 s / batch. (data: 1.60e-02). ETA=14:20:49, max mem: 27.1 GB 
[11/21 19:49:00 visual_prompt]: 	Training 200/553. train loss: 0.9821,	1.5680 s / batch. (data: 6.22e-01). ETA=23:45:29, max mem: 27.1 GB 
[11/21 19:50:37 visual_prompt]: 	Training 300/553. train loss: 0.7339,	0.9283 s / batch. (data: 5.37e-03). ETA=14:02:23, max mem: 27.1 GB 
[11/21 19:52:15 visual_prompt]: 	Training 400/553. train loss: 1.0459,	0.9480 s / batch. (data: 2.58e-04). ETA=14:18:40, max mem: 27.1 GB 
[11/21 19:53:53 visual_prompt]: 	Training 500/553. train loss: 1.1003,	0.9406 s / batch. (data: 1.66e-02). ETA=14:10:26, max mem: 27.1 GB 
[11/21 19:54:42 visual_prompt]: Epoch 2 / 100: avg data time: 5.09e-02, avg batch time: 0.9829, average train loss: 1.0186
[11/21 19:55:40 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.3050, average loss: 0.8957
[11/21 19:55:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.87	
[11/21 19:55:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/21 19:57:27 visual_prompt]: 	Training 100/553. train loss: 0.6814,	0.9295 s / batch. (data: 1.15e-03). ETA=13:57:57, max mem: 27.1 GB 
[11/21 19:59:04 visual_prompt]: 	Training 200/553. train loss: 3.2389,	0.9436 s / batch. (data: 1.05e-02). ETA=14:09:09, max mem: 27.1 GB 
[11/21 20:00:41 visual_prompt]: 	Training 300/553. train loss: 0.6906,	0.9480 s / batch. (data: 2.83e-04). ETA=14:11:30, max mem: 27.1 GB 
[11/21 20:02:17 visual_prompt]: 	Training 400/553. train loss: 0.9153,	0.9349 s / batch. (data: 2.63e-04). ETA=13:58:09, max mem: 27.1 GB 
[11/21 20:03:52 visual_prompt]: 	Training 500/553. train loss: 1.2037,	0.9409 s / batch. (data: 7.46e-04). ETA=14:02:01, max mem: 27.1 GB 
[11/21 20:04:42 visual_prompt]: Epoch 3 / 100: avg data time: 4.69e-02, avg batch time: 0.9801, average train loss: 0.8724
[11/21 20:05:40 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3061, average loss: 0.7652
[11/21 20:05:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.96	
[11/21 20:05:40 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/21 20:07:27 visual_prompt]: 	Training 100/553. train loss: 0.8575,	0.9190 s / batch. (data: 2.90e-04). ETA=13:40:03, max mem: 27.1 GB 
[11/21 20:09:02 visual_prompt]: 	Training 200/553. train loss: 1.0000,	0.9154 s / batch. (data: 5.40e-03). ETA=13:35:20, max mem: 27.1 GB 
[11/21 20:10:39 visual_prompt]: 	Training 300/553. train loss: 1.4151,	0.9398 s / batch. (data: 7.53e-03). ETA=13:55:28, max mem: 27.1 GB 
[11/21 20:12:16 visual_prompt]: 	Training 400/553. train loss: 0.4533,	0.9367 s / batch. (data: 7.95e-03). ETA=13:51:08, max mem: 27.1 GB 
[11/21 20:13:53 visual_prompt]: 	Training 500/553. train loss: 0.9945,	0.9596 s / batch. (data: 1.68e-02). ETA=14:09:51, max mem: 27.1 GB 
[11/21 20:14:45 visual_prompt]: Epoch 4 / 100: avg data time: 5.24e-02, avg batch time: 0.9849, average train loss: 0.8375
[11/21 20:15:43 visual_prompt]: Inference (val):avg data time: 7.39e-04, avg batch time: 0.3053, average loss: 0.7399
[11/21 20:15:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 60.44	
[11/21 20:15:43 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/21 20:17:29 visual_prompt]: 	Training 100/553. train loss: 0.6238,	0.9120 s / batch. (data: 2.64e-04). ETA=13:25:24, max mem: 27.1 GB 
[11/21 20:19:07 visual_prompt]: 	Training 200/553. train loss: 0.4982,	0.9340 s / batch. (data: 8.02e-03). ETA=13:43:15, max mem: 27.1 GB 
[11/21 20:20:42 visual_prompt]: 	Training 300/553. train loss: 1.0932,	0.9238 s / batch. (data: 1.04e-02). ETA=13:32:46, max mem: 27.1 GB 
[11/21 20:22:19 visual_prompt]: 	Training 400/553. train loss: 1.3641,	2.8781 s / batch. (data: 1.95e+00). ETA=1 day, 18:07:22, max mem: 27.1 GB 
[11/21 20:23:57 visual_prompt]: 	Training 500/553. train loss: 0.6989,	0.9207 s / batch. (data: 1.04e-02). ETA=13:26:57, max mem: 27.1 GB 
[11/21 20:24:47 visual_prompt]: Epoch 5 / 100: avg data time: 5.22e-02, avg batch time: 0.9827, average train loss: 0.8048
[11/21 20:25:45 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3052, average loss: 0.7277
[11/21 20:25:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.70	
[11/21 20:25:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/21 20:27:33 visual_prompt]: 	Training 100/553. train loss: 0.7199,	0.9166 s / batch. (data: 3.12e-04). ETA=13:20:59, max mem: 27.1 GB 
[11/21 20:29:08 visual_prompt]: 	Training 200/553. train loss: 0.8584,	0.9570 s / batch. (data: 3.99e-03). ETA=13:54:44, max mem: 27.1 GB 
[11/21 20:30:45 visual_prompt]: 	Training 300/553. train loss: 1.0704,	0.9567 s / batch. (data: 1.55e-02). ETA=13:52:53, max mem: 27.1 GB 
[11/21 20:32:20 visual_prompt]: 	Training 400/553. train loss: 0.6795,	0.9360 s / batch. (data: 2.97e-04). ETA=13:33:18, max mem: 27.1 GB 
[11/21 20:33:58 visual_prompt]: 	Training 500/553. train loss: 0.8588,	0.9400 s / batch. (data: 2.80e-04). ETA=13:35:12, max mem: 27.1 GB 
[11/21 20:34:48 visual_prompt]: Epoch 6 / 100: avg data time: 4.73e-02, avg batch time: 0.9806, average train loss: 0.7991
[11/21 20:35:46 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3066, average loss: 0.6681
[11/21 20:35:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 61.42	
[11/21 20:35:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/21 20:37:37 visual_prompt]: 	Training 100/553. train loss: 0.7152,	0.9160 s / batch. (data: 2.64e-04). ETA=13:12:03, max mem: 27.1 GB 
[11/21 20:39:12 visual_prompt]: 	Training 200/553. train loss: 0.5179,	0.9142 s / batch. (data: 7.23e-04). ETA=13:08:57, max mem: 27.1 GB 
[11/21 20:40:47 visual_prompt]: 	Training 300/553. train loss: 0.7661,	0.9440 s / batch. (data: 2.51e-03). ETA=13:33:07, max mem: 27.1 GB 
[11/21 20:42:21 visual_prompt]: 	Training 400/553. train loss: 0.6877,	0.9501 s / batch. (data: 5.50e-03). ETA=13:36:49, max mem: 27.1 GB 
[11/21 20:43:59 visual_prompt]: 	Training 500/553. train loss: 0.6076,	0.9400 s / batch. (data: 2.56e-04). ETA=13:26:33, max mem: 27.1 GB 
[11/21 20:44:48 visual_prompt]: Epoch 7 / 100: avg data time: 4.91e-02, avg batch time: 0.9806, average train loss: 0.7796
[11/21 20:45:46 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3051, average loss: 0.6786
[11/21 20:45:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 58.36	
[11/21 20:45:46 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/21 20:47:35 visual_prompt]: 	Training 100/553. train loss: 0.5862,	0.9782 s / batch. (data: 5.82e-03). ETA=13:56:49, max mem: 27.1 GB 
[11/21 20:49:14 visual_prompt]: 	Training 200/553. train loss: 0.7173,	0.9261 s / batch. (data: 3.56e-04). ETA=13:10:41, max mem: 27.1 GB 
[11/21 20:50:50 visual_prompt]: 	Training 300/553. train loss: 0.7461,	0.9468 s / batch. (data: 1.09e-02). ETA=13:26:50, max mem: 27.1 GB 
[11/21 20:52:28 visual_prompt]: 	Training 400/553. train loss: 0.6209,	0.9654 s / batch. (data: 8.00e-03). ETA=13:41:03, max mem: 27.1 GB 
[11/21 20:54:05 visual_prompt]: 	Training 500/553. train loss: 0.5542,	0.9211 s / batch. (data: 5.65e-03). ETA=13:01:52, max mem: 27.1 GB 
[11/21 20:54:55 visual_prompt]: Epoch 8 / 100: avg data time: 6.04e-02, avg batch time: 0.9918, average train loss: 0.7787
[11/21 20:55:53 visual_prompt]: Inference (val):avg data time: 2.47e-04, avg batch time: 0.3036, average loss: 0.6814
[11/21 20:55:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.61	
[11/21 20:55:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/21 20:57:43 visual_prompt]: 	Training 100/553. train loss: 0.9599,	0.9360 s / batch. (data: 2.56e-04). ETA=13:12:06, max mem: 27.1 GB 
[11/21 20:59:20 visual_prompt]: 	Training 200/553. train loss: 1.1409,	0.9199 s / batch. (data: 2.99e-04). ETA=12:56:56, max mem: 27.1 GB 
[11/21 21:00:55 visual_prompt]: 	Training 300/553. train loss: 0.6248,	0.9525 s / batch. (data: 5.37e-03). ETA=13:22:54, max mem: 27.1 GB 
[11/21 21:02:30 visual_prompt]: 	Training 400/553. train loss: 0.7206,	0.9588 s / batch. (data: 1.60e-02). ETA=13:26:34, max mem: 27.1 GB 
[11/21 21:04:04 visual_prompt]: 	Training 500/553. train loss: 0.5469,	0.9440 s / batch. (data: 2.74e-04). ETA=13:12:35, max mem: 27.1 GB 
[11/21 21:04:55 visual_prompt]: Epoch 9 / 100: avg data time: 4.80e-02, avg batch time: 0.9801, average train loss: 0.7837
[11/21 21:05:53 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3039, average loss: 0.7514
[11/21 21:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 57.36	
[11/21 21:05:53 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/21 21:07:41 visual_prompt]: 	Training 100/553. train loss: 0.8688,	0.9520 s / batch. (data: 2.82e-04). ETA=13:16:51, max mem: 27.1 GB 
[11/21 21:09:20 visual_prompt]: 	Training 200/553. train loss: 0.7610,	0.9193 s / batch. (data: 8.38e-03). ETA=12:47:55, max mem: 27.1 GB 
[11/21 21:10:54 visual_prompt]: 	Training 300/553. train loss: 0.6997,	0.9403 s / batch. (data: 1.05e-02). ETA=13:03:57, max mem: 27.1 GB 
[11/21 21:12:29 visual_prompt]: 	Training 400/553. train loss: 0.6713,	0.9415 s / batch. (data: 5.39e-03). ETA=13:03:23, max mem: 27.1 GB 
[11/21 21:14:06 visual_prompt]: 	Training 500/553. train loss: 0.6940,	0.9189 s / batch. (data: 2.66e-04). ETA=12:43:00, max mem: 27.1 GB 
[11/21 21:14:57 visual_prompt]: Epoch 10 / 100: avg data time: 5.11e-02, avg batch time: 0.9834, average train loss: 0.7554
[11/21 21:15:56 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3056, average loss: 0.8037
[11/21 21:15:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.26	
[11/21 21:15:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/21 21:17:45 visual_prompt]: 	Training 100/553. train loss: 0.9984,	0.9339 s / batch. (data: 5.36e-03). ETA=12:53:04, max mem: 27.1 GB 
[11/21 21:19:21 visual_prompt]: 	Training 200/553. train loss: 0.8098,	0.9417 s / batch. (data: 9.62e-03). ETA=12:58:02, max mem: 27.1 GB 
[11/21 21:20:55 visual_prompt]: 	Training 300/553. train loss: 0.4962,	0.9357 s / batch. (data: 1.72e-02). ETA=12:51:30, max mem: 27.1 GB 
[11/21 21:22:31 visual_prompt]: 	Training 400/553. train loss: 0.8099,	0.9316 s / batch. (data: 2.55e-04). ETA=12:46:31, max mem: 27.1 GB 
[11/21 21:24:08 visual_prompt]: 	Training 500/553. train loss: 0.5335,	0.9519 s / batch. (data: 8.01e-03). ETA=13:01:41, max mem: 27.1 GB 
[11/21 21:24:59 visual_prompt]: Epoch 11 / 100: avg data time: 5.06e-02, avg batch time: 0.9820, average train loss: 0.7336
[11/21 21:25:57 visual_prompt]: Inference (val):avg data time: 2.43e-04, avg batch time: 0.3069, average loss: 0.6839
[11/21 21:25:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 57.47	
[11/21 21:25:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/21 21:27:40 visual_prompt]: 	Training 100/553. train loss: 0.5041,	0.9560 s / batch. (data: 2.62e-04). ETA=13:02:36, max mem: 27.1 GB 
[11/21 21:29:21 visual_prompt]: 	Training 200/553. train loss: 0.8378,	0.9240 s / batch. (data: 2.54e-04). ETA=12:34:51, max mem: 27.1 GB 
[11/21 21:30:57 visual_prompt]: 	Training 300/553. train loss: 0.8381,	0.9360 s / batch. (data: 2.68e-04). ETA=12:43:07, max mem: 27.1 GB 
[11/21 21:32:33 visual_prompt]: 	Training 400/553. train loss: 1.0070,	0.9299 s / batch. (data: 5.58e-03). ETA=12:36:36, max mem: 27.1 GB 
[11/21 21:34:09 visual_prompt]: 	Training 500/553. train loss: 0.6541,	0.9433 s / batch. (data: 5.83e-03). ETA=12:45:55, max mem: 27.1 GB 
[11/21 21:35:01 visual_prompt]: Epoch 12 / 100: avg data time: 5.19e-02, avg batch time: 0.9836, average train loss: 0.7571
[11/21 21:35:59 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3025, average loss: 1.0201
[11/21 21:35:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.13	
[11/21 21:35:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/21 21:37:46 visual_prompt]: 	Training 100/553. train loss: 0.6235,	0.9222 s / batch. (data: 3.01e-04). ETA=12:26:25, max mem: 27.1 GB 
[11/21 21:39:21 visual_prompt]: 	Training 200/553. train loss: 0.9202,	0.9297 s / batch. (data: 5.36e-03). ETA=12:30:58, max mem: 27.1 GB 
[11/21 21:40:59 visual_prompt]: 	Training 300/553. train loss: 0.6266,	0.9116 s / batch. (data: 2.66e-04). ETA=12:14:49, max mem: 27.1 GB 
[11/21 21:42:47 visual_prompt]: 	Training 400/553. train loss: 0.6309,	0.9120 s / batch. (data: 2.62e-04). ETA=12:13:36, max mem: 27.1 GB 
[11/21 21:44:22 visual_prompt]: 	Training 500/553. train loss: 0.7717,	0.9233 s / batch. (data: 2.94e-04). ETA=12:21:11, max mem: 27.1 GB 
[11/21 21:45:11 visual_prompt]: Epoch 13 / 100: avg data time: 6.75e-02, avg batch time: 0.9983, average train loss: 0.7363
[11/21 21:46:09 visual_prompt]: Inference (val):avg data time: 1.37e-04, avg batch time: 0.3043, average loss: 0.7189
[11/21 21:46:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.98	
[11/21 21:46:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/21 21:47:59 visual_prompt]: 	Training 100/553. train loss: 0.6707,	0.9225 s / batch. (data: 5.37e-03). ETA=12:18:08, max mem: 27.1 GB 
[11/21 21:49:39 visual_prompt]: 	Training 200/553. train loss: 0.3225,	0.9287 s / batch. (data: 1.04e-02). ETA=12:21:36, max mem: 27.1 GB 
[11/21 21:51:14 visual_prompt]: 	Training 300/553. train loss: 0.6807,	0.9320 s / batch. (data: 2.93e-04). ETA=12:22:40, max mem: 27.1 GB 
[11/21 21:52:49 visual_prompt]: 	Training 400/553. train loss: 1.0113,	0.9136 s / batch. (data: 2.82e-04). ETA=12:06:28, max mem: 27.1 GB 
[11/21 21:54:25 visual_prompt]: 	Training 500/553. train loss: 0.7530,	0.9366 s / batch. (data: 1.26e-02). ETA=12:23:12, max mem: 27.1 GB 
[11/21 21:55:14 visual_prompt]: Epoch 14 / 100: avg data time: 5.33e-02, avg batch time: 0.9844, average train loss: 0.7427
[11/21 21:56:12 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3049, average loss: 0.6982
[11/21 21:56:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 43.90	rocauc: 53.94	
[11/21 21:56:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/21 21:58:00 visual_prompt]: 	Training 100/553. train loss: 0.7560,	0.9400 s / batch. (data: 7.97e-03). ETA=12:23:29, max mem: 27.1 GB 
[11/21 21:59:39 visual_prompt]: 	Training 200/553. train loss: 0.7077,	0.9400 s / batch. (data: 7.97e-03). ETA=12:21:57, max mem: 27.1 GB 
[11/21 22:01:15 visual_prompt]: 	Training 300/553. train loss: 1.0354,	0.9388 s / batch. (data: 1.34e-02). ETA=12:19:25, max mem: 27.1 GB 
[11/21 22:02:51 visual_prompt]: 	Training 400/553. train loss: 0.5889,	0.9360 s / batch. (data: 2.55e-04). ETA=12:15:40, max mem: 27.1 GB 
[11/21 22:04:26 visual_prompt]: 	Training 500/553. train loss: 0.8548,	0.9166 s / batch. (data: 2.54e-04). ETA=11:58:54, max mem: 27.1 GB 
[11/21 22:05:16 visual_prompt]: Epoch 15 / 100: avg data time: 5.23e-02, avg batch time: 0.9837, average train loss: 0.7305
[11/21 22:06:14 visual_prompt]: Inference (val):avg data time: 2.51e-04, avg batch time: 0.3048, average loss: 0.7390
[11/21 22:06:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.93	
[11/21 22:06:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/21 22:08:00 visual_prompt]: 	Training 100/553. train loss: 0.7418,	0.9189 s / batch. (data: 2.39e-04). ETA=11:58:20, max mem: 27.1 GB 
[11/21 22:09:34 visual_prompt]: 	Training 200/553. train loss: 0.5816,	0.9661 s / batch. (data: 1.03e-02). ETA=12:33:39, max mem: 27.1 GB 
[11/21 22:11:14 visual_prompt]: 	Training 300/553. train loss: 0.8484,	0.9480 s / batch. (data: 3.72e-04). ETA=12:17:57, max mem: 27.1 GB 
[11/21 22:12:51 visual_prompt]: 	Training 400/553. train loss: 0.5919,	0.9386 s / batch. (data: 6.99e-03). ETA=12:09:03, max mem: 27.1 GB 
[11/21 22:14:26 visual_prompt]: 	Training 500/553. train loss: 0.7396,	0.9268 s / batch. (data: 1.09e-02). ETA=11:58:20, max mem: 27.1 GB 
[11/21 22:15:16 visual_prompt]: Epoch 16 / 100: avg data time: 4.79e-02, avg batch time: 0.9796, average train loss: 0.7362
[11/21 22:16:14 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3056, average loss: 0.8229
[11/21 22:16:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.53	
[11/21 22:16:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/21 22:17:57 visual_prompt]: 	Training 100/553. train loss: 0.8076,	0.9211 s / batch. (data: 1.04e-02). ETA=11:51:35, max mem: 27.1 GB 
[11/21 22:19:37 visual_prompt]: 	Training 200/553. train loss: 0.8583,	0.9503 s / batch. (data: 5.44e-03). ETA=12:12:34, max mem: 27.1 GB 
[11/21 22:21:11 visual_prompt]: 	Training 300/553. train loss: 1.2270,	0.9610 s / batch. (data: 1.63e-02). ETA=12:19:11, max mem: 27.1 GB 
[11/21 22:22:48 visual_prompt]: 	Training 400/553. train loss: 0.6818,	0.9400 s / batch. (data: 1.66e-02). ETA=12:01:28, max mem: 27.1 GB 
[11/21 22:24:24 visual_prompt]: 	Training 500/553. train loss: 0.7746,	0.9393 s / batch. (data: 5.36e-03). ETA=11:59:23, max mem: 27.1 GB 
[11/21 22:25:15 visual_prompt]: Epoch 17 / 100: avg data time: 4.70e-02, avg batch time: 0.9783, average train loss: 0.7279
[11/21 22:26:13 visual_prompt]: Inference (val):avg data time: 3.15e-04, avg batch time: 0.3033, average loss: 0.6847
[11/21 22:26:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.85	
[11/21 22:26:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/21 22:28:02 visual_prompt]: 	Training 100/553. train loss: 0.7336,	0.9739 s / batch. (data: 1.60e-02). ETA=12:23:24, max mem: 27.1 GB 
[11/21 22:29:40 visual_prompt]: 	Training 200/553. train loss: 0.7207,	0.9636 s / batch. (data: 5.39e-03). ETA=12:13:56, max mem: 27.1 GB 
[11/21 22:31:15 visual_prompt]: 	Training 300/553. train loss: 0.7297,	0.9280 s / batch. (data: 2.86e-04). ETA=11:45:17, max mem: 27.1 GB 
[11/21 22:32:50 visual_prompt]: 	Training 400/553. train loss: 0.7555,	0.9266 s / batch. (data: 5.38e-03). ETA=11:42:39, max mem: 27.1 GB 
[11/21 22:34:28 visual_prompt]: 	Training 500/553. train loss: 0.7146,	0.9416 s / batch. (data: 1.60e-02). ETA=11:52:28, max mem: 27.1 GB 
[11/21 22:35:18 visual_prompt]: Epoch 18 / 100: avg data time: 5.15e-02, avg batch time: 0.9842, average train loss: 0.7242
[11/21 22:36:16 visual_prompt]: Inference (val):avg data time: 7.52e-04, avg batch time: 0.3049, average loss: 0.7349
[11/21 22:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.34	
[11/21 22:36:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/21 22:38:02 visual_prompt]: 	Training 100/553. train loss: 0.7208,	0.9302 s / batch. (data: 5.50e-03). ETA=11:41:27, max mem: 27.1 GB 
[11/21 22:39:41 visual_prompt]: 	Training 200/553. train loss: 0.6909,	2.7301 s / batch. (data: 1.82e+00). ETA=1 day, 10:14:13, max mem: 27.1 GB 
[11/21 22:41:16 visual_prompt]: 	Training 300/553. train loss: 0.6764,	0.9280 s / batch. (data: 7.94e-03). ETA=11:36:42, max mem: 27.1 GB 
[11/21 22:42:52 visual_prompt]: 	Training 400/553. train loss: 0.7070,	0.9595 s / batch. (data: 7.39e-03). ETA=11:58:43, max mem: 27.1 GB 
[11/21 22:44:28 visual_prompt]: 	Training 500/553. train loss: 0.4152,	0.9320 s / batch. (data: 3.15e-04). ETA=11:36:34, max mem: 27.1 GB 
[11/21 22:45:18 visual_prompt]: Epoch 19 / 100: avg data time: 4.90e-02, avg batch time: 0.9807, average train loss: 0.7153
[11/21 22:46:16 visual_prompt]: Inference (val):avg data time: 3.71e-04, avg batch time: 0.3076, average loss: 0.8956
[11/21 22:46:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.76	
[11/21 22:46:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/21 22:48:06 visual_prompt]: 	Training 100/553. train loss: 0.6801,	0.9473 s / batch. (data: 1.54e-02). ETA=11:45:37, max mem: 27.1 GB 
[11/21 22:49:40 visual_prompt]: 	Training 200/553. train loss: 0.7235,	0.9384 s / batch. (data: 1.04e-02). ETA=11:37:24, max mem: 27.1 GB 
[11/21 22:51:17 visual_prompt]: 	Training 300/553. train loss: 0.6835,	0.9198 s / batch. (data: 2.66e-04). ETA=11:22:04, max mem: 27.1 GB 
[11/21 22:52:53 visual_prompt]: 	Training 400/553. train loss: 0.7431,	0.9275 s / batch. (data: 7.33e-04). ETA=11:26:14, max mem: 27.1 GB 
[11/21 22:54:27 visual_prompt]: 	Training 500/553. train loss: 0.7430,	0.9569 s / batch. (data: 2.86e-04). ETA=11:46:22, max mem: 27.1 GB 
[11/21 22:55:20 visual_prompt]: Epoch 20 / 100: avg data time: 5.21e-02, avg batch time: 0.9829, average train loss: 0.7239
[11/21 22:56:18 visual_prompt]: Inference (val):avg data time: 3.34e-04, avg batch time: 0.3039, average loss: 0.6826
[11/21 22:56:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.55	
[11/21 22:56:18 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/21 22:58:08 visual_prompt]: 	Training 100/553. train loss: 0.6845,	0.9522 s / batch. (data: 2.48e-02). ETA=11:40:29, max mem: 27.1 GB 
[11/21 22:59:44 visual_prompt]: 	Training 200/553. train loss: 0.7147,	0.9640 s / batch. (data: 8.04e-04). ETA=11:47:34, max mem: 27.1 GB 
[11/21 23:01:21 visual_prompt]: 	Training 300/553. train loss: 0.6773,	0.9466 s / batch. (data: 2.26e-02). ETA=11:33:11, max mem: 27.1 GB 
[11/21 23:02:58 visual_prompt]: 	Training 400/553. train loss: 1.0057,	0.9506 s / batch. (data: 5.38e-03). ETA=11:34:34, max mem: 27.1 GB 
[11/21 23:04:32 visual_prompt]: 	Training 500/553. train loss: 0.6185,	0.9239 s / batch. (data: 2.76e-04). ETA=11:13:32, max mem: 27.1 GB 
[11/21 23:05:22 visual_prompt]: Epoch 21 / 100: avg data time: 5.25e-02, avg batch time: 0.9839, average train loss: 0.7064
[11/21 23:06:20 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3029, average loss: 0.6820
[11/21 23:06:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.66	
[11/21 23:06:20 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/21 23:08:06 visual_prompt]: 	Training 100/553. train loss: 0.6993,	0.9360 s / batch. (data: 2.78e-04). ETA=11:19:58, max mem: 27.1 GB 
[11/21 23:09:45 visual_prompt]: 	Training 200/553. train loss: 1.0229,	0.9665 s / batch. (data: 7.32e-04). ETA=11:40:30, max mem: 27.1 GB 
[11/21 23:11:24 visual_prompt]: 	Training 300/553. train loss: 0.5747,	0.9420 s / batch. (data: 7.26e-04). ETA=11:21:09, max mem: 27.1 GB 
[11/21 23:12:58 visual_prompt]: 	Training 400/553. train loss: 0.8392,	0.9536 s / batch. (data: 1.55e-02). ETA=11:27:56, max mem: 27.1 GB 
[11/21 23:14:33 visual_prompt]: 	Training 500/553. train loss: 0.7800,	0.9241 s / batch. (data: 2.93e-04). ETA=11:05:08, max mem: 27.1 GB 
[11/21 23:15:24 visual_prompt]: Epoch 22 / 100: avg data time: 5.27e-02, avg batch time: 0.9840, average train loss: 0.7067
[11/21 23:16:22 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3033, average loss: 0.7341
[11/21 23:16:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.05	
[11/21 23:16:22 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/21 23:18:12 visual_prompt]: 	Training 100/553. train loss: 0.6927,	0.9303 s / batch. (data: 7.35e-03). ETA=11:07:16, max mem: 27.1 GB 
[11/21 23:19:48 visual_prompt]: 	Training 200/553. train loss: 0.6289,	0.9406 s / batch. (data: 7.14e-04). ETA=11:13:05, max mem: 27.1 GB 
[11/21 23:21:22 visual_prompt]: 	Training 300/553. train loss: 0.6465,	0.9520 s / batch. (data: 3.13e-04). ETA=11:19:38, max mem: 27.1 GB 
[11/21 23:23:00 visual_prompt]: 	Training 400/553. train loss: 0.8526,	0.9652 s / batch. (data: 5.85e-03). ETA=11:27:25, max mem: 27.1 GB 
[11/21 23:24:34 visual_prompt]: 	Training 500/553. train loss: 0.5003,	0.9341 s / batch. (data: 5.38e-03). ETA=11:03:44, max mem: 27.1 GB 
[11/21 23:25:26 visual_prompt]: Epoch 23 / 100: avg data time: 5.13e-02, avg batch time: 0.9827, average train loss: 0.7335
[11/21 23:26:25 visual_prompt]: Inference (val):avg data time: 8.90e-04, avg batch time: 0.3061, average loss: 0.8699
[11/21 23:26:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[11/21 23:26:25 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/21 23:28:10 visual_prompt]: 	Training 100/553. train loss: 0.7553,	0.9562 s / batch. (data: 2.03e-02). ETA=11:16:58, max mem: 27.1 GB 
[11/21 23:29:48 visual_prompt]: 	Training 200/553. train loss: 0.6950,	0.9507 s / batch. (data: 2.70e-04). ETA=11:11:32, max mem: 27.1 GB 
[11/21 23:31:27 visual_prompt]: 	Training 300/553. train loss: 0.6716,	0.9567 s / batch. (data: 3.09e-04). ETA=11:14:09, max mem: 27.1 GB 
[11/21 23:33:03 visual_prompt]: 	Training 400/553. train loss: 0.6974,	0.9280 s / batch. (data: 2.85e-04). ETA=10:52:23, max mem: 27.1 GB 
[11/21 23:34:40 visual_prompt]: 	Training 500/553. train loss: 0.7440,	0.9495 s / batch. (data: 3.20e-02). ETA=11:05:56, max mem: 27.1 GB 
[11/21 23:35:30 visual_prompt]: Epoch 24 / 100: avg data time: 5.30e-02, avg batch time: 0.9847, average train loss: 0.7145
[11/21 23:36:28 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3049, average loss: 0.7040
[11/21 23:36:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.13	
[11/21 23:36:28 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/21 23:38:12 visual_prompt]: 	Training 100/553. train loss: 0.6086,	0.9523 s / batch. (data: 2.44e-02). ETA=11:05:28, max mem: 27.1 GB 
[11/21 23:39:52 visual_prompt]: 	Training 200/553. train loss: 0.4504,	0.9235 s / batch. (data: 3.98e-03). ETA=10:43:46, max mem: 27.1 GB 
[11/21 23:41:29 visual_prompt]: 	Training 300/553. train loss: 0.6836,	0.9316 s / batch. (data: 2.54e-04). ETA=10:47:53, max mem: 27.1 GB 
[11/21 23:43:06 visual_prompt]: 	Training 400/553. train loss: 0.7045,	0.9253 s / batch. (data: 2.90e-04). ETA=10:41:58, max mem: 27.1 GB 
[11/21 23:44:41 visual_prompt]: 	Training 500/553. train loss: 0.6779,	0.9253 s / batch. (data: 2.91e-04). ETA=10:40:27, max mem: 27.1 GB 
[11/21 23:45:33 visual_prompt]: Epoch 25 / 100: avg data time: 5.46e-02, avg batch time: 0.9855, average train loss: 0.7095
[11/21 23:46:31 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3042, average loss: 0.7005
[11/21 23:46:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.17	
[11/21 23:46:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/21 23:48:18 visual_prompt]: 	Training 100/553. train loss: 0.5357,	0.9171 s / batch. (data: 7.48e-04). ETA=10:32:23, max mem: 27.1 GB 
[11/21 23:49:55 visual_prompt]: 	Training 200/553. train loss: 0.3950,	0.9520 s / batch. (data: 2.73e-04). ETA=10:54:54, max mem: 27.1 GB 
[11/21 23:51:34 visual_prompt]: 	Training 300/553. train loss: 0.6084,	0.9080 s / batch. (data: 2.59e-04). ETA=10:23:07, max mem: 27.1 GB 
[11/21 23:53:08 visual_prompt]: 	Training 400/553. train loss: 0.6954,	0.9180 s / batch. (data: 2.79e-04). ETA=10:28:27, max mem: 27.1 GB 
[11/21 23:54:45 visual_prompt]: 	Training 500/553. train loss: 0.5629,	0.9560 s / batch. (data: 4.02e-03). ETA=10:52:51, max mem: 27.1 GB 
[11/21 23:55:36 visual_prompt]: Epoch 26 / 100: avg data time: 5.50e-02, avg batch time: 0.9854, average train loss: 0.7103
[11/21 23:56:34 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.3056, average loss: 0.7006
[11/21 23:56:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 51.89	
[11/21 23:56:34 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/21 23:58:20 visual_prompt]: 	Training 100/553. train loss: 0.6793,	0.9416 s / batch. (data: 1.55e-02). ETA=10:40:39, max mem: 27.1 GB 
[11/21 23:59:57 visual_prompt]: 	Training 200/553. train loss: 0.6953,	0.9544 s / batch. (data: 1.64e-02). ETA=10:47:43, max mem: 27.1 GB 
[11/22 00:01:31 visual_prompt]: 	Training 300/553. train loss: 0.7631,	0.9423 s / batch. (data: 5.87e-03). ETA=10:37:59, max mem: 27.1 GB 
[11/22 00:03:07 visual_prompt]: 	Training 400/553. train loss: 0.9639,	0.9376 s / batch. (data: 1.04e-02). ETA=10:33:12, max mem: 27.1 GB 
[11/22 00:04:47 visual_prompt]: 	Training 500/553. train loss: 0.8253,	0.9109 s / batch. (data: 2.74e-04). ETA=10:13:39, max mem: 27.1 GB 
[11/22 00:05:37 visual_prompt]: Epoch 27 / 100: avg data time: 5.21e-02, avg batch time: 0.9815, average train loss: 0.7109
[11/22 00:06:35 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3050, average loss: 0.7014
[11/22 00:06:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.13	
[11/22 00:06:35 visual_prompt]: Stopping early.
[11/22 00:06:35 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 00:06:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 00:06:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/22 00:06:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/22 00:06:35 visual_prompt]: Training with config:
[11/22 00:06:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.001_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/22 00:06:35 visual_prompt]: Loading training data...
[11/22 00:06:35 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 00:06:35 visual_prompt]: Loading validation data...
[11/22 00:06:35 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 00:06:35 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 00:06:37 visual_prompt]: Enable all parameters update during training
[11/22 00:06:37 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/22 00:06:37 visual_prompt]: tuned percent:100.000
[11/22 00:06:37 visual_prompt]: Device used for model: 0
[11/22 00:06:37 visual_prompt]: Setting up Evaluator...
[11/22 00:06:37 visual_prompt]: Setting up Trainer...
[11/22 00:06:37 visual_prompt]: 	Setting up the optimizer...
[11/22 00:06:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 00:08:21 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9383 s / batch. (data: 2.80e-04). ETA=14:23:14, max mem: 28.4 GB 
[11/22 00:10:02 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9130 s / batch. (data: 6.98e-03). ETA=13:58:28, max mem: 28.4 GB 
[11/22 00:11:39 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9261 s / batch. (data: 2.73e-04). ETA=14:08:57, max mem: 28.4 GB 
[11/22 00:13:13 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9560 s / batch. (data: 2.97e-04). ETA=14:34:44, max mem: 28.4 GB 
[11/22 00:14:52 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9165 s / batch. (data: 2.53e-04). ETA=13:57:05, max mem: 28.4 GB 
[11/22 00:15:41 visual_prompt]: Epoch 1 / 100: avg data time: 4.86e-02, avg batch time: 0.9839, average train loss: 7.6130
[11/22 00:16:39 visual_prompt]: Inference (val):avg data time: 3.18e-04, avg batch time: 0.3063, average loss: 6.9126
[11/22 00:16:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/22 00:16:39 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/22 00:18:27 visual_prompt]: 	Training 100/553. train loss: 1.3854,	0.9360 s / batch. (data: 7.80e-04). ETA=14:12:29, max mem: 28.4 GB 
[11/22 00:20:02 visual_prompt]: 	Training 200/553. train loss: 0.9869,	1.8624 s / batch. (data: 9.47e-01). ETA=1 day, 4:13:08, max mem: 28.4 GB 
[11/22 00:21:40 visual_prompt]: 	Training 300/553. train loss: 0.7381,	0.9229 s / batch. (data: 1.31e-02). ETA=13:57:31, max mem: 28.4 GB 
[11/22 00:23:17 visual_prompt]: 	Training 400/553. train loss: 1.0488,	0.9787 s / batch. (data: 7.30e-04). ETA=14:46:31, max mem: 28.4 GB 
[11/22 00:24:54 visual_prompt]: 	Training 500/553. train loss: 1.1102,	0.9440 s / batch. (data: 2.87e-04). ETA=14:13:30, max mem: 28.4 GB 
[11/22 00:25:44 visual_prompt]: Epoch 2 / 100: avg data time: 5.20e-02, avg batch time: 0.9845, average train loss: 1.0187
[11/22 00:26:42 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3029, average loss: 0.8945
[11/22 00:26:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.01	
[11/22 00:26:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/22 00:28:28 visual_prompt]: 	Training 100/553. train loss: 0.6794,	0.9533 s / batch. (data: 8.76e-04). ETA=14:19:29, max mem: 28.4 GB 
[11/22 00:30:06 visual_prompt]: 	Training 200/553. train loss: 3.2372,	0.9395 s / batch. (data: 2.77e-04). ETA=14:05:25, max mem: 28.4 GB 
[11/22 00:31:42 visual_prompt]: 	Training 300/553. train loss: 0.6858,	0.9519 s / batch. (data: 5.90e-03). ETA=14:15:02, max mem: 28.4 GB 
[11/22 00:33:18 visual_prompt]: 	Training 400/553. train loss: 0.9141,	0.9520 s / batch. (data: 7.31e-04). ETA=14:13:32, max mem: 28.4 GB 
[11/22 00:34:52 visual_prompt]: 	Training 500/553. train loss: 1.2106,	0.9230 s / batch. (data: 8.26e-04). ETA=13:45:57, max mem: 28.4 GB 
[11/22 00:35:44 visual_prompt]: Epoch 3 / 100: avg data time: 4.65e-02, avg batch time: 0.9790, average train loss: 0.8727
[11/22 00:36:42 visual_prompt]: Inference (val):avg data time: 1.33e-04, avg batch time: 0.3073, average loss: 0.7661
[11/22 00:36:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.89	
[11/22 00:36:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/22 00:38:29 visual_prompt]: 	Training 100/553. train loss: 0.8639,	0.9400 s / batch. (data: 3.99e-03). ETA=13:58:47, max mem: 28.4 GB 
[11/22 00:40:05 visual_prompt]: 	Training 200/553. train loss: 1.0041,	1.3144 s / batch. (data: 4.03e-01). ETA=19:30:40, max mem: 28.4 GB 
[11/22 00:41:41 visual_prompt]: 	Training 300/553. train loss: 1.4119,	0.9400 s / batch. (data: 2.83e-04). ETA=13:55:41, max mem: 28.4 GB 
[11/22 00:43:17 visual_prompt]: 	Training 400/553. train loss: 0.4526,	0.9466 s / batch. (data: 5.41e-03). ETA=14:00:00, max mem: 28.4 GB 
[11/22 00:44:55 visual_prompt]: 	Training 500/553. train loss: 0.9937,	0.9842 s / batch. (data: 2.02e-02). ETA=14:31:39, max mem: 28.4 GB 
[11/22 00:45:46 visual_prompt]: Epoch 4 / 100: avg data time: 5.08e-02, avg batch time: 0.9831, average train loss: 0.8370
[11/22 00:46:44 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3054, average loss: 0.7342
[11/22 00:46:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 61.08	
[11/22 00:46:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/22 00:48:30 visual_prompt]: 	Training 100/553. train loss: 0.5939,	0.9399 s / batch. (data: 2.93e-04). ETA=13:50:05, max mem: 28.4 GB 
[11/22 00:50:07 visual_prompt]: 	Training 200/553. train loss: 0.4967,	0.9217 s / batch. (data: 3.01e-04). ETA=13:32:24, max mem: 28.4 GB 
[11/22 00:51:42 visual_prompt]: 	Training 300/553. train loss: 1.0789,	0.9480 s / batch. (data: 2.72e-04). ETA=13:54:04, max mem: 28.4 GB 
[11/22 00:53:20 visual_prompt]: 	Training 400/553. train loss: 1.4169,	3.2147 s / batch. (data: 2.26e+00). ETA=1 day, 23:02:54, max mem: 28.4 GB 
[11/22 00:54:57 visual_prompt]: 	Training 500/553. train loss: 0.7025,	0.9136 s / batch. (data: 2.58e-04). ETA=13:20:46, max mem: 28.4 GB 
[11/22 00:55:47 visual_prompt]: Epoch 5 / 100: avg data time: 4.93e-02, avg batch time: 0.9810, average train loss: 0.8091
[11/22 00:56:45 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3044, average loss: 0.7412
[11/22 00:56:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.38	
[11/22 00:56:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/22 00:58:33 visual_prompt]: 	Training 100/553. train loss: 0.7679,	0.9320 s / batch. (data: 5.43e-03). ETA=13:34:29, max mem: 28.4 GB 
[11/22 01:00:08 visual_prompt]: 	Training 200/553. train loss: 0.8377,	0.9372 s / batch. (data: 3.95e-03). ETA=13:37:27, max mem: 28.4 GB 
[11/22 01:01:44 visual_prompt]: 	Training 300/553. train loss: 1.0777,	0.9379 s / batch. (data: 3.20e-04). ETA=13:36:30, max mem: 28.4 GB 
[11/22 01:03:21 visual_prompt]: 	Training 400/553. train loss: 0.6802,	1.8303 s / batch. (data: 8.96e-01). ETA=1 day, 2:30:21, max mem: 28.4 GB 
[11/22 01:05:00 visual_prompt]: 	Training 500/553. train loss: 0.8102,	0.9146 s / batch. (data: 2.81e-04). ETA=13:13:09, max mem: 28.4 GB 
[11/22 01:05:49 visual_prompt]: Epoch 6 / 100: avg data time: 5.06e-02, avg batch time: 0.9834, average train loss: 0.7946
[11/22 01:06:47 visual_prompt]: Inference (val):avg data time: 4.14e-04, avg batch time: 0.3054, average loss: 0.6709
[11/22 01:06:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.01	
[11/22 01:06:47 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/22 01:08:40 visual_prompt]: 	Training 100/553. train loss: 0.6635,	0.9067 s / batch. (data: 2.92e-04). ETA=13:04:03, max mem: 28.4 GB 
[11/22 01:10:14 visual_prompt]: 	Training 200/553. train loss: 0.5893,	0.9276 s / batch. (data: 8.02e-03). ETA=13:20:31, max mem: 28.4 GB 
[11/22 01:11:50 visual_prompt]: 	Training 300/553. train loss: 0.7790,	0.9866 s / batch. (data: 5.83e-03). ETA=14:09:48, max mem: 28.4 GB 
[11/22 01:13:24 visual_prompt]: 	Training 400/553. train loss: 0.7855,	0.9556 s / batch. (data: 2.92e-04). ETA=13:41:29, max mem: 28.4 GB 
[11/22 01:15:00 visual_prompt]: 	Training 500/553. train loss: 0.5892,	0.9280 s / batch. (data: 1.64e-02). ETA=13:16:17, max mem: 28.4 GB 
[11/22 01:15:50 visual_prompt]: Epoch 7 / 100: avg data time: 4.83e-02, avg batch time: 0.9814, average train loss: 0.7776
[11/22 01:16:48 visual_prompt]: Inference (val):avg data time: 6.88e-04, avg batch time: 0.3072, average loss: 0.6688
[11/22 01:16:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 60.83	
[11/22 01:16:48 visual_prompt]: Best epoch 7: best metric: -0.669
[11/22 01:16:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/22 01:18:34 visual_prompt]: 	Training 100/553. train loss: 0.5798,	0.9238 s / batch. (data: 2.68e-04). ETA=13:10:18, max mem: 28.4 GB 
[11/22 01:20:12 visual_prompt]: 	Training 200/553. train loss: 0.6763,	0.9185 s / batch. (data: 2.88e-04). ETA=13:04:11, max mem: 28.4 GB 
[11/22 01:21:48 visual_prompt]: 	Training 300/553. train loss: 0.8442,	0.9226 s / batch. (data: 2.61e-04). ETA=13:06:14, max mem: 28.4 GB 
[11/22 01:23:26 visual_prompt]: 	Training 400/553. train loss: 0.6503,	0.9440 s / batch. (data: 1.63e-02). ETA=13:22:50, max mem: 28.4 GB 
[11/22 01:25:03 visual_prompt]: 	Training 500/553. train loss: 0.5778,	0.9655 s / batch. (data: 3.75e-02). ETA=13:39:31, max mem: 28.4 GB 
[11/22 01:25:53 visual_prompt]: Epoch 8 / 100: avg data time: 5.33e-02, avg batch time: 0.9852, average train loss: 0.7764
[11/22 01:26:52 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3064, average loss: 0.6827
[11/22 01:26:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.63	
[11/22 01:26:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/22 01:28:41 visual_prompt]: 	Training 100/553. train loss: 0.8387,	0.9032 s / batch. (data: 2.57e-04). ETA=12:44:20, max mem: 28.4 GB 
[11/22 01:30:17 visual_prompt]: 	Training 200/553. train loss: 1.2029,	0.9221 s / batch. (data: 3.36e-04). ETA=12:58:50, max mem: 28.4 GB 
[11/22 01:31:52 visual_prompt]: 	Training 300/553. train loss: 0.6014,	0.9301 s / batch. (data: 2.61e-04). ETA=13:04:03, max mem: 28.4 GB 
[11/22 01:33:29 visual_prompt]: 	Training 400/553. train loss: 0.8381,	0.9562 s / batch. (data: 5.83e-03). ETA=13:24:25, max mem: 28.4 GB 
[11/22 01:35:03 visual_prompt]: 	Training 500/553. train loss: 0.6482,	0.9600 s / batch. (data: 2.64e-04). ETA=13:25:59, max mem: 28.4 GB 
[11/22 01:35:54 visual_prompt]: Epoch 9 / 100: avg data time: 4.76e-02, avg batch time: 0.9797, average train loss: 0.7865
[11/22 01:36:51 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3083, average loss: 0.7001
[11/22 01:36:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 60.62	
[11/22 01:36:51 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/22 01:38:39 visual_prompt]: 	Training 100/553. train loss: 0.8809,	0.9180 s / batch. (data: 1.45e-02). ETA=12:48:23, max mem: 28.4 GB 
[11/22 01:40:16 visual_prompt]: 	Training 200/553. train loss: 0.8389,	0.9226 s / batch. (data: 1.04e-02). ETA=12:50:41, max mem: 28.4 GB 
[11/22 01:41:51 visual_prompt]: 	Training 300/553. train loss: 0.6895,	0.9440 s / batch. (data: 2.56e-04). ETA=13:07:02, max mem: 28.4 GB 
[11/22 01:43:25 visual_prompt]: 	Training 400/553. train loss: 0.6946,	0.9382 s / batch. (data: 2.80e-04). ETA=13:00:39, max mem: 28.4 GB 
[11/22 01:45:03 visual_prompt]: 	Training 500/553. train loss: 0.6795,	0.9400 s / batch. (data: 3.04e-04). ETA=13:00:32, max mem: 28.4 GB 
[11/22 01:45:55 visual_prompt]: Epoch 10 / 100: avg data time: 4.98e-02, avg batch time: 0.9819, average train loss: 0.7484
[11/22 01:46:52 visual_prompt]: Inference (val):avg data time: 9.32e-04, avg batch time: 0.3054, average loss: 0.7779
[11/22 01:46:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.41	
[11/22 01:46:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/22 01:48:42 visual_prompt]: 	Training 100/553. train loss: 1.0949,	0.9240 s / batch. (data: 2.61e-04). ETA=12:44:57, max mem: 28.4 GB 
[11/22 01:50:18 visual_prompt]: 	Training 200/553. train loss: 0.8069,	0.9420 s / batch. (data: 2.82e-04). ETA=12:58:16, max mem: 28.4 GB 
[11/22 01:51:52 visual_prompt]: 	Training 300/553. train loss: 0.5032,	0.9228 s / batch. (data: 5.46e-03). ETA=12:40:49, max mem: 28.4 GB 
[11/22 01:53:30 visual_prompt]: 	Training 400/553. train loss: 0.9519,	0.9308 s / batch. (data: 5.36e-03). ETA=12:45:51, max mem: 28.4 GB 
[11/22 01:55:05 visual_prompt]: 	Training 500/553. train loss: 0.5093,	0.9160 s / batch. (data: 2.85e-04). ETA=12:32:09, max mem: 28.4 GB 
[11/22 01:55:55 visual_prompt]: Epoch 11 / 100: avg data time: 4.89e-02, avg batch time: 0.9806, average train loss: 0.7376
[11/22 01:56:53 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3024, average loss: 0.6851
[11/22 01:56:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 58.74	
[11/22 01:56:53 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/22 01:58:37 visual_prompt]: 	Training 100/553. train loss: 0.5612,	0.9640 s / batch. (data: 5.86e-03). ETA=13:09:09, max mem: 28.4 GB 
[11/22 02:00:18 visual_prompt]: 	Training 200/553. train loss: 0.8509,	0.9380 s / batch. (data: 6.52e-04). ETA=12:46:17, max mem: 28.4 GB 
[11/22 02:01:54 visual_prompt]: 	Training 300/553. train loss: 0.8443,	0.9251 s / batch. (data: 2.50e-04). ETA=12:34:12, max mem: 28.4 GB 
[11/22 02:03:30 visual_prompt]: 	Training 400/553. train loss: 1.0416,	0.9320 s / batch. (data: 7.43e-04). ETA=12:38:17, max mem: 28.4 GB 
[11/22 02:05:07 visual_prompt]: 	Training 500/553. train loss: 0.6627,	0.9266 s / batch. (data: 1.04e-02). ETA=12:32:19, max mem: 28.4 GB 
[11/22 02:05:57 visual_prompt]: Epoch 12 / 100: avg data time: 5.11e-02, avg batch time: 0.9829, average train loss: 0.7618
[11/22 02:06:55 visual_prompt]: Inference (val):avg data time: 8.31e-05, avg batch time: 0.3050, average loss: 1.0238
[11/22 02:06:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.99	
[11/22 02:06:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/22 02:08:44 visual_prompt]: 	Training 100/553. train loss: 0.5736,	0.9516 s / batch. (data: 5.45e-03). ETA=12:50:12, max mem: 28.4 GB 
[11/22 02:10:20 visual_prompt]: 	Training 200/553. train loss: 1.0416,	0.9440 s / batch. (data: 4.51e-03). ETA=12:42:31, max mem: 28.4 GB 
[11/22 02:11:57 visual_prompt]: 	Training 300/553. train loss: 0.6649,	0.9089 s / batch. (data: 2.69e-04). ETA=12:12:36, max mem: 28.4 GB 
[11/22 02:13:37 visual_prompt]: 	Training 400/553. train loss: 0.6735,	0.9360 s / batch. (data: 2.64e-04). ETA=12:32:56, max mem: 28.4 GB 
[11/22 02:15:12 visual_prompt]: 	Training 500/553. train loss: 0.7456,	0.9191 s / batch. (data: 2.91e-04). ETA=12:17:47, max mem: 28.4 GB 
[11/22 02:16:02 visual_prompt]: Epoch 13 / 100: avg data time: 5.63e-02, avg batch time: 0.9879, average train loss: 0.7365
[11/22 02:17:00 visual_prompt]: Inference (val):avg data time: 5.90e-05, avg batch time: 0.3051, average loss: 0.7327
[11/22 02:17:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.42	
[11/22 02:17:00 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/22 02:18:48 visual_prompt]: 	Training 100/553. train loss: 0.7210,	0.9186 s / batch. (data: 6.97e-03). ETA=12:15:03, max mem: 28.4 GB 
[11/22 02:20:25 visual_prompt]: 	Training 200/553. train loss: 0.3030,	0.9386 s / batch. (data: 1.04e-02). ETA=12:29:29, max mem: 28.4 GB 
[11/22 02:22:01 visual_prompt]: 	Training 300/553. train loss: 0.7213,	0.9304 s / batch. (data: 5.42e-03). ETA=12:21:24, max mem: 28.4 GB 
[11/22 02:23:35 visual_prompt]: 	Training 400/553. train loss: 0.9602,	0.9224 s / batch. (data: 1.04e-02). ETA=12:13:27, max mem: 28.4 GB 
[11/22 02:25:10 visual_prompt]: 	Training 500/553. train loss: 0.6957,	0.9384 s / batch. (data: 7.99e-03). ETA=12:24:38, max mem: 28.4 GB 
[11/22 02:26:00 visual_prompt]: Epoch 14 / 100: avg data time: 4.44e-02, avg batch time: 0.9759, average train loss: 0.7353
[11/22 02:26:57 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3060, average loss: 0.6984
[11/22 02:26:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 58.17	
[11/22 02:26:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/22 02:28:45 visual_prompt]: 	Training 100/553. train loss: 0.6630,	0.9535 s / batch. (data: 2.80e-04). ETA=12:34:11, max mem: 28.4 GB 
[11/22 02:30:20 visual_prompt]: 	Training 200/553. train loss: 0.6688,	0.9288 s / batch. (data: 5.39e-03). ETA=12:13:05, max mem: 28.4 GB 
[11/22 02:31:55 visual_prompt]: 	Training 300/553. train loss: 0.5528,	0.9284 s / batch. (data: 1.06e-02). ETA=12:11:14, max mem: 28.4 GB 
[11/22 02:33:30 visual_prompt]: 	Training 400/553. train loss: 0.6018,	0.9611 s / batch. (data: 2.36e-04). ETA=12:35:21, max mem: 28.4 GB 
[11/22 02:35:04 visual_prompt]: 	Training 500/553. train loss: 0.8460,	0.9263 s / batch. (data: 2.67e-04). ETA=12:06:27, max mem: 28.4 GB 
[11/22 02:35:53 visual_prompt]: Epoch 15 / 100: avg data time: 3.85e-02, avg batch time: 0.9686, average train loss: 0.7997
[11/22 02:36:50 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3017, average loss: 0.7568
[11/22 02:36:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.43	
[11/22 02:36:50 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/22 02:38:35 visual_prompt]: 	Training 100/553. train loss: 0.7292,	0.9280 s / batch. (data: 2.65e-04). ETA=12:05:29, max mem: 28.4 GB 
[11/22 02:40:09 visual_prompt]: 	Training 200/553. train loss: 0.6542,	0.9440 s / batch. (data: 7.99e-03). ETA=12:16:24, max mem: 28.4 GB 
[11/22 02:41:46 visual_prompt]: 	Training 300/553. train loss: 0.9113,	0.9341 s / batch. (data: 2.53e-04). ETA=12:07:07, max mem: 28.4 GB 
[11/22 02:43:22 visual_prompt]: 	Training 400/553. train loss: 0.6732,	0.9319 s / batch. (data: 7.95e-03). ETA=12:03:52, max mem: 28.4 GB 
[11/22 02:44:56 visual_prompt]: 	Training 500/553. train loss: 0.7198,	0.9253 s / batch. (data: 2.64e-04). ETA=11:57:13, max mem: 28.4 GB 
[11/22 02:45:46 visual_prompt]: Epoch 16 / 100: avg data time: 3.64e-02, avg batch time: 0.9675, average train loss: 0.7277
[11/22 02:46:43 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3045, average loss: 0.7721
[11/22 02:46:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.82	
[11/22 02:46:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/22 02:48:25 visual_prompt]: 	Training 100/553. train loss: 0.8402,	0.9330 s / batch. (data: 5.13e-03). ETA=12:00:47, max mem: 28.4 GB 
[11/22 02:50:04 visual_prompt]: 	Training 200/553. train loss: 0.7709,	0.9436 s / batch. (data: 8.49e-03). ETA=12:07:21, max mem: 28.4 GB 
[11/22 02:51:38 visual_prompt]: 	Training 300/553. train loss: 1.1698,	0.9670 s / batch. (data: 5.89e-03). ETA=12:23:48, max mem: 28.4 GB 
[11/22 02:53:13 visual_prompt]: 	Training 400/553. train loss: 0.6986,	0.9316 s / batch. (data: 1.56e-02). ETA=11:55:01, max mem: 28.4 GB 
[11/22 02:54:48 visual_prompt]: 	Training 500/553. train loss: 0.7741,	0.9134 s / batch. (data: 2.54e-04). ETA=11:39:33, max mem: 28.4 GB 
[11/22 02:55:38 visual_prompt]: Epoch 17 / 100: avg data time: 3.63e-02, avg batch time: 0.9663, average train loss: 0.7199
[11/22 02:56:35 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3047, average loss: 0.6843
[11/22 02:56:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 56.53	
[11/22 02:56:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/22 02:58:23 visual_prompt]: 	Training 100/553. train loss: 0.7164,	0.9227 s / batch. (data: 2.38e-04). ETA=11:44:17, max mem: 28.4 GB 
[11/22 02:59:59 visual_prompt]: 	Training 200/553. train loss: 0.7084,	0.9460 s / batch. (data: 7.94e-03). ETA=12:00:33, max mem: 28.4 GB 
[11/22 03:01:33 visual_prompt]: 	Training 300/553. train loss: 0.7147,	0.9604 s / batch. (data: 2.15e-02). ETA=12:09:52, max mem: 28.4 GB 
[11/22 03:03:07 visual_prompt]: 	Training 400/553. train loss: 0.7675,	0.9720 s / batch. (data: 8.00e-03). ETA=12:17:06, max mem: 28.4 GB 
[11/22 03:04:43 visual_prompt]: 	Training 500/553. train loss: 0.6622,	0.9288 s / batch. (data: 1.04e-02). ETA=11:42:46, max mem: 28.4 GB 
[11/22 03:05:32 visual_prompt]: Epoch 18 / 100: avg data time: 4.12e-02, avg batch time: 0.9712, average train loss: 0.7193
[11/22 03:06:30 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3043, average loss: 0.7442
[11/22 03:06:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.44	
[11/22 03:06:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/22 03:08:15 visual_prompt]: 	Training 100/553. train loss: 0.7133,	0.9315 s / batch. (data: 7.53e-03). ETA=11:42:26, max mem: 28.4 GB 
[11/22 03:09:52 visual_prompt]: 	Training 200/553. train loss: 0.7431,	2.5440 s / batch. (data: 1.59e+00). ETA=1 day, 7:54:11, max mem: 28.4 GB 
[11/22 03:11:26 visual_prompt]: 	Training 300/553. train loss: 0.6891,	0.9159 s / batch. (data: 3.06e-04). ETA=11:27:38, max mem: 28.4 GB 
[11/22 03:13:01 visual_prompt]: 	Training 400/553. train loss: 0.7038,	0.9480 s / batch. (data: 2.75e-04). ETA=11:50:08, max mem: 28.4 GB 
[11/22 03:14:35 visual_prompt]: 	Training 500/553. train loss: 0.4517,	0.9213 s / batch. (data: 7.71e-04). ETA=11:28:36, max mem: 28.4 GB 
[11/22 03:15:25 visual_prompt]: Epoch 19 / 100: avg data time: 3.71e-02, avg batch time: 0.9675, average train loss: 0.7151
[11/22 03:16:22 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3026, average loss: 0.8898
[11/22 03:16:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.72	
[11/22 03:16:22 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/22 03:18:11 visual_prompt]: 	Training 100/553. train loss: 0.6636,	0.9483 s / batch. (data: 5.36e-03). ETA=11:46:21, max mem: 28.4 GB 
[11/22 03:19:44 visual_prompt]: 	Training 200/553. train loss: 0.7100,	0.9329 s / batch. (data: 5.36e-03). ETA=11:33:22, max mem: 28.4 GB 
[11/22 03:21:18 visual_prompt]: 	Training 300/553. train loss: 0.6285,	0.9332 s / batch. (data: 5.38e-03). ETA=11:32:00, max mem: 28.4 GB 
[11/22 03:22:54 visual_prompt]: 	Training 400/553. train loss: 0.7189,	0.9241 s / batch. (data: 5.35e-03). ETA=11:23:44, max mem: 28.4 GB 
[11/22 03:24:27 visual_prompt]: 	Training 500/553. train loss: 0.7488,	0.9439 s / batch. (data: 5.61e-03). ETA=11:36:50, max mem: 28.4 GB 
[11/22 03:25:19 visual_prompt]: Epoch 20 / 100: avg data time: 3.90e-02, avg batch time: 0.9697, average train loss: 0.7222
[11/22 03:26:16 visual_prompt]: Inference (val):avg data time: 2.66e-04, avg batch time: 0.3048, average loss: 0.6815
[11/22 03:26:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.75	
[11/22 03:26:16 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/22 03:28:04 visual_prompt]: 	Training 100/553. train loss: 0.6810,	0.9399 s / batch. (data: 2.69e-04). ETA=11:31:26, max mem: 28.4 GB 
[11/22 03:29:40 visual_prompt]: 	Training 200/553. train loss: 0.7475,	0.9440 s / batch. (data: 7.85e-04). ETA=11:32:53, max mem: 28.4 GB 
[11/22 03:31:15 visual_prompt]: 	Training 300/553. train loss: 0.6996,	0.9260 s / batch. (data: 2.87e-04). ETA=11:18:08, max mem: 28.4 GB 
[11/22 03:32:49 visual_prompt]: 	Training 400/553. train loss: 0.9976,	0.9332 s / batch. (data: 6.67e-03). ETA=11:21:49, max mem: 28.4 GB 
[11/22 03:34:23 visual_prompt]: 	Training 500/553. train loss: 0.5939,	0.9378 s / batch. (data: 2.86e-04). ETA=11:23:39, max mem: 28.4 GB 
[11/22 03:35:13 visual_prompt]: Epoch 21 / 100: avg data time: 4.02e-02, avg batch time: 0.9703, average train loss: 0.7028
[11/22 03:36:10 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3048, average loss: 0.6797
[11/22 03:36:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.53	
[11/22 03:36:10 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/22 03:37:54 visual_prompt]: 	Training 100/553. train loss: 0.7081,	0.9194 s / batch. (data: 7.32e-03). ETA=11:07:52, max mem: 28.4 GB 
[11/22 03:39:32 visual_prompt]: 	Training 200/553. train loss: 0.8641,	0.9625 s / batch. (data: 2.63e-02). ETA=11:37:35, max mem: 28.4 GB 
[11/22 03:41:07 visual_prompt]: 	Training 300/553. train loss: 0.5950,	0.9086 s / batch. (data: 5.37e-03). ETA=10:57:01, max mem: 28.4 GB 
[11/22 03:42:41 visual_prompt]: 	Training 400/553. train loss: 0.8491,	0.9316 s / batch. (data: 1.04e-02). ETA=11:12:05, max mem: 28.4 GB 
[11/22 03:44:15 visual_prompt]: 	Training 500/553. train loss: 0.8084,	0.9617 s / batch. (data: 5.36e-03). ETA=11:32:11, max mem: 28.4 GB 
[11/22 03:45:04 visual_prompt]: Epoch 22 / 100: avg data time: 3.59e-02, avg batch time: 0.9662, average train loss: 0.7081
[11/22 03:46:02 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3048, average loss: 0.7275
[11/22 03:46:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.16	
[11/22 03:46:02 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/22 03:47:48 visual_prompt]: 	Training 100/553. train loss: 0.6659,	0.9393 s / batch. (data: 7.23e-03). ETA=11:13:41, max mem: 28.4 GB 
[11/22 03:49:24 visual_prompt]: 	Training 200/553. train loss: 0.6544,	0.9156 s / batch. (data: 2.64e-04). ETA=10:55:12, max mem: 28.4 GB 
[11/22 03:50:58 visual_prompt]: 	Training 300/553. train loss: 0.3640,	0.9231 s / batch. (data: 2.75e-04). ETA=10:58:58, max mem: 28.4 GB 
[11/22 03:52:32 visual_prompt]: 	Training 400/553. train loss: 0.8574,	0.9573 s / batch. (data: 7.87e-04). ETA=11:21:49, max mem: 28.4 GB 
[11/22 03:54:06 visual_prompt]: 	Training 500/553. train loss: 0.4630,	0.9387 s / batch. (data: 1.04e-02). ETA=11:06:59, max mem: 28.4 GB 
[11/22 03:54:57 visual_prompt]: Epoch 23 / 100: avg data time: 3.81e-02, avg batch time: 0.9673, average train loss: 0.7110
[11/22 03:55:54 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3053, average loss: 0.8469
[11/22 03:55:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.55	
[11/22 03:55:54 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/22 03:57:39 visual_prompt]: 	Training 100/553. train loss: 0.7659,	0.9544 s / batch. (data: 5.39e-03). ETA=11:15:45, max mem: 28.4 GB 
[11/22 03:59:13 visual_prompt]: 	Training 200/553. train loss: 0.6774,	0.9520 s / batch. (data: 2.66e-04). ETA=11:12:26, max mem: 28.4 GB 
[11/22 04:00:51 visual_prompt]: 	Training 300/553. train loss: 0.5946,	0.9684 s / batch. (data: 1.55e-02). ETA=11:22:22, max mem: 28.4 GB 
[11/22 04:02:25 visual_prompt]: 	Training 400/553. train loss: 0.7040,	0.9220 s / batch. (data: 2.47e-04). ETA=10:48:09, max mem: 28.4 GB 
[11/22 04:04:02 visual_prompt]: 	Training 500/553. train loss: 0.6915,	0.9520 s / batch. (data: 7.36e-04). ETA=11:07:39, max mem: 28.4 GB 
[11/22 04:04:51 visual_prompt]: Epoch 24 / 100: avg data time: 4.06e-02, avg batch time: 0.9704, average train loss: 0.7095
[11/22 04:05:48 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3054, average loss: 0.7174
[11/22 04:05:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.70	
[11/22 04:05:48 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/22 04:07:30 visual_prompt]: 	Training 100/553. train loss: 0.6773,	0.9440 s / batch. (data: 2.97e-04). ETA=10:59:39, max mem: 28.4 GB 
[11/22 04:09:10 visual_prompt]: 	Training 200/553. train loss: 0.4426,	0.9466 s / batch. (data: 7.41e-04). ETA=10:59:52, max mem: 28.4 GB 
[11/22 04:10:43 visual_prompt]: 	Training 300/553. train loss: 0.6395,	0.9507 s / batch. (data: 1.09e-02). ETA=11:01:11, max mem: 28.4 GB 
[11/22 04:12:20 visual_prompt]: 	Training 400/553. train loss: 0.7243,	0.9353 s / batch. (data: 2.96e-04). ETA=10:48:55, max mem: 28.4 GB 
[11/22 04:13:54 visual_prompt]: 	Training 500/553. train loss: 0.6421,	0.9440 s / batch. (data: 2.54e-04). ETA=10:53:23, max mem: 28.4 GB 
[11/22 04:14:45 visual_prompt]: Epoch 25 / 100: avg data time: 4.00e-02, avg batch time: 0.9701, average train loss: 0.7031
[11/22 04:15:42 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3031, average loss: 0.6960
[11/22 04:15:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 56.97	
[11/22 04:15:42 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/22 04:17:28 visual_prompt]: 	Training 100/553. train loss: 0.5198,	0.9463 s / batch. (data: 7.34e-04). ETA=10:52:32, max mem: 28.4 GB 
[11/22 04:19:04 visual_prompt]: 	Training 200/553. train loss: 0.4166,	0.9311 s / batch. (data: 7.99e-03). ETA=10:40:30, max mem: 28.4 GB 
[11/22 04:20:41 visual_prompt]: 	Training 300/553. train loss: 0.5851,	0.9290 s / batch. (data: 7.94e-04). ETA=10:37:30, max mem: 28.4 GB 
[11/22 04:22:14 visual_prompt]: 	Training 400/553. train loss: 0.6918,	0.9105 s / batch. (data: 4.35e-03). ETA=10:23:17, max mem: 28.4 GB 
[11/22 04:23:48 visual_prompt]: 	Training 500/553. train loss: 0.5652,	0.9555 s / batch. (data: 8.46e-03). ETA=10:52:30, max mem: 28.4 GB 
[11/22 04:24:38 visual_prompt]: Epoch 26 / 100: avg data time: 3.87e-02, avg batch time: 0.9689, average train loss: 0.7055
[11/22 04:25:36 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3044, average loss: 0.8216
[11/22 04:25:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.24	
[11/22 04:25:36 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/22 04:27:19 visual_prompt]: 	Training 100/553. train loss: 0.7113,	0.9362 s / batch. (data: 1.63e-02). ETA=10:36:55, max mem: 28.4 GB 
[11/22 04:28:56 visual_prompt]: 	Training 200/553. train loss: 0.6789,	0.9520 s / batch. (data: 5.39e-03). ETA=10:46:07, max mem: 28.4 GB 
[11/22 04:30:30 visual_prompt]: 	Training 300/553. train loss: 0.7819,	0.9390 s / batch. (data: 5.84e-03). ETA=10:35:44, max mem: 28.4 GB 
[11/22 04:32:04 visual_prompt]: 	Training 400/553. train loss: 0.9991,	0.9640 s / batch. (data: 4.53e-03). ETA=10:51:04, max mem: 28.4 GB 
[11/22 04:33:41 visual_prompt]: 	Training 500/553. train loss: 0.8773,	0.9304 s / batch. (data: 3.04e-04). ETA=10:26:47, max mem: 28.4 GB 
[11/22 04:34:30 visual_prompt]: Epoch 27 / 100: avg data time: 3.67e-02, avg batch time: 0.9663, average train loss: 0.7028
[11/22 04:35:27 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3052, average loss: 0.6990
[11/22 04:35:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 53.78	
[11/22 04:35:27 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.000873410738492077
[11/22 04:37:11 visual_prompt]: 	Training 100/553. train loss: 0.7741,	0.9480 s / batch. (data: 2.82e-04). ETA=10:36:15, max mem: 28.4 GB 
[11/22 04:38:47 visual_prompt]: 	Training 200/553. train loss: 0.5779,	0.9520 s / batch. (data: 5.86e-03). ETA=10:37:21, max mem: 28.4 GB 
[11/22 04:40:22 visual_prompt]: 	Training 300/553. train loss: 1.1032,	0.9160 s / batch. (data: 2.46e-04). ETA=10:11:43, max mem: 28.4 GB 
[11/22 04:41:58 visual_prompt]: 	Training 400/553. train loss: 0.6833,	1.4803 s / batch. (data: 5.71e-01). ETA=16:26:04, max mem: 28.4 GB 
[11/22 04:43:33 visual_prompt]: 	Training 500/553. train loss: 0.9976,	0.9290 s / batch. (data: 1.05e-02). ETA=10:17:17, max mem: 28.4 GB 
[11/22 04:44:22 visual_prompt]: Epoch 28 / 100: avg data time: 3.73e-02, avg batch time: 0.9670, average train loss: 0.7046
[11/22 04:45:20 visual_prompt]: Inference (val):avg data time: 2.64e-04, avg batch time: 0.3055, average loss: 0.7047
[11/22 04:45:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.27	
[11/22 04:45:20 visual_prompt]: Stopping early.
[11/22 04:45:20 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 04:45:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 04:45:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/22 04:45:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/22 04:45:20 visual_prompt]: Training with config:
[11/22 04:45:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.001_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/22 04:45:20 visual_prompt]: Loading training data...
[11/22 04:45:20 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 04:45:20 visual_prompt]: Loading validation data...
[11/22 04:45:20 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 04:45:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 04:45:21 visual_prompt]: Enable all parameters update during training
[11/22 04:45:21 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/22 04:45:21 visual_prompt]: tuned percent:100.000
[11/22 04:45:22 visual_prompt]: Device used for model: 0
[11/22 04:45:22 visual_prompt]: Setting up Evaluator...
[11/22 04:45:22 visual_prompt]: Setting up Trainer...
[11/22 04:45:22 visual_prompt]: 	Setting up the optimizer...
[11/22 04:45:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 04:47:05 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9320 s / batch. (data: 7.97e-03). ETA=14:17:27, max mem: 29.4 GB 
[11/22 04:48:43 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9453 s / batch. (data: 1.73e-02). ETA=14:28:06, max mem: 29.4 GB 
[11/22 04:50:20 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9080 s / batch. (data: 3.29e-04). ETA=13:52:19, max mem: 29.4 GB 
[11/22 04:51:54 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9360 s / batch. (data: 3.12e-04). ETA=14:16:25, max mem: 29.4 GB 
[11/22 04:53:28 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9041 s / batch. (data: 3.12e-04). ETA=13:45:42, max mem: 29.4 GB 
[11/22 04:54:18 visual_prompt]: Epoch 1 / 100: avg data time: 4.17e-02, avg batch time: 0.9692, average train loss: 7.6130
[11/22 04:55:15 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3051, average loss: 6.9126
[11/22 04:55:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/22 04:55:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/22 04:57:01 visual_prompt]: 	Training 100/553. train loss: 1.5441,	0.9455 s / batch. (data: 1.09e-02). ETA=14:21:08, max mem: 29.4 GB 
[11/22 04:58:35 visual_prompt]: 	Training 200/553. train loss: 0.7940,	1.5040 s / batch. (data: 5.67e-01). ETA=22:47:18, max mem: 29.4 GB 
[11/22 05:00:10 visual_prompt]: 	Training 300/553. train loss: 0.9119,	0.9280 s / batch. (data: 3.35e-04). ETA=14:02:06, max mem: 29.4 GB 
[11/22 05:01:48 visual_prompt]: 	Training 400/553. train loss: 1.2284,	0.9256 s / batch. (data: 2.84e-04). ETA=13:58:20, max mem: 29.4 GB 
[11/22 05:03:23 visual_prompt]: 	Training 500/553. train loss: 0.7834,	0.9400 s / batch. (data: 2.99e-04). ETA=14:09:50, max mem: 29.4 GB 
[11/22 05:04:13 visual_prompt]: Epoch 2 / 100: avg data time: 4.55e-02, avg batch time: 0.9714, average train loss: 1.1133
[11/22 05:05:10 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3039, average loss: 0.8919
[11/22 05:05:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.64	
[11/22 05:05:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/22 05:06:55 visual_prompt]: 	Training 100/553. train loss: 0.7072,	0.9198 s / batch. (data: 2.71e-04). ETA=13:49:16, max mem: 29.4 GB 
[11/22 05:08:32 visual_prompt]: 	Training 200/553. train loss: 3.2339,	0.9376 s / batch. (data: 3.76e-02). ETA=14:03:45, max mem: 29.4 GB 
[11/22 05:10:06 visual_prompt]: 	Training 300/553. train loss: 0.8950,	0.9455 s / batch. (data: 2.56e-02). ETA=14:09:16, max mem: 29.4 GB 
[11/22 05:11:40 visual_prompt]: 	Training 400/553. train loss: 0.7544,	0.9360 s / batch. (data: 8.14e-04). ETA=13:59:11, max mem: 29.4 GB 
[11/22 05:13:14 visual_prompt]: 	Training 500/553. train loss: 1.3501,	0.9200 s / batch. (data: 3.03e-04). ETA=13:43:17, max mem: 29.4 GB 
[11/22 05:14:04 visual_prompt]: Epoch 3 / 100: avg data time: 3.71e-02, avg batch time: 0.9648, average train loss: 0.9473
[11/22 05:15:01 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3074, average loss: 1.0275
[11/22 05:15:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.69	
[11/22 05:15:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/22 05:16:46 visual_prompt]: 	Training 100/553. train loss: 1.4454,	0.9400 s / batch. (data: 7.97e-03). ETA=13:58:47, max mem: 29.4 GB 
[11/22 05:18:20 visual_prompt]: 	Training 200/553. train loss: 1.0376,	0.9183 s / batch. (data: 5.43e-03). ETA=13:37:56, max mem: 29.4 GB 
[11/22 05:19:56 visual_prompt]: 	Training 300/553. train loss: 0.7706,	0.9232 s / batch. (data: 3.10e-04). ETA=13:40:43, max mem: 29.4 GB 
[11/22 05:21:30 visual_prompt]: 	Training 400/553. train loss: 0.9187,	0.9361 s / batch. (data: 2.89e-04). ETA=13:50:37, max mem: 29.4 GB 
[11/22 05:23:06 visual_prompt]: 	Training 500/553. train loss: 0.7762,	0.9282 s / batch. (data: 3.26e-04). ETA=13:42:05, max mem: 29.4 GB 
[11/22 05:23:57 visual_prompt]: Epoch 4 / 100: avg data time: 4.29e-02, avg batch time: 0.9697, average train loss: 0.9292
[11/22 05:24:55 visual_prompt]: Inference (val):avg data time: 2.63e-04, avg batch time: 0.3055, average loss: 0.6649
[11/22 05:24:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 62.63	
[11/22 05:24:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/22 05:26:39 visual_prompt]: 	Training 100/553. train loss: 0.7397,	0.9440 s / batch. (data: 2.83e-04). ETA=13:53:41, max mem: 29.4 GB 
[11/22 05:28:16 visual_prompt]: 	Training 200/553. train loss: 0.4729,	0.9280 s / batch. (data: 3.06e-04). ETA=13:37:59, max mem: 29.4 GB 
[11/22 05:29:49 visual_prompt]: 	Training 300/553. train loss: 2.5891,	0.9322 s / batch. (data: 5.45e-03). ETA=13:40:09, max mem: 29.4 GB 
[11/22 05:31:24 visual_prompt]: 	Training 400/553. train loss: 0.9368,	2.1031 s / batch. (data: 1.19e+00). ETA=1 day, 6:46:45, max mem: 29.4 GB 
[11/22 05:33:02 visual_prompt]: 	Training 500/553. train loss: 0.5361,	0.9073 s / batch. (data: 2.85e-04). ETA=13:15:10, max mem: 29.4 GB 
[11/22 05:33:51 visual_prompt]: Epoch 5 / 100: avg data time: 4.24e-02, avg batch time: 0.9694, average train loss: 0.9879
[11/22 05:34:48 visual_prompt]: Inference (val):avg data time: 6.67e-04, avg batch time: 0.3045, average loss: 1.2175
[11/22 05:34:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.55	
[11/22 05:34:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/22 05:36:34 visual_prompt]: 	Training 100/553. train loss: 0.5760,	0.9440 s / batch. (data: 3.36e-04). ETA=13:44:59, max mem: 29.4 GB 
[11/22 05:38:08 visual_prompt]: 	Training 200/553. train loss: 1.6524,	0.9614 s / batch. (data: 5.36e-03). ETA=13:58:33, max mem: 29.4 GB 
[11/22 05:39:44 visual_prompt]: 	Training 300/553. train loss: 0.4990,	0.9200 s / batch. (data: 3.11e-04). ETA=13:20:57, max mem: 29.4 GB 
[11/22 05:41:18 visual_prompt]: 	Training 400/553. train loss: 0.8932,	1.3720 s / batch. (data: 4.71e-01). ETA=19:52:07, max mem: 29.4 GB 
[11/22 05:42:56 visual_prompt]: 	Training 500/553. train loss: 1.1900,	0.9280 s / batch. (data: 2.42e-04). ETA=13:24:48, max mem: 29.4 GB 
[11/22 05:43:45 visual_prompt]: Epoch 6 / 100: avg data time: 4.43e-02, avg batch time: 0.9700, average train loss: 1.0083
[11/22 05:44:42 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3041, average loss: 1.2373
[11/22 05:44:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.90	
[11/22 05:44:42 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/22 05:46:32 visual_prompt]: 	Training 100/553. train loss: 0.4162,	0.9129 s / batch. (data: 4.82e-03). ETA=13:09:22, max mem: 29.4 GB 
[11/22 05:48:07 visual_prompt]: 	Training 200/553. train loss: 0.0385,	0.9399 s / batch. (data: 5.91e-03). ETA=13:31:12, max mem: 29.4 GB 
[11/22 05:49:40 visual_prompt]: 	Training 300/553. train loss: 0.5786,	0.9080 s / batch. (data: 2.62e-04). ETA=13:02:09, max mem: 29.4 GB 
[11/22 05:51:14 visual_prompt]: 	Training 400/553. train loss: 0.9377,	0.9200 s / batch. (data: 3.29e-04). ETA=13:10:54, max mem: 29.4 GB 
[11/22 05:52:48 visual_prompt]: 	Training 500/553. train loss: 0.4670,	0.9400 s / batch. (data: 2.89e-04). ETA=13:26:33, max mem: 29.4 GB 
[11/22 05:53:37 visual_prompt]: Epoch 7 / 100: avg data time: 3.97e-02, avg batch time: 0.9666, average train loss: 1.0154
[11/22 05:54:34 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3063, average loss: 0.7184
[11/22 05:54:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 65.08	
[11/22 05:54:34 visual_prompt]: Best epoch 7: best metric: -0.718
[11/22 05:54:34 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/22 05:56:20 visual_prompt]: 	Training 100/553. train loss: 0.4061,	0.9385 s / batch. (data: 2.88e-04). ETA=13:22:50, max mem: 29.4 GB 
[11/22 05:57:57 visual_prompt]: 	Training 200/553. train loss: 0.6604,	0.9314 s / batch. (data: 5.40e-03). ETA=13:15:13, max mem: 29.4 GB 
[11/22 05:59:31 visual_prompt]: 	Training 300/553. train loss: 1.3828,	0.9077 s / batch. (data: 2.69e-04). ETA=12:53:32, max mem: 29.4 GB 
[11/22 06:01:07 visual_prompt]: 	Training 400/553. train loss: 0.5025,	0.9543 s / batch. (data: 3.28e-02). ETA=13:31:37, max mem: 29.4 GB 
[11/22 06:02:43 visual_prompt]: 	Training 500/553. train loss: 0.9866,	0.9263 s / batch. (data: 2.80e-04). ETA=13:06:13, max mem: 29.4 GB 
[11/22 06:03:32 visual_prompt]: Epoch 8 / 100: avg data time: 4.50e-02, avg batch time: 0.9723, average train loss: 1.0406
[11/22 06:04:30 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3038, average loss: 0.8943
[11/22 06:04:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.89	
[11/22 06:04:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/22 06:06:18 visual_prompt]: 	Training 100/553. train loss: 1.3964,	0.9160 s / batch. (data: 3.04e-04). ETA=12:55:11, max mem: 29.4 GB 
[11/22 06:07:54 visual_prompt]: 	Training 200/553. train loss: 1.0616,	0.9280 s / batch. (data: 3.25e-04). ETA=13:03:46, max mem: 29.4 GB 
[11/22 06:09:28 visual_prompt]: 	Training 300/553. train loss: 2.0850,	0.9200 s / batch. (data: 2.97e-04). ETA=12:55:30, max mem: 29.4 GB 
[11/22 06:11:01 visual_prompt]: 	Training 400/553. train loss: 0.4790,	0.9408 s / batch. (data: 2.81e-04). ETA=13:11:28, max mem: 29.4 GB 
[11/22 06:12:35 visual_prompt]: 	Training 500/553. train loss: 1.8809,	0.9802 s / batch. (data: 3.25e-02). ETA=13:43:00, max mem: 29.4 GB 
[11/22 06:13:24 visual_prompt]: Epoch 9 / 100: avg data time: 3.72e-02, avg batch time: 0.9658, average train loss: 1.1163
[11/22 06:14:21 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3050, average loss: 1.1794
[11/22 06:14:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 64.10	
[11/22 06:14:21 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/22 06:16:07 visual_prompt]: 	Training 100/553. train loss: 1.1036,	0.9560 s / batch. (data: 2.84e-04). ETA=13:20:14, max mem: 29.4 GB 
[11/22 06:17:44 visual_prompt]: 	Training 200/553. train loss: 1.0095,	0.9200 s / batch. (data: 2.80e-04). ETA=12:48:33, max mem: 29.4 GB 
[11/22 06:19:17 visual_prompt]: 	Training 300/553. train loss: 3.0785,	0.9320 s / batch. (data: 8.25e-04). ETA=12:56:59, max mem: 29.4 GB 
[11/22 06:20:50 visual_prompt]: 	Training 400/553. train loss: 2.1415,	0.9200 s / batch. (data: 2.96e-04). ETA=12:45:29, max mem: 29.4 GB 
[11/22 06:22:25 visual_prompt]: 	Training 500/553. train loss: 0.2070,	0.9154 s / batch. (data: 3.10e-04). ETA=12:40:08, max mem: 29.4 GB 
[11/22 06:23:16 visual_prompt]: Epoch 10 / 100: avg data time: 4.01e-02, avg batch time: 0.9663, average train loss: 0.9939
[11/22 06:24:13 visual_prompt]: Inference (val):avg data time: 3.40e-04, avg batch time: 0.3045, average loss: 0.8262
[11/22 06:24:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 69.62	
[11/22 06:24:13 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/22 06:26:02 visual_prompt]: 	Training 100/553. train loss: 2.1758,	0.9320 s / batch. (data: 2.85e-04). ETA=12:51:31, max mem: 29.4 GB 
[11/22 06:27:36 visual_prompt]: 	Training 200/553. train loss: 0.9436,	0.9240 s / batch. (data: 1.05e-02). ETA=12:43:21, max mem: 29.4 GB 
[11/22 06:29:10 visual_prompt]: 	Training 300/553. train loss: 1.9549,	0.9433 s / batch. (data: 7.98e-04). ETA=12:57:45, max mem: 29.4 GB 
[11/22 06:30:44 visual_prompt]: 	Training 400/553. train loss: 0.2625,	0.9414 s / batch. (data: 1.05e-02). ETA=12:54:37, max mem: 29.4 GB 
[11/22 06:32:19 visual_prompt]: 	Training 500/553. train loss: 0.6130,	0.9360 s / batch. (data: 3.97e-03). ETA=12:48:38, max mem: 29.4 GB 
[11/22 06:33:08 visual_prompt]: Epoch 11 / 100: avg data time: 3.90e-02, avg batch time: 0.9674, average train loss: 0.9997
[11/22 06:34:06 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3034, average loss: 0.8162
[11/22 06:34:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 66.79	
[11/22 06:34:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/22 06:35:48 visual_prompt]: 	Training 100/553. train loss: 1.1304,	0.9302 s / batch. (data: 7.83e-04). ETA=12:41:29, max mem: 29.4 GB 
[11/22 06:37:27 visual_prompt]: 	Training 200/553. train loss: 0.6011,	0.9321 s / batch. (data: 7.62e-04). ETA=12:41:27, max mem: 29.4 GB 
[11/22 06:39:02 visual_prompt]: 	Training 300/553. train loss: 1.0077,	0.9560 s / batch. (data: 3.92e-03). ETA=12:59:26, max mem: 29.4 GB 
[11/22 06:40:37 visual_prompt]: 	Training 400/553. train loss: 0.8249,	0.9261 s / batch. (data: 7.66e-04). ETA=12:33:29, max mem: 29.4 GB 
[11/22 06:42:11 visual_prompt]: 	Training 500/553. train loss: 0.5676,	0.9214 s / batch. (data: 8.28e-04). ETA=12:28:07, max mem: 29.4 GB 
[11/22 06:43:02 visual_prompt]: Epoch 12 / 100: avg data time: 4.36e-02, avg batch time: 0.9687, average train loss: 0.9928
[11/22 06:43:59 visual_prompt]: Inference (val):avg data time: 8.16e-05, avg batch time: 0.3034, average loss: 0.6584
[11/22 06:43:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 68.17	
[11/22 06:43:59 visual_prompt]: Best epoch 12: best metric: -0.658
[11/22 06:43:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/22 06:45:46 visual_prompt]: 	Training 100/553. train loss: 0.9750,	0.9200 s / batch. (data: 8.02e-03). ETA=12:24:39, max mem: 29.4 GB 
[11/22 06:47:19 visual_prompt]: 	Training 200/553. train loss: 1.8002,	0.9320 s / batch. (data: 2.70e-04). ETA=12:32:48, max mem: 29.4 GB 
[11/22 06:48:55 visual_prompt]: 	Training 300/553. train loss: 1.7161,	0.9160 s / batch. (data: 2.82e-04). ETA=12:18:21, max mem: 29.4 GB 
[11/22 06:50:32 visual_prompt]: 	Training 400/553. train loss: 1.4118,	0.9520 s / batch. (data: 7.96e-03). ETA=12:45:49, max mem: 29.4 GB 
[11/22 06:52:06 visual_prompt]: 	Training 500/553. train loss: 0.9238,	0.9401 s / batch. (data: 7.05e-04). ETA=12:34:37, max mem: 29.4 GB 
[11/22 06:52:55 visual_prompt]: Epoch 13 / 100: avg data time: 4.31e-02, avg batch time: 0.9692, average train loss: 0.8471
[11/22 06:53:52 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3059, average loss: 0.7183
[11/22 06:53:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 68.77	
[11/22 06:53:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/22 06:55:40 visual_prompt]: 	Training 100/553. train loss: 0.9262,	0.9320 s / batch. (data: 2.97e-04). ETA=12:25:46, max mem: 29.4 GB 
[11/22 06:57:18 visual_prompt]: 	Training 200/553. train loss: 1.7493,	0.9200 s / batch. (data: 7.92e-04). ETA=12:14:37, max mem: 29.4 GB 
[11/22 06:58:53 visual_prompt]: 	Training 300/553. train loss: 1.1101,	0.9473 s / batch. (data: 3.38e-04). ETA=12:34:49, max mem: 29.4 GB 
[11/22 07:00:26 visual_prompt]: 	Training 400/553. train loss: 2.2048,	0.9006 s / batch. (data: 3.23e-04). ETA=11:56:06, max mem: 29.4 GB 
[11/22 07:02:01 visual_prompt]: 	Training 500/553. train loss: 0.7321,	0.9009 s / batch. (data: 3.12e-04). ETA=11:54:55, max mem: 29.4 GB 
[11/22 07:02:50 visual_prompt]: Epoch 14 / 100: avg data time: 4.53e-02, avg batch time: 0.9725, average train loss: 0.9106
[11/22 07:03:48 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3059, average loss: 0.7181
[11/22 07:03:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 69.46	
[11/22 07:03:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/22 07:05:35 visual_prompt]: 	Training 100/553. train loss: 0.4848,	0.9355 s / batch. (data: 2.06e-02). ETA=12:19:55, max mem: 29.4 GB 
[11/22 07:07:08 visual_prompt]: 	Training 200/553. train loss: 0.5252,	0.9240 s / batch. (data: 7.18e-04). ETA=12:09:20, max mem: 29.4 GB 
[11/22 07:08:42 visual_prompt]: 	Training 300/553. train loss: 0.5244,	0.9648 s / batch. (data: 3.27e-02). ETA=12:39:53, max mem: 29.4 GB 
[11/22 07:10:15 visual_prompt]: 	Training 400/553. train loss: 0.5611,	0.9400 s / batch. (data: 7.92e-04). ETA=12:18:50, max mem: 29.4 GB 
[11/22 07:11:48 visual_prompt]: 	Training 500/553. train loss: 0.8310,	0.9016 s / batch. (data: 2.81e-04). ETA=11:47:08, max mem: 29.4 GB 
[11/22 07:12:37 visual_prompt]: Epoch 15 / 100: avg data time: 3.08e-02, avg batch time: 0.9570, average train loss: 0.9226
[11/22 07:13:32 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3056, average loss: 0.6813
[11/22 07:13:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 65.63	
[11/22 07:13:32 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/22 07:15:15 visual_prompt]: 	Training 100/553. train loss: 0.8388,	0.9179 s / batch. (data: 2.98e-04). ETA=11:57:32, max mem: 29.4 GB 
[11/22 07:16:48 visual_prompt]: 	Training 200/553. train loss: 0.3581,	0.9233 s / batch. (data: 5.42e-03). ETA=12:00:17, max mem: 29.4 GB 
[11/22 07:18:21 visual_prompt]: 	Training 300/553. train loss: 0.7068,	0.9118 s / batch. (data: 2.75e-04). ETA=11:49:45, max mem: 29.4 GB 
[11/22 07:19:54 visual_prompt]: 	Training 400/553. train loss: 0.4344,	0.9386 s / batch. (data: 8.29e-04). ETA=12:09:03, max mem: 29.4 GB 
[11/22 07:21:27 visual_prompt]: 	Training 500/553. train loss: 0.4328,	0.9240 s / batch. (data: 5.40e-03). ETA=11:56:11, max mem: 29.4 GB 
[11/22 07:22:17 visual_prompt]: Epoch 16 / 100: avg data time: 2.18e-02, avg batch time: 0.9474, average train loss: 0.8123
[11/22 07:23:12 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.3069, average loss: 0.7064
[11/22 07:23:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.73	
[11/22 07:23:12 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/22 07:24:52 visual_prompt]: 	Training 100/553. train loss: 0.5402,	0.9360 s / batch. (data: 5.40e-03). ETA=12:03:06, max mem: 29.4 GB 
[11/22 07:26:27 visual_prompt]: 	Training 200/553. train loss: 1.0385,	0.9560 s / batch. (data: 1.09e-03). ETA=12:16:56, max mem: 29.4 GB 
[11/22 07:28:00 visual_prompt]: 	Training 300/553. train loss: 0.5389,	0.9038 s / batch. (data: 3.36e-04). ETA=11:35:13, max mem: 29.4 GB 
[11/22 07:29:33 visual_prompt]: 	Training 400/553. train loss: 1.1433,	0.9280 s / batch. (data: 1.10e-03). ETA=11:52:17, max mem: 29.4 GB 
[11/22 07:31:06 visual_prompt]: 	Training 500/553. train loss: 0.5852,	0.9491 s / batch. (data: 2.11e-02). ETA=12:06:53, max mem: 29.4 GB 
[11/22 07:31:55 visual_prompt]: Epoch 17 / 100: avg data time: 2.08e-02, avg batch time: 0.9462, average train loss: 0.9040
[11/22 07:32:50 visual_prompt]: Inference (val):avg data time: 3.18e-04, avg batch time: 0.3035, average loss: 1.2267
[11/22 07:32:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 69.98	
[11/22 07:32:50 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/22 07:34:35 visual_prompt]: 	Training 100/553. train loss: 0.7147,	0.9400 s / batch. (data: 7.99e-03). ETA=11:57:29, max mem: 29.4 GB 
[11/22 07:36:08 visual_prompt]: 	Training 200/553. train loss: 0.8335,	0.9200 s / batch. (data: 5.39e-03). ETA=11:40:42, max mem: 29.4 GB 
[11/22 07:37:42 visual_prompt]: 	Training 300/553. train loss: 0.8185,	0.9400 s / batch. (data: 7.99e-03). ETA=11:54:22, max mem: 29.4 GB 
[11/22 07:39:15 visual_prompt]: 	Training 400/553. train loss: 0.2235,	0.9688 s / batch. (data: 1.10e-02). ETA=12:14:41, max mem: 29.4 GB 
[11/22 07:40:48 visual_prompt]: 	Training 500/553. train loss: 0.6770,	0.9240 s / batch. (data: 7.59e-04). ETA=11:39:09, max mem: 29.4 GB 
[11/22 07:41:37 visual_prompt]: Epoch 18 / 100: avg data time: 2.57e-02, avg batch time: 0.9524, average train loss: 0.8494
[11/22 07:42:33 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3067, average loss: 0.6301
[11/22 07:42:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 68.34	
[11/22 07:42:33 visual_prompt]: Best epoch 18: best metric: -0.630
[11/22 07:42:33 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/22 07:44:15 visual_prompt]: 	Training 100/553. train loss: 0.1922,	0.9434 s / batch. (data: 2.06e-02). ETA=11:51:23, max mem: 29.4 GB 
[11/22 07:45:49 visual_prompt]: 	Training 200/553. train loss: 0.9667,	0.9230 s / batch. (data: 2.93e-04). ETA=11:34:29, max mem: 29.4 GB 
[11/22 07:47:22 visual_prompt]: 	Training 300/553. train loss: 0.4425,	0.9520 s / batch. (data: 7.26e-04). ETA=11:54:45, max mem: 29.4 GB 
[11/22 07:48:55 visual_prompt]: 	Training 400/553. train loss: 0.6977,	0.9173 s / batch. (data: 2.89e-04). ETA=11:27:07, max mem: 29.4 GB 
[11/22 07:50:29 visual_prompt]: 	Training 500/553. train loss: 1.2356,	0.9160 s / batch. (data: 2.90e-04). ETA=11:24:39, max mem: 29.4 GB 
[11/22 07:51:18 visual_prompt]: Epoch 19 / 100: avg data time: 2.28e-02, avg batch time: 0.9494, average train loss: 0.7592
[11/22 07:52:13 visual_prompt]: Inference (val):avg data time: 4.98e-04, avg batch time: 0.3056, average loss: 0.6541
[11/22 07:52:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 66.98	
[11/22 07:52:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/22 07:53:58 visual_prompt]: 	Training 100/553. train loss: 1.5860,	0.9411 s / batch. (data: 2.49e-02). ETA=11:41:00, max mem: 29.4 GB 
[11/22 07:55:31 visual_prompt]: 	Training 200/553. train loss: 0.9729,	0.9264 s / batch. (data: 7.97e-03). ETA=11:28:32, max mem: 29.4 GB 
[11/22 07:57:04 visual_prompt]: 	Training 300/553. train loss: 0.4977,	0.9174 s / batch. (data: 1.56e-02). ETA=11:20:16, max mem: 29.4 GB 
[11/22 07:58:37 visual_prompt]: 	Training 400/553. train loss: 0.8670,	0.9211 s / batch. (data: 8.02e-04). ETA=11:21:29, max mem: 29.4 GB 
[11/22 08:00:11 visual_prompt]: 	Training 500/553. train loss: 0.9174,	0.9360 s / batch. (data: 3.02e-04). ETA=11:30:57, max mem: 29.4 GB 
[11/22 08:01:00 visual_prompt]: Epoch 20 / 100: avg data time: 2.67e-02, avg batch time: 0.9513, average train loss: 0.7880
[11/22 08:01:55 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3043, average loss: 0.8112
[11/22 08:01:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.47	
[11/22 08:01:55 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/22 08:03:39 visual_prompt]: 	Training 100/553. train loss: 0.6835,	0.9423 s / batch. (data: 3.21e-02). ETA=11:33:13, max mem: 29.4 GB 
[11/22 08:05:12 visual_prompt]: 	Training 200/553. train loss: 0.3897,	0.9485 s / batch. (data: 1.56e-02). ETA=11:36:12, max mem: 29.4 GB 
[11/22 08:06:45 visual_prompt]: 	Training 300/553. train loss: 0.4159,	0.9408 s / batch. (data: 3.17e-04). ETA=11:28:58, max mem: 29.4 GB 
[11/22 08:08:19 visual_prompt]: 	Training 400/553. train loss: 0.8408,	0.9320 s / batch. (data: 2.61e-04). ETA=11:20:58, max mem: 29.4 GB 
[11/22 08:09:52 visual_prompt]: 	Training 500/553. train loss: 0.4146,	0.9443 s / batch. (data: 1.10e-02). ETA=11:28:21, max mem: 29.4 GB 
[11/22 08:10:41 visual_prompt]: Epoch 21 / 100: avg data time: 2.50e-02, avg batch time: 0.9506, average train loss: 0.6728
[11/22 08:11:36 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3049, average loss: 0.7049
[11/22 08:11:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.21	
[11/22 08:11:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/22 08:13:18 visual_prompt]: 	Training 100/553. train loss: 1.0262,	0.9449 s / batch. (data: 1.05e-02). ETA=11:26:24, max mem: 29.4 GB 
[11/22 08:14:51 visual_prompt]: 	Training 200/553. train loss: 0.9339,	0.9120 s / batch. (data: 2.86e-04). ETA=11:01:01, max mem: 29.4 GB 
[11/22 08:16:24 visual_prompt]: 	Training 300/553. train loss: 0.1141,	0.9756 s / batch. (data: 1.55e-02). ETA=11:45:26, max mem: 29.4 GB 
[11/22 08:17:57 visual_prompt]: 	Training 400/553. train loss: 1.0969,	0.9400 s / batch. (data: 5.82e-03). ETA=11:18:10, max mem: 29.4 GB 
[11/22 08:19:30 visual_prompt]: 	Training 500/553. train loss: 0.5511,	0.9360 s / batch. (data: 5.39e-03). ETA=11:13:44, max mem: 29.4 GB 
[11/22 08:20:20 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e-02, avg batch time: 0.9466, average train loss: 0.8079
[11/22 08:21:15 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.3049, average loss: 1.2176
[11/22 08:21:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 64.91	
[11/22 08:21:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/22 08:22:59 visual_prompt]: 	Training 100/553. train loss: 0.2275,	0.9240 s / batch. (data: 7.98e-03). ETA=11:02:42, max mem: 29.4 GB 
[11/22 08:24:32 visual_prompt]: 	Training 200/553. train loss: 0.4747,	0.9600 s / batch. (data: 7.60e-04). ETA=11:26:56, max mem: 29.4 GB 
[11/22 08:26:05 visual_prompt]: 	Training 300/553. train loss: 0.2265,	0.9440 s / batch. (data: 7.96e-03). ETA=11:13:55, max mem: 29.4 GB 
[11/22 08:27:38 visual_prompt]: 	Training 400/553. train loss: 0.3594,	0.9120 s / batch. (data: 2.65e-04). ETA=10:49:33, max mem: 29.4 GB 
[11/22 08:29:12 visual_prompt]: 	Training 500/553. train loss: 0.3039,	0.9400 s / batch. (data: 2.83e-04). ETA=11:07:54, max mem: 29.4 GB 
[11/22 08:30:01 visual_prompt]: Epoch 23 / 100: avg data time: 2.54e-02, avg batch time: 0.9508, average train loss: 0.7622
[11/22 08:30:56 visual_prompt]: Inference (val):avg data time: 8.24e-05, avg batch time: 0.3062, average loss: 1.6382
[11/22 08:30:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 67.34	
[11/22 08:30:56 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/22 08:32:39 visual_prompt]: 	Training 100/553. train loss: 1.0096,	0.9098 s / batch. (data: 1.05e-02). ETA=10:44:08, max mem: 29.4 GB 
[11/22 08:34:12 visual_prompt]: 	Training 200/553. train loss: 0.2964,	0.9360 s / batch. (data: 1.60e-02). ETA=11:01:08, max mem: 29.4 GB 
[11/22 08:35:46 visual_prompt]: 	Training 300/553. train loss: 0.4679,	0.9405 s / batch. (data: 1.55e-02). ETA=11:02:47, max mem: 29.4 GB 
[11/22 08:37:19 visual_prompt]: 	Training 400/553. train loss: 0.6632,	0.9280 s / batch. (data: 3.11e-04). ETA=10:52:24, max mem: 29.4 GB 
[11/22 08:38:52 visual_prompt]: 	Training 500/553. train loss: 0.3537,	0.9443 s / batch. (data: 7.61e-04). ETA=11:02:15, max mem: 29.4 GB 
[11/22 08:39:41 visual_prompt]: Epoch 24 / 100: avg data time: 2.31e-02, avg batch time: 0.9497, average train loss: 0.6743
[11/22 08:40:36 visual_prompt]: Inference (val):avg data time: 2.64e-04, avg batch time: 0.3043, average loss: 1.0036
[11/22 08:40:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 66.81	
[11/22 08:40:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/22 08:42:17 visual_prompt]: 	Training 100/553. train loss: 0.7608,	0.9200 s / batch. (data: 3.01e-04). ETA=10:42:52, max mem: 29.4 GB 
[11/22 08:43:53 visual_prompt]: 	Training 200/553. train loss: 0.4759,	0.9606 s / batch. (data: 3.26e-02). ETA=11:09:39, max mem: 29.4 GB 
[11/22 08:45:26 visual_prompt]: 	Training 300/553. train loss: 1.0175,	0.9423 s / batch. (data: 2.06e-02). ETA=10:55:18, max mem: 29.4 GB 
[11/22 08:47:00 visual_prompt]: 	Training 400/553. train loss: 0.9609,	0.9521 s / batch. (data: 1.04e-03). ETA=11:00:32, max mem: 29.4 GB 
[11/22 08:48:33 visual_prompt]: 	Training 500/553. train loss: 0.3009,	0.9440 s / batch. (data: 3.00e-04). ETA=10:53:20, max mem: 29.4 GB 
[11/22 08:49:22 visual_prompt]: Epoch 25 / 100: avg data time: 2.44e-02, avg batch time: 0.9497, average train loss: 0.6543
[11/22 08:50:17 visual_prompt]: Inference (val):avg data time: 5.45e-04, avg batch time: 0.3053, average loss: 1.0879
[11/22 08:50:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.06	
[11/22 08:50:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/22 08:52:00 visual_prompt]: 	Training 100/553. train loss: 0.5319,	0.9068 s / batch. (data: 2.87e-04). ETA=10:25:20, max mem: 29.4 GB 
[11/22 08:53:36 visual_prompt]: 	Training 200/553. train loss: 0.0708,	0.9279 s / batch. (data: 4.85e-04). ETA=10:38:20, max mem: 29.4 GB 
[11/22 08:55:18 visual_prompt]: 	Training 300/553. train loss: 0.0500,	0.9928 s / batch. (data: 1.60e-02). ETA=11:21:19, max mem: 29.4 GB 
[11/22 08:56:53 visual_prompt]: 	Training 400/553. train loss: 0.2659,	0.9198 s / batch. (data: 3.25e-04). ETA=10:29:39, max mem: 29.4 GB 
[11/22 08:58:33 visual_prompt]: 	Training 500/553. train loss: 0.4814,	0.9160 s / batch. (data: 3.20e-04). ETA=10:25:33, max mem: 29.4 GB 
[11/22 08:59:25 visual_prompt]: Epoch 26 / 100: avg data time: 6.36e-02, avg batch time: 0.9905, average train loss: 0.7001
[11/22 09:00:25 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3040, average loss: 1.0430
[11/22 09:00:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 67.75	
[11/22 09:00:25 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/22 09:02:13 visual_prompt]: 	Training 100/553. train loss: 0.7079,	0.9480 s / batch. (data: 3.95e-03). ETA=10:44:58, max mem: 29.4 GB 
[11/22 09:03:52 visual_prompt]: 	Training 200/553. train loss: 0.3752,	0.9160 s / batch. (data: 2.89e-04). ETA=10:21:40, max mem: 29.4 GB 
[11/22 09:05:26 visual_prompt]: 	Training 300/553. train loss: 2.2622,	0.9522 s / batch. (data: 1.56e-02). ETA=10:44:41, max mem: 29.4 GB 
[11/22 09:07:06 visual_prompt]: 	Training 400/553. train loss: 0.6919,	0.9345 s / batch. (data: 2.99e-04). ETA=10:31:09, max mem: 29.4 GB 
[11/22 09:08:44 visual_prompt]: 	Training 500/553. train loss: 0.7499,	0.9160 s / batch. (data: 3.14e-04). ETA=10:17:06, max mem: 29.4 GB 
[11/22 09:09:35 visual_prompt]: Epoch 27 / 100: avg data time: 6.85e-02, avg batch time: 0.9937, average train loss: 0.6598
[11/22 09:10:33 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3073, average loss: 0.9431
[11/22 09:10:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 67.51	
[11/22 09:10:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.000873410738492077
[11/22 09:12:19 visual_prompt]: 	Training 100/553. train loss: 0.6120,	0.9320 s / batch. (data: 2.94e-04). ETA=10:25:31, max mem: 29.4 GB 
[11/22 09:13:58 visual_prompt]: 	Training 200/553. train loss: 0.1296,	0.9495 s / batch. (data: 2.95e-02). ETA=10:35:41, max mem: 29.4 GB 
[11/22 09:15:35 visual_prompt]: 	Training 300/553. train loss: 0.4450,	0.9108 s / batch. (data: 7.95e-03). ETA=10:08:16, max mem: 29.4 GB 
[11/22 09:17:12 visual_prompt]: 	Training 400/553. train loss: 0.5401,	0.9239 s / batch. (data: 3.42e-04). ETA=10:15:26, max mem: 29.4 GB 
[11/22 09:18:49 visual_prompt]: 	Training 500/553. train loss: 0.2993,	0.9053 s / batch. (data: 3.19e-04). ETA=10:01:32, max mem: 29.4 GB 
[11/22 09:19:40 visual_prompt]: Epoch 28 / 100: avg data time: 6.20e-02, avg batch time: 0.9877, average train loss: 0.5550
[11/22 09:20:38 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3068, average loss: 1.0183
[11/22 09:20:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 67.51	
[11/22 09:20:38 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.0008622126023955446
[11/22 09:22:27 visual_prompt]: 	Training 100/553. train loss: 1.2148,	0.9592 s / batch. (data: 1.05e-02). ETA=10:34:56, max mem: 29.4 GB 
[11/22 09:24:05 visual_prompt]: 	Training 200/553. train loss: 0.4066,	0.9480 s / batch. (data: 2.91e-04). ETA=10:25:56, max mem: 29.4 GB 
[11/22 09:25:44 visual_prompt]: 	Training 300/553. train loss: 0.5358,	0.9409 s / batch. (data: 5.39e-03). ETA=10:19:42, max mem: 29.4 GB 
[11/22 09:27:23 visual_prompt]: 	Training 400/553. train loss: 1.1989,	0.9562 s / batch. (data: 5.92e-03). ETA=10:28:09, max mem: 29.4 GB 
[11/22 09:28:56 visual_prompt]: 	Training 500/553. train loss: 0.5499,	0.9640 s / batch. (data: 7.83e-04). ETA=10:31:40, max mem: 29.4 GB 
[11/22 09:29:46 visual_prompt]: Epoch 29 / 100: avg data time: 6.32e-02, avg batch time: 0.9895, average train loss: 0.5484
[11/22 09:30:44 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3039, average loss: 1.0285
[11/22 09:30:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 66.56	
[11/22 09:30:44 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0008506183921362443
[11/22 09:32:31 visual_prompt]: 	Training 100/553. train loss: 0.2707,	0.9454 s / batch. (data: 2.06e-02). ETA=10:17:05, max mem: 29.4 GB 
[11/22 09:34:09 visual_prompt]: 	Training 200/553. train loss: 0.1479,	0.9360 s / batch. (data: 3.16e-04). ETA=10:09:23, max mem: 29.4 GB 
[11/22 09:35:45 visual_prompt]: 	Training 300/553. train loss: 0.9145,	0.9495 s / batch. (data: 1.60e-02). ETA=10:16:33, max mem: 29.4 GB 
[11/22 09:37:21 visual_prompt]: 	Training 400/553. train loss: 0.0970,	0.9240 s / batch. (data: 2.95e-04). ETA=9:58:29, max mem: 29.4 GB 
[11/22 09:38:58 visual_prompt]: 	Training 500/553. train loss: 0.2780,	0.9480 s / batch. (data: 3.23e-04). ETA=10:12:27, max mem: 29.4 GB 
[11/22 09:39:47 visual_prompt]: Epoch 30 / 100: avg data time: 5.56e-02, avg batch time: 0.9816, average train loss: 0.5232
[11/22 09:40:46 visual_prompt]: Inference (val):avg data time: 1.44e-04, avg batch time: 0.3038, average loss: 1.0534
[11/22 09:40:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 64.69	
[11/22 09:40:46 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0008386407858128706
[11/22 09:42:36 visual_prompt]: 	Training 100/553. train loss: 1.1749,	0.9520 s / batch. (data: 3.34e-04). ETA=10:12:37, max mem: 29.4 GB 
[11/22 09:44:10 visual_prompt]: 	Training 200/553. train loss: 0.2663,	0.9472 s / batch. (data: 2.31e-02). ETA=10:07:55, max mem: 29.4 GB 
[11/22 09:45:48 visual_prompt]: 	Training 300/553. train loss: 0.5624,	0.9178 s / batch. (data: 7.95e-03). ETA=9:47:31, max mem: 29.4 GB 
[11/22 09:47:24 visual_prompt]: 	Training 400/553. train loss: 1.3082,	0.9121 s / batch. (data: 2.98e-04). ETA=9:42:24, max mem: 29.4 GB 
[11/22 09:48:59 visual_prompt]: 	Training 500/553. train loss: 0.0032,	0.9280 s / batch. (data: 7.96e-03). ETA=9:50:58, max mem: 29.4 GB 
[11/22 09:49:51 visual_prompt]: Epoch 31 / 100: avg data time: 5.93e-02, avg batch time: 0.9856, average train loss: 0.4903
[11/22 09:50:49 visual_prompt]: Inference (val):avg data time: 9.07e-05, avg batch time: 0.3057, average loss: 1.0726
[11/22 09:50:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 66.34	
[11/22 09:50:49 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.0008262928807620843
[11/22 09:52:35 visual_prompt]: 	Training 100/553. train loss: 0.4368,	0.9240 s / batch. (data: 5.42e-03). ETA=9:46:03, max mem: 29.4 GB 
[11/22 09:54:13 visual_prompt]: 	Training 200/553. train loss: 1.6586,	0.9440 s / batch. (data: 8.05e-04). ETA=9:57:10, max mem: 29.4 GB 
[11/22 09:55:49 visual_prompt]: 	Training 300/553. train loss: 0.9096,	0.9280 s / batch. (data: 5.52e-03). ETA=9:45:32, max mem: 29.4 GB 
[11/22 09:57:27 visual_prompt]: 	Training 400/553. train loss: 0.1668,	0.9400 s / batch. (data: 7.96e-03). ETA=9:51:31, max mem: 29.4 GB 
[11/22 09:59:07 visual_prompt]: 	Training 500/553. train loss: 0.1498,	0.9782 s / batch. (data: 2.22e-02). ETA=10:13:57, max mem: 29.4 GB 
[11/22 09:59:56 visual_prompt]: Epoch 32 / 100: avg data time: 6.05e-02, avg batch time: 0.9875, average train loss: 0.5281
[11/22 10:00:54 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3031, average loss: 1.0493
[11/22 10:00:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.45	
[11/22 10:00:54 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.0008135881792367685
[11/22 10:02:42 visual_prompt]: 	Training 100/553. train loss: 0.1903,	0.9099 s / batch. (data: 3.15e-04). ETA=9:28:45, max mem: 29.4 GB 
[11/22 10:04:19 visual_prompt]: 	Training 200/553. train loss: 0.5249,	0.9280 s / batch. (data: 2.91e-04). ETA=9:38:30, max mem: 29.4 GB 
[11/22 10:05:55 visual_prompt]: 	Training 300/553. train loss: 0.7549,	0.9285 s / batch. (data: 5.44e-03). ETA=9:37:18, max mem: 29.4 GB 
[11/22 10:07:32 visual_prompt]: 	Training 400/553. train loss: 0.5422,	0.9299 s / batch. (data: 5.42e-03). ETA=9:36:35, max mem: 29.4 GB 
[11/22 10:09:08 visual_prompt]: 	Training 500/553. train loss: 0.3651,	0.9361 s / batch. (data: 3.94e-03). ETA=9:38:54, max mem: 29.4 GB 
[11/22 10:09:59 visual_prompt]: Epoch 33 / 100: avg data time: 5.84e-02, avg batch time: 0.9845, average train loss: 0.4317
[11/22 10:10:57 visual_prompt]: Inference (val):avg data time: 2.54e-04, avg batch time: 0.3048, average loss: 1.3553
[11/22 10:10:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 61.30	
[11/22 10:10:57 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.0008005405736415125
[11/22 10:12:44 visual_prompt]: 	Training 100/553. train loss: 0.1839,	0.9440 s / batch. (data: 2.92e-04). ETA=9:41:21, max mem: 29.4 GB 
[11/22 10:14:20 visual_prompt]: 	Training 200/553. train loss: 0.0146,	0.9296 s / batch. (data: 3.45e-04). ETA=9:30:55, max mem: 29.4 GB 
[11/22 10:15:59 visual_prompt]: 	Training 300/553. train loss: 0.5667,	0.9360 s / batch. (data: 1.05e-02). ETA=9:33:20, max mem: 29.4 GB 
[11/22 10:17:35 visual_prompt]: 	Training 400/553. train loss: 1.0859,	0.9481 s / batch. (data: 1.05e-02). ETA=9:39:09, max mem: 29.4 GB 
[11/22 10:19:11 visual_prompt]: 	Training 500/553. train loss: 1.2327,	0.9185 s / batch. (data: 3.03e-04). ETA=9:19:32, max mem: 29.4 GB 
[11/22 10:20:02 visual_prompt]: Epoch 34 / 100: avg data time: 5.78e-02, avg batch time: 0.9836, average train loss: 0.3914
[11/22 10:21:00 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3064, average loss: 1.0909
[11/22 10:21:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 64.30	
[11/22 10:21:00 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0007871643313414718
[11/22 10:22:46 visual_prompt]: 	Training 100/553. train loss: 0.0772,	0.9304 s / batch. (data: 6.40e-03). ETA=9:24:25, max mem: 29.4 GB 
[11/22 10:24:22 visual_prompt]: 	Training 200/553. train loss: 0.0290,	0.9254 s / batch. (data: 3.96e-03). ETA=9:19:50, max mem: 29.4 GB 
[11/22 10:25:56 visual_prompt]: 	Training 300/553. train loss: 0.1241,	0.9760 s / batch. (data: 5.95e-03). ETA=9:48:49, max mem: 29.4 GB 
[11/22 10:27:31 visual_prompt]: 	Training 400/553. train loss: 0.2575,	0.9120 s / batch. (data: 2.86e-04). ETA=9:08:41, max mem: 29.4 GB 
[11/22 10:29:12 visual_prompt]: 	Training 500/553. train loss: 0.3813,	0.9367 s / batch. (data: 2.07e-02). ETA=9:21:59, max mem: 29.4 GB 
[11/22 10:30:03 visual_prompt]: Epoch 35 / 100: avg data time: 5.29e-02, avg batch time: 0.9807, average train loss: 0.3767
[11/22 10:31:01 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3034, average loss: 1.2554
[11/22 10:31:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 62.55	
[11/22 10:31:01 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.0007734740790612135
[11/22 10:32:46 visual_prompt]: 	Training 100/553. train loss: 0.6458,	0.9480 s / batch. (data: 2.85e-04). ETA=9:26:21, max mem: 29.4 GB 
[11/22 10:34:25 visual_prompt]: 	Training 200/553. train loss: 0.1360,	0.9200 s / batch. (data: 3.96e-03). ETA=9:08:05, max mem: 29.4 GB 
[11/22 10:36:00 visual_prompt]: 	Training 300/553. train loss: 0.1410,	0.9381 s / batch. (data: 1.05e-02). ETA=9:17:19, max mem: 29.4 GB 
[11/22 10:37:36 visual_prompt]: 	Training 400/553. train loss: 0.0964,	0.9280 s / batch. (data: 3.11e-04). ETA=9:09:47, max mem: 29.4 GB 
[11/22 10:39:14 visual_prompt]: 	Training 500/553. train loss: 0.1409,	0.9322 s / batch. (data: 5.01e-04). ETA=9:10:40, max mem: 29.4 GB 
[11/22 10:40:04 visual_prompt]: Epoch 36 / 100: avg data time: 5.42e-02, avg batch time: 0.9805, average train loss: 0.3180
[11/22 10:41:02 visual_prompt]: Inference (val):avg data time: 8.42e-05, avg batch time: 0.3057, average loss: 1.4340
[11/22 10:41:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 62.38	
[11/22 10:41:02 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.0007594847868906076
[11/22 10:42:51 visual_prompt]: 	Training 100/553. train loss: 0.0043,	0.9160 s / batch. (data: 3.94e-03). ETA=8:58:46, max mem: 29.4 GB 
[11/22 10:44:28 visual_prompt]: 	Training 200/553. train loss: 0.0210,	0.9520 s / batch. (data: 1.05e-02). ETA=9:18:23, max mem: 29.4 GB 
[11/22 10:46:06 visual_prompt]: 	Training 300/553. train loss: 0.0328,	0.9180 s / batch. (data: 3.22e-04). ETA=8:56:52, max mem: 29.4 GB 
[11/22 10:47:42 visual_prompt]: 	Training 400/553. train loss: 0.4086,	0.9280 s / batch. (data: 2.79e-04). ETA=9:01:12, max mem: 29.4 GB 
[11/22 10:49:15 visual_prompt]: 	Training 500/553. train loss: 0.7909,	0.9010 s / batch. (data: 2.90e-04). ETA=8:43:55, max mem: 29.4 GB 
[11/22 10:50:06 visual_prompt]: Epoch 37 / 100: avg data time: 5.82e-02, avg batch time: 0.9843, average train loss: 0.2990
[11/22 10:51:05 visual_prompt]: Inference (val):avg data time: 3.82e-04, avg batch time: 0.3037, average loss: 1.3647
[11/22 10:51:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 67.81	
[11/22 10:51:05 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.0007452117519152541
[11/22 10:52:53 visual_prompt]: 	Training 100/553. train loss: 0.0133,	0.9360 s / batch. (data: 1.05e-02). ETA=9:01:55, max mem: 29.4 GB 
[11/22 10:54:31 visual_prompt]: 	Training 200/553. train loss: 0.1646,	0.9231 s / batch. (data: 6.57e-03). ETA=8:52:53, max mem: 29.4 GB 
[11/22 10:56:07 visual_prompt]: 	Training 300/553. train loss: 0.2071,	0.9573 s / batch. (data: 2.22e-02). ETA=9:11:03, max mem: 29.4 GB 
[11/22 10:57:45 visual_prompt]: 	Training 400/553. train loss: 0.0027,	0.9336 s / batch. (data: 8.00e-03). ETA=8:55:52, max mem: 29.4 GB 
[11/22 10:59:18 visual_prompt]: 	Training 500/553. train loss: 0.6393,	0.9386 s / batch. (data: 2.90e-04). ETA=8:57:09, max mem: 29.4 GB 
[11/22 11:00:10 visual_prompt]: Epoch 38 / 100: avg data time: 5.82e-02, avg batch time: 0.9849, average train loss: 0.2660
[11/22 11:01:08 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3049, average loss: 1.2148
[11/22 11:01:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.90	
[11/22 11:01:08 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.000730670581489344
[11/22 11:02:55 visual_prompt]: 	Training 100/553. train loss: 0.0071,	0.9209 s / batch. (data: 2.92e-04). ETA=8:44:40, max mem: 29.4 GB 
[11/22 11:04:31 visual_prompt]: 	Training 200/553. train loss: 0.0040,	0.9560 s / batch. (data: 3.43e-04). ETA=9:03:06, max mem: 29.4 GB 
[11/22 11:06:09 visual_prompt]: 	Training 300/553. train loss: 0.0090,	0.8966 s / batch. (data: 3.06e-04). ETA=8:27:51, max mem: 29.4 GB 
[11/22 11:07:47 visual_prompt]: 	Training 400/553. train loss: 0.1065,	0.9280 s / batch. (data: 7.92e-04). ETA=8:44:05, max mem: 29.4 GB 
[11/22 11:09:24 visual_prompt]: 	Training 500/553. train loss: 0.0151,	0.9400 s / batch. (data: 5.92e-03). ETA=8:49:19, max mem: 29.4 GB 
[11/22 11:10:13 visual_prompt]: Epoch 39 / 100: avg data time: 5.84e-02, avg batch time: 0.9852, average train loss: 0.2484
[11/22 11:11:12 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3058, average loss: 1.2225
[11/22 11:11:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 63.44	
[11/22 11:11:12 visual_prompt]: Stopping early.
[11/22 11:11:12 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 11:11:12 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 11:11:12 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/22 11:11:12 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/22 11:11:12 visual_prompt]: Training with config:
[11/22 11:11:12 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/22 11:11:12 visual_prompt]: Loading training data...
[11/22 11:11:12 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 11:11:12 visual_prompt]: Loading validation data...
[11/22 11:11:12 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 11:11:12 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 11:11:14 visual_prompt]: Enable all parameters update during training
[11/22 11:11:14 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/22 11:11:14 visual_prompt]: tuned percent:100.000
[11/22 11:11:14 visual_prompt]: Device used for model: 0
[11/22 11:11:14 visual_prompt]: Setting up Evaluator...
[11/22 11:11:14 visual_prompt]: Setting up Trainer...
[11/22 11:11:14 visual_prompt]: 	Setting up the optimizer...
[11/22 11:11:14 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 11:12:59 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9240 s / batch. (data: 2.77e-04). ETA=14:10:05, max mem: 30.7 GB 
[11/22 11:14:39 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9320 s / batch. (data: 3.18e-04). ETA=14:15:52, max mem: 30.7 GB 
[11/22 11:16:17 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9300 s / batch. (data: 2.43e-04). ETA=14:12:31, max mem: 30.7 GB 
[11/22 11:17:51 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9511 s / batch. (data: 5.38e-03). ETA=14:30:13, max mem: 30.7 GB 
[11/22 11:19:29 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9240 s / batch. (data: 2.85e-04). ETA=14:03:54, max mem: 30.7 GB 
[11/22 11:20:18 visual_prompt]: Epoch 1 / 100: avg data time: 5.16e-02, avg batch time: 0.9838, average train loss: 7.6130
[11/22 11:21:17 visual_prompt]: Inference (val):avg data time: 8.44e-05, avg batch time: 0.3044, average loss: 6.9126
[11/22 11:21:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/22 11:21:17 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/22 11:23:04 visual_prompt]: 	Training 100/553. train loss: 0.8513,	0.9480 s / batch. (data: 7.25e-04). ETA=14:23:27, max mem: 30.7 GB 
[11/22 11:24:39 visual_prompt]: 	Training 200/553. train loss: 1.5982,	2.1736 s / batch. (data: 1.26e+00). ETA=1 day, 8:56:03, max mem: 30.7 GB 
[11/22 11:26:16 visual_prompt]: 	Training 300/553. train loss: 0.8438,	0.9195 s / batch. (data: 5.12e-03). ETA=13:54:23, max mem: 30.7 GB 
[11/22 11:27:54 visual_prompt]: 	Training 400/553. train loss: 0.6157,	0.9423 s / batch. (data: 5.52e-03). ETA=14:13:31, max mem: 30.7 GB 
[11/22 11:29:31 visual_prompt]: 	Training 500/553. train loss: 0.9069,	0.9320 s / batch. (data: 3.30e-04). ETA=14:02:37, max mem: 30.7 GB 
[11/22 11:30:21 visual_prompt]: Epoch 2 / 100: avg data time: 5.22e-02, avg batch time: 0.9834, average train loss: 0.9656
[11/22 11:31:19 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3061, average loss: 1.1204
[11/22 11:31:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.22	
[11/22 11:31:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/22 11:33:06 visual_prompt]: 	Training 100/553. train loss: 0.6865,	0.9478 s / batch. (data: 5.85e-03). ETA=14:14:32, max mem: 30.7 GB 
[11/22 11:34:43 visual_prompt]: 	Training 200/553. train loss: 2.5361,	0.9263 s / batch. (data: 2.91e-04). ETA=13:53:34, max mem: 30.7 GB 
[11/22 11:36:20 visual_prompt]: 	Training 300/553. train loss: 0.9344,	0.9286 s / batch. (data: 2.57e-04). ETA=13:54:08, max mem: 30.7 GB 
[11/22 11:37:56 visual_prompt]: 	Training 400/553. train loss: 0.7066,	0.9440 s / batch. (data: 2.71e-04). ETA=14:06:20, max mem: 30.7 GB 
[11/22 11:39:31 visual_prompt]: 	Training 500/553. train loss: 1.0653,	0.9600 s / batch. (data: 2.71e-04). ETA=14:19:07, max mem: 30.7 GB 
[11/22 11:40:21 visual_prompt]: Epoch 3 / 100: avg data time: 4.75e-02, avg batch time: 0.9796, average train loss: 0.8295
[11/22 11:41:19 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3045, average loss: 0.6953
[11/22 11:41:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 61.64	
[11/22 11:41:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/22 11:43:06 visual_prompt]: 	Training 100/553. train loss: 0.6718,	0.9400 s / batch. (data: 2.71e-04). ETA=13:58:51, max mem: 30.7 GB 
[11/22 11:44:42 visual_prompt]: 	Training 200/553. train loss: 0.9921,	1.4685 s / batch. (data: 5.21e-01). ETA=21:47:58, max mem: 30.7 GB 
[11/22 11:46:18 visual_prompt]: 	Training 300/553. train loss: 0.9739,	0.9513 s / batch. (data: 5.37e-03). ETA=14:05:43, max mem: 30.7 GB 
[11/22 11:47:55 visual_prompt]: 	Training 400/553. train loss: 0.5086,	0.9527 s / batch. (data: 7.99e-03). ETA=14:05:21, max mem: 30.7 GB 
[11/22 11:49:32 visual_prompt]: 	Training 500/553. train loss: 0.9723,	0.9415 s / batch. (data: 3.07e-04). ETA=13:53:54, max mem: 30.7 GB 
[11/22 11:50:24 visual_prompt]: Epoch 4 / 100: avg data time: 5.38e-02, avg batch time: 0.9848, average train loss: 0.8106
[11/22 11:51:22 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3074, average loss: 0.6956
[11/22 11:51:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.68	
[11/22 11:51:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/22 11:53:11 visual_prompt]: 	Training 100/553. train loss: 0.6789,	0.9073 s / batch. (data: 2.45e-04). ETA=13:21:15, max mem: 30.7 GB 
[11/22 11:54:48 visual_prompt]: 	Training 200/553. train loss: 0.6411,	0.9328 s / batch. (data: 3.04e-04). ETA=13:42:15, max mem: 30.7 GB 
[11/22 11:56:24 visual_prompt]: 	Training 300/553. train loss: 0.7581,	0.9469 s / batch. (data: 5.43e-03). ETA=13:53:03, max mem: 30.7 GB 
[11/22 11:58:03 visual_prompt]: 	Training 400/553. train loss: 1.1937,	3.9567 s / batch. (data: 3.04e+00). ETA=2 days, 9:54:32, max mem: 30.7 GB 
[11/22 11:59:40 visual_prompt]: 	Training 500/553. train loss: 0.7503,	0.9360 s / batch. (data: 7.36e-04). ETA=13:40:23, max mem: 30.7 GB 
[11/22 12:00:30 visual_prompt]: Epoch 5 / 100: avg data time: 5.81e-02, avg batch time: 0.9901, average train loss: 0.7724
[11/22 12:01:29 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3046, average loss: 0.8424
[11/22 12:01:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 64.18	
[11/22 12:01:29 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/22 12:03:17 visual_prompt]: 	Training 100/553. train loss: 0.7041,	0.9200 s / batch. (data: 5.42e-03). ETA=13:23:59, max mem: 30.7 GB 
[11/22 12:04:52 visual_prompt]: 	Training 200/553. train loss: 0.7292,	0.9186 s / batch. (data: 2.88e-04). ETA=13:21:14, max mem: 30.7 GB 
[11/22 12:06:29 visual_prompt]: 	Training 300/553. train loss: 1.2312,	0.9418 s / batch. (data: 3.98e-03). ETA=13:39:54, max mem: 30.7 GB 
[11/22 12:08:06 visual_prompt]: 	Training 400/553. train loss: 0.8251,	2.1763 s / batch. (data: 1.24e+00). ETA=1 day, 7:30:58, max mem: 30.7 GB 
[11/22 12:09:45 visual_prompt]: 	Training 500/553. train loss: 1.0673,	0.9258 s / batch. (data: 7.39e-04). ETA=13:22:53, max mem: 30.7 GB 
[11/22 12:10:35 visual_prompt]: Epoch 6 / 100: avg data time: 5.69e-02, avg batch time: 0.9879, average train loss: 0.7446
[11/22 12:11:34 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3039, average loss: 0.6716
[11/22 12:11:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 63.45	
[11/22 12:11:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/22 12:13:26 visual_prompt]: 	Training 100/553. train loss: 0.6907,	0.9160 s / batch. (data: 2.67e-04). ETA=13:12:03, max mem: 30.7 GB 
[11/22 12:15:01 visual_prompt]: 	Training 200/553. train loss: 0.5593,	0.9310 s / batch. (data: 1.13e-02). ETA=13:23:29, max mem: 30.7 GB 
[11/22 12:16:37 visual_prompt]: 	Training 300/553. train loss: 0.6930,	0.9523 s / batch. (data: 7.49e-04). ETA=13:40:16, max mem: 30.7 GB 
[11/22 12:18:11 visual_prompt]: 	Training 400/553. train loss: 0.6873,	0.9182 s / batch. (data: 3.08e-04). ETA=13:09:24, max mem: 30.7 GB 
[11/22 12:19:49 visual_prompt]: 	Training 500/553. train loss: 0.6061,	0.9500 s / batch. (data: 1.05e-02). ETA=13:35:08, max mem: 30.7 GB 
[11/22 12:20:39 visual_prompt]: Epoch 7 / 100: avg data time: 5.44e-02, avg batch time: 0.9861, average train loss: 0.7468
[11/22 12:21:38 visual_prompt]: Inference (val):avg data time: 8.35e-05, avg batch time: 0.3081, average loss: 0.6723
[11/22 12:21:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.31	
[11/22 12:21:38 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/22 12:23:25 visual_prompt]: 	Training 100/553. train loss: 0.6438,	0.9358 s / batch. (data: 1.05e-02). ETA=13:20:33, max mem: 30.7 GB 
[11/22 12:25:03 visual_prompt]: 	Training 200/553. train loss: 0.5069,	0.9403 s / batch. (data: 5.37e-03). ETA=13:22:52, max mem: 30.7 GB 
[11/22 12:26:41 visual_prompt]: 	Training 300/553. train loss: 0.8445,	0.9473 s / batch. (data: 7.71e-04). ETA=13:27:15, max mem: 30.7 GB 
[11/22 12:28:18 visual_prompt]: 	Training 400/553. train loss: 0.5897,	0.9425 s / batch. (data: 2.24e-04). ETA=13:21:35, max mem: 30.7 GB 
[11/22 12:29:56 visual_prompt]: 	Training 500/553. train loss: 0.6475,	0.9193 s / batch. (data: 2.81e-04). ETA=13:00:17, max mem: 30.7 GB 
[11/22 12:30:45 visual_prompt]: Epoch 8 / 100: avg data time: 5.83e-02, avg batch time: 0.9892, average train loss: 0.7387
[11/22 12:31:44 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3044, average loss: 0.6565
[11/22 12:31:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 66.30	
[11/22 12:31:44 visual_prompt]: Best epoch 8: best metric: -0.657
[11/22 12:31:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/22 12:33:34 visual_prompt]: 	Training 100/553. train loss: 0.8672,	0.9365 s / batch. (data: 1.00e-02). ETA=13:12:32, max mem: 30.7 GB 
[11/22 12:35:12 visual_prompt]: 	Training 200/553. train loss: 1.1425,	0.9331 s / batch. (data: 5.39e-03). ETA=13:08:04, max mem: 30.7 GB 
[11/22 12:36:47 visual_prompt]: 	Training 300/553. train loss: 0.5695,	0.9280 s / batch. (data: 2.62e-04). ETA=13:02:14, max mem: 30.7 GB 
[11/22 12:38:23 visual_prompt]: 	Training 400/553. train loss: 0.7844,	0.9240 s / batch. (data: 8.29e-04). ETA=12:57:21, max mem: 30.7 GB 
[11/22 12:39:57 visual_prompt]: 	Training 500/553. train loss: 0.7561,	0.9442 s / batch. (data: 4.13e-03). ETA=13:12:43, max mem: 30.7 GB 
[11/22 12:40:48 visual_prompt]: Epoch 9 / 100: avg data time: 5.39e-02, avg batch time: 0.9846, average train loss: 0.7547
[11/22 12:41:47 visual_prompt]: Inference (val):avg data time: 2.31e-04, avg batch time: 0.3057, average loss: 0.7306
[11/22 12:41:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 66.02	
[11/22 12:41:47 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/22 12:43:34 visual_prompt]: 	Training 100/553. train loss: 0.6737,	0.9606 s / batch. (data: 5.39e-03). ETA=13:24:02, max mem: 30.7 GB 
[11/22 12:45:13 visual_prompt]: 	Training 200/553. train loss: 0.9183,	0.9280 s / batch. (data: 2.68e-04). ETA=12:55:13, max mem: 30.7 GB 
[11/22 12:46:48 visual_prompt]: 	Training 300/553. train loss: 0.7657,	0.9361 s / batch. (data: 7.84e-04). ETA=13:00:25, max mem: 30.7 GB 
[11/22 12:48:23 visual_prompt]: 	Training 400/553. train loss: 0.5113,	0.9549 s / batch. (data: 1.48e-02). ETA=13:14:30, max mem: 30.7 GB 
[11/22 12:49:59 visual_prompt]: 	Training 500/553. train loss: 0.6439,	0.9564 s / batch. (data: 1.50e-02). ETA=13:14:11, max mem: 30.7 GB 
[11/22 12:50:51 visual_prompt]: Epoch 10 / 100: avg data time: 5.25e-02, avg batch time: 0.9841, average train loss: 0.7352
[11/22 12:51:50 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3036, average loss: 0.7702
[11/22 12:51:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.84	
[11/22 12:51:50 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/22 12:53:41 visual_prompt]: 	Training 100/553. train loss: 1.0376,	0.9200 s / batch. (data: 2.64e-04). ETA=12:41:36, max mem: 30.7 GB 
[11/22 12:55:16 visual_prompt]: 	Training 200/553. train loss: 0.8345,	0.9440 s / batch. (data: 2.88e-04). ETA=12:59:53, max mem: 30.7 GB 
[11/22 12:56:49 visual_prompt]: 	Training 300/553. train loss: 0.5044,	0.9480 s / batch. (data: 5.84e-03). ETA=13:01:38, max mem: 30.7 GB 
[11/22 12:58:28 visual_prompt]: 	Training 400/553. train loss: 0.9051,	0.9312 s / batch. (data: 2.60e-04). ETA=12:46:13, max mem: 30.7 GB 
[11/22 13:00:03 visual_prompt]: 	Training 500/553. train loss: 0.4996,	0.9266 s / batch. (data: 2.68e-04). ETA=12:40:53, max mem: 30.7 GB 
[11/22 13:00:53 visual_prompt]: Epoch 11 / 100: avg data time: 5.27e-02, avg batch time: 0.9827, average train loss: 0.7196
[11/22 13:01:52 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3058, average loss: 0.6777
[11/22 13:01:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.90	
[11/22 13:01:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/22 13:03:36 visual_prompt]: 	Training 100/553. train loss: 0.5457,	0.9260 s / batch. (data: 5.37e-03). ETA=12:38:01, max mem: 30.7 GB 
[11/22 13:05:17 visual_prompt]: 	Training 200/553. train loss: 0.8439,	0.9354 s / batch. (data: 7.80e-03). ETA=12:44:11, max mem: 30.7 GB 
[11/22 13:06:53 visual_prompt]: 	Training 300/553. train loss: 0.8217,	0.9580 s / batch. (data: 3.11e-04). ETA=13:01:01, max mem: 30.7 GB 
[11/22 13:08:30 visual_prompt]: 	Training 400/553. train loss: 0.9145,	0.9465 s / batch. (data: 2.57e-04). ETA=12:50:06, max mem: 30.7 GB 
[11/22 13:10:06 visual_prompt]: 	Training 500/553. train loss: 0.6788,	0.9320 s / batch. (data: 2.68e-04). ETA=12:36:44, max mem: 30.7 GB 
[11/22 13:10:57 visual_prompt]: Epoch 12 / 100: avg data time: 5.58e-02, avg batch time: 0.9859, average train loss: 0.7851
[11/22 13:11:56 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3037, average loss: 1.0087
[11/22 13:11:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.25	
[11/22 13:11:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/22 13:13:44 visual_prompt]: 	Training 100/553. train loss: 0.6654,	0.9030 s / batch. (data: 2.89e-04). ETA=12:10:53, max mem: 30.7 GB 
[11/22 13:15:21 visual_prompt]: 	Training 200/553. train loss: 0.8363,	0.9160 s / batch. (data: 7.99e-03). ETA=12:19:52, max mem: 30.7 GB 
[11/22 13:16:58 visual_prompt]: 	Training 300/553. train loss: 0.6305,	0.9241 s / batch. (data: 2.66e-04). ETA=12:24:52, max mem: 30.7 GB 
[11/22 13:18:36 visual_prompt]: 	Training 400/553. train loss: 0.6475,	0.9320 s / batch. (data: 8.91e-03). ETA=12:29:43, max mem: 30.7 GB 
[11/22 13:20:11 visual_prompt]: 	Training 500/553. train loss: 0.7910,	0.9351 s / batch. (data: 7.58e-04). ETA=12:30:39, max mem: 30.7 GB 
[11/22 13:21:01 visual_prompt]: Epoch 13 / 100: avg data time: 5.29e-02, avg batch time: 0.9848, average train loss: 0.7442
[11/22 13:21:59 visual_prompt]: Inference (val):avg data time: 4.36e-04, avg batch time: 0.3052, average loss: 0.7244
[11/22 13:21:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.42	
[11/22 13:21:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/22 13:23:49 visual_prompt]: 	Training 100/553. train loss: 0.6473,	0.9436 s / batch. (data: 2.49e-04). ETA=12:35:03, max mem: 30.7 GB 
[11/22 13:25:28 visual_prompt]: 	Training 200/553. train loss: 0.2841,	0.9231 s / batch. (data: 2.62e-04). ETA=12:17:04, max mem: 30.7 GB 
[11/22 13:27:04 visual_prompt]: 	Training 300/553. train loss: 0.7002,	1.2138 s / batch. (data: 3.00e-01). ETA=16:07:11, max mem: 30.7 GB 
[11/22 13:28:39 visual_prompt]: 	Training 400/553. train loss: 0.9454,	0.9290 s / batch. (data: 3.04e-04). ETA=12:18:45, max mem: 30.7 GB 
[11/22 13:30:16 visual_prompt]: 	Training 500/553. train loss: 0.6671,	0.9640 s / batch. (data: 7.95e-03). ETA=12:44:58, max mem: 30.7 GB 
[11/22 13:31:05 visual_prompt]: Epoch 14 / 100: avg data time: 5.80e-02, avg batch time: 0.9871, average train loss: 0.7437
[11/22 13:32:04 visual_prompt]: Inference (val):avg data time: 8.51e-05, avg batch time: 0.3013, average loss: 0.6930
[11/22 13:32:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.07	
[11/22 13:32:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/22 13:33:53 visual_prompt]: 	Training 100/553. train loss: 0.6939,	0.9303 s / batch. (data: 2.69e-04). ETA=12:15:52, max mem: 30.7 GB 
[11/22 13:35:30 visual_prompt]: 	Training 200/553. train loss: 0.7180,	0.9768 s / batch. (data: 5.85e-03). ETA=12:50:57, max mem: 30.7 GB 
[11/22 13:37:08 visual_prompt]: 	Training 300/553. train loss: 0.9971,	0.9524 s / batch. (data: 1.05e-02). ETA=12:30:06, max mem: 30.7 GB 
[11/22 13:38:44 visual_prompt]: 	Training 400/553. train loss: 0.4975,	0.9539 s / batch. (data: 1.79e-02). ETA=12:29:44, max mem: 30.7 GB 
[11/22 13:40:20 visual_prompt]: 	Training 500/553. train loss: 0.8928,	0.9109 s / batch. (data: 2.62e-04). ETA=11:54:24, max mem: 30.7 GB 
[11/22 13:41:09 visual_prompt]: Epoch 15 / 100: avg data time: 5.50e-02, avg batch time: 0.9867, average train loss: 0.7284
[11/22 13:42:08 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3044, average loss: 0.7401
[11/22 13:42:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.80	
[11/22 13:42:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/22 13:43:54 visual_prompt]: 	Training 100/553. train loss: 0.7895,	0.9108 s / batch. (data: 3.02e-04). ETA=11:52:00, max mem: 30.7 GB 
[11/22 13:45:28 visual_prompt]: 	Training 200/553. train loss: 0.5870,	1.1520 s / batch. (data: 2.08e-01). ETA=14:58:39, max mem: 30.7 GB 
[11/22 13:47:10 visual_prompt]: 	Training 300/553. train loss: 0.9182,	0.9545 s / batch. (data: 6.49e-03). ETA=12:23:01, max mem: 30.7 GB 
[11/22 13:48:47 visual_prompt]: 	Training 400/553. train loss: 0.6374,	0.9626 s / batch. (data: 1.04e-02). ETA=12:27:44, max mem: 30.7 GB 
[11/22 13:50:21 visual_prompt]: 	Training 500/553. train loss: 0.7078,	0.9468 s / batch. (data: 5.82e-03). ETA=12:13:53, max mem: 30.7 GB 
[11/22 13:51:12 visual_prompt]: Epoch 16 / 100: avg data time: 5.20e-02, avg batch time: 0.9832, average train loss: 0.7177
[11/22 13:52:12 visual_prompt]: Inference (val):avg data time: 1.24e-04, avg batch time: 0.3049, average loss: 0.8127
[11/22 13:52:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.18	
[11/22 13:52:12 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/22 13:53:56 visual_prompt]: 	Training 100/553. train loss: 0.7705,	0.9381 s / batch. (data: 2.46e-04). ETA=12:04:44, max mem: 30.7 GB 
[11/22 13:55:37 visual_prompt]: 	Training 200/553. train loss: 0.9846,	0.9222 s / batch. (data: 3.47e-04). ETA=11:50:54, max mem: 30.7 GB 
[11/22 13:57:10 visual_prompt]: 	Training 300/553. train loss: 1.3557,	0.9389 s / batch. (data: 2.96e-04). ETA=12:02:11, max mem: 30.7 GB 
[11/22 13:58:49 visual_prompt]: 	Training 400/553. train loss: 0.5184,	0.9484 s / batch. (data: 5.39e-03). ETA=12:07:54, max mem: 30.7 GB 
[11/22 14:00:26 visual_prompt]: 	Training 500/553. train loss: 0.7559,	0.9277 s / batch. (data: 2.44e-04). ETA=11:50:29, max mem: 30.7 GB 
[11/22 14:01:17 visual_prompt]: Epoch 17 / 100: avg data time: 5.53e-02, avg batch time: 0.9856, average train loss: 0.7242
[11/22 14:02:16 visual_prompt]: Inference (val):avg data time: 3.18e-04, avg batch time: 0.3037, average loss: 0.6796
[11/22 14:02:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 57.46	
[11/22 14:02:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/22 14:04:06 visual_prompt]: 	Training 100/553. train loss: 0.7188,	0.9093 s / batch. (data: 7.32e-03). ETA=11:34:05, max mem: 30.7 GB 
[11/22 14:05:43 visual_prompt]: 	Training 200/553. train loss: 0.7266,	0.9267 s / batch. (data: 2.56e-04). ETA=11:45:48, max mem: 30.7 GB 
[11/22 14:07:18 visual_prompt]: 	Training 300/553. train loss: 0.7207,	0.9303 s / batch. (data: 7.76e-03). ETA=11:47:00, max mem: 30.7 GB 
[11/22 14:08:53 visual_prompt]: 	Training 400/553. train loss: 0.7670,	0.9186 s / batch. (data: 2.75e-04). ETA=11:36:33, max mem: 30.7 GB 
[11/22 14:10:33 visual_prompt]: 	Training 500/553. train loss: 0.6052,	0.9373 s / batch. (data: 7.01e-04). ETA=11:49:13, max mem: 30.7 GB 
[11/22 14:11:22 visual_prompt]: Epoch 18 / 100: avg data time: 5.78e-02, avg batch time: 0.9877, average train loss: 0.7196
[11/22 14:12:21 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3018, average loss: 0.7644
[11/22 14:12:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.12	
[11/22 14:12:21 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/22 14:14:08 visual_prompt]: 	Training 100/553. train loss: 0.6528,	0.9438 s / batch. (data: 7.75e-04). ETA=11:51:42, max mem: 30.7 GB 
[11/22 14:15:46 visual_prompt]: 	Training 200/553. train loss: 0.7099,	2.8248 s / batch. (data: 1.92e+00). ETA=1 day, 11:25:28, max mem: 30.7 GB 
[11/22 14:17:22 visual_prompt]: 	Training 300/553. train loss: 0.6649,	0.9175 s / batch. (data: 3.01e-04). ETA=11:28:49, max mem: 30.7 GB 
[11/22 14:18:58 visual_prompt]: 	Training 400/553. train loss: 0.6931,	0.9558 s / batch. (data: 5.38e-03). ETA=11:55:59, max mem: 30.7 GB 
[11/22 14:20:34 visual_prompt]: 	Training 500/553. train loss: 0.3876,	0.9463 s / batch. (data: 5.38e-03). ETA=11:47:18, max mem: 30.7 GB 
[11/22 14:21:24 visual_prompt]: Epoch 19 / 100: avg data time: 5.04e-02, avg batch time: 0.9821, average train loss: 0.7145
[11/22 14:22:22 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3035, average loss: 0.8298
[11/22 14:22:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.33	
[11/22 14:22:22 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/22 14:24:12 visual_prompt]: 	Training 100/553. train loss: 0.6449,	0.9222 s / batch. (data: 2.77e-04). ETA=11:26:56, max mem: 30.7 GB 
[11/22 14:25:46 visual_prompt]: 	Training 200/553. train loss: 0.7310,	0.9256 s / batch. (data: 3.16e-04). ETA=11:27:53, max mem: 30.7 GB 
[11/22 14:27:23 visual_prompt]: 	Training 300/553. train loss: 0.6181,	0.9400 s / batch. (data: 2.82e-04). ETA=11:37:05, max mem: 30.7 GB 
[11/22 14:28:59 visual_prompt]: 	Training 400/553. train loss: 0.6592,	0.9250 s / batch. (data: 5.59e-03). ETA=11:24:23, max mem: 30.7 GB 
[11/22 14:30:34 visual_prompt]: 	Training 500/553. train loss: 0.7501,	0.9214 s / batch. (data: 5.42e-03). ETA=11:20:10, max mem: 30.7 GB 
[11/22 14:31:27 visual_prompt]: Epoch 20 / 100: avg data time: 5.40e-02, avg batch time: 0.9847, average train loss: 0.7239
[11/22 14:32:25 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3043, average loss: 0.7013
[11/22 14:32:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 54.77	
[11/22 14:32:25 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/22 14:34:15 visual_prompt]: 	Training 100/553. train loss: 0.6826,	0.9361 s / batch. (data: 1.04e-03). ETA=11:28:37, max mem: 30.7 GB 
[11/22 14:35:52 visual_prompt]: 	Training 200/553. train loss: 0.7575,	0.9226 s / batch. (data: 2.42e-04). ETA=11:17:13, max mem: 30.7 GB 
[11/22 14:37:29 visual_prompt]: 	Training 300/553. train loss: 0.7348,	0.9370 s / batch. (data: 1.85e-03). ETA=11:26:11, max mem: 30.7 GB 
[11/22 14:39:05 visual_prompt]: 	Training 400/553. train loss: 0.9551,	0.9186 s / batch. (data: 2.99e-04). ETA=11:11:10, max mem: 30.7 GB 
[11/22 14:40:39 visual_prompt]: 	Training 500/553. train loss: 0.5895,	0.9151 s / batch. (data: 2.76e-04). ETA=11:07:08, max mem: 30.7 GB 
[11/22 14:41:29 visual_prompt]: Epoch 21 / 100: avg data time: 5.17e-02, avg batch time: 0.9825, average train loss: 0.7049
[11/22 14:42:27 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3044, average loss: 0.6910
[11/22 14:42:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 53.35	
[11/22 14:42:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/22 14:44:13 visual_prompt]: 	Training 100/553. train loss: 0.7590,	0.9520 s / batch. (data: 7.98e-03). ETA=11:31:36, max mem: 30.7 GB 
[11/22 14:45:52 visual_prompt]: 	Training 200/553. train loss: 0.9307,	0.9488 s / batch. (data: 5.85e-03). ETA=11:27:41, max mem: 30.7 GB 
[11/22 14:47:30 visual_prompt]: 	Training 300/553. train loss: 0.5014,	0.9169 s / batch. (data: 2.93e-04). ETA=11:03:02, max mem: 30.7 GB 
[11/22 14:49:04 visual_prompt]: 	Training 400/553. train loss: 0.8776,	0.9483 s / batch. (data: 2.77e-04). ETA=11:24:08, max mem: 30.7 GB 
[11/22 14:50:39 visual_prompt]: 	Training 500/553. train loss: 0.8144,	0.9279 s / batch. (data: 4.27e-04). ETA=11:07:53, max mem: 30.7 GB 
[11/22 14:51:31 visual_prompt]: Epoch 22 / 100: avg data time: 5.09e-02, avg batch time: 0.9826, average train loss: 0.7317
[11/22 14:52:29 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3050, average loss: 0.7270
[11/22 14:52:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.72	
[11/22 14:52:29 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/22 14:54:18 visual_prompt]: 	Training 100/553. train loss: 0.6756,	0.9480 s / batch. (data: 7.84e-04). ETA=11:19:56, max mem: 30.7 GB 
[11/22 14:55:54 visual_prompt]: 	Training 200/553. train loss: 0.7566,	0.9579 s / batch. (data: 2.67e-04). ETA=11:25:26, max mem: 30.7 GB 
[11/22 14:57:28 visual_prompt]: 	Training 300/553. train loss: 0.3510,	0.9679 s / batch. (data: 8.03e-03). ETA=11:30:58, max mem: 30.7 GB 
[11/22 14:59:04 visual_prompt]: 	Training 400/553. train loss: 0.8258,	0.9521 s / batch. (data: 2.71e-04). ETA=11:18:07, max mem: 30.7 GB 
[11/22 15:00:39 visual_prompt]: 	Training 500/553. train loss: 0.3822,	0.9066 s / batch. (data: 2.57e-04). ETA=10:44:11, max mem: 30.7 GB 
[11/22 15:01:31 visual_prompt]: Epoch 23 / 100: avg data time: 4.93e-02, avg batch time: 0.9802, average train loss: 0.7102
[11/22 15:02:29 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3053, average loss: 0.8528
[11/22 15:02:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.85	
[11/22 15:02:29 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/22 15:04:15 visual_prompt]: 	Training 100/553. train loss: 0.7599,	0.9481 s / batch. (data: 2.57e-04). ETA=11:11:14, max mem: 30.7 GB 
[11/22 15:05:52 visual_prompt]: 	Training 200/553. train loss: 0.6438,	0.9360 s / batch. (data: 3.97e-03). ETA=11:01:07, max mem: 30.7 GB 
[11/22 15:07:31 visual_prompt]: 	Training 300/553. train loss: 0.6012,	0.9225 s / batch. (data: 2.76e-04). ETA=10:50:05, max mem: 30.7 GB 
[11/22 15:09:06 visual_prompt]: 	Training 400/553. train loss: 0.6673,	0.9279 s / batch. (data: 3.96e-03). ETA=10:52:21, max mem: 30.7 GB 
[11/22 15:10:45 visual_prompt]: 	Training 500/553. train loss: 0.7354,	0.9358 s / batch. (data: 5.92e-03). ETA=10:56:20, max mem: 30.7 GB 
[11/22 15:11:34 visual_prompt]: Epoch 24 / 100: avg data time: 5.42e-02, avg batch time: 0.9844, average train loss: 0.7096
[11/22 15:12:32 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3050, average loss: 0.7463
[11/22 15:12:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.46	
[11/22 15:12:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/22 15:14:16 visual_prompt]: 	Training 100/553. train loss: 0.7269,	0.9280 s / batch. (data: 5.41e-03). ETA=10:48:28, max mem: 30.7 GB 
[11/22 15:15:56 visual_prompt]: 	Training 200/553. train loss: 0.4366,	0.9273 s / batch. (data: 5.38e-03). ETA=10:46:26, max mem: 30.7 GB 
[11/22 15:17:32 visual_prompt]: 	Training 300/553. train loss: 0.6073,	0.9369 s / batch. (data: 2.96e-04). ETA=10:51:33, max mem: 30.7 GB 
[11/22 15:19:09 visual_prompt]: 	Training 400/553. train loss: 0.7271,	0.9348 s / batch. (data: 7.97e-03). ETA=10:48:33, max mem: 30.7 GB 
[11/22 15:20:45 visual_prompt]: 	Training 500/553. train loss: 0.6491,	0.9345 s / batch. (data: 2.56e-04). ETA=10:46:45, max mem: 30.7 GB 
[11/22 15:21:37 visual_prompt]: Epoch 25 / 100: avg data time: 5.53e-02, avg batch time: 0.9859, average train loss: 0.7060
[11/22 15:22:36 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3029, average loss: 0.7096
[11/22 15:22:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 56.34	
[11/22 15:22:36 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/22 15:24:22 visual_prompt]: 	Training 100/553. train loss: 0.5507,	0.9384 s / batch. (data: 1.60e-02). ETA=10:47:07, max mem: 30.7 GB 
[11/22 15:26:00 visual_prompt]: 	Training 200/553. train loss: 0.4685,	0.9405 s / batch. (data: 3.07e-04). ETA=10:47:00, max mem: 30.7 GB 
[11/22 15:27:38 visual_prompt]: 	Training 300/553. train loss: 0.6631,	0.9232 s / batch. (data: 5.86e-03). ETA=10:33:31, max mem: 30.7 GB 
[11/22 15:29:12 visual_prompt]: 	Training 400/553. train loss: 0.6480,	0.9334 s / batch. (data: 3.07e-04). ETA=10:39:01, max mem: 30.7 GB 
[11/22 15:30:48 visual_prompt]: 	Training 500/553. train loss: 0.5298,	0.9409 s / batch. (data: 7.23e-04). ETA=10:42:35, max mem: 30.7 GB 
[11/22 15:31:38 visual_prompt]: Epoch 26 / 100: avg data time: 5.04e-02, avg batch time: 0.9814, average train loss: 0.7069
[11/22 15:32:37 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3064, average loss: 0.8508
[11/22 15:32:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.36	
[11/22 15:32:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/22 15:34:23 visual_prompt]: 	Training 100/553. train loss: 0.6230,	0.9240 s / batch. (data: 2.83e-04). ETA=10:28:38, max mem: 30.7 GB 
[11/22 15:36:00 visual_prompt]: 	Training 200/553. train loss: 0.6139,	0.9260 s / batch. (data: 5.38e-03). ETA=10:28:26, max mem: 30.7 GB 
[11/22 15:37:34 visual_prompt]: 	Training 300/553. train loss: 0.7190,	0.9274 s / batch. (data: 6.90e-04). ETA=10:27:54, max mem: 30.7 GB 
[11/22 15:39:10 visual_prompt]: 	Training 400/553. train loss: 0.9160,	0.9563 s / batch. (data: 5.81e-03). ETA=10:45:50, max mem: 30.7 GB 
[11/22 15:40:49 visual_prompt]: 	Training 500/553. train loss: 0.8499,	0.9600 s / batch. (data: 2.74e-04). ETA=10:46:47, max mem: 30.7 GB 
[11/22 15:41:39 visual_prompt]: Epoch 27 / 100: avg data time: 4.99e-02, avg batch time: 0.9803, average train loss: 0.7023
[11/22 15:42:37 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3054, average loss: 0.7010
[11/22 15:42:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 56.45	
[11/22 15:42:37 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.0004367053692460385
[11/22 15:44:22 visual_prompt]: 	Training 100/553. train loss: 0.7943,	0.9585 s / batch. (data: 1.45e-02). ETA=10:43:18, max mem: 30.7 GB 
[11/22 15:46:00 visual_prompt]: 	Training 200/553. train loss: 0.5091,	0.9378 s / batch. (data: 2.87e-04). ETA=10:27:49, max mem: 30.7 GB 
[11/22 15:47:36 visual_prompt]: 	Training 300/553. train loss: 1.2747,	0.9485 s / batch. (data: 1.04e-02). ETA=10:33:26, max mem: 30.7 GB 
[11/22 15:49:14 visual_prompt]: 	Training 400/553. train loss: 0.6864,	1.3239 s / batch. (data: 3.78e-01). ETA=14:41:56, max mem: 30.7 GB 
[11/22 15:50:50 visual_prompt]: 	Training 500/553. train loss: 1.0298,	0.9120 s / batch. (data: 2.66e-04). ETA=10:06:00, max mem: 30.7 GB 
[11/22 15:51:39 visual_prompt]: Epoch 28 / 100: avg data time: 4.88e-02, avg batch time: 0.9796, average train loss: 0.7019
[11/22 15:52:38 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3028, average loss: 0.7152
[11/22 15:52:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.72	
[11/22 15:52:38 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.0004311063011977723
[11/22 15:54:25 visual_prompt]: 	Training 100/553. train loss: 0.7407,	0.9295 s / batch. (data: 3.35e-04). ETA=10:15:16, max mem: 30.7 GB 
[11/22 15:56:03 visual_prompt]: 	Training 200/553. train loss: 0.6046,	0.9149 s / batch. (data: 5.37e-03). ETA=10:04:03, max mem: 30.7 GB 
[11/22 15:57:43 visual_prompt]: 	Training 300/553. train loss: 0.8130,	0.9157 s / batch. (data: 3.75e-04). ETA=10:03:05, max mem: 30.7 GB 
[11/22 15:59:21 visual_prompt]: 	Training 400/553. train loss: 0.6579,	0.9244 s / batch. (data: 7.92e-04). ETA=10:07:17, max mem: 30.7 GB 
[11/22 16:00:54 visual_prompt]: 	Training 500/553. train loss: 0.7277,	0.9725 s / batch. (data: 7.22e-04). ETA=10:37:15, max mem: 30.7 GB 
[11/22 16:01:44 visual_prompt]: Epoch 29 / 100: avg data time: 5.90e-02, avg batch time: 0.9876, average train loss: 0.7047
[11/22 16:02:42 visual_prompt]: Inference (val):avg data time: 8.45e-05, avg batch time: 0.3066, average loss: 0.7228
[11/22 16:02:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.16	
[11/22 16:02:42 visual_prompt]: Stopping early.
[11/22 16:02:42 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 16:02:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 16:02:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/22 16:02:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/22 16:02:42 visual_prompt]: Training with config:
[11/22 16:02:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/22 16:02:42 visual_prompt]: Loading training data...
[11/22 16:02:42 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 16:02:42 visual_prompt]: Loading validation data...
[11/22 16:02:42 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 16:02:42 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 16:02:44 visual_prompt]: Enable all parameters update during training
[11/22 16:02:44 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/22 16:02:44 visual_prompt]: tuned percent:100.000
[11/22 16:02:44 visual_prompt]: Device used for model: 0
[11/22 16:02:44 visual_prompt]: Setting up Evaluator...
[11/22 16:02:44 visual_prompt]: Setting up Trainer...
[11/22 16:02:44 visual_prompt]: 	Setting up the optimizer...
[11/22 16:02:44 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 16:04:30 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9479 s / batch. (data: 7.86e-03). ETA=14:32:02, max mem: 30.7 GB 
[11/22 16:06:10 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9174 s / batch. (data: 2.83e-04). ETA=14:02:26, max mem: 30.7 GB 
[11/22 16:07:48 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9385 s / batch. (data: 5.41e-03). ETA=14:20:18, max mem: 30.7 GB 
[11/22 16:09:22 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9527 s / batch. (data: 1.67e-02). ETA=14:31:41, max mem: 30.7 GB 
[11/22 16:11:00 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9282 s / batch. (data: 5.38e-03). ETA=14:07:43, max mem: 30.7 GB 
[11/22 16:11:49 visual_prompt]: Epoch 1 / 100: avg data time: 5.39e-02, avg batch time: 0.9849, average train loss: 7.6130
[11/22 16:12:47 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3036, average loss: 6.9126
[11/22 16:12:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/22 16:12:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/22 16:14:35 visual_prompt]: 	Training 100/553. train loss: 0.8600,	0.9396 s / batch. (data: 5.88e-03). ETA=14:15:45, max mem: 30.7 GB 
[11/22 16:16:10 visual_prompt]: 	Training 200/553. train loss: 1.5947,	2.0539 s / batch. (data: 1.13e+00). ETA=1 day, 7:07:12, max mem: 30.7 GB 
[11/22 16:17:47 visual_prompt]: 	Training 300/553. train loss: 0.8411,	0.9369 s / batch. (data: 5.37e-03). ETA=14:10:09, max mem: 30.7 GB 
[11/22 16:19:25 visual_prompt]: 	Training 400/553. train loss: 0.6282,	0.9320 s / batch. (data: 7.44e-04). ETA=14:04:11, max mem: 30.7 GB 
[11/22 16:21:02 visual_prompt]: 	Training 500/553. train loss: 0.9062,	0.9325 s / batch. (data: 5.50e-03). ETA=14:03:06, max mem: 30.7 GB 
[11/22 16:21:52 visual_prompt]: Epoch 2 / 100: avg data time: 5.19e-02, avg batch time: 0.9838, average train loss: 0.9658
[11/22 16:22:50 visual_prompt]: Inference (val):avg data time: 2.89e-04, avg batch time: 0.3037, average loss: 1.1192
[11/22 16:22:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.64	
[11/22 16:22:50 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/22 16:24:37 visual_prompt]: 	Training 100/553. train loss: 0.6871,	0.9156 s / batch. (data: 5.38e-03). ETA=13:45:27, max mem: 30.7 GB 
[11/22 16:26:14 visual_prompt]: 	Training 200/553. train loss: 2.5390,	0.9360 s / batch. (data: 3.08e-04). ETA=14:02:18, max mem: 30.7 GB 
[11/22 16:27:52 visual_prompt]: 	Training 300/553. train loss: 0.9445,	0.9206 s / batch. (data: 3.58e-03). ETA=13:46:53, max mem: 30.7 GB 
[11/22 16:29:26 visual_prompt]: 	Training 400/553. train loss: 0.7065,	0.9275 s / batch. (data: 3.96e-03). ETA=13:51:33, max mem: 30.7 GB 
[11/22 16:31:01 visual_prompt]: 	Training 500/553. train loss: 1.0384,	0.9560 s / batch. (data: 7.46e-04). ETA=14:15:31, max mem: 30.7 GB 
[11/22 16:31:52 visual_prompt]: Epoch 3 / 100: avg data time: 4.76e-02, avg batch time: 0.9799, average train loss: 0.8298
[11/22 16:32:51 visual_prompt]: Inference (val):avg data time: 1.99e-04, avg batch time: 0.3061, average loss: 0.6957
[11/22 16:32:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 61.62	
[11/22 16:32:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/22 16:34:37 visual_prompt]: 	Training 100/553. train loss: 0.6687,	0.9160 s / batch. (data: 2.88e-04). ETA=13:37:23, max mem: 30.7 GB 
[11/22 16:36:14 visual_prompt]: 	Training 200/553. train loss: 0.9843,	1.2880 s / batch. (data: 3.65e-01). ETA=19:07:11, max mem: 30.7 GB 
[11/22 16:37:50 visual_prompt]: 	Training 300/553. train loss: 0.9576,	0.9281 s / batch. (data: 1.05e-02). ETA=13:45:03, max mem: 30.7 GB 
[11/22 16:39:27 visual_prompt]: 	Training 400/553. train loss: 0.4990,	0.9584 s / batch. (data: 5.40e-03). ETA=14:10:28, max mem: 30.7 GB 
[11/22 16:41:04 visual_prompt]: 	Training 500/553. train loss: 0.9503,	0.9507 s / batch. (data: 5.38e-03). ETA=14:02:02, max mem: 30.7 GB 
[11/22 16:41:56 visual_prompt]: Epoch 4 / 100: avg data time: 5.48e-02, avg batch time: 0.9853, average train loss: 0.8096
[11/22 16:42:54 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3036, average loss: 0.7018
[11/22 16:42:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 62.17	
[11/22 16:42:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/22 16:44:40 visual_prompt]: 	Training 100/553. train loss: 0.6776,	0.9232 s / batch. (data: 2.70e-04). ETA=13:35:18, max mem: 30.7 GB 
[11/22 16:46:18 visual_prompt]: 	Training 200/553. train loss: 0.6369,	0.9360 s / batch. (data: 7.95e-03). ETA=13:45:04, max mem: 30.7 GB 
[11/22 16:47:53 visual_prompt]: 	Training 300/553. train loss: 0.7604,	0.9567 s / batch. (data: 1.67e-02). ETA=14:01:40, max mem: 30.7 GB 
[11/22 16:49:31 visual_prompt]: 	Training 400/553. train loss: 1.2175,	3.7577 s / batch. (data: 2.81e+00). ETA=2 days, 6:59:44, max mem: 30.7 GB 
[11/22 16:51:09 visual_prompt]: 	Training 500/553. train loss: 0.7473,	0.9338 s / batch. (data: 5.36e-03). ETA=13:38:26, max mem: 30.7 GB 
[11/22 16:51:58 visual_prompt]: Epoch 5 / 100: avg data time: 5.29e-02, avg batch time: 0.9832, average train loss: 0.7728
[11/22 16:52:56 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3051, average loss: 0.8557
[11/22 16:52:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.45	
[11/22 16:52:56 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/22 16:54:44 visual_prompt]: 	Training 100/553. train loss: 0.7675,	0.9810 s / batch. (data: 1.63e-02). ETA=14:17:19, max mem: 30.7 GB 
[11/22 16:56:20 visual_prompt]: 	Training 200/553. train loss: 0.7312,	0.9443 s / batch. (data: 5.35e-03). ETA=13:43:39, max mem: 30.7 GB 
[11/22 16:57:56 visual_prompt]: 	Training 300/553. train loss: 1.1643,	0.9245 s / batch. (data: 1.05e-02). ETA=13:24:53, max mem: 30.7 GB 
[11/22 16:59:33 visual_prompt]: 	Training 400/553. train loss: 0.6809,	2.9960 s / batch. (data: 2.07e+00). ETA=1 day, 19:23:16, max mem: 30.7 GB 
[11/22 17:01:25 visual_prompt]: 	Training 500/553. train loss: 0.9766,	0.9090 s / batch. (data: 6.84e-04). ETA=13:08:17, max mem: 30.7 GB 
[11/22 17:02:17 visual_prompt]: Epoch 6 / 100: avg data time: 8.42e-02, avg batch time: 1.0125, average train loss: 0.7450
[11/22 17:03:17 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3028, average loss: 0.6821
[11/22 17:03:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.37	
[11/22 17:03:17 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/22 17:05:10 visual_prompt]: 	Training 100/553. train loss: 0.6055,	0.9258 s / batch. (data: 4.00e-03). ETA=13:20:32, max mem: 30.7 GB 
[11/22 17:06:48 visual_prompt]: 	Training 200/553. train loss: 0.5154,	0.9609 s / batch. (data: 2.17e-02). ETA=13:49:17, max mem: 30.7 GB 
[11/22 17:08:24 visual_prompt]: 	Training 300/553. train loss: 0.6617,	0.9456 s / batch. (data: 5.67e-03). ETA=13:34:32, max mem: 30.7 GB 
[11/22 17:09:59 visual_prompt]: 	Training 400/553. train loss: 0.7811,	0.9314 s / batch. (data: 5.41e-03). ETA=13:20:41, max mem: 30.7 GB 
[11/22 17:11:34 visual_prompt]: 	Training 500/553. train loss: 0.5695,	0.9780 s / batch. (data: 2.06e-02). ETA=13:59:11, max mem: 30.7 GB 
[11/22 17:12:24 visual_prompt]: Epoch 7 / 100: avg data time: 6.03e-02, avg batch time: 0.9887, average train loss: 0.7459
[11/22 17:13:23 visual_prompt]: Inference (val):avg data time: 1.34e-04, avg batch time: 0.3051, average loss: 0.6720
[11/22 17:13:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.31	
[11/22 17:13:23 visual_prompt]: Best epoch 7: best metric: -0.672
[11/22 17:13:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/22 17:15:09 visual_prompt]: 	Training 100/553. train loss: 0.6615,	0.9321 s / batch. (data: 2.56e-04). ETA=13:17:23, max mem: 30.7 GB 
[11/22 17:16:45 visual_prompt]: 	Training 200/553. train loss: 0.5396,	0.9439 s / batch. (data: 1.60e-02). ETA=13:25:53, max mem: 30.7 GB 
[11/22 17:18:19 visual_prompt]: 	Training 300/553. train loss: 0.7562,	0.9580 s / batch. (data: 5.31e-03). ETA=13:36:19, max mem: 30.7 GB 
[11/22 17:19:53 visual_prompt]: 	Training 400/553. train loss: 0.5987,	0.9280 s / batch. (data: 2.77e-04). ETA=13:09:13, max mem: 30.7 GB 
[11/22 17:21:27 visual_prompt]: 	Training 500/553. train loss: 0.6752,	0.9268 s / batch. (data: 5.37e-03). ETA=13:06:41, max mem: 30.7 GB 
[11/22 17:22:17 visual_prompt]: Epoch 8 / 100: avg data time: 3.34e-02, avg batch time: 0.9641, average train loss: 0.7415
[11/22 17:23:14 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3054, average loss: 0.6599
[11/22 17:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 63.97	
[11/22 17:23:14 visual_prompt]: Best epoch 8: best metric: -0.660
[11/22 17:23:14 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/22 17:24:59 visual_prompt]: 	Training 100/553. train loss: 0.7802,	0.9355 s / batch. (data: 2.43e-04). ETA=13:11:39, max mem: 30.7 GB 
[11/22 17:26:32 visual_prompt]: 	Training 200/553. train loss: 1.1765,	0.9144 s / batch. (data: 1.73e-03). ETA=12:52:19, max mem: 30.7 GB 
[11/22 17:28:06 visual_prompt]: 	Training 300/553. train loss: 0.4718,	0.9219 s / batch. (data: 2.38e-04). ETA=12:57:04, max mem: 30.7 GB 
[11/22 17:29:39 visual_prompt]: 	Training 400/553. train loss: 0.6642,	0.9550 s / batch. (data: 1.59e-02). ETA=13:23:26, max mem: 30.7 GB 
[11/22 17:31:13 visual_prompt]: 	Training 500/553. train loss: 0.7192,	0.9312 s / batch. (data: 2.84e-04). ETA=13:01:51, max mem: 30.7 GB 
[11/22 17:32:02 visual_prompt]: Epoch 9 / 100: avg data time: 2.64e-02, avg batch time: 0.9554, average train loss: 0.7512
[11/22 17:32:58 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3053, average loss: 0.7262
[11/22 17:32:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 65.25	
[11/22 17:32:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/22 17:34:41 visual_prompt]: 	Training 100/553. train loss: 0.7253,	0.9290 s / batch. (data: 1.09e-02). ETA=12:57:35, max mem: 30.7 GB 
[11/22 17:36:15 visual_prompt]: 	Training 200/553. train loss: 0.8745,	0.9474 s / batch. (data: 2.54e-04). ETA=13:11:25, max mem: 30.7 GB 
[11/22 17:37:48 visual_prompt]: 	Training 300/553. train loss: 0.7233,	0.9245 s / batch. (data: 5.36e-03). ETA=12:50:46, max mem: 30.7 GB 
[11/22 17:39:22 visual_prompt]: 	Training 400/553. train loss: 0.5812,	0.9560 s / batch. (data: 2.82e-04). ETA=13:15:26, max mem: 30.7 GB 
[11/22 17:40:56 visual_prompt]: 	Training 500/553. train loss: 0.6619,	0.9445 s / batch. (data: 1.10e-02). ETA=13:04:19, max mem: 30.7 GB 
[11/22 17:41:45 visual_prompt]: Epoch 10 / 100: avg data time: 2.29e-02, avg batch time: 0.9532, average train loss: 0.7295
[11/22 17:42:41 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3051, average loss: 0.7400
[11/22 17:42:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 61.34	
[11/22 17:42:41 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/22 17:44:24 visual_prompt]: 	Training 100/553. train loss: 1.0796,	0.9200 s / batch. (data: 8.06e-03). ETA=12:41:36, max mem: 30.7 GB 
[11/22 17:45:59 visual_prompt]: 	Training 200/553. train loss: 0.8216,	0.9338 s / batch. (data: 2.52e-04). ETA=12:51:27, max mem: 30.7 GB 
[11/22 17:47:32 visual_prompt]: 	Training 300/553. train loss: 0.4664,	0.9499 s / batch. (data: 5.39e-03). ETA=13:03:11, max mem: 30.7 GB 
[11/22 17:49:06 visual_prompt]: 	Training 400/553. train loss: 0.7764,	0.9174 s / batch. (data: 2.53e-04). ETA=12:34:51, max mem: 30.7 GB 
[11/22 17:50:39 visual_prompt]: 	Training 500/553. train loss: 0.5289,	0.9338 s / batch. (data: 7.53e-04). ETA=12:46:47, max mem: 30.7 GB 
[11/22 17:51:28 visual_prompt]: Epoch 11 / 100: avg data time: 2.52e-02, avg batch time: 0.9539, average train loss: 0.7191
[11/22 17:52:24 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3061, average loss: 0.6795
[11/22 17:52:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.33	
[11/22 17:52:24 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/22 17:54:04 visual_prompt]: 	Training 100/553. train loss: 0.5202,	0.9734 s / batch. (data: 5.84e-03). ETA=13:16:48, max mem: 30.7 GB 
[11/22 17:55:39 visual_prompt]: 	Training 200/553. train loss: 0.7365,	0.9215 s / batch. (data: 8.00e-03). ETA=12:32:46, max mem: 30.7 GB 
[11/22 17:57:13 visual_prompt]: 	Training 300/553. train loss: 0.6492,	0.9326 s / batch. (data: 5.50e-03). ETA=12:40:20, max mem: 30.7 GB 
[11/22 17:58:47 visual_prompt]: 	Training 400/553. train loss: 0.7977,	0.9489 s / batch. (data: 5.42e-03). ETA=12:52:03, max mem: 30.7 GB 
[11/22 18:00:21 visual_prompt]: 	Training 500/553. train loss: 0.6561,	0.9374 s / batch. (data: 7.54e-04). ETA=12:41:05, max mem: 30.7 GB 
[11/22 18:01:10 visual_prompt]: Epoch 12 / 100: avg data time: 2.07e-02, avg batch time: 0.9514, average train loss: 0.7565
[11/22 18:02:06 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3048, average loss: 1.0210
[11/22 18:02:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.43	
[11/22 18:02:06 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/22 18:03:50 visual_prompt]: 	Training 100/553. train loss: 0.5190,	0.9179 s / batch. (data: 2.62e-04). ETA=12:22:55, max mem: 30.7 GB 
[11/22 18:05:24 visual_prompt]: 	Training 200/553. train loss: 0.9553,	0.9189 s / batch. (data: 2.49e-04). ETA=12:22:12, max mem: 30.7 GB 
[11/22 18:06:58 visual_prompt]: 	Training 300/553. train loss: 0.7893,	0.9574 s / batch. (data: 5.50e-03). ETA=12:51:45, max mem: 30.7 GB 
[11/22 18:08:32 visual_prompt]: 	Training 400/553. train loss: 0.6668,	0.9499 s / batch. (data: 2.63e-04). ETA=12:44:07, max mem: 30.7 GB 
[11/22 18:10:05 visual_prompt]: 	Training 500/553. train loss: 0.7027,	0.9615 s / batch. (data: 5.82e-03). ETA=12:51:50, max mem: 30.7 GB 
[11/22 18:10:55 visual_prompt]: Epoch 13 / 100: avg data time: 2.53e-02, avg batch time: 0.9555, average train loss: 0.7216
[11/22 18:11:50 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3042, average loss: 0.7742
[11/22 18:11:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 61.33	
[11/22 18:11:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/22 18:13:35 visual_prompt]: 	Training 100/553. train loss: 0.5763,	0.9178 s / batch. (data: 2.31e-04). ETA=12:14:25, max mem: 30.7 GB 
[11/22 18:15:10 visual_prompt]: 	Training 200/553. train loss: 0.2941,	0.9400 s / batch. (data: 7.14e-04). ETA=12:30:37, max mem: 30.7 GB 
[11/22 18:16:43 visual_prompt]: 	Training 300/553. train loss: 0.8454,	0.9375 s / batch. (data: 7.99e-03). ETA=12:27:03, max mem: 30.7 GB 
[11/22 18:18:17 visual_prompt]: 	Training 400/553. train loss: 1.0912,	0.9229 s / batch. (data: 6.33e-03). ETA=12:13:50, max mem: 30.7 GB 
[11/22 18:19:51 visual_prompt]: 	Training 500/553. train loss: 0.6638,	0.9246 s / batch. (data: 6.95e-04). ETA=12:13:42, max mem: 30.7 GB 
[11/22 18:20:40 visual_prompt]: Epoch 14 / 100: avg data time: 2.86e-02, avg batch time: 0.9577, average train loss: 0.7260
[11/22 18:21:36 visual_prompt]: Inference (val):avg data time: 2.01e-04, avg batch time: 0.3047, average loss: 0.6893
[11/22 18:21:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.96	
[11/22 18:21:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/22 18:23:21 visual_prompt]: 	Training 100/553. train loss: 0.6593,	0.9320 s / batch. (data: 3.98e-03). ETA=12:17:11, max mem: 30.7 GB 
[11/22 18:24:55 visual_prompt]: 	Training 200/553. train loss: 0.7451,	0.9560 s / batch. (data: 7.37e-04). ETA=12:34:34, max mem: 30.7 GB 
[11/22 18:26:29 visual_prompt]: 	Training 300/553. train loss: 0.8400,	0.9280 s / batch. (data: 2.71e-04). ETA=12:10:54, max mem: 30.7 GB 
[11/22 18:28:02 visual_prompt]: 	Training 400/553. train loss: 0.5398,	0.9569 s / batch. (data: 5.77e-03). ETA=12:32:04, max mem: 30.7 GB 
[11/22 18:29:36 visual_prompt]: 	Training 500/553. train loss: 0.8927,	0.9273 s / batch. (data: 2.37e-04). ETA=12:07:16, max mem: 30.7 GB 
[11/22 18:30:25 visual_prompt]: Epoch 15 / 100: avg data time: 2.72e-02, avg batch time: 0.9563, average train loss: 0.7302
[11/22 18:31:20 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3053, average loss: 0.7291
[11/22 18:31:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.22	
[11/22 18:31:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/22 18:33:03 visual_prompt]: 	Training 100/553. train loss: 0.7616,	0.9153 s / batch. (data: 2.74e-04). ETA=11:55:30, max mem: 30.7 GB 
[11/22 18:34:36 visual_prompt]: 	Training 200/553. train loss: 0.5435,	0.9280 s / batch. (data: 2.69e-04). ETA=12:03:54, max mem: 30.7 GB 
[11/22 18:36:10 visual_prompt]: 	Training 300/553. train loss: 1.0376,	0.9393 s / batch. (data: 2.63e-04). ETA=12:11:09, max mem: 30.7 GB 
[11/22 18:37:44 visual_prompt]: 	Training 400/553. train loss: 0.5467,	0.9455 s / batch. (data: 6.73e-04). ETA=12:14:25, max mem: 30.7 GB 
[11/22 18:39:17 visual_prompt]: 	Training 500/553. train loss: 0.7376,	0.9445 s / batch. (data: 1.09e-02). ETA=12:12:05, max mem: 30.7 GB 
[11/22 18:40:07 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e-02, avg batch time: 0.9514, average train loss: 0.7090
[11/22 18:41:02 visual_prompt]: Inference (val):avg data time: 3.97e-04, avg batch time: 0.3041, average loss: 0.7950
[11/22 18:41:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.11	
[11/22 18:41:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/22 18:42:43 visual_prompt]: 	Training 100/553. train loss: 0.6762,	0.9367 s / batch. (data: 2.50e-04). ETA=12:03:36, max mem: 30.7 GB 
[11/22 18:44:18 visual_prompt]: 	Training 200/553. train loss: 1.0688,	0.9445 s / batch. (data: 5.86e-03). ETA=12:08:05, max mem: 30.7 GB 
[11/22 18:45:51 visual_prompt]: 	Training 300/553. train loss: 1.3087,	0.9402 s / batch. (data: 1.06e-02). ETA=12:03:12, max mem: 30.7 GB 
[11/22 18:47:25 visual_prompt]: 	Training 400/553. train loss: 0.6070,	0.9378 s / batch. (data: 7.99e-03). ETA=11:59:45, max mem: 30.7 GB 
[11/22 18:48:59 visual_prompt]: 	Training 500/553. train loss: 0.6948,	0.9560 s / batch. (data: 7.27e-04). ETA=12:12:10, max mem: 30.7 GB 
[11/22 18:49:48 visual_prompt]: Epoch 17 / 100: avg data time: 2.16e-02, avg batch time: 0.9507, average train loss: 0.7196
[11/22 18:50:43 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.3051, average loss: 0.6823
[11/22 18:50:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.29	
[11/22 18:50:43 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/22 18:52:29 visual_prompt]: 	Training 100/553. train loss: 0.7103,	0.9363 s / batch. (data: 8.01e-04). ETA=11:54:39, max mem: 30.7 GB 
[11/22 18:54:02 visual_prompt]: 	Training 200/553. train loss: 0.7176,	0.9280 s / batch. (data: 7.97e-03). ETA=11:46:46, max mem: 30.7 GB 
[11/22 18:55:36 visual_prompt]: 	Training 300/553. train loss: 0.7087,	0.9127 s / batch. (data: 5.33e-03). ETA=11:33:36, max mem: 30.7 GB 
[11/22 18:57:09 visual_prompt]: 	Training 400/553. train loss: 1.2964,	0.9286 s / batch. (data: 7.41e-04). ETA=11:44:08, max mem: 30.7 GB 
[11/22 18:58:43 visual_prompt]: 	Training 500/553. train loss: 0.5821,	0.9275 s / batch. (data: 7.29e-03). ETA=11:41:48, max mem: 30.7 GB 
[11/22 18:59:32 visual_prompt]: Epoch 18 / 100: avg data time: 2.70e-02, avg batch time: 0.9553, average train loss: 0.8041
[11/22 19:00:27 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.3055, average loss: 0.7254
[11/22 19:00:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.19	
[11/22 19:00:27 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/22 19:02:10 visual_prompt]: 	Training 100/553. train loss: 0.6915,	0.9276 s / batch. (data: 6.88e-04). ETA=11:39:30, max mem: 30.7 GB 
[11/22 19:03:44 visual_prompt]: 	Training 200/553. train loss: 0.7263,	0.9374 s / batch. (data: 5.38e-03). ETA=11:45:18, max mem: 30.7 GB 
[11/22 19:05:17 visual_prompt]: 	Training 300/553. train loss: 0.6961,	0.9463 s / batch. (data: 7.21e-04). ETA=11:50:26, max mem: 30.7 GB 
[11/22 19:06:51 visual_prompt]: 	Training 400/553. train loss: 0.6466,	0.9295 s / batch. (data: 7.26e-04). ETA=11:36:15, max mem: 30.7 GB 
[11/22 19:08:24 visual_prompt]: 	Training 500/553. train loss: 0.3394,	0.9279 s / batch. (data: 9.64e-03). ETA=11:33:31, max mem: 30.7 GB 
[11/22 19:09:14 visual_prompt]: Epoch 19 / 100: avg data time: 2.28e-02, avg batch time: 0.9512, average train loss: 0.7344
[11/22 19:10:08 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3033, average loss: 0.8158
[11/22 19:10:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.61	
[11/22 19:10:08 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/22 19:11:54 visual_prompt]: 	Training 100/553. train loss: 0.6622,	0.9240 s / batch. (data: 9.72e-03). ETA=11:28:16, max mem: 30.7 GB 
[11/22 19:13:27 visual_prompt]: 	Training 200/553. train loss: 0.7217,	0.9332 s / batch. (data: 5.36e-03). ETA=11:33:36, max mem: 30.7 GB 
[11/22 19:15:00 visual_prompt]: 	Training 300/553. train loss: 0.6760,	0.9180 s / batch. (data: 5.39e-03). ETA=11:20:45, max mem: 30.7 GB 
[11/22 19:16:34 visual_prompt]: 	Training 400/553. train loss: 0.6693,	0.9231 s / batch. (data: 7.32e-04). ETA=11:22:57, max mem: 30.7 GB 
[11/22 19:18:07 visual_prompt]: 	Training 500/553. train loss: 0.7785,	0.9360 s / batch. (data: 2.92e-04). ETA=11:30:57, max mem: 30.7 GB 
[11/22 19:18:57 visual_prompt]: Epoch 20 / 100: avg data time: 2.64e-02, avg batch time: 0.9547, average train loss: 0.7316
[11/22 19:19:52 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3040, average loss: 0.6898
[11/22 19:19:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.62	
[11/22 19:19:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/22 19:21:36 visual_prompt]: 	Training 100/553. train loss: 0.6402,	0.9109 s / batch. (data: 2.47e-04). ETA=11:10:05, max mem: 30.7 GB 
[11/22 19:23:10 visual_prompt]: 	Training 200/553. train loss: 0.7763,	0.9190 s / batch. (data: 2.83e-04). ETA=11:14:30, max mem: 30.7 GB 
[11/22 19:24:43 visual_prompt]: 	Training 300/553. train loss: 0.7236,	0.9226 s / batch. (data: 2.67e-04). ETA=11:15:37, max mem: 30.7 GB 
[11/22 19:26:17 visual_prompt]: 	Training 400/553. train loss: 0.9421,	0.9066 s / batch. (data: 2.60e-04). ETA=11:02:25, max mem: 30.7 GB 
[11/22 19:27:50 visual_prompt]: 	Training 500/553. train loss: 0.6206,	0.9298 s / batch. (data: 7.39e-04). ETA=11:17:48, max mem: 30.7 GB 
[11/22 19:28:40 visual_prompt]: Epoch 21 / 100: avg data time: 2.51e-02, avg batch time: 0.9545, average train loss: 0.7119
[11/22 19:29:35 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3042, average loss: 0.6820
[11/22 19:29:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 57.48	
[11/22 19:29:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/22 19:31:18 visual_prompt]: 	Training 100/553. train loss: 0.7311,	0.9507 s / batch. (data: 2.23e-04). ETA=11:30:40, max mem: 30.7 GB 
[11/22 19:32:51 visual_prompt]: 	Training 200/553. train loss: 0.8651,	0.9415 s / batch. (data: 1.05e-02). ETA=11:22:23, max mem: 30.7 GB 
[11/22 19:34:25 visual_prompt]: 	Training 300/553. train loss: 0.5495,	0.9445 s / batch. (data: 6.57e-03). ETA=11:22:58, max mem: 30.7 GB 
[11/22 19:35:59 visual_prompt]: 	Training 400/553. train loss: 0.8353,	0.9320 s / batch. (data: 7.99e-03). ETA=11:12:22, max mem: 30.7 GB 
[11/22 19:37:33 visual_prompt]: 	Training 500/553. train loss: 0.8330,	0.9327 s / batch. (data: 8.04e-04). ETA=11:11:21, max mem: 30.7 GB 
[11/22 19:38:22 visual_prompt]: Epoch 22 / 100: avg data time: 2.21e-02, avg batch time: 0.9519, average train loss: 0.7113
[11/22 19:39:17 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3038, average loss: 0.7142
[11/22 19:39:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.50	
[11/22 19:39:17 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/22 19:41:01 visual_prompt]: 	Training 100/553. train loss: 0.6852,	0.9504 s / batch. (data: 1.59e-02). ETA=11:21:40, max mem: 30.7 GB 
[11/22 19:42:35 visual_prompt]: 	Training 200/553. train loss: 0.6739,	0.9480 s / batch. (data: 7.83e-04). ETA=11:18:21, max mem: 30.7 GB 
[11/22 19:44:09 visual_prompt]: 	Training 300/553. train loss: 0.3710,	0.9389 s / batch. (data: 5.36e-03). ETA=11:10:18, max mem: 30.7 GB 
[11/22 19:45:42 visual_prompt]: 	Training 400/553. train loss: 0.8110,	0.9219 s / batch. (data: 5.65e-03). ETA=10:56:36, max mem: 30.7 GB 
[11/22 19:47:16 visual_prompt]: 	Training 500/553. train loss: 0.4068,	0.9355 s / batch. (data: 2.64e-04). ETA=11:04:42, max mem: 30.7 GB 
[11/22 19:48:05 visual_prompt]: Epoch 23 / 100: avg data time: 2.49e-02, avg batch time: 0.9545, average train loss: 0.7159
[11/22 19:49:01 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3034, average loss: 0.8573
[11/22 19:49:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.90	
[11/22 19:49:01 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/22 19:50:44 visual_prompt]: 	Training 100/553. train loss: 0.8666,	0.9148 s / batch. (data: 2.22e-04). ETA=10:47:41, max mem: 30.7 GB 
[11/22 19:52:18 visual_prompt]: 	Training 200/553. train loss: 0.6987,	0.9726 s / batch. (data: 1.47e-02). ETA=11:27:01, max mem: 30.7 GB 
[11/22 19:53:51 visual_prompt]: 	Training 300/553. train loss: 0.6549,	0.9338 s / batch. (data: 7.22e-04). ETA=10:58:02, max mem: 30.7 GB 
[11/22 19:55:25 visual_prompt]: 	Training 400/553. train loss: 0.6962,	0.9440 s / batch. (data: 2.96e-04). ETA=11:03:39, max mem: 30.7 GB 
[11/22 19:56:59 visual_prompt]: 	Training 500/553. train loss: 0.6926,	0.9317 s / batch. (data: 8.89e-03). ETA=10:53:24, max mem: 30.7 GB 
[11/22 19:57:48 visual_prompt]: Epoch 24 / 100: avg data time: 2.31e-02, avg batch time: 0.9524, average train loss: 0.7150
[11/22 19:58:43 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3064, average loss: 0.7453
[11/22 19:58:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.46	
[11/22 19:58:43 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/22 20:00:24 visual_prompt]: 	Training 100/553. train loss: 0.6618,	0.9363 s / batch. (data: 2.61e-04). ETA=10:54:16, max mem: 30.7 GB 
[11/22 20:02:00 visual_prompt]: 	Training 200/553. train loss: 0.4371,	0.9202 s / batch. (data: 3.20e-04). ETA=10:41:30, max mem: 30.7 GB 
[11/22 20:03:34 visual_prompt]: 	Training 300/553. train loss: 0.6548,	0.9305 s / batch. (data: 7.71e-04). ETA=10:47:06, max mem: 30.7 GB 
[11/22 20:05:07 visual_prompt]: 	Training 400/553. train loss: 0.7774,	0.9461 s / batch. (data: 7.11e-04). ETA=10:56:24, max mem: 30.7 GB 
[11/22 20:06:41 visual_prompt]: 	Training 500/553. train loss: 0.5698,	0.9623 s / batch. (data: 5.39e-03). ETA=11:06:01, max mem: 30.7 GB 
[11/22 20:07:30 visual_prompt]: Epoch 25 / 100: avg data time: 2.32e-02, avg batch time: 0.9526, average train loss: 0.7098
[11/22 20:08:26 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3046, average loss: 0.7127
[11/22 20:08:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 58.28	
[11/22 20:08:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/22 20:10:09 visual_prompt]: 	Training 100/553. train loss: 0.5405,	0.9550 s / batch. (data: 1.09e-02). ETA=10:58:31, max mem: 30.7 GB 
[11/22 20:11:42 visual_prompt]: 	Training 200/553. train loss: 0.4819,	0.9141 s / batch. (data: 5.40e-03). ETA=10:28:50, max mem: 30.7 GB 
[11/22 20:13:15 visual_prompt]: 	Training 300/553. train loss: 0.6776,	0.9099 s / batch. (data: 2.72e-04). ETA=10:24:24, max mem: 30.7 GB 
[11/22 20:14:49 visual_prompt]: 	Training 400/553. train loss: 0.6610,	0.9214 s / batch. (data: 7.63e-04). ETA=10:30:45, max mem: 30.7 GB 
[11/22 20:16:22 visual_prompt]: 	Training 500/553. train loss: 0.5649,	0.9368 s / batch. (data: 6.76e-04). ETA=10:39:47, max mem: 30.7 GB 
[11/22 20:17:12 visual_prompt]: Epoch 26 / 100: avg data time: 2.24e-02, avg batch time: 0.9510, average train loss: 0.7123
[11/22 20:18:07 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.3044, average loss: 0.8647
[11/22 20:18:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.01	
[11/22 20:18:07 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/22 20:19:48 visual_prompt]: 	Training 100/553. train loss: 0.6137,	0.9166 s / batch. (data: 2.56e-04). ETA=10:23:35, max mem: 30.7 GB 
[11/22 20:21:22 visual_prompt]: 	Training 200/553. train loss: 0.6680,	0.9050 s / batch. (data: 2.67e-04). ETA=10:14:12, max mem: 30.7 GB 
[11/22 20:22:56 visual_prompt]: 	Training 300/553. train loss: 0.7954,	0.9371 s / batch. (data: 7.51e-04). ETA=10:34:26, max mem: 30.7 GB 
[11/22 20:24:30 visual_prompt]: 	Training 400/553. train loss: 0.7658,	0.9600 s / batch. (data: 7.18e-04). ETA=10:48:20, max mem: 30.7 GB 
[11/22 20:26:03 visual_prompt]: 	Training 500/553. train loss: 0.7071,	0.9374 s / batch. (data: 1.60e-02). ETA=10:31:32, max mem: 30.7 GB 
[11/22 20:26:53 visual_prompt]: Epoch 27 / 100: avg data time: 2.09e-02, avg batch time: 0.9504, average train loss: 0.7275
[11/22 20:27:48 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3034, average loss: 0.7148
[11/22 20:27:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.09	
[11/22 20:27:48 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.0004367053692460385
[11/22 20:29:30 visual_prompt]: 	Training 100/553. train loss: 0.7533,	0.9565 s / batch. (data: 7.99e-03). ETA=10:41:57, max mem: 30.7 GB 
[11/22 20:31:04 visual_prompt]: 	Training 200/553. train loss: 0.5348,	0.9162 s / batch. (data: 2.47e-04). ETA=10:13:23, max mem: 30.7 GB 
[11/22 20:32:37 visual_prompt]: 	Training 300/553. train loss: 1.3317,	0.9343 s / batch. (data: 7.40e-04). ETA=10:23:55, max mem: 30.7 GB 
[11/22 20:34:11 visual_prompt]: 	Training 400/553. train loss: 0.7082,	0.9249 s / batch. (data: 5.82e-03). ETA=10:16:06, max mem: 30.7 GB 
[11/22 20:35:44 visual_prompt]: 	Training 500/553. train loss: 1.0550,	0.9352 s / batch. (data: 7.95e-04). ETA=10:21:25, max mem: 30.7 GB 
[11/22 20:36:33 visual_prompt]: Epoch 28 / 100: avg data time: 2.07e-02, avg batch time: 0.9495, average train loss: 0.7091
[11/22 20:37:29 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3064, average loss: 0.7196
[11/22 20:37:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.24	
[11/22 20:37:29 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.0004311063011977723
[11/22 20:39:12 visual_prompt]: 	Training 100/553. train loss: 0.8283,	0.9373 s / batch. (data: 5.37e-03). ETA=10:20:25, max mem: 30.7 GB 
[11/22 20:40:45 visual_prompt]: 	Training 200/553. train loss: 0.6320,	0.9319 s / batch. (data: 2.65e-04). ETA=10:15:19, max mem: 30.7 GB 
[11/22 20:42:22 visual_prompt]: 	Training 300/553. train loss: 0.7194,	0.9053 s / batch. (data: 2.96e-04). ETA=9:56:13, max mem: 30.7 GB 
[11/22 20:43:55 visual_prompt]: 	Training 400/553. train loss: 0.6684,	0.9625 s / batch. (data: 1.45e-02). ETA=10:32:17, max mem: 30.7 GB 
[11/22 20:45:28 visual_prompt]: 	Training 500/553. train loss: 0.7695,	0.9053 s / batch. (data: 2.63e-04). ETA=9:53:13, max mem: 30.7 GB 
[11/22 20:46:18 visual_prompt]: Epoch 29 / 100: avg data time: 2.83e-02, avg batch time: 0.9561, average train loss: 0.7085
[11/22 20:47:13 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.3066, average loss: 0.7192
[11/22 20:47:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.97	
[11/22 20:47:13 visual_prompt]: Stopping early.
[11/22 20:47:13 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 20:47:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 20:47:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/22 20:47:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/22 20:47:13 visual_prompt]: Training with config:
[11/22 20:47:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/22 20:47:13 visual_prompt]: Loading training data...
[11/22 20:47:13 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 20:47:13 visual_prompt]: Loading validation data...
[11/22 20:47:13 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 20:47:13 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 20:47:18 visual_prompt]: Enable all parameters update during training
[11/22 20:47:18 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/22 20:47:18 visual_prompt]: tuned percent:100.000
[11/22 20:47:18 visual_prompt]: Device used for model: 0
[11/22 20:47:18 visual_prompt]: Setting up Evaluator...
[11/22 20:47:18 visual_prompt]: Setting up Trainer...
[11/22 20:47:18 visual_prompt]: 	Setting up the optimizer...
[11/22 20:47:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 20:48:59 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9322 s / batch. (data: 2.91e-04). ETA=14:17:37, max mem: 30.7 GB 
[11/22 20:50:34 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9665 s / batch. (data: 1.40e-02). ETA=14:47:31, max mem: 30.7 GB 
[11/22 20:52:08 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9247 s / batch. (data: 5.40e-03). ETA=14:07:36, max mem: 30.7 GB 
[11/22 20:53:41 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9479 s / batch. (data: 1.04e-02). ETA=14:27:19, max mem: 30.7 GB 
[11/22 20:55:15 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9280 s / batch. (data: 4.69e-03). ETA=14:07:32, max mem: 30.7 GB 
[11/22 20:56:05 visual_prompt]: Epoch 1 / 100: avg data time: 2.16e-02, avg batch time: 0.9515, average train loss: 7.6130
[11/22 20:57:00 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3045, average loss: 6.9126
[11/22 20:57:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/22 20:57:00 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/22 20:58:42 visual_prompt]: 	Training 100/553. train loss: 0.8526,	0.9296 s / batch. (data: 2.83e-04). ETA=14:06:38, max mem: 30.7 GB 
[11/22 21:00:15 visual_prompt]: 	Training 200/553. train loss: 1.5953,	0.9396 s / batch. (data: 5.39e-03). ETA=14:14:10, max mem: 30.7 GB 
[11/22 21:01:50 visual_prompt]: 	Training 300/553. train loss: 0.8421,	0.9493 s / batch. (data: 5.38e-03). ETA=14:21:28, max mem: 30.7 GB 
[11/22 21:03:23 visual_prompt]: 	Training 400/553. train loss: 0.6142,	0.9480 s / batch. (data: 7.51e-04). ETA=14:18:40, max mem: 30.7 GB 
[11/22 21:04:57 visual_prompt]: 	Training 500/553. train loss: 0.9033,	0.9543 s / batch. (data: 7.52e-04). ETA=14:22:49, max mem: 30.7 GB 
[11/22 21:05:46 visual_prompt]: Epoch 2 / 100: avg data time: 2.08e-02, avg batch time: 0.9509, average train loss: 0.9654
[11/22 21:06:41 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3037, average loss: 1.1215
[11/22 21:06:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.57	
[11/22 21:06:41 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/22 21:08:23 visual_prompt]: 	Training 100/553. train loss: 0.6819,	0.9093 s / batch. (data: 2.52e-04). ETA=13:39:47, max mem: 30.7 GB 
[11/22 21:09:57 visual_prompt]: 	Training 200/553. train loss: 2.5447,	0.9248 s / batch. (data: 6.60e-03). ETA=13:52:16, max mem: 30.7 GB 
[11/22 21:11:31 visual_prompt]: 	Training 300/553. train loss: 0.9270,	0.9278 s / batch. (data: 6.78e-04). ETA=13:53:24, max mem: 30.7 GB 
[11/22 21:13:04 visual_prompt]: 	Training 400/553. train loss: 0.7015,	0.9250 s / batch. (data: 7.08e-04). ETA=13:49:19, max mem: 30.7 GB 
[11/22 21:14:38 visual_prompt]: 	Training 500/553. train loss: 1.0379,	0.9467 s / batch. (data: 5.82e-03). ETA=14:07:13, max mem: 30.7 GB 
[11/22 21:15:27 visual_prompt]: Epoch 3 / 100: avg data time: 2.09e-02, avg batch time: 0.9505, average train loss: 0.8299
[11/22 21:16:22 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3061, average loss: 0.6949
[11/22 21:16:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 61.69	
[11/22 21:16:22 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/22 21:18:08 visual_prompt]: 	Training 100/553. train loss: 0.6776,	0.9088 s / batch. (data: 5.37e-03). ETA=13:30:57, max mem: 30.7 GB 
[11/22 21:19:41 visual_prompt]: 	Training 200/553. train loss: 0.9906,	0.9390 s / batch. (data: 2.25e-04). ETA=13:56:18, max mem: 30.7 GB 
[11/22 21:21:14 visual_prompt]: 	Training 300/553. train loss: 0.9607,	0.9080 s / batch. (data: 2.38e-04). ETA=13:27:11, max mem: 30.7 GB 
[11/22 21:22:48 visual_prompt]: 	Training 400/553. train loss: 0.5014,	0.9283 s / batch. (data: 5.40e-03). ETA=13:43:44, max mem: 30.7 GB 
[11/22 21:24:22 visual_prompt]: 	Training 500/553. train loss: 0.9467,	0.9286 s / batch. (data: 9.98e-04). ETA=13:42:25, max mem: 30.7 GB 
[11/22 21:25:11 visual_prompt]: Epoch 4 / 100: avg data time: 2.72e-02, avg batch time: 0.9560, average train loss: 0.8103
[11/22 21:26:07 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3046, average loss: 0.7004
[11/22 21:26:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.35	
[11/22 21:26:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/22 21:27:48 visual_prompt]: 	Training 100/553. train loss: 0.6725,	0.9772 s / batch. (data: 5.34e-03). ETA=14:23:02, max mem: 30.7 GB 
[11/22 21:29:22 visual_prompt]: 	Training 200/553. train loss: 0.6452,	0.9145 s / batch. (data: 2.46e-04). ETA=13:26:06, max mem: 30.7 GB 
[11/22 21:30:56 visual_prompt]: 	Training 300/553. train loss: 0.7457,	0.9259 s / batch. (data: 9.09e-04). ETA=13:34:37, max mem: 30.7 GB 
[11/22 21:32:30 visual_prompt]: 	Training 400/553. train loss: 1.2095,	0.9343 s / batch. (data: 2.88e-04). ETA=13:40:25, max mem: 30.7 GB 
[11/22 21:34:03 visual_prompt]: 	Training 500/553. train loss: 0.7363,	0.9366 s / batch. (data: 1.04e-03). ETA=13:40:56, max mem: 30.7 GB 
[11/22 21:34:53 visual_prompt]: Epoch 5 / 100: avg data time: 2.12e-02, avg batch time: 0.9513, average train loss: 0.7722
[11/22 21:35:48 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3051, average loss: 0.8596
[11/22 21:35:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 63.96	
[11/22 21:35:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/22 21:37:32 visual_prompt]: 	Training 100/553. train loss: 0.7581,	0.9280 s / batch. (data: 3.53e-04). ETA=13:30:57, max mem: 30.7 GB 
[11/22 21:39:05 visual_prompt]: 	Training 200/553. train loss: 0.7223,	0.9079 s / batch. (data: 2.56e-04). ETA=13:11:55, max mem: 30.7 GB 
[11/22 21:40:39 visual_prompt]: 	Training 300/553. train loss: 1.2884,	0.9537 s / batch. (data: 7.57e-04). ETA=13:50:14, max mem: 30.7 GB 
[11/22 21:42:12 visual_prompt]: 	Training 400/553. train loss: 0.8487,	0.9314 s / batch. (data: 7.99e-03). ETA=13:29:18, max mem: 30.7 GB 
[11/22 21:43:46 visual_prompt]: 	Training 500/553. train loss: 1.0659,	0.9254 s / batch. (data: 7.69e-04). ETA=13:22:35, max mem: 30.7 GB 
[11/22 21:44:35 visual_prompt]: Epoch 6 / 100: avg data time: 2.43e-02, avg batch time: 0.9531, average train loss: 0.7469
[11/22 21:45:31 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3037, average loss: 0.6653
[11/22 21:45:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 64.25	
[11/22 21:45:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/22 21:47:17 visual_prompt]: 	Training 100/553. train loss: 0.5841,	0.9275 s / batch. (data: 5.33e-03). ETA=13:22:02, max mem: 30.7 GB 
[11/22 21:48:51 visual_prompt]: 	Training 200/553. train loss: 0.4469,	0.9142 s / batch. (data: 7.75e-03). ETA=13:09:00, max mem: 30.7 GB 
[11/22 21:50:24 visual_prompt]: 	Training 300/553. train loss: 0.6574,	0.9192 s / batch. (data: 7.41e-04). ETA=13:11:46, max mem: 30.7 GB 
[11/22 21:51:57 visual_prompt]: 	Training 400/553. train loss: 0.7025,	0.9607 s / batch. (data: 1.60e-02). ETA=13:45:53, max mem: 30.7 GB 
[11/22 21:53:31 visual_prompt]: 	Training 500/553. train loss: 0.5821,	0.9281 s / batch. (data: 5.34e-03). ETA=13:16:19, max mem: 30.7 GB 
[11/22 21:54:20 visual_prompt]: Epoch 7 / 100: avg data time: 2.96e-02, avg batch time: 0.9570, average train loss: 0.7393
[11/22 21:55:15 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.3059, average loss: 0.6681
[11/22 21:55:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 65.26	
[11/22 21:55:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/22 21:56:59 visual_prompt]: 	Training 100/553. train loss: 0.5912,	0.9492 s / batch. (data: 5.81e-03). ETA=13:32:00, max mem: 30.7 GB 
[11/22 21:58:32 visual_prompt]: 	Training 200/553. train loss: 0.5609,	0.9582 s / batch. (data: 1.09e-02). ETA=13:38:06, max mem: 30.7 GB 
[11/22 22:00:06 visual_prompt]: 	Training 300/553. train loss: 0.7576,	0.9623 s / batch. (data: 3.73e-02). ETA=13:40:03, max mem: 30.7 GB 
[11/22 22:01:40 visual_prompt]: 	Training 400/553. train loss: 0.5636,	0.9361 s / batch. (data: 1.04e-02). ETA=13:16:09, max mem: 30.7 GB 
[11/22 22:03:14 visual_prompt]: 	Training 500/553. train loss: 0.6628,	0.9635 s / batch. (data: 1.10e-02). ETA=13:37:52, max mem: 30.7 GB 
[11/22 22:04:03 visual_prompt]: Epoch 8 / 100: avg data time: 2.56e-02, avg batch time: 0.9534, average train loss: 0.7446
[11/22 22:04:58 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3043, average loss: 0.6775
[11/22 22:04:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 63.88	
[11/22 22:04:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/22 22:06:43 visual_prompt]: 	Training 100/553. train loss: 0.8776,	0.9399 s / batch. (data: 2.89e-04). ETA=13:15:26, max mem: 30.7 GB 
[11/22 22:08:17 visual_prompt]: 	Training 200/553. train loss: 1.2485,	0.9262 s / batch. (data: 7.49e-04). ETA=13:02:17, max mem: 30.7 GB 
[11/22 22:09:50 visual_prompt]: 	Training 300/553. train loss: 0.5917,	0.9311 s / batch. (data: 2.24e-04). ETA=13:04:50, max mem: 30.7 GB 
[11/22 22:11:23 visual_prompt]: 	Training 400/553. train loss: 0.7019,	0.9316 s / batch. (data: 5.37e-03). ETA=13:03:44, max mem: 30.7 GB 
[11/22 22:12:57 visual_prompt]: 	Training 500/553. train loss: 0.7658,	0.9060 s / batch. (data: 2.61e-04). ETA=12:40:38, max mem: 30.7 GB 
[11/22 22:13:46 visual_prompt]: Epoch 9 / 100: avg data time: 2.64e-02, avg batch time: 0.9546, average train loss: 0.7647
[11/22 22:14:41 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3038, average loss: 0.6855
[11/22 22:14:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 64.91	
[11/22 22:14:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/22 22:16:24 visual_prompt]: 	Training 100/553. train loss: 0.6796,	0.9192 s / batch. (data: 7.27e-04). ETA=12:49:22, max mem: 30.7 GB 
[11/22 22:17:57 visual_prompt]: 	Training 200/553. train loss: 0.6728,	0.9057 s / batch. (data: 2.55e-04). ETA=12:36:36, max mem: 30.7 GB 
[11/22 22:19:31 visual_prompt]: 	Training 300/553. train loss: 0.7446,	0.9348 s / batch. (data: 2.70e-04). ETA=12:59:22, max mem: 30.7 GB 
[11/22 22:21:04 visual_prompt]: 	Training 400/553. train loss: 0.6538,	0.9652 s / batch. (data: 7.19e-04). ETA=13:23:04, max mem: 30.7 GB 
[11/22 22:22:38 visual_prompt]: 	Training 500/553. train loss: 0.7005,	0.9648 s / batch. (data: 2.62e-04). ETA=13:21:08, max mem: 30.7 GB 
[11/22 22:23:27 visual_prompt]: Epoch 10 / 100: avg data time: 2.14e-02, avg batch time: 0.9506, average train loss: 0.7303
[11/22 22:24:22 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3021, average loss: 0.7987
[11/22 22:24:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.99	
[11/22 22:24:22 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/22 22:26:06 visual_prompt]: 	Training 100/553. train loss: 1.0127,	0.9640 s / batch. (data: 3.99e-03). ETA=13:18:01, max mem: 30.7 GB 
[11/22 22:27:40 visual_prompt]: 	Training 200/553. train loss: 0.8107,	0.9419 s / batch. (data: 2.33e-04). ETA=12:58:08, max mem: 30.7 GB 
[11/22 22:29:14 visual_prompt]: 	Training 300/553. train loss: 0.4992,	0.9421 s / batch. (data: 1.04e-02). ETA=12:56:46, max mem: 30.7 GB 
[11/22 22:30:47 visual_prompt]: 	Training 400/553. train loss: 0.8313,	0.9103 s / batch. (data: 4.02e-03). ETA=12:29:01, max mem: 30.7 GB 
[11/22 22:32:21 visual_prompt]: 	Training 500/553. train loss: 0.5601,	0.9313 s / batch. (data: 7.13e-04). ETA=12:44:47, max mem: 30.7 GB 
[11/22 22:33:10 visual_prompt]: Epoch 11 / 100: avg data time: 2.61e-02, avg batch time: 0.9545, average train loss: 0.7261
[11/22 22:34:05 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3054, average loss: 0.6945
[11/22 22:34:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 55.37	
[11/22 22:34:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/22 22:35:45 visual_prompt]: 	Training 100/553. train loss: 0.5367,	0.9345 s / batch. (data: 1.09e-02). ETA=12:44:57, max mem: 30.7 GB 
[11/22 22:37:20 visual_prompt]: 	Training 200/553. train loss: 0.7365,	0.9543 s / batch. (data: 1.43e-02). ETA=12:59:38, max mem: 30.7 GB 
[11/22 22:38:53 visual_prompt]: 	Training 300/553. train loss: 0.7730,	0.9316 s / batch. (data: 5.82e-03). ETA=12:39:31, max mem: 30.7 GB 
[11/22 22:40:27 visual_prompt]: 	Training 400/553. train loss: 0.9645,	0.9413 s / batch. (data: 6.74e-04). ETA=12:45:49, max mem: 30.7 GB 
[11/22 22:42:01 visual_prompt]: 	Training 500/553. train loss: 0.6489,	0.9227 s / batch. (data: 9.96e-03). ETA=12:29:09, max mem: 30.7 GB 
[11/22 22:42:50 visual_prompt]: Epoch 12 / 100: avg data time: 1.88e-02, avg batch time: 0.9487, average train loss: 0.7627
[11/22 22:43:45 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3046, average loss: 1.0113
[11/22 22:43:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.29	
[11/22 22:43:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/22 22:45:29 visual_prompt]: 	Training 100/553. train loss: 0.5978,	0.9426 s / batch. (data: 1.09e-02). ETA=12:42:55, max mem: 30.7 GB 
[11/22 22:47:03 visual_prompt]: 	Training 200/553. train loss: 0.8807,	0.9474 s / batch. (data: 7.95e-03). ETA=12:45:15, max mem: 30.7 GB 
[11/22 22:48:36 visual_prompt]: 	Training 300/553. train loss: 0.7164,	0.9389 s / batch. (data: 5.36e-03). ETA=12:36:48, max mem: 30.7 GB 
[11/22 22:50:10 visual_prompt]: 	Training 400/553. train loss: 0.7859,	0.9210 s / batch. (data: 1.55e-02). ETA=12:20:50, max mem: 30.7 GB 
[11/22 22:51:43 visual_prompt]: 	Training 500/553. train loss: 0.6204,	0.9429 s / batch. (data: 7.32e-04). ETA=12:36:51, max mem: 30.7 GB 
[11/22 22:52:32 visual_prompt]: Epoch 13 / 100: avg data time: 2.38e-02, avg batch time: 0.9524, average train loss: 0.7881
[11/22 22:53:28 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3020, average loss: 0.8635
[11/22 22:53:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.67	
[11/22 22:53:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/22 22:55:12 visual_prompt]: 	Training 100/553. train loss: 0.6539,	0.9255 s / batch. (data: 5.47e-03). ETA=12:20:33, max mem: 30.7 GB 
[11/22 22:56:47 visual_prompt]: 	Training 200/553. train loss: 0.2364,	0.9224 s / batch. (data: 1.73e-04). ETA=12:16:31, max mem: 30.7 GB 
[11/22 22:58:20 visual_prompt]: 	Training 300/553. train loss: 0.7144,	0.9262 s / batch. (data: 3.21e-04). ETA=12:18:03, max mem: 30.7 GB 
[11/22 22:59:53 visual_prompt]: 	Training 400/553. train loss: 1.0492,	0.9421 s / batch. (data: 2.33e-02). ETA=12:29:08, max mem: 30.7 GB 
[11/22 23:01:27 visual_prompt]: 	Training 500/553. train loss: 0.6857,	0.9323 s / batch. (data: 2.39e-04). ETA=12:19:50, max mem: 30.7 GB 
[11/22 23:02:16 visual_prompt]: Epoch 14 / 100: avg data time: 2.74e-02, avg batch time: 0.9551, average train loss: 0.7541
[11/22 23:03:11 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.3040, average loss: 0.7058
[11/22 23:03:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 59.31	
[11/22 23:03:11 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/22 23:04:56 visual_prompt]: 	Training 100/553. train loss: 0.7047,	0.9342 s / batch. (data: 4.50e-03). ETA=12:18:54, max mem: 30.7 GB 
[11/22 23:06:30 visual_prompt]: 	Training 200/553. train loss: 0.6130,	0.9396 s / batch. (data: 2.29e-04). ETA=12:21:39, max mem: 30.7 GB 
[11/22 23:08:04 visual_prompt]: 	Training 300/553. train loss: 0.9048,	0.9475 s / batch. (data: 2.70e-04). ETA=12:26:15, max mem: 30.7 GB 
[11/22 23:09:37 visual_prompt]: 	Training 400/553. train loss: 0.5793,	0.9515 s / batch. (data: 2.20e-02). ETA=12:27:48, max mem: 30.7 GB 
[11/22 23:11:11 visual_prompt]: 	Training 500/553. train loss: 0.9027,	0.9491 s / batch. (data: 5.82e-03). ETA=12:24:22, max mem: 30.7 GB 
[11/22 23:12:00 visual_prompt]: Epoch 15 / 100: avg data time: 2.66e-02, avg batch time: 0.9550, average train loss: 0.7315
[11/22 23:12:55 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3032, average loss: 0.7469
[11/22 23:12:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.03	
[11/22 23:12:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/22 23:14:37 visual_prompt]: 	Training 100/553. train loss: 0.7567,	0.9560 s / batch. (data: 1.16e-03). ETA=12:27:20, max mem: 30.7 GB 
[11/22 23:16:10 visual_prompt]: 	Training 200/553. train loss: 0.6094,	0.9346 s / batch. (data: 8.13e-04). ETA=12:09:04, max mem: 30.7 GB 
[11/22 23:17:44 visual_prompt]: 	Training 300/553. train loss: 0.9184,	0.9707 s / batch. (data: 7.07e-04). ETA=12:35:37, max mem: 30.7 GB 
[11/22 23:19:18 visual_prompt]: 	Training 400/553. train loss: 0.5748,	0.9256 s / batch. (data: 3.70e-04). ETA=11:58:57, max mem: 30.7 GB 
[11/22 23:20:51 visual_prompt]: 	Training 500/553. train loss: 0.7349,	0.9408 s / batch. (data: 7.46e-04). ETA=12:09:14, max mem: 30.7 GB 
[11/22 23:21:41 visual_prompt]: Epoch 16 / 100: avg data time: 2.08e-02, avg batch time: 0.9505, average train loss: 0.7176
[11/22 23:22:36 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3018, average loss: 0.8257
[11/22 23:22:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.83	
[11/22 23:22:36 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/22 23:24:17 visual_prompt]: 	Training 100/553. train loss: 0.7615,	0.9356 s / batch. (data: 2.54e-04). ETA=12:02:48, max mem: 30.7 GB 
[11/22 23:25:52 visual_prompt]: 	Training 200/553. train loss: 0.8960,	0.9609 s / batch. (data: 5.81e-03). ETA=12:20:44, max mem: 30.7 GB 
[11/22 23:27:25 visual_prompt]: 	Training 300/553. train loss: 1.3785,	0.9413 s / batch. (data: 1.63e-02). ETA=12:04:04, max mem: 30.7 GB 
[11/22 23:28:59 visual_prompt]: 	Training 400/553. train loss: 0.6241,	0.9344 s / batch. (data: 7.03e-04). ETA=11:57:11, max mem: 30.7 GB 
[11/22 23:30:32 visual_prompt]: 	Training 500/553. train loss: 0.7120,	0.9187 s / batch. (data: 7.66e-04). ETA=11:43:37, max mem: 30.7 GB 
[11/22 23:31:22 visual_prompt]: Epoch 17 / 100: avg data time: 2.09e-02, avg batch time: 0.9502, average train loss: 0.7216
[11/22 23:32:17 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3045, average loss: 0.6813
[11/22 23:32:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 57.05	
[11/22 23:32:17 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/22 23:34:01 visual_prompt]: 	Training 100/553. train loss: 0.7193,	0.9379 s / batch. (data: 5.42e-03). ETA=11:55:54, max mem: 30.7 GB 
[11/22 23:35:35 visual_prompt]: 	Training 200/553. train loss: 0.6866,	0.9520 s / batch. (data: 2.62e-04). ETA=12:05:05, max mem: 30.7 GB 
[11/22 23:37:08 visual_prompt]: 	Training 300/553. train loss: 0.7589,	0.9376 s / batch. (data: 5.46e-03). ETA=11:52:35, max mem: 30.7 GB 
[11/22 23:38:42 visual_prompt]: 	Training 400/553. train loss: 0.7689,	0.9370 s / batch. (data: 1.45e-02). ETA=11:50:33, max mem: 30.7 GB 
[11/22 23:40:15 visual_prompt]: 	Training 500/553. train loss: 0.6126,	0.9136 s / batch. (data: 3.44e-04). ETA=11:31:15, max mem: 30.7 GB 
[11/22 23:41:04 visual_prompt]: Epoch 18 / 100: avg data time: 2.55e-02, avg batch time: 0.9530, average train loss: 0.7170
[11/22 23:42:00 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3018, average loss: 0.7935
[11/22 23:42:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.43	
[11/22 23:42:00 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/22 23:43:43 visual_prompt]: 	Training 100/553. train loss: 0.7102,	0.9394 s / batch. (data: 7.29e-04). ETA=11:48:24, max mem: 30.7 GB 
[11/22 23:45:16 visual_prompt]: 	Training 200/553. train loss: 0.7384,	0.9160 s / batch. (data: 2.94e-04). ETA=11:29:15, max mem: 30.7 GB 
[11/22 23:46:50 visual_prompt]: 	Training 300/553. train loss: 0.6816,	0.9861 s / batch. (data: 7.73e-04). ETA=12:20:18, max mem: 30.7 GB 
[11/22 23:48:24 visual_prompt]: 	Training 400/553. train loss: 0.6765,	0.9275 s / batch. (data: 7.32e-04). ETA=11:34:46, max mem: 30.7 GB 
[11/22 23:49:57 visual_prompt]: 	Training 500/553. train loss: 0.3715,	0.9534 s / batch. (data: 7.25e-04). ETA=11:52:36, max mem: 30.7 GB 
[11/22 23:50:47 visual_prompt]: Epoch 19 / 100: avg data time: 2.35e-02, avg batch time: 0.9525, average train loss: 0.7144
[11/22 23:51:42 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3049, average loss: 0.8273
[11/22 23:51:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.80	
[11/22 23:51:42 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/22 23:53:27 visual_prompt]: 	Training 100/553. train loss: 0.6698,	0.9365 s / batch. (data: 1.25e-02). ETA=11:37:36, max mem: 30.7 GB 
[11/22 23:55:00 visual_prompt]: 	Training 200/553. train loss: 0.7048,	0.9270 s / batch. (data: 5.36e-03). ETA=11:28:57, max mem: 30.7 GB 
[11/22 23:56:34 visual_prompt]: 	Training 300/553. train loss: 0.6373,	0.9478 s / batch. (data: 2.38e-02). ETA=11:42:48, max mem: 30.7 GB 
[11/22 23:58:08 visual_prompt]: 	Training 400/553. train loss: 0.6441,	0.9573 s / batch. (data: 3.99e-02). ETA=11:48:15, max mem: 30.7 GB 
[11/22 23:59:41 visual_prompt]: 	Training 500/553. train loss: 0.7479,	0.9543 s / batch. (data: 6.32e-03). ETA=11:44:30, max mem: 30.7 GB 
[11/23 00:00:31 visual_prompt]: Epoch 20 / 100: avg data time: 2.71e-02, avg batch time: 0.9559, average train loss: 0.7274
[11/23 00:01:26 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.3053, average loss: 0.7215
[11/23 00:01:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.69	
[11/23 00:01:26 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/23 00:03:11 visual_prompt]: 	Training 100/553. train loss: 0.7021,	0.9388 s / batch. (data: 1.10e-02). ETA=11:30:39, max mem: 30.7 GB 
[11/23 00:04:45 visual_prompt]: 	Training 200/553. train loss: 0.7739,	0.9642 s / batch. (data: 8.30e-03). ETA=11:47:45, max mem: 30.7 GB 
[11/23 00:06:18 visual_prompt]: 	Training 300/553. train loss: 0.7016,	0.9400 s / batch. (data: 7.97e-03). ETA=11:28:23, max mem: 30.7 GB 
[11/23 00:07:52 visual_prompt]: 	Training 400/553. train loss: 0.9387,	0.9231 s / batch. (data: 1.28e-02). ETA=11:14:30, max mem: 30.7 GB 
[11/23 00:09:25 visual_prompt]: 	Training 500/553. train loss: 0.6150,	0.9252 s / batch. (data: 1.56e-02). ETA=11:14:30, max mem: 30.7 GB 
[11/23 00:10:14 visual_prompt]: Epoch 21 / 100: avg data time: 2.64e-02, avg batch time: 0.9551, average train loss: 0.7093
[11/23 00:11:10 visual_prompt]: Inference (val):avg data time: 2.75e-04, avg batch time: 0.3047, average loss: 0.6880
[11/23 00:11:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 54.84	
[11/23 00:11:10 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/23 00:12:52 visual_prompt]: 	Training 100/553. train loss: 0.6388,	0.9319 s / batch. (data: 2.79e-04). ETA=11:17:00, max mem: 30.7 GB 
[11/23 00:14:26 visual_prompt]: 	Training 200/553. train loss: 0.9697,	0.9545 s / batch. (data: 7.33e-04). ETA=11:31:49, max mem: 30.7 GB 
[11/23 00:16:00 visual_prompt]: 	Training 300/553. train loss: 0.6084,	0.9428 s / batch. (data: 1.09e-02). ETA=11:21:43, max mem: 30.7 GB 
[11/23 00:17:33 visual_prompt]: 	Training 400/553. train loss: 0.8066,	0.9408 s / batch. (data: 2.72e-04). ETA=11:18:42, max mem: 30.7 GB 
[11/23 00:19:07 visual_prompt]: 	Training 500/553. train loss: 0.7782,	0.9473 s / batch. (data: 2.00e-02). ETA=11:21:49, max mem: 30.7 GB 
[11/23 00:19:56 visual_prompt]: Epoch 22 / 100: avg data time: 2.26e-02, avg batch time: 0.9516, average train loss: 0.7141
[11/23 00:20:52 visual_prompt]: Inference (val):avg data time: 1.41e-04, avg batch time: 0.3022, average loss: 0.7248
[11/23 00:20:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.00	
[11/23 00:20:52 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/23 00:22:35 visual_prompt]: 	Training 100/553. train loss: 0.6891,	0.9160 s / batch. (data: 7.96e-03). ETA=10:56:59, max mem: 30.7 GB 
[11/23 00:24:09 visual_prompt]: 	Training 200/553. train loss: 0.6398,	0.9193 s / batch. (data: 7.62e-04). ETA=10:57:48, max mem: 30.7 GB 
[11/23 00:25:42 visual_prompt]: 	Training 300/553. train loss: 0.3495,	0.9477 s / batch. (data: 9.29e-03). ETA=11:16:35, max mem: 30.7 GB 
[11/23 00:27:16 visual_prompt]: 	Training 400/553. train loss: 0.7985,	0.9170 s / batch. (data: 1.04e-02). ETA=10:53:08, max mem: 30.7 GB 
[11/23 00:28:50 visual_prompt]: 	Training 500/553. train loss: 0.4103,	0.9259 s / batch. (data: 1.12e-03). ETA=10:57:55, max mem: 30.7 GB 
[11/23 00:29:39 visual_prompt]: Epoch 23 / 100: avg data time: 2.43e-02, avg batch time: 0.9540, average train loss: 0.7133
[11/23 00:30:35 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3049, average loss: 0.8492
[11/23 00:30:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.81	
[11/23 00:30:35 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/23 00:32:18 visual_prompt]: 	Training 100/553. train loss: 0.7790,	0.9237 s / batch. (data: 8.62e-03). ETA=10:54:01, max mem: 30.7 GB 
[11/23 00:33:51 visual_prompt]: 	Training 200/553. train loss: 0.6580,	0.9600 s / batch. (data: 2.53e-04). ETA=11:18:05, max mem: 30.7 GB 
[11/23 00:35:25 visual_prompt]: 	Training 300/553. train loss: 0.5974,	0.9128 s / batch. (data: 3.75e-03). ETA=10:43:13, max mem: 30.7 GB 
[11/23 00:36:59 visual_prompt]: 	Training 400/553. train loss: 0.6833,	0.9228 s / batch. (data: 2.72e-04). ETA=10:48:46, max mem: 30.7 GB 
[11/23 00:38:32 visual_prompt]: 	Training 500/553. train loss: 0.6720,	0.9184 s / batch. (data: 8.45e-04). ETA=10:44:05, max mem: 30.7 GB 
[11/23 00:39:21 visual_prompt]: Epoch 24 / 100: avg data time: 2.35e-02, avg batch time: 0.9521, average train loss: 0.7115
[11/23 00:40:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3043, average loss: 0.7397
[11/23 00:40:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.43	
[11/23 00:40:17 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/23 00:41:58 visual_prompt]: 	Training 100/553. train loss: 0.6786,	0.9569 s / batch. (data: 2.49e-02). ETA=11:08:41, max mem: 30.7 GB 
[11/23 00:43:32 visual_prompt]: 	Training 200/553. train loss: 0.4462,	0.9220 s / batch. (data: 5.81e-03). ETA=10:42:47, max mem: 30.7 GB 
[11/23 00:45:06 visual_prompt]: 	Training 300/553. train loss: 0.6178,	0.9427 s / batch. (data: 5.36e-03). ETA=10:55:36, max mem: 30.7 GB 
[11/23 00:46:40 visual_prompt]: 	Training 400/553. train loss: 0.7164,	0.9486 s / batch. (data: 1.04e-02). ETA=10:58:06, max mem: 30.7 GB 
[11/23 00:48:13 visual_prompt]: 	Training 500/553. train loss: 0.6511,	0.9400 s / batch. (data: 2.75e-04). ETA=10:50:36, max mem: 30.7 GB 
[11/23 00:49:02 visual_prompt]: Epoch 25 / 100: avg data time: 2.23e-02, avg batch time: 0.9504, average train loss: 0.7079
[11/23 00:49:58 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3026, average loss: 0.7206
[11/23 00:49:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 57.23	
[11/23 00:49:58 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/23 00:51:41 visual_prompt]: 	Training 100/553. train loss: 0.5800,	0.9447 s / batch. (data: 1.09e-02). ETA=10:51:25, max mem: 30.7 GB 
[11/23 00:53:14 visual_prompt]: 	Training 200/553. train loss: 0.4635,	0.9225 s / batch. (data: 2.97e-04). ETA=10:34:35, max mem: 30.7 GB 
[11/23 00:54:48 visual_prompt]: 	Training 300/553. train loss: 0.6331,	0.9135 s / batch. (data: 2.77e-04). ETA=10:26:53, max mem: 30.7 GB 
[11/23 00:56:22 visual_prompt]: 	Training 400/553. train loss: 0.6871,	0.9363 s / batch. (data: 2.91e-04). ETA=10:40:56, max mem: 30.7 GB 
[11/23 00:57:56 visual_prompt]: 	Training 500/553. train loss: 0.5078,	0.9773 s / batch. (data: 1.59e-02). ETA=11:07:25, max mem: 30.7 GB 
[11/23 00:58:45 visual_prompt]: Epoch 26 / 100: avg data time: 2.25e-02, avg batch time: 0.9524, average train loss: 0.7096
[11/23 00:59:41 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3061, average loss: 0.8417
[11/23 00:59:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.26	
[11/23 00:59:41 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/23 01:01:24 visual_prompt]: 	Training 100/553. train loss: 0.6734,	0.9476 s / batch. (data: 5.36e-03). ETA=10:44:42, max mem: 30.7 GB 
[11/23 01:03:00 visual_prompt]: 	Training 200/553. train loss: 0.7051,	0.9435 s / batch. (data: 7.41e-04). ETA=10:40:22, max mem: 30.7 GB 
[11/23 01:04:33 visual_prompt]: 	Training 300/553. train loss: 0.7200,	0.9239 s / batch. (data: 5.40e-03). ETA=10:25:32, max mem: 30.7 GB 
[11/23 01:06:06 visual_prompt]: 	Training 400/553. train loss: 0.9063,	0.9444 s / batch. (data: 7.21e-04). ETA=10:37:48, max mem: 30.7 GB 
[11/23 01:07:40 visual_prompt]: 	Training 500/553. train loss: 0.9203,	0.9256 s / batch. (data: 1.59e-02). ETA=10:23:34, max mem: 30.7 GB 
[11/23 01:08:29 visual_prompt]: Epoch 27 / 100: avg data time: 2.65e-02, avg batch time: 0.9540, average train loss: 0.7019
[11/23 01:09:23 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3050, average loss: 0.7013
[11/23 01:09:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 57.17	
[11/23 01:09:23 visual_prompt]: Stopping early.
[11/23 01:09:24 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 01:09:24 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 01:09:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/23 01:09:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/23 01:09:24 visual_prompt]: Training with config:
[11/23 01:09:24 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/23 01:09:24 visual_prompt]: Loading training data...
[11/23 01:09:24 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 01:09:24 visual_prompt]: Loading validation data...
[11/23 01:09:24 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 01:09:24 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 01:09:25 visual_prompt]: Enable all parameters update during training
[11/23 01:09:25 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/23 01:09:25 visual_prompt]: tuned percent:100.000
[11/23 01:09:25 visual_prompt]: Device used for model: 0
[11/23 01:09:25 visual_prompt]: Setting up Evaluator...
[11/23 01:09:25 visual_prompt]: Setting up Trainer...
[11/23 01:09:25 visual_prompt]: 	Setting up the optimizer...
[11/23 01:09:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 01:11:06 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9559 s / batch. (data: 1.55e-02). ETA=14:39:26, max mem: 30.7 GB 
[11/23 01:12:39 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9440 s / batch. (data: 3.14e-04). ETA=14:26:54, max mem: 30.7 GB 
[11/23 01:14:12 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9323 s / batch. (data: 8.22e-04). ETA=14:14:39, max mem: 30.7 GB 
[11/23 01:15:45 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9370 s / batch. (data: 3.76e-02). ETA=14:17:22, max mem: 30.7 GB 
[11/23 01:17:18 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9217 s / batch. (data: 7.71e-04). ETA=14:01:49, max mem: 30.7 GB 
[11/23 01:18:07 visual_prompt]: Epoch 1 / 100: avg data time: 1.81e-02, avg batch time: 0.9432, average train loss: 7.6130
[11/23 01:19:01 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3036, average loss: 6.9126
[11/23 01:19:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/23 01:19:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/23 01:20:42 visual_prompt]: 	Training 100/553. train loss: 2.2763,	0.9031 s / batch. (data: 3.10e-04). ETA=13:42:29, max mem: 30.7 GB 
[11/23 01:22:15 visual_prompt]: 	Training 200/553. train loss: 1.4720,	0.9424 s / batch. (data: 4.01e-02). ETA=14:16:42, max mem: 30.7 GB 
[11/23 01:23:49 visual_prompt]: 	Training 300/553. train loss: 0.9791,	0.9269 s / batch. (data: 9.60e-04). ETA=14:01:08, max mem: 30.7 GB 
[11/23 01:25:22 visual_prompt]: 	Training 400/553. train loss: 0.6726,	0.9040 s / batch. (data: 2.53e-04). ETA=13:38:50, max mem: 30.7 GB 
[11/23 01:26:55 visual_prompt]: 	Training 500/553. train loss: 0.8340,	0.9164 s / batch. (data: 7.11e-04). ETA=13:48:32, max mem: 30.7 GB 
[11/23 01:27:43 visual_prompt]: Epoch 2 / 100: avg data time: 1.98e-02, avg batch time: 0.9440, average train loss: 0.9601
[11/23 01:28:38 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3053, average loss: 0.9600
[11/23 01:28:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 56.89	
[11/23 01:28:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/23 01:30:18 visual_prompt]: 	Training 100/553. train loss: 0.6687,	0.9281 s / batch. (data: 2.79e-04). ETA=13:56:43, max mem: 30.7 GB 
[11/23 01:31:53 visual_prompt]: 	Training 200/553. train loss: 2.6828,	0.9280 s / batch. (data: 2.81e-04). ETA=13:55:06, max mem: 30.7 GB 
[11/23 01:33:28 visual_prompt]: 	Training 300/553. train loss: 0.9033,	0.9480 s / batch. (data: 7.36e-04). ETA=14:11:32, max mem: 30.7 GB 
[11/23 01:35:01 visual_prompt]: 	Training 400/553. train loss: 0.8121,	0.9310 s / batch. (data: 5.66e-03). ETA=13:54:44, max mem: 30.7 GB 
[11/23 01:36:35 visual_prompt]: 	Training 500/553. train loss: 1.4673,	0.9120 s / batch. (data: 7.36e-04). ETA=13:36:08, max mem: 30.7 GB 
[11/23 01:37:24 visual_prompt]: Epoch 3 / 100: avg data time: 2.51e-02, avg batch time: 0.9510, average train loss: 0.8693
[11/23 01:38:20 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3039, average loss: 0.6903
[11/23 01:38:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 58.40	
[11/23 01:38:20 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/23 01:40:05 visual_prompt]: 	Training 100/553. train loss: 0.6416,	0.9159 s / batch. (data: 3.08e-04). ETA=13:37:19, max mem: 30.7 GB 
[11/23 01:41:38 visual_prompt]: 	Training 200/553. train loss: 1.4717,	0.9351 s / batch. (data: 2.91e-04). ETA=13:52:53, max mem: 30.7 GB 
[11/23 01:43:12 visual_prompt]: 	Training 300/553. train loss: 1.2814,	0.9394 s / batch. (data: 3.04e-04). ETA=13:55:09, max mem: 30.7 GB 
[11/23 01:44:45 visual_prompt]: 	Training 400/553. train loss: 0.6688,	0.9186 s / batch. (data: 2.84e-04). ETA=13:35:06, max mem: 30.7 GB 
[11/23 01:46:18 visual_prompt]: 	Training 500/553. train loss: 0.8399,	0.9320 s / batch. (data: 3.56e-04). ETA=13:45:25, max mem: 30.7 GB 
[11/23 01:47:08 visual_prompt]: Epoch 4 / 100: avg data time: 2.78e-02, avg batch time: 0.9534, average train loss: 0.8363
[11/23 01:48:03 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3046, average loss: 0.7211
[11/23 01:48:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.50	
[11/23 01:48:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/23 01:49:44 visual_prompt]: 	Training 100/553. train loss: 0.4869,	0.9428 s / batch. (data: 5.39e-03). ETA=13:52:35, max mem: 30.7 GB 
[11/23 01:51:18 visual_prompt]: 	Training 200/553. train loss: 0.3760,	0.9208 s / batch. (data: 2.93e-04). ETA=13:31:38, max mem: 30.7 GB 
[11/23 01:52:50 visual_prompt]: 	Training 300/553. train loss: 1.7946,	0.9179 s / batch. (data: 2.81e-04). ETA=13:27:35, max mem: 30.7 GB 
[11/23 01:54:23 visual_prompt]: 	Training 400/553. train loss: 0.9542,	0.9322 s / batch. (data: 1.05e-02). ETA=13:38:37, max mem: 30.7 GB 
[11/23 01:55:57 visual_prompt]: 	Training 500/553. train loss: 0.6993,	0.9239 s / batch. (data: 3.13e-04). ETA=13:29:48, max mem: 30.7 GB 
[11/23 01:56:46 visual_prompt]: Epoch 5 / 100: avg data time: 1.98e-02, avg batch time: 0.9440, average train loss: 0.7994
[11/23 01:57:40 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.3043, average loss: 0.7101
[11/23 01:57:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.68	
[11/23 01:57:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/23 01:59:21 visual_prompt]: 	Training 100/553. train loss: 0.4840,	0.9166 s / batch. (data: 3.30e-04). ETA=13:21:00, max mem: 30.7 GB 
[11/23 02:00:54 visual_prompt]: 	Training 200/553. train loss: 1.1728,	0.9365 s / batch. (data: 5.88e-03). ETA=13:36:50, max mem: 30.7 GB 
[11/23 02:02:27 visual_prompt]: 	Training 300/553. train loss: 0.6205,	0.9433 s / batch. (data: 1.60e-02). ETA=13:41:12, max mem: 30.7 GB 
[11/23 02:04:00 visual_prompt]: 	Training 400/553. train loss: 0.9573,	0.9280 s / batch. (data: 4.01e-03). ETA=13:26:21, max mem: 30.7 GB 
[11/23 02:05:34 visual_prompt]: 	Training 500/553. train loss: 1.1753,	0.9406 s / batch. (data: 5.82e-03). ETA=13:35:42, max mem: 30.7 GB 
[11/23 02:06:23 visual_prompt]: Epoch 6 / 100: avg data time: 2.07e-02, avg batch time: 0.9455, average train loss: 0.8535
[11/23 02:07:19 visual_prompt]: Inference (val):avg data time: 4.96e-04, avg batch time: 0.3044, average loss: 0.6462
[11/23 02:07:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 64.87	
[11/23 02:07:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/23 02:09:12 visual_prompt]: 	Training 100/553. train loss: 0.4422,	0.9440 s / batch. (data: 8.45e-04). ETA=13:36:16, max mem: 30.7 GB 
[11/23 02:10:46 visual_prompt]: 	Training 200/553. train loss: 0.5661,	0.9360 s / batch. (data: 2.87e-04). ETA=13:27:46, max mem: 30.7 GB 
[11/23 02:12:20 visual_prompt]: 	Training 300/553. train loss: 0.6176,	0.9380 s / batch. (data: 8.69e-03). ETA=13:27:58, max mem: 30.7 GB 
[11/23 02:13:53 visual_prompt]: 	Training 400/553. train loss: 1.0401,	0.8999 s / batch. (data: 2.99e-04). ETA=12:53:40, max mem: 30.7 GB 
[11/23 02:15:26 visual_prompt]: 	Training 500/553. train loss: 0.5189,	0.9320 s / batch. (data: 7.26e-04). ETA=13:19:39, max mem: 30.7 GB 
[11/23 02:16:15 visual_prompt]: Epoch 7 / 100: avg data time: 4.47e-02, avg batch time: 0.9695, average train loss: 0.8280
[11/23 02:17:09 visual_prompt]: Inference (val):avg data time: 2.10e-04, avg batch time: 0.3035, average loss: 0.6482
[11/23 02:17:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 66.74	
[11/23 02:17:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/23 02:18:52 visual_prompt]: 	Training 100/553. train loss: 0.4103,	0.9480 s / batch. (data: 7.52e-04). ETA=13:31:00, max mem: 30.7 GB 
[11/23 02:20:25 visual_prompt]: 	Training 200/553. train loss: 0.7126,	0.9673 s / batch. (data: 1.59e-02). ETA=13:45:56, max mem: 30.7 GB 
[11/23 02:21:59 visual_prompt]: 	Training 300/553. train loss: 0.9793,	0.9302 s / batch. (data: 7.21e-04). ETA=13:12:39, max mem: 30.7 GB 
[11/23 02:23:32 visual_prompt]: 	Training 400/553. train loss: 0.6417,	0.9208 s / batch. (data: 8.43e-04). ETA=13:03:09, max mem: 30.7 GB 
[11/23 02:25:05 visual_prompt]: 	Training 500/553. train loss: 0.7592,	0.9159 s / batch. (data: 5.83e-03). ETA=12:57:27, max mem: 30.7 GB 
[11/23 02:25:54 visual_prompt]: Epoch 8 / 100: avg data time: 2.28e-02, avg batch time: 0.9480, average train loss: 0.8261
[11/23 02:26:48 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.3037, average loss: 0.6207
[11/23 02:26:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 69.58	
[11/23 02:26:48 visual_prompt]: Best epoch 8: best metric: -0.621
[11/23 02:26:48 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/23 02:28:31 visual_prompt]: 	Training 100/553. train loss: 1.0805,	0.9323 s / batch. (data: 7.59e-04). ETA=13:08:56, max mem: 30.7 GB 
[11/23 02:30:04 visual_prompt]: 	Training 200/553. train loss: 0.6797,	0.9198 s / batch. (data: 7.06e-04). ETA=12:56:49, max mem: 30.7 GB 
[11/23 02:31:37 visual_prompt]: 	Training 300/553. train loss: 0.5675,	0.9599 s / batch. (data: 1.09e-02). ETA=13:29:08, max mem: 30.7 GB 
[11/23 02:33:10 visual_prompt]: 	Training 400/553. train loss: 1.1837,	0.9200 s / batch. (data: 5.82e-03). ETA=12:53:58, max mem: 30.7 GB 
[11/23 02:34:43 visual_prompt]: 	Training 500/553. train loss: 1.2788,	0.9320 s / batch. (data: 7.67e-04). ETA=13:02:31, max mem: 30.7 GB 
[11/23 02:35:32 visual_prompt]: Epoch 9 / 100: avg data time: 2.36e-02, avg batch time: 0.9472, average train loss: 0.8615
[11/23 02:36:26 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3050, average loss: 0.7843
[11/23 02:36:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 68.40	
[11/23 02:36:26 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/23 02:38:06 visual_prompt]: 	Training 100/553. train loss: 1.1196,	0.9468 s / batch. (data: 8.19e-04). ETA=13:12:30, max mem: 30.7 GB 
[11/23 02:39:40 visual_prompt]: 	Training 200/553. train loss: 0.6746,	0.9240 s / batch. (data: 3.02e-04). ETA=12:51:56, max mem: 30.7 GB 
[11/23 02:41:13 visual_prompt]: 	Training 300/553. train loss: 1.4257,	0.9325 s / batch. (data: 5.36e-03). ETA=12:57:28, max mem: 30.7 GB 
[11/23 02:42:46 visual_prompt]: 	Training 400/553. train loss: 1.4576,	0.9308 s / batch. (data: 5.36e-03). ETA=12:54:29, max mem: 30.7 GB 
[11/23 02:44:19 visual_prompt]: 	Training 500/553. train loss: 0.3996,	0.9474 s / batch. (data: 2.61e-02). ETA=13:06:41, max mem: 30.7 GB 
[11/23 02:45:08 visual_prompt]: Epoch 10 / 100: avg data time: 1.89e-02, avg batch time: 0.9433, average train loss: 0.8035
[11/23 02:46:02 visual_prompt]: Inference (val):avg data time: 1.36e-04, avg batch time: 0.3036, average loss: 0.6554
[11/23 02:46:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 68.71	
[11/23 02:46:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/23 02:47:45 visual_prompt]: 	Training 100/553. train loss: 0.9824,	0.9160 s / batch. (data: 3.98e-03). ETA=12:38:19, max mem: 30.7 GB 
[11/23 02:49:18 visual_prompt]: 	Training 200/553. train loss: 1.1395,	0.9343 s / batch. (data: 2.05e-02). ETA=12:51:54, max mem: 30.7 GB 
[11/23 02:50:50 visual_prompt]: 	Training 300/553. train loss: 0.5716,	0.9240 s / batch. (data: 7.25e-04). ETA=12:41:51, max mem: 30.7 GB 
[11/23 02:52:23 visual_prompt]: 	Training 400/553. train loss: 0.4501,	0.9201 s / batch. (data: 6.88e-04). ETA=12:37:03, max mem: 30.7 GB 
[11/23 02:53:57 visual_prompt]: 	Training 500/553. train loss: 0.4674,	0.9080 s / batch. (data: 5.40e-03). ETA=12:25:35, max mem: 30.7 GB 
[11/23 02:54:46 visual_prompt]: Epoch 11 / 100: avg data time: 2.26e-02, avg batch time: 0.9471, average train loss: 0.7774
[11/23 02:55:40 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3043, average loss: 0.9311
[11/23 02:55:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 69.82	
[11/23 02:55:40 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/23 02:57:18 visual_prompt]: 	Training 100/553. train loss: 0.8429,	0.9595 s / batch. (data: 5.87e-03). ETA=13:05:28, max mem: 30.7 GB 
[11/23 02:58:52 visual_prompt]: 	Training 200/553. train loss: 0.8458,	0.9480 s / batch. (data: 3.93e-04). ETA=12:54:26, max mem: 30.7 GB 
[11/23 03:00:25 visual_prompt]: 	Training 300/553. train loss: 1.1359,	0.9143 s / batch. (data: 7.75e-04). ETA=12:25:26, max mem: 30.7 GB 
[11/23 03:01:58 visual_prompt]: 	Training 400/553. train loss: 0.3422,	0.9710 s / batch. (data: 4.70e-02). ETA=13:10:01, max mem: 30.7 GB 
[11/23 03:03:31 visual_prompt]: 	Training 500/553. train loss: 0.3879,	0.9360 s / batch. (data: 8.04e-03). ETA=12:39:58, max mem: 30.7 GB 
[11/23 03:04:20 visual_prompt]: Epoch 12 / 100: avg data time: 1.58e-02, avg batch time: 0.9399, average train loss: 0.8042
[11/23 03:05:14 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.3052, average loss: 0.7282
[11/23 03:05:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.48	
[11/23 03:05:14 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/23 03:06:55 visual_prompt]: 	Training 100/553. train loss: 0.5093,	0.9400 s / batch. (data: 8.17e-04). ETA=12:40:48, max mem: 30.7 GB 
[11/23 03:08:28 visual_prompt]: 	Training 200/553. train loss: 1.8804,	0.9520 s / batch. (data: 7.97e-03). ETA=12:48:57, max mem: 30.7 GB 
[11/23 03:10:01 visual_prompt]: 	Training 300/553. train loss: 1.7858,	0.9282 s / batch. (data: 7.96e-03). ETA=12:28:10, max mem: 30.7 GB 
[11/23 03:11:34 visual_prompt]: 	Training 400/553. train loss: 0.7881,	0.9083 s / batch. (data: 3.24e-04). ETA=12:10:37, max mem: 30.7 GB 
[11/23 03:13:07 visual_prompt]: 	Training 500/553. train loss: 0.2751,	0.9233 s / batch. (data: 7.11e-04). ETA=12:21:08, max mem: 30.7 GB 
[11/23 03:13:56 visual_prompt]: Epoch 13 / 100: avg data time: 1.99e-02, avg batch time: 0.9445, average train loss: 0.7344
[11/23 03:14:50 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3064, average loss: 0.8648
[11/23 03:14:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 68.17	
[11/23 03:14:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/23 03:16:32 visual_prompt]: 	Training 100/553. train loss: 0.9305,	0.9306 s / batch. (data: 1.05e-02). ETA=12:24:37, max mem: 30.7 GB 
[11/23 03:18:05 visual_prompt]: 	Training 200/553. train loss: 0.1216,	0.9041 s / batch. (data: 2.48e-04). ETA=12:01:54, max mem: 30.7 GB 
[11/23 03:19:38 visual_prompt]: 	Training 300/553. train loss: 0.8701,	0.9254 s / batch. (data: 1.05e-02). ETA=12:17:22, max mem: 30.7 GB 
[11/23 03:21:11 visual_prompt]: 	Training 400/553. train loss: 1.3667,	0.9380 s / batch. (data: 3.82e-02). ETA=12:25:53, max mem: 30.7 GB 
[11/23 03:22:44 visual_prompt]: 	Training 500/553. train loss: 0.7954,	0.9218 s / batch. (data: 2.92e-04). ETA=12:11:27, max mem: 30.7 GB 
[11/23 03:23:33 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e-02, avg batch time: 0.9450, average train loss: 0.7684
[11/23 03:24:27 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3037, average loss: 0.7544
[11/23 03:24:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 68.67	
[11/23 03:24:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/23 03:26:11 visual_prompt]: 	Training 100/553. train loss: 0.3708,	0.9281 s / batch. (data: 1.05e-02). ETA=12:14:05, max mem: 30.7 GB 
[11/23 03:27:44 visual_prompt]: 	Training 200/553. train loss: 0.7023,	0.9365 s / batch. (data: 1.67e-02). ETA=12:19:12, max mem: 30.7 GB 
[11/23 03:29:17 visual_prompt]: 	Training 300/553. train loss: 0.3173,	0.9200 s / batch. (data: 5.35e-03). ETA=12:04:35, max mem: 30.7 GB 
[11/23 03:30:50 visual_prompt]: 	Training 400/553. train loss: 0.2730,	0.9042 s / batch. (data: 2.63e-04). ETA=11:50:41, max mem: 30.7 GB 
[11/23 03:32:23 visual_prompt]: 	Training 500/553. train loss: 0.4333,	0.9321 s / batch. (data: 7.89e-04). ETA=12:11:00, max mem: 30.7 GB 
[11/23 03:33:12 visual_prompt]: Epoch 15 / 100: avg data time: 2.44e-02, avg batch time: 0.9483, average train loss: 0.7430
[11/23 03:34:06 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.3063, average loss: 1.0890
[11/23 03:34:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 67.59	
[11/23 03:34:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/23 03:35:47 visual_prompt]: 	Training 100/553. train loss: 1.1151,	0.9440 s / batch. (data: 6.99e-04). ETA=12:17:58, max mem: 30.7 GB 
[11/23 03:37:19 visual_prompt]: 	Training 200/553. train loss: 0.1964,	0.9080 s / batch. (data: 3.38e-04). ETA=11:48:18, max mem: 30.7 GB 
[11/23 03:38:52 visual_prompt]: 	Training 300/553. train loss: 0.3651,	0.9400 s / batch. (data: 3.14e-04). ETA=12:11:42, max mem: 30.7 GB 
[11/23 03:40:26 visual_prompt]: 	Training 400/553. train loss: 0.4899,	0.9200 s / batch. (data: 3.37e-04). ETA=11:54:35, max mem: 30.7 GB 
[11/23 03:41:58 visual_prompt]: 	Training 500/553. train loss: 1.1686,	0.9291 s / batch. (data: 7.55e-04). ETA=12:00:06, max mem: 30.7 GB 
[11/23 03:42:48 visual_prompt]: Epoch 16 / 100: avg data time: 1.89e-02, avg batch time: 0.9436, average train loss: 0.7380
[11/23 03:43:42 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.3037, average loss: 0.6948
[11/23 03:43:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 67.93	
[11/23 03:43:42 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/23 03:45:21 visual_prompt]: 	Training 100/553. train loss: 0.8789,	0.9262 s / batch. (data: 1.05e-02). ETA=11:55:31, max mem: 30.7 GB 
[11/23 03:46:54 visual_prompt]: 	Training 200/553. train loss: 2.0718,	0.9061 s / batch. (data: 2.98e-04). ETA=11:38:31, max mem: 30.7 GB 
[11/23 03:48:27 visual_prompt]: 	Training 300/553. train loss: 0.2911,	0.9219 s / batch. (data: 1.05e-02). ETA=11:49:07, max mem: 30.7 GB 
[11/23 03:50:00 visual_prompt]: 	Training 400/553. train loss: 0.6696,	0.9360 s / batch. (data: 2.79e-04). ETA=11:58:25, max mem: 30.7 GB 
[11/23 03:51:34 visual_prompt]: 	Training 500/553. train loss: 0.9520,	0.9480 s / batch. (data: 2.82e-04). ETA=12:06:02, max mem: 30.7 GB 
[11/23 03:52:23 visual_prompt]: Epoch 17 / 100: avg data time: 1.80e-02, avg batch time: 0.9416, average train loss: 0.7368
[11/23 03:53:17 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3066, average loss: 0.8032
[11/23 03:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 69.04	
[11/23 03:53:17 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/23 03:54:59 visual_prompt]: 	Training 100/553. train loss: 0.3487,	0.9341 s / batch. (data: 2.01e-02). ETA=11:53:02, max mem: 30.7 GB 
[11/23 03:56:32 visual_prompt]: 	Training 200/553. train loss: 0.5497,	0.9153 s / batch. (data: 7.58e-04). ETA=11:37:08, max mem: 30.7 GB 
[11/23 03:58:05 visual_prompt]: 	Training 300/553. train loss: 0.7305,	0.9428 s / batch. (data: 7.54e-04). ETA=11:56:30, max mem: 30.7 GB 
[11/23 03:59:37 visual_prompt]: 	Training 400/553. train loss: 0.2934,	0.9494 s / batch. (data: 2.14e-02). ETA=11:59:58, max mem: 30.7 GB 
[11/23 04:01:10 visual_prompt]: 	Training 500/553. train loss: 0.6135,	0.9440 s / batch. (data: 7.69e-04). ETA=11:54:17, max mem: 30.7 GB 
[11/23 04:01:59 visual_prompt]: Epoch 18 / 100: avg data time: 2.24e-02, avg batch time: 0.9450, average train loss: 0.6837
[11/23 04:02:53 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.3057, average loss: 0.6163
[11/23 04:02:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 71.50	
[11/23 04:02:53 visual_prompt]: Best epoch 18: best metric: -0.616
[11/23 04:02:53 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/23 04:04:35 visual_prompt]: 	Training 100/553. train loss: 0.1259,	0.9305 s / batch. (data: 6.64e-04). ETA=11:41:39, max mem: 30.7 GB 
[11/23 04:06:08 visual_prompt]: 	Training 200/553. train loss: 0.7318,	0.9440 s / batch. (data: 2.50e-04). ETA=11:50:17, max mem: 30.7 GB 
[11/23 04:07:41 visual_prompt]: 	Training 300/553. train loss: 0.2510,	0.9200 s / batch. (data: 7.96e-04). ETA=11:30:41, max mem: 30.7 GB 
[11/23 04:09:14 visual_prompt]: 	Training 400/553. train loss: 0.4414,	0.9439 s / batch. (data: 7.51e-04). ETA=11:47:06, max mem: 30.7 GB 
[11/23 04:10:47 visual_prompt]: 	Training 500/553. train loss: 0.2205,	0.9200 s / batch. (data: 3.39e-04). ETA=11:27:36, max mem: 30.7 GB 
[11/23 04:11:37 visual_prompt]: Epoch 19 / 100: avg data time: 2.10e-02, avg batch time: 0.9455, average train loss: 0.6547
[11/23 04:12:30 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3038, average loss: 0.7324
[11/23 04:12:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 68.83	
[11/23 04:12:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/23 04:14:13 visual_prompt]: 	Training 100/553. train loss: 0.8923,	0.9243 s / batch. (data: 5.34e-03). ETA=11:28:31, max mem: 30.7 GB 
[11/23 04:15:46 visual_prompt]: 	Training 200/553. train loss: 0.6398,	0.9322 s / batch. (data: 7.42e-04). ETA=11:32:49, max mem: 30.7 GB 
[11/23 04:17:19 visual_prompt]: 	Training 300/553. train loss: 0.5928,	0.9520 s / batch. (data: 7.40e-04). ETA=11:45:57, max mem: 30.7 GB 
[11/23 04:18:52 visual_prompt]: 	Training 400/553. train loss: 0.8332,	0.9408 s / batch. (data: 5.45e-03). ETA=11:36:06, max mem: 30.7 GB 
[11/23 04:20:25 visual_prompt]: 	Training 500/553. train loss: 0.4948,	0.9093 s / batch. (data: 2.95e-04). ETA=11:11:14, max mem: 30.7 GB 
[11/23 04:21:14 visual_prompt]: Epoch 20 / 100: avg data time: 2.37e-02, avg batch time: 0.9469, average train loss: 0.6536
[11/23 04:22:08 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.3065, average loss: 0.6728
[11/23 04:22:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.15	
[11/23 04:22:08 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/23 04:23:51 visual_prompt]: 	Training 100/553. train loss: 0.3169,	0.9320 s / batch. (data: 7.46e-04). ETA=11:25:36, max mem: 30.7 GB 
[11/23 04:25:24 visual_prompt]: 	Training 200/553. train loss: 0.3651,	0.9682 s / batch. (data: 2.61e-02). ETA=11:50:41, max mem: 30.7 GB 
[11/23 04:26:56 visual_prompt]: 	Training 300/553. train loss: 0.6579,	0.9320 s / batch. (data: 2.84e-04). ETA=11:22:31, max mem: 30.7 GB 
[11/23 04:28:29 visual_prompt]: 	Training 400/553. train loss: 0.7536,	0.9200 s / batch. (data: 2.62e-04). ETA=11:12:13, max mem: 30.7 GB 
[11/23 04:30:02 visual_prompt]: 	Training 500/553. train loss: 0.4979,	0.9191 s / batch. (data: 5.47e-03). ETA=11:09:59, max mem: 30.7 GB 
[11/23 04:30:51 visual_prompt]: Epoch 21 / 100: avg data time: 2.19e-02, avg batch time: 0.9454, average train loss: 0.5979
[11/23 04:31:45 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3043, average loss: 0.7796
[11/23 04:31:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 70.68	
[11/23 04:31:45 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/23 04:33:26 visual_prompt]: 	Training 100/553. train loss: 0.8048,	0.9440 s / batch. (data: 1.05e-02). ETA=11:25:47, max mem: 30.7 GB 
[11/23 04:34:59 visual_prompt]: 	Training 200/553. train loss: 0.8792,	0.9371 s / batch. (data: 5.87e-03). ETA=11:19:10, max mem: 30.7 GB 
[11/23 04:36:32 visual_prompt]: 	Training 300/553. train loss: 0.4028,	0.9400 s / batch. (data: 7.86e-04). ETA=11:19:43, max mem: 30.7 GB 
[11/23 04:38:05 visual_prompt]: 	Training 400/553. train loss: 0.2525,	0.9000 s / batch. (data: 3.17e-04). ETA=10:49:16, max mem: 30.7 GB 
[11/23 04:39:38 visual_prompt]: 	Training 500/553. train loss: 0.7689,	0.9288 s / batch. (data: 7.41e-04). ETA=11:08:31, max mem: 30.7 GB 
[11/23 04:40:27 visual_prompt]: Epoch 22 / 100: avg data time: 1.83e-02, avg batch time: 0.9425, average train loss: 0.5991
[11/23 04:41:21 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3018, average loss: 1.2781
[11/23 04:41:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 67.68	
[11/23 04:41:21 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/23 04:43:02 visual_prompt]: 	Training 100/553. train loss: 0.4069,	0.9240 s / batch. (data: 8.36e-04). ETA=11:02:43, max mem: 30.7 GB 
[11/23 04:44:35 visual_prompt]: 	Training 200/553. train loss: 0.7653,	0.9031 s / batch. (data: 2.44e-04). ETA=10:46:12, max mem: 30.7 GB 
[11/23 04:46:08 visual_prompt]: 	Training 300/553. train loss: 0.0925,	0.9153 s / batch. (data: 2.84e-03). ETA=10:53:23, max mem: 30.7 GB 
[11/23 04:47:41 visual_prompt]: 	Training 400/553. train loss: 0.0480,	0.9293 s / batch. (data: 7.27e-04). ETA=11:01:50, max mem: 30.7 GB 
[11/23 04:49:14 visual_prompt]: 	Training 500/553. train loss: 0.0903,	0.9374 s / batch. (data: 7.14e-04). ETA=11:06:06, max mem: 30.7 GB 
[11/23 04:50:04 visual_prompt]: Epoch 23 / 100: avg data time: 2.11e-02, avg batch time: 0.9454, average train loss: 0.6164
[11/23 04:50:58 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3036, average loss: 0.8593
[11/23 04:50:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 71.77	
[11/23 04:50:58 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/23 04:52:40 visual_prompt]: 	Training 100/553. train loss: 0.6013,	0.9560 s / batch. (data: 7.77e-04). ETA=11:16:52, max mem: 30.7 GB 
[11/23 04:54:13 visual_prompt]: 	Training 200/553. train loss: 0.1326,	0.9160 s / batch. (data: 2.77e-04). ETA=10:47:01, max mem: 30.7 GB 
[11/23 04:55:46 visual_prompt]: 	Training 300/553. train loss: 0.6811,	0.9240 s / batch. (data: 7.09e-04). ETA=10:51:09, max mem: 30.7 GB 
[11/23 04:57:19 visual_prompt]: 	Training 400/553. train loss: 0.2469,	0.9473 s / batch. (data: 1.04e-02). ETA=11:05:57, max mem: 30.7 GB 
[11/23 04:58:52 visual_prompt]: 	Training 500/553. train loss: 0.7204,	0.9544 s / batch. (data: 3.73e-02). ETA=11:09:22, max mem: 30.7 GB 
[11/23 04:59:41 visual_prompt]: Epoch 24 / 100: avg data time: 2.18e-02, avg batch time: 0.9465, average train loss: 0.5481
[11/23 05:00:35 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.3037, average loss: 0.6727
[11/23 05:00:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 69.78	
[11/23 05:00:35 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/23 05:02:15 visual_prompt]: 	Training 100/553. train loss: 0.5673,	0.9030 s / batch. (data: 5.10e-03). ETA=10:31:00, max mem: 30.7 GB 
[11/23 05:03:48 visual_prompt]: 	Training 200/553. train loss: 0.3740,	0.9144 s / batch. (data: 3.25e-04). ETA=10:37:28, max mem: 30.7 GB 
[11/23 05:05:21 visual_prompt]: 	Training 300/553. train loss: 0.2799,	0.9579 s / batch. (data: 5.92e-03). ETA=11:06:13, max mem: 30.7 GB 
[11/23 05:06:54 visual_prompt]: 	Training 400/553. train loss: 0.4082,	0.9080 s / batch. (data: 2.85e-04). ETA=10:29:58, max mem: 30.7 GB 
[11/23 05:08:27 visual_prompt]: 	Training 500/553. train loss: 0.4242,	0.9320 s / batch. (data: 3.08e-04). ETA=10:45:04, max mem: 30.7 GB 
[11/23 05:09:16 visual_prompt]: Epoch 25 / 100: avg data time: 1.69e-02, avg batch time: 0.9422, average train loss: 0.5258
[11/23 05:10:10 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3045, average loss: 0.8589
[11/23 05:10:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 68.77	
[11/23 05:10:10 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/23 05:11:53 visual_prompt]: 	Training 100/553. train loss: 0.8243,	0.9509 s / batch. (data: 1.05e-02). ETA=10:55:45, max mem: 30.7 GB 
[11/23 05:13:26 visual_prompt]: 	Training 200/553. train loss: 0.0335,	0.9400 s / batch. (data: 2.89e-04). ETA=10:46:38, max mem: 30.7 GB 
[11/23 05:14:59 visual_prompt]: 	Training 300/553. train loss: 0.0822,	0.9226 s / batch. (data: 1.20e-02). ETA=10:33:07, max mem: 30.7 GB 
[11/23 05:16:32 visual_prompt]: 	Training 400/553. train loss: 0.1771,	0.9188 s / batch. (data: 3.04e-04). ETA=10:29:01, max mem: 30.7 GB 
[11/23 05:18:05 visual_prompt]: 	Training 500/553. train loss: 0.4856,	0.9400 s / batch. (data: 7.49e-04). ETA=10:41:56, max mem: 30.7 GB 
[11/23 05:18:54 visual_prompt]: Epoch 26 / 100: avg data time: 2.21e-02, avg batch time: 0.9462, average train loss: 0.5290
[11/23 05:19:48 visual_prompt]: Inference (val):avg data time: 1.74e-04, avg batch time: 0.3040, average loss: 0.9010
[11/23 05:19:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 67.05	
[11/23 05:19:48 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/23 05:21:29 visual_prompt]: 	Training 100/553. train loss: 0.1717,	0.9329 s / batch. (data: 1.10e-02). ETA=10:34:41, max mem: 30.7 GB 
[11/23 05:23:02 visual_prompt]: 	Training 200/553. train loss: 0.3648,	0.9480 s / batch. (data: 2.50e-04). ETA=10:43:24, max mem: 30.7 GB 
[11/23 05:24:35 visual_prompt]: 	Training 300/553. train loss: 0.3438,	0.9185 s / batch. (data: 5.87e-03). ETA=10:21:50, max mem: 30.7 GB 
[11/23 05:26:08 visual_prompt]: 	Training 400/553. train loss: 0.2575,	0.9289 s / batch. (data: 5.84e-03). ETA=10:27:20, max mem: 30.7 GB 
[11/23 05:27:41 visual_prompt]: 	Training 500/553. train loss: 0.3751,	0.9240 s / batch. (data: 8.05e-04). ETA=10:22:31, max mem: 30.7 GB 
[11/23 05:28:30 visual_prompt]: Epoch 27 / 100: avg data time: 1.96e-02, avg batch time: 0.9444, average train loss: 0.5343
[11/23 05:29:24 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3059, average loss: 0.8907
[11/23 05:29:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 68.02	
[11/23 05:29:24 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.0004367053692460385
[11/23 05:31:05 visual_prompt]: 	Training 100/553. train loss: 0.6077,	0.9209 s / batch. (data: 1.13e-03). ETA=10:18:03, max mem: 30.7 GB 
[11/23 05:32:38 visual_prompt]: 	Training 200/553. train loss: 0.0471,	0.9265 s / batch. (data: 6.44e-03). ETA=10:20:16, max mem: 30.7 GB 
[11/23 05:34:11 visual_prompt]: 	Training 300/553. train loss: 0.5071,	0.9304 s / batch. (data: 8.01e-04). ETA=10:21:18, max mem: 30.7 GB 
[11/23 05:35:44 visual_prompt]: 	Training 400/553. train loss: 0.2599,	0.9263 s / batch. (data: 1.05e-02). ETA=10:17:05, max mem: 30.7 GB 
[11/23 05:37:17 visual_prompt]: 	Training 500/553. train loss: 0.3073,	0.9526 s / batch. (data: 1.65e-02). ETA=10:32:58, max mem: 30.7 GB 
[11/23 05:38:06 visual_prompt]: Epoch 28 / 100: avg data time: 1.98e-02, avg batch time: 0.9431, average train loss: 0.4887
[11/23 05:39:00 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3056, average loss: 0.8463
[11/23 05:39:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 69.39	
[11/23 05:39:00 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.0004311063011977723
[11/23 05:40:42 visual_prompt]: 	Training 100/553. train loss: 0.9766,	0.9160 s / batch. (data: 2.92e-04). ETA=10:06:21, max mem: 30.7 GB 
[11/23 05:42:15 visual_prompt]: 	Training 200/553. train loss: 0.1941,	0.9398 s / batch. (data: 7.82e-04). ETA=10:20:29, max mem: 30.7 GB 
[11/23 05:43:48 visual_prompt]: 	Training 300/553. train loss: 0.6304,	0.9280 s / batch. (data: 3.03e-04). ETA=10:11:10, max mem: 30.7 GB 
[11/23 05:45:21 visual_prompt]: 	Training 400/553. train loss: 0.6377,	0.9271 s / batch. (data: 5.41e-03). ETA=10:09:02, max mem: 30.7 GB 
[11/23 05:46:54 visual_prompt]: 	Training 500/553. train loss: 0.0112,	0.9274 s / batch. (data: 1.63e-02). ETA=10:07:43, max mem: 30.7 GB 
[11/23 05:47:43 visual_prompt]: Epoch 29 / 100: avg data time: 2.13e-02, avg batch time: 0.9455, average train loss: 0.4410
[11/23 05:48:37 visual_prompt]: Inference (val):avg data time: 1.77e-04, avg batch time: 0.3063, average loss: 0.9972
[11/23 05:48:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 68.06	
[11/23 05:48:37 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.00042530919606812215
[11/23 05:50:18 visual_prompt]: 	Training 100/553. train loss: 0.3390,	0.9182 s / batch. (data: 2.46e-04). ETA=9:59:20, max mem: 30.7 GB 
[11/23 05:51:51 visual_prompt]: 	Training 200/553. train loss: 0.4405,	0.9320 s / batch. (data: 7.90e-04). ETA=10:06:46, max mem: 30.7 GB 
[11/23 05:53:24 visual_prompt]: 	Training 300/553. train loss: 0.3091,	0.9270 s / batch. (data: 1.26e-03). ETA=10:01:58, max mem: 30.7 GB 
[11/23 05:54:57 visual_prompt]: 	Training 400/553. train loss: 0.0832,	0.9480 s / batch. (data: 7.02e-04). ETA=10:14:03, max mem: 30.7 GB 
[11/23 05:56:30 visual_prompt]: 	Training 500/553. train loss: 0.1616,	0.9360 s / batch. (data: 2.61e-04). ETA=10:04:42, max mem: 30.7 GB 
[11/23 05:57:19 visual_prompt]: Epoch 30 / 100: avg data time: 1.92e-02, avg batch time: 0.9438, average train loss: 0.4282
[11/23 05:58:13 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.3040, average loss: 0.8812
[11/23 05:58:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.82	
[11/23 05:58:13 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0004193203929064353
[11/23 05:59:56 visual_prompt]: 	Training 100/553. train loss: 0.0864,	0.9280 s / batch. (data: 3.95e-03). ETA=9:57:10, max mem: 30.7 GB 
[11/23 06:01:28 visual_prompt]: 	Training 200/553. train loss: 0.2413,	0.9254 s / batch. (data: 1.20e-02). ETA=9:53:58, max mem: 30.7 GB 
[11/23 06:03:02 visual_prompt]: 	Training 300/553. train loss: 0.2745,	0.9160 s / batch. (data: 2.96e-04). ETA=9:46:23, max mem: 30.7 GB 
[11/23 06:04:35 visual_prompt]: 	Training 400/553. train loss: 0.4980,	0.9120 s / batch. (data: 2.94e-04). ETA=9:42:17, max mem: 30.7 GB 
[11/23 06:06:08 visual_prompt]: 	Training 500/553. train loss: 0.0038,	0.9320 s / batch. (data: 3.14e-04). ETA=9:53:31, max mem: 30.7 GB 
[11/23 06:06:57 visual_prompt]: Epoch 31 / 100: avg data time: 2.21e-02, avg batch time: 0.9467, average train loss: 0.3052
[11/23 06:07:51 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.3053, average loss: 1.0975
[11/23 06:07:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 70.24	
[11/23 06:07:51 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.00041314644038104216
[11/23 06:09:31 visual_prompt]: 	Training 100/553. train loss: 0.0115,	0.9406 s / batch. (data: 1.14e-02). ETA=9:56:37, max mem: 30.7 GB 
[11/23 06:11:04 visual_prompt]: 	Training 200/553. train loss: 0.0323,	0.9240 s / batch. (data: 2.73e-04). ETA=9:44:32, max mem: 30.7 GB 
[11/23 06:12:37 visual_prompt]: 	Training 300/553. train loss: 0.1342,	0.9261 s / batch. (data: 7.34e-04). ETA=9:44:20, max mem: 30.7 GB 
[11/23 06:14:10 visual_prompt]: 	Training 400/553. train loss: 0.1263,	0.9200 s / batch. (data: 7.38e-04). ETA=9:38:57, max mem: 30.7 GB 
[11/23 06:15:44 visual_prompt]: 	Training 500/553. train loss: 0.2380,	0.9446 s / batch. (data: 7.37e-04). ETA=9:52:50, max mem: 30.7 GB 
[11/23 06:16:33 visual_prompt]: Epoch 32 / 100: avg data time: 1.89e-02, avg batch time: 0.9432, average train loss: 0.3518
[11/23 06:17:27 visual_prompt]: Inference (val):avg data time: 4.14e-04, avg batch time: 0.3058, average loss: 0.9817
[11/23 06:17:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 67.68	
[11/23 06:17:27 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.00040679408961838426
[11/23 06:19:09 visual_prompt]: 	Training 100/553. train loss: 0.3849,	0.9320 s / batch. (data: 5.40e-03). ETA=9:42:32, max mem: 30.7 GB 
[11/23 06:20:42 visual_prompt]: 	Training 200/553. train loss: 0.3172,	0.9400 s / batch. (data: 7.61e-04). ETA=9:45:59, max mem: 30.7 GB 
[11/23 06:22:15 visual_prompt]: 	Training 300/553. train loss: 0.1229,	0.9400 s / batch. (data: 3.08e-04). ETA=9:44:24, max mem: 30.7 GB 
[11/23 06:23:48 visual_prompt]: 	Training 400/553. train loss: 0.0722,	0.9013 s / batch. (data: 2.95e-04). ETA=9:18:50, max mem: 30.7 GB 
[11/23 06:25:22 visual_prompt]: 	Training 500/553. train loss: 0.1165,	0.9238 s / batch. (data: 2.33e-04). ETA=9:31:18, max mem: 30.7 GB 
[11/23 06:26:11 visual_prompt]: Epoch 33 / 100: avg data time: 2.17e-02, avg batch time: 0.9462, average train loss: 0.3033
[11/23 06:27:05 visual_prompt]: Inference (val):avg data time: 8.31e-05, avg batch time: 0.3049, average loss: 1.2102
[11/23 06:27:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 67.14	
[11/23 06:27:05 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.00040027028682075626
[11/23 06:28:46 visual_prompt]: 	Training 100/553. train loss: 0.1010,	0.9074 s / batch. (data: 2.93e-04). ETA=9:18:51, max mem: 30.7 GB 
[11/23 06:30:19 visual_prompt]: 	Training 200/553. train loss: 0.0024,	0.9440 s / batch. (data: 7.46e-04). ETA=9:39:47, max mem: 30.7 GB 
[11/23 06:31:52 visual_prompt]: 	Training 300/553. train loss: 0.3239,	0.9326 s / batch. (data: 2.36e-02). ETA=9:31:13, max mem: 30.7 GB 
[11/23 06:33:25 visual_prompt]: 	Training 400/553. train loss: 1.2755,	0.9519 s / batch. (data: 1.09e-02). ETA=9:41:26, max mem: 30.7 GB 
[11/23 06:34:58 visual_prompt]: 	Training 500/553. train loss: 0.0951,	0.9675 s / batch. (data: 1.13e-02). ETA=9:49:22, max mem: 30.7 GB 
[11/23 06:35:47 visual_prompt]: Epoch 34 / 100: avg data time: 1.92e-02, avg batch time: 0.9441, average train loss: 0.3346
[11/23 06:36:41 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3032, average loss: 1.1322
[11/23 06:36:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 65.51	
[11/23 06:36:41 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0003935821656707359
[11/23 06:38:23 visual_prompt]: 	Training 100/553. train loss: 0.6133,	0.9225 s / batch. (data: 5.42e-03). ETA=9:19:38, max mem: 30.7 GB 
[11/23 06:39:56 visual_prompt]: 	Training 200/553. train loss: 0.0429,	0.9094 s / batch. (data: 2.92e-04). ETA=9:10:09, max mem: 30.7 GB 
[11/23 06:41:28 visual_prompt]: 	Training 300/553. train loss: 0.2215,	0.9230 s / batch. (data: 3.20e-04). ETA=9:16:51, max mem: 30.7 GB 
[11/23 06:43:01 visual_prompt]: 	Training 400/553. train loss: 0.0040,	0.9120 s / batch. (data: 2.75e-04). ETA=9:08:42, max mem: 30.7 GB 
[11/23 06:44:35 visual_prompt]: 	Training 500/553. train loss: 0.5269,	0.9359 s / batch. (data: 7.96e-03). ETA=9:21:28, max mem: 30.7 GB 
[11/23 06:45:24 visual_prompt]: Epoch 35 / 100: avg data time: 2.15e-02, avg batch time: 0.9444, average train loss: 0.3035
[11/23 06:46:18 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.3063, average loss: 1.3779
[11/23 06:46:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 64.24	
[11/23 06:46:18 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.00038673703953060677
[11/23 06:47:59 visual_prompt]: 	Training 100/553. train loss: 0.6414,	0.9480 s / batch. (data: 7.35e-04). ETA=9:26:22, max mem: 30.7 GB 
[11/23 06:49:32 visual_prompt]: 	Training 200/553. train loss: 0.4226,	0.9320 s / batch. (data: 3.00e-04). ETA=9:15:13, max mem: 30.7 GB 
[11/23 06:51:05 visual_prompt]: 	Training 300/553. train loss: 0.5548,	0.8990 s / batch. (data: 3.42e-04). ETA=8:54:03, max mem: 30.7 GB 
[11/23 06:52:38 visual_prompt]: 	Training 400/553. train loss: 0.0081,	0.9280 s / batch. (data: 7.37e-04). ETA=9:09:45, max mem: 30.7 GB 
[11/23 06:54:11 visual_prompt]: 	Training 500/553. train loss: 0.1269,	0.9476 s / batch. (data: 1.60e-02). ETA=9:19:48, max mem: 30.7 GB 
[11/23 06:55:00 visual_prompt]: Epoch 36 / 100: avg data time: 2.02e-02, avg batch time: 0.9436, average train loss: 0.2608
[11/23 06:55:54 visual_prompt]: Inference (val):avg data time: 1.44e-04, avg batch time: 0.3033, average loss: 1.4907
[11/23 06:55:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 63.27	
[11/23 06:55:54 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.0003797423934453038
[11/23 06:57:38 visual_prompt]: 	Training 100/553. train loss: 0.0755,	0.9360 s / batch. (data: 7.96e-03). ETA=9:10:34, max mem: 30.7 GB 
[11/23 06:59:10 visual_prompt]: 	Training 200/553. train loss: 0.0240,	0.9280 s / batch. (data: 7.81e-04). ETA=9:04:19, max mem: 30.7 GB 
[11/23 07:00:44 visual_prompt]: 	Training 300/553. train loss: 0.0791,	0.9221 s / batch. (data: 8.60e-04). ETA=8:59:19, max mem: 30.7 GB 
[11/23 07:02:17 visual_prompt]: 	Training 400/553. train loss: 0.7457,	0.9446 s / batch. (data: 1.60e-02). ETA=9:10:52, max mem: 30.7 GB 
[11/23 07:03:50 visual_prompt]: 	Training 500/553. train loss: 0.0930,	0.9440 s / batch. (data: 7.00e-04). ETA=9:08:58, max mem: 30.7 GB 
[11/23 07:04:39 visual_prompt]: Epoch 37 / 100: avg data time: 2.46e-02, avg batch time: 0.9484, average train loss: 0.2198
[11/23 07:05:33 visual_prompt]: Inference (val):avg data time: 2.72e-04, avg batch time: 0.3048, average loss: 1.6556
[11/23 07:05:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.44	
[11/23 07:05:33 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.00037260587595762705
[11/23 07:07:15 visual_prompt]: 	Training 100/553. train loss: 0.0324,	0.9280 s / batch. (data: 8.01e-03). ETA=8:57:17, max mem: 30.7 GB 
[11/23 07:08:48 visual_prompt]: 	Training 200/553. train loss: 0.0400,	0.9235 s / batch. (data: 2.72e-04). ETA=8:53:08, max mem: 30.7 GB 
[11/23 07:10:21 visual_prompt]: 	Training 300/553. train loss: 0.9955,	0.9588 s / batch. (data: 2.01e-02). ETA=9:11:57, max mem: 30.7 GB 
[11/23 07:11:54 visual_prompt]: 	Training 400/553. train loss: 0.0703,	0.9520 s / batch. (data: 8.49e-04). ETA=9:06:24, max mem: 30.7 GB 
[11/23 07:13:26 visual_prompt]: 	Training 500/553. train loss: 0.0541,	0.9160 s / batch. (data: 7.27e-04). ETA=8:44:15, max mem: 30.7 GB 
[11/23 07:14:15 visual_prompt]: Epoch 38 / 100: avg data time: 2.15e-02, avg batch time: 0.9453, average train loss: 0.2523
[11/23 07:15:10 visual_prompt]: Inference (val):avg data time: 2.69e-04, avg batch time: 0.3043, average loss: 1.3072
[11/23 07:15:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 66.57	
[11/23 07:15:10 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.000365335290744672
[11/23 07:16:50 visual_prompt]: 	Training 100/553. train loss: 1.1150,	0.9503 s / batch. (data: 5.42e-03). ETA=9:01:25, max mem: 30.7 GB 
[11/23 07:18:23 visual_prompt]: 	Training 200/553. train loss: 0.0164,	0.9160 s / batch. (data: 7.17e-04). ETA=8:40:22, max mem: 30.7 GB 
[11/23 07:19:56 visual_prompt]: 	Training 300/553. train loss: 0.1054,	0.9280 s / batch. (data: 1.12e-03). ETA=8:45:38, max mem: 30.7 GB 
[11/23 07:21:29 visual_prompt]: 	Training 400/553. train loss: 0.1849,	0.9285 s / batch. (data: 8.21e-04). ETA=8:44:24, max mem: 30.7 GB 
[11/23 07:23:02 visual_prompt]: 	Training 500/553. train loss: 0.0173,	0.9280 s / batch. (data: 7.90e-04). ETA=8:42:34, max mem: 30.7 GB 
[11/23 07:23:51 visual_prompt]: Epoch 39 / 100: avg data time: 1.86e-02, avg batch time: 0.9426, average train loss: 0.1867
[11/23 07:24:45 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3069, average loss: 2.1790
[11/23 07:24:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 65.61	
[11/23 07:24:45 visual_prompt]: Stopping early.
[11/23 07:24:45 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 07:24:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 07:24:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/23 07:24:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/23 07:24:45 visual_prompt]: Training with config:
[11/23 07:24:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0001_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/23 07:24:45 visual_prompt]: Loading training data...
[11/23 07:24:45 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 07:24:45 visual_prompt]: Loading validation data...
[11/23 07:24:45 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 07:24:45 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 07:24:47 visual_prompt]: Enable all parameters update during training
[11/23 07:24:47 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/23 07:24:47 visual_prompt]: tuned percent:100.000
[11/23 07:24:47 visual_prompt]: Device used for model: 0
[11/23 07:24:47 visual_prompt]: Setting up Evaluator...
[11/23 07:24:47 visual_prompt]: Setting up Trainer...
[11/23 07:24:47 visual_prompt]: 	Setting up the optimizer...
[11/23 07:24:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 07:26:28 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9236 s / batch. (data: 2.99e-04). ETA=14:09:42, max mem: 30.7 GB 
[11/23 07:28:01 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9130 s / batch. (data: 4.01e-03). ETA=13:58:27, max mem: 30.7 GB 
[11/23 07:29:34 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9308 s / batch. (data: 6.71e-04). ETA=14:13:14, max mem: 30.7 GB 
[11/23 07:31:08 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9248 s / batch. (data: 3.17e-04). ETA=14:06:08, max mem: 30.7 GB 
[11/23 07:32:41 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9257 s / batch. (data: 2.68e-04). ETA=14:05:26, max mem: 30.7 GB 
[11/23 07:33:30 visual_prompt]: Epoch 1 / 100: avg data time: 1.76e-02, avg batch time: 0.9454, average train loss: 7.6130
[11/23 07:34:24 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3064, average loss: 6.9126
[11/23 07:34:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/23 07:34:24 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/23 07:36:06 visual_prompt]: 	Training 100/553. train loss: 1.6284,	0.9183 s / batch. (data: 7.22e-04). ETA=13:56:23, max mem: 30.7 GB 
[11/23 07:37:39 visual_prompt]: 	Training 200/553. train loss: 1.0644,	0.9576 s / batch. (data: 2.06e-02). ETA=14:30:36, max mem: 30.7 GB 
[11/23 07:39:13 visual_prompt]: 	Training 300/553. train loss: 1.0035,	0.9261 s / batch. (data: 2.75e-04). ETA=14:00:24, max mem: 30.7 GB 
[11/23 07:40:46 visual_prompt]: 	Training 400/553. train loss: 0.4651,	0.9634 s / batch. (data: 1.59e-02). ETA=14:32:36, max mem: 30.7 GB 
[11/23 07:42:19 visual_prompt]: 	Training 500/553. train loss: 1.1089,	0.9271 s / batch. (data: 7.19e-04). ETA=13:58:13, max mem: 30.7 GB 
[11/23 07:43:08 visual_prompt]: Epoch 2 / 100: avg data time: 1.89e-02, avg batch time: 0.9472, average train loss: 0.9249
[11/23 07:44:02 visual_prompt]: Inference (val):avg data time: 1.16e-04, avg batch time: 0.3066, average loss: 0.8557
[11/23 07:44:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.28	
[11/23 07:44:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/23 07:45:43 visual_prompt]: 	Training 100/553. train loss: 0.7312,	0.9247 s / batch. (data: 2.58e-04). ETA=13:53:41, max mem: 30.7 GB 
[11/23 07:47:17 visual_prompt]: 	Training 200/553. train loss: 1.9844,	0.9274 s / batch. (data: 1.04e-02). ETA=13:54:35, max mem: 30.7 GB 
[11/23 07:48:50 visual_prompt]: 	Training 300/553. train loss: 1.1168,	0.9271 s / batch. (data: 2.73e-04). ETA=13:52:45, max mem: 30.7 GB 
[11/23 07:50:23 visual_prompt]: 	Training 400/553. train loss: 0.3048,	0.9279 s / batch. (data: 5.79e-03). ETA=13:51:54, max mem: 30.7 GB 
[11/23 07:51:56 visual_prompt]: 	Training 500/553. train loss: 0.8605,	0.9518 s / batch. (data: 1.55e-02). ETA=14:11:45, max mem: 30.7 GB 
[11/23 07:52:45 visual_prompt]: Epoch 3 / 100: avg data time: 1.85e-02, avg batch time: 0.9454, average train loss: 0.7992
[11/23 07:53:39 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.3054, average loss: 0.7165
[11/23 07:53:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 60.73	
[11/23 07:53:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/23 07:55:24 visual_prompt]: 	Training 100/553. train loss: 0.5863,	0.9367 s / batch. (data: 2.47e-02). ETA=13:55:54, max mem: 30.7 GB 
[11/23 07:56:57 visual_prompt]: 	Training 200/553. train loss: 0.9437,	0.9318 s / batch. (data: 8.18e-04). ETA=13:49:56, max mem: 30.7 GB 
[11/23 07:58:31 visual_prompt]: 	Training 300/553. train loss: 0.8335,	0.9063 s / batch. (data: 1.50e-04). ETA=13:25:44, max mem: 30.7 GB 
[11/23 08:00:04 visual_prompt]: 	Training 400/553. train loss: 0.5993,	0.9249 s / batch. (data: 7.27e-04). ETA=13:40:43, max mem: 30.7 GB 
[11/23 08:01:37 visual_prompt]: 	Training 500/553. train loss: 1.3429,	0.9399 s / batch. (data: 6.86e-04). ETA=13:52:25, max mem: 30.7 GB 
[11/23 08:02:26 visual_prompt]: Epoch 4 / 100: avg data time: 2.60e-02, avg batch time: 0.9517, average train loss: 0.7800
[11/23 08:03:20 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3079, average loss: 0.6799
[11/23 08:03:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 64.34	
[11/23 08:03:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/23 08:05:00 visual_prompt]: 	Training 100/553. train loss: 0.6444,	0.9269 s / batch. (data: 3.38e-03). ETA=13:38:32, max mem: 30.7 GB 
[11/23 08:06:33 visual_prompt]: 	Training 200/553. train loss: 0.4983,	0.9145 s / batch. (data: 2.76e-04). ETA=13:26:04, max mem: 30.7 GB 
[11/23 08:08:06 visual_prompt]: 	Training 300/553. train loss: 0.8851,	0.9339 s / batch. (data: 5.81e-03). ETA=13:41:40, max mem: 30.7 GB 
[11/23 08:09:39 visual_prompt]: 	Training 400/553. train loss: 0.9381,	0.9065 s / batch. (data: 2.86e-04). ETA=13:16:04, max mem: 30.7 GB 
[11/23 08:11:13 visual_prompt]: 	Training 500/553. train loss: 0.8456,	0.9087 s / batch. (data: 2.82e-04). ETA=13:16:25, max mem: 30.7 GB 
[11/23 08:12:02 visual_prompt]: Epoch 5 / 100: avg data time: 1.60e-02, avg batch time: 0.9436, average train loss: 0.7314
[11/23 08:12:56 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3064, average loss: 0.6576
[11/23 08:12:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 71.95	
[11/23 08:12:56 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/23 08:14:38 visual_prompt]: 	Training 100/553. train loss: 0.6300,	0.9091 s / batch. (data: 3.15e-04). ETA=13:14:27, max mem: 30.7 GB 
[11/23 08:16:11 visual_prompt]: 	Training 200/553. train loss: 0.5821,	0.9716 s / batch. (data: 7.13e-04). ETA=14:07:28, max mem: 30.7 GB 
[11/23 08:17:44 visual_prompt]: 	Training 300/553. train loss: 0.6768,	0.9378 s / batch. (data: 7.08e-04). ETA=13:36:27, max mem: 30.7 GB 
[11/23 08:19:18 visual_prompt]: 	Training 400/553. train loss: 0.9623,	0.9186 s / batch. (data: 2.64e-04). ETA=13:18:13, max mem: 30.7 GB 
[11/23 08:20:51 visual_prompt]: 	Training 500/553. train loss: 1.3375,	0.9163 s / batch. (data: 1.03e-02). ETA=13:14:39, max mem: 30.7 GB 
[11/23 08:21:40 visual_prompt]: Epoch 6 / 100: avg data time: 2.05e-02, avg batch time: 0.9480, average train loss: 0.7063
[11/23 08:22:35 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3054, average loss: 0.6399
[11/23 08:22:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 69.29	
[11/23 08:22:35 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/23 08:24:19 visual_prompt]: 	Training 100/553. train loss: 0.5237,	0.9276 s / batch. (data: 7.94e-03). ETA=13:22:04, max mem: 30.7 GB 
[11/23 08:25:53 visual_prompt]: 	Training 200/553. train loss: 0.4076,	0.9274 s / batch. (data: 2.26e-04). ETA=13:20:23, max mem: 30.7 GB 
[11/23 08:27:26 visual_prompt]: 	Training 300/553. train loss: 0.6247,	0.9341 s / batch. (data: 1.55e-02). ETA=13:24:38, max mem: 30.7 GB 
[11/23 08:28:59 visual_prompt]: 	Training 400/553. train loss: 0.7132,	0.9209 s / batch. (data: 6.94e-04). ETA=13:11:41, max mem: 30.7 GB 
[11/23 08:30:32 visual_prompt]: 	Training 500/553. train loss: 0.6034,	0.9367 s / batch. (data: 1.55e-02). ETA=13:23:45, max mem: 30.7 GB 
[11/23 08:31:22 visual_prompt]: Epoch 7 / 100: avg data time: 2.58e-02, avg batch time: 0.9521, average train loss: 0.6934
[11/23 08:32:16 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3044, average loss: 0.6270
[11/23 08:32:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 71.71	
[11/23 08:32:16 visual_prompt]: Best epoch 7: best metric: -0.627
[11/23 08:32:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/23 08:33:59 visual_prompt]: 	Training 100/553. train loss: 0.4947,	0.9330 s / batch. (data: 2.17e-02). ETA=13:18:07, max mem: 30.7 GB 
[11/23 08:35:32 visual_prompt]: 	Training 200/553. train loss: 0.6653,	0.9553 s / batch. (data: 1.04e-02). ETA=13:35:36, max mem: 30.7 GB 
[11/23 08:37:05 visual_prompt]: 	Training 300/553. train loss: 0.5893,	0.9110 s / batch. (data: 5.34e-03). ETA=12:56:16, max mem: 30.7 GB 
[11/23 08:38:39 visual_prompt]: 	Training 400/553. train loss: 0.5932,	0.9581 s / batch. (data: 5.77e-03). ETA=13:34:49, max mem: 30.7 GB 
[11/23 08:40:12 visual_prompt]: 	Training 500/553. train loss: 0.4721,	0.9248 s / batch. (data: 1.46e-02). ETA=13:05:01, max mem: 30.7 GB 
[11/23 08:41:01 visual_prompt]: Epoch 8 / 100: avg data time: 2.35e-02, avg batch time: 0.9503, average train loss: 0.6783
[11/23 08:41:55 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3044, average loss: 0.5859
[11/23 08:41:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.11	rocauc: 75.70	
[11/23 08:41:55 visual_prompt]: Best epoch 8: best metric: -0.586
[11/23 08:41:55 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/23 08:43:39 visual_prompt]: 	Training 100/553. train loss: 0.7885,	0.9450 s / batch. (data: 1.05e-02). ETA=13:19:45, max mem: 30.7 GB 
[11/23 08:45:13 visual_prompt]: 	Training 200/553. train loss: 0.9068,	0.9248 s / batch. (data: 5.77e-03). ETA=13:01:02, max mem: 30.7 GB 
[11/23 08:46:46 visual_prompt]: 	Training 300/553. train loss: 0.4493,	0.9179 s / batch. (data: 2.36e-04). ETA=12:53:43, max mem: 30.7 GB 
[11/23 08:48:19 visual_prompt]: 	Training 400/553. train loss: 0.5149,	0.9515 s / batch. (data: 2.74e-04). ETA=13:20:29, max mem: 30.7 GB 
[11/23 08:49:52 visual_prompt]: 	Training 500/553. train loss: 1.0193,	0.9104 s / batch. (data: 2.68e-04). ETA=12:44:20, max mem: 30.7 GB 
[11/23 08:50:41 visual_prompt]: Epoch 9 / 100: avg data time: 2.42e-02, avg batch time: 0.9510, average train loss: 0.6698
[11/23 08:51:35 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.3040, average loss: 0.6290
[11/23 08:51:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 72.78	
[11/23 08:51:35 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/23 08:53:17 visual_prompt]: 	Training 100/553. train loss: 0.7059,	0.9328 s / batch. (data: 1.60e-02). ETA=13:00:48, max mem: 30.7 GB 
[11/23 08:54:50 visual_prompt]: 	Training 200/553. train loss: 0.4262,	0.9237 s / batch. (data: 2.63e-04). ETA=12:51:36, max mem: 30.7 GB 
[11/23 08:56:24 visual_prompt]: 	Training 300/553. train loss: 0.9075,	0.9344 s / batch. (data: 7.06e-04). ETA=12:59:03, max mem: 30.7 GB 
[11/23 08:57:57 visual_prompt]: 	Training 400/553. train loss: 0.6054,	0.9226 s / batch. (data: 8.95e-03). ETA=12:47:39, max mem: 30.7 GB 
[11/23 08:59:30 visual_prompt]: 	Training 500/553. train loss: 0.6340,	0.9103 s / batch. (data: 4.02e-03). ETA=12:35:53, max mem: 30.7 GB 
[11/23 09:00:19 visual_prompt]: Epoch 10 / 100: avg data time: 1.98e-02, avg batch time: 0.9470, average train loss: 0.6360
[11/23 09:01:13 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3061, average loss: 0.7319
[11/23 09:01:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 72.33	
[11/23 09:01:13 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/23 09:02:55 visual_prompt]: 	Training 100/553. train loss: 0.7425,	0.9470 s / batch. (data: 2.54e-04). ETA=13:03:55, max mem: 30.7 GB 
[11/23 09:04:29 visual_prompt]: 	Training 200/553. train loss: 0.9348,	0.9487 s / batch. (data: 2.69e-04). ETA=13:03:47, max mem: 30.7 GB 
[11/23 09:06:02 visual_prompt]: 	Training 300/553. train loss: 0.5472,	0.9149 s / batch. (data: 2.40e-04). ETA=12:34:21, max mem: 30.7 GB 
[11/23 09:07:35 visual_prompt]: 	Training 400/553. train loss: 0.3845,	0.9234 s / batch. (data: 5.79e-03). ETA=12:39:50, max mem: 30.7 GB 
[11/23 09:09:09 visual_prompt]: 	Training 500/553. train loss: 0.4289,	0.9356 s / batch. (data: 1.27e-02). ETA=12:48:18, max mem: 30.7 GB 
[11/23 09:09:58 visual_prompt]: Epoch 11 / 100: avg data time: 2.11e-02, avg batch time: 0.9483, average train loss: 0.6129
[11/23 09:10:52 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3065, average loss: 0.6211
[11/23 09:10:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.51	
[11/23 09:10:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/23 09:12:31 visual_prompt]: 	Training 100/553. train loss: 0.6543,	0.9402 s / batch. (data: 2.05e-02). ETA=12:49:38, max mem: 30.7 GB 
[11/23 09:14:04 visual_prompt]: 	Training 200/553. train loss: 0.6374,	0.9647 s / batch. (data: 7.36e-04). ETA=13:08:05, max mem: 30.7 GB 
[11/23 09:15:38 visual_prompt]: 	Training 300/553. train loss: 0.4774,	0.9136 s / batch. (data: 2.41e-04). ETA=12:24:49, max mem: 30.7 GB 
[11/23 09:17:11 visual_prompt]: 	Training 400/553. train loss: 0.3734,	0.9466 s / batch. (data: 1.55e-02). ETA=12:50:10, max mem: 30.7 GB 
[11/23 09:18:44 visual_prompt]: 	Training 500/553. train loss: 0.3542,	0.9233 s / batch. (data: 1.04e-02). ETA=12:29:39, max mem: 30.7 GB 
[11/23 09:19:33 visual_prompt]: Epoch 12 / 100: avg data time: 1.57e-02, avg batch time: 0.9422, average train loss: 0.6198
[11/23 09:20:27 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.3072, average loss: 1.0570
[11/23 09:20:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 65.06	
[11/23 09:20:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/23 09:22:08 visual_prompt]: 	Training 100/553. train loss: 0.5635,	0.9147 s / batch. (data: 8.04e-03). ETA=12:20:20, max mem: 30.7 GB 
[11/23 09:23:41 visual_prompt]: 	Training 200/553. train loss: 0.5468,	0.9241 s / batch. (data: 5.35e-03). ETA=12:26:24, max mem: 30.7 GB 
[11/23 09:25:15 visual_prompt]: 	Training 300/553. train loss: 0.6606,	0.9430 s / batch. (data: 6.81e-04). ETA=12:40:06, max mem: 30.7 GB 
[11/23 09:26:48 visual_prompt]: 	Training 400/553. train loss: 0.5208,	0.9169 s / batch. (data: 7.25e-04). ETA=12:17:35, max mem: 30.7 GB 
[11/23 09:28:21 visual_prompt]: 	Training 500/553. train loss: 0.2899,	0.9078 s / batch. (data: 2.30e-04). ETA=12:08:41, max mem: 30.7 GB 
[11/23 09:29:11 visual_prompt]: Epoch 13 / 100: avg data time: 1.93e-02, avg batch time: 0.9463, average train loss: 0.5832
[11/23 09:30:05 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3028, average loss: 0.6800
[11/23 09:30:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 69.91	
[11/23 09:30:05 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/23 09:31:46 visual_prompt]: 	Training 100/553. train loss: 0.3294,	0.9424 s / batch. (data: 1.04e-02). ETA=12:34:06, max mem: 30.7 GB 
[11/23 09:33:20 visual_prompt]: 	Training 200/553. train loss: 0.1084,	0.9382 s / batch. (data: 1.04e-02). ETA=12:29:08, max mem: 30.7 GB 
[11/23 09:34:53 visual_prompt]: 	Training 300/553. train loss: 0.8012,	0.9404 s / batch. (data: 2.76e-04). ETA=12:29:21, max mem: 30.7 GB 
[11/23 09:36:26 visual_prompt]: 	Training 400/553. train loss: 1.0243,	0.9191 s / batch. (data: 2.59e-04). ETA=12:10:52, max mem: 30.7 GB 
[11/23 09:37:59 visual_prompt]: 	Training 500/553. train loss: 0.7782,	0.9290 s / batch. (data: 6.95e-04). ETA=12:17:08, max mem: 30.7 GB 
[11/23 09:38:48 visual_prompt]: Epoch 14 / 100: avg data time: 1.96e-02, avg batch time: 0.9468, average train loss: 0.5792
[11/23 09:39:42 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3048, average loss: 0.7522
[11/23 09:39:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 69.39	
[11/23 09:39:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/23 09:41:26 visual_prompt]: 	Training 100/553. train loss: 0.2051,	0.9424 s / batch. (data: 5.35e-03). ETA=12:25:25, max mem: 30.7 GB 
[11/23 09:43:00 visual_prompt]: 	Training 200/553. train loss: 0.2132,	0.9410 s / batch. (data: 2.43e-04). ETA=12:22:45, max mem: 30.7 GB 
[11/23 09:44:33 visual_prompt]: 	Training 300/553. train loss: 0.8610,	0.9356 s / batch. (data: 8.00e-03). ETA=12:16:54, max mem: 30.7 GB 
[11/23 09:46:06 visual_prompt]: 	Training 400/553. train loss: 0.3478,	0.9205 s / batch. (data: 7.15e-04). ETA=12:03:29, max mem: 30.7 GB 
[11/23 09:47:39 visual_prompt]: 	Training 500/553. train loss: 0.8386,	0.9063 s / batch. (data: 2.76e-04). ETA=11:50:47, max mem: 30.7 GB 
[11/23 09:48:28 visual_prompt]: Epoch 15 / 100: avg data time: 2.40e-02, avg batch time: 0.9506, average train loss: 0.5180
[11/23 09:49:22 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3055, average loss: 0.8761
[11/23 09:49:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 61.73	
[11/23 09:49:22 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/23 09:51:03 visual_prompt]: 	Training 100/553. train loss: 0.5816,	0.9220 s / batch. (data: 2.65e-04). ETA=12:00:46, max mem: 30.7 GB 
[11/23 09:52:36 visual_prompt]: 	Training 200/553. train loss: 0.4104,	0.9361 s / batch. (data: 7.52e-04). ETA=12:10:14, max mem: 30.7 GB 
[11/23 09:54:09 visual_prompt]: 	Training 300/553. train loss: 0.5728,	0.9255 s / batch. (data: 7.05e-04). ETA=12:00:23, max mem: 30.7 GB 
[11/23 09:55:43 visual_prompt]: 	Training 400/553. train loss: 0.2201,	0.9062 s / batch. (data: 2.38e-04). ETA=11:43:51, max mem: 30.7 GB 
[11/23 09:57:16 visual_prompt]: 	Training 500/553. train loss: 0.3596,	0.9221 s / batch. (data: 2.32e-04). ETA=11:54:43, max mem: 30.7 GB 
[11/23 09:58:05 visual_prompt]: Epoch 16 / 100: avg data time: 1.83e-02, avg batch time: 0.9454, average train loss: 0.5249
[11/23 09:58:59 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3064, average loss: 0.8056
[11/23 09:58:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 64.70	
[11/23 09:58:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/23 10:00:40 visual_prompt]: 	Training 100/553. train loss: 0.2038,	0.9473 s / batch. (data: 1.09e-02). ETA=12:11:49, max mem: 30.7 GB 
[11/23 10:02:17 visual_prompt]: 	Training 200/553. train loss: 1.2606,	0.9452 s / batch. (data: 1.06e-02). ETA=12:08:36, max mem: 30.7 GB 
[11/23 10:03:51 visual_prompt]: 	Training 300/553. train loss: 0.8624,	0.9128 s / batch. (data: 2.99e-04). ETA=11:42:05, max mem: 30.7 GB 
[11/23 10:05:28 visual_prompt]: 	Training 400/553. train loss: 0.0700,	0.9325 s / batch. (data: 2.96e-04). ETA=11:55:42, max mem: 30.7 GB 
[11/23 10:07:05 visual_prompt]: 	Training 500/553. train loss: 0.5407,	0.9320 s / batch. (data: 2.35e-04). ETA=11:53:47, max mem: 30.7 GB 
[11/23 10:07:55 visual_prompt]: Epoch 17 / 100: avg data time: 3.74e-02, avg batch time: 0.9682, average train loss: 0.4791
[11/23 10:08:53 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3059, average loss: 0.7342
[11/23 10:08:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 67.80	
[11/23 10:08:54 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/23 10:10:37 visual_prompt]: 	Training 100/553. train loss: 0.4862,	0.9495 s / batch. (data: 2.55e-02). ETA=12:04:45, max mem: 30.7 GB 
[11/23 10:12:10 visual_prompt]: 	Training 200/553. train loss: 1.1694,	0.9103 s / batch. (data: 2.46e-04). ETA=11:33:17, max mem: 30.7 GB 
[11/23 10:13:43 visual_prompt]: 	Training 300/553. train loss: 0.2437,	0.9333 s / batch. (data: 5.74e-03). ETA=11:49:19, max mem: 30.7 GB 
[11/23 10:15:17 visual_prompt]: 	Training 400/553. train loss: 0.4732,	0.9434 s / batch. (data: 5.41e-03). ETA=11:55:21, max mem: 30.7 GB 
[11/23 10:16:50 visual_prompt]: 	Training 500/553. train loss: 0.7083,	0.9233 s / batch. (data: 5.87e-03). ETA=11:38:34, max mem: 30.7 GB 
[11/23 10:17:39 visual_prompt]: Epoch 18 / 100: avg data time: 2.42e-02, avg batch time: 0.9499, average train loss: 0.4216
[11/23 10:18:33 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3071, average loss: 0.8512
[11/23 10:18:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.08	
[11/23 10:18:33 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/23 10:20:15 visual_prompt]: 	Training 100/553. train loss: 0.0089,	0.9577 s / batch. (data: 5.83e-03). ETA=12:02:11, max mem: 30.7 GB 
[11/23 10:21:48 visual_prompt]: 	Training 200/553. train loss: 0.3638,	0.9256 s / batch. (data: 2.60e-04). ETA=11:36:25, max mem: 30.7 GB 
[11/23 10:23:21 visual_prompt]: 	Training 300/553. train loss: 0.1305,	0.9304 s / batch. (data: 2.52e-04). ETA=11:38:28, max mem: 30.7 GB 
[11/23 10:24:55 visual_prompt]: 	Training 400/553. train loss: 0.0633,	0.9256 s / batch. (data: 5.35e-03). ETA=11:33:22, max mem: 30.7 GB 
[11/23 10:26:29 visual_prompt]: 	Training 500/553. train loss: 0.0308,	0.9443 s / batch. (data: 1.55e-02). ETA=11:45:48, max mem: 30.7 GB 
[11/23 10:27:18 visual_prompt]: Epoch 19 / 100: avg data time: 1.99e-02, avg batch time: 0.9490, average train loss: 0.3942
[11/23 10:28:15 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3033, average loss: 0.9334
[11/23 10:28:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.90	
[11/23 10:28:15 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/23 10:29:59 visual_prompt]: 	Training 100/553. train loss: 0.2544,	0.9093 s / batch. (data: 3.09e-04). ETA=11:17:17, max mem: 30.7 GB 
[11/23 10:31:32 visual_prompt]: 	Training 200/553. train loss: 0.5614,	0.9356 s / batch. (data: 2.14e-02). ETA=11:35:19, max mem: 30.7 GB 
[11/23 10:33:05 visual_prompt]: 	Training 300/553. train loss: 0.1988,	0.9312 s / batch. (data: 9.11e-03). ETA=11:30:32, max mem: 30.7 GB 
[11/23 10:34:38 visual_prompt]: 	Training 400/553. train loss: 1.2182,	0.9063 s / batch. (data: 2.57e-04). ETA=11:10:32, max mem: 30.7 GB 
[11/23 10:36:12 visual_prompt]: 	Training 500/553. train loss: 0.1389,	0.9142 s / batch. (data: 5.34e-03). ETA=11:14:53, max mem: 30.7 GB 
[11/23 10:37:01 visual_prompt]: Epoch 20 / 100: avg data time: 2.46e-02, avg batch time: 0.9514, average train loss: 0.3420
[11/23 10:37:56 visual_prompt]: Inference (val):avg data time: 1.98e-04, avg batch time: 0.3061, average loss: 0.8443
[11/23 10:37:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.56	
[11/23 10:37:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/23 10:39:39 visual_prompt]: 	Training 100/553. train loss: 0.2326,	0.9079 s / batch. (data: 5.35e-03). ETA=11:07:56, max mem: 30.7 GB 
[11/23 10:41:12 visual_prompt]: 	Training 200/553. train loss: 0.4229,	0.9209 s / batch. (data: 6.63e-04). ETA=11:15:57, max mem: 30.7 GB 
[11/23 10:42:45 visual_prompt]: 	Training 300/553. train loss: 0.0612,	0.9075 s / batch. (data: 2.56e-04). ETA=11:04:34, max mem: 30.7 GB 
[11/23 10:44:19 visual_prompt]: 	Training 400/553. train loss: 0.4128,	0.9608 s / batch. (data: 7.17e-04). ETA=11:42:00, max mem: 30.7 GB 
[11/23 10:45:52 visual_prompt]: 	Training 500/553. train loss: 0.0088,	0.9173 s / batch. (data: 2.68e-04). ETA=11:08:43, max mem: 30.7 GB 
[11/23 10:46:41 visual_prompt]: Epoch 21 / 100: avg data time: 2.31e-02, avg batch time: 0.9498, average train loss: 0.3017
[11/23 10:47:36 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3059, average loss: 0.9092
[11/23 10:47:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 64.81	
[11/23 10:47:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/23 10:49:17 visual_prompt]: 	Training 100/553. train loss: 0.1263,	0.9193 s / batch. (data: 5.36e-03). ETA=11:07:47, max mem: 30.7 GB 
[11/23 10:50:51 visual_prompt]: 	Training 200/553. train loss: 0.0555,	0.9101 s / batch. (data: 5.38e-03). ETA=10:59:39, max mem: 30.7 GB 
[11/23 10:52:24 visual_prompt]: 	Training 300/553. train loss: 0.0124,	0.9149 s / batch. (data: 2.53e-04). ETA=11:01:32, max mem: 30.7 GB 
[11/23 10:53:57 visual_prompt]: 	Training 400/553. train loss: 0.1889,	0.9351 s / batch. (data: 5.43e-03). ETA=11:14:38, max mem: 30.7 GB 
[11/23 10:55:31 visual_prompt]: 	Training 500/553. train loss: 0.3948,	0.9199 s / batch. (data: 1.39e-02). ETA=11:02:06, max mem: 30.7 GB 
[11/23 10:56:20 visual_prompt]: Epoch 22 / 100: avg data time: 1.92e-02, avg batch time: 0.9474, average train loss: 0.2710
[11/23 10:57:15 visual_prompt]: Inference (val):avg data time: 2.63e-04, avg batch time: 0.3068, average loss: 1.0281
[11/23 10:57:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.67	
[11/23 10:57:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/23 10:58:57 visual_prompt]: 	Training 100/553. train loss: 0.2381,	0.9403 s / batch. (data: 5.79e-03). ETA=11:14:25, max mem: 30.7 GB 
[11/23 11:00:31 visual_prompt]: 	Training 200/553. train loss: 0.3953,	0.9449 s / batch. (data: 2.52e-04). ETA=11:16:08, max mem: 30.7 GB 
[11/23 11:02:04 visual_prompt]: 	Training 300/553. train loss: 0.0295,	0.9436 s / batch. (data: 6.64e-04). ETA=11:13:37, max mem: 30.7 GB 
[11/23 11:03:38 visual_prompt]: 	Training 400/553. train loss: 0.0014,	0.9110 s / batch. (data: 5.36e-03). ETA=10:48:50, max mem: 30.7 GB 
[11/23 11:05:11 visual_prompt]: 	Training 500/553. train loss: 0.0373,	0.9183 s / batch. (data: 2.79e-04). ETA=10:52:32, max mem: 30.7 GB 
[11/23 11:06:01 visual_prompt]: Epoch 23 / 100: avg data time: 2.22e-02, avg batch time: 0.9509, average train loss: 0.2421
[11/23 11:06:55 visual_prompt]: Inference (val):avg data time: 1.25e-04, avg batch time: 0.3053, average loss: 1.0808
[11/23 11:06:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 66.88	
[11/23 11:06:55 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/23 11:08:38 visual_prompt]: 	Training 100/553. train loss: 0.0566,	0.9214 s / batch. (data: 2.69e-04). ETA=10:52:21, max mem: 30.7 GB 
[11/23 11:10:11 visual_prompt]: 	Training 200/553. train loss: 0.0186,	0.9267 s / batch. (data: 7.97e-03). ETA=10:54:32, max mem: 30.7 GB 
[11/23 11:11:45 visual_prompt]: 	Training 300/553. train loss: 0.0013,	0.9053 s / batch. (data: 2.67e-04). ETA=10:37:57, max mem: 30.7 GB 
[11/23 11:13:18 visual_prompt]: 	Training 400/553. train loss: 0.0499,	0.9298 s / batch. (data: 7.64e-04). ETA=10:53:37, max mem: 30.7 GB 
[11/23 11:14:52 visual_prompt]: 	Training 500/553. train loss: 0.0430,	0.9302 s / batch. (data: 7.64e-04). ETA=10:52:24, max mem: 30.7 GB 
[11/23 11:15:41 visual_prompt]: Epoch 24 / 100: avg data time: 2.27e-02, avg batch time: 0.9496, average train loss: 0.2501
[11/23 11:16:35 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.3047, average loss: 1.2360
[11/23 11:16:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 67.34	
[11/23 11:16:35 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/23 11:18:15 visual_prompt]: 	Training 100/553. train loss: 0.5591,	0.9106 s / batch. (data: 2.58e-04). ETA=10:36:17, max mem: 30.7 GB 
[11/23 11:19:49 visual_prompt]: 	Training 200/553. train loss: 0.0008,	0.9099 s / batch. (data: 2.41e-04). ETA=10:34:18, max mem: 30.7 GB 
[11/23 11:21:22 visual_prompt]: 	Training 300/553. train loss: 0.6683,	0.9298 s / batch. (data: 2.66e-04). ETA=10:46:39, max mem: 30.7 GB 
[11/23 11:22:56 visual_prompt]: 	Training 400/553. train loss: 0.0158,	0.9377 s / batch. (data: 2.06e-02). ETA=10:50:34, max mem: 30.7 GB 
[11/23 11:24:29 visual_prompt]: 	Training 500/553. train loss: 0.3285,	0.9627 s / batch. (data: 5.89e-03). ETA=11:06:18, max mem: 30.7 GB 
[11/23 11:25:18 visual_prompt]: Epoch 25 / 100: avg data time: 1.78e-02, avg batch time: 0.9461, average train loss: 0.2111
[11/23 11:26:13 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.3066, average loss: 1.0337
[11/23 11:26:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 68.49	
[11/23 11:26:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/23 11:27:55 visual_prompt]: 	Training 100/553. train loss: 0.4153,	0.9279 s / batch. (data: 1.10e-02). ETA=10:39:51, max mem: 30.7 GB 
[11/23 11:29:29 visual_prompt]: 	Training 200/553. train loss: 0.0357,	0.9543 s / batch. (data: 7.30e-04). ETA=10:56:29, max mem: 30.7 GB 
[11/23 11:31:02 visual_prompt]: 	Training 300/553. train loss: 0.0437,	0.9153 s / batch. (data: 2.67e-04). ETA=10:28:09, max mem: 30.7 GB 
[11/23 11:32:36 visual_prompt]: 	Training 400/553. train loss: 0.0120,	0.9300 s / batch. (data: 7.37e-04). ETA=10:36:40, max mem: 30.7 GB 
[11/23 11:34:09 visual_prompt]: 	Training 500/553. train loss: 0.2609,	0.9223 s / batch. (data: 2.52e-04). ETA=10:29:50, max mem: 30.7 GB 
[11/23 11:34:59 visual_prompt]: Epoch 26 / 100: avg data time: 2.14e-02, avg batch time: 0.9496, average train loss: 0.2341
[11/23 11:35:53 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3044, average loss: 1.3733
[11/23 11:35:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 64.65	
[11/23 11:35:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/23 11:37:35 visual_prompt]: 	Training 100/553. train loss: 0.3173,	0.9560 s / batch. (data: 7.27e-04). ETA=10:50:26, max mem: 30.7 GB 
[11/23 11:39:08 visual_prompt]: 	Training 200/553. train loss: 0.3137,	0.9290 s / batch. (data: 5.34e-03). ETA=10:30:31, max mem: 30.7 GB 
[11/23 11:40:41 visual_prompt]: 	Training 300/553. train loss: 0.4550,	0.9376 s / batch. (data: 2.53e-04). ETA=10:34:48, max mem: 30.7 GB 
[11/23 11:42:15 visual_prompt]: 	Training 400/553. train loss: 0.3299,	0.9520 s / batch. (data: 2.51e-02). ETA=10:42:57, max mem: 30.7 GB 
[11/23 11:43:48 visual_prompt]: 	Training 500/553. train loss: 0.1222,	0.9319 s / batch. (data: 7.27e-04). ETA=10:27:50, max mem: 30.7 GB 
[11/23 11:44:38 visual_prompt]: Epoch 27 / 100: avg data time: 1.98e-02, avg batch time: 0.9481, average train loss: 0.1568
[11/23 11:45:33 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3070, average loss: 1.3553
[11/23 11:45:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.31	
[11/23 11:45:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/23 11:47:14 visual_prompt]: 	Training 100/553. train loss: 0.0193,	0.9548 s / batch. (data: 1.09e-02). ETA=10:40:47, max mem: 30.7 GB 
[11/23 11:48:48 visual_prompt]: 	Training 200/553. train loss: 0.0049,	0.9302 s / batch. (data: 7.48e-04). ETA=10:22:46, max mem: 30.7 GB 
[11/23 11:50:21 visual_prompt]: 	Training 300/553. train loss: 0.1237,	0.9265 s / batch. (data: 5.88e-03). ETA=10:18:44, max mem: 30.7 GB 
[11/23 11:51:55 visual_prompt]: 	Training 400/553. train loss: 0.4197,	0.9199 s / batch. (data: 2.68e-04). ETA=10:12:49, max mem: 30.7 GB 
[11/23 11:53:28 visual_prompt]: 	Training 500/553. train loss: 0.3207,	0.9132 s / batch. (data: 2.58e-04). ETA=10:06:50, max mem: 30.7 GB 
[11/23 11:54:18 visual_prompt]: Epoch 28 / 100: avg data time: 2.03e-02, avg batch time: 0.9485, average train loss: 0.1375
[11/23 11:55:12 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.3058, average loss: 1.2135
[11/23 11:55:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 65.89	
[11/23 11:55:12 visual_prompt]: Training 29 / 100 epoch, with learning rate 8.622126023955446e-05
[11/23 11:56:54 visual_prompt]: 	Training 100/553. train loss: 0.0158,	0.9602 s / batch. (data: 1.63e-02). ETA=10:35:34, max mem: 30.7 GB 
[11/23 11:58:28 visual_prompt]: 	Training 200/553. train loss: 0.0265,	0.9502 s / batch. (data: 1.09e-02). ETA=10:27:23, max mem: 30.7 GB 
[11/23 12:00:02 visual_prompt]: 	Training 300/553. train loss: 0.0216,	0.9347 s / batch. (data: 2.70e-04). ETA=10:15:36, max mem: 30.7 GB 
[11/23 12:01:35 visual_prompt]: 	Training 400/553. train loss: 0.2313,	0.9166 s / batch. (data: 4.12e-04). ETA=10:02:09, max mem: 30.7 GB 
[11/23 12:03:08 visual_prompt]: 	Training 500/553. train loss: 0.0041,	0.9429 s / batch. (data: 5.83e-03). ETA=10:17:49, max mem: 30.7 GB 
[11/23 12:03:58 visual_prompt]: Epoch 29 / 100: avg data time: 2.10e-02, avg batch time: 0.9499, average train loss: 0.1598
[11/23 12:04:55 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3052, average loss: 1.6090
[11/23 12:04:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 66.39	
[11/23 12:04:55 visual_prompt]: Stopping early.
[11/23 12:04:55 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 12:04:55 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 12:04:55 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/23 12:04:55 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/23 12:04:55 visual_prompt]: Training with config:
[11/23 12:04:55 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0001_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/23 12:04:55 visual_prompt]: Loading training data...
[11/23 12:04:55 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 12:04:55 visual_prompt]: Loading validation data...
[11/23 12:04:55 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 12:04:55 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 12:04:59 visual_prompt]: Enable all parameters update during training
[11/23 12:04:59 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/23 12:04:59 visual_prompt]: tuned percent:100.000
[11/23 12:04:59 visual_prompt]: Device used for model: 0
[11/23 12:04:59 visual_prompt]: Setting up Evaluator...
[11/23 12:04:59 visual_prompt]: Setting up Trainer...
[11/23 12:04:59 visual_prompt]: 	Setting up the optimizer...
[11/23 12:04:59 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 12:06:42 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9354 s / batch. (data: 4.39e-03). ETA=14:20:35, max mem: 30.7 GB 
[11/23 12:08:19 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9403 s / batch. (data: 8.29e-03). ETA=14:23:32, max mem: 30.7 GB 
[11/23 12:09:55 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9246 s / batch. (data: 2.61e-04). ETA=14:07:32, max mem: 30.7 GB 
[11/23 12:11:31 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9357 s / batch. (data: 5.75e-03). ETA=14:16:09, max mem: 30.7 GB 
[11/23 12:13:09 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9467 s / batch. (data: 2.60e-04). ETA=14:24:38, max mem: 30.7 GB 
[11/23 12:13:59 visual_prompt]: Epoch 1 / 100: avg data time: 4.47e-02, avg batch time: 0.9755, average train loss: 7.6130
[11/23 12:14:58 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3074, average loss: 6.9126
[11/23 12:14:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/23 12:14:58 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/23 12:16:45 visual_prompt]: 	Training 100/553. train loss: 1.6301,	0.9217 s / batch. (data: 2.59e-04). ETA=13:59:28, max mem: 30.7 GB 
[11/23 12:18:19 visual_prompt]: 	Training 200/553. train loss: 1.0704,	0.9372 s / batch. (data: 3.03e-04). ETA=14:12:03, max mem: 30.7 GB 
[11/23 12:19:53 visual_prompt]: 	Training 300/553. train loss: 1.0092,	0.9322 s / batch. (data: 1.04e-02). ETA=14:05:57, max mem: 30.7 GB 
[11/23 12:21:31 visual_prompt]: 	Training 400/553. train loss: 0.4551,	0.9484 s / batch. (data: 2.61e-04). ETA=14:19:04, max mem: 30.7 GB 
[11/23 12:23:10 visual_prompt]: 	Training 500/553. train loss: 1.1040,	0.9438 s / batch. (data: 5.43e-03). ETA=14:13:20, max mem: 30.7 GB 
[11/23 12:24:00 visual_prompt]: Epoch 2 / 100: avg data time: 4.74e-02, avg batch time: 0.9798, average train loss: 0.9250
[11/23 12:25:00 visual_prompt]: Inference (val):avg data time: 2.39e-04, avg batch time: 0.3057, average loss: 0.8518
[11/23 12:25:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 56.43	
[11/23 12:25:00 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/23 12:26:49 visual_prompt]: 	Training 100/553. train loss: 0.7185,	0.9575 s / batch. (data: 1.51e-02). ETA=14:23:12, max mem: 30.7 GB 
[11/23 12:28:27 visual_prompt]: 	Training 200/553. train loss: 1.9264,	0.9079 s / batch. (data: 2.50e-04). ETA=13:37:03, max mem: 30.7 GB 
[11/23 12:30:02 visual_prompt]: 	Training 300/553. train loss: 1.1247,	0.9413 s / batch. (data: 6.86e-04). ETA=14:05:30, max mem: 30.7 GB 
[11/23 12:31:38 visual_prompt]: 	Training 400/553. train loss: 0.3069,	0.9155 s / batch. (data: 3.94e-03). ETA=13:40:50, max mem: 30.7 GB 
[11/23 12:33:12 visual_prompt]: 	Training 500/553. train loss: 0.8494,	0.9616 s / batch. (data: 2.56e-02). ETA=14:20:33, max mem: 30.7 GB 
[11/23 12:34:01 visual_prompt]: Epoch 3 / 100: avg data time: 4.62e-02, avg batch time: 0.9772, average train loss: 0.7978
[11/23 12:34:57 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3056, average loss: 0.7069
[11/23 12:34:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.92	
[11/23 12:34:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/23 12:36:43 visual_prompt]: 	Training 100/553. train loss: 0.5763,	0.9185 s / batch. (data: 5.37e-03). ETA=13:39:35, max mem: 30.7 GB 
[11/23 12:38:17 visual_prompt]: 	Training 200/553. train loss: 0.9535,	0.9526 s / batch. (data: 3.01e-04). ETA=14:08:25, max mem: 30.7 GB 
[11/23 12:39:52 visual_prompt]: 	Training 300/553. train loss: 0.7968,	0.9579 s / batch. (data: 5.38e-03). ETA=14:11:35, max mem: 30.7 GB 
[11/23 12:41:26 visual_prompt]: 	Training 400/553. train loss: 0.6167,	0.9400 s / batch. (data: 2.96e-04). ETA=13:54:06, max mem: 30.7 GB 
[11/23 12:42:59 visual_prompt]: 	Training 500/553. train loss: 1.3182,	0.9440 s / batch. (data: 7.27e-04). ETA=13:56:06, max mem: 30.7 GB 
[11/23 12:43:49 visual_prompt]: Epoch 4 / 100: avg data time: 2.96e-02, avg batch time: 0.9605, average train loss: 0.7796
[11/23 12:44:46 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3046, average loss: 0.6794
[11/23 12:44:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 63.87	
[11/23 12:44:46 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/23 12:46:28 visual_prompt]: 	Training 100/553. train loss: 0.6098,	0.9394 s / batch. (data: 1.04e-02). ETA=13:49:36, max mem: 30.7 GB 
[11/23 12:48:03 visual_prompt]: 	Training 200/553. train loss: 0.4624,	0.9581 s / batch. (data: 2.06e-02). ETA=14:04:32, max mem: 30.7 GB 
[11/23 12:49:37 visual_prompt]: 	Training 300/553. train loss: 0.8468,	0.9355 s / batch. (data: 2.34e-04). ETA=13:43:03, max mem: 30.7 GB 
[11/23 12:51:11 visual_prompt]: 	Training 400/553. train loss: 0.9697,	0.9311 s / batch. (data: 1.55e-02). ETA=13:37:35, max mem: 30.7 GB 
[11/23 12:52:46 visual_prompt]: 	Training 500/553. train loss: 0.8711,	0.9268 s / batch. (data: 6.76e-03). ETA=13:32:17, max mem: 30.7 GB 
[11/23 12:53:35 visual_prompt]: Epoch 5 / 100: avg data time: 2.60e-02, avg batch time: 0.9567, average train loss: 0.7325
[11/23 12:54:32 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3045, average loss: 0.6528
[11/23 12:54:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 71.00	
[11/23 12:54:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/23 12:56:16 visual_prompt]: 	Training 100/553. train loss: 0.5231,	0.9428 s / batch. (data: 1.30e-02). ETA=13:43:56, max mem: 30.7 GB 
[11/23 12:57:50 visual_prompt]: 	Training 200/553. train loss: 0.6228,	0.9312 s / batch. (data: 2.69e-04). ETA=13:32:16, max mem: 30.7 GB 
[11/23 12:59:23 visual_prompt]: 	Training 300/553. train loss: 0.7374,	0.9681 s / batch. (data: 2.08e-02). ETA=14:02:47, max mem: 30.7 GB 
[11/23 13:00:58 visual_prompt]: 	Training 400/553. train loss: 0.9731,	0.9561 s / batch. (data: 3.28e-04). ETA=13:50:45, max mem: 30.7 GB 
[11/23 13:02:39 visual_prompt]: 	Training 500/553. train loss: 1.3833,	0.9457 s / batch. (data: 7.22e-04). ETA=13:40:11, max mem: 30.7 GB 
[11/23 13:03:29 visual_prompt]: Epoch 6 / 100: avg data time: 3.80e-02, avg batch time: 0.9711, average train loss: 0.7062
[11/23 13:04:25 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3052, average loss: 0.6370
[11/23 13:04:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.48	
[11/23 13:04:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/23 13:06:12 visual_prompt]: 	Training 100/553. train loss: 0.5425,	0.9206 s / batch. (data: 5.36e-03). ETA=13:16:03, max mem: 30.7 GB 
[11/23 13:07:48 visual_prompt]: 	Training 200/553. train loss: 0.3960,	0.9428 s / batch. (data: 5.96e-03). ETA=13:33:40, max mem: 30.7 GB 
[11/23 13:09:26 visual_prompt]: 	Training 300/553. train loss: 0.6166,	0.9822 s / batch. (data: 1.10e-02). ETA=14:06:00, max mem: 30.7 GB 
[11/23 13:11:02 visual_prompt]: 	Training 400/553. train loss: 0.7075,	0.9560 s / batch. (data: 3.19e-04). ETA=13:41:53, max mem: 30.7 GB 
[11/23 13:12:38 visual_prompt]: 	Training 500/553. train loss: 0.6128,	0.9732 s / batch. (data: 2.02e-02). ETA=13:55:03, max mem: 30.7 GB 
[11/23 13:13:28 visual_prompt]: Epoch 7 / 100: avg data time: 4.80e-02, avg batch time: 0.9822, average train loss: 0.6931
[11/23 13:14:26 visual_prompt]: Inference (val):avg data time: 1.24e-04, avg batch time: 0.3028, average loss: 1.0656
[11/23 13:14:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 70.37	
[11/23 13:14:26 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/23 13:16:18 visual_prompt]: 	Training 100/553. train loss: 0.4294,	0.9514 s / batch. (data: 2.54e-04). ETA=13:33:56, max mem: 30.7 GB 
[11/23 13:17:57 visual_prompt]: 	Training 200/553. train loss: 0.5636,	0.9647 s / batch. (data: 8.57e-03). ETA=13:43:40, max mem: 30.7 GB 
[11/23 13:19:32 visual_prompt]: 	Training 300/553. train loss: 0.6664,	0.9117 s / batch. (data: 2.70e-04). ETA=12:56:56, max mem: 30.7 GB 
[11/23 13:21:10 visual_prompt]: 	Training 400/553. train loss: 0.5699,	0.9271 s / batch. (data: 2.93e-04). ETA=13:08:26, max mem: 30.7 GB 
[11/23 13:22:47 visual_prompt]: 	Training 500/553. train loss: 0.4857,	0.9255 s / batch. (data: 3.52e-03). ETA=13:05:33, max mem: 30.7 GB 
[11/23 13:23:37 visual_prompt]: Epoch 8 / 100: avg data time: 6.24e-02, avg batch time: 0.9954, average train loss: 0.6882
[11/23 13:24:35 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3053, average loss: 0.6099
[11/23 13:24:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 73.14	
[11/23 13:24:35 visual_prompt]: Best epoch 8: best metric: -0.610
[11/23 13:24:35 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/23 13:26:25 visual_prompt]: 	Training 100/553. train loss: 0.7594,	0.9264 s / batch. (data: 2.70e-04). ETA=13:03:56, max mem: 30.7 GB 
[11/23 13:28:01 visual_prompt]: 	Training 200/553. train loss: 0.9344,	0.9248 s / batch. (data: 5.37e-03). ETA=13:01:05, max mem: 30.7 GB 
[11/23 13:29:35 visual_prompt]: 	Training 300/553. train loss: 0.5010,	0.9190 s / batch. (data: 5.39e-03). ETA=12:54:39, max mem: 30.7 GB 
[11/23 13:31:09 visual_prompt]: 	Training 400/553. train loss: 0.3758,	0.9312 s / batch. (data: 3.16e-04). ETA=13:03:20, max mem: 30.7 GB 
[11/23 13:32:43 visual_prompt]: 	Training 500/553. train loss: 0.9860,	0.9684 s / batch. (data: 1.05e-02). ETA=13:33:04, max mem: 30.7 GB 
[11/23 13:33:33 visual_prompt]: Epoch 9 / 100: avg data time: 3.95e-02, avg batch time: 0.9731, average train loss: 0.6750
[11/23 13:34:31 visual_prompt]: Inference (val):avg data time: 1.62e-04, avg batch time: 0.3035, average loss: 0.6308
[11/23 13:34:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 72.86	
[11/23 13:34:31 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/23 13:36:17 visual_prompt]: 	Training 100/553. train loss: 0.7033,	0.9417 s / batch. (data: 7.92e-03). ETA=13:08:14, max mem: 30.7 GB 
[11/23 13:37:54 visual_prompt]: 	Training 200/553. train loss: 0.4638,	0.9388 s / batch. (data: 8.97e-03). ETA=13:04:17, max mem: 30.7 GB 
[11/23 13:39:28 visual_prompt]: 	Training 300/553. train loss: 0.9259,	0.9560 s / batch. (data: 1.20e-02). ETA=13:17:00, max mem: 30.7 GB 
[11/23 13:41:02 visual_prompt]: 	Training 400/553. train loss: 0.5648,	0.9578 s / batch. (data: 1.04e-02). ETA=13:16:54, max mem: 30.7 GB 
[11/23 13:42:36 visual_prompt]: 	Training 500/553. train loss: 0.6633,	0.9400 s / batch. (data: 3.12e-04). ETA=13:00:34, max mem: 30.7 GB 
[11/23 13:43:27 visual_prompt]: Epoch 10 / 100: avg data time: 3.60e-02, avg batch time: 0.9679, average train loss: 0.6380
[11/23 13:44:24 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3072, average loss: 0.7756
[11/23 13:44:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 69.87	
[11/23 13:44:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/23 13:46:12 visual_prompt]: 	Training 100/553. train loss: 0.8577,	0.9498 s / batch. (data: 2.43e-02). ETA=13:06:14, max mem: 30.7 GB 
[11/23 13:47:47 visual_prompt]: 	Training 200/553. train loss: 1.0287,	0.9239 s / batch. (data: 3.10e-04). ETA=12:43:16, max mem: 30.7 GB 
[11/23 13:49:21 visual_prompt]: 	Training 300/553. train loss: 0.5660,	0.9344 s / batch. (data: 7.63e-04). ETA=12:50:23, max mem: 30.7 GB 
[11/23 13:50:56 visual_prompt]: 	Training 400/553. train loss: 0.3955,	0.9410 s / batch. (data: 5.16e-03). ETA=12:54:19, max mem: 30.7 GB 
[11/23 13:52:31 visual_prompt]: 	Training 500/553. train loss: 0.5259,	0.9508 s / batch. (data: 3.99e-03). ETA=13:00:44, max mem: 30.7 GB 
[11/23 13:53:21 visual_prompt]: Epoch 11 / 100: avg data time: 3.76e-02, avg batch time: 0.9695, average train loss: 0.6096
[11/23 13:54:18 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3040, average loss: 0.6828
[11/23 13:54:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 69.23	
[11/23 13:54:18 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/23 13:56:01 visual_prompt]: 	Training 100/553. train loss: 0.6483,	0.9273 s / batch. (data: 6.88e-04). ETA=12:39:04, max mem: 30.7 GB 
[11/23 13:57:40 visual_prompt]: 	Training 200/553. train loss: 0.6711,	0.9560 s / batch. (data: 7.00e-04). ETA=13:00:58, max mem: 30.7 GB 
[11/23 13:59:16 visual_prompt]: 	Training 300/553. train loss: 0.5236,	0.9299 s / batch. (data: 1.05e-02). ETA=12:38:07, max mem: 30.7 GB 
[11/23 14:00:50 visual_prompt]: 	Training 400/553. train loss: 0.3988,	0.9426 s / batch. (data: 2.42e-04). ETA=12:46:53, max mem: 30.7 GB 
[11/23 14:02:26 visual_prompt]: 	Training 500/553. train loss: 0.2242,	0.9120 s / batch. (data: 2.57e-04). ETA=12:20:29, max mem: 30.7 GB 
[11/23 14:03:16 visual_prompt]: Epoch 12 / 100: avg data time: 3.98e-02, avg batch time: 0.9712, average train loss: 0.6273
[11/23 14:04:13 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3040, average loss: 1.0328
[11/23 14:04:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 62.99	
[11/23 14:04:13 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/23 14:06:01 visual_prompt]: 	Training 100/553. train loss: 0.5392,	0.9373 s / batch. (data: 7.12e-04). ETA=12:38:41, max mem: 30.7 GB 
[11/23 14:07:35 visual_prompt]: 	Training 200/553. train loss: 0.4646,	0.9507 s / batch. (data: 5.43e-03). ETA=12:47:54, max mem: 30.7 GB 
[11/23 14:09:13 visual_prompt]: 	Training 300/553. train loss: 0.8751,	0.9320 s / batch. (data: 2.64e-04). ETA=12:31:17, max mem: 30.7 GB 
[11/23 14:10:50 visual_prompt]: 	Training 400/553. train loss: 0.6430,	0.9367 s / batch. (data: 1.34e-02). ETA=12:33:26, max mem: 30.7 GB 
[11/23 14:12:25 visual_prompt]: 	Training 500/553. train loss: 0.2968,	0.9600 s / batch. (data: 7.33e-04). ETA=12:50:35, max mem: 30.7 GB 
[11/23 14:13:14 visual_prompt]: Epoch 13 / 100: avg data time: 4.61e-02, avg batch time: 0.9779, average train loss: 0.5921
[11/23 14:14:13 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3055, average loss: 0.7365
[11/23 14:14:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.05	
[11/23 14:14:13 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/23 14:16:02 visual_prompt]: 	Training 100/553. train loss: 0.3231,	0.9440 s / batch. (data: 2.83e-04). ETA=12:35:23, max mem: 30.7 GB 
[11/23 14:17:40 visual_prompt]: 	Training 200/553. train loss: 0.1382,	0.9317 s / batch. (data: 5.39e-03). ETA=12:23:57, max mem: 30.7 GB 
[11/23 14:19:15 visual_prompt]: 	Training 300/553. train loss: 0.9660,	0.9339 s / batch. (data: 2.90e-04). ETA=12:24:11, max mem: 30.7 GB 
[11/23 14:20:50 visual_prompt]: 	Training 400/553. train loss: 0.4532,	0.9275 s / batch. (data: 1.04e-02). ETA=12:17:29, max mem: 30.7 GB 
[11/23 14:22:26 visual_prompt]: 	Training 500/553. train loss: 0.6816,	0.9482 s / batch. (data: 1.63e-02). ETA=12:32:23, max mem: 30.7 GB 
[11/23 14:23:16 visual_prompt]: Epoch 14 / 100: avg data time: 4.99e-02, avg batch time: 0.9814, average train loss: 0.5684
[11/23 14:24:14 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3068, average loss: 0.8393
[11/23 14:24:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 71.64	
[11/23 14:24:14 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/23 14:26:02 visual_prompt]: 	Training 100/553. train loss: 0.1655,	0.9316 s / batch. (data: 2.82e-04). ETA=12:16:50, max mem: 30.7 GB 
[11/23 14:27:40 visual_prompt]: 	Training 200/553. train loss: 0.1464,	0.9510 s / batch. (data: 1.09e-02). ETA=12:30:37, max mem: 30.7 GB 
[11/23 14:29:15 visual_prompt]: 	Training 300/553. train loss: 0.4043,	0.9593 s / batch. (data: 1.65e-02). ETA=12:35:34, max mem: 30.7 GB 
[11/23 14:30:51 visual_prompt]: 	Training 400/553. train loss: 0.1582,	0.9451 s / batch. (data: 5.35e-03). ETA=12:22:47, max mem: 30.7 GB 
[11/23 14:32:26 visual_prompt]: 	Training 500/553. train loss: 0.8310,	0.9300 s / batch. (data: 2.67e-04). ETA=12:09:22, max mem: 30.7 GB 
[11/23 14:33:15 visual_prompt]: Epoch 15 / 100: avg data time: 4.64e-02, avg batch time: 0.9781, average train loss: 0.5291
[11/23 14:34:13 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3049, average loss: 0.8093
[11/23 14:34:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.87	
[11/23 14:34:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/23 14:35:59 visual_prompt]: 	Training 100/553. train loss: 0.3361,	0.9579 s / batch. (data: 5.90e-03). ETA=12:28:52, max mem: 30.7 GB 
[11/23 14:37:33 visual_prompt]: 	Training 200/553. train loss: 0.4413,	0.9184 s / batch. (data: 7.98e-03). ETA=11:56:26, max mem: 30.7 GB 
[11/23 14:39:12 visual_prompt]: 	Training 300/553. train loss: 0.4570,	0.9343 s / batch. (data: 7.98e-03). ETA=12:07:17, max mem: 30.7 GB 
[11/23 14:40:49 visual_prompt]: 	Training 400/553. train loss: 0.2150,	0.9189 s / batch. (data: 5.38e-03). ETA=11:53:47, max mem: 30.7 GB 
[11/23 14:42:23 visual_prompt]: 	Training 500/553. train loss: 0.1098,	0.9696 s / batch. (data: 5.81e-03). ETA=12:31:28, max mem: 30.7 GB 
[11/23 14:43:13 visual_prompt]: Epoch 16 / 100: avg data time: 4.27e-02, avg batch time: 0.9755, average train loss: 0.4949
[11/23 14:44:11 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3045, average loss: 0.7315
[11/23 14:44:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 69.28	
[11/23 14:44:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/23 14:45:55 visual_prompt]: 	Training 100/553. train loss: 0.3602,	0.9400 s / batch. (data: 3.97e-03). ETA=12:06:09, max mem: 30.7 GB 
[11/23 14:47:34 visual_prompt]: 	Training 200/553. train loss: 0.8078,	0.9154 s / batch. (data: 3.14e-04). ETA=11:45:37, max mem: 30.7 GB 
[11/23 14:49:08 visual_prompt]: 	Training 300/553. train loss: 0.9709,	0.9240 s / batch. (data: 2.96e-04). ETA=11:50:45, max mem: 30.7 GB 
[11/23 14:50:44 visual_prompt]: 	Training 400/553. train loss: 0.1328,	0.9360 s / batch. (data: 3.07e-04). ETA=11:58:24, max mem: 30.7 GB 
[11/23 14:52:20 visual_prompt]: 	Training 500/553. train loss: 0.5086,	0.9423 s / batch. (data: 5.36e-03). ETA=12:01:40, max mem: 30.7 GB 
[11/23 14:53:10 visual_prompt]: Epoch 17 / 100: avg data time: 4.13e-02, avg batch time: 0.9741, average train loss: 0.4724
[11/23 14:54:08 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3058, average loss: 0.7419
[11/23 14:54:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 66.36	
[11/23 14:54:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/23 14:55:57 visual_prompt]: 	Training 100/553. train loss: 0.3558,	0.9360 s / batch. (data: 6.84e-04). ETA=11:54:28, max mem: 30.7 GB 
[11/23 14:57:34 visual_prompt]: 	Training 200/553. train loss: 1.0016,	0.9182 s / batch. (data: 2.51e-04). ETA=11:39:20, max mem: 30.7 GB 
[11/23 14:59:08 visual_prompt]: 	Training 300/553. train loss: 0.4475,	0.9480 s / batch. (data: 3.98e-03). ETA=12:00:27, max mem: 30.7 GB 
[11/23 15:00:43 visual_prompt]: 	Training 400/553. train loss: 0.0438,	0.9464 s / batch. (data: 2.73e-04). ETA=11:57:39, max mem: 30.7 GB 
[11/23 15:02:21 visual_prompt]: 	Training 500/553. train loss: 0.8394,	0.9051 s / batch. (data: 2.56e-04). ETA=11:24:48, max mem: 30.7 GB 
[11/23 15:03:10 visual_prompt]: Epoch 18 / 100: avg data time: 4.78e-02, avg batch time: 0.9801, average train loss: 0.4386
[11/23 15:04:08 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3047, average loss: 0.7687
[11/23 15:04:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 67.42	
[11/23 15:04:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/23 15:05:55 visual_prompt]: 	Training 100/553. train loss: 0.0538,	0.9399 s / batch. (data: 7.03e-04). ETA=11:48:46, max mem: 30.7 GB 
[11/23 15:07:33 visual_prompt]: 	Training 200/553. train loss: 0.8637,	3.6000 s / batch. (data: 2.68e+00). ETA=1 day, 21:08:46, max mem: 30.7 GB 
[11/23 15:09:07 visual_prompt]: 	Training 300/553. train loss: 0.0691,	0.9114 s / batch. (data: 3.16e-04). ETA=11:24:15, max mem: 30.7 GB 
[11/23 15:10:43 visual_prompt]: 	Training 400/553. train loss: 0.0503,	0.9600 s / batch. (data: 2.81e-04). ETA=11:59:08, max mem: 30.7 GB 
[11/23 15:12:19 visual_prompt]: 	Training 500/553. train loss: 0.1152,	0.9379 s / batch. (data: 2.70e-04). ETA=11:41:01, max mem: 30.7 GB 
[11/23 15:13:09 visual_prompt]: Epoch 19 / 100: avg data time: 4.49e-02, avg batch time: 0.9771, average train loss: 0.3811
[11/23 15:14:05 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3046, average loss: 0.8323
[11/23 15:14:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 67.69	
[11/23 15:14:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/23 15:15:55 visual_prompt]: 	Training 100/553. train loss: 0.0123,	0.9282 s / batch. (data: 8.19e-03). ETA=11:31:22, max mem: 30.7 GB 
[11/23 15:17:32 visual_prompt]: 	Training 200/553. train loss: 0.1315,	0.9317 s / batch. (data: 3.13e-04). ETA=11:32:26, max mem: 30.7 GB 
[11/23 15:19:09 visual_prompt]: 	Training 300/553. train loss: 0.1339,	0.9352 s / batch. (data: 5.41e-03). ETA=11:33:28, max mem: 30.7 GB 
[11/23 15:20:45 visual_prompt]: 	Training 400/553. train loss: 0.9129,	0.9480 s / batch. (data: 7.56e-04). ETA=11:41:25, max mem: 30.7 GB 
[11/23 15:22:19 visual_prompt]: 	Training 500/553. train loss: 0.2613,	0.9354 s / batch. (data: 4.95e-03). ETA=11:30:31, max mem: 30.7 GB 
[11/23 15:23:08 visual_prompt]: Epoch 20 / 100: avg data time: 4.81e-02, avg batch time: 0.9814, average train loss: 0.3657
[11/23 15:24:05 visual_prompt]: Inference (val):avg data time: 1.10e-04, avg batch time: 0.3063, average loss: 0.8700
[11/23 15:24:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.13	
[11/23 15:24:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/23 15:25:51 visual_prompt]: 	Training 100/553. train loss: 0.2007,	0.9513 s / batch. (data: 7.79e-04). ETA=11:39:50, max mem: 30.7 GB 
[11/23 15:27:24 visual_prompt]: 	Training 200/553. train loss: 0.6843,	0.9317 s / batch. (data: 5.38e-03). ETA=11:23:49, max mem: 30.7 GB 
[11/23 15:29:00 visual_prompt]: 	Training 300/553. train loss: 0.4690,	0.9443 s / batch. (data: 7.72e-04). ETA=11:31:30, max mem: 30.7 GB 
[11/23 15:30:34 visual_prompt]: 	Training 400/553. train loss: 0.7746,	0.9155 s / batch. (data: 5.38e-03). ETA=11:08:55, max mem: 30.7 GB 
[11/23 15:32:08 visual_prompt]: 	Training 500/553. train loss: 0.0671,	0.9076 s / batch. (data: 3.64e-04). ETA=11:01:38, max mem: 30.7 GB 
[11/23 15:32:57 visual_prompt]: Epoch 21 / 100: avg data time: 3.26e-02, avg batch time: 0.9630, average train loss: 0.3102
[11/23 15:33:54 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3053, average loss: 0.9785
[11/23 15:33:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 65.04	
[11/23 15:33:54 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/23 15:35:37 visual_prompt]: 	Training 100/553. train loss: 1.1205,	0.9320 s / batch. (data: 2.58e-04). ETA=11:17:04, max mem: 30.7 GB 
[11/23 15:37:12 visual_prompt]: 	Training 200/553. train loss: 0.6425,	0.9408 s / batch. (data: 1.09e-02). ETA=11:21:52, max mem: 30.7 GB 
[11/23 15:38:46 visual_prompt]: 	Training 300/553. train loss: 0.0566,	0.9219 s / batch. (data: 5.36e-03). ETA=11:06:37, max mem: 30.7 GB 
[11/23 15:40:20 visual_prompt]: 	Training 400/553. train loss: 0.3136,	0.9360 s / batch. (data: 5.37e-03). ETA=11:15:16, max mem: 30.7 GB 
[11/23 15:41:54 visual_prompt]: 	Training 500/553. train loss: 0.0481,	0.9271 s / batch. (data: 5.37e-03). ETA=11:07:18, max mem: 30.7 GB 
[11/23 15:42:44 visual_prompt]: Epoch 22 / 100: avg data time: 2.65e-02, avg batch time: 0.9580, average train loss: 0.2795
[11/23 15:43:40 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3059, average loss: 1.2979
[11/23 15:43:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 61.71	
[11/23 15:43:40 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/23 15:45:25 visual_prompt]: 	Training 100/553. train loss: 0.0079,	0.9252 s / batch. (data: 5.36e-03). ETA=11:03:33, max mem: 30.7 GB 
[11/23 15:46:59 visual_prompt]: 	Training 200/553. train loss: 0.6295,	0.9517 s / batch. (data: 1.34e-02). ETA=11:21:00, max mem: 30.7 GB 
[11/23 15:48:33 visual_prompt]: 	Training 300/553. train loss: 0.0324,	0.9104 s / batch. (data: 2.82e-04). ETA=10:49:57, max mem: 30.7 GB 
[11/23 15:50:07 visual_prompt]: 	Training 400/553. train loss: 0.0157,	0.9259 s / batch. (data: 5.38e-03). ETA=10:59:27, max mem: 30.7 GB 
[11/23 15:51:40 visual_prompt]: 	Training 500/553. train loss: 0.1186,	0.9396 s / batch. (data: 1.05e-02). ETA=11:07:37, max mem: 30.7 GB 
[11/23 15:52:30 visual_prompt]: Epoch 23 / 100: avg data time: 2.68e-02, avg batch time: 0.9581, average train loss: 0.2302
[11/23 15:53:26 visual_prompt]: Inference (val):avg data time: 2.64e-04, avg batch time: 0.3048, average loss: 1.2135
[11/23 15:53:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.14	
[11/23 15:53:26 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/23 15:55:10 visual_prompt]: 	Training 100/553. train loss: 0.0642,	0.9268 s / batch. (data: 5.40e-03). ETA=10:56:13, max mem: 30.7 GB 
[11/23 15:56:44 visual_prompt]: 	Training 200/553. train loss: 0.2222,	0.9490 s / batch. (data: 5.85e-03). ETA=11:10:17, max mem: 30.7 GB 
[11/23 15:58:25 visual_prompt]: 	Training 300/553. train loss: 0.0822,	1.3668 s / batch. (data: 4.18e-01). ETA=16:03:09, max mem: 30.7 GB 
[11/23 16:00:03 visual_prompt]: 	Training 400/553. train loss: 0.0729,	0.9449 s / batch. (data: 6.98e-04). ETA=11:04:18, max mem: 30.7 GB 
[11/23 16:01:37 visual_prompt]: 	Training 500/553. train loss: 0.0526,	0.9430 s / batch. (data: 5.34e-03). ETA=11:01:22, max mem: 30.7 GB 
[11/23 16:02:27 visual_prompt]: Epoch 24 / 100: avg data time: 4.32e-02, avg batch time: 0.9766, average train loss: 0.2396
[11/23 16:03:24 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3058, average loss: 1.4485
[11/23 16:03:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 66.59	
[11/23 16:03:24 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/23 16:05:06 visual_prompt]: 	Training 100/553. train loss: 0.1935,	0.9068 s / batch. (data: 2.69e-04). ETA=10:33:41, max mem: 30.7 GB 
[11/23 16:06:43 visual_prompt]: 	Training 200/553. train loss: 0.0101,	0.9247 s / batch. (data: 5.83e-03). ETA=10:44:40, max mem: 30.7 GB 
[11/23 16:08:17 visual_prompt]: 	Training 300/553. train loss: 0.0294,	0.9223 s / batch. (data: 8.05e-04). ETA=10:41:24, max mem: 30.7 GB 
[11/23 16:09:51 visual_prompt]: 	Training 400/553. train loss: 0.0103,	0.9440 s / batch. (data: 2.62e-04). ETA=10:54:56, max mem: 30.7 GB 
[11/23 16:11:26 visual_prompt]: 	Training 500/553. train loss: 0.3352,	0.9434 s / batch. (data: 5.37e-03). ETA=10:52:58, max mem: 30.7 GB 
[11/23 16:12:15 visual_prompt]: Epoch 25 / 100: avg data time: 2.83e-02, avg batch time: 0.9603, average train loss: 0.2029
[11/23 16:13:13 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3060, average loss: 1.3075
[11/23 16:13:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 66.47	
[11/23 16:13:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/23 16:14:57 visual_prompt]: 	Training 100/553. train loss: 0.0141,	0.9513 s / batch. (data: 5.86e-03). ETA=10:56:01, max mem: 30.7 GB 
[11/23 16:16:32 visual_prompt]: 	Training 200/553. train loss: 0.0594,	0.9318 s / batch. (data: 2.74e-04). ETA=10:41:00, max mem: 30.7 GB 
[11/23 16:18:07 visual_prompt]: 	Training 300/553. train loss: 0.0043,	0.9224 s / batch. (data: 2.89e-04). ETA=10:32:59, max mem: 30.7 GB 
[11/23 16:19:41 visual_prompt]: 	Training 400/553. train loss: 0.0085,	0.9644 s / batch. (data: 1.05e-02). ETA=11:00:13, max mem: 30.7 GB 
[11/23 16:21:15 visual_prompt]: 	Training 500/553. train loss: 0.5429,	0.9400 s / batch. (data: 2.94e-04). ETA=10:41:55, max mem: 30.7 GB 
[11/23 16:22:05 visual_prompt]: Epoch 26 / 100: avg data time: 2.91e-02, avg batch time: 0.9620, average train loss: 0.1649
[11/23 16:23:01 visual_prompt]: Inference (val):avg data time: 1.87e-04, avg batch time: 0.3027, average loss: 1.1253
[11/23 16:23:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 66.75	
[11/23 16:23:01 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/23 16:24:44 visual_prompt]: 	Training 100/553. train loss: 0.0007,	0.9768 s / batch. (data: 2.47e-02). ETA=11:04:35, max mem: 30.7 GB 
[11/23 16:26:21 visual_prompt]: 	Training 200/553. train loss: 0.0440,	0.9295 s / batch. (data: 9.35e-03). ETA=10:30:52, max mem: 30.7 GB 
[11/23 16:27:54 visual_prompt]: 	Training 300/553. train loss: 0.0709,	0.9524 s / batch. (data: 7.45e-04). ETA=10:44:46, max mem: 30.7 GB 
[11/23 16:29:29 visual_prompt]: 	Training 400/553. train loss: 0.3806,	0.9687 s / batch. (data: 1.67e-02). ETA=10:54:15, max mem: 30.7 GB 
[11/23 16:31:05 visual_prompt]: 	Training 500/553. train loss: 0.0036,	0.9240 s / batch. (data: 3.96e-03). ETA=10:22:28, max mem: 30.7 GB 
[11/23 16:31:55 visual_prompt]: Epoch 27 / 100: avg data time: 3.23e-02, avg batch time: 0.9643, average train loss: 0.1662
[11/23 16:32:53 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3045, average loss: 1.2015
[11/23 16:32:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 66.72	
[11/23 16:32:53 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/23 16:34:37 visual_prompt]: 	Training 100/553. train loss: 0.0062,	0.9311 s / batch. (data: 7.99e-03). ETA=10:24:55, max mem: 30.7 GB 
[11/23 16:36:14 visual_prompt]: 	Training 200/553. train loss: 0.0034,	0.9600 s / batch. (data: 7.99e-03). ETA=10:42:42, max mem: 30.7 GB 
[11/23 16:37:47 visual_prompt]: 	Training 300/553. train loss: 0.0259,	0.9240 s / batch. (data: 2.91e-04). ETA=10:17:03, max mem: 30.7 GB 
[11/23 16:39:21 visual_prompt]: 	Training 400/553. train loss: 0.0077,	0.9447 s / batch. (data: 8.82e-03). ETA=10:29:18, max mem: 30.7 GB 
[11/23 16:40:57 visual_prompt]: 	Training 500/553. train loss: 0.4243,	0.9440 s / batch. (data: 2.81e-04). ETA=10:27:16, max mem: 30.7 GB 
[11/23 16:41:47 visual_prompt]: Epoch 28 / 100: avg data time: 3.34e-02, avg batch time: 0.9651, average train loss: 0.1353
[11/23 16:42:44 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3082, average loss: 1.2359
[11/23 16:42:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 63.86	
[11/23 16:42:44 visual_prompt]: Training 29 / 100 epoch, with learning rate 8.622126023955446e-05
[11/23 16:44:27 visual_prompt]: 	Training 100/553. train loss: 0.0251,	0.9360 s / batch. (data: 2.51e-04). ETA=10:19:34, max mem: 30.7 GB 
[11/23 16:46:01 visual_prompt]: 	Training 200/553. train loss: 0.0141,	0.9351 s / batch. (data: 1.63e-02). ETA=10:17:23, max mem: 30.7 GB 
[11/23 16:47:36 visual_prompt]: 	Training 300/553. train loss: 0.3898,	0.9309 s / batch. (data: 2.62e-04). ETA=10:13:06, max mem: 30.7 GB 
[11/23 16:49:10 visual_prompt]: 	Training 400/553. train loss: 0.0020,	0.9408 s / batch. (data: 7.00e-04). ETA=10:18:03, max mem: 30.7 GB 
[11/23 16:50:43 visual_prompt]: 	Training 500/553. train loss: 0.1211,	0.9240 s / batch. (data: 7.29e-04). ETA=10:05:28, max mem: 30.7 GB 
[11/23 16:51:32 visual_prompt]: Epoch 29 / 100: avg data time: 2.63e-02, avg batch time: 0.9533, average train loss: 0.1966
[11/23 16:52:27 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3056, average loss: 0.7299
[11/23 16:52:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.88	
[11/23 16:52:27 visual_prompt]: Stopping early.
[11/23 16:52:27 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 16:52:27 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 16:52:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/23 16:52:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/23 16:52:27 visual_prompt]: Training with config:
[11/23 16:52:27 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0001_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/23 16:52:27 visual_prompt]: Loading training data...
[11/23 16:52:27 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 16:52:27 visual_prompt]: Loading validation data...
[11/23 16:52:27 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 16:52:27 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 16:52:32 visual_prompt]: Enable all parameters update during training
[11/23 16:52:32 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/23 16:52:32 visual_prompt]: tuned percent:100.000
[11/23 16:52:32 visual_prompt]: Device used for model: 0
[11/23 16:52:32 visual_prompt]: Setting up Evaluator...
[11/23 16:52:32 visual_prompt]: Setting up Trainer...
[11/23 16:52:32 visual_prompt]: 	Setting up the optimizer...
[11/23 16:52:32 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 16:54:13 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9160 s / batch. (data: 5.40e-03). ETA=14:02:43, max mem: 30.7 GB 
[11/23 16:55:47 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9291 s / batch. (data: 5.36e-03). ETA=14:13:11, max mem: 30.7 GB 
[11/23 16:57:20 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9133 s / batch. (data: 6.92e-04). ETA=13:57:09, max mem: 30.7 GB 
[11/23 16:58:54 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9123 s / batch. (data: 5.37e-03). ETA=13:54:43, max mem: 30.7 GB 
[11/23 17:00:27 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9424 s / batch. (data: 7.73e-04). ETA=14:20:42, max mem: 30.7 GB 
[11/23 17:01:16 visual_prompt]: Epoch 1 / 100: avg data time: 1.91e-02, avg batch time: 0.9475, average train loss: 7.6130
[11/23 17:02:11 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3076, average loss: 6.9126
[11/23 17:02:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/23 17:02:11 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/23 17:03:53 visual_prompt]: 	Training 100/553. train loss: 1.6339,	0.9668 s / batch. (data: 2.48e-02). ETA=14:40:31, max mem: 30.7 GB 
[11/23 17:05:27 visual_prompt]: 	Training 200/553. train loss: 1.0665,	0.9371 s / batch. (data: 1.31e-02). ETA=14:11:53, max mem: 30.7 GB 
[11/23 17:07:01 visual_prompt]: 	Training 300/553. train loss: 1.0076,	0.9231 s / batch. (data: 2.48e-04). ETA=13:57:41, max mem: 30.7 GB 
[11/23 17:08:35 visual_prompt]: 	Training 400/553. train loss: 0.4593,	0.9382 s / batch. (data: 5.82e-03). ETA=14:09:49, max mem: 30.7 GB 
[11/23 17:10:08 visual_prompt]: 	Training 500/553. train loss: 1.0950,	0.9367 s / batch. (data: 1.12e-03). ETA=14:06:51, max mem: 30.7 GB 
[11/23 17:10:57 visual_prompt]: Epoch 2 / 100: avg data time: 2.16e-02, avg batch time: 0.9504, average train loss: 0.9247
[11/23 17:11:51 visual_prompt]: Inference (val):avg data time: 4.72e-04, avg batch time: 0.3050, average loss: 0.8591
[11/23 17:11:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.46	
[11/23 17:11:51 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/23 17:13:32 visual_prompt]: 	Training 100/553. train loss: 0.7375,	0.9319 s / batch. (data: 2.61e-04). ETA=14:00:07, max mem: 30.7 GB 
[11/23 17:15:06 visual_prompt]: 	Training 200/553. train loss: 1.9872,	0.9197 s / batch. (data: 2.81e-04). ETA=13:47:40, max mem: 30.7 GB 
[11/23 17:16:39 visual_prompt]: 	Training 300/553. train loss: 1.1307,	0.9399 s / batch. (data: 1.04e-02). ETA=14:04:13, max mem: 30.7 GB 
[11/23 17:18:12 visual_prompt]: 	Training 400/553. train loss: 0.3073,	0.9111 s / batch. (data: 2.73e-04). ETA=13:36:50, max mem: 30.7 GB 
[11/23 17:19:46 visual_prompt]: 	Training 500/553. train loss: 0.8520,	0.9347 s / batch. (data: 1.05e-02). ETA=13:56:26, max mem: 30.7 GB 
[11/23 17:20:35 visual_prompt]: Epoch 3 / 100: avg data time: 1.85e-02, avg batch time: 0.9464, average train loss: 0.7987
[11/23 17:21:29 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3051, average loss: 0.7150
[11/23 17:21:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 60.87	
[11/23 17:21:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/23 17:23:14 visual_prompt]: 	Training 100/553. train loss: 0.5859,	0.9378 s / batch. (data: 7.60e-04). ETA=13:56:53, max mem: 30.7 GB 
[11/23 17:24:47 visual_prompt]: 	Training 200/553. train loss: 0.9466,	0.9272 s / batch. (data: 2.59e-04). ETA=13:45:48, max mem: 30.7 GB 
[11/23 17:26:21 visual_prompt]: 	Training 300/553. train loss: 0.8159,	0.9485 s / batch. (data: 6.86e-04). ETA=14:03:15, max mem: 30.7 GB 
[11/23 17:27:54 visual_prompt]: 	Training 400/553. train loss: 0.6014,	0.9629 s / batch. (data: 3.08e-02). ETA=14:14:25, max mem: 30.7 GB 
[11/23 17:29:27 visual_prompt]: 	Training 500/553. train loss: 1.3442,	0.9518 s / batch. (data: 5.36e-03). ETA=14:03:02, max mem: 30.7 GB 
[11/23 17:30:17 visual_prompt]: Epoch 4 / 100: avg data time: 2.66e-02, avg batch time: 0.9534, average train loss: 0.7791
[11/23 17:31:11 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.3046, average loss: 0.6801
[11/23 17:31:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.26	
[11/23 17:31:11 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/23 17:32:51 visual_prompt]: 	Training 100/553. train loss: 0.5745,	0.9458 s / batch. (data: 1.04e-02). ETA=13:55:17, max mem: 30.7 GB 
[11/23 17:34:24 visual_prompt]: 	Training 200/553. train loss: 0.4841,	0.9055 s / batch. (data: 2.71e-04). ETA=13:18:12, max mem: 30.7 GB 
[11/23 17:35:57 visual_prompt]: 	Training 300/553. train loss: 0.8860,	0.9150 s / batch. (data: 8.35e-03). ETA=13:25:02, max mem: 30.7 GB 
[11/23 17:37:31 visual_prompt]: 	Training 400/553. train loss: 0.9437,	0.9453 s / batch. (data: 5.36e-03). ETA=13:50:06, max mem: 30.7 GB 
[11/23 17:39:04 visual_prompt]: 	Training 500/553. train loss: 0.8211,	0.9290 s / batch. (data: 7.11e-04). ETA=13:34:11, max mem: 30.7 GB 
[11/23 17:39:54 visual_prompt]: Epoch 5 / 100: avg data time: 1.63e-02, avg batch time: 0.9447, average train loss: 0.7313
[11/23 17:40:48 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3070, average loss: 0.6548
[11/23 17:40:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 71.39	
[11/23 17:40:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/23 17:42:30 visual_prompt]: 	Training 100/553. train loss: 0.5414,	0.9498 s / batch. (data: 2.57e-02). ETA=13:50:03, max mem: 30.7 GB 
[11/23 17:44:03 visual_prompt]: 	Training 200/553. train loss: 0.5915,	0.9376 s / batch. (data: 7.48e-04). ETA=13:37:50, max mem: 30.7 GB 
[11/23 17:45:37 visual_prompt]: 	Training 300/553. train loss: 0.6791,	0.9504 s / batch. (data: 1.09e-02). ETA=13:47:23, max mem: 30.7 GB 
[11/23 17:47:10 visual_prompt]: 	Training 400/553. train loss: 0.9199,	0.9242 s / batch. (data: 3.15e-03). ETA=13:23:05, max mem: 30.7 GB 
[11/23 17:48:44 visual_prompt]: 	Training 500/553. train loss: 1.5401,	0.9468 s / batch. (data: 7.12e-04). ETA=13:41:08, max mem: 30.7 GB 
[11/23 17:49:33 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e-02, avg batch time: 0.9491, average train loss: 0.7090
[11/23 17:50:27 visual_prompt]: Inference (val):avg data time: 6.17e-04, avg batch time: 0.3094, average loss: 0.6376
[11/23 17:50:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.57	
[11/23 17:50:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/23 17:52:11 visual_prompt]: 	Training 100/553. train loss: 0.5258,	0.9324 s / batch. (data: 5.36e-03). ETA=13:26:17, max mem: 30.7 GB 
[11/23 17:53:45 visual_prompt]: 	Training 200/553. train loss: 0.5318,	0.9233 s / batch. (data: 1.07e-02). ETA=13:16:48, max mem: 30.7 GB 
[11/23 17:55:18 visual_prompt]: 	Training 300/553. train loss: 0.6061,	0.9479 s / batch. (data: 9.35e-03). ETA=13:36:27, max mem: 30.7 GB 
[11/23 17:56:51 visual_prompt]: 	Training 400/553. train loss: 0.7265,	0.9160 s / batch. (data: 2.62e-04). ETA=13:07:30, max mem: 30.7 GB 
[11/23 17:58:25 visual_prompt]: 	Training 500/553. train loss: 0.6375,	0.9178 s / batch. (data: 5.35e-03). ETA=13:07:30, max mem: 30.7 GB 
[11/23 17:59:14 visual_prompt]: Epoch 7 / 100: avg data time: 2.50e-02, avg batch time: 0.9531, average train loss: 0.6930
[11/23 18:00:09 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.3056, average loss: 0.6167
[11/23 18:00:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 71.83	
[11/23 18:00:09 visual_prompt]: Best epoch 7: best metric: -0.617
[11/23 18:00:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/23 18:01:52 visual_prompt]: 	Training 100/553. train loss: 0.4527,	0.9375 s / batch. (data: 6.87e-04). ETA=13:22:02, max mem: 30.7 GB 
[11/23 18:03:25 visual_prompt]: 	Training 200/553. train loss: 0.6266,	0.9334 s / batch. (data: 7.51e-04). ETA=13:16:56, max mem: 30.7 GB 
[11/23 18:04:58 visual_prompt]: 	Training 300/553. train loss: 0.5343,	0.9740 s / batch. (data: 2.74e-02). ETA=13:49:57, max mem: 30.7 GB 
[11/23 18:06:32 visual_prompt]: 	Training 400/553. train loss: 0.5959,	0.9434 s / batch. (data: 2.44e-04). ETA=13:22:23, max mem: 30.7 GB 
[11/23 18:08:05 visual_prompt]: 	Training 500/553. train loss: 0.2989,	0.9400 s / batch. (data: 3.68e-04). ETA=13:17:51, max mem: 30.7 GB 
[11/23 18:08:55 visual_prompt]: Epoch 8 / 100: avg data time: 2.33e-02, avg batch time: 0.9503, average train loss: 0.6835
[11/23 18:09:49 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3058, average loss: 0.6060
[11/23 18:09:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 73.02	
[11/23 18:09:49 visual_prompt]: Best epoch 8: best metric: -0.606
[11/23 18:09:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/23 18:11:33 visual_prompt]: 	Training 100/553. train loss: 0.7385,	0.9760 s / batch. (data: 7.84e-04). ETA=13:45:57, max mem: 30.7 GB 
[11/23 18:13:06 visual_prompt]: 	Training 200/553. train loss: 0.9508,	0.9252 s / batch. (data: 2.25e-04). ETA=13:01:26, max mem: 30.7 GB 
[11/23 18:14:39 visual_prompt]: 	Training 300/553. train loss: 0.3820,	0.9320 s / batch. (data: 3.27e-04). ETA=13:05:37, max mem: 30.7 GB 
[11/23 18:16:13 visual_prompt]: 	Training 400/553. train loss: 0.5621,	0.9706 s / batch. (data: 7.43e-04). ETA=13:36:30, max mem: 30.7 GB 
[11/23 18:17:46 visual_prompt]: 	Training 500/553. train loss: 0.8353,	0.9343 s / batch. (data: 6.98e-04). ETA=13:04:25, max mem: 30.7 GB 
[11/23 18:18:35 visual_prompt]: Epoch 9 / 100: avg data time: 2.35e-02, avg batch time: 0.9514, average train loss: 0.6957
[11/23 18:19:29 visual_prompt]: Inference (val):avg data time: 8.13e-04, avg batch time: 0.3059, average loss: 0.6575
[11/23 18:19:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.60	
[11/23 18:19:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/23 18:21:11 visual_prompt]: 	Training 100/553. train loss: 0.9486,	0.9338 s / batch. (data: 7.34e-04). ETA=13:01:36, max mem: 30.7 GB 
[11/23 18:22:44 visual_prompt]: 	Training 200/553. train loss: 0.5115,	0.9493 s / batch. (data: 2.72e-04). ETA=13:13:02, max mem: 30.7 GB 
[11/23 18:24:17 visual_prompt]: 	Training 300/553. train loss: 0.8498,	0.9282 s / batch. (data: 5.84e-03). ETA=12:53:52, max mem: 30.7 GB 
[11/23 18:25:51 visual_prompt]: 	Training 400/553. train loss: 0.6073,	0.9478 s / batch. (data: 2.49e-02). ETA=13:08:38, max mem: 30.7 GB 
[11/23 18:27:24 visual_prompt]: 	Training 500/553. train loss: 0.5823,	0.9485 s / batch. (data: 7.01e-04). ETA=13:07:37, max mem: 30.7 GB 
[11/23 18:28:14 visual_prompt]: Epoch 10 / 100: avg data time: 1.94e-02, avg batch time: 0.9476, average train loss: 0.6521
[11/23 18:29:08 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.3074, average loss: 0.7318
[11/23 18:29:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 67.34	
[11/23 18:29:08 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/23 18:30:51 visual_prompt]: 	Training 100/553. train loss: 0.9661,	0.9606 s / batch. (data: 1.04e-02). ETA=13:15:10, max mem: 30.7 GB 
[11/23 18:32:24 visual_prompt]: 	Training 200/553. train loss: 1.1474,	0.9133 s / batch. (data: 2.32e-04). ETA=12:34:31, max mem: 30.7 GB 
[11/23 18:33:57 visual_prompt]: 	Training 300/553. train loss: 0.5471,	0.9591 s / batch. (data: 5.83e-03). ETA=13:10:47, max mem: 30.7 GB 
[11/23 18:35:31 visual_prompt]: 	Training 400/553. train loss: 0.5357,	0.9233 s / batch. (data: 2.42e-04). ETA=12:39:44, max mem: 30.7 GB 
[11/23 18:37:04 visual_prompt]: 	Training 500/553. train loss: 0.5820,	0.9127 s / batch. (data: 2.79e-04). ETA=12:29:29, max mem: 30.7 GB 
[11/23 18:37:53 visual_prompt]: Epoch 11 / 100: avg data time: 2.24e-02, avg batch time: 0.9503, average train loss: 0.6286
[11/23 18:38:48 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3078, average loss: 0.6665
[11/23 18:38:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 67.22	
[11/23 18:38:48 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/23 18:40:28 visual_prompt]: 	Training 100/553. train loss: 0.5904,	0.9189 s / batch. (data: 2.50e-04). ETA=12:32:11, max mem: 30.7 GB 
[11/23 18:42:01 visual_prompt]: 	Training 200/553. train loss: 0.6226,	0.9400 s / batch. (data: 5.53e-03). ETA=12:47:55, max mem: 30.7 GB 
[11/23 18:43:35 visual_prompt]: 	Training 300/553. train loss: 0.5501,	0.9273 s / batch. (data: 7.30e-04). ETA=12:36:00, max mem: 30.7 GB 
[11/23 18:45:08 visual_prompt]: 	Training 400/553. train loss: 0.3879,	0.9358 s / batch. (data: 7.79e-04). ETA=12:41:25, max mem: 30.7 GB 
[11/23 18:46:42 visual_prompt]: 	Training 500/553. train loss: 0.5503,	0.9323 s / batch. (data: 2.74e-04). ETA=12:37:00, max mem: 30.7 GB 
[11/23 18:47:31 visual_prompt]: Epoch 12 / 100: avg data time: 1.49e-02, avg batch time: 0.9453, average train loss: 0.6419
[11/23 18:48:26 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3057, average loss: 1.0364
[11/23 18:48:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 64.73	
[11/23 18:48:26 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/23 18:50:09 visual_prompt]: 	Training 100/553. train loss: 0.6314,	0.9302 s / batch. (data: 7.49e-04). ETA=12:32:56, max mem: 30.7 GB 
[11/23 18:51:42 visual_prompt]: 	Training 200/553. train loss: 0.6246,	0.9349 s / batch. (data: 7.94e-03). ETA=12:35:08, max mem: 30.7 GB 
[11/23 18:53:16 visual_prompt]: 	Training 300/553. train loss: 1.1126,	0.9530 s / batch. (data: 6.73e-04). ETA=12:48:09, max mem: 30.7 GB 
[11/23 18:54:50 visual_prompt]: 	Training 400/553. train loss: 0.6498,	0.9210 s / batch. (data: 2.74e-04). ETA=12:20:51, max mem: 30.7 GB 
[11/23 18:56:23 visual_prompt]: 	Training 500/553. train loss: 0.2855,	0.9361 s / batch. (data: 7.22e-04). ETA=12:31:27, max mem: 30.7 GB 
[11/23 18:57:12 visual_prompt]: Epoch 13 / 100: avg data time: 2.23e-02, avg batch time: 0.9513, average train loss: 0.6106
[11/23 18:58:07 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3061, average loss: 0.8772
[11/23 18:58:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 66.66	
[11/23 18:58:07 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/23 18:59:50 visual_prompt]: 	Training 100/553. train loss: 0.3075,	0.9368 s / batch. (data: 5.81e-03). ETA=12:29:36, max mem: 30.7 GB 
[11/23 19:01:24 visual_prompt]: 	Training 200/553. train loss: 0.1811,	0.9339 s / batch. (data: 2.46e-02). ETA=12:25:42, max mem: 30.7 GB 
[11/23 19:02:57 visual_prompt]: 	Training 300/553. train loss: 1.0509,	0.9341 s / batch. (data: 7.56e-04). ETA=12:24:20, max mem: 30.7 GB 
[11/23 19:04:30 visual_prompt]: 	Training 400/553. train loss: 1.0574,	0.9242 s / batch. (data: 7.00e-04). ETA=12:14:54, max mem: 30.7 GB 
[11/23 19:06:04 visual_prompt]: 	Training 500/553. train loss: 0.9083,	0.9362 s / batch. (data: 7.40e-04). ETA=12:22:52, max mem: 30.7 GB 
[11/23 19:06:53 visual_prompt]: Epoch 14 / 100: avg data time: 2.30e-02, avg batch time: 0.9508, average train loss: 0.6283
[11/23 19:07:48 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3073, average loss: 0.6957
[11/23 19:07:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 67.52	
[11/23 19:07:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/23 19:09:32 visual_prompt]: 	Training 100/553. train loss: 0.5498,	0.9225 s / batch. (data: 3.97e-03). ETA=12:09:41, max mem: 30.7 GB 
[11/23 19:11:06 visual_prompt]: 	Training 200/553. train loss: 0.4357,	0.9154 s / batch. (data: 5.37e-03). ETA=12:02:33, max mem: 30.7 GB 
[11/23 19:12:39 visual_prompt]: 	Training 300/553. train loss: 0.7045,	0.9149 s / batch. (data: 5.36e-03). ETA=12:00:37, max mem: 30.7 GB 
[11/23 19:14:13 visual_prompt]: 	Training 400/553. train loss: 0.3559,	0.9377 s / batch. (data: 2.63e-04). ETA=12:16:59, max mem: 30.7 GB 
[11/23 19:15:46 visual_prompt]: 	Training 500/553. train loss: 1.0065,	0.9272 s / batch. (data: 2.05e-02). ETA=12:07:10, max mem: 30.7 GB 
[11/23 19:16:35 visual_prompt]: Epoch 15 / 100: avg data time: 2.60e-02, avg batch time: 0.9532, average train loss: 0.5850
[11/23 19:17:30 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3048, average loss: 0.9089
[11/23 19:17:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.85	
[11/23 19:17:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/23 19:19:11 visual_prompt]: 	Training 100/553. train loss: 0.6027,	0.9039 s / batch. (data: 2.79e-04). ETA=11:46:37, max mem: 30.7 GB 
[11/23 19:20:44 visual_prompt]: 	Training 200/553. train loss: 0.5969,	0.9386 s / batch. (data: 7.63e-04). ETA=12:12:11, max mem: 30.7 GB 
[11/23 19:22:18 visual_prompt]: 	Training 300/553. train loss: 0.7653,	0.9520 s / batch. (data: 7.38e-04). ETA=12:21:03, max mem: 30.7 GB 
[11/23 19:23:52 visual_prompt]: 	Training 400/553. train loss: 0.3602,	0.9312 s / batch. (data: 7.20e-04). ETA=12:03:19, max mem: 30.7 GB 
[11/23 19:25:25 visual_prompt]: 	Training 500/553. train loss: 0.3785,	0.9565 s / batch. (data: 8.06e-04). ETA=12:21:21, max mem: 30.7 GB 
[11/23 19:26:14 visual_prompt]: Epoch 16 / 100: avg data time: 2.01e-02, avg batch time: 0.9483, average train loss: 0.5657
[11/23 19:27:09 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3074, average loss: 0.7650
[11/23 19:27:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.02	
[11/23 19:27:09 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/23 19:28:50 visual_prompt]: 	Training 100/553. train loss: 0.8054,	0.9392 s / batch. (data: 4.01e-04). ETA=12:05:34, max mem: 30.7 GB 
[11/23 19:30:24 visual_prompt]: 	Training 200/553. train loss: 0.8746,	0.9057 s / batch. (data: 2.79e-04). ETA=11:38:11, max mem: 30.7 GB 
[11/23 19:31:57 visual_prompt]: 	Training 300/553. train loss: 0.9360,	0.9160 s / batch. (data: 2.64e-04). ETA=11:44:37, max mem: 30.7 GB 
[11/23 19:33:30 visual_prompt]: 	Training 400/553. train loss: 0.0502,	0.9500 s / batch. (data: 2.60e-04). ETA=12:09:09, max mem: 30.7 GB 
[11/23 19:35:04 visual_prompt]: 	Training 500/553. train loss: 0.5820,	0.9362 s / batch. (data: 5.34e-03). ETA=11:56:59, max mem: 30.7 GB 
[11/23 19:35:54 visual_prompt]: Epoch 17 / 100: avg data time: 1.94e-02, avg batch time: 0.9481, average train loss: 0.5295
[11/23 19:36:48 visual_prompt]: Inference (val):avg data time: 9.36e-05, avg batch time: 0.3058, average loss: 0.6990
[11/23 19:36:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.45	
[11/23 19:36:48 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/23 19:38:32 visual_prompt]: 	Training 100/553. train loss: 0.6677,	0.9031 s / batch. (data: 4.13e-04). ETA=11:29:20, max mem: 30.7 GB 
[11/23 19:40:05 visual_prompt]: 	Training 200/553. train loss: 1.2635,	0.9404 s / batch. (data: 7.41e-04). ETA=11:56:17, max mem: 30.7 GB 
[11/23 19:41:39 visual_prompt]: 	Training 300/553. train loss: 0.3710,	0.9064 s / batch. (data: 9.59e-04). ETA=11:28:50, max mem: 30.7 GB 
[11/23 19:43:12 visual_prompt]: 	Training 400/553. train loss: 0.0540,	0.9558 s / batch. (data: 3.42e-02). ETA=12:04:46, max mem: 30.7 GB 
[11/23 19:44:46 visual_prompt]: 	Training 500/553. train loss: 0.3724,	0.9243 s / batch. (data: 7.42e-04). ETA=11:39:20, max mem: 30.7 GB 
[11/23 19:45:35 visual_prompt]: Epoch 18 / 100: avg data time: 2.42e-02, avg batch time: 0.9519, average train loss: 0.4889
[11/23 19:46:30 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3038, average loss: 0.8196
[11/23 19:46:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.60	
[11/23 19:46:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/23 19:48:12 visual_prompt]: 	Training 100/553. train loss: 0.0493,	0.9435 s / batch. (data: 7.92e-04). ETA=11:51:30, max mem: 30.7 GB 
[11/23 19:49:46 visual_prompt]: 	Training 200/553. train loss: 0.7044,	0.9200 s / batch. (data: 2.84e-04). ETA=11:32:15, max mem: 30.7 GB 
[11/23 19:51:19 visual_prompt]: 	Training 300/553. train loss: 0.3607,	0.9122 s / batch. (data: 5.36e-03). ETA=11:24:49, max mem: 30.7 GB 
[11/23 19:52:53 visual_prompt]: 	Training 400/553. train loss: 0.1564,	0.9400 s / batch. (data: 7.36e-04). ETA=11:44:09, max mem: 30.7 GB 
[11/23 19:54:26 visual_prompt]: 	Training 500/553. train loss: 0.0848,	0.9262 s / batch. (data: 7.48e-04). ETA=11:32:16, max mem: 30.7 GB 
[11/23 19:55:16 visual_prompt]: Epoch 19 / 100: avg data time: 2.27e-02, avg batch time: 0.9508, average train loss: 0.5099
[11/23 19:56:11 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3039, average loss: 0.8225
[11/23 19:56:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.34	
[11/23 19:56:11 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/23 19:57:55 visual_prompt]: 	Training 100/553. train loss: 0.5613,	0.9320 s / batch. (data: 2.46e-04). ETA=11:34:13, max mem: 30.7 GB 
[11/23 19:59:28 visual_prompt]: 	Training 200/553. train loss: 0.3483,	0.9372 s / batch. (data: 2.71e-04). ETA=11:36:31, max mem: 30.7 GB 
[11/23 20:01:01 visual_prompt]: 	Training 300/553. train loss: 0.1435,	0.9222 s / batch. (data: 7.51e-04). ETA=11:23:50, max mem: 30.7 GB 
[11/23 20:02:35 visual_prompt]: 	Training 400/553. train loss: 1.0330,	0.9461 s / batch. (data: 7.41e-04). ETA=11:40:01, max mem: 30.7 GB 
[11/23 20:04:08 visual_prompt]: 	Training 500/553. train loss: 0.5676,	0.9400 s / batch. (data: 2.62e-04). ETA=11:33:56, max mem: 30.7 GB 
[11/23 20:04:58 visual_prompt]: Epoch 20 / 100: avg data time: 2.45e-02, avg batch time: 0.9530, average train loss: 0.4536
[11/23 20:05:53 visual_prompt]: Inference (val):avg data time: 6.23e-04, avg batch time: 0.3060, average loss: 0.8259
[11/23 20:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 62.26	
[11/23 20:05:53 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/23 20:07:36 visual_prompt]: 	Training 100/553. train loss: 0.0365,	0.9449 s / batch. (data: 2.52e-04). ETA=11:35:07, max mem: 30.7 GB 
[11/23 20:09:10 visual_prompt]: 	Training 200/553. train loss: 0.6019,	0.9481 s / batch. (data: 7.35e-04). ETA=11:35:53, max mem: 30.7 GB 
[11/23 20:10:43 visual_prompt]: 	Training 300/553. train loss: 0.4397,	0.9279 s / batch. (data: 7.87e-03). ETA=11:19:33, max mem: 30.7 GB 
[11/23 20:12:17 visual_prompt]: 	Training 400/553. train loss: 0.7116,	0.9259 s / batch. (data: 8.34e-03). ETA=11:16:32, max mem: 30.7 GB 
[11/23 20:13:50 visual_prompt]: 	Training 500/553. train loss: 0.0367,	0.9236 s / batch. (data: 4.47e-04). ETA=11:13:16, max mem: 30.7 GB 
[11/23 20:14:40 visual_prompt]: Epoch 21 / 100: avg data time: 2.31e-02, avg batch time: 0.9524, average train loss: 0.3785
[11/23 20:15:35 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3046, average loss: 0.8350
[11/23 20:15:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.33	
[11/23 20:15:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/23 20:17:16 visual_prompt]: 	Training 100/553. train loss: 0.7783,	0.9529 s / batch. (data: 5.34e-03). ETA=11:32:14, max mem: 30.7 GB 
[11/23 20:18:50 visual_prompt]: 	Training 200/553. train loss: 0.2944,	0.9350 s / batch. (data: 2.74e-04). ETA=11:17:41, max mem: 30.7 GB 
[11/23 20:20:23 visual_prompt]: 	Training 300/553. train loss: 0.0133,	0.9356 s / batch. (data: 5.38e-03). ETA=11:16:31, max mem: 30.7 GB 
[11/23 20:21:57 visual_prompt]: 	Training 400/553. train loss: 0.2648,	0.9377 s / batch. (data: 1.05e-02). ETA=11:16:32, max mem: 30.7 GB 
[11/23 20:23:30 visual_prompt]: 	Training 500/553. train loss: 0.2110,	0.9520 s / batch. (data: 7.52e-04). ETA=11:25:11, max mem: 30.7 GB 
[11/23 20:24:19 visual_prompt]: Epoch 22 / 100: avg data time: 2.04e-02, avg batch time: 0.9487, average train loss: 0.3436
[11/23 20:25:14 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3046, average loss: 1.1898
[11/23 20:25:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.87	
[11/23 20:25:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/23 20:26:57 visual_prompt]: 	Training 100/553. train loss: 0.6514,	0.9407 s / batch. (data: 1.04e-02). ETA=11:14:40, max mem: 30.7 GB 
[11/23 20:28:30 visual_prompt]: 	Training 200/553. train loss: 0.3580,	0.9096 s / batch. (data: 2.59e-04). ETA=10:50:51, max mem: 30.7 GB 
[11/23 20:30:04 visual_prompt]: 	Training 300/553. train loss: 0.1595,	0.9251 s / batch. (data: 5.34e-03). ETA=11:00:25, max mem: 30.7 GB 
[11/23 20:31:37 visual_prompt]: 	Training 400/553. train loss: 0.1485,	0.9145 s / batch. (data: 2.73e-04). ETA=10:51:22, max mem: 30.7 GB 
[11/23 20:33:11 visual_prompt]: 	Training 500/553. train loss: 0.3109,	0.9407 s / batch. (data: 2.45e-04). ETA=11:08:24, max mem: 30.7 GB 
[11/23 20:34:00 visual_prompt]: Epoch 23 / 100: avg data time: 2.24e-02, avg batch time: 0.9508, average train loss: 0.3161
[11/23 20:34:55 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3052, average loss: 1.1623
[11/23 20:34:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 63.51	
[11/23 20:34:55 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/23 20:36:38 visual_prompt]: 	Training 100/553. train loss: 0.1409,	0.9411 s / batch. (data: 5.83e-03). ETA=11:06:19, max mem: 30.7 GB 
[11/23 20:38:11 visual_prompt]: 	Training 200/553. train loss: 0.0970,	0.9292 s / batch. (data: 1.40e-02). ETA=10:56:21, max mem: 30.7 GB 
[11/23 20:39:45 visual_prompt]: 	Training 300/553. train loss: 0.2481,	0.9244 s / batch. (data: 2.79e-04). ETA=10:51:23, max mem: 30.7 GB 
[11/23 20:41:18 visual_prompt]: 	Training 400/553. train loss: 0.6480,	0.9374 s / batch. (data: 2.93e-04). ETA=10:59:00, max mem: 30.7 GB 
[11/23 20:42:52 visual_prompt]: 	Training 500/553. train loss: 0.1112,	0.9170 s / batch. (data: 2.80e-04). ETA=10:43:09, max mem: 30.7 GB 
[11/23 20:43:41 visual_prompt]: Epoch 24 / 100: avg data time: 2.25e-02, avg batch time: 0.9511, average train loss: 0.2976
[11/23 20:44:36 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3059, average loss: 1.4567
[11/23 20:44:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 65.35	
[11/23 20:44:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/23 20:46:16 visual_prompt]: 	Training 100/553. train loss: 1.1915,	0.9618 s / batch. (data: 5.44e-03). ETA=11:12:07, max mem: 30.7 GB 
[11/23 20:47:50 visual_prompt]: 	Training 200/553. train loss: 0.1967,	0.9292 s / batch. (data: 1.09e-02). ETA=10:47:47, max mem: 30.7 GB 
[11/23 20:49:23 visual_prompt]: 	Training 300/553. train loss: 1.2705,	0.9271 s / batch. (data: 5.33e-03). ETA=10:44:45, max mem: 30.7 GB 
[11/23 20:50:57 visual_prompt]: 	Training 400/553. train loss: 0.0499,	0.9424 s / batch. (data: 6.36e-03). ETA=10:53:50, max mem: 30.7 GB 
[11/23 20:52:30 visual_prompt]: 	Training 500/553. train loss: 0.0463,	0.9158 s / batch. (data: 2.80e-04). ETA=10:33:52, max mem: 30.7 GB 
[11/23 20:53:20 visual_prompt]: Epoch 25 / 100: avg data time: 1.78e-02, avg batch time: 0.9471, average train loss: 0.2696
[11/23 20:54:14 visual_prompt]: Inference (val):avg data time: 1.95e-04, avg batch time: 0.3062, average loss: 1.3148
[11/23 20:54:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 60.40	
[11/23 20:54:14 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/23 20:55:58 visual_prompt]: 	Training 100/553. train loss: 0.1006,	0.9405 s / batch. (data: 7.05e-04). ETA=10:48:33, max mem: 30.7 GB 
[11/23 20:57:31 visual_prompt]: 	Training 200/553. train loss: 0.0068,	0.9513 s / batch. (data: 5.84e-03). ETA=10:54:25, max mem: 30.7 GB 
[11/23 20:59:05 visual_prompt]: 	Training 300/553. train loss: 0.0049,	0.9172 s / batch. (data: 2.16e-04). ETA=10:29:26, max mem: 30.7 GB 
[11/23 21:00:38 visual_prompt]: 	Training 400/553. train loss: 0.6005,	0.9406 s / batch. (data: 7.49e-04). ETA=10:43:56, max mem: 30.7 GB 
[11/23 21:02:11 visual_prompt]: 	Training 500/553. train loss: 0.2315,	0.9250 s / batch. (data: 5.81e-03). ETA=10:31:40, max mem: 30.7 GB 
[11/23 21:03:01 visual_prompt]: Epoch 26 / 100: avg data time: 2.25e-02, avg batch time: 0.9513, average train loss: 0.2165
[11/23 21:03:55 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3037, average loss: 1.0268
[11/23 21:03:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 62.28	
[11/23 21:03:55 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/23 21:05:37 visual_prompt]: 	Training 100/553. train loss: 0.0088,	0.9262 s / batch. (data: 7.52e-04). ETA=10:30:10, max mem: 30.7 GB 
[11/23 21:07:11 visual_prompt]: 	Training 200/553. train loss: 0.0959,	0.9484 s / batch. (data: 8.33e-04). ETA=10:43:42, max mem: 30.7 GB 
[11/23 21:08:44 visual_prompt]: 	Training 300/553. train loss: 0.3526,	0.9283 s / batch. (data: 5.46e-03). ETA=10:28:27, max mem: 30.7 GB 
[11/23 21:10:18 visual_prompt]: 	Training 400/553. train loss: 0.2413,	0.9124 s / batch. (data: 5.98e-03). ETA=10:16:10, max mem: 30.7 GB 
[11/23 21:11:51 visual_prompt]: 	Training 500/553. train loss: 0.0361,	0.9436 s / batch. (data: 7.61e-03). ETA=10:35:43, max mem: 30.7 GB 
[11/23 21:12:41 visual_prompt]: Epoch 27 / 100: avg data time: 2.01e-02, avg batch time: 0.9493, average train loss: 0.2112
[11/23 21:13:35 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3060, average loss: 1.4783
[11/23 21:13:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 61.63	
[11/23 21:13:35 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/23 21:15:17 visual_prompt]: 	Training 100/553. train loss: 0.0647,	0.9218 s / batch. (data: 7.13e-04). ETA=10:18:38, max mem: 30.7 GB 
[11/23 21:16:51 visual_prompt]: 	Training 200/553. train loss: 0.0724,	0.9537 s / batch. (data: 5.38e-03). ETA=10:38:27, max mem: 30.7 GB 
[11/23 21:18:25 visual_prompt]: 	Training 300/553. train loss: 0.3548,	0.9183 s / batch. (data: 4.05e-03). ETA=10:13:16, max mem: 30.7 GB 
[11/23 21:19:58 visual_prompt]: 	Training 400/553. train loss: 0.3550,	0.9156 s / batch. (data: 7.90e-04). ETA=10:09:54, max mem: 30.7 GB 
[11/23 21:21:32 visual_prompt]: 	Training 500/553. train loss: 0.0058,	0.9354 s / batch. (data: 5.84e-03). ETA=10:21:31, max mem: 30.7 GB 
[11/23 21:22:21 visual_prompt]: Epoch 28 / 100: avg data time: 2.08e-02, avg batch time: 0.9505, average train loss: 0.1935
[11/23 21:23:16 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3059, average loss: 1.1671
[11/23 21:23:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.38	
[11/23 21:23:16 visual_prompt]: Training 29 / 100 epoch, with learning rate 8.622126023955446e-05
[11/23 21:24:58 visual_prompt]: 	Training 100/553. train loss: 0.1035,	0.9192 s / batch. (data: 2.42e-04). ETA=10:08:28, max mem: 30.7 GB 
[11/23 21:26:32 visual_prompt]: 	Training 200/553. train loss: 0.6491,	0.9663 s / batch. (data: 7.57e-04). ETA=10:37:58, max mem: 30.7 GB 
[11/23 21:28:06 visual_prompt]: 	Training 300/553. train loss: 0.0167,	0.9159 s / batch. (data: 5.39e-03). ETA=10:03:12, max mem: 30.7 GB 
[11/23 21:29:40 visual_prompt]: 	Training 400/553. train loss: 0.1140,	0.9245 s / batch. (data: 2.76e-04). ETA=10:07:18, max mem: 30.7 GB 
[11/23 21:31:13 visual_prompt]: 	Training 500/553. train loss: 0.2071,	0.9409 s / batch. (data: 1.09e-02). ETA=10:16:30, max mem: 30.7 GB 
[11/23 21:32:02 visual_prompt]: Epoch 29 / 100: avg data time: 2.19e-02, avg batch time: 0.9508, average train loss: 0.1628
[11/23 21:32:57 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3034, average loss: 1.4946
[11/23 21:32:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 60.16	
[11/23 21:32:57 visual_prompt]: Stopping early.
[11/23 21:32:57 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 21:32:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 21:32:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/23 21:32:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/23 21:32:57 visual_prompt]: Training with config:
[11/23 21:32:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0001_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/23 21:32:57 visual_prompt]: Loading training data...
[11/23 21:32:57 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 21:32:57 visual_prompt]: Loading validation data...
[11/23 21:32:57 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 21:32:57 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 21:32:59 visual_prompt]: Enable all parameters update during training
[11/23 21:32:59 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/23 21:32:59 visual_prompt]: tuned percent:100.000
[11/23 21:32:59 visual_prompt]: Device used for model: 0
[11/23 21:32:59 visual_prompt]: Setting up Evaluator...
[11/23 21:32:59 visual_prompt]: Setting up Trainer...
[11/23 21:32:59 visual_prompt]: 	Setting up the optimizer...
[11/23 21:32:59 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 21:34:40 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9095 s / batch. (data: 5.48e-03). ETA=13:56:41, max mem: 30.7 GB 
[11/23 21:36:13 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9400 s / batch. (data: 3.94e-03). ETA=14:23:12, max mem: 30.7 GB 
[11/23 21:37:46 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9484 s / batch. (data: 5.41e-03). ETA=14:29:22, max mem: 30.7 GB 
[11/23 21:39:20 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9270 s / batch. (data: 3.06e-04). ETA=14:08:10, max mem: 30.7 GB 
[11/23 21:40:53 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9439 s / batch. (data: 5.92e-03). ETA=14:22:05, max mem: 30.7 GB 
[11/23 21:41:42 visual_prompt]: Epoch 1 / 100: avg data time: 1.95e-02, avg batch time: 0.9457, average train loss: 7.6130
[11/23 21:42:37 visual_prompt]: Inference (val):avg data time: 1.69e-04, avg batch time: 0.3074, average loss: 6.9126
[11/23 21:42:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/23 21:42:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/23 21:44:18 visual_prompt]: 	Training 100/553. train loss: 1.0994,	0.9033 s / batch. (data: 5.43e-03). ETA=13:42:40, max mem: 30.7 GB 
[11/23 21:45:51 visual_prompt]: 	Training 200/553. train loss: 0.8613,	0.9160 s / batch. (data: 3.19e-04). ETA=13:52:46, max mem: 30.7 GB 
[11/23 21:47:25 visual_prompt]: 	Training 300/553. train loss: 1.2953,	0.9313 s / batch. (data: 2.92e-04). ETA=14:05:05, max mem: 30.7 GB 
[11/23 21:48:58 visual_prompt]: 	Training 400/553. train loss: 1.0671,	0.9113 s / batch. (data: 4.01e-03). ETA=13:45:25, max mem: 30.7 GB 
[11/23 21:50:32 visual_prompt]: 	Training 500/553. train loss: 1.1454,	0.9496 s / batch. (data: 1.09e-02). ETA=14:18:35, max mem: 30.7 GB 
[11/23 21:51:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.94e-02, avg batch time: 0.9467, average train loss: 0.9770
[11/23 21:52:15 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.3088, average loss: 0.7126
[11/23 21:52:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 54.46	
[11/23 21:52:15 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/23 21:53:57 visual_prompt]: 	Training 100/553. train loss: 0.9801,	0.9274 s / batch. (data: 6.87e-04). ETA=13:56:07, max mem: 30.7 GB 
[11/23 21:55:31 visual_prompt]: 	Training 200/553. train loss: 0.8761,	0.9240 s / batch. (data: 2.94e-04). ETA=13:51:30, max mem: 30.7 GB 
[11/23 21:57:03 visual_prompt]: 	Training 300/553. train loss: 0.7740,	0.9403 s / batch. (data: 1.65e-02). ETA=14:04:36, max mem: 30.7 GB 
[11/23 21:58:37 visual_prompt]: 	Training 400/553. train loss: 0.2584,	0.9520 s / batch. (data: 7.96e-04). ETA=14:13:32, max mem: 30.7 GB 
[11/23 22:00:10 visual_prompt]: 	Training 500/553. train loss: 0.5309,	0.9557 s / batch. (data: 5.42e-03). ETA=14:15:15, max mem: 30.7 GB 
[11/23 22:00:59 visual_prompt]: Epoch 3 / 100: avg data time: 2.14e-02, avg batch time: 0.9464, average train loss: 0.7789
[11/23 22:01:54 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3081, average loss: 1.1125
[11/23 22:01:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.95	
[11/23 22:01:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/23 22:03:38 visual_prompt]: 	Training 100/553. train loss: 0.6202,	0.9320 s / batch. (data: 7.88e-04). ETA=13:51:39, max mem: 30.7 GB 
[11/23 22:05:11 visual_prompt]: 	Training 200/553. train loss: 1.3336,	0.8999 s / batch. (data: 3.17e-04). ETA=13:21:32, max mem: 30.7 GB 
[11/23 22:06:45 visual_prompt]: 	Training 300/553. train loss: 0.6464,	0.9140 s / batch. (data: 7.74e-04). ETA=13:32:35, max mem: 30.7 GB 
[11/23 22:08:18 visual_prompt]: 	Training 400/553. train loss: 0.6435,	0.9175 s / batch. (data: 3.07e-04). ETA=13:34:08, max mem: 30.7 GB 
[11/23 22:09:51 visual_prompt]: 	Training 500/553. train loss: 1.2257,	0.9440 s / batch. (data: 8.10e-04). ETA=13:56:05, max mem: 30.7 GB 
[11/23 22:10:41 visual_prompt]: Epoch 4 / 100: avg data time: 2.66e-02, avg batch time: 0.9527, average train loss: 0.7770
[11/23 22:11:36 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.3064, average loss: 0.7835
[11/23 22:11:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 58.30	
[11/23 22:11:36 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/23 22:13:16 visual_prompt]: 	Training 100/553. train loss: 0.5623,	0.9069 s / batch. (data: 7.95e-03). ETA=13:20:52, max mem: 30.7 GB 
[11/23 22:14:50 visual_prompt]: 	Training 200/553. train loss: 0.3947,	0.9120 s / batch. (data: 3.12e-04). ETA=13:23:54, max mem: 30.7 GB 
[11/23 22:16:23 visual_prompt]: 	Training 300/553. train loss: 1.0369,	0.9487 s / batch. (data: 1.86e-04). ETA=13:54:40, max mem: 30.7 GB 
[11/23 22:17:57 visual_prompt]: 	Training 400/553. train loss: 0.7966,	0.9320 s / batch. (data: 3.09e-04). ETA=13:38:25, max mem: 30.7 GB 
[11/23 22:19:30 visual_prompt]: 	Training 500/553. train loss: 1.2318,	0.9587 s / batch. (data: 7.22e-04). ETA=14:00:18, max mem: 30.7 GB 
[11/23 22:20:19 visual_prompt]: Epoch 5 / 100: avg data time: 1.89e-02, avg batch time: 0.9463, average train loss: 0.7455
[11/23 22:21:14 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3071, average loss: 0.6973
[11/23 22:21:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 57.91	
[11/23 22:21:14 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/23 22:22:56 visual_prompt]: 	Training 100/553. train loss: 0.4591,	0.9200 s / batch. (data: 3.15e-04). ETA=13:24:02, max mem: 30.7 GB 
[11/23 22:24:30 visual_prompt]: 	Training 200/553. train loss: 0.6106,	0.9360 s / batch. (data: 2.90e-04). ETA=13:36:25, max mem: 30.7 GB 
[11/23 22:26:03 visual_prompt]: 	Training 300/553. train loss: 0.7239,	0.9240 s / batch. (data: 7.16e-04). ETA=13:24:26, max mem: 30.7 GB 
[11/23 22:27:36 visual_prompt]: 	Training 400/553. train loss: 0.9670,	0.9334 s / batch. (data: 2.94e-04). ETA=13:31:00, max mem: 30.7 GB 
[11/23 22:29:10 visual_prompt]: 	Training 500/553. train loss: 1.0792,	0.9042 s / batch. (data: 2.82e-04). ETA=13:04:10, max mem: 30.7 GB 
[11/23 22:29:59 visual_prompt]: Epoch 6 / 100: avg data time: 2.27e-02, avg batch time: 0.9491, average train loss: 0.7184
[11/23 22:30:54 visual_prompt]: Inference (val):avg data time: 2.08e-04, avg batch time: 0.3072, average loss: 0.7348
[11/23 22:30:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.57	
[11/23 22:30:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/23 22:32:38 visual_prompt]: 	Training 100/553. train loss: 0.5450,	0.9308 s / batch. (data: 2.72e-04). ETA=13:24:53, max mem: 30.7 GB 
[11/23 22:34:12 visual_prompt]: 	Training 200/553. train loss: 0.7955,	0.9368 s / batch. (data: 5.84e-03). ETA=13:28:28, max mem: 30.7 GB 
[11/23 22:35:45 visual_prompt]: 	Training 300/553. train loss: 0.6339,	0.9237 s / batch. (data: 2.89e-04). ETA=13:15:37, max mem: 30.7 GB 
[11/23 22:37:18 visual_prompt]: 	Training 400/553. train loss: 0.5996,	0.9321 s / batch. (data: 2.26e-04). ETA=13:21:17, max mem: 30.7 GB 
[11/23 22:38:51 visual_prompt]: 	Training 500/553. train loss: 0.5233,	0.9262 s / batch. (data: 5.41e-03). ETA=13:14:41, max mem: 30.7 GB 
[11/23 22:39:40 visual_prompt]: Epoch 7 / 100: avg data time: 2.82e-02, avg batch time: 0.9524, average train loss: 0.7174
[11/23 22:40:35 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3059, average loss: 0.6615
[11/23 22:40:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 62.54	
[11/23 22:40:35 visual_prompt]: Best epoch 7: best metric: -0.661
[11/23 22:40:35 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/23 22:42:17 visual_prompt]: 	Training 100/553. train loss: 0.6047,	0.9480 s / batch. (data: 7.99e-04). ETA=13:30:59, max mem: 30.7 GB 
[11/23 22:43:51 visual_prompt]: 	Training 200/553. train loss: 0.5789,	0.9600 s / batch. (data: 7.73e-04). ETA=13:39:38, max mem: 30.7 GB 
[11/23 22:45:24 visual_prompt]: 	Training 300/553. train loss: 0.6418,	0.9268 s / batch. (data: 7.56e-04). ETA=13:09:48, max mem: 30.7 GB 
[11/23 22:46:57 visual_prompt]: 	Training 400/553. train loss: 0.6005,	0.9118 s / batch. (data: 2.74e-04). ETA=12:55:26, max mem: 30.7 GB 
[11/23 22:48:30 visual_prompt]: 	Training 500/553. train loss: 0.5929,	0.9279 s / batch. (data: 5.91e-03). ETA=13:07:38, max mem: 30.7 GB 
[11/23 22:49:19 visual_prompt]: Epoch 8 / 100: avg data time: 2.31e-02, avg batch time: 0.9480, average train loss: 0.7052
[11/23 22:50:13 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3030, average loss: 0.6638
[11/23 22:50:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 62.63	
[11/23 22:50:13 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/23 22:51:57 visual_prompt]: 	Training 100/553. train loss: 0.6842,	0.9244 s / batch. (data: 7.51e-04). ETA=13:02:15, max mem: 30.7 GB 
[11/23 22:53:30 visual_prompt]: 	Training 200/553. train loss: 1.0289,	0.9325 s / batch. (data: 3.06e-04). ETA=13:07:34, max mem: 30.7 GB 
[11/23 22:55:03 visual_prompt]: 	Training 300/553. train loss: 0.6224,	0.9200 s / batch. (data: 2.68e-04). ETA=12:55:28, max mem: 30.7 GB 
[11/23 22:56:36 visual_prompt]: 	Training 400/553. train loss: 0.4164,	0.9113 s / batch. (data: 2.87e-04). ETA=12:46:39, max mem: 30.7 GB 
[11/23 22:58:09 visual_prompt]: 	Training 500/553. train loss: 1.0589,	0.9320 s / batch. (data: 8.41e-04). ETA=13:02:28, max mem: 30.7 GB 
[11/23 22:58:58 visual_prompt]: Epoch 9 / 100: avg data time: 2.43e-02, avg batch time: 0.9483, average train loss: 0.6978
[11/23 22:59:52 visual_prompt]: Inference (val):avg data time: 4.07e-04, avg batch time: 0.3047, average loss: 0.6695
[11/23 22:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 64.05	
[11/23 22:59:52 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/23 23:01:33 visual_prompt]: 	Training 100/553. train loss: 0.9260,	0.9200 s / batch. (data: 9.23e-04). ETA=12:50:04, max mem: 30.7 GB 
[11/23 23:03:06 visual_prompt]: 	Training 200/553. train loss: 0.6234,	0.9317 s / batch. (data: 1.05e-02). ETA=12:58:21, max mem: 30.7 GB 
[11/23 23:04:39 visual_prompt]: 	Training 300/553. train loss: 0.8365,	0.9170 s / batch. (data: 5.43e-03). ETA=12:44:32, max mem: 30.7 GB 
[11/23 23:06:12 visual_prompt]: 	Training 400/553. train loss: 0.7391,	0.9360 s / batch. (data: 7.28e-04). ETA=12:58:49, max mem: 30.7 GB 
[11/23 23:07:45 visual_prompt]: 	Training 500/553. train loss: 0.7374,	0.9238 s / batch. (data: 7.97e-04). ETA=12:47:05, max mem: 30.7 GB 
[11/23 23:08:35 visual_prompt]: Epoch 10 / 100: avg data time: 1.89e-02, avg batch time: 0.9447, average train loss: 0.6652
[11/23 23:09:28 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3050, average loss: 0.6432
[11/23 23:09:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 65.73	
[11/23 23:09:28 visual_prompt]: Best epoch 10: best metric: -0.643
[11/23 23:09:28 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/23 23:11:11 visual_prompt]: 	Training 100/553. train loss: 0.8946,	0.9520 s / batch. (data: 1.20e-02). ETA=13:08:06, max mem: 30.7 GB 
[11/23 23:12:44 visual_prompt]: 	Training 200/553. train loss: 1.0529,	0.9390 s / batch. (data: 3.87e-02). ETA=12:55:46, max mem: 30.7 GB 
[11/23 23:14:17 visual_prompt]: 	Training 300/553. train loss: 0.4280,	0.9446 s / batch. (data: 5.81e-03). ETA=12:58:49, max mem: 30.7 GB 
[11/23 23:15:50 visual_prompt]: 	Training 400/553. train loss: 0.5537,	0.9225 s / batch. (data: 2.67e-04). ETA=12:39:05, max mem: 30.7 GB 
[11/23 23:17:24 visual_prompt]: 	Training 500/553. train loss: 0.6233,	0.9255 s / batch. (data: 7.88e-04). ETA=12:39:59, max mem: 30.7 GB 
[11/23 23:18:13 visual_prompt]: Epoch 11 / 100: avg data time: 2.20e-02, avg batch time: 0.9477, average train loss: 0.6468
[11/23 23:19:07 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.3047, average loss: 0.6584
[11/23 23:19:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 65.18	
[11/23 23:19:07 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/23 23:20:45 visual_prompt]: 	Training 100/553. train loss: 0.3570,	0.9376 s / batch. (data: 1.55e-02). ETA=12:47:33, max mem: 30.7 GB 
[11/23 23:22:19 visual_prompt]: 	Training 200/553. train loss: 0.6593,	0.9771 s / batch. (data: 4.59e-02). ETA=13:18:15, max mem: 30.7 GB 
[11/23 23:23:52 visual_prompt]: 	Training 300/553. train loss: 0.8788,	0.9200 s / batch. (data: 7.84e-04). ETA=12:30:04, max mem: 30.7 GB 
[11/23 23:25:26 visual_prompt]: 	Training 400/553. train loss: 0.6730,	0.9318 s / batch. (data: 2.94e-04). ETA=12:38:07, max mem: 30.7 GB 
[11/23 23:26:59 visual_prompt]: 	Training 500/553. train loss: 0.4351,	0.9544 s / batch. (data: 7.18e-04). ETA=12:54:56, max mem: 30.7 GB 
[11/23 23:27:48 visual_prompt]: Epoch 12 / 100: avg data time: 1.52e-02, avg batch time: 0.9431, average train loss: 0.6785
[11/23 23:28:45 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3053, average loss: 1.0592
[11/23 23:28:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 64.63	
[11/23 23:28:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/23 23:30:28 visual_prompt]: 	Training 100/553. train loss: 0.5274,	0.9157 s / batch. (data: 3.09e-04). ETA=12:21:08, max mem: 30.7 GB 
[11/23 23:32:01 visual_prompt]: 	Training 200/553. train loss: 0.7441,	0.9406 s / batch. (data: 2.72e-04). ETA=12:39:46, max mem: 30.7 GB 
[11/23 23:33:34 visual_prompt]: 	Training 300/553. train loss: 1.0976,	0.9193 s / batch. (data: 2.78e-04). ETA=12:21:00, max mem: 30.7 GB 
[11/23 23:35:08 visual_prompt]: 	Training 400/553. train loss: 0.6167,	0.9520 s / batch. (data: 7.59e-04). ETA=12:45:45, max mem: 30.7 GB 
[11/23 23:36:41 visual_prompt]: 	Training 500/553. train loss: 0.4876,	0.9200 s / batch. (data: 6.91e-04). ETA=12:18:32, max mem: 30.7 GB 
[11/23 23:37:30 visual_prompt]: Epoch 13 / 100: avg data time: 2.31e-02, avg batch time: 0.9488, average train loss: 0.6520
[11/23 23:38:25 visual_prompt]: Inference (val):avg data time: 2.65e-04, avg batch time: 0.3059, average loss: 0.7203
[11/23 23:38:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 63.71	
[11/23 23:38:25 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/23 23:40:08 visual_prompt]: 	Training 100/553. train loss: 0.4660,	0.9494 s / batch. (data: 1.55e-02). ETA=12:39:41, max mem: 30.7 GB 
[11/23 23:41:41 visual_prompt]: 	Training 200/553. train loss: 0.4965,	0.9360 s / batch. (data: 7.83e-04). ETA=12:27:24, max mem: 30.7 GB 
[11/23 23:43:14 visual_prompt]: 	Training 300/553. train loss: 0.8491,	0.9120 s / batch. (data: 5.42e-03). ETA=12:06:42, max mem: 30.7 GB 
[11/23 23:44:48 visual_prompt]: 	Training 400/553. train loss: 1.2004,	0.9320 s / batch. (data: 7.70e-04). ETA=12:21:05, max mem: 30.7 GB 
[11/23 23:46:21 visual_prompt]: 	Training 500/553. train loss: 0.6208,	0.9385 s / batch. (data: 1.60e-02). ETA=12:24:41, max mem: 30.7 GB 
[11/23 23:47:10 visual_prompt]: Epoch 14 / 100: avg data time: 2.40e-02, avg batch time: 0.9495, average train loss: 0.6444
[11/23 23:48:04 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3062, average loss: 0.7025
[11/23 23:48:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.23	
[11/23 23:48:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/23 23:49:48 visual_prompt]: 	Training 100/553. train loss: 0.4465,	0.9248 s / batch. (data: 5.41e-03). ETA=12:11:31, max mem: 30.7 GB 
[11/23 23:51:21 visual_prompt]: 	Training 200/553. train loss: 0.6569,	0.9351 s / batch. (data: 2.30e-02). ETA=12:18:04, max mem: 30.7 GB 
[11/23 23:52:55 visual_prompt]: 	Training 300/553. train loss: 0.5769,	0.9371 s / batch. (data: 5.37e-03). ETA=12:18:05, max mem: 30.7 GB 
[11/23 23:54:28 visual_prompt]: 	Training 400/553. train loss: 0.1898,	0.9404 s / batch. (data: 1.09e-02). ETA=12:19:06, max mem: 30.7 GB 
[11/23 23:56:01 visual_prompt]: 	Training 500/553. train loss: 0.9063,	0.9328 s / batch. (data: 1.82e-02). ETA=12:11:34, max mem: 30.7 GB 
[11/23 23:56:50 visual_prompt]: Epoch 15 / 100: avg data time: 2.46e-02, avg batch time: 0.9510, average train loss: 0.6188
[11/23 23:57:45 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3057, average loss: 0.7234
[11/23 23:57:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.56	
[11/23 23:57:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/23 23:59:27 visual_prompt]: 	Training 100/553. train loss: 0.6100,	0.9623 s / batch. (data: 1.55e-02). ETA=12:32:18, max mem: 30.7 GB 
[11/24 00:01:00 visual_prompt]: 	Training 200/553. train loss: 0.1989,	0.9400 s / batch. (data: 5.92e-03). ETA=12:13:16, max mem: 30.7 GB 
[11/24 00:02:34 visual_prompt]: 	Training 300/553. train loss: 0.7329,	0.9195 s / batch. (data: 7.82e-04). ETA=11:55:46, max mem: 30.7 GB 
[11/24 00:04:07 visual_prompt]: 	Training 400/553. train loss: 0.4411,	0.9400 s / batch. (data: 7.96e-04). ETA=12:10:09, max mem: 30.7 GB 
[11/24 00:05:40 visual_prompt]: 	Training 500/553. train loss: 0.4815,	0.9366 s / batch. (data: 5.44e-03). ETA=12:05:58, max mem: 30.7 GB 
[11/24 00:06:30 visual_prompt]: Epoch 16 / 100: avg data time: 2.06e-02, avg batch time: 0.9480, average train loss: 0.6146
[11/24 00:07:25 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3055, average loss: 0.7905
[11/24 00:07:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 65.83	
[11/24 00:07:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/24 00:09:05 visual_prompt]: 	Training 100/553. train loss: 0.5997,	0.9185 s / batch. (data: 2.77e-04). ETA=11:49:36, max mem: 30.7 GB 
[11/24 00:10:39 visual_prompt]: 	Training 200/553. train loss: 1.1659,	0.9240 s / batch. (data: 3.08e-04). ETA=11:52:17, max mem: 30.7 GB 
[11/24 00:12:12 visual_prompt]: 	Training 300/553. train loss: 1.1384,	0.9292 s / batch. (data: 1.31e-02). ETA=11:54:43, max mem: 30.7 GB 
[11/24 00:13:45 visual_prompt]: 	Training 400/553. train loss: 0.1082,	0.9445 s / batch. (data: 3.62e-02). ETA=12:04:54, max mem: 30.7 GB 
[11/24 00:15:19 visual_prompt]: 	Training 500/553. train loss: 0.6873,	0.9239 s / batch. (data: 2.76e-04). ETA=11:47:33, max mem: 30.7 GB 
[11/24 00:16:08 visual_prompt]: Epoch 17 / 100: avg data time: 1.90e-02, avg batch time: 0.9464, average train loss: 0.6117
[11/24 00:17:03 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.3055, average loss: 0.6799
[11/24 00:17:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 65.67	
[11/24 00:17:03 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/24 00:18:47 visual_prompt]: 	Training 100/553. train loss: 0.3352,	0.9470 s / batch. (data: 5.83e-03). ETA=12:02:51, max mem: 30.7 GB 
[11/24 00:20:20 visual_prompt]: 	Training 200/553. train loss: 0.8404,	0.9240 s / batch. (data: 2.72e-04). ETA=11:43:46, max mem: 30.7 GB 
[11/24 00:21:53 visual_prompt]: 	Training 300/553. train loss: 0.7034,	0.9099 s / batch. (data: 8.03e-03). ETA=11:31:32, max mem: 30.7 GB 
[11/24 00:23:27 visual_prompt]: 	Training 400/553. train loss: 0.4592,	0.9322 s / batch. (data: 2.36e-02). ETA=11:46:53, max mem: 30.7 GB 
[11/24 00:25:00 visual_prompt]: 	Training 500/553. train loss: 0.4856,	0.9360 s / batch. (data: 8.02e-03). ETA=11:48:12, max mem: 30.7 GB 
[11/24 00:25:49 visual_prompt]: Epoch 18 / 100: avg data time: 2.43e-02, avg batch time: 0.9511, average train loss: 0.5866
[11/24 00:26:44 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3053, average loss: 0.7391
[11/24 00:26:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.95	
[11/24 00:26:44 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/24 00:28:26 visual_prompt]: 	Training 100/553. train loss: 0.2465,	0.9410 s / batch. (data: 1.55e-02). ETA=11:49:35, max mem: 30.7 GB 
[11/24 00:29:59 visual_prompt]: 	Training 200/553. train loss: 0.5469,	0.9240 s / batch. (data: 2.89e-04). ETA=11:35:16, max mem: 30.7 GB 
[11/24 00:31:33 visual_prompt]: 	Training 300/553. train loss: 0.5150,	0.9400 s / batch. (data: 8.15e-04). ETA=11:45:42, max mem: 30.7 GB 
[11/24 00:33:07 visual_prompt]: 	Training 400/553. train loss: 0.2203,	0.9336 s / batch. (data: 7.99e-04). ETA=11:39:23, max mem: 30.7 GB 
[11/24 00:34:40 visual_prompt]: 	Training 500/553. train loss: 0.3046,	0.9240 s / batch. (data: 5.65e-03). ETA=11:30:37, max mem: 30.7 GB 
[11/24 00:35:29 visual_prompt]: Epoch 19 / 100: avg data time: 2.10e-02, avg batch time: 0.9491, average train loss: 0.5621
[11/24 00:36:24 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3050, average loss: 0.7893
[11/24 00:36:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.82	
[11/24 00:36:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/24 00:38:08 visual_prompt]: 	Training 100/553. train loss: 0.4355,	0.9360 s / batch. (data: 7.98e-03). ETA=11:37:13, max mem: 30.7 GB 
[11/24 00:39:41 visual_prompt]: 	Training 200/553. train loss: 0.4246,	0.9491 s / batch. (data: 7.39e-04). ETA=11:45:21, max mem: 30.7 GB 
[11/24 00:41:14 visual_prompt]: 	Training 300/553. train loss: 0.3094,	0.9280 s / batch. (data: 1.99e-02). ETA=11:28:09, max mem: 30.7 GB 
[11/24 00:42:48 visual_prompt]: 	Training 400/553. train loss: 0.6871,	0.9486 s / batch. (data: 7.84e-04). ETA=11:41:51, max mem: 30.7 GB 
[11/24 00:44:21 visual_prompt]: 	Training 500/553. train loss: 0.3088,	0.9520 s / batch. (data: 3.34e-04). ETA=11:42:46, max mem: 30.7 GB 
[11/24 00:45:10 visual_prompt]: Epoch 20 / 100: avg data time: 2.55e-02, avg batch time: 0.9512, average train loss: 0.5673
[11/24 00:46:05 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3045, average loss: 0.7081
[11/24 00:46:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 63.72	
[11/24 00:46:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/24 00:47:48 visual_prompt]: 	Training 100/553. train loss: 0.6014,	0.9336 s / batch. (data: 7.81e-04). ETA=11:26:49, max mem: 30.7 GB 
[11/24 00:49:22 visual_prompt]: 	Training 200/553. train loss: 0.4616,	0.9779 s / batch. (data: 7.98e-04). ETA=11:57:47, max mem: 30.7 GB 
[11/24 00:50:55 visual_prompt]: 	Training 300/553. train loss: 0.5500,	0.9520 s / batch. (data: 3.06e-04). ETA=11:37:11, max mem: 30.7 GB 
[11/24 00:52:28 visual_prompt]: 	Training 400/553. train loss: 0.6637,	0.9159 s / batch. (data: 7.91e-03). ETA=11:09:15, max mem: 30.7 GB 
[11/24 00:54:01 visual_prompt]: 	Training 500/553. train loss: 0.4731,	0.9277 s / batch. (data: 7.75e-04). ETA=11:16:15, max mem: 30.7 GB 
[11/24 00:54:50 visual_prompt]: Epoch 21 / 100: avg data time: 2.36e-02, avg batch time: 0.9498, average train loss: 0.5343
[11/24 00:55:45 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3068, average loss: 0.7959
[11/24 00:55:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 62.82	
[11/24 00:55:45 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/24 00:57:26 visual_prompt]: 	Training 100/553. train loss: 0.8527,	0.9400 s / batch. (data: 7.96e-03). ETA=11:22:52, max mem: 30.7 GB 
[11/24 00:58:59 visual_prompt]: 	Training 200/553. train loss: 0.9569,	0.9320 s / batch. (data: 3.48e-04). ETA=11:15:28, max mem: 30.7 GB 
[11/24 01:00:33 visual_prompt]: 	Training 300/553. train loss: 0.2971,	0.9236 s / batch. (data: 3.51e-04). ETA=11:07:51, max mem: 30.7 GB 
[11/24 01:02:06 visual_prompt]: 	Training 400/553. train loss: 0.5552,	0.9360 s / batch. (data: 3.05e-04). ETA=11:15:16, max mem: 30.7 GB 
[11/24 01:03:39 visual_prompt]: 	Training 500/553. train loss: 0.7553,	0.9160 s / batch. (data: 2.66e-04). ETA=10:59:20, max mem: 30.7 GB 
[11/24 01:04:28 visual_prompt]: Epoch 22 / 100: avg data time: 1.83e-02, avg batch time: 0.9458, average train loss: 0.5139
[11/24 01:05:23 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3063, average loss: 0.9259
[11/24 01:05:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 60.36	
[11/24 01:05:23 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/24 01:07:06 visual_prompt]: 	Training 100/553. train loss: 0.5713,	0.9200 s / batch. (data: 7.75e-04). ETA=10:59:52, max mem: 30.7 GB 
[11/24 01:08:39 visual_prompt]: 	Training 200/553. train loss: 1.0932,	0.9440 s / batch. (data: 7.91e-04). ETA=11:15:29, max mem: 30.7 GB 
[11/24 01:10:12 visual_prompt]: 	Training 300/553. train loss: 0.2435,	0.9560 s / batch. (data: 8.08e-04). ETA=11:22:29, max mem: 30.7 GB 
[11/24 01:11:46 visual_prompt]: 	Training 400/553. train loss: 0.1850,	0.9120 s / batch. (data: 2.88e-04). ETA=10:49:33, max mem: 30.7 GB 
[11/24 01:13:19 visual_prompt]: 	Training 500/553. train loss: 0.0501,	0.9360 s / batch. (data: 8.32e-04). ETA=11:05:03, max mem: 30.7 GB 
[11/24 01:14:08 visual_prompt]: Epoch 23 / 100: avg data time: 2.18e-02, avg batch time: 0.9489, average train loss: 0.5281
[11/24 01:15:03 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3072, average loss: 1.0627
[11/24 01:15:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 62.57	
[11/24 01:15:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/24 01:16:45 visual_prompt]: 	Training 100/553. train loss: 0.5460,	0.9320 s / batch. (data: 2.86e-04). ETA=10:59:52, max mem: 30.7 GB 
[11/24 01:18:19 visual_prompt]: 	Training 200/553. train loss: 0.5259,	0.9520 s / batch. (data: 2.63e-04). ETA=11:12:27, max mem: 30.7 GB 
[11/24 01:19:52 visual_prompt]: 	Training 300/553. train loss: 0.7034,	0.9089 s / batch. (data: 2.98e-04). ETA=10:40:27, max mem: 30.7 GB 
[11/24 01:21:25 visual_prompt]: 	Training 400/553. train loss: 0.5534,	0.9560 s / batch. (data: 6.21e-03). ETA=11:12:05, max mem: 30.7 GB 
[11/24 01:22:58 visual_prompt]: 	Training 500/553. train loss: 0.2597,	0.9120 s / batch. (data: 7.60e-04). ETA=10:39:38, max mem: 30.7 GB 
[11/24 01:23:47 visual_prompt]: Epoch 24 / 100: avg data time: 2.23e-02, avg batch time: 0.9485, average train loss: 0.5083
[11/24 01:24:42 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3061, average loss: 0.8711
[11/24 01:24:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 61.70	
[11/24 01:24:42 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/24 01:26:23 visual_prompt]: 	Training 100/553. train loss: 0.1565,	0.9280 s / batch. (data: 3.04e-04). ETA=10:48:28, max mem: 30.7 GB 
[11/24 01:27:56 visual_prompt]: 	Training 200/553. train loss: 0.2152,	0.9078 s / batch. (data: 2.39e-04). ETA=10:32:52, max mem: 30.7 GB 
[11/24 01:29:30 visual_prompt]: 	Training 300/553. train loss: 0.2566,	0.9515 s / batch. (data: 1.60e-02). ETA=11:01:44, max mem: 30.7 GB 
[11/24 01:31:03 visual_prompt]: 	Training 400/553. train loss: 0.7641,	0.9440 s / batch. (data: 2.83e-04). ETA=10:54:56, max mem: 30.7 GB 
[11/24 01:32:37 visual_prompt]: 	Training 500/553. train loss: 0.3120,	0.9440 s / batch. (data: 7.68e-04). ETA=10:53:23, max mem: 30.7 GB 
[11/24 01:33:26 visual_prompt]: Epoch 25 / 100: avg data time: 1.83e-02, avg batch time: 0.9467, average train loss: 0.4843
[11/24 01:34:22 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.3214, average loss: 0.8499
[11/24 01:34:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 63.37	
[11/24 01:34:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/24 01:36:04 visual_prompt]: 	Training 100/553. train loss: 0.1854,	0.9362 s / batch. (data: 2.44e-02). ETA=10:45:34, max mem: 30.7 GB 
[11/24 01:37:37 visual_prompt]: 	Training 200/553. train loss: 0.3236,	0.9212 s / batch. (data: 8.06e-04). ETA=10:33:42, max mem: 30.7 GB 
[11/24 01:39:11 visual_prompt]: 	Training 300/553. train loss: 0.1856,	0.9240 s / batch. (data: 1.20e-02). ETA=10:34:04, max mem: 30.7 GB 
[11/24 01:40:44 visual_prompt]: 	Training 400/553. train loss: 0.2804,	0.9149 s / batch. (data: 7.92e-04). ETA=10:26:20, max mem: 30.7 GB 
[11/24 01:42:17 visual_prompt]: 	Training 500/553. train loss: 0.3401,	0.9521 s / batch. (data: 7.49e-04). ETA=10:50:10, max mem: 30.7 GB 
[11/24 01:43:06 visual_prompt]: Epoch 26 / 100: avg data time: 2.21e-02, avg batch time: 0.9482, average train loss: 0.4589
[11/24 01:44:01 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3069, average loss: 0.8187
[11/24 01:44:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 64.34	
[11/24 01:44:01 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/24 01:45:42 visual_prompt]: 	Training 100/553. train loss: 0.1795,	0.9240 s / batch. (data: 5.38e-03). ETA=10:28:40, max mem: 30.7 GB 
[11/24 01:47:15 visual_prompt]: 	Training 200/553. train loss: 0.1478,	0.9652 s / batch. (data: 1.00e-02). ETA=10:55:05, max mem: 30.7 GB 
[11/24 01:48:48 visual_prompt]: 	Training 300/553. train loss: 0.7258,	0.9256 s / batch. (data: 8.12e-04). ETA=10:26:40, max mem: 30.7 GB 
[11/24 01:50:22 visual_prompt]: 	Training 400/553. train loss: 0.4693,	0.9499 s / batch. (data: 2.95e-04). ETA=10:41:30, max mem: 30.7 GB 
[11/24 01:51:55 visual_prompt]: 	Training 500/553. train loss: 0.4934,	0.9311 s / batch. (data: 8.24e-04). ETA=10:27:16, max mem: 30.7 GB 
[11/24 01:52:44 visual_prompt]: Epoch 27 / 100: avg data time: 1.94e-02, avg batch time: 0.9462, average train loss: 0.4491
[11/24 01:53:39 visual_prompt]: Inference (val):avg data time: 4.09e-04, avg batch time: 0.3032, average loss: 1.0382
[11/24 01:53:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 62.42	
[11/24 01:53:39 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/24 01:55:21 visual_prompt]: 	Training 100/553. train loss: 0.3575,	0.9137 s / batch. (data: 3.05e-04). ETA=10:13:14, max mem: 30.7 GB 
[11/24 01:56:54 visual_prompt]: 	Training 200/553. train loss: 0.4576,	0.9240 s / batch. (data: 3.08e-04). ETA=10:18:36, max mem: 30.7 GB 
[11/24 01:58:28 visual_prompt]: 	Training 300/553. train loss: 0.8035,	0.9295 s / batch. (data: 3.09e-04). ETA=10:20:44, max mem: 30.7 GB 
[11/24 02:00:01 visual_prompt]: 	Training 400/553. train loss: 0.0810,	0.9112 s / batch. (data: 3.46e-04). ETA=10:06:58, max mem: 30.7 GB 
[11/24 02:01:34 visual_prompt]: 	Training 500/553. train loss: 0.3033,	0.9264 s / batch. (data: 1.48e-02). ETA=10:15:35, max mem: 30.7 GB 
[11/24 02:02:24 visual_prompt]: Epoch 28 / 100: avg data time: 1.99e-02, avg batch time: 0.9478, average train loss: 0.4304
[11/24 02:03:20 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3060, average loss: 1.0107
[11/24 02:03:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 61.79	
[11/24 02:03:20 visual_prompt]: Training 29 / 100 epoch, with learning rate 8.622126023955446e-05
[11/24 02:05:04 visual_prompt]: 	Training 100/553. train loss: 0.7179,	0.9280 s / batch. (data: 3.11e-04). ETA=10:14:16, max mem: 30.7 GB 
[11/24 02:06:38 visual_prompt]: 	Training 200/553. train loss: 0.5778,	0.9792 s / batch. (data: 1.60e-02). ETA=10:46:33, max mem: 30.7 GB 
[11/24 02:08:15 visual_prompt]: 	Training 300/553. train loss: 0.7536,	0.9120 s / batch. (data: 3.96e-03). ETA=10:00:37, max mem: 30.7 GB 
[11/24 02:09:48 visual_prompt]: 	Training 400/553. train loss: 0.8828,	0.9132 s / batch. (data: 9.96e-04). ETA=9:59:55, max mem: 30.7 GB 
[11/24 02:11:21 visual_prompt]: 	Training 500/553. train loss: 0.2188,	0.9343 s / batch. (data: 5.40e-03). ETA=10:12:13, max mem: 30.7 GB 
[11/24 02:12:10 visual_prompt]: Epoch 29 / 100: avg data time: 3.10e-02, avg batch time: 0.9582, average train loss: 0.3967
[11/24 02:13:05 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3060, average loss: 0.9719
[11/24 02:13:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 63.61	
[11/24 02:13:05 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.506183921362443e-05
[11/24 02:14:46 visual_prompt]: 	Training 100/553. train loss: 0.4038,	0.9360 s / batch. (data: 8.13e-04). ETA=10:10:55, max mem: 30.7 GB 
[11/24 02:16:20 visual_prompt]: 	Training 200/553. train loss: 0.0768,	0.9188 s / batch. (data: 5.41e-03). ETA=9:58:10, max mem: 30.7 GB 
[11/24 02:17:53 visual_prompt]: 	Training 300/553. train loss: 0.5360,	0.9200 s / batch. (data: 7.85e-04). ETA=9:57:26, max mem: 30.7 GB 
[11/24 02:19:26 visual_prompt]: 	Training 400/553. train loss: 0.1289,	0.9469 s / batch. (data: 2.90e-04). ETA=10:13:17, max mem: 30.7 GB 
[11/24 02:20:59 visual_prompt]: 	Training 500/553. train loss: 0.2694,	0.9080 s / batch. (data: 2.64e-04). ETA=9:46:37, max mem: 30.7 GB 
[11/24 02:21:48 visual_prompt]: Epoch 30 / 100: avg data time: 2.08e-02, avg batch time: 0.9465, average train loss: 0.3700
[11/24 02:22:43 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.3052, average loss: 1.0636
[11/24 02:22:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 63.33	
[11/24 02:22:43 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.386407858128706e-05
[11/24 02:24:25 visual_prompt]: 	Training 100/553. train loss: 0.1632,	0.9400 s / batch. (data: 3.29e-04). ETA=10:04:51, max mem: 30.7 GB 
[11/24 02:25:58 visual_prompt]: 	Training 200/553. train loss: 0.1260,	0.9190 s / batch. (data: 8.46e-04). ETA=9:49:49, max mem: 30.7 GB 
[11/24 02:27:32 visual_prompt]: 	Training 300/553. train loss: 0.3465,	0.9213 s / batch. (data: 3.13e-04). ETA=9:49:46, max mem: 30.7 GB 
[11/24 02:29:05 visual_prompt]: 	Training 400/553. train loss: 0.1618,	0.9120 s / batch. (data: 2.93e-04). ETA=9:42:18, max mem: 30.7 GB 
[11/24 02:30:38 visual_prompt]: 	Training 500/553. train loss: 0.0262,	0.9360 s / batch. (data: 2.78e-04). ETA=9:56:04, max mem: 30.7 GB 
[11/24 02:31:27 visual_prompt]: Epoch 31 / 100: avg data time: 2.24e-02, avg batch time: 0.9483, average train loss: 0.3391
[11/24 02:32:22 visual_prompt]: Inference (val):avg data time: 2.65e-04, avg batch time: 0.3072, average loss: 1.3063
[11/24 02:32:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 59.08	
[11/24 02:32:22 visual_prompt]: Stopping early.
[11/24 02:32:24 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 02:32:24 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 02:32:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/24 02:32:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/24 02:32:24 visual_prompt]: Training with config:
[11/24 02:32:24 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/test/seed9805/lr0.0001_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 9805, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/24 02:32:24 visual_prompt]: Loading training data...
[11/24 02:32:24 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 02:32:24 visual_prompt]: Loading validation data...
[11/24 02:32:24 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 02:32:24 visual_prompt]: Loading test data...
[11/24 02:32:24 visual_prompt]: Constructing mammo-cbis dataset test...
[11/24 02:32:24 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 02:32:27 visual_prompt]: Enable all parameters update during training
[11/24 02:32:27 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/24 02:32:27 visual_prompt]: tuned percent:100.000
[11/24 02:32:27 visual_prompt]: Device used for model: 0
[11/24 02:32:27 visual_prompt]: Setting up Evaluator...
[11/24 02:32:27 visual_prompt]: Setting up Trainer...
[11/24 02:32:27 visual_prompt]: 	Setting up the optimizer...
[11/24 02:32:27 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 02:34:08 visual_prompt]: 	Training 100/553. train loss: 2.0214,	0.9174 s / batch. (data: 2.82e-04). ETA=14:04:00, max mem: 30.7 GB 
[11/24 02:35:42 visual_prompt]: 	Training 200/553. train loss: 2.2297,	0.9099 s / batch. (data: 7.25e-04). ETA=13:55:34, max mem: 30.7 GB 
[11/24 02:37:15 visual_prompt]: 	Training 300/553. train loss: 2.0502,	0.9159 s / batch. (data: 2.88e-04). ETA=13:59:35, max mem: 30.7 GB 
[11/24 02:38:49 visual_prompt]: 	Training 400/553. train loss: 3.7213,	0.9356 s / batch. (data: 1.55e-02). ETA=14:16:05, max mem: 30.7 GB 
[11/24 02:40:22 visual_prompt]: 	Training 500/553. train loss: 4.3927,	0.9116 s / batch. (data: 2.54e-04). ETA=13:52:33, max mem: 30.7 GB 
[11/24 02:41:11 visual_prompt]: Epoch 1 / 100: avg data time: 1.89e-02, avg batch time: 0.9469, average train loss: 2.8260
[11/24 02:42:06 visual_prompt]: Inference (val):avg data time: 4.11e-04, avg batch time: 0.3061, average loss: 2.9664
[11/24 02:42:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.18	
[11/24 02:43:41 visual_prompt]: 	Test 100/162. loss: 4.158, 0.3002 s / batch. (data: 3.60e-05)max mem: 30.66532 GB 
[11/24 02:44:29 visual_prompt]: Inference (test):avg data time: 3.05e-05, avg batch time: 0.3064, average loss: 3.2786
[11/24 02:44:29 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.24	rocauc: 47.99	
[11/24 02:44:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/24 02:46:11 visual_prompt]: 	Training 100/553. train loss: 0.4544,	0.9213 s / batch. (data: 2.64e-04). ETA=13:59:08, max mem: 30.7 GB 
[11/24 02:47:44 visual_prompt]: 	Training 200/553. train loss: 0.9010,	0.9428 s / batch. (data: 6.49e-03). ETA=14:17:08, max mem: 30.7 GB 
[11/24 02:49:18 visual_prompt]: 	Training 300/553. train loss: 0.6443,	0.9625 s / batch. (data: 7.25e-04). ETA=14:33:26, max mem: 30.7 GB 
[11/24 02:50:52 visual_prompt]: 	Training 400/553. train loss: 1.1718,	0.9326 s / batch. (data: 2.58e-04). ETA=14:04:45, max mem: 30.7 GB 
[11/24 02:52:25 visual_prompt]: 	Training 500/553. train loss: 0.6231,	0.9420 s / batch. (data: 7.18e-04). ETA=14:11:42, max mem: 30.7 GB 
[11/24 02:53:14 visual_prompt]: Epoch 2 / 100: avg data time: 2.07e-02, avg batch time: 0.9489, average train loss: 0.9712
[11/24 02:54:09 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3067, average loss: 0.6530
[11/24 02:54:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 62.72	
[11/24 02:55:43 visual_prompt]: 	Test 100/162. loss: 0.523, 0.3189 s / batch. (data: 4.15e-05)max mem: 30.66532 GB 
[11/24 02:56:29 visual_prompt]: Inference (test):avg data time: 3.29e-05, avg batch time: 0.3051, average loss: 0.6512
[11/24 02:56:29 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.62	rocauc: 62.15	
[11/24 02:56:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/24 02:58:12 visual_prompt]: 	Training 100/553. train loss: 0.9755,	0.9528 s / batch. (data: 7.18e-04). ETA=14:18:58, max mem: 30.7 GB 
[11/24 02:59:46 visual_prompt]: 	Training 200/553. train loss: 0.3536,	0.9521 s / batch. (data: 1.09e-02). ETA=14:16:50, max mem: 30.7 GB 
[11/24 03:01:20 visual_prompt]: 	Training 300/553. train loss: 0.6998,	0.9501 s / batch. (data: 5.88e-03). ETA=14:13:27, max mem: 30.7 GB 
[11/24 03:02:53 visual_prompt]: 	Training 400/553. train loss: 0.3102,	0.9477 s / batch. (data: 7.07e-04). ETA=14:09:39, max mem: 30.7 GB 
[11/24 03:04:27 visual_prompt]: 	Training 500/553. train loss: 0.7439,	0.9568 s / batch. (data: 5.91e-03). ETA=14:16:12, max mem: 30.7 GB 
[11/24 03:05:16 visual_prompt]: Epoch 3 / 100: avg data time: 2.41e-02, avg batch time: 0.9524, average train loss: 0.7674
[11/24 03:06:11 visual_prompt]: Inference (val):avg data time: 2.25e-04, avg batch time: 0.3058, average loss: 0.6247
[11/24 03:06:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 67.24	
[11/24 03:07:44 visual_prompt]: 	Test 100/162. loss: 0.522, 0.2943 s / batch. (data: 4.86e-05)max mem: 30.66532 GB 
[11/24 03:08:31 visual_prompt]: Inference (test):avg data time: 7.49e-05, avg batch time: 0.3056, average loss: 0.6739
[11/24 03:08:31 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.84	rocauc: 60.57	
[11/24 03:08:31 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/24 03:10:11 visual_prompt]: 	Training 100/553. train loss: 0.6416,	0.9530 s / batch. (data: 2.50e-02). ETA=14:10:24, max mem: 30.7 GB 
[11/24 03:11:45 visual_prompt]: 	Training 200/553. train loss: 0.4614,	0.9718 s / batch. (data: 5.86e-03). ETA=14:25:34, max mem: 30.7 GB 
[11/24 03:13:18 visual_prompt]: 	Training 300/553. train loss: 0.8686,	0.9455 s / batch. (data: 5.83e-03). ETA=14:00:34, max mem: 30.7 GB 
[11/24 03:14:52 visual_prompt]: 	Training 400/553. train loss: 0.3367,	0.9414 s / batch. (data: 7.25e-04). ETA=13:55:18, max mem: 30.7 GB 
[11/24 03:16:25 visual_prompt]: 	Training 500/553. train loss: 0.5063,	0.9359 s / batch. (data: 1.66e-02). ETA=13:48:56, max mem: 30.7 GB 
[11/24 03:17:15 visual_prompt]: Epoch 4 / 100: avg data time: 1.83e-02, avg batch time: 0.9471, average train loss: 0.7345
[11/24 03:18:09 visual_prompt]: Inference (val):avg data time: 2.81e-04, avg batch time: 0.3047, average loss: 0.6493
[11/24 03:18:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 69.44	
[11/24 03:19:41 visual_prompt]: 	Test 100/162. loss: 0.382, 0.2948 s / batch. (data: 3.53e-05)max mem: 30.66532 GB 
[11/24 03:20:28 visual_prompt]: Inference (test):avg data time: 3.07e-04, avg batch time: 0.3061, average loss: 0.7287
[11/24 03:20:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.36	rocauc: 64.78	
[11/24 03:20:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/24 03:22:09 visual_prompt]: 	Training 100/553. train loss: 0.4605,	0.9360 s / batch. (data: 2.55e-04). ETA=13:46:36, max mem: 30.7 GB 
[11/24 03:23:43 visual_prompt]: 	Training 200/553. train loss: 1.6189,	0.9480 s / batch. (data: 5.38e-03). ETA=13:55:38, max mem: 30.7 GB 
[11/24 03:25:16 visual_prompt]: 	Training 300/553. train loss: 0.5699,	0.9464 s / batch. (data: 5.82e-03). ETA=13:52:37, max mem: 30.7 GB 
[11/24 03:26:50 visual_prompt]: 	Training 400/553. train loss: 0.7310,	0.9490 s / batch. (data: 8.77e-03). ETA=13:53:18, max mem: 30.7 GB 
[11/24 03:28:22 visual_prompt]: 	Training 500/553. train loss: 0.8602,	0.9456 s / batch. (data: 5.84e-03). ETA=13:48:45, max mem: 30.7 GB 
[11/24 03:29:12 visual_prompt]: Epoch 5 / 100: avg data time: 2.08e-02, avg batch time: 0.9473, average train loss: 0.7432
[11/24 03:30:06 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3050, average loss: 0.6627
[11/24 03:30:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 67.47	
[11/24 03:31:39 visual_prompt]: 	Test 100/162. loss: 0.539, 0.2951 s / batch. (data: 3.17e-05)max mem: 30.66532 GB 
[11/24 03:32:25 visual_prompt]: Inference (test):avg data time: 1.52e-04, avg batch time: 0.3059, average loss: 0.6991
[11/24 03:32:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.98	rocauc: 66.49	
[11/24 03:32:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/24 03:34:08 visual_prompt]: 	Training 100/553. train loss: 0.4991,	0.9238 s / batch. (data: 1.04e-02). ETA=13:27:21, max mem: 30.7 GB 
[11/24 03:35:41 visual_prompt]: 	Training 200/553. train loss: 0.4014,	0.9498 s / batch. (data: 7.26e-04). ETA=13:48:26, max mem: 30.7 GB 
[11/24 03:37:14 visual_prompt]: 	Training 300/553. train loss: 1.0018,	0.9187 s / batch. (data: 2.64e-04). ETA=13:19:48, max mem: 30.7 GB 
[11/24 03:38:47 visual_prompt]: 	Training 400/553. train loss: 0.6111,	0.9270 s / batch. (data: 7.38e-04). ETA=13:25:31, max mem: 30.7 GB 
[11/24 03:40:21 visual_prompt]: 	Training 500/553. train loss: 0.6002,	0.9510 s / batch. (data: 2.20e-02). ETA=13:44:43, max mem: 30.7 GB 
[11/24 03:41:11 visual_prompt]: Epoch 6 / 100: avg data time: 2.42e-02, avg batch time: 0.9508, average train loss: 0.7007
[11/24 03:42:05 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3095, average loss: 0.7821
[11/24 03:42:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 67.56	
[11/24 03:43:37 visual_prompt]: 	Test 100/162. loss: 0.459, 0.2950 s / batch. (data: 4.89e-05)max mem: 30.66532 GB 
[11/24 03:44:23 visual_prompt]: Inference (test):avg data time: 7.86e-05, avg batch time: 0.3046, average loss: 0.8759
[11/24 03:44:23 visual_prompt]: Classification results with test_mammo-cbis: top1: 48.53	rocauc: 64.17	
[11/24 03:44:23 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/24 03:46:05 visual_prompt]: 	Training 100/553. train loss: 0.5568,	0.9444 s / batch. (data: 2.98e-02). ETA=13:36:39, max mem: 30.7 GB 
[11/24 03:47:38 visual_prompt]: 	Training 200/553. train loss: 0.6396,	0.9346 s / batch. (data: 1.13e-03). ETA=13:26:33, max mem: 30.7 GB 
[11/24 03:49:12 visual_prompt]: 	Training 300/553. train loss: 0.6752,	0.9201 s / batch. (data: 5.37e-03). ETA=13:12:30, max mem: 30.7 GB 
[11/24 03:50:45 visual_prompt]: 	Training 400/553. train loss: 0.7579,	0.9440 s / batch. (data: 7.40e-04). ETA=13:31:31, max mem: 30.7 GB 
[11/24 03:52:19 visual_prompt]: 	Training 500/553. train loss: 0.5511,	0.9472 s / batch. (data: 7.64e-04). ETA=13:32:41, max mem: 30.7 GB 
[11/24 03:53:08 visual_prompt]: Epoch 7 / 100: avg data time: 2.11e-02, avg batch time: 0.9491, average train loss: 0.6703
[11/24 03:54:03 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3059, average loss: 0.6792
[11/24 03:54:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 68.27	
[11/24 03:55:35 visual_prompt]: 	Test 100/162. loss: 0.498, 0.2982 s / batch. (data: 3.98e-05)max mem: 30.66532 GB 
[11/24 03:56:21 visual_prompt]: Inference (test):avg data time: 2.25e-04, avg batch time: 0.3063, average loss: 0.7276
[11/24 03:56:21 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.28	rocauc: 67.52	
[11/24 03:56:21 visual_prompt]: Best epoch 7: best metric: -0.679
[11/24 03:56:21 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/24 03:58:03 visual_prompt]: 	Training 100/553. train loss: 0.5659,	0.9390 s / batch. (data: 5.34e-03). ETA=13:23:19, max mem: 30.7 GB 
[11/24 03:59:36 visual_prompt]: 	Training 200/553. train loss: 0.4966,	0.9099 s / batch. (data: 2.73e-04). ETA=12:56:51, max mem: 30.7 GB 
[11/24 04:01:10 visual_prompt]: 	Training 300/553. train loss: 0.6415,	0.9305 s / batch. (data: 5.34e-03). ETA=13:12:53, max mem: 30.7 GB 
[11/24 04:02:43 visual_prompt]: 	Training 400/553. train loss: 0.5245,	0.9330 s / batch. (data: 2.88e-04). ETA=13:13:31, max mem: 30.7 GB 
[11/24 04:04:17 visual_prompt]: 	Training 500/553. train loss: 1.2683,	0.9347 s / batch. (data: 6.79e-04). ETA=13:13:23, max mem: 30.7 GB 
[11/24 04:05:06 visual_prompt]: Epoch 8 / 100: avg data time: 2.23e-02, avg batch time: 0.9494, average train loss: 0.6514
[11/24 04:06:00 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.3078, average loss: 0.8992
[11/24 04:06:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 65.67	
[11/24 04:07:33 visual_prompt]: 	Test 100/162. loss: 1.045, 0.3067 s / batch. (data: 3.46e-05)max mem: 30.66532 GB 
[11/24 04:08:18 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.3073, average loss: 0.8359
[11/24 04:08:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.93	rocauc: 64.05	
[11/24 04:08:18 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/24 04:10:02 visual_prompt]: 	Training 100/553. train loss: 0.7881,	0.9426 s / batch. (data: 2.96e-04). ETA=13:17:39, max mem: 30.7 GB 
[11/24 04:11:35 visual_prompt]: 	Training 200/553. train loss: 0.4946,	0.9490 s / batch. (data: 2.53e-04). ETA=13:21:31, max mem: 30.7 GB 
[11/24 04:13:08 visual_prompt]: 	Training 300/553. train loss: 0.7645,	0.9334 s / batch. (data: 7.25e-04). ETA=13:06:47, max mem: 30.7 GB 
[11/24 04:14:42 visual_prompt]: 	Training 400/553. train loss: 0.7883,	0.9494 s / batch. (data: 9.90e-03). ETA=13:18:40, max mem: 30.7 GB 
[11/24 04:16:15 visual_prompt]: 	Training 500/553. train loss: 0.2494,	0.9518 s / batch. (data: 1.59e-02). ETA=13:19:09, max mem: 30.7 GB 
[11/24 04:17:05 visual_prompt]: Epoch 9 / 100: avg data time: 2.34e-02, avg batch time: 0.9509, average train loss: 0.6627
[11/24 04:18:00 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3057, average loss: 0.7348
[11/24 04:18:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 67.54	
[11/24 04:19:33 visual_prompt]: 	Test 100/162. loss: 0.482, 0.3143 s / batch. (data: 2.86e-05)max mem: 30.66532 GB 
[11/24 04:20:19 visual_prompt]: Inference (test):avg data time: 2.45e-04, avg batch time: 0.3045, average loss: 0.7041
[11/24 04:20:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.17	rocauc: 63.72	
[11/24 04:20:19 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/24 04:22:01 visual_prompt]: 	Training 100/553. train loss: 0.6681,	0.9522 s / batch. (data: 1.63e-02). ETA=13:17:02, max mem: 30.7 GB 
[11/24 04:23:35 visual_prompt]: 	Training 200/553. train loss: 1.6761,	0.9300 s / batch. (data: 5.35e-03). ETA=12:56:52, max mem: 30.7 GB 
[11/24 04:25:09 visual_prompt]: 	Training 300/553. train loss: 0.9252,	0.9717 s / batch. (data: 2.13e-02). ETA=13:30:05, max mem: 30.7 GB 
[11/24 04:26:42 visual_prompt]: 	Training 400/553. train loss: 0.8803,	0.9257 s / batch. (data: 2.94e-04). ETA=12:50:16, max mem: 30.7 GB 
[11/24 04:28:15 visual_prompt]: 	Training 500/553. train loss: 0.7096,	0.9686 s / batch. (data: 1.09e-02). ETA=13:24:20, max mem: 30.7 GB 
[11/24 04:29:05 visual_prompt]: Epoch 10 / 100: avg data time: 2.22e-02, avg batch time: 0.9513, average train loss: 0.6278
[11/24 04:30:00 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.3056, average loss: 0.6938
[11/24 04:30:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 72.11	
[11/24 04:31:33 visual_prompt]: 	Test 100/162. loss: 0.586, 0.3054 s / batch. (data: 5.29e-05)max mem: 30.66532 GB 
[11/24 04:32:20 visual_prompt]: Inference (test):avg data time: 6.90e-05, avg batch time: 0.3046, average loss: 0.6856
[11/24 04:32:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.64	rocauc: 68.89	
[11/24 04:32:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/24 04:34:01 visual_prompt]: 	Training 100/553. train loss: 0.6053,	0.9166 s / batch. (data: 9.38e-03). ETA=12:38:48, max mem: 30.7 GB 
[11/24 04:35:34 visual_prompt]: 	Training 200/553. train loss: 0.2346,	0.9560 s / batch. (data: 7.95e-03). ETA=13:09:47, max mem: 30.7 GB 
[11/24 04:37:08 visual_prompt]: 	Training 300/553. train loss: 1.1976,	0.9431 s / batch. (data: 8.03e-04). ETA=12:57:34, max mem: 30.7 GB 
[11/24 04:38:41 visual_prompt]: 	Training 400/553. train loss: 0.8249,	0.9215 s / batch. (data: 2.67e-04). ETA=12:38:16, max mem: 30.7 GB 
[11/24 04:40:14 visual_prompt]: 	Training 500/553. train loss: 0.7100,	0.9281 s / batch. (data: 2.57e-04). ETA=12:42:05, max mem: 30.7 GB 
[11/24 04:41:04 visual_prompt]: Epoch 11 / 100: avg data time: 2.08e-02, avg batch time: 0.9472, average train loss: 0.6023
[11/24 04:41:58 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3045, average loss: 0.9674
[11/24 04:41:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 67.25	
[11/24 04:43:30 visual_prompt]: 	Test 100/162. loss: 0.479, 0.3104 s / batch. (data: 4.91e-05)max mem: 30.66532 GB 
[11/24 04:44:16 visual_prompt]: Inference (test):avg data time: 2.72e-04, avg batch time: 0.3064, average loss: 1.1071
[11/24 04:44:16 visual_prompt]: Classification results with test_mammo-cbis: top1: 47.29	rocauc: 66.18	
[11/24 04:44:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/24 04:45:58 visual_prompt]: 	Training 100/553. train loss: 0.4010,	0.9229 s / batch. (data: 8.00e-03). ETA=12:35:32, max mem: 30.7 GB 
[11/24 04:47:31 visual_prompt]: 	Training 200/553. train loss: 0.3563,	0.9496 s / batch. (data: 1.09e-02). ETA=12:55:44, max mem: 30.7 GB 
[11/24 04:49:05 visual_prompt]: 	Training 300/553. train loss: 0.4521,	0.9412 s / batch. (data: 7.49e-04). ETA=12:47:19, max mem: 30.7 GB 
[11/24 04:50:38 visual_prompt]: 	Training 400/553. train loss: 1.0679,	0.9330 s / batch. (data: 1.05e-02). ETA=12:39:05, max mem: 30.7 GB 
[11/24 04:52:12 visual_prompt]: 	Training 500/553. train loss: 0.6814,	0.9476 s / batch. (data: 2.43e-02). ETA=12:49:24, max mem: 30.7 GB 
[11/24 04:53:01 visual_prompt]: Epoch 12 / 100: avg data time: 2.19e-02, avg batch time: 0.9490, average train loss: 0.5780
[11/24 04:53:56 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3047, average loss: 0.6451
[11/24 04:53:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 69.53	
[11/24 04:55:29 visual_prompt]: 	Test 100/162. loss: 0.477, 0.3025 s / batch. (data: 3.46e-05)max mem: 30.66532 GB 
[11/24 04:56:16 visual_prompt]: Inference (test):avg data time: 3.20e-05, avg batch time: 0.3063, average loss: 0.6747
[11/24 04:56:16 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.64	rocauc: 69.13	
[11/24 04:56:16 visual_prompt]: Best epoch 12: best metric: -0.645
[11/24 04:56:16 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/24 04:57:59 visual_prompt]: 	Training 100/553. train loss: 0.8120,	0.9140 s / batch. (data: 2.74e-04). ETA=12:19:45, max mem: 30.7 GB 
[11/24 04:59:32 visual_prompt]: 	Training 200/553. train loss: 0.4028,	0.9248 s / batch. (data: 2.63e-04). ETA=12:27:00, max mem: 30.7 GB 
[11/24 05:01:05 visual_prompt]: 	Training 300/553. train loss: 0.2624,	0.9177 s / batch. (data: 3.20e-04). ETA=12:19:41, max mem: 30.7 GB 
[11/24 05:02:39 visual_prompt]: 	Training 400/553. train loss: 0.8874,	0.9445 s / batch. (data: 5.36e-03). ETA=12:39:44, max mem: 30.7 GB 
[11/24 05:04:12 visual_prompt]: 	Training 500/553. train loss: 0.8540,	0.9355 s / batch. (data: 1.09e-02). ETA=12:30:59, max mem: 30.7 GB 
[11/24 05:05:02 visual_prompt]: Epoch 13 / 100: avg data time: 2.40e-02, avg batch time: 0.9510, average train loss: 0.5771
[11/24 05:05:56 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3071, average loss: 0.7879
[11/24 05:05:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 65.77	
[11/24 05:07:28 visual_prompt]: 	Test 100/162. loss: 0.514, 0.3153 s / batch. (data: 3.60e-05)max mem: 30.66532 GB 
[11/24 05:08:14 visual_prompt]: Inference (test):avg data time: 3.17e-05, avg batch time: 0.3061, average loss: 0.8977
[11/24 05:08:14 visual_prompt]: Classification results with test_mammo-cbis: top1: 53.18	rocauc: 63.34	
[11/24 05:08:14 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/24 05:09:56 visual_prompt]: 	Training 100/553. train loss: 0.3660,	0.9126 s / batch. (data: 2.89e-04). ETA=12:10:15, max mem: 30.7 GB 
[11/24 05:11:29 visual_prompt]: 	Training 200/553. train loss: 0.2396,	0.9365 s / batch. (data: 1.09e-02). ETA=12:27:50, max mem: 30.7 GB 
[11/24 05:13:03 visual_prompt]: 	Training 300/553. train loss: 1.0566,	0.9247 s / batch. (data: 2.85e-04). ETA=12:16:51, max mem: 30.7 GB 
[11/24 05:14:37 visual_prompt]: 	Training 400/553. train loss: 0.7547,	0.9841 s / batch. (data: 4.08e-02). ETA=13:02:34, max mem: 30.7 GB 
[11/24 05:16:10 visual_prompt]: 	Training 500/553. train loss: 0.7092,	0.9191 s / batch. (data: 7.29e-04). ETA=12:09:18, max mem: 30.7 GB 
[11/24 05:16:59 visual_prompt]: Epoch 14 / 100: avg data time: 2.20e-02, avg batch time: 0.9493, average train loss: 0.5372
[11/24 05:17:54 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3072, average loss: 0.7244
[11/24 05:17:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 67.58	
[11/24 05:19:27 visual_prompt]: 	Test 100/162. loss: 0.700, 0.2950 s / batch. (data: 3.27e-05)max mem: 30.66532 GB 
[11/24 05:20:13 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.3081, average loss: 0.7586
[11/24 05:20:13 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 65.70	
[11/24 05:20:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/24 05:21:55 visual_prompt]: 	Training 100/553. train loss: 0.8008,	0.9269 s / batch. (data: 2.42e-04). ETA=12:13:06, max mem: 30.7 GB 
[11/24 05:23:29 visual_prompt]: 	Training 200/553. train loss: 0.8672,	0.9480 s / batch. (data: 7.60e-04). ETA=12:28:14, max mem: 30.7 GB 
[11/24 05:25:02 visual_prompt]: 	Training 300/553. train loss: 1.0690,	0.9441 s / batch. (data: 5.37e-03). ETA=12:23:36, max mem: 30.7 GB 
[11/24 05:26:35 visual_prompt]: 	Training 400/553. train loss: 0.5700,	0.9266 s / batch. (data: 5.91e-03). ETA=12:08:16, max mem: 30.7 GB 
[11/24 05:28:09 visual_prompt]: 	Training 500/553. train loss: 0.4690,	0.9565 s / batch. (data: 5.36e-03). ETA=12:30:11, max mem: 30.7 GB 
[11/24 05:28:58 visual_prompt]: Epoch 15 / 100: avg data time: 2.06e-02, avg batch time: 0.9485, average train loss: 0.4958
[11/24 05:29:53 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3057, average loss: 0.8291
[11/24 05:29:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 65.89	
[11/24 05:31:26 visual_prompt]: 	Test 100/162. loss: 0.474, 0.3023 s / batch. (data: 3.24e-05)max mem: 30.66532 GB 
[11/24 05:32:13 visual_prompt]: Inference (test):avg data time: 1.17e-04, avg batch time: 0.3081, average loss: 0.8665
[11/24 05:32:13 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.86	rocauc: 66.26	
[11/24 05:32:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/24 05:33:53 visual_prompt]: 	Training 100/553. train loss: 0.0616,	0.9330 s / batch. (data: 2.73e-04). ETA=12:09:20, max mem: 30.7 GB 
[11/24 05:35:26 visual_prompt]: 	Training 200/553. train loss: 0.2462,	0.9400 s / batch. (data: 7.96e-03). ETA=12:13:16, max mem: 30.7 GB 
[11/24 05:37:00 visual_prompt]: 	Training 300/553. train loss: 0.7561,	0.9156 s / batch. (data: 5.38e-03). ETA=11:52:41, max mem: 30.7 GB 
[11/24 05:38:33 visual_prompt]: 	Training 400/553. train loss: 0.4889,	0.9430 s / batch. (data: 2.42e-02). ETA=12:12:28, max mem: 30.7 GB 
[11/24 05:40:07 visual_prompt]: 	Training 500/553. train loss: 0.4455,	0.9084 s / batch. (data: 2.45e-04). ETA=11:44:04, max mem: 30.7 GB 
[11/24 05:40:56 visual_prompt]: Epoch 16 / 100: avg data time: 1.83e-02, avg batch time: 0.9465, average train loss: 0.4941
[11/24 05:41:51 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3055, average loss: 0.8169
[11/24 05:41:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 65.82	
[11/24 05:43:23 visual_prompt]: 	Test 100/162. loss: 0.139, 0.3178 s / batch. (data: 3.12e-05)max mem: 30.66532 GB 
[11/24 05:44:09 visual_prompt]: Inference (test):avg data time: 7.67e-05, avg batch time: 0.3057, average loss: 0.8404
[11/24 05:44:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.86	rocauc: 67.62	
[11/24 05:44:09 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/24 05:45:51 visual_prompt]: 	Training 100/553. train loss: 0.6213,	0.9245 s / batch. (data: 5.38e-03). ETA=11:54:10, max mem: 30.7 GB 
[11/24 05:47:24 visual_prompt]: 	Training 200/553. train loss: 0.3979,	0.9169 s / batch. (data: 5.36e-03). ETA=11:46:50, max mem: 30.7 GB 
[11/24 05:48:58 visual_prompt]: 	Training 300/553. train loss: 0.6561,	0.9351 s / batch. (data: 7.37e-04). ETA=11:59:18, max mem: 30.7 GB 
[11/24 05:50:31 visual_prompt]: 	Training 400/553. train loss: 0.0751,	0.9272 s / batch. (data: 3.50e-04). ETA=11:51:38, max mem: 30.7 GB 
[11/24 05:52:05 visual_prompt]: 	Training 500/553. train loss: 0.3472,	0.9408 s / batch. (data: 7.17e-04). ETA=12:00:32, max mem: 30.7 GB 
[11/24 05:52:54 visual_prompt]: Epoch 17 / 100: avg data time: 2.17e-02, avg batch time: 0.9492, average train loss: 0.4370
[11/24 05:53:49 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3054, average loss: 0.8528
[11/24 05:53:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 62.38	
[11/24 05:55:22 visual_prompt]: 	Test 100/162. loss: 0.772, 0.2983 s / batch. (data: 7.61e-05)max mem: 30.66532 GB 
[11/24 05:56:08 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.3052, average loss: 0.8636
[11/24 05:56:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.60	rocauc: 63.19	
[11/24 05:56:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/24 05:57:50 visual_prompt]: 	Training 100/553. train loss: 0.2539,	0.9278 s / batch. (data: 2.66e-04). ETA=11:48:13, max mem: 30.7 GB 
[11/24 05:59:23 visual_prompt]: 	Training 200/553. train loss: 0.4244,	0.9335 s / batch. (data: 7.14e-04). ETA=11:50:57, max mem: 30.7 GB 
[11/24 06:00:57 visual_prompt]: 	Training 300/553. train loss: 0.7340,	0.9259 s / batch. (data: 7.78e-04). ETA=11:43:40, max mem: 30.7 GB 
[11/24 06:02:30 visual_prompt]: 	Training 400/553. train loss: 1.4462,	0.9481 s / batch. (data: 5.85e-03). ETA=11:58:56, max mem: 30.7 GB 
[11/24 06:04:04 visual_prompt]: 	Training 500/553. train loss: 0.2227,	0.9211 s / batch. (data: 5.38e-03). ETA=11:36:57, max mem: 30.7 GB 
[11/24 06:04:53 visual_prompt]: Epoch 18 / 100: avg data time: 2.04e-02, avg batch time: 0.9489, average train loss: 0.4089
[11/24 06:05:48 visual_prompt]: Inference (val):avg data time: 2.40e-04, avg batch time: 0.3055, average loss: 0.9615
[11/24 06:05:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.62	
[11/24 06:07:21 visual_prompt]: 	Test 100/162. loss: 1.816, 0.3191 s / batch. (data: 3.72e-05)max mem: 30.66532 GB 
[11/24 06:08:07 visual_prompt]: Inference (test):avg data time: 1.22e-04, avg batch time: 0.3053, average loss: 1.0843
[11/24 06:08:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 51.47	rocauc: 62.64	
[11/24 06:08:07 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/24 06:09:50 visual_prompt]: 	Training 100/553. train loss: 0.1494,	0.9332 s / batch. (data: 5.87e-03). ETA=11:43:44, max mem: 30.7 GB 
[11/24 06:11:23 visual_prompt]: 	Training 200/553. train loss: 0.2616,	0.9640 s / batch. (data: 7.59e-04). ETA=12:05:20, max mem: 30.7 GB 
[11/24 06:12:57 visual_prompt]: 	Training 300/553. train loss: 1.0278,	0.9268 s / batch. (data: 2.59e-04). ETA=11:35:47, max mem: 30.7 GB 
[11/24 06:14:30 visual_prompt]: 	Training 400/553. train loss: 0.7087,	0.9267 s / batch. (data: 5.37e-03). ETA=11:34:13, max mem: 30.7 GB 
[11/24 06:16:04 visual_prompt]: 	Training 500/553. train loss: 0.3177,	0.9164 s / batch. (data: 2.59e-03). ETA=11:24:57, max mem: 30.7 GB 
[11/24 06:16:53 visual_prompt]: Epoch 19 / 100: avg data time: 2.35e-02, avg batch time: 0.9509, average train loss: 0.3533
[11/24 06:17:47 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3058, average loss: 0.8850
[11/24 06:17:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 66.94	
[11/24 06:19:19 visual_prompt]: 	Test 100/162. loss: 1.030, 0.3184 s / batch. (data: 3.22e-05)max mem: 30.66532 GB 
[11/24 06:20:05 visual_prompt]: Inference (test):avg data time: 1.97e-04, avg batch time: 0.3050, average loss: 0.8773
[11/24 06:20:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.57	rocauc: 69.02	
[11/24 06:20:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/24 06:21:48 visual_prompt]: 	Training 100/553. train loss: 0.5213,	0.9436 s / batch. (data: 1.04e-02). ETA=11:42:52, max mem: 30.7 GB 
[11/24 06:23:21 visual_prompt]: 	Training 200/553. train loss: 0.3264,	0.9434 s / batch. (data: 2.66e-04). ETA=11:41:09, max mem: 30.7 GB 
[11/24 06:24:55 visual_prompt]: 	Training 300/553. train loss: 0.0360,	0.9226 s / batch. (data: 5.39e-03). ETA=11:24:09, max mem: 30.7 GB 
[11/24 06:26:28 visual_prompt]: 	Training 400/553. train loss: 0.1649,	0.9585 s / batch. (data: 6.01e-03). ETA=11:49:10, max mem: 30.7 GB 
[11/24 06:28:01 visual_prompt]: 	Training 500/553. train loss: 1.1197,	0.9309 s / batch. (data: 2.58e-04). ETA=11:27:11, max mem: 30.7 GB 
[11/24 06:28:51 visual_prompt]: Epoch 20 / 100: avg data time: 2.19e-02, avg batch time: 0.9494, average train loss: 0.3311
[11/24 06:29:45 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3064, average loss: 0.9324
[11/24 06:29:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 64.70	
[11/24 06:31:17 visual_prompt]: 	Test 100/162. loss: 1.972, 0.3177 s / batch. (data: 4.05e-05)max mem: 30.66532 GB 
[11/24 06:32:03 visual_prompt]: Inference (test):avg data time: 2.04e-04, avg batch time: 0.3062, average loss: 1.0117
[11/24 06:32:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.67	rocauc: 61.56	
[11/24 06:32:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/24 06:33:45 visual_prompt]: 	Training 100/553. train loss: 0.6176,	0.9151 s / batch. (data: 2.75e-04). ETA=11:13:10, max mem: 30.7 GB 
[11/24 06:35:18 visual_prompt]: 	Training 200/553. train loss: 0.2248,	0.9292 s / batch. (data: 1.13e-02). ETA=11:22:00, max mem: 30.7 GB 
[11/24 06:36:51 visual_prompt]: 	Training 300/553. train loss: 1.2101,	0.9235 s / batch. (data: 2.61e-04). ETA=11:16:17, max mem: 30.7 GB 
[11/24 06:38:25 visual_prompt]: 	Training 400/553. train loss: 0.1364,	0.9211 s / batch. (data: 9.10e-03). ETA=11:13:01, max mem: 30.7 GB 
[11/24 06:39:58 visual_prompt]: 	Training 500/553. train loss: 0.0113,	0.9279 s / batch. (data: 7.25e-04). ETA=11:16:25, max mem: 30.7 GB 
[11/24 06:40:48 visual_prompt]: Epoch 21 / 100: avg data time: 2.07e-02, avg batch time: 0.9480, average train loss: 0.2954
[11/24 06:41:42 visual_prompt]: Inference (val):avg data time: 8.28e-05, avg batch time: 0.3066, average loss: 1.2381
[11/24 06:41:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 64.26	
[11/24 06:43:14 visual_prompt]: 	Test 100/162. loss: 0.243, 0.2969 s / batch. (data: 7.46e-05)max mem: 30.66532 GB 
[11/24 06:44:00 visual_prompt]: Inference (test):avg data time: 3.18e-05, avg batch time: 0.3061, average loss: 1.2612
[11/24 06:44:00 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.83	rocauc: 61.92	
[11/24 06:44:00 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/24 06:45:43 visual_prompt]: 	Training 100/553. train loss: 0.3430,	0.9238 s / batch. (data: 7.70e-04). ETA=11:11:07, max mem: 30.7 GB 
[11/24 06:47:16 visual_prompt]: 	Training 200/553. train loss: 0.0173,	0.9653 s / batch. (data: 2.48e-02). ETA=11:39:36, max mem: 30.7 GB 
[11/24 06:48:50 visual_prompt]: 	Training 300/553. train loss: 0.1102,	0.9237 s / batch. (data: 7.28e-04). ETA=11:07:56, max mem: 30.7 GB 
[11/24 06:50:23 visual_prompt]: 	Training 400/553. train loss: 0.0824,	0.9211 s / batch. (data: 2.65e-04). ETA=11:04:29, max mem: 30.7 GB 
[11/24 06:51:56 visual_prompt]: 	Training 500/553. train loss: 0.1453,	0.9179 s / batch. (data: 7.25e-04). ETA=11:00:41, max mem: 30.7 GB 
[11/24 06:52:46 visual_prompt]: Epoch 22 / 100: avg data time: 2.29e-02, avg batch time: 0.9498, average train loss: 0.3409
[11/24 06:53:40 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3073, average loss: 1.1567
[11/24 06:53:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 64.88	
[11/24 06:55:12 visual_prompt]: 	Test 100/162. loss: 1.683, 0.2948 s / batch. (data: 5.44e-05)max mem: 30.66532 GB 
[11/24 06:55:58 visual_prompt]: Inference (test):avg data time: 5.63e-05, avg batch time: 0.3066, average loss: 1.1690
[11/24 06:55:58 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.71	rocauc: 61.31	
[11/24 06:55:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/24 06:57:40 visual_prompt]: 	Training 100/553. train loss: 0.2752,	0.9397 s / batch. (data: 7.61e-03). ETA=11:13:57, max mem: 30.7 GB 
[11/24 06:59:14 visual_prompt]: 	Training 200/553. train loss: 0.7016,	0.9408 s / batch. (data: 2.20e-02). ETA=11:13:11, max mem: 30.7 GB 
[11/24 07:00:47 visual_prompt]: 	Training 300/553. train loss: 0.4074,	0.9485 s / batch. (data: 7.45e-04). ETA=11:17:06, max mem: 30.7 GB 
[11/24 07:02:21 visual_prompt]: 	Training 400/553. train loss: 0.2980,	0.9116 s / batch. (data: 5.33e-03). ETA=10:49:16, max mem: 30.7 GB 
[11/24 07:03:54 visual_prompt]: 	Training 500/553. train loss: 0.1921,	0.9076 s / batch. (data: 2.72e-04). ETA=10:44:53, max mem: 30.7 GB 
[11/24 07:04:43 visual_prompt]: Epoch 23 / 100: avg data time: 2.19e-02, avg batch time: 0.9489, average train loss: 0.2354
[11/24 07:05:38 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.3073, average loss: 0.9824
[11/24 07:05:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 65.80	
[11/24 07:07:10 visual_prompt]: 	Test 100/162. loss: 1.296, 0.3121 s / batch. (data: 3.15e-05)max mem: 30.66532 GB 
[11/24 07:07:56 visual_prompt]: Inference (test):avg data time: 1.24e-04, avg batch time: 0.3063, average loss: 1.1904
[11/24 07:07:56 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.36	rocauc: 62.43	
[11/24 07:07:56 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/24 07:09:39 visual_prompt]: 	Training 100/553. train loss: 0.3157,	0.9191 s / batch. (data: 7.10e-04). ETA=10:50:45, max mem: 30.7 GB 
[11/24 07:11:12 visual_prompt]: 	Training 200/553. train loss: 0.0401,	0.9174 s / batch. (data: 2.70e-04). ETA=10:47:59, max mem: 30.7 GB 
[11/24 07:12:45 visual_prompt]: 	Training 300/553. train loss: 0.0757,	0.9417 s / batch. (data: 7.21e-04). ETA=11:03:36, max mem: 30.7 GB 
[11/24 07:14:19 visual_prompt]: 	Training 400/553. train loss: 0.0483,	0.9286 s / batch. (data: 2.54e-04). ETA=10:52:48, max mem: 30.7 GB 
[11/24 07:15:52 visual_prompt]: 	Training 500/553. train loss: 1.2887,	0.9226 s / batch. (data: 5.35e-03). ETA=10:47:01, max mem: 30.7 GB 
[11/24 07:16:42 visual_prompt]: Epoch 24 / 100: avg data time: 2.30e-02, avg batch time: 0.9506, average train loss: 0.2055
[11/24 07:17:36 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3073, average loss: 1.1678
[11/24 07:17:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.52	
[11/24 07:19:09 visual_prompt]: 	Test 100/162. loss: 3.281, 0.3222 s / batch. (data: 5.72e-05)max mem: 30.66532 GB 
[11/24 07:19:54 visual_prompt]: Inference (test):avg data time: 2.45e-04, avg batch time: 0.3061, average loss: 1.1850
[11/24 07:19:54 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.84	rocauc: 61.88	
[11/24 07:19:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/24 07:21:37 visual_prompt]: 	Training 100/553. train loss: 0.0162,	0.9481 s / batch. (data: 8.90e-03). ETA=11:02:32, max mem: 30.7 GB 
[11/24 07:23:11 visual_prompt]: 	Training 200/553. train loss: 0.7058,	0.9365 s / batch. (data: 5.83e-03). ETA=10:52:50, max mem: 30.7 GB 
[11/24 07:24:45 visual_prompt]: 	Training 300/553. train loss: 0.2612,	0.9249 s / batch. (data: 7.34e-04). ETA=10:43:12, max mem: 30.7 GB 
[11/24 07:26:18 visual_prompt]: 	Training 400/553. train loss: 0.4043,	0.9069 s / batch. (data: 2.54e-04). ETA=10:29:14, max mem: 30.7 GB 
[11/24 07:27:51 visual_prompt]: 	Training 500/553. train loss: 0.1160,	0.9275 s / batch. (data: 2.38e-04). ETA=10:41:58, max mem: 30.7 GB 
[11/24 07:28:40 visual_prompt]: Epoch 25 / 100: avg data time: 2.37e-02, avg batch time: 0.9511, average train loss: 0.1941
[11/24 07:29:35 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3075, average loss: 1.2699
[11/24 07:29:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 67.90	
[11/24 07:31:07 visual_prompt]: 	Test 100/162. loss: 0.241, 0.3003 s / batch. (data: 5.25e-05)max mem: 30.66532 GB 
[11/24 07:31:53 visual_prompt]: Inference (test):avg data time: 3.01e-05, avg batch time: 0.3053, average loss: 1.4154
[11/24 07:31:53 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.60	rocauc: 65.19	
[11/24 07:31:53 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/24 07:33:36 visual_prompt]: 	Training 100/553. train loss: 0.3336,	0.9388 s / batch. (data: 5.85e-03). ETA=10:47:21, max mem: 30.7 GB 
[11/24 07:35:10 visual_prompt]: 	Training 200/553. train loss: 0.0786,	0.9391 s / batch. (data: 7.04e-04). ETA=10:45:59, max mem: 30.7 GB 
[11/24 07:36:43 visual_prompt]: 	Training 300/553. train loss: 0.0180,	0.9127 s / batch. (data: 5.38e-03). ETA=10:26:21, max mem: 30.7 GB 
[11/24 07:38:16 visual_prompt]: 	Training 400/553. train loss: 0.0107,	0.9323 s / batch. (data: 5.36e-03). ETA=10:38:13, max mem: 30.7 GB 
[11/24 07:39:49 visual_prompt]: 	Training 500/553. train loss: 0.1640,	0.9323 s / batch. (data: 1.24e-02). ETA=10:36:40, max mem: 30.7 GB 
[11/24 07:40:39 visual_prompt]: Epoch 26 / 100: avg data time: 2.44e-02, avg batch time: 0.9505, average train loss: 0.1666
[11/24 07:41:33 visual_prompt]: Inference (val):avg data time: 2.28e-04, avg batch time: 0.3083, average loss: 1.2048
[11/24 07:41:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 68.03	
[11/24 07:43:05 visual_prompt]: 	Test 100/162. loss: 0.040, 0.3245 s / batch. (data: 3.10e-05)max mem: 30.66532 GB 
[11/24 07:43:51 visual_prompt]: Inference (test):avg data time: 5.07e-05, avg batch time: 0.3071, average loss: 1.3423
[11/24 07:43:51 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.47	rocauc: 64.36	
[11/24 07:43:51 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/24 07:45:31 visual_prompt]: 	Training 100/553. train loss: 0.1459,	0.9196 s / batch. (data: 8.81e-03). ETA=10:25:41, max mem: 30.7 GB 
[11/24 07:47:05 visual_prompt]: 	Training 200/553. train loss: 0.9382,	0.9480 s / batch. (data: 1.04e-02). ETA=10:43:24, max mem: 30.7 GB 
[11/24 07:48:38 visual_prompt]: 	Training 300/553. train loss: 1.0756,	0.9310 s / batch. (data: 1.12e-02). ETA=10:30:17, max mem: 30.7 GB 
[11/24 07:50:11 visual_prompt]: 	Training 400/553. train loss: 0.2074,	0.9413 s / batch. (data: 7.48e-04). ETA=10:35:43, max mem: 30.7 GB 
[11/24 07:51:45 visual_prompt]: 	Training 500/553. train loss: 0.1930,	0.9180 s / batch. (data: 2.86e-04). ETA=10:18:29, max mem: 30.7 GB 
[11/24 07:52:34 visual_prompt]: Epoch 27 / 100: avg data time: 1.78e-02, avg batch time: 0.9452, average train loss: 0.1640
[11/24 07:53:29 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3053, average loss: 1.2235
[11/24 07:53:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.90	
[11/24 07:55:01 visual_prompt]: 	Test 100/162. loss: 0.074, 0.3071 s / batch. (data: 3.15e-05)max mem: 30.66532 GB 
[11/24 07:55:47 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.3057, average loss: 1.3611
[11/24 07:55:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.09	rocauc: 65.92	
[11/24 07:55:47 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/24 07:57:28 visual_prompt]: 	Training 100/553. train loss: 0.0003,	0.9323 s / batch. (data: 1.09e-02). ETA=10:25:43, max mem: 30.7 GB 
[11/24 07:59:01 visual_prompt]: 	Training 200/553. train loss: 0.1527,	0.9232 s / batch. (data: 2.43e-04). ETA=10:18:03, max mem: 30.7 GB 
[11/24 08:00:35 visual_prompt]: 	Training 300/553. train loss: 0.7385,	0.9465 s / batch. (data: 5.84e-03). ETA=10:32:06, max mem: 30.7 GB 
[11/24 08:02:09 visual_prompt]: 	Training 400/553. train loss: 0.2107,	0.9316 s / batch. (data: 7.18e-04). ETA=10:20:36, max mem: 30.7 GB 
[11/24 08:03:42 visual_prompt]: 	Training 500/553. train loss: 0.0039,	0.9352 s / batch. (data: 7.26e-04). ETA=10:21:26, max mem: 30.7 GB 
[11/24 08:04:31 visual_prompt]: Epoch 28 / 100: avg data time: 2.09e-02, avg batch time: 0.9483, average train loss: 0.1377
[11/24 08:05:26 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3068, average loss: 1.2585
[11/24 08:05:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 70.14	
[11/24 08:06:58 visual_prompt]: 	Test 100/162. loss: 0.953, 0.2953 s / batch. (data: 5.60e-05)max mem: 30.66532 GB 
[11/24 08:07:44 visual_prompt]: Inference (test):avg data time: 3.53e-05, avg batch time: 0.3052, average loss: 1.6845
[11/24 08:07:44 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 64.95	
[11/24 08:07:44 visual_prompt]: Training 29 / 100 epoch, with learning rate 8.622126023955446e-05
[11/24 08:09:25 visual_prompt]: 	Training 100/553. train loss: 0.2392,	0.9295 s / batch. (data: 8.07e-03). ETA=10:15:14, max mem: 30.7 GB 
[11/24 08:10:58 visual_prompt]: 	Training 200/553. train loss: 0.0008,	0.9392 s / batch. (data: 7.34e-04). ETA=10:20:06, max mem: 30.7 GB 
[11/24 08:12:32 visual_prompt]: 	Training 300/553. train loss: 0.0200,	0.9228 s / batch. (data: 5.34e-03). ETA=10:07:43, max mem: 30.7 GB 
[11/24 08:14:05 visual_prompt]: 	Training 400/553. train loss: 0.1391,	0.9142 s / batch. (data: 8.88e-03). ETA=10:00:32, max mem: 30.7 GB 
[11/24 08:15:39 visual_prompt]: 	Training 500/553. train loss: 0.0287,	0.9279 s / batch. (data: 2.54e-04). ETA=10:08:01, max mem: 30.7 GB 
[11/24 08:16:28 visual_prompt]: Epoch 29 / 100: avg data time: 1.92e-02, avg batch time: 0.9475, average train loss: 0.1483
[11/24 08:17:23 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.3061, average loss: 1.2908
[11/24 08:17:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 63.34	
[11/24 08:18:56 visual_prompt]: 	Test 100/162. loss: 1.355, 0.2945 s / batch. (data: 3.19e-05)max mem: 30.66532 GB 
[11/24 08:19:43 visual_prompt]: Inference (test):avg data time: 3.07e-05, avg batch time: 0.3059, average loss: 1.3736
[11/24 08:19:43 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.59	rocauc: 60.99	
[11/24 08:19:43 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.506183921362443e-05
[11/24 08:21:23 visual_prompt]: 	Training 100/553. train loss: 0.3243,	0.9457 s / batch. (data: 1.10e-02). ETA=10:17:16, max mem: 30.7 GB 
[11/24 08:22:57 visual_prompt]: 	Training 200/553. train loss: 0.0032,	0.9529 s / batch. (data: 2.65e-04). ETA=10:20:23, max mem: 30.7 GB 
[11/24 08:24:30 visual_prompt]: 	Training 300/553. train loss: 0.0182,	0.9481 s / batch. (data: 8.29e-04). ETA=10:15:42, max mem: 30.7 GB 
[11/24 08:26:04 visual_prompt]: 	Training 400/553. train loss: 0.0107,	0.9250 s / batch. (data: 3.31e-04). ETA=9:59:06, max mem: 30.7 GB 
[11/24 08:27:38 visual_prompt]: 	Training 500/553. train loss: 0.0137,	0.9158 s / batch. (data: 2.61e-04). ETA=9:51:39, max mem: 30.7 GB 
[11/24 08:28:27 visual_prompt]: Epoch 30 / 100: avg data time: 1.84e-02, avg batch time: 0.9479, average train loss: 0.1178
[11/24 08:29:22 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3067, average loss: 1.3261
[11/24 08:29:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 68.77	
[11/24 08:30:56 visual_prompt]: 	Test 100/162. loss: 1.150, 0.2949 s / batch. (data: 3.77e-05)max mem: 30.66532 GB 
[11/24 08:31:42 visual_prompt]: Inference (test):avg data time: 7.57e-05, avg batch time: 0.3069, average loss: 1.6084
[11/24 08:31:42 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.84	rocauc: 62.51	
[11/24 08:31:42 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.386407858128706e-05
[11/24 08:33:26 visual_prompt]: 	Training 100/553. train loss: 0.1176,	0.9200 s / batch. (data: 2.58e-04). ETA=9:52:01, max mem: 30.7 GB 
[11/24 08:34:59 visual_prompt]: 	Training 200/553. train loss: 0.1749,	0.9301 s / batch. (data: 5.35e-03). ETA=9:56:57, max mem: 30.7 GB 
[11/24 08:36:33 visual_prompt]: 	Training 300/553. train loss: 0.6733,	0.9406 s / batch. (data: 8.80e-03). ETA=10:02:07, max mem: 30.7 GB 
[11/24 08:38:06 visual_prompt]: 	Training 400/553. train loss: 0.0603,	0.9494 s / batch. (data: 2.91e-02). ETA=10:06:09, max mem: 30.7 GB 
[11/24 08:39:39 visual_prompt]: 	Training 500/553. train loss: 0.0551,	0.9204 s / batch. (data: 1.04e-02). ETA=9:46:07, max mem: 30.7 GB 
[11/24 08:40:29 visual_prompt]: Epoch 31 / 100: avg data time: 2.43e-02, avg batch time: 0.9523, average train loss: 0.0950
[11/24 08:41:24 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.3085, average loss: 1.5085
[11/24 08:41:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 67.28	
[11/24 08:42:57 visual_prompt]: 	Test 100/162. loss: 1.418, 0.3100 s / batch. (data: 4.01e-05)max mem: 30.66532 GB 
[11/24 08:43:43 visual_prompt]: Inference (test):avg data time: 3.48e-05, avg batch time: 0.3066, average loss: 1.8731
[11/24 08:43:43 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.59	rocauc: 61.25	
[11/24 08:43:43 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.262928807620843e-05
[11/24 08:45:27 visual_prompt]: 	Training 100/553. train loss: 0.0788,	0.9443 s / batch. (data: 7.11e-04). ETA=9:58:56, max mem: 30.7 GB 
[11/24 08:47:01 visual_prompt]: 	Training 200/553. train loss: 0.0002,	0.9229 s / batch. (data: 1.04e-02). ETA=9:43:50, max mem: 30.7 GB 
[11/24 08:48:35 visual_prompt]: 	Training 300/553. train loss: 0.0153,	0.9262 s / batch. (data: 2.60e-04). ETA=9:44:22, max mem: 30.7 GB 
[11/24 08:50:09 visual_prompt]: 	Training 400/553. train loss: 0.0258,	0.9223 s / batch. (data: 1.11e-02). ETA=9:40:22, max mem: 30.7 GB 
[11/24 08:51:42 visual_prompt]: 	Training 500/553. train loss: 0.0125,	0.9378 s / batch. (data: 7.46e-04). ETA=9:48:33, max mem: 30.7 GB 
[11/24 08:52:32 visual_prompt]: Epoch 32 / 100: avg data time: 2.51e-02, avg batch time: 0.9549, average train loss: 0.1190
[11/24 08:53:27 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3056, average loss: 1.2729
[11/24 08:53:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 67.14	
[11/24 08:55:00 visual_prompt]: 	Test 100/162. loss: 1.101, 0.3107 s / batch. (data: 3.17e-05)max mem: 30.66532 GB 
[11/24 08:55:46 visual_prompt]: Inference (test):avg data time: 1.04e-04, avg batch time: 0.3056, average loss: 1.5626
[11/24 08:55:46 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 62.31	
[11/24 08:55:46 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.135881792367686e-05
[11/24 08:57:30 visual_prompt]: 	Training 100/553. train loss: 0.2689,	0.9533 s / batch. (data: 2.45e-02). ETA=9:55:52, max mem: 30.7 GB 
[11/24 08:59:03 visual_prompt]: 	Training 200/553. train loss: 0.1525,	0.9414 s / batch. (data: 6.94e-04). ETA=9:46:51, max mem: 30.7 GB 
[11/24 09:00:37 visual_prompt]: 	Training 300/553. train loss: 0.0045,	0.9244 s / batch. (data: 8.65e-03). ETA=9:34:42, max mem: 30.7 GB 
[11/24 09:02:11 visual_prompt]: 	Training 400/553. train loss: 0.0131,	0.9310 s / batch. (data: 2.46e-04). ETA=9:37:17, max mem: 30.7 GB 
[11/24 09:03:44 visual_prompt]: 	Training 500/553. train loss: 0.0065,	0.9271 s / batch. (data: 7.26e-04). ETA=9:33:17, max mem: 30.7 GB 
[11/24 09:04:33 visual_prompt]: Epoch 33 / 100: avg data time: 2.40e-02, avg batch time: 0.9528, average train loss: 0.1050
[11/24 09:05:28 visual_prompt]: Inference (val):avg data time: 3.07e-04, avg batch time: 0.3057, average loss: 1.3111
[11/24 09:05:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 69.56	
[11/24 09:07:01 visual_prompt]: 	Test 100/162. loss: 1.578, 0.2952 s / batch. (data: 3.98e-05)max mem: 30.66532 GB 
[11/24 09:07:48 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.3063, average loss: 1.5428
[11/24 09:07:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.47	rocauc: 61.08	
[11/24 09:07:48 visual_prompt]: Stopping early.
[11/24 09:07:48 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 09:07:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 09:07:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/24 09:07:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/24 09:07:48 visual_prompt]: Training with config:
[11/24 09:07:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/test/seed875/lr0.0001_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 875, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/24 09:07:48 visual_prompt]: Loading training data...
[11/24 09:07:48 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 09:07:48 visual_prompt]: Loading validation data...
[11/24 09:07:48 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 09:07:48 visual_prompt]: Loading test data...
[11/24 09:07:48 visual_prompt]: Constructing mammo-cbis dataset test...
[11/24 09:07:48 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 09:07:50 visual_prompt]: Enable all parameters update during training
[11/24 09:07:50 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/24 09:07:50 visual_prompt]: tuned percent:100.000
[11/24 09:07:50 visual_prompt]: Device used for model: 0
[11/24 09:07:50 visual_prompt]: Setting up Evaluator...
[11/24 09:07:50 visual_prompt]: Setting up Trainer...
[11/24 09:07:50 visual_prompt]: 	Setting up the optimizer...
[11/24 09:07:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 09:09:34 visual_prompt]: 	Training 100/553. train loss: 0.7594,	0.9448 s / batch. (data: 2.58e-04). ETA=14:29:14, max mem: 30.7 GB 
[11/24 09:11:07 visual_prompt]: 	Training 200/553. train loss: 1.2505,	0.9271 s / batch. (data: 2.46e-04). ETA=14:11:20, max mem: 30.7 GB 
[11/24 09:12:41 visual_prompt]: 	Training 300/553. train loss: 1.6675,	0.9237 s / batch. (data: 2.99e-04). ETA=14:06:42, max mem: 30.7 GB 
[11/24 09:14:14 visual_prompt]: 	Training 400/553. train loss: 0.8681,	0.9284 s / batch. (data: 2.66e-04). ETA=14:09:26, max mem: 30.7 GB 
[11/24 09:15:48 visual_prompt]: 	Training 500/553. train loss: 2.3298,	0.9319 s / batch. (data: 1.55e-02). ETA=14:11:07, max mem: 30.7 GB 
[11/24 09:16:37 visual_prompt]: Epoch 1 / 100: avg data time: 2.46e-02, avg batch time: 0.9529, average train loss: 1.8801
[11/24 09:17:32 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3073, average loss: 2.2501
[11/24 09:17:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 53.01	
[11/24 09:19:05 visual_prompt]: 	Test 100/162. loss: 0.464, 0.3172 s / batch. (data: 3.27e-05)max mem: 30.66532 GB 
[11/24 09:19:51 visual_prompt]: Inference (test):avg data time: 1.19e-04, avg batch time: 0.3046, average loss: 2.4060
[11/24 09:19:51 visual_prompt]: Classification results with test_mammo-cbis: top1: 49.30	rocauc: 48.51	
[11/24 09:19:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/24 09:21:33 visual_prompt]: 	Training 100/553. train loss: 0.7237,	0.9395 s / batch. (data: 1.55e-02). ETA=14:15:38, max mem: 30.7 GB 
[11/24 09:23:06 visual_prompt]: 	Training 200/553. train loss: 0.5434,	0.9183 s / batch. (data: 7.99e-04). ETA=13:54:51, max mem: 30.7 GB 
[11/24 09:24:39 visual_prompt]: 	Training 300/553. train loss: 1.8921,	0.9271 s / batch. (data: 1.11e-02). ETA=14:01:18, max mem: 30.7 GB 
[11/24 09:26:13 visual_prompt]: 	Training 400/553. train loss: 0.6106,	0.9531 s / batch. (data: 2.80e-02). ETA=14:23:19, max mem: 30.7 GB 
[11/24 09:27:47 visual_prompt]: 	Training 500/553. train loss: 0.3141,	0.9235 s / batch. (data: 5.39e-03). ETA=13:54:55, max mem: 30.7 GB 
[11/24 09:28:36 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e-02, avg batch time: 0.9494, average train loss: 0.8655
[11/24 09:29:31 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3042, average loss: 0.6771
[11/24 09:29:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 63.48	
[11/24 09:31:05 visual_prompt]: 	Test 100/162. loss: 0.454, 0.3281 s / batch. (data: 2.98e-05)max mem: 30.66532 GB 
[11/24 09:31:51 visual_prompt]: Inference (test):avg data time: 3.10e-05, avg batch time: 0.3041, average loss: 0.7193
[11/24 09:31:51 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.43	rocauc: 59.41	
[11/24 09:31:51 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/24 09:33:39 visual_prompt]: 	Training 100/553. train loss: 1.1044,	0.9172 s / batch. (data: 7.85e-04). ETA=13:46:52, max mem: 30.7 GB 
[11/24 09:35:12 visual_prompt]: 	Training 200/553. train loss: 1.3005,	0.9465 s / batch. (data: 5.38e-03). ETA=14:11:46, max mem: 30.7 GB 
[11/24 09:36:46 visual_prompt]: 	Training 300/553. train loss: 0.5329,	0.9343 s / batch. (data: 6.56e-03). ETA=13:59:15, max mem: 30.7 GB 
[11/24 09:38:19 visual_prompt]: 	Training 400/553. train loss: 0.8753,	0.9659 s / batch. (data: 2.79e-02). ETA=14:25:56, max mem: 30.7 GB 
[11/24 09:39:53 visual_prompt]: 	Training 500/553. train loss: 0.7587,	0.9145 s / batch. (data: 3.21e-04). ETA=13:38:23, max mem: 30.7 GB 
[11/24 09:40:42 visual_prompt]: Epoch 3 / 100: avg data time: 3.28e-02, avg batch time: 0.9600, average train loss: 0.7746
[11/24 09:41:37 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3034, average loss: 0.6482
[11/24 09:41:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 67.26	
[11/24 09:43:10 visual_prompt]: 	Test 100/162. loss: 0.644, 0.3017 s / batch. (data: 3.15e-05)max mem: 30.66532 GB 
[11/24 09:43:56 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.3056, average loss: 0.6375
[11/24 09:43:56 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.26	rocauc: 64.15	
[11/24 09:43:56 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/24 09:45:39 visual_prompt]: 	Training 100/553. train loss: 0.5178,	0.9346 s / batch. (data: 2.11e-04). ETA=13:53:58, max mem: 30.7 GB 
[11/24 09:47:12 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.9240 s / batch. (data: 2.64e-04). ETA=13:42:59, max mem: 30.7 GB 
[11/24 09:48:46 visual_prompt]: 	Training 300/553. train loss: 0.5756,	0.9655 s / batch. (data: 1.09e-02). ETA=14:18:20, max mem: 30.7 GB 
[11/24 09:50:19 visual_prompt]: 	Training 400/553. train loss: 0.5748,	0.9208 s / batch. (data: 2.82e-04). ETA=13:37:03, max mem: 30.7 GB 
[11/24 09:51:53 visual_prompt]: 	Training 500/553. train loss: 1.3852,	0.9396 s / batch. (data: 7.67e-04). ETA=13:52:11, max mem: 30.7 GB 
[11/24 09:52:43 visual_prompt]: Epoch 4 / 100: avg data time: 2.10e-02, avg batch time: 0.9510, average train loss: 0.7298
[11/24 09:53:38 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3062, average loss: 0.6357
[11/24 09:53:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 67.08	
[11/24 09:55:10 visual_prompt]: 	Test 100/162. loss: 0.536, 0.2946 s / batch. (data: 3.24e-05)max mem: 30.66532 GB 
[11/24 09:55:57 visual_prompt]: Inference (test):avg data time: 6.61e-05, avg batch time: 0.3053, average loss: 0.6556
[11/24 09:55:57 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.79	rocauc: 65.02	
[11/24 09:55:57 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/24 09:57:42 visual_prompt]: 	Training 100/553. train loss: 1.0224,	0.9234 s / batch. (data: 1.55e-02). ETA=13:35:28, max mem: 30.7 GB 
[11/24 09:59:16 visual_prompt]: 	Training 200/553. train loss: 0.5661,	0.9426 s / batch. (data: 1.97e-02). ETA=13:50:53, max mem: 30.7 GB 
[11/24 10:00:49 visual_prompt]: 	Training 300/553. train loss: 0.0890,	0.9359 s / batch. (data: 2.67e-04). ETA=13:43:22, max mem: 30.7 GB 
[11/24 10:02:23 visual_prompt]: 	Training 400/553. train loss: 0.5450,	0.9720 s / batch. (data: 7.55e-04). ETA=14:13:31, max mem: 30.7 GB 
[11/24 10:03:56 visual_prompt]: 	Training 500/553. train loss: 0.7077,	0.9113 s / batch. (data: 2.56e-04). ETA=13:18:44, max mem: 30.7 GB 
[11/24 10:04:46 visual_prompt]: Epoch 5 / 100: avg data time: 2.79e-02, avg batch time: 0.9557, average train loss: 0.7426
[11/24 10:05:40 visual_prompt]: Inference (val):avg data time: 8.44e-05, avg batch time: 0.3064, average loss: 0.6637
[11/24 10:05:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 68.40	
[11/24 10:07:12 visual_prompt]: 	Test 100/162. loss: 0.648, 0.2945 s / batch. (data: 6.10e-05)max mem: 30.66532 GB 
[11/24 10:07:58 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.3068, average loss: 0.6309
[11/24 10:07:58 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.79	rocauc: 67.68	
[11/24 10:07:58 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/24 10:09:40 visual_prompt]: 	Training 100/553. train loss: 0.5812,	0.9429 s / batch. (data: 1.83e-02). ETA=13:43:58, max mem: 30.7 GB 
[11/24 10:11:14 visual_prompt]: 	Training 200/553. train loss: 0.4160,	0.9202 s / batch. (data: 1.04e-02). ETA=13:22:36, max mem: 30.7 GB 
[11/24 10:12:47 visual_prompt]: 	Training 300/553. train loss: 0.5871,	0.9360 s / batch. (data: 2.58e-04). ETA=13:34:54, max mem: 30.7 GB 
[11/24 10:14:21 visual_prompt]: 	Training 400/553. train loss: 1.1414,	0.9227 s / batch. (data: 2.98e-04). ETA=13:21:43, max mem: 30.7 GB 
[11/24 10:15:55 visual_prompt]: 	Training 500/553. train loss: 0.5884,	0.9259 s / batch. (data: 3.99e-03). ETA=13:22:58, max mem: 30.7 GB 
[11/24 10:16:44 visual_prompt]: Epoch 6 / 100: avg data time: 2.14e-02, avg batch time: 0.9497, average train loss: 0.6957
[11/24 10:17:38 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3064, average loss: 0.6004
[11/24 10:17:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 74.76	
[11/24 10:19:11 visual_prompt]: 	Test 100/162. loss: 0.576, 0.3124 s / batch. (data: 3.03e-05)max mem: 30.66532 GB 
[11/24 10:19:56 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.3048, average loss: 0.6211
[11/24 10:19:56 visual_prompt]: Classification results with test_mammo-cbis: top1: 64.65	rocauc: 69.32	
[11/24 10:19:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/24 10:21:38 visual_prompt]: 	Training 100/553. train loss: 0.5012,	0.9310 s / batch. (data: 5.40e-03). ETA=13:25:04, max mem: 30.7 GB 
[11/24 10:23:11 visual_prompt]: 	Training 200/553. train loss: 0.8132,	0.9503 s / batch. (data: 7.31e-04). ETA=13:40:08, max mem: 30.7 GB 
[11/24 10:24:45 visual_prompt]: 	Training 300/553. train loss: 0.8412,	0.9199 s / batch. (data: 7.39e-04). ETA=13:12:20, max mem: 30.7 GB 
[11/24 10:26:18 visual_prompt]: 	Training 400/553. train loss: 0.5109,	0.9539 s / batch. (data: 5.88e-03). ETA=13:40:05, max mem: 30.7 GB 
[11/24 10:27:52 visual_prompt]: 	Training 500/553. train loss: 0.9293,	0.9329 s / batch. (data: 1.42e-02). ETA=13:20:29, max mem: 30.7 GB 
[11/24 10:28:41 visual_prompt]: Epoch 7 / 100: avg data time: 2.17e-02, avg batch time: 0.9488, average train loss: 0.6626
[11/24 10:29:36 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.3060, average loss: 0.7452
[11/24 10:29:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 74.27	
[11/24 10:31:08 visual_prompt]: 	Test 100/162. loss: 0.834, 0.2957 s / batch. (data: 4.01e-05)max mem: 30.66532 GB 
[11/24 10:31:54 visual_prompt]: Inference (test):avg data time: 7.75e-05, avg batch time: 0.3046, average loss: 0.8758
[11/24 10:31:54 visual_prompt]: Classification results with test_mammo-cbis: top1: 46.05	rocauc: 70.04	
[11/24 10:31:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/24 10:33:38 visual_prompt]: 	Training 100/553. train loss: 0.7651,	0.9648 s / batch. (data: 1.09e-02). ETA=13:45:20, max mem: 30.7 GB 
[11/24 10:35:11 visual_prompt]: 	Training 200/553. train loss: 1.3167,	0.9223 s / batch. (data: 5.34e-03). ETA=13:07:26, max mem: 30.7 GB 
[11/24 10:36:44 visual_prompt]: 	Training 300/553. train loss: 0.8135,	0.9328 s / batch. (data: 2.67e-04). ETA=13:14:53, max mem: 30.7 GB 
[11/24 10:38:18 visual_prompt]: 	Training 400/553. train loss: 0.4510,	0.9233 s / batch. (data: 1.44e-02). ETA=13:05:12, max mem: 30.7 GB 
[11/24 10:39:51 visual_prompt]: 	Training 500/553. train loss: 0.6356,	0.9259 s / batch. (data: 5.39e-03). ETA=13:05:56, max mem: 30.7 GB 
[11/24 10:40:41 visual_prompt]: Epoch 8 / 100: avg data time: 2.46e-02, avg batch time: 0.9517, average train loss: 0.6602
[11/24 10:41:35 visual_prompt]: Inference (val):avg data time: 1.32e-04, avg batch time: 0.3084, average loss: 0.6764
[11/24 10:41:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 72.64	
[11/24 10:43:07 visual_prompt]: 	Test 100/162. loss: 0.661, 0.2947 s / batch. (data: 4.27e-05)max mem: 30.66532 GB 
[11/24 10:43:53 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.3059, average loss: 0.6868
[11/24 10:43:53 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.24	rocauc: 65.13	
[11/24 10:43:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/24 10:45:38 visual_prompt]: 	Training 100/553. train loss: 1.5860,	0.9291 s / batch. (data: 5.37e-03). ETA=13:06:16, max mem: 30.7 GB 
[11/24 10:47:11 visual_prompt]: 	Training 200/553. train loss: 0.7147,	0.9251 s / batch. (data: 7.26e-04). ETA=13:01:18, max mem: 30.7 GB 
[11/24 10:48:45 visual_prompt]: 	Training 300/553. train loss: 1.0297,	0.9237 s / batch. (data: 7.51e-04). ETA=12:58:38, max mem: 30.7 GB 
[11/24 10:50:18 visual_prompt]: 	Training 400/553. train loss: 0.4326,	0.9169 s / batch. (data: 9.44e-03). ETA=12:51:20, max mem: 30.7 GB 
[11/24 10:51:52 visual_prompt]: 	Training 500/553. train loss: 0.5943,	0.9162 s / batch. (data: 2.57e-04). ETA=12:49:14, max mem: 30.7 GB 
[11/24 10:52:41 visual_prompt]: Epoch 9 / 100: avg data time: 2.77e-02, avg batch time: 0.9535, average train loss: 0.6592
[11/24 10:53:35 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3068, average loss: 0.7498
[11/24 10:53:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 71.20	
[11/24 10:55:07 visual_prompt]: 	Test 100/162. loss: 0.696, 0.2947 s / batch. (data: 5.15e-05)max mem: 30.66532 GB 
[11/24 10:55:53 visual_prompt]: Inference (test):avg data time: 3.17e-05, avg batch time: 0.3060, average loss: 0.7117
[11/24 10:55:53 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.09	rocauc: 68.08	
[11/24 10:55:53 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/24 10:57:36 visual_prompt]: 	Training 100/553. train loss: 0.7398,	0.9399 s / batch. (data: 4.17e-04). ETA=13:06:45, max mem: 30.7 GB 
[11/24 10:59:10 visual_prompt]: 	Training 200/553. train loss: 0.4476,	0.9324 s / batch. (data: 1.23e-02). ETA=12:58:52, max mem: 30.7 GB 
[11/24 11:00:43 visual_prompt]: 	Training 300/553. train loss: 0.2307,	0.9316 s / batch. (data: 2.64e-04). ETA=12:56:40, max mem: 30.7 GB 
[11/24 11:02:17 visual_prompt]: 	Training 400/553. train loss: 0.8411,	0.9217 s / batch. (data: 5.38e-03). ETA=12:46:54, max mem: 30.7 GB 
[11/24 11:03:50 visual_prompt]: 	Training 500/553. train loss: 0.4581,	0.9443 s / batch. (data: 7.14e-04). ETA=13:04:06, max mem: 30.7 GB 
[11/24 11:04:39 visual_prompt]: Epoch 10 / 100: avg data time: 2.31e-02, avg batch time: 0.9510, average train loss: 0.6129
[11/24 11:05:34 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3051, average loss: 0.6210
[11/24 11:05:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 71.56	
[11/24 11:07:07 visual_prompt]: 	Test 100/162. loss: 0.565, 0.2948 s / batch. (data: 3.19e-05)max mem: 30.66532 GB 
[11/24 11:07:53 visual_prompt]: Inference (test):avg data time: 3.07e-05, avg batch time: 0.3067, average loss: 0.6619
[11/24 11:07:53 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.41	rocauc: 66.94	
[11/24 11:07:53 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/24 11:09:35 visual_prompt]: 	Training 100/553. train loss: 0.5597,	0.9415 s / batch. (data: 5.82e-03). ETA=12:59:24, max mem: 30.7 GB 
[11/24 11:11:08 visual_prompt]: 	Training 200/553. train loss: 1.2874,	0.9259 s / batch. (data: 2.40e-04). ETA=12:44:57, max mem: 30.7 GB 
[11/24 11:12:42 visual_prompt]: 	Training 300/553. train loss: 0.2569,	0.9412 s / batch. (data: 7.39e-04). ETA=12:56:02, max mem: 30.7 GB 
[11/24 11:14:15 visual_prompt]: 	Training 400/553. train loss: 0.4382,	0.9372 s / batch. (data: 2.43e-04). ETA=12:51:11, max mem: 30.7 GB 
[11/24 11:15:49 visual_prompt]: 	Training 500/553. train loss: 0.8125,	0.9420 s / batch. (data: 5.81e-03). ETA=12:53:33, max mem: 30.7 GB 
[11/24 11:16:38 visual_prompt]: Epoch 11 / 100: avg data time: 2.14e-02, avg batch time: 0.9500, average train loss: 0.5940
[11/24 11:17:33 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.3056, average loss: 0.6491
[11/24 11:17:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.50	
[11/24 11:19:05 visual_prompt]: 	Test 100/162. loss: 0.623, 0.3138 s / batch. (data: 4.32e-05)max mem: 30.66532 GB 
[11/24 11:19:51 visual_prompt]: Inference (test):avg data time: 3.18e-05, avg batch time: 0.3056, average loss: 0.6860
[11/24 11:19:51 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.91	rocauc: 65.80	
[11/24 11:19:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/24 11:21:31 visual_prompt]: 	Training 100/553. train loss: 0.1930,	0.9480 s / batch. (data: 3.37e-04). ETA=12:56:01, max mem: 30.7 GB 
[11/24 11:23:05 visual_prompt]: 	Training 200/553. train loss: 0.6062,	0.9296 s / batch. (data: 2.49e-04). ETA=12:39:27, max mem: 30.7 GB 
[11/24 11:24:38 visual_prompt]: 	Training 300/553. train loss: 0.6999,	0.9294 s / batch. (data: 1.05e-02). ETA=12:37:42, max mem: 30.7 GB 
[11/24 11:26:12 visual_prompt]: 	Training 400/553. train loss: 0.3693,	0.9560 s / batch. (data: 2.83e-04). ETA=12:57:48, max mem: 30.7 GB 
[11/24 11:27:46 visual_prompt]: 	Training 500/553. train loss: 0.8497,	0.9131 s / batch. (data: 5.38e-03). ETA=12:21:23, max mem: 30.7 GB 
[11/24 11:28:35 visual_prompt]: Epoch 12 / 100: avg data time: 1.86e-02, avg batch time: 0.9483, average train loss: 0.5933
[11/24 11:29:32 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3038, average loss: 0.6596
[11/24 11:29:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 68.35	
[11/24 11:31:09 visual_prompt]: 	Test 100/162. loss: 1.016, 0.3360 s / batch. (data: 9.56e-05)max mem: 30.66532 GB 
[11/24 11:31:57 visual_prompt]: Inference (test):avg data time: 3.63e-05, avg batch time: 0.3052, average loss: 0.6433
[11/24 11:31:57 visual_prompt]: Classification results with test_mammo-cbis: top1: 64.96	rocauc: 66.48	
[11/24 11:31:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/24 11:33:42 visual_prompt]: 	Training 100/553. train loss: 0.4868,	0.9262 s / batch. (data: 2.75e-04). ETA=12:29:40, max mem: 30.7 GB 
[11/24 11:35:15 visual_prompt]: 	Training 200/553. train loss: 0.3324,	0.9146 s / batch. (data: 2.60e-04). ETA=12:18:46, max mem: 30.7 GB 
[11/24 11:36:49 visual_prompt]: 	Training 300/553. train loss: 0.7280,	0.9212 s / batch. (data: 2.73e-04). ETA=12:22:35, max mem: 30.7 GB 
[11/24 11:38:22 visual_prompt]: 	Training 400/553. train loss: 0.9791,	0.9480 s / batch. (data: 1.02e-03). ETA=12:42:32, max mem: 30.7 GB 
[11/24 11:39:56 visual_prompt]: 	Training 500/553. train loss: 0.4276,	0.9310 s / batch. (data: 7.97e-04). ETA=12:27:22, max mem: 30.7 GB 
[11/24 11:40:45 visual_prompt]: Epoch 13 / 100: avg data time: 2.63e-02, avg batch time: 0.9542, average train loss: 0.5730
[11/24 11:41:40 visual_prompt]: Inference (val):avg data time: 3.83e-04, avg batch time: 0.3066, average loss: 0.6934
[11/24 11:41:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 64.41	
[11/24 11:43:12 visual_prompt]: 	Test 100/162. loss: 0.500, 0.2991 s / batch. (data: 3.74e-05)max mem: 30.66532 GB 
[11/24 11:43:59 visual_prompt]: Inference (test):avg data time: 1.62e-04, avg batch time: 0.3061, average loss: 0.6582
[11/24 11:43:59 visual_prompt]: Classification results with test_mammo-cbis: top1: 64.65	rocauc: 68.29	
[11/24 11:43:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/24 11:45:42 visual_prompt]: 	Training 100/553. train loss: 0.1816,	0.9221 s / batch. (data: 7.50e-04). ETA=12:17:50, max mem: 30.7 GB 
[11/24 11:47:15 visual_prompt]: 	Training 200/553. train loss: 0.2033,	0.9405 s / batch. (data: 1.09e-02). ETA=12:31:02, max mem: 30.7 GB 
[11/24 11:48:49 visual_prompt]: 	Training 300/553. train loss: 0.7907,	0.9382 s / batch. (data: 2.43e-02). ETA=12:27:37, max mem: 30.7 GB 
[11/24 11:50:22 visual_prompt]: 	Training 400/553. train loss: 1.0723,	0.9382 s / batch. (data: 6.99e-04). ETA=12:26:00, max mem: 30.7 GB 
[11/24 11:51:56 visual_prompt]: 	Training 500/553. train loss: 0.5031,	0.9615 s / batch. (data: 5.82e-03). ETA=12:42:59, max mem: 30.7 GB 
[11/24 11:52:46 visual_prompt]: Epoch 14 / 100: avg data time: 2.33e-02, avg batch time: 0.9523, average train loss: 0.5275
[11/24 11:53:41 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3054, average loss: 0.7516
[11/24 11:53:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.49	
[11/24 11:55:17 visual_prompt]: 	Test 100/162. loss: 1.208, 0.2940 s / batch. (data: 3.27e-05)max mem: 30.66532 GB 
[11/24 11:56:05 visual_prompt]: Inference (test):avg data time: 1.72e-04, avg batch time: 0.3038, average loss: 0.7666
[11/24 11:56:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.41	rocauc: 65.41	
[11/24 11:56:05 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/24 11:57:50 visual_prompt]: 	Training 100/553. train loss: 0.3695,	0.9159 s / batch. (data: 2.45e-04). ETA=12:04:26, max mem: 30.7 GB 
[11/24 11:59:24 visual_prompt]: 	Training 200/553. train loss: 0.1944,	0.9472 s / batch. (data: 2.48e-04). ETA=12:27:36, max mem: 30.7 GB 
[11/24 12:00:57 visual_prompt]: 	Training 300/553. train loss: 0.3285,	0.9209 s / batch. (data: 1.09e-02). ETA=12:05:19, max mem: 30.7 GB 
[11/24 12:02:31 visual_prompt]: 	Training 400/553. train loss: 0.7595,	0.9685 s / batch. (data: 1.61e-02). ETA=12:41:10, max mem: 30.7 GB 
[11/24 12:04:05 visual_prompt]: 	Training 500/553. train loss: 0.5264,	0.9360 s / batch. (data: 2.76e-04). ETA=12:14:06, max mem: 30.7 GB 
[11/24 12:04:54 visual_prompt]: Epoch 15 / 100: avg data time: 2.80e-02, avg batch time: 0.9560, average train loss: 0.5052
[11/24 12:05:49 visual_prompt]: Inference (val):avg data time: 3.91e-04, avg batch time: 0.3050, average loss: 0.8788
[11/24 12:05:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.44	
[11/24 12:07:22 visual_prompt]: 	Test 100/162. loss: 0.732, 0.3081 s / batch. (data: 3.74e-05)max mem: 30.66532 GB 
[11/24 12:08:08 visual_prompt]: Inference (test):avg data time: 7.67e-05, avg batch time: 0.3031, average loss: 1.0301
[11/24 12:08:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 55.97	rocauc: 64.12	
[11/24 12:08:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/24 12:09:50 visual_prompt]: 	Training 100/553. train loss: 0.4383,	0.9340 s / batch. (data: 5.34e-03). ETA=12:10:09, max mem: 30.7 GB 
[11/24 12:11:24 visual_prompt]: 	Training 200/553. train loss: 0.2102,	0.9425 s / batch. (data: 2.67e-04). ETA=12:15:14, max mem: 30.7 GB 
[11/24 12:12:57 visual_prompt]: 	Training 300/553. train loss: 0.4587,	0.9280 s / batch. (data: 7.75e-04). ETA=12:02:20, max mem: 30.7 GB 
[11/24 12:14:31 visual_prompt]: 	Training 400/553. train loss: 0.3234,	0.9136 s / batch. (data: 7.65e-04). ETA=11:49:38, max mem: 30.7 GB 
[11/24 12:16:05 visual_prompt]: 	Training 500/553. train loss: 0.3443,	0.9195 s / batch. (data: 2.67e-04). ETA=11:52:41, max mem: 30.7 GB 
[11/24 12:16:54 visual_prompt]: Epoch 16 / 100: avg data time: 2.36e-02, avg batch time: 0.9515, average train loss: 0.4938
[11/24 12:17:51 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3051, average loss: 0.7491
[11/24 12:17:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 65.52	
[11/24 12:19:28 visual_prompt]: 	Test 100/162. loss: 0.890, 0.2940 s / batch. (data: 5.82e-05)max mem: 30.66532 GB 
[11/24 12:20:15 visual_prompt]: Inference (test):avg data time: 1.25e-04, avg batch time: 0.3039, average loss: 0.7956
[11/24 12:20:15 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.47	rocauc: 65.76	
[11/24 12:20:15 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/24 12:22:01 visual_prompt]: 	Training 100/553. train loss: 0.1345,	0.9301 s / batch. (data: 2.64e-04). ETA=11:58:30, max mem: 30.7 GB 
[11/24 12:23:34 visual_prompt]: 	Training 200/553. train loss: 0.6868,	0.9205 s / batch. (data: 5.34e-03). ETA=11:49:34, max mem: 30.7 GB 
[11/24 12:25:08 visual_prompt]: 	Training 300/553. train loss: 0.2172,	0.9233 s / batch. (data: 2.48e-04). ETA=11:50:11, max mem: 30.7 GB 
[11/24 12:26:42 visual_prompt]: 	Training 400/553. train loss: 0.8169,	0.9379 s / batch. (data: 7.40e-04). ETA=11:59:50, max mem: 30.7 GB 
[11/24 12:28:15 visual_prompt]: 	Training 500/553. train loss: 1.4272,	0.9230 s / batch. (data: 2.66e-04). ETA=11:46:53, max mem: 30.7 GB 
[11/24 12:29:04 visual_prompt]: Epoch 17 / 100: avg data time: 2.73e-02, avg batch time: 0.9565, average train loss: 0.4371
[11/24 12:29:59 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3055, average loss: 0.7632
[11/24 12:29:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.04	
[11/24 12:31:32 visual_prompt]: 	Test 100/162. loss: 1.179, 0.2943 s / batch. (data: 4.46e-05)max mem: 30.66532 GB 
[11/24 12:32:18 visual_prompt]: Inference (test):avg data time: 7.09e-05, avg batch time: 0.3034, average loss: 0.8219
[11/24 12:32:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.62	rocauc: 65.46	
[11/24 12:32:18 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/24 12:34:00 visual_prompt]: 	Training 100/553. train loss: 0.3766,	0.9602 s / batch. (data: 9.66e-03). ETA=12:12:56, max mem: 30.7 GB 
[11/24 12:35:33 visual_prompt]: 	Training 200/553. train loss: 0.3697,	0.9445 s / batch. (data: 7.21e-04). ETA=11:59:24, max mem: 30.7 GB 
[11/24 12:37:07 visual_prompt]: 	Training 300/553. train loss: 0.1260,	0.9563 s / batch. (data: 2.23e-02). ETA=12:06:46, max mem: 30.7 GB 
[11/24 12:38:41 visual_prompt]: 	Training 400/553. train loss: 1.0186,	0.9270 s / batch. (data: 5.35e-03). ETA=11:42:56, max mem: 30.7 GB 
[11/24 12:40:14 visual_prompt]: 	Training 500/553. train loss: 0.1165,	0.9678 s / batch. (data: 3.71e-02). ETA=12:12:16, max mem: 30.7 GB 
[11/24 12:41:04 visual_prompt]: Epoch 18 / 100: avg data time: 2.21e-02, avg batch time: 0.9508, average train loss: 0.3963
[11/24 12:41:59 visual_prompt]: Inference (val):avg data time: 3.83e-04, avg batch time: 0.3059, average loss: 0.8144
[11/24 12:41:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.16	
[11/24 12:43:31 visual_prompt]: 	Test 100/162. loss: 1.026, 0.3087 s / batch. (data: 6.63e-05)max mem: 30.66532 GB 
[11/24 12:44:17 visual_prompt]: Inference (test):avg data time: 7.67e-05, avg batch time: 0.3065, average loss: 0.8903
[11/24 12:44:17 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.59	rocauc: 63.15	
[11/24 12:44:17 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/24 12:45:59 visual_prompt]: 	Training 100/553. train loss: 0.5684,	0.9426 s / batch. (data: 8.29e-03). ETA=11:50:48, max mem: 30.7 GB 
[11/24 12:47:32 visual_prompt]: 	Training 200/553. train loss: 0.0884,	0.9312 s / batch. (data: 5.42e-03). ETA=11:40:40, max mem: 30.7 GB 
[11/24 12:49:07 visual_prompt]: 	Training 300/553. train loss: 1.0476,	0.9440 s / batch. (data: 2.90e-04). ETA=11:48:43, max mem: 30.7 GB 
[11/24 12:50:40 visual_prompt]: 	Training 400/553. train loss: 0.5436,	0.9493 s / batch. (data: 6.81e-04). ETA=11:51:05, max mem: 30.7 GB 
[11/24 12:52:14 visual_prompt]: 	Training 500/553. train loss: 0.1840,	0.9357 s / batch. (data: 5.40e-03). ETA=11:39:23, max mem: 30.7 GB 
[11/24 12:53:04 visual_prompt]: Epoch 19 / 100: avg data time: 2.12e-02, avg batch time: 0.9515, average train loss: 0.3679
[11/24 12:54:01 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3039, average loss: 0.8509
[11/24 12:54:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 68.90	
[11/24 12:55:37 visual_prompt]: 	Test 100/162. loss: 0.887, 0.3081 s / batch. (data: 4.03e-05)max mem: 30.66532 GB 
[11/24 12:56:24 visual_prompt]: Inference (test):avg data time: 1.22e-04, avg batch time: 0.3047, average loss: 0.9409
[11/24 12:56:24 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.88	rocauc: 65.79	
[11/24 12:56:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/24 12:58:07 visual_prompt]: 	Training 100/553. train loss: 0.4614,	0.9413 s / batch. (data: 2.53e-04). ETA=11:41:11, max mem: 30.7 GB 
[11/24 12:59:40 visual_prompt]: 	Training 200/553. train loss: 0.0686,	0.9498 s / batch. (data: 7.19e-04). ETA=11:45:52, max mem: 30.7 GB 
[11/24 13:01:14 visual_prompt]: 	Training 300/553. train loss: 0.2962,	0.9400 s / batch. (data: 4.29e-04). ETA=11:37:02, max mem: 30.7 GB 
[11/24 13:02:48 visual_prompt]: 	Training 400/553. train loss: 0.0821,	0.9154 s / batch. (data: 2.49e-04). ETA=11:17:17, max mem: 30.7 GB 
[11/24 13:04:22 visual_prompt]: 	Training 500/553. train loss: 0.0936,	0.9416 s / batch. (data: 7.72e-04). ETA=11:35:08, max mem: 30.7 GB 
[11/24 13:05:11 visual_prompt]: Epoch 20 / 100: avg data time: 2.17e-02, avg batch time: 0.9523, average train loss: 0.3360
[11/24 13:06:06 visual_prompt]: Inference (val):avg data time: 3.74e-04, avg batch time: 0.3050, average loss: 1.0802
[11/24 13:06:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 64.50	
[11/24 13:07:41 visual_prompt]: 	Test 100/162. loss: 1.615, 0.2941 s / batch. (data: 5.79e-05)max mem: 30.66532 GB 
[11/24 13:08:27 visual_prompt]: Inference (test):avg data time: 6.96e-05, avg batch time: 0.3044, average loss: 0.9613
[11/24 13:08:27 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.88	rocauc: 66.12	
[11/24 13:08:27 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/24 13:10:10 visual_prompt]: 	Training 100/553. train loss: 0.8414,	0.9168 s / batch. (data: 5.34e-03). ETA=11:14:29, max mem: 30.7 GB 
[11/24 13:11:44 visual_prompt]: 	Training 200/553. train loss: 0.7258,	0.9466 s / batch. (data: 1.32e-02). ETA=11:34:48, max mem: 30.7 GB 
[11/24 13:13:17 visual_prompt]: 	Training 300/553. train loss: 0.6676,	0.9279 s / batch. (data: 2.42e-04). ETA=11:19:31, max mem: 30.7 GB 
[11/24 13:14:50 visual_prompt]: 	Training 400/553. train loss: 0.0518,	0.9354 s / batch. (data: 7.06e-04). ETA=11:23:27, max mem: 30.7 GB 
[11/24 13:16:23 visual_prompt]: 	Training 500/553. train loss: 0.3943,	0.9504 s / batch. (data: 7.88e-04). ETA=11:32:48, max mem: 30.7 GB 
[11/24 13:17:13 visual_prompt]: Epoch 21 / 100: avg data time: 2.23e-02, avg batch time: 0.9501, average train loss: 0.3108
[11/24 13:18:08 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3071, average loss: 0.9910
[11/24 13:18:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 66.54	
[11/24 13:19:40 visual_prompt]: 	Test 100/162. loss: 1.147, 0.3108 s / batch. (data: 3.03e-05)max mem: 30.66532 GB 
[11/24 13:20:26 visual_prompt]: Inference (test):avg data time: 7.41e-05, avg batch time: 0.3069, average loss: 0.9471
[11/24 13:20:26 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.72	rocauc: 66.75	
[11/24 13:20:26 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/24 13:22:08 visual_prompt]: 	Training 100/553. train loss: 0.0062,	0.9509 s / batch. (data: 1.51e-02). ETA=11:30:44, max mem: 30.7 GB 
[11/24 13:23:41 visual_prompt]: 	Training 200/553. train loss: 0.0930,	0.9233 s / batch. (data: 5.35e-03). ETA=11:09:12, max mem: 30.7 GB 
[11/24 13:25:14 visual_prompt]: 	Training 300/553. train loss: 0.4479,	0.9163 s / batch. (data: 2.83e-04). ETA=11:02:35, max mem: 30.7 GB 
[11/24 13:26:48 visual_prompt]: 	Training 400/553. train loss: 0.0043,	0.9416 s / batch. (data: 1.55e-02). ETA=11:19:19, max mem: 30.7 GB 
[11/24 13:28:22 visual_prompt]: 	Training 500/553. train loss: 0.9375,	0.9326 s / batch. (data: 1.08e-02). ETA=11:11:17, max mem: 30.7 GB 
[11/24 13:29:11 visual_prompt]: Epoch 22 / 100: avg data time: 2.14e-02, avg batch time: 0.9493, average train loss: 0.2774
[11/24 13:30:06 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.3053, average loss: 1.1511
[11/24 13:30:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.22	
[11/24 13:31:38 visual_prompt]: 	Test 100/162. loss: 2.864, 0.2949 s / batch. (data: 3.79e-05)max mem: 30.66532 GB 
[11/24 13:32:24 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.3046, average loss: 1.0373
[11/24 13:32:24 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.79	rocauc: 64.43	
[11/24 13:32:24 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/24 13:34:07 visual_prompt]: 	Training 100/553. train loss: 0.4469,	0.9485 s / batch. (data: 7.08e-04). ETA=11:20:18, max mem: 30.7 GB 
[11/24 13:35:41 visual_prompt]: 	Training 200/553. train loss: 0.0191,	0.9266 s / batch. (data: 2.62e-04). ETA=11:03:02, max mem: 30.7 GB 
[11/24 13:37:14 visual_prompt]: 	Training 300/553. train loss: 0.0923,	0.9448 s / batch. (data: 1.55e-02). ETA=11:14:28, max mem: 30.7 GB 
[11/24 13:38:48 visual_prompt]: 	Training 400/553. train loss: 0.9145,	0.9226 s / batch. (data: 2.63e-04). ETA=10:57:05, max mem: 30.7 GB 
[11/24 13:40:22 visual_prompt]: 	Training 500/553. train loss: 0.1431,	0.9233 s / batch. (data: 5.36e-03). ETA=10:56:02, max mem: 30.7 GB 
[11/24 13:41:11 visual_prompt]: Epoch 23 / 100: avg data time: 2.29e-02, avg batch time: 0.9524, average train loss: 0.2129
[11/24 13:42:06 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3057, average loss: 1.1742
[11/24 13:42:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 65.27	
[11/24 13:43:39 visual_prompt]: 	Test 100/162. loss: 2.284, 0.3183 s / batch. (data: 5.89e-05)max mem: 30.66532 GB 
[11/24 13:44:24 visual_prompt]: Inference (test):avg data time: 5.26e-05, avg batch time: 0.3041, average loss: 1.2042
[11/24 13:44:24 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.33	rocauc: 65.08	
[11/24 13:44:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/24 13:46:07 visual_prompt]: 	Training 100/553. train loss: 0.2438,	0.9131 s / batch. (data: 2.59e-04). ETA=10:46:29, max mem: 30.7 GB 
[11/24 13:47:41 visual_prompt]: 	Training 200/553. train loss: 0.0697,	0.9475 s / batch. (data: 8.68e-03). ETA=11:09:18, max mem: 30.7 GB 
[11/24 13:49:14 visual_prompt]: 	Training 300/553. train loss: 0.2612,	0.9343 s / batch. (data: 1.52e-02). ETA=10:58:21, max mem: 30.7 GB 
[11/24 13:50:48 visual_prompt]: 	Training 400/553. train loss: 0.0823,	0.9455 s / batch. (data: 2.96e-04). ETA=11:04:43, max mem: 30.7 GB 
[11/24 13:52:22 visual_prompt]: 	Training 500/553. train loss: 0.0589,	0.9423 s / batch. (data: 8.19e-03). ETA=11:00:53, max mem: 30.7 GB 
[11/24 13:53:11 visual_prompt]: Epoch 24 / 100: avg data time: 2.30e-02, avg batch time: 0.9525, average train loss: 0.2184
[11/24 13:54:08 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3050, average loss: 1.2529
[11/24 13:54:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.49	
[11/24 13:55:45 visual_prompt]: 	Test 100/162. loss: 2.663, 0.3124 s / batch. (data: 3.31e-05)max mem: 30.66532 GB 
[11/24 13:56:32 visual_prompt]: Inference (test):avg data time: 8.03e-05, avg batch time: 0.3047, average loss: 1.1353
[11/24 13:56:32 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.24	rocauc: 61.43	
[11/24 13:56:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/24 13:58:16 visual_prompt]: 	Training 100/553. train loss: 0.0783,	0.9280 s / batch. (data: 2.68e-04). ETA=10:48:27, max mem: 30.7 GB 
[11/24 13:59:51 visual_prompt]: 	Training 200/553. train loss: 0.0269,	0.9622 s / batch. (data: 1.09e-02). ETA=11:10:44, max mem: 30.7 GB 
[11/24 14:01:25 visual_prompt]: 	Training 300/553. train loss: 0.0159,	0.9330 s / batch. (data: 5.48e-03). ETA=10:48:51, max mem: 30.7 GB 
[11/24 14:02:58 visual_prompt]: 	Training 400/553. train loss: 0.0034,	0.9239 s / batch. (data: 2.55e-04). ETA=10:40:58, max mem: 30.7 GB 
[11/24 14:04:31 visual_prompt]: 	Training 500/553. train loss: 0.0339,	0.9150 s / batch. (data: 7.93e-03). ETA=10:33:16, max mem: 30.7 GB 
[11/24 14:05:21 visual_prompt]: Epoch 25 / 100: avg data time: 2.83e-02, avg batch time: 0.9557, average train loss: 0.1749
[11/24 14:06:16 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3056, average loss: 1.2216
[11/24 14:06:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 62.67	
[11/24 14:07:49 visual_prompt]: 	Test 100/162. loss: 2.103, 0.2950 s / batch. (data: 4.12e-05)max mem: 30.66532 GB 
[11/24 14:08:35 visual_prompt]: Inference (test):avg data time: 8.01e-05, avg batch time: 0.3052, average loss: 1.2772
[11/24 14:08:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.33	rocauc: 63.96	
[11/24 14:08:35 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/24 14:10:19 visual_prompt]: 	Training 100/553. train loss: 0.0314,	0.9728 s / batch. (data: 5.88e-03). ETA=11:10:49, max mem: 30.7 GB 
[11/24 14:11:53 visual_prompt]: 	Training 200/553. train loss: 0.0213,	0.9324 s / batch. (data: 1.08e-02). ETA=10:41:23, max mem: 30.7 GB 
[11/24 14:13:26 visual_prompt]: 	Training 300/553. train loss: 0.0894,	0.9294 s / batch. (data: 1.04e-02). ETA=10:37:47, max mem: 30.7 GB 
[11/24 14:15:00 visual_prompt]: 	Training 400/553. train loss: 0.0313,	0.9302 s / batch. (data: 2.57e-04). ETA=10:36:49, max mem: 30.7 GB 
[11/24 14:16:33 visual_prompt]: 	Training 500/553. train loss: 0.1866,	0.9250 s / batch. (data: 7.73e-04). ETA=10:31:41, max mem: 30.7 GB 
[11/24 14:17:22 visual_prompt]: Epoch 26 / 100: avg data time: 2.42e-02, avg batch time: 0.9523, average train loss: 0.1592
[11/24 14:18:17 visual_prompt]: Inference (val):avg data time: 6.28e-05, avg batch time: 0.3052, average loss: 1.1144
[11/24 14:18:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 65.74	
[11/24 14:19:49 visual_prompt]: 	Test 100/162. loss: 1.783, 0.3045 s / batch. (data: 3.15e-05)max mem: 30.66532 GB 
[11/24 14:20:35 visual_prompt]: Inference (test):avg data time: 7.66e-05, avg batch time: 0.3048, average loss: 1.2617
[11/24 14:20:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.45	rocauc: 63.20	
[11/24 14:20:35 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/24 14:22:16 visual_prompt]: 	Training 100/553. train loss: 0.0028,	0.9320 s / batch. (data: 2.46e-04). ETA=10:34:06, max mem: 30.7 GB 
[11/24 14:23:49 visual_prompt]: 	Training 200/553. train loss: 0.0435,	0.9374 s / batch. (data: 5.35e-03). ETA=10:36:13, max mem: 30.7 GB 
[11/24 14:25:23 visual_prompt]: 	Training 300/553. train loss: 0.0862,	0.9480 s / batch. (data: 7.12e-04). ETA=10:41:51, max mem: 30.7 GB 
[11/24 14:26:57 visual_prompt]: 	Training 400/553. train loss: 0.2331,	0.9606 s / batch. (data: 1.09e-02). ETA=10:48:45, max mem: 30.7 GB 
[11/24 14:28:30 visual_prompt]: 	Training 500/553. train loss: 0.0109,	0.9300 s / batch. (data: 7.27e-04). ETA=10:26:32, max mem: 30.7 GB 
[11/24 14:29:19 visual_prompt]: Epoch 27 / 100: avg data time: 1.85e-02, avg batch time: 0.9469, average train loss: 0.1401
[11/24 14:30:14 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3061, average loss: 1.4549
[11/24 14:30:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.14	
[11/24 14:31:46 visual_prompt]: 	Test 100/162. loss: 4.812, 0.3117 s / batch. (data: 3.34e-05)max mem: 30.66532 GB 
[11/24 14:32:32 visual_prompt]: Inference (test):avg data time: 3.18e-05, avg batch time: 0.3054, average loss: 1.5517
[11/24 14:32:32 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.36	rocauc: 62.07	
[11/24 14:32:32 visual_prompt]: Stopping early.
[11/24 14:32:32 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 14:32:32 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 14:32:32 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/24 14:32:32 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/24 14:32:32 visual_prompt]: Training with config:
[11/24 14:32:32 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/test/seed4536/lr0.0001_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 4536, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/24 14:32:32 visual_prompt]: Loading training data...
[11/24 14:32:32 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 14:32:32 visual_prompt]: Loading validation data...
[11/24 14:32:32 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 14:32:32 visual_prompt]: Loading test data...
[11/24 14:32:32 visual_prompt]: Constructing mammo-cbis dataset test...
[11/24 14:32:32 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 14:32:38 visual_prompt]: Enable all parameters update during training
[11/24 14:32:38 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/24 14:32:38 visual_prompt]: tuned percent:100.000
[11/24 14:32:38 visual_prompt]: Device used for model: 0
[11/24 14:32:38 visual_prompt]: Setting up Evaluator...
[11/24 14:32:38 visual_prompt]: Setting up Trainer...
[11/24 14:32:38 visual_prompt]: 	Setting up the optimizer...
[11/24 14:32:38 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 14:34:20 visual_prompt]: 	Training 100/553. train loss: 3.9899,	0.9503 s / batch. (data: 1.72e-02). ETA=14:34:16, max mem: 30.7 GB 
[11/24 14:35:53 visual_prompt]: 	Training 200/553. train loss: 2.9413,	0.9090 s / batch. (data: 2.00e-03). ETA=13:54:43, max mem: 30.7 GB 
[11/24 14:37:26 visual_prompt]: 	Training 300/553. train loss: 4.5725,	0.9233 s / batch. (data: 8.45e-04). ETA=14:06:20, max mem: 30.7 GB 
[11/24 14:39:00 visual_prompt]: 	Training 400/553. train loss: 5.6711,	0.9424 s / batch. (data: 8.82e-03). ETA=14:22:19, max mem: 30.7 GB 
[11/24 14:40:34 visual_prompt]: 	Training 500/553. train loss: 2.0566,	0.9212 s / batch. (data: 5.34e-03). ETA=14:01:22, max mem: 30.7 GB 
[11/24 14:41:23 visual_prompt]: Epoch 1 / 100: avg data time: 1.96e-02, avg batch time: 0.9486, average train loss: 4.0989
[11/24 14:42:18 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.3078, average loss: 4.2766
[11/24 14:42:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.05	
[11/24 14:43:50 visual_prompt]: 	Test 100/162. loss: 4.810, 0.3073 s / batch. (data: 3.84e-05)max mem: 30.66532 GB 
[11/24 14:44:36 visual_prompt]: Inference (test):avg data time: 5.25e-05, avg batch time: 0.3049, average loss: 4.1827
[11/24 14:44:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.60	rocauc: 48.23	
[11/24 14:44:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/24 14:46:18 visual_prompt]: 	Training 100/553. train loss: 2.6774,	0.9156 s / batch. (data: 2.62e-04). ETA=13:53:55, max mem: 30.7 GB 
[11/24 14:47:51 visual_prompt]: 	Training 200/553. train loss: 0.7544,	0.9571 s / batch. (data: 2.93e-04). ETA=14:30:09, max mem: 30.7 GB 
[11/24 14:49:25 visual_prompt]: 	Training 300/553. train loss: 0.7851,	0.9454 s / batch. (data: 1.04e-02). ETA=14:17:54, max mem: 30.7 GB 
[11/24 14:50:59 visual_prompt]: 	Training 400/553. train loss: 0.9698,	0.9505 s / batch. (data: 7.28e-04). ETA=14:20:57, max mem: 30.7 GB 
[11/24 14:52:32 visual_prompt]: 	Training 500/553. train loss: 0.7385,	0.9173 s / batch. (data: 3.40e-04). ETA=13:49:18, max mem: 30.7 GB 
[11/24 14:53:22 visual_prompt]: Epoch 2 / 100: avg data time: 2.07e-02, avg batch time: 0.9496, average train loss: 0.9171
[11/24 14:54:19 visual_prompt]: Inference (val):avg data time: 1.92e-04, avg batch time: 0.3045, average loss: 0.6789
[11/24 14:54:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 60.41	
[11/24 14:55:55 visual_prompt]: 	Test 100/162. loss: 0.693, 0.3375 s / batch. (data: 3.22e-05)max mem: 30.66532 GB 
[11/24 14:56:43 visual_prompt]: Inference (test):avg data time: 7.92e-05, avg batch time: 0.3046, average loss: 0.6865
[11/24 14:56:43 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.16	rocauc: 54.24	
[11/24 14:56:43 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/24 14:58:26 visual_prompt]: 	Training 100/553. train loss: 0.6109,	0.9065 s / batch. (data: 2.70e-04). ETA=13:37:17, max mem: 30.7 GB 
[11/24 15:00:00 visual_prompt]: 	Training 200/553. train loss: 1.0391,	0.9531 s / batch. (data: 5.37e-03). ETA=14:17:39, max mem: 30.7 GB 
[11/24 15:01:34 visual_prompt]: 	Training 300/553. train loss: 0.5928,	0.9399 s / batch. (data: 7.70e-04). ETA=14:04:15, max mem: 30.7 GB 
[11/24 15:03:07 visual_prompt]: 	Training 400/553. train loss: 0.9678,	0.9645 s / batch. (data: 7.72e-04). ETA=14:24:43, max mem: 30.7 GB 
[11/24 15:04:42 visual_prompt]: 	Training 500/553. train loss: 0.6647,	0.9520 s / batch. (data: 8.17e-04). ETA=14:11:57, max mem: 30.7 GB 
[11/24 15:05:31 visual_prompt]: Epoch 3 / 100: avg data time: 2.33e-02, avg batch time: 0.9550, average train loss: 0.7618
[11/24 15:06:28 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3064, average loss: 0.6511
[11/24 15:06:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 65.23	
[11/24 15:08:05 visual_prompt]: 	Test 100/162. loss: 0.644, 0.3145 s / batch. (data: 3.39e-05)max mem: 30.66532 GB 
[11/24 15:08:54 visual_prompt]: Inference (test):avg data time: 1.80e-04, avg batch time: 0.3052, average loss: 0.6876
[11/24 15:08:54 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.16	rocauc: 60.48	
[11/24 15:08:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/24 15:10:38 visual_prompt]: 	Training 100/553. train loss: 0.7277,	0.9117 s / batch. (data: 2.49e-04). ETA=13:33:34, max mem: 30.7 GB 
[11/24 15:12:12 visual_prompt]: 	Training 200/553. train loss: 1.1588,	0.9310 s / batch. (data: 4.17e-04). ETA=13:49:13, max mem: 30.7 GB 
[11/24 15:13:47 visual_prompt]: 	Training 300/553. train loss: 0.2713,	0.9480 s / batch. (data: 3.04e-04). ETA=14:02:46, max mem: 30.7 GB 
[11/24 15:15:21 visual_prompt]: 	Training 400/553. train loss: 0.4652,	0.9535 s / batch. (data: 5.35e-03). ETA=14:06:05, max mem: 30.7 GB 
[11/24 15:16:55 visual_prompt]: 	Training 500/553. train loss: 0.6884,	0.9271 s / batch. (data: 4.00e-03). ETA=13:41:08, max mem: 30.7 GB 
[11/24 15:17:45 visual_prompt]: Epoch 4 / 100: avg data time: 2.81e-02, avg batch time: 0.9599, average train loss: 0.7611
[11/24 15:18:42 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3044, average loss: 0.9042
[11/24 15:18:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.65	
[11/24 15:20:18 visual_prompt]: 	Test 100/162. loss: 0.778, 0.3085 s / batch. (data: 5.15e-05)max mem: 30.66532 GB 
[11/24 15:21:06 visual_prompt]: Inference (test):avg data time: 2.24e-04, avg batch time: 0.3043, average loss: 0.8214
[11/24 15:21:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.84	rocauc: 64.54	
[11/24 15:21:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/24 15:22:51 visual_prompt]: 	Training 100/553. train loss: 0.5130,	0.9507 s / batch. (data: 7.80e-04). ETA=13:59:34, max mem: 30.7 GB 
[11/24 15:24:24 visual_prompt]: 	Training 200/553. train loss: 0.6931,	0.9289 s / batch. (data: 2.73e-04). ETA=13:38:45, max mem: 30.7 GB 
[11/24 15:25:58 visual_prompt]: 	Training 300/553. train loss: 0.7061,	0.9364 s / batch. (data: 8.35e-03). ETA=13:43:49, max mem: 30.7 GB 
[11/24 15:27:32 visual_prompt]: 	Training 400/553. train loss: 0.6709,	0.9301 s / batch. (data: 7.99e-03). ETA=13:36:46, max mem: 30.7 GB 
[11/24 15:29:07 visual_prompt]: 	Training 500/553. train loss: 0.5307,	0.9760 s / batch. (data: 5.37e-03). ETA=14:15:26, max mem: 30.7 GB 
[11/24 15:29:56 visual_prompt]: Epoch 5 / 100: avg data time: 2.62e-02, avg batch time: 0.9591, average train loss: 0.7089
[11/24 15:30:53 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3066, average loss: 0.6447
[11/24 15:30:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 69.70	
[11/24 15:32:30 visual_prompt]: 	Test 100/162. loss: 0.443, 0.3144 s / batch. (data: 3.31e-05)max mem: 30.66532 GB 
[11/24 15:33:18 visual_prompt]: Inference (test):avg data time: 7.85e-05, avg batch time: 0.3047, average loss: 0.6293
[11/24 15:33:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.79	rocauc: 66.98	
[11/24 15:33:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/24 15:35:02 visual_prompt]: 	Training 100/553. train loss: 0.4645,	0.9331 s / batch. (data: 1.41e-02). ETA=13:35:25, max mem: 30.7 GB 
[11/24 15:36:36 visual_prompt]: 	Training 200/553. train loss: 0.8862,	0.9141 s / batch. (data: 2.57e-04). ETA=13:17:20, max mem: 30.7 GB 
[11/24 15:38:10 visual_prompt]: 	Training 300/553. train loss: 0.9946,	0.9280 s / batch. (data: 2.49e-04). ETA=13:27:54, max mem: 30.7 GB 
[11/24 15:39:44 visual_prompt]: 	Training 400/553. train loss: 0.5244,	0.9520 s / batch. (data: 2.78e-04). ETA=13:47:12, max mem: 30.7 GB 
[11/24 15:41:17 visual_prompt]: 	Training 500/553. train loss: 0.5205,	0.9402 s / batch. (data: 1.05e-02). ETA=13:35:25, max mem: 30.7 GB 
[11/24 15:42:07 visual_prompt]: Epoch 6 / 100: avg data time: 2.53e-02, avg batch time: 0.9565, average train loss: 0.7011
[11/24 15:43:04 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3071, average loss: 0.6691
[11/24 15:43:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 71.39	
[11/24 15:44:41 visual_prompt]: 	Test 100/162. loss: 0.594, 0.3210 s / batch. (data: 5.22e-05)max mem: 30.66532 GB 
[11/24 15:45:29 visual_prompt]: Inference (test):avg data time: 3.34e-05, avg batch time: 0.3046, average loss: 0.6298
[11/24 15:45:29 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.17	rocauc: 70.75	
[11/24 15:45:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/24 15:47:14 visual_prompt]: 	Training 100/553. train loss: 0.7509,	0.9374 s / batch. (data: 5.39e-03). ETA=13:30:32, max mem: 30.7 GB 
[11/24 15:48:49 visual_prompt]: 	Training 200/553. train loss: 0.8068,	0.9761 s / batch. (data: 2.81e-02). ETA=14:02:25, max mem: 30.7 GB 
[11/24 15:50:24 visual_prompt]: 	Training 300/553. train loss: 0.4706,	0.9481 s / batch. (data: 1.63e-02). ETA=13:36:40, max mem: 30.7 GB 
[11/24 15:51:57 visual_prompt]: 	Training 400/553. train loss: 0.6828,	0.9606 s / batch. (data: 7.28e-04). ETA=13:45:48, max mem: 30.7 GB 
[11/24 15:53:32 visual_prompt]: 	Training 500/553. train loss: 0.5463,	0.9438 s / batch. (data: 7.05e-04). ETA=13:29:46, max mem: 30.7 GB 
[11/24 15:54:21 visual_prompt]: Epoch 7 / 100: avg data time: 2.97e-02, avg batch time: 0.9617, average train loss: 0.6670
[11/24 15:55:18 visual_prompt]: Inference (val):avg data time: 3.95e-04, avg batch time: 0.3060, average loss: 0.5882
[11/24 15:55:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 75.33	
[11/24 15:56:55 visual_prompt]: 	Test 100/162. loss: 0.571, 0.3039 s / batch. (data: 3.84e-05)max mem: 30.66532 GB 
[11/24 15:57:43 visual_prompt]: Inference (test):avg data time: 3.14e-04, avg batch time: 0.3049, average loss: 0.6302
[11/24 15:57:43 visual_prompt]: Classification results with test_mammo-cbis: top1: 66.98	rocauc: 67.96	
[11/24 15:57:43 visual_prompt]: Best epoch 7: best metric: -0.588
[11/24 15:57:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/24 15:59:25 visual_prompt]: 	Training 100/553. train loss: 0.6186,	0.9520 s / batch. (data: 7.85e-04). ETA=13:34:24, max mem: 30.7 GB 
[11/24 16:00:58 visual_prompt]: 	Training 200/553. train loss: 0.8336,	0.9403 s / batch. (data: 2.92e-04). ETA=13:22:52, max mem: 30.7 GB 
[11/24 16:02:32 visual_prompt]: 	Training 300/553. train loss: 1.4786,	0.9258 s / batch. (data: 3.28e-03). ETA=13:08:57, max mem: 30.7 GB 
[11/24 16:04:06 visual_prompt]: 	Training 400/553. train loss: 1.5383,	0.9480 s / batch. (data: 7.49e-04). ETA=13:26:14, max mem: 30.7 GB 
[11/24 16:05:39 visual_prompt]: 	Training 500/553. train loss: 0.4355,	0.9476 s / batch. (data: 1.09e-02). ETA=13:24:18, max mem: 30.7 GB 
[11/24 16:06:28 visual_prompt]: Epoch 8 / 100: avg data time: 2.10e-02, avg batch time: 0.9507, average train loss: 0.6667
[11/24 16:07:24 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3064, average loss: 0.6786
[11/24 16:07:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.75	
[11/24 16:08:55 visual_prompt]: 	Test 100/162. loss: 0.450, 0.3123 s / batch. (data: 5.01e-05)max mem: 30.66532 GB 
[11/24 16:09:42 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.3052, average loss: 0.7314
[11/24 16:09:42 visual_prompt]: Classification results with test_mammo-cbis: top1: 55.04	rocauc: 68.28	
[11/24 16:09:42 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/24 16:11:23 visual_prompt]: 	Training 100/553. train loss: 0.5344,	0.9461 s / batch. (data: 3.35e-04). ETA=13:20:41, max mem: 30.7 GB 
[11/24 16:12:56 visual_prompt]: 	Training 200/553. train loss: 0.6758,	0.9469 s / batch. (data: 1.08e-02). ETA=13:19:45, max mem: 30.7 GB 
[11/24 16:14:30 visual_prompt]: 	Training 300/553. train loss: 0.2822,	0.9402 s / batch. (data: 5.86e-03). ETA=13:12:30, max mem: 30.7 GB 
[11/24 16:16:04 visual_prompt]: 	Training 400/553. train loss: 0.2612,	0.9233 s / batch. (data: 3.43e-03). ETA=12:56:45, max mem: 30.7 GB 
[11/24 16:17:38 visual_prompt]: 	Training 500/553. train loss: 0.3479,	0.9426 s / batch. (data: 5.85e-03). ETA=13:11:26, max mem: 30.7 GB 
[11/24 16:18:27 visual_prompt]: Epoch 9 / 100: avg data time: 1.91e-02, avg batch time: 0.9500, average train loss: 0.6342
[11/24 16:19:24 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3042, average loss: 0.6810
[11/24 16:19:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 66.77	
[11/24 16:21:00 visual_prompt]: 	Test 100/162. loss: 0.474, 0.3148 s / batch. (data: 3.22e-05)max mem: 30.66532 GB 
[11/24 16:21:48 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.3038, average loss: 0.7628
[11/24 16:21:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 51.78	rocauc: 61.40	
[11/24 16:21:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/24 16:23:30 visual_prompt]: 	Training 100/553. train loss: 0.6299,	0.9315 s / batch. (data: 1.05e-02). ETA=12:59:41, max mem: 30.7 GB 
[11/24 16:25:04 visual_prompt]: 	Training 200/553. train loss: 0.5243,	0.9369 s / batch. (data: 1.55e-02). ETA=13:02:40, max mem: 30.7 GB 
[11/24 16:26:38 visual_prompt]: 	Training 300/553. train loss: 0.2738,	0.9398 s / batch. (data: 2.73e-04). ETA=13:03:33, max mem: 30.7 GB 
[11/24 16:28:14 visual_prompt]: 	Training 400/553. train loss: 0.5522,	0.9229 s / batch. (data: 3.04e-04). ETA=12:47:53, max mem: 30.7 GB 
[11/24 16:29:48 visual_prompt]: 	Training 500/553. train loss: 0.9121,	0.9263 s / batch. (data: 5.80e-03). ETA=12:49:11, max mem: 30.7 GB 
[11/24 16:30:38 visual_prompt]: Epoch 10 / 100: avg data time: 2.54e-02, avg batch time: 0.9582, average train loss: 0.5919
[11/24 16:31:34 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3054, average loss: 0.6463
[11/24 16:31:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.73	rocauc: 74.15	
[11/24 16:33:12 visual_prompt]: 	Test 100/162. loss: 0.714, 0.3021 s / batch. (data: 3.86e-05)max mem: 30.66532 GB 
[11/24 16:34:00 visual_prompt]: Inference (test):avg data time: 3.61e-05, avg batch time: 0.3060, average loss: 0.7658
[11/24 16:34:00 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.62	rocauc: 68.46	
[11/24 16:34:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/24 16:35:43 visual_prompt]: 	Training 100/553. train loss: 0.2792,	0.9264 s / batch. (data: 5.35e-03). ETA=12:46:56, max mem: 30.7 GB 
[11/24 16:37:20 visual_prompt]: 	Training 200/553. train loss: 0.4361,	0.9334 s / batch. (data: 2.82e-04). ETA=12:51:09, max mem: 30.7 GB 
[11/24 16:38:54 visual_prompt]: 	Training 300/553. train loss: 0.7424,	0.9202 s / batch. (data: 2.61e-04). ETA=12:38:44, max mem: 30.7 GB 
[11/24 16:40:28 visual_prompt]: 	Training 400/553. train loss: 0.7496,	0.9561 s / batch. (data: 7.07e-04). ETA=13:06:44, max mem: 30.7 GB 
[11/24 16:42:02 visual_prompt]: 	Training 500/553. train loss: 0.4911,	0.9248 s / batch. (data: 1.63e-02). ETA=12:39:27, max mem: 30.7 GB 
[11/24 16:42:51 visual_prompt]: Epoch 11 / 100: avg data time: 3.01e-02, avg batch time: 0.9603, average train loss: 0.6244
[11/24 16:43:46 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3071, average loss: 0.6043
[11/24 16:43:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 74.64	
[11/24 16:45:21 visual_prompt]: 	Test 100/162. loss: 0.352, 0.3134 s / batch. (data: 5.87e-05)max mem: 30.66532 GB 
[11/24 16:46:09 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.3049, average loss: 0.7278
[11/24 16:46:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.78	rocauc: 66.01	
[11/24 16:46:09 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/24 16:47:52 visual_prompt]: 	Training 100/553. train loss: 0.4032,	0.9199 s / batch. (data: 2.73e-04). ETA=12:33:05, max mem: 30.7 GB 
[11/24 16:49:25 visual_prompt]: 	Training 200/553. train loss: 0.3485,	0.9223 s / batch. (data: 7.31e-04). ETA=12:33:27, max mem: 30.7 GB 
[11/24 16:50:59 visual_prompt]: 	Training 300/553. train loss: 1.1444,	0.9421 s / batch. (data: 1.27e-03). ETA=12:48:05, max mem: 30.7 GB 
[11/24 16:52:33 visual_prompt]: 	Training 400/553. train loss: 0.8565,	0.9270 s / batch. (data: 6.44e-04). ETA=12:34:14, max mem: 30.7 GB 
[11/24 16:54:06 visual_prompt]: 	Training 500/553. train loss: 0.6973,	0.9523 s / batch. (data: 1.63e-02). ETA=12:53:12, max mem: 30.7 GB 
[11/24 16:54:56 visual_prompt]: Epoch 12 / 100: avg data time: 2.37e-02, avg batch time: 0.9521, average train loss: 0.5945
[11/24 16:55:53 visual_prompt]: Inference (val):avg data time: 5.50e-04, avg batch time: 0.3060, average loss: 0.8374
[11/24 16:55:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.94	
[11/24 16:57:29 visual_prompt]: 	Test 100/162. loss: 0.330, 0.3351 s / batch. (data: 4.65e-05)max mem: 30.66532 GB 
[11/24 16:58:17 visual_prompt]: Inference (test):avg data time: 7.90e-05, avg batch time: 0.3056, average loss: 0.7987
[11/24 16:58:17 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.17	rocauc: 65.91	
[11/24 16:58:17 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/24 17:00:01 visual_prompt]: 	Training 100/553. train loss: 1.1882,	0.9420 s / batch. (data: 2.57e-04). ETA=12:42:27, max mem: 30.7 GB 
[11/24 17:01:36 visual_prompt]: 	Training 200/553. train loss: 0.2764,	0.9640 s / batch. (data: 7.26e-04). ETA=12:58:40, max mem: 30.7 GB 
[11/24 17:03:10 visual_prompt]: 	Training 300/553. train loss: 0.7661,	0.9370 s / batch. (data: 5.34e-03). ETA=12:35:18, max mem: 30.7 GB 
[11/24 17:04:43 visual_prompt]: 	Training 400/553. train loss: 0.8036,	0.9507 s / batch. (data: 1.09e-02). ETA=12:44:43, max mem: 30.7 GB 
[11/24 17:06:17 visual_prompt]: 	Training 500/553. train loss: 1.0599,	0.9383 s / batch. (data: 5.35e-03). ETA=12:33:13, max mem: 30.7 GB 
[11/24 17:07:07 visual_prompt]: Epoch 13 / 100: avg data time: 2.68e-02, avg batch time: 0.9574, average train loss: 0.5760
[11/24 17:08:04 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3049, average loss: 0.8312
[11/24 17:08:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 66.72	
[11/24 17:09:39 visual_prompt]: 	Test 100/162. loss: 0.586, 0.2954 s / batch. (data: 3.55e-05)max mem: 30.66532 GB 
[11/24 17:10:27 visual_prompt]: Inference (test):avg data time: 2.78e-04, avg batch time: 0.3054, average loss: 0.7743
[11/24 17:10:27 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.95	rocauc: 65.77	
[11/24 17:10:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/24 17:12:12 visual_prompt]: 	Training 100/553. train loss: 0.6359,	0.9295 s / batch. (data: 1.78e-02). ETA=12:23:44, max mem: 30.7 GB 
[11/24 17:13:48 visual_prompt]: 	Training 200/553. train loss: 0.9658,	0.9400 s / batch. (data: 7.42e-04). ETA=12:30:36, max mem: 30.7 GB 
[11/24 17:15:22 visual_prompt]: 	Training 300/553. train loss: 0.2967,	0.9184 s / batch. (data: 2.59e-04). ETA=12:11:49, max mem: 30.7 GB 
[11/24 17:16:56 visual_prompt]: 	Training 400/553. train loss: 0.7165,	0.9433 s / batch. (data: 1.09e-02). ETA=12:30:03, max mem: 30.7 GB 
[11/24 17:18:29 visual_prompt]: 	Training 500/553. train loss: 0.6758,	0.9225 s / batch. (data: 2.78e-04). ETA=12:11:59, max mem: 30.7 GB 
[11/24 17:19:19 visual_prompt]: Epoch 14 / 100: avg data time: 3.04e-02, avg batch time: 0.9613, average train loss: 0.5474
[11/24 17:20:14 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3053, average loss: 0.6948
[11/24 17:20:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 65.68	
[11/24 17:21:50 visual_prompt]: 	Test 100/162. loss: 0.855, 0.2944 s / batch. (data: 3.46e-05)max mem: 30.66532 GB 
[11/24 17:22:38 visual_prompt]: Inference (test):avg data time: 1.50e-04, avg batch time: 0.3043, average loss: 0.7178
[11/24 17:22:38 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.16	rocauc: 64.58	
[11/24 17:22:38 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/24 17:24:18 visual_prompt]: 	Training 100/553. train loss: 0.3649,	0.9464 s / batch. (data: 7.21e-04). ETA=12:28:36, max mem: 30.7 GB 
[11/24 17:25:52 visual_prompt]: 	Training 200/553. train loss: 0.9441,	0.9090 s / batch. (data: 3.01e-04). ETA=11:57:26, max mem: 30.7 GB 
[11/24 17:27:25 visual_prompt]: 	Training 300/553. train loss: 0.2984,	0.9266 s / batch. (data: 3.97e-03). ETA=12:09:47, max mem: 30.7 GB 
[11/24 17:28:59 visual_prompt]: 	Training 400/553. train loss: 0.4526,	0.9285 s / batch. (data: 2.76e-04). ETA=12:09:48, max mem: 30.7 GB 
[11/24 17:30:35 visual_prompt]: 	Training 500/553. train loss: 0.6110,	0.9440 s / batch. (data: 2.85e-04). ETA=12:20:22, max mem: 30.7 GB 
[11/24 17:31:25 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e-02, avg batch time: 0.9536, average train loss: 0.5419
[11/24 17:32:22 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3051, average loss: 0.6439
[11/24 17:32:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.15	
[11/24 17:33:58 visual_prompt]: 	Test 100/162. loss: 0.546, 0.3187 s / batch. (data: 3.05e-05)max mem: 30.66532 GB 
[11/24 17:34:47 visual_prompt]: Inference (test):avg data time: 1.83e-04, avg batch time: 0.3059, average loss: 0.7273
[11/24 17:34:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.57	rocauc: 65.30	
[11/24 17:34:47 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/24 17:36:34 visual_prompt]: 	Training 100/553. train loss: 0.3145,	0.9131 s / batch. (data: 2.75e-04). ETA=11:53:50, max mem: 30.7 GB 
[11/24 17:38:07 visual_prompt]: 	Training 200/553. train loss: 0.2052,	0.9471 s / batch. (data: 1.36e-02). ETA=12:18:49, max mem: 30.7 GB 
[11/24 17:39:42 visual_prompt]: 	Training 300/553. train loss: 0.6397,	0.9313 s / batch. (data: 2.79e-04). ETA=12:04:55, max mem: 30.7 GB 
[11/24 17:41:16 visual_prompt]: 	Training 400/553. train loss: 0.4191,	0.9283 s / batch. (data: 2.93e-04). ETA=12:01:02, max mem: 30.7 GB 
[11/24 17:42:49 visual_prompt]: 	Training 500/553. train loss: 0.5622,	0.9299 s / batch. (data: 2.74e-04). ETA=12:00:45, max mem: 30.7 GB 
[11/24 17:43:39 visual_prompt]: Epoch 16 / 100: avg data time: 3.14e-02, avg batch time: 0.9617, average train loss: 0.4863
[11/24 17:44:33 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.3041, average loss: 0.9686
[11/24 17:44:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 64.89	
[11/24 17:46:09 visual_prompt]: 	Test 100/162. loss: 0.329, 0.3017 s / batch. (data: 4.91e-05)max mem: 30.66532 GB 
[11/24 17:46:56 visual_prompt]: Inference (test):avg data time: 7.65e-05, avg batch time: 0.3049, average loss: 1.0574
[11/24 17:46:56 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.45	rocauc: 64.04	
[11/24 17:46:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/24 17:48:39 visual_prompt]: 	Training 100/553. train loss: 2.4406,	0.9320 s / batch. (data: 8.01e-03). ETA=11:59:59, max mem: 30.7 GB 
[11/24 17:50:12 visual_prompt]: 	Training 200/553. train loss: 1.2067,	0.9280 s / batch. (data: 7.51e-04). ETA=11:55:22, max mem: 30.7 GB 
[11/24 17:51:46 visual_prompt]: 	Training 300/553. train loss: 0.3969,	0.9520 s / batch. (data: 7.30e-04). ETA=12:12:16, max mem: 30.7 GB 
[11/24 17:53:20 visual_prompt]: 	Training 400/553. train loss: 0.3673,	0.9461 s / batch. (data: 7.24e-04). ETA=12:06:07, max mem: 30.7 GB 
[11/24 17:54:53 visual_prompt]: 	Training 500/553. train loss: 0.2170,	0.9262 s / batch. (data: 7.40e-03). ETA=11:49:19, max mem: 30.7 GB 
[11/24 17:55:43 visual_prompt]: Epoch 17 / 100: avg data time: 2.31e-02, avg batch time: 0.9511, average train loss: 0.4516
[11/24 17:56:37 visual_prompt]: Inference (val):avg data time: 2.75e-04, avg batch time: 0.3067, average loss: 0.8663
[11/24 17:56:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.97	
[11/24 17:58:10 visual_prompt]: 	Test 100/162. loss: 0.885, 0.3051 s / batch. (data: 5.03e-05)max mem: 30.66532 GB 
[11/24 17:58:55 visual_prompt]: Inference (test):avg data time: 3.07e-05, avg batch time: 0.3058, average loss: 0.9876
[11/24 17:58:55 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.29	rocauc: 64.18	
[11/24 17:58:55 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/24 18:00:39 visual_prompt]: 	Training 100/553. train loss: 0.0512,	0.9400 s / batch. (data: 7.99e-03). ETA=11:57:30, max mem: 30.7 GB 
[11/24 18:02:17 visual_prompt]: 	Training 200/553. train loss: 1.3648,	0.9776 s / batch. (data: 5.82e-03). ETA=12:24:33, max mem: 30.7 GB 
[11/24 18:03:50 visual_prompt]: 	Training 300/553. train loss: 1.2858,	0.9280 s / batch. (data: 2.85e-04). ETA=11:45:14, max mem: 30.7 GB 
[11/24 18:05:25 visual_prompt]: 	Training 400/553. train loss: 0.0705,	0.9389 s / batch. (data: 5.82e-03). ETA=11:52:00, max mem: 30.7 GB 
[11/24 18:06:59 visual_prompt]: 	Training 500/553. train loss: 0.7180,	0.9187 s / batch. (data: 6.98e-04). ETA=11:35:08, max mem: 30.7 GB 
[11/24 18:07:48 visual_prompt]: Epoch 18 / 100: avg data time: 3.23e-02, avg batch time: 0.9631, average train loss: 0.4273
[11/24 18:08:45 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3058, average loss: 0.8836
[11/24 18:08:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 64.80	
[11/24 18:10:21 visual_prompt]: 	Test 100/162. loss: 0.622, 0.3055 s / batch. (data: 3.48e-05)max mem: 30.66532 GB 
[11/24 18:11:09 visual_prompt]: Inference (test):avg data time: 1.33e-04, avg batch time: 0.3037, average loss: 0.8691
[11/24 18:11:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.48	rocauc: 62.86	
[11/24 18:11:09 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/24 18:12:50 visual_prompt]: 	Training 100/553. train loss: 0.1947,	0.9393 s / batch. (data: 2.54e-04). ETA=11:48:21, max mem: 30.7 GB 
[11/24 18:14:24 visual_prompt]: 	Training 200/553. train loss: 0.2761,	0.9567 s / batch. (data: 2.47e-02). ETA=11:59:50, max mem: 30.7 GB 
[11/24 18:15:57 visual_prompt]: 	Training 300/553. train loss: 1.0731,	0.9489 s / batch. (data: 2.30e-02). ETA=11:52:25, max mem: 30.7 GB 
[11/24 18:17:31 visual_prompt]: 	Training 400/553. train loss: 0.3753,	0.9223 s / batch. (data: 2.56e-04). ETA=11:30:52, max mem: 30.7 GB 
[11/24 18:19:05 visual_prompt]: 	Training 500/553. train loss: 0.5476,	0.9520 s / batch. (data: 9.70e-04). ETA=11:51:32, max mem: 30.7 GB 
[11/24 18:19:54 visual_prompt]: Epoch 19 / 100: avg data time: 2.06e-02, avg batch time: 0.9499, average train loss: 0.4118
[11/24 18:20:49 visual_prompt]: Inference (val):avg data time: 3.56e-04, avg batch time: 0.3069, average loss: 0.8114
[11/24 18:20:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 70.60	
[11/24 18:22:21 visual_prompt]: 	Test 100/162. loss: 0.821, 0.3171 s / batch. (data: 4.82e-05)max mem: 30.66532 GB 
[11/24 18:23:07 visual_prompt]: Inference (test):avg data time: 7.58e-05, avg batch time: 0.3064, average loss: 0.9861
[11/24 18:23:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.84	rocauc: 63.41	
[11/24 18:23:07 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/24 18:24:49 visual_prompt]: 	Training 100/553. train loss: 0.2273,	0.9255 s / batch. (data: 2.47e-04). ETA=11:29:21, max mem: 30.7 GB 
[11/24 18:26:23 visual_prompt]: 	Training 200/553. train loss: 0.0215,	0.9557 s / batch. (data: 7.17e-04). ETA=11:50:19, max mem: 30.7 GB 
[11/24 18:27:56 visual_prompt]: 	Training 300/553. train loss: 0.1880,	0.9336 s / batch. (data: 2.70e-02). ETA=11:32:17, max mem: 30.7 GB 
[11/24 18:29:30 visual_prompt]: 	Training 400/553. train loss: 0.8724,	0.9134 s / batch. (data: 6.73e-03). ETA=11:15:47, max mem: 30.7 GB 
[11/24 18:31:03 visual_prompt]: 	Training 500/553. train loss: 0.4216,	0.9270 s / batch. (data: 5.38e-03). ETA=11:24:18, max mem: 30.7 GB 
[11/24 18:31:53 visual_prompt]: Epoch 20 / 100: avg data time: 2.15e-02, avg batch time: 0.9501, average train loss: 0.3580
[11/24 18:32:48 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3059, average loss: 0.8119
[11/24 18:32:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 64.78	
[11/24 18:34:21 visual_prompt]: 	Test 100/162. loss: 0.693, 0.3054 s / batch. (data: 3.96e-05)max mem: 30.66532 GB 
[11/24 18:35:07 visual_prompt]: Inference (test):avg data time: 3.11e-05, avg batch time: 0.3056, average loss: 0.8162
[11/24 18:35:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.47	rocauc: 63.42	
[11/24 18:35:07 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/24 18:36:49 visual_prompt]: 	Training 100/553. train loss: 0.3667,	0.9077 s / batch. (data: 2.65e-04). ETA=11:07:44, max mem: 30.7 GB 
[11/24 18:38:22 visual_prompt]: 	Training 200/553. train loss: 0.0438,	0.9582 s / batch. (data: 7.50e-04). ETA=11:43:20, max mem: 30.7 GB 
[11/24 18:39:56 visual_prompt]: 	Training 300/553. train loss: 0.6046,	0.9177 s / batch. (data: 3.04e-04). ETA=11:12:01, max mem: 30.7 GB 
[11/24 18:41:30 visual_prompt]: 	Training 400/553. train loss: 0.3324,	0.9290 s / batch. (data: 7.16e-04). ETA=11:18:49, max mem: 30.7 GB 
[11/24 18:43:03 visual_prompt]: 	Training 500/553. train loss: 0.1843,	0.9640 s / batch. (data: 6.99e-04). ETA=11:42:46, max mem: 30.7 GB 
[11/24 18:43:53 visual_prompt]: Epoch 21 / 100: avg data time: 2.05e-02, avg batch time: 0.9504, average train loss: 0.3045
[11/24 18:44:48 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3065, average loss: 0.8673
[11/24 18:44:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.74	
[11/24 18:46:21 visual_prompt]: 	Test 100/162. loss: 0.055, 0.3090 s / batch. (data: 3.24e-05)max mem: 30.66532 GB 
[11/24 18:47:08 visual_prompt]: Inference (test):avg data time: 3.08e-05, avg batch time: 0.3044, average loss: 0.8654
[11/24 18:47:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.17	rocauc: 63.60	
[11/24 18:47:08 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/24 18:48:51 visual_prompt]: 	Training 100/553. train loss: 0.0793,	0.9210 s / batch. (data: 2.49e-04). ETA=11:09:04, max mem: 30.7 GB 
[11/24 18:50:25 visual_prompt]: 	Training 200/553. train loss: 0.7835,	0.9810 s / batch. (data: 5.81e-03). ETA=11:50:59, max mem: 30.7 GB 
[11/24 18:51:58 visual_prompt]: 	Training 300/553. train loss: 0.0959,	0.9444 s / batch. (data: 5.84e-03). ETA=11:22:54, max mem: 30.7 GB 
[11/24 18:53:32 visual_prompt]: 	Training 400/553. train loss: 0.0892,	0.9153 s / batch. (data: 2.26e-04). ETA=11:00:21, max mem: 30.7 GB 
[11/24 18:55:05 visual_prompt]: 	Training 500/553. train loss: 0.3528,	0.9210 s / batch. (data: 9.60e-03). ETA=11:02:55, max mem: 30.7 GB 
[11/24 18:55:55 visual_prompt]: Epoch 22 / 100: avg data time: 2.40e-02, avg batch time: 0.9529, average train loss: 0.2834
[11/24 18:56:50 visual_prompt]: Inference (val):avg data time: 2.76e-04, avg batch time: 0.3056, average loss: 1.2028
[11/24 18:56:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 63.36	
[11/24 18:58:23 visual_prompt]: 	Test 100/162. loss: 0.435, 0.3069 s / batch. (data: 3.29e-05)max mem: 30.66532 GB 
[11/24 18:59:09 visual_prompt]: Inference (test):avg data time: 3.17e-05, avg batch time: 0.3046, average loss: 1.0892
[11/24 18:59:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 64.65	rocauc: 66.35	
[11/24 18:59:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/24 19:00:51 visual_prompt]: 	Training 100/553. train loss: 0.0557,	0.9240 s / batch. (data: 2.54e-04). ETA=11:02:44, max mem: 30.7 GB 
[11/24 19:02:25 visual_prompt]: 	Training 200/553. train loss: 0.0106,	0.9194 s / batch. (data: 2.56e-04). ETA=10:57:51, max mem: 30.7 GB 
[11/24 19:03:59 visual_prompt]: 	Training 300/553. train loss: 0.0077,	0.9270 s / batch. (data: 1.06e-03). ETA=11:01:47, max mem: 30.7 GB 
[11/24 19:05:32 visual_prompt]: 	Training 400/553. train loss: 0.3380,	0.9207 s / batch. (data: 3.78e-04). ETA=10:55:46, max mem: 30.7 GB 
[11/24 19:07:06 visual_prompt]: 	Training 500/553. train loss: 0.3324,	0.9188 s / batch. (data: 7.78e-03). ETA=10:52:53, max mem: 30.7 GB 
[11/24 19:07:55 visual_prompt]: Epoch 23 / 100: avg data time: 2.15e-02, avg batch time: 0.9509, average train loss: 0.2379
[11/24 19:08:50 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3067, average loss: 1.0374
[11/24 19:08:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 62.87	
[11/24 19:10:23 visual_prompt]: 	Test 100/162. loss: 1.394, 0.3180 s / batch. (data: 5.27e-05)max mem: 30.66532 GB 
[11/24 19:11:10 visual_prompt]: Inference (test):avg data time: 3.30e-05, avg batch time: 0.3058, average loss: 1.1233
[11/24 19:11:10 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.59	rocauc: 62.17	
[11/24 19:11:10 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/24 19:12:52 visual_prompt]: 	Training 100/553. train loss: 0.0263,	0.9226 s / batch. (data: 2.69e-04). ETA=10:53:14, max mem: 30.7 GB 
[11/24 19:14:26 visual_prompt]: 	Training 200/553. train loss: 0.3159,	0.9440 s / batch. (data: 7.58e-04). ETA=11:06:49, max mem: 30.7 GB 
[11/24 19:16:00 visual_prompt]: 	Training 300/553. train loss: 0.0057,	0.9177 s / batch. (data: 2.64e-04). ETA=10:46:42, max mem: 30.7 GB 
[11/24 19:17:33 visual_prompt]: 	Training 400/553. train loss: 0.1005,	0.9205 s / batch. (data: 5.36e-03). ETA=10:47:06, max mem: 30.7 GB 
[11/24 19:19:07 visual_prompt]: 	Training 500/553. train loss: 0.0459,	0.9623 s / batch. (data: 1.09e-02). ETA=11:14:53, max mem: 30.7 GB 
[11/24 19:19:56 visual_prompt]: Epoch 24 / 100: avg data time: 2.25e-02, avg batch time: 0.9510, average train loss: 0.2233
[11/24 19:20:51 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3037, average loss: 1.2633
[11/24 19:20:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.35	
[11/24 19:22:24 visual_prompt]: 	Test 100/162. loss: 0.443, 0.3060 s / batch. (data: 3.55e-05)max mem: 30.66532 GB 
[11/24 19:23:10 visual_prompt]: Inference (test):avg data time: 1.23e-04, avg batch time: 0.3053, average loss: 1.1862
[11/24 19:23:10 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 62.92	
[11/24 19:23:10 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/24 19:24:54 visual_prompt]: 	Training 100/553. train loss: 1.1824,	0.9115 s / batch. (data: 1.05e-02). ETA=10:36:58, max mem: 30.7 GB 
[11/24 19:26:27 visual_prompt]: 	Training 200/553. train loss: 0.0173,	0.9485 s / batch. (data: 5.34e-03). ETA=11:01:12, max mem: 30.7 GB 
[11/24 19:28:00 visual_prompt]: 	Training 300/553. train loss: 0.0088,	0.9426 s / batch. (data: 5.87e-03). ETA=10:55:33, max mem: 30.7 GB 
[11/24 19:29:34 visual_prompt]: 	Training 400/553. train loss: 2.8017,	0.9478 s / batch. (data: 1.10e-02). ETA=10:57:34, max mem: 30.7 GB 
[11/24 19:31:08 visual_prompt]: 	Training 500/553. train loss: 0.5611,	0.9600 s / batch. (data: 7.05e-04). ETA=11:04:27, max mem: 30.7 GB 
[11/24 19:31:58 visual_prompt]: Epoch 25 / 100: avg data time: 2.39e-02, avg batch time: 0.9532, average train loss: 0.1913
[11/24 19:32:53 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3070, average loss: 1.4564
[11/24 19:32:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 66.85	
[11/24 19:34:26 visual_prompt]: 	Test 100/162. loss: 0.005, 0.3019 s / batch. (data: 3.50e-05)max mem: 30.66532 GB 
[11/24 19:35:12 visual_prompt]: Inference (test):avg data time: 2.55e-04, avg batch time: 0.3055, average loss: 1.5212
[11/24 19:35:12 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.64	rocauc: 63.16	
[11/24 19:35:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/24 19:36:58 visual_prompt]: 	Training 100/553. train loss: 0.0606,	0.9135 s / batch. (data: 1.02e-02). ETA=10:29:54, max mem: 30.7 GB 
[11/24 19:38:31 visual_prompt]: 	Training 200/553. train loss: 0.0033,	0.9256 s / batch. (data: 2.78e-04). ETA=10:36:43, max mem: 30.7 GB 
[11/24 19:40:04 visual_prompt]: 	Training 300/553. train loss: 0.3088,	0.9226 s / batch. (data: 7.26e-04). ETA=10:33:08, max mem: 30.7 GB 
[11/24 19:41:38 visual_prompt]: 	Training 400/553. train loss: 0.0123,	0.9586 s / batch. (data: 7.21e-04). ETA=10:56:13, max mem: 30.7 GB 
[11/24 19:43:11 visual_prompt]: 	Training 500/553. train loss: 1.6068,	0.9488 s / batch. (data: 5.36e-03). ETA=10:47:57, max mem: 30.7 GB 
[11/24 19:44:01 visual_prompt]: Epoch 26 / 100: avg data time: 2.76e-02, avg batch time: 0.9552, average train loss: 0.1754
[11/24 19:44:55 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3093, average loss: 1.4384
[11/24 19:44:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.67	
[11/24 19:46:28 visual_prompt]: 	Test 100/162. loss: 0.393, 0.3053 s / batch. (data: 3.84e-05)max mem: 30.66532 GB 
[11/24 19:47:14 visual_prompt]: Inference (test):avg data time: 2.31e-04, avg batch time: 0.3053, average loss: 1.6246
[11/24 19:47:14 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.95	rocauc: 64.29	
[11/24 19:47:14 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/24 19:48:56 visual_prompt]: 	Training 100/553. train loss: 0.0089,	0.9160 s / batch. (data: 2.93e-04). ETA=10:23:12, max mem: 30.7 GB 
[11/24 19:50:30 visual_prompt]: 	Training 200/553. train loss: 0.0110,	0.9295 s / batch. (data: 7.51e-04). ETA=10:30:52, max mem: 30.7 GB 
[11/24 19:52:03 visual_prompt]: 	Training 300/553. train loss: 0.2233,	0.9275 s / batch. (data: 7.27e-04). ETA=10:27:55, max mem: 30.7 GB 
[11/24 19:53:37 visual_prompt]: 	Training 400/553. train loss: 0.0386,	0.9129 s / batch. (data: 5.35e-03). ETA=10:16:30, max mem: 30.7 GB 
[11/24 19:55:11 visual_prompt]: 	Training 500/553. train loss: 0.6752,	0.9193 s / batch. (data: 1.04e-02). ETA=10:19:19, max mem: 30.7 GB 
[11/24 19:56:00 visual_prompt]: Epoch 27 / 100: avg data time: 2.21e-02, avg batch time: 0.9508, average train loss: 0.1834
[11/24 19:56:55 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.3081, average loss: 1.5983
[11/24 19:56:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 63.01	
[11/24 19:58:28 visual_prompt]: 	Test 100/162. loss: 0.311, 0.2991 s / batch. (data: 3.93e-05)max mem: 30.66532 GB 
[11/24 19:59:15 visual_prompt]: Inference (test):avg data time: 7.70e-05, avg batch time: 0.3052, average loss: 1.5319
[11/24 19:59:15 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.17	rocauc: 64.90	
[11/24 19:59:15 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/24 20:01:01 visual_prompt]: 	Training 100/553. train loss: 0.2199,	0.9293 s / batch. (data: 8.03e-03). ETA=10:23:40, max mem: 30.7 GB 
[11/24 20:02:34 visual_prompt]: 	Training 200/553. train loss: 0.0003,	0.9429 s / batch. (data: 7.42e-04). ETA=10:31:16, max mem: 30.7 GB 
[11/24 20:04:08 visual_prompt]: 	Training 300/553. train loss: 0.3629,	0.9335 s / batch. (data: 2.84e-04). ETA=10:23:23, max mem: 30.7 GB 
[11/24 20:05:42 visual_prompt]: 	Training 400/553. train loss: 0.0809,	0.9600 s / batch. (data: 5.84e-03). ETA=10:39:30, max mem: 30.7 GB 
[11/24 20:07:15 visual_prompt]: 	Training 500/553. train loss: 0.3770,	0.9441 s / batch. (data: 2.95e-02). ETA=10:27:18, max mem: 30.7 GB 
[11/24 20:08:05 visual_prompt]: Epoch 28 / 100: avg data time: 3.03e-02, avg batch time: 0.9583, average train loss: 0.1597
[11/24 20:09:00 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.3051, average loss: 1.1864
[11/24 20:09:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 63.88	
[11/24 20:10:33 visual_prompt]: 	Test 100/162. loss: 0.766, 0.3136 s / batch. (data: 6.20e-05)max mem: 30.66532 GB 
[11/24 20:11:20 visual_prompt]: Inference (test):avg data time: 2.74e-04, avg batch time: 0.3063, average loss: 1.2000
[11/24 20:11:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.21	rocauc: 62.89	
[11/24 20:11:20 visual_prompt]: Stopping early.
[11/24 20:11:20 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 20:11:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 20:11:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/24 20:11:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/24 20:11:20 visual_prompt]: Training with config:
[11/24 20:11:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/test/seed3172/lr0.0001_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 3172, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/24 20:11:20 visual_prompt]: Loading training data...
[11/24 20:11:20 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 20:11:20 visual_prompt]: Loading validation data...
[11/24 20:11:20 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 20:11:20 visual_prompt]: Loading test data...
[11/24 20:11:20 visual_prompt]: Constructing mammo-cbis dataset test...
[11/24 20:11:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 20:11:26 visual_prompt]: Enable all parameters update during training
[11/24 20:11:26 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/24 20:11:26 visual_prompt]: tuned percent:100.000
[11/24 20:11:26 visual_prompt]: Device used for model: 0
[11/24 20:11:26 visual_prompt]: Setting up Evaluator...
[11/24 20:11:26 visual_prompt]: Setting up Trainer...
[11/24 20:11:26 visual_prompt]: 	Setting up the optimizer...
[11/24 20:11:26 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 20:13:07 visual_prompt]: 	Training 100/553. train loss: 3.8026,	0.9200 s / batch. (data: 2.88e-04). ETA=14:06:22, max mem: 30.7 GB 
[11/24 20:14:40 visual_prompt]: 	Training 200/553. train loss: 2.5182,	0.9231 s / batch. (data: 5.90e-03). ETA=14:07:44, max mem: 30.7 GB 
[11/24 20:16:14 visual_prompt]: 	Training 300/553. train loss: 0.0305,	0.9495 s / batch. (data: 1.60e-02). ETA=14:30:24, max mem: 30.7 GB 
[11/24 20:17:48 visual_prompt]: 	Training 400/553. train loss: 7.3812,	0.9491 s / batch. (data: 2.30e-02). ETA=14:28:24, max mem: 30.7 GB 
[11/24 20:19:22 visual_prompt]: 	Training 500/553. train loss: 0.1586,	1.0072 s / batch. (data: 5.82e-03). ETA=15:19:56, max mem: 30.7 GB 
[11/24 20:20:11 visual_prompt]: Epoch 1 / 100: avg data time: 2.00e-02, avg batch time: 0.9493, average train loss: 4.2771
[11/24 20:21:06 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.3056, average loss: 4.3620
[11/24 20:21:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/24 20:22:39 visual_prompt]: 	Test 100/162. loss: 2.528, 0.3018 s / batch. (data: 3.15e-05)max mem: 30.66532 GB 
[11/24 20:23:26 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.3055, average loss: 4.5524
[11/24 20:23:26 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.09	rocauc: 46.83	
[11/24 20:23:26 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/24 20:25:10 visual_prompt]: 	Training 100/553. train loss: 0.3322,	0.9360 s / batch. (data: 2.99e-04). ETA=14:12:27, max mem: 30.7 GB 
[11/24 20:26:44 visual_prompt]: 	Training 200/553. train loss: 0.9368,	0.9494 s / batch. (data: 1.10e-02). ETA=14:23:07, max mem: 30.7 GB 
[11/24 20:28:17 visual_prompt]: 	Training 300/553. train loss: 0.9348,	0.9267 s / batch. (data: 2.93e-04). ETA=14:00:55, max mem: 30.7 GB 
[11/24 20:29:51 visual_prompt]: 	Training 400/553. train loss: 1.4455,	0.9377 s / batch. (data: 7.27e-04). ETA=14:09:21, max mem: 30.7 GB 
[11/24 20:31:24 visual_prompt]: 	Training 500/553. train loss: 0.6902,	0.9664 s / batch. (data: 1.09e-02). ETA=14:33:41, max mem: 30.7 GB 
[11/24 20:32:14 visual_prompt]: Epoch 2 / 100: avg data time: 2.62e-02, avg batch time: 0.9544, average train loss: 0.9733
[11/24 20:33:08 visual_prompt]: Inference (val):avg data time: 8.22e-05, avg batch time: 0.3062, average loss: 0.6562
[11/24 20:33:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.78	
[11/24 20:34:40 visual_prompt]: 	Test 100/162. loss: 0.387, 0.3028 s / batch. (data: 3.29e-05)max mem: 30.66532 GB 
[11/24 20:35:26 visual_prompt]: Inference (test):avg data time: 1.23e-04, avg batch time: 0.3057, average loss: 0.6794
[11/24 20:35:26 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.00	rocauc: 60.67	
[11/24 20:35:26 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/24 20:37:09 visual_prompt]: 	Training 100/553. train loss: 0.5421,	0.9200 s / batch. (data: 7.05e-04). ETA=13:49:27, max mem: 30.7 GB 
[11/24 20:38:42 visual_prompt]: 	Training 200/553. train loss: 0.8741,	0.9376 s / batch. (data: 5.97e-03). ETA=14:03:42, max mem: 30.7 GB 
[11/24 20:40:15 visual_prompt]: 	Training 300/553. train loss: 0.4144,	0.9255 s / batch. (data: 7.65e-04). ETA=13:51:19, max mem: 30.7 GB 
[11/24 20:41:49 visual_prompt]: 	Training 400/553. train loss: 0.9853,	0.9320 s / batch. (data: 7.46e-04). ETA=13:55:34, max mem: 30.7 GB 
[11/24 20:43:23 visual_prompt]: 	Training 500/553. train loss: 0.3301,	0.9240 s / batch. (data: 2.93e-04). ETA=13:46:55, max mem: 30.7 GB 
[11/24 20:44:12 visual_prompt]: Epoch 3 / 100: avg data time: 2.35e-02, avg batch time: 0.9519, average train loss: 0.7445
[11/24 20:45:07 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3067, average loss: 0.6068
[11/24 20:45:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 71.14	rocauc: 70.94	
[11/24 20:46:39 visual_prompt]: 	Test 100/162. loss: 0.453, 0.3064 s / batch. (data: 3.19e-05)max mem: 30.66532 GB 
[11/24 20:47:25 visual_prompt]: Inference (test):avg data time: 7.64e-05, avg batch time: 0.3062, average loss: 0.6689
[11/24 20:47:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.33	rocauc: 66.57	
[11/24 20:47:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/24 20:49:07 visual_prompt]: 	Training 100/553. train loss: 0.9802,	0.9450 s / batch. (data: 7.41e-04). ETA=14:03:17, max mem: 30.7 GB 
[11/24 20:50:40 visual_prompt]: 	Training 200/553. train loss: 0.6113,	0.9291 s / batch. (data: 7.53e-04). ETA=13:47:30, max mem: 30.7 GB 
[11/24 20:52:14 visual_prompt]: 	Training 300/553. train loss: 0.7689,	0.9760 s / batch. (data: 5.82e-03). ETA=14:27:42, max mem: 30.7 GB 
[11/24 20:53:47 visual_prompt]: 	Training 400/553. train loss: 0.6507,	0.9530 s / batch. (data: 5.79e-03). ETA=14:05:39, max mem: 30.7 GB 
[11/24 20:55:21 visual_prompt]: 	Training 500/553. train loss: 0.4632,	0.9373 s / batch. (data: 7.09e-04). ETA=13:50:11, max mem: 30.7 GB 
[11/24 20:56:10 visual_prompt]: Epoch 4 / 100: avg data time: 2.18e-02, avg batch time: 0.9498, average train loss: 0.7268
[11/24 20:57:04 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3084, average loss: 0.6019
[11/24 20:57:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 71.50	
[11/24 20:58:37 visual_prompt]: 	Test 100/162. loss: 0.575, 0.3128 s / batch. (data: 7.92e-05)max mem: 30.66532 GB 
[11/24 20:59:23 visual_prompt]: Inference (test):avg data time: 8.58e-05, avg batch time: 0.3072, average loss: 0.6510
[11/24 20:59:23 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.55	rocauc: 64.59	
[11/24 20:59:23 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/24 21:01:04 visual_prompt]: 	Training 100/553. train loss: 0.5420,	0.9357 s / batch. (data: 7.15e-04). ETA=13:46:20, max mem: 30.7 GB 
[11/24 21:02:37 visual_prompt]: 	Training 200/553. train loss: 0.4917,	0.9183 s / batch. (data: 5.37e-03). ETA=13:29:26, max mem: 30.7 GB 
[11/24 21:04:11 visual_prompt]: 	Training 300/553. train loss: 0.7960,	0.9262 s / batch. (data: 2.53e-04). ETA=13:34:50, max mem: 30.7 GB 
[11/24 21:05:45 visual_prompt]: 	Training 400/553. train loss: 0.4947,	0.9719 s / batch. (data: 8.92e-03). ETA=14:13:26, max mem: 30.7 GB 
[11/24 21:07:18 visual_prompt]: 	Training 500/553. train loss: 1.6333,	0.9505 s / batch. (data: 2.56e-04). ETA=13:53:07, max mem: 30.7 GB 
[11/24 21:08:08 visual_prompt]: Epoch 5 / 100: avg data time: 2.11e-02, avg batch time: 0.9493, average train loss: 0.6936
[11/24 21:09:02 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3062, average loss: 0.6071
[11/24 21:09:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 72.54	
[11/24 21:10:34 visual_prompt]: 	Test 100/162. loss: 0.631, 0.3067 s / batch. (data: 5.13e-05)max mem: 30.66532 GB 
[11/24 21:11:20 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.3055, average loss: 0.6647
[11/24 21:11:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.09	rocauc: 67.73	
[11/24 21:11:20 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/24 21:13:02 visual_prompt]: 	Training 100/553. train loss: 0.3992,	0.9480 s / batch. (data: 7.91e-04). ETA=13:48:28, max mem: 30.7 GB 
[11/24 21:14:36 visual_prompt]: 	Training 200/553. train loss: 0.8687,	0.9285 s / batch. (data: 2.67e-04). ETA=13:29:54, max mem: 30.7 GB 
[11/24 21:16:09 visual_prompt]: 	Training 300/553. train loss: 0.3493,	0.9204 s / batch. (data: 2.66e-04). ETA=13:21:15, max mem: 30.7 GB 
[11/24 21:17:43 visual_prompt]: 	Training 400/553. train loss: 0.6362,	0.9442 s / batch. (data: 5.74e-03). ETA=13:40:26, max mem: 30.7 GB 
[11/24 21:19:17 visual_prompt]: 	Training 500/553. train loss: 0.5740,	0.9232 s / batch. (data: 7.69e-04). ETA=13:20:37, max mem: 30.7 GB 
[11/24 21:20:06 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e-02, avg batch time: 0.9507, average train loss: 0.6786
[11/24 21:21:00 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3050, average loss: 0.5834
[11/24 21:21:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 76.09	
[11/24 21:22:33 visual_prompt]: 	Test 100/162. loss: 0.555, 0.2954 s / batch. (data: 5.13e-05)max mem: 30.66532 GB 
[11/24 21:23:18 visual_prompt]: Inference (test):avg data time: 3.35e-05, avg batch time: 0.3036, average loss: 0.6156
[11/24 21:23:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.88	rocauc: 69.91	
[11/24 21:23:18 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/24 21:25:01 visual_prompt]: 	Training 100/553. train loss: 0.8667,	0.9410 s / batch. (data: 7.32e-04). ETA=13:33:39, max mem: 30.7 GB 
[11/24 21:26:34 visual_prompt]: 	Training 200/553. train loss: 0.5132,	0.9402 s / batch. (data: 1.10e-02). ETA=13:31:25, max mem: 30.7 GB 
[11/24 21:28:08 visual_prompt]: 	Training 300/553. train loss: 0.5966,	0.9406 s / batch. (data: 8.57e-03). ETA=13:30:13, max mem: 30.7 GB 
[11/24 21:29:42 visual_prompt]: 	Training 400/553. train loss: 0.5685,	0.9506 s / batch. (data: 7.23e-04). ETA=13:37:12, max mem: 30.7 GB 
[11/24 21:31:15 visual_prompt]: 	Training 500/553. train loss: 0.5454,	0.9332 s / batch. (data: 7.47e-04). ETA=13:20:42, max mem: 30.7 GB 
[11/24 21:32:05 visual_prompt]: Epoch 7 / 100: avg data time: 2.33e-02, avg batch time: 0.9514, average train loss: 0.6315
[11/24 21:32:59 visual_prompt]: Inference (val):avg data time: 5.40e-04, avg batch time: 0.3051, average loss: 0.7378
[11/24 21:32:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 75.83	
[11/24 21:34:31 visual_prompt]: 	Test 100/162. loss: 0.487, 0.2992 s / batch. (data: 3.17e-05)max mem: 30.66532 GB 
[11/24 21:35:17 visual_prompt]: Inference (test):avg data time: 3.42e-05, avg batch time: 0.3063, average loss: 0.8632
[11/24 21:35:17 visual_prompt]: Classification results with test_mammo-cbis: top1: 51.63	rocauc: 69.68	
[11/24 21:35:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/24 21:37:02 visual_prompt]: 	Training 100/553. train loss: 0.6019,	0.9311 s / batch. (data: 2.64e-04). ETA=13:16:33, max mem: 30.7 GB 
[11/24 21:38:35 visual_prompt]: 	Training 200/553. train loss: 0.4527,	0.9333 s / batch. (data: 1.06e-02). ETA=13:16:51, max mem: 30.7 GB 
[11/24 21:40:08 visual_prompt]: 	Training 300/553. train loss: 0.6666,	0.9343 s / batch. (data: 5.38e-03). ETA=13:16:07, max mem: 30.7 GB 
[11/24 21:41:41 visual_prompt]: 	Training 400/553. train loss: 0.8196,	0.9411 s / batch. (data: 1.05e-02). ETA=13:20:23, max mem: 30.7 GB 
[11/24 21:43:15 visual_prompt]: 	Training 500/553. train loss: 0.3946,	0.9315 s / batch. (data: 1.44e-02). ETA=13:10:39, max mem: 30.7 GB 
[11/24 21:44:05 visual_prompt]: Epoch 8 / 100: avg data time: 2.57e-02, avg batch time: 0.9535, average train loss: 0.6130
[11/24 21:44:59 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3058, average loss: 0.5865
[11/24 21:44:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 77.65	
[11/24 21:46:31 visual_prompt]: 	Test 100/162. loss: 0.713, 0.2950 s / batch. (data: 3.34e-05)max mem: 30.66532 GB 
[11/24 21:47:17 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.3068, average loss: 0.7186
[11/24 21:47:17 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.14	rocauc: 69.73	
[11/24 21:47:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/24 21:49:01 visual_prompt]: 	Training 100/553. train loss: 0.4170,	0.9516 s / batch. (data: 7.38e-04). ETA=13:25:19, max mem: 30.7 GB 
[11/24 21:50:34 visual_prompt]: 	Training 200/553. train loss: 0.4182,	0.9225 s / batch. (data: 7.29e-04). ETA=12:59:08, max mem: 30.7 GB 
[11/24 21:52:08 visual_prompt]: 	Training 300/553. train loss: 0.9515,	0.9096 s / batch. (data: 2.59e-04). ETA=12:46:41, max mem: 30.7 GB 
[11/24 21:53:41 visual_prompt]: 	Training 400/553. train loss: 0.6332,	0.9322 s / batch. (data: 7.77e-04). ETA=13:04:15, max mem: 30.7 GB 
[11/24 21:55:14 visual_prompt]: 	Training 500/553. train loss: 0.2847,	0.9288 s / batch. (data: 2.86e-04). ETA=12:59:50, max mem: 30.7 GB 
[11/24 21:56:03 visual_prompt]: Epoch 9 / 100: avg data time: 2.51e-02, avg batch time: 0.9523, average train loss: 0.6072
[11/24 21:56:58 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.3060, average loss: 0.6797
[11/24 21:56:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 76.60	
[11/24 21:58:29 visual_prompt]: 	Test 100/162. loss: 0.379, 0.2949 s / batch. (data: 3.67e-05)max mem: 30.66532 GB 
[11/24 21:59:15 visual_prompt]: Inference (test):avg data time: 3.10e-05, avg batch time: 0.3072, average loss: 0.6923
[11/24 21:59:15 visual_prompt]: Classification results with test_mammo-cbis: top1: 64.81	rocauc: 70.41	
[11/24 21:59:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/24 22:00:56 visual_prompt]: 	Training 100/553. train loss: 0.6982,	0.9162 s / batch. (data: 2.54e-04). ETA=12:46:55, max mem: 30.7 GB 
[11/24 22:02:30 visual_prompt]: 	Training 200/553. train loss: 0.9632,	0.9231 s / batch. (data: 2.56e-04). ETA=12:51:10, max mem: 30.7 GB 
[11/24 22:04:03 visual_prompt]: 	Training 300/553. train loss: 0.5183,	0.9239 s / batch. (data: 2.56e-04). ETA=12:50:18, max mem: 30.7 GB 
[11/24 22:05:37 visual_prompt]: 	Training 400/553. train loss: 1.2239,	0.9374 s / batch. (data: 7.28e-04). ETA=12:59:58, max mem: 30.7 GB 
[11/24 22:07:10 visual_prompt]: 	Training 500/553. train loss: 0.5564,	0.9527 s / batch. (data: 7.21e-04). ETA=13:11:05, max mem: 30.7 GB 
[11/24 22:07:59 visual_prompt]: Epoch 10 / 100: avg data time: 1.90e-02, avg batch time: 0.9470, average train loss: 0.5869
[11/24 22:08:54 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3071, average loss: 0.6580
[11/24 22:08:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 72.55	
[11/24 22:10:25 visual_prompt]: 	Test 100/162. loss: 0.457, 0.3060 s / batch. (data: 5.98e-05)max mem: 30.66532 GB 
[11/24 22:11:11 visual_prompt]: Inference (test):avg data time: 7.67e-05, avg batch time: 0.3042, average loss: 0.7019
[11/24 22:11:11 visual_prompt]: Classification results with test_mammo-cbis: top1: 64.50	rocauc: 70.36	
[11/24 22:11:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/24 22:12:55 visual_prompt]: 	Training 100/553. train loss: 0.4485,	0.9250 s / batch. (data: 5.35e-03). ETA=12:45:43, max mem: 30.7 GB 
[11/24 22:14:28 visual_prompt]: 	Training 200/553. train loss: 1.2016,	0.9061 s / batch. (data: 3.10e-04). ETA=12:28:36, max mem: 30.7 GB 
[11/24 22:16:02 visual_prompt]: 	Training 300/553. train loss: 0.7066,	0.9356 s / batch. (data: 7.33e-04). ETA=12:51:24, max mem: 30.7 GB 
[11/24 22:17:35 visual_prompt]: 	Training 400/553. train loss: 0.6543,	0.9404 s / batch. (data: 5.37e-03). ETA=12:53:46, max mem: 30.7 GB 
[11/24 22:19:09 visual_prompt]: 	Training 500/553. train loss: 0.5121,	0.9250 s / batch. (data: 7.72e-04). ETA=12:39:34, max mem: 30.7 GB 
[11/24 22:19:58 visual_prompt]: Epoch 11 / 100: avg data time: 2.47e-02, avg batch time: 0.9521, average train loss: 0.5743
[11/24 22:20:52 visual_prompt]: Inference (val):avg data time: 4.28e-04, avg batch time: 0.3060, average loss: 0.6574
[11/24 22:20:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 70.60	
[11/24 22:22:25 visual_prompt]: 	Test 100/162. loss: 0.505, 0.2946 s / batch. (data: 3.72e-05)max mem: 30.66532 GB 
[11/24 22:23:10 visual_prompt]: Inference (test):avg data time: 1.89e-04, avg batch time: 0.3064, average loss: 0.7281
[11/24 22:23:10 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.48	rocauc: 68.49	
[11/24 22:23:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/24 22:24:53 visual_prompt]: 	Training 100/553. train loss: 0.7228,	0.9014 s / batch. (data: 2.52e-04). ETA=12:17:51, max mem: 30.7 GB 
[11/24 22:26:26 visual_prompt]: 	Training 200/553. train loss: 0.2686,	0.9324 s / batch. (data: 7.18e-04). ETA=12:41:45, max mem: 30.7 GB 
[11/24 22:28:00 visual_prompt]: 	Training 300/553. train loss: 0.1553,	0.9520 s / batch. (data: 3.24e-04). ETA=12:56:07, max mem: 30.7 GB 
[11/24 22:29:33 visual_prompt]: 	Training 400/553. train loss: 0.4202,	0.9501 s / batch. (data: 1.60e-02). ETA=12:53:02, max mem: 30.7 GB 
[11/24 22:31:07 visual_prompt]: 	Training 500/553. train loss: 1.0001,	0.9224 s / batch. (data: 5.44e-03). ETA=12:28:54, max mem: 30.7 GB 
[11/24 22:31:56 visual_prompt]: Epoch 12 / 100: avg data time: 2.38e-02, avg batch time: 0.9500, average train loss: 0.5508
[11/24 22:32:50 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3052, average loss: 0.7814
[11/24 22:32:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 67.55	
[11/24 22:34:23 visual_prompt]: 	Test 100/162. loss: 0.538, 0.3129 s / batch. (data: 3.46e-05)max mem: 30.66532 GB 
[11/24 22:35:09 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.3044, average loss: 0.8188
[11/24 22:35:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 66.26	
[11/24 22:35:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/24 22:36:51 visual_prompt]: 	Training 100/553. train loss: 0.3126,	0.9357 s / batch. (data: 7.14e-04). ETA=12:37:19, max mem: 30.7 GB 
[11/24 22:38:24 visual_prompt]: 	Training 200/553. train loss: 0.3920,	0.9398 s / batch. (data: 7.11e-04). ETA=12:39:07, max mem: 30.7 GB 
[11/24 22:39:58 visual_prompt]: 	Training 300/553. train loss: 1.2143,	0.9224 s / batch. (data: 7.18e-04). ETA=12:23:32, max mem: 30.7 GB 
[11/24 22:41:31 visual_prompt]: 	Training 400/553. train loss: 0.9687,	0.9097 s / batch. (data: 2.85e-04). ETA=12:11:47, max mem: 30.7 GB 
[11/24 22:43:05 visual_prompt]: 	Training 500/553. train loss: 0.2983,	0.9185 s / batch. (data: 1.05e-02). ETA=12:17:20, max mem: 30.7 GB 
[11/24 22:43:54 visual_prompt]: Epoch 13 / 100: avg data time: 2.21e-02, avg batch time: 0.9502, average train loss: 0.5633
[11/24 22:44:49 visual_prompt]: Inference (val):avg data time: 3.81e-04, avg batch time: 0.3081, average loss: 0.6557
[11/24 22:44:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 67.91	
[11/24 22:46:21 visual_prompt]: 	Test 100/162. loss: 0.501, 0.3146 s / batch. (data: 3.62e-05)max mem: 30.66532 GB 
[11/24 22:47:06 visual_prompt]: Inference (test):avg data time: 7.66e-05, avg batch time: 0.3053, average loss: 0.6750
[11/24 22:47:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.55	rocauc: 67.20	
[11/24 22:47:06 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/24 22:48:48 visual_prompt]: 	Training 100/553. train loss: 0.2543,	0.9277 s / batch. (data: 9.85e-03). ETA=12:22:19, max mem: 30.7 GB 
[11/24 22:50:22 visual_prompt]: 	Training 200/553. train loss: 0.6493,	0.9390 s / batch. (data: 1.60e-02). ETA=12:29:49, max mem: 30.7 GB 
[11/24 22:51:55 visual_prompt]: 	Training 300/553. train loss: 0.2519,	0.9444 s / batch. (data: 3.08e-02). ETA=12:32:32, max mem: 30.7 GB 
[11/24 22:53:29 visual_prompt]: 	Training 400/553. train loss: 0.4933,	0.9600 s / batch. (data: 7.43e-04). ETA=12:43:22, max mem: 30.7 GB 
[11/24 22:55:02 visual_prompt]: 	Training 500/553. train loss: 0.2309,	0.9440 s / batch. (data: 2.62e-04). ETA=12:29:04, max mem: 30.7 GB 
[11/24 22:55:52 visual_prompt]: Epoch 14 / 100: avg data time: 2.18e-02, avg batch time: 0.9491, average train loss: 0.4806
[11/24 22:56:46 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3067, average loss: 0.8248
[11/24 22:56:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 68.75	
[11/24 22:58:18 visual_prompt]: 	Test 100/162. loss: 0.634, 0.3143 s / batch. (data: 4.46e-05)max mem: 30.66532 GB 
[11/24 22:59:04 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.3057, average loss: 0.7360
[11/24 22:59:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.41	rocauc: 69.59	
[11/24 22:59:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/24 23:00:46 visual_prompt]: 	Training 100/553. train loss: 0.6293,	0.9257 s / batch. (data: 7.02e-04). ETA=12:12:11, max mem: 30.7 GB 
[11/24 23:02:19 visual_prompt]: 	Training 200/553. train loss: 0.1910,	0.9151 s / batch. (data: 7.87e-04). ETA=12:02:17, max mem: 30.7 GB 
[11/24 23:03:53 visual_prompt]: 	Training 300/553. train loss: 0.2853,	0.9590 s / batch. (data: 1.04e-02). ETA=12:35:21, max mem: 30.7 GB 
[11/24 23:05:27 visual_prompt]: 	Training 400/553. train loss: 0.9708,	0.9416 s / batch. (data: 2.78e-04). ETA=12:20:01, max mem: 30.7 GB 
[11/24 23:07:01 visual_prompt]: 	Training 500/553. train loss: 0.5907,	0.9241 s / batch. (data: 7.00e-04). ETA=12:04:48, max mem: 30.7 GB 
[11/24 23:07:50 visual_prompt]: Epoch 15 / 100: avg data time: 2.07e-02, avg batch time: 0.9518, average train loss: 0.4580
[11/24 23:08:48 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3047, average loss: 0.6813
[11/24 23:08:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 68.24	
[11/24 23:10:24 visual_prompt]: 	Test 100/162. loss: 0.496, 0.3100 s / batch. (data: 3.84e-05)max mem: 30.66532 GB 
[11/24 23:11:13 visual_prompt]: Inference (test):avg data time: 7.86e-05, avg batch time: 0.3047, average loss: 0.6818
[11/24 23:11:13 visual_prompt]: Classification results with test_mammo-cbis: top1: 65.89	rocauc: 71.30	
[11/24 23:11:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/24 23:12:56 visual_prompt]: 	Training 100/553. train loss: 0.2184,	0.9322 s / batch. (data: 1.87e-02). ETA=12:08:43, max mem: 30.7 GB 
[11/24 23:14:30 visual_prompt]: 	Training 200/553. train loss: 0.7826,	0.9720 s / batch. (data: 8.10e-04). ETA=12:38:15, max mem: 30.7 GB 
[11/24 23:16:04 visual_prompt]: 	Training 300/553. train loss: 0.7512,	0.9464 s / batch. (data: 7.36e-04). ETA=12:16:43, max mem: 30.7 GB 
[11/24 23:17:38 visual_prompt]: 	Training 400/553. train loss: 0.5193,	0.9431 s / batch. (data: 2.15e-02). ETA=12:12:31, max mem: 30.7 GB 
[11/24 23:19:11 visual_prompt]: 	Training 500/553. train loss: 0.2365,	0.9482 s / batch. (data: 7.89e-04). ETA=12:14:54, max mem: 30.7 GB 
[11/24 23:20:01 visual_prompt]: Epoch 16 / 100: avg data time: 2.36e-02, avg batch time: 0.9539, average train loss: 0.4333
[11/24 23:20:56 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3076, average loss: 1.0749
[11/24 23:20:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 67.24	
[11/24 23:22:29 visual_prompt]: 	Test 100/162. loss: 0.353, 0.3045 s / batch. (data: 3.65e-05)max mem: 30.66532 GB 
[11/24 23:23:16 visual_prompt]: Inference (test):avg data time: 5.30e-05, avg batch time: 0.3060, average loss: 0.9223
[11/24 23:23:16 visual_prompt]: Classification results with test_mammo-cbis: top1: 65.58	rocauc: 69.80	
[11/24 23:23:16 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/24 23:25:00 visual_prompt]: 	Training 100/553. train loss: 0.1979,	0.9212 s / batch. (data: 3.11e-04). ETA=11:51:39, max mem: 30.7 GB 
[11/24 23:26:33 visual_prompt]: 	Training 200/553. train loss: 0.4869,	0.9260 s / batch. (data: 8.10e-04). ETA=11:53:47, max mem: 30.7 GB 
[11/24 23:28:07 visual_prompt]: 	Training 300/553. train loss: 0.0738,	0.9575 s / batch. (data: 1.05e-02). ETA=12:16:31, max mem: 30.7 GB 
[11/24 23:29:41 visual_prompt]: 	Training 400/553. train loss: 0.2933,	0.9310 s / batch. (data: 1.33e-02). ETA=11:54:35, max mem: 30.7 GB 
[11/24 23:31:18 visual_prompt]: 	Training 500/553. train loss: 0.6509,	0.9520 s / batch. (data: 5.35e-03). ETA=12:09:06, max mem: 30.7 GB 
[11/24 23:32:08 visual_prompt]: Epoch 17 / 100: avg data time: 3.04e-02, avg batch time: 0.9615, average train loss: 0.3793
[11/24 23:33:03 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3038, average loss: 0.9608
[11/24 23:33:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 66.46	
[11/24 23:34:39 visual_prompt]: 	Test 100/162. loss: 1.171, 0.3008 s / batch. (data: 4.53e-05)max mem: 30.66532 GB 
[11/24 23:35:27 visual_prompt]: Inference (test):avg data time: 7.69e-05, avg batch time: 0.3042, average loss: 1.0350
[11/24 23:35:27 visual_prompt]: Classification results with test_mammo-cbis: top1: 52.87	rocauc: 64.48	
[11/24 23:35:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/24 23:37:10 visual_prompt]: 	Training 100/553. train loss: 0.0681,	0.9508 s / batch. (data: 1.04e-02). ETA=12:05:44, max mem: 30.7 GB 
[11/24 23:38:44 visual_prompt]: 	Training 200/553. train loss: 0.1267,	0.9755 s / batch. (data: 2.14e-02). ETA=12:22:59, max mem: 30.7 GB 
[11/24 23:40:17 visual_prompt]: 	Training 300/553. train loss: 0.8832,	0.9321 s / batch. (data: 5.34e-03). ETA=11:48:21, max mem: 30.7 GB 
[11/24 23:41:51 visual_prompt]: 	Training 400/553. train loss: 0.0694,	0.9419 s / batch. (data: 2.52e-04). ETA=11:54:16, max mem: 30.7 GB 
[11/24 23:43:24 visual_prompt]: 	Training 500/553. train loss: 0.0882,	0.9320 s / batch. (data: 8.09e-04). ETA=11:45:09, max mem: 30.7 GB 
[11/24 23:44:14 visual_prompt]: Epoch 18 / 100: avg data time: 2.44e-02, avg batch time: 0.9529, average train loss: 0.3577
[11/24 23:45:09 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.3061, average loss: 1.5989
[11/24 23:45:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 65.92	
[11/24 23:46:42 visual_prompt]: 	Test 100/162. loss: 1.697, 0.3012 s / batch. (data: 3.12e-05)max mem: 30.66532 GB 
[11/24 23:47:28 visual_prompt]: Inference (test):avg data time: 5.22e-05, avg batch time: 0.3043, average loss: 1.4207
[11/24 23:47:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.10	rocauc: 64.79	
[11/24 23:47:28 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/24 23:49:10 visual_prompt]: 	Training 100/553. train loss: 0.6613,	0.9191 s / batch. (data: 1.55e-02). ETA=11:33:06, max mem: 30.7 GB 
[11/24 23:50:44 visual_prompt]: 	Training 200/553. train loss: 0.7982,	0.9538 s / batch. (data: 5.85e-03). ETA=11:57:41, max mem: 30.7 GB 
[11/24 23:52:17 visual_prompt]: 	Training 300/553. train loss: 0.2185,	0.9121 s / batch. (data: 2.67e-04). ETA=11:24:45, max mem: 30.7 GB 
[11/24 23:53:51 visual_prompt]: 	Training 400/553. train loss: 0.0925,	0.9305 s / batch. (data: 7.99e-03). ETA=11:37:00, max mem: 30.7 GB 
[11/24 23:55:25 visual_prompt]: 	Training 500/553. train loss: 0.2579,	0.9189 s / batch. (data: 5.39e-03). ETA=11:26:47, max mem: 30.7 GB 
[11/24 23:56:14 visual_prompt]: Epoch 19 / 100: avg data time: 2.16e-02, avg batch time: 0.9513, average train loss: 0.3278
[11/24 23:57:11 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.3069, average loss: 1.1744
[11/24 23:57:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 66.47	
[11/24 23:58:49 visual_prompt]: 	Test 100/162. loss: 1.399, 0.3142 s / batch. (data: 3.48e-05)max mem: 30.66532 GB 
[11/24 23:59:37 visual_prompt]: Inference (test):avg data time: 1.03e-04, avg batch time: 0.3044, average loss: 1.1065
[11/24 23:59:38 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.41	rocauc: 66.93	
[11/24 23:59:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/25 00:01:20 visual_prompt]: 	Training 100/553. train loss: 0.5002,	0.9221 s / batch. (data: 2.79e-04). ETA=11:26:52, max mem: 30.7 GB 
[11/25 00:02:54 visual_prompt]: 	Training 200/553. train loss: 0.4771,	0.9360 s / batch. (data: 2.68e-04). ETA=11:35:40, max mem: 30.7 GB 
[11/25 00:04:28 visual_prompt]: 	Training 300/553. train loss: 0.0987,	0.9253 s / batch. (data: 3.55e-04). ETA=11:26:09, max mem: 30.7 GB 
[11/25 00:06:02 visual_prompt]: 	Training 400/553. train loss: 0.0757,	0.9317 s / batch. (data: 3.19e-04). ETA=11:29:20, max mem: 30.7 GB 
[11/25 00:07:36 visual_prompt]: 	Training 500/553. train loss: 0.6285,	0.9356 s / batch. (data: 2.87e-04). ETA=11:30:42, max mem: 30.7 GB 
[11/25 00:08:25 visual_prompt]: Epoch 20 / 100: avg data time: 2.18e-02, avg batch time: 0.9539, average train loss: 0.2796
[11/25 00:09:20 visual_prompt]: Inference (val):avg data time: 4.05e-04, avg batch time: 0.3042, average loss: 1.2053
[11/25 00:09:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 65.77	
[11/25 00:10:56 visual_prompt]: 	Test 100/162. loss: 0.404, 0.3251 s / batch. (data: 3.29e-05)max mem: 30.66532 GB 
[11/25 00:11:44 visual_prompt]: Inference (test):avg data time: 3.40e-05, avg batch time: 0.3073, average loss: 1.4022
[11/25 00:11:44 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.28	rocauc: 62.72	
[11/25 00:11:44 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/25 00:13:29 visual_prompt]: 	Training 100/553. train loss: 0.0084,	0.9400 s / batch. (data: 7.99e-03). ETA=11:31:31, max mem: 30.7 GB 
[11/25 00:15:04 visual_prompt]: 	Training 200/553. train loss: 0.1201,	0.9295 s / batch. (data: 2.96e-04). ETA=11:22:16, max mem: 30.7 GB 
[11/25 00:16:40 visual_prompt]: 	Training 300/553. train loss: 0.1699,	0.9142 s / batch. (data: 7.10e-04). ETA=11:09:31, max mem: 30.7 GB 
[11/25 00:18:15 visual_prompt]: 	Training 400/553. train loss: 0.0688,	0.9591 s / batch. (data: 7.50e-04). ETA=11:40:47, max mem: 30.7 GB 
[11/25 00:19:49 visual_prompt]: 	Training 500/553. train loss: 0.0932,	0.9311 s / batch. (data: 6.89e-04). ETA=11:18:44, max mem: 30.7 GB 
[11/25 00:20:38 visual_prompt]: Epoch 21 / 100: avg data time: 3.32e-02, avg batch time: 0.9648, average train loss: 0.2424
[11/25 00:21:35 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3047, average loss: 1.0491
[11/25 00:21:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 66.03	
[11/25 00:23:10 visual_prompt]: 	Test 100/162. loss: 0.727, 0.3103 s / batch. (data: 5.17e-05)max mem: 30.66532 GB 
[11/25 00:23:58 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.3032, average loss: 1.1860
[11/25 00:23:58 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.59	rocauc: 64.97	
[11/25 00:23:58 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/25 00:25:39 visual_prompt]: 	Training 100/553. train loss: 0.0835,	0.9562 s / batch. (data: 5.94e-03). ETA=11:34:39, max mem: 30.7 GB 
[11/25 00:27:13 visual_prompt]: 	Training 200/553. train loss: 0.1625,	0.9376 s / batch. (data: 2.42e-04). ETA=11:19:34, max mem: 30.7 GB 
[11/25 00:28:47 visual_prompt]: 	Training 300/553. train loss: 0.2018,	0.9560 s / batch. (data: 2.38e-04). ETA=11:31:19, max mem: 30.7 GB 
[11/25 00:30:20 visual_prompt]: 	Training 400/553. train loss: 0.3009,	0.9095 s / batch. (data: 2.57e-04). ETA=10:56:10, max mem: 30.7 GB 
[11/25 00:31:54 visual_prompt]: 	Training 500/553. train loss: 0.0466,	0.9540 s / batch. (data: 7.22e-04). ETA=11:26:41, max mem: 30.7 GB 
[11/25 00:32:44 visual_prompt]: Epoch 22 / 100: avg data time: 2.09e-02, avg batch time: 0.9510, average train loss: 0.2314
[11/25 00:33:39 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3070, average loss: 1.2870
[11/25 00:33:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 63.68	
[11/25 00:35:14 visual_prompt]: 	Test 100/162. loss: 1.477, 0.3004 s / batch. (data: 3.29e-05)max mem: 30.66532 GB 
[11/25 00:36:03 visual_prompt]: Inference (test):avg data time: 7.72e-05, avg batch time: 0.3049, average loss: 1.3280
[11/25 00:36:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.86	rocauc: 65.94	
[11/25 00:36:03 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/25 00:37:45 visual_prompt]: 	Training 100/553. train loss: 0.1921,	0.9446 s / batch. (data: 1.66e-02). ETA=11:17:31, max mem: 30.7 GB 
[11/25 00:39:23 visual_prompt]: 	Training 200/553. train loss: 0.0764,	0.9495 s / batch. (data: 1.09e-02). ETA=11:19:26, max mem: 30.7 GB 
[11/25 00:40:58 visual_prompt]: 	Training 300/553. train loss: 0.0028,	0.9600 s / batch. (data: 7.21e-04). ETA=11:25:22, max mem: 30.7 GB 
[11/25 00:42:32 visual_prompt]: 	Training 400/553. train loss: 0.0360,	0.9481 s / batch. (data: 2.55e-04). ETA=11:15:14, max mem: 30.7 GB 
[11/25 00:44:06 visual_prompt]: 	Training 500/553. train loss: 0.7415,	0.9190 s / batch. (data: 2.57e-04). ETA=10:53:01, max mem: 30.7 GB 
[11/25 00:44:55 visual_prompt]: Epoch 23 / 100: avg data time: 3.06e-02, avg batch time: 0.9625, average train loss: 0.2124
[11/25 00:45:50 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3064, average loss: 1.2535
[11/25 00:45:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 62.64	
[11/25 00:47:26 visual_prompt]: 	Test 100/162. loss: 2.042, 0.2944 s / batch. (data: 5.27e-05)max mem: 30.66532 GB 
[11/25 00:48:13 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.3044, average loss: 1.2432
[11/25 00:48:13 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.05	rocauc: 62.80	
[11/25 00:48:13 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/25 00:49:59 visual_prompt]: 	Training 100/553. train loss: 0.5562,	0.9318 s / batch. (data: 7.98e-03). ETA=10:59:42, max mem: 30.7 GB 
[11/25 00:51:32 visual_prompt]: 	Training 200/553. train loss: 0.5694,	0.9630 s / batch. (data: 4.06e-02). ETA=11:20:14, max mem: 30.7 GB 
[11/25 00:53:06 visual_prompt]: 	Training 300/553. train loss: 0.2622,	0.9245 s / batch. (data: 2.62e-04). ETA=10:51:26, max mem: 30.7 GB 
[11/25 00:54:39 visual_prompt]: 	Training 400/553. train loss: 0.0355,	0.9585 s / batch. (data: 3.98e-02). ETA=11:13:49, max mem: 30.7 GB 
[11/25 00:56:13 visual_prompt]: 	Training 500/553. train loss: 0.0088,	0.9631 s / batch. (data: 2.17e-02). ETA=11:15:28, max mem: 30.7 GB 
[11/25 00:57:03 visual_prompt]: Epoch 24 / 100: avg data time: 2.78e-02, avg batch time: 0.9573, average train loss: 0.1684
[11/25 00:57:58 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3070, average loss: 1.5026
[11/25 00:57:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.46	
[11/25 00:59:32 visual_prompt]: 	Test 100/162. loss: 0.020, 0.3030 s / batch. (data: 2.98e-05)max mem: 30.66532 GB 
[11/25 01:00:18 visual_prompt]: Inference (test):avg data time: 8.69e-05, avg batch time: 0.3086, average loss: 1.5149
[11/25 01:00:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 65.43	rocauc: 68.50	
[11/25 01:00:18 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/25 01:02:01 visual_prompt]: 	Training 100/553. train loss: 0.0117,	0.9520 s / batch. (data: 7.59e-04). ETA=11:05:14, max mem: 30.7 GB 
[11/25 01:03:35 visual_prompt]: 	Training 200/553. train loss: 0.1496,	0.9381 s / batch. (data: 1.60e-02). ETA=10:53:58, max mem: 30.7 GB 
[11/25 01:05:08 visual_prompt]: 	Training 300/553. train loss: 0.0004,	0.9434 s / batch. (data: 1.36e-02). ETA=10:56:04, max mem: 30.7 GB 
[11/25 01:06:42 visual_prompt]: 	Training 400/553. train loss: 0.0026,	0.9281 s / batch. (data: 2.63e-04). ETA=10:43:53, max mem: 30.7 GB 
[11/25 01:08:16 visual_prompt]: 	Training 500/553. train loss: 0.0053,	0.9484 s / batch. (data: 2.24e-02). ETA=10:56:26, max mem: 30.7 GB 
[11/25 01:09:05 visual_prompt]: Epoch 25 / 100: avg data time: 2.38e-02, avg batch time: 0.9528, average train loss: 0.1622
[11/25 01:10:00 visual_prompt]: Inference (val):avg data time: 8.32e-05, avg batch time: 0.3055, average loss: 1.3752
[11/25 01:10:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 64.56	
[11/25 01:11:33 visual_prompt]: 	Test 100/162. loss: 1.402, 0.3158 s / batch. (data: 3.27e-05)max mem: 30.66532 GB 
[11/25 01:12:20 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.3063, average loss: 1.4748
[11/25 01:12:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.67	rocauc: 65.81	
[11/25 01:12:20 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/25 01:14:05 visual_prompt]: 	Training 100/553. train loss: 0.1103,	0.9560 s / batch. (data: 2.79e-04). ETA=10:59:14, max mem: 30.7 GB 
[11/25 01:15:39 visual_prompt]: 	Training 200/553. train loss: 0.0064,	0.9192 s / batch. (data: 9.87e-03). ETA=10:32:19, max mem: 30.7 GB 
[11/25 01:17:13 visual_prompt]: 	Training 300/553. train loss: 1.6547,	0.9301 s / batch. (data: 1.10e-03). ETA=10:38:18, max mem: 30.7 GB 
[11/25 01:18:46 visual_prompt]: 	Training 400/553. train loss: 0.1724,	0.9324 s / batch. (data: 5.38e-03). ETA=10:38:17, max mem: 30.7 GB 
[11/25 01:20:20 visual_prompt]: 	Training 500/553. train loss: 0.0040,	0.9465 s / batch. (data: 1.04e-02). ETA=10:46:22, max mem: 30.7 GB 
[11/25 01:21:10 visual_prompt]: Epoch 26 / 100: avg data time: 2.80e-02, avg batch time: 0.9580, average train loss: 0.1458
[11/25 01:22:05 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3054, average loss: 1.1343
[11/25 01:22:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 68.19	
[11/25 01:23:38 visual_prompt]: 	Test 100/162. loss: 0.360, 0.3034 s / batch. (data: 3.15e-05)max mem: 30.66532 GB 
[11/25 01:24:25 visual_prompt]: Inference (test):avg data time: 1.13e-04, avg batch time: 0.3062, average loss: 1.4916
[11/25 01:24:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.83	rocauc: 63.01	
[11/25 01:24:25 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/25 01:26:10 visual_prompt]: 	Training 100/553. train loss: 0.0075,	0.9517 s / batch. (data: 1.59e-02). ETA=10:47:29, max mem: 30.7 GB 
[11/25 01:27:43 visual_prompt]: 	Training 200/553. train loss: 0.0656,	0.9496 s / batch. (data: 7.13e-04). ETA=10:44:28, max mem: 30.7 GB 
[11/25 01:29:17 visual_prompt]: 	Training 300/553. train loss: 0.0585,	0.9178 s / batch. (data: 2.70e-04). ETA=10:21:24, max mem: 30.7 GB 
[11/25 01:30:50 visual_prompt]: 	Training 400/553. train loss: 0.3242,	0.9088 s / batch. (data: 2.68e-04). ETA=10:13:46, max mem: 30.7 GB 
[11/25 01:32:24 visual_prompt]: 	Training 500/553. train loss: 0.0093,	0.9308 s / batch. (data: 2.47e-04). ETA=10:27:05, max mem: 30.7 GB 
[11/25 01:33:14 visual_prompt]: Epoch 27 / 100: avg data time: 2.73e-02, avg batch time: 0.9562, average train loss: 0.1661
[11/25 01:34:09 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3065, average loss: 1.0004
[11/25 01:34:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 70.96	
[11/25 01:35:42 visual_prompt]: 	Test 100/162. loss: 0.020, 0.3083 s / batch. (data: 4.20e-05)max mem: 30.66532 GB 
[11/25 01:36:28 visual_prompt]: Inference (test):avg data time: 3.47e-05, avg batch time: 0.3060, average loss: 1.4163
[11/25 01:36:28 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.53	rocauc: 61.92	
[11/25 01:36:28 visual_prompt]: Stopping early.
[11/25 01:36:28 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 01:36:28 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 01:36:28 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/25 01:36:28 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/25 01:36:28 visual_prompt]: Training with config:
[11/25 01:36:28 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/test/seed8393/lr0.0001_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 8393, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/25 01:36:28 visual_prompt]: Loading training data...
[11/25 01:36:28 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 01:36:28 visual_prompt]: Loading validation data...
[11/25 01:36:28 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 01:36:28 visual_prompt]: Loading test data...
[11/25 01:36:28 visual_prompt]: Constructing mammo-cbis dataset test...
[11/25 01:36:28 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 01:36:35 visual_prompt]: Enable all parameters update during training
[11/25 01:36:35 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/25 01:36:35 visual_prompt]: tuned percent:100.000
[11/25 01:36:35 visual_prompt]: Device used for model: 0
[11/25 01:36:35 visual_prompt]: Setting up Evaluator...
[11/25 01:36:35 visual_prompt]: Setting up Trainer...
[11/25 01:36:35 visual_prompt]: 	Setting up the optimizer...
[11/25 01:36:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 01:38:18 visual_prompt]: 	Training 100/553. train loss: 0.0285,	0.9027 s / batch. (data: 2.37e-04). ETA=13:50:31, max mem: 30.7 GB 
[11/25 01:39:52 visual_prompt]: 	Training 200/553. train loss: 3.4493,	0.9120 s / batch. (data: 2.58e-04). ETA=13:57:32, max mem: 30.7 GB 
[11/25 01:41:26 visual_prompt]: 	Training 300/553. train loss: 1.6621,	0.9733 s / batch. (data: 5.83e-03). ETA=14:52:11, max mem: 30.7 GB 
[11/25 01:42:59 visual_prompt]: 	Training 400/553. train loss: 3.3399,	0.9250 s / batch. (data: 9.72e-03). ETA=14:06:21, max mem: 30.7 GB 
[11/25 01:44:33 visual_prompt]: 	Training 500/553. train loss: 0.1827,	0.9577 s / batch. (data: 1.85e-02). ETA=14:34:43, max mem: 30.7 GB 
[11/25 01:45:23 visual_prompt]: Epoch 1 / 100: avg data time: 2.38e-02, avg batch time: 0.9535, average train loss: 2.1069
[11/25 01:46:17 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.3057, average loss: 2.0883
[11/25 01:46:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 56.45	
[11/25 01:47:49 visual_prompt]: 	Test 100/162. loss: 0.147, 0.3198 s / batch. (data: 4.60e-05)max mem: 30.66532 GB 
[11/25 01:48:36 visual_prompt]: Inference (test):avg data time: 3.17e-05, avg batch time: 0.3053, average loss: 2.1041
[11/25 01:48:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 51.47	rocauc: 50.78	
[11/25 01:48:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/25 01:50:15 visual_prompt]: 	Training 100/553. train loss: 1.1122,	0.9560 s / batch. (data: 2.63e-04). ETA=14:30:42, max mem: 30.7 GB 
[11/25 01:51:49 visual_prompt]: 	Training 200/553. train loss: 0.6878,	0.9149 s / batch. (data: 5.43e-03). ETA=13:51:45, max mem: 30.7 GB 
[11/25 01:53:22 visual_prompt]: 	Training 300/553. train loss: 1.0865,	0.9319 s / batch. (data: 4.28e-04). ETA=14:05:40, max mem: 30.7 GB 
[11/25 01:54:57 visual_prompt]: 	Training 400/553. train loss: 1.1567,	0.9254 s / batch. (data: 7.77e-04). ETA=13:58:10, max mem: 30.7 GB 
[11/25 01:56:31 visual_prompt]: 	Training 500/553. train loss: 1.3822,	0.9324 s / batch. (data: 2.88e-04). ETA=14:02:58, max mem: 30.7 GB 
[11/25 01:57:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.65e-02, avg batch time: 0.9481, average train loss: 0.9040
[11/25 01:58:15 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3063, average loss: 0.6799
[11/25 01:58:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 58.49	
[11/25 01:59:48 visual_prompt]: 	Test 100/162. loss: 0.514, 0.2950 s / batch. (data: 3.74e-05)max mem: 30.66532 GB 
[11/25 02:00:35 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.3055, average loss: 0.6712
[11/25 02:00:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.62	rocauc: 60.74	
[11/25 02:00:35 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/25 02:02:20 visual_prompt]: 	Training 100/553. train loss: 0.8776,	0.9305 s / batch. (data: 7.17e-04). ETA=13:58:55, max mem: 30.7 GB 
[11/25 02:03:53 visual_prompt]: 	Training 200/553. train loss: 0.4771,	0.9520 s / batch. (data: 7.51e-04). ETA=14:16:42, max mem: 30.7 GB 
[11/25 02:05:27 visual_prompt]: 	Training 300/553. train loss: 1.0598,	0.9360 s / batch. (data: 8.08e-04). ETA=14:00:43, max mem: 30.7 GB 
[11/25 02:07:00 visual_prompt]: 	Training 400/553. train loss: 0.7800,	0.9137 s / batch. (data: 2.72e-04). ETA=13:39:10, max mem: 30.7 GB 
[11/25 02:08:34 visual_prompt]: 	Training 500/553. train loss: 0.5935,	0.9310 s / batch. (data: 7.48e-04). ETA=13:53:11, max mem: 30.7 GB 
[11/25 02:09:23 visual_prompt]: Epoch 3 / 100: avg data time: 2.62e-02, avg batch time: 0.9554, average train loss: 0.7843
[11/25 02:10:19 visual_prompt]: Inference (val):avg data time: 1.92e-04, avg batch time: 0.3060, average loss: 0.7525
[11/25 02:10:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 63.26	
[11/25 02:11:52 visual_prompt]: 	Test 100/162. loss: 0.670, 0.3034 s / batch. (data: 3.17e-05)max mem: 30.66532 GB 
[11/25 02:12:39 visual_prompt]: Inference (test):avg data time: 1.62e-04, avg batch time: 0.3078, average loss: 0.7007
[11/25 02:12:39 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 62.30	
[11/25 02:12:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/25 02:14:21 visual_prompt]: 	Training 100/553. train loss: 0.9210,	0.9440 s / batch. (data: 2.81e-04). ETA=14:02:22, max mem: 30.7 GB 
[11/25 02:15:55 visual_prompt]: 	Training 200/553. train loss: 1.2606,	0.9300 s / batch. (data: 2.50e-04). ETA=13:48:22, max mem: 30.7 GB 
[11/25 02:17:29 visual_prompt]: 	Training 300/553. train loss: 0.9073,	0.9457 s / batch. (data: 7.57e-04). ETA=14:00:46, max mem: 30.7 GB 
[11/25 02:19:03 visual_prompt]: 	Training 400/553. train loss: 0.7808,	0.9331 s / batch. (data: 2.40e-04). ETA=13:48:01, max mem: 30.7 GB 
[11/25 02:20:36 visual_prompt]: 	Training 500/553. train loss: 0.8568,	0.9282 s / batch. (data: 7.45e-04). ETA=13:42:05, max mem: 30.7 GB 
[11/25 02:21:26 visual_prompt]: Epoch 4 / 100: avg data time: 2.09e-02, avg batch time: 0.9524, average train loss: 0.7385
[11/25 02:22:21 visual_prompt]: Inference (val):avg data time: 4.73e-04, avg batch time: 0.3201, average loss: 0.6824
[11/25 02:22:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 67.17	
[11/25 02:23:54 visual_prompt]: 	Test 100/162. loss: 0.591, 0.2947 s / batch. (data: 4.22e-05)max mem: 30.66532 GB 
[11/25 02:24:40 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.3070, average loss: 0.6486
[11/25 02:24:40 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.09	rocauc: 65.90	
[11/25 02:24:40 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/25 02:26:22 visual_prompt]: 	Training 100/553. train loss: 0.6383,	0.9640 s / batch. (data: 2.45e-04). ETA=14:11:19, max mem: 30.7 GB 
[11/25 02:27:56 visual_prompt]: 	Training 200/553. train loss: 0.6074,	0.9394 s / batch. (data: 5.86e-03). ETA=13:48:04, max mem: 30.7 GB 
[11/25 02:29:29 visual_prompt]: 	Training 300/553. train loss: 0.8359,	0.9296 s / batch. (data: 1.04e-02). ETA=13:37:51, max mem: 30.7 GB 
[11/25 02:31:03 visual_prompt]: 	Training 400/553. train loss: 0.7694,	0.9406 s / batch. (data: 6.97e-04). ETA=13:45:57, max mem: 30.7 GB 
[11/25 02:32:37 visual_prompt]: 	Training 500/553. train loss: 0.3580,	0.9332 s / batch. (data: 5.37e-03). ETA=13:37:56, max mem: 30.7 GB 
[11/25 02:33:27 visual_prompt]: Epoch 5 / 100: avg data time: 2.20e-02, avg batch time: 0.9524, average train loss: 0.7194
[11/25 02:34:22 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3075, average loss: 0.6084
[11/25 02:34:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 72.28	
[11/25 02:35:55 visual_prompt]: 	Test 100/162. loss: 0.584, 0.2951 s / batch. (data: 3.27e-05)max mem: 30.66532 GB 
[11/25 02:36:41 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.3056, average loss: 0.6212
[11/25 02:36:41 visual_prompt]: Classification results with test_mammo-cbis: top1: 64.50	rocauc: 68.33	
[11/25 02:36:41 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/25 02:38:26 visual_prompt]: 	Training 100/553. train loss: 1.0505,	0.9030 s / batch. (data: 2.72e-04). ETA=13:09:09, max mem: 30.7 GB 
[11/25 02:39:59 visual_prompt]: 	Training 200/553. train loss: 0.7419,	0.9426 s / batch. (data: 2.87e-02). ETA=13:42:09, max mem: 30.7 GB 
[11/25 02:41:33 visual_prompt]: 	Training 300/553. train loss: 0.8604,	0.9572 s / batch. (data: 8.33e-04). ETA=13:53:21, max mem: 30.7 GB 
[11/25 02:43:07 visual_prompt]: 	Training 400/553. train loss: 0.2515,	0.9721 s / batch. (data: 1.09e-02). ETA=14:04:38, max mem: 30.7 GB 
[11/25 02:44:41 visual_prompt]: 	Training 500/553. train loss: 0.9458,	0.9298 s / batch. (data: 2.63e-04). ETA=13:26:21, max mem: 30.7 GB 
[11/25 02:45:31 visual_prompt]: Epoch 6 / 100: avg data time: 2.62e-02, avg batch time: 0.9566, average train loss: 0.7042
[11/25 02:46:25 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3031, average loss: 0.6180
[11/25 02:46:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 72.05	
[11/25 02:47:58 visual_prompt]: 	Test 100/162. loss: 0.525, 0.3022 s / batch. (data: 3.67e-05)max mem: 30.66532 GB 
[11/25 02:48:45 visual_prompt]: Inference (test):avg data time: 2.19e-04, avg batch time: 0.3050, average loss: 0.6170
[11/25 02:48:45 visual_prompt]: Classification results with test_mammo-cbis: top1: 65.74	rocauc: 69.36	
[11/25 02:48:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/25 02:50:27 visual_prompt]: 	Training 100/553. train loss: 1.4082,	0.9280 s / batch. (data: 2.50e-04). ETA=13:22:25, max mem: 30.7 GB 
[11/25 02:52:00 visual_prompt]: 	Training 200/553. train loss: 0.5940,	0.9308 s / batch. (data: 2.03e-02). ETA=13:23:17, max mem: 30.7 GB 
[11/25 02:53:33 visual_prompt]: 	Training 300/553. train loss: 0.5503,	0.9576 s / batch. (data: 1.55e-02). ETA=13:44:50, max mem: 30.7 GB 
[11/25 02:55:07 visual_prompt]: 	Training 400/553. train loss: 0.6449,	0.9325 s / batch. (data: 7.50e-04). ETA=13:21:39, max mem: 30.7 GB 
[11/25 02:56:41 visual_prompt]: 	Training 500/553. train loss: 1.0192,	0.9510 s / batch. (data: 9.65e-03). ETA=13:36:00, max mem: 30.7 GB 
[11/25 02:57:31 visual_prompt]: Epoch 7 / 100: avg data time: 1.95e-02, avg batch time: 0.9499, average train loss: 0.6502
[11/25 02:58:26 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3069, average loss: 0.6465
[11/25 02:58:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 74.35	
[11/25 02:59:59 visual_prompt]: 	Test 100/162. loss: 0.565, 0.3003 s / batch. (data: 3.17e-05)max mem: 30.66532 GB 
[11/25 03:00:45 visual_prompt]: Inference (test):avg data time: 3.20e-05, avg batch time: 0.3058, average loss: 0.6456
[11/25 03:00:45 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.10	rocauc: 69.05	
[11/25 03:00:45 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/25 03:02:29 visual_prompt]: 	Training 100/553. train loss: 0.8893,	0.9188 s / batch. (data: 4.39e-03). ETA=13:05:58, max mem: 30.7 GB 
[11/25 03:04:02 visual_prompt]: 	Training 200/553. train loss: 0.4031,	0.9200 s / batch. (data: 7.93e-04). ETA=13:05:30, max mem: 30.7 GB 
[11/25 03:05:36 visual_prompt]: 	Training 300/553. train loss: 0.6574,	0.9376 s / batch. (data: 7.00e-04). ETA=13:18:57, max mem: 30.7 GB 
[11/25 03:07:10 visual_prompt]: 	Training 400/553. train loss: 0.6082,	0.9424 s / batch. (data: 5.40e-03). ETA=13:21:29, max mem: 30.7 GB 
[11/25 03:08:43 visual_prompt]: 	Training 500/553. train loss: 0.5372,	0.9444 s / batch. (data: 8.23e-04). ETA=13:21:37, max mem: 30.7 GB 
[11/25 03:09:33 visual_prompt]: Epoch 8 / 100: avg data time: 2.42e-02, avg batch time: 0.9538, average train loss: 0.7074
[11/25 03:10:28 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3054, average loss: 0.6664
[11/25 03:10:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 66.12	
[11/25 03:12:02 visual_prompt]: 	Test 100/162. loss: 0.441, 0.3129 s / batch. (data: 3.58e-05)max mem: 30.66532 GB 
[11/25 03:12:48 visual_prompt]: Inference (test):avg data time: 8.61e-05, avg batch time: 0.3066, average loss: 0.6438
[11/25 03:12:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.71	rocauc: 65.97	
[11/25 03:12:48 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/25 03:14:30 visual_prompt]: 	Training 100/553. train loss: 0.8953,	0.9560 s / batch. (data: 7.99e-03). ETA=13:29:02, max mem: 30.7 GB 
[11/25 03:16:03 visual_prompt]: 	Training 200/553. train loss: 0.9220,	0.9299 s / batch. (data: 2.96e-04). ETA=13:05:22, max mem: 30.7 GB 
[11/25 03:17:37 visual_prompt]: 	Training 300/553. train loss: 0.8403,	0.9186 s / batch. (data: 2.73e-04). ETA=12:54:19, max mem: 30.7 GB 
[11/25 03:19:11 visual_prompt]: 	Training 400/553. train loss: 0.8433,	0.9262 s / batch. (data: 8.89e-03). ETA=12:59:12, max mem: 30.7 GB 
[11/25 03:20:44 visual_prompt]: 	Training 500/553. train loss: 0.6647,	0.9348 s / batch. (data: 5.34e-03). ETA=13:04:49, max mem: 30.7 GB 
[11/25 03:21:34 visual_prompt]: Epoch 9 / 100: avg data time: 2.19e-02, avg batch time: 0.9507, average train loss: 0.6539
[11/25 03:22:29 visual_prompt]: Inference (val):avg data time: 5.52e-05, avg batch time: 0.3082, average loss: 0.6385
[11/25 03:22:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 68.70	
[11/25 03:24:02 visual_prompt]: 	Test 100/162. loss: 0.498, 0.3124 s / batch. (data: 3.81e-05)max mem: 30.66532 GB 
[11/25 03:24:49 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.3076, average loss: 0.6488
[11/25 03:24:49 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.64	rocauc: 66.49	
[11/25 03:24:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/25 03:26:30 visual_prompt]: 	Training 100/553. train loss: 0.8726,	0.9150 s / batch. (data: 2.72e-04). ETA=12:45:52, max mem: 30.7 GB 
[11/25 03:28:04 visual_prompt]: 	Training 200/553. train loss: 1.0205,	0.9442 s / batch. (data: 5.36e-03). ETA=13:08:47, max mem: 30.7 GB 
[11/25 03:29:38 visual_prompt]: 	Training 300/553. train loss: 0.9012,	0.9116 s / batch. (data: 2.78e-04). ETA=12:40:02, max mem: 30.7 GB 
[11/25 03:31:11 visual_prompt]: 	Training 400/553. train loss: 0.8488,	0.9526 s / batch. (data: 1.09e-02). ETA=13:12:36, max mem: 30.7 GB 
[11/25 03:32:46 visual_prompt]: 	Training 500/553. train loss: 0.7606,	0.9265 s / batch. (data: 5.37e-03). ETA=12:49:18, max mem: 30.7 GB 
[11/25 03:33:35 visual_prompt]: Epoch 10 / 100: avg data time: 2.11e-02, avg batch time: 0.9522, average train loss: 0.6509
[11/25 03:34:30 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3059, average loss: 0.7864
[11/25 03:34:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 66.90	
[11/25 03:36:04 visual_prompt]: 	Test 100/162. loss: 0.558, 0.3005 s / batch. (data: 5.05e-05)max mem: 30.66532 GB 
[11/25 03:36:50 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.3040, average loss: 0.7227
[11/25 03:36:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 64.77	
[11/25 03:36:50 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/25 03:38:34 visual_prompt]: 	Training 100/553. train loss: 0.4861,	0.9321 s / batch. (data: 1.76e-02). ETA=12:51:36, max mem: 30.7 GB 
[11/25 03:40:08 visual_prompt]: 	Training 200/553. train loss: 0.3156,	0.9399 s / batch. (data: 2.41e-04). ETA=12:56:30, max mem: 30.7 GB 
[11/25 03:41:42 visual_prompt]: 	Training 300/553. train loss: 0.3993,	0.9579 s / batch. (data: 7.57e-04). ETA=13:09:46, max mem: 30.7 GB 
[11/25 03:43:15 visual_prompt]: 	Training 400/553. train loss: 0.6629,	0.9440 s / batch. (data: 7.93e-04). ETA=12:56:47, max mem: 30.7 GB 
[11/25 03:44:49 visual_prompt]: 	Training 500/553. train loss: 0.8258,	0.9594 s / batch. (data: 5.85e-03). ETA=13:07:51, max mem: 30.7 GB 
[11/25 03:45:39 visual_prompt]: Epoch 11 / 100: avg data time: 2.55e-02, avg batch time: 0.9562, average train loss: 0.6281
[11/25 03:46:34 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3047, average loss: 0.6573
[11/25 03:46:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.62	
[11/25 03:48:08 visual_prompt]: 	Test 100/162. loss: 0.533, 0.3135 s / batch. (data: 5.01e-05)max mem: 30.66532 GB 
[11/25 03:48:54 visual_prompt]: Inference (test):avg data time: 2.84e-04, avg batch time: 0.3077, average loss: 0.7028
[11/25 03:48:54 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.48	rocauc: 67.39	
[11/25 03:48:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/25 03:50:37 visual_prompt]: 	Training 100/553. train loss: 0.2144,	0.9280 s / batch. (data: 2.89e-04). ETA=12:39:39, max mem: 30.7 GB 
[11/25 03:52:11 visual_prompt]: 	Training 200/553. train loss: 1.2417,	0.9402 s / batch. (data: 8.28e-03). ETA=12:48:06, max mem: 30.7 GB 
[11/25 03:53:45 visual_prompt]: 	Training 300/553. train loss: 1.0868,	0.9274 s / batch. (data: 4.83e-04). ETA=12:36:03, max mem: 30.7 GB 
[11/25 03:55:19 visual_prompt]: 	Training 400/553. train loss: 0.5936,	0.9305 s / batch. (data: 6.82e-04). ETA=12:37:06, max mem: 30.7 GB 
[11/25 03:56:52 visual_prompt]: 	Training 500/553. train loss: 0.7357,	0.9347 s / batch. (data: 7.40e-04). ETA=12:38:57, max mem: 30.7 GB 
[11/25 03:57:42 visual_prompt]: Epoch 12 / 100: avg data time: 2.38e-02, avg batch time: 0.9543, average train loss: 0.6142
[11/25 03:58:37 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3061, average loss: 0.6877
[11/25 03:58:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 66.05	
[11/25 04:00:10 visual_prompt]: 	Test 100/162. loss: 0.456, 0.3061 s / batch. (data: 6.56e-05)max mem: 30.66532 GB 
[11/25 04:00:56 visual_prompt]: Inference (test):avg data time: 1.55e-04, avg batch time: 0.3042, average loss: 0.6614
[11/25 04:00:56 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.64	rocauc: 66.54	
[11/25 04:00:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/25 04:02:40 visual_prompt]: 	Training 100/553. train loss: 0.4972,	0.9337 s / batch. (data: 2.40e-04). ETA=12:35:45, max mem: 30.7 GB 
[11/25 04:04:13 visual_prompt]: 	Training 200/553. train loss: 0.3323,	0.9358 s / batch. (data: 8.57e-03). ETA=12:35:51, max mem: 30.7 GB 
[11/25 04:05:47 visual_prompt]: 	Training 300/553. train loss: 0.5848,	0.9387 s / batch. (data: 5.82e-03). ETA=12:36:40, max mem: 30.7 GB 
[11/25 04:07:21 visual_prompt]: 	Training 400/553. train loss: 0.5360,	0.9808 s / batch. (data: 1.04e-02). ETA=13:08:59, max mem: 30.7 GB 
[11/25 04:08:55 visual_prompt]: 	Training 500/553. train loss: 0.6494,	0.9355 s / batch. (data: 5.35e-03). ETA=12:30:56, max mem: 30.7 GB 
[11/25 04:09:44 visual_prompt]: Epoch 13 / 100: avg data time: 2.46e-02, avg batch time: 0.9546, average train loss: 0.6010
[11/25 04:10:39 visual_prompt]: Inference (val):avg data time: 4.21e-04, avg batch time: 0.3076, average loss: 0.7572
[11/25 04:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 67.53	
[11/25 04:12:12 visual_prompt]: 	Test 100/162. loss: 0.645, 0.3040 s / batch. (data: 4.63e-05)max mem: 30.66532 GB 
[11/25 04:12:59 visual_prompt]: Inference (test):avg data time: 1.06e-04, avg batch time: 0.3064, average loss: 0.8462
[11/25 04:12:59 visual_prompt]: Classification results with test_mammo-cbis: top1: 55.35	rocauc: 63.93	
[11/25 04:12:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/25 04:14:40 visual_prompt]: 	Training 100/553. train loss: 0.5017,	0.9481 s / batch. (data: 1.04e-02). ETA=12:38:40, max mem: 30.7 GB 
[11/25 04:16:14 visual_prompt]: 	Training 200/553. train loss: 0.6561,	0.9431 s / batch. (data: 7.02e-04). ETA=12:33:02, max mem: 30.7 GB 
[11/25 04:17:47 visual_prompt]: 	Training 300/553. train loss: 0.8638,	0.9222 s / batch. (data: 2.46e-04). ETA=12:14:49, max mem: 30.7 GB 
[11/25 04:19:21 visual_prompt]: 	Training 400/553. train loss: 0.6052,	0.9604 s / batch. (data: 2.68e-04). ETA=12:43:40, max mem: 30.7 GB 
[11/25 04:20:55 visual_prompt]: 	Training 500/553. train loss: 0.4583,	0.9177 s / batch. (data: 2.60e-04). ETA=12:08:14, max mem: 30.7 GB 
[11/25 04:21:44 visual_prompt]: Epoch 14 / 100: avg data time: 2.09e-02, avg batch time: 0.9504, average train loss: 0.5790
[11/25 04:22:39 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3064, average loss: 0.7271
[11/25 04:22:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 65.32	
[11/25 04:24:13 visual_prompt]: 	Test 100/162. loss: 0.638, 0.2972 s / batch. (data: 4.82e-05)max mem: 30.66532 GB 
[11/25 04:24:59 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.3073, average loss: 0.7557
[11/25 04:24:59 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 62.98	
[11/25 04:24:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/25 04:26:41 visual_prompt]: 	Training 100/553. train loss: 0.6478,	0.9295 s / batch. (data: 2.71e-04). ETA=12:15:09, max mem: 30.7 GB 
[11/25 04:28:16 visual_prompt]: 	Training 200/553. train loss: 0.3736,	0.9520 s / batch. (data: 2.65e-04). ETA=12:31:24, max mem: 30.7 GB 
[11/25 04:29:49 visual_prompt]: 	Training 300/553. train loss: 0.6833,	0.9308 s / batch. (data: 7.04e-04). ETA=12:13:07, max mem: 30.7 GB 
[11/25 04:31:23 visual_prompt]: 	Training 400/553. train loss: 0.2432,	0.9310 s / batch. (data: 2.60e-04). ETA=12:11:42, max mem: 30.7 GB 
[11/25 04:32:57 visual_prompt]: 	Training 500/553. train loss: 0.4072,	0.9193 s / batch. (data: 8.02e-03). ETA=12:01:00, max mem: 30.7 GB 
[11/25 04:33:46 visual_prompt]: Epoch 15 / 100: avg data time: 2.23e-02, avg batch time: 0.9532, average train loss: 0.5405
[11/25 04:34:41 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3056, average loss: 0.7022
[11/25 04:34:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 65.35	
[11/25 04:36:14 visual_prompt]: 	Test 100/162. loss: 0.676, 0.2952 s / batch. (data: 4.91e-05)max mem: 30.66532 GB 
[11/25 04:37:01 visual_prompt]: Inference (test):avg data time: 3.10e-05, avg batch time: 0.3042, average loss: 0.7078
[11/25 04:37:01 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.62	rocauc: 62.82	
[11/25 04:37:01 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/25 04:38:44 visual_prompt]: 	Training 100/553. train loss: 0.6139,	0.9164 s / batch. (data: 3.20e-04). ETA=11:56:24, max mem: 30.7 GB 
[11/25 04:40:18 visual_prompt]: 	Training 200/553. train loss: 1.2752,	0.9393 s / batch. (data: 9.35e-04). ETA=12:12:44, max mem: 30.7 GB 
[11/25 04:41:52 visual_prompt]: 	Training 300/553. train loss: 0.2953,	0.9575 s / batch. (data: 1.59e-02). ETA=12:25:21, max mem: 30.7 GB 
[11/25 04:43:25 visual_prompt]: 	Training 400/553. train loss: 0.2899,	0.9243 s / batch. (data: 2.85e-04). ETA=11:57:57, max mem: 30.7 GB 
[11/25 04:44:59 visual_prompt]: 	Training 500/553. train loss: 0.8063,	0.9517 s / batch. (data: 2.53e-04). ETA=12:17:38, max mem: 30.7 GB 
[11/25 04:45:48 visual_prompt]: Epoch 16 / 100: avg data time: 2.17e-02, avg batch time: 0.9533, average train loss: 0.5091
[11/25 04:46:43 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3058, average loss: 0.7680
[11/25 04:46:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.87	
[11/25 04:48:17 visual_prompt]: 	Test 100/162. loss: 0.343, 0.3049 s / batch. (data: 3.48e-05)max mem: 30.66532 GB 
[11/25 04:49:03 visual_prompt]: Inference (test):avg data time: 9.25e-05, avg batch time: 0.3054, average loss: 0.7824
[11/25 04:49:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.29	rocauc: 62.66	
[11/25 04:49:03 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/25 04:50:46 visual_prompt]: 	Training 100/553. train loss: 0.3020,	0.9280 s / batch. (data: 2.70e-04). ETA=11:56:52, max mem: 30.7 GB 
[11/25 04:52:20 visual_prompt]: 	Training 200/553. train loss: 0.7762,	0.9089 s / batch. (data: 2.53e-04). ETA=11:40:39, max mem: 30.7 GB 
[11/25 04:53:54 visual_prompt]: 	Training 300/553. train loss: 0.0866,	0.9191 s / batch. (data: 8.78e-03). ETA=11:47:00, max mem: 30.7 GB 
[11/25 04:55:27 visual_prompt]: 	Training 400/553. train loss: 0.7933,	0.9432 s / batch. (data: 7.48e-04). ETA=12:03:55, max mem: 30.7 GB 
[11/25 04:57:01 visual_prompt]: 	Training 500/553. train loss: 0.1610,	0.9308 s / batch. (data: 2.42e-04). ETA=11:52:54, max mem: 30.7 GB 
[11/25 04:57:51 visual_prompt]: Epoch 17 / 100: avg data time: 2.30e-02, avg batch time: 0.9535, average train loss: 0.5002
[11/25 04:58:46 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3059, average loss: 0.8472
[11/25 04:58:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.71	
[11/25 05:00:19 visual_prompt]: 	Test 100/162. loss: 0.423, 0.2954 s / batch. (data: 3.08e-05)max mem: 30.66532 GB 
[11/25 05:01:05 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.3068, average loss: 0.8346
[11/25 05:01:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.31	rocauc: 60.85	
[11/25 05:01:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/25 05:02:46 visual_prompt]: 	Training 100/553. train loss: 0.3718,	0.9509 s / batch. (data: 7.52e-03). ETA=12:05:48, max mem: 30.7 GB 
[11/25 05:04:20 visual_prompt]: 	Training 200/553. train loss: 0.7473,	0.9511 s / batch. (data: 7.36e-04). ETA=12:04:25, max mem: 30.7 GB 
[11/25 05:05:54 visual_prompt]: 	Training 300/553. train loss: 0.2020,	0.9244 s / batch. (data: 7.24e-04). ETA=11:42:32, max mem: 30.7 GB 
[11/25 05:07:27 visual_prompt]: 	Training 400/553. train loss: 0.4772,	0.9401 s / batch. (data: 5.86e-03). ETA=11:52:55, max mem: 30.7 GB 
[11/25 05:09:01 visual_prompt]: 	Training 500/553. train loss: 0.3220,	0.9345 s / batch. (data: 1.55e-02). ETA=11:47:04, max mem: 30.7 GB 
[11/25 05:09:51 visual_prompt]: Epoch 18 / 100: avg data time: 1.94e-02, avg batch time: 0.9498, average train loss: 0.4489
[11/25 05:10:46 visual_prompt]: Inference (val):avg data time: 8.31e-05, avg batch time: 0.3062, average loss: 0.7681
[11/25 05:10:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 65.03	
[11/25 05:12:19 visual_prompt]: 	Test 100/162. loss: 0.484, 0.3163 s / batch. (data: 3.79e-05)max mem: 30.66532 GB 
[11/25 05:13:05 visual_prompt]: Inference (test):avg data time: 7.73e-05, avg batch time: 0.3065, average loss: 0.8385
[11/25 05:13:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.29	rocauc: 60.28	
[11/25 05:13:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/25 05:14:48 visual_prompt]: 	Training 100/553. train loss: 0.7818,	0.9220 s / batch. (data: 2.75e-04). ETA=11:35:17, max mem: 30.7 GB 
[11/25 05:16:22 visual_prompt]: 	Training 200/553. train loss: 0.2450,	0.9680 s / batch. (data: 7.50e-04). ETA=12:08:22, max mem: 30.7 GB 
[11/25 05:17:55 visual_prompt]: 	Training 300/553. train loss: 0.7224,	0.9247 s / batch. (data: 2.90e-04). ETA=11:34:14, max mem: 30.7 GB 
[11/25 05:19:30 visual_prompt]: 	Training 400/553. train loss: 0.2889,	0.9184 s / batch. (data: 3.04e-04). ETA=11:28:00, max mem: 30.7 GB 
[11/25 05:21:04 visual_prompt]: 	Training 500/553. train loss: 0.0376,	0.9390 s / batch. (data: 5.36e-03). ETA=11:41:48, max mem: 30.7 GB 
[11/25 05:21:53 visual_prompt]: Epoch 19 / 100: avg data time: 2.10e-02, avg batch time: 0.9537, average train loss: 0.4153
[11/25 05:22:48 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3053, average loss: 0.8815
[11/25 05:22:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 64.63	
[11/25 05:24:21 visual_prompt]: 	Test 100/162. loss: 0.124, 0.2988 s / batch. (data: 3.41e-05)max mem: 30.66532 GB 
[11/25 05:25:08 visual_prompt]: Inference (test):avg data time: 1.27e-04, avg batch time: 0.3062, average loss: 0.9274
[11/25 05:25:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.60	rocauc: 59.08	
[11/25 05:25:08 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/25 05:26:50 visual_prompt]: 	Training 100/553. train loss: 0.1945,	0.9440 s / batch. (data: 2.55e-04). ETA=11:43:10, max mem: 30.7 GB 
[11/25 05:28:24 visual_prompt]: 	Training 200/553. train loss: 0.3420,	0.9429 s / batch. (data: 7.61e-04). ETA=11:40:44, max mem: 30.7 GB 
[11/25 05:29:57 visual_prompt]: 	Training 300/553. train loss: 0.0946,	0.9079 s / batch. (data: 2.22e-04). ETA=11:13:15, max mem: 30.7 GB 
[11/25 05:31:31 visual_prompt]: 	Training 400/553. train loss: 0.4003,	0.9498 s / batch. (data: 1.48e-02). ETA=11:42:45, max mem: 30.7 GB 
[11/25 05:33:05 visual_prompt]: 	Training 500/553. train loss: 0.2833,	0.9581 s / batch. (data: 7.33e-04). ETA=11:47:19, max mem: 30.7 GB 
[11/25 05:33:54 visual_prompt]: Epoch 20 / 100: avg data time: 2.16e-02, avg batch time: 0.9509, average train loss: 0.3953
[11/25 05:34:49 visual_prompt]: Inference (val):avg data time: 2.51e-04, avg batch time: 0.3064, average loss: 1.0277
[11/25 05:34:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.22	
[11/25 05:36:22 visual_prompt]: 	Test 100/162. loss: 0.893, 0.3184 s / batch. (data: 3.19e-05)max mem: 30.66532 GB 
[11/25 05:37:09 visual_prompt]: Inference (test):avg data time: 3.07e-05, avg batch time: 0.3060, average loss: 0.9058
[11/25 05:37:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.02	rocauc: 63.10	
[11/25 05:37:09 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/25 05:38:52 visual_prompt]: 	Training 100/553. train loss: 0.1892,	0.9346 s / batch. (data: 7.62e-04). ETA=11:27:32, max mem: 30.7 GB 
[11/25 05:40:25 visual_prompt]: 	Training 200/553. train loss: 0.2588,	0.9531 s / batch. (data: 1.88e-02). ETA=11:39:34, max mem: 30.7 GB 
[11/25 05:41:59 visual_prompt]: 	Training 300/553. train loss: 0.4180,	0.9246 s / batch. (data: 2.62e-04). ETA=11:17:08, max mem: 30.7 GB 
[11/25 05:43:32 visual_prompt]: 	Training 400/553. train loss: 0.7452,	0.9198 s / batch. (data: 1.04e-02). ETA=11:12:03, max mem: 30.7 GB 
[11/25 05:45:07 visual_prompt]: 	Training 500/553. train loss: 0.0300,	0.9593 s / batch. (data: 1.04e-02). ETA=11:39:21, max mem: 30.7 GB 
[11/25 05:45:57 visual_prompt]: Epoch 21 / 100: avg data time: 2.35e-02, avg batch time: 0.9543, average train loss: 0.3758
[11/25 05:46:52 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3064, average loss: 0.8517
[11/25 05:46:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 66.73	
[11/25 05:48:26 visual_prompt]: 	Test 100/162. loss: 1.606, 0.3110 s / batch. (data: 3.24e-05)max mem: 30.66532 GB 
[11/25 05:49:13 visual_prompt]: Inference (test):avg data time: 7.69e-05, avg batch time: 0.3051, average loss: 0.9112
[11/25 05:49:13 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.55	rocauc: 63.82	
[11/25 05:49:13 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/25 05:50:54 visual_prompt]: 	Training 100/553. train loss: 0.8431,	0.9559 s / batch. (data: 7.82e-04). ETA=11:34:25, max mem: 30.7 GB 
[11/25 05:52:27 visual_prompt]: 	Training 200/553. train loss: 0.6009,	0.9339 s / batch. (data: 6.95e-04). ETA=11:16:54, max mem: 30.7 GB 
[11/25 05:54:01 visual_prompt]: 	Training 300/553. train loss: 0.0878,	0.9313 s / batch. (data: 2.72e-04). ETA=11:13:25, max mem: 30.7 GB 
[11/25 05:55:35 visual_prompt]: 	Training 400/553. train loss: 0.5728,	0.9137 s / batch. (data: 2.91e-04). ETA=10:59:11, max mem: 30.7 GB 
[11/25 05:57:09 visual_prompt]: 	Training 500/553. train loss: 0.1088,	0.9231 s / batch. (data: 2.74e-04). ETA=11:04:27, max mem: 30.7 GB 
[11/25 05:57:59 visual_prompt]: Epoch 22 / 100: avg data time: 2.06e-02, avg batch time: 0.9509, average train loss: 0.3174
[11/25 05:58:54 visual_prompt]: Inference (val):avg data time: 4.22e-04, avg batch time: 0.3080, average loss: 0.9235
[11/25 05:58:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.51	
[11/25 06:00:27 visual_prompt]: 	Test 100/162. loss: 1.267, 0.3076 s / batch. (data: 3.62e-05)max mem: 30.66532 GB 
[11/25 06:01:14 visual_prompt]: Inference (test):avg data time: 1.22e-04, avg batch time: 0.3064, average loss: 0.9685
[11/25 06:01:14 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 63.21	
[11/25 06:01:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/25 06:02:55 visual_prompt]: 	Training 100/553. train loss: 0.0084,	0.9186 s / batch. (data: 3.00e-04). ETA=10:58:49, max mem: 30.7 GB 
[11/25 06:04:30 visual_prompt]: 	Training 200/553. train loss: 0.0183,	0.9280 s / batch. (data: 7.69e-04). ETA=11:04:04, max mem: 30.7 GB 
[11/25 06:06:04 visual_prompt]: 	Training 300/553. train loss: 0.3595,	0.9590 s / batch. (data: 7.21e-04). ETA=11:24:39, max mem: 30.7 GB 
[11/25 06:07:37 visual_prompt]: 	Training 400/553. train loss: 0.6494,	0.9400 s / batch. (data: 8.00e-03). ETA=11:09:29, max mem: 30.7 GB 
[11/25 06:09:11 visual_prompt]: 	Training 500/553. train loss: 0.3001,	0.9556 s / batch. (data: 1.06e-02). ETA=11:19:02, max mem: 30.7 GB 
[11/25 06:10:01 visual_prompt]: Epoch 23 / 100: avg data time: 2.07e-02, avg batch time: 0.9521, average train loss: 0.2687
[11/25 06:10:56 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3070, average loss: 1.0969
[11/25 06:10:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 66.87	
[11/25 06:12:29 visual_prompt]: 	Test 100/162. loss: 2.078, 0.2947 s / batch. (data: 3.91e-05)max mem: 30.66532 GB 
[11/25 06:13:16 visual_prompt]: Inference (test):avg data time: 7.70e-05, avg batch time: 0.3066, average loss: 1.2505
[11/25 06:13:16 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.09	rocauc: 63.02	
[11/25 06:13:16 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/25 06:14:58 visual_prompt]: 	Training 100/553. train loss: 0.2801,	0.9094 s / batch. (data: 7.95e-03). ETA=10:43:53, max mem: 30.7 GB 
[11/25 06:16:31 visual_prompt]: 	Training 200/553. train loss: 0.0424,	0.9070 s / batch. (data: 2.69e-04). ETA=10:40:40, max mem: 30.7 GB 
[11/25 06:18:05 visual_prompt]: 	Training 300/553. train loss: 0.0343,	0.9404 s / batch. (data: 5.38e-03). ETA=11:02:41, max mem: 30.7 GB 
[11/25 06:19:38 visual_prompt]: 	Training 400/553. train loss: 0.0001,	0.9290 s / batch. (data: 6.90e-04). ETA=10:53:04, max mem: 30.7 GB 
[11/25 06:21:12 visual_prompt]: 	Training 500/553. train loss: 0.7233,	0.9114 s / batch. (data: 2.69e-04). ETA=10:39:13, max mem: 30.7 GB 
[11/25 06:22:02 visual_prompt]: Epoch 24 / 100: avg data time: 2.23e-02, avg batch time: 0.9510, average train loss: 0.2457
[11/25 06:22:57 visual_prompt]: Inference (val):avg data time: 4.05e-04, avg batch time: 0.3057, average loss: 1.0592
[11/25 06:22:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 65.50	
[11/25 06:24:30 visual_prompt]: 	Test 100/162. loss: 0.717, 0.3178 s / batch. (data: 3.12e-05)max mem: 30.66532 GB 
[11/25 06:25:17 visual_prompt]: Inference (test):avg data time: 3.42e-05, avg batch time: 0.3060, average loss: 1.1891
[11/25 06:25:17 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.45	rocauc: 61.02	
[11/25 06:25:17 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/25 06:27:00 visual_prompt]: 	Training 100/553. train loss: 0.0817,	0.9240 s / batch. (data: 2.48e-04). ETA=10:45:41, max mem: 30.7 GB 
[11/25 06:28:33 visual_prompt]: 	Training 200/553. train loss: 0.2228,	0.9395 s / batch. (data: 7.31e-04). ETA=10:54:59, max mem: 30.7 GB 
[11/25 06:30:07 visual_prompt]: 	Training 300/553. train loss: 0.0504,	0.9340 s / batch. (data: 9.80e-04). ETA=10:49:32, max mem: 30.7 GB 
[11/25 06:31:41 visual_prompt]: 	Training 400/553. train loss: 0.1186,	0.9301 s / batch. (data: 3.47e-04). ETA=10:45:16, max mem: 30.7 GB 
[11/25 06:33:15 visual_prompt]: 	Training 500/553. train loss: 0.0639,	0.9465 s / batch. (data: 5.85e-03). ETA=10:55:04, max mem: 30.7 GB 
[11/25 06:34:04 visual_prompt]: Epoch 25 / 100: avg data time: 2.33e-02, avg batch time: 0.9530, average train loss: 0.1961
[11/25 06:34:59 visual_prompt]: Inference (val):avg data time: 1.22e-04, avg batch time: 0.3056, average loss: 2.8177
[11/25 06:34:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.18	
[11/25 06:36:32 visual_prompt]: 	Test 100/162. loss: 1.854, 0.3115 s / batch. (data: 5.20e-05)max mem: 30.66532 GB 
[11/25 06:37:19 visual_prompt]: Inference (test):avg data time: 3.18e-05, avg batch time: 0.3060, average loss: 1.9514
[11/25 06:37:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.76	rocauc: 62.84	
[11/25 06:37:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/25 06:39:03 visual_prompt]: 	Training 100/553. train loss: 0.2919,	0.9405 s / batch. (data: 3.95e-03). ETA=10:48:32, max mem: 30.7 GB 
[11/25 06:40:37 visual_prompt]: 	Training 200/553. train loss: 0.4927,	0.9165 s / batch. (data: 5.60e-03). ETA=10:30:30, max mem: 30.7 GB 
[11/25 06:42:10 visual_prompt]: 	Training 300/553. train loss: 0.0793,	0.9471 s / batch. (data: 2.13e-02). ETA=10:49:57, max mem: 30.7 GB 
[11/25 06:43:44 visual_prompt]: 	Training 400/553. train loss: 0.1180,	0.9164 s / batch. (data: 2.18e-04). ETA=10:27:19, max mem: 30.7 GB 
[11/25 06:45:18 visual_prompt]: 	Training 500/553. train loss: 0.0327,	0.9241 s / batch. (data: 2.62e-04). ETA=10:31:03, max mem: 30.7 GB 
[11/25 06:46:07 visual_prompt]: Epoch 26 / 100: avg data time: 2.52e-02, avg batch time: 0.9551, average train loss: 0.2213
[11/25 06:47:02 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3056, average loss: 1.6074
[11/25 06:47:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 62.11	
[11/25 06:48:36 visual_prompt]: 	Test 100/162. loss: 1.406, 0.3079 s / batch. (data: 3.36e-05)max mem: 30.66532 GB 
[11/25 06:49:22 visual_prompt]: Inference (test):avg data time: 3.18e-05, avg batch time: 0.3070, average loss: 1.5070
[11/25 06:49:22 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 64.40	
[11/25 06:49:22 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/25 06:51:08 visual_prompt]: 	Training 100/553. train loss: 0.0075,	0.9315 s / batch. (data: 1.04e-02). ETA=10:33:45, max mem: 30.7 GB 
[11/25 06:52:41 visual_prompt]: 	Training 200/553. train loss: 0.1589,	0.9371 s / batch. (data: 7.28e-04). ETA=10:35:58, max mem: 30.7 GB 
[11/25 06:54:15 visual_prompt]: 	Training 300/553. train loss: 0.1494,	0.9473 s / batch. (data: 5.36e-03). ETA=10:41:23, max mem: 30.7 GB 
[11/25 06:55:48 visual_prompt]: 	Training 400/553. train loss: 0.6905,	0.9303 s / batch. (data: 2.62e-04). ETA=10:28:17, max mem: 30.7 GB 
[11/25 06:57:22 visual_prompt]: 	Training 500/553. train loss: 0.0194,	0.9483 s / batch. (data: 2.10e-02). ETA=10:38:50, max mem: 30.7 GB 
[11/25 06:58:12 visual_prompt]: Epoch 27 / 100: avg data time: 2.90e-02, avg batch time: 0.9567, average train loss: 0.1772
[11/25 06:59:07 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3069, average loss: 1.2528
[11/25 06:59:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 63.29	
[11/25 07:00:40 visual_prompt]: 	Test 100/162. loss: 1.571, 0.3168 s / batch. (data: 4.03e-05)max mem: 30.66532 GB 
[11/25 07:01:26 visual_prompt]: Inference (test):avg data time: 5.23e-05, avg batch time: 0.3067, average loss: 1.4934
[11/25 07:01:26 visual_prompt]: Classification results with test_mammo-cbis: top1: 55.50	rocauc: 58.54	
[11/25 07:01:27 visual_prompt]: Stopping early.
visual_prompt_tuning/experiments/vit_mammo.sh: error reading input file: Stale file handle
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/13 19:17:31 visual_prompt]: Rank of current process: 0. World size: 1
[11/13 19:17:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/13 19:17:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/13 19:17:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/13 19:17:31 visual_prompt]: Training with config:
[11/13 19:17:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.005_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/13 19:17:31 visual_prompt]: Loading training data...
[11/13 19:17:31 visual_prompt]: Constructing mammo-cbis dataset train...
[11/13 19:17:31 visual_prompt]: Loading validation data...
[11/13 19:17:31 visual_prompt]: Constructing mammo-cbis dataset val...
[11/13 19:17:31 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/13 19:17:33 visual_prompt]: Enable all parameters update during training
[11/13 19:17:33 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/13 19:17:33 visual_prompt]: tuned percent:100.000
[11/13 19:17:34 visual_prompt]: Device used for model: 0
[11/13 19:17:34 visual_prompt]: Setting up Evaluator...
[11/13 19:17:34 visual_prompt]: Setting up Trainer...
[11/13 19:17:34 visual_prompt]: 	Setting up the optimizer...
[11/13 19:17:34 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/13 19:24:14 visual_prompt]: Epoch 1 / 100: avg data time: 1.08e+01, avg batch time: 11.4292, average train loss: 6.9791
[11/13 19:24:57 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1709, average loss: 6.3857
[11/13 19:24:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/13 19:24:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/13 19:31:23 visual_prompt]: Epoch 2 / 100: avg data time: 1.05e+01, avg batch time: 11.0119, average train loss: 3.9256
[11/13 19:32:06 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1745, average loss: 1.0828
[11/13 19:32:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/13 19:32:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/13 19:38:32 visual_prompt]: Epoch 3 / 100: avg data time: 1.05e+01, avg batch time: 11.0125, average train loss: 0.9834
[11/13 19:39:15 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1786, average loss: 0.7383
[11/13 19:39:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.75	
[11/13 19:39:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/13 19:45:41 visual_prompt]: Epoch 4 / 100: avg data time: 1.05e+01, avg batch time: 11.0076, average train loss: 0.8579
[11/13 19:46:24 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1736, average loss: 0.6852
[11/13 19:46:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.68	
[11/13 19:46:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/13 19:52:50 visual_prompt]: Epoch 5 / 100: avg data time: 1.05e+01, avg batch time: 11.0010, average train loss: 0.7967
[11/13 19:53:33 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1788, average loss: 0.9260
[11/13 19:53:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.99	
[11/13 19:53:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/13 19:59:59 visual_prompt]: Epoch 6 / 100: avg data time: 1.05e+01, avg batch time: 11.0203, average train loss: 0.7849
[11/13 20:00:43 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1705, average loss: 0.8692
[11/13 20:00:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.99	
[11/13 20:00:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/13 20:07:08 visual_prompt]: Epoch 7 / 100: avg data time: 1.05e+01, avg batch time: 11.0162, average train loss: 0.8302
[11/13 20:07:52 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1746, average loss: 0.7134
[11/13 20:07:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 56.09	
[11/13 20:07:52 visual_prompt]: Best epoch 7: best metric: -0.713
[11/13 20:07:52 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/13 20:14:18 visual_prompt]: Epoch 8 / 100: avg data time: 1.05e+01, avg batch time: 11.0180, average train loss: 0.7933
[11/13 20:15:01 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1764, average loss: 0.9476
[11/13 20:15:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[11/13 20:15:01 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/13 20:21:27 visual_prompt]: Epoch 9 / 100: avg data time: 1.05e+01, avg batch time: 11.0049, average train loss: 0.8801
[11/13 20:22:10 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1796, average loss: 0.7957
[11/13 20:22:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.37	
[11/13 20:22:10 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/13 20:28:35 visual_prompt]: Epoch 10 / 100: avg data time: 1.05e+01, avg batch time: 11.0085, average train loss: 0.7332
[11/13 20:29:19 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1792, average loss: 0.9599
[11/13 20:29:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.88	
[11/13 20:29:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/13 20:35:44 visual_prompt]: Epoch 11 / 100: avg data time: 1.05e+01, avg batch time: 10.9993, average train loss: 0.7308
[11/13 20:36:28 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1785, average loss: 0.6863
[11/13 20:36:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.49	
[11/13 20:36:28 visual_prompt]: Best epoch 11: best metric: -0.686
[11/13 20:36:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/13 20:42:53 visual_prompt]: Epoch 12 / 100: avg data time: 1.05e+01, avg batch time: 11.0088, average train loss: 0.7546
[11/13 20:43:37 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1685, average loss: 0.8950
[11/13 20:43:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.32	
[11/13 20:43:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/13 20:50:02 visual_prompt]: Epoch 13 / 100: avg data time: 1.05e+01, avg batch time: 11.0087, average train loss: 0.7412
[11/13 20:50:46 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1787, average loss: 0.7153
[11/13 20:50:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 54.43	
[11/13 20:50:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/13 20:57:12 visual_prompt]: Epoch 14 / 100: avg data time: 1.05e+01, avg batch time: 11.0437, average train loss: 0.7038
[11/13 20:57:56 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1774, average loss: 0.6802
[11/13 20:57:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.29	
[11/13 20:57:56 visual_prompt]: Best epoch 14: best metric: -0.680
[11/13 20:57:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/13 21:04:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.05e+01, avg batch time: 11.0248, average train loss: 0.7309
[11/13 21:05:05 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1713, average loss: 0.6958
[11/13 21:05:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.02	
[11/13 21:05:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/13 21:11:31 visual_prompt]: Epoch 16 / 100: avg data time: 1.05e+01, avg batch time: 11.0134, average train loss: 0.7284
[11/13 21:12:14 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1752, average loss: 0.7002
[11/13 21:12:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.10	
[11/13 21:12:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/13 21:18:40 visual_prompt]: Epoch 17 / 100: avg data time: 1.05e+01, avg batch time: 11.0096, average train loss: 0.7242
[11/13 21:19:23 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1782, average loss: 0.9191
[11/13 21:19:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.68	
[11/13 21:19:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/13 21:25:49 visual_prompt]: Epoch 18 / 100: avg data time: 1.05e+01, avg batch time: 11.0152, average train loss: 0.7764
[11/13 21:26:33 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1770, average loss: 0.6813
[11/13 21:26:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.21	
[11/13 21:26:33 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/13 21:32:59 visual_prompt]: Epoch 19 / 100: avg data time: 1.05e+01, avg batch time: 11.0290, average train loss: 0.6973
[11/13 21:33:42 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1753, average loss: 0.7781
[11/13 21:33:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.34	
[11/13 21:33:42 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/13 21:40:08 visual_prompt]: Epoch 20 / 100: avg data time: 1.05e+01, avg batch time: 11.0175, average train loss: 0.7387
[11/13 21:40:52 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1783, average loss: 0.7487
[11/13 21:40:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 57.24	
[11/13 21:40:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/13 21:47:18 visual_prompt]: Epoch 21 / 100: avg data time: 1.05e+01, avg batch time: 11.0324, average train loss: 0.6933
[11/13 21:48:01 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1764, average loss: 0.6942
[11/13 21:48:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.48	
[11/13 21:48:01 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/13 21:54:27 visual_prompt]: Epoch 22 / 100: avg data time: 1.05e+01, avg batch time: 11.0153, average train loss: 0.7029
[11/13 21:55:10 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1803, average loss: 0.8667
[11/13 21:55:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.37	
[11/13 21:55:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/13 22:01:36 visual_prompt]: Epoch 23 / 100: avg data time: 1.05e+01, avg batch time: 11.0030, average train loss: 0.7319
[11/13 22:02:19 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1804, average loss: 0.7806
[11/13 22:02:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.98	
[11/13 22:02:19 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/13 22:08:45 visual_prompt]: Epoch 24 / 100: avg data time: 1.05e+01, avg batch time: 11.0239, average train loss: 0.7007
[11/13 22:09:29 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1742, average loss: 0.7004
[11/13 22:09:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.17	
[11/13 22:09:29 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/13 22:15:54 visual_prompt]: Epoch 25 / 100: avg data time: 1.05e+01, avg batch time: 11.0141, average train loss: 0.6932
[11/13 22:16:38 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1771, average loss: 0.8300
[11/13 22:16:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.44	
[11/13 22:16:38 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/13 22:23:04 visual_prompt]: Epoch 26 / 100: avg data time: 1.05e+01, avg batch time: 11.0173, average train loss: 0.7216
[11/13 22:23:47 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1731, average loss: 0.6856
[11/13 22:23:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.77	
[11/13 22:23:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/13 22:30:12 visual_prompt]: Epoch 27 / 100: avg data time: 1.05e+01, avg batch time: 10.9994, average train loss: 0.7146
[11/13 22:30:56 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1763, average loss: 0.6925
[11/13 22:30:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.64	
[11/13 22:30:56 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/13 22:37:21 visual_prompt]: Epoch 28 / 100: avg data time: 1.05e+01, avg batch time: 11.0145, average train loss: 0.7048
[11/13 22:38:05 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1727, average loss: 0.7163
[11/13 22:38:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 58.61	
[11/13 22:38:05 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/13 22:44:30 visual_prompt]: Epoch 29 / 100: avg data time: 1.05e+01, avg batch time: 11.0052, average train loss: 0.7263
[11/13 22:45:14 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1745, average loss: 0.6836
[11/13 22:45:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 58.84	
[11/13 22:45:14 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/13 22:51:39 visual_prompt]: Epoch 30 / 100: avg data time: 1.05e+01, avg batch time: 10.9982, average train loss: 0.7269
[11/13 22:52:22 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1698, average loss: 0.7672
[11/13 22:52:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 57.57	
[11/13 22:52:22 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/13 22:58:47 visual_prompt]: Epoch 31 / 100: avg data time: 1.04e+01, avg batch time: 10.9915, average train loss: 0.7479
[11/13 22:59:30 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1756, average loss: 0.6959
[11/13 22:59:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.87	
[11/13 22:59:30 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/13 23:05:56 visual_prompt]: Epoch 32 / 100: avg data time: 1.05e+01, avg batch time: 11.0057, average train loss: 0.6833
[11/13 23:06:39 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1823, average loss: 0.7424
[11/13 23:06:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.15	
[11/13 23:06:39 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/13 23:13:04 visual_prompt]: Epoch 33 / 100: avg data time: 1.05e+01, avg batch time: 10.9968, average train loss: 0.7050
[11/13 23:13:48 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1780, average loss: 0.6927
[11/13 23:13:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 58.85	
[11/13 23:13:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/13 23:20:13 visual_prompt]: Epoch 34 / 100: avg data time: 1.05e+01, avg batch time: 10.9956, average train loss: 0.7155
[11/13 23:20:56 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1779, average loss: 0.6930
[11/13 23:20:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 56.31	
[11/13 23:20:56 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/13 23:27:22 visual_prompt]: Epoch 35 / 100: avg data time: 1.05e+01, avg batch time: 11.0242, average train loss: 0.6822
[11/13 23:28:05 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1766, average loss: 0.6978
[11/13 23:28:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.06	
[11/13 23:28:05 visual_prompt]: Stopping early.
[11/13 23:28:06 visual_prompt]: Rank of current process: 0. World size: 1
[11/13 23:28:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/13 23:28:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/13 23:28:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/13 23:28:06 visual_prompt]: Training with config:
[11/13 23:28:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.005_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/13 23:28:06 visual_prompt]: Loading training data...
[11/13 23:28:06 visual_prompt]: Constructing mammo-cbis dataset train...
[11/13 23:28:06 visual_prompt]: Loading validation data...
[11/13 23:28:06 visual_prompt]: Constructing mammo-cbis dataset val...
[11/13 23:28:06 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/13 23:28:08 visual_prompt]: Enable all parameters update during training
[11/13 23:28:08 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/13 23:28:08 visual_prompt]: tuned percent:100.000
[11/13 23:28:08 visual_prompt]: Device used for model: 0
[11/13 23:28:08 visual_prompt]: Setting up Evaluator...
[11/13 23:28:08 visual_prompt]: Setting up Trainer...
[11/13 23:28:08 visual_prompt]: 	Setting up the optimizer...
[11/13 23:28:08 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/13 23:34:32 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9600, average train loss: 6.9791
[11/13 23:35:16 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1760, average loss: 6.3857
[11/13 23:35:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/13 23:35:16 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/13 23:41:36 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.8739, average train loss: 3.9256
[11/13 23:42:20 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1722, average loss: 1.0828
[11/13 23:42:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/13 23:42:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/13 23:48:41 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.8841, average train loss: 0.9834
[11/13 23:49:24 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1786, average loss: 0.7383
[11/13 23:49:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.75	
[11/13 23:49:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/13 23:55:45 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.8802, average train loss: 0.8579
[11/13 23:56:29 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1758, average loss: 0.6852
[11/13 23:56:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.68	
[11/13 23:56:29 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/14 00:02:50 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.8840, average train loss: 0.7967
[11/14 00:03:33 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1778, average loss: 0.9260
[11/14 00:03:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.99	
[11/14 00:03:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/14 00:09:54 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.8896, average train loss: 0.7849
[11/14 00:10:38 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1764, average loss: 0.8692
[11/14 00:10:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.99	
[11/14 00:10:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/14 00:16:59 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.8903, average train loss: 0.8302
[11/14 00:17:42 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1795, average loss: 0.7134
[11/14 00:17:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 56.09	
[11/14 00:17:42 visual_prompt]: Best epoch 7: best metric: -0.713
[11/14 00:17:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/14 00:24:04 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.8904, average train loss: 0.7933
[11/14 00:24:47 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1766, average loss: 0.9476
[11/14 00:24:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[11/14 00:24:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/14 00:31:08 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.8841, average train loss: 0.8801
[11/14 00:31:51 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1707, average loss: 0.7957
[11/14 00:31:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.37	
[11/14 00:31:51 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/14 00:38:12 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.8750, average train loss: 0.7332
[11/14 00:38:55 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1758, average loss: 0.9599
[11/14 00:38:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.88	
[11/14 00:38:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/14 00:45:16 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.8740, average train loss: 0.7308
[11/14 00:45:59 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1758, average loss: 0.6863
[11/14 00:45:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.49	
[11/14 00:45:59 visual_prompt]: Best epoch 11: best metric: -0.686
[11/14 00:45:59 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/14 00:52:20 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.8862, average train loss: 0.7546
[11/14 00:53:04 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1804, average loss: 0.8950
[11/14 00:53:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.32	
[11/14 00:53:04 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/14 00:59:25 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.8765, average train loss: 0.7412
[11/14 01:00:08 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1802, average loss: 0.7153
[11/14 01:00:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 54.43	
[11/14 01:00:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/14 01:06:29 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.8884, average train loss: 0.7038
[11/14 01:07:13 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1778, average loss: 0.6802
[11/14 01:07:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.29	
[11/14 01:07:13 visual_prompt]: Best epoch 14: best metric: -0.680
[11/14 01:07:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/14 01:13:35 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9032, average train loss: 0.7309
[11/14 01:14:18 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1707, average loss: 0.6958
[11/14 01:14:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.02	
[11/14 01:14:18 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/14 01:20:39 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.8840, average train loss: 0.7284
[11/14 01:21:22 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1778, average loss: 0.7002
[11/14 01:21:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.10	
[11/14 01:21:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/14 01:27:44 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.9060, average train loss: 0.7242
[11/14 01:28:27 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1768, average loss: 0.9191
[11/14 01:28:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.68	
[11/14 01:28:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/14 01:34:49 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8912, average train loss: 0.7764
[11/14 01:35:32 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1773, average loss: 0.6813
[11/14 01:35:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.21	
[11/14 01:35:32 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/14 01:41:54 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9081, average train loss: 0.6973
[11/14 01:42:37 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1762, average loss: 0.7781
[11/14 01:42:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.34	
[11/14 01:42:37 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/14 01:48:58 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.8885, average train loss: 0.7387
[11/14 01:49:42 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1706, average loss: 0.7487
[11/14 01:49:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 57.24	
[11/14 01:49:42 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/14 01:56:04 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.9106, average train loss: 0.6933
[11/14 01:56:47 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1694, average loss: 0.6942
[11/14 01:56:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.48	
[11/14 01:56:47 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/14 02:03:09 visual_prompt]: Epoch 22 / 100: avg data time: 1.04e+01, avg batch time: 10.8967, average train loss: 0.7029
[11/14 02:03:52 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1748, average loss: 0.8667
[11/14 02:03:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.37	
[11/14 02:03:52 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/14 02:10:13 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 10.8726, average train loss: 0.7319
[11/14 02:10:56 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1783, average loss: 0.7806
[11/14 02:10:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.98	
[11/14 02:10:56 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/14 02:17:17 visual_prompt]: Epoch 24 / 100: avg data time: 1.04e+01, avg batch time: 10.8946, average train loss: 0.7007
[11/14 02:18:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1707, average loss: 0.7004
[11/14 02:18:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.17	
[11/14 02:18:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/14 02:24:22 visual_prompt]: Epoch 25 / 100: avg data time: 1.04e+01, avg batch time: 10.8976, average train loss: 0.6932
[11/14 02:25:05 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1792, average loss: 0.8300
[11/14 02:25:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.44	
[11/14 02:25:05 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/14 02:31:28 visual_prompt]: Epoch 26 / 100: avg data time: 1.04e+01, avg batch time: 10.9148, average train loss: 0.7216
[11/14 02:32:11 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1742, average loss: 0.6856
[11/14 02:32:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.77	
[11/14 02:32:11 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/14 02:38:32 visual_prompt]: Epoch 27 / 100: avg data time: 1.03e+01, avg batch time: 10.8870, average train loss: 0.7146
[11/14 02:39:15 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1770, average loss: 0.6925
[11/14 02:39:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.64	
[11/14 02:39:15 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/14 02:45:37 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.8902, average train loss: 0.7048
[11/14 02:46:20 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1770, average loss: 0.7163
[11/14 02:46:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 58.61	
[11/14 02:46:20 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/14 02:52:41 visual_prompt]: Epoch 29 / 100: avg data time: 1.04e+01, avg batch time: 10.8926, average train loss: 0.7263
[11/14 02:53:25 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1782, average loss: 0.6836
[11/14 02:53:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 58.84	
[11/14 02:53:25 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/14 02:59:46 visual_prompt]: Epoch 30 / 100: avg data time: 1.03e+01, avg batch time: 10.8856, average train loss: 0.7269
[11/14 03:00:29 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1797, average loss: 0.7672
[11/14 03:00:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 57.57	
[11/14 03:00:29 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/14 03:06:50 visual_prompt]: Epoch 31 / 100: avg data time: 1.03e+01, avg batch time: 10.8779, average train loss: 0.7479
[11/14 03:07:34 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1767, average loss: 0.6959
[11/14 03:07:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.87	
[11/14 03:07:34 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/14 03:13:56 visual_prompt]: Epoch 32 / 100: avg data time: 1.04e+01, avg batch time: 10.9074, average train loss: 0.6833
[11/14 03:14:39 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1779, average loss: 0.7424
[11/14 03:14:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.15	
[11/14 03:14:39 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/14 03:21:00 visual_prompt]: Epoch 33 / 100: avg data time: 1.04e+01, avg batch time: 10.8999, average train loss: 0.7050
[11/14 03:21:44 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1721, average loss: 0.6927
[11/14 03:21:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 58.85	
[11/14 03:21:44 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/14 03:28:05 visual_prompt]: Epoch 34 / 100: avg data time: 1.04e+01, avg batch time: 10.8998, average train loss: 0.7155
[11/14 03:28:49 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1726, average loss: 0.6930
[11/14 03:28:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 56.31	
[11/14 03:28:49 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/14 03:35:11 visual_prompt]: Epoch 35 / 100: avg data time: 1.04e+01, avg batch time: 10.9049, average train loss: 0.6822
[11/14 03:35:54 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1680, average loss: 0.6978
[11/14 03:35:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.06	
[11/14 03:35:54 visual_prompt]: Stopping early.
[11/14 03:35:54 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 03:35:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 03:35:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/14 03:35:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 03:35:54 visual_prompt]: Training with config:
[11/14 03:35:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.005_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 03:35:54 visual_prompt]: Loading training data...
[11/14 03:35:54 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 03:35:54 visual_prompt]: Loading validation data...
[11/14 03:35:54 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 03:35:54 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 03:35:55 visual_prompt]: Enable all parameters update during training
[11/14 03:35:55 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 03:35:55 visual_prompt]: tuned percent:100.000
[11/14 03:35:56 visual_prompt]: Device used for model: 0
[11/14 03:35:56 visual_prompt]: Setting up Evaluator...
[11/14 03:35:56 visual_prompt]: Setting up Trainer...
[11/14 03:35:56 visual_prompt]: 	Setting up the optimizer...
[11/14 03:35:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 03:42:17 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9031, average train loss: 6.9791
[11/14 03:43:01 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1722, average loss: 6.3857
[11/14 03:43:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 03:43:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/14 03:49:22 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.8740, average train loss: 3.9256
[11/14 03:50:05 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1864, average loss: 1.0828
[11/14 03:50:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/14 03:50:05 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/14 03:56:26 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.8761, average train loss: 0.9834
[11/14 03:57:09 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1783, average loss: 0.7383
[11/14 03:57:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.75	
[11/14 03:57:09 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/14 04:03:30 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.8643, average train loss: 0.8579
[11/14 04:04:13 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1788, average loss: 0.6852
[11/14 04:04:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.68	
[11/14 04:04:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/14 04:10:34 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.8600, average train loss: 0.7967
[11/14 04:11:17 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1679, average loss: 0.9260
[11/14 04:11:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.99	
[11/14 04:11:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/14 04:17:38 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.8780, average train loss: 0.7849
[11/14 04:18:21 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1773, average loss: 0.8692
[11/14 04:18:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.99	
[11/14 04:18:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/14 04:24:42 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.8876, average train loss: 0.8302
[11/14 04:25:26 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1729, average loss: 0.7134
[11/14 04:25:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 56.09	
[11/14 04:25:26 visual_prompt]: Best epoch 7: best metric: -0.713
[11/14 04:25:26 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/14 04:31:47 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.8802, average train loss: 0.7933
[11/14 04:32:30 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1771, average loss: 0.9476
[11/14 04:32:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[11/14 04:32:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/14 04:38:51 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.8629, average train loss: 0.8801
[11/14 04:39:34 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1765, average loss: 0.7957
[11/14 04:39:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.37	
[11/14 04:39:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/14 04:45:55 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.8677, average train loss: 0.7332
[11/14 04:46:38 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1720, average loss: 0.9599
[11/14 04:46:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.88	
[11/14 04:46:38 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/14 04:52:59 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.8691, average train loss: 0.7308
[11/14 04:53:42 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1706, average loss: 0.6863
[11/14 04:53:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.49	
[11/14 04:53:42 visual_prompt]: Best epoch 11: best metric: -0.686
[11/14 04:53:42 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/14 05:00:03 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.8664, average train loss: 0.7546
[11/14 05:00:46 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1783, average loss: 0.8950
[11/14 05:00:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.32	
[11/14 05:00:46 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/14 05:07:07 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.8701, average train loss: 0.7412
[11/14 05:07:50 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1744, average loss: 0.7153
[11/14 05:07:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 54.43	
[11/14 05:07:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/14 05:14:11 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.8666, average train loss: 0.7038
[11/14 05:14:54 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1682, average loss: 0.6802
[11/14 05:14:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.29	
[11/14 05:14:54 visual_prompt]: Best epoch 14: best metric: -0.680
[11/14 05:14:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/14 05:21:15 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.8840, average train loss: 0.7309
[11/14 05:21:59 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1754, average loss: 0.6958
[11/14 05:21:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.02	
[11/14 05:21:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/14 05:28:19 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.8611, average train loss: 0.7284
[11/14 05:29:02 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1783, average loss: 0.7002
[11/14 05:29:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.10	
[11/14 05:29:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/14 05:35:23 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.8780, average train loss: 0.7242
[11/14 05:36:07 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1759, average loss: 0.9191
[11/14 05:36:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.68	
[11/14 05:36:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/14 05:42:28 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.8790, average train loss: 0.7764
[11/14 05:43:11 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1718, average loss: 0.6813
[11/14 05:43:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.21	
[11/14 05:43:11 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/14 05:49:32 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.8875, average train loss: 0.6973
[11/14 05:50:16 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1679, average loss: 0.7781
[11/14 05:50:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.34	
[11/14 05:50:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/14 05:56:37 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.8827, average train loss: 0.7387
[11/14 05:57:21 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1797, average loss: 0.7487
[11/14 05:57:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 57.24	
[11/14 05:57:21 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/14 06:03:42 visual_prompt]: Epoch 21 / 100: avg data time: 1.03e+01, avg batch time: 10.8855, average train loss: 0.6933
[11/14 06:04:25 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1753, average loss: 0.6942
[11/14 06:04:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.48	
[11/14 06:04:25 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/14 06:10:46 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e+01, avg batch time: 10.8812, average train loss: 0.7029
[11/14 06:11:29 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1722, average loss: 0.8667
[11/14 06:11:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.37	
[11/14 06:11:29 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/14 06:17:50 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 10.8733, average train loss: 0.7319
[11/14 06:18:34 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1706, average loss: 0.7806
[11/14 06:18:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.98	
[11/14 06:18:34 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/14 06:24:55 visual_prompt]: Epoch 24 / 100: avg data time: 1.04e+01, avg batch time: 10.8966, average train loss: 0.7007
[11/14 06:25:39 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1771, average loss: 0.7004
[11/14 06:25:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.17	
[11/14 06:25:39 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/14 06:32:00 visual_prompt]: Epoch 25 / 100: avg data time: 1.03e+01, avg batch time: 10.8741, average train loss: 0.6932
[11/14 06:32:43 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1757, average loss: 0.8300
[11/14 06:32:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.44	
[11/14 06:32:43 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/14 06:39:04 visual_prompt]: Epoch 26 / 100: avg data time: 1.04e+01, avg batch time: 10.8837, average train loss: 0.7216
[11/14 06:39:48 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1744, average loss: 0.6856
[11/14 06:39:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.77	
[11/14 06:39:48 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/14 06:46:08 visual_prompt]: Epoch 27 / 100: avg data time: 1.03e+01, avg batch time: 10.8567, average train loss: 0.7146
[11/14 06:46:51 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1744, average loss: 0.6925
[11/14 06:46:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.64	
[11/14 06:46:51 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/14 06:53:12 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.8718, average train loss: 0.7048
[11/14 06:53:55 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1719, average loss: 0.7163
[11/14 06:53:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 58.61	
[11/14 06:53:55 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/14 07:00:14 visual_prompt]: Epoch 29 / 100: avg data time: 1.03e+01, avg batch time: 10.8275, average train loss: 0.7263
[11/14 07:00:58 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1701, average loss: 0.6836
[11/14 07:00:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 58.84	
[11/14 07:00:58 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/14 07:07:18 visual_prompt]: Epoch 30 / 100: avg data time: 1.03e+01, avg batch time: 10.8681, average train loss: 0.7269
[11/14 07:08:02 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1736, average loss: 0.7672
[11/14 07:08:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 57.57	
[11/14 07:08:02 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/14 07:14:22 visual_prompt]: Epoch 31 / 100: avg data time: 1.03e+01, avg batch time: 10.8594, average train loss: 0.7479
[11/14 07:15:05 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1821, average loss: 0.6959
[11/14 07:15:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.87	
[11/14 07:15:05 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/14 07:21:27 visual_prompt]: Epoch 32 / 100: avg data time: 1.04e+01, avg batch time: 10.8893, average train loss: 0.6833
[11/14 07:22:10 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1704, average loss: 0.7424
[11/14 07:22:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.15	
[11/14 07:22:10 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/14 07:28:32 visual_prompt]: Epoch 33 / 100: avg data time: 1.03e+01, avg batch time: 10.8910, average train loss: 0.7050
[11/14 07:29:15 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1713, average loss: 0.6927
[11/14 07:29:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 58.85	
[11/14 07:29:15 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/14 07:35:35 visual_prompt]: Epoch 34 / 100: avg data time: 1.03e+01, avg batch time: 10.8549, average train loss: 0.7155
[11/14 07:36:19 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1785, average loss: 0.6930
[11/14 07:36:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 56.31	
[11/14 07:36:19 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/14 07:42:40 visual_prompt]: Epoch 35 / 100: avg data time: 1.03e+01, avg batch time: 10.8822, average train loss: 0.6822
[11/14 07:43:23 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1763, average loss: 0.6978
[11/14 07:43:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.06	
[11/14 07:43:23 visual_prompt]: Stopping early.
[11/14 07:43:23 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 07:43:23 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 07:43:23 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/14 07:43:23 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 07:43:23 visual_prompt]: Training with config:
[11/14 07:43:23 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.005_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 07:43:23 visual_prompt]: Loading training data...
[11/14 07:43:23 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 07:43:23 visual_prompt]: Loading validation data...
[11/14 07:43:23 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 07:43:23 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 07:43:25 visual_prompt]: Enable all parameters update during training
[11/14 07:43:25 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 07:43:25 visual_prompt]: tuned percent:100.000
[11/14 07:43:25 visual_prompt]: Device used for model: 0
[11/14 07:43:25 visual_prompt]: Setting up Evaluator...
[11/14 07:43:25 visual_prompt]: Setting up Trainer...
[11/14 07:43:25 visual_prompt]: 	Setting up the optimizer...
[11/14 07:43:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 07:49:46 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.8799, average train loss: 6.9791
[11/14 07:50:29 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1716, average loss: 6.3857
[11/14 07:50:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 07:50:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/14 07:56:50 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.8845, average train loss: 7.1642
[11/14 07:57:34 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1708, average loss: 1.1776
[11/14 07:57:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 55.47	
[11/14 07:57:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/14 08:03:55 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.8798, average train loss: 3.3523
[11/14 08:04:38 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1751, average loss: 1.1512
[11/14 08:04:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.33	
[11/14 08:04:38 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/14 08:10:59 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.8729, average train loss: 1.1312
[11/14 08:11:43 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1741, average loss: 1.3375
[11/14 08:11:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.19	
[11/14 08:11:43 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/14 08:18:03 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.8711, average train loss: 2.5304
[11/14 08:18:47 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1680, average loss: 0.7134
[11/14 08:18:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 54.01	
[11/14 08:18:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/14 08:25:08 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.8940, average train loss: 2.5519
[11/14 08:25:52 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1757, average loss: 3.9798
[11/14 08:25:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.87	
[11/14 08:25:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/14 08:32:12 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.8726, average train loss: 1.8713
[11/14 08:32:56 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1761, average loss: 1.1052
[11/14 08:32:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.46	
[11/14 08:32:56 visual_prompt]: Best epoch 7: best metric: -1.105
[11/14 08:32:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/14 08:39:17 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.8815, average train loss: 1.1114
[11/14 08:40:01 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1747, average loss: 0.7703
[11/14 08:40:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 54.86	
[11/14 08:40:01 visual_prompt]: Best epoch 8: best metric: -0.770
[11/14 08:40:01 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/14 08:46:21 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.8740, average train loss: 0.8901
[11/14 08:47:05 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1766, average loss: 2.3425
[11/14 08:47:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.45	
[11/14 08:47:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/14 08:53:26 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.8925, average train loss: 1.5516
[11/14 08:54:10 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1706, average loss: 0.9545
[11/14 08:54:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/14 08:54:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/14 09:00:30 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.8706, average train loss: 3.1054
[11/14 09:01:14 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1772, average loss: 1.3142
[11/14 09:01:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.80	
[11/14 09:01:14 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/14 09:07:35 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.8819, average train loss: 1.0938
[11/14 09:08:18 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1701, average loss: 1.0759
[11/14 09:08:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.48	
[11/14 09:08:18 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/14 09:14:39 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.8745, average train loss: 0.8543
[11/14 09:15:22 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1738, average loss: 0.9467
[11/14 09:15:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[11/14 09:15:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/14 09:21:43 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.8790, average train loss: 0.9961
[11/14 09:22:27 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1750, average loss: 0.9333
[11/14 09:22:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.33	
[11/14 09:22:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/14 09:28:48 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.8890, average train loss: 0.9825
[11/14 09:29:32 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1707, average loss: 0.8608
[11/14 09:29:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.63	
[11/14 09:29:32 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/14 09:35:52 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.8738, average train loss: 0.7708
[11/14 09:36:36 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1774, average loss: 0.6881
[11/14 09:36:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 54.88	
[11/14 09:36:36 visual_prompt]: Best epoch 16: best metric: -0.688
[11/14 09:36:36 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/14 09:42:57 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.8744, average train loss: 0.8546
[11/14 09:43:40 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1757, average loss: 1.0498
[11/14 09:43:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.29	
[11/14 09:43:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/14 09:50:01 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8834, average train loss: 1.2939
[11/14 09:50:45 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1777, average loss: 0.7597
[11/14 09:50:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 54.71	
[11/14 09:50:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/14 09:57:06 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9024, average train loss: 0.7812
[11/14 09:57:50 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1725, average loss: 1.2632
[11/14 09:57:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.13	
[11/14 09:57:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/14 10:04:11 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.8745, average train loss: 1.1966
[11/14 10:04:54 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1746, average loss: 1.1624
[11/14 10:04:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.46	
[11/14 10:04:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/14 10:11:15 visual_prompt]: Epoch 21 / 100: avg data time: 1.03e+01, avg batch time: 10.8821, average train loss: 0.9163
[11/14 10:11:59 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1718, average loss: 0.6865
[11/14 10:11:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 54.50	
[11/14 10:11:59 visual_prompt]: Best epoch 21: best metric: -0.687
[11/14 10:11:59 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/14 10:18:20 visual_prompt]: Epoch 22 / 100: avg data time: 1.04e+01, avg batch time: 10.8855, average train loss: 0.7402
[11/14 10:19:04 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1752, average loss: 0.9014
[11/14 10:19:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.13	
[11/14 10:19:04 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/14 10:25:25 visual_prompt]: Epoch 23 / 100: avg data time: 1.04e+01, avg batch time: 10.8858, average train loss: 0.7794
[11/14 10:26:08 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1746, average loss: 0.8975
[11/14 10:26:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.01	
[11/14 10:26:08 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/14 10:32:29 visual_prompt]: Epoch 24 / 100: avg data time: 1.04e+01, avg batch time: 10.8743, average train loss: 0.7435
[11/14 10:33:13 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1732, average loss: 0.7279
[11/14 10:33:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.21	
[11/14 10:33:13 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/14 10:39:34 visual_prompt]: Epoch 25 / 100: avg data time: 1.03e+01, avg batch time: 10.8790, average train loss: 0.8491
[11/14 10:40:17 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1795, average loss: 0.7528
[11/14 10:40:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.75	
[11/14 10:40:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/14 10:46:38 visual_prompt]: Epoch 26 / 100: avg data time: 1.04e+01, avg batch time: 10.8884, average train loss: 0.7604
[11/14 10:47:22 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1736, average loss: 0.7216
[11/14 10:47:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.07	
[11/14 10:47:22 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/14 10:53:43 visual_prompt]: Epoch 27 / 100: avg data time: 1.04e+01, avg batch time: 10.8762, average train loss: 0.8739
[11/14 10:54:26 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1710, average loss: 0.9465
[11/14 10:54:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.28	
[11/14 10:54:26 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/14 11:00:47 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.8732, average train loss: 0.8653
[11/14 11:01:30 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1759, average loss: 0.7812
[11/14 11:01:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.08	
[11/14 11:01:30 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/14 11:08:26 visual_prompt]: Epoch 29 / 100: avg data time: 1.13e+01, avg batch time: 11.8697, average train loss: 0.7649
[11/14 11:09:34 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1795, average loss: 1.1866
[11/14 11:09:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/14 11:09:34 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/14 11:16:05 visual_prompt]: Epoch 30 / 100: avg data time: 1.06e+01, avg batch time: 11.1567, average train loss: 0.7701
[11/14 11:16:48 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1704, average loss: 1.0235
[11/14 11:16:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.47	
[11/14 11:16:48 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/14 11:23:10 visual_prompt]: Epoch 31 / 100: avg data time: 1.04e+01, avg batch time: 10.9019, average train loss: 0.8795
[11/14 11:23:53 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1688, average loss: 0.9722
[11/14 11:23:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.95	
[11/14 11:23:53 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/14 11:30:17 visual_prompt]: Epoch 32 / 100: avg data time: 1.04e+01, avg batch time: 10.9588, average train loss: 0.7626
[11/14 11:31:01 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1730, average loss: 0.7604
[11/14 11:31:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.10	
[11/14 11:31:01 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/14 11:37:22 visual_prompt]: Epoch 33 / 100: avg data time: 1.04e+01, avg batch time: 10.8862, average train loss: 0.8340
[11/14 11:38:06 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1798, average loss: 0.8713
[11/14 11:38:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[11/14 11:38:06 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/14 11:44:27 visual_prompt]: Epoch 34 / 100: avg data time: 1.03e+01, avg batch time: 10.8715, average train loss: 0.8578
[11/14 11:45:11 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1706, average loss: 0.7660
[11/14 11:45:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.65	
[11/14 11:45:11 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/14 11:51:36 visual_prompt]: Epoch 35 / 100: avg data time: 1.05e+01, avg batch time: 10.9922, average train loss: 0.9168
[11/14 11:52:20 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1725, average loss: 0.9473
[11/14 11:52:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.49	
[11/14 11:52:20 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.003867370395306068
[11/14 11:58:41 visual_prompt]: Epoch 36 / 100: avg data time: 1.04e+01, avg batch time: 10.9067, average train loss: 0.7977
[11/14 11:59:25 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1758, average loss: 0.7602
[11/14 11:59:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.96	
[11/14 11:59:25 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.0037974239344530382
[11/14 12:05:46 visual_prompt]: Epoch 37 / 100: avg data time: 1.03e+01, avg batch time: 10.8727, average train loss: 0.7534
[11/14 12:06:29 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1708, average loss: 0.7273
[11/14 12:06:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 54.96	
[11/14 12:06:29 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.0037260587595762708
[11/14 12:12:51 visual_prompt]: Epoch 38 / 100: avg data time: 1.04e+01, avg batch time: 10.9050, average train loss: 0.7575
[11/14 12:13:35 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1776, average loss: 0.9907
[11/14 12:13:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.02	
[11/14 12:13:35 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.0036533529074467197
[11/14 12:19:55 visual_prompt]: Epoch 39 / 100: avg data time: 1.03e+01, avg batch time: 10.8707, average train loss: 0.7893
[11/14 12:20:39 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1754, average loss: 0.7218
[11/14 12:20:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 55.27	
[11/14 12:20:39 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.003579385880846232
[11/14 12:27:01 visual_prompt]: Epoch 40 / 100: avg data time: 1.04e+01, avg batch time: 10.9116, average train loss: 0.7238
[11/14 12:27:44 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1723, average loss: 0.7052
[11/14 12:27:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 55.21	
[11/14 12:27:44 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.003504238561632424
[11/14 12:34:06 visual_prompt]: Epoch 41 / 100: avg data time: 1.04e+01, avg batch time: 10.9008, average train loss: 0.7214
[11/14 12:34:50 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1707, average loss: 0.6842
[11/14 12:34:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 55.03	
[11/14 12:34:50 visual_prompt]: Best epoch 41: best metric: -0.684
[11/14 12:34:50 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.003427993122295552
[11/14 12:41:10 visual_prompt]: Epoch 42 / 100: avg data time: 1.03e+01, avg batch time: 10.8673, average train loss: 0.7188
[11/14 12:41:54 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1810, average loss: 0.6845
[11/14 12:41:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 54.94	
[11/14 12:41:54 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.003350732936104108
[11/14 12:48:15 visual_prompt]: Epoch 43 / 100: avg data time: 1.04e+01, avg batch time: 10.8879, average train loss: 0.7068
[11/14 12:48:59 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1773, average loss: 0.6841
[11/14 12:48:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 55.17	
[11/14 12:48:59 visual_prompt]: Best epoch 43: best metric: -0.684
[11/14 12:48:59 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.0032725424859373687
[11/14 12:55:20 visual_prompt]: Epoch 44 / 100: avg data time: 1.04e+01, avg batch time: 10.8819, average train loss: 0.8180
[11/14 12:56:03 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1791, average loss: 1.1056
[11/14 12:56:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.08	
[11/14 12:56:03 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0031935072719046116
[11/14 13:02:24 visual_prompt]: Epoch 45 / 100: avg data time: 1.03e+01, avg batch time: 10.8709, average train loss: 0.7676
[11/14 13:03:07 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1689, average loss: 0.9259
[11/14 13:03:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.37	
[11/14 13:03:07 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.003113713717851998
[11/14 13:09:29 visual_prompt]: Epoch 46 / 100: avg data time: 1.04e+01, avg batch time: 10.8833, average train loss: 0.8324
[11/14 13:10:12 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1776, average loss: 0.7013
[11/14 13:10:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 55.15	
[11/14 13:10:12 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.0030332490768593674
[11/14 13:16:33 visual_prompt]: Epoch 47 / 100: avg data time: 1.03e+01, avg batch time: 10.8769, average train loss: 0.7614
[11/14 13:17:16 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1775, average loss: 0.9900
[11/14 13:17:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.11	
[11/14 13:17:16 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.0029522013358302753
[11/14 13:23:38 visual_prompt]: Epoch 48 / 100: avg data time: 1.03e+01, avg batch time: 10.8830, average train loss: 0.7706
[11/14 13:24:21 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1776, average loss: 0.8747
[11/14 13:24:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.93	
[11/14 13:24:21 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.002870659119279605
[11/14 13:30:42 visual_prompt]: Epoch 49 / 100: avg data time: 1.03e+01, avg batch time: 10.8745, average train loss: 0.7146
[11/14 13:31:25 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1734, average loss: 0.7029
[11/14 13:31:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.21	
[11/14 13:31:25 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.002788711592423966
[11/14 13:37:46 visual_prompt]: Epoch 50 / 100: avg data time: 1.03e+01, avg batch time: 10.8707, average train loss: 0.7365
[11/14 13:38:30 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1739, average loss: 0.7234
[11/14 13:38:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.02	
[11/14 13:38:30 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.0027064483636808313
[11/14 13:44:54 visual_prompt]: Epoch 51 / 100: avg data time: 1.04e+01, avg batch time: 10.9714, average train loss: 0.7003
[11/14 13:45:37 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1745, average loss: 0.9142
[11/14 13:45:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.88	
[11/14 13:45:37 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.0026239593866830557
[11/14 13:51:56 visual_prompt]: Epoch 52 / 100: avg data time: 1.03e+01, avg batch time: 10.8121, average train loss: 0.7425
[11/14 13:52:39 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1776, average loss: 0.7150
[11/14 13:52:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 54.84	
[11/14 13:52:39 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.0025413348619158964
[11/14 13:59:00 visual_prompt]: Epoch 53 / 100: avg data time: 1.03e+01, avg batch time: 10.8783, average train loss: 0.8263
[11/14 13:59:44 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1749, average loss: 0.6895
[11/14 13:59:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 55.31	
[11/14 13:59:44 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.0024586651380841037
[11/14 14:06:05 visual_prompt]: Epoch 54 / 100: avg data time: 1.04e+01, avg batch time: 10.8826, average train loss: 0.7105
[11/14 14:06:48 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1707, average loss: 0.9331
[11/14 14:06:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.36	
[11/14 14:06:48 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.002376040613316944
[11/14 14:13:09 visual_prompt]: Epoch 55 / 100: avg data time: 1.03e+01, avg batch time: 10.8659, average train loss: 0.7447
[11/14 14:13:52 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1769, average loss: 0.8663
[11/14 14:13:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.08	
[11/14 14:13:52 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.0022935516363191692
[11/14 14:20:12 visual_prompt]: Epoch 56 / 100: avg data time: 1.03e+01, avg batch time: 10.8598, average train loss: 0.6908
[11/14 14:20:55 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1710, average loss: 0.6830
[11/14 14:20:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.38	
[11/14 14:20:55 visual_prompt]: Best epoch 56: best metric: -0.683
[11/14 14:20:55 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.0022112884075760346
[11/14 14:27:30 visual_prompt]: Epoch 57 / 100: avg data time: 1.07e+01, avg batch time: 11.2657, average train loss: 0.7064
[11/14 14:28:17 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1787, average loss: 0.6823
[11/14 14:28:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 55.62	
[11/14 14:28:17 visual_prompt]: Best epoch 57: best metric: -0.682
[11/14 14:28:17 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.002129340880720395
[11/14 14:35:04 visual_prompt]: Epoch 58 / 100: avg data time: 1.11e+01, avg batch time: 11.6038, average train loss: 0.6831
[11/14 14:35:50 visual_prompt]: Inference (val):avg data time: 4.89e-05, avg batch time: 0.1791, average loss: 0.6978
[11/14 14:35:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 55.43	
[11/14 14:35:50 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.002047798664169726
[11/14 14:42:18 visual_prompt]: Epoch 59 / 100: avg data time: 1.05e+01, avg batch time: 11.0846, average train loss: 0.6909
[11/14 14:43:02 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1733, average loss: 0.7878
[11/14 14:43:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.39	
[11/14 14:43:02 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.001966750923140633
[11/14 14:49:32 visual_prompt]: Epoch 60 / 100: avg data time: 1.06e+01, avg batch time: 11.1199, average train loss: 0.7733
[11/14 14:50:16 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1727, average loss: 0.6919
[11/14 14:50:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 55.52	
[11/14 14:50:16 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.0018862862821480023
[11/14 14:56:46 visual_prompt]: Epoch 61 / 100: avg data time: 1.06e+01, avg batch time: 11.1260, average train loss: 0.6957
[11/14 14:57:30 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1800, average loss: 0.7118
[11/14 14:57:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 55.20	
[11/14 14:57:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0018064927280953891
[11/14 15:04:08 visual_prompt]: Epoch 62 / 100: avg data time: 1.08e+01, avg batch time: 11.3490, average train loss: 0.7363
[11/14 15:04:54 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1723, average loss: 0.7867
[11/14 15:04:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.51	
[11/14 15:04:54 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.0017274575140626316
[11/14 15:11:34 visual_prompt]: Epoch 63 / 100: avg data time: 1.09e+01, avg batch time: 11.4258, average train loss: 0.6829
[11/14 15:12:19 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1803, average loss: 0.6932
[11/14 15:12:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.69	
[11/14 15:12:19 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.0016492670638958923
[11/14 15:18:54 visual_prompt]: Epoch 64 / 100: avg data time: 1.07e+01, avg batch time: 11.2772, average train loss: 0.6930
[11/14 15:19:39 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1744, average loss: 0.7262
[11/14 15:19:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.33	
[11/14 15:19:39 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.0015720068777044477
[11/14 15:26:17 visual_prompt]: Epoch 65 / 100: avg data time: 1.08e+01, avg batch time: 11.3426, average train loss: 0.6835
[11/14 15:27:02 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1739, average loss: 0.6833
[11/14 15:27:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.20	
[11/14 15:27:02 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.0014957614383675768
[11/14 15:33:40 visual_prompt]: Epoch 66 / 100: avg data time: 1.08e+01, avg batch time: 11.3491, average train loss: 0.7066
[11/14 15:34:25 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1786, average loss: 0.7193
[11/14 15:34:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.11	
[11/14 15:34:25 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.001420614119153768
[11/14 15:41:00 visual_prompt]: Epoch 67 / 100: avg data time: 1.08e+01, avg batch time: 11.2944, average train loss: 0.6865
[11/14 15:41:46 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1798, average loss: 0.7417
[11/14 15:41:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 55.88	
[11/14 15:41:46 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.0013466470925532809
[11/14 15:48:20 visual_prompt]: Epoch 68 / 100: avg data time: 1.07e+01, avg batch time: 11.2537, average train loss: 0.7082
[11/14 15:49:03 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1735, average loss: 0.6860
[11/14 15:49:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 55.78	
[11/14 15:49:03 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.0012739412404237304
[11/14 15:55:18 visual_prompt]: Epoch 69 / 100: avg data time: 1.02e+01, avg batch time: 10.7157, average train loss: 0.6898
[11/14 15:56:01 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1743, average loss: 0.7135
[11/14 15:56:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.88	
[11/14 15:56:01 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.0012025760655469627
[11/14 16:02:16 visual_prompt]: Epoch 70 / 100: avg data time: 1.02e+01, avg batch time: 10.7235, average train loss: 0.6984
[11/14 16:02:59 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1798, average loss: 0.6800
[11/14 16:02:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.06	
[11/14 16:02:59 visual_prompt]: Best epoch 70: best metric: -0.680
[11/14 16:02:59 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.0011326296046939332
[11/14 16:09:15 visual_prompt]: Epoch 71 / 100: avg data time: 1.02e+01, avg batch time: 10.7354, average train loss: 0.6992
[11/14 16:09:58 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1774, average loss: 0.6963
[11/14 16:09:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 56.30	
[11/14 16:09:58 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.001064178343292641
[11/14 16:16:14 visual_prompt]: Epoch 72 / 100: avg data time: 1.02e+01, avg batch time: 10.7271, average train loss: 0.6913
[11/14 16:16:56 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1809, average loss: 0.6828
[11/14 16:16:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.10	
[11/14 16:16:56 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.0009972971317924373
[11/14 16:23:12 visual_prompt]: Epoch 73 / 100: avg data time: 1.02e+01, avg batch time: 10.7303, average train loss: 0.6883
[11/14 16:23:55 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1719, average loss: 0.6934
[11/14 16:23:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 56.20	
[11/14 16:23:55 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.0009320591038161574
[11/14 16:30:10 visual_prompt]: Epoch 74 / 100: avg data time: 1.02e+01, avg batch time: 10.7150, average train loss: 0.6912
[11/14 16:30:53 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1759, average loss: 0.7324
[11/14 16:30:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.21	
[11/14 16:30:53 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.0008685355961895783
[11/14 16:37:17 visual_prompt]: Epoch 75 / 100: avg data time: 1.04e+01, avg batch time: 10.9556, average train loss: 0.6893
[11/14 16:38:07 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1759, average loss: 0.6816
[11/14 16:38:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.24	
[11/14 16:38:07 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.0008067960709356478
[11/14 16:44:38 visual_prompt]: Epoch 76 / 100: avg data time: 1.06e+01, avg batch time: 11.1628, average train loss: 0.6799
[11/14 16:45:21 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1804, average loss: 0.6809
[11/14 16:45:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 56.18	
[11/14 16:45:21 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.0007469080393187785
[11/14 16:51:41 visual_prompt]: Epoch 77 / 100: avg data time: 1.03e+01, avg batch time: 10.8586, average train loss: 0.6791
[11/14 16:52:25 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1773, average loss: 0.6853
[11/14 16:52:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 56.15	
[11/14 16:52:25 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.0006889369880222776
[11/14 16:58:46 visual_prompt]: Epoch 78 / 100: avg data time: 1.03e+01, avg batch time: 10.8871, average train loss: 0.6911
[11/14 16:59:30 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1718, average loss: 0.6816
[11/14 16:59:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 56.27	
[11/14 16:59:30 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0006329463075396161
[11/14 17:05:51 visual_prompt]: Epoch 79 / 100: avg data time: 1.03e+01, avg batch time: 10.8869, average train loss: 0.7060
[11/14 17:06:34 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1842, average loss: 0.6899
[11/14 17:06:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.39	
[11/14 17:06:34 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.000578997222857853
[11/14 17:12:55 visual_prompt]: Epoch 80 / 100: avg data time: 1.03e+01, avg batch time: 10.8794, average train loss: 0.6760
[11/14 17:13:39 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1766, average loss: 0.6888
[11/14 17:13:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.40	
[11/14 17:13:39 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.0005271487265090163
[11/14 17:19:59 visual_prompt]: Epoch 81 / 100: avg data time: 1.03e+01, avg batch time: 10.8649, average train loss: 0.6839
[11/14 17:20:43 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1776, average loss: 0.7101
[11/14 17:20:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 56.42	
[11/14 17:20:43 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.00047745751406263163
[11/14 17:27:04 visual_prompt]: Epoch 82 / 100: avg data time: 1.03e+01, avg batch time: 10.8788, average train loss: 0.6797
[11/14 17:27:47 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1824, average loss: 0.6863
[11/14 17:27:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.41	
[11/14 17:27:47 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.0004299779221299499
[11/14 17:34:23 visual_prompt]: Epoch 83 / 100: avg data time: 1.08e+01, avg batch time: 11.3153, average train loss: 0.6769
[11/14 17:35:10 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1783, average loss: 0.6853
[11/14 17:35:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 56.53	
[11/14 17:35:10 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.0003847618689476612
[11/14 17:41:54 visual_prompt]: Epoch 84 / 100: avg data time: 1.10e+01, avg batch time: 11.5375, average train loss: 0.6828
[11/14 17:42:40 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1774, average loss: 0.6881
[11/14 17:42:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.34	
[11/14 17:42:40 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.00034185879760606524
[11/14 17:49:23 visual_prompt]: Epoch 85 / 100: avg data time: 1.09e+01, avg batch time: 11.4901, average train loss: 0.6775
[11/14 17:50:09 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1844, average loss: 0.6848
[11/14 17:50:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 56.43	
[11/14 17:50:09 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0003013156219837776
[11/14 17:56:52 visual_prompt]: Epoch 86 / 100: avg data time: 1.10e+01, avg batch time: 11.5038, average train loss: 0.6804
[11/14 17:57:37 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1783, average loss: 0.6806
[11/14 17:57:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.34	
[11/14 17:57:37 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.00026317667544809134
[11/14 18:04:20 visual_prompt]: Epoch 87 / 100: avg data time: 1.10e+01, avg batch time: 11.4951, average train loss: 0.6768
[11/14 18:05:06 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1798, average loss: 0.6827
[11/14 18:05:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.31	
[11/14 18:05:06 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00022748366237709372
[11/14 18:11:48 visual_prompt]: Epoch 88 / 100: avg data time: 1.10e+01, avg batch time: 11.5058, average train loss: 0.6755
[11/14 18:12:35 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1811, average loss: 0.6923
[11/14 18:12:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 56.32	
[11/14 18:12:35 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.00019427561255653815
[11/14 18:19:18 visual_prompt]: Epoch 89 / 100: avg data time: 1.10e+01, avg batch time: 11.5034, average train loss: 0.6742
[11/14 18:20:00 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1723, average loss: 0.6830
[11/14 18:20:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.24	
[11/14 18:20:00 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.00016358883850134815
[11/14 18:26:23 visual_prompt]: Epoch 90 / 100: avg data time: 1.04e+01, avg batch time: 10.9279, average train loss: 0.6759
[11/14 18:27:06 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1758, average loss: 0.6807
[11/14 18:27:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.29	
[11/14 18:27:06 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0001354568957484134
[11/14 18:33:32 visual_prompt]: Epoch 91 / 100: avg data time: 1.05e+01, avg batch time: 11.0245, average train loss: 0.6729
[11/14 18:34:15 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1703, average loss: 0.6814
[11/14 18:34:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.24	
[11/14 18:34:15 visual_prompt]: Stopping early.
[11/14 18:34:16 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 18:34:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 18:34:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/14 18:34:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 18:34:16 visual_prompt]: Training with config:
[11/14 18:34:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 18:34:16 visual_prompt]: Loading training data...
[11/14 18:34:16 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 18:34:16 visual_prompt]: Loading validation data...
[11/14 18:34:16 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 18:34:16 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 18:34:18 visual_prompt]: Enable all parameters update during training
[11/14 18:34:18 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 18:34:18 visual_prompt]: tuned percent:100.000
[11/14 18:34:18 visual_prompt]: Device used for model: 0
[11/14 18:34:18 visual_prompt]: Setting up Evaluator...
[11/14 18:34:18 visual_prompt]: Setting up Trainer...
[11/14 18:34:18 visual_prompt]: 	Setting up the optimizer...
[11/14 18:34:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 18:40:51 visual_prompt]: Epoch 1 / 100: avg data time: 1.07e+01, avg batch time: 11.2476, average train loss: 6.9791
[11/14 18:41:35 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1774, average loss: 6.3857
[11/14 18:41:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 18:41:35 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/14 18:47:58 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.9468, average train loss: 2.9795
[11/14 18:48:42 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1753, average loss: 1.0191
[11/14 18:48:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[11/14 18:48:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/14 18:56:37 visual_prompt]: Epoch 3 / 100: avg data time: 1.30e+01, avg batch time: 13.5491, average train loss: 0.9363
[11/14 18:57:40 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1689, average loss: 0.7546
[11/14 18:57:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.68	
[11/14 18:57:40 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/14 19:05:52 visual_prompt]: Epoch 4 / 100: avg data time: 1.35e+01, avg batch time: 14.0356, average train loss: 0.8156
[11/14 19:06:55 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1765, average loss: 0.6800
[11/14 19:06:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.91	
[11/14 19:06:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/14 19:15:05 visual_prompt]: Epoch 5 / 100: avg data time: 1.35e+01, avg batch time: 13.9881, average train loss: 0.7900
[11/14 19:16:03 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1819, average loss: 0.6689
[11/14 19:16:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.97	
[11/14 19:16:03 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/14 19:24:15 visual_prompt]: Epoch 6 / 100: avg data time: 1.35e+01, avg batch time: 14.0330, average train loss: 0.8251
[11/14 19:25:19 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1776, average loss: 0.7127
[11/14 19:25:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.79	
[11/14 19:25:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/14 19:33:20 visual_prompt]: Epoch 7 / 100: avg data time: 1.32e+01, avg batch time: 13.7503, average train loss: 0.7236
[11/14 19:34:22 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1681, average loss: 0.6896
[11/14 19:34:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.59	
[11/14 19:34:22 visual_prompt]: Best epoch 7: best metric: -0.690
[11/14 19:34:22 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/14 19:42:15 visual_prompt]: Epoch 8 / 100: avg data time: 1.30e+01, avg batch time: 13.5081, average train loss: 0.7060
[11/14 19:43:27 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.1769, average loss: 0.6624
[11/14 19:43:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.49	
[11/14 19:43:27 visual_prompt]: Best epoch 8: best metric: -0.662
[11/14 19:43:27 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/14 19:51:08 visual_prompt]: Epoch 9 / 100: avg data time: 1.26e+01, avg batch time: 13.1833, average train loss: 0.7656
[11/14 19:52:16 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.1692, average loss: 0.6902
[11/14 19:52:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 64.84	
[11/14 19:52:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/14 19:59:56 visual_prompt]: Epoch 10 / 100: avg data time: 1.26e+01, avg batch time: 13.1394, average train loss: 0.6893
[11/14 20:01:06 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1725, average loss: 0.7023
[11/14 20:01:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 67.18	
[11/14 20:01:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/14 20:09:01 visual_prompt]: Epoch 11 / 100: avg data time: 1.30e+01, avg batch time: 13.5609, average train loss: 0.6646
[11/14 20:10:01 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1701, average loss: 0.6843
[11/14 20:10:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.84	
[11/14 20:10:01 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/14 20:17:25 visual_prompt]: Epoch 12 / 100: avg data time: 1.21e+01, avg batch time: 12.6850, average train loss: 0.7185
[11/14 20:18:12 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1755, average loss: 0.7410
[11/14 20:18:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.83	
[11/14 20:18:12 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/14 20:24:44 visual_prompt]: Epoch 13 / 100: avg data time: 1.07e+01, avg batch time: 11.2130, average train loss: 0.6984
[11/14 20:25:28 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1792, average loss: 0.6548
[11/14 20:25:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.62	
[11/14 20:25:28 visual_prompt]: Best epoch 13: best metric: -0.655
[11/14 20:25:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/14 20:32:09 visual_prompt]: Epoch 14 / 100: avg data time: 1.09e+01, avg batch time: 11.4362, average train loss: 0.6217
[11/14 20:32:54 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1805, average loss: 0.6592
[11/14 20:32:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.11	
[11/14 20:32:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/14 20:39:28 visual_prompt]: Epoch 15 / 100: avg data time: 1.07e+01, avg batch time: 11.2585, average train loss: 0.6511
[11/14 20:40:13 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1774, average loss: 0.7042
[11/14 20:40:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.20	
[11/14 20:40:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/14 20:46:49 visual_prompt]: Epoch 16 / 100: avg data time: 1.07e+01, avg batch time: 11.2891, average train loss: 0.5915
[11/14 20:47:34 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1738, average loss: 0.6971
[11/14 20:47:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 67.61	
[11/14 20:47:34 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/14 20:54:09 visual_prompt]: Epoch 17 / 100: avg data time: 1.07e+01, avg batch time: 11.2796, average train loss: 0.6150
[11/14 20:54:54 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1764, average loss: 0.7181
[11/14 20:54:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.96	
[11/14 20:54:54 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/14 21:01:23 visual_prompt]: Epoch 18 / 100: avg data time: 1.06e+01, avg batch time: 11.1188, average train loss: 0.5716
[11/14 21:02:06 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1774, average loss: 0.6619
[11/14 21:02:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.74	
[11/14 21:02:06 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/14 21:08:26 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.8455, average train loss: 0.5241
[11/14 21:09:09 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1742, average loss: 0.6877
[11/14 21:09:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.08	
[11/14 21:09:09 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/14 21:15:29 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.8325, average train loss: 0.5085
[11/14 21:16:11 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1799, average loss: 0.7454
[11/14 21:16:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.68	
[11/14 21:16:11 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/14 21:22:26 visual_prompt]: Epoch 21 / 100: avg data time: 1.01e+01, avg batch time: 10.6875, average train loss: 0.5087
[11/14 21:23:09 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1811, average loss: 0.9573
[11/14 21:23:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.39	
[11/14 21:23:09 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/14 21:29:23 visual_prompt]: Epoch 22 / 100: avg data time: 1.01e+01, avg batch time: 10.6906, average train loss: 0.5063
[11/14 21:30:06 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1726, average loss: 0.9121
[11/14 21:30:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.89	
[11/14 21:30:06 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/14 21:36:19 visual_prompt]: Epoch 23 / 100: avg data time: 1.01e+01, avg batch time: 10.6658, average train loss: 0.5157
[11/14 21:37:02 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1799, average loss: 1.0141
[11/14 21:37:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 65.51	
[11/14 21:37:02 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/14 21:43:16 visual_prompt]: Epoch 24 / 100: avg data time: 1.01e+01, avg batch time: 10.6782, average train loss: 0.6337
[11/14 21:43:58 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1791, average loss: 0.6935
[11/14 21:43:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 65.54	
[11/14 21:43:58 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/14 21:50:12 visual_prompt]: Epoch 25 / 100: avg data time: 1.01e+01, avg batch time: 10.6759, average train loss: 0.5709
[11/14 21:50:55 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1744, average loss: 0.7454
[11/14 21:50:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 64.57	
[11/14 21:50:55 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/14 21:57:27 visual_prompt]: Epoch 26 / 100: avg data time: 1.07e+01, avg batch time: 11.2004, average train loss: 0.4873
[11/14 21:58:11 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1795, average loss: 0.7384
[11/14 21:58:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 65.34	
[11/14 21:58:11 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/14 22:04:36 visual_prompt]: Epoch 27 / 100: avg data time: 1.04e+01, avg batch time: 10.9849, average train loss: 0.4226
[11/14 22:05:20 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1815, average loss: 1.0856
[11/14 22:05:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 59.35	
[11/14 22:05:20 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.000873410738492077
[11/14 22:12:50 visual_prompt]: Epoch 28 / 100: avg data time: 1.23e+01, avg batch time: 12.8550, average train loss: 0.4047
[11/14 22:13:46 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1704, average loss: 0.9283
[11/14 22:13:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 64.42	
[11/14 22:13:46 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.0008622126023955446
[11/14 22:21:17 visual_prompt]: Epoch 29 / 100: avg data time: 1.24e+01, avg batch time: 12.8911, average train loss: 0.5592
[11/14 22:22:09 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1781, average loss: 0.8879
[11/14 22:22:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 64.39	
[11/14 22:22:09 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0008506183921362443
[11/14 22:29:49 visual_prompt]: Epoch 30 / 100: avg data time: 1.26e+01, avg batch time: 13.1310, average train loss: 0.5081
[11/14 22:30:44 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1704, average loss: 1.0256
[11/14 22:30:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.07	
[11/14 22:30:44 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0008386407858128706
[11/14 22:38:11 visual_prompt]: Epoch 31 / 100: avg data time: 1.23e+01, avg batch time: 12.7875, average train loss: 0.3552
[11/14 22:39:11 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1714, average loss: 1.1456
[11/14 22:39:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 66.64	
[11/14 22:39:11 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.0008262928807620843
[11/14 22:47:00 visual_prompt]: Epoch 32 / 100: avg data time: 1.29e+01, avg batch time: 13.3955, average train loss: 0.3627
[11/14 22:48:12 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1713, average loss: 0.9899
[11/14 22:48:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 63.68	
[11/14 22:48:12 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.0008135881792367685
[11/14 22:55:54 visual_prompt]: Epoch 33 / 100: avg data time: 1.27e+01, avg batch time: 13.2040, average train loss: 0.3186
[11/14 22:56:59 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1767, average loss: 1.0038
[11/14 22:56:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 62.56	
[11/14 22:56:59 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.0008005405736415125
[11/14 23:04:34 visual_prompt]: Epoch 34 / 100: avg data time: 1.25e+01, avg batch time: 12.9976, average train loss: 0.2810
[11/14 23:05:30 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1698, average loss: 0.9527
[11/14 23:05:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.54	
[11/14 23:05:30 visual_prompt]: Stopping early.
[11/14 23:05:31 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 23:05:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 23:05:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/14 23:05:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 23:05:31 visual_prompt]: Training with config:
[11/14 23:05:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 23:05:31 visual_prompt]: Loading training data...
[11/14 23:05:31 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 23:05:31 visual_prompt]: Loading validation data...
[11/14 23:05:31 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 23:05:31 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 23:05:59 visual_prompt]: Enable all parameters update during training
[11/14 23:05:59 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 23:05:59 visual_prompt]: tuned percent:100.000
[11/14 23:06:00 visual_prompt]: Device used for model: 0
[11/14 23:06:00 visual_prompt]: Setting up Evaluator...
[11/14 23:06:00 visual_prompt]: Setting up Trainer...
[11/14 23:06:00 visual_prompt]: 	Setting up the optimizer...
[11/14 23:06:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 23:13:23 visual_prompt]: Epoch 1 / 100: avg data time: 1.21e+01, avg batch time: 12.6692, average train loss: 6.9791
[11/14 23:14:28 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1738, average loss: 6.3857
[11/14 23:14:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 23:14:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/14 23:21:49 visual_prompt]: Epoch 2 / 100: avg data time: 1.21e+01, avg batch time: 12.6067, average train loss: 2.9795
[11/14 23:22:51 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.1749, average loss: 1.0191
[11/14 23:22:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[11/14 23:22:51 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/14 23:30:29 visual_prompt]: Epoch 3 / 100: avg data time: 1.25e+01, avg batch time: 13.0787, average train loss: 0.9363
[11/14 23:31:12 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1792, average loss: 0.7546
[11/14 23:31:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.68	
[11/14 23:31:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/14 23:37:25 visual_prompt]: Epoch 4 / 100: avg data time: 1.01e+01, avg batch time: 10.6619, average train loss: 0.8156
[11/14 23:38:08 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1735, average loss: 0.6800
[11/14 23:38:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.91	
[11/14 23:38:08 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/14 23:44:21 visual_prompt]: Epoch 5 / 100: avg data time: 1.01e+01, avg batch time: 10.6594, average train loss: 0.7900
[11/14 23:45:04 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1806, average loss: 0.6689
[11/14 23:45:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.97	
[11/14 23:45:04 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/14 23:51:18 visual_prompt]: Epoch 6 / 100: avg data time: 1.01e+01, avg batch time: 10.6773, average train loss: 0.8251
[11/14 23:52:01 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1797, average loss: 0.7127
[11/14 23:52:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.79	
[11/14 23:52:01 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/14 23:58:15 visual_prompt]: Epoch 7 / 100: avg data time: 1.01e+01, avg batch time: 10.6883, average train loss: 0.7236
[11/14 23:58:58 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1747, average loss: 0.6896
[11/14 23:58:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.59	
[11/14 23:58:58 visual_prompt]: Best epoch 7: best metric: -0.690
[11/14 23:58:58 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/15 00:05:12 visual_prompt]: Epoch 8 / 100: avg data time: 1.01e+01, avg batch time: 10.6839, average train loss: 0.7060
[11/15 00:05:54 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1770, average loss: 0.6624
[11/15 00:05:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.49	
[11/15 00:05:54 visual_prompt]: Best epoch 8: best metric: -0.662
[11/15 00:05:54 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/15 00:12:07 visual_prompt]: Epoch 9 / 100: avg data time: 1.01e+01, avg batch time: 10.6534, average train loss: 0.7656
[11/15 00:12:50 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1752, average loss: 0.6902
[11/15 00:12:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 64.84	
[11/15 00:12:50 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/15 00:19:03 visual_prompt]: Epoch 10 / 100: avg data time: 1.01e+01, avg batch time: 10.6494, average train loss: 0.6893
[11/15 00:19:46 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1799, average loss: 0.7023
[11/15 00:19:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 67.18	
[11/15 00:19:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/15 00:25:59 visual_prompt]: Epoch 11 / 100: avg data time: 1.01e+01, avg batch time: 10.6594, average train loss: 0.6646
[11/15 00:26:42 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1805, average loss: 0.6843
[11/15 00:26:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.84	
[11/15 00:26:42 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/15 00:32:54 visual_prompt]: Epoch 12 / 100: avg data time: 1.01e+01, avg batch time: 10.6394, average train loss: 0.7185
[11/15 00:33:37 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1770, average loss: 0.7410
[11/15 00:33:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.83	
[11/15 00:33:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/15 00:39:50 visual_prompt]: Epoch 13 / 100: avg data time: 1.01e+01, avg batch time: 10.6488, average train loss: 0.6984
[11/15 00:40:32 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1808, average loss: 0.6548
[11/15 00:40:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.62	
[11/15 00:40:32 visual_prompt]: Best epoch 13: best metric: -0.655
[11/15 00:40:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/15 00:46:46 visual_prompt]: Epoch 14 / 100: avg data time: 1.01e+01, avg batch time: 10.6564, average train loss: 0.6217
[11/15 00:47:28 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1792, average loss: 0.6592
[11/15 00:47:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.11	
[11/15 00:47:28 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/15 00:53:42 visual_prompt]: Epoch 15 / 100: avg data time: 1.01e+01, avg batch time: 10.6729, average train loss: 0.6511
[11/15 00:54:24 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1749, average loss: 0.7042
[11/15 00:54:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.20	
[11/15 00:54:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/15 01:00:38 visual_prompt]: Epoch 16 / 100: avg data time: 1.01e+01, avg batch time: 10.6551, average train loss: 0.5915
[11/15 01:01:20 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1784, average loss: 0.6971
[11/15 01:01:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 67.61	
[11/15 01:01:20 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/15 01:07:33 visual_prompt]: Epoch 17 / 100: avg data time: 1.01e+01, avg batch time: 10.6527, average train loss: 0.6150
[11/15 01:08:16 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1774, average loss: 0.7181
[11/15 01:08:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.96	
[11/15 01:08:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/15 01:14:29 visual_prompt]: Epoch 18 / 100: avg data time: 1.01e+01, avg batch time: 10.6481, average train loss: 0.5716
[11/15 01:15:12 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1814, average loss: 0.6619
[11/15 01:15:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.74	
[11/15 01:15:12 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/15 01:21:25 visual_prompt]: Epoch 19 / 100: avg data time: 1.01e+01, avg batch time: 10.6691, average train loss: 0.5241
[11/15 01:22:08 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1826, average loss: 0.6877
[11/15 01:22:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.08	
[11/15 01:22:08 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/15 01:28:21 visual_prompt]: Epoch 20 / 100: avg data time: 1.01e+01, avg batch time: 10.6577, average train loss: 0.5085
[11/15 01:29:04 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1798, average loss: 0.7454
[11/15 01:29:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.68	
[11/15 01:29:04 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/15 01:35:17 visual_prompt]: Epoch 21 / 100: avg data time: 1.01e+01, avg batch time: 10.6689, average train loss: 0.5087
[11/15 01:36:00 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1801, average loss: 0.9573
[11/15 01:36:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.39	
[11/15 01:36:00 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/15 01:42:14 visual_prompt]: Epoch 22 / 100: avg data time: 1.01e+01, avg batch time: 10.6689, average train loss: 0.5063
[11/15 01:42:56 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1777, average loss: 0.9121
[11/15 01:42:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.89	
[11/15 01:42:56 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/15 01:49:09 visual_prompt]: Epoch 23 / 100: avg data time: 1.01e+01, avg batch time: 10.6542, average train loss: 0.5157
[11/15 01:49:52 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1744, average loss: 1.0141
[11/15 01:49:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 65.51	
[11/15 01:49:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/15 01:56:06 visual_prompt]: Epoch 24 / 100: avg data time: 1.01e+01, avg batch time: 10.6825, average train loss: 0.6337
[11/15 01:56:49 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1793, average loss: 0.6935
[11/15 01:56:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 65.54	
[11/15 01:56:49 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/15 02:03:03 visual_prompt]: Epoch 25 / 100: avg data time: 1.01e+01, avg batch time: 10.6661, average train loss: 0.5709
[11/15 02:03:45 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1782, average loss: 0.7454
[11/15 02:03:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 64.57	
[11/15 02:03:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/15 02:09:59 visual_prompt]: Epoch 26 / 100: avg data time: 1.01e+01, avg batch time: 10.6806, average train loss: 0.4873
[11/15 02:10:42 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1781, average loss: 0.7384
[11/15 02:10:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 65.34	
[11/15 02:10:42 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/15 02:16:55 visual_prompt]: Epoch 27 / 100: avg data time: 1.01e+01, avg batch time: 10.6588, average train loss: 0.4226
[11/15 02:17:38 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1815, average loss: 1.0856
[11/15 02:17:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 59.35	
[11/15 02:17:38 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.000873410738492077
[11/15 02:23:52 visual_prompt]: Epoch 28 / 100: avg data time: 1.01e+01, avg batch time: 10.6731, average train loss: 0.4047
[11/15 02:24:34 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1814, average loss: 0.9283
[11/15 02:24:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 64.42	
[11/15 02:24:34 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.0008622126023955446
[11/15 02:30:48 visual_prompt]: Epoch 29 / 100: avg data time: 1.01e+01, avg batch time: 10.6703, average train loss: 0.5592
[11/15 02:31:31 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1823, average loss: 0.8879
[11/15 02:31:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 64.39	
[11/15 02:31:31 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0008506183921362443
[11/15 02:37:44 visual_prompt]: Epoch 30 / 100: avg data time: 1.01e+01, avg batch time: 10.6619, average train loss: 0.5081
[11/15 02:38:27 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1790, average loss: 1.0256
[11/15 02:38:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.07	
[11/15 02:38:27 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0008386407858128706
[11/15 02:44:40 visual_prompt]: Epoch 31 / 100: avg data time: 1.01e+01, avg batch time: 10.6477, average train loss: 0.3552
[11/15 02:45:22 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1807, average loss: 1.1456
[11/15 02:45:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 66.64	
[11/15 02:45:22 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.0008262928807620843
[11/15 02:51:36 visual_prompt]: Epoch 32 / 100: avg data time: 1.01e+01, avg batch time: 10.6749, average train loss: 0.3627
[11/15 02:52:19 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1773, average loss: 0.9899
[11/15 02:52:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 63.68	
[11/15 02:52:19 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.0008135881792367685
[11/15 02:58:32 visual_prompt]: Epoch 33 / 100: avg data time: 1.01e+01, avg batch time: 10.6659, average train loss: 0.3186
[11/15 02:59:15 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1747, average loss: 1.0038
[11/15 02:59:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 62.56	
[11/15 02:59:15 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.0008005405736415125
[11/15 03:05:28 visual_prompt]: Epoch 34 / 100: avg data time: 1.01e+01, avg batch time: 10.6543, average train loss: 0.2810
[11/15 03:06:11 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1798, average loss: 0.9527
[11/15 03:06:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.54	
[11/15 03:06:11 visual_prompt]: Stopping early.
[11/15 03:06:11 visual_prompt]: Rank of current process: 0. World size: 1
[11/15 03:06:11 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/15 03:06:11 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/15 03:06:11 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/15 03:06:11 visual_prompt]: Training with config:
[11/15 03:06:11 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/15 03:06:11 visual_prompt]: Loading training data...
[11/15 03:06:11 visual_prompt]: Constructing mammo-cbis dataset train...
[11/15 03:06:11 visual_prompt]: Loading validation data...
[11/15 03:06:11 visual_prompt]: Constructing mammo-cbis dataset val...
[11/15 03:06:11 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/15 03:06:12 visual_prompt]: Enable all parameters update during training
[11/15 03:06:12 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/15 03:06:12 visual_prompt]: tuned percent:100.000
[11/15 03:06:12 visual_prompt]: Device used for model: 0
[11/15 03:06:12 visual_prompt]: Setting up Evaluator...
[11/15 03:06:12 visual_prompt]: Setting up Trainer...
[11/15 03:06:12 visual_prompt]: 	Setting up the optimizer...
[11/15 03:06:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/15 03:12:26 visual_prompt]: Epoch 1 / 100: avg data time: 1.01e+01, avg batch time: 10.6651, average train loss: 6.9791
[11/15 03:13:08 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1798, average loss: 6.3857
[11/15 03:13:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/15 03:13:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/15 03:19:22 visual_prompt]: Epoch 2 / 100: avg data time: 1.01e+01, avg batch time: 10.6573, average train loss: 2.9795
[11/15 03:20:04 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1760, average loss: 1.0191
[11/15 03:20:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[11/15 03:20:04 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/15 03:26:18 visual_prompt]: Epoch 3 / 100: avg data time: 1.01e+01, avg batch time: 10.6632, average train loss: 0.9363
[11/15 03:27:00 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1749, average loss: 0.7546
[11/15 03:27:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.68	
[11/15 03:27:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/15 03:33:13 visual_prompt]: Epoch 4 / 100: avg data time: 1.01e+01, avg batch time: 10.6473, average train loss: 0.8156
[11/15 03:33:56 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1778, average loss: 0.6800
[11/15 03:33:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.91	
[11/15 03:33:56 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/15 03:40:09 visual_prompt]: Epoch 5 / 100: avg data time: 1.01e+01, avg batch time: 10.6489, average train loss: 0.7900
[11/15 03:40:51 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1768, average loss: 0.6689
[11/15 03:40:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.97	
[11/15 03:40:51 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/15 03:47:05 visual_prompt]: Epoch 6 / 100: avg data time: 1.01e+01, avg batch time: 10.6648, average train loss: 0.8251
[11/15 03:47:47 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1748, average loss: 0.7127
[11/15 03:47:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.79	
[11/15 03:47:47 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/15 03:54:01 visual_prompt]: Epoch 7 / 100: avg data time: 1.01e+01, avg batch time: 10.6762, average train loss: 0.7236
[11/15 03:54:44 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1747, average loss: 0.6896
[11/15 03:54:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.59	
[11/15 03:54:44 visual_prompt]: Best epoch 7: best metric: -0.690
[11/15 03:54:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/15 04:00:57 visual_prompt]: Epoch 8 / 100: avg data time: 1.01e+01, avg batch time: 10.6601, average train loss: 0.7060
[11/15 04:01:40 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1786, average loss: 0.6624
[11/15 04:01:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.49	
[11/15 04:01:40 visual_prompt]: Best epoch 8: best metric: -0.662
[11/15 04:01:40 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/15 04:07:53 visual_prompt]: Epoch 9 / 100: avg data time: 1.01e+01, avg batch time: 10.6521, average train loss: 0.7656
[11/15 04:08:35 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1809, average loss: 0.6902
[11/15 04:08:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 64.84	
[11/15 04:08:35 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/15 04:14:49 visual_prompt]: Epoch 10 / 100: avg data time: 1.01e+01, avg batch time: 10.6536, average train loss: 0.6893
[11/15 04:15:31 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1793, average loss: 0.7023
[11/15 04:15:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 67.18	
[11/15 04:15:31 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/15 04:21:44 visual_prompt]: Epoch 11 / 100: avg data time: 1.01e+01, avg batch time: 10.6543, average train loss: 0.6646
[11/15 04:22:27 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1808, average loss: 0.6843
[11/15 04:22:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.84	
[11/15 04:22:27 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/15 04:28:40 visual_prompt]: Epoch 12 / 100: avg data time: 1.01e+01, avg batch time: 10.6568, average train loss: 0.7185
[11/15 04:29:23 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1739, average loss: 0.7410
[11/15 04:29:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.83	
[11/15 04:29:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/15 04:35:36 visual_prompt]: Epoch 13 / 100: avg data time: 1.01e+01, avg batch time: 10.6542, average train loss: 0.6984
[11/15 04:36:18 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1748, average loss: 0.6548
[11/15 04:36:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.62	
[11/15 04:36:18 visual_prompt]: Best epoch 13: best metric: -0.655
[11/15 04:36:18 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/15 04:42:31 visual_prompt]: Epoch 14 / 100: avg data time: 1.01e+01, avg batch time: 10.6461, average train loss: 0.6217
[11/15 04:43:14 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1729, average loss: 0.6592
[11/15 04:43:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.11	
[11/15 04:43:14 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/15 04:49:27 visual_prompt]: Epoch 15 / 100: avg data time: 1.01e+01, avg batch time: 10.6681, average train loss: 0.6511
[11/15 04:50:10 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1772, average loss: 0.7042
[11/15 04:50:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.20	
[11/15 04:50:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/15 04:56:23 visual_prompt]: Epoch 16 / 100: avg data time: 1.01e+01, avg batch time: 10.6626, average train loss: 0.5915
[11/15 04:57:06 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1792, average loss: 0.6971
[11/15 04:57:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 67.61	
[11/15 04:57:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/15 05:03:19 visual_prompt]: Epoch 17 / 100: avg data time: 1.01e+01, avg batch time: 10.6564, average train loss: 0.6150
[11/15 05:04:02 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1726, average loss: 0.7181
[11/15 05:04:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.96	
[11/15 05:04:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/15 05:10:15 visual_prompt]: Epoch 18 / 100: avg data time: 1.01e+01, avg batch time: 10.6548, average train loss: 0.5716
[11/15 05:10:58 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1732, average loss: 0.6619
[11/15 05:10:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.74	
[11/15 05:10:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/15 05:17:11 visual_prompt]: Epoch 19 / 100: avg data time: 1.01e+01, avg batch time: 10.6733, average train loss: 0.5241
[11/15 05:17:54 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1785, average loss: 0.6877
[11/15 05:17:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.08	
[11/15 05:17:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/15 05:24:08 visual_prompt]: Epoch 20 / 100: avg data time: 1.01e+01, avg batch time: 10.6731, average train loss: 0.5085
[11/15 05:24:51 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1793, average loss: 0.7454
[11/15 05:24:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.68	
[11/15 05:24:51 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/15 05:31:04 visual_prompt]: Epoch 21 / 100: avg data time: 1.01e+01, avg batch time: 10.6644, average train loss: 0.5087
[11/15 05:31:47 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1804, average loss: 0.9573
[11/15 05:31:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.39	
[11/15 05:31:47 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/15 05:38:00 visual_prompt]: Epoch 22 / 100: avg data time: 1.01e+01, avg batch time: 10.6703, average train loss: 0.5063
[11/15 05:38:43 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1808, average loss: 0.9121
[11/15 05:38:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.89	
[11/15 05:38:43 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/15 05:44:56 visual_prompt]: Epoch 23 / 100: avg data time: 1.01e+01, avg batch time: 10.6444, average train loss: 0.5157
[11/15 05:45:39 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1790, average loss: 1.0141
[11/15 05:45:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 65.51	
[11/15 05:45:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/15 05:51:52 visual_prompt]: Epoch 24 / 100: avg data time: 1.01e+01, avg batch time: 10.6733, average train loss: 0.6337
[11/15 05:52:35 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1804, average loss: 0.6935
[11/15 05:52:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 65.54	
[11/15 05:52:35 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/15 05:58:48 visual_prompt]: Epoch 25 / 100: avg data time: 1.01e+01, avg batch time: 10.6601, average train loss: 0.5709
[11/15 05:59:31 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1749, average loss: 0.7454
[11/15 05:59:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 64.57	
[11/15 05:59:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/15 06:05:45 visual_prompt]: Epoch 26 / 100: avg data time: 1.01e+01, avg batch time: 10.6705, average train loss: 0.4873
[11/15 06:06:27 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1816, average loss: 0.7384
[11/15 06:06:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 65.34	
[11/15 06:06:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/15 06:12:40 visual_prompt]: Epoch 27 / 100: avg data time: 1.01e+01, avg batch time: 10.6510, average train loss: 0.4226
[11/15 06:13:23 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1736, average loss: 1.0856
[11/15 06:13:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 59.35	
[11/15 06:13:23 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.000873410738492077
[11/15 06:19:36 visual_prompt]: Epoch 28 / 100: avg data time: 1.01e+01, avg batch time: 10.6616, average train loss: 0.4047
[11/15 06:20:19 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1766, average loss: 0.9283
[11/15 06:20:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 64.42	
[11/15 06:20:19 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.0008622126023955446
[11/15 06:26:32 visual_prompt]: Epoch 29 / 100: avg data time: 1.01e+01, avg batch time: 10.6631, average train loss: 0.5592
[11/15 06:27:15 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1744, average loss: 0.8879
[11/15 06:27:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 64.39	
[11/15 06:27:15 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0008506183921362443
[11/15 06:33:28 visual_prompt]: Epoch 30 / 100: avg data time: 1.01e+01, avg batch time: 10.6487, average train loss: 0.5081
[11/15 06:34:11 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1803, average loss: 1.0256
[11/15 06:34:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.07	
[11/15 06:34:11 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0008386407858128706
[11/15 06:40:24 visual_prompt]: Epoch 31 / 100: avg data time: 1.01e+01, avg batch time: 10.6488, average train loss: 0.3552
[11/15 06:41:06 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1826, average loss: 1.1456
[11/15 06:41:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 66.64	
[11/15 06:41:06 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.0008262928807620843
[11/15 06:47:20 visual_prompt]: Epoch 32 / 100: avg data time: 1.01e+01, avg batch time: 10.6777, average train loss: 0.3627
[11/15 06:48:03 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1819, average loss: 0.9899
[11/15 06:48:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 63.68	
[11/15 06:48:03 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.0008135881792367685
[11/15 06:54:17 visual_prompt]: Epoch 33 / 100: avg data time: 1.01e+01, avg batch time: 10.6694, average train loss: 0.3186
[11/15 06:54:59 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1782, average loss: 1.0038
[11/15 06:54:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 62.56	
[11/15 06:54:59 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.0008005405736415125
[11/15 07:01:13 visual_prompt]: Epoch 34 / 100: avg data time: 1.01e+01, avg batch time: 10.6602, average train loss: 0.2810
[11/15 07:01:55 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1769, average loss: 0.9527
[11/15 07:01:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.54	
[11/15 07:01:55 visual_prompt]: Stopping early.
[11/15 07:01:55 visual_prompt]: Rank of current process: 0. World size: 1
[11/15 07:01:55 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/15 07:01:55 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/15 07:01:55 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/15 07:01:55 visual_prompt]: Training with config:
[11/15 07:01:55 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/15 07:01:55 visual_prompt]: Loading training data...
[11/15 07:01:55 visual_prompt]: Constructing mammo-cbis dataset train...
[11/15 07:01:55 visual_prompt]: Loading validation data...
[11/15 07:01:55 visual_prompt]: Constructing mammo-cbis dataset val...
[11/15 07:01:55 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/15 07:01:57 visual_prompt]: Enable all parameters update during training
[11/15 07:01:57 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/15 07:01:57 visual_prompt]: tuned percent:100.000
[11/15 07:01:57 visual_prompt]: Device used for model: 0
[11/15 07:01:57 visual_prompt]: Setting up Evaluator...
[11/15 07:01:57 visual_prompt]: Setting up Trainer...
[11/15 07:01:57 visual_prompt]: 	Setting up the optimizer...
[11/15 07:01:57 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/15 07:08:11 visual_prompt]: Epoch 1 / 100: avg data time: 1.01e+01, avg batch time: 10.6710, average train loss: 6.9791
[11/15 07:08:53 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1771, average loss: 6.3857
[11/15 07:08:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/15 07:08:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/15 07:15:06 visual_prompt]: Epoch 2 / 100: avg data time: 1.01e+01, avg batch time: 10.6525, average train loss: 4.7319
[11/15 07:15:49 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.1766, average loss: 1.0649
[11/15 07:15:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 51.03	
[11/15 07:15:49 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/15 07:22:03 visual_prompt]: Epoch 3 / 100: avg data time: 1.01e+01, avg batch time: 10.6653, average train loss: 0.9320
[11/15 07:22:45 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1779, average loss: 0.7129
[11/15 07:22:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.31	
[11/15 07:22:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/15 07:28:59 visual_prompt]: Epoch 4 / 100: avg data time: 1.01e+01, avg batch time: 10.6583, average train loss: 0.7922
[11/15 07:29:41 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1801, average loss: 0.7879
[11/15 07:29:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.62	
[11/15 07:29:41 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/15 07:35:55 visual_prompt]: Epoch 5 / 100: avg data time: 1.01e+01, avg batch time: 10.6533, average train loss: 0.8139
[11/15 07:36:37 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1789, average loss: 0.6712
[11/15 07:36:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.10	
[11/15 07:36:37 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/15 07:42:51 visual_prompt]: Epoch 6 / 100: avg data time: 1.01e+01, avg batch time: 10.6691, average train loss: 0.7711
[11/15 07:43:34 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1797, average loss: 0.7228
[11/15 07:43:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.26	
[11/15 07:43:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/15 07:49:47 visual_prompt]: Epoch 7 / 100: avg data time: 1.01e+01, avg batch time: 10.6663, average train loss: 0.7319
[11/15 07:50:30 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1732, average loss: 0.8022
[11/15 07:50:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.77	
[11/15 07:50:30 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/15 07:56:44 visual_prompt]: Epoch 8 / 100: avg data time: 1.01e+01, avg batch time: 10.6762, average train loss: 0.8037
[11/15 07:57:27 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1816, average loss: 0.7268
[11/15 07:57:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 60.55	
[11/15 07:57:27 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/15 08:03:40 visual_prompt]: Epoch 9 / 100: avg data time: 1.01e+01, avg batch time: 10.6593, average train loss: 0.7855
[11/15 08:04:23 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1746, average loss: 0.7112
[11/15 08:04:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 61.17	
[11/15 08:04:23 visual_prompt]: Best epoch 9: best metric: -0.711
[11/15 08:04:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/15 08:10:36 visual_prompt]: Epoch 10 / 100: avg data time: 1.01e+01, avg batch time: 10.6491, average train loss: 0.7169
[11/15 08:11:18 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1796, average loss: 0.7654
[11/15 08:11:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 60.58	
[11/15 08:11:18 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/15 08:17:31 visual_prompt]: Epoch 11 / 100: avg data time: 1.01e+01, avg batch time: 10.6528, average train loss: 0.6711
[11/15 08:18:14 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1787, average loss: 0.7312
[11/15 08:18:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.53	
[11/15 08:18:14 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/15 08:24:27 visual_prompt]: Epoch 12 / 100: avg data time: 1.01e+01, avg batch time: 10.6593, average train loss: 0.6936
[11/15 08:25:10 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1789, average loss: 0.7229
[11/15 08:25:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.16	
[11/15 08:25:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/15 08:31:23 visual_prompt]: Epoch 13 / 100: avg data time: 1.01e+01, avg batch time: 10.6584, average train loss: 0.7212
[11/15 08:32:06 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1734, average loss: 0.7368
[11/15 08:32:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.90	
[11/15 08:32:06 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/15 08:38:19 visual_prompt]: Epoch 14 / 100: avg data time: 1.01e+01, avg batch time: 10.6567, average train loss: 0.6471
[11/15 08:39:02 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1804, average loss: 0.6434
[11/15 08:39:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 62.60	
[11/15 08:39:02 visual_prompt]: Best epoch 14: best metric: -0.643
[11/15 08:39:02 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/15 08:45:16 visual_prompt]: Epoch 15 / 100: avg data time: 1.01e+01, avg batch time: 10.6755, average train loss: 0.7018
[11/15 08:45:59 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1839, average loss: 0.6955
[11/15 08:45:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.30	
[11/15 08:45:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/15 08:52:12 visual_prompt]: Epoch 16 / 100: avg data time: 1.01e+01, avg batch time: 10.6587, average train loss: 0.6473
[11/15 08:52:55 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1788, average loss: 0.9287
[11/15 08:52:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 62.04	
[11/15 08:52:55 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/15 08:59:08 visual_prompt]: Epoch 17 / 100: avg data time: 1.01e+01, avg batch time: 10.6638, average train loss: 0.6998
[11/15 08:59:51 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1853, average loss: 0.7083
[11/15 08:59:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 62.37	
[11/15 08:59:51 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/15 09:06:04 visual_prompt]: Epoch 18 / 100: avg data time: 1.01e+01, avg batch time: 10.6620, average train loss: 0.6598
[11/15 09:06:47 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1819, average loss: 0.6687
[11/15 09:06:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 63.06	
[11/15 09:06:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/15 09:13:01 visual_prompt]: Epoch 19 / 100: avg data time: 1.01e+01, avg batch time: 10.6796, average train loss: 0.6186
[11/15 09:13:44 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1787, average loss: 0.7331
[11/15 09:13:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 62.74	
[11/15 09:13:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/15 09:19:57 visual_prompt]: Epoch 20 / 100: avg data time: 1.01e+01, avg batch time: 10.6638, average train loss: 0.6756
[11/15 09:20:40 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1758, average loss: 0.7132
[11/15 09:20:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.61	
[11/15 09:20:40 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/15 09:26:54 visual_prompt]: Epoch 21 / 100: avg data time: 1.01e+01, avg batch time: 10.6770, average train loss: 0.6154
[11/15 09:27:37 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1746, average loss: 0.7064
[11/15 09:27:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.66	
[11/15 09:27:37 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/15 09:33:50 visual_prompt]: Epoch 22 / 100: avg data time: 1.01e+01, avg batch time: 10.6683, average train loss: 0.6016
[11/15 09:34:33 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1780, average loss: 0.7497
[11/15 09:34:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 60.56	
[11/15 09:34:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/15 09:40:46 visual_prompt]: Epoch 23 / 100: avg data time: 1.01e+01, avg batch time: 10.6513, average train loss: 0.6263
[11/15 09:41:29 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1746, average loss: 1.0093
[11/15 09:41:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.65	
[11/15 09:41:29 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/15 09:47:43 visual_prompt]: Epoch 24 / 100: avg data time: 1.01e+01, avg batch time: 10.6767, average train loss: 0.5848
[11/15 09:48:26 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1826, average loss: 0.7263
[11/15 09:48:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 62.03	
[11/15 09:48:26 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/15 09:54:39 visual_prompt]: Epoch 25 / 100: avg data time: 1.01e+01, avg batch time: 10.6767, average train loss: 0.5785
[11/15 09:55:22 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1817, average loss: 0.7352
[11/15 09:55:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.23	
[11/15 09:55:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/15 10:01:36 visual_prompt]: Epoch 26 / 100: avg data time: 1.01e+01, avg batch time: 10.6756, average train loss: 0.5963
[11/15 10:02:19 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1792, average loss: 0.6996
[11/15 10:02:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 60.71	
[11/15 10:02:19 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/15 10:08:37 visual_prompt]: Epoch 27 / 100: avg data time: 1.03e+01, avg batch time: 10.7977, average train loss: 0.6058
[11/15 10:09:21 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1805, average loss: 0.7311
[11/15 10:09:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 60.73	
[11/15 10:09:21 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.000873410738492077
[11/15 10:15:55 visual_prompt]: Epoch 28 / 100: avg data time: 1.07e+01, avg batch time: 11.2305, average train loss: 0.5594
[11/15 10:16:37 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1794, average loss: 0.7719
[11/15 10:16:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 59.71	
[11/15 10:16:37 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.0008622126023955446
[11/15 10:22:51 visual_prompt]: Epoch 29 / 100: avg data time: 1.01e+01, avg batch time: 10.6717, average train loss: 0.5620
[11/15 10:23:34 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1782, average loss: 0.7923
[11/15 10:23:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 60.25	
[11/15 10:23:34 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0008506183921362443
[11/15 10:29:47 visual_prompt]: Epoch 30 / 100: avg data time: 1.01e+01, avg batch time: 10.6564, average train loss: 0.5767
[11/15 10:30:30 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1731, average loss: 0.8302
[11/15 10:30:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 59.75	
[11/15 10:30:30 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0008386407858128706
[11/15 10:36:43 visual_prompt]: Epoch 31 / 100: avg data time: 1.01e+01, avg batch time: 10.6579, average train loss: 0.5688
[11/15 10:37:26 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1715, average loss: 0.7248
[11/15 10:37:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 61.97	
[11/15 10:37:26 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.0008262928807620843
[11/15 10:43:40 visual_prompt]: Epoch 32 / 100: avg data time: 1.01e+01, avg batch time: 10.6774, average train loss: 0.5404
[11/15 10:44:23 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1800, average loss: 0.8570
[11/15 10:44:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 60.95	
[11/15 10:44:23 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.0008135881792367685
[11/15 10:50:59 visual_prompt]: Epoch 33 / 100: avg data time: 1.08e+01, avg batch time: 11.3188, average train loss: 0.5431
[11/15 10:51:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1778, average loss: 0.8312
[11/15 10:51:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 60.92	
[11/15 10:51:44 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.0008005405736415125
[11/15 10:58:02 visual_prompt]: Epoch 34 / 100: avg data time: 1.02e+01, avg batch time: 10.7761, average train loss: 0.5774
[11/15 10:58:45 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1761, average loss: 0.7914
[11/15 10:58:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.49	
[11/15 10:58:45 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0007871643313414718
[11/15 11:05:03 visual_prompt]: Epoch 35 / 100: avg data time: 1.03e+01, avg batch time: 10.7826, average train loss: 0.5556
[11/15 11:05:46 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1779, average loss: 0.7693
[11/15 11:05:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 60.58	
[11/15 11:05:46 visual_prompt]: Stopping early.
[11/15 11:05:46 visual_prompt]: Rank of current process: 0. World size: 1
[11/15 11:05:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/15 11:05:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/15 11:05:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/15 11:05:46 visual_prompt]: Training with config:
[11/15 11:05:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/15 11:05:46 visual_prompt]: Loading training data...
[11/15 11:05:46 visual_prompt]: Constructing mammo-cbis dataset train...
[11/15 11:05:46 visual_prompt]: Loading validation data...
[11/15 11:05:46 visual_prompt]: Constructing mammo-cbis dataset val...
[11/15 11:05:46 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/15 11:05:48 visual_prompt]: Enable all parameters update during training
[11/15 11:05:48 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/15 11:05:48 visual_prompt]: tuned percent:100.000
[11/15 11:05:48 visual_prompt]: Device used for model: 0
[11/15 11:05:48 visual_prompt]: Setting up Evaluator...
[11/15 11:05:48 visual_prompt]: Setting up Trainer...
[11/15 11:05:48 visual_prompt]: 	Setting up the optimizer...
[11/15 11:05:48 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/15 11:12:08 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.8491, average train loss: 6.9791
[11/15 11:12:51 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1781, average loss: 6.3857
[11/15 11:12:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/15 11:12:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/15 11:19:08 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.7671, average train loss: 3.1853
[11/15 11:19:51 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1688, average loss: 0.8753
[11/15 11:19:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.69	
[11/15 11:19:51 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/15 11:26:09 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.7948, average train loss: 0.9599
[11/15 11:26:53 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1761, average loss: 0.7334
[11/15 11:26:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.81	
[11/15 11:26:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/15 11:33:10 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.7638, average train loss: 0.7770
[11/15 11:33:53 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1683, average loss: 0.6492
[11/15 11:33:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.12	
[11/15 11:33:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/15 11:40:10 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.7675, average train loss: 0.7371
[11/15 11:40:53 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1697, average loss: 0.7053
[11/15 11:40:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.47	
[11/15 11:40:53 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/15 11:47:10 visual_prompt]: Epoch 6 / 100: avg data time: 1.02e+01, avg batch time: 10.7831, average train loss: 0.7302
[11/15 11:47:54 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1792, average loss: 0.6638
[11/15 11:47:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.38	
[11/15 11:47:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/15 11:54:11 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.7850, average train loss: 0.6865
[11/15 11:55:03 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1678, average loss: 0.6231
[11/15 11:55:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.25	
[11/15 11:55:03 visual_prompt]: Best epoch 7: best metric: -0.623
[11/15 11:55:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/15 12:01:28 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9751, average train loss: 0.6633
[11/15 12:02:11 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1780, average loss: 0.6259
[11/15 12:02:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.63	
[11/15 12:02:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/15 12:08:28 visual_prompt]: Epoch 9 / 100: avg data time: 1.02e+01, avg batch time: 10.7743, average train loss: 0.6471
[11/15 12:09:12 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1733, average loss: 0.8654
[11/15 12:09:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 71.31	
[11/15 12:09:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/15 12:15:29 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.7694, average train loss: 0.6220
[11/15 12:16:12 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1747, average loss: 0.8537
[11/15 12:16:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 69.46	
[11/15 12:16:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/15 12:22:29 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.7668, average train loss: 0.5798
[11/15 12:23:12 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1701, average loss: 0.9848
[11/15 12:23:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.86	
[11/15 12:23:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/15 12:29:30 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.7781, average train loss: 0.5611
[11/15 12:30:13 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1747, average loss: 0.8405
[11/15 12:30:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 73.09	
[11/15 12:30:13 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/15 12:36:31 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.8041, average train loss: 0.5306
[11/15 12:37:15 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1744, average loss: 0.8182
[11/15 12:37:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.86	
[11/15 12:37:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/15 12:43:36 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.8707, average train loss: 0.5167
[11/15 12:44:19 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1710, average loss: 0.7376
[11/15 12:44:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.13	
[11/15 12:44:19 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/15 12:50:36 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.7779, average train loss: 0.4869
[11/15 12:51:20 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1710, average loss: 0.6656
[11/15 12:51:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.43	
[11/15 12:51:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/15 12:57:37 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.7808, average train loss: 0.4100
[11/15 12:58:20 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1795, average loss: 0.7873
[11/15 12:58:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.13	
[11/15 12:58:20 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/15 13:04:38 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.7849, average train loss: 0.4407
[11/15 13:05:21 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1773, average loss: 1.0832
[11/15 13:05:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 69.68	
[11/15 13:05:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/15 13:11:39 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.7772, average train loss: 0.3789
[11/15 13:12:22 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1716, average loss: 0.7727
[11/15 13:12:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 72.00	
[11/15 13:12:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/15 13:18:39 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.7719, average train loss: 0.2827
[11/15 13:19:22 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1708, average loss: 0.8068
[11/15 13:19:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.25	
[11/15 13:19:22 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/15 13:25:40 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.7868, average train loss: 0.2560
[11/15 13:26:23 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1779, average loss: 1.2641
[11/15 13:26:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.72	
[11/15 13:26:23 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/15 13:32:41 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.7837, average train loss: 0.2641
[11/15 13:33:24 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1803, average loss: 1.0899
[11/15 13:33:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.41	
[11/15 13:33:24 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/15 13:39:41 visual_prompt]: Epoch 22 / 100: avg data time: 1.02e+01, avg batch time: 10.7821, average train loss: 0.2304
[11/15 13:40:24 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1806, average loss: 1.0525
[11/15 13:40:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 66.76	
[11/15 13:40:24 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/15 13:46:42 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e+01, avg batch time: 10.7688, average train loss: 0.2344
[11/15 13:47:25 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1771, average loss: 1.0677
[11/15 13:47:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.60	
[11/15 13:47:25 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/15 13:53:43 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e+01, avg batch time: 10.7917, average train loss: 0.1750
[11/15 13:54:26 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1686, average loss: 1.0889
[11/15 13:54:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 67.52	
[11/15 13:54:26 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/15 14:00:43 visual_prompt]: Epoch 25 / 100: avg data time: 1.02e+01, avg batch time: 10.7660, average train loss: 0.1654
[11/15 14:01:26 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1743, average loss: 1.0928
[11/15 14:01:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.06	
[11/15 14:01:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/15 14:07:44 visual_prompt]: Epoch 26 / 100: avg data time: 1.03e+01, avg batch time: 10.7857, average train loss: 0.1619
[11/15 14:08:27 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1708, average loss: 1.4202
[11/15 14:08:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.14	
[11/15 14:08:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/15 14:14:44 visual_prompt]: Epoch 27 / 100: avg data time: 1.02e+01, avg batch time: 10.7612, average train loss: 0.1295
[11/15 14:15:27 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1742, average loss: 1.2576
[11/15 14:15:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 68.62	
[11/15 14:15:27 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.0004367053692460385
[11/15 14:21:47 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.8562, average train loss: 0.1032
[11/15 14:22:30 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1721, average loss: 1.6015
[11/15 14:22:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.32	
[11/15 14:22:30 visual_prompt]: Stopping early.
[11/15 14:22:31 visual_prompt]: Rank of current process: 0. World size: 1
[11/15 14:22:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/15 14:22:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/15 14:22:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/15 14:22:31 visual_prompt]: Training with config:
[11/15 14:22:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/15 14:22:31 visual_prompt]: Loading training data...
[11/15 14:22:31 visual_prompt]: Constructing mammo-cbis dataset train...
[11/15 14:22:31 visual_prompt]: Loading validation data...
[11/15 14:22:31 visual_prompt]: Constructing mammo-cbis dataset val...
[11/15 14:22:31 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/15 14:22:32 visual_prompt]: Enable all parameters update during training
[11/15 14:22:32 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/15 14:22:32 visual_prompt]: tuned percent:100.000
[11/15 14:22:32 visual_prompt]: Device used for model: 0
[11/15 14:22:32 visual_prompt]: Setting up Evaluator...
[11/15 14:22:32 visual_prompt]: Setting up Trainer...
[11/15 14:22:32 visual_prompt]: 	Setting up the optimizer...
[11/15 14:22:32 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/15 14:28:50 visual_prompt]: Epoch 1 / 100: avg data time: 1.03e+01, avg batch time: 10.7964, average train loss: 6.9791
[11/15 14:29:34 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1791, average loss: 6.3857
[11/15 14:29:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/15 14:29:34 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/15 14:35:50 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.7524, average train loss: 3.1853
[11/15 14:36:33 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1705, average loss: 0.8753
[11/15 14:36:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.69	
[11/15 14:36:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/15 14:42:51 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.7945, average train loss: 0.9599
[11/15 14:43:35 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1679, average loss: 0.7334
[11/15 14:43:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.81	
[11/15 14:43:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/15 14:49:52 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.7674, average train loss: 0.7770
[11/15 14:50:35 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1744, average loss: 0.6492
[11/15 14:50:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.12	
[11/15 14:50:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/15 14:56:52 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.7670, average train loss: 0.7371
[11/15 14:57:35 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1727, average loss: 0.7053
[11/15 14:57:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.47	
[11/15 14:57:35 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/15 15:03:54 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.8189, average train loss: 0.7302
[11/15 15:04:37 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1732, average loss: 0.6638
[11/15 15:04:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.38	
[11/15 15:04:37 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/15 15:10:58 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.8718, average train loss: 0.6865
[11/15 15:11:42 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1682, average loss: 0.6231
[11/15 15:11:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.25	
[11/15 15:11:42 visual_prompt]: Best epoch 7: best metric: -0.623
[11/15 15:11:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/15 15:18:00 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.8009, average train loss: 0.6633
[11/15 15:18:43 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1784, average loss: 0.6259
[11/15 15:18:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.63	
[11/15 15:18:43 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/15 15:25:01 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.8006, average train loss: 0.6471
[11/15 15:25:45 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1753, average loss: 0.8654
[11/15 15:25:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 71.31	
[11/15 15:25:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/15 15:32:02 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.7701, average train loss: 0.6220
[11/15 15:32:45 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1767, average loss: 0.8537
[11/15 15:32:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 69.46	
[11/15 15:32:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/15 15:39:04 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.8072, average train loss: 0.5798
[11/15 15:39:48 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1789, average loss: 0.9848
[11/15 15:39:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.86	
[11/15 15:39:48 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/15 15:46:06 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.7768, average train loss: 0.5611
[11/15 15:46:49 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1743, average loss: 0.8405
[11/15 15:46:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 73.09	
[11/15 15:46:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/15 15:53:07 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.7875, average train loss: 0.5306
[11/15 15:53:50 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1688, average loss: 0.8182
[11/15 15:53:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.86	
[11/15 15:53:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/15 16:00:08 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.7908, average train loss: 0.5167
[11/15 16:00:51 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1760, average loss: 0.7376
[11/15 16:00:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.13	
[11/15 16:00:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/15 16:07:09 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.7941, average train loss: 0.4869
[11/15 16:07:52 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1774, average loss: 0.6656
[11/15 16:07:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.43	
[11/15 16:07:52 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/15 16:14:10 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.7854, average train loss: 0.4100
[11/15 16:14:53 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1711, average loss: 0.7873
[11/15 16:14:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.13	
[11/15 16:14:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/15 16:21:11 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.7777, average train loss: 0.4407
[11/15 16:21:54 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1706, average loss: 1.0832
[11/15 16:21:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 69.68	
[11/15 16:21:54 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/15 16:28:11 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.7787, average train loss: 0.3789
[11/15 16:28:55 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1738, average loss: 0.7727
[11/15 16:28:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 72.00	
[11/15 16:28:55 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/15 16:35:13 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.7873, average train loss: 0.2827
[11/15 16:35:56 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1731, average loss: 0.8068
[11/15 16:35:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.25	
[11/15 16:35:56 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/15 16:42:13 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.7780, average train loss: 0.2560
[11/15 16:42:56 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1740, average loss: 1.2641
[11/15 16:42:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.72	
[11/15 16:42:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/15 16:49:14 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.7842, average train loss: 0.2641
[11/15 16:49:57 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1782, average loss: 1.0899
[11/15 16:49:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.41	
[11/15 16:49:57 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/15 16:56:15 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e+01, avg batch time: 10.7902, average train loss: 0.2304
[11/15 16:56:58 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1748, average loss: 1.0525
[11/15 16:56:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 66.76	
[11/15 16:56:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/15 17:03:16 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e+01, avg batch time: 10.7708, average train loss: 0.2344
[11/15 17:03:59 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1752, average loss: 1.0677
[11/15 17:03:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.60	
[11/15 17:03:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/15 17:10:16 visual_prompt]: Epoch 24 / 100: avg data time: 1.02e+01, avg batch time: 10.7639, average train loss: 0.1750
[11/15 17:10:59 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1776, average loss: 1.0889
[11/15 17:10:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 67.52	
[11/15 17:10:59 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/15 17:17:18 visual_prompt]: Epoch 25 / 100: avg data time: 1.03e+01, avg batch time: 10.8331, average train loss: 0.1654
[11/15 17:18:02 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1708, average loss: 1.0928
[11/15 17:18:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.06	
[11/15 17:18:02 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/15 17:24:21 visual_prompt]: Epoch 26 / 100: avg data time: 1.03e+01, avg batch time: 10.8227, average train loss: 0.1619
[11/15 17:25:04 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1683, average loss: 1.4202
[11/15 17:25:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.14	
[11/15 17:25:04 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/15 17:31:25 visual_prompt]: Epoch 27 / 100: avg data time: 1.03e+01, avg batch time: 10.8736, average train loss: 0.1295
[11/15 17:32:08 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1684, average loss: 1.2576
[11/15 17:32:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 68.62	
[11/15 17:32:08 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.0004367053692460385
[11/15 17:38:29 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.8843, average train loss: 0.1032
[11/15 17:39:13 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1767, average loss: 1.6015
[11/15 17:39:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.32	
[11/15 17:39:13 visual_prompt]: Stopping early.
[11/15 17:39:13 visual_prompt]: Rank of current process: 0. World size: 1
[11/15 17:39:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/15 17:39:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/15 17:39:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/15 17:39:13 visual_prompt]: Training with config:
[11/15 17:39:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/15 17:39:13 visual_prompt]: Loading training data...
[11/15 17:39:13 visual_prompt]: Constructing mammo-cbis dataset train...
[11/15 17:39:13 visual_prompt]: Loading validation data...
[11/15 17:39:13 visual_prompt]: Constructing mammo-cbis dataset val...
[11/15 17:39:13 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/15 17:39:14 visual_prompt]: Enable all parameters update during training
[11/15 17:39:14 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/15 17:39:14 visual_prompt]: tuned percent:100.000
[11/15 17:39:14 visual_prompt]: Device used for model: 0
[11/15 17:39:14 visual_prompt]: Setting up Evaluator...
[11/15 17:39:14 visual_prompt]: Setting up Trainer...
[11/15 17:39:14 visual_prompt]: 	Setting up the optimizer...
[11/15 17:39:14 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/15 17:45:32 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.7817, average train loss: 6.9791
[11/15 17:46:15 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1706, average loss: 6.3857
[11/15 17:46:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/15 17:46:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/15 17:52:33 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.7828, average train loss: 3.1853
[11/15 17:53:16 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1692, average loss: 0.8753
[11/15 17:53:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.69	
[11/15 17:53:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/15 17:59:34 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.7881, average train loss: 0.9599
[11/15 18:00:17 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.1746, average loss: 0.7334
[11/15 18:00:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.81	
[11/15 18:00:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/15 18:06:34 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.7680, average train loss: 0.7770
[11/15 18:07:18 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1706, average loss: 0.6492
[11/15 18:07:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.12	
[11/15 18:07:18 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/15 18:13:45 visual_prompt]: Epoch 5 / 100: avg data time: 1.05e+01, avg batch time: 11.0391, average train loss: 0.7371
[11/15 18:14:33 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1754, average loss: 0.7053
[11/15 18:14:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.47	
[11/15 18:14:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/15 18:20:57 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9742, average train loss: 0.7302
[11/15 18:21:40 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1682, average loss: 0.6638
[11/15 18:21:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.38	
[11/15 18:21:40 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/15 18:28:09 visual_prompt]: Epoch 7 / 100: avg data time: 1.06e+01, avg batch time: 11.0946, average train loss: 0.6865
[11/15 18:28:52 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1779, average loss: 0.6231
[11/15 18:28:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.25	
[11/15 18:28:52 visual_prompt]: Best epoch 7: best metric: -0.623
[11/15 18:28:52 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/15 18:35:12 visual_prompt]: Epoch 8 / 100: avg data time: 1.03e+01, avg batch time: 10.8561, average train loss: 0.6633
[11/15 18:35:59 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1747, average loss: 0.6259
[11/15 18:35:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.63	
[11/15 18:35:59 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/15 18:43:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.24e+01, avg batch time: 12.9479, average train loss: 0.6471
[11/15 18:44:27 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1779, average loss: 0.8654
[11/15 18:44:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 71.31	
[11/15 18:44:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/15 18:51:11 visual_prompt]: Epoch 10 / 100: avg data time: 1.10e+01, avg batch time: 11.5307, average train loss: 0.6220
[11/15 18:51:55 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1710, average loss: 0.8537
[11/15 18:51:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 69.46	
[11/15 18:51:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/15 18:58:21 visual_prompt]: Epoch 11 / 100: avg data time: 1.05e+01, avg batch time: 11.0228, average train loss: 0.5798
[11/15 18:59:11 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1689, average loss: 0.9848
[11/15 18:59:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.86	
[11/15 18:59:11 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/15 19:05:52 visual_prompt]: Epoch 12 / 100: avg data time: 1.09e+01, avg batch time: 11.4544, average train loss: 0.5611
[11/15 19:06:40 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1743, average loss: 0.8405
[11/15 19:06:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 73.09	
[11/15 19:06:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/15 19:13:01 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.8920, average train loss: 0.5306
[11/15 19:13:46 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1775, average loss: 0.8182
[11/15 19:13:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.86	
[11/15 19:13:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/15 19:20:18 visual_prompt]: Epoch 14 / 100: avg data time: 1.07e+01, avg batch time: 11.2032, average train loss: 0.5167
[11/15 19:21:01 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1712, average loss: 0.7376
[11/15 19:21:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.13	
[11/15 19:21:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/15 19:27:43 visual_prompt]: Epoch 15 / 100: avg data time: 1.09e+01, avg batch time: 11.4609, average train loss: 0.4869
[11/15 19:28:29 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1707, average loss: 0.6656
[11/15 19:28:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.43	
[11/15 19:28:29 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/15 19:34:59 visual_prompt]: Epoch 16 / 100: avg data time: 1.06e+01, avg batch time: 11.1488, average train loss: 0.4100
[11/15 19:35:43 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1793, average loss: 0.7873
[11/15 19:35:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.13	
[11/15 19:35:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/15 19:42:12 visual_prompt]: Epoch 17 / 100: avg data time: 1.06e+01, avg batch time: 11.1201, average train loss: 0.4407
[11/15 19:42:58 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1751, average loss: 1.0832
[11/15 19:42:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 69.68	
[11/15 19:42:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/15 19:49:26 visual_prompt]: Epoch 18 / 100: avg data time: 1.05e+01, avg batch time: 11.0665, average train loss: 0.3789
[11/15 19:50:14 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1746, average loss: 0.7727
[11/15 19:50:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 72.00	
[11/15 19:50:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/15 19:56:43 visual_prompt]: Epoch 19 / 100: avg data time: 1.06e+01, avg batch time: 11.0877, average train loss: 0.2827
[11/15 19:57:28 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1774, average loss: 0.8068
[11/15 19:57:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.25	
[11/15 19:57:28 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/15 20:03:55 visual_prompt]: Epoch 20 / 100: avg data time: 1.05e+01, avg batch time: 11.0619, average train loss: 0.2560
[11/15 20:04:44 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1729, average loss: 1.2641
[11/15 20:04:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.72	
[11/15 20:04:44 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/15 20:11:17 visual_prompt]: Epoch 21 / 100: avg data time: 1.07e+01, avg batch time: 11.2381, average train loss: 0.2641
[11/15 20:12:08 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1799, average loss: 1.0899
[11/15 20:12:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.41	
[11/15 20:12:08 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/15 20:18:36 visual_prompt]: Epoch 22 / 100: avg data time: 1.05e+01, avg batch time: 11.0594, average train loss: 0.2304
[11/15 20:19:20 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1778, average loss: 1.0525
[11/15 20:19:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 66.76	
[11/15 20:19:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/15 20:25:50 visual_prompt]: Epoch 23 / 100: avg data time: 1.06e+01, avg batch time: 11.1452, average train loss: 0.2344
[11/15 20:26:35 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1736, average loss: 1.0677
[11/15 20:26:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.60	
[11/15 20:26:35 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/15 20:33:00 visual_prompt]: Epoch 24 / 100: avg data time: 1.05e+01, avg batch time: 10.9949, average train loss: 0.1750
[11/15 20:33:46 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1742, average loss: 1.0889
[11/15 20:33:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 67.52	
[11/15 20:33:46 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/15 20:40:24 visual_prompt]: Epoch 25 / 100: avg data time: 1.08e+01, avg batch time: 11.3816, average train loss: 0.1654
[11/15 20:41:11 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1709, average loss: 1.0928
[11/15 20:41:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.06	
[11/15 20:41:11 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/15 20:47:34 visual_prompt]: Epoch 26 / 100: avg data time: 1.04e+01, avg batch time: 10.9426, average train loss: 0.1619
[11/15 20:48:18 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1778, average loss: 1.4202
[11/15 20:48:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.14	
[11/15 20:48:18 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/15 20:54:44 visual_prompt]: Epoch 27 / 100: avg data time: 1.05e+01, avg batch time: 11.0069, average train loss: 0.1295
[11/15 20:55:29 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1709, average loss: 1.2576
[11/15 20:55:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 68.62	
[11/15 20:55:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.0004367053692460385
[11/15 21:02:00 visual_prompt]: Epoch 28 / 100: avg data time: 1.06e+01, avg batch time: 11.1625, average train loss: 0.1032
[11/15 21:02:45 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1802, average loss: 1.6015
[11/15 21:02:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.32	
[11/15 21:02:45 visual_prompt]: Stopping early.
[11/15 21:02:45 visual_prompt]: Rank of current process: 0. World size: 1
[11/15 21:02:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/15 21:02:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/15 21:02:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/15 21:02:45 visual_prompt]: Training with config:
[11/15 21:02:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/15 21:02:45 visual_prompt]: Loading training data...
[11/15 21:02:45 visual_prompt]: Constructing mammo-cbis dataset train...
[11/15 21:02:45 visual_prompt]: Loading validation data...
[11/15 21:02:45 visual_prompt]: Constructing mammo-cbis dataset val...
[11/15 21:02:45 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/15 21:03:15 visual_prompt]: Enable all parameters update during training
[11/15 21:03:15 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/15 21:03:15 visual_prompt]: tuned percent:100.000
[11/15 21:03:15 visual_prompt]: Device used for model: 0
[11/15 21:03:15 visual_prompt]: Setting up Evaluator...
[11/15 21:03:15 visual_prompt]: Setting up Trainer...
[11/15 21:03:15 visual_prompt]: 	Setting up the optimizer...
[11/15 21:03:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/15 21:09:52 visual_prompt]: Epoch 1 / 100: avg data time: 1.08e+01, avg batch time: 11.3455, average train loss: 6.9791
[11/15 21:10:39 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1783, average loss: 6.3857
[11/15 21:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/15 21:10:39 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/15 21:17:04 visual_prompt]: Epoch 2 / 100: avg data time: 1.05e+01, avg batch time: 10.9889, average train loss: 3.7696
[11/15 21:17:53 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1750, average loss: 0.8737
[11/15 21:17:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 52.91	
[11/15 21:17:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/15 21:24:14 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.8760, average train loss: 0.9347
[11/15 21:24:58 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.1692, average loss: 0.7837
[11/15 21:24:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.09	
[11/15 21:24:58 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/15 21:31:16 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.7803, average train loss: 0.8079
[11/15 21:31:59 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1686, average loss: 0.7091
[11/15 21:31:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 53.89	
[11/15 21:31:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/15 21:38:16 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.7763, average train loss: 0.7853
[11/15 21:39:01 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1776, average loss: 0.7754
[11/15 21:39:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 56.30	
[11/15 21:39:01 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/15 21:46:17 visual_prompt]: Epoch 6 / 100: avg data time: 1.19e+01, avg batch time: 12.4634, average train loss: 0.7465
[11/15 21:47:02 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1766, average loss: 0.6865
[11/15 21:47:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 58.04	
[11/15 21:47:02 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/15 21:53:59 visual_prompt]: Epoch 7 / 100: avg data time: 1.14e+01, avg batch time: 11.9075, average train loss: 0.7096
[11/15 21:54:45 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1781, average loss: 0.6843
[11/15 21:54:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.42	
[11/15 21:54:45 visual_prompt]: Best epoch 7: best metric: -0.684
[11/15 21:54:45 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/15 22:01:26 visual_prompt]: Epoch 8 / 100: avg data time: 1.09e+01, avg batch time: 11.4431, average train loss: 0.7223
[11/15 22:02:10 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1753, average loss: 0.6571
[11/15 22:02:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 59.36	
[11/15 22:02:10 visual_prompt]: Best epoch 8: best metric: -0.657
[11/15 22:02:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/15 22:08:39 visual_prompt]: Epoch 9 / 100: avg data time: 1.06e+01, avg batch time: 11.1163, average train loss: 0.6881
[11/15 22:09:24 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1763, average loss: 0.7079
[11/15 22:09:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.67	
[11/15 22:09:24 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/15 22:15:53 visual_prompt]: Epoch 10 / 100: avg data time: 1.06e+01, avg batch time: 11.1214, average train loss: 0.7266
[11/15 22:16:37 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1767, average loss: 0.8433
[11/15 22:16:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 59.97	
[11/15 22:16:37 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/15 22:22:53 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.7424, average train loss: 0.6806
[11/15 22:23:36 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1788, average loss: 0.6778
[11/15 22:23:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.03	
[11/15 22:23:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/15 22:29:55 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.8227, average train loss: 0.6837
[11/15 22:30:38 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1776, average loss: 0.9075
[11/15 22:30:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.03	
[11/15 22:30:38 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/15 22:36:54 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.7440, average train loss: 0.6630
[11/15 22:37:38 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1712, average loss: 0.6550
[11/15 22:37:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 61.25	
[11/15 22:37:38 visual_prompt]: Best epoch 13: best metric: -0.655
[11/15 22:37:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/15 22:43:58 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.8331, average train loss: 0.6617
[11/15 22:44:41 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1770, average loss: 0.6443
[11/15 22:44:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 61.14	
[11/15 22:44:41 visual_prompt]: Best epoch 14: best metric: -0.644
[11/15 22:44:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/15 22:50:57 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.7539, average train loss: 0.6704
[11/15 22:51:40 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1681, average loss: 0.7420
[11/15 22:51:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.24	
[11/15 22:51:40 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/15 22:57:57 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.7455, average train loss: 0.6734
[11/15 22:58:39 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1771, average loss: 0.6462
[11/15 22:58:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 62.21	
[11/15 22:58:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/15 23:04:56 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.7389, average train loss: 0.6687
[11/15 23:05:39 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1790, average loss: 0.7409
[11/15 23:05:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.45	
[11/15 23:05:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/15 23:11:54 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.7341, average train loss: 0.6681
[11/15 23:12:38 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1798, average loss: 0.7146
[11/15 23:12:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 62.45	
[11/15 23:12:38 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/15 23:18:54 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.7442, average train loss: 0.6348
[11/15 23:19:37 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1792, average loss: 0.6495
[11/15 23:19:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 62.18	
[11/15 23:19:37 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/15 23:25:53 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.7453, average train loss: 0.6323
[11/15 23:26:36 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1687, average loss: 0.6532
[11/15 23:26:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 61.87	
[11/15 23:26:36 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/15 23:32:52 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.7492, average train loss: 0.6121
[11/15 23:33:35 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1752, average loss: 0.6540
[11/15 23:33:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 61.38	
[11/15 23:33:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/15 23:39:51 visual_prompt]: Epoch 22 / 100: avg data time: 1.02e+01, avg batch time: 10.7389, average train loss: 0.6386
[11/15 23:40:35 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1773, average loss: 0.7101
[11/15 23:40:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 60.68	
[11/15 23:40:35 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/15 23:46:50 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e+01, avg batch time: 10.7296, average train loss: 0.6404
[11/15 23:47:33 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1736, average loss: 0.7702
[11/15 23:47:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.74	
[11/15 23:47:33 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/15 23:53:49 visual_prompt]: Epoch 24 / 100: avg data time: 1.02e+01, avg batch time: 10.7431, average train loss: 0.6111
[11/15 23:54:32 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1757, average loss: 0.6646
[11/15 23:54:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 61.47	
[11/15 23:54:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/16 00:00:49 visual_prompt]: Epoch 25 / 100: avg data time: 1.02e+01, avg batch time: 10.7453, average train loss: 0.6119
[11/16 00:01:32 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1772, average loss: 0.6632
[11/16 00:01:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 60.90	
[11/16 00:01:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/16 00:07:48 visual_prompt]: Epoch 26 / 100: avg data time: 1.02e+01, avg batch time: 10.7454, average train loss: 0.6072
[11/16 00:08:31 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1698, average loss: 0.6658
[11/16 00:08:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 62.08	
[11/16 00:08:31 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/16 00:14:47 visual_prompt]: Epoch 27 / 100: avg data time: 1.02e+01, avg batch time: 10.7290, average train loss: 0.6085
[11/16 00:15:30 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1772, average loss: 0.6609
[11/16 00:15:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 62.12	
[11/16 00:15:30 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.0004367053692460385
[11/16 00:21:46 visual_prompt]: Epoch 28 / 100: avg data time: 1.02e+01, avg batch time: 10.7425, average train loss: 0.6329
[11/16 00:22:29 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1747, average loss: 0.6926
[11/16 00:22:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 60.52	
[11/16 00:22:29 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.0004311063011977723
[11/16 00:28:45 visual_prompt]: Epoch 29 / 100: avg data time: 1.02e+01, avg batch time: 10.7305, average train loss: 0.6012
[11/16 00:29:28 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1775, average loss: 0.6953
[11/16 00:29:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.48	
[11/16 00:29:28 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.00042530919606812215
[11/16 00:35:43 visual_prompt]: Epoch 30 / 100: avg data time: 1.02e+01, avg batch time: 10.7248, average train loss: 0.5986
[11/16 00:36:26 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1762, average loss: 0.7216
[11/16 00:36:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 60.23	
[11/16 00:36:26 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0004193203929064353
[11/16 00:42:42 visual_prompt]: Epoch 31 / 100: avg data time: 1.02e+01, avg batch time: 10.7297, average train loss: 0.5805
[11/16 00:43:25 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1799, average loss: 0.7046
[11/16 00:43:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 60.52	
[11/16 00:43:25 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.00041314644038104216
[11/16 00:49:41 visual_prompt]: Epoch 32 / 100: avg data time: 1.02e+01, avg batch time: 10.7496, average train loss: 0.5838
[11/16 00:50:24 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1756, average loss: 0.7187
[11/16 00:50:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 60.28	
[11/16 00:50:24 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.00040679408961838426
[11/16 00:56:40 visual_prompt]: Epoch 33 / 100: avg data time: 1.02e+01, avg batch time: 10.7422, average train loss: 0.5857
[11/16 00:57:26 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1786, average loss: 0.8567
[11/16 00:57:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 60.59	
[11/16 00:57:26 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.00040027028682075626
[11/16 01:03:42 visual_prompt]: Epoch 34 / 100: avg data time: 1.02e+01, avg batch time: 10.7363, average train loss: 0.5988
[11/16 01:04:24 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1758, average loss: 0.7641
[11/16 01:04:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 60.01	
[11/16 01:04:24 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0003935821656707359
[11/16 01:10:41 visual_prompt]: Epoch 35 / 100: avg data time: 1.02e+01, avg batch time: 10.7557, average train loss: 0.5652
[11/16 01:11:24 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1704, average loss: 0.7676
[11/16 01:11:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 59.61	
[11/16 01:11:24 visual_prompt]: Stopping early.
[11/16 01:11:24 visual_prompt]: Rank of current process: 0. World size: 1
[11/16 01:11:24 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/16 01:11:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/16 01:11:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/16 01:11:24 visual_prompt]: Training with config:
[11/16 01:11:24 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/16 01:11:24 visual_prompt]: Loading training data...
[11/16 01:11:24 visual_prompt]: Constructing mammo-cbis dataset train...
[11/16 01:11:24 visual_prompt]: Loading validation data...
[11/16 01:11:24 visual_prompt]: Constructing mammo-cbis dataset val...
[11/16 01:11:24 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/16 01:11:29 visual_prompt]: Enable all parameters update during training
[11/16 01:11:29 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/16 01:11:29 visual_prompt]: tuned percent:100.000
[11/16 01:11:30 visual_prompt]: Device used for model: 0
[11/16 01:11:30 visual_prompt]: Setting up Evaluator...
[11/16 01:11:30 visual_prompt]: Setting up Trainer...
[11/16 01:11:30 visual_prompt]: 	Setting up the optimizer...
[11/16 01:11:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/16 01:17:46 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.7349, average train loss: 6.9791
[11/16 01:18:28 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1726, average loss: 6.3857
[11/16 01:18:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/16 01:18:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/16 01:24:45 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.7533, average train loss: 2.2494
[11/16 01:25:29 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1706, average loss: 0.8412
[11/16 01:25:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 51.72	
[11/16 01:25:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/16 01:31:50 visual_prompt]: Epoch 3 / 100: avg data time: 1.03e+01, avg batch time: 10.8679, average train loss: 0.8962
[11/16 01:32:33 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1790, average loss: 0.7044
[11/16 01:32:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.92	
[11/16 01:32:33 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/16 01:38:52 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.8177, average train loss: 0.7820
[11/16 01:39:35 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1779, average loss: 0.7135
[11/16 01:39:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.72	
[11/16 01:39:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/16 01:45:49 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.6933, average train loss: 0.7455
[11/16 01:46:32 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1780, average loss: 0.7162
[11/16 01:46:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.65	
[11/16 01:46:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/16 01:52:47 visual_prompt]: Epoch 6 / 100: avg data time: 1.02e+01, avg batch time: 10.7134, average train loss: 0.6861
[11/16 01:53:30 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1769, average loss: 0.6859
[11/16 01:53:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.65	
[11/16 01:53:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/16 01:59:45 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.7123, average train loss: 0.6416
[11/16 02:00:29 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1751, average loss: 0.6250
[11/16 02:00:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 68.25	
[11/16 02:00:29 visual_prompt]: Best epoch 7: best metric: -0.625
[11/16 02:00:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/16 02:06:45 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.7352, average train loss: 0.6677
[11/16 02:07:28 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1779, average loss: 0.8172
[11/16 02:07:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.84	
[11/16 02:07:28 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/16 02:13:44 visual_prompt]: Epoch 9 / 100: avg data time: 1.02e+01, avg batch time: 10.7411, average train loss: 0.5855
[11/16 02:14:27 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1794, average loss: 0.6318
[11/16 02:14:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 68.24	
[11/16 02:14:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/16 02:20:43 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.7142, average train loss: 0.5394
[11/16 02:21:25 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1786, average loss: 0.6388
[11/16 02:21:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.99	
[11/16 02:21:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/16 02:27:40 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.6970, average train loss: 0.4939
[11/16 02:28:23 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1764, average loss: 0.8244
[11/16 02:28:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.68	
[11/16 02:28:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/16 02:34:38 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.7190, average train loss: 0.5270
[11/16 02:35:21 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1804, average loss: 0.8798
[11/16 02:35:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 69.06	
[11/16 02:35:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/16 02:41:37 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.7205, average train loss: 0.4742
[11/16 02:42:20 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1788, average loss: 0.7313
[11/16 02:42:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.39	
[11/16 02:42:20 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/16 02:48:35 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.7210, average train loss: 0.4177
[11/16 02:49:18 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1791, average loss: 0.6655
[11/16 02:49:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.63	
[11/16 02:49:18 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/16 02:55:33 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.7146, average train loss: 0.3513
[11/16 02:56:16 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1679, average loss: 0.7925
[11/16 02:56:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.87	
[11/16 02:56:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/16 03:02:31 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.7105, average train loss: 0.2968
[11/16 03:03:14 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1768, average loss: 1.1919
[11/16 03:03:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 63.99	
[11/16 03:03:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/16 03:09:30 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.7263, average train loss: 0.3106
[11/16 03:10:13 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1689, average loss: 1.0007
[11/16 03:10:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 66.76	
[11/16 03:10:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/16 03:16:29 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.7304, average train loss: 0.3144
[11/16 03:17:12 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1779, average loss: 0.9425
[11/16 03:17:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.84	
[11/16 03:17:12 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/16 03:23:27 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.7291, average train loss: 0.2037
[11/16 03:24:10 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1710, average loss: 1.0811
[11/16 03:24:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 72.10	
[11/16 03:24:10 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/16 03:30:25 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.7052, average train loss: 0.1522
[11/16 03:31:08 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1726, average loss: 1.3032
[11/16 03:31:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 64.65	
[11/16 03:31:08 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/16 03:37:23 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.7127, average train loss: 0.1307
[11/16 03:38:06 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1747, average loss: 1.3926
[11/16 03:38:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 64.94	
[11/16 03:38:06 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/16 03:44:21 visual_prompt]: Epoch 22 / 100: avg data time: 1.02e+01, avg batch time: 10.7124, average train loss: 0.0855
[11/16 03:45:04 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1777, average loss: 1.7717
[11/16 03:45:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 67.63	
[11/16 03:45:04 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/16 03:51:19 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e+01, avg batch time: 10.7015, average train loss: 0.0874
[11/16 03:52:02 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1723, average loss: 1.8961
[11/16 03:52:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 66.14	
[11/16 03:52:02 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/16 03:58:18 visual_prompt]: Epoch 24 / 100: avg data time: 1.02e+01, avg batch time: 10.7423, average train loss: 0.0807
[11/16 03:59:01 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1789, average loss: 2.6094
[11/16 03:59:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 67.83	
[11/16 03:59:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/16 04:05:16 visual_prompt]: Epoch 25 / 100: avg data time: 1.02e+01, avg batch time: 10.7239, average train loss: 0.1227
[11/16 04:05:59 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1696, average loss: 1.6174
[11/16 04:05:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 68.04	
[11/16 04:05:59 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/16 04:12:16 visual_prompt]: Epoch 26 / 100: avg data time: 1.02e+01, avg batch time: 10.7455, average train loss: 0.1395
[11/16 04:12:59 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1710, average loss: 1.4218
[11/16 04:12:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.02	
[11/16 04:12:59 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/16 04:19:14 visual_prompt]: Epoch 27 / 100: avg data time: 1.02e+01, avg batch time: 10.7184, average train loss: 0.0827
[11/16 04:19:57 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1712, average loss: 1.5908
[11/16 04:19:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.18	
[11/16 04:19:57 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/16 04:26:13 visual_prompt]: Epoch 28 / 100: avg data time: 1.02e+01, avg batch time: 10.7241, average train loss: 0.0590
[11/16 04:26:55 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1735, average loss: 1.5857
[11/16 04:26:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 67.48	
[11/16 04:26:55 visual_prompt]: Stopping early.
[11/16 04:26:56 visual_prompt]: Rank of current process: 0. World size: 1
[11/16 04:26:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/16 04:26:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/16 04:26:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/16 04:26:56 visual_prompt]: Training with config:
[11/16 04:26:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/16 04:26:56 visual_prompt]: Loading training data...
[11/16 04:26:56 visual_prompt]: Constructing mammo-cbis dataset train...
[11/16 04:26:56 visual_prompt]: Loading validation data...
[11/16 04:26:56 visual_prompt]: Constructing mammo-cbis dataset val...
[11/16 04:26:56 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/16 04:26:57 visual_prompt]: Enable all parameters update during training
[11/16 04:26:57 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/16 04:26:57 visual_prompt]: tuned percent:100.000
[11/16 04:26:57 visual_prompt]: Device used for model: 0
[11/16 04:26:57 visual_prompt]: Setting up Evaluator...
[11/16 04:26:57 visual_prompt]: Setting up Trainer...
[11/16 04:26:57 visual_prompt]: 	Setting up the optimizer...
[11/16 04:26:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/16 04:33:14 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.7408, average train loss: 6.9791
[11/16 04:33:57 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1728, average loss: 6.3857
[11/16 04:33:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/16 04:33:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/16 04:40:12 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.7213, average train loss: 2.2494
[11/16 04:40:55 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1772, average loss: 0.8412
[11/16 04:40:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 51.72	
[11/16 04:40:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/16 04:47:11 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.7327, average train loss: 0.8962
[11/16 04:47:54 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1778, average loss: 0.7044
[11/16 04:47:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.92	
[11/16 04:47:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/16 04:54:09 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.7261, average train loss: 0.7820
[11/16 04:54:52 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1729, average loss: 0.7135
[11/16 04:54:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.72	
[11/16 04:54:52 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/16 05:01:08 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.7168, average train loss: 0.7455
[11/16 05:01:51 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1676, average loss: 0.7162
[11/16 05:01:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.65	
[11/16 05:01:51 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/16 05:08:06 visual_prompt]: Epoch 6 / 100: avg data time: 1.02e+01, avg batch time: 10.7098, average train loss: 0.6861
[11/16 05:08:48 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1770, average loss: 0.6859
[11/16 05:08:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.65	
[11/16 05:08:48 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/16 05:15:04 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.7327, average train loss: 0.6416
[11/16 05:15:47 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1709, average loss: 0.6250
[11/16 05:15:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 68.25	
[11/16 05:15:47 visual_prompt]: Best epoch 7: best metric: -0.625
[11/16 05:15:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/16 05:22:03 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.7318, average train loss: 0.6677
[11/16 05:22:46 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1773, average loss: 0.8172
[11/16 05:22:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.84	
[11/16 05:22:46 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/16 05:29:05 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.8301, average train loss: 0.5855
[11/16 05:29:49 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1770, average loss: 0.6318
[11/16 05:29:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 68.24	
[11/16 05:29:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/16 05:36:08 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.8355, average train loss: 0.5394
[11/16 05:36:52 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1705, average loss: 0.6388
[11/16 05:36:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.99	
[11/16 05:36:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/16 05:43:11 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.8373, average train loss: 0.4939
[11/16 05:43:55 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1726, average loss: 0.8244
[11/16 05:43:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.68	
[11/16 05:43:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/16 05:50:11 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.7576, average train loss: 0.5270
[11/16 05:50:54 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1786, average loss: 0.8798
[11/16 05:50:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 69.06	
[11/16 05:50:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/16 05:57:10 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.7382, average train loss: 0.4742
[11/16 05:57:53 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1711, average loss: 0.7313
[11/16 05:57:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.39	
[11/16 05:57:53 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/16 06:04:09 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.7270, average train loss: 0.4177
[11/16 06:04:52 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1739, average loss: 0.6655
[11/16 06:04:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.63	
[11/16 06:04:52 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/16 06:11:10 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.7914, average train loss: 0.3513
[11/16 06:11:54 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1758, average loss: 0.7925
[11/16 06:11:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.87	
[11/16 06:11:54 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/16 06:18:13 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.8436, average train loss: 0.2968
[11/16 06:18:57 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1742, average loss: 1.1919
[11/16 06:18:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 63.99	
[11/16 06:18:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/16 06:25:16 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.8334, average train loss: 0.3106
[11/16 06:26:00 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1800, average loss: 1.0007
[11/16 06:26:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 66.76	
[11/16 06:26:00 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/16 06:32:19 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.8312, average train loss: 0.3144
[11/16 06:33:02 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1691, average loss: 0.9425
[11/16 06:33:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.84	
[11/16 06:33:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/16 06:39:23 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.8701, average train loss: 0.2037
[11/16 06:40:06 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1738, average loss: 1.0811
[11/16 06:40:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 72.10	
[11/16 06:40:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/16 06:46:26 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.8494, average train loss: 0.1522
[11/16 06:47:10 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1772, average loss: 1.3032
[11/16 06:47:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 64.65	
[11/16 06:47:10 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/16 06:53:29 visual_prompt]: Epoch 21 / 100: avg data time: 1.03e+01, avg batch time: 10.8338, average train loss: 0.1307
[11/16 06:54:13 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1756, average loss: 1.3926
[11/16 06:54:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 64.94	
[11/16 06:54:13 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/16 07:00:32 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e+01, avg batch time: 10.8309, average train loss: 0.0855
[11/16 07:01:15 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1701, average loss: 1.7717
[11/16 07:01:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 67.63	
[11/16 07:01:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/16 07:07:34 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 10.8181, average train loss: 0.0874
[11/16 07:08:17 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1694, average loss: 1.8961
[11/16 07:08:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 66.14	
[11/16 07:08:17 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/16 07:14:37 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e+01, avg batch time: 10.8502, average train loss: 0.0807
[11/16 07:15:21 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1740, average loss: 2.6094
[11/16 07:15:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 67.83	
[11/16 07:15:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/16 07:21:40 visual_prompt]: Epoch 25 / 100: avg data time: 1.03e+01, avg batch time: 10.8318, average train loss: 0.1227
[11/16 07:22:23 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1688, average loss: 1.6174
[11/16 07:22:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 68.04	
[11/16 07:22:23 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/16 07:28:40 visual_prompt]: Epoch 26 / 100: avg data time: 1.02e+01, avg batch time: 10.7586, average train loss: 0.1395
[11/16 07:29:23 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1796, average loss: 1.4218
[11/16 07:29:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.02	
[11/16 07:29:23 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/16 07:35:38 visual_prompt]: Epoch 27 / 100: avg data time: 1.02e+01, avg batch time: 10.7205, average train loss: 0.0827
[11/16 07:36:21 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1748, average loss: 1.5908
[11/16 07:36:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.18	
[11/16 07:36:21 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/16 07:42:37 visual_prompt]: Epoch 28 / 100: avg data time: 1.02e+01, avg batch time: 10.7353, average train loss: 0.0590
[11/16 07:43:20 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1787, average loss: 1.5857
[11/16 07:43:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 67.48	
[11/16 07:43:20 visual_prompt]: Stopping early.
[11/16 07:43:20 visual_prompt]: Rank of current process: 0. World size: 1
[11/16 07:43:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/16 07:43:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/16 07:43:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/16 07:43:20 visual_prompt]: Training with config:
[11/16 07:43:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/16 07:43:20 visual_prompt]: Loading training data...
[11/16 07:43:20 visual_prompt]: Constructing mammo-cbis dataset train...
[11/16 07:43:20 visual_prompt]: Loading validation data...
[11/16 07:43:20 visual_prompt]: Constructing mammo-cbis dataset val...
[11/16 07:43:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/16 07:43:22 visual_prompt]: Enable all parameters update during training
[11/16 07:43:22 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/16 07:43:22 visual_prompt]: tuned percent:100.000
[11/16 07:43:22 visual_prompt]: Device used for model: 0
[11/16 07:43:22 visual_prompt]: Setting up Evaluator...
[11/16 07:43:22 visual_prompt]: Setting up Trainer...
[11/16 07:43:22 visual_prompt]: 	Setting up the optimizer...
[11/16 07:43:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/16 07:49:38 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.7355, average train loss: 6.9791
[11/16 07:50:21 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1761, average loss: 6.3857
[11/16 07:50:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/16 07:50:21 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/16 07:56:37 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.7284, average train loss: 2.2494
[11/16 07:57:20 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1755, average loss: 0.8412
[11/16 07:57:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 51.72	
[11/16 07:57:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/16 08:03:35 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.7200, average train loss: 0.8962
[11/16 08:04:18 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1752, average loss: 0.7044
[11/16 08:04:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.92	
[11/16 08:04:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/16 08:10:33 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e+01, avg batch time: 10.7115, average train loss: 0.7820
[11/16 08:11:16 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1748, average loss: 0.7135
[11/16 08:11:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.72	
[11/16 08:11:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/16 08:17:32 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.7210, average train loss: 0.7455
[11/16 08:18:15 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1800, average loss: 0.7162
[11/16 08:18:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.65	
[11/16 08:18:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/16 08:24:31 visual_prompt]: Epoch 6 / 100: avg data time: 1.02e+01, avg batch time: 10.7403, average train loss: 0.6861
[11/16 08:25:14 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1767, average loss: 0.6859
[11/16 08:25:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.65	
[11/16 08:25:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/16 08:31:30 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.7332, average train loss: 0.6416
[11/16 08:32:13 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1691, average loss: 0.6250
[11/16 08:32:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 68.25	
[11/16 08:32:13 visual_prompt]: Best epoch 7: best metric: -0.625
[11/16 08:32:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/16 08:38:29 visual_prompt]: Epoch 8 / 100: avg data time: 1.02e+01, avg batch time: 10.7451, average train loss: 0.6677
[11/16 08:39:12 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1709, average loss: 0.8172
[11/16 08:39:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.84	
[11/16 08:39:12 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/16 08:45:28 visual_prompt]: Epoch 9 / 100: avg data time: 1.02e+01, avg batch time: 10.7272, average train loss: 0.5855
[11/16 08:46:11 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1706, average loss: 0.6318
[11/16 08:46:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 68.24	
[11/16 08:46:11 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/16 08:52:26 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e+01, avg batch time: 10.7284, average train loss: 0.5394
[11/16 08:53:10 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1761, average loss: 0.6388
[11/16 08:53:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.99	
[11/16 08:53:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/16 08:59:26 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.7341, average train loss: 0.4939
[11/16 09:00:09 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1773, average loss: 0.8244
[11/16 09:00:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.68	
[11/16 09:00:09 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/16 09:06:24 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.7283, average train loss: 0.5270
[11/16 09:07:07 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1761, average loss: 0.8798
[11/16 09:07:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 69.06	
[11/16 09:07:07 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/16 09:13:23 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.7368, average train loss: 0.4742
[11/16 09:14:06 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1761, average loss: 0.7313
[11/16 09:14:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.39	
[11/16 09:14:06 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/16 09:20:22 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.7230, average train loss: 0.4177
[11/16 09:21:05 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1733, average loss: 0.6655
[11/16 09:21:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.63	
[11/16 09:21:05 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/16 09:27:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.7652, average train loss: 0.3513
[11/16 09:28:05 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1751, average loss: 0.7925
[11/16 09:28:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.87	
[11/16 09:28:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/16 09:34:21 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.7341, average train loss: 0.2968
[11/16 09:35:04 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1813, average loss: 1.1919
[11/16 09:35:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 63.99	
[11/16 09:35:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/16 09:41:19 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.7084, average train loss: 0.3106
[11/16 09:42:02 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1712, average loss: 1.0007
[11/16 09:42:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 66.76	
[11/16 09:42:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/16 09:48:17 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.7069, average train loss: 0.3144
[11/16 09:48:59 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1780, average loss: 0.9425
[11/16 09:48:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.84	
[11/16 09:48:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/16 09:55:16 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.7595, average train loss: 0.2037
[11/16 09:55:59 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1771, average loss: 1.0811
[11/16 09:55:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 72.10	
[11/16 09:55:59 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/16 10:02:14 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.7174, average train loss: 0.1522
[11/16 10:02:57 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1767, average loss: 1.3032
[11/16 10:02:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 64.65	
[11/16 10:02:57 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/16 10:09:31 visual_prompt]: Epoch 21 / 100: avg data time: 1.07e+01, avg batch time: 11.2389, average train loss: 0.1307
[11/16 10:10:17 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1768, average loss: 1.3926
[11/16 10:10:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 64.94	
[11/16 10:10:17 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/16 10:16:55 visual_prompt]: Epoch 22 / 100: avg data time: 1.08e+01, avg batch time: 11.3844, average train loss: 0.0855
[11/16 10:17:41 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1712, average loss: 1.7717
[11/16 10:17:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 67.63	
[11/16 10:17:41 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/16 10:24:08 visual_prompt]: Epoch 23 / 100: avg data time: 1.05e+01, avg batch time: 11.0544, average train loss: 0.0874
[11/16 10:24:52 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1778, average loss: 1.8961
[11/16 10:24:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 66.14	
[11/16 10:24:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/16 10:31:11 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e+01, avg batch time: 10.8251, average train loss: 0.0807
[11/16 10:31:54 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1721, average loss: 2.6094
[11/16 10:31:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 67.83	
[11/16 10:31:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/16 10:38:13 visual_prompt]: Epoch 25 / 100: avg data time: 1.03e+01, avg batch time: 10.8174, average train loss: 0.1227
[11/16 10:38:56 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1772, average loss: 1.6174
[11/16 10:38:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 68.04	
[11/16 10:38:56 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/16 10:45:17 visual_prompt]: Epoch 26 / 100: avg data time: 1.03e+01, avg batch time: 10.8630, average train loss: 0.1395
[11/16 10:46:00 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1754, average loss: 1.4218
[11/16 10:46:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.02	
[11/16 10:46:00 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/16 10:52:20 visual_prompt]: Epoch 27 / 100: avg data time: 1.03e+01, avg batch time: 10.8412, average train loss: 0.0827
[11/16 10:53:04 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1749, average loss: 1.5908
[11/16 10:53:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.18	
[11/16 10:53:04 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/16 10:59:21 visual_prompt]: Epoch 28 / 100: avg data time: 1.02e+01, avg batch time: 10.7854, average train loss: 0.0590
[11/16 11:00:05 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1770, average loss: 1.5857
[11/16 11:00:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 67.48	
[11/16 11:00:05 visual_prompt]: Stopping early.
[11/16 11:00:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/16 11:00:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/16 11:00:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/16 11:00:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/16 11:00:05 visual_prompt]: Training with config:
[11/16 11:00:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/16 11:00:05 visual_prompt]: Loading training data...
[11/16 11:00:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/16 11:00:05 visual_prompt]: Loading validation data...
[11/16 11:00:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/16 11:00:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/16 11:00:10 visual_prompt]: Enable all parameters update during training
[11/16 11:00:10 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/16 11:00:10 visual_prompt]: tuned percent:100.000
[11/16 11:00:10 visual_prompt]: Device used for model: 0
[11/16 11:00:10 visual_prompt]: Setting up Evaluator...
[11/16 11:00:10 visual_prompt]: Setting up Trainer...
[11/16 11:00:10 visual_prompt]: 	Setting up the optimizer...
[11/16 11:00:10 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/16 11:06:26 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.7307, average train loss: 6.9791
[11/16 11:07:09 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1695, average loss: 6.3857
[11/16 11:07:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/16 11:07:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/16 11:13:24 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.7240, average train loss: 2.4083
[11/16 11:14:08 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1709, average loss: 1.1167
[11/16 11:14:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 50.01	
[11/16 11:14:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/16 11:20:24 visual_prompt]: Epoch 3 / 100: avg data time: 1.02e+01, avg batch time: 10.7370, average train loss: 1.1687
[11/16 11:21:07 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1708, average loss: 1.2126
[11/16 11:21:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 53.98	
[11/16 11:21:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/16 11:27:33 visual_prompt]: Epoch 4 / 100: avg data time: 1.05e+01, avg batch time: 11.0045, average train loss: 1.0270
[11/16 11:28:21 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.1722, average loss: 0.9335
[11/16 11:28:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 54.21	
[11/16 11:28:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/16 11:35:08 visual_prompt]: Epoch 5 / 100: avg data time: 1.11e+01, avg batch time: 11.6386, average train loss: 0.9454
[11/16 11:35:52 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1767, average loss: 0.9480
[11/16 11:35:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 51.83	
[11/16 11:35:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/16 11:42:14 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.8832, average train loss: 0.9100
[11/16 11:42:57 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1710, average loss: 0.9315
[11/16 11:42:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 52.85	
[11/16 11:42:57 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/16 11:49:19 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.8912, average train loss: 0.8703
[11/16 11:50:02 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1795, average loss: 1.0649
[11/16 11:50:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 52.85	
[11/16 11:50:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/16 11:56:24 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9006, average train loss: 0.8369
[11/16 11:57:08 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1778, average loss: 0.7743
[11/16 11:57:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.98	
[11/16 11:57:08 visual_prompt]: Best epoch 8: best metric: -0.774
[11/16 11:57:08 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/16 12:03:28 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.8699, average train loss: 0.7901
[11/16 12:04:12 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1705, average loss: 0.9431
[11/16 12:04:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 54.30	
[11/16 12:04:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/16 12:10:33 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.8796, average train loss: 0.8237
[11/16 12:11:17 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1777, average loss: 0.7581
[11/16 12:11:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 54.80	
[11/16 12:11:17 visual_prompt]: Best epoch 10: best metric: -0.758
[11/16 12:11:17 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/16 12:17:37 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.8568, average train loss: 0.7639
[11/16 12:18:21 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1730, average loss: 0.9742
[11/16 12:18:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.16	
[11/16 12:18:21 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/16 12:24:42 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.8788, average train loss: 0.7347
[11/16 12:25:25 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1760, average loss: 0.8635
[11/16 12:25:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.80	
[11/16 12:25:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/16 12:31:46 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.8830, average train loss: 0.7321
[11/16 12:32:30 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1740, average loss: 0.8714
[11/16 12:32:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.47	
[11/16 12:32:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/16 12:38:51 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.8855, average train loss: 0.7440
[11/16 12:39:35 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1746, average loss: 0.7656
[11/16 12:39:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.45	
[11/16 12:39:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/16 12:45:56 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.8867, average train loss: 0.7128
[11/16 12:46:40 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1775, average loss: 0.8723
[11/16 12:46:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.15	
[11/16 12:46:40 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/16 12:53:03 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.9282, average train loss: 0.7028
[11/16 12:53:46 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1779, average loss: 0.8690
[11/16 12:53:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.74	
[11/16 12:53:46 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/16 13:00:08 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.8976, average train loss: 0.7061
[11/16 13:00:51 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1709, average loss: 0.7900
[11/16 13:00:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 57.69	
[11/16 13:00:51 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/16 13:07:13 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8849, average train loss: 0.6954
[11/16 13:07:56 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1724, average loss: 0.8938
[11/16 13:07:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.13	
[11/16 13:07:56 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/16 13:14:18 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9027, average train loss: 0.7020
[11/16 13:15:02 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1710, average loss: 0.7740
[11/16 13:15:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.89	
[11/16 13:15:02 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/16 13:21:23 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.8884, average train loss: 0.6806
[11/16 13:22:07 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1760, average loss: 0.7090
[11/16 13:22:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 58.81	
[11/16 13:22:07 visual_prompt]: Best epoch 20: best metric: -0.709
[11/16 13:22:07 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/16 13:28:29 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.9261, average train loss: 0.6853
[11/16 13:29:13 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1799, average loss: 0.7283
[11/16 13:29:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 58.72	
[11/16 13:29:13 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/16 13:35:34 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e+01, avg batch time: 10.8630, average train loss: 0.6825
[11/16 13:36:17 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1763, average loss: 0.7366
[11/16 13:36:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.25	
[11/16 13:36:17 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/16 13:42:38 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e+01, avg batch time: 10.8754, average train loss: 0.6956
[11/16 13:43:22 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1710, average loss: 0.7761
[11/16 13:43:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.45	
[11/16 13:43:22 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/16 13:49:43 visual_prompt]: Epoch 24 / 100: avg data time: 1.04e+01, avg batch time: 10.8979, average train loss: 0.6743
[11/16 13:50:27 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1737, average loss: 0.7708
[11/16 13:50:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 59.74	
[11/16 13:50:27 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/16 13:56:49 visual_prompt]: Epoch 25 / 100: avg data time: 1.04e+01, avg batch time: 10.9042, average train loss: 0.6717
[11/16 13:57:32 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1772, average loss: 0.7443
[11/16 13:57:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.67	
[11/16 13:57:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/16 14:03:54 visual_prompt]: Epoch 26 / 100: avg data time: 1.04e+01, avg batch time: 10.8978, average train loss: 0.6576
[11/16 14:04:37 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1754, average loss: 0.8329
[11/16 14:04:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.82	
[11/16 14:04:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/16 14:10:58 visual_prompt]: Epoch 27 / 100: avg data time: 1.03e+01, avg batch time: 10.8733, average train loss: 0.6736
[11/16 14:11:42 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1708, average loss: 0.7449
[11/16 14:11:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.93	
[11/16 14:11:42 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/16 14:18:03 visual_prompt]: Epoch 28 / 100: avg data time: 1.03e+01, avg batch time: 10.8832, average train loss: 0.6646
[11/16 14:18:47 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1721, average loss: 0.8364
[11/16 14:18:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.99	
[11/16 14:18:47 visual_prompt]: Training 29 / 100 epoch, with learning rate 8.622126023955446e-05
[11/16 14:25:08 visual_prompt]: Epoch 29 / 100: avg data time: 1.04e+01, avg batch time: 10.8968, average train loss: 0.6529
[11/16 14:25:52 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1773, average loss: 0.7338
[11/16 14:25:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.29	
[11/16 14:25:52 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.506183921362443e-05
[11/16 14:32:13 visual_prompt]: Epoch 30 / 100: avg data time: 1.04e+01, avg batch time: 10.8853, average train loss: 0.6584
[11/16 14:32:57 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1767, average loss: 0.7477
[11/16 14:32:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.34	
[11/16 14:32:57 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.386407858128706e-05
[11/16 14:39:18 visual_prompt]: Epoch 31 / 100: avg data time: 1.04e+01, avg batch time: 10.8819, average train loss: 0.6417
[11/16 14:40:01 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1801, average loss: 0.7687
[11/16 14:40:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 60.70	
[11/16 14:40:01 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.262928807620843e-05
[11/16 14:46:24 visual_prompt]: Epoch 32 / 100: avg data time: 1.04e+01, avg batch time: 10.9321, average train loss: 0.6558
[11/16 14:47:08 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1764, average loss: 0.7276
[11/16 14:47:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.68	
[11/16 14:47:08 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.135881792367686e-05
[11/16 14:53:28 visual_prompt]: Epoch 33 / 100: avg data time: 1.03e+01, avg batch time: 10.8732, average train loss: 0.6629
[11/16 14:54:12 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1780, average loss: 0.7263
[11/16 14:54:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.74	
[11/16 14:54:12 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.005405736415126e-05
[11/16 15:00:33 visual_prompt]: Epoch 34 / 100: avg data time: 1.03e+01, avg batch time: 10.8795, average train loss: 0.6403
[11/16 15:01:17 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1784, average loss: 0.7662
[11/16 15:01:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.58	
[11/16 15:01:17 visual_prompt]: Training 35 / 100 epoch, with learning rate 7.871643313414718e-05
[11/16 15:07:39 visual_prompt]: Epoch 35 / 100: avg data time: 1.04e+01, avg batch time: 10.9057, average train loss: 0.6451
[11/16 15:08:22 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1710, average loss: 0.7407
[11/16 15:08:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 60.92	
[11/16 15:08:22 visual_prompt]: Training 36 / 100 epoch, with learning rate 7.734740790612136e-05
[11/16 15:14:45 visual_prompt]: Epoch 36 / 100: avg data time: 1.04e+01, avg batch time: 10.9128, average train loss: 0.6444
[11/16 15:15:28 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1804, average loss: 0.7471
[11/16 15:15:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 60.91	
[11/16 15:15:28 visual_prompt]: Training 37 / 100 epoch, with learning rate 7.594847868906076e-05
[11/16 15:21:49 visual_prompt]: Epoch 37 / 100: avg data time: 1.04e+01, avg batch time: 10.8839, average train loss: 0.6578
[11/16 15:22:33 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1773, average loss: 0.7204
[11/16 15:22:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 61.28	
[11/16 15:22:33 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.452117519152542e-05
[11/16 15:28:55 visual_prompt]: Epoch 38 / 100: avg data time: 1.04e+01, avg batch time: 10.9143, average train loss: 0.6531
[11/16 15:29:39 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1748, average loss: 0.7350
[11/16 15:29:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.49	
[11/16 15:29:39 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.30670581489344e-05
[11/16 15:36:02 visual_prompt]: Epoch 39 / 100: avg data time: 1.04e+01, avg batch time: 10.9488, average train loss: 0.6398
[11/16 15:36:46 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1680, average loss: 0.7296
[11/16 15:36:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 61.56	
[11/16 15:36:46 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.158771761692464e-05
[11/16 15:43:08 visual_prompt]: Epoch 40 / 100: avg data time: 1.04e+01, avg batch time: 10.8989, average train loss: 0.6422
[11/16 15:43:51 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1802, average loss: 0.7616
[11/16 15:43:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.53	
[11/16 15:43:51 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.008477123264848e-05
[11/16 15:50:13 visual_prompt]: Epoch 41 / 100: avg data time: 1.04e+01, avg batch time: 10.9089, average train loss: 0.6466
[11/16 15:50:57 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1785, average loss: 0.6987
[11/16 15:50:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.51	
[11/16 15:50:57 visual_prompt]: Best epoch 41: best metric: -0.699
[11/16 15:50:57 visual_prompt]: Training 42 / 100 epoch, with learning rate 6.855986244591104e-05
[11/16 15:57:29 visual_prompt]: Epoch 42 / 100: avg data time: 1.07e+01, avg batch time: 11.1978, average train loss: 0.6394
[11/16 15:58:19 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1765, average loss: 0.7204
[11/16 15:58:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 61.66	
[11/16 15:58:19 visual_prompt]: Training 43 / 100 epoch, with learning rate 6.701465872208216e-05
[11/16 16:04:40 visual_prompt]: Epoch 43 / 100: avg data time: 1.04e+01, avg batch time: 10.8926, average train loss: 0.6378
[11/16 16:05:24 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1746, average loss: 0.8275
[11/16 16:05:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.53	
[11/16 16:05:24 visual_prompt]: Training 44 / 100 epoch, with learning rate 6.545084971874738e-05
[11/16 16:11:45 visual_prompt]: Epoch 44 / 100: avg data time: 1.04e+01, avg batch time: 10.8932, average train loss: 0.6391
[11/16 16:12:31 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1770, average loss: 0.6979
[11/16 16:12:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 61.80	
[11/16 16:12:31 visual_prompt]: Best epoch 44: best metric: -0.698
[11/16 16:12:31 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.387014543809223e-05
[11/16 16:18:59 visual_prompt]: Epoch 45 / 100: avg data time: 1.05e+01, avg batch time: 11.0771, average train loss: 0.6323
[11/16 16:19:42 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1757, average loss: 0.7394
[11/16 16:19:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 61.87	
[11/16 16:19:42 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.227427435703997e-05
[11/16 16:26:08 visual_prompt]: Epoch 46 / 100: avg data time: 1.05e+01, avg batch time: 11.0056, average train loss: 0.6303
[11/16 16:26:52 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1695, average loss: 0.7813
[11/16 16:26:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 61.64	
[11/16 16:26:52 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.066498153718735e-05
[11/16 16:33:15 visual_prompt]: Epoch 47 / 100: avg data time: 1.04e+01, avg batch time: 10.9419, average train loss: 0.6243
[11/16 16:33:58 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1698, average loss: 0.7295
[11/16 16:33:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 61.66	
[11/16 16:33:58 visual_prompt]: Training 48 / 100 epoch, with learning rate 5.90440267166055e-05
[11/16 16:40:23 visual_prompt]: Epoch 48 / 100: avg data time: 1.04e+01, avg batch time: 10.9734, average train loss: 0.6425
[11/16 16:41:07 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1783, average loss: 0.8135
[11/16 16:41:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 61.95	
[11/16 16:41:07 visual_prompt]: Training 49 / 100 epoch, with learning rate 5.74131823855921e-05
[11/16 16:47:32 visual_prompt]: Epoch 49 / 100: avg data time: 1.05e+01, avg batch time: 10.9927, average train loss: 0.6217
[11/16 16:48:18 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1769, average loss: 0.7816
[11/16 16:48:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 61.92	
[11/16 16:48:18 visual_prompt]: Training 50 / 100 epoch, with learning rate 5.577423184847932e-05
[11/16 16:54:40 visual_prompt]: Epoch 50 / 100: avg data time: 1.04e+01, avg batch time: 10.9132, average train loss: 0.6314
[11/16 16:55:24 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1794, average loss: 0.7543
[11/16 16:55:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 62.14	
[11/16 16:55:24 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.4128967273616625e-05
[11/16 17:01:52 visual_prompt]: Epoch 51 / 100: avg data time: 1.05e+01, avg batch time: 11.0725, average train loss: 0.6290
[11/16 17:02:35 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1751, average loss: 0.8044
[11/16 17:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 62.13	
[11/16 17:02:35 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.247918773366112e-05
[11/16 17:09:06 visual_prompt]: Epoch 52 / 100: avg data time: 1.06e+01, avg batch time: 11.1491, average train loss: 0.6260
[11/16 17:09:49 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1730, average loss: 0.7175
[11/16 17:09:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 62.01	
[11/16 17:09:49 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.0826697238317935e-05
[11/16 17:16:15 visual_prompt]: Epoch 53 / 100: avg data time: 1.05e+01, avg batch time: 11.0003, average train loss: 0.6256
[11/16 17:16:58 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1751, average loss: 0.7866
[11/16 17:16:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 61.99	
[11/16 17:16:58 visual_prompt]: Training 54 / 100 epoch, with learning rate 4.917330276168208e-05
[11/16 17:23:22 visual_prompt]: Epoch 54 / 100: avg data time: 1.04e+01, avg batch time: 10.9459, average train loss: 0.6331
[11/16 17:24:07 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1701, average loss: 0.7685
[11/16 17:24:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.09	
[11/16 17:24:07 visual_prompt]: Training 55 / 100 epoch, with learning rate 4.7520812266338885e-05
[11/16 17:30:31 visual_prompt]: Epoch 55 / 100: avg data time: 1.04e+01, avg batch time: 10.9656, average train loss: 0.6197
[11/16 17:31:14 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1749, average loss: 0.7521
[11/16 17:31:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 61.97	
[11/16 17:31:14 visual_prompt]: Training 56 / 100 epoch, with learning rate 4.5871032726383386e-05
[11/16 17:37:36 visual_prompt]: Epoch 56 / 100: avg data time: 1.04e+01, avg batch time: 10.9090, average train loss: 0.6273
[11/16 17:38:20 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1750, average loss: 0.7200
[11/16 17:38:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 62.05	
[11/16 17:38:20 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.4225768151520694e-05
[11/16 17:44:43 visual_prompt]: Epoch 57 / 100: avg data time: 1.04e+01, avg batch time: 10.9325, average train loss: 0.6292
[11/16 17:45:27 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1763, average loss: 0.7499
[11/16 17:45:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 61.93	
[11/16 17:45:27 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.2586817614407895e-05
[11/16 17:51:50 visual_prompt]: Epoch 58 / 100: avg data time: 1.04e+01, avg batch time: 10.9515, average train loss: 0.6267
[11/16 17:52:34 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1702, average loss: 0.7138
[11/16 17:52:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 62.06	
[11/16 17:52:34 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.095597328339452e-05
[11/16 17:58:58 visual_prompt]: Epoch 59 / 100: avg data time: 1.05e+01, avg batch time: 10.9840, average train loss: 0.6353
[11/16 17:59:42 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1779, average loss: 0.7157
[11/16 17:59:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 62.21	
[11/16 17:59:42 visual_prompt]: Training 60 / 100 epoch, with learning rate 3.933501846281267e-05
[11/16 18:06:04 visual_prompt]: Epoch 60 / 100: avg data time: 1.04e+01, avg batch time: 10.9040, average train loss: 0.6290
[11/16 18:06:48 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1729, average loss: 0.7546
[11/16 18:06:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 62.24	
[11/16 18:06:48 visual_prompt]: Training 61 / 100 epoch, with learning rate 3.772572564296005e-05
[11/16 18:13:10 visual_prompt]: Epoch 61 / 100: avg data time: 1.04e+01, avg batch time: 10.9124, average train loss: 0.6236
[11/16 18:13:54 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1767, average loss: 0.7765
[11/16 18:13:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 62.27	
[11/16 18:13:54 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.612985456190778e-05
[11/16 18:20:15 visual_prompt]: Epoch 62 / 100: avg data time: 1.04e+01, avg batch time: 10.8845, average train loss: 0.6294
[11/16 18:20:59 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1800, average loss: 0.7792
[11/16 18:20:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 62.29	
[11/16 18:20:59 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.4549150281252636e-05
[11/16 18:27:20 visual_prompt]: Epoch 63 / 100: avg data time: 1.04e+01, avg batch time: 10.8991, average train loss: 0.6183
[11/16 18:28:04 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1713, average loss: 0.7533
[11/16 18:28:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 62.25	
[11/16 18:28:04 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.298534127791785e-05
[11/16 18:34:25 visual_prompt]: Epoch 64 / 100: avg data time: 1.03e+01, avg batch time: 10.8729, average train loss: 0.6167
[11/16 18:35:08 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1732, average loss: 0.7183
[11/16 18:35:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 62.30	
[11/16 18:35:08 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.144013755408895e-05
[11/16 18:41:30 visual_prompt]: Epoch 65 / 100: avg data time: 1.04e+01, avg batch time: 10.8867, average train loss: 0.6169
[11/16 18:42:13 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1702, average loss: 0.7539
[11/16 18:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.37	
[11/16 18:42:13 visual_prompt]: Stopping early.
[11/16 18:42:13 visual_prompt]: Rank of current process: 0. World size: 1
[11/16 18:42:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/16 18:42:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/16 18:42:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/16 18:42:13 visual_prompt]: Training with config:
[11/16 18:42:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/test/seed9805/lrNone_wdNone/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 9805, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': None, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': None, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/16 18:42:13 visual_prompt]: Loading training data...
[11/16 18:42:13 visual_prompt]: Constructing mammo-cbis dataset train...
[11/16 18:42:13 visual_prompt]: Loading validation data...
[11/16 18:42:13 visual_prompt]: Constructing mammo-cbis dataset val...
[11/16 18:42:14 visual_prompt]: Loading test data...
[11/16 18:42:14 visual_prompt]: Constructing mammo-cbis dataset test...
[11/16 18:42:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/16 18:42:16 visual_prompt]: Enable all parameters update during training
[11/16 18:42:16 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/16 18:42:16 visual_prompt]: tuned percent:100.000
[11/16 18:42:16 visual_prompt]: Device used for model: 0
[11/16 18:42:16 visual_prompt]: Setting up Evaluator...
[11/16 18:42:16 visual_prompt]: Setting up Trainer...
[11/16 18:42:16 visual_prompt]: 	Setting up the optimizer...
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 99, in <module>
    main(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 94, in main
    train(cfg, args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 43, in train
    trainer = Trainer(cfg, model, evaluator, cur_device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/engine/trainer.py", line 46, in __init__
    self.optimizer = make_optimizer([self.model], cfg.SOLVER)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/solver/optimizer.py", line 36, in make_optimizer
    if train_params.WEIGHT_DECAY > 0:
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: '>' not supported between instances of 'NoneType' and 'int'
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

[09/16 07:43:53 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 07:43:53 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 07:43:53 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed42'], train_type='')
[09/16 07:43:53 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 07:43:53 visual_prompt]: Training with config:
[09/16 07:43:53 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-svhn',
          'NO_TEST': False,
          'NUMBER_CLASSES': 10,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed42/vtab-svhn/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 07:43:53 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 07:43:54.132162: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 07:43:54.308783: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 07:43:59.598048: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 07:43:59.598167: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 07:43:59.598183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 07:44:07.338811: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 07:44:07.338924: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 07:44:07.338939: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 07:44:07 visual_prompt]: Constructing vtab-svhn dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
2023-09-16 07:44:07.485914: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split train[:800]+train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 07:44:10 visual_prompt]: Number of images: 1000
[09/16 07:44:10 visual_prompt]: Number of classes: 10 / 10
[09/16 07:44:10 visual_prompt]: Loading validation data...
[09/16 07:44:10 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 07:44:10 visual_prompt]: Number of images: 200
[09/16 07:44:10 visual_prompt]: Number of classes: 10 / 10
[09/16 07:44:10 visual_prompt]: Loading test data...
[09/16 07:44:10 visual_prompt]: Constructing vtab-svhn dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split test, from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 07:44:44 visual_prompt]: Number of images: 26032
[09/16 07:44:44 visual_prompt]: Number of classes: 10 / 10
[09/16 07:44:44 visual_prompt]: Constructing models...
[09/16 07:44:47 visual_prompt]: Total Parameters: 86727946	 Gradient Parameters: 929290
[09/16 07:44:47 visual_prompt]: tuned percent:1.072
[09/16 07:44:49 visual_prompt]: Device used for model: 0
[09/16 07:44:49 visual_prompt]: Setting up Evalutator...
[09/16 07:44:49 visual_prompt]: Setting up Trainer...
[09/16 07:44:49 visual_prompt]: 	Setting up the optimizer...
[09/16 07:44:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 07:45:01 visual_prompt]: Epoch 1 / 100: avg data time: 1.47e-01, avg batch time: 0.6368, average train loss: 2.5931
[09/16 07:45:06 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1417, average loss: 2.5459
[09/16 07:45:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 15.00	top5: 56.00	
[09/16 07:45:27 visual_prompt]: 	Test 100/407. loss: 2.922, 0.1911 s / batch. (data: 1.05e-02)max mem: 17.22449 GB 
[09/16 07:45:47 visual_prompt]: 	Test 200/407. loss: 2.733, 0.1818 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 07:46:06 visual_prompt]: 	Test 300/407. loss: 2.868, 0.1962 s / batch. (data: 1.45e-02)max mem: 17.22449 GB 
[09/16 07:46:26 visual_prompt]: 	Test 400/407. loss: 2.715, 0.1837 s / batch. (data: 3.34e-05)max mem: 17.22449 GB 
[09/16 07:46:29 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1931, average loss: 2.6825
[09/16 07:46:29 visual_prompt]: Classification results with test_vtab-svhn: top1: 12.12	top5: 54.22	
[09/16 07:46:29 visual_prompt]: Best epoch 1: best metric: 0.150
[09/16 07:46:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 07:46:40 visual_prompt]: Epoch 2 / 100: avg data time: 1.47e-01, avg batch time: 0.5529, average train loss: 2.9775
[09/16 07:46:44 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1422, average loss: 2.7181
[09/16 07:46:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/16 07:47:06 visual_prompt]: 	Test 100/407. loss: 2.848, 0.1914 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 07:47:25 visual_prompt]: 	Test 200/407. loss: 3.023, 0.2010 s / batch. (data: 1.92e-02)max mem: 17.22449 GB 
[09/16 07:47:45 visual_prompt]: 	Test 300/407. loss: 2.586, 0.1964 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 07:48:04 visual_prompt]: 	Test 400/407. loss: 2.798, 0.1942 s / batch. (data: 3.84e-05)max mem: 17.22449 GB 
[09/16 07:48:08 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1932, average loss: 2.7636
[09/16 07:48:08 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 63.51	
[09/16 07:48:08 visual_prompt]: Best epoch 2: best metric: 0.230
[09/16 07:48:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 07:48:18 visual_prompt]: Epoch 3 / 100: avg data time: 1.47e-01, avg batch time: 0.5509, average train loss: 2.5613
[09/16 07:48:23 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1427, average loss: 2.5932
[09/16 07:48:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 50.50	
[09/16 07:48:45 visual_prompt]: 	Test 100/407. loss: 2.801, 0.1824 s / batch. (data: 8.27e-05)max mem: 17.22449 GB 
[09/16 07:49:04 visual_prompt]: 	Test 200/407. loss: 2.731, 0.1827 s / batch. (data: 9.27e-05)max mem: 17.22449 GB 
[09/16 07:49:23 visual_prompt]: 	Test 300/407. loss: 2.766, 0.1999 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 07:49:43 visual_prompt]: 	Test 400/407. loss: 2.766, 0.1911 s / batch. (data: 3.39e-05)max mem: 17.22449 GB 
[09/16 07:49:46 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1938, average loss: 2.6639
[09/16 07:49:47 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 46.55	
[09/16 07:49:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 07:49:57 visual_prompt]: Epoch 4 / 100: avg data time: 1.60e-01, avg batch time: 0.5618, average train loss: 2.5420
[09/16 07:50:02 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1424, average loss: 2.3953
[09/16 07:50:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/16 07:50:23 visual_prompt]: 	Test 100/407. loss: 2.618, 0.1819 s / batch. (data: 1.08e-04)max mem: 17.22449 GB 
[09/16 07:50:43 visual_prompt]: 	Test 200/407. loss: 2.550, 0.1824 s / batch. (data: 1.38e-04)max mem: 17.22449 GB 
[09/16 07:51:02 visual_prompt]: 	Test 300/407. loss: 2.395, 0.1826 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 07:51:22 visual_prompt]: 	Test 400/407. loss: 2.327, 0.1821 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 07:51:25 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1941, average loss: 2.4109
[09/16 07:51:25 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 64.04	
[09/16 07:51:25 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 07:51:36 visual_prompt]: Epoch 5 / 100: avg data time: 1.42e-01, avg batch time: 0.5447, average train loss: 2.5804
[09/16 07:51:40 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1426, average loss: 2.3444
[09/16 07:51:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/16 07:52:02 visual_prompt]: 	Test 100/407. loss: 2.496, 0.1822 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 07:52:21 visual_prompt]: 	Test 200/407. loss: 2.573, 0.1988 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 07:52:41 visual_prompt]: 	Test 300/407. loss: 2.229, 0.1825 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 07:53:01 visual_prompt]: 	Test 400/407. loss: 2.392, 0.1821 s / batch. (data: 2.79e-05)max mem: 17.22449 GB 
[09/16 07:53:04 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1942, average loss: 2.3544
[09/16 07:53:04 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 58.98	
[09/16 07:53:04 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 07:53:15 visual_prompt]: Epoch 6 / 100: avg data time: 1.56e-01, avg batch time: 0.5581, average train loss: 2.6004
[09/16 07:53:19 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1425, average loss: 2.9080
[09/16 07:53:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 47.50	
[09/16 07:53:41 visual_prompt]: 	Test 100/407. loss: 2.793, 0.1819 s / batch. (data: 8.77e-05)max mem: 17.22449 GB 
[09/16 07:54:00 visual_prompt]: 	Test 200/407. loss: 2.859, 0.1912 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 07:54:20 visual_prompt]: 	Test 300/407. loss: 2.809, 0.1977 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 07:54:40 visual_prompt]: 	Test 400/407. loss: 2.720, 0.1830 s / batch. (data: 3.60e-05)max mem: 17.22449 GB 
[09/16 07:54:43 visual_prompt]: Inference (test):avg data time: 6.72e-03, avg batch time: 0.1938, average loss: 2.8153
[09/16 07:54:43 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 51.99	
[09/16 07:54:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 07:54:54 visual_prompt]: Epoch 7 / 100: avg data time: 1.58e-01, avg batch time: 0.5596, average train loss: 3.0564
[09/16 07:54:58 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1430, average loss: 2.9815
[09/16 07:54:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 49.00	
[09/16 07:55:20 visual_prompt]: 	Test 100/407. loss: 2.949, 0.2038 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 07:55:40 visual_prompt]: 	Test 200/407. loss: 2.814, 0.1967 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 07:55:59 visual_prompt]: 	Test 300/407. loss: 2.873, 0.1986 s / batch. (data: 1.12e-02)max mem: 17.22449 GB 
[09/16 07:56:18 visual_prompt]: 	Test 400/407. loss: 2.640, 0.1824 s / batch. (data: 4.12e-05)max mem: 17.22449 GB 
[09/16 07:56:22 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1933, average loss: 2.9081
[09/16 07:56:22 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.09	top5: 52.05	
[09/16 07:56:22 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 07:56:32 visual_prompt]: Epoch 8 / 100: avg data time: 1.64e-01, avg batch time: 0.5645, average train loss: 3.3639
[09/16 07:56:37 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1426, average loss: 3.4764
[09/16 07:56:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.50	
[09/16 07:56:59 visual_prompt]: 	Test 100/407. loss: 3.378, 0.1956 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 07:57:18 visual_prompt]: 	Test 200/407. loss: 3.669, 0.1949 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 07:57:38 visual_prompt]: 	Test 300/407. loss: 3.443, 0.1829 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 07:57:57 visual_prompt]: 	Test 400/407. loss: 3.590, 0.1942 s / batch. (data: 3.62e-05)max mem: 17.22449 GB 
[09/16 07:58:00 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1934, average loss: 3.4812
[09/16 07:58:00 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.76	top5: 58.67	
[09/16 07:58:00 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 07:58:11 visual_prompt]: Epoch 9 / 100: avg data time: 1.54e-01, avg batch time: 0.5596, average train loss: 5.6048
[09/16 07:58:15 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1424, average loss: 7.2314
[09/16 07:58:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.50	
[09/16 07:58:37 visual_prompt]: 	Test 100/407. loss: 6.875, 0.2035 s / batch. (data: 1.11e-02)max mem: 17.22449 GB 
[09/16 07:58:57 visual_prompt]: 	Test 200/407. loss: 7.384, 0.1956 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 07:59:16 visual_prompt]: 	Test 300/407. loss: 7.161, 0.1887 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 07:59:36 visual_prompt]: 	Test 400/407. loss: 7.207, 0.1829 s / batch. (data: 3.84e-05)max mem: 17.22449 GB 
[09/16 07:59:39 visual_prompt]: Inference (test):avg data time: 7.11e-03, avg batch time: 0.1932, average loss: 7.1209
[09/16 07:59:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.90	top5: 55.79	
[09/16 07:59:39 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 07:59:49 visual_prompt]: Epoch 10 / 100: avg data time: 1.46e-01, avg batch time: 0.5482, average train loss: 16.9552
[09/16 07:59:54 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1426, average loss: 24.8077
[09/16 07:59:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/16 08:00:15 visual_prompt]: 	Test 100/407. loss: 28.611, 0.1827 s / batch. (data: 1.41e-04)max mem: 17.22449 GB 
[09/16 08:00:35 visual_prompt]: 	Test 200/407. loss: 26.663, 0.1830 s / batch. (data: 1.01e-04)max mem: 17.22449 GB 
[09/16 08:00:54 visual_prompt]: 	Test 300/407. loss: 24.748, 0.1969 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 08:01:14 visual_prompt]: 	Test 400/407. loss: 25.370, 0.1826 s / batch. (data: 3.81e-05)max mem: 17.22449 GB 
[09/16 08:01:17 visual_prompt]: Inference (test):avg data time: 6.83e-03, avg batch time: 0.1936, average loss: 24.5701
[09/16 08:01:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 62.67	
[09/16 08:01:17 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 08:01:28 visual_prompt]: Epoch 11 / 100: avg data time: 1.53e-01, avg batch time: 0.5548, average train loss: 16.9343
[09/16 08:01:32 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1496, average loss: 16.8531
[09/16 08:01:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.00	
[09/16 08:01:54 visual_prompt]: 	Test 100/407. loss: 16.958, 0.1909 s / batch. (data: 1.14e-04)max mem: 17.22449 GB 
[09/16 08:02:14 visual_prompt]: 	Test 200/407. loss: 17.331, 0.1973 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 08:02:33 visual_prompt]: 	Test 300/407. loss: 17.577, 0.1959 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 08:02:53 visual_prompt]: 	Test 400/407. loss: 18.275, 0.1827 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 08:02:56 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1941, average loss: 17.4735
[09/16 08:02:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.38	top5: 54.48	
[09/16 08:02:56 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 08:03:07 visual_prompt]: Epoch 12 / 100: avg data time: 1.55e-01, avg batch time: 0.5572, average train loss: 17.1122
[09/16 08:03:12 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1424, average loss: 15.5663
[09/16 08:03:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 53.50	
[09/16 08:03:34 visual_prompt]: 	Test 100/407. loss: 16.157, 0.1823 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 08:03:53 visual_prompt]: 	Test 200/407. loss: 15.534, 0.1997 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 08:04:12 visual_prompt]: 	Test 300/407. loss: 17.601, 0.1900 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 08:04:32 visual_prompt]: 	Test 400/407. loss: 17.289, 0.1820 s / batch. (data: 3.36e-05)max mem: 17.22449 GB 
[09/16 08:04:35 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1941, average loss: 15.7284
[09/16 08:04:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.70	top5: 51.51	
[09/16 08:04:35 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 08:04:46 visual_prompt]: Epoch 13 / 100: avg data time: 1.55e-01, avg batch time: 0.5553, average train loss: 15.7417
[09/16 08:04:51 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1463, average loss: 19.0948
[09/16 08:04:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 54.00	
[09/16 08:05:12 visual_prompt]: 	Test 100/407. loss: 20.109, 0.1816 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 08:05:32 visual_prompt]: 	Test 200/407. loss: 19.233, 0.1832 s / batch. (data: 9.35e-05)max mem: 17.22449 GB 
[09/16 08:05:51 visual_prompt]: 	Test 300/407. loss: 19.392, 0.1959 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 08:06:11 visual_prompt]: 	Test 400/407. loss: 20.787, 0.1825 s / batch. (data: 3.31e-05)max mem: 17.22449 GB 
[09/16 08:06:14 visual_prompt]: Inference (test):avg data time: 6.88e-03, avg batch time: 0.1933, average loss: 18.9573
[09/16 08:06:14 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.38	top5: 49.86	
[09/16 08:06:14 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 08:06:25 visual_prompt]: Epoch 14 / 100: avg data time: 1.59e-01, avg batch time: 0.5582, average train loss: 17.9778
[09/16 08:06:29 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1424, average loss: 13.9450
[09/16 08:06:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/16 08:06:51 visual_prompt]: 	Test 100/407. loss: 15.008, 0.2088 s / batch. (data: 2.70e-02)max mem: 17.22449 GB 
[09/16 08:07:10 visual_prompt]: 	Test 200/407. loss: 16.803, 0.1823 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 08:07:30 visual_prompt]: 	Test 300/407. loss: 12.888, 0.1958 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 08:07:49 visual_prompt]: 	Test 400/407. loss: 15.422, 0.1822 s / batch. (data: 3.79e-05)max mem: 17.22449 GB 
[09/16 08:07:52 visual_prompt]: Inference (test):avg data time: 8.37e-03, avg batch time: 0.1933, average loss: 14.7599
[09/16 08:07:52 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 50.47	
[09/16 08:07:52 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 08:08:03 visual_prompt]: Epoch 15 / 100: avg data time: 1.62e-01, avg batch time: 0.5634, average train loss: 15.2416
[09/16 08:08:08 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1426, average loss: 9.2970
[09/16 08:08:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 57.50	
[09/16 08:08:30 visual_prompt]: 	Test 100/407. loss: 10.024, 0.1827 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 08:08:49 visual_prompt]: 	Test 200/407. loss: 9.704, 0.1856 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 08:09:08 visual_prompt]: 	Test 300/407. loss: 8.834, 0.1830 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 08:09:28 visual_prompt]: 	Test 400/407. loss: 8.604, 0.1835 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 08:09:31 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1939, average loss: 9.2062
[09/16 08:09:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.59	top5: 58.94	
[09/16 08:09:32 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 08:09:42 visual_prompt]: Epoch 16 / 100: avg data time: 1.40e-01, avg batch time: 0.5464, average train loss: 8.2421
[09/16 08:09:46 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1454, average loss: 7.9869
[09/16 08:09:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.00	
[09/16 08:10:08 visual_prompt]: 	Test 100/407. loss: 7.983, 0.2076 s / batch. (data: 2.61e-02)max mem: 17.22449 GB 
[09/16 08:10:28 visual_prompt]: 	Test 200/407. loss: 9.392, 0.1822 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 08:10:47 visual_prompt]: 	Test 300/407. loss: 6.323, 0.2145 s / batch. (data: 3.27e-02)max mem: 17.22449 GB 
[09/16 08:11:07 visual_prompt]: 	Test 400/407. loss: 8.688, 0.1823 s / batch. (data: 2.67e-05)max mem: 17.22449 GB 
[09/16 08:11:10 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1940, average loss: 8.0972
[09/16 08:11:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.38	top5: 58.65	
[09/16 08:11:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 08:11:21 visual_prompt]: Epoch 17 / 100: avg data time: 1.55e-01, avg batch time: 0.5576, average train loss: 8.1200
[09/16 08:11:25 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1426, average loss: 8.6381
[09/16 08:11:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 54.50	
[09/16 08:11:47 visual_prompt]: 	Test 100/407. loss: 10.540, 0.2178 s / batch. (data: 1.70e-02)max mem: 17.22449 GB 
[09/16 08:12:06 visual_prompt]: 	Test 200/407. loss: 10.151, 0.1953 s / batch. (data: 1.31e-02)max mem: 17.22449 GB 
[09/16 08:12:26 visual_prompt]: 	Test 300/407. loss: 9.488, 0.1960 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 08:12:45 visual_prompt]: 	Test 400/407. loss: 8.789, 0.1833 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 08:12:49 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1932, average loss: 9.0373
[09/16 08:12:49 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 52.16	
[09/16 08:12:49 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 08:12:59 visual_prompt]: Epoch 18 / 100: avg data time: 1.43e-01, avg batch time: 0.5453, average train loss: 7.6339
[09/16 08:13:04 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1424, average loss: 7.8931
[09/16 08:13:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/16 08:13:25 visual_prompt]: 	Test 100/407. loss: 9.118, 0.1951 s / batch. (data: 1.30e-02)max mem: 17.22449 GB 
[09/16 08:13:45 visual_prompt]: 	Test 200/407. loss: 8.883, 0.1967 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 08:14:04 visual_prompt]: 	Test 300/407. loss: 8.297, 0.1825 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 08:14:24 visual_prompt]: 	Test 400/407. loss: 8.601, 0.1830 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 08:14:27 visual_prompt]: Inference (test):avg data time: 8.64e-03, avg batch time: 0.1940, average loss: 8.2452
[09/16 08:14:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 52.41	
[09/16 08:14:27 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 08:14:37 visual_prompt]: Epoch 19 / 100: avg data time: 1.41e-01, avg batch time: 0.5442, average train loss: 5.5115
[09/16 08:14:42 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1426, average loss: 4.1240
[09/16 08:14:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/16 08:15:04 visual_prompt]: 	Test 100/407. loss: 5.198, 0.1965 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 08:15:23 visual_prompt]: 	Test 200/407. loss: 5.053, 0.2083 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 08:15:43 visual_prompt]: 	Test 300/407. loss: 4.469, 0.1822 s / batch. (data: 1.60e-04)max mem: 17.22449 GB 
[09/16 08:16:02 visual_prompt]: 	Test 400/407. loss: 4.471, 0.1828 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 08:16:05 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1933, average loss: 4.4905
[09/16 08:16:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 53.54	
[09/16 08:16:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 08:16:16 visual_prompt]: Epoch 20 / 100: avg data time: 1.56e-01, avg batch time: 0.5580, average train loss: 3.6266
[09/16 08:16:21 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1424, average loss: 2.5963
[09/16 08:16:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 58.50	
[09/16 08:16:42 visual_prompt]: 	Test 100/407. loss: 3.019, 0.1952 s / batch. (data: 1.32e-02)max mem: 17.22449 GB 
[09/16 08:17:02 visual_prompt]: 	Test 200/407. loss: 2.902, 0.2004 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 08:17:21 visual_prompt]: 	Test 300/407. loss: 2.655, 0.1954 s / batch. (data: 1.52e-04)max mem: 17.22449 GB 
[09/16 08:17:41 visual_prompt]: 	Test 400/407. loss: 2.694, 0.1832 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 08:17:44 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1935, average loss: 2.7103
[09/16 08:17:44 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.59	top5: 52.71	
[09/16 08:17:44 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 08:17:55 visual_prompt]: Epoch 21 / 100: avg data time: 1.52e-01, avg batch time: 0.5534, average train loss: 2.9243
[09/16 08:17:59 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1424, average loss: 2.6349
[09/16 08:17:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 60.00	
[09/16 08:18:21 visual_prompt]: 	Test 100/407. loss: 2.929, 0.1956 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 08:18:40 visual_prompt]: 	Test 200/407. loss: 2.813, 0.1945 s / batch. (data: 1.22e-02)max mem: 17.22449 GB 
[09/16 08:19:00 visual_prompt]: 	Test 300/407. loss: 2.672, 0.2188 s / batch. (data: 3.65e-02)max mem: 17.22449 GB 
[09/16 08:19:19 visual_prompt]: 	Test 400/407. loss: 2.844, 0.1821 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 08:19:23 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1935, average loss: 2.6279
[09/16 08:19:23 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.70	top5: 59.67	
[09/16 08:19:23 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 08:19:34 visual_prompt]: Epoch 22 / 100: avg data time: 1.52e-01, avg batch time: 0.5522, average train loss: 2.7841
[09/16 08:19:38 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1424, average loss: 2.7374
[09/16 08:19:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 63.00	
[09/16 08:20:00 visual_prompt]: 	Test 100/407. loss: 2.815, 0.1942 s / batch. (data: 1.22e-02)max mem: 17.22449 GB 
[09/16 08:20:19 visual_prompt]: 	Test 200/407. loss: 2.912, 0.1831 s / batch. (data: 1.08e-04)max mem: 17.22449 GB 
[09/16 08:20:38 visual_prompt]: 	Test 300/407. loss: 2.690, 0.1827 s / batch. (data: 8.75e-05)max mem: 17.22449 GB 
[09/16 08:20:58 visual_prompt]: 	Test 400/407. loss: 2.795, 0.1825 s / batch. (data: 3.84e-05)max mem: 17.22449 GB 
[09/16 08:21:01 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1930, average loss: 2.7384
[09/16 08:21:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 62.45	
[09/16 08:21:01 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 08:21:12 visual_prompt]: Epoch 23 / 100: avg data time: 1.47e-01, avg batch time: 0.5471, average train loss: 2.6525
[09/16 08:21:16 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1424, average loss: 2.4720
[09/16 08:21:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.50	
[09/16 08:21:38 visual_prompt]: 	Test 100/407. loss: 2.543, 0.1970 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 08:21:57 visual_prompt]: 	Test 200/407. loss: 2.557, 0.1832 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 08:22:17 visual_prompt]: 	Test 300/407. loss: 2.493, 0.2326 s / batch. (data: 3.78e-02)max mem: 17.22449 GB 
[09/16 08:22:37 visual_prompt]: 	Test 400/407. loss: 2.413, 0.1823 s / batch. (data: 3.39e-05)max mem: 17.22449 GB 
[09/16 08:22:40 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1939, average loss: 2.4630
[09/16 08:22:40 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 62.77	
[09/16 08:22:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 08:22:50 visual_prompt]: Epoch 24 / 100: avg data time: 1.37e-01, avg batch time: 0.5426, average train loss: 2.6353
[09/16 08:22:55 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1435, average loss: 2.6524
[09/16 08:22:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 59.00	
[09/16 08:23:17 visual_prompt]: 	Test 100/407. loss: 2.579, 0.1903 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 08:23:37 visual_prompt]: 	Test 200/407. loss: 2.747, 0.1937 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 08:23:56 visual_prompt]: 	Test 300/407. loss: 2.576, 0.1829 s / batch. (data: 1.41e-04)max mem: 17.22449 GB 
[09/16 08:24:16 visual_prompt]: 	Test 400/407. loss: 2.779, 0.1828 s / batch. (data: 3.79e-05)max mem: 17.22449 GB 
[09/16 08:24:19 visual_prompt]: Inference (test):avg data time: 6.53e-03, avg batch time: 0.1947, average loss: 2.6234
[09/16 08:24:19 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 59.15	
[09/16 08:24:19 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 08:24:30 visual_prompt]: Epoch 25 / 100: avg data time: 1.62e-01, avg batch time: 0.5643, average train loss: 2.7074
[09/16 08:24:34 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1426, average loss: 2.3779
[09/16 08:24:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/16 08:24:56 visual_prompt]: 	Test 100/407. loss: 2.664, 0.1827 s / batch. (data: 1.48e-04)max mem: 17.22449 GB 
[09/16 08:25:16 visual_prompt]: 	Test 200/407. loss: 2.570, 0.1942 s / batch. (data: 1.24e-02)max mem: 17.22449 GB 
[09/16 08:25:36 visual_prompt]: 	Test 300/407. loss: 2.434, 0.1832 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 08:25:56 visual_prompt]: 	Test 400/407. loss: 2.393, 0.1823 s / batch. (data: 3.86e-05)max mem: 17.22449 GB 
[09/16 08:25:59 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1967, average loss: 2.4246
[09/16 08:25:59 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.13	top5: 64.06	
[09/16 08:25:59 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 08:26:10 visual_prompt]: Epoch 26 / 100: avg data time: 1.64e-01, avg batch time: 0.5648, average train loss: 2.7946
[09/16 08:26:14 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1424, average loss: 2.7522
[09/16 08:26:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 57.50	
[09/16 08:26:37 visual_prompt]: 	Test 100/407. loss: 2.649, 0.2078 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 08:26:56 visual_prompt]: 	Test 200/407. loss: 2.798, 0.1819 s / batch. (data: 1.26e-04)max mem: 17.22449 GB 
[09/16 08:27:15 visual_prompt]: 	Test 300/407. loss: 2.656, 0.2059 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 08:27:35 visual_prompt]: 	Test 400/407. loss: 2.759, 0.1819 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 08:27:39 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1941, average loss: 2.7330
[09/16 08:27:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.72	top5: 59.19	
[09/16 08:27:39 visual_prompt]: Best epoch 26: best metric: 0.235
[09/16 08:27:39 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 08:27:50 visual_prompt]: Epoch 27 / 100: avg data time: 1.45e-01, avg batch time: 0.5492, average train loss: 2.7879
[09/16 08:27:54 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.1425, average loss: 2.6354
[09/16 08:27:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/16 08:28:16 visual_prompt]: 	Test 100/407. loss: 3.033, 0.1817 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 08:28:36 visual_prompt]: 	Test 200/407. loss: 2.912, 0.1827 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 08:28:55 visual_prompt]: 	Test 300/407. loss: 2.684, 0.1978 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 08:29:15 visual_prompt]: 	Test 400/407. loss: 2.745, 0.1826 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 08:29:18 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1945, average loss: 2.6864
[09/16 08:29:18 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 59.51	
[09/16 08:29:18 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 08:29:29 visual_prompt]: Epoch 28 / 100: avg data time: 1.48e-01, avg batch time: 0.5500, average train loss: 2.5401
[09/16 08:29:34 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1426, average loss: 2.4295
[09/16 08:29:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/16 08:29:56 visual_prompt]: 	Test 100/407. loss: 2.816, 0.1973 s / batch. (data: 1.47e-02)max mem: 17.22449 GB 
[09/16 08:30:15 visual_prompt]: 	Test 200/407. loss: 2.713, 0.2078 s / batch. (data: 2.57e-02)max mem: 17.22449 GB 
[09/16 08:30:35 visual_prompt]: 	Test 300/407. loss: 2.509, 0.1831 s / batch. (data: 1.56e-04)max mem: 17.22449 GB 
[09/16 08:30:55 visual_prompt]: 	Test 400/407. loss: 2.528, 0.1824 s / batch. (data: 2.31e-05)max mem: 17.22449 GB 
[09/16 08:30:58 visual_prompt]: Inference (test):avg data time: 8.37e-03, avg batch time: 0.1956, average loss: 2.5333
[09/16 08:30:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 56.57	
[09/16 08:30:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 08:31:09 visual_prompt]: Epoch 29 / 100: avg data time: 1.55e-01, avg batch time: 0.5569, average train loss: 2.4570
[09/16 08:31:13 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1425, average loss: 2.4477
[09/16 08:31:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 66.50	
[09/16 08:31:35 visual_prompt]: 	Test 100/407. loss: 2.515, 0.2065 s / batch. (data: 1.55e-02)max mem: 17.22449 GB 
[09/16 08:31:55 visual_prompt]: 	Test 200/407. loss: 2.733, 0.1957 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 08:32:14 visual_prompt]: 	Test 300/407. loss: 2.203, 0.1957 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 08:32:34 visual_prompt]: 	Test 400/407. loss: 2.401, 0.1825 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 08:32:37 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1949, average loss: 2.4795
[09/16 08:32:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 23.00	top5: 61.43	
[09/16 08:32:37 visual_prompt]: Best epoch 29: best metric: 0.290
[09/16 08:32:37 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 08:32:48 visual_prompt]: Epoch 30 / 100: avg data time: 1.49e-01, avg batch time: 0.5526, average train loss: 2.4240
[09/16 08:32:52 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1427, average loss: 2.3430
[09/16 08:32:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 63.00	
[09/16 08:33:14 visual_prompt]: 	Test 100/407. loss: 2.488, 0.1823 s / batch. (data: 3.48e-05)max mem: 17.22449 GB 
[09/16 08:33:34 visual_prompt]: 	Test 200/407. loss: 2.655, 0.1998 s / batch. (data: 1.76e-02)max mem: 17.22449 GB 
[09/16 08:33:53 visual_prompt]: 	Test 300/407. loss: 2.272, 0.1970 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 08:34:13 visual_prompt]: 	Test 400/407. loss: 2.537, 0.1825 s / batch. (data: 3.91e-05)max mem: 17.22449 GB 
[09/16 08:34:16 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1942, average loss: 2.4132
[09/16 08:34:16 visual_prompt]: Classification results with test_vtab-svhn: top1: 23.73	top5: 60.76	
[09/16 08:34:16 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 08:34:27 visual_prompt]: Epoch 31 / 100: avg data time: 1.55e-01, avg batch time: 0.5618, average train loss: 2.5516
[09/16 08:34:31 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1425, average loss: 3.1303
[09/16 08:34:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.00	
[09/16 08:34:53 visual_prompt]: 	Test 100/407. loss: 2.673, 0.1990 s / batch. (data: 1.74e-02)max mem: 17.22449 GB 
[09/16 08:35:13 visual_prompt]: 	Test 200/407. loss: 2.842, 0.1824 s / batch. (data: 1.69e-04)max mem: 17.22449 GB 
[09/16 08:35:32 visual_prompt]: 	Test 300/407. loss: 2.899, 0.2001 s / batch. (data: 1.64e-04)max mem: 17.22449 GB 
[09/16 08:35:52 visual_prompt]: 	Test 400/407. loss: 2.880, 0.1825 s / batch. (data: 3.62e-05)max mem: 17.22449 GB 
[09/16 08:35:56 visual_prompt]: Inference (test):avg data time: 7.02e-03, avg batch time: 0.1942, average loss: 3.0000
[09/16 08:35:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 49.24	
[09/16 08:35:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 08:36:06 visual_prompt]: Epoch 32 / 100: avg data time: 1.55e-01, avg batch time: 0.5598, average train loss: 2.5448
[09/16 08:36:11 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1425, average loss: 2.2774
[09/16 08:36:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.50	top5: 69.00	
[09/16 08:36:33 visual_prompt]: 	Test 100/407. loss: 2.485, 0.1828 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 08:36:52 visual_prompt]: 	Test 200/407. loss: 2.476, 0.1972 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 08:37:11 visual_prompt]: 	Test 300/407. loss: 2.208, 0.1979 s / batch. (data: 1.11e-02)max mem: 17.22449 GB 
[09/16 08:37:31 visual_prompt]: 	Test 400/407. loss: 2.317, 0.1828 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 08:37:34 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1924, average loss: 2.3217
[09/16 08:37:34 visual_prompt]: Classification results with test_vtab-svhn: top1: 16.23	top5: 66.29	
[09/16 08:37:34 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 08:37:45 visual_prompt]: Epoch 33 / 100: avg data time: 1.55e-01, avg batch time: 0.5562, average train loss: 2.2084
[09/16 08:37:49 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1427, average loss: 1.7654
[09/16 08:37:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.00	top5: 81.50	
[09/16 08:38:11 visual_prompt]: 	Test 100/407. loss: 2.077, 0.1977 s / batch. (data: 1.38e-04)max mem: 17.22449 GB 
[09/16 08:38:31 visual_prompt]: 	Test 200/407. loss: 2.025, 0.1958 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 08:38:50 visual_prompt]: 	Test 300/407. loss: 1.749, 0.2000 s / batch. (data: 1.77e-02)max mem: 17.22449 GB 
[09/16 08:39:10 visual_prompt]: 	Test 400/407. loss: 1.847, 0.1827 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 08:39:13 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1948, average loss: 1.8518
[09/16 08:39:13 visual_prompt]: Classification results with test_vtab-svhn: top1: 38.84	top5: 76.78	
[09/16 08:39:13 visual_prompt]: Best epoch 33: best metric: 0.430
[09/16 08:39:13 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 08:39:24 visual_prompt]: Epoch 34 / 100: avg data time: 1.56e-01, avg batch time: 0.5579, average train loss: 2.1953
[09/16 08:39:28 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1425, average loss: 3.1228
[09/16 08:39:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 16.00	top5: 53.00	
[09/16 08:39:50 visual_prompt]: 	Test 100/407. loss: 3.358, 0.1987 s / batch. (data: 1.57e-04)max mem: 17.22449 GB 
[09/16 08:40:10 visual_prompt]: 	Test 200/407. loss: 3.059, 0.1963 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 08:40:29 visual_prompt]: 	Test 300/407. loss: 3.490, 0.1933 s / batch. (data: 1.64e-04)max mem: 17.22449 GB 
[09/16 08:40:49 visual_prompt]: 	Test 400/407. loss: 3.372, 0.1822 s / batch. (data: 3.89e-05)max mem: 17.22449 GB 
[09/16 08:40:52 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1938, average loss: 3.2852
[09/16 08:40:52 visual_prompt]: Classification results with test_vtab-svhn: top1: 14.60	top5: 47.60	
[09/16 08:40:52 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 08:41:02 visual_prompt]: Epoch 35 / 100: avg data time: 1.37e-01, avg batch time: 0.5418, average train loss: 2.2548
[09/16 08:41:07 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1426, average loss: 2.0367
[09/16 08:41:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 88.50	
[09/16 08:41:29 visual_prompt]: 	Test 100/407. loss: 2.260, 0.1956 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 08:41:48 visual_prompt]: 	Test 200/407. loss: 2.166, 0.1956 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 08:42:07 visual_prompt]: 	Test 300/407. loss: 2.140, 0.1838 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 08:42:27 visual_prompt]: 	Test 400/407. loss: 2.343, 0.2123 s / batch. (data: 2.88e-05)max mem: 17.22449 GB 
[09/16 08:42:30 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1939, average loss: 2.2153
[09/16 08:42:30 visual_prompt]: Classification results with test_vtab-svhn: top1: 22.17	top5: 82.68	
[09/16 08:42:30 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 08:42:41 visual_prompt]: Epoch 36 / 100: avg data time: 1.74e-01, avg batch time: 0.5770, average train loss: 1.8180
[09/16 08:42:46 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1426, average loss: 1.5637
[09/16 08:42:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 40.00	top5: 88.50	
[09/16 08:43:08 visual_prompt]: 	Test 100/407. loss: 2.144, 0.1820 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 08:43:28 visual_prompt]: 	Test 200/407. loss: 2.045, 0.1830 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 08:43:47 visual_prompt]: 	Test 300/407. loss: 1.595, 0.1959 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 08:44:07 visual_prompt]: 	Test 400/407. loss: 2.099, 0.1825 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 08:44:10 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1949, average loss: 1.8120
[09/16 08:44:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 33.14	top5: 85.60	
[09/16 08:44:10 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 08:44:21 visual_prompt]: Epoch 37 / 100: avg data time: 1.54e-01, avg batch time: 0.5578, average train loss: 1.8928
[09/16 08:44:25 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1427, average loss: 1.9569
[09/16 08:44:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 38.50	top5: 87.00	
[09/16 08:44:47 visual_prompt]: 	Test 100/407. loss: 2.652, 0.1821 s / batch. (data: 6.18e-05)max mem: 17.22449 GB 
[09/16 08:45:07 visual_prompt]: 	Test 200/407. loss: 2.459, 0.2076 s / batch. (data: 2.57e-02)max mem: 17.22449 GB 
[09/16 08:45:26 visual_prompt]: 	Test 300/407. loss: 2.087, 0.2302 s / batch. (data: 4.20e-02)max mem: 17.22449 GB 
[09/16 08:45:46 visual_prompt]: 	Test 400/407. loss: 2.188, 0.1829 s / batch. (data: 2.31e-05)max mem: 17.22449 GB 
[09/16 08:45:49 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1937, average loss: 2.2198
[09/16 08:45:49 visual_prompt]: Classification results with test_vtab-svhn: top1: 31.91	top5: 81.90	
[09/16 08:45:49 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 08:46:00 visual_prompt]: Epoch 38 / 100: avg data time: 1.58e-01, avg batch time: 0.5587, average train loss: 1.7017
[09/16 08:46:04 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1425, average loss: 1.2792
[09/16 08:46:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 50.00	top5: 91.50	
[09/16 08:46:26 visual_prompt]: 	Test 100/407. loss: 1.914, 0.1977 s / batch. (data: 1.60e-02)max mem: 17.22449 GB 
[09/16 08:46:46 visual_prompt]: 	Test 200/407. loss: 1.711, 0.2153 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 08:47:05 visual_prompt]: 	Test 300/407. loss: 1.409, 0.1985 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 08:47:25 visual_prompt]: 	Test 400/407. loss: 1.793, 0.1825 s / batch. (data: 2.91e-05)max mem: 17.22449 GB 
[09/16 08:47:28 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1943, average loss: 1.6065
[09/16 08:47:28 visual_prompt]: Classification results with test_vtab-svhn: top1: 42.74	top5: 88.75	
[09/16 08:47:28 visual_prompt]: Best epoch 38: best metric: 0.500
[09/16 08:47:28 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 08:47:39 visual_prompt]: Epoch 39 / 100: avg data time: 1.57e-01, avg batch time: 0.5573, average train loss: 1.5085
[09/16 08:47:43 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1426, average loss: 1.2347
[09/16 08:47:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.50	top5: 86.50	
[09/16 08:48:05 visual_prompt]: 	Test 100/407. loss: 1.555, 0.1826 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 08:48:25 visual_prompt]: 	Test 200/407. loss: 1.457, 0.1960 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 08:48:44 visual_prompt]: 	Test 300/407. loss: 1.428, 0.1957 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 08:49:03 visual_prompt]: 	Test 400/407. loss: 1.632, 0.1837 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 08:49:07 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1927, average loss: 1.4586
[09/16 08:49:07 visual_prompt]: Classification results with test_vtab-svhn: top1: 52.03	top5: 85.69	
[09/16 08:49:07 visual_prompt]: Best epoch 39: best metric: 0.565
[09/16 08:49:07 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 08:49:17 visual_prompt]: Epoch 40 / 100: avg data time: 1.63e-01, avg batch time: 0.5629, average train loss: 1.4595
[09/16 08:49:22 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1433, average loss: 1.1352
[09/16 08:49:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.00	top5: 94.00	
[09/16 08:49:44 visual_prompt]: 	Test 100/407. loss: 1.567, 0.2274 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 08:50:03 visual_prompt]: 	Test 200/407. loss: 1.380, 0.1971 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 08:50:23 visual_prompt]: 	Test 300/407. loss: 1.316, 0.1834 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 08:50:42 visual_prompt]: 	Test 400/407. loss: 1.438, 0.1826 s / batch. (data: 4.46e-05)max mem: 17.22449 GB 
[09/16 08:50:45 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1938, average loss: 1.4287
[09/16 08:50:46 visual_prompt]: Classification results with test_vtab-svhn: top1: 49.28	top5: 89.92	
[09/16 08:50:46 visual_prompt]: Best epoch 40: best metric: 0.600
[09/16 08:50:46 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 08:50:56 visual_prompt]: Epoch 41 / 100: avg data time: 1.62e-01, avg batch time: 0.5640, average train loss: 1.2823
[09/16 08:51:01 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1424, average loss: 1.1884
[09/16 08:51:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 62.00	top5: 91.50	
[09/16 08:51:22 visual_prompt]: 	Test 100/407. loss: 1.773, 0.1929 s / batch. (data: 1.11e-02)max mem: 17.22449 GB 
[09/16 08:51:42 visual_prompt]: 	Test 200/407. loss: 1.719, 0.2052 s / batch. (data: 1.26e-02)max mem: 17.22449 GB 
[09/16 08:52:01 visual_prompt]: 	Test 300/407. loss: 1.378, 0.1828 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 08:52:21 visual_prompt]: 	Test 400/407. loss: 1.587, 0.1825 s / batch. (data: 2.26e-05)max mem: 17.22449 GB 
[09/16 08:52:24 visual_prompt]: Inference (test):avg data time: 6.80e-03, avg batch time: 0.1930, average loss: 1.5497
[09/16 08:52:24 visual_prompt]: Classification results with test_vtab-svhn: top1: 55.29	top5: 88.34	
[09/16 08:52:24 visual_prompt]: Best epoch 41: best metric: 0.620
[09/16 08:52:24 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 08:52:35 visual_prompt]: Epoch 42 / 100: avg data time: 1.55e-01, avg batch time: 0.5554, average train loss: 1.3115
[09/16 08:52:39 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1425, average loss: 0.8900
[09/16 08:52:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.00	top5: 96.50	
[09/16 08:53:01 visual_prompt]: 	Test 100/407. loss: 1.523, 0.1824 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 08:53:20 visual_prompt]: 	Test 200/407. loss: 1.280, 0.2116 s / batch. (data: 3.00e-02)max mem: 17.22449 GB 
[09/16 08:53:39 visual_prompt]: 	Test 300/407. loss: 1.085, 0.1947 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 08:53:59 visual_prompt]: 	Test 400/407. loss: 1.406, 0.1824 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 08:54:03 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1935, average loss: 1.2616
[09/16 08:54:03 visual_prompt]: Classification results with test_vtab-svhn: top1: 55.84	top5: 93.76	
[09/16 08:54:03 visual_prompt]: Best epoch 42: best metric: 0.660
[09/16 08:54:03 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 08:54:13 visual_prompt]: Epoch 43 / 100: avg data time: 1.60e-01, avg batch time: 0.5597, average train loss: 1.3296
[09/16 08:54:18 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1424, average loss: 1.6602
[09/16 08:54:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 54.00	top5: 84.50	
[09/16 08:54:40 visual_prompt]: 	Test 100/407. loss: 1.670, 0.2080 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 08:54:59 visual_prompt]: 	Test 200/407. loss: 1.821, 0.1990 s / batch. (data: 1.58e-02)max mem: 17.22449 GB 
[09/16 08:55:19 visual_prompt]: 	Test 300/407. loss: 1.747, 0.1934 s / batch. (data: 1.12e-02)max mem: 17.22449 GB 
[09/16 08:55:39 visual_prompt]: 	Test 400/407. loss: 1.980, 0.1827 s / batch. (data: 3.00e-05)max mem: 17.22449 GB 
[09/16 08:55:43 visual_prompt]: Inference (test):avg data time: 7.09e-03, avg batch time: 0.1947, average loss: 1.7934
[09/16 08:55:43 visual_prompt]: Classification results with test_vtab-svhn: top1: 48.97	top5: 83.49	
[09/16 08:55:43 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 08:55:53 visual_prompt]: Epoch 44 / 100: avg data time: 1.60e-01, avg batch time: 0.5610, average train loss: 1.4563
[09/16 08:55:58 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1423, average loss: 1.1435
[09/16 08:55:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.00	top5: 93.50	
[09/16 08:56:20 visual_prompt]: 	Test 100/407. loss: 1.712, 0.1967 s / batch. (data: 1.30e-02)max mem: 17.22449 GB 
[09/16 08:56:39 visual_prompt]: 	Test 200/407. loss: 1.562, 0.2150 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 08:56:59 visual_prompt]: 	Test 300/407. loss: 1.587, 0.2058 s / batch. (data: 1.32e-02)max mem: 17.22449 GB 
[09/16 08:57:18 visual_prompt]: 	Test 400/407. loss: 1.732, 0.1829 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 08:57:21 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1932, average loss: 1.5121
[09/16 08:57:22 visual_prompt]: Classification results with test_vtab-svhn: top1: 54.54	top5: 90.35	
[09/16 08:57:22 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 08:57:32 visual_prompt]: Epoch 45 / 100: avg data time: 1.53e-01, avg batch time: 0.5559, average train loss: 1.1418
[09/16 08:57:37 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1428, average loss: 0.8847
[09/16 08:57:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 98.50	
[09/16 08:57:59 visual_prompt]: 	Test 100/407. loss: 1.382, 0.1969 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 08:58:18 visual_prompt]: 	Test 200/407. loss: 1.161, 0.1955 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 08:58:38 visual_prompt]: 	Test 300/407. loss: 1.030, 0.1969 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 08:58:57 visual_prompt]: 	Test 400/407. loss: 1.382, 0.1830 s / batch. (data: 3.43e-05)max mem: 17.22449 GB 
[09/16 08:59:01 visual_prompt]: Inference (test):avg data time: 8.48e-03, avg batch time: 0.1943, average loss: 1.2471
[09/16 08:59:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 59.07	top5: 92.30	
[09/16 08:59:01 visual_prompt]: Best epoch 45: best metric: 0.715
[09/16 08:59:01 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 08:59:11 visual_prompt]: Epoch 46 / 100: avg data time: 1.62e-01, avg batch time: 0.5648, average train loss: 0.7548
[09/16 08:59:16 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1424, average loss: 1.1249
[09/16 08:59:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.50	top5: 94.00	
[09/16 08:59:39 visual_prompt]: 	Test 100/407. loss: 1.920, 0.1958 s / batch. (data: 1.08e-02)max mem: 17.22449 GB 
[09/16 08:59:58 visual_prompt]: 	Test 200/407. loss: 1.746, 0.1904 s / batch. (data: 3.81e-05)max mem: 17.22449 GB 
[09/16 09:00:18 visual_prompt]: 	Test 300/407. loss: 1.488, 0.1862 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 09:00:37 visual_prompt]: 	Test 400/407. loss: 1.747, 0.1833 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 09:00:40 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1954, average loss: 1.6603
[09/16 09:00:40 visual_prompt]: Classification results with test_vtab-svhn: top1: 57.30	top5: 90.31	
[09/16 09:00:40 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 09:00:52 visual_prompt]: Epoch 47 / 100: avg data time: 1.65e-01, avg batch time: 0.5909, average train loss: 0.9979
[09/16 09:00:56 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1435, average loss: 0.8079
[09/16 09:00:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 94.50	
[09/16 09:01:18 visual_prompt]: 	Test 100/407. loss: 1.505, 0.1953 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 09:01:38 visual_prompt]: 	Test 200/407. loss: 1.423, 0.1925 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 09:01:57 visual_prompt]: 	Test 300/407. loss: 0.970, 0.1947 s / batch. (data: 1.24e-02)max mem: 17.22449 GB 
[09/16 09:02:16 visual_prompt]: 	Test 400/407. loss: 1.223, 0.1825 s / batch. (data: 2.86e-05)max mem: 17.22449 GB 
[09/16 09:02:20 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1939, average loss: 1.2306
[09/16 09:02:20 visual_prompt]: Classification results with test_vtab-svhn: top1: 65.88	top5: 91.63	
[09/16 09:02:20 visual_prompt]: Best epoch 47: best metric: 0.755
[09/16 09:02:20 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 09:02:30 visual_prompt]: Epoch 48 / 100: avg data time: 1.56e-01, avg batch time: 0.5580, average train loss: 0.6650
[09/16 09:02:35 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1424, average loss: 0.5805
[09/16 09:02:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 99.50	
[09/16 09:02:57 visual_prompt]: 	Test 100/407. loss: 1.625, 0.1868 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 09:03:16 visual_prompt]: 	Test 200/407. loss: 1.336, 0.1871 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 09:03:36 visual_prompt]: 	Test 300/407. loss: 1.041, 0.1832 s / batch. (data: 1.56e-04)max mem: 17.22449 GB 
[09/16 09:03:55 visual_prompt]: 	Test 400/407. loss: 1.662, 0.1825 s / batch. (data: 3.67e-05)max mem: 17.22449 GB 
[09/16 09:03:58 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1932, average loss: 1.2597
[09/16 09:03:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 64.54	top5: 95.39	
[09/16 09:03:58 visual_prompt]: Best epoch 48: best metric: 0.835
[09/16 09:03:58 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 09:04:09 visual_prompt]: Epoch 49 / 100: avg data time: 1.58e-01, avg batch time: 0.5594, average train loss: 0.6786
[09/16 09:04:14 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1426, average loss: 0.6642
[09/16 09:04:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 95.50	
[09/16 09:04:35 visual_prompt]: 	Test 100/407. loss: 0.995, 0.1955 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 09:04:55 visual_prompt]: 	Test 200/407. loss: 1.051, 0.2029 s / batch. (data: 2.12e-02)max mem: 17.22449 GB 
[09/16 09:05:14 visual_prompt]: 	Test 300/407. loss: 0.828, 0.2422 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 09:05:34 visual_prompt]: 	Test 400/407. loss: 1.052, 0.1823 s / batch. (data: 3.74e-05)max mem: 17.22449 GB 
[09/16 09:05:37 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1935, average loss: 1.0628
[09/16 09:05:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 69.34	top5: 92.92	
[09/16 09:05:37 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 09:05:48 visual_prompt]: Epoch 50 / 100: avg data time: 1.60e-01, avg batch time: 0.5607, average train loss: 0.5161
[09/16 09:05:53 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1425, average loss: 0.6576
[09/16 09:05:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.00	
[09/16 09:06:15 visual_prompt]: 	Test 100/407. loss: 1.635, 0.1820 s / batch. (data: 1.14e-04)max mem: 17.22449 GB 
[09/16 09:06:34 visual_prompt]: 	Test 200/407. loss: 1.391, 0.1821 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 09:06:54 visual_prompt]: 	Test 300/407. loss: 0.986, 0.1827 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 09:07:13 visual_prompt]: 	Test 400/407. loss: 1.612, 0.1827 s / batch. (data: 2.96e-05)max mem: 17.22449 GB 
[09/16 09:07:17 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1950, average loss: 1.3319
[09/16 09:07:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 65.30	top5: 92.20	
[09/16 09:07:17 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 09:07:27 visual_prompt]: Epoch 51 / 100: avg data time: 1.47e-01, avg batch time: 0.5509, average train loss: 0.4704
[09/16 09:07:32 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1425, average loss: 0.4592
[09/16 09:07:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 100.00	
[09/16 09:07:54 visual_prompt]: 	Test 100/407. loss: 1.333, 0.2120 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 09:08:13 visual_prompt]: 	Test 200/407. loss: 0.979, 0.1828 s / batch. (data: 1.48e-04)max mem: 17.22449 GB 
[09/16 09:08:33 visual_prompt]: 	Test 300/407. loss: 0.876, 0.1981 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 09:08:53 visual_prompt]: 	Test 400/407. loss: 1.191, 0.1836 s / batch. (data: 4.32e-05)max mem: 17.22449 GB 
[09/16 09:08:56 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1938, average loss: 0.9395
[09/16 09:08:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.58	top5: 96.26	
[09/16 09:08:56 visual_prompt]: Best epoch 51: best metric: 0.850
[09/16 09:08:56 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 09:09:07 visual_prompt]: Epoch 52 / 100: avg data time: 1.60e-01, avg batch time: 0.5649, average train loss: 0.3840
[09/16 09:09:11 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1423, average loss: 0.2673
[09/16 09:09:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 91.00	top5: 99.50	
[09/16 09:09:33 visual_prompt]: 	Test 100/407. loss: 1.375, 0.2040 s / batch. (data: 2.22e-02)max mem: 17.22449 GB 
[09/16 09:09:53 visual_prompt]: 	Test 200/407. loss: 0.928, 0.1960 s / batch. (data: 1.03e-04)max mem: 17.22449 GB 
[09/16 09:10:12 visual_prompt]: 	Test 300/407. loss: 0.868, 0.1832 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 09:10:32 visual_prompt]: 	Test 400/407. loss: 1.193, 0.1826 s / batch. (data: 4.55e-05)max mem: 17.22449 GB 
[09/16 09:10:35 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1936, average loss: 0.9569
[09/16 09:10:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 74.69	top5: 96.24	
[09/16 09:10:35 visual_prompt]: Best epoch 52: best metric: 0.910
[09/16 09:10:35 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 09:10:46 visual_prompt]: Epoch 53 / 100: avg data time: 1.57e-01, avg batch time: 0.5582, average train loss: 0.2820
[09/16 09:10:50 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1425, average loss: 0.3620
[09/16 09:10:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 98.50	
[09/16 09:11:12 visual_prompt]: 	Test 100/407. loss: 1.700, 0.1963 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 09:11:32 visual_prompt]: 	Test 200/407. loss: 1.736, 0.2108 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 09:11:51 visual_prompt]: 	Test 300/407. loss: 1.303, 0.1833 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 09:12:11 visual_prompt]: 	Test 400/407. loss: 1.408, 0.1829 s / batch. (data: 3.39e-05)max mem: 17.22449 GB 
[09/16 09:12:14 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1936, average loss: 1.3325
[09/16 09:12:14 visual_prompt]: Classification results with test_vtab-svhn: top1: 71.25	top5: 94.95	
[09/16 09:12:14 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 09:12:25 visual_prompt]: Epoch 54 / 100: avg data time: 1.55e-01, avg batch time: 0.5561, average train loss: 0.3021
[09/16 09:12:29 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1426, average loss: 0.3688
[09/16 09:12:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 99.50	
[09/16 09:12:51 visual_prompt]: 	Test 100/407. loss: 1.969, 0.1931 s / batch. (data: 1.03e-02)max mem: 17.22449 GB 
[09/16 09:13:11 visual_prompt]: 	Test 200/407. loss: 1.139, 0.1818 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 09:13:30 visual_prompt]: 	Test 300/407. loss: 1.018, 0.1957 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 09:13:49 visual_prompt]: 	Test 400/407. loss: 1.711, 0.1867 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 09:13:53 visual_prompt]: Inference (test):avg data time: 7.06e-03, avg batch time: 0.1929, average loss: 1.2199
[09/16 09:13:53 visual_prompt]: Classification results with test_vtab-svhn: top1: 70.24	top5: 95.49	
[09/16 09:13:53 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 09:14:03 visual_prompt]: Epoch 55 / 100: avg data time: 1.58e-01, avg batch time: 0.5592, average train loss: 0.3231
[09/16 09:14:08 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1424, average loss: 0.2152
[09/16 09:14:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 93.00	top5: 100.00	
[09/16 09:14:30 visual_prompt]: 	Test 100/407. loss: 1.399, 0.1960 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 09:14:50 visual_prompt]: 	Test 200/407. loss: 0.978, 0.1849 s / batch. (data: 1.40e-04)max mem: 17.22449 GB 
[09/16 09:15:09 visual_prompt]: 	Test 300/407. loss: 0.711, 0.1826 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 09:15:28 visual_prompt]: 	Test 400/407. loss: 1.455, 0.1834 s / batch. (data: 3.72e-05)max mem: 17.22449 GB 
[09/16 09:15:32 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1947, average loss: 0.9842
[09/16 09:15:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.30	top5: 96.54	
[09/16 09:15:32 visual_prompt]: Best epoch 55: best metric: 0.930
[09/16 09:15:32 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 09:15:42 visual_prompt]: Epoch 56 / 100: avg data time: 1.57e-01, avg batch time: 0.5567, average train loss: 0.2843
[09/16 09:15:47 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1425, average loss: 0.2705
[09/16 09:15:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 100.00	
[09/16 09:16:09 visual_prompt]: 	Test 100/407. loss: 1.402, 0.2131 s / batch. (data: 1.55e-02)max mem: 17.22449 GB 
[09/16 09:16:28 visual_prompt]: 	Test 200/407. loss: 1.193, 0.1926 s / batch. (data: 1.47e-04)max mem: 17.22449 GB 
[09/16 09:16:47 visual_prompt]: 	Test 300/407. loss: 0.786, 0.1963 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 09:17:07 visual_prompt]: 	Test 400/407. loss: 1.315, 0.1829 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 09:17:11 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1938, average loss: 1.0420
[09/16 09:17:11 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.58	top5: 96.25	
[09/16 09:17:11 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 09:17:21 visual_prompt]: Epoch 57 / 100: avg data time: 1.59e-01, avg batch time: 0.5615, average train loss: 0.2791
[09/16 09:17:26 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1422, average loss: 0.1847
[09/16 09:17:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 94.00	top5: 100.00	
[09/16 09:17:48 visual_prompt]: 	Test 100/407. loss: 1.178, 0.1830 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 09:18:07 visual_prompt]: 	Test 200/407. loss: 1.201, 0.1971 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 09:18:27 visual_prompt]: 	Test 300/407. loss: 0.673, 0.1876 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 09:18:46 visual_prompt]: 	Test 400/407. loss: 1.051, 0.1832 s / batch. (data: 3.41e-05)max mem: 17.22449 GB 
[09/16 09:18:50 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1938, average loss: 0.9858
[09/16 09:18:50 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.66	top5: 96.78	
[09/16 09:18:50 visual_prompt]: Best epoch 57: best metric: 0.940
[09/16 09:18:50 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 09:19:00 visual_prompt]: Epoch 58 / 100: avg data time: 1.60e-01, avg batch time: 0.5592, average train loss: 0.1977
[09/16 09:19:05 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1426, average loss: 0.1247
[09/16 09:19:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 94.00	top5: 100.00	
[09/16 09:19:27 visual_prompt]: 	Test 100/407. loss: 0.936, 0.1954 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 09:19:46 visual_prompt]: 	Test 200/407. loss: 0.767, 0.2001 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 09:20:06 visual_prompt]: 	Test 300/407. loss: 0.628, 0.1983 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 09:20:25 visual_prompt]: 	Test 400/407. loss: 1.027, 0.1834 s / batch. (data: 3.31e-05)max mem: 17.22449 GB 
[09/16 09:20:28 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1932, average loss: 0.8478
[09/16 09:20:29 visual_prompt]: Classification results with test_vtab-svhn: top1: 78.82	top5: 97.58	
[09/16 09:20:29 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 09:20:39 visual_prompt]: Epoch 59 / 100: avg data time: 1.58e-01, avg batch time: 0.5590, average train loss: 0.1661
[09/16 09:20:44 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1424, average loss: 0.1131
[09/16 09:20:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 97.50	top5: 100.00	
[09/16 09:21:06 visual_prompt]: 	Test 100/407. loss: 1.090, 0.2076 s / batch. (data: 2.63e-02)max mem: 17.22449 GB 
[09/16 09:21:25 visual_prompt]: 	Test 200/407. loss: 1.276, 0.1958 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 09:21:45 visual_prompt]: 	Test 300/407. loss: 0.579, 0.1824 s / batch. (data: 1.53e-04)max mem: 17.22449 GB 
[09/16 09:22:04 visual_prompt]: 	Test 400/407. loss: 1.047, 0.1829 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 09:22:07 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1926, average loss: 0.9265
[09/16 09:22:07 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.59	top5: 97.43	
[09/16 09:22:07 visual_prompt]: Best epoch 59: best metric: 0.975
[09/16 09:22:07 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 09:22:18 visual_prompt]: Epoch 60 / 100: avg data time: 1.68e-01, avg batch time: 0.5698, average train loss: 0.1005
[09/16 09:22:22 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1426, average loss: 0.0952
[09/16 09:22:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 96.50	top5: 100.00	
[09/16 09:22:44 visual_prompt]: 	Test 100/407. loss: 1.994, 0.1826 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 09:23:04 visual_prompt]: 	Test 200/407. loss: 1.388, 0.1939 s / batch. (data: 1.43e-04)max mem: 17.22449 GB 
[09/16 09:23:23 visual_prompt]: 	Test 300/407. loss: 0.660, 0.2147 s / batch. (data: 1.58e-02)max mem: 17.22449 GB 
[09/16 09:23:43 visual_prompt]: 	Test 400/407. loss: 1.913, 0.1873 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 09:23:46 visual_prompt]: Inference (test):avg data time: 8.46e-03, avg batch time: 0.1935, average loss: 1.3576
[09/16 09:23:46 visual_prompt]: Classification results with test_vtab-svhn: top1: 75.92	top5: 97.16	
[09/16 09:23:46 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 09:23:56 visual_prompt]: Epoch 61 / 100: avg data time: 1.56e-01, avg batch time: 0.5580, average train loss: 0.1244
[09/16 09:24:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1425, average loss: 0.1200
[09/16 09:24:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 96.00	top5: 100.00	
[09/16 09:24:23 visual_prompt]: 	Test 100/407. loss: 1.903, 0.1828 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 09:24:42 visual_prompt]: 	Test 200/407. loss: 1.191, 0.1944 s / batch. (data: 1.28e-02)max mem: 17.22449 GB 
[09/16 09:25:02 visual_prompt]: 	Test 300/407. loss: 1.006, 0.1913 s / batch. (data: 9.44e-03)max mem: 17.22449 GB 
[09/16 09:25:21 visual_prompt]: 	Test 400/407. loss: 1.879, 0.1826 s / batch. (data: 3.77e-05)max mem: 17.22449 GB 
[09/16 09:25:24 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1934, average loss: 1.2252
[09/16 09:25:24 visual_prompt]: Classification results with test_vtab-svhn: top1: 78.24	top5: 96.32	
[09/16 09:25:24 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 09:25:35 visual_prompt]: Epoch 62 / 100: avg data time: 1.61e-01, avg batch time: 0.5617, average train loss: 0.2074
[09/16 09:25:40 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1426, average loss: 0.2308
[09/16 09:25:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 90.00	top5: 100.00	
[09/16 09:26:01 visual_prompt]: 	Test 100/407. loss: 1.301, 0.1971 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 09:26:21 visual_prompt]: 	Test 200/407. loss: 1.089, 0.2076 s / batch. (data: 2.56e-02)max mem: 17.22449 GB 
[09/16 09:26:41 visual_prompt]: 	Test 300/407. loss: 1.085, 0.1923 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 09:27:00 visual_prompt]: 	Test 400/407. loss: 1.530, 0.2159 s / batch. (data: 3.36e-05)max mem: 17.22449 GB 
[09/16 09:27:03 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1940, average loss: 1.1187
[09/16 09:27:03 visual_prompt]: Classification results with test_vtab-svhn: top1: 75.13	top5: 97.16	
[09/16 09:27:03 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 09:27:14 visual_prompt]: Epoch 63 / 100: avg data time: 1.70e-01, avg batch time: 0.5695, average train loss: 0.2132
[09/16 09:27:19 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1424, average loss: 0.1142
[09/16 09:27:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 97.00	top5: 100.00	
[09/16 09:27:41 visual_prompt]: 	Test 100/407. loss: 1.020, 0.1930 s / batch. (data: 1.12e-02)max mem: 17.22449 GB 
[09/16 09:28:00 visual_prompt]: 	Test 200/407. loss: 0.938, 0.1829 s / batch. (data: 1.09e-04)max mem: 17.22449 GB 
[09/16 09:28:20 visual_prompt]: 	Test 300/407. loss: 0.745, 0.1828 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 09:28:39 visual_prompt]: 	Test 400/407. loss: 0.851, 0.1826 s / batch. (data: 3.53e-05)max mem: 17.22449 GB 
[09/16 09:28:42 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1937, average loss: 0.8159
[09/16 09:28:42 visual_prompt]: Classification results with test_vtab-svhn: top1: 79.66	top5: 97.70	
[09/16 09:28:42 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 09:28:53 visual_prompt]: Epoch 64 / 100: avg data time: 1.54e-01, avg batch time: 0.5533, average train loss: 0.0955
[09/16 09:28:58 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1423, average loss: 0.0290
[09/16 09:28:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 98.50	top5: 100.00	
[09/16 09:29:20 visual_prompt]: 	Test 100/407. loss: 1.979, 0.2202 s / batch. (data: 3.84e-02)max mem: 17.22449 GB 
[09/16 09:29:39 visual_prompt]: 	Test 200/407. loss: 1.339, 0.2198 s / batch. (data: 1.23e-02)max mem: 17.22449 GB 
[09/16 09:29:59 visual_prompt]: 	Test 300/407. loss: 1.153, 0.2028 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 09:30:18 visual_prompt]: 	Test 400/407. loss: 1.379, 0.1823 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 09:30:21 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1939, average loss: 1.2176
[09/16 09:30:21 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.64	top5: 96.83	
[09/16 09:30:21 visual_prompt]: Best epoch 64: best metric: 0.985
[09/16 09:30:21 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 09:30:32 visual_prompt]: Epoch 65 / 100: avg data time: 1.37e-01, avg batch time: 0.5404, average train loss: 0.1260
[09/16 09:30:36 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1425, average loss: 0.1714
[09/16 09:30:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 96.50	top5: 99.50	
[09/16 09:30:58 visual_prompt]: 	Test 100/407. loss: 1.836, 0.1824 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 09:31:17 visual_prompt]: 	Test 200/407. loss: 1.264, 0.1827 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 09:31:37 visual_prompt]: 	Test 300/407. loss: 1.467, 0.1843 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 09:31:56 visual_prompt]: 	Test 400/407. loss: 1.485, 0.1823 s / batch. (data: 4.01e-05)max mem: 17.22449 GB 
[09/16 09:31:59 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1927, average loss: 1.3177
[09/16 09:31:59 visual_prompt]: Classification results with test_vtab-svhn: top1: 75.19	top5: 95.13	
[09/16 09:31:59 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 09:32:10 visual_prompt]: Epoch 66 / 100: avg data time: 1.57e-01, avg batch time: 0.5580, average train loss: 0.1487
[09/16 09:32:15 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1492, average loss: 0.0765
[09/16 09:32:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 97.50	top5: 100.00	
[09/16 09:32:36 visual_prompt]: 	Test 100/407. loss: 1.532, 0.1828 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 09:32:56 visual_prompt]: 	Test 200/407. loss: 0.728, 0.1964 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 09:33:16 visual_prompt]: 	Test 300/407. loss: 1.050, 0.1885 s / batch. (data: 1.56e-04)max mem: 17.22449 GB 
[09/16 09:33:35 visual_prompt]: 	Test 400/407. loss: 1.419, 0.1832 s / batch. (data: 4.77e-05)max mem: 17.22449 GB 
[09/16 09:33:38 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1939, average loss: 1.0179
[09/16 09:33:38 visual_prompt]: Classification results with test_vtab-svhn: top1: 78.17	top5: 97.50	
[09/16 09:33:38 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 09:33:49 visual_prompt]: Epoch 67 / 100: avg data time: 1.62e-01, avg batch time: 0.5628, average train loss: 0.0898
[09/16 09:33:54 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1425, average loss: 0.1166
[09/16 09:33:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 95.50	top5: 100.00	
[09/16 09:34:15 visual_prompt]: 	Test 100/407. loss: 2.007, 0.1984 s / batch. (data: 1.67e-02)max mem: 17.22449 GB 
[09/16 09:34:35 visual_prompt]: 	Test 200/407. loss: 1.374, 0.1840 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 09:34:54 visual_prompt]: 	Test 300/407. loss: 1.279, 0.2077 s / batch. (data: 2.57e-02)max mem: 17.22449 GB 
[09/16 09:35:13 visual_prompt]: 	Test 400/407. loss: 1.436, 0.1835 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 09:35:17 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1928, average loss: 1.2065
[09/16 09:35:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.87	top5: 96.77	
[09/16 09:35:17 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 09:35:27 visual_prompt]: Epoch 68 / 100: avg data time: 1.49e-01, avg batch time: 0.5506, average train loss: 0.0672
[09/16 09:35:32 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1425, average loss: 0.0126
[09/16 09:35:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 09:35:54 visual_prompt]: 	Test 100/407. loss: 1.658, 0.1827 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 09:36:13 visual_prompt]: 	Test 200/407. loss: 0.947, 0.1830 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 09:36:32 visual_prompt]: 	Test 300/407. loss: 0.783, 0.1825 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 09:36:52 visual_prompt]: 	Test 400/407. loss: 1.671, 0.1827 s / batch. (data: 3.62e-05)max mem: 17.22449 GB 
[09/16 09:36:55 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1925, average loss: 1.0440
[09/16 09:36:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 80.97	top5: 97.66	
[09/16 09:36:55 visual_prompt]: Best epoch 68: best metric: 1.000
[09/16 09:36:55 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 09:37:06 visual_prompt]: Epoch 69 / 100: avg data time: 1.63e-01, avg batch time: 0.5639, average train loss: 0.0443
[09/16 09:37:10 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1427, average loss: 0.0441
[09/16 09:37:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 98.50	top5: 100.00	
[09/16 09:37:32 visual_prompt]: 	Test 100/407. loss: 1.544, 0.1961 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 09:37:52 visual_prompt]: 	Test 200/407. loss: 1.289, 0.2040 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 09:38:12 visual_prompt]: 	Test 300/407. loss: 1.047, 0.2370 s / batch. (data: 5.57e-02)max mem: 17.22449 GB 
[09/16 09:38:31 visual_prompt]: 	Test 400/407. loss: 1.992, 0.1826 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 09:38:35 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1951, average loss: 1.2092
[09/16 09:38:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 79.91	top5: 97.18	
[09/16 09:38:35 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 09:38:46 visual_prompt]: Epoch 70 / 100: avg data time: 1.66e-01, avg batch time: 0.5844, average train loss: 0.0675
[09/16 09:38:51 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1425, average loss: 0.0158
[09/16 09:38:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 09:39:13 visual_prompt]: 	Test 100/407. loss: 1.696, 0.1820 s / batch. (data: 1.41e-04)max mem: 17.22449 GB 
[09/16 09:39:32 visual_prompt]: 	Test 200/407. loss: 1.136, 0.1945 s / batch. (data: 1.25e-02)max mem: 17.22449 GB 
[09/16 09:39:51 visual_prompt]: 	Test 300/407. loss: 1.164, 0.1952 s / batch. (data: 1.25e-02)max mem: 17.22449 GB 
[09/16 09:40:11 visual_prompt]: 	Test 400/407. loss: 1.841, 0.1827 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 09:40:14 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1932, average loss: 1.2509
[09/16 09:40:14 visual_prompt]: Classification results with test_vtab-svhn: top1: 79.58	top5: 96.90	
[09/16 09:40:14 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 09:40:25 visual_prompt]: Epoch 71 / 100: avg data time: 1.62e-01, avg batch time: 0.5638, average train loss: 0.0685
[09/16 09:40:30 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1425, average loss: 0.0395
[09/16 09:40:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 97.50	top5: 100.00	
[09/16 09:40:51 visual_prompt]: 	Test 100/407. loss: 1.855, 0.2052 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 09:41:11 visual_prompt]: 	Test 200/407. loss: 1.382, 0.1820 s / batch. (data: 1.41e-04)max mem: 17.22449 GB 
[09/16 09:41:30 visual_prompt]: 	Test 300/407. loss: 1.052, 0.2028 s / batch. (data: 2.12e-02)max mem: 17.22449 GB 
[09/16 09:41:50 visual_prompt]: 	Test 400/407. loss: 1.673, 0.1827 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 09:41:53 visual_prompt]: Inference (test):avg data time: 8.94e-03, avg batch time: 0.1933, average loss: 1.1276
[09/16 09:41:53 visual_prompt]: Classification results with test_vtab-svhn: top1: 79.84	top5: 96.96	
[09/16 09:41:53 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 09:42:04 visual_prompt]: Epoch 72 / 100: avg data time: 1.64e-01, avg batch time: 0.5661, average train loss: 0.0485
[09/16 09:42:08 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1424, average loss: 0.0615
[09/16 09:42:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 98.00	top5: 100.00	
[09/16 09:42:30 visual_prompt]: 	Test 100/407. loss: 1.995, 0.2048 s / batch. (data: 2.32e-02)max mem: 17.22449 GB 
[09/16 09:42:50 visual_prompt]: 	Test 200/407. loss: 1.705, 0.1997 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 09:43:09 visual_prompt]: 	Test 300/407. loss: 1.259, 0.1821 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 09:43:29 visual_prompt]: 	Test 400/407. loss: 2.259, 0.1830 s / batch. (data: 3.46e-05)max mem: 17.22449 GB 
[09/16 09:43:32 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1935, average loss: 1.3564
[09/16 09:43:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 78.30	top5: 96.88	
[09/16 09:43:32 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 09:43:42 visual_prompt]: Epoch 73 / 100: avg data time: 1.51e-01, avg batch time: 0.5531, average train loss: 0.0335
[09/16 09:43:47 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1426, average loss: 0.0071
[09/16 09:43:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 09:44:09 visual_prompt]: 	Test 100/407. loss: 1.794, 0.1821 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 09:44:28 visual_prompt]: 	Test 200/407. loss: 1.221, 0.1966 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 09:44:48 visual_prompt]: 	Test 300/407. loss: 0.959, 0.1962 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 09:45:07 visual_prompt]: 	Test 400/407. loss: 1.990, 0.1826 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 09:45:10 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1931, average loss: 1.1967
[09/16 09:45:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 80.01	top5: 97.26	
[09/16 09:45:10 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 09:45:21 visual_prompt]: Epoch 74 / 100: avg data time: 1.59e-01, avg batch time: 0.5618, average train loss: 0.0179
[09/16 09:45:25 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1425, average loss: 0.0017
[09/16 09:45:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 09:45:47 visual_prompt]: 	Test 100/407. loss: 2.025, 0.1956 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 09:46:07 visual_prompt]: 	Test 200/407. loss: 1.259, 0.1827 s / batch. (data: 9.44e-05)max mem: 17.22449 GB 
[09/16 09:46:26 visual_prompt]: 	Test 300/407. loss: 0.961, 0.1968 s / batch. (data: 1.48e-02)max mem: 17.22449 GB 
[09/16 09:46:46 visual_prompt]: 	Test 400/407. loss: 2.065, 0.1828 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 09:46:49 visual_prompt]: Inference (test):avg data time: 8.05e-03, avg batch time: 0.1935, average loss: 1.3108
[09/16 09:46:49 visual_prompt]: Classification results with test_vtab-svhn: top1: 80.98	top5: 97.34	
[09/16 09:46:49 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 09:47:00 visual_prompt]: Epoch 75 / 100: avg data time: 1.53e-01, avg batch time: 0.5585, average train loss: 0.0147
[09/16 09:47:04 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1425, average loss: 0.0247
[09/16 09:47:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 09:47:26 visual_prompt]: 	Test 100/407. loss: 2.798, 0.1827 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 09:47:46 visual_prompt]: 	Test 200/407. loss: 1.792, 0.1824 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 09:48:05 visual_prompt]: 	Test 300/407. loss: 1.506, 0.2072 s / batch. (data: 2.60e-02)max mem: 17.22449 GB 
[09/16 09:48:24 visual_prompt]: 	Test 400/407. loss: 2.655, 0.1823 s / batch. (data: 4.70e-05)max mem: 17.22449 GB 
[09/16 09:48:28 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1928, average loss: 1.7182
[09/16 09:48:28 visual_prompt]: Classification results with test_vtab-svhn: top1: 79.02	top5: 96.98	
[09/16 09:48:28 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 09:48:38 visual_prompt]: Epoch 76 / 100: avg data time: 1.48e-01, avg batch time: 0.5496, average train loss: 0.0178
[09/16 09:48:43 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1426, average loss: 0.0061
[09/16 09:48:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 09:49:04 visual_prompt]: 	Test 100/407. loss: 2.350, 0.1817 s / batch. (data: 9.61e-05)max mem: 17.22449 GB 
[09/16 09:49:24 visual_prompt]: 	Test 200/407. loss: 1.620, 0.1968 s / batch. (data: 1.50e-02)max mem: 17.22449 GB 
[09/16 09:49:43 visual_prompt]: 	Test 300/407. loss: 1.375, 0.2086 s / batch. (data: 2.64e-02)max mem: 17.22449 GB 
[09/16 09:50:03 visual_prompt]: 	Test 400/407. loss: 1.926, 0.1821 s / batch. (data: 3.19e-05)max mem: 17.22449 GB 
[09/16 09:50:06 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1930, average loss: 1.5277
[09/16 09:50:06 visual_prompt]: Classification results with test_vtab-svhn: top1: 79.87	top5: 97.27	
[09/16 09:50:06 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 09:50:17 visual_prompt]: Epoch 77 / 100: avg data time: 1.60e-01, avg batch time: 0.5630, average train loss: 0.0123
[09/16 09:50:22 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1426, average loss: 0.0013
[09/16 09:50:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 09:50:43 visual_prompt]: 	Test 100/407. loss: 2.443, 0.1824 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 09:51:03 visual_prompt]: 	Test 200/407. loss: 1.453, 0.1822 s / batch. (data: 1.26e-04)max mem: 17.22449 GB 
[09/16 09:51:22 visual_prompt]: 	Test 300/407. loss: 1.234, 0.1954 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 09:51:42 visual_prompt]: 	Test 400/407. loss: 2.461, 0.1819 s / batch. (data: 3.60e-05)max mem: 17.22449 GB 
[09/16 09:51:46 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1951, average loss: 1.5638
[09/16 09:51:46 visual_prompt]: Classification results with test_vtab-svhn: top1: 80.84	top5: 97.43	
[09/16 09:51:46 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 09:51:56 visual_prompt]: Epoch 78 / 100: avg data time: 1.61e-01, avg batch time: 0.5608, average train loss: 0.0073
[09/16 09:52:01 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1425, average loss: 0.0068
[09/16 09:52:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 09:52:23 visual_prompt]: 	Test 100/407. loss: 1.985, 0.1969 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 09:52:42 visual_prompt]: 	Test 200/407. loss: 1.513, 0.2039 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 09:53:02 visual_prompt]: 	Test 300/407. loss: 1.451, 0.2123 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 09:53:21 visual_prompt]: 	Test 400/407. loss: 2.016, 0.1826 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 09:53:25 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1946, average loss: 1.5318
[09/16 09:53:25 visual_prompt]: Classification results with test_vtab-svhn: top1: 80.29	top5: 97.38	
[09/16 09:53:25 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 09:53:36 visual_prompt]: Epoch 79 / 100: avg data time: 1.54e-01, avg batch time: 0.5547, average train loss: 0.0021
[09/16 09:53:40 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1424, average loss: 0.0017
[09/16 09:53:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 09:54:02 visual_prompt]: 	Test 100/407. loss: 2.449, 0.1950 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 09:54:21 visual_prompt]: 	Test 200/407. loss: 1.877, 0.1974 s / batch. (data: 1.57e-02)max mem: 17.22449 GB 
[09/16 09:54:41 visual_prompt]: 	Test 300/407. loss: 1.621, 0.1958 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 09:55:00 visual_prompt]: 	Test 400/407. loss: 2.160, 0.2376 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 09:55:04 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1924, average loss: 1.6252
[09/16 09:55:04 visual_prompt]: Classification results with test_vtab-svhn: top1: 79.99	top5: 97.39	
[09/16 09:55:04 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 09:55:14 visual_prompt]: Epoch 80 / 100: avg data time: 1.63e-01, avg batch time: 0.5628, average train loss: 0.0025
[09/16 09:55:19 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1424, average loss: 0.0002
[09/16 09:55:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 09:55:41 visual_prompt]: 	Test 100/407. loss: 2.417, 0.1825 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 09:56:00 visual_prompt]: 	Test 200/407. loss: 1.753, 0.1954 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 09:56:20 visual_prompt]: 	Test 300/407. loss: 1.387, 0.1953 s / batch. (data: 1.49e-04)max mem: 17.22449 GB 
[09/16 09:56:39 visual_prompt]: 	Test 400/407. loss: 2.265, 0.1830 s / batch. (data: 3.46e-05)max mem: 17.22449 GB 
[09/16 09:56:43 visual_prompt]: Inference (test):avg data time: 8.55e-03, avg batch time: 0.1937, average loss: 1.5288
[09/16 09:56:43 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.47	top5: 97.61	
[09/16 09:56:43 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 09:56:53 visual_prompt]: Epoch 81 / 100: avg data time: 1.60e-01, avg batch time: 0.5632, average train loss: 0.0010
[09/16 09:56:58 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1425, average loss: 0.0001
[09/16 09:56:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 09:57:20 visual_prompt]: 	Test 100/407. loss: 2.187, 0.2028 s / batch. (data: 6.37e-05)max mem: 17.22449 GB 
[09/16 09:57:39 visual_prompt]: 	Test 200/407. loss: 1.632, 0.2191 s / batch. (data: 3.71e-02)max mem: 17.22449 GB 
[09/16 09:57:59 visual_prompt]: 	Test 300/407. loss: 1.368, 0.2182 s / batch. (data: 3.63e-02)max mem: 17.22449 GB 
[09/16 09:58:19 visual_prompt]: 	Test 400/407. loss: 2.260, 0.1829 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 09:58:22 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1943, average loss: 1.5179
[09/16 09:58:22 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.52	top5: 97.60	
[09/16 09:58:22 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 09:58:33 visual_prompt]: Epoch 82 / 100: avg data time: 1.60e-01, avg batch time: 0.5817, average train loss: 0.0036
[09/16 09:58:38 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1438, average loss: 0.0003
[09/16 09:58:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 09:59:00 visual_prompt]: 	Test 100/407. loss: 2.068, 0.1831 s / batch. (data: 1.11e-04)max mem: 17.22449 GB 
[09/16 09:59:19 visual_prompt]: 	Test 200/407. loss: 1.589, 0.2015 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 09:59:38 visual_prompt]: 	Test 300/407. loss: 1.360, 0.2181 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 09:59:58 visual_prompt]: 	Test 400/407. loss: 2.052, 0.1819 s / batch. (data: 3.79e-05)max mem: 17.22449 GB 
[09/16 10:00:01 visual_prompt]: Inference (test):avg data time: 8.06e-03, avg batch time: 0.1931, average loss: 1.4491
[09/16 10:00:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.67	top5: 97.72	
[09/16 10:00:01 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 10:00:12 visual_prompt]: Epoch 83 / 100: avg data time: 1.57e-01, avg batch time: 0.5567, average train loss: 0.0008
[09/16 10:00:16 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1437, average loss: 0.0005
[09/16 10:00:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:00:38 visual_prompt]: 	Test 100/407. loss: 2.075, 0.1956 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 10:00:58 visual_prompt]: 	Test 200/407. loss: 1.680, 0.1968 s / batch. (data: 1.61e-04)max mem: 17.22449 GB 
[09/16 10:01:17 visual_prompt]: 	Test 300/407. loss: 1.326, 0.1957 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 10:01:37 visual_prompt]: 	Test 400/407. loss: 2.147, 0.1825 s / batch. (data: 3.67e-05)max mem: 17.22449 GB 
[09/16 10:01:40 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1940, average loss: 1.5129
[09/16 10:01:40 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.18	top5: 97.61	
[09/16 10:01:40 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 10:01:51 visual_prompt]: Epoch 84 / 100: avg data time: 1.52e-01, avg batch time: 0.5558, average train loss: 0.0004
[09/16 10:01:55 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1424, average loss: 0.0002
[09/16 10:01:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:02:17 visual_prompt]: 	Test 100/407. loss: 2.087, 0.1963 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 10:02:37 visual_prompt]: 	Test 200/407. loss: 1.662, 0.1825 s / batch. (data: 1.65e-04)max mem: 17.22449 GB 
[09/16 10:02:56 visual_prompt]: 	Test 300/407. loss: 1.289, 0.2079 s / batch. (data: 1.50e-04)max mem: 17.22449 GB 
[09/16 10:03:16 visual_prompt]: 	Test 400/407. loss: 2.171, 0.1825 s / batch. (data: 3.89e-05)max mem: 17.22449 GB 
[09/16 10:03:19 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1939, average loss: 1.5007
[09/16 10:03:19 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.23	top5: 97.59	
[09/16 10:03:19 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 10:03:30 visual_prompt]: Epoch 85 / 100: avg data time: 1.53e-01, avg batch time: 0.5558, average train loss: 0.0004
[09/16 10:03:34 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1426, average loss: 0.0001
[09/16 10:03:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:03:56 visual_prompt]: 	Test 100/407. loss: 2.110, 0.1825 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 10:04:16 visual_prompt]: 	Test 200/407. loss: 1.656, 0.1827 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 10:04:35 visual_prompt]: 	Test 300/407. loss: 1.281, 0.2377 s / batch. (data: 5.63e-02)max mem: 17.22449 GB 
[09/16 10:04:55 visual_prompt]: 	Test 400/407. loss: 2.178, 0.1823 s / batch. (data: 3.58e-05)max mem: 17.22449 GB 
[09/16 10:04:58 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1942, average loss: 1.4947
[09/16 10:04:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.35	top5: 97.58	
[09/16 10:04:58 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 10:05:09 visual_prompt]: Epoch 86 / 100: avg data time: 1.47e-01, avg batch time: 0.5547, average train loss: 0.0002
[09/16 10:05:13 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1426, average loss: 0.0001
[09/16 10:05:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:05:35 visual_prompt]: 	Test 100/407. loss: 2.089, 0.1824 s / batch. (data: 1.09e-04)max mem: 17.22449 GB 
[09/16 10:05:55 visual_prompt]: 	Test 200/407. loss: 1.633, 0.2016 s / batch. (data: 1.98e-02)max mem: 17.22449 GB 
[09/16 10:06:14 visual_prompt]: 	Test 300/407. loss: 1.273, 0.2075 s / batch. (data: 2.59e-02)max mem: 17.22449 GB 
[09/16 10:06:34 visual_prompt]: 	Test 400/407. loss: 2.159, 0.1825 s / batch. (data: 3.98e-05)max mem: 17.22449 GB 
[09/16 10:06:37 visual_prompt]: Inference (test):avg data time: 8.26e-03, avg batch time: 0.1931, average loss: 1.4806
[09/16 10:06:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.41	top5: 97.60	
[09/16 10:06:37 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 10:06:48 visual_prompt]: Epoch 87 / 100: avg data time: 1.59e-01, avg batch time: 0.5624, average train loss: 0.0003
[09/16 10:06:52 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1424, average loss: 0.0001
[09/16 10:06:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:07:14 visual_prompt]: 	Test 100/407. loss: 2.073, 0.1948 s / batch. (data: 1.32e-02)max mem: 17.22449 GB 
[09/16 10:07:33 visual_prompt]: 	Test 200/407. loss: 1.617, 0.1967 s / batch. (data: 1.50e-04)max mem: 17.22449 GB 
[09/16 10:07:53 visual_prompt]: 	Test 300/407. loss: 1.260, 0.1827 s / batch. (data: 1.50e-04)max mem: 17.22449 GB 
[09/16 10:08:12 visual_prompt]: 	Test 400/407. loss: 2.147, 0.1819 s / batch. (data: 2.48e-05)max mem: 17.22449 GB 
[09/16 10:08:15 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1924, average loss: 1.4687
[09/16 10:08:15 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.47	top5: 97.61	
[09/16 10:08:15 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 10:08:26 visual_prompt]: Epoch 88 / 100: avg data time: 1.65e-01, avg batch time: 0.5652, average train loss: 0.0002
[09/16 10:08:31 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1426, average loss: 0.0001
[09/16 10:08:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:08:52 visual_prompt]: 	Test 100/407. loss: 2.057, 0.1816 s / batch. (data: 1.53e-04)max mem: 17.22449 GB 
[09/16 10:09:12 visual_prompt]: 	Test 200/407. loss: 1.600, 0.1828 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 10:09:31 visual_prompt]: 	Test 300/407. loss: 1.242, 0.2020 s / batch. (data: 1.75e-02)max mem: 17.22449 GB 
[09/16 10:09:50 visual_prompt]: 	Test 400/407. loss: 2.140, 0.1924 s / batch. (data: 3.60e-05)max mem: 17.22449 GB 
[09/16 10:09:54 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1928, average loss: 1.4579
[09/16 10:09:54 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.54	top5: 97.62	
[09/16 10:09:54 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 10:10:05 visual_prompt]: Epoch 89 / 100: avg data time: 1.67e-01, avg batch time: 0.5670, average train loss: 0.0002
[09/16 10:10:09 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1426, average loss: 0.0000
[09/16 10:10:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:10:31 visual_prompt]: 	Test 100/407. loss: 2.039, 0.1946 s / batch. (data: 1.29e-02)max mem: 17.22449 GB 
[09/16 10:10:50 visual_prompt]: 	Test 200/407. loss: 1.587, 0.2101 s / batch. (data: 9.16e-05)max mem: 17.22449 GB 
[09/16 10:11:10 visual_prompt]: 	Test 300/407. loss: 1.229, 0.1992 s / batch. (data: 1.72e-02)max mem: 17.22449 GB 
[09/16 10:11:29 visual_prompt]: 	Test 400/407. loss: 2.141, 0.1822 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 10:11:32 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1933, average loss: 1.4498
[09/16 10:11:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.60	top5: 97.62	
[09/16 10:11:32 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 10:11:43 visual_prompt]: Epoch 90 / 100: avg data time: 1.53e-01, avg batch time: 0.5584, average train loss: 0.0002
[09/16 10:11:48 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1425, average loss: 0.0000
[09/16 10:11:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:12:10 visual_prompt]: 	Test 100/407. loss: 2.029, 0.2051 s / batch. (data: 1.05e-04)max mem: 17.22449 GB 
[09/16 10:12:29 visual_prompt]: 	Test 200/407. loss: 1.577, 0.1828 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 10:12:49 visual_prompt]: 	Test 300/407. loss: 1.223, 0.1832 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 10:13:08 visual_prompt]: 	Test 400/407. loss: 2.140, 0.1827 s / batch. (data: 3.81e-05)max mem: 17.22449 GB 
[09/16 10:13:12 visual_prompt]: Inference (test):avg data time: 8.21e-03, avg batch time: 0.1946, average loss: 1.4435
[09/16 10:13:12 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.61	top5: 97.63	
[09/16 10:13:12 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 10:13:23 visual_prompt]: Epoch 91 / 100: avg data time: 1.64e-01, avg batch time: 0.5656, average train loss: 0.0002
[09/16 10:13:27 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1425, average loss: 0.0000
[09/16 10:13:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:13:49 visual_prompt]: 	Test 100/407. loss: 2.031, 0.1820 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 10:14:08 visual_prompt]: 	Test 200/407. loss: 1.573, 0.1956 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 10:14:28 visual_prompt]: 	Test 300/407. loss: 1.222, 0.2010 s / batch. (data: 1.24e-02)max mem: 17.22449 GB 
[09/16 10:14:47 visual_prompt]: 	Test 400/407. loss: 2.144, 0.1831 s / batch. (data: 3.34e-05)max mem: 17.22449 GB 
[09/16 10:14:51 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1943, average loss: 1.4412
[09/16 10:14:51 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.65	top5: 97.63	
[09/16 10:14:51 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 10:15:01 visual_prompt]: Epoch 92 / 100: avg data time: 1.46e-01, avg batch time: 0.5488, average train loss: 0.0002
[09/16 10:15:06 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1426, average loss: 0.0000
[09/16 10:15:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:15:28 visual_prompt]: 	Test 100/407. loss: 2.030, 0.1825 s / batch. (data: 1.48e-04)max mem: 17.22449 GB 
[09/16 10:15:47 visual_prompt]: 	Test 200/407. loss: 1.573, 0.1829 s / batch. (data: 1.49e-04)max mem: 17.22449 GB 
[09/16 10:16:07 visual_prompt]: 	Test 300/407. loss: 1.222, 0.1956 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 10:16:26 visual_prompt]: 	Test 400/407. loss: 2.140, 0.1828 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 10:16:30 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1929, average loss: 1.4396
[09/16 10:16:30 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.62	top5: 97.61	
[09/16 10:16:30 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 10:16:40 visual_prompt]: Epoch 93 / 100: avg data time: 1.55e-01, avg batch time: 0.5571, average train loss: 0.0002
[09/16 10:16:45 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1424, average loss: 0.0000
[09/16 10:16:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:17:07 visual_prompt]: 	Test 100/407. loss: 2.033, 0.1826 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 10:17:26 visual_prompt]: 	Test 200/407. loss: 1.575, 0.1829 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 10:17:46 visual_prompt]: 	Test 300/407. loss: 1.224, 0.1950 s / batch. (data: 1.29e-02)max mem: 17.22449 GB 
[09/16 10:18:05 visual_prompt]: 	Test 400/407. loss: 2.139, 0.1818 s / batch. (data: 4.27e-05)max mem: 17.22449 GB 
[09/16 10:18:08 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1930, average loss: 1.4394
[09/16 10:18:08 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.62	top5: 97.62	
[09/16 10:18:08 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 10:18:19 visual_prompt]: Epoch 94 / 100: avg data time: 1.50e-01, avg batch time: 0.5515, average train loss: 0.0001
[09/16 10:18:23 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1425, average loss: 0.0000
[09/16 10:18:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:18:46 visual_prompt]: 	Test 100/407. loss: 2.030, 0.1823 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 10:19:05 visual_prompt]: 	Test 200/407. loss: 1.573, 0.1973 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 10:19:25 visual_prompt]: 	Test 300/407. loss: 1.223, 0.2026 s / batch. (data: 1.78e-02)max mem: 17.22449 GB 
[09/16 10:19:44 visual_prompt]: 	Test 400/407. loss: 2.137, 0.1825 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 10:19:47 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1943, average loss: 1.4374
[09/16 10:19:47 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.61	top5: 97.61	
[09/16 10:19:47 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 10:19:58 visual_prompt]: Epoch 95 / 100: avg data time: 1.55e-01, avg batch time: 0.5588, average train loss: 0.0002
[09/16 10:20:03 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1425, average loss: 0.0000
[09/16 10:20:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:20:25 visual_prompt]: 	Test 100/407. loss: 2.030, 0.1820 s / batch. (data: 9.06e-05)max mem: 17.22449 GB 
[09/16 10:20:44 visual_prompt]: 	Test 200/407. loss: 1.571, 0.1829 s / batch. (data: 1.07e-04)max mem: 17.22449 GB 
[09/16 10:21:04 visual_prompt]: 	Test 300/407. loss: 1.220, 0.1949 s / batch. (data: 1.31e-02)max mem: 17.22449 GB 
[09/16 10:21:24 visual_prompt]: 	Test 400/407. loss: 2.138, 0.1828 s / batch. (data: 3.62e-05)max mem: 17.22449 GB 
[09/16 10:21:27 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1958, average loss: 1.4361
[09/16 10:21:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.64	top5: 97.61	
[09/16 10:21:27 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 10:21:38 visual_prompt]: Epoch 96 / 100: avg data time: 1.62e-01, avg batch time: 0.5610, average train loss: 0.0001
[09/16 10:21:42 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1425, average loss: 0.0000
[09/16 10:21:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:22:05 visual_prompt]: 	Test 100/407. loss: 2.029, 0.1958 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 10:22:24 visual_prompt]: 	Test 200/407. loss: 1.570, 0.1960 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 10:22:43 visual_prompt]: 	Test 300/407. loss: 1.219, 0.1828 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 10:23:03 visual_prompt]: 	Test 400/407. loss: 2.138, 0.1826 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 10:23:07 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1949, average loss: 1.4355
[09/16 10:23:07 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.64	top5: 97.61	
[09/16 10:23:07 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 10:23:18 visual_prompt]: Epoch 97 / 100: avg data time: 1.62e-01, avg batch time: 0.5626, average train loss: 0.0002
[09/16 10:23:22 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1423, average loss: 0.0000
[09/16 10:23:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:23:44 visual_prompt]: 	Test 100/407. loss: 2.028, 0.1956 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 10:24:03 visual_prompt]: 	Test 200/407. loss: 1.569, 0.1956 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 10:24:23 visual_prompt]: 	Test 300/407. loss: 1.219, 0.1961 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 10:24:42 visual_prompt]: 	Test 400/407. loss: 2.138, 0.1827 s / batch. (data: 4.01e-05)max mem: 17.22449 GB 
[09/16 10:24:46 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1936, average loss: 1.4350
[09/16 10:24:46 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.64	top5: 97.61	
[09/16 10:24:46 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 10:24:56 visual_prompt]: Epoch 98 / 100: avg data time: 1.58e-01, avg batch time: 0.5590, average train loss: 0.0002
[09/16 10:25:01 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1427, average loss: 0.0000
[09/16 10:25:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:25:23 visual_prompt]: 	Test 100/407. loss: 2.028, 0.1825 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 10:25:42 visual_prompt]: 	Test 200/407. loss: 1.570, 0.2107 s / batch. (data: 1.65e-04)max mem: 17.22449 GB 
[09/16 10:26:02 visual_prompt]: 	Test 300/407. loss: 1.219, 0.1818 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 10:26:21 visual_prompt]: 	Test 400/407. loss: 2.138, 0.1828 s / batch. (data: 2.91e-05)max mem: 17.22449 GB 
[09/16 10:26:24 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1927, average loss: 1.4346
[09/16 10:26:24 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.65	top5: 97.61	
[09/16 10:26:24 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 10:26:35 visual_prompt]: Epoch 99 / 100: avg data time: 1.45e-01, avg batch time: 0.5469, average train loss: 0.0002
[09/16 10:26:40 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1424, average loss: 0.0000
[09/16 10:26:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:27:02 visual_prompt]: 	Test 100/407. loss: 2.027, 0.1930 s / batch. (data: 1.13e-02)max mem: 17.22449 GB 
[09/16 10:27:21 visual_prompt]: 	Test 200/407. loss: 1.569, 0.1834 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 10:27:41 visual_prompt]: 	Test 300/407. loss: 1.218, 0.1829 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 10:28:00 visual_prompt]: 	Test 400/407. loss: 2.137, 0.1823 s / batch. (data: 3.43e-05)max mem: 17.22449 GB 
[09/16 10:28:03 visual_prompt]: Inference (test):avg data time: 8.11e-03, avg batch time: 0.1940, average loss: 1.4342
[09/16 10:28:03 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.65	top5: 97.61	
[09/16 10:28:03 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 10:28:14 visual_prompt]: Epoch 100 / 100: avg data time: 1.56e-01, avg batch time: 0.5562, average train loss: 0.0002
[09/16 10:28:19 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1426, average loss: 0.0000
[09/16 10:28:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 10:28:41 visual_prompt]: 	Test 100/407. loss: 2.027, 0.1957 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 10:29:00 visual_prompt]: 	Test 200/407. loss: 1.569, 0.1826 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 10:29:20 visual_prompt]: 	Test 300/407. loss: 1.218, 0.1963 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 10:29:39 visual_prompt]: 	Test 400/407. loss: 2.138, 0.1824 s / batch. (data: 4.20e-05)max mem: 17.22449 GB 
[09/16 10:29:42 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1932, average loss: 1.4342
[09/16 10:29:42 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.65	top5: 97.61	
[09/16 10:30:25 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 10:30:26 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 10:30:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed44'], train_type='')
[09/16 10:30:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 10:30:26 visual_prompt]: Training with config:
[09/16 10:30:26 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-svhn',
          'NO_TEST': False,
          'NUMBER_CLASSES': 10,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed44/vtab-svhn/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 10:30:26 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 10:30:26.687042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 10:30:27.190341: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 10:30:29.992821: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 10:30:29.993129: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 10:30:29.993155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 10:30:37.323724: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 10:30:37.324201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 10:30:37.324252: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 10:30:37 visual_prompt]: Constructing vtab-svhn dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
2023-09-16 10:30:37.354290: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split train[:800]+train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 10:30:39 visual_prompt]: Number of images: 1000
[09/16 10:30:39 visual_prompt]: Number of classes: 10 / 10
[09/16 10:30:39 visual_prompt]: Loading validation data...
[09/16 10:30:39 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 10:30:39 visual_prompt]: Number of images: 200
[09/16 10:30:39 visual_prompt]: Number of classes: 10 / 10
[09/16 10:30:39 visual_prompt]: Loading test data...
[09/16 10:30:39 visual_prompt]: Constructing vtab-svhn dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split test, from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 10:31:11 visual_prompt]: Number of images: 26032
[09/16 10:31:11 visual_prompt]: Number of classes: 10 / 10
[09/16 10:31:11 visual_prompt]: Constructing models...
[09/16 10:31:13 visual_prompt]: Total Parameters: 86727946	 Gradient Parameters: 929290
[09/16 10:31:13 visual_prompt]: tuned percent:1.072
[09/16 10:31:16 visual_prompt]: Device used for model: 0
[09/16 10:31:16 visual_prompt]: Setting up Evalutator...
[09/16 10:31:16 visual_prompt]: Setting up Trainer...
[09/16 10:31:16 visual_prompt]: 	Setting up the optimizer...
[09/16 10:31:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 10:31:28 visual_prompt]: Epoch 1 / 100: avg data time: 1.49e-01, avg batch time: 0.6321, average train loss: 2.4384
[09/16 10:31:32 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.1414, average loss: 2.4273
[09/16 10:31:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 54.00	
[09/16 10:31:54 visual_prompt]: 	Test 100/407. loss: 2.448, 0.2076 s / batch. (data: 2.69e-02)max mem: 17.22449 GB 
[09/16 10:32:13 visual_prompt]: 	Test 200/407. loss: 2.444, 0.2080 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 10:32:33 visual_prompt]: 	Test 300/407. loss: 2.432, 0.1819 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 10:32:52 visual_prompt]: 	Test 400/407. loss: 2.453, 0.1827 s / batch. (data: 2.57e-05)max mem: 17.22449 GB 
[09/16 10:32:55 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1937, average loss: 2.4596
[09/16 10:32:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 10.66	top5: 50.76	
[09/16 10:32:55 visual_prompt]: Best epoch 1: best metric: 0.120
[09/16 10:32:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 10:33:06 visual_prompt]: Epoch 2 / 100: avg data time: 1.48e-01, avg batch time: 0.5505, average train loss: 2.7828
[09/16 10:33:10 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1425, average loss: 2.3508
[09/16 10:33:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 60.00	
[09/16 10:33:32 visual_prompt]: 	Test 100/407. loss: 2.442, 0.1841 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 10:33:52 visual_prompt]: 	Test 200/407. loss: 2.441, 0.1878 s / batch. (data: 5.15e-03)max mem: 17.22449 GB 
[09/16 10:34:11 visual_prompt]: 	Test 300/407. loss: 2.331, 0.1976 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 10:34:31 visual_prompt]: 	Test 400/407. loss: 2.429, 0.1826 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 10:34:34 visual_prompt]: Inference (test):avg data time: 6.84e-03, avg batch time: 0.1942, average loss: 2.3448
[09/16 10:34:34 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.38	top5: 59.67	
[09/16 10:34:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 10:34:44 visual_prompt]: Epoch 3 / 100: avg data time: 1.40e-01, avg batch time: 0.5425, average train loss: 2.4086
[09/16 10:34:49 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1426, average loss: 2.3648
[09/16 10:34:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/16 10:35:10 visual_prompt]: 	Test 100/407. loss: 2.400, 0.1894 s / batch. (data: 1.07e-04)max mem: 17.22449 GB 
[09/16 10:35:30 visual_prompt]: 	Test 200/407. loss: 2.415, 0.2165 s / batch. (data: 8.03e-05)max mem: 17.22449 GB 
[09/16 10:35:49 visual_prompt]: 	Test 300/407. loss: 2.295, 0.1864 s / batch. (data: 1.38e-04)max mem: 17.22449 GB 
[09/16 10:36:09 visual_prompt]: 	Test 400/407. loss: 2.317, 0.1825 s / batch. (data: 2.50e-05)max mem: 17.22449 GB 
[09/16 10:36:12 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1937, average loss: 2.3401
[09/16 10:36:12 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 52.56	
[09/16 10:36:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 10:36:23 visual_prompt]: Epoch 4 / 100: avg data time: 1.47e-01, avg batch time: 0.5493, average train loss: 2.4940
[09/16 10:36:27 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1424, average loss: 2.3985
[09/16 10:36:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/16 10:36:49 visual_prompt]: 	Test 100/407. loss: 2.499, 0.1826 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 10:37:08 visual_prompt]: 	Test 200/407. loss: 2.564, 0.1922 s / batch. (data: 1.04e-02)max mem: 17.22449 GB 
[09/16 10:37:27 visual_prompt]: 	Test 300/407. loss: 2.259, 0.1981 s / batch. (data: 1.55e-02)max mem: 17.22449 GB 
[09/16 10:37:47 visual_prompt]: 	Test 400/407. loss: 2.338, 0.1827 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 10:37:50 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1936, average loss: 2.3768
[09/16 10:37:50 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 61.97	
[09/16 10:37:50 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 10:38:01 visual_prompt]: Epoch 5 / 100: avg data time: 1.44e-01, avg batch time: 0.5457, average train loss: 2.4137
[09/16 10:38:05 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1426, average loss: 2.5167
[09/16 10:38:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 55.00	
[09/16 10:38:27 visual_prompt]: 	Test 100/407. loss: 2.462, 0.1895 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 10:38:46 visual_prompt]: 	Test 200/407. loss: 2.525, 0.1826 s / batch. (data: 1.49e-04)max mem: 17.22449 GB 
[09/16 10:39:06 visual_prompt]: 	Test 300/407. loss: 2.538, 0.1833 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 10:39:26 visual_prompt]: 	Test 400/407. loss: 2.483, 0.1829 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 10:39:29 visual_prompt]: Inference (test):avg data time: 6.90e-03, avg batch time: 0.1945, average loss: 2.4865
[09/16 10:39:29 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 54.10	
[09/16 10:39:29 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 10:39:39 visual_prompt]: Epoch 6 / 100: avg data time: 1.48e-01, avg batch time: 0.5488, average train loss: 2.4390
[09/16 10:39:44 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.1441, average loss: 2.7078
[09/16 10:39:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.50	
[09/16 10:40:06 visual_prompt]: 	Test 100/407. loss: 2.794, 0.1949 s / batch. (data: 1.32e-02)max mem: 17.22449 GB 
[09/16 10:40:25 visual_prompt]: 	Test 200/407. loss: 2.642, 0.1933 s / batch. (data: 1.12e-02)max mem: 17.22449 GB 
[09/16 10:40:45 visual_prompt]: 	Test 300/407. loss: 2.889, 0.1950 s / batch. (data: 1.27e-02)max mem: 17.22449 GB 
[09/16 10:41:04 visual_prompt]: 	Test 400/407. loss: 2.725, 0.1823 s / batch. (data: 3.67e-05)max mem: 17.22449 GB 
[09/16 10:41:07 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1931, average loss: 2.7157
[09/16 10:41:07 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 53.44	
[09/16 10:41:07 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 10:41:17 visual_prompt]: Epoch 7 / 100: avg data time: 1.53e-01, avg batch time: 0.5548, average train loss: 2.9170
[09/16 10:41:22 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1425, average loss: 2.8666
[09/16 10:41:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/16 10:41:44 visual_prompt]: 	Test 100/407. loss: 3.003, 0.1974 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 10:42:03 visual_prompt]: 	Test 200/407. loss: 3.212, 0.1951 s / batch. (data: 1.26e-02)max mem: 17.22449 GB 
[09/16 10:42:23 visual_prompt]: 	Test 300/407. loss: 2.538, 0.1958 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 10:42:42 visual_prompt]: 	Test 400/407. loss: 2.878, 0.1826 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 10:42:45 visual_prompt]: Inference (test):avg data time: 8.38e-03, avg batch time: 0.1939, average loss: 2.9354
[09/16 10:42:45 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 52.39	
[09/16 10:42:45 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 10:42:56 visual_prompt]: Epoch 8 / 100: avg data time: 1.36e-01, avg batch time: 0.5397, average train loss: 3.9574
[09/16 10:43:00 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1424, average loss: 3.9146
[09/16 10:43:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/16 10:43:22 visual_prompt]: 	Test 100/407. loss: 4.209, 0.1823 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 10:43:41 visual_prompt]: 	Test 200/407. loss: 4.393, 0.1960 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 10:44:00 visual_prompt]: 	Test 300/407. loss: 3.441, 0.1827 s / batch. (data: 1.06e-04)max mem: 17.22449 GB 
[09/16 10:44:20 visual_prompt]: 	Test 400/407. loss: 3.933, 0.1821 s / batch. (data: 3.55e-05)max mem: 17.22449 GB 
[09/16 10:44:23 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1926, average loss: 3.8046
[09/16 10:44:23 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 62.13	
[09/16 10:44:23 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 10:44:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.39e-01, avg batch time: 0.5412, average train loss: 3.5399
[09/16 10:44:38 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1425, average loss: 2.3837
[09/16 10:44:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/16 10:44:59 visual_prompt]: 	Test 100/407. loss: 2.767, 0.2030 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 10:45:19 visual_prompt]: 	Test 200/407. loss: 2.702, 0.2081 s / batch. (data: 2.59e-02)max mem: 17.22449 GB 
[09/16 10:45:38 visual_prompt]: 	Test 300/407. loss: 2.464, 0.1969 s / batch. (data: 1.48e-02)max mem: 17.22449 GB 
[09/16 10:45:58 visual_prompt]: 	Test 400/407. loss: 2.505, 0.1825 s / batch. (data: 3.31e-05)max mem: 17.22449 GB 
[09/16 10:46:01 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1943, average loss: 2.4760
[09/16 10:46:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 54.25	
[09/16 10:46:01 visual_prompt]: Best epoch 9: best metric: 0.230
[09/16 10:46:01 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 10:46:11 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.5473, average train loss: 3.7889
[09/16 10:46:16 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1426, average loss: 3.8237
[09/16 10:46:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.50	
[09/16 10:46:38 visual_prompt]: 	Test 100/407. loss: 3.252, 0.1999 s / batch. (data: 1.85e-02)max mem: 17.22449 GB 
[09/16 10:46:57 visual_prompt]: 	Test 200/407. loss: 3.337, 0.2312 s / batch. (data: 3.30e-02)max mem: 17.22449 GB 
[09/16 10:47:17 visual_prompt]: 	Test 300/407. loss: 3.761, 0.1826 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 10:47:36 visual_prompt]: 	Test 400/407. loss: 3.432, 0.1845 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 10:47:39 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1927, average loss: 3.6247
[09/16 10:47:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 47.29	
[09/16 10:47:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 10:47:50 visual_prompt]: Epoch 11 / 100: avg data time: 1.47e-01, avg batch time: 0.5485, average train loss: 10.8087
[09/16 10:47:54 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1427, average loss: 19.7975
[09/16 10:47:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.00	
[09/16 10:48:16 visual_prompt]: 	Test 100/407. loss: 23.608, 0.2024 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 10:48:36 visual_prompt]: 	Test 200/407. loss: 21.965, 0.1960 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 10:48:55 visual_prompt]: 	Test 300/407. loss: 22.560, 0.1936 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 10:49:15 visual_prompt]: 	Test 400/407. loss: 21.328, 0.1827 s / batch. (data: 3.62e-05)max mem: 17.22449 GB 
[09/16 10:49:18 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1954, average loss: 21.0838
[09/16 10:49:19 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 53.54	
[09/16 10:49:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 10:49:29 visual_prompt]: Epoch 12 / 100: avg data time: 1.50e-01, avg batch time: 0.5510, average train loss: 24.4532
[09/16 10:49:33 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1424, average loss: 32.1398
[09/16 10:49:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 52.50	
[09/16 10:49:55 visual_prompt]: 	Test 100/407. loss: 35.352, 0.1959 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 10:50:14 visual_prompt]: 	Test 200/407. loss: 33.059, 0.2078 s / batch. (data: 2.62e-02)max mem: 17.22449 GB 
[09/16 10:50:34 visual_prompt]: 	Test 300/407. loss: 33.712, 0.1828 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 10:50:53 visual_prompt]: 	Test 400/407. loss: 34.246, 0.1836 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 10:50:56 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1934, average loss: 33.5498
[09/16 10:50:57 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.38	top5: 49.95	
[09/16 10:50:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 10:51:07 visual_prompt]: Epoch 13 / 100: avg data time: 1.44e-01, avg batch time: 0.5451, average train loss: 22.8183
[09/16 10:51:11 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1425, average loss: 23.1833
[09/16 10:51:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 60.00	
[09/16 10:51:33 visual_prompt]: 	Test 100/407. loss: 21.402, 0.1822 s / batch. (data: 1.52e-04)max mem: 17.22449 GB 
[09/16 10:51:53 visual_prompt]: 	Test 200/407. loss: 23.402, 0.1831 s / batch. (data: 1.58e-04)max mem: 17.22449 GB 
[09/16 10:52:12 visual_prompt]: 	Test 300/407. loss: 20.633, 0.1830 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 10:52:31 visual_prompt]: 	Test 400/407. loss: 21.377, 0.1825 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 10:52:35 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1948, average loss: 23.0830
[09/16 10:52:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.59	top5: 60.57	
[09/16 10:52:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 10:52:45 visual_prompt]: Epoch 14 / 100: avg data time: 1.46e-01, avg batch time: 0.5469, average train loss: 15.7300
[09/16 10:52:50 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1430, average loss: 13.2177
[09/16 10:52:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/16 10:53:12 visual_prompt]: 	Test 100/407. loss: 12.087, 0.1960 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 10:53:31 visual_prompt]: 	Test 200/407. loss: 14.354, 0.1824 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 10:53:51 visual_prompt]: 	Test 300/407. loss: 12.395, 0.1953 s / batch. (data: 1.30e-02)max mem: 17.22449 GB 
[09/16 10:54:10 visual_prompt]: 	Test 400/407. loss: 13.799, 0.1833 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 10:54:13 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1940, average loss: 13.1458
[09/16 10:54:13 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 59.14	
[09/16 10:54:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 10:54:24 visual_prompt]: Epoch 15 / 100: avg data time: 1.42e-01, avg batch time: 0.5467, average train loss: 17.0017
[09/16 10:54:28 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1423, average loss: 17.8752
[09/16 10:54:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.50	
[09/16 10:54:50 visual_prompt]: 	Test 100/407. loss: 20.151, 0.1834 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 10:55:09 visual_prompt]: 	Test 200/407. loss: 17.528, 0.1960 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 10:55:29 visual_prompt]: 	Test 300/407. loss: 19.302, 0.1826 s / batch. (data: 1.06e-04)max mem: 17.22449 GB 
[09/16 10:55:48 visual_prompt]: 	Test 400/407. loss: 17.453, 0.1825 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 10:55:52 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1945, average loss: 17.9141
[09/16 10:55:52 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 54.07	
[09/16 10:55:52 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 10:56:02 visual_prompt]: Epoch 16 / 100: avg data time: 1.45e-01, avg batch time: 0.5465, average train loss: 19.1380
[09/16 10:56:06 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1424, average loss: 22.0697
[09/16 10:56:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/16 10:56:28 visual_prompt]: 	Test 100/407. loss: 27.320, 0.1966 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 10:56:48 visual_prompt]: 	Test 200/407. loss: 26.603, 0.1826 s / batch. (data: 1.08e-04)max mem: 17.22449 GB 
[09/16 10:57:08 visual_prompt]: 	Test 300/407. loss: 24.537, 0.2322 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 10:57:28 visual_prompt]: 	Test 400/407. loss: 25.670, 0.1823 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 10:57:31 visual_prompt]: Inference (test):avg data time: 8.58e-03, avg batch time: 0.1958, average loss: 24.7265
[09/16 10:57:31 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 52.39	
[09/16 10:57:31 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 10:57:41 visual_prompt]: Epoch 17 / 100: avg data time: 1.43e-01, avg batch time: 0.5432, average train loss: 13.3782
[09/16 10:57:46 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1423, average loss: 9.0408
[09/16 10:57:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 65.00	
[09/16 10:58:08 visual_prompt]: 	Test 100/407. loss: 9.662, 0.1851 s / batch. (data: 8.56e-05)max mem: 17.22449 GB 
[09/16 10:58:27 visual_prompt]: 	Test 200/407. loss: 10.912, 0.1824 s / batch. (data: 8.32e-05)max mem: 17.22449 GB 
[09/16 10:58:46 visual_prompt]: 	Test 300/407. loss: 8.788, 0.1951 s / batch. (data: 1.28e-02)max mem: 17.22449 GB 
[09/16 10:59:06 visual_prompt]: 	Test 400/407. loss: 8.938, 0.1827 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 10:59:09 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1931, average loss: 8.9398
[09/16 10:59:09 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 61.59	
[09/16 10:59:09 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 10:59:20 visual_prompt]: Epoch 18 / 100: avg data time: 1.49e-01, avg batch time: 0.5491, average train loss: 11.1720
[09/16 10:59:24 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.1426, average loss: 12.4530
[09/16 10:59:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 39.50	
[09/16 10:59:46 visual_prompt]: 	Test 100/407. loss: 11.612, 0.1841 s / batch. (data: 1.97e-03)max mem: 17.22449 GB 
[09/16 11:00:05 visual_prompt]: 	Test 200/407. loss: 10.619, 0.1828 s / batch. (data: 1.43e-04)max mem: 17.22449 GB 
[09/16 11:00:25 visual_prompt]: 	Test 300/407. loss: 12.426, 0.1827 s / batch. (data: 1.55e-04)max mem: 17.22449 GB 
[09/16 11:00:44 visual_prompt]: 	Test 400/407. loss: 11.301, 0.1826 s / batch. (data: 2.60e-05)max mem: 17.22449 GB 
[09/16 11:00:47 visual_prompt]: Inference (test):avg data time: 8.57e-03, avg batch time: 0.1935, average loss: 11.5931
[09/16 11:00:47 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 44.83	
[09/16 11:00:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 11:00:58 visual_prompt]: Epoch 19 / 100: avg data time: 1.54e-01, avg batch time: 0.5609, average train loss: 13.0142
[09/16 11:01:02 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1426, average loss: 6.9906
[09/16 11:01:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/16 11:01:24 visual_prompt]: 	Test 100/407. loss: 8.272, 0.1824 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 11:01:43 visual_prompt]: 	Test 200/407. loss: 7.605, 0.1969 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 11:02:03 visual_prompt]: 	Test 300/407. loss: 7.584, 0.1975 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 11:02:23 visual_prompt]: 	Test 400/407. loss: 7.789, 0.1827 s / batch. (data: 3.50e-05)max mem: 17.22449 GB 
[09/16 11:02:26 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1954, average loss: 7.3224
[09/16 11:02:26 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 54.32	
[09/16 11:02:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 11:02:37 visual_prompt]: Epoch 20 / 100: avg data time: 1.49e-01, avg batch time: 0.5499, average train loss: 6.7343
[09/16 11:02:41 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1431, average loss: 6.1179
[09/16 11:02:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 60.50	
[09/16 11:03:03 visual_prompt]: 	Test 100/407. loss: 7.634, 0.1875 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 11:03:23 visual_prompt]: 	Test 200/407. loss: 6.040, 0.1830 s / batch. (data: 1.64e-04)max mem: 17.22449 GB 
[09/16 11:03:42 visual_prompt]: 	Test 300/407. loss: 6.320, 0.1827 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 11:04:01 visual_prompt]: 	Test 400/407. loss: 5.687, 0.1827 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 11:04:05 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1939, average loss: 6.1128
[09/16 11:04:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.59	top5: 57.08	
[09/16 11:04:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 11:04:15 visual_prompt]: Epoch 21 / 100: avg data time: 1.51e-01, avg batch time: 0.5515, average train loss: 6.4673
[09/16 11:04:19 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1426, average loss: 10.0285
[09/16 11:04:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/16 11:04:41 visual_prompt]: 	Test 100/407. loss: 10.110, 0.1826 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 11:05:00 visual_prompt]: 	Test 200/407. loss: 10.109, 0.1960 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 11:05:20 visual_prompt]: 	Test 300/407. loss: 9.869, 0.2080 s / batch. (data: 2.60e-02)max mem: 17.22449 GB 
[09/16 11:05:39 visual_prompt]: 	Test 400/407. loss: 9.115, 0.1830 s / batch. (data: 2.81e-05)max mem: 17.22449 GB 
[09/16 11:05:42 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1925, average loss: 9.6981
[09/16 11:05:42 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 50.58	
[09/16 11:05:42 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 11:05:53 visual_prompt]: Epoch 22 / 100: avg data time: 1.46e-01, avg batch time: 0.5463, average train loss: 10.3753
[09/16 11:05:57 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1428, average loss: 12.9345
[09/16 11:05:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.00	
[09/16 11:06:19 visual_prompt]: 	Test 100/407. loss: 14.129, 0.1824 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 11:06:38 visual_prompt]: 	Test 200/407. loss: 14.707, 0.2040 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 11:06:58 visual_prompt]: 	Test 300/407. loss: 12.088, 0.2039 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 11:07:18 visual_prompt]: 	Test 400/407. loss: 12.920, 0.1866 s / batch. (data: 2.62e-05)max mem: 17.22449 GB 
[09/16 11:07:21 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1940, average loss: 12.8722
[09/16 11:07:21 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 63.35	
[09/16 11:07:21 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 11:07:31 visual_prompt]: Epoch 23 / 100: avg data time: 1.37e-01, avg batch time: 0.5404, average train loss: 10.1350
[09/16 11:07:35 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1445, average loss: 10.8426
[09/16 11:07:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 56.00	
[09/16 11:07:58 visual_prompt]: 	Test 100/407. loss: 14.136, 0.1972 s / batch. (data: 1.50e-02)max mem: 17.22449 GB 
[09/16 11:08:17 visual_prompt]: 	Test 200/407. loss: 12.050, 0.1923 s / batch. (data: 1.03e-02)max mem: 17.22449 GB 
[09/16 11:08:36 visual_prompt]: 	Test 300/407. loss: 11.954, 0.1823 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 11:08:55 visual_prompt]: 	Test 400/407. loss: 9.720, 0.1824 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 11:08:59 visual_prompt]: Inference (test):avg data time: 7.11e-03, avg batch time: 0.1932, average loss: 11.0407
[09/16 11:08:59 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.59	top5: 55.95	
[09/16 11:08:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 11:09:09 visual_prompt]: Epoch 24 / 100: avg data time: 1.44e-01, avg batch time: 0.5459, average train loss: 12.5541
[09/16 11:09:13 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1424, average loss: 13.1405
[09/16 11:09:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 44.50	
[09/16 11:09:35 visual_prompt]: 	Test 100/407. loss: 12.769, 0.1830 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 11:09:54 visual_prompt]: 	Test 200/407. loss: 12.833, 0.2074 s / batch. (data: 2.58e-02)max mem: 17.22449 GB 
[09/16 11:10:14 visual_prompt]: 	Test 300/407. loss: 12.143, 0.1941 s / batch. (data: 1.22e-02)max mem: 17.22449 GB 
[09/16 11:10:33 visual_prompt]: 	Test 400/407. loss: 12.332, 0.1826 s / batch. (data: 3.34e-05)max mem: 17.22449 GB 
[09/16 11:10:37 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1928, average loss: 12.8791
[09/16 11:10:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.38	top5: 47.27	
[09/16 11:10:37 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 11:10:47 visual_prompt]: Epoch 25 / 100: avg data time: 1.48e-01, avg batch time: 0.5535, average train loss: 9.6730
[09/16 11:10:52 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1423, average loss: 8.9742
[09/16 11:10:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 42.50	
[09/16 11:11:13 visual_prompt]: 	Test 100/407. loss: 6.963, 0.1831 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 11:11:33 visual_prompt]: 	Test 200/407. loss: 7.094, 0.1820 s / batch. (data: 1.03e-04)max mem: 17.22449 GB 
[09/16 11:11:52 visual_prompt]: 	Test 300/407. loss: 9.045, 0.2217 s / batch. (data: 1.47e-02)max mem: 17.22449 GB 
[09/16 11:12:12 visual_prompt]: 	Test 400/407. loss: 7.600, 0.1834 s / batch. (data: 2.77e-05)max mem: 17.22449 GB 
[09/16 11:12:15 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1931, average loss: 8.3318
[09/16 11:12:15 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 47.61	
[09/16 11:12:15 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 11:12:25 visual_prompt]: Epoch 26 / 100: avg data time: 1.47e-01, avg batch time: 0.5485, average train loss: 6.8016
[09/16 11:12:30 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1425, average loss: 5.7510
[09/16 11:12:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 47.00	
[09/16 11:12:51 visual_prompt]: 	Test 100/407. loss: 5.569, 0.1968 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 11:13:11 visual_prompt]: 	Test 200/407. loss: 5.826, 0.2142 s / batch. (data: 2.79e-02)max mem: 17.22449 GB 
[09/16 11:13:30 visual_prompt]: 	Test 300/407. loss: 5.663, 0.1979 s / batch. (data: 1.74e-04)max mem: 17.22449 GB 
[09/16 11:13:50 visual_prompt]: 	Test 400/407. loss: 5.109, 0.1821 s / batch. (data: 4.03e-05)max mem: 17.22449 GB 
[09/16 11:13:53 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1934, average loss: 5.6629
[09/16 11:13:53 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.76	top5: 50.58	
[09/16 11:13:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 11:14:03 visual_prompt]: Epoch 27 / 100: avg data time: 1.37e-01, avg batch time: 0.5399, average train loss: 4.4049
[09/16 11:14:07 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1426, average loss: 3.8788
[09/16 11:14:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/16 11:14:30 visual_prompt]: 	Test 100/407. loss: 3.853, 0.1971 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 11:14:50 visual_prompt]: 	Test 200/407. loss: 3.729, 0.1943 s / batch. (data: 1.06e-04)max mem: 17.22449 GB 
[09/16 11:15:09 visual_prompt]: 	Test 300/407. loss: 3.884, 0.1828 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 11:15:29 visual_prompt]: 	Test 400/407. loss: 3.640, 0.1832 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 11:15:32 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1958, average loss: 3.7740
[09/16 11:15:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 62.68	
[09/16 11:15:32 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 11:15:42 visual_prompt]: Epoch 28 / 100: avg data time: 1.46e-01, avg batch time: 0.5480, average train loss: 3.4659
[09/16 11:15:46 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1425, average loss: 3.0399
[09/16 11:15:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/16 11:16:08 visual_prompt]: 	Test 100/407. loss: 3.533, 0.1828 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 11:16:27 visual_prompt]: 	Test 200/407. loss: 3.505, 0.1917 s / batch. (data: 9.70e-05)max mem: 17.22449 GB 
[09/16 11:16:47 visual_prompt]: 	Test 300/407. loss: 3.072, 0.1819 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 11:17:06 visual_prompt]: 	Test 400/407. loss: 3.308, 0.1826 s / batch. (data: 3.58e-05)max mem: 17.22449 GB 
[09/16 11:17:10 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1934, average loss: 3.1107
[09/16 11:17:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.52	top5: 61.88	
[09/16 11:17:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 11:17:20 visual_prompt]: Epoch 29 / 100: avg data time: 1.50e-01, avg batch time: 0.5523, average train loss: 3.3449
[09/16 11:17:25 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1428, average loss: 2.6316
[09/16 11:17:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.50	
[09/16 11:17:46 visual_prompt]: 	Test 100/407. loss: 2.874, 0.1831 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 11:18:06 visual_prompt]: 	Test 200/407. loss: 2.977, 0.1957 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 11:18:25 visual_prompt]: 	Test 300/407. loss: 2.611, 0.1835 s / batch. (data: 1.47e-04)max mem: 17.22449 GB 
[09/16 11:18:45 visual_prompt]: 	Test 400/407. loss: 2.933, 0.1827 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 11:18:48 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1937, average loss: 2.7611
[09/16 11:18:48 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.40	top5: 53.95	
[09/16 11:18:48 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 11:18:58 visual_prompt]: Epoch 30 / 100: avg data time: 1.49e-01, avg batch time: 0.5508, average train loss: 2.7234
[09/16 11:19:03 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1427, average loss: 3.1239
[09/16 11:19:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/16 11:19:25 visual_prompt]: 	Test 100/407. loss: 3.667, 0.2035 s / batch. (data: 2.13e-02)max mem: 17.22449 GB 
[09/16 11:19:44 visual_prompt]: 	Test 200/407. loss: 3.335, 0.1823 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 11:20:03 visual_prompt]: 	Test 300/407. loss: 3.217, 0.1846 s / batch. (data: 1.07e-04)max mem: 17.22449 GB 
[09/16 11:20:23 visual_prompt]: 	Test 400/407. loss: 3.072, 0.1830 s / batch. (data: 2.91e-05)max mem: 17.22449 GB 
[09/16 11:20:26 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1927, average loss: 3.1984
[09/16 11:20:26 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 59.09	
[09/16 11:20:26 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 11:20:36 visual_prompt]: Epoch 31 / 100: avg data time: 1.48e-01, avg batch time: 0.5498, average train loss: 3.2442
[09/16 11:20:41 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1428, average loss: 3.4241
[09/16 11:20:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/16 11:21:03 visual_prompt]: 	Test 100/407. loss: 4.106, 0.1828 s / batch. (data: 1.51e-04)max mem: 17.22449 GB 
[09/16 11:21:22 visual_prompt]: 	Test 200/407. loss: 3.909, 0.1990 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 11:21:42 visual_prompt]: 	Test 300/407. loss: 3.589, 0.2113 s / batch. (data: 1.22e-02)max mem: 17.22449 GB 
[09/16 11:22:01 visual_prompt]: 	Test 400/407. loss: 3.664, 0.1825 s / batch. (data: 3.77e-05)max mem: 17.22449 GB 
[09/16 11:22:04 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1933, average loss: 3.5680
[09/16 11:22:04 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 62.99	
[09/16 11:22:04 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 11:22:15 visual_prompt]: Epoch 32 / 100: avg data time: 1.50e-01, avg batch time: 0.5524, average train loss: 3.0758
[09/16 11:22:19 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1425, average loss: 2.8207
[09/16 11:22:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/16 11:22:41 visual_prompt]: 	Test 100/407. loss: 3.133, 0.1836 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 11:23:01 visual_prompt]: 	Test 200/407. loss: 2.931, 0.1859 s / batch. (data: 1.08e-04)max mem: 17.22449 GB 
[09/16 11:23:20 visual_prompt]: 	Test 300/407. loss: 2.664, 0.1967 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 11:23:40 visual_prompt]: 	Test 400/407. loss: 2.624, 0.1822 s / batch. (data: 2.86e-05)max mem: 17.22449 GB 
[09/16 11:23:43 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1938, average loss: 2.8275
[09/16 11:23:43 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 60.57	
[09/16 11:23:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 11:23:53 visual_prompt]: Epoch 33 / 100: avg data time: 1.44e-01, avg batch time: 0.5450, average train loss: 2.8597
[09/16 11:23:58 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1427, average loss: 3.1056
[09/16 11:23:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 57.50	
[09/16 11:24:19 visual_prompt]: 	Test 100/407. loss: 3.653, 0.1831 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 11:24:39 visual_prompt]: 	Test 200/407. loss: 3.213, 0.2020 s / batch. (data: 1.59e-02)max mem: 17.22449 GB 
[09/16 11:24:58 visual_prompt]: 	Test 300/407. loss: 3.225, 0.1826 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 11:25:17 visual_prompt]: 	Test 400/407. loss: 2.967, 0.1834 s / batch. (data: 2.86e-05)max mem: 17.22449 GB 
[09/16 11:25:21 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1931, average loss: 3.2283
[09/16 11:25:21 visual_prompt]: Classification results with test_vtab-svhn: top1: 8.75	top5: 52.37	
[09/16 11:25:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 11:25:31 visual_prompt]: Epoch 34 / 100: avg data time: 1.45e-01, avg batch time: 0.5458, average train loss: 2.7643
[09/16 11:25:36 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1425, average loss: 2.5811
[09/16 11:25:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.00	
[09/16 11:25:57 visual_prompt]: 	Test 100/407. loss: 2.546, 0.1824 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 11:26:17 visual_prompt]: 	Test 200/407. loss: 2.641, 0.2044 s / batch. (data: 2.27e-02)max mem: 17.22449 GB 
[09/16 11:26:36 visual_prompt]: 	Test 300/407. loss: 2.610, 0.1993 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 11:26:56 visual_prompt]: 	Test 400/407. loss: 2.580, 0.1877 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 11:26:59 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1940, average loss: 2.6077
[09/16 11:26:59 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.57	top5: 43.80	
[09/16 11:26:59 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 11:27:10 visual_prompt]: Epoch 35 / 100: avg data time: 1.48e-01, avg batch time: 0.5488, average train loss: 2.7457
[09/16 11:27:15 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1425, average loss: 2.6541
[09/16 11:27:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/16 11:27:37 visual_prompt]: 	Test 100/407. loss: 2.882, 0.1857 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 11:27:56 visual_prompt]: 	Test 200/407. loss: 2.914, 0.1832 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 11:28:16 visual_prompt]: 	Test 300/407. loss: 2.682, 0.1983 s / batch. (data: 1.61e-02)max mem: 17.22449 GB 
[09/16 11:28:35 visual_prompt]: 	Test 400/407. loss: 2.779, 0.1837 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 11:28:38 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1941, average loss: 2.6952
[09/16 11:28:38 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 55.91	
[09/16 11:28:38 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 11:28:49 visual_prompt]: Epoch 36 / 100: avg data time: 1.51e-01, avg batch time: 0.5515, average train loss: 2.5345
[09/16 11:28:53 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1424, average loss: 2.7355
[09/16 11:28:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/16 11:29:15 visual_prompt]: 	Test 100/407. loss: 3.065, 0.1834 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 11:29:34 visual_prompt]: 	Test 200/407. loss: 2.910, 0.1981 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 11:29:54 visual_prompt]: 	Test 300/407. loss: 2.874, 0.1984 s / batch. (data: 1.58e-02)max mem: 17.22449 GB 
[09/16 11:30:13 visual_prompt]: 	Test 400/407. loss: 2.778, 0.1829 s / batch. (data: 2.91e-05)max mem: 17.22449 GB 
[09/16 11:30:16 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1932, average loss: 2.7649
[09/16 11:30:16 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 62.75	
[09/16 11:30:16 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 11:30:27 visual_prompt]: Epoch 37 / 100: avg data time: 1.50e-01, avg batch time: 0.5526, average train loss: 2.5831
[09/16 11:30:31 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1425, average loss: 2.2745
[09/16 11:30:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/16 11:30:53 visual_prompt]: 	Test 100/407. loss: 2.445, 0.1829 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 11:31:12 visual_prompt]: 	Test 200/407. loss: 2.446, 0.1834 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 11:31:32 visual_prompt]: 	Test 300/407. loss: 2.361, 0.1829 s / batch. (data: 9.23e-05)max mem: 17.22449 GB 
[09/16 11:31:51 visual_prompt]: 	Test 400/407. loss: 2.382, 0.1832 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 11:31:54 visual_prompt]: Inference (test):avg data time: 6.94e-03, avg batch time: 0.1924, average loss: 2.3262
[09/16 11:31:54 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 59.33	
[09/16 11:31:54 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 11:32:04 visual_prompt]: Epoch 38 / 100: avg data time: 1.40e-01, avg batch time: 0.5444, average train loss: 2.5141
[09/16 11:32:09 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1425, average loss: 2.5267
[09/16 11:32:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 57.50	
[09/16 11:32:30 visual_prompt]: 	Test 100/407. loss: 2.427, 0.1834 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 11:32:50 visual_prompt]: 	Test 200/407. loss: 2.616, 0.1825 s / batch. (data: 1.43e-04)max mem: 17.22449 GB 
[09/16 11:33:09 visual_prompt]: 	Test 300/407. loss: 2.358, 0.1828 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 11:33:29 visual_prompt]: 	Test 400/407. loss: 2.399, 0.1893 s / batch. (data: 3.34e-05)max mem: 17.22449 GB 
[09/16 11:33:32 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1939, average loss: 2.4978
[09/16 11:33:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.60	top5: 57.64	
[09/16 11:33:32 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 11:33:42 visual_prompt]: Epoch 39 / 100: avg data time: 1.34e-01, avg batch time: 0.5362, average train loss: 2.5056
[09/16 11:33:47 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1427, average loss: 2.2866
[09/16 11:33:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 56.00	
[09/16 11:34:09 visual_prompt]: 	Test 100/407. loss: 2.358, 0.1946 s / batch. (data: 1.25e-02)max mem: 17.22449 GB 
[09/16 11:34:28 visual_prompt]: 	Test 200/407. loss: 2.432, 0.1830 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 11:34:47 visual_prompt]: 	Test 300/407. loss: 2.329, 0.1870 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 11:35:07 visual_prompt]: 	Test 400/407. loss: 2.362, 0.1818 s / batch. (data: 3.46e-05)max mem: 17.22449 GB 
[09/16 11:35:10 visual_prompt]: Inference (test):avg data time: 8.80e-03, avg batch time: 0.1944, average loss: 2.3335
[09/16 11:35:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 16.73	top5: 55.09	
[09/16 11:35:10 visual_prompt]: Best epoch 39: best metric: 0.235
[09/16 11:35:10 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 11:35:21 visual_prompt]: Epoch 40 / 100: avg data time: 1.49e-01, avg batch time: 0.5511, average train loss: 2.3993
[09/16 11:35:25 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1425, average loss: 2.3068
[09/16 11:35:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/16 11:35:47 visual_prompt]: 	Test 100/407. loss: 2.568, 0.1823 s / batch. (data: 9.37e-05)max mem: 17.22449 GB 
[09/16 11:36:06 visual_prompt]: 	Test 200/407. loss: 2.524, 0.2036 s / batch. (data: 2.17e-02)max mem: 17.22449 GB 
[09/16 11:36:26 visual_prompt]: 	Test 300/407. loss: 2.434, 0.2014 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 11:36:45 visual_prompt]: 	Test 400/407. loss: 2.467, 0.1827 s / batch. (data: 3.00e-05)max mem: 17.22449 GB 
[09/16 11:36:49 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1935, average loss: 2.4261
[09/16 11:36:49 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.52	top5: 51.03	
[09/16 11:36:49 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 11:36:59 visual_prompt]: Epoch 41 / 100: avg data time: 1.50e-01, avg batch time: 0.5514, average train loss: 2.4786
[09/16 11:37:03 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1425, average loss: 2.5350
[09/16 11:37:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/16 11:37:25 visual_prompt]: 	Test 100/407. loss: 2.592, 0.1934 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 11:37:45 visual_prompt]: 	Test 200/407. loss: 2.573, 0.1829 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 11:38:04 visual_prompt]: 	Test 300/407. loss: 2.622, 0.1827 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 11:38:23 visual_prompt]: 	Test 400/407. loss: 2.578, 0.1827 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 11:38:27 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1933, average loss: 2.5669
[09/16 11:38:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.48	top5: 52.93	
[09/16 11:38:27 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 11:38:37 visual_prompt]: Epoch 42 / 100: avg data time: 1.48e-01, avg batch time: 0.5494, average train loss: 2.5571
[09/16 11:38:42 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1425, average loss: 2.6631
[09/16 11:38:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/16 11:39:03 visual_prompt]: 	Test 100/407. loss: 2.681, 0.1963 s / batch. (data: 1.48e-02)max mem: 17.22449 GB 
[09/16 11:39:23 visual_prompt]: 	Test 200/407. loss: 3.006, 0.1823 s / batch. (data: 1.08e-04)max mem: 17.22449 GB 
[09/16 11:39:42 visual_prompt]: 	Test 300/407. loss: 2.362, 0.1827 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 11:40:01 visual_prompt]: 	Test 400/407. loss: 2.730, 0.1836 s / batch. (data: 3.74e-05)max mem: 17.22449 GB 
[09/16 11:40:05 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1921, average loss: 2.6790
[09/16 11:40:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 60.03	
[09/16 11:40:05 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 11:40:15 visual_prompt]: Epoch 43 / 100: avg data time: 1.47e-01, avg batch time: 0.5494, average train loss: 2.5198
[09/16 11:40:19 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1425, average loss: 2.4434
[09/16 11:40:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 65.00	
[09/16 11:40:41 visual_prompt]: 	Test 100/407. loss: 2.620, 0.1971 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 11:41:00 visual_prompt]: 	Test 200/407. loss: 2.580, 0.2021 s / batch. (data: 1.45e-02)max mem: 17.22449 GB 
[09/16 11:41:20 visual_prompt]: 	Test 300/407. loss: 2.529, 0.1992 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 11:41:39 visual_prompt]: 	Test 400/407. loss: 2.497, 0.1821 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 11:41:42 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1924, average loss: 2.4635
[09/16 11:41:42 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.02	top5: 62.18	
[09/16 11:41:42 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 11:41:53 visual_prompt]: Epoch 44 / 100: avg data time: 1.46e-01, avg batch time: 0.5469, average train loss: 2.4357
[09/16 11:41:57 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1427, average loss: 2.2985
[09/16 11:41:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/16 11:42:19 visual_prompt]: 	Test 100/407. loss: 2.427, 0.1958 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 11:42:39 visual_prompt]: 	Test 200/407. loss: 2.512, 0.2280 s / batch. (data: 2.50e-02)max mem: 17.22449 GB 
[09/16 11:42:59 visual_prompt]: 	Test 300/407. loss: 2.290, 0.1914 s / batch. (data: 1.54e-04)max mem: 17.22449 GB 
[09/16 11:43:18 visual_prompt]: 	Test 400/407. loss: 2.413, 0.1822 s / batch. (data: 3.19e-05)max mem: 17.22449 GB 
[09/16 11:43:21 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1954, average loss: 2.3691
[09/16 11:43:21 visual_prompt]: Classification results with test_vtab-svhn: top1: 16.29	top5: 53.95	
[09/16 11:43:21 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 11:43:32 visual_prompt]: Epoch 45 / 100: avg data time: 1.46e-01, avg batch time: 0.5477, average train loss: 2.3914
[09/16 11:43:36 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1425, average loss: 2.2960
[09/16 11:43:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/16 11:43:58 visual_prompt]: 	Test 100/407. loss: 2.358, 0.1827 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 11:44:17 visual_prompt]: 	Test 200/407. loss: 2.431, 0.1976 s / batch. (data: 1.57e-02)max mem: 17.22449 GB 
[09/16 11:44:37 visual_prompt]: 	Test 300/407. loss: 2.276, 0.1936 s / batch. (data: 1.13e-02)max mem: 17.22449 GB 
[09/16 11:44:56 visual_prompt]: 	Test 400/407. loss: 2.300, 0.1831 s / batch. (data: 3.58e-05)max mem: 17.22449 GB 
[09/16 11:44:59 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1935, average loss: 2.3218
[09/16 11:44:59 visual_prompt]: Classification results with test_vtab-svhn: top1: 18.25	top5: 61.01	
[09/16 11:44:59 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 11:45:10 visual_prompt]: Epoch 46 / 100: avg data time: 1.52e-01, avg batch time: 0.5514, average train loss: 2.4144
[09/16 11:45:14 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1425, average loss: 2.3338
[09/16 11:45:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 59.50	
[09/16 11:45:36 visual_prompt]: 	Test 100/407. loss: 2.500, 0.1832 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 11:45:55 visual_prompt]: 	Test 200/407. loss: 2.409, 0.1831 s / batch. (data: 9.18e-05)max mem: 17.22449 GB 
[09/16 11:46:14 visual_prompt]: 	Test 300/407. loss: 2.429, 0.1828 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 11:46:34 visual_prompt]: 	Test 400/407. loss: 2.349, 0.1823 s / batch. (data: 3.98e-05)max mem: 17.22449 GB 
[09/16 11:46:37 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1925, average loss: 2.3824
[09/16 11:46:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.55	top5: 58.83	
[09/16 11:46:37 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 11:46:47 visual_prompt]: Epoch 47 / 100: avg data time: 1.45e-01, avg batch time: 0.5466, average train loss: 2.4009
[09/16 11:46:52 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1425, average loss: 2.5875
[09/16 11:46:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 65.00	
[09/16 11:47:14 visual_prompt]: 	Test 100/407. loss: 2.730, 0.1824 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 11:47:33 visual_prompt]: 	Test 200/407. loss: 2.736, 0.1891 s / batch. (data: 1.07e-04)max mem: 17.22449 GB 
[09/16 11:47:53 visual_prompt]: 	Test 300/407. loss: 2.655, 0.1929 s / batch. (data: 1.08e-02)max mem: 17.22449 GB 
[09/16 11:48:12 visual_prompt]: 	Test 400/407. loss: 2.532, 0.1825 s / batch. (data: 2.86e-05)max mem: 17.22449 GB 
[09/16 11:48:15 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1943, average loss: 2.6046
[09/16 11:48:15 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 64.17	
[09/16 11:48:15 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 11:48:26 visual_prompt]: Epoch 48 / 100: avg data time: 1.54e-01, avg batch time: 0.5551, average train loss: 2.4568
[09/16 11:48:31 visual_prompt]: Inference (val):avg data time: 4.21e-04, avg batch time: 0.2736, average loss: 2.5327
[09/16 11:48:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/16 11:48:53 visual_prompt]: 	Test 100/407. loss: 2.436, 0.1960 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 11:49:12 visual_prompt]: 	Test 200/407. loss: 2.476, 0.1839 s / batch. (data: 1.47e-04)max mem: 17.22449 GB 
[09/16 11:49:32 visual_prompt]: 	Test 300/407. loss: 2.446, 0.1890 s / batch. (data: 9.80e-05)max mem: 17.22449 GB 
[09/16 11:49:51 visual_prompt]: 	Test 400/407. loss: 2.436, 0.1826 s / batch. (data: 3.62e-05)max mem: 17.22449 GB 
[09/16 11:49:55 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1942, average loss: 2.5103
[09/16 11:49:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 50.39	
[09/16 11:49:55 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 11:50:05 visual_prompt]: Epoch 49 / 100: avg data time: 1.51e-01, avg batch time: 0.5520, average train loss: 2.4639
[09/16 11:50:09 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1429, average loss: 2.2527
[09/16 11:50:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 66.50	
[09/16 11:50:31 visual_prompt]: 	Test 100/407. loss: 2.481, 0.1913 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 11:50:51 visual_prompt]: 	Test 200/407. loss: 2.583, 0.1936 s / batch. (data: 1.12e-02)max mem: 17.22449 GB 
[09/16 11:51:10 visual_prompt]: 	Test 300/407. loss: 2.222, 0.1833 s / batch. (data: 1.55e-04)max mem: 17.22449 GB 
[09/16 11:51:30 visual_prompt]: 	Test 400/407. loss: 2.391, 0.1824 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 11:51:33 visual_prompt]: Inference (test):avg data time: 8.68e-03, avg batch time: 0.1941, average loss: 2.3335
[09/16 11:51:33 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.44	top5: 61.46	
[09/16 11:51:33 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 11:51:43 visual_prompt]: Epoch 50 / 100: avg data time: 1.49e-01, avg batch time: 0.5502, average train loss: 2.3219
[09/16 11:51:48 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1425, average loss: 2.2667
[09/16 11:51:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 66.50	
[09/16 11:52:09 visual_prompt]: 	Test 100/407. loss: 2.443, 0.1819 s / batch. (data: 3.91e-05)max mem: 17.22449 GB 
[09/16 11:52:29 visual_prompt]: 	Test 200/407. loss: 2.528, 0.1864 s / batch. (data: 1.14e-04)max mem: 17.22449 GB 
[09/16 11:52:48 visual_prompt]: 	Test 300/407. loss: 2.191, 0.1829 s / batch. (data: 1.11e-04)max mem: 17.22449 GB 
[09/16 11:53:07 visual_prompt]: 	Test 400/407. loss: 2.313, 0.1877 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 11:53:11 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1923, average loss: 2.3183
[09/16 11:53:11 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.54	top5: 61.99	
[09/16 11:53:11 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 11:53:21 visual_prompt]: Epoch 51 / 100: avg data time: 1.45e-01, avg batch time: 0.5468, average train loss: 2.2997
[09/16 11:53:25 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1426, average loss: 2.3576
[09/16 11:53:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 63.00	
[09/16 11:53:47 visual_prompt]: 	Test 100/407. loss: 2.579, 0.1974 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 11:54:06 visual_prompt]: 	Test 200/407. loss: 2.663, 0.2052 s / batch. (data: 1.25e-02)max mem: 17.22449 GB 
[09/16 11:54:26 visual_prompt]: 	Test 300/407. loss: 2.316, 0.1835 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 11:54:45 visual_prompt]: 	Test 400/407. loss: 2.508, 0.1901 s / batch. (data: 3.50e-05)max mem: 17.22449 GB 
[09/16 11:54:48 visual_prompt]: Inference (test):avg data time: 7.26e-03, avg batch time: 0.1924, average loss: 2.4180
[09/16 11:54:48 visual_prompt]: Classification results with test_vtab-svhn: top1: 18.28	top5: 58.17	
[09/16 11:54:48 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 11:54:59 visual_prompt]: Epoch 52 / 100: avg data time: 1.46e-01, avg batch time: 0.5469, average train loss: 2.3485
[09/16 11:55:03 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1426, average loss: 2.3519
[09/16 11:55:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.50	top5: 62.00	
[09/16 11:55:25 visual_prompt]: 	Test 100/407. loss: 2.589, 0.1966 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 11:55:45 visual_prompt]: 	Test 200/407. loss: 2.523, 0.1955 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 11:56:04 visual_prompt]: 	Test 300/407. loss: 2.443, 0.1958 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 11:56:23 visual_prompt]: 	Test 400/407. loss: 2.364, 0.1827 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 11:56:26 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1935, average loss: 2.4355
[09/16 11:56:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 10.28	top5: 57.50	
[09/16 11:56:27 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 11:56:37 visual_prompt]: Epoch 53 / 100: avg data time: 1.42e-01, avg batch time: 0.5433, average train loss: 2.4165
[09/16 11:56:41 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1425, average loss: 2.3517
[09/16 11:56:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.00	top5: 59.00	
[09/16 11:57:03 visual_prompt]: 	Test 100/407. loss: 2.320, 0.2078 s / batch. (data: 2.57e-02)max mem: 17.22449 GB 
[09/16 11:57:22 visual_prompt]: 	Test 200/407. loss: 2.406, 0.1830 s / batch. (data: 1.04e-04)max mem: 17.22449 GB 
[09/16 11:57:42 visual_prompt]: 	Test 300/407. loss: 2.370, 0.1878 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 11:58:01 visual_prompt]: 	Test 400/407. loss: 2.394, 0.1828 s / batch. (data: 3.62e-05)max mem: 17.22449 GB 
[09/16 11:58:05 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1933, average loss: 2.3677
[09/16 11:58:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.90	top5: 57.20	
[09/16 11:58:05 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 11:58:15 visual_prompt]: Epoch 54 / 100: avg data time: 1.54e-01, avg batch time: 0.5567, average train loss: 2.3359
[09/16 11:58:20 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1424, average loss: 2.1930
[09/16 11:58:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.00	top5: 67.00	
[09/16 11:58:41 visual_prompt]: 	Test 100/407. loss: 2.404, 0.1829 s / batch. (data: 1.09e-04)max mem: 17.22449 GB 
[09/16 11:59:00 visual_prompt]: 	Test 200/407. loss: 2.432, 0.1864 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 11:59:20 visual_prompt]: 	Test 300/407. loss: 2.230, 0.1834 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 11:59:39 visual_prompt]: 	Test 400/407. loss: 2.255, 0.1829 s / batch. (data: 2.31e-05)max mem: 17.22449 GB 
[09/16 11:59:42 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1924, average loss: 2.2639
[09/16 11:59:42 visual_prompt]: Classification results with test_vtab-svhn: top1: 18.19	top5: 63.35	
[09/16 11:59:42 visual_prompt]: Best epoch 54: best metric: 0.260
[09/16 11:59:42 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 11:59:53 visual_prompt]: Epoch 55 / 100: avg data time: 1.47e-01, avg batch time: 0.5483, average train loss: 2.2996
[09/16 11:59:57 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1426, average loss: 2.2392
[09/16 11:59:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.00	top5: 64.50	
[09/16 12:00:19 visual_prompt]: 	Test 100/407. loss: 2.389, 0.1830 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 12:00:38 visual_prompt]: 	Test 200/407. loss: 2.471, 0.1821 s / batch. (data: 9.39e-05)max mem: 17.22449 GB 
[09/16 12:00:58 visual_prompt]: 	Test 300/407. loss: 2.260, 0.1828 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 12:01:17 visual_prompt]: 	Test 400/407. loss: 2.326, 0.1826 s / batch. (data: 3.74e-05)max mem: 17.22449 GB 
[09/16 12:01:20 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1928, average loss: 2.3457
[09/16 12:01:20 visual_prompt]: Classification results with test_vtab-svhn: top1: 12.54	top5: 56.91	
[09/16 12:01:20 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 12:01:31 visual_prompt]: Epoch 56 / 100: avg data time: 1.44e-01, avg batch time: 0.5450, average train loss: 2.3186
[09/16 12:01:35 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1426, average loss: 2.1764
[09/16 12:01:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 65.50	
[09/16 12:01:57 visual_prompt]: 	Test 100/407. loss: 2.455, 0.1820 s / batch. (data: 3.29e-05)max mem: 17.22449 GB 
[09/16 12:02:16 visual_prompt]: 	Test 200/407. loss: 2.505, 0.2195 s / batch. (data: 3.73e-02)max mem: 17.22449 GB 
[09/16 12:02:35 visual_prompt]: 	Test 300/407. loss: 2.212, 0.1826 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 12:02:55 visual_prompt]: 	Test 400/407. loss: 2.285, 0.1819 s / batch. (data: 3.62e-05)max mem: 17.22449 GB 
[09/16 12:02:58 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1934, average loss: 2.2729
[09/16 12:02:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 18.35	top5: 62.51	
[09/16 12:02:58 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 12:03:09 visual_prompt]: Epoch 57 / 100: avg data time: 1.51e-01, avg batch time: 0.5553, average train loss: 2.3086
[09/16 12:03:13 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1423, average loss: 2.2901
[09/16 12:03:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 64.00	
[09/16 12:03:35 visual_prompt]: 	Test 100/407. loss: 2.453, 0.1853 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 12:03:54 visual_prompt]: 	Test 200/407. loss: 2.483, 0.1960 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 12:04:14 visual_prompt]: 	Test 300/407. loss: 2.329, 0.1826 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 12:04:34 visual_prompt]: 	Test 400/407. loss: 2.420, 0.1820 s / batch. (data: 2.57e-05)max mem: 17.22449 GB 
[09/16 12:04:37 visual_prompt]: Inference (test):avg data time: 8.40e-03, avg batch time: 0.1947, average loss: 2.3514
[09/16 12:04:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 16.74	top5: 58.68	
[09/16 12:04:37 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 12:04:47 visual_prompt]: Epoch 58 / 100: avg data time: 1.44e-01, avg batch time: 0.5487, average train loss: 2.3812
[09/16 12:04:52 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1425, average loss: 2.4025
[09/16 12:04:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 60.00	
[09/16 12:05:13 visual_prompt]: 	Test 100/407. loss: 2.415, 0.1822 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 12:05:33 visual_prompt]: 	Test 200/407. loss: 2.497, 0.1973 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 12:05:52 visual_prompt]: 	Test 300/407. loss: 2.399, 0.1963 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 12:06:12 visual_prompt]: 	Test 400/407. loss: 2.496, 0.1827 s / batch. (data: 3.60e-05)max mem: 17.22449 GB 
[09/16 12:06:15 visual_prompt]: Inference (test):avg data time: 8.06e-03, avg batch time: 0.1937, average loss: 2.4557
[09/16 12:06:15 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.38	top5: 54.39	
[09/16 12:06:15 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 12:06:26 visual_prompt]: Epoch 59 / 100: avg data time: 1.54e-01, avg batch time: 0.5559, average train loss: 2.4279
[09/16 12:06:30 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1425, average loss: 2.3009
[09/16 12:06:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.00	top5: 61.50	
[09/16 12:06:52 visual_prompt]: 	Test 100/407. loss: 2.532, 0.1952 s / batch. (data: 1.38e-04)max mem: 17.22449 GB 
[09/16 12:07:11 visual_prompt]: 	Test 200/407. loss: 2.573, 0.2070 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 12:07:30 visual_prompt]: 	Test 300/407. loss: 2.282, 0.2067 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 12:07:50 visual_prompt]: 	Test 400/407. loss: 2.465, 0.1908 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 12:07:53 visual_prompt]: Inference (test):avg data time: 7.06e-03, avg batch time: 0.1926, average loss: 2.3768
[09/16 12:07:53 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.70	top5: 55.60	
[09/16 12:07:53 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 12:08:03 visual_prompt]: Epoch 60 / 100: avg data time: 1.44e-01, avg batch time: 0.5446, average train loss: 2.3106
[09/16 12:08:08 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1425, average loss: 2.2885
[09/16 12:08:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.00	top5: 66.00	
[09/16 12:08:30 visual_prompt]: 	Test 100/407. loss: 2.438, 0.2042 s / batch. (data: 2.28e-02)max mem: 17.22449 GB 
[09/16 12:08:49 visual_prompt]: 	Test 200/407. loss: 2.534, 0.1827 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 12:09:09 visual_prompt]: 	Test 300/407. loss: 2.224, 0.1827 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 12:09:28 visual_prompt]: 	Test 400/407. loss: 2.276, 0.1822 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 12:09:31 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1941, average loss: 2.3463
[09/16 12:09:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 13.24	top5: 64.35	
[09/16 12:09:32 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 12:09:42 visual_prompt]: Epoch 61 / 100: avg data time: 1.51e-01, avg batch time: 0.5563, average train loss: 2.3093
[09/16 12:09:46 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1426, average loss: 2.3424
[09/16 12:09:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 59.00	
[09/16 12:10:09 visual_prompt]: 	Test 100/407. loss: 2.563, 0.1826 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 12:10:28 visual_prompt]: 	Test 200/407. loss: 2.481, 0.2021 s / batch. (data: 2.03e-02)max mem: 17.22449 GB 
[09/16 12:10:48 visual_prompt]: 	Test 300/407. loss: 2.602, 0.1831 s / batch. (data: 1.51e-04)max mem: 17.22449 GB 
[09/16 12:11:07 visual_prompt]: 	Test 400/407. loss: 2.522, 0.1830 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 12:11:10 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1945, average loss: 2.4575
[09/16 12:11:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.30	top5: 55.89	
[09/16 12:11:10 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 12:11:21 visual_prompt]: Epoch 62 / 100: avg data time: 1.48e-01, avg batch time: 0.5503, average train loss: 2.3303
[09/16 12:11:25 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1434, average loss: 2.2673
[09/16 12:11:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.00	top5: 62.00	
[09/16 12:11:46 visual_prompt]: 	Test 100/407. loss: 2.408, 0.1819 s / batch. (data: 1.04e-04)max mem: 17.22449 GB 
[09/16 12:12:06 visual_prompt]: 	Test 200/407. loss: 2.407, 0.1962 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 12:12:25 visual_prompt]: 	Test 300/407. loss: 2.332, 0.1837 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 12:12:45 visual_prompt]: 	Test 400/407. loss: 2.315, 0.1831 s / batch. (data: 2.72e-05)max mem: 17.22449 GB 
[09/16 12:12:48 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1935, average loss: 2.3549
[09/16 12:12:48 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.98	top5: 57.32	
[09/16 12:12:48 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 12:13:00 visual_prompt]: Epoch 63 / 100: avg data time: 1.52e-01, avg batch time: 0.6157, average train loss: 2.3045
[09/16 12:13:04 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1436, average loss: 2.2092
[09/16 12:13:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 67.00	
[09/16 12:13:26 visual_prompt]: 	Test 100/407. loss: 2.478, 0.2033 s / batch. (data: 1.47e-02)max mem: 17.22449 GB 
[09/16 12:13:45 visual_prompt]: 	Test 200/407. loss: 2.426, 0.2040 s / batch. (data: 7.26e-03)max mem: 17.22449 GB 
[09/16 12:14:04 visual_prompt]: 	Test 300/407. loss: 2.324, 0.1993 s / batch. (data: 1.70e-02)max mem: 17.22449 GB 
[09/16 12:14:24 visual_prompt]: 	Test 400/407. loss: 2.278, 0.1827 s / batch. (data: 3.29e-05)max mem: 17.22449 GB 
[09/16 12:14:27 visual_prompt]: Inference (test):avg data time: 7.26e-03, avg batch time: 0.1928, average loss: 2.3085
[09/16 12:14:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.40	top5: 62.27	
[09/16 12:14:27 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 12:14:38 visual_prompt]: Epoch 64 / 100: avg data time: 1.52e-01, avg batch time: 0.5535, average train loss: 2.2492
[09/16 12:14:42 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1434, average loss: 2.2989
[09/16 12:14:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 65.50	
[09/16 12:15:05 visual_prompt]: 	Test 100/407. loss: 2.397, 0.1983 s / batch. (data: 1.60e-02)max mem: 17.22449 GB 
[09/16 12:15:24 visual_prompt]: 	Test 200/407. loss: 2.445, 0.1963 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 12:15:44 visual_prompt]: 	Test 300/407. loss: 2.316, 0.1824 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 12:16:04 visual_prompt]: 	Test 400/407. loss: 2.279, 0.1827 s / batch. (data: 2.31e-05)max mem: 17.22449 GB 
[09/16 12:16:07 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1962, average loss: 2.3760
[09/16 12:16:07 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.96	top5: 56.33	
[09/16 12:16:07 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 12:16:17 visual_prompt]: Epoch 65 / 100: avg data time: 1.54e-01, avg batch time: 0.5551, average train loss: 2.3070
[09/16 12:16:22 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1426, average loss: 2.1887
[09/16 12:16:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 69.00	
[09/16 12:16:43 visual_prompt]: 	Test 100/407. loss: 2.514, 0.1952 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 12:17:03 visual_prompt]: 	Test 200/407. loss: 2.501, 0.1958 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 12:17:22 visual_prompt]: 	Test 300/407. loss: 2.263, 0.2066 s / batch. (data: 2.49e-02)max mem: 17.22449 GB 
[09/16 12:17:41 visual_prompt]: 	Test 400/407. loss: 2.279, 0.1835 s / batch. (data: 3.55e-05)max mem: 17.22449 GB 
[09/16 12:17:45 visual_prompt]: Inference (test):avg data time: 6.93e-03, avg batch time: 0.1923, average loss: 2.2980
[09/16 12:17:45 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.61	top5: 61.97	
[09/16 12:17:45 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 12:17:55 visual_prompt]: Epoch 66 / 100: avg data time: 1.35e-01, avg batch time: 0.5372, average train loss: 2.2665
[09/16 12:18:00 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1426, average loss: 2.2647
[09/16 12:18:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/16 12:18:21 visual_prompt]: 	Test 100/407. loss: 2.544, 0.2156 s / batch. (data: 2.08e-02)max mem: 17.22449 GB 
[09/16 12:18:41 visual_prompt]: 	Test 200/407. loss: 2.517, 0.1826 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 12:19:00 visual_prompt]: 	Test 300/407. loss: 2.402, 0.1835 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 12:19:20 visual_prompt]: 	Test 400/407. loss: 2.356, 0.1827 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 12:19:23 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1944, average loss: 2.3880
[09/16 12:19:23 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.58	top5: 57.29	
[09/16 12:19:23 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 12:19:34 visual_prompt]: Epoch 67 / 100: avg data time: 1.52e-01, avg batch time: 0.5537, average train loss: 2.2775
[09/16 12:19:38 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1426, average loss: 2.2247
[09/16 12:19:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.00	top5: 67.50	
[09/16 12:20:00 visual_prompt]: 	Test 100/407. loss: 2.375, 0.1952 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 12:20:19 visual_prompt]: 	Test 200/407. loss: 2.475, 0.1828 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 12:20:39 visual_prompt]: 	Test 300/407. loss: 2.247, 0.1958 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 12:20:58 visual_prompt]: 	Test 400/407. loss: 2.265, 0.1833 s / batch. (data: 4.55e-05)max mem: 17.22449 GB 
[09/16 12:21:01 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1930, average loss: 2.3341
[09/16 12:21:02 visual_prompt]: Classification results with test_vtab-svhn: top1: 18.86	top5: 61.50	
[09/16 12:21:02 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 12:21:12 visual_prompt]: Epoch 68 / 100: avg data time: 1.45e-01, avg batch time: 0.5569, average train loss: 2.2441
[09/16 12:21:16 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1427, average loss: 2.1679
[09/16 12:21:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.00	top5: 67.50	
[09/16 12:21:38 visual_prompt]: 	Test 100/407. loss: 2.380, 0.1829 s / batch. (data: 1.47e-04)max mem: 17.22449 GB 
[09/16 12:21:57 visual_prompt]: 	Test 200/407. loss: 2.462, 0.1829 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 12:22:17 visual_prompt]: 	Test 300/407. loss: 2.260, 0.1824 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 12:22:36 visual_prompt]: 	Test 400/407. loss: 2.268, 0.1822 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 12:22:40 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1935, average loss: 2.2945
[09/16 12:22:40 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.35	top5: 64.31	
[09/16 12:22:40 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 12:22:50 visual_prompt]: Epoch 69 / 100: avg data time: 1.41e-01, avg batch time: 0.5418, average train loss: 2.2725
[09/16 12:22:54 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1427, average loss: 2.1718
[09/16 12:22:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 28.00	top5: 64.50	
[09/16 12:23:16 visual_prompt]: 	Test 100/407. loss: 2.470, 0.2014 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 12:23:36 visual_prompt]: 	Test 200/407. loss: 2.542, 0.2066 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 12:23:55 visual_prompt]: 	Test 300/407. loss: 2.219, 0.1843 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 12:24:14 visual_prompt]: 	Test 400/407. loss: 2.275, 0.1820 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 12:24:18 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1932, average loss: 2.2977
[09/16 12:24:18 visual_prompt]: Classification results with test_vtab-svhn: top1: 18.78	top5: 65.39	
[09/16 12:24:18 visual_prompt]: Best epoch 69: best metric: 0.280
[09/16 12:24:18 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 12:24:28 visual_prompt]: Epoch 70 / 100: avg data time: 1.52e-01, avg batch time: 0.5570, average train loss: 2.2652
[09/16 12:24:33 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1425, average loss: 2.2009
[09/16 12:24:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 70.50	
[09/16 12:24:54 visual_prompt]: 	Test 100/407. loss: 2.557, 0.1829 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 12:25:14 visual_prompt]: 	Test 200/407. loss: 2.609, 0.1827 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 12:25:33 visual_prompt]: 	Test 300/407. loss: 2.305, 0.1835 s / batch. (data: 1.38e-04)max mem: 17.22449 GB 
[09/16 12:25:53 visual_prompt]: 	Test 400/407. loss: 2.354, 0.1823 s / batch. (data: 3.79e-05)max mem: 17.22449 GB 
[09/16 12:25:56 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1940, average loss: 2.3521
[09/16 12:25:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.57	top5: 63.63	
[09/16 12:25:56 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 12:26:07 visual_prompt]: Epoch 71 / 100: avg data time: 1.57e-01, avg batch time: 0.5578, average train loss: 2.2311
[09/16 12:26:11 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1428, average loss: 2.3144
[09/16 12:26:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.00	top5: 57.00	
[09/16 12:26:33 visual_prompt]: 	Test 100/407. loss: 2.328, 0.1820 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 12:26:52 visual_prompt]: 	Test 200/407. loss: 2.374, 0.2064 s / batch. (data: 2.44e-02)max mem: 17.22449 GB 
[09/16 12:27:12 visual_prompt]: 	Test 300/407. loss: 2.357, 0.1909 s / batch. (data: 9.14e-03)max mem: 17.22449 GB 
[09/16 12:27:32 visual_prompt]: 	Test 400/407. loss: 2.261, 0.1825 s / batch. (data: 3.86e-05)max mem: 17.22449 GB 
[09/16 12:27:35 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1947, average loss: 2.3753
[09/16 12:27:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 12.55	top5: 53.34	
[09/16 12:27:35 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 12:27:45 visual_prompt]: Epoch 72 / 100: avg data time: 1.49e-01, avg batch time: 0.5491, average train loss: 2.1849
[09/16 12:27:50 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1426, average loss: 2.0674
[09/16 12:27:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 28.50	top5: 71.50	
[09/16 12:28:11 visual_prompt]: 	Test 100/407. loss: 2.329, 0.1950 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 12:28:31 visual_prompt]: 	Test 200/407. loss: 2.388, 0.1829 s / batch. (data: 1.76e-04)max mem: 17.22449 GB 
[09/16 12:28:50 visual_prompt]: 	Test 300/407. loss: 2.268, 0.1933 s / batch. (data: 1.13e-02)max mem: 17.22449 GB 
[09/16 12:29:09 visual_prompt]: 	Test 400/407. loss: 2.181, 0.1822 s / batch. (data: 2.88e-05)max mem: 17.22449 GB 
[09/16 12:29:13 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1923, average loss: 2.2585
[09/16 12:29:13 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.54	top5: 60.21	
[09/16 12:29:13 visual_prompt]: Best epoch 72: best metric: 0.285
[09/16 12:29:13 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 12:29:23 visual_prompt]: Epoch 73 / 100: avg data time: 1.47e-01, avg batch time: 0.5490, average train loss: 2.1812
[09/16 12:29:28 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1427, average loss: 2.2185
[09/16 12:29:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.50	top5: 67.00	
[09/16 12:29:49 visual_prompt]: 	Test 100/407. loss: 2.518, 0.1827 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 12:30:09 visual_prompt]: 	Test 200/407. loss: 2.482, 0.1827 s / batch. (data: 1.05e-04)max mem: 17.22449 GB 
[09/16 12:30:28 visual_prompt]: 	Test 300/407. loss: 2.327, 0.1824 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 12:30:48 visual_prompt]: 	Test 400/407. loss: 2.291, 0.1828 s / batch. (data: 2.77e-05)max mem: 17.22449 GB 
[09/16 12:30:51 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1938, average loss: 2.3536
[09/16 12:30:51 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.81	top5: 60.87	
[09/16 12:30:51 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 12:31:02 visual_prompt]: Epoch 74 / 100: avg data time: 1.48e-01, avg batch time: 0.5511, average train loss: 2.2055
[09/16 12:31:06 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1441, average loss: 2.1219
[09/16 12:31:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.50	top5: 69.00	
[09/16 12:31:28 visual_prompt]: 	Test 100/407. loss: 2.362, 0.1961 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 12:31:47 visual_prompt]: 	Test 200/407. loss: 2.481, 0.1997 s / batch. (data: 1.80e-02)max mem: 17.22449 GB 
[09/16 12:32:07 visual_prompt]: 	Test 300/407. loss: 2.167, 0.1831 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 12:32:26 visual_prompt]: 	Test 400/407. loss: 2.193, 0.1830 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 12:32:29 visual_prompt]: Inference (test):avg data time: 8.36e-03, avg batch time: 0.1932, average loss: 2.2631
[09/16 12:32:30 visual_prompt]: Classification results with test_vtab-svhn: top1: 20.62	top5: 65.07	
[09/16 12:32:30 visual_prompt]: Best epoch 74: best metric: 0.295
[09/16 12:32:30 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 12:32:40 visual_prompt]: Epoch 75 / 100: avg data time: 1.50e-01, avg batch time: 0.5516, average train loss: 2.1559
[09/16 12:32:44 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1424, average loss: 2.0292
[09/16 12:32:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.00	top5: 75.50	
[09/16 12:33:06 visual_prompt]: 	Test 100/407. loss: 2.267, 0.1828 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 12:33:25 visual_prompt]: 	Test 200/407. loss: 2.338, 0.1979 s / batch. (data: 1.60e-02)max mem: 17.22449 GB 
[09/16 12:33:44 visual_prompt]: 	Test 300/407. loss: 2.110, 0.1976 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 12:34:04 visual_prompt]: 	Test 400/407. loss: 2.125, 0.1824 s / batch. (data: 3.79e-05)max mem: 17.22449 GB 
[09/16 12:34:07 visual_prompt]: Inference (test):avg data time: 7.09e-03, avg batch time: 0.1928, average loss: 2.1845
[09/16 12:34:08 visual_prompt]: Classification results with test_vtab-svhn: top1: 21.76	top5: 66.97	
[09/16 12:34:08 visual_prompt]: Best epoch 75: best metric: 0.320
[09/16 12:34:08 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 12:34:18 visual_prompt]: Epoch 76 / 100: avg data time: 1.54e-01, avg batch time: 0.5565, average train loss: 2.0764
[09/16 12:34:22 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1425, average loss: 1.8885
[09/16 12:34:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.00	top5: 77.50	
[09/16 12:34:45 visual_prompt]: 	Test 100/407. loss: 2.311, 0.1983 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 12:35:04 visual_prompt]: 	Test 200/407. loss: 2.327, 0.1829 s / batch. (data: 9.61e-05)max mem: 17.22449 GB 
[09/16 12:35:23 visual_prompt]: 	Test 300/407. loss: 2.082, 0.1833 s / batch. (data: 1.72e-04)max mem: 17.22449 GB 
[09/16 12:35:43 visual_prompt]: 	Test 400/407. loss: 2.024, 0.1825 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 12:35:46 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1938, average loss: 2.0883
[09/16 12:35:46 visual_prompt]: Classification results with test_vtab-svhn: top1: 26.25	top5: 69.21	
[09/16 12:35:46 visual_prompt]: Best epoch 76: best metric: 0.350
[09/16 12:35:46 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 12:35:56 visual_prompt]: Epoch 77 / 100: avg data time: 1.58e-01, avg batch time: 0.5586, average train loss: 2.0993
[09/16 12:36:01 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1427, average loss: 2.6597
[09/16 12:36:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/16 12:36:23 visual_prompt]: 	Test 100/407. loss: 3.073, 0.1917 s / batch. (data: 1.02e-04)max mem: 17.22449 GB 
[09/16 12:36:42 visual_prompt]: 	Test 200/407. loss: 3.034, 0.1825 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 12:37:01 visual_prompt]: 	Test 300/407. loss: 2.662, 0.1998 s / batch. (data: 1.71e-02)max mem: 17.22449 GB 
[09/16 12:37:21 visual_prompt]: 	Test 400/407. loss: 2.770, 0.1814 s / batch. (data: 2.62e-05)max mem: 17.22449 GB 
[09/16 12:37:24 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1935, average loss: 2.7485
[09/16 12:37:24 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 60.33	
[09/16 12:37:24 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 12:37:35 visual_prompt]: Epoch 78 / 100: avg data time: 1.49e-01, avg batch time: 0.5837, average train loss: 2.2064
[09/16 12:37:40 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1425, average loss: 1.9524
[09/16 12:37:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 31.50	top5: 73.50	
[09/16 12:38:02 visual_prompt]: 	Test 100/407. loss: 2.297, 0.1986 s / batch. (data: 1.58e-02)max mem: 17.22449 GB 
[09/16 12:38:21 visual_prompt]: 	Test 200/407. loss: 2.225, 0.1976 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 12:38:40 visual_prompt]: 	Test 300/407. loss: 2.038, 0.1970 s / batch. (data: 1.04e-04)max mem: 17.22449 GB 
[09/16 12:39:00 visual_prompt]: 	Test 400/407. loss: 2.074, 0.1820 s / batch. (data: 2.67e-05)max mem: 17.22449 GB 
[09/16 12:39:03 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1929, average loss: 2.1040
[09/16 12:39:03 visual_prompt]: Classification results with test_vtab-svhn: top1: 25.67	top5: 68.50	
[09/16 12:39:03 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 12:39:13 visual_prompt]: Epoch 79 / 100: avg data time: 1.50e-01, avg batch time: 0.5520, average train loss: 1.9357
[09/16 12:39:18 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1425, average loss: 1.7448
[09/16 12:39:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.50	top5: 83.50	
[09/16 12:39:40 visual_prompt]: 	Test 100/407. loss: 1.987, 0.2081 s / batch. (data: 5.23e-03)max mem: 17.22449 GB 
[09/16 12:39:59 visual_prompt]: 	Test 200/407. loss: 2.013, 0.1822 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 12:40:19 visual_prompt]: 	Test 300/407. loss: 1.788, 0.2245 s / batch. (data: 1.50e-04)max mem: 17.22449 GB 
[09/16 12:40:38 visual_prompt]: 	Test 400/407. loss: 1.845, 0.1822 s / batch. (data: 3.29e-05)max mem: 17.22449 GB 
[09/16 12:40:41 visual_prompt]: Inference (test):avg data time: 8.21e-03, avg batch time: 0.1936, average loss: 1.9322
[09/16 12:40:41 visual_prompt]: Classification results with test_vtab-svhn: top1: 29.03	top5: 75.67	
[09/16 12:40:41 visual_prompt]: Best epoch 79: best metric: 0.355
[09/16 12:40:41 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 12:40:52 visual_prompt]: Epoch 80 / 100: avg data time: 1.48e-01, avg batch time: 0.5575, average train loss: 1.9476
[09/16 12:40:56 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1426, average loss: 1.9851
[09/16 12:40:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.50	top5: 72.50	
[09/16 12:41:18 visual_prompt]: 	Test 100/407. loss: 2.214, 0.2060 s / batch. (data: 2.46e-02)max mem: 17.22449 GB 
[09/16 12:41:37 visual_prompt]: 	Test 200/407. loss: 2.212, 0.2060 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 12:41:57 visual_prompt]: 	Test 300/407. loss: 2.029, 0.1828 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 12:42:16 visual_prompt]: 	Test 400/407. loss: 2.114, 0.1825 s / batch. (data: 4.17e-05)max mem: 17.22449 GB 
[09/16 12:42:20 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1937, average loss: 2.0931
[09/16 12:42:20 visual_prompt]: Classification results with test_vtab-svhn: top1: 22.32	top5: 66.93	
[09/16 12:42:20 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 12:42:30 visual_prompt]: Epoch 81 / 100: avg data time: 1.44e-01, avg batch time: 0.5477, average train loss: 1.9040
[09/16 12:42:34 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1426, average loss: 1.6772
[09/16 12:42:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 40.50	top5: 80.00	
[09/16 12:42:56 visual_prompt]: 	Test 100/407. loss: 1.860, 0.1824 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 12:43:16 visual_prompt]: 	Test 200/407. loss: 1.955, 0.1937 s / batch. (data: 1.13e-02)max mem: 17.22449 GB 
[09/16 12:43:35 visual_prompt]: 	Test 300/407. loss: 1.668, 0.1927 s / batch. (data: 1.07e-02)max mem: 17.22449 GB 
[09/16 12:43:55 visual_prompt]: 	Test 400/407. loss: 1.765, 0.1824 s / batch. (data: 2.88e-05)max mem: 17.22449 GB 
[09/16 12:43:59 visual_prompt]: Inference (test):avg data time: 6.96e-03, avg batch time: 0.1950, average loss: 1.8606
[09/16 12:43:59 visual_prompt]: Classification results with test_vtab-svhn: top1: 33.91	top5: 76.69	
[09/16 12:43:59 visual_prompt]: Best epoch 81: best metric: 0.405
[09/16 12:43:59 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 12:44:09 visual_prompt]: Epoch 82 / 100: avg data time: 1.39e-01, avg batch time: 0.5421, average train loss: 1.8641
[09/16 12:44:13 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1434, average loss: 1.8462
[09/16 12:44:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 36.00	top5: 80.50	
[09/16 12:44:35 visual_prompt]: 	Test 100/407. loss: 2.108, 0.1838 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 12:44:54 visual_prompt]: 	Test 200/407. loss: 2.150, 0.1827 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 12:45:14 visual_prompt]: 	Test 300/407. loss: 1.869, 0.1958 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 12:45:33 visual_prompt]: 	Test 400/407. loss: 2.046, 0.1869 s / batch. (data: 3.55e-05)max mem: 17.22449 GB 
[09/16 12:45:37 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1933, average loss: 2.0643
[09/16 12:45:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 24.30	top5: 72.13	
[09/16 12:45:37 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 12:45:47 visual_prompt]: Epoch 83 / 100: avg data time: 1.36e-01, avg batch time: 0.5398, average train loss: 1.8614
[09/16 12:45:51 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1425, average loss: 1.8792
[09/16 12:45:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.00	top5: 75.00	
[09/16 12:46:13 visual_prompt]: 	Test 100/407. loss: 2.118, 0.1938 s / batch. (data: 1.22e-02)max mem: 17.22449 GB 
[09/16 12:46:32 visual_prompt]: 	Test 200/407. loss: 2.391, 0.1829 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 12:46:52 visual_prompt]: 	Test 300/407. loss: 1.878, 0.1956 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 12:47:11 visual_prompt]: 	Test 400/407. loss: 2.155, 0.1824 s / batch. (data: 2.91e-05)max mem: 17.22449 GB 
[09/16 12:47:15 visual_prompt]: Inference (test):avg data time: 8.03e-03, avg batch time: 0.1934, average loss: 2.1796
[09/16 12:47:15 visual_prompt]: Classification results with test_vtab-svhn: top1: 24.55	top5: 71.96	
[09/16 12:47:15 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 12:47:26 visual_prompt]: Epoch 84 / 100: avg data time: 1.54e-01, avg batch time: 0.5876, average train loss: 1.7264
[09/16 12:47:30 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1431, average loss: 1.7309
[09/16 12:47:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 36.00	top5: 85.00	
[09/16 12:47:52 visual_prompt]: 	Test 100/407. loss: 2.081, 0.1823 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 12:48:11 visual_prompt]: 	Test 200/407. loss: 2.192, 0.1994 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 12:48:31 visual_prompt]: 	Test 300/407. loss: 1.767, 0.1951 s / batch. (data: 1.27e-02)max mem: 17.22449 GB 
[09/16 12:48:50 visual_prompt]: 	Test 400/407. loss: 1.890, 0.1823 s / batch. (data: 4.29e-05)max mem: 17.22449 GB 
[09/16 12:48:53 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1935, average loss: 1.9162
[09/16 12:48:54 visual_prompt]: Classification results with test_vtab-svhn: top1: 31.52	top5: 80.62	
[09/16 12:48:54 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 12:49:04 visual_prompt]: Epoch 85 / 100: avg data time: 1.50e-01, avg batch time: 0.5592, average train loss: 1.7947
[09/16 12:49:08 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1425, average loss: 1.6953
[09/16 12:49:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 38.00	top5: 85.00	
[09/16 12:49:30 visual_prompt]: 	Test 100/407. loss: 2.017, 0.1957 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 12:49:50 visual_prompt]: 	Test 200/407. loss: 2.022, 0.2247 s / batch. (data: 3.67e-02)max mem: 17.22449 GB 
[09/16 12:50:09 visual_prompt]: 	Test 300/407. loss: 1.734, 0.1998 s / batch. (data: 1.80e-02)max mem: 17.22449 GB 
[09/16 12:50:28 visual_prompt]: 	Test 400/407. loss: 1.896, 0.1847 s / batch. (data: 3.46e-05)max mem: 17.22449 GB 
[09/16 12:50:31 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1928, average loss: 1.9438
[09/16 12:50:31 visual_prompt]: Classification results with test_vtab-svhn: top1: 30.49	top5: 76.83	
[09/16 12:50:31 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 12:50:42 visual_prompt]: Epoch 86 / 100: avg data time: 1.56e-01, avg batch time: 0.5551, average train loss: 1.6475
[09/16 12:50:46 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1424, average loss: 1.4821
[09/16 12:50:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 46.00	top5: 88.50	
[09/16 12:51:08 visual_prompt]: 	Test 100/407. loss: 1.907, 0.1824 s / batch. (data: 9.89e-05)max mem: 17.22449 GB 
[09/16 12:51:27 visual_prompt]: 	Test 200/407. loss: 1.839, 0.2075 s / batch. (data: 2.58e-02)max mem: 17.22449 GB 
[09/16 12:51:47 visual_prompt]: 	Test 300/407. loss: 1.621, 0.1819 s / batch. (data: 8.61e-05)max mem: 17.22449 GB 
[09/16 12:52:07 visual_prompt]: 	Test 400/407. loss: 1.685, 0.1819 s / batch. (data: 3.34e-05)max mem: 17.22449 GB 
[09/16 12:52:10 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1934, average loss: 1.7324
[09/16 12:52:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 37.61	top5: 83.36	
[09/16 12:52:10 visual_prompt]: Best epoch 86: best metric: 0.460
[09/16 12:52:10 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 12:52:21 visual_prompt]: Epoch 87 / 100: avg data time: 1.50e-01, avg batch time: 0.5528, average train loss: 1.5343
[09/16 12:52:25 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1423, average loss: 1.4178
[09/16 12:52:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 50.00	top5: 92.50	
[09/16 12:52:47 visual_prompt]: 	Test 100/407. loss: 1.733, 0.1962 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 12:53:06 visual_prompt]: 	Test 200/407. loss: 1.764, 0.1826 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 12:53:25 visual_prompt]: 	Test 300/407. loss: 1.591, 0.2108 s / batch. (data: 2.11e-02)max mem: 17.22449 GB 
[09/16 12:53:45 visual_prompt]: 	Test 400/407. loss: 1.653, 0.1828 s / batch. (data: 3.36e-05)max mem: 17.22449 GB 
[09/16 12:53:48 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1933, average loss: 1.7074
[09/16 12:53:48 visual_prompt]: Classification results with test_vtab-svhn: top1: 38.97	top5: 85.41	
[09/16 12:53:48 visual_prompt]: Best epoch 87: best metric: 0.500
[09/16 12:53:48 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 12:53:58 visual_prompt]: Epoch 88 / 100: avg data time: 1.46e-01, avg batch time: 0.5467, average train loss: 1.4337
[09/16 12:54:03 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1426, average loss: 1.3044
[09/16 12:54:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.00	top5: 92.00	
[09/16 12:54:25 visual_prompt]: 	Test 100/407. loss: 1.742, 0.2061 s / batch. (data: 1.29e-02)max mem: 17.22449 GB 
[09/16 12:54:44 visual_prompt]: 	Test 200/407. loss: 1.708, 0.1978 s / batch. (data: 3.58e-05)max mem: 17.22449 GB 
[09/16 12:55:04 visual_prompt]: 	Test 300/407. loss: 1.557, 0.1984 s / batch. (data: 1.07e-04)max mem: 17.22449 GB 
[09/16 12:55:23 visual_prompt]: 	Test 400/407. loss: 1.580, 0.1827 s / batch. (data: 2.86e-05)max mem: 17.22449 GB 
[09/16 12:55:27 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1943, average loss: 1.6835
[09/16 12:55:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 39.84	top5: 85.79	
[09/16 12:55:27 visual_prompt]: Best epoch 88: best metric: 0.550
[09/16 12:55:27 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 12:55:37 visual_prompt]: Epoch 89 / 100: avg data time: 1.49e-01, avg batch time: 0.5522, average train loss: 1.3745
[09/16 12:55:41 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1423, average loss: 1.2765
[09/16 12:55:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.50	top5: 95.00	
[09/16 12:56:03 visual_prompt]: 	Test 100/407. loss: 1.715, 0.2045 s / batch. (data: 2.27e-02)max mem: 17.22449 GB 
[09/16 12:56:23 visual_prompt]: 	Test 200/407. loss: 1.742, 0.1965 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 12:56:42 visual_prompt]: 	Test 300/407. loss: 1.563, 0.1832 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 12:57:02 visual_prompt]: 	Test 400/407. loss: 1.612, 0.1826 s / batch. (data: 3.31e-05)max mem: 17.22449 GB 
[09/16 12:57:05 visual_prompt]: Inference (test):avg data time: 8.81e-03, avg batch time: 0.1945, average loss: 1.6718
[09/16 12:57:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 40.77	top5: 86.62	
[09/16 12:57:05 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 12:57:16 visual_prompt]: Epoch 90 / 100: avg data time: 1.57e-01, avg batch time: 0.5566, average train loss: 1.2811
[09/16 12:57:20 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1425, average loss: 1.1569
[09/16 12:57:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 58.00	top5: 94.50	
[09/16 12:57:42 visual_prompt]: 	Test 100/407. loss: 1.684, 0.1889 s / batch. (data: 1.14e-04)max mem: 17.22449 GB 
[09/16 12:58:01 visual_prompt]: 	Test 200/407. loss: 1.657, 0.2125 s / batch. (data: 1.56e-02)max mem: 17.22449 GB 
[09/16 12:58:21 visual_prompt]: 	Test 300/407. loss: 1.524, 0.2069 s / batch. (data: 2.53e-02)max mem: 17.22449 GB 
[09/16 12:58:40 visual_prompt]: 	Test 400/407. loss: 1.534, 0.1830 s / batch. (data: 3.43e-05)max mem: 17.22449 GB 
[09/16 12:58:44 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1942, average loss: 1.6206
[09/16 12:58:44 visual_prompt]: Classification results with test_vtab-svhn: top1: 43.62	top5: 86.49	
[09/16 12:58:44 visual_prompt]: Best epoch 90: best metric: 0.580
[09/16 12:58:44 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 12:58:54 visual_prompt]: Epoch 91 / 100: avg data time: 1.51e-01, avg batch time: 0.5517, average train loss: 1.2088
[09/16 12:58:58 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1428, average loss: 1.1222
[09/16 12:58:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 59.50	top5: 95.50	
[09/16 12:59:20 visual_prompt]: 	Test 100/407. loss: 1.708, 0.1951 s / batch. (data: 1.28e-02)max mem: 17.22449 GB 
[09/16 12:59:39 visual_prompt]: 	Test 200/407. loss: 1.539, 0.1977 s / batch. (data: 9.44e-05)max mem: 17.22449 GB 
[09/16 12:59:59 visual_prompt]: 	Test 300/407. loss: 1.587, 0.1964 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 13:00:18 visual_prompt]: 	Test 400/407. loss: 1.623, 0.1822 s / batch. (data: 2.69e-05)max mem: 17.22449 GB 
[09/16 13:00:22 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1932, average loss: 1.6376
[09/16 13:00:22 visual_prompt]: Classification results with test_vtab-svhn: top1: 42.78	top5: 86.56	
[09/16 13:00:22 visual_prompt]: Best epoch 91: best metric: 0.595
[09/16 13:00:22 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 13:00:32 visual_prompt]: Epoch 92 / 100: avg data time: 1.50e-01, avg batch time: 0.5530, average train loss: 1.1504
[09/16 13:00:37 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1424, average loss: 1.0036
[09/16 13:00:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.00	top5: 98.50	
[09/16 13:00:58 visual_prompt]: 	Test 100/407. loss: 1.672, 0.1829 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 13:01:18 visual_prompt]: 	Test 200/407. loss: 1.540, 0.1833 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 13:01:37 visual_prompt]: 	Test 300/407. loss: 1.423, 0.2018 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 13:01:56 visual_prompt]: 	Test 400/407. loss: 1.463, 0.1824 s / batch. (data: 3.84e-05)max mem: 17.22449 GB 
[09/16 13:02:00 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1933, average loss: 1.5922
[09/16 13:02:00 visual_prompt]: Classification results with test_vtab-svhn: top1: 45.18	top5: 87.27	
[09/16 13:02:00 visual_prompt]: Best epoch 92: best metric: 0.630
[09/16 13:02:00 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 13:02:10 visual_prompt]: Epoch 93 / 100: avg data time: 1.49e-01, avg batch time: 0.5500, average train loss: 1.0703
[09/16 13:02:15 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1426, average loss: 0.9453
[09/16 13:02:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 97.50	
[09/16 13:02:36 visual_prompt]: 	Test 100/407. loss: 1.699, 0.1955 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 13:02:56 visual_prompt]: 	Test 200/407. loss: 1.677, 0.1970 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 13:03:15 visual_prompt]: 	Test 300/407. loss: 1.485, 0.1828 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 13:03:35 visual_prompt]: 	Test 400/407. loss: 1.485, 0.1825 s / batch. (data: 3.19e-05)max mem: 17.22449 GB 
[09/16 13:03:38 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1932, average loss: 1.6030
[09/16 13:03:38 visual_prompt]: Classification results with test_vtab-svhn: top1: 45.83	top5: 88.10	
[09/16 13:03:38 visual_prompt]: Best epoch 93: best metric: 0.670
[09/16 13:03:38 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 13:03:48 visual_prompt]: Epoch 94 / 100: avg data time: 1.54e-01, avg batch time: 0.5548, average train loss: 1.0144
[09/16 13:03:53 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1427, average loss: 0.9119
[09/16 13:03:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 99.00	
[09/16 13:04:15 visual_prompt]: 	Test 100/407. loss: 1.714, 0.1825 s / batch. (data: 1.62e-04)max mem: 17.22449 GB 
[09/16 13:04:34 visual_prompt]: 	Test 200/407. loss: 1.560, 0.1823 s / batch. (data: 8.80e-05)max mem: 17.22449 GB 
[09/16 13:04:54 visual_prompt]: 	Test 300/407. loss: 1.509, 0.1936 s / batch. (data: 1.13e-02)max mem: 17.22449 GB 
[09/16 13:05:13 visual_prompt]: 	Test 400/407. loss: 1.597, 0.1822 s / batch. (data: 3.67e-05)max mem: 17.22449 GB 
[09/16 13:05:17 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1946, average loss: 1.6150
[09/16 13:05:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 44.60	top5: 87.80	
[09/16 13:05:17 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 13:05:27 visual_prompt]: Epoch 95 / 100: avg data time: 1.46e-01, avg batch time: 0.5494, average train loss: 0.9466
[09/16 13:05:32 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1426, average loss: 0.8296
[09/16 13:05:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 98.50	
[09/16 13:05:53 visual_prompt]: 	Test 100/407. loss: 1.663, 0.1831 s / batch. (data: 1.53e-04)max mem: 17.22449 GB 
[09/16 13:06:13 visual_prompt]: 	Test 200/407. loss: 1.583, 0.2170 s / batch. (data: 3.52e-02)max mem: 17.22449 GB 
[09/16 13:06:32 visual_prompt]: 	Test 300/407. loss: 1.452, 0.1916 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 13:06:52 visual_prompt]: 	Test 400/407. loss: 1.514, 0.1832 s / batch. (data: 3.70e-05)max mem: 17.22449 GB 
[09/16 13:06:55 visual_prompt]: Inference (test):avg data time: 8.36e-03, avg batch time: 0.1936, average loss: 1.6161
[09/16 13:06:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 46.45	top5: 87.78	
[09/16 13:06:55 visual_prompt]: Best epoch 95: best metric: 0.710
[09/16 13:06:55 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 13:07:05 visual_prompt]: Epoch 96 / 100: avg data time: 1.34e-01, avg batch time: 0.5367, average train loss: 0.8959
[09/16 13:07:10 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1427, average loss: 0.7606
[09/16 13:07:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 99.50	
[09/16 13:07:32 visual_prompt]: 	Test 100/407. loss: 1.693, 0.1954 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 13:07:52 visual_prompt]: 	Test 200/407. loss: 1.561, 0.1957 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 13:08:11 visual_prompt]: 	Test 300/407. loss: 1.469, 0.1973 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 13:08:31 visual_prompt]: 	Test 400/407. loss: 1.480, 0.1821 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 13:08:34 visual_prompt]: Inference (test):avg data time: 8.45e-03, avg batch time: 0.1949, average loss: 1.5937
[09/16 13:08:34 visual_prompt]: Classification results with test_vtab-svhn: top1: 48.11	top5: 88.46	
[09/16 13:08:34 visual_prompt]: Best epoch 96: best metric: 0.750
[09/16 13:08:34 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 13:08:44 visual_prompt]: Epoch 97 / 100: avg data time: 1.50e-01, avg batch time: 0.5517, average train loss: 0.8443
[09/16 13:08:49 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1425, average loss: 0.7541
[09/16 13:08:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 99.00	
[09/16 13:09:10 visual_prompt]: 	Test 100/407. loss: 1.761, 0.1832 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 13:09:30 visual_prompt]: 	Test 200/407. loss: 1.639, 0.1825 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 13:09:49 visual_prompt]: 	Test 300/407. loss: 1.439, 0.2081 s / batch. (data: 2.57e-02)max mem: 17.22449 GB 
[09/16 13:10:08 visual_prompt]: 	Test 400/407. loss: 1.488, 0.1997 s / batch. (data: 8.63e-05)max mem: 17.22449 GB 
[09/16 13:10:11 visual_prompt]: Inference (test):avg data time: 7.26e-03, avg batch time: 0.1921, average loss: 1.6465
[09/16 13:10:11 visual_prompt]: Classification results with test_vtab-svhn: top1: 47.36	top5: 87.60	
[09/16 13:10:11 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 13:10:22 visual_prompt]: Epoch 98 / 100: avg data time: 1.52e-01, avg batch time: 0.5525, average train loss: 0.8288
[09/16 13:10:27 visual_prompt]: Inference (val):avg data time: 3.37e-04, avg batch time: 0.2116, average loss: 0.7134
[09/16 13:10:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 99.50	
[09/16 13:10:48 visual_prompt]: 	Test 100/407. loss: 1.735, 0.1827 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 13:11:08 visual_prompt]: 	Test 200/407. loss: 1.592, 0.1909 s / batch. (data: 3.93e-05)max mem: 17.22449 GB 
[09/16 13:11:27 visual_prompt]: 	Test 300/407. loss: 1.461, 0.2053 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 13:11:46 visual_prompt]: 	Test 400/407. loss: 1.492, 0.1823 s / batch. (data: 3.79e-05)max mem: 17.22449 GB 
[09/16 13:11:50 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1928, average loss: 1.6377
[09/16 13:11:50 visual_prompt]: Classification results with test_vtab-svhn: top1: 48.04	top5: 88.24	
[09/16 13:11:50 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 13:12:00 visual_prompt]: Epoch 99 / 100: avg data time: 1.47e-01, avg batch time: 0.5476, average train loss: 0.7974
[09/16 13:12:05 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1423, average loss: 0.7217
[09/16 13:12:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 100.00	
[09/16 13:12:27 visual_prompt]: 	Test 100/407. loss: 1.752, 0.1826 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 13:12:46 visual_prompt]: 	Test 200/407. loss: 1.643, 0.1826 s / batch. (data: 1.90e-04)max mem: 17.22449 GB 
[09/16 13:13:06 visual_prompt]: 	Test 300/407. loss: 1.581, 0.1825 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 13:13:25 visual_prompt]: 	Test 400/407. loss: 1.538, 0.1826 s / batch. (data: 2.91e-05)max mem: 17.22449 GB 
[09/16 13:13:28 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1934, average loss: 1.6557
[09/16 13:13:28 visual_prompt]: Classification results with test_vtab-svhn: top1: 47.14	top5: 88.39	
[09/16 13:13:28 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 13:13:39 visual_prompt]: Epoch 100 / 100: avg data time: 1.36e-01, avg batch time: 0.5378, average train loss: 0.7887
[09/16 13:13:43 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1426, average loss: 0.7076
[09/16 13:13:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 99.50	
[09/16 13:14:05 visual_prompt]: 	Test 100/407. loss: 1.730, 0.1835 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 13:14:24 visual_prompt]: 	Test 200/407. loss: 1.626, 0.1833 s / batch. (data: 1.09e-04)max mem: 17.22449 GB 
[09/16 13:14:43 visual_prompt]: 	Test 300/407. loss: 1.538, 0.1958 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 13:15:03 visual_prompt]: 	Test 400/407. loss: 1.516, 0.1816 s / batch. (data: 4.96e-05)max mem: 17.22449 GB 
[09/16 13:15:06 visual_prompt]: Inference (test):avg data time: 7.03e-03, avg batch time: 0.1923, average loss: 1.6465
[09/16 13:15:06 visual_prompt]: Classification results with test_vtab-svhn: top1: 47.57	top5: 88.41	
[09/16 13:15:36 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 13:15:36 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 13:15:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed82'], train_type='')
[09/16 13:15:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 13:15:36 visual_prompt]: Training with config:
[09/16 13:15:36 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-svhn',
          'NO_TEST': False,
          'NUMBER_CLASSES': 10,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed82/vtab-svhn/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 13:15:36 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 13:15:36.258146: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 13:15:36.435227: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 13:15:37.342466: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 13:15:37.342551: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 13:15:37.342560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 13:15:39.374618: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 13:15:39.374734: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 13:15:39.374747: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 13:15:39 visual_prompt]: Constructing vtab-svhn dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
2023-09-16 13:15:39.420859: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split train[:800]+train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 13:15:41 visual_prompt]: Number of images: 1000
[09/16 13:15:41 visual_prompt]: Number of classes: 10 / 10
[09/16 13:15:41 visual_prompt]: Loading validation data...
[09/16 13:15:41 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 13:15:41 visual_prompt]: Number of images: 200
[09/16 13:15:41 visual_prompt]: Number of classes: 10 / 10
[09/16 13:15:41 visual_prompt]: Loading test data...
[09/16 13:15:41 visual_prompt]: Constructing vtab-svhn dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split test, from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 13:16:13 visual_prompt]: Number of images: 26032
[09/16 13:16:13 visual_prompt]: Number of classes: 10 / 10
[09/16 13:16:13 visual_prompt]: Constructing models...
[09/16 13:16:16 visual_prompt]: Total Parameters: 86727946	 Gradient Parameters: 929290
[09/16 13:16:16 visual_prompt]: tuned percent:1.072
[09/16 13:16:18 visual_prompt]: Device used for model: 0
[09/16 13:16:18 visual_prompt]: Setting up Evalutator...
[09/16 13:16:18 visual_prompt]: Setting up Trainer...
[09/16 13:16:18 visual_prompt]: 	Setting up the optimizer...
[09/16 13:16:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 13:16:30 visual_prompt]: Epoch 1 / 100: avg data time: 1.56e-01, avg batch time: 0.6478, average train loss: 2.7789
[09/16 13:16:35 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1419, average loss: 2.7503
[09/16 13:16:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 39.50	
[09/16 13:16:56 visual_prompt]: 	Test 100/407. loss: 2.774, 0.2079 s / batch. (data: 1.75e-02)max mem: 17.22449 GB 
[09/16 13:17:16 visual_prompt]: 	Test 200/407. loss: 2.766, 0.1827 s / batch. (data: 1.49e-04)max mem: 17.22449 GB 
[09/16 13:17:35 visual_prompt]: 	Test 300/407. loss: 2.835, 0.1974 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 13:17:54 visual_prompt]: 	Test 400/407. loss: 2.938, 0.1826 s / batch. (data: 3.19e-05)max mem: 17.22449 GB 
[09/16 13:17:58 visual_prompt]: Inference (test):avg data time: 6.67e-03, avg batch time: 0.1927, average loss: 2.8256
[09/16 13:17:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.38	top5: 37.93	
[09/16 13:17:58 visual_prompt]: Best epoch 1: best metric: 0.060
[09/16 13:17:58 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 13:18:08 visual_prompt]: Epoch 2 / 100: avg data time: 1.53e-01, avg batch time: 0.5526, average train loss: 4.0922
[09/16 13:18:12 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1422, average loss: 3.3309
[09/16 13:18:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 60.00	
[09/16 13:18:34 visual_prompt]: 	Test 100/407. loss: 4.466, 0.1957 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 13:18:54 visual_prompt]: 	Test 200/407. loss: 3.941, 0.1952 s / batch. (data: 1.30e-02)max mem: 17.22449 GB 
[09/16 13:19:13 visual_prompt]: 	Test 300/407. loss: 3.628, 0.1832 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 13:19:33 visual_prompt]: 	Test 400/407. loss: 2.826, 0.1829 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 13:19:37 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1948, average loss: 3.4007
[09/16 13:19:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.93	top5: 60.57	
[09/16 13:19:37 visual_prompt]: Best epoch 2: best metric: 0.080
[09/16 13:19:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 13:19:47 visual_prompt]: Epoch 3 / 100: avg data time: 1.46e-01, avg batch time: 0.5466, average train loss: 2.7147
[09/16 13:19:52 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1427, average loss: 2.3818
[09/16 13:19:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 53.00	
[09/16 13:20:14 visual_prompt]: 	Test 100/407. loss: 2.418, 0.2208 s / batch. (data: 3.88e-02)max mem: 17.22449 GB 
[09/16 13:20:33 visual_prompt]: 	Test 200/407. loss: 2.419, 0.1961 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 13:20:54 visual_prompt]: 	Test 300/407. loss: 2.465, 0.1830 s / batch. (data: 1.58e-04)max mem: 17.22449 GB 
[09/16 13:21:13 visual_prompt]: 	Test 400/407. loss: 2.430, 0.1827 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 13:21:16 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1964, average loss: 2.3928
[09/16 13:21:16 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.69	top5: 50.11	
[09/16 13:21:16 visual_prompt]: Best epoch 3: best metric: 0.085
[09/16 13:21:16 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 13:21:27 visual_prompt]: Epoch 4 / 100: avg data time: 1.49e-01, avg batch time: 0.5521, average train loss: 2.5142
[09/16 13:21:31 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1426, average loss: 2.6115
[09/16 13:21:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 38.50	
[09/16 13:21:53 visual_prompt]: 	Test 100/407. loss: 2.395, 0.1958 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 13:22:13 visual_prompt]: 	Test 200/407. loss: 2.476, 0.1823 s / batch. (data: 1.15e-04)max mem: 17.22449 GB 
[09/16 13:22:32 visual_prompt]: 	Test 300/407. loss: 2.640, 0.1883 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 13:22:51 visual_prompt]: 	Test 400/407. loss: 2.628, 0.1829 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 13:22:55 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1936, average loss: 2.5816
[09/16 13:22:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.11	top5: 39.57	
[09/16 13:22:55 visual_prompt]: Best epoch 4: best metric: 0.090
[09/16 13:22:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 13:23:05 visual_prompt]: Epoch 5 / 100: avg data time: 1.43e-01, avg batch time: 0.5462, average train loss: 2.5628
[09/16 13:23:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1426, average loss: 2.6152
[09/16 13:23:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.00	
[09/16 13:23:31 visual_prompt]: 	Test 100/407. loss: 2.424, 0.2003 s / batch. (data: 1.83e-02)max mem: 17.22449 GB 
[09/16 13:23:51 visual_prompt]: 	Test 200/407. loss: 2.464, 0.1970 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 13:24:11 visual_prompt]: 	Test 300/407. loss: 2.697, 0.1822 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 13:24:30 visual_prompt]: 	Test 400/407. loss: 2.569, 0.1823 s / batch. (data: 3.58e-05)max mem: 17.22449 GB 
[09/16 13:24:34 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1950, average loss: 2.5784
[09/16 13:24:34 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 61.34	
[09/16 13:24:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 13:24:44 visual_prompt]: Epoch 6 / 100: avg data time: 1.56e-01, avg batch time: 0.5566, average train loss: 3.1257
[09/16 13:24:49 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1424, average loss: 3.3588
[09/16 13:24:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.00	
[09/16 13:25:11 visual_prompt]: 	Test 100/407. loss: 3.323, 0.1826 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 13:25:30 visual_prompt]: 	Test 200/407. loss: 3.334, 0.1827 s / batch. (data: 1.15e-04)max mem: 17.22449 GB 
[09/16 13:25:49 visual_prompt]: 	Test 300/407. loss: 3.214, 0.1826 s / batch. (data: 1.41e-04)max mem: 17.22449 GB 
[09/16 13:26:09 visual_prompt]: 	Test 400/407. loss: 3.358, 0.1826 s / batch. (data: 3.84e-05)max mem: 17.22449 GB 
[09/16 13:26:12 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1931, average loss: 3.2386
[09/16 13:26:12 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 55.95	
[09/16 13:26:12 visual_prompt]: Best epoch 6: best metric: 0.120
[09/16 13:26:12 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 13:26:23 visual_prompt]: Epoch 7 / 100: avg data time: 1.59e-01, avg batch time: 0.5611, average train loss: 3.8656
[09/16 13:26:27 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1424, average loss: 3.8796
[09/16 13:26:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/16 13:26:49 visual_prompt]: 	Test 100/407. loss: 4.645, 0.1824 s / batch. (data: 3.89e-05)max mem: 17.22449 GB 
[09/16 13:27:08 visual_prompt]: 	Test 200/407. loss: 5.033, 0.2216 s / batch. (data: 3.99e-02)max mem: 17.22449 GB 
[09/16 13:27:28 visual_prompt]: 	Test 300/407. loss: 3.371, 0.1966 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 13:27:47 visual_prompt]: 	Test 400/407. loss: 4.179, 0.1826 s / batch. (data: 3.29e-05)max mem: 17.22449 GB 
[09/16 13:27:51 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1948, average loss: 3.9940
[09/16 13:27:51 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 63.51	
[09/16 13:27:51 visual_prompt]: Best epoch 7: best metric: 0.230
[09/16 13:27:51 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 13:28:01 visual_prompt]: Epoch 8 / 100: avg data time: 1.37e-01, avg batch time: 0.5420, average train loss: 4.0375
[09/16 13:28:06 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1424, average loss: 4.7042
[09/16 13:28:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 39.50	
[09/16 13:28:28 visual_prompt]: 	Test 100/407. loss: 4.794, 0.2053 s / batch. (data: 1.57e-02)max mem: 17.22449 GB 
[09/16 13:28:47 visual_prompt]: 	Test 200/407. loss: 4.727, 0.1896 s / batch. (data: 1.52e-04)max mem: 17.22449 GB 
[09/16 13:29:07 visual_prompt]: 	Test 300/407. loss: 4.808, 0.1901 s / batch. (data: 9.87e-05)max mem: 17.22449 GB 
[09/16 13:29:26 visual_prompt]: 	Test 400/407. loss: 4.737, 0.1840 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 13:29:29 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1933, average loss: 4.7888
[09/16 13:29:29 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 38.03	
[09/16 13:29:29 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 13:29:40 visual_prompt]: Epoch 9 / 100: avg data time: 1.54e-01, avg batch time: 0.5553, average train loss: 4.5117
[09/16 13:29:44 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1427, average loss: 3.8849
[09/16 13:29:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 64.00	
[09/16 13:30:06 visual_prompt]: 	Test 100/407. loss: 5.248, 0.1829 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 13:30:25 visual_prompt]: 	Test 200/407. loss: 4.734, 0.1959 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 13:30:45 visual_prompt]: 	Test 300/407. loss: 3.898, 0.1942 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 13:31:04 visual_prompt]: 	Test 400/407. loss: 3.361, 0.1820 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 13:31:08 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1936, average loss: 3.9387
[09/16 13:31:08 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.59	top5: 63.35	
[09/16 13:31:08 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 13:31:18 visual_prompt]: Epoch 10 / 100: avg data time: 1.53e-01, avg batch time: 0.5547, average train loss: 4.7865
[09/16 13:31:23 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1426, average loss: 4.9898
[09/16 13:31:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 58.00	
[09/16 13:31:45 visual_prompt]: 	Test 100/407. loss: 6.733, 0.1963 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 13:32:04 visual_prompt]: 	Test 200/407. loss: 6.552, 0.1931 s / batch. (data: 1.11e-02)max mem: 17.22449 GB 
[09/16 13:32:24 visual_prompt]: 	Test 300/407. loss: 4.797, 0.1956 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 13:32:43 visual_prompt]: 	Test 400/407. loss: 5.140, 0.1825 s / batch. (data: 3.96e-05)max mem: 17.22449 GB 
[09/16 13:32:46 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1941, average loss: 5.3649
[09/16 13:32:47 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.59	top5: 53.54	
[09/16 13:32:47 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 13:32:57 visual_prompt]: Epoch 11 / 100: avg data time: 1.48e-01, avg batch time: 0.5488, average train loss: 9.7558
[09/16 13:33:01 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1425, average loss: 11.2587
[09/16 13:33:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.00	top5: 62.50	
[09/16 13:33:23 visual_prompt]: 	Test 100/407. loss: 15.144, 0.1834 s / batch. (data: 1.54e-04)max mem: 17.22449 GB 
[09/16 13:33:43 visual_prompt]: 	Test 200/407. loss: 13.948, 0.1839 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 13:34:02 visual_prompt]: 	Test 300/407. loss: 12.072, 0.1945 s / batch. (data: 1.22e-02)max mem: 17.22449 GB 
[09/16 13:34:22 visual_prompt]: 	Test 400/407. loss: 11.426, 0.1823 s / batch. (data: 3.41e-05)max mem: 17.22449 GB 
[09/16 13:34:25 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1950, average loss: 11.7242
[09/16 13:34:26 visual_prompt]: Classification results with test_vtab-svhn: top1: 16.68	top5: 61.05	
[09/16 13:34:26 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 13:34:36 visual_prompt]: Epoch 12 / 100: avg data time: 1.50e-01, avg batch time: 0.5530, average train loss: 11.9494
[09/16 13:34:40 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1426, average loss: 8.7646
[09/16 13:34:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.50	
[09/16 13:35:02 visual_prompt]: 	Test 100/407. loss: 8.918, 0.1825 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 13:35:22 visual_prompt]: 	Test 200/407. loss: 7.849, 0.1989 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 13:35:41 visual_prompt]: 	Test 300/407. loss: 9.773, 0.2097 s / batch. (data: 3.51e-03)max mem: 17.22449 GB 
[09/16 13:36:01 visual_prompt]: 	Test 400/407. loss: 8.664, 0.1829 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 13:36:04 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1942, average loss: 8.7319
[09/16 13:36:04 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 44.21	
[09/16 13:36:04 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 13:36:15 visual_prompt]: Epoch 13 / 100: avg data time: 1.50e-01, avg batch time: 0.5504, average train loss: 19.2361
[09/16 13:36:19 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1425, average loss: 27.1029
[09/16 13:36:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/16 13:36:41 visual_prompt]: 	Test 100/407. loss: 30.249, 0.1827 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 13:37:01 visual_prompt]: 	Test 200/407. loss: 26.801, 0.1962 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 13:37:20 visual_prompt]: 	Test 300/407. loss: 32.837, 0.1984 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 13:37:40 visual_prompt]: 	Test 400/407. loss: 31.263, 0.1831 s / batch. (data: 3.79e-05)max mem: 17.22449 GB 
[09/16 13:37:43 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1943, average loss: 29.7886
[09/16 13:37:43 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 39.50	
[09/16 13:37:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 13:37:53 visual_prompt]: Epoch 14 / 100: avg data time: 1.45e-01, avg batch time: 0.5460, average train loss: 22.2261
[09/16 13:37:58 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1425, average loss: 14.1239
[09/16 13:37:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/16 13:38:20 visual_prompt]: 	Test 100/407. loss: 19.150, 0.1832 s / batch. (data: 1.62e-04)max mem: 17.22449 GB 
[09/16 13:38:39 visual_prompt]: 	Test 200/407. loss: 17.642, 0.1938 s / batch. (data: 1.17e-02)max mem: 17.22449 GB 
[09/16 13:38:59 visual_prompt]: 	Test 300/407. loss: 15.623, 0.1963 s / batch. (data: 1.29e-02)max mem: 17.22449 GB 
[09/16 13:39:18 visual_prompt]: 	Test 400/407. loss: 14.994, 0.1832 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 13:39:21 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1938, average loss: 15.0532
[09/16 13:39:22 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 54.81	
[09/16 13:39:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 13:39:32 visual_prompt]: Epoch 15 / 100: avg data time: 1.55e-01, avg batch time: 0.5590, average train loss: 17.1285
[09/16 13:39:37 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1424, average loss: 16.9788
[09/16 13:39:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/16 13:39:58 visual_prompt]: 	Test 100/407. loss: 18.327, 0.1823 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 13:40:18 visual_prompt]: 	Test 200/407. loss: 18.591, 0.2236 s / batch. (data: 3.55e-02)max mem: 17.22449 GB 
[09/16 13:40:37 visual_prompt]: 	Test 300/407. loss: 17.394, 0.1832 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 13:40:57 visual_prompt]: 	Test 400/407. loss: 19.024, 0.1827 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 13:41:00 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1946, average loss: 17.4759
[09/16 13:41:00 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 54.73	
[09/16 13:41:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 13:41:11 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.5513, average train loss: 14.2399
[09/16 13:41:15 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1425, average loss: 22.8726
[09/16 13:41:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.50	
[09/16 13:41:37 visual_prompt]: 	Test 100/407. loss: 20.109, 0.2072 s / batch. (data: 2.58e-02)max mem: 17.22449 GB 
[09/16 13:41:56 visual_prompt]: 	Test 200/407. loss: 19.608, 0.1826 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 13:42:16 visual_prompt]: 	Test 300/407. loss: 22.122, 0.1958 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 13:42:35 visual_prompt]: 	Test 400/407. loss: 19.662, 0.2018 s / batch. (data: 4.24e-05)max mem: 17.22449 GB 
[09/16 13:42:39 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1942, average loss: 21.0336
[09/16 13:42:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 49.53	
[09/16 13:42:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 13:42:49 visual_prompt]: Epoch 17 / 100: avg data time: 1.51e-01, avg batch time: 0.5518, average train loss: 13.4391
[09/16 13:42:54 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1425, average loss: 11.6385
[09/16 13:42:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 53.00	
[09/16 13:43:16 visual_prompt]: 	Test 100/407. loss: 13.076, 0.2144 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 13:43:35 visual_prompt]: 	Test 200/407. loss: 12.039, 0.1824 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 13:43:55 visual_prompt]: 	Test 300/407. loss: 14.046, 0.1912 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 13:44:15 visual_prompt]: 	Test 400/407. loss: 13.015, 0.1829 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 13:44:18 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1947, average loss: 12.5001
[09/16 13:44:18 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.76	top5: 50.11	
[09/16 13:44:18 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 13:44:28 visual_prompt]: Epoch 18 / 100: avg data time: 1.35e-01, avg batch time: 0.5369, average train loss: 11.4395
[09/16 13:44:33 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1425, average loss: 10.4925
[09/16 13:44:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.00	
[09/16 13:44:54 visual_prompt]: 	Test 100/407. loss: 9.042, 0.2046 s / batch. (data: 3.58e-05)max mem: 17.22449 GB 
[09/16 13:45:14 visual_prompt]: 	Test 200/407. loss: 9.761, 0.1957 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 13:45:33 visual_prompt]: 	Test 300/407. loss: 9.428, 0.1936 s / batch. (data: 1.13e-02)max mem: 17.22449 GB 
[09/16 13:45:53 visual_prompt]: 	Test 400/407. loss: 9.241, 0.1821 s / batch. (data: 3.50e-05)max mem: 17.22449 GB 
[09/16 13:45:56 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1935, average loss: 10.1458
[09/16 13:45:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.76	top5: 47.36	
[09/16 13:45:56 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 13:46:06 visual_prompt]: Epoch 19 / 100: avg data time: 1.47e-01, avg batch time: 0.5515, average train loss: 7.2407
[09/16 13:46:11 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1424, average loss: 4.9027
[09/16 13:46:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 47.00	
[09/16 13:46:33 visual_prompt]: 	Test 100/407. loss: 4.427, 0.1827 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 13:46:52 visual_prompt]: 	Test 200/407. loss: 5.168, 0.2083 s / batch. (data: 2.64e-02)max mem: 17.22449 GB 
[09/16 13:47:11 visual_prompt]: 	Test 300/407. loss: 4.056, 0.2024 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 13:47:31 visual_prompt]: 	Test 400/407. loss: 4.464, 0.1826 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 13:47:34 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1940, average loss: 4.8542
[09/16 13:47:34 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.76	top5: 50.83	
[09/16 13:47:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 13:47:45 visual_prompt]: Epoch 20 / 100: avg data time: 1.47e-01, avg batch time: 0.5491, average train loss: 7.2127
[09/16 13:47:49 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1427, average loss: 7.3874
[09/16 13:47:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/16 13:48:11 visual_prompt]: 	Test 100/407. loss: 7.010, 0.1825 s / batch. (data: 9.27e-05)max mem: 17.22449 GB 
[09/16 13:48:31 visual_prompt]: 	Test 200/407. loss: 7.358, 0.1826 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 13:48:50 visual_prompt]: 	Test 300/407. loss: 7.264, 0.2014 s / batch. (data: 1.26e-02)max mem: 17.22449 GB 
[09/16 13:49:10 visual_prompt]: 	Test 400/407. loss: 7.152, 0.1841 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 13:49:13 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1945, average loss: 7.3328
[09/16 13:49:13 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 59.35	
[09/16 13:49:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 13:49:24 visual_prompt]: Epoch 21 / 100: avg data time: 1.38e-01, avg batch time: 0.5396, average train loss: 7.0993
[09/16 13:49:28 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1440, average loss: 4.9910
[09/16 13:49:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/16 13:49:50 visual_prompt]: 	Test 100/407. loss: 5.143, 0.1959 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 13:50:09 visual_prompt]: 	Test 200/407. loss: 6.089, 0.1954 s / batch. (data: 1.30e-02)max mem: 17.22449 GB 
[09/16 13:50:29 visual_prompt]: 	Test 300/407. loss: 3.988, 0.1973 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 13:50:49 visual_prompt]: 	Test 400/407. loss: 4.826, 0.1833 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 13:50:52 visual_prompt]: Inference (test):avg data time: 9.07e-03, avg batch time: 0.1948, average loss: 5.1084
[09/16 13:50:52 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 54.82	
[09/16 13:50:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 13:51:02 visual_prompt]: Epoch 22 / 100: avg data time: 1.51e-01, avg batch time: 0.5569, average train loss: 5.9525
[09/16 13:51:07 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1425, average loss: 5.1106
[09/16 13:51:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 63.50	
[09/16 13:51:29 visual_prompt]: 	Test 100/407. loss: 5.258, 0.1971 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 13:51:49 visual_prompt]: 	Test 200/407. loss: 5.396, 0.2156 s / batch. (data: 3.33e-02)max mem: 17.22449 GB 
[09/16 13:52:08 visual_prompt]: 	Test 300/407. loss: 4.516, 0.2079 s / batch. (data: 2.55e-02)max mem: 17.22449 GB 
[09/16 13:52:27 visual_prompt]: 	Test 400/407. loss: 4.804, 0.1829 s / batch. (data: 4.55e-05)max mem: 17.22449 GB 
[09/16 13:52:31 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1941, average loss: 5.0532
[09/16 13:52:31 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.59	top5: 63.88	
[09/16 13:52:31 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 13:52:41 visual_prompt]: Epoch 23 / 100: avg data time: 1.41e-01, avg batch time: 0.5462, average train loss: 3.5123
[09/16 13:52:46 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1426, average loss: 2.9793
[09/16 13:52:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/16 13:53:07 visual_prompt]: 	Test 100/407. loss: 3.344, 0.1888 s / batch. (data: 1.02e-04)max mem: 17.22449 GB 
[09/16 13:53:27 visual_prompt]: 	Test 200/407. loss: 3.351, 0.2242 s / batch. (data: 7.01e-05)max mem: 17.22449 GB 
[09/16 13:53:47 visual_prompt]: 	Test 300/407. loss: 3.075, 0.1827 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 13:54:06 visual_prompt]: 	Test 400/407. loss: 2.844, 0.1840 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 13:54:09 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1944, average loss: 2.9749
[09/16 13:54:09 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.84	top5: 62.13	
[09/16 13:54:09 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 13:54:20 visual_prompt]: Epoch 24 / 100: avg data time: 1.43e-01, avg batch time: 0.5515, average train loss: 2.9973
[09/16 13:54:24 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1423, average loss: 3.1351
[09/16 13:54:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.00	top5: 57.50	
[09/16 13:54:46 visual_prompt]: 	Test 100/407. loss: 3.264, 0.2046 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 13:55:06 visual_prompt]: 	Test 200/407. loss: 3.308, 0.1930 s / batch. (data: 1.12e-02)max mem: 17.22449 GB 
[09/16 13:55:25 visual_prompt]: 	Test 300/407. loss: 2.996, 0.2009 s / batch. (data: 1.04e-04)max mem: 17.22449 GB 
[09/16 13:55:45 visual_prompt]: 	Test 400/407. loss: 3.105, 0.1824 s / batch. (data: 3.19e-05)max mem: 17.22449 GB 
[09/16 13:55:48 visual_prompt]: Inference (test):avg data time: 8.87e-03, avg batch time: 0.1947, average loss: 3.1069
[09/16 13:55:48 visual_prompt]: Classification results with test_vtab-svhn: top1: 18.41	top5: 59.19	
[09/16 13:55:48 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 13:55:59 visual_prompt]: Epoch 25 / 100: avg data time: 1.50e-01, avg batch time: 0.5587, average train loss: 2.7259
[09/16 13:56:03 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1428, average loss: 2.6170
[09/16 13:56:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/16 13:56:25 visual_prompt]: 	Test 100/407. loss: 2.920, 0.1967 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 13:56:45 visual_prompt]: 	Test 200/407. loss: 2.730, 0.1958 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 13:57:04 visual_prompt]: 	Test 300/407. loss: 2.847, 0.1954 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 13:57:24 visual_prompt]: 	Test 400/407. loss: 2.704, 0.1830 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 13:57:27 visual_prompt]: Inference (test):avg data time: 8.54e-03, avg batch time: 0.1940, average loss: 2.6678
[09/16 13:57:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 55.64	
[09/16 13:57:27 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 13:57:37 visual_prompt]: Epoch 26 / 100: avg data time: 1.47e-01, avg batch time: 0.5482, average train loss: 2.6956
[09/16 13:57:42 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1426, average loss: 3.3346
[09/16 13:57:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 60.50	
[09/16 13:58:04 visual_prompt]: 	Test 100/407. loss: 3.999, 0.1941 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 13:58:23 visual_prompt]: 	Test 200/407. loss: 3.796, 0.1937 s / batch. (data: 1.12e-02)max mem: 17.22449 GB 
[09/16 13:58:42 visual_prompt]: 	Test 300/407. loss: 3.605, 0.1966 s / batch. (data: 1.47e-02)max mem: 17.22449 GB 
[09/16 13:59:02 visual_prompt]: 	Test 400/407. loss: 3.652, 0.1829 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 13:59:05 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1931, average loss: 3.5088
[09/16 13:59:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.49	top5: 54.81	
[09/16 13:59:05 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 13:59:16 visual_prompt]: Epoch 27 / 100: avg data time: 1.36e-01, avg batch time: 0.5421, average train loss: 3.0306
[09/16 13:59:20 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1426, average loss: 2.8779
[09/16 13:59:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.00	top5: 47.00	
[09/16 13:59:42 visual_prompt]: 	Test 100/407. loss: 2.757, 0.2093 s / batch. (data: 2.73e-02)max mem: 17.22449 GB 
[09/16 14:00:01 visual_prompt]: 	Test 200/407. loss: 2.996, 0.1821 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 14:00:21 visual_prompt]: 	Test 300/407. loss: 2.606, 0.1825 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 14:00:40 visual_prompt]: 	Test 400/407. loss: 2.796, 0.1817 s / batch. (data: 3.43e-05)max mem: 17.22449 GB 
[09/16 14:00:43 visual_prompt]: Inference (test):avg data time: 7.11e-03, avg batch time: 0.1931, average loss: 2.8407
[09/16 14:00:43 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.29	top5: 50.14	
[09/16 14:00:43 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 14:00:54 visual_prompt]: Epoch 28 / 100: avg data time: 1.49e-01, avg batch time: 0.5513, average train loss: 2.6994
[09/16 14:00:58 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1426, average loss: 2.3514
[09/16 14:00:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/16 14:01:20 visual_prompt]: 	Test 100/407. loss: 2.437, 0.1993 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 14:01:39 visual_prompt]: 	Test 200/407. loss: 2.504, 0.1959 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 14:01:59 visual_prompt]: 	Test 300/407. loss: 2.258, 0.2023 s / batch. (data: 5.52e-03)max mem: 17.22449 GB 
[09/16 14:02:18 visual_prompt]: 	Test 400/407. loss: 2.360, 0.1829 s / batch. (data: 3.41e-05)max mem: 17.22449 GB 
[09/16 14:02:21 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1927, average loss: 2.3368
[09/16 14:02:21 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 61.44	
[09/16 14:02:21 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 14:02:32 visual_prompt]: Epoch 29 / 100: avg data time: 1.49e-01, avg batch time: 0.5514, average train loss: 2.6634
[09/16 14:02:36 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1426, average loss: 2.2896
[09/16 14:02:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.00	top5: 65.00	
[09/16 14:02:58 visual_prompt]: 	Test 100/407. loss: 2.404, 0.1954 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 14:03:18 visual_prompt]: 	Test 200/407. loss: 2.371, 0.1832 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 14:03:37 visual_prompt]: 	Test 300/407. loss: 2.305, 0.1970 s / batch. (data: 1.48e-02)max mem: 17.22449 GB 
[09/16 14:03:56 visual_prompt]: 	Test 400/407. loss: 2.241, 0.1821 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 14:04:00 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1938, average loss: 2.3015
[09/16 14:04:00 visual_prompt]: Classification results with test_vtab-svhn: top1: 14.54	top5: 64.31	
[09/16 14:04:00 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 14:04:10 visual_prompt]: Epoch 30 / 100: avg data time: 1.61e-01, avg batch time: 0.5621, average train loss: 2.4556
[09/16 14:04:15 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1424, average loss: 2.5789
[09/16 14:04:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 59.00	
[09/16 14:04:37 visual_prompt]: 	Test 100/407. loss: 2.485, 0.2088 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 14:04:56 visual_prompt]: 	Test 200/407. loss: 2.562, 0.1958 s / batch. (data: 1.32e-02)max mem: 17.22449 GB 
[09/16 14:05:16 visual_prompt]: 	Test 300/407. loss: 2.609, 0.2198 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 14:05:35 visual_prompt]: 	Test 400/407. loss: 2.579, 0.1824 s / batch. (data: 2.81e-05)max mem: 17.22449 GB 
[09/16 14:05:38 visual_prompt]: Inference (test):avg data time: 8.51e-03, avg batch time: 0.1938, average loss: 2.5571
[09/16 14:05:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 18.99	top5: 58.93	
[09/16 14:05:39 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 14:05:49 visual_prompt]: Epoch 31 / 100: avg data time: 1.58e-01, avg batch time: 0.5578, average train loss: 2.6266
[09/16 14:05:54 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1500, average loss: 2.5194
[09/16 14:05:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/16 14:06:15 visual_prompt]: 	Test 100/407. loss: 2.723, 0.1965 s / batch. (data: 1.47e-02)max mem: 17.22449 GB 
[09/16 14:06:35 visual_prompt]: 	Test 200/407. loss: 2.728, 0.1829 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 14:06:55 visual_prompt]: 	Test 300/407. loss: 2.605, 0.2060 s / batch. (data: 2.00e-02)max mem: 17.22449 GB 
[09/16 14:07:14 visual_prompt]: 	Test 400/407. loss: 2.540, 0.1837 s / batch. (data: 8.68e-05)max mem: 17.22449 GB 
[09/16 14:07:17 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1940, average loss: 2.5762
[09/16 14:07:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 59.23	
[09/16 14:07:17 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 14:07:28 visual_prompt]: Epoch 32 / 100: avg data time: 1.47e-01, avg batch time: 0.5479, average train loss: 2.5548
[09/16 14:07:32 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1433, average loss: 2.6458
[09/16 14:07:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.00	
[09/16 14:07:54 visual_prompt]: 	Test 100/407. loss: 2.481, 0.2070 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 14:08:13 visual_prompt]: 	Test 200/407. loss: 2.521, 0.1859 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 14:08:32 visual_prompt]: 	Test 300/407. loss: 2.608, 0.1976 s / batch. (data: 1.29e-02)max mem: 17.22449 GB 
[09/16 14:08:52 visual_prompt]: 	Test 400/407. loss: 2.462, 0.1828 s / batch. (data: 2.67e-05)max mem: 17.22449 GB 
[09/16 14:08:55 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1931, average loss: 2.6053
[09/16 14:08:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 8.07	top5: 47.85	
[09/16 14:08:55 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 14:09:06 visual_prompt]: Epoch 33 / 100: avg data time: 1.48e-01, avg batch time: 0.5510, average train loss: 2.7617
[09/16 14:09:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1451, average loss: 3.2834
[09/16 14:09:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/16 14:09:32 visual_prompt]: 	Test 100/407. loss: 3.718, 0.1996 s / batch. (data: 1.77e-02)max mem: 17.22449 GB 
[09/16 14:09:51 visual_prompt]: 	Test 200/407. loss: 3.878, 0.2225 s / batch. (data: 2.46e-02)max mem: 17.22449 GB 
[09/16 14:10:11 visual_prompt]: 	Test 300/407. loss: 3.212, 0.2099 s / batch. (data: 5.89e-05)max mem: 17.22449 GB 
[09/16 14:10:30 visual_prompt]: 	Test 400/407. loss: 3.561, 0.1827 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 14:10:33 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1933, average loss: 3.4093
[09/16 14:10:34 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 56.36	
[09/16 14:10:34 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 14:10:44 visual_prompt]: Epoch 34 / 100: avg data time: 1.54e-01, avg batch time: 0.5557, average train loss: 2.6030
[09/16 14:10:48 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1426, average loss: 2.4044
[09/16 14:10:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/16 14:11:10 visual_prompt]: 	Test 100/407. loss: 2.581, 0.1829 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 14:11:30 visual_prompt]: 	Test 200/407. loss: 2.660, 0.1827 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 14:11:49 visual_prompt]: 	Test 300/407. loss: 2.231, 0.1956 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 14:12:08 visual_prompt]: 	Test 400/407. loss: 2.509, 0.1820 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 14:12:12 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1931, average loss: 2.4246
[09/16 14:12:12 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 58.65	
[09/16 14:12:12 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 14:12:22 visual_prompt]: Epoch 35 / 100: avg data time: 1.45e-01, avg batch time: 0.5500, average train loss: 2.4077
[09/16 14:12:27 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1427, average loss: 2.4937
[09/16 14:12:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/16 14:12:49 visual_prompt]: 	Test 100/407. loss: 2.754, 0.2126 s / batch. (data: 3.04e-02)max mem: 17.22449 GB 
[09/16 14:13:08 visual_prompt]: 	Test 200/407. loss: 2.765, 0.1887 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 14:13:27 visual_prompt]: 	Test 300/407. loss: 2.509, 0.2118 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 14:13:47 visual_prompt]: 	Test 400/407. loss: 2.600, 0.1827 s / batch. (data: 3.48e-05)max mem: 17.22449 GB 
[09/16 14:13:50 visual_prompt]: Inference (test):avg data time: 8.97e-03, avg batch time: 0.1943, average loss: 2.5367
[09/16 14:13:50 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 59.66	
[09/16 14:13:50 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 14:14:01 visual_prompt]: Epoch 36 / 100: avg data time: 1.49e-01, avg batch time: 0.5516, average train loss: 2.4645
[09/16 14:14:05 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1431, average loss: 2.3332
[09/16 14:14:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/16 14:14:27 visual_prompt]: 	Test 100/407. loss: 2.456, 0.1822 s / batch. (data: 1.56e-04)max mem: 17.22449 GB 
[09/16 14:14:47 visual_prompt]: 	Test 200/407. loss: 2.454, 0.1961 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 14:15:06 visual_prompt]: 	Test 300/407. loss: 2.342, 0.1823 s / batch. (data: 9.63e-05)max mem: 17.22449 GB 
[09/16 14:15:25 visual_prompt]: 	Test 400/407. loss: 2.307, 0.1827 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 14:15:28 visual_prompt]: Inference (test):avg data time: 6.73e-03, avg batch time: 0.1925, average loss: 2.3557
[09/16 14:15:29 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.81	top5: 62.42	
[09/16 14:15:29 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 14:15:39 visual_prompt]: Epoch 37 / 100: avg data time: 1.57e-01, avg batch time: 0.5570, average train loss: 2.4835
[09/16 14:15:44 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1427, average loss: 2.4784
[09/16 14:15:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 66.50	
[09/16 14:16:05 visual_prompt]: 	Test 100/407. loss: 2.708, 0.2234 s / batch. (data: 4.15e-02)max mem: 17.22449 GB 
[09/16 14:16:25 visual_prompt]: 	Test 200/407. loss: 2.737, 0.2095 s / batch. (data: 2.76e-02)max mem: 17.22449 GB 
[09/16 14:16:44 visual_prompt]: 	Test 300/407. loss: 2.385, 0.1929 s / batch. (data: 1.12e-02)max mem: 17.22449 GB 
[09/16 14:17:04 visual_prompt]: 	Test 400/407. loss: 2.334, 0.1834 s / batch. (data: 3.79e-05)max mem: 17.22449 GB 
[09/16 14:17:07 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1928, average loss: 2.4989
[09/16 14:17:07 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 62.69	
[09/16 14:17:07 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 14:17:17 visual_prompt]: Epoch 38 / 100: avg data time: 1.52e-01, avg batch time: 0.5544, average train loss: 2.4427
[09/16 14:17:22 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1426, average loss: 2.4628
[09/16 14:17:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/16 14:17:44 visual_prompt]: 	Test 100/407. loss: 2.563, 0.1826 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 14:18:04 visual_prompt]: 	Test 200/407. loss: 2.776, 0.1829 s / batch. (data: 1.26e-04)max mem: 17.22449 GB 
[09/16 14:18:23 visual_prompt]: 	Test 300/407. loss: 2.305, 0.1959 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 14:18:43 visual_prompt]: 	Test 400/407. loss: 2.575, 0.1829 s / batch. (data: 2.79e-05)max mem: 17.22449 GB 
[09/16 14:18:46 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1947, average loss: 2.5072
[09/16 14:18:46 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 60.34	
[09/16 14:18:46 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 14:18:56 visual_prompt]: Epoch 39 / 100: avg data time: 1.48e-01, avg batch time: 0.5499, average train loss: 2.5356
[09/16 14:19:01 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1427, average loss: 2.5861
[09/16 14:19:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/16 14:19:23 visual_prompt]: 	Test 100/407. loss: 2.758, 0.2049 s / batch. (data: 7.30e-03)max mem: 17.22449 GB 
[09/16 14:19:42 visual_prompt]: 	Test 200/407. loss: 2.811, 0.2022 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 14:20:01 visual_prompt]: 	Test 300/407. loss: 2.644, 0.1831 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 14:20:21 visual_prompt]: 	Test 400/407. loss: 2.567, 0.1823 s / batch. (data: 2.17e-05)max mem: 17.22449 GB 
[09/16 14:20:24 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1926, average loss: 2.6345
[09/16 14:20:24 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.76	top5: 58.40	
[09/16 14:20:24 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 14:20:35 visual_prompt]: Epoch 40 / 100: avg data time: 1.53e-01, avg batch time: 0.5542, average train loss: 2.5558
[09/16 14:20:39 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1427, average loss: 2.2281
[09/16 14:20:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/16 14:21:01 visual_prompt]: 	Test 100/407. loss: 2.325, 0.1828 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 14:21:20 visual_prompt]: 	Test 200/407. loss: 2.396, 0.2199 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 14:21:40 visual_prompt]: 	Test 300/407. loss: 2.199, 0.2043 s / batch. (data: 1.27e-02)max mem: 17.22449 GB 
[09/16 14:21:59 visual_prompt]: 	Test 400/407. loss: 2.264, 0.1824 s / batch. (data: 3.72e-05)max mem: 17.22449 GB 
[09/16 14:22:03 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1933, average loss: 2.2725
[09/16 14:22:03 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.83	top5: 56.67	
[09/16 14:22:03 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 14:22:13 visual_prompt]: Epoch 41 / 100: avg data time: 1.45e-01, avg batch time: 0.5474, average train loss: 2.3530
[09/16 14:22:18 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1427, average loss: 2.1718
[09/16 14:22:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 71.50	
[09/16 14:22:39 visual_prompt]: 	Test 100/407. loss: 2.176, 0.1901 s / batch. (data: 1.51e-04)max mem: 17.22449 GB 
[09/16 14:22:59 visual_prompt]: 	Test 200/407. loss: 2.239, 0.1957 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 14:23:19 visual_prompt]: 	Test 300/407. loss: 2.160, 0.2316 s / batch. (data: 1.66e-02)max mem: 17.22449 GB 
[09/16 14:23:38 visual_prompt]: 	Test 400/407. loss: 2.185, 0.1830 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 14:23:42 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1954, average loss: 2.2006
[09/16 14:23:42 visual_prompt]: Classification results with test_vtab-svhn: top1: 27.08	top5: 70.02	
[09/16 14:23:42 visual_prompt]: Best epoch 41: best metric: 0.290
[09/16 14:23:42 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 14:23:52 visual_prompt]: Epoch 42 / 100: avg data time: 1.51e-01, avg batch time: 0.5528, average train loss: 2.7493
[09/16 14:23:57 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1435, average loss: 2.7069
[09/16 14:23:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.50	
[09/16 14:24:18 visual_prompt]: 	Test 100/407. loss: 2.952, 0.1959 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 14:24:38 visual_prompt]: 	Test 200/407. loss: 2.748, 0.2022 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 14:24:57 visual_prompt]: 	Test 300/407. loss: 2.811, 0.1982 s / batch. (data: 1.57e-04)max mem: 17.22449 GB 
[09/16 14:25:17 visual_prompt]: 	Test 400/407. loss: 2.730, 0.1830 s / batch. (data: 2.96e-05)max mem: 17.22449 GB 
[09/16 14:25:20 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1937, average loss: 2.7402
[09/16 14:25:20 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 42.85	
[09/16 14:25:20 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 14:25:31 visual_prompt]: Epoch 43 / 100: avg data time: 1.48e-01, avg batch time: 0.5512, average train loss: 2.6489
[09/16 14:25:35 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1423, average loss: 2.4982
[09/16 14:25:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/16 14:25:57 visual_prompt]: 	Test 100/407. loss: 2.824, 0.1981 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 14:26:16 visual_prompt]: 	Test 200/407. loss: 2.845, 0.1976 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 14:26:36 visual_prompt]: 	Test 300/407. loss: 2.436, 0.1958 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 14:26:55 visual_prompt]: 	Test 400/407. loss: 2.596, 0.1824 s / batch. (data: 3.74e-05)max mem: 17.22449 GB 
[09/16 14:26:58 visual_prompt]: Inference (test):avg data time: 7.01e-03, avg batch time: 0.1930, average loss: 2.5972
[09/16 14:26:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 52.77	
[09/16 14:26:58 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 14:27:09 visual_prompt]: Epoch 44 / 100: avg data time: 1.59e-01, avg batch time: 0.5677, average train loss: 2.4671
[09/16 14:27:14 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1427, average loss: 2.3917
[09/16 14:27:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 44.50	
[09/16 14:27:35 visual_prompt]: 	Test 100/407. loss: 2.413, 0.1971 s / batch. (data: 1.52e-04)max mem: 17.22449 GB 
[09/16 14:27:55 visual_prompt]: 	Test 200/407. loss: 2.406, 0.1967 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 14:28:14 visual_prompt]: 	Test 300/407. loss: 2.417, 0.1948 s / batch. (data: 1.29e-02)max mem: 17.22449 GB 
[09/16 14:28:33 visual_prompt]: 	Test 400/407. loss: 2.344, 0.1824 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 14:28:37 visual_prompt]: Inference (test):avg data time: 7.03e-03, avg batch time: 0.1930, average loss: 2.3816
[09/16 14:28:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.52	top5: 47.59	
[09/16 14:28:37 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 14:28:47 visual_prompt]: Epoch 45 / 100: avg data time: 1.35e-01, avg batch time: 0.5404, average train loss: 2.4077
[09/16 14:28:52 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1425, average loss: 2.3759
[09/16 14:28:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 65.00	
[09/16 14:29:13 visual_prompt]: 	Test 100/407. loss: 2.550, 0.1956 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 14:29:33 visual_prompt]: 	Test 200/407. loss: 2.622, 0.1950 s / batch. (data: 1.28e-02)max mem: 17.22449 GB 
[09/16 14:29:52 visual_prompt]: 	Test 300/407. loss: 2.223, 0.1851 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 14:30:12 visual_prompt]: 	Test 400/407. loss: 2.332, 0.1823 s / batch. (data: 5.39e-05)max mem: 17.22449 GB 
[09/16 14:30:16 visual_prompt]: Inference (test):avg data time: 8.47e-03, avg batch time: 0.1953, average loss: 2.3905
[09/16 14:30:16 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 63.59	
[09/16 14:30:16 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 14:30:26 visual_prompt]: Epoch 46 / 100: avg data time: 1.59e-01, avg batch time: 0.5591, average train loss: 2.4284
[09/16 14:30:31 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1425, average loss: 2.3745
[09/16 14:30:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 62.50	
[09/16 14:30:53 visual_prompt]: 	Test 100/407. loss: 2.502, 0.1828 s / batch. (data: 1.11e-04)max mem: 17.22449 GB 
[09/16 14:31:13 visual_prompt]: 	Test 200/407. loss: 2.592, 0.1826 s / batch. (data: 1.26e-04)max mem: 17.22449 GB 
[09/16 14:31:32 visual_prompt]: 	Test 300/407. loss: 2.315, 0.1826 s / batch. (data: 3.41e-05)max mem: 17.22449 GB 
[09/16 14:31:51 visual_prompt]: 	Test 400/407. loss: 2.392, 0.1827 s / batch. (data: 3.93e-05)max mem: 17.22449 GB 
[09/16 14:31:55 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1944, average loss: 2.4434
[09/16 14:31:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.57	top5: 56.91	
[09/16 14:31:55 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 14:32:06 visual_prompt]: Epoch 47 / 100: avg data time: 1.51e-01, avg batch time: 0.5530, average train loss: 2.4568
[09/16 14:32:10 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1423, average loss: 2.4010
[09/16 14:32:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.00	top5: 64.00	
[09/16 14:32:32 visual_prompt]: 	Test 100/407. loss: 2.755, 0.1828 s / batch. (data: 5.41e-05)max mem: 17.22449 GB 
[09/16 14:32:51 visual_prompt]: 	Test 200/407. loss: 2.919, 0.1833 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 14:33:10 visual_prompt]: 	Test 300/407. loss: 2.036, 0.2851 s / batch. (data: 1.45e-02)max mem: 17.22449 GB 
[09/16 14:33:30 visual_prompt]: 	Test 400/407. loss: 2.604, 0.1829 s / batch. (data: 3.70e-05)max mem: 17.22449 GB 
[09/16 14:33:33 visual_prompt]: Inference (test):avg data time: 7.12e-03, avg batch time: 0.1935, average loss: 2.4208
[09/16 14:33:33 visual_prompt]: Classification results with test_vtab-svhn: top1: 32.16	top5: 61.59	
[09/16 14:33:33 visual_prompt]: Best epoch 47: best metric: 0.340
[09/16 14:33:33 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 14:33:44 visual_prompt]: Epoch 48 / 100: avg data time: 1.48e-01, avg batch time: 0.5509, average train loss: 2.2452
[09/16 14:33:48 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1423, average loss: 2.3713
[09/16 14:33:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.50	top5: 56.50	
[09/16 14:34:10 visual_prompt]: 	Test 100/407. loss: 2.668, 0.1969 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 14:34:30 visual_prompt]: 	Test 200/407. loss: 2.649, 0.1915 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 14:34:49 visual_prompt]: 	Test 300/407. loss: 2.486, 0.2044 s / batch. (data: 1.56e-02)max mem: 17.22449 GB 
[09/16 14:35:08 visual_prompt]: 	Test 400/407. loss: 2.625, 0.1828 s / batch. (data: 2.38e-05)max mem: 17.22449 GB 
[09/16 14:35:12 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1935, average loss: 2.4562
[09/16 14:35:12 visual_prompt]: Classification results with test_vtab-svhn: top1: 25.89	top5: 53.48	
[09/16 14:35:12 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 14:35:22 visual_prompt]: Epoch 49 / 100: avg data time: 1.45e-01, avg batch time: 0.5488, average train loss: 2.6073
[09/16 14:35:27 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1425, average loss: 2.6201
[09/16 14:35:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/16 14:35:49 visual_prompt]: 	Test 100/407. loss: 3.025, 0.2037 s / batch. (data: 1.02e-02)max mem: 17.22449 GB 
[09/16 14:36:09 visual_prompt]: 	Test 200/407. loss: 2.884, 0.1825 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 14:36:28 visual_prompt]: 	Test 300/407. loss: 2.710, 0.1835 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 14:36:48 visual_prompt]: 	Test 400/407. loss: 2.756, 0.1828 s / batch. (data: 5.10e-05)max mem: 17.22449 GB 
[09/16 14:36:51 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1952, average loss: 2.6934
[09/16 14:36:51 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 52.85	
[09/16 14:36:51 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 14:37:01 visual_prompt]: Epoch 50 / 100: avg data time: 1.48e-01, avg batch time: 0.5495, average train loss: 2.7053
[09/16 14:37:06 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1422, average loss: 2.7529
[09/16 14:37:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.50	
[09/16 14:37:28 visual_prompt]: 	Test 100/407. loss: 2.951, 0.1973 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 14:37:48 visual_prompt]: 	Test 200/407. loss: 2.902, 0.2239 s / batch. (data: 1.45e-02)max mem: 17.22449 GB 
[09/16 14:38:07 visual_prompt]: 	Test 300/407. loss: 2.713, 0.2299 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 14:38:27 visual_prompt]: 	Test 400/407. loss: 2.680, 0.1832 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 14:38:30 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1951, average loss: 2.7787
[09/16 14:38:30 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 41.72	
[09/16 14:38:30 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 14:38:41 visual_prompt]: Epoch 51 / 100: avg data time: 1.56e-01, avg batch time: 0.5575, average train loss: 2.5426
[09/16 14:38:45 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1421, average loss: 2.3602
[09/16 14:38:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/16 14:39:07 visual_prompt]: 	Test 100/407. loss: 2.381, 0.1903 s / batch. (data: 8.51e-03)max mem: 17.22449 GB 
[09/16 14:39:27 visual_prompt]: 	Test 200/407. loss: 2.500, 0.1955 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 14:39:46 visual_prompt]: 	Test 300/407. loss: 2.322, 0.1977 s / batch. (data: 1.58e-02)max mem: 17.22449 GB 
[09/16 14:40:06 visual_prompt]: 	Test 400/407. loss: 2.423, 0.1819 s / batch. (data: 2.72e-05)max mem: 17.22449 GB 
[09/16 14:40:09 visual_prompt]: Inference (test):avg data time: 8.90e-03, avg batch time: 0.1955, average loss: 2.3642
[09/16 14:40:09 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 60.75	
[09/16 14:40:09 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 14:40:20 visual_prompt]: Epoch 52 / 100: avg data time: 1.44e-01, avg batch time: 0.5636, average train loss: 2.5351
[09/16 14:40:25 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1425, average loss: 2.4493
[09/16 14:40:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.50	top5: 62.50	
[09/16 14:40:46 visual_prompt]: 	Test 100/407. loss: 3.160, 0.1977 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 14:41:06 visual_prompt]: 	Test 200/407. loss: 3.026, 0.1825 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 14:41:25 visual_prompt]: 	Test 300/407. loss: 2.451, 0.1847 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 14:41:45 visual_prompt]: 	Test 400/407. loss: 2.589, 0.1833 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 14:41:48 visual_prompt]: Inference (test):avg data time: 8.41e-03, avg batch time: 0.1939, average loss: 2.5701
[09/16 14:41:48 visual_prompt]: Classification results with test_vtab-svhn: top1: 25.49	top5: 61.05	
[09/16 14:41:48 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 14:41:59 visual_prompt]: Epoch 53 / 100: avg data time: 1.53e-01, avg batch time: 0.5534, average train loss: 2.2405
[09/16 14:42:03 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1425, average loss: 2.0312
[09/16 14:42:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.50	top5: 74.00	
[09/16 14:42:25 visual_prompt]: 	Test 100/407. loss: 2.405, 0.1825 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 14:42:44 visual_prompt]: 	Test 200/407. loss: 2.223, 0.1824 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 14:43:04 visual_prompt]: 	Test 300/407. loss: 2.144, 0.1857 s / batch. (data: 1.09e-04)max mem: 17.22449 GB 
[09/16 14:43:23 visual_prompt]: 	Test 400/407. loss: 2.140, 0.1833 s / batch. (data: 3.55e-05)max mem: 17.22449 GB 
[09/16 14:43:27 visual_prompt]: Inference (test):avg data time: 8.26e-03, avg batch time: 0.1938, average loss: 2.1356
[09/16 14:43:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 23.42	top5: 69.61	
[09/16 14:43:27 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 14:43:38 visual_prompt]: Epoch 54 / 100: avg data time: 1.45e-01, avg batch time: 0.5911, average train loss: 2.3022
[09/16 14:43:42 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1425, average loss: 2.2122
[09/16 14:43:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 68.50	
[09/16 14:44:04 visual_prompt]: 	Test 100/407. loss: 2.294, 0.1823 s / batch. (data: 1.50e-04)max mem: 17.22449 GB 
[09/16 14:44:23 visual_prompt]: 	Test 200/407. loss: 2.305, 0.1887 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 14:44:43 visual_prompt]: 	Test 300/407. loss: 2.230, 0.1924 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 14:45:02 visual_prompt]: 	Test 400/407. loss: 2.333, 0.1828 s / batch. (data: 2.88e-05)max mem: 17.22449 GB 
[09/16 14:45:05 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1926, average loss: 2.2405
[09/16 14:45:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.33	top5: 63.57	
[09/16 14:45:05 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 14:45:16 visual_prompt]: Epoch 55 / 100: avg data time: 1.53e-01, avg batch time: 0.5591, average train loss: 2.2351
[09/16 14:45:20 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1425, average loss: 2.0460
[09/16 14:45:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 67.00	
[09/16 14:45:42 visual_prompt]: 	Test 100/407. loss: 2.456, 0.1959 s / batch. (data: 1.54e-04)max mem: 17.22449 GB 
[09/16 14:46:01 visual_prompt]: 	Test 200/407. loss: 2.295, 0.1828 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 14:46:21 visual_prompt]: 	Test 300/407. loss: 2.228, 0.1917 s / batch. (data: 1.51e-04)max mem: 17.22449 GB 
[09/16 14:46:40 visual_prompt]: 	Test 400/407. loss: 2.277, 0.1828 s / batch. (data: 3.70e-05)max mem: 17.22449 GB 
[09/16 14:46:43 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1926, average loss: 2.1488
[09/16 14:46:43 visual_prompt]: Classification results with test_vtab-svhn: top1: 20.92	top5: 62.76	
[09/16 14:46:43 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 14:46:54 visual_prompt]: Epoch 56 / 100: avg data time: 1.53e-01, avg batch time: 0.5558, average train loss: 2.0681
[09/16 14:46:58 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1426, average loss: 2.2494
[09/16 14:46:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.50	top5: 62.50	
[09/16 14:47:20 visual_prompt]: 	Test 100/407. loss: 2.730, 0.1829 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 14:47:40 visual_prompt]: 	Test 200/407. loss: 2.704, 0.1828 s / batch. (data: 1.43e-04)max mem: 17.22449 GB 
[09/16 14:47:59 visual_prompt]: 	Test 300/407. loss: 2.358, 0.1974 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 14:48:19 visual_prompt]: 	Test 400/407. loss: 2.566, 0.1830 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 14:48:22 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1942, average loss: 2.4163
[09/16 14:48:22 visual_prompt]: Classification results with test_vtab-svhn: top1: 27.18	top5: 59.65	
[09/16 14:48:22 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 14:48:33 visual_prompt]: Epoch 57 / 100: avg data time: 1.46e-01, avg batch time: 0.5464, average train loss: 1.8923
[09/16 14:48:37 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1425, average loss: 1.7355
[09/16 14:48:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 33.00	top5: 86.00	
[09/16 14:48:59 visual_prompt]: 	Test 100/407. loss: 2.049, 0.1829 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 14:49:19 visual_prompt]: 	Test 200/407. loss: 1.937, 0.1972 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 14:49:38 visual_prompt]: 	Test 300/407. loss: 1.915, 0.1829 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 14:49:58 visual_prompt]: 	Test 400/407. loss: 1.922, 0.1830 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 14:50:01 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1937, average loss: 1.9124
[09/16 14:50:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 27.70	top5: 82.18	
[09/16 14:50:01 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 14:50:12 visual_prompt]: Epoch 58 / 100: avg data time: 1.51e-01, avg batch time: 0.5533, average train loss: 1.6717
[09/16 14:50:16 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1424, average loss: 1.4696
[09/16 14:50:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 50.50	top5: 85.50	
[09/16 14:50:38 visual_prompt]: 	Test 100/407. loss: 1.895, 0.1968 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 14:50:57 visual_prompt]: 	Test 200/407. loss: 1.691, 0.1822 s / batch. (data: 1.03e-04)max mem: 17.22449 GB 
[09/16 14:51:17 visual_prompt]: 	Test 300/407. loss: 1.759, 0.1833 s / batch. (data: 1.01e-04)max mem: 17.22449 GB 
[09/16 14:51:36 visual_prompt]: 	Test 400/407. loss: 1.792, 0.1830 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 14:51:40 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1945, average loss: 1.7104
[09/16 14:51:40 visual_prompt]: Classification results with test_vtab-svhn: top1: 40.12	top5: 81.71	
[09/16 14:51:40 visual_prompt]: Best epoch 58: best metric: 0.505
[09/16 14:51:40 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 14:51:50 visual_prompt]: Epoch 59 / 100: avg data time: 1.54e-01, avg batch time: 0.5716, average train loss: 1.5313
[09/16 14:51:55 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1426, average loss: 1.5806
[09/16 14:51:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.00	top5: 87.00	
[09/16 14:52:17 visual_prompt]: 	Test 100/407. loss: 1.758, 0.1831 s / batch. (data: 1.60e-04)max mem: 17.22449 GB 
[09/16 14:52:36 visual_prompt]: 	Test 200/407. loss: 1.557, 0.1824 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 14:52:56 visual_prompt]: 	Test 300/407. loss: 1.665, 0.1837 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 14:53:16 visual_prompt]: 	Test 400/407. loss: 1.586, 0.1828 s / batch. (data: 4.05e-05)max mem: 17.22449 GB 
[09/16 14:53:19 visual_prompt]: Inference (test):avg data time: 8.48e-03, avg batch time: 0.1948, average loss: 1.7686
[09/16 14:53:19 visual_prompt]: Classification results with test_vtab-svhn: top1: 34.47	top5: 82.61	
[09/16 14:53:19 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 14:53:29 visual_prompt]: Epoch 60 / 100: avg data time: 1.46e-01, avg batch time: 0.5487, average train loss: 1.4242
[09/16 14:53:34 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1427, average loss: 1.5908
[09/16 14:53:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 33.50	top5: 90.50	
[09/16 14:53:56 visual_prompt]: 	Test 100/407. loss: 1.906, 0.2096 s / batch. (data: 2.78e-02)max mem: 17.22449 GB 
[09/16 14:54:15 visual_prompt]: 	Test 200/407. loss: 1.731, 0.1919 s / batch. (data: 1.52e-04)max mem: 17.22449 GB 
[09/16 14:54:35 visual_prompt]: 	Test 300/407. loss: 1.648, 0.1827 s / batch. (data: 1.00e-04)max mem: 17.22449 GB 
[09/16 14:54:54 visual_prompt]: 	Test 400/407. loss: 1.783, 0.1828 s / batch. (data: 3.48e-05)max mem: 17.22449 GB 
[09/16 14:54:58 visual_prompt]: Inference (test):avg data time: 8.41e-03, avg batch time: 0.1947, average loss: 1.8740
[09/16 14:54:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 31.54	top5: 83.31	
[09/16 14:54:58 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 14:55:08 visual_prompt]: Epoch 61 / 100: avg data time: 1.49e-01, avg batch time: 0.5503, average train loss: 1.4786
[09/16 14:55:13 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1424, average loss: 1.3657
[09/16 14:55:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.50	top5: 88.50	
[09/16 14:55:35 visual_prompt]: 	Test 100/407. loss: 1.609, 0.1943 s / batch. (data: 1.28e-02)max mem: 17.22449 GB 
[09/16 14:55:54 visual_prompt]: 	Test 200/407. loss: 1.728, 0.1868 s / batch. (data: 9.20e-05)max mem: 17.22449 GB 
[09/16 14:56:13 visual_prompt]: 	Test 300/407. loss: 1.458, 0.1825 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 14:56:33 visual_prompt]: 	Test 400/407. loss: 1.810, 0.1946 s / batch. (data: 2.12e-05)max mem: 17.22449 GB 
[09/16 14:56:36 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1943, average loss: 1.6385
[09/16 14:56:36 visual_prompt]: Classification results with test_vtab-svhn: top1: 48.04	top5: 84.91	
[09/16 14:56:36 visual_prompt]: Best epoch 61: best metric: 0.565
[09/16 14:56:36 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 14:56:47 visual_prompt]: Epoch 62 / 100: avg data time: 1.49e-01, avg batch time: 0.5482, average train loss: 1.4014
[09/16 14:56:51 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1426, average loss: 1.3584
[09/16 14:56:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 49.00	top5: 93.00	
[09/16 14:57:13 visual_prompt]: 	Test 100/407. loss: 1.733, 0.1948 s / batch. (data: 1.27e-02)max mem: 17.22449 GB 
[09/16 14:57:33 visual_prompt]: 	Test 200/407. loss: 1.963, 0.1971 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 14:57:52 visual_prompt]: 	Test 300/407. loss: 1.277, 0.1853 s / batch. (data: 1.11e-04)max mem: 17.22449 GB 
[09/16 14:58:11 visual_prompt]: 	Test 400/407. loss: 1.788, 0.1829 s / batch. (data: 3.91e-05)max mem: 17.22449 GB 
[09/16 14:58:15 visual_prompt]: Inference (test):avg data time: 6.91e-03, avg batch time: 0.1929, average loss: 1.7025
[09/16 14:58:15 visual_prompt]: Classification results with test_vtab-svhn: top1: 40.80	top5: 90.39	
[09/16 14:58:15 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 14:58:25 visual_prompt]: Epoch 63 / 100: avg data time: 1.47e-01, avg batch time: 0.5500, average train loss: 1.1784
[09/16 14:58:30 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1425, average loss: 0.9110
[09/16 14:58:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.00	top5: 99.50	
[09/16 14:58:51 visual_prompt]: 	Test 100/407. loss: 1.513, 0.1966 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 14:59:11 visual_prompt]: 	Test 200/407. loss: 1.439, 0.1960 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 14:59:30 visual_prompt]: 	Test 300/407. loss: 1.161, 0.1839 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 14:59:50 visual_prompt]: 	Test 400/407. loss: 1.236, 0.1823 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 14:59:53 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1931, average loss: 1.3163
[09/16 14:59:53 visual_prompt]: Classification results with test_vtab-svhn: top1: 55.14	top5: 93.72	
[09/16 14:59:53 visual_prompt]: Best epoch 63: best metric: 0.660
[09/16 14:59:53 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 15:00:03 visual_prompt]: Epoch 64 / 100: avg data time: 1.60e-01, avg batch time: 0.5611, average train loss: 0.9634
[09/16 15:00:08 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1425, average loss: 0.7988
[09/16 15:00:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 98.50	
[09/16 15:00:29 visual_prompt]: 	Test 100/407. loss: 1.400, 0.1826 s / batch. (data: 1.55e-04)max mem: 17.22449 GB 
[09/16 15:00:49 visual_prompt]: 	Test 200/407. loss: 1.322, 0.1984 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 15:01:08 visual_prompt]: 	Test 300/407. loss: 1.084, 0.1899 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 15:01:28 visual_prompt]: 	Test 400/407. loss: 1.165, 0.1828 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 15:01:31 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1936, average loss: 1.2411
[09/16 15:01:31 visual_prompt]: Classification results with test_vtab-svhn: top1: 59.70	top5: 94.95	
[09/16 15:01:31 visual_prompt]: Best epoch 64: best metric: 0.695
[09/16 15:01:31 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 15:01:42 visual_prompt]: Epoch 65 / 100: avg data time: 1.52e-01, avg batch time: 0.5586, average train loss: 0.8518
[09/16 15:01:46 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1426, average loss: 0.8534
[09/16 15:01:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 98.50	
[09/16 15:02:08 visual_prompt]: 	Test 100/407. loss: 1.626, 0.1879 s / batch. (data: 1.09e-04)max mem: 17.22449 GB 
[09/16 15:02:27 visual_prompt]: 	Test 200/407. loss: 1.556, 0.1972 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 15:02:47 visual_prompt]: 	Test 300/407. loss: 1.154, 0.1826 s / batch. (data: 1.14e-04)max mem: 17.22449 GB 
[09/16 15:03:07 visual_prompt]: 	Test 400/407. loss: 1.365, 0.1880 s / batch. (data: 3.89e-05)max mem: 17.22449 GB 
[09/16 15:03:10 visual_prompt]: Inference (test):avg data time: 6.94e-03, avg batch time: 0.1943, average loss: 1.3992
[09/16 15:03:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 59.90	top5: 93.15	
[09/16 15:03:10 visual_prompt]: Best epoch 65: best metric: 0.700
[09/16 15:03:10 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 15:03:20 visual_prompt]: Epoch 66 / 100: avg data time: 1.52e-01, avg batch time: 0.5514, average train loss: 0.7777
[09/16 15:03:25 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1425, average loss: 0.5261
[09/16 15:03:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 100.00	
[09/16 15:03:47 visual_prompt]: 	Test 100/407. loss: 1.147, 0.2176 s / batch. (data: 3.70e-05)max mem: 17.22449 GB 
[09/16 15:04:06 visual_prompt]: 	Test 200/407. loss: 1.158, 0.1959 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 15:04:26 visual_prompt]: 	Test 300/407. loss: 1.007, 0.2048 s / batch. (data: 3.53e-05)max mem: 17.22449 GB 
[09/16 15:04:45 visual_prompt]: 	Test 400/407. loss: 1.208, 0.1828 s / batch. (data: 3.60e-05)max mem: 17.22449 GB 
[09/16 15:04:48 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1942, average loss: 1.1029
[09/16 15:04:48 visual_prompt]: Classification results with test_vtab-svhn: top1: 65.41	top5: 94.96	
[09/16 15:04:48 visual_prompt]: Best epoch 66: best metric: 0.795
[09/16 15:04:48 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 15:04:59 visual_prompt]: Epoch 67 / 100: avg data time: 1.39e-01, avg batch time: 0.5406, average train loss: 0.8372
[09/16 15:05:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1425, average loss: 0.7713
[09/16 15:05:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 98.00	
[09/16 15:05:25 visual_prompt]: 	Test 100/407. loss: 1.384, 0.1959 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 15:05:44 visual_prompt]: 	Test 200/407. loss: 1.238, 0.1992 s / batch. (data: 1.47e-04)max mem: 17.22449 GB 
[09/16 15:06:04 visual_prompt]: 	Test 300/407. loss: 1.011, 0.2077 s / batch. (data: 2.56e-02)max mem: 17.22449 GB 
[09/16 15:06:23 visual_prompt]: 	Test 400/407. loss: 1.232, 0.1840 s / batch. (data: 4.43e-05)max mem: 17.22449 GB 
[09/16 15:06:26 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1934, average loss: 1.2291
[09/16 15:06:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 59.68	top5: 93.67	
[09/16 15:06:27 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 15:06:37 visual_prompt]: Epoch 68 / 100: avg data time: 1.51e-01, avg batch time: 0.5497, average train loss: 0.8229
[09/16 15:06:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1426, average loss: 0.5990
[09/16 15:06:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.50	
[09/16 15:07:03 visual_prompt]: 	Test 100/407. loss: 1.221, 0.1999 s / batch. (data: 1.81e-02)max mem: 17.22449 GB 
[09/16 15:07:23 visual_prompt]: 	Test 200/407. loss: 1.036, 0.1825 s / batch. (data: 1.40e-04)max mem: 17.22449 GB 
[09/16 15:07:42 visual_prompt]: 	Test 300/407. loss: 1.222, 0.1835 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 15:08:01 visual_prompt]: 	Test 400/407. loss: 1.326, 0.1823 s / batch. (data: 3.93e-05)max mem: 17.22449 GB 
[09/16 15:08:05 visual_prompt]: Inference (test):avg data time: 6.59e-03, avg batch time: 0.1925, average loss: 1.1883
[09/16 15:08:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 64.31	top5: 95.06	
[09/16 15:08:05 visual_prompt]: Best epoch 68: best metric: 0.810
[09/16 15:08:05 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 15:08:15 visual_prompt]: Epoch 69 / 100: avg data time: 1.55e-01, avg batch time: 0.5583, average train loss: 0.7823
[09/16 15:08:20 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1426, average loss: 0.6362
[09/16 15:08:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 100.00	
[09/16 15:08:42 visual_prompt]: 	Test 100/407. loss: 1.259, 0.2069 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 15:09:01 visual_prompt]: 	Test 200/407. loss: 1.291, 0.1908 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 15:09:21 visual_prompt]: 	Test 300/407. loss: 1.117, 0.2077 s / batch. (data: 1.11e-02)max mem: 17.22449 GB 
[09/16 15:09:40 visual_prompt]: 	Test 400/407. loss: 1.039, 0.1834 s / batch. (data: 2.81e-05)max mem: 17.22449 GB 
[09/16 15:09:44 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1939, average loss: 1.2692
[09/16 15:09:44 visual_prompt]: Classification results with test_vtab-svhn: top1: 62.47	top5: 95.13	
[09/16 15:09:44 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 15:09:54 visual_prompt]: Epoch 70 / 100: avg data time: 1.42e-01, avg batch time: 0.5457, average train loss: 0.5008
[09/16 15:09:59 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1425, average loss: 0.3698
[09/16 15:09:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 100.00	
[09/16 15:10:21 visual_prompt]: 	Test 100/407. loss: 1.224, 0.1937 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 15:10:41 visual_prompt]: 	Test 200/407. loss: 1.150, 0.1866 s / batch. (data: 1.11e-04)max mem: 17.22449 GB 
[09/16 15:11:00 visual_prompt]: 	Test 300/407. loss: 0.803, 0.1930 s / batch. (data: 1.12e-02)max mem: 17.22449 GB 
[09/16 15:11:20 visual_prompt]: 	Test 400/407. loss: 0.874, 0.1827 s / batch. (data: 4.10e-05)max mem: 17.22449 GB 
[09/16 15:11:23 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1948, average loss: 1.0053
[09/16 15:11:23 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.26	top5: 96.40	
[09/16 15:11:23 visual_prompt]: Best epoch 70: best metric: 0.865
[09/16 15:11:23 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 15:11:33 visual_prompt]: Epoch 71 / 100: avg data time: 1.55e-01, avg batch time: 0.5573, average train loss: 0.3881
[09/16 15:11:38 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1427, average loss: 0.2234
[09/16 15:11:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 91.50	top5: 100.00	
[09/16 15:12:00 visual_prompt]: 	Test 100/407. loss: 1.224, 0.1865 s / batch. (data: 5.23e-03)max mem: 17.22449 GB 
[09/16 15:12:19 visual_prompt]: 	Test 200/407. loss: 1.096, 0.1859 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 15:12:39 visual_prompt]: 	Test 300/407. loss: 0.903, 0.1824 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 15:12:58 visual_prompt]: 	Test 400/407. loss: 0.828, 0.1830 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 15:13:01 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1927, average loss: 1.0446
[09/16 15:13:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.78	top5: 96.37	
[09/16 15:13:01 visual_prompt]: Best epoch 71: best metric: 0.915
[09/16 15:13:01 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 15:13:11 visual_prompt]: Epoch 72 / 100: avg data time: 1.46e-01, avg batch time: 0.5488, average train loss: 0.3955
[09/16 15:13:16 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1424, average loss: 0.2774
[09/16 15:13:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 90.50	top5: 99.00	
[09/16 15:13:38 visual_prompt]: 	Test 100/407. loss: 1.179, 0.1958 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 15:13:57 visual_prompt]: 	Test 200/407. loss: 1.092, 0.1825 s / batch. (data: 1.47e-04)max mem: 17.22449 GB 
[09/16 15:14:16 visual_prompt]: 	Test 300/407. loss: 0.944, 0.1971 s / batch. (data: 1.50e-02)max mem: 17.22449 GB 
[09/16 15:14:36 visual_prompt]: 	Test 400/407. loss: 1.183, 0.1825 s / batch. (data: 2.81e-05)max mem: 17.22449 GB 
[09/16 15:14:39 visual_prompt]: Inference (test):avg data time: 6.99e-03, avg batch time: 0.1929, average loss: 1.0493
[09/16 15:14:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 73.11	top5: 94.82	
[09/16 15:14:39 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 15:14:50 visual_prompt]: Epoch 73 / 100: avg data time: 1.48e-01, avg batch time: 0.5735, average train loss: 0.3023
[09/16 15:14:54 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1425, average loss: 0.2669
[09/16 15:14:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 89.50	top5: 100.00	
[09/16 15:15:17 visual_prompt]: 	Test 100/407. loss: 1.274, 0.2088 s / batch. (data: 2.64e-02)max mem: 17.22449 GB 
[09/16 15:15:36 visual_prompt]: 	Test 200/407. loss: 1.705, 0.1833 s / batch. (data: 1.38e-04)max mem: 17.22449 GB 
[09/16 15:15:56 visual_prompt]: 	Test 300/407. loss: 0.932, 0.1830 s / batch. (data: 1.00e-04)max mem: 17.22449 GB 
[09/16 15:16:15 visual_prompt]: 	Test 400/407. loss: 1.412, 0.1823 s / batch. (data: 3.98e-05)max mem: 17.22449 GB 
[09/16 15:16:18 visual_prompt]: Inference (test):avg data time: 8.23e-03, avg batch time: 0.1950, average loss: 1.1961
[09/16 15:16:18 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.07	top5: 96.11	
[09/16 15:16:18 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 15:16:29 visual_prompt]: Epoch 74 / 100: avg data time: 1.61e-01, avg batch time: 0.5613, average train loss: 0.2409
[09/16 15:16:34 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1425, average loss: 0.1848
[09/16 15:16:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 92.50	top5: 100.00	
[09/16 15:16:56 visual_prompt]: 	Test 100/407. loss: 1.193, 0.2003 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 15:17:15 visual_prompt]: 	Test 200/407. loss: 1.229, 0.1957 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 15:17:34 visual_prompt]: 	Test 300/407. loss: 1.082, 0.1830 s / batch. (data: 1.54e-04)max mem: 17.22449 GB 
[09/16 15:17:54 visual_prompt]: 	Test 400/407. loss: 0.979, 0.1834 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 15:17:57 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1940, average loss: 1.1333
[09/16 15:17:57 visual_prompt]: Classification results with test_vtab-svhn: top1: 71.77	top5: 96.31	
[09/16 15:17:57 visual_prompt]: Best epoch 74: best metric: 0.925
[09/16 15:17:57 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 15:18:08 visual_prompt]: Epoch 75 / 100: avg data time: 1.54e-01, avg batch time: 0.5791, average train loss: 0.2029
[09/16 15:18:13 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1424, average loss: 0.1392
[09/16 15:18:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 95.00	top5: 100.00	
[09/16 15:18:35 visual_prompt]: 	Test 100/407. loss: 1.266, 0.1825 s / batch. (data: 1.43e-04)max mem: 17.22449 GB 
[09/16 15:18:54 visual_prompt]: 	Test 200/407. loss: 1.542, 0.1993 s / batch. (data: 1.45e-02)max mem: 17.22449 GB 
[09/16 15:19:13 visual_prompt]: 	Test 300/407. loss: 1.014, 0.1830 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 15:19:33 visual_prompt]: 	Test 400/407. loss: 1.269, 0.1829 s / batch. (data: 5.82e-05)max mem: 17.22449 GB 
[09/16 15:19:36 visual_prompt]: Inference (test):avg data time: 6.78e-03, avg batch time: 0.1931, average loss: 1.2668
[09/16 15:19:36 visual_prompt]: Classification results with test_vtab-svhn: top1: 73.81	top5: 96.76	
[09/16 15:19:36 visual_prompt]: Best epoch 75: best metric: 0.950
[09/16 15:19:36 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 15:19:47 visual_prompt]: Epoch 76 / 100: avg data time: 1.41e-01, avg batch time: 0.5442, average train loss: 0.1994
[09/16 15:19:51 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1427, average loss: 0.0826
[09/16 15:19:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 98.50	top5: 100.00	
[09/16 15:20:13 visual_prompt]: 	Test 100/407. loss: 1.064, 0.2101 s / batch. (data: 2.81e-02)max mem: 17.22449 GB 
[09/16 15:20:32 visual_prompt]: 	Test 200/407. loss: 1.401, 0.2066 s / batch. (data: 2.47e-02)max mem: 17.22449 GB 
[09/16 15:20:52 visual_prompt]: 	Test 300/407. loss: 0.880, 0.2149 s / batch. (data: 3.32e-02)max mem: 17.22449 GB 
[09/16 15:21:12 visual_prompt]: 	Test 400/407. loss: 1.152, 0.1823 s / batch. (data: 4.03e-05)max mem: 17.22449 GB 
[09/16 15:21:15 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1950, average loss: 1.0797
[09/16 15:21:15 visual_prompt]: Classification results with test_vtab-svhn: top1: 74.99	top5: 96.58	
[09/16 15:21:15 visual_prompt]: Best epoch 76: best metric: 0.985
[09/16 15:21:15 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 15:21:26 visual_prompt]: Epoch 77 / 100: avg data time: 1.52e-01, avg batch time: 0.5523, average train loss: 0.1072
[09/16 15:21:30 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1424, average loss: 0.1153
[09/16 15:21:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 95.00	top5: 100.00	
[09/16 15:21:52 visual_prompt]: 	Test 100/407. loss: 1.552, 0.2004 s / batch. (data: 1.87e-02)max mem: 17.22449 GB 
[09/16 15:22:11 visual_prompt]: 	Test 200/407. loss: 1.824, 0.1968 s / batch. (data: 1.48e-02)max mem: 17.22449 GB 
[09/16 15:22:31 visual_prompt]: 	Test 300/407. loss: 1.143, 0.1828 s / batch. (data: 1.48e-04)max mem: 17.22449 GB 
[09/16 15:22:50 visual_prompt]: 	Test 400/407. loss: 1.366, 0.1822 s / batch. (data: 5.63e-05)max mem: 17.22449 GB 
[09/16 15:22:53 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1931, average loss: 1.3685
[09/16 15:22:54 visual_prompt]: Classification results with test_vtab-svhn: top1: 74.60	top5: 96.17	
[09/16 15:22:54 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 15:23:04 visual_prompt]: Epoch 78 / 100: avg data time: 1.53e-01, avg batch time: 0.5596, average train loss: 0.1017
[09/16 15:23:09 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1431, average loss: 0.0999
[09/16 15:23:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 97.00	top5: 100.00	
[09/16 15:23:31 visual_prompt]: 	Test 100/407. loss: 1.477, 0.2193 s / batch. (data: 3.77e-02)max mem: 17.22449 GB 
[09/16 15:23:50 visual_prompt]: 	Test 200/407. loss: 1.722, 0.1945 s / batch. (data: 1.23e-02)max mem: 17.22449 GB 
[09/16 15:24:09 visual_prompt]: 	Test 300/407. loss: 1.186, 0.1964 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 15:24:29 visual_prompt]: 	Test 400/407. loss: 1.267, 0.1823 s / batch. (data: 3.39e-05)max mem: 17.22449 GB 
[09/16 15:24:32 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1936, average loss: 1.3001
[09/16 15:24:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 75.75	top5: 96.55	
[09/16 15:24:32 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 15:24:43 visual_prompt]: Epoch 79 / 100: avg data time: 1.54e-01, avg batch time: 0.5574, average train loss: 0.1457
[09/16 15:24:47 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1425, average loss: 0.1957
[09/16 15:24:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 95.00	top5: 99.50	
[09/16 15:25:09 visual_prompt]: 	Test 100/407. loss: 2.510, 0.1878 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 15:25:29 visual_prompt]: 	Test 200/407. loss: 2.389, 0.1850 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 15:25:48 visual_prompt]: 	Test 300/407. loss: 1.616, 0.1825 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 15:26:08 visual_prompt]: 	Test 400/407. loss: 2.244, 0.1906 s / batch. (data: 3.41e-05)max mem: 17.22449 GB 
[09/16 15:26:11 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1941, average loss: 1.7770
[09/16 15:26:11 visual_prompt]: Classification results with test_vtab-svhn: top1: 71.07	top5: 93.88	
[09/16 15:26:11 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 15:26:21 visual_prompt]: Epoch 80 / 100: avg data time: 1.38e-01, avg batch time: 0.5440, average train loss: 0.2093
[09/16 15:26:26 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1425, average loss: 0.0655
[09/16 15:26:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 98.00	top5: 100.00	
[09/16 15:26:48 visual_prompt]: 	Test 100/407. loss: 1.174, 0.1886 s / batch. (data: 1.26e-04)max mem: 17.22449 GB 
[09/16 15:27:07 visual_prompt]: 	Test 200/407. loss: 1.276, 0.1918 s / batch. (data: 9.21e-03)max mem: 17.22449 GB 
[09/16 15:27:27 visual_prompt]: 	Test 300/407. loss: 1.088, 0.1967 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 15:27:47 visual_prompt]: 	Test 400/407. loss: 1.516, 0.2085 s / batch. (data: 4.55e-05)max mem: 17.22449 GB 
[09/16 15:27:50 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1946, average loss: 1.1024
[09/16 15:27:50 visual_prompt]: Classification results with test_vtab-svhn: top1: 75.40	top5: 95.84	
[09/16 15:27:50 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 15:28:00 visual_prompt]: Epoch 81 / 100: avg data time: 1.52e-01, avg batch time: 0.5514, average train loss: 0.1099
[09/16 15:28:05 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1424, average loss: 0.0364
[09/16 15:28:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 15:28:27 visual_prompt]: 	Test 100/407. loss: 1.048, 0.1823 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 15:28:46 visual_prompt]: 	Test 200/407. loss: 1.488, 0.1981 s / batch. (data: 1.59e-02)max mem: 17.22449 GB 
[09/16 15:29:06 visual_prompt]: 	Test 300/407. loss: 1.036, 0.2033 s / batch. (data: 2.40e-04)max mem: 17.22449 GB 
[09/16 15:29:25 visual_prompt]: 	Test 400/407. loss: 1.191, 0.1823 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 15:29:28 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1936, average loss: 1.0920
[09/16 15:29:29 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.14	top5: 96.70	
[09/16 15:29:29 visual_prompt]: Best epoch 81: best metric: 0.990
[09/16 15:29:29 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 15:29:39 visual_prompt]: Epoch 82 / 100: avg data time: 1.52e-01, avg batch time: 0.5564, average train loss: 0.0420
[09/16 15:29:44 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1422, average loss: 0.0354
[09/16 15:29:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 15:30:06 visual_prompt]: 	Test 100/407. loss: 1.540, 0.2089 s / batch. (data: 2.69e-02)max mem: 17.22449 GB 
[09/16 15:30:25 visual_prompt]: 	Test 200/407. loss: 1.922, 0.1824 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 15:30:44 visual_prompt]: 	Test 300/407. loss: 1.343, 0.1948 s / batch. (data: 1.24e-02)max mem: 17.22449 GB 
[09/16 15:31:04 visual_prompt]: 	Test 400/407. loss: 1.479, 0.1830 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 15:31:07 visual_prompt]: Inference (test):avg data time: 8.86e-03, avg batch time: 0.1939, average loss: 1.3721
[09/16 15:31:08 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.68	top5: 96.55	
[09/16 15:31:08 visual_prompt]: Best epoch 82: best metric: 0.995
[09/16 15:31:08 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 15:31:18 visual_prompt]: Epoch 83 / 100: avg data time: 1.36e-01, avg batch time: 0.5421, average train loss: 0.0299
[09/16 15:31:22 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1423, average loss: 0.0398
[09/16 15:31:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 98.50	top5: 100.00	
[09/16 15:31:44 visual_prompt]: 	Test 100/407. loss: 1.947, 0.1957 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 15:32:04 visual_prompt]: 	Test 200/407. loss: 2.454, 0.1957 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 15:32:23 visual_prompt]: 	Test 300/407. loss: 1.454, 0.1923 s / batch. (data: 9.66e-03)max mem: 17.22449 GB 
[09/16 15:32:42 visual_prompt]: 	Test 400/407. loss: 1.963, 0.1826 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 15:32:46 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1932, average loss: 1.6562
[09/16 15:32:46 visual_prompt]: Classification results with test_vtab-svhn: top1: 74.92	top5: 95.75	
[09/16 15:32:46 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 15:32:56 visual_prompt]: Epoch 84 / 100: avg data time: 1.52e-01, avg batch time: 0.5546, average train loss: 0.0205
[09/16 15:33:01 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1425, average loss: 0.0071
[09/16 15:33:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 15:33:22 visual_prompt]: 	Test 100/407. loss: 1.828, 0.1973 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 15:33:42 visual_prompt]: 	Test 200/407. loss: 2.227, 0.1834 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 15:34:01 visual_prompt]: 	Test 300/407. loss: 1.434, 0.1832 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 15:34:21 visual_prompt]: 	Test 400/407. loss: 1.824, 0.1831 s / batch. (data: 4.29e-05)max mem: 17.22449 GB 
[09/16 15:34:24 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1941, average loss: 1.5283
[09/16 15:34:24 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.47	top5: 96.25	
[09/16 15:34:24 visual_prompt]: Best epoch 84: best metric: 1.000
[09/16 15:34:24 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 15:34:35 visual_prompt]: Epoch 85 / 100: avg data time: 1.64e-01, avg batch time: 0.5633, average train loss: 0.0082
[09/16 15:34:39 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1423, average loss: 0.0422
[09/16 15:34:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 98.00	top5: 100.00	
[09/16 15:35:02 visual_prompt]: 	Test 100/407. loss: 2.037, 0.1925 s / batch. (data: 1.13e-02)max mem: 17.22449 GB 
[09/16 15:35:22 visual_prompt]: 	Test 200/407. loss: 2.462, 0.1826 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 15:35:41 visual_prompt]: 	Test 300/407. loss: 1.537, 0.1972 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 15:36:01 visual_prompt]: 	Test 400/407. loss: 2.182, 0.1831 s / batch. (data: 4.29e-05)max mem: 17.22449 GB 
[09/16 15:36:04 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1967, average loss: 1.7471
[09/16 15:36:04 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.03	top5: 96.00	
[09/16 15:36:04 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 15:36:15 visual_prompt]: Epoch 86 / 100: avg data time: 1.54e-01, avg batch time: 0.5535, average train loss: 0.0091
[09/16 15:36:19 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1458, average loss: 0.0085
[09/16 15:36:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 15:36:42 visual_prompt]: 	Test 100/407. loss: 1.748, 0.1955 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 15:37:01 visual_prompt]: 	Test 200/407. loss: 2.301, 0.1968 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 15:37:20 visual_prompt]: 	Test 300/407. loss: 1.412, 0.2129 s / batch. (data: 2.54e-02)max mem: 17.22449 GB 
[09/16 15:37:40 visual_prompt]: 	Test 400/407. loss: 2.037, 0.1828 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 15:37:43 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1946, average loss: 1.6625
[09/16 15:37:43 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.82	top5: 96.43	
[09/16 15:37:43 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 15:37:53 visual_prompt]: Epoch 87 / 100: avg data time: 1.49e-01, avg batch time: 0.5497, average train loss: 0.0052
[09/16 15:37:58 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1428, average loss: 0.0072
[09/16 15:37:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 15:38:20 visual_prompt]: 	Test 100/407. loss: 1.828, 0.1960 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 15:38:40 visual_prompt]: 	Test 200/407. loss: 2.403, 0.2039 s / batch. (data: 1.30e-02)max mem: 17.22449 GB 
[09/16 15:38:59 visual_prompt]: 	Test 300/407. loss: 1.432, 0.1830 s / batch. (data: 1.08e-04)max mem: 17.22449 GB 
[09/16 15:39:19 visual_prompt]: 	Test 400/407. loss: 1.926, 0.1902 s / batch. (data: 3.84e-05)max mem: 17.22449 GB 
[09/16 15:39:23 visual_prompt]: Inference (test):avg data time: 8.43e-03, avg batch time: 0.1968, average loss: 1.7308
[09/16 15:39:23 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.98	top5: 96.30	
[09/16 15:39:23 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 15:39:33 visual_prompt]: Epoch 88 / 100: avg data time: 1.53e-01, avg batch time: 0.5531, average train loss: 0.0024
[09/16 15:39:38 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1427, average loss: 0.0076
[09/16 15:39:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 15:39:59 visual_prompt]: 	Test 100/407. loss: 1.839, 0.1825 s / batch. (data: 2.72e-05)max mem: 17.22449 GB 
[09/16 15:40:18 visual_prompt]: 	Test 200/407. loss: 2.431, 0.2077 s / batch. (data: 1.50e-02)max mem: 17.22449 GB 
[09/16 15:40:38 visual_prompt]: 	Test 300/407. loss: 1.446, 0.1834 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 15:40:58 visual_prompt]: 	Test 400/407. loss: 2.071, 0.2008 s / batch. (data: 1.90e-02)max mem: 17.22449 GB 
[09/16 15:41:01 visual_prompt]: Inference (test):avg data time: 7.01e-03, avg batch time: 0.1930, average loss: 1.7639
[09/16 15:41:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.04	top5: 96.42	
[09/16 15:41:01 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 15:41:11 visual_prompt]: Epoch 89 / 100: avg data time: 1.51e-01, avg batch time: 0.5515, average train loss: 0.0018
[09/16 15:41:16 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1425, average loss: 0.0074
[09/16 15:41:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 15:41:38 visual_prompt]: 	Test 100/407. loss: 1.904, 0.1969 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 15:41:57 visual_prompt]: 	Test 200/407. loss: 2.493, 0.1826 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 15:42:17 visual_prompt]: 	Test 300/407. loss: 1.495, 0.1967 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 15:42:36 visual_prompt]: 	Test 400/407. loss: 2.105, 0.1830 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 15:42:39 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1936, average loss: 1.7941
[09/16 15:42:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.94	top5: 96.39	
[09/16 15:42:39 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 15:42:50 visual_prompt]: Epoch 90 / 100: avg data time: 1.56e-01, avg batch time: 0.5567, average train loss: 0.0040
[09/16 15:42:55 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1425, average loss: 0.0411
[09/16 15:42:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 98.50	top5: 100.00	
[09/16 15:43:16 visual_prompt]: 	Test 100/407. loss: 2.312, 0.1823 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 15:43:36 visual_prompt]: 	Test 200/407. loss: 2.656, 0.2047 s / batch. (data: 1.31e-02)max mem: 17.22449 GB 
[09/16 15:43:55 visual_prompt]: 	Test 300/407. loss: 1.777, 0.2115 s / batch. (data: 2.58e-02)max mem: 17.22449 GB 
[09/16 15:44:15 visual_prompt]: 	Test 400/407. loss: 2.282, 0.1835 s / batch. (data: 2.77e-05)max mem: 17.22449 GB 
[09/16 15:44:18 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1937, average loss: 1.9785
[09/16 15:44:18 visual_prompt]: Classification results with test_vtab-svhn: top1: 75.88	top5: 95.88	
[09/16 15:44:18 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 15:44:29 visual_prompt]: Epoch 91 / 100: avg data time: 1.46e-01, avg batch time: 0.5493, average train loss: 0.0035
[09/16 15:44:33 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1426, average loss: 0.0381
[09/16 15:44:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 15:44:55 visual_prompt]: 	Test 100/407. loss: 2.308, 0.2292 s / batch. (data: 4.77e-02)max mem: 17.22449 GB 
[09/16 15:45:14 visual_prompt]: 	Test 200/407. loss: 2.607, 0.2043 s / batch. (data: 2.25e-02)max mem: 17.22449 GB 
[09/16 15:45:34 visual_prompt]: 	Test 300/407. loss: 1.768, 0.2003 s / batch. (data: 1.74e-02)max mem: 17.22449 GB 
[09/16 15:45:53 visual_prompt]: 	Test 400/407. loss: 2.181, 0.1828 s / batch. (data: 4.27e-05)max mem: 17.22449 GB 
[09/16 15:45:56 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1936, average loss: 1.9272
[09/16 15:45:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.45	top5: 96.02	
[09/16 15:45:56 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 15:46:07 visual_prompt]: Epoch 92 / 100: avg data time: 1.52e-01, avg batch time: 0.5530, average train loss: 0.0017
[09/16 15:46:11 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1426, average loss: 0.0150
[09/16 15:46:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 15:46:33 visual_prompt]: 	Test 100/407. loss: 2.105, 0.1974 s / batch. (data: 1.58e-02)max mem: 17.22449 GB 
[09/16 15:46:52 visual_prompt]: 	Test 200/407. loss: 2.530, 0.1956 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 15:47:12 visual_prompt]: 	Test 300/407. loss: 1.686, 0.1982 s / batch. (data: 1.56e-02)max mem: 17.22449 GB 
[09/16 15:47:32 visual_prompt]: 	Test 400/407. loss: 2.072, 0.1834 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 15:47:35 visual_prompt]: Inference (test):avg data time: 6.90e-03, avg batch time: 0.1948, average loss: 1.8393
[09/16 15:47:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.99	top5: 96.28	
[09/16 15:47:35 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 15:47:46 visual_prompt]: Epoch 93 / 100: avg data time: 1.54e-01, avg batch time: 0.5543, average train loss: 0.0015
[09/16 15:47:51 visual_prompt]: Inference (val):avg data time: 1.31e-04, avg batch time: 0.1916, average loss: 0.0134
[09/16 15:47:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 15:48:12 visual_prompt]: 	Test 100/407. loss: 2.104, 0.1970 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 15:48:32 visual_prompt]: 	Test 200/407. loss: 2.522, 0.1906 s / batch. (data: 8.44e-03)max mem: 17.22449 GB 
[09/16 15:48:51 visual_prompt]: 	Test 300/407. loss: 1.676, 0.1875 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 15:49:11 visual_prompt]: 	Test 400/407. loss: 2.086, 0.1828 s / batch. (data: 3.00e-05)max mem: 17.22449 GB 
[09/16 15:49:14 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1938, average loss: 1.8322
[09/16 15:49:14 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.06	top5: 96.29	
[09/16 15:49:14 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 15:49:25 visual_prompt]: Epoch 94 / 100: avg data time: 1.53e-01, avg batch time: 0.5529, average train loss: 0.0016
[09/16 15:49:29 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1426, average loss: 0.0141
[09/16 15:49:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 15:49:51 visual_prompt]: 	Test 100/407. loss: 2.076, 0.1827 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 15:50:10 visual_prompt]: 	Test 200/407. loss: 2.511, 0.1984 s / batch. (data: 1.61e-02)max mem: 17.22449 GB 
[09/16 15:50:30 visual_prompt]: 	Test 300/407. loss: 1.627, 0.1828 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 15:50:49 visual_prompt]: 	Test 400/407. loss: 2.090, 0.1826 s / batch. (data: 4.79e-05)max mem: 17.22449 GB 
[09/16 15:50:53 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1933, average loss: 1.8006
[09/16 15:50:53 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.19	top5: 96.35	
[09/16 15:50:53 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 15:51:03 visual_prompt]: Epoch 95 / 100: avg data time: 1.51e-01, avg batch time: 0.5538, average train loss: 0.0011
[09/16 15:51:08 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1426, average loss: 0.0141
[09/16 15:51:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 15:51:29 visual_prompt]: 	Test 100/407. loss: 2.071, 0.1832 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 15:51:49 visual_prompt]: 	Test 200/407. loss: 2.520, 0.1917 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 15:52:08 visual_prompt]: 	Test 300/407. loss: 1.626, 0.1837 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 15:52:28 visual_prompt]: 	Test 400/407. loss: 2.091, 0.1827 s / batch. (data: 4.03e-05)max mem: 17.22449 GB 
[09/16 15:52:31 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1934, average loss: 1.8043
[09/16 15:52:31 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.11	top5: 96.32	
[09/16 15:52:31 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 15:52:41 visual_prompt]: Epoch 96 / 100: avg data time: 1.43e-01, avg batch time: 0.5466, average train loss: 0.0013
[09/16 15:52:46 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.1423, average loss: 0.0136
[09/16 15:52:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 15:53:07 visual_prompt]: 	Test 100/407. loss: 2.070, 0.1968 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 15:53:27 visual_prompt]: 	Test 200/407. loss: 2.524, 0.1821 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 15:53:46 visual_prompt]: 	Test 300/407. loss: 1.633, 0.2078 s / batch. (data: 5.35e-03)max mem: 17.22449 GB 
[09/16 15:54:06 visual_prompt]: 	Test 400/407. loss: 2.092, 0.1826 s / batch. (data: 4.12e-05)max mem: 17.22449 GB 
[09/16 15:54:09 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1946, average loss: 1.8102
[09/16 15:54:09 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.08	top5: 96.31	
[09/16 15:54:09 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 15:54:20 visual_prompt]: Epoch 97 / 100: avg data time: 1.48e-01, avg batch time: 0.5498, average train loss: 0.0007
[09/16 15:54:24 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1425, average loss: 0.0134
[09/16 15:54:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 15:54:46 visual_prompt]: 	Test 100/407. loss: 2.069, 0.1942 s / batch. (data: 1.22e-02)max mem: 17.22449 GB 
[09/16 15:55:06 visual_prompt]: 	Test 200/407. loss: 2.528, 0.2225 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 15:55:25 visual_prompt]: 	Test 300/407. loss: 1.639, 0.1972 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 15:55:46 visual_prompt]: 	Test 400/407. loss: 2.093, 0.1826 s / batch. (data: 2.62e-05)max mem: 17.22449 GB 
[09/16 15:55:49 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1968, average loss: 1.8150
[09/16 15:55:49 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.04	top5: 96.30	
[09/16 15:55:49 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 15:56:00 visual_prompt]: Epoch 98 / 100: avg data time: 1.50e-01, avg batch time: 0.5521, average train loss: 0.0007
[09/16 15:56:05 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1425, average loss: 0.0133
[09/16 15:56:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 15:56:26 visual_prompt]: 	Test 100/407. loss: 2.069, 0.1822 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 15:56:46 visual_prompt]: 	Test 200/407. loss: 2.529, 0.1826 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 15:57:05 visual_prompt]: 	Test 300/407. loss: 1.640, 0.1839 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 15:57:24 visual_prompt]: 	Test 400/407. loss: 2.094, 0.1828 s / batch. (data: 3.43e-05)max mem: 17.22449 GB 
[09/16 15:57:28 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1932, average loss: 1.8160
[09/16 15:57:28 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.02	top5: 96.30	
[09/16 15:57:28 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 15:57:38 visual_prompt]: Epoch 99 / 100: avg data time: 1.47e-01, avg batch time: 0.5477, average train loss: 0.0009
[09/16 15:57:43 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1425, average loss: 0.0131
[09/16 15:57:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 15:58:05 visual_prompt]: 	Test 100/407. loss: 2.068, 0.1919 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 15:58:24 visual_prompt]: 	Test 200/407. loss: 2.528, 0.1962 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 15:58:43 visual_prompt]: 	Test 300/407. loss: 1.639, 0.1990 s / batch. (data: 1.40e-04)max mem: 17.22449 GB 
[09/16 15:59:03 visual_prompt]: 	Test 400/407. loss: 2.093, 0.1829 s / batch. (data: 4.48e-05)max mem: 17.22449 GB 
[09/16 15:59:06 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1933, average loss: 1.8159
[09/16 15:59:06 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.04	top5: 96.30	
[09/16 15:59:06 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 15:59:17 visual_prompt]: Epoch 100 / 100: avg data time: 1.56e-01, avg batch time: 0.5568, average train loss: 0.0009
[09/16 15:59:21 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1425, average loss: 0.0131
[09/16 15:59:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 15:59:43 visual_prompt]: 	Test 100/407. loss: 2.068, 0.1962 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 16:00:02 visual_prompt]: 	Test 200/407. loss: 2.528, 0.1961 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 16:00:22 visual_prompt]: 	Test 300/407. loss: 1.640, 0.1825 s / batch. (data: 1.05e-04)max mem: 17.22449 GB 
[09/16 16:00:41 visual_prompt]: 	Test 400/407. loss: 2.093, 0.1824 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 16:00:45 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1934, average loss: 1.8159
[09/16 16:00:45 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.03	top5: 96.30	
[09/16 16:01:17 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 16:01:17 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 16:01:17 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed100'], train_type='')
[09/16 16:01:17 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 16:01:17 visual_prompt]: Training with config:
[09/16 16:01:17 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-svhn',
          'NO_TEST': False,
          'NUMBER_CLASSES': 10,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed100/vtab-svhn/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 16:01:17 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 16:01:17.794493: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 16:01:17.953904: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 16:01:18.837450: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 16:01:18.837530: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 16:01:18.837540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 16:01:20.872049: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 16:01:20.872157: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 16:01:20.872170: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 16:01:20 visual_prompt]: Constructing vtab-svhn dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
2023-09-16 16:01:20.888455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split train[:800]+train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 16:01:22 visual_prompt]: Number of images: 1000
[09/16 16:01:22 visual_prompt]: Number of classes: 10 / 10
[09/16 16:01:22 visual_prompt]: Loading validation data...
[09/16 16:01:22 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 16:01:23 visual_prompt]: Number of images: 200
[09/16 16:01:23 visual_prompt]: Number of classes: 10 / 10
[09/16 16:01:23 visual_prompt]: Loading test data...
[09/16 16:01:23 visual_prompt]: Constructing vtab-svhn dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split test, from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 16:01:55 visual_prompt]: Number of images: 26032
[09/16 16:01:55 visual_prompt]: Number of classes: 10 / 10
[09/16 16:01:55 visual_prompt]: Constructing models...
[09/16 16:01:57 visual_prompt]: Total Parameters: 86727946	 Gradient Parameters: 929290
[09/16 16:01:57 visual_prompt]: tuned percent:1.072
[09/16 16:02:00 visual_prompt]: Device used for model: 0
[09/16 16:02:00 visual_prompt]: Setting up Evalutator...
[09/16 16:02:00 visual_prompt]: Setting up Trainer...
[09/16 16:02:00 visual_prompt]: 	Setting up the optimizer...
[09/16 16:02:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 16:02:12 visual_prompt]: Epoch 1 / 100: avg data time: 1.67e-01, avg batch time: 0.6380, average train loss: 2.6572
[09/16 16:02:17 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1417, average loss: 2.5881
[09/16 16:02:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/16 16:02:38 visual_prompt]: 	Test 100/407. loss: 2.678, 0.1810 s / batch. (data: 1.14e-04)max mem: 17.22449 GB 
[09/16 16:02:58 visual_prompt]: 	Test 200/407. loss: 2.821, 0.1811 s / batch. (data: 1.05e-04)max mem: 17.22449 GB 
[09/16 16:03:17 visual_prompt]: 	Test 300/407. loss: 2.549, 0.1824 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 16:03:37 visual_prompt]: 	Test 400/407. loss: 2.783, 0.1820 s / batch. (data: 4.36e-05)max mem: 17.22449 GB 
[09/16 16:03:40 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1934, average loss: 2.6049
[09/16 16:03:40 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 57.77	
[09/16 16:03:40 visual_prompt]: Best epoch 1: best metric: 0.230
[09/16 16:03:40 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 16:03:50 visual_prompt]: Epoch 2 / 100: avg data time: 1.42e-01, avg batch time: 0.5466, average train loss: 3.0320
[09/16 16:03:55 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1425, average loss: 2.3229
[09/16 16:03:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/16 16:04:17 visual_prompt]: 	Test 100/407. loss: 2.490, 0.1823 s / batch. (data: 1.53e-04)max mem: 17.22449 GB 
[09/16 16:04:36 visual_prompt]: 	Test 200/407. loss: 2.550, 0.1831 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 16:04:56 visual_prompt]: 	Test 300/407. loss: 2.257, 0.2067 s / batch. (data: 9.99e-05)max mem: 17.22449 GB 
[09/16 16:05:16 visual_prompt]: 	Test 400/407. loss: 2.385, 0.1825 s / batch. (data: 3.77e-05)max mem: 17.22449 GB 
[09/16 16:05:19 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1940, average loss: 2.3471
[09/16 16:05:19 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 59.22	
[09/16 16:05:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 16:05:30 visual_prompt]: Epoch 3 / 100: avg data time: 1.56e-01, avg batch time: 0.5543, average train loss: 2.4300
[09/16 16:05:34 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1426, average loss: 2.2743
[09/16 16:05:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/16 16:05:56 visual_prompt]: 	Test 100/407. loss: 2.318, 0.1826 s / batch. (data: 1.03e-04)max mem: 17.22449 GB 
[09/16 16:06:15 visual_prompt]: 	Test 200/407. loss: 2.375, 0.1838 s / batch. (data: 1.54e-04)max mem: 17.22449 GB 
[09/16 16:06:35 visual_prompt]: 	Test 300/407. loss: 2.211, 0.1974 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 16:06:55 visual_prompt]: 	Test 400/407. loss: 2.307, 0.1823 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 16:06:58 visual_prompt]: Inference (test):avg data time: 8.26e-03, avg batch time: 0.1937, average loss: 2.2664
[09/16 16:06:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 62.13	
[09/16 16:06:58 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 16:07:08 visual_prompt]: Epoch 4 / 100: avg data time: 1.47e-01, avg batch time: 0.5475, average train loss: 2.3505
[09/16 16:07:13 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1424, average loss: 2.3638
[09/16 16:07:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 47.00	
[09/16 16:07:36 visual_prompt]: 	Test 100/407. loss: 2.332, 0.1918 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 16:07:55 visual_prompt]: 	Test 200/407. loss: 2.372, 0.1894 s / batch. (data: 5.22e-03)max mem: 17.22449 GB 
[09/16 16:08:14 visual_prompt]: 	Test 300/407. loss: 2.360, 0.2048 s / batch. (data: 3.36e-05)max mem: 17.22449 GB 
[09/16 16:08:34 visual_prompt]: 	Test 400/407. loss: 2.325, 0.1826 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 16:08:37 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1949, average loss: 2.3582
[09/16 16:08:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 45.18	
[09/16 16:08:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 16:08:48 visual_prompt]: Epoch 5 / 100: avg data time: 1.55e-01, avg batch time: 0.5563, average train loss: 2.5133
[09/16 16:08:52 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1426, average loss: 2.5764
[09/16 16:08:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 62.00	
[09/16 16:09:14 visual_prompt]: 	Test 100/407. loss: 2.538, 0.2036 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 16:09:34 visual_prompt]: 	Test 200/407. loss: 2.699, 0.1963 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 16:09:53 visual_prompt]: 	Test 300/407. loss: 2.512, 0.1827 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 16:10:13 visual_prompt]: 	Test 400/407. loss: 2.650, 0.1827 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 16:10:16 visual_prompt]: Inference (test):avg data time: 8.56e-03, avg batch time: 0.1938, average loss: 2.5569
[09/16 16:10:16 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 62.13	
[09/16 16:10:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 16:10:26 visual_prompt]: Epoch 6 / 100: avg data time: 1.40e-01, avg batch time: 0.5464, average train loss: 2.7431
[09/16 16:10:31 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1421, average loss: 2.7751
[09/16 16:10:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.50	
[09/16 16:10:52 visual_prompt]: 	Test 100/407. loss: 2.893, 0.2159 s / batch. (data: 9.23e-05)max mem: 17.22449 GB 
[09/16 16:11:12 visual_prompt]: 	Test 200/407. loss: 2.813, 0.1829 s / batch. (data: 1.61e-04)max mem: 17.22449 GB 
[09/16 16:11:31 visual_prompt]: 	Test 300/407. loss: 2.833, 0.1822 s / batch. (data: 1.59e-04)max mem: 17.22449 GB 
[09/16 16:11:51 visual_prompt]: 	Test 400/407. loss: 2.708, 0.1832 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 16:11:54 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1932, average loss: 2.7664
[09/16 16:11:54 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 63.88	
[09/16 16:11:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 16:12:05 visual_prompt]: Epoch 7 / 100: avg data time: 1.58e-01, avg batch time: 0.5590, average train loss: 3.7773
[09/16 16:12:09 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1424, average loss: 4.0517
[09/16 16:12:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/16 16:12:31 visual_prompt]: 	Test 100/407. loss: 4.826, 0.1961 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 16:12:50 visual_prompt]: 	Test 200/407. loss: 4.625, 0.1995 s / batch. (data: 1.76e-02)max mem: 17.22449 GB 
[09/16 16:13:10 visual_prompt]: 	Test 300/407. loss: 4.206, 0.1826 s / batch. (data: 1.02e-04)max mem: 17.22449 GB 
[09/16 16:13:29 visual_prompt]: 	Test 400/407. loss: 4.363, 0.1831 s / batch. (data: 4.05e-05)max mem: 17.22449 GB 
[09/16 16:13:33 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1932, average loss: 4.3051
[09/16 16:13:33 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 54.32	
[09/16 16:13:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 16:13:43 visual_prompt]: Epoch 8 / 100: avg data time: 1.46e-01, avg batch time: 0.5454, average train loss: 4.7493
[09/16 16:13:48 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1425, average loss: 5.1254
[09/16 16:13:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 52.50	
[09/16 16:14:10 visual_prompt]: 	Test 100/407. loss: 5.939, 0.1824 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 16:14:29 visual_prompt]: 	Test 200/407. loss: 5.260, 0.2095 s / batch. (data: 2.76e-02)max mem: 17.22449 GB 
[09/16 16:14:48 visual_prompt]: 	Test 300/407. loss: 5.749, 0.2086 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 16:15:08 visual_prompt]: 	Test 400/407. loss: 5.753, 0.1890 s / batch. (data: 3.58e-05)max mem: 17.22449 GB 
[09/16 16:15:11 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1936, average loss: 5.1674
[09/16 16:15:11 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 49.70	
[09/16 16:15:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 16:15:22 visual_prompt]: Epoch 9 / 100: avg data time: 1.45e-01, avg batch time: 0.5477, average train loss: 6.3703
[09/16 16:15:26 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1446, average loss: 5.0434
[09/16 16:15:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/16 16:15:48 visual_prompt]: 	Test 100/407. loss: 6.370, 0.1973 s / batch. (data: 1.56e-02)max mem: 17.22449 GB 
[09/16 16:16:07 visual_prompt]: 	Test 200/407. loss: 6.128, 0.1973 s / batch. (data: 1.55e-02)max mem: 17.22449 GB 
[09/16 16:16:27 visual_prompt]: 	Test 300/407. loss: 5.289, 0.2388 s / batch. (data: 1.98e-02)max mem: 17.22449 GB 
[09/16 16:16:46 visual_prompt]: 	Test 400/407. loss: 5.379, 0.1830 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 16:16:49 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1931, average loss: 5.3719
[09/16 16:16:50 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 53.54	
[09/16 16:16:50 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 16:17:00 visual_prompt]: Epoch 10 / 100: avg data time: 1.50e-01, avg batch time: 0.5509, average train loss: 10.1167
[09/16 16:17:04 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1625, average loss: 20.4078
[09/16 16:17:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.50	
[09/16 16:17:26 visual_prompt]: 	Test 100/407. loss: 24.320, 0.1827 s / batch. (data: 1.38e-04)max mem: 17.22449 GB 
[09/16 16:17:45 visual_prompt]: 	Test 200/407. loss: 27.055, 0.1858 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 16:18:05 visual_prompt]: 	Test 300/407. loss: 19.042, 0.2040 s / batch. (data: 1.51e-03)max mem: 17.22449 GB 
[09/16 16:18:24 visual_prompt]: 	Test 400/407. loss: 23.647, 0.1829 s / batch. (data: 4.82e-05)max mem: 17.22449 GB 
[09/16 16:18:27 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1929, average loss: 21.3714
[09/16 16:18:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 53.70	
[09/16 16:18:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 16:18:38 visual_prompt]: Epoch 11 / 100: avg data time: 1.52e-01, avg batch time: 0.5527, average train loss: 21.8350
[09/16 16:18:42 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1425, average loss: 10.2393
[09/16 16:18:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 60.00	
[09/16 16:19:04 visual_prompt]: 	Test 100/407. loss: 10.542, 0.2075 s / batch. (data: 2.59e-02)max mem: 17.22449 GB 
[09/16 16:19:24 visual_prompt]: 	Test 200/407. loss: 10.256, 0.1901 s / batch. (data: 1.11e-04)max mem: 17.22449 GB 
[09/16 16:19:43 visual_prompt]: 	Test 300/407. loss: 9.987, 0.2020 s / batch. (data: 2.05e-02)max mem: 17.22449 GB 
[09/16 16:20:02 visual_prompt]: 	Test 400/407. loss: 10.044, 0.1829 s / batch. (data: 3.36e-05)max mem: 17.22449 GB 
[09/16 16:20:06 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1932, average loss: 10.0812
[09/16 16:20:06 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.38	top5: 59.67	
[09/16 16:20:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 16:20:16 visual_prompt]: Epoch 12 / 100: avg data time: 1.47e-01, avg batch time: 0.5504, average train loss: 12.2669
[09/16 16:20:21 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1424, average loss: 6.1223
[09/16 16:20:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 60.50	
[09/16 16:20:43 visual_prompt]: 	Test 100/407. loss: 6.982, 0.1828 s / batch. (data: 1.61e-04)max mem: 17.22449 GB 
[09/16 16:21:02 visual_prompt]: 	Test 200/407. loss: 6.765, 0.1918 s / batch. (data: 7.21e-03)max mem: 17.22449 GB 
[09/16 16:21:22 visual_prompt]: 	Test 300/407. loss: 6.424, 0.2073 s / batch. (data: 2.51e-02)max mem: 17.22449 GB 
[09/16 16:21:41 visual_prompt]: 	Test 400/407. loss: 6.370, 0.1879 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 16:21:45 visual_prompt]: Inference (test):avg data time: 8.93e-03, avg batch time: 0.1948, average loss: 6.3874
[09/16 16:21:45 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.70	top5: 55.54	
[09/16 16:21:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 16:21:55 visual_prompt]: Epoch 13 / 100: avg data time: 1.54e-01, avg batch time: 0.5547, average train loss: 7.1135
[09/16 16:22:00 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1422, average loss: 5.8366
[09/16 16:22:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.00	
[09/16 16:22:22 visual_prompt]: 	Test 100/407. loss: 6.817, 0.1959 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 16:22:41 visual_prompt]: 	Test 200/407. loss: 6.196, 0.1829 s / batch. (data: 1.26e-04)max mem: 17.22449 GB 
[09/16 16:23:00 visual_prompt]: 	Test 300/407. loss: 6.945, 0.2027 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 16:23:20 visual_prompt]: 	Test 400/407. loss: 6.041, 0.1827 s / batch. (data: 2.74e-05)max mem: 17.22449 GB 
[09/16 16:23:23 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1945, average loss: 6.2507
[09/16 16:23:23 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.76	top5: 51.31	
[09/16 16:23:23 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 16:23:34 visual_prompt]: Epoch 14 / 100: avg data time: 1.46e-01, avg batch time: 0.5493, average train loss: 4.5398
[09/16 16:23:38 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1422, average loss: 4.5623
[09/16 16:23:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 41.00	
[09/16 16:24:00 visual_prompt]: 	Test 100/407. loss: 3.975, 0.2073 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 16:24:20 visual_prompt]: 	Test 200/407. loss: 4.466, 0.2014 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 16:24:39 visual_prompt]: 	Test 300/407. loss: 4.087, 0.1834 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 16:24:58 visual_prompt]: 	Test 400/407. loss: 4.672, 0.1822 s / batch. (data: 3.39e-05)max mem: 17.22449 GB 
[09/16 16:25:02 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1927, average loss: 4.4083
[09/16 16:25:02 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.38	top5: 45.21	
[09/16 16:25:02 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 16:25:12 visual_prompt]: Epoch 15 / 100: avg data time: 1.44e-01, avg batch time: 0.5458, average train loss: 3.6992
[09/16 16:25:16 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1425, average loss: 4.6001
[09/16 16:25:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/16 16:25:38 visual_prompt]: 	Test 100/407. loss: 5.835, 0.2379 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 16:25:57 visual_prompt]: 	Test 200/407. loss: 5.547, 0.1963 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 16:26:17 visual_prompt]: 	Test 300/407. loss: 4.893, 0.1829 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 16:26:36 visual_prompt]: 	Test 400/407. loss: 5.487, 0.1828 s / batch. (data: 5.17e-05)max mem: 17.22449 GB 
[09/16 16:26:40 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1932, average loss: 4.9088
[09/16 16:26:40 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 52.90	
[09/16 16:26:40 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 16:26:50 visual_prompt]: Epoch 16 / 100: avg data time: 1.44e-01, avg batch time: 0.5477, average train loss: 3.2496
[09/16 16:26:55 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1423, average loss: 2.3396
[09/16 16:26:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/16 16:27:16 visual_prompt]: 	Test 100/407. loss: 2.549, 0.1955 s / batch. (data: 1.19e-02)max mem: 17.22449 GB 
[09/16 16:27:36 visual_prompt]: 	Test 200/407. loss: 2.521, 0.2063 s / batch. (data: 3.36e-05)max mem: 17.22449 GB 
[09/16 16:27:55 visual_prompt]: 	Test 300/407. loss: 2.403, 0.1835 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 16:28:14 visual_prompt]: 	Test 400/407. loss: 2.472, 0.1833 s / batch. (data: 3.60e-05)max mem: 17.22449 GB 
[09/16 16:28:18 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1923, average loss: 2.3934
[09/16 16:28:18 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 58.70	
[09/16 16:28:18 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 16:28:28 visual_prompt]: Epoch 17 / 100: avg data time: 1.59e-01, avg batch time: 0.5591, average train loss: 2.5561
[09/16 16:28:33 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1446, average loss: 2.6072
[09/16 16:28:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 58.50	
[09/16 16:28:55 visual_prompt]: 	Test 100/407. loss: 2.868, 0.1959 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 16:29:14 visual_prompt]: 	Test 200/407. loss: 2.861, 0.1959 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 16:29:34 visual_prompt]: 	Test 300/407. loss: 2.739, 0.1953 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 16:29:53 visual_prompt]: 	Test 400/407. loss: 2.823, 0.1825 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 16:29:57 visual_prompt]: Inference (test):avg data time: 9.22e-03, avg batch time: 0.1948, average loss: 2.7210
[09/16 16:29:57 visual_prompt]: Classification results with test_vtab-svhn: top1: 16.67	top5: 55.89	
[09/16 16:29:57 visual_prompt]: Best epoch 17: best metric: 0.235
[09/16 16:29:57 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 16:30:07 visual_prompt]: Epoch 18 / 100: avg data time: 1.45e-01, avg batch time: 0.5467, average train loss: 2.5612
[09/16 16:30:11 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1424, average loss: 2.6550
[09/16 16:30:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/16 16:30:33 visual_prompt]: 	Test 100/407. loss: 2.874, 0.2095 s / batch. (data: 1.29e-02)max mem: 17.22449 GB 
[09/16 16:30:53 visual_prompt]: 	Test 200/407. loss: 3.024, 0.1829 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 16:31:12 visual_prompt]: 	Test 300/407. loss: 2.485, 0.1965 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 16:31:32 visual_prompt]: 	Test 400/407. loss: 2.745, 0.1821 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 16:31:35 visual_prompt]: Inference (test):avg data time: 8.10e-03, avg batch time: 0.1939, average loss: 2.6713
[09/16 16:31:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 60.73	
[09/16 16:31:35 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 16:31:45 visual_prompt]: Epoch 19 / 100: avg data time: 1.51e-01, avg batch time: 0.5530, average train loss: 2.7275
[09/16 16:31:50 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.2016, average loss: 3.3019
[09/16 16:31:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 52.50	
[09/16 16:32:12 visual_prompt]: 	Test 100/407. loss: 3.149, 0.1826 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 16:32:31 visual_prompt]: 	Test 200/407. loss: 3.357, 0.2096 s / batch. (data: 2.80e-02)max mem: 17.22449 GB 
[09/16 16:32:50 visual_prompt]: 	Test 300/407. loss: 3.549, 0.2198 s / batch. (data: 3.74e-02)max mem: 17.22449 GB 
[09/16 16:33:10 visual_prompt]: 	Test 400/407. loss: 3.653, 0.1822 s / batch. (data: 3.77e-05)max mem: 17.22449 GB 
[09/16 16:33:13 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1935, average loss: 3.3894
[09/16 16:33:13 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 49.53	
[09/16 16:33:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 16:33:24 visual_prompt]: Epoch 20 / 100: avg data time: 1.42e-01, avg batch time: 0.5429, average train loss: 3.6737
[09/16 16:33:28 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1453, average loss: 2.7508
[09/16 16:33:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/16 16:33:50 visual_prompt]: 	Test 100/407. loss: 2.882, 0.1823 s / batch. (data: 9.73e-05)max mem: 17.22449 GB 
[09/16 16:34:09 visual_prompt]: 	Test 200/407. loss: 2.844, 0.1965 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 16:34:29 visual_prompt]: 	Test 300/407. loss: 2.785, 0.1825 s / batch. (data: 9.37e-05)max mem: 17.22449 GB 
[09/16 16:34:49 visual_prompt]: 	Test 400/407. loss: 2.651, 0.1828 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 16:34:52 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1942, average loss: 2.6947
[09/16 16:34:52 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 62.99	
[09/16 16:34:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 16:35:02 visual_prompt]: Epoch 21 / 100: avg data time: 1.51e-01, avg batch time: 0.5509, average train loss: 2.9752
[09/16 16:35:07 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1424, average loss: 3.1214
[09/16 16:35:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/16 16:35:29 visual_prompt]: 	Test 100/407. loss: 3.164, 0.1825 s / batch. (data: 1.07e-04)max mem: 17.22449 GB 
[09/16 16:35:49 visual_prompt]: 	Test 200/407. loss: 3.079, 0.2066 s / batch. (data: 2.42e-02)max mem: 17.22449 GB 
[09/16 16:36:08 visual_prompt]: 	Test 300/407. loss: 3.284, 0.1947 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 16:36:28 visual_prompt]: 	Test 400/407. loss: 3.066, 0.1828 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 16:36:31 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1950, average loss: 3.1347
[09/16 16:36:31 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 56.57	
[09/16 16:36:31 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 16:36:41 visual_prompt]: Epoch 22 / 100: avg data time: 1.52e-01, avg batch time: 0.5538, average train loss: 3.1688
[09/16 16:36:46 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1424, average loss: 3.1405
[09/16 16:36:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 50.50	
[09/16 16:37:08 visual_prompt]: 	Test 100/407. loss: 3.395, 0.1970 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 16:37:27 visual_prompt]: 	Test 200/407. loss: 3.340, 0.1824 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 16:37:46 visual_prompt]: 	Test 300/407. loss: 3.436, 0.1834 s / batch. (data: 1.08e-04)max mem: 17.22449 GB 
[09/16 16:38:06 visual_prompt]: 	Test 400/407. loss: 3.422, 0.1828 s / batch. (data: 3.50e-05)max mem: 17.22449 GB 
[09/16 16:38:09 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1921, average loss: 3.1933
[09/16 16:38:09 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 48.48	
[09/16 16:38:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 16:38:19 visual_prompt]: Epoch 23 / 100: avg data time: 1.51e-01, avg batch time: 0.5553, average train loss: 2.7503
[09/16 16:38:24 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1425, average loss: 3.1015
[09/16 16:38:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/16 16:38:46 visual_prompt]: 	Test 100/407. loss: 3.640, 0.1979 s / batch. (data: 1.60e-02)max mem: 17.22449 GB 
[09/16 16:39:05 visual_prompt]: 	Test 200/407. loss: 3.626, 0.1984 s / batch. (data: 1.60e-02)max mem: 17.22449 GB 
[09/16 16:39:25 visual_prompt]: 	Test 300/407. loss: 3.143, 0.1827 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 16:39:44 visual_prompt]: 	Test 400/407. loss: 3.382, 0.1824 s / batch. (data: 4.05e-05)max mem: 17.22449 GB 
[09/16 16:39:47 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1934, average loss: 3.1496
[09/16 16:39:47 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 62.67	
[09/16 16:39:47 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 16:39:58 visual_prompt]: Epoch 24 / 100: avg data time: 1.61e-01, avg batch time: 0.5636, average train loss: 2.9283
[09/16 16:40:02 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1425, average loss: 2.7438
[09/16 16:40:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/16 16:40:24 visual_prompt]: 	Test 100/407. loss: 3.503, 0.2084 s / batch. (data: 2.69e-02)max mem: 17.22449 GB 
[09/16 16:40:44 visual_prompt]: 	Test 200/407. loss: 3.140, 0.1890 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 16:41:03 visual_prompt]: 	Test 300/407. loss: 2.884, 0.1846 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 16:41:23 visual_prompt]: 	Test 400/407. loss: 2.711, 0.1823 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 16:41:26 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1941, average loss: 2.8537
[09/16 16:41:26 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 54.94	
[09/16 16:41:26 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 16:41:37 visual_prompt]: Epoch 25 / 100: avg data time: 1.50e-01, avg batch time: 0.5685, average train loss: 2.8010
[09/16 16:41:41 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1426, average loss: 2.7718
[09/16 16:41:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/16 16:42:03 visual_prompt]: 	Test 100/407. loss: 3.072, 0.2111 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 16:42:22 visual_prompt]: 	Test 200/407. loss: 3.120, 0.1829 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 16:42:42 visual_prompt]: 	Test 300/407. loss: 2.704, 0.1823 s / batch. (data: 1.72e-04)max mem: 17.22449 GB 
[09/16 16:43:01 visual_prompt]: 	Test 400/407. loss: 2.927, 0.1829 s / batch. (data: 3.29e-05)max mem: 17.22449 GB 
[09/16 16:43:04 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1929, average loss: 2.8337
[09/16 16:43:04 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 56.36	
[09/16 16:43:04 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 16:43:15 visual_prompt]: Epoch 26 / 100: avg data time: 1.43e-01, avg batch time: 0.5459, average train loss: 2.5458
[09/16 16:43:19 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1427, average loss: 2.3702
[09/16 16:43:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.00	
[09/16 16:43:41 visual_prompt]: 	Test 100/407. loss: 2.693, 0.1821 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 16:44:00 visual_prompt]: 	Test 200/407. loss: 2.663, 0.1821 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 16:44:20 visual_prompt]: 	Test 300/407. loss: 2.336, 0.1828 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 16:44:39 visual_prompt]: 	Test 400/407. loss: 2.472, 0.1820 s / batch. (data: 3.84e-05)max mem: 17.22449 GB 
[09/16 16:44:42 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1935, average loss: 2.4230
[09/16 16:44:42 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 62.57	
[09/16 16:44:42 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 16:44:53 visual_prompt]: Epoch 27 / 100: avg data time: 1.49e-01, avg batch time: 0.5569, average train loss: 2.5199
[09/16 16:44:58 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1425, average loss: 2.5475
[09/16 16:44:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/16 16:45:19 visual_prompt]: 	Test 100/407. loss: 3.118, 0.1904 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 16:45:39 visual_prompt]: 	Test 200/407. loss: 2.875, 0.1838 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 16:45:58 visual_prompt]: 	Test 300/407. loss: 2.750, 0.1956 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 16:46:17 visual_prompt]: 	Test 400/407. loss: 2.685, 0.1831 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 16:46:21 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1926, average loss: 2.6832
[09/16 16:46:21 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 54.63	
[09/16 16:46:21 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 16:46:31 visual_prompt]: Epoch 28 / 100: avg data time: 1.52e-01, avg batch time: 0.5538, average train loss: 2.5894
[09/16 16:46:36 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1425, average loss: 2.6015
[09/16 16:46:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.00	top5: 58.00	
[09/16 16:46:58 visual_prompt]: 	Test 100/407. loss: 2.785, 0.1863 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 16:47:17 visual_prompt]: 	Test 200/407. loss: 2.749, 0.1825 s / batch. (data: 1.09e-04)max mem: 17.22449 GB 
[09/16 16:47:36 visual_prompt]: 	Test 300/407. loss: 2.741, 0.1864 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 16:47:56 visual_prompt]: 	Test 400/407. loss: 2.743, 0.1877 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 16:47:59 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1938, average loss: 2.6877
[09/16 16:47:59 visual_prompt]: Classification results with test_vtab-svhn: top1: 10.83	top5: 54.48	
[09/16 16:47:59 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 16:48:10 visual_prompt]: Epoch 29 / 100: avg data time: 1.48e-01, avg batch time: 0.5787, average train loss: 2.4427
[09/16 16:48:15 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1426, average loss: 2.3305
[09/16 16:48:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 56.50	
[09/16 16:48:36 visual_prompt]: 	Test 100/407. loss: 2.501, 0.2071 s / batch. (data: 1.96e-02)max mem: 17.22449 GB 
[09/16 16:48:56 visual_prompt]: 	Test 200/407. loss: 2.406, 0.1828 s / batch. (data: 1.01e-04)max mem: 17.22449 GB 
[09/16 16:49:15 visual_prompt]: 	Test 300/407. loss: 2.473, 0.1827 s / batch. (data: 1.01e-04)max mem: 17.22449 GB 
[09/16 16:49:34 visual_prompt]: 	Test 400/407. loss: 2.372, 0.1820 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 16:49:37 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1930, average loss: 2.3797
[09/16 16:49:38 visual_prompt]: Classification results with test_vtab-svhn: top1: 10.99	top5: 53.30	
[09/16 16:49:38 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 16:49:48 visual_prompt]: Epoch 30 / 100: avg data time: 1.33e-01, avg batch time: 0.5449, average train loss: 2.5191
[09/16 16:49:52 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1425, average loss: 2.6437
[09/16 16:49:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 54.00	
[09/16 16:50:14 visual_prompt]: 	Test 100/407. loss: 2.702, 0.1822 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 16:50:34 visual_prompt]: 	Test 200/407. loss: 2.520, 0.1938 s / batch. (data: 1.11e-02)max mem: 17.22449 GB 
[09/16 16:50:53 visual_prompt]: 	Test 300/407. loss: 2.806, 0.1944 s / batch. (data: 1.48e-04)max mem: 17.22449 GB 
[09/16 16:51:13 visual_prompt]: 	Test 400/407. loss: 2.641, 0.1830 s / batch. (data: 3.39e-05)max mem: 17.22449 GB 
[09/16 16:51:16 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1941, average loss: 2.6490
[09/16 16:51:16 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.38	top5: 48.98	
[09/16 16:51:16 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 16:51:26 visual_prompt]: Epoch 31 / 100: avg data time: 1.51e-01, avg batch time: 0.5521, average train loss: 2.4500
[09/16 16:51:31 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1426, average loss: 2.5335
[09/16 16:51:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 50.00	
[09/16 16:51:53 visual_prompt]: 	Test 100/407. loss: 2.499, 0.1974 s / batch. (data: 1.59e-02)max mem: 17.22449 GB 
[09/16 16:52:12 visual_prompt]: 	Test 200/407. loss: 2.620, 0.1917 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 16:52:32 visual_prompt]: 	Test 300/407. loss: 2.445, 0.1958 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 16:52:51 visual_prompt]: 	Test 400/407. loss: 2.421, 0.1829 s / batch. (data: 3.31e-05)max mem: 17.22449 GB 
[09/16 16:52:55 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1938, average loss: 2.5197
[09/16 16:52:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 53.61	
[09/16 16:52:55 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 16:53:05 visual_prompt]: Epoch 32 / 100: avg data time: 1.45e-01, avg batch time: 0.5472, average train loss: 2.5314
[09/16 16:53:10 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1424, average loss: 2.5034
[09/16 16:53:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 53.50	
[09/16 16:53:32 visual_prompt]: 	Test 100/407. loss: 2.508, 0.1825 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 16:53:51 visual_prompt]: 	Test 200/407. loss: 2.469, 0.1953 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 16:54:10 visual_prompt]: 	Test 300/407. loss: 2.681, 0.1823 s / batch. (data: 9.37e-05)max mem: 17.22449 GB 
[09/16 16:54:30 visual_prompt]: 	Test 400/407. loss: 2.642, 0.1826 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 16:54:33 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1937, average loss: 2.5448
[09/16 16:54:33 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.70	top5: 51.51	
[09/16 16:54:33 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 16:54:44 visual_prompt]: Epoch 33 / 100: avg data time: 1.51e-01, avg batch time: 0.5521, average train loss: 2.4091
[09/16 16:54:48 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1424, average loss: 2.3524
[09/16 16:54:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/16 16:55:10 visual_prompt]: 	Test 100/407. loss: 2.710, 0.1978 s / batch. (data: 1.57e-02)max mem: 17.22449 GB 
[09/16 16:55:29 visual_prompt]: 	Test 200/407. loss: 2.686, 0.2034 s / batch. (data: 2.17e-02)max mem: 17.22449 GB 
[09/16 16:55:49 visual_prompt]: 	Test 300/407. loss: 2.366, 0.2971 s / batch. (data: 2.16e-04)max mem: 17.22449 GB 
[09/16 16:56:08 visual_prompt]: 	Test 400/407. loss: 2.441, 0.1825 s / batch. (data: 3.79e-05)max mem: 17.22449 GB 
[09/16 16:56:11 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1934, average loss: 2.4238
[09/16 16:56:11 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 63.35	
[09/16 16:56:11 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 16:56:22 visual_prompt]: Epoch 34 / 100: avg data time: 1.51e-01, avg batch time: 0.5525, average train loss: 2.4530
[09/16 16:56:26 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1427, average loss: 2.4296
[09/16 16:56:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/16 16:56:48 visual_prompt]: 	Test 100/407. loss: 2.445, 0.2072 s / batch. (data: 2.56e-02)max mem: 17.22449 GB 
[09/16 16:57:07 visual_prompt]: 	Test 200/407. loss: 2.495, 0.2004 s / batch. (data: 1.15e-04)max mem: 17.22449 GB 
[09/16 16:57:27 visual_prompt]: 	Test 300/407. loss: 2.438, 0.1957 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 16:57:46 visual_prompt]: 	Test 400/407. loss: 2.425, 0.1819 s / batch. (data: 3.48e-05)max mem: 17.22449 GB 
[09/16 16:57:50 visual_prompt]: Inference (test):avg data time: 7.44e-03, avg batch time: 0.1931, average loss: 2.4130
[09/16 16:57:50 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 59.74	
[09/16 16:57:50 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 16:58:00 visual_prompt]: Epoch 35 / 100: avg data time: 1.46e-01, avg batch time: 0.5500, average train loss: 2.5063
[09/16 16:58:05 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1427, average loss: 2.5259
[09/16 16:58:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.50	
[09/16 16:58:27 visual_prompt]: 	Test 100/407. loss: 2.734, 0.1891 s / batch. (data: 1.65e-04)max mem: 17.22449 GB 
[09/16 16:58:46 visual_prompt]: 	Test 200/407. loss: 2.865, 0.1826 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 16:59:05 visual_prompt]: 	Test 300/407. loss: 2.260, 0.1841 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 16:59:24 visual_prompt]: 	Test 400/407. loss: 2.542, 0.1826 s / batch. (data: 3.31e-05)max mem: 17.22449 GB 
[09/16 16:59:28 visual_prompt]: Inference (test):avg data time: 7.10e-03, avg batch time: 0.1923, average loss: 2.5736
[09/16 16:59:28 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.76	top5: 61.95	
[09/16 16:59:28 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 16:59:38 visual_prompt]: Epoch 36 / 100: avg data time: 1.43e-01, avg batch time: 0.5500, average train loss: 2.4679
[09/16 16:59:43 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1423, average loss: 2.3304
[09/16 16:59:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.00	
[09/16 17:00:05 visual_prompt]: 	Test 100/407. loss: 2.281, 0.1960 s / batch. (data: 1.45e-02)max mem: 17.22449 GB 
[09/16 17:00:24 visual_prompt]: 	Test 200/407. loss: 2.364, 0.1929 s / batch. (data: 1.13e-02)max mem: 17.22449 GB 
[09/16 17:00:44 visual_prompt]: 	Test 300/407. loss: 2.242, 0.2152 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 17:01:03 visual_prompt]: 	Test 400/407. loss: 2.337, 0.1828 s / batch. (data: 3.41e-05)max mem: 17.22449 GB 
[09/16 17:01:07 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1948, average loss: 2.3160
[09/16 17:01:07 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 57.19	
[09/16 17:01:07 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 17:01:17 visual_prompt]: Epoch 37 / 100: avg data time: 1.44e-01, avg batch time: 0.5472, average train loss: 2.4261
[09/16 17:01:22 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1427, average loss: 2.3411
[09/16 17:01:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 60.00	
[09/16 17:01:43 visual_prompt]: 	Test 100/407. loss: 2.492, 0.1880 s / batch. (data: 6.40e-03)max mem: 17.22449 GB 
[09/16 17:02:04 visual_prompt]: 	Test 200/407. loss: 2.450, 0.1918 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 17:02:23 visual_prompt]: 	Test 300/407. loss: 2.325, 0.1823 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 17:02:42 visual_prompt]: 	Test 400/407. loss: 2.324, 0.1973 s / batch. (data: 3.72e-05)max mem: 17.22449 GB 
[09/16 17:02:45 visual_prompt]: Inference (test):avg data time: 6.78e-03, avg batch time: 0.1945, average loss: 2.3884
[09/16 17:02:46 visual_prompt]: Classification results with test_vtab-svhn: top1: 8.43	top5: 55.70	
[09/16 17:02:46 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 17:02:56 visual_prompt]: Epoch 38 / 100: avg data time: 1.33e-01, avg batch time: 0.5388, average train loss: 2.5011
[09/16 17:03:00 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1426, average loss: 2.4606
[09/16 17:03:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/16 17:03:22 visual_prompt]: 	Test 100/407. loss: 2.628, 0.1824 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 17:03:41 visual_prompt]: 	Test 200/407. loss: 2.764, 0.1965 s / batch. (data: 1.50e-02)max mem: 17.22449 GB 
[09/16 17:04:01 visual_prompt]: 	Test 300/407. loss: 2.234, 0.1971 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 17:04:20 visual_prompt]: 	Test 400/407. loss: 2.390, 0.1819 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 17:04:23 visual_prompt]: Inference (test):avg data time: 8.54e-03, avg batch time: 0.1931, average loss: 2.4773
[09/16 17:04:23 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 62.00	
[09/16 17:04:23 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 17:04:34 visual_prompt]: Epoch 39 / 100: avg data time: 1.48e-01, avg batch time: 0.5836, average train loss: 2.4584
[09/16 17:04:39 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1463, average loss: 3.2796
[09/16 17:04:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/16 17:05:01 visual_prompt]: 	Test 100/407. loss: 4.058, 0.1827 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 17:05:20 visual_prompt]: 	Test 200/407. loss: 3.767, 0.1880 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 17:05:40 visual_prompt]: 	Test 300/407. loss: 3.509, 0.1882 s / batch. (data: 1.61e-04)max mem: 17.22449 GB 
[09/16 17:05:59 visual_prompt]: 	Test 400/407. loss: 3.589, 0.1827 s / batch. (data: 3.89e-05)max mem: 17.22449 GB 
[09/16 17:06:02 visual_prompt]: Inference (test):avg data time: 6.72e-03, avg batch time: 0.1932, average loss: 3.4105
[09/16 17:06:02 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 53.43	
[09/16 17:06:02 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 17:06:13 visual_prompt]: Epoch 40 / 100: avg data time: 1.59e-01, avg batch time: 0.5604, average train loss: 3.4911
[09/16 17:06:17 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1423, average loss: 4.1397
[09/16 17:06:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 42.50	
[09/16 17:06:39 visual_prompt]: 	Test 100/407. loss: 4.110, 0.1823 s / batch. (data: 1.00e-04)max mem: 17.22449 GB 
[09/16 17:06:59 visual_prompt]: 	Test 200/407. loss: 4.439, 0.1974 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 17:07:18 visual_prompt]: 	Test 300/407. loss: 3.967, 0.1962 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 17:07:38 visual_prompt]: 	Test 400/407. loss: 4.483, 0.1835 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 17:07:41 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1939, average loss: 4.1818
[09/16 17:07:41 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 41.06	
[09/16 17:07:41 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 17:07:52 visual_prompt]: Epoch 41 / 100: avg data time: 1.51e-01, avg batch time: 0.5506, average train loss: 3.1485
[09/16 17:07:56 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1426, average loss: 2.6152
[09/16 17:07:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 61.00	
[09/16 17:08:18 visual_prompt]: 	Test 100/407. loss: 2.711, 0.1901 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 17:08:37 visual_prompt]: 	Test 200/407. loss: 2.765, 0.1825 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 17:08:56 visual_prompt]: 	Test 300/407. loss: 2.663, 0.1982 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 17:09:16 visual_prompt]: 	Test 400/407. loss: 2.678, 0.1828 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 17:09:19 visual_prompt]: Inference (test):avg data time: 8.11e-03, avg batch time: 0.1936, average loss: 2.6849
[09/16 17:09:19 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 57.26	
[09/16 17:09:19 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 17:09:30 visual_prompt]: Epoch 42 / 100: avg data time: 1.55e-01, avg batch time: 0.5559, average train loss: 2.5953
[09/16 17:09:34 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1425, average loss: 2.4435
[09/16 17:09:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/16 17:09:56 visual_prompt]: 	Test 100/407. loss: 2.546, 0.1824 s / batch. (data: 4.22e-05)max mem: 17.22449 GB 
[09/16 17:10:15 visual_prompt]: 	Test 200/407. loss: 2.531, 0.1968 s / batch. (data: 1.50e-02)max mem: 17.22449 GB 
[09/16 17:10:35 visual_prompt]: 	Test 300/407. loss: 2.431, 0.2022 s / batch. (data: 1.64e-02)max mem: 17.22449 GB 
[09/16 17:10:54 visual_prompt]: 	Test 400/407. loss: 2.395, 0.1824 s / batch. (data: 3.81e-05)max mem: 17.22449 GB 
[09/16 17:10:57 visual_prompt]: Inference (test):avg data time: 6.85e-03, avg batch time: 0.1932, average loss: 2.4448
[09/16 17:10:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 63.05	
[09/16 17:10:58 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 17:11:08 visual_prompt]: Epoch 43 / 100: avg data time: 1.47e-01, avg batch time: 0.5482, average train loss: 2.4956
[09/16 17:11:13 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1426, average loss: 2.7379
[09/16 17:11:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 39.50	
[09/16 17:11:35 visual_prompt]: 	Test 100/407. loss: 2.671, 0.1962 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 17:11:55 visual_prompt]: 	Test 200/407. loss: 2.709, 0.1850 s / batch. (data: 4.67e-05)max mem: 17.22449 GB 
[09/16 17:12:14 visual_prompt]: 	Test 300/407. loss: 2.604, 0.2077 s / batch. (data: 2.59e-02)max mem: 17.22449 GB 
[09/16 17:12:33 visual_prompt]: 	Test 400/407. loss: 2.669, 0.1820 s / batch. (data: 2.81e-05)max mem: 17.22449 GB 
[09/16 17:12:37 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1941, average loss: 2.6974
[09/16 17:12:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 6.52	top5: 42.90	
[09/16 17:12:37 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 17:12:47 visual_prompt]: Epoch 44 / 100: avg data time: 1.43e-01, avg batch time: 0.5458, average train loss: 2.7080
[09/16 17:12:51 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1427, average loss: 2.5212
[09/16 17:12:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/16 17:13:13 visual_prompt]: 	Test 100/407. loss: 2.955, 0.2026 s / batch. (data: 2.04e-02)max mem: 17.22449 GB 
[09/16 17:13:33 visual_prompt]: 	Test 200/407. loss: 2.889, 0.2066 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 17:13:52 visual_prompt]: 	Test 300/407. loss: 2.417, 0.1927 s / batch. (data: 1.56e-04)max mem: 17.22449 GB 
[09/16 17:14:12 visual_prompt]: 	Test 400/407. loss: 2.536, 0.1830 s / batch. (data: 3.65e-05)max mem: 17.22449 GB 
[09/16 17:14:15 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1941, average loss: 2.5643
[09/16 17:14:15 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 61.05	
[09/16 17:14:15 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 17:14:25 visual_prompt]: Epoch 45 / 100: avg data time: 1.51e-01, avg batch time: 0.5520, average train loss: 2.5883
[09/16 17:14:30 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1425, average loss: 2.5911
[09/16 17:14:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/16 17:14:51 visual_prompt]: 	Test 100/407. loss: 2.902, 0.2081 s / batch. (data: 2.64e-02)max mem: 17.22449 GB 
[09/16 17:15:11 visual_prompt]: 	Test 200/407. loss: 2.880, 0.1972 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 17:15:30 visual_prompt]: 	Test 300/407. loss: 2.590, 0.1846 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 17:15:50 visual_prompt]: 	Test 400/407. loss: 2.626, 0.1826 s / batch. (data: 4.17e-05)max mem: 17.22449 GB 
[09/16 17:15:53 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1932, average loss: 2.5856
[09/16 17:15:53 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.50	top5: 62.99	
[09/16 17:15:53 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 17:16:04 visual_prompt]: Epoch 46 / 100: avg data time: 1.56e-01, avg batch time: 0.5619, average train loss: 2.4436
[09/16 17:16:08 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1447, average loss: 2.5456
[09/16 17:16:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/16 17:16:30 visual_prompt]: 	Test 100/407. loss: 3.041, 0.1966 s / batch. (data: 1.48e-02)max mem: 17.22449 GB 
[09/16 17:16:49 visual_prompt]: 	Test 200/407. loss: 2.964, 0.1941 s / batch. (data: 3.75e-03)max mem: 17.22449 GB 
[09/16 17:17:08 visual_prompt]: 	Test 300/407. loss: 2.496, 0.1953 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 17:17:28 visual_prompt]: 	Test 400/407. loss: 2.750, 0.1825 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 17:17:31 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1926, average loss: 2.6465
[09/16 17:17:31 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 53.79	
[09/16 17:17:31 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 17:17:42 visual_prompt]: Epoch 47 / 100: avg data time: 1.53e-01, avg batch time: 0.5559, average train loss: 2.3785
[09/16 17:17:46 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1424, average loss: 2.3675
[09/16 17:17:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.50	
[09/16 17:18:08 visual_prompt]: 	Test 100/407. loss: 2.624, 0.2074 s / batch. (data: 2.54e-02)max mem: 17.22449 GB 
[09/16 17:18:27 visual_prompt]: 	Test 200/407. loss: 2.584, 0.1884 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 17:18:47 visual_prompt]: 	Test 300/407. loss: 2.388, 0.1832 s / batch. (data: 1.48e-04)max mem: 17.22449 GB 
[09/16 17:19:07 visual_prompt]: 	Test 400/407. loss: 2.426, 0.1822 s / batch. (data: 3.41e-05)max mem: 17.22449 GB 
[09/16 17:19:10 visual_prompt]: Inference (test):avg data time: 8.27e-03, avg batch time: 0.1944, average loss: 2.4144
[09/16 17:19:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 54.33	
[09/16 17:19:10 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 17:19:20 visual_prompt]: Epoch 48 / 100: avg data time: 1.52e-01, avg batch time: 0.5529, average train loss: 2.3244
[09/16 17:19:25 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1424, average loss: 2.3929
[09/16 17:19:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/16 17:19:46 visual_prompt]: 	Test 100/407. loss: 2.634, 0.2088 s / batch. (data: 2.70e-02)max mem: 17.22449 GB 
[09/16 17:20:06 visual_prompt]: 	Test 200/407. loss: 2.573, 0.1828 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 17:20:25 visual_prompt]: 	Test 300/407. loss: 2.294, 0.1827 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 17:20:45 visual_prompt]: 	Test 400/407. loss: 2.421, 0.1962 s / batch. (data: 4.94e-05)max mem: 17.22449 GB 
[09/16 17:20:48 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1932, average loss: 2.3974
[09/16 17:20:48 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 60.89	
[09/16 17:20:48 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 17:20:58 visual_prompt]: Epoch 49 / 100: avg data time: 1.53e-01, avg batch time: 0.5539, average train loss: 2.4053
[09/16 17:21:03 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1424, average loss: 2.2986
[09/16 17:21:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/16 17:21:25 visual_prompt]: 	Test 100/407. loss: 2.321, 0.1966 s / batch. (data: 1.45e-02)max mem: 17.22449 GB 
[09/16 17:21:44 visual_prompt]: 	Test 200/407. loss: 2.383, 0.2177 s / batch. (data: 3.56e-02)max mem: 17.22449 GB 
[09/16 17:22:03 visual_prompt]: 	Test 300/407. loss: 2.186, 0.1977 s / batch. (data: 1.65e-04)max mem: 17.22449 GB 
[09/16 17:22:23 visual_prompt]: 	Test 400/407. loss: 2.229, 0.1825 s / batch. (data: 2.53e-05)max mem: 17.22449 GB 
[09/16 17:22:26 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1934, average loss: 2.2759
[09/16 17:22:26 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 65.46	
[09/16 17:22:26 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 17:22:37 visual_prompt]: Epoch 50 / 100: avg data time: 1.50e-01, avg batch time: 0.5517, average train loss: 2.3591
[09/16 17:22:41 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1425, average loss: 2.3826
[09/16 17:22:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.00	top5: 58.50	
[09/16 17:23:03 visual_prompt]: 	Test 100/407. loss: 2.514, 0.1955 s / batch. (data: 1.28e-02)max mem: 17.22449 GB 
[09/16 17:23:22 visual_prompt]: 	Test 200/407. loss: 2.559, 0.1825 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 17:23:42 visual_prompt]: 	Test 300/407. loss: 2.264, 0.1834 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 17:24:01 visual_prompt]: 	Test 400/407. loss: 2.398, 0.1831 s / batch. (data: 2.96e-05)max mem: 17.22449 GB 
[09/16 17:24:04 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1930, average loss: 2.3996
[09/16 17:24:04 visual_prompt]: Classification results with test_vtab-svhn: top1: 17.06	top5: 57.58	
[09/16 17:24:04 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 17:24:15 visual_prompt]: Epoch 51 / 100: avg data time: 1.51e-01, avg batch time: 0.5543, average train loss: 2.3582
[09/16 17:24:19 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1425, average loss: 2.4110
[09/16 17:24:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/16 17:24:42 visual_prompt]: 	Test 100/407. loss: 2.584, 0.1830 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 17:25:02 visual_prompt]: 	Test 200/407. loss: 2.634, 0.1965 s / batch. (data: 1.45e-02)max mem: 17.22449 GB 
[09/16 17:25:21 visual_prompt]: 	Test 300/407. loss: 2.306, 0.1821 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 17:25:40 visual_prompt]: 	Test 400/407. loss: 2.347, 0.1827 s / batch. (data: 4.20e-05)max mem: 17.22449 GB 
[09/16 17:25:44 visual_prompt]: Inference (test):avg data time: 8.74e-03, avg batch time: 0.1957, average loss: 2.4288
[09/16 17:25:44 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 63.51	
[09/16 17:25:44 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 17:25:54 visual_prompt]: Epoch 52 / 100: avg data time: 1.58e-01, avg batch time: 0.5593, average train loss: 2.3410
[09/16 17:25:59 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1425, average loss: 2.3231
[09/16 17:25:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/16 17:26:21 visual_prompt]: 	Test 100/407. loss: 2.601, 0.1881 s / batch. (data: 5.95e-03)max mem: 17.22449 GB 
[09/16 17:26:40 visual_prompt]: 	Test 200/407. loss: 2.464, 0.1832 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 17:26:59 visual_prompt]: 	Test 300/407. loss: 2.418, 0.2080 s / batch. (data: 2.59e-02)max mem: 17.22449 GB 
[09/16 17:27:19 visual_prompt]: 	Test 400/407. loss: 2.426, 0.1826 s / batch. (data: 3.65e-05)max mem: 17.22449 GB 
[09/16 17:27:22 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1932, average loss: 2.3724
[09/16 17:27:22 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 54.64	
[09/16 17:27:22 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 17:27:32 visual_prompt]: Epoch 53 / 100: avg data time: 1.49e-01, avg batch time: 0.5520, average train loss: 2.3977
[09/16 17:27:37 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1426, average loss: 2.4661
[09/16 17:27:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/16 17:27:58 visual_prompt]: 	Test 100/407. loss: 2.641, 0.1825 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 17:28:18 visual_prompt]: 	Test 200/407. loss: 2.703, 0.2133 s / batch. (data: 3.17e-02)max mem: 17.22449 GB 
[09/16 17:28:37 visual_prompt]: 	Test 300/407. loss: 2.425, 0.1968 s / batch. (data: 1.48e-02)max mem: 17.22449 GB 
[09/16 17:28:57 visual_prompt]: 	Test 400/407. loss: 2.562, 0.1826 s / batch. (data: 3.31e-05)max mem: 17.22449 GB 
[09/16 17:29:00 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1935, average loss: 2.4823
[09/16 17:29:00 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 61.97	
[09/16 17:29:00 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 17:29:11 visual_prompt]: Epoch 54 / 100: avg data time: 1.55e-01, avg batch time: 0.5577, average train loss: 2.3889
[09/16 17:29:15 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1426, average loss: 2.3598
[09/16 17:29:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 58.00	
[09/16 17:29:37 visual_prompt]: 	Test 100/407. loss: 2.386, 0.1865 s / batch. (data: 1.08e-04)max mem: 17.22449 GB 
[09/16 17:29:56 visual_prompt]: 	Test 200/407. loss: 2.372, 0.1826 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 17:30:16 visual_prompt]: 	Test 300/407. loss: 2.422, 0.1951 s / batch. (data: 1.27e-02)max mem: 17.22449 GB 
[09/16 17:30:35 visual_prompt]: 	Test 400/407. loss: 2.378, 0.1829 s / batch. (data: 3.58e-05)max mem: 17.22449 GB 
[09/16 17:30:39 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1931, average loss: 2.3989
[09/16 17:30:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 8.79	top5: 54.48	
[09/16 17:30:39 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 17:30:49 visual_prompt]: Epoch 55 / 100: avg data time: 1.58e-01, avg batch time: 0.5574, average train loss: 2.3297
[09/16 17:30:54 visual_prompt]: Inference (val):avg data time: 4.97e-05, avg batch time: 0.1491, average loss: 2.3228
[09/16 17:30:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.00	top5: 63.00	
[09/16 17:31:16 visual_prompt]: 	Test 100/407. loss: 2.537, 0.1881 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 17:31:35 visual_prompt]: 	Test 200/407. loss: 2.552, 0.1947 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 17:31:54 visual_prompt]: 	Test 300/407. loss: 2.286, 0.1952 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 17:32:14 visual_prompt]: 	Test 400/407. loss: 2.435, 0.1832 s / batch. (data: 3.29e-05)max mem: 17.22449 GB 
[09/16 17:32:17 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1937, average loss: 2.3554
[09/16 17:32:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 16.74	top5: 62.45	
[09/16 17:32:17 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 17:32:28 visual_prompt]: Epoch 56 / 100: avg data time: 1.49e-01, avg batch time: 0.5521, average train loss: 2.3825
[09/16 17:32:32 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1427, average loss: 2.4019
[09/16 17:32:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/16 17:32:54 visual_prompt]: 	Test 100/407. loss: 2.538, 0.1828 s / batch. (data: 9.85e-05)max mem: 17.22449 GB 
[09/16 17:33:14 visual_prompt]: 	Test 200/407. loss: 2.608, 0.1962 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 17:33:33 visual_prompt]: 	Test 300/407. loss: 2.363, 0.1824 s / batch. (data: 3.67e-05)max mem: 17.22449 GB 
[09/16 17:33:52 visual_prompt]: 	Test 400/407. loss: 2.563, 0.1826 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 17:33:56 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1932, average loss: 2.4596
[09/16 17:33:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 56.13	
[09/16 17:33:56 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 17:34:06 visual_prompt]: Epoch 57 / 100: avg data time: 1.52e-01, avg batch time: 0.5552, average train loss: 2.3666
[09/16 17:34:11 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1426, average loss: 2.2641
[09/16 17:34:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/16 17:34:33 visual_prompt]: 	Test 100/407. loss: 2.567, 0.2235 s / batch. (data: 2.61e-02)max mem: 17.22449 GB 
[09/16 17:34:52 visual_prompt]: 	Test 200/407. loss: 2.522, 0.1959 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 17:35:12 visual_prompt]: 	Test 300/407. loss: 2.313, 0.2027 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 17:35:31 visual_prompt]: 	Test 400/407. loss: 2.367, 0.1827 s / batch. (data: 3.29e-05)max mem: 17.22449 GB 
[09/16 17:35:35 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1938, average loss: 2.3180
[09/16 17:35:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 65.45	
[09/16 17:35:35 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 17:35:45 visual_prompt]: Epoch 58 / 100: avg data time: 1.58e-01, avg batch time: 0.5601, average train loss: 2.3175
[09/16 17:35:50 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1426, average loss: 2.4838
[09/16 17:35:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/16 17:36:12 visual_prompt]: 	Test 100/407. loss: 2.599, 0.1835 s / batch. (data: 1.52e-04)max mem: 17.22449 GB 
[09/16 17:36:31 visual_prompt]: 	Test 200/407. loss: 2.675, 0.1971 s / batch. (data: 1.48e-02)max mem: 17.22449 GB 
[09/16 17:36:51 visual_prompt]: 	Test 300/407. loss: 2.480, 0.1957 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 17:37:10 visual_prompt]: 	Test 400/407. loss: 2.518, 0.1830 s / batch. (data: 3.00e-05)max mem: 17.22449 GB 
[09/16 17:37:13 visual_prompt]: Inference (test):avg data time: 8.31e-03, avg batch time: 0.1941, average loss: 2.4763
[09/16 17:37:13 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 61.07	
[09/16 17:37:13 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 17:37:24 visual_prompt]: Epoch 59 / 100: avg data time: 1.52e-01, avg batch time: 0.5557, average train loss: 2.3710
[09/16 17:37:29 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1426, average loss: 2.3259
[09/16 17:37:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/16 17:37:50 visual_prompt]: 	Test 100/407. loss: 2.683, 0.1979 s / batch. (data: 1.07e-04)max mem: 17.22449 GB 
[09/16 17:38:10 visual_prompt]: 	Test 200/407. loss: 2.559, 0.1830 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 17:38:29 visual_prompt]: 	Test 300/407. loss: 2.445, 0.1822 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 17:38:49 visual_prompt]: 	Test 400/407. loss: 2.342, 0.1822 s / batch. (data: 3.96e-05)max mem: 17.22449 GB 
[09/16 17:38:52 visual_prompt]: Inference (test):avg data time: 8.43e-03, avg batch time: 0.1938, average loss: 2.4048
[09/16 17:38:52 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 54.81	
[09/16 17:38:52 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 17:39:03 visual_prompt]: Epoch 60 / 100: avg data time: 1.43e-01, avg batch time: 0.5446, average train loss: 2.3298
[09/16 17:39:07 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1425, average loss: 2.2533
[09/16 17:39:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.50	top5: 67.00	
[09/16 17:39:29 visual_prompt]: 	Test 100/407. loss: 2.370, 0.1983 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 17:39:48 visual_prompt]: 	Test 200/407. loss: 2.408, 0.1975 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 17:40:08 visual_prompt]: 	Test 300/407. loss: 2.187, 0.1937 s / batch. (data: 3.81e-05)max mem: 17.22449 GB 
[09/16 17:40:27 visual_prompt]: 	Test 400/407. loss: 2.238, 0.1830 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 17:40:31 visual_prompt]: Inference (test):avg data time: 8.49e-03, avg batch time: 0.1940, average loss: 2.2615
[09/16 17:40:31 visual_prompt]: Classification results with test_vtab-svhn: top1: 25.30	top5: 65.52	
[09/16 17:40:31 visual_prompt]: Best epoch 60: best metric: 0.275
[09/16 17:40:31 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 17:40:41 visual_prompt]: Epoch 61 / 100: avg data time: 1.55e-01, avg batch time: 0.5588, average train loss: 2.3231
[09/16 17:40:46 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1424, average loss: 2.3634
[09/16 17:40:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/16 17:41:08 visual_prompt]: 	Test 100/407. loss: 2.436, 0.1829 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 17:41:27 visual_prompt]: 	Test 200/407. loss: 2.400, 0.2071 s / batch. (data: 2.24e-02)max mem: 17.22449 GB 
[09/16 17:41:46 visual_prompt]: 	Test 300/407. loss: 2.461, 0.1829 s / batch. (data: 1.15e-04)max mem: 17.22449 GB 
[09/16 17:42:05 visual_prompt]: 	Test 400/407. loss: 2.382, 0.1825 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 17:42:09 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1922, average loss: 2.3674
[09/16 17:42:09 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 61.95	
[09/16 17:42:09 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 17:42:19 visual_prompt]: Epoch 62 / 100: avg data time: 1.45e-01, avg batch time: 0.5481, average train loss: 2.2872
[09/16 17:42:24 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1425, average loss: 2.2992
[09/16 17:42:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/16 17:42:45 visual_prompt]: 	Test 100/407. loss: 2.723, 0.1826 s / batch. (data: 1.31e-04)max mem: 17.22449 GB 
[09/16 17:43:05 visual_prompt]: 	Test 200/407. loss: 2.614, 0.1838 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 17:43:24 visual_prompt]: 	Test 300/407. loss: 2.446, 0.1829 s / batch. (data: 1.11e-04)max mem: 17.22449 GB 
[09/16 17:43:44 visual_prompt]: 	Test 400/407. loss: 2.504, 0.1823 s / batch. (data: 6.96e-05)max mem: 17.22449 GB 
[09/16 17:43:47 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1932, average loss: 2.4044
[09/16 17:43:47 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.66	top5: 56.02	
[09/16 17:43:47 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 17:43:57 visual_prompt]: Epoch 63 / 100: avg data time: 1.48e-01, avg batch time: 0.5519, average train loss: 2.3865
[09/16 17:44:02 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1427, average loss: 2.4261
[09/16 17:44:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/16 17:44:24 visual_prompt]: 	Test 100/407. loss: 2.650, 0.1942 s / batch. (data: 1.24e-02)max mem: 17.22449 GB 
[09/16 17:44:43 visual_prompt]: 	Test 200/407. loss: 2.601, 0.1834 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 17:45:02 visual_prompt]: 	Test 300/407. loss: 2.378, 0.1827 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 17:45:22 visual_prompt]: 	Test 400/407. loss: 2.427, 0.1824 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 17:45:25 visual_prompt]: Inference (test):avg data time: 8.03e-03, avg batch time: 0.1934, average loss: 2.4823
[09/16 17:45:25 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 62.90	
[09/16 17:45:25 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 17:45:36 visual_prompt]: Epoch 64 / 100: avg data time: 1.57e-01, avg batch time: 0.5580, average train loss: 2.4772
[09/16 17:45:40 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1427, average loss: 2.3566
[09/16 17:45:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.50	
[09/16 17:46:03 visual_prompt]: 	Test 100/407. loss: 2.421, 0.1953 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 17:46:23 visual_prompt]: 	Test 200/407. loss: 2.474, 0.1887 s / batch. (data: 1.40e-04)max mem: 17.22449 GB 
[09/16 17:46:42 visual_prompt]: 	Test 300/407. loss: 2.245, 0.1829 s / batch. (data: 9.80e-05)max mem: 17.22449 GB 
[09/16 17:47:02 visual_prompt]: 	Test 400/407. loss: 2.328, 0.1825 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 17:47:05 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1959, average loss: 2.3842
[09/16 17:47:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.76	top5: 61.95	
[09/16 17:47:05 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 17:47:16 visual_prompt]: Epoch 65 / 100: avg data time: 1.58e-01, avg batch time: 0.5586, average train loss: 2.3695
[09/16 17:47:20 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1425, average loss: 2.2514
[09/16 17:47:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.00	top5: 66.50	
[09/16 17:47:42 visual_prompt]: 	Test 100/407. loss: 2.314, 0.1822 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 17:48:02 visual_prompt]: 	Test 200/407. loss: 2.383, 0.2075 s / batch. (data: 2.53e-02)max mem: 17.22449 GB 
[09/16 17:48:21 visual_prompt]: 	Test 300/407. loss: 2.197, 0.1952 s / batch. (data: 1.25e-02)max mem: 17.22449 GB 
[09/16 17:48:40 visual_prompt]: 	Test 400/407. loss: 2.257, 0.1938 s / batch. (data: 3.48e-05)max mem: 17.22449 GB 
[09/16 17:48:44 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1931, average loss: 2.2657
[09/16 17:48:44 visual_prompt]: Classification results with test_vtab-svhn: top1: 22.44	top5: 66.93	
[09/16 17:48:44 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 17:48:54 visual_prompt]: Epoch 66 / 100: avg data time: 1.60e-01, avg batch time: 0.5608, average train loss: 2.3403
[09/16 17:48:59 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1422, average loss: 2.2821
[09/16 17:48:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/16 17:49:21 visual_prompt]: 	Test 100/407. loss: 2.321, 0.1966 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 17:49:40 visual_prompt]: 	Test 200/407. loss: 2.402, 0.1956 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 17:50:00 visual_prompt]: 	Test 300/407. loss: 2.167, 0.1958 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 17:50:19 visual_prompt]: 	Test 400/407. loss: 2.216, 0.1830 s / batch. (data: 3.81e-05)max mem: 17.22449 GB 
[09/16 17:50:23 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1942, average loss: 2.2825
[09/16 17:50:23 visual_prompt]: Classification results with test_vtab-svhn: top1: 16.03	top5: 64.41	
[09/16 17:50:23 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 17:50:33 visual_prompt]: Epoch 67 / 100: avg data time: 1.59e-01, avg batch time: 0.5594, average train loss: 2.2389
[09/16 17:50:38 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1423, average loss: 2.1601
[09/16 17:50:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.50	top5: 62.50	
[09/16 17:50:59 visual_prompt]: 	Test 100/407. loss: 2.371, 0.1958 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 17:51:19 visual_prompt]: 	Test 200/407. loss: 2.380, 0.1893 s / batch. (data: 4.59e-03)max mem: 17.22449 GB 
[09/16 17:51:38 visual_prompt]: 	Test 300/407. loss: 2.177, 0.1947 s / batch. (data: 1.23e-02)max mem: 17.22449 GB 
[09/16 17:51:57 visual_prompt]: 	Test 400/407. loss: 2.294, 0.1823 s / batch. (data: 2.79e-05)max mem: 17.22449 GB 
[09/16 17:52:01 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1931, average loss: 2.2330
[09/16 17:52:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 25.09	top5: 58.07	
[09/16 17:52:01 visual_prompt]: Best epoch 67: best metric: 0.295
[09/16 17:52:01 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 17:52:11 visual_prompt]: Epoch 68 / 100: avg data time: 1.58e-01, avg batch time: 0.5576, average train loss: 2.2735
[09/16 17:52:16 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1431, average loss: 2.1673
[09/16 17:52:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 66.00	
[09/16 17:52:37 visual_prompt]: 	Test 100/407. loss: 2.380, 0.1825 s / batch. (data: 4.72e-05)max mem: 17.22449 GB 
[09/16 17:52:57 visual_prompt]: 	Test 200/407. loss: 2.383, 0.1830 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 17:53:16 visual_prompt]: 	Test 300/407. loss: 2.189, 0.1969 s / batch. (data: 1.45e-02)max mem: 17.22449 GB 
[09/16 17:53:35 visual_prompt]: 	Test 400/407. loss: 2.311, 0.1827 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 17:53:39 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1925, average loss: 2.2237
[09/16 17:53:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 25.30	top5: 64.79	
[09/16 17:53:39 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 17:53:49 visual_prompt]: Epoch 69 / 100: avg data time: 1.61e-01, avg batch time: 0.5608, average train loss: 2.2357
[09/16 17:53:54 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1424, average loss: 2.2532
[09/16 17:53:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.00	top5: 65.50	
[09/16 17:54:16 visual_prompt]: 	Test 100/407. loss: 2.371, 0.1821 s / batch. (data: 9.70e-05)max mem: 17.22449 GB 
[09/16 17:54:35 visual_prompt]: 	Test 200/407. loss: 2.338, 0.1830 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 17:54:55 visual_prompt]: 	Test 300/407. loss: 2.335, 0.2121 s / batch. (data: 1.55e-02)max mem: 17.22449 GB 
[09/16 17:55:14 visual_prompt]: 	Test 400/407. loss: 2.304, 0.1829 s / batch. (data: 2.46e-05)max mem: 17.22449 GB 
[09/16 17:55:17 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1943, average loss: 2.2697
[09/16 17:55:18 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.24	top5: 62.26	
[09/16 17:55:18 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 17:55:28 visual_prompt]: Epoch 70 / 100: avg data time: 1.52e-01, avg batch time: 0.5572, average train loss: 2.2381
[09/16 17:55:33 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1425, average loss: 2.1100
[09/16 17:55:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.50	top5: 67.50	
[09/16 17:55:55 visual_prompt]: 	Test 100/407. loss: 2.264, 0.1964 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 17:56:14 visual_prompt]: 	Test 200/407. loss: 2.283, 0.1818 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 17:56:34 visual_prompt]: 	Test 300/407. loss: 2.162, 0.2075 s / batch. (data: 2.58e-02)max mem: 17.22449 GB 
[09/16 17:56:53 visual_prompt]: 	Test 400/407. loss: 2.211, 0.1826 s / batch. (data: 3.55e-05)max mem: 17.22449 GB 
[09/16 17:56:56 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1944, average loss: 2.1557
[09/16 17:56:57 visual_prompt]: Classification results with test_vtab-svhn: top1: 24.74	top5: 65.43	
[09/16 17:56:57 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 17:57:07 visual_prompt]: Epoch 71 / 100: avg data time: 1.52e-01, avg batch time: 0.5529, average train loss: 2.1742
[09/16 17:57:12 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1425, average loss: 2.0975
[09/16 17:57:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.50	top5: 74.00	
[09/16 17:57:34 visual_prompt]: 	Test 100/407. loss: 2.261, 0.1820 s / batch. (data: 1.08e-04)max mem: 17.22449 GB 
[09/16 17:57:53 visual_prompt]: 	Test 200/407. loss: 2.183, 0.1964 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 17:58:13 visual_prompt]: 	Test 300/407. loss: 2.236, 0.2297 s / batch. (data: 2.18e-04)max mem: 17.22449 GB 
[09/16 17:58:32 visual_prompt]: 	Test 400/407. loss: 2.194, 0.1834 s / batch. (data: 4.05e-05)max mem: 17.22449 GB 
[09/16 17:58:35 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1948, average loss: 2.1249
[09/16 17:58:36 visual_prompt]: Classification results with test_vtab-svhn: top1: 14.72	top5: 72.41	
[09/16 17:58:36 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 17:58:46 visual_prompt]: Epoch 72 / 100: avg data time: 1.47e-01, avg batch time: 0.5484, average train loss: 2.1042
[09/16 17:58:51 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1422, average loss: 1.9440
[09/16 17:58:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.50	top5: 76.00	
[09/16 17:59:12 visual_prompt]: 	Test 100/407. loss: 2.214, 0.1969 s / batch. (data: 1.55e-02)max mem: 17.22449 GB 
[09/16 17:59:32 visual_prompt]: 	Test 200/407. loss: 2.268, 0.1827 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 17:59:51 visual_prompt]: 	Test 300/407. loss: 1.923, 0.1918 s / batch. (data: 1.09e-04)max mem: 17.22449 GB 
[09/16 18:00:10 visual_prompt]: 	Test 400/407. loss: 2.076, 0.1827 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 18:00:14 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1931, average loss: 2.0213
[09/16 18:00:14 visual_prompt]: Classification results with test_vtab-svhn: top1: 32.04	top5: 75.12	
[09/16 18:00:14 visual_prompt]: Best epoch 72: best metric: 0.355
[09/16 18:00:14 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 18:00:24 visual_prompt]: Epoch 73 / 100: avg data time: 1.42e-01, avg batch time: 0.5487, average train loss: 2.0031
[09/16 18:00:29 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1424, average loss: 2.0953
[09/16 18:00:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.50	top5: 69.00	
[09/16 18:00:51 visual_prompt]: 	Test 100/407. loss: 2.276, 0.1938 s / batch. (data: 1.07e-04)max mem: 17.22449 GB 
[09/16 18:01:10 visual_prompt]: 	Test 200/407. loss: 2.246, 0.1946 s / batch. (data: 1.27e-02)max mem: 17.22449 GB 
[09/16 18:01:29 visual_prompt]: 	Test 300/407. loss: 2.103, 0.1955 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 18:01:49 visual_prompt]: 	Test 400/407. loss: 2.149, 0.1828 s / batch. (data: 3.74e-05)max mem: 17.22449 GB 
[09/16 18:01:52 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1932, average loss: 2.1974
[09/16 18:01:52 visual_prompt]: Classification results with test_vtab-svhn: top1: 23.07	top5: 64.84	
[09/16 18:01:52 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 18:02:02 visual_prompt]: Epoch 74 / 100: avg data time: 1.52e-01, avg batch time: 0.5509, average train loss: 1.9063
[09/16 18:02:07 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1424, average loss: 1.8693
[09/16 18:02:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.50	top5: 78.00	
[09/16 18:02:29 visual_prompt]: 	Test 100/407. loss: 2.180, 0.1906 s / batch. (data: 1.05e-04)max mem: 17.22449 GB 
[09/16 18:02:48 visual_prompt]: 	Test 200/407. loss: 2.092, 0.2063 s / batch. (data: 2.47e-02)max mem: 17.22449 GB 
[09/16 18:03:08 visual_prompt]: 	Test 300/407. loss: 1.893, 0.1978 s / batch. (data: 1.55e-02)max mem: 17.22449 GB 
[09/16 18:03:27 visual_prompt]: 	Test 400/407. loss: 2.038, 0.1832 s / batch. (data: 4.43e-05)max mem: 17.22449 GB 
[09/16 18:03:30 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1940, average loss: 1.9237
[09/16 18:03:30 visual_prompt]: Classification results with test_vtab-svhn: top1: 30.22	top5: 75.74	
[09/16 18:03:30 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 18:03:41 visual_prompt]: Epoch 75 / 100: avg data time: 1.51e-01, avg batch time: 0.5532, average train loss: 1.8664
[09/16 18:03:45 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1436, average loss: 1.7422
[09/16 18:03:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.00	top5: 80.50	
[09/16 18:04:07 visual_prompt]: 	Test 100/407. loss: 2.232, 0.1950 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 18:04:27 visual_prompt]: 	Test 200/407. loss: 2.224, 0.1831 s / batch. (data: 1.43e-04)max mem: 17.22449 GB 
[09/16 18:04:46 visual_prompt]: 	Test 300/407. loss: 1.972, 0.2172 s / batch. (data: 3.56e-02)max mem: 17.22449 GB 
[09/16 18:05:05 visual_prompt]: 	Test 400/407. loss: 2.023, 0.1838 s / batch. (data: 3.91e-05)max mem: 17.22449 GB 
[09/16 18:05:09 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1935, average loss: 1.9697
[09/16 18:05:09 visual_prompt]: Classification results with test_vtab-svhn: top1: 30.45	top5: 76.07	
[09/16 18:05:09 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 18:05:19 visual_prompt]: Epoch 76 / 100: avg data time: 1.57e-01, avg batch time: 0.5564, average train loss: 1.7321
[09/16 18:05:24 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1426, average loss: 1.6291
[09/16 18:05:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 40.00	top5: 88.00	
[09/16 18:05:46 visual_prompt]: 	Test 100/407. loss: 1.993, 0.1951 s / batch. (data: 1.32e-02)max mem: 17.22449 GB 
[09/16 18:06:05 visual_prompt]: 	Test 200/407. loss: 1.972, 0.1958 s / batch. (data: 1.31e-02)max mem: 17.22449 GB 
[09/16 18:06:25 visual_prompt]: 	Test 300/407. loss: 1.815, 0.2226 s / batch. (data: 4.09e-02)max mem: 17.22449 GB 
[09/16 18:06:44 visual_prompt]: 	Test 400/407. loss: 1.931, 0.1826 s / batch. (data: 2.81e-05)max mem: 17.22449 GB 
[09/16 18:06:48 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1939, average loss: 1.8220
[09/16 18:06:48 visual_prompt]: Classification results with test_vtab-svhn: top1: 36.04	top5: 81.83	
[09/16 18:06:48 visual_prompt]: Best epoch 76: best metric: 0.400
[09/16 18:06:48 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 18:06:58 visual_prompt]: Epoch 77 / 100: avg data time: 1.42e-01, avg batch time: 0.5424, average train loss: 1.7684
[09/16 18:07:03 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1420, average loss: 1.6453
[09/16 18:07:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 40.00	top5: 85.50	
[09/16 18:07:25 visual_prompt]: 	Test 100/407. loss: 2.107, 0.2095 s / batch. (data: 2.78e-02)max mem: 17.22449 GB 
[09/16 18:07:44 visual_prompt]: 	Test 200/407. loss: 2.186, 0.1949 s / batch. (data: 1.54e-04)max mem: 17.22449 GB 
[09/16 18:08:04 visual_prompt]: 	Test 300/407. loss: 1.768, 0.2065 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 18:08:23 visual_prompt]: 	Test 400/407. loss: 1.902, 0.1822 s / batch. (data: 2.31e-05)max mem: 17.22449 GB 
[09/16 18:08:27 visual_prompt]: Inference (test):avg data time: 8.11e-03, avg batch time: 0.1940, average loss: 1.8907
[09/16 18:08:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 35.79	top5: 77.29	
[09/16 18:08:27 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 18:08:37 visual_prompt]: Epoch 78 / 100: avg data time: 1.55e-01, avg batch time: 0.5555, average train loss: 1.6582
[09/16 18:08:42 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1423, average loss: 1.6338
[09/16 18:08:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 41.00	top5: 84.00	
[09/16 18:09:04 visual_prompt]: 	Test 100/407. loss: 1.979, 0.1827 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 18:09:23 visual_prompt]: 	Test 200/407. loss: 1.913, 0.1959 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 18:09:42 visual_prompt]: 	Test 300/407. loss: 1.962, 0.1859 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 18:10:02 visual_prompt]: 	Test 400/407. loss: 2.064, 0.1827 s / batch. (data: 3.50e-05)max mem: 17.22449 GB 
[09/16 18:10:05 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1930, average loss: 1.7901
[09/16 18:10:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 39.14	top5: 81.82	
[09/16 18:10:05 visual_prompt]: Best epoch 78: best metric: 0.410
[09/16 18:10:05 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 18:10:15 visual_prompt]: Epoch 79 / 100: avg data time: 1.45e-01, avg batch time: 0.5489, average train loss: 1.6856
[09/16 18:10:20 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1426, average loss: 1.7592
[09/16 18:10:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.50	top5: 77.00	
[09/16 18:10:42 visual_prompt]: 	Test 100/407. loss: 2.148, 0.2148 s / batch. (data: 3.28e-02)max mem: 17.22449 GB 
[09/16 18:11:01 visual_prompt]: 	Test 200/407. loss: 1.931, 0.1830 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 18:11:21 visual_prompt]: 	Test 300/407. loss: 1.962, 0.2031 s / batch. (data: 1.04e-04)max mem: 17.22449 GB 
[09/16 18:11:40 visual_prompt]: 	Test 400/407. loss: 2.053, 0.1825 s / batch. (data: 3.91e-05)max mem: 17.22449 GB 
[09/16 18:11:43 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1934, average loss: 1.8796
[09/16 18:11:44 visual_prompt]: Classification results with test_vtab-svhn: top1: 36.26	top5: 72.65	
[09/16 18:11:44 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 18:11:54 visual_prompt]: Epoch 80 / 100: avg data time: 1.43e-01, avg batch time: 0.5533, average train loss: 1.6814
[09/16 18:11:58 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1424, average loss: 1.6855
[09/16 18:11:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.00	top5: 83.50	
[09/16 18:12:20 visual_prompt]: 	Test 100/407. loss: 1.900, 0.2019 s / batch. (data: 1.98e-02)max mem: 17.22449 GB 
[09/16 18:12:40 visual_prompt]: 	Test 200/407. loss: 1.869, 0.1889 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 18:12:59 visual_prompt]: 	Test 300/407. loss: 1.843, 0.1823 s / batch. (data: 9.44e-05)max mem: 17.22449 GB 
[09/16 18:13:19 visual_prompt]: 	Test 400/407. loss: 1.903, 0.1821 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 18:13:22 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1934, average loss: 1.8959
[09/16 18:13:22 visual_prompt]: Classification results with test_vtab-svhn: top1: 35.10	top5: 78.63	
[09/16 18:13:22 visual_prompt]: Best epoch 80: best metric: 0.430
[09/16 18:13:22 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 18:13:32 visual_prompt]: Epoch 81 / 100: avg data time: 1.36e-01, avg batch time: 0.5404, average train loss: 1.4612
[09/16 18:13:37 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1424, average loss: 1.2791
[09/16 18:13:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 53.50	top5: 93.50	
[09/16 18:13:58 visual_prompt]: 	Test 100/407. loss: 1.609, 0.1827 s / batch. (data: 1.65e-04)max mem: 17.22449 GB 
[09/16 18:14:18 visual_prompt]: 	Test 200/407. loss: 1.596, 0.1836 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 18:14:37 visual_prompt]: 	Test 300/407. loss: 1.744, 0.1821 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 18:14:57 visual_prompt]: 	Test 400/407. loss: 1.929, 0.1820 s / batch. (data: 3.65e-05)max mem: 17.22449 GB 
[09/16 18:15:00 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1931, average loss: 1.5847
[09/16 18:15:00 visual_prompt]: Classification results with test_vtab-svhn: top1: 43.53	top5: 88.05	
[09/16 18:15:00 visual_prompt]: Best epoch 81: best metric: 0.535
[09/16 18:15:00 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 18:15:11 visual_prompt]: Epoch 82 / 100: avg data time: 1.50e-01, avg batch time: 0.5505, average train loss: 1.3520
[09/16 18:15:16 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1426, average loss: 1.2706
[09/16 18:15:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.50	top5: 91.50	
[09/16 18:15:37 visual_prompt]: 	Test 100/407. loss: 1.702, 0.1861 s / batch. (data: 1.69e-04)max mem: 17.22449 GB 
[09/16 18:15:57 visual_prompt]: 	Test 200/407. loss: 1.582, 0.1830 s / batch. (data: 1.38e-04)max mem: 17.22449 GB 
[09/16 18:16:16 visual_prompt]: 	Test 300/407. loss: 1.636, 0.1913 s / batch. (data: 9.16e-03)max mem: 17.22449 GB 
[09/16 18:16:36 visual_prompt]: 	Test 400/407. loss: 1.786, 0.1856 s / batch. (data: 6.06e-05)max mem: 17.22449 GB 
[09/16 18:16:39 visual_prompt]: Inference (test):avg data time: 8.05e-03, avg batch time: 0.1939, average loss: 1.5781
[09/16 18:16:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 46.34	top5: 86.32	
[09/16 18:16:39 visual_prompt]: Best epoch 82: best metric: 0.565
[09/16 18:16:39 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 18:16:50 visual_prompt]: Epoch 83 / 100: avg data time: 1.34e-01, avg batch time: 0.5372, average train loss: 1.2029
[09/16 18:16:54 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1423, average loss: 1.2210
[09/16 18:16:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 59.00	top5: 94.50	
[09/16 18:17:16 visual_prompt]: 	Test 100/407. loss: 1.695, 0.4844 s / batch. (data: 4.78e-03)max mem: 17.22449 GB 
[09/16 18:17:35 visual_prompt]: 	Test 200/407. loss: 1.673, 0.1974 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 18:17:55 visual_prompt]: 	Test 300/407. loss: 1.839, 0.1948 s / batch. (data: 1.23e-02)max mem: 17.22449 GB 
[09/16 18:18:14 visual_prompt]: 	Test 400/407. loss: 2.179, 0.1831 s / batch. (data: 3.91e-05)max mem: 17.22449 GB 
[09/16 18:18:17 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1938, average loss: 1.7887
[09/16 18:18:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 48.83	top5: 89.04	
[09/16 18:18:17 visual_prompt]: Best epoch 83: best metric: 0.590
[09/16 18:18:17 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 18:18:28 visual_prompt]: Epoch 84 / 100: avg data time: 1.59e-01, avg batch time: 0.5608, average train loss: 1.1607
[09/16 18:18:33 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1423, average loss: 1.0261
[09/16 18:18:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.00	top5: 96.50	
[09/16 18:18:54 visual_prompt]: 	Test 100/407. loss: 1.523, 0.2026 s / batch. (data: 9.24e-03)max mem: 17.22449 GB 
[09/16 18:19:14 visual_prompt]: 	Test 200/407. loss: 1.491, 0.1953 s / batch. (data: 1.32e-02)max mem: 17.22449 GB 
[09/16 18:19:33 visual_prompt]: 	Test 300/407. loss: 1.683, 0.1944 s / batch. (data: 1.12e-02)max mem: 17.22449 GB 
[09/16 18:19:53 visual_prompt]: 	Test 400/407. loss: 1.690, 0.1954 s / batch. (data: 3.50e-05)max mem: 17.22449 GB 
[09/16 18:19:56 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1934, average loss: 1.4338
[09/16 18:19:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 47.39	top5: 90.35	
[09/16 18:19:56 visual_prompt]: Best epoch 84: best metric: 0.600
[09/16 18:19:56 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 18:20:06 visual_prompt]: Epoch 85 / 100: avg data time: 1.44e-01, avg batch time: 0.5451, average train loss: 1.0610
[09/16 18:20:11 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1426, average loss: 1.0859
[09/16 18:20:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.50	top5: 92.50	
[09/16 18:20:33 visual_prompt]: 	Test 100/407. loss: 1.971, 0.1821 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 18:20:52 visual_prompt]: 	Test 200/407. loss: 1.638, 0.2086 s / batch. (data: 3.36e-05)max mem: 17.22449 GB 
[09/16 18:21:12 visual_prompt]: 	Test 300/407. loss: 1.566, 0.2081 s / batch. (data: 2.58e-02)max mem: 17.22449 GB 
[09/16 18:21:31 visual_prompt]: 	Test 400/407. loss: 1.855, 0.1821 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 18:21:34 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1944, average loss: 1.5913
[09/16 18:21:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 49.73	top5: 89.58	
[09/16 18:21:35 visual_prompt]: Best epoch 85: best metric: 0.645
[09/16 18:21:35 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 18:21:45 visual_prompt]: Epoch 86 / 100: avg data time: 1.46e-01, avg batch time: 0.5461, average train loss: 1.0459
[09/16 18:21:49 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1423, average loss: 0.8007
[09/16 18:21:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 98.50	
[09/16 18:22:12 visual_prompt]: 	Test 100/407. loss: 1.449, 0.1825 s / batch. (data: 1.65e-04)max mem: 17.22449 GB 
[09/16 18:22:31 visual_prompt]: 	Test 200/407. loss: 1.315, 0.1906 s / batch. (data: 3.41e-05)max mem: 17.22449 GB 
[09/16 18:22:50 visual_prompt]: 	Test 300/407. loss: 1.371, 0.1980 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 18:23:10 visual_prompt]: 	Test 400/407. loss: 1.478, 0.1826 s / batch. (data: 4.17e-05)max mem: 17.22449 GB 
[09/16 18:23:14 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1955, average loss: 1.2816
[09/16 18:23:14 visual_prompt]: Classification results with test_vtab-svhn: top1: 58.04	top5: 92.83	
[09/16 18:23:14 visual_prompt]: Best epoch 86: best metric: 0.710
[09/16 18:23:14 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 18:23:24 visual_prompt]: Epoch 87 / 100: avg data time: 1.42e-01, avg batch time: 0.5487, average train loss: 0.9132
[09/16 18:23:29 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1441, average loss: 0.7129
[09/16 18:23:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 98.50	
[09/16 18:23:50 visual_prompt]: 	Test 100/407. loss: 1.245, 0.2079 s / batch. (data: 2.60e-02)max mem: 17.22449 GB 
[09/16 18:24:10 visual_prompt]: 	Test 200/407. loss: 1.217, 0.2100 s / batch. (data: 2.80e-02)max mem: 17.22449 GB 
[09/16 18:24:30 visual_prompt]: 	Test 300/407. loss: 1.216, 0.1978 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 18:24:50 visual_prompt]: 	Test 400/407. loss: 1.483, 0.1836 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 18:24:53 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1954, average loss: 1.3029
[09/16 18:24:53 visual_prompt]: Classification results with test_vtab-svhn: top1: 58.52	top5: 92.56	
[09/16 18:24:53 visual_prompt]: Best epoch 87: best metric: 0.755
[09/16 18:24:53 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 18:25:04 visual_prompt]: Epoch 88 / 100: avg data time: 1.56e-01, avg batch time: 0.5592, average train loss: 0.7988
[09/16 18:25:08 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.1443, average loss: 0.6806
[09/16 18:25:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 99.00	
[09/16 18:25:30 visual_prompt]: 	Test 100/407. loss: 1.360, 0.1883 s / batch. (data: 5.90e-03)max mem: 17.22449 GB 
[09/16 18:25:50 visual_prompt]: 	Test 200/407. loss: 1.280, 0.1830 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 18:26:09 visual_prompt]: 	Test 300/407. loss: 1.322, 0.1829 s / batch. (data: 1.40e-04)max mem: 17.22449 GB 
[09/16 18:26:29 visual_prompt]: 	Test 400/407. loss: 1.464, 0.1832 s / batch. (data: 3.55e-05)max mem: 17.22449 GB 
[09/16 18:26:32 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1937, average loss: 1.2791
[09/16 18:26:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 56.53	top5: 92.70	
[09/16 18:26:32 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 18:26:43 visual_prompt]: Epoch 89 / 100: avg data time: 1.46e-01, avg batch time: 0.5485, average train loss: 0.7661
[09/16 18:26:47 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1423, average loss: 0.7824
[09/16 18:26:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 97.50	
[09/16 18:27:09 visual_prompt]: 	Test 100/407. loss: 1.678, 0.2036 s / batch. (data: 9.70e-05)max mem: 17.22449 GB 
[09/16 18:27:28 visual_prompt]: 	Test 200/407. loss: 1.508, 0.1826 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 18:27:48 visual_prompt]: 	Test 300/407. loss: 1.334, 0.1836 s / batch. (data: 1.59e-04)max mem: 17.22449 GB 
[09/16 18:28:07 visual_prompt]: 	Test 400/407. loss: 1.483, 0.1830 s / batch. (data: 2.77e-05)max mem: 17.22449 GB 
[09/16 18:28:11 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1942, average loss: 1.3985
[09/16 18:28:11 visual_prompt]: Classification results with test_vtab-svhn: top1: 58.12	top5: 93.20	
[09/16 18:28:11 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 18:28:21 visual_prompt]: Epoch 90 / 100: avg data time: 1.57e-01, avg batch time: 0.5583, average train loss: 0.7379
[09/16 18:28:26 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1424, average loss: 0.5817
[09/16 18:28:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 99.00	
[09/16 18:28:48 visual_prompt]: 	Test 100/407. loss: 1.183, 0.1821 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 18:29:07 visual_prompt]: 	Test 200/407. loss: 1.272, 0.1835 s / batch. (data: 1.47e-04)max mem: 17.22449 GB 
[09/16 18:29:26 visual_prompt]: 	Test 300/407. loss: 1.349, 0.1931 s / batch. (data: 1.07e-02)max mem: 17.22449 GB 
[09/16 18:29:46 visual_prompt]: 	Test 400/407. loss: 1.554, 0.1821 s / batch. (data: 2.96e-05)max mem: 17.22449 GB 
[09/16 18:29:49 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1929, average loss: 1.2889
[09/16 18:29:49 visual_prompt]: Classification results with test_vtab-svhn: top1: 61.10	top5: 93.37	
[09/16 18:29:49 visual_prompt]: Best epoch 90: best metric: 0.765
[09/16 18:29:49 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 18:30:00 visual_prompt]: Epoch 91 / 100: avg data time: 1.40e-01, avg batch time: 0.5430, average train loss: 0.6478
[09/16 18:30:04 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1427, average loss: 0.5242
[09/16 18:30:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 100.00	
[09/16 18:30:26 visual_prompt]: 	Test 100/407. loss: 1.352, 0.1833 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 18:30:45 visual_prompt]: 	Test 200/407. loss: 1.324, 0.1957 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 18:31:04 visual_prompt]: 	Test 300/407. loss: 1.373, 0.1933 s / batch. (data: 1.11e-02)max mem: 17.22449 GB 
[09/16 18:31:24 visual_prompt]: 	Test 400/407. loss: 1.489, 0.1840 s / batch. (data: 3.55e-05)max mem: 17.22449 GB 
[09/16 18:31:27 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1925, average loss: 1.3221
[09/16 18:31:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 63.15	top5: 94.17	
[09/16 18:31:27 visual_prompt]: Best epoch 91: best metric: 0.815
[09/16 18:31:27 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 18:31:38 visual_prompt]: Epoch 92 / 100: avg data time: 1.45e-01, avg batch time: 0.5495, average train loss: 0.5909
[09/16 18:31:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1431, average loss: 0.5474
[09/16 18:31:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 100.00	
[09/16 18:32:04 visual_prompt]: 	Test 100/407. loss: 1.548, 0.1984 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 18:32:23 visual_prompt]: 	Test 200/407. loss: 1.331, 0.1851 s / batch. (data: 1.64e-04)max mem: 17.22449 GB 
[09/16 18:32:43 visual_prompt]: 	Test 300/407. loss: 1.352, 0.1974 s / batch. (data: 1.28e-02)max mem: 17.22449 GB 
[09/16 18:33:02 visual_prompt]: 	Test 400/407. loss: 1.468, 0.1907 s / batch. (data: 3.74e-05)max mem: 17.22449 GB 
[09/16 18:33:05 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1930, average loss: 1.3689
[09/16 18:33:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 63.00	top5: 93.80	
[09/16 18:33:05 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 18:33:16 visual_prompt]: Epoch 93 / 100: avg data time: 1.51e-01, avg batch time: 0.5510, average train loss: 0.5164
[09/16 18:33:21 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1424, average loss: 0.4238
[09/16 18:33:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 100.00	
[09/16 18:33:42 visual_prompt]: 	Test 100/407. loss: 1.509, 0.2045 s / batch. (data: 1.59e-02)max mem: 17.22449 GB 
[09/16 18:34:02 visual_prompt]: 	Test 200/407. loss: 1.423, 0.1960 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 18:34:22 visual_prompt]: 	Test 300/407. loss: 1.542, 0.1831 s / batch. (data: 1.14e-04)max mem: 17.22449 GB 
[09/16 18:34:41 visual_prompt]: 	Test 400/407. loss: 1.594, 0.1823 s / batch. (data: 2.43e-05)max mem: 17.22449 GB 
[09/16 18:34:44 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1941, average loss: 1.3817
[09/16 18:34:44 visual_prompt]: Classification results with test_vtab-svhn: top1: 64.49	top5: 94.30	
[09/16 18:34:44 visual_prompt]: Best epoch 93: best metric: 0.825
[09/16 18:34:44 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 18:34:55 visual_prompt]: Epoch 94 / 100: avg data time: 1.49e-01, avg batch time: 0.5492, average train loss: 0.4734
[09/16 18:34:59 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1424, average loss: 0.4434
[09/16 18:34:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 99.50	
[09/16 18:35:21 visual_prompt]: 	Test 100/407. loss: 1.640, 0.1898 s / batch. (data: 8.87e-05)max mem: 17.22449 GB 
[09/16 18:35:40 visual_prompt]: 	Test 200/407. loss: 1.490, 0.1834 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 18:36:00 visual_prompt]: 	Test 300/407. loss: 1.756, 0.1973 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 18:36:19 visual_prompt]: 	Test 400/407. loss: 1.624, 0.1830 s / batch. (data: 4.22e-05)max mem: 17.22449 GB 
[09/16 18:36:23 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1940, average loss: 1.5013
[09/16 18:36:23 visual_prompt]: Classification results with test_vtab-svhn: top1: 62.82	top5: 92.97	
[09/16 18:36:23 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 18:36:33 visual_prompt]: Epoch 95 / 100: avg data time: 1.51e-01, avg batch time: 0.5547, average train loss: 0.4525
[09/16 18:36:38 visual_prompt]: Inference (val):avg data time: 4.59e-05, avg batch time: 0.1425, average loss: 0.4200
[09/16 18:36:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.50	
[09/16 18:36:59 visual_prompt]: 	Test 100/407. loss: 1.570, 0.2116 s / batch. (data: 6.77e-05)max mem: 17.22449 GB 
[09/16 18:37:19 visual_prompt]: 	Test 200/407. loss: 1.360, 0.1962 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 18:37:38 visual_prompt]: 	Test 300/407. loss: 1.621, 0.1837 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 18:37:58 visual_prompt]: 	Test 400/407. loss: 1.550, 0.1827 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 18:38:01 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1927, average loss: 1.3722
[09/16 18:38:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 62.84	top5: 93.52	
[09/16 18:38:01 visual_prompt]: Best epoch 95: best metric: 0.850
[09/16 18:38:01 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 18:38:11 visual_prompt]: Epoch 96 / 100: avg data time: 1.57e-01, avg batch time: 0.5555, average train loss: 0.3887
[09/16 18:38:16 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1424, average loss: 0.3592
[09/16 18:38:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 100.00	
[09/16 18:38:38 visual_prompt]: 	Test 100/407. loss: 1.736, 0.1933 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 18:38:57 visual_prompt]: 	Test 200/407. loss: 1.478, 0.2245 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 18:39:16 visual_prompt]: 	Test 300/407. loss: 1.569, 0.1945 s / batch. (data: 1.25e-02)max mem: 17.22449 GB 
[09/16 18:39:36 visual_prompt]: 	Test 400/407. loss: 1.568, 0.1839 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 18:39:39 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1932, average loss: 1.4188
[09/16 18:39:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 64.60	top5: 94.31	
[09/16 18:39:39 visual_prompt]: Best epoch 96: best metric: 0.865
[09/16 18:39:39 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 18:39:49 visual_prompt]: Epoch 97 / 100: avg data time: 1.55e-01, avg batch time: 0.5561, average train loss: 0.3598
[09/16 18:39:54 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1425, average loss: 0.3301
[09/16 18:39:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 99.50	
[09/16 18:40:16 visual_prompt]: 	Test 100/407. loss: 1.736, 0.1955 s / batch. (data: 1.49e-04)max mem: 17.22449 GB 
[09/16 18:40:35 visual_prompt]: 	Test 200/407. loss: 1.556, 0.1994 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 18:40:54 visual_prompt]: 	Test 300/407. loss: 1.698, 0.1832 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 18:41:14 visual_prompt]: 	Test 400/407. loss: 1.618, 0.1823 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 18:41:17 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1933, average loss: 1.5143
[09/16 18:41:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 64.65	top5: 93.76	
[09/16 18:41:17 visual_prompt]: Best epoch 97: best metric: 0.885
[09/16 18:41:17 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 18:41:28 visual_prompt]: Epoch 98 / 100: avg data time: 1.50e-01, avg batch time: 0.5512, average train loss: 0.3321
[09/16 18:41:32 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1425, average loss: 0.3491
[09/16 18:41:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 99.50	
[09/16 18:41:54 visual_prompt]: 	Test 100/407. loss: 1.846, 0.1848 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 18:42:13 visual_prompt]: 	Test 200/407. loss: 1.680, 0.1963 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 18:42:33 visual_prompt]: 	Test 300/407. loss: 1.699, 0.2005 s / batch. (data: 1.72e-04)max mem: 17.22449 GB 
[09/16 18:42:52 visual_prompt]: 	Test 400/407. loss: 1.624, 0.1827 s / batch. (data: 3.48e-05)max mem: 17.22449 GB 
[09/16 18:42:56 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1931, average loss: 1.5684
[09/16 18:42:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 64.70	top5: 93.74	
[09/16 18:42:56 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 18:43:06 visual_prompt]: Epoch 99 / 100: avg data time: 1.54e-01, avg batch time: 0.5538, average train loss: 0.3237
[09/16 18:43:11 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1426, average loss: 0.3203
[09/16 18:43:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 90.50	top5: 99.50	
[09/16 18:43:33 visual_prompt]: 	Test 100/407. loss: 1.785, 0.1823 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 18:43:53 visual_prompt]: 	Test 200/407. loss: 1.653, 0.1960 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 18:44:12 visual_prompt]: 	Test 300/407. loss: 1.747, 0.1849 s / batch. (data: 1.43e-04)max mem: 17.22449 GB 
[09/16 18:44:31 visual_prompt]: 	Test 400/407. loss: 1.654, 0.1829 s / batch. (data: 3.19e-05)max mem: 17.22449 GB 
[09/16 18:44:35 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1939, average loss: 1.5491
[09/16 18:44:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 64.91	top5: 93.88	
[09/16 18:44:35 visual_prompt]: Best epoch 99: best metric: 0.905
[09/16 18:44:35 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 18:44:45 visual_prompt]: Epoch 100 / 100: avg data time: 1.53e-01, avg batch time: 0.5537, average train loss: 0.3056
[09/16 18:44:50 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1424, average loss: 0.3167
[09/16 18:44:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 89.50	top5: 99.50	
[09/16 18:45:12 visual_prompt]: 	Test 100/407. loss: 1.791, 0.1961 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 18:45:31 visual_prompt]: 	Test 200/407. loss: 1.660, 0.1972 s / batch. (data: 1.50e-02)max mem: 17.22449 GB 
[09/16 18:45:51 visual_prompt]: 	Test 300/407. loss: 1.729, 0.1827 s / batch. (data: 1.11e-04)max mem: 17.22449 GB 
[09/16 18:46:10 visual_prompt]: 	Test 400/407. loss: 1.658, 0.1827 s / batch. (data: 2.81e-05)max mem: 17.22449 GB 
[09/16 18:46:13 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1936, average loss: 1.5484
[09/16 18:46:13 visual_prompt]: Classification results with test_vtab-svhn: top1: 64.94	top5: 93.91	
[09/16 18:46:36 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 18:46:36 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 18:46:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed800'], train_type='')
[09/16 18:46:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 18:46:36 visual_prompt]: Training with config:
[09/16 18:46:36 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-svhn',
          'NO_TEST': False,
          'NUMBER_CLASSES': 10,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed800/vtab-svhn/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 18:46:36 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 18:46:36.599535: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 18:46:36.769602: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 18:46:37.680700: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 18:46:37.680778: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 18:46:37.680787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 18:46:39.784433: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 18:46:39.784544: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 18:46:39.784556: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 18:46:39 visual_prompt]: Constructing vtab-svhn dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
2023-09-16 18:46:39.799735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split train[:800]+train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 18:46:41 visual_prompt]: Number of images: 1000
[09/16 18:46:41 visual_prompt]: Number of classes: 10 / 10
[09/16 18:46:41 visual_prompt]: Loading validation data...
[09/16 18:46:41 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 18:46:41 visual_prompt]: Number of images: 200
[09/16 18:46:41 visual_prompt]: Number of classes: 10 / 10
[09/16 18:46:41 visual_prompt]: Loading test data...
[09/16 18:46:41 visual_prompt]: Constructing vtab-svhn dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset svhn_cropped for split test, from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/16 18:47:13 visual_prompt]: Number of images: 26032
[09/16 18:47:13 visual_prompt]: Number of classes: 10 / 10
[09/16 18:47:13 visual_prompt]: Constructing models...
[09/16 18:47:16 visual_prompt]: Total Parameters: 86727946	 Gradient Parameters: 929290
[09/16 18:47:16 visual_prompt]: tuned percent:1.072
[09/16 18:47:18 visual_prompt]: Device used for model: 0
[09/16 18:47:18 visual_prompt]: Setting up Evalutator...
[09/16 18:47:18 visual_prompt]: Setting up Trainer...
[09/16 18:47:18 visual_prompt]: 	Setting up the optimizer...
[09/16 18:47:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 18:47:30 visual_prompt]: Epoch 1 / 100: avg data time: 1.59e-01, avg batch time: 0.6378, average train loss: 2.4125
[09/16 18:47:35 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1418, average loss: 2.3973
[09/16 18:47:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/16 18:47:57 visual_prompt]: 	Test 100/407. loss: 2.419, 0.1812 s / batch. (data: 1.14e-04)max mem: 17.22449 GB 
[09/16 18:48:16 visual_prompt]: 	Test 200/407. loss: 2.453, 0.2055 s / batch. (data: 1.32e-02)max mem: 17.22449 GB 
[09/16 18:48:36 visual_prompt]: 	Test 300/407. loss: 2.412, 0.2035 s / batch. (data: 2.16e-02)max mem: 17.22449 GB 
[09/16 18:48:55 visual_prompt]: 	Test 400/407. loss: 2.506, 0.1825 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 18:48:58 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1937, average loss: 2.4224
[09/16 18:48:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.56	top5: 58.21	
[09/16 18:48:58 visual_prompt]: Best epoch 1: best metric: 0.230
[09/16 18:48:58 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 18:49:09 visual_prompt]: Epoch 2 / 100: avg data time: 1.64e-01, avg batch time: 0.5636, average train loss: 2.8168
[09/16 18:49:14 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1427, average loss: 2.3250
[09/16 18:49:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/16 18:49:36 visual_prompt]: 	Test 100/407. loss: 2.419, 0.2096 s / batch. (data: 2.72e-02)max mem: 17.22449 GB 
[09/16 18:49:56 visual_prompt]: 	Test 200/407. loss: 2.415, 0.1964 s / batch. (data: 1.50e-02)max mem: 17.22449 GB 
[09/16 18:50:15 visual_prompt]: 	Test 300/407. loss: 2.316, 0.1953 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 18:50:35 visual_prompt]: 	Test 400/407. loss: 2.374, 0.1828 s / batch. (data: 3.34e-05)max mem: 17.22449 GB 
[09/16 18:50:38 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1952, average loss: 2.3358
[09/16 18:50:38 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 59.19	
[09/16 18:50:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 18:50:49 visual_prompt]: Epoch 3 / 100: avg data time: 1.55e-01, avg batch time: 0.5560, average train loss: 2.3804
[09/16 18:50:53 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1424, average loss: 2.3755
[09/16 18:50:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/16 18:51:15 visual_prompt]: 	Test 100/407. loss: 2.409, 0.2082 s / batch. (data: 1.13e-02)max mem: 17.22449 GB 
[09/16 18:51:34 visual_prompt]: 	Test 200/407. loss: 2.498, 0.1877 s / batch. (data: 1.53e-04)max mem: 17.22449 GB 
[09/16 18:51:54 visual_prompt]: 	Test 300/407. loss: 2.226, 0.1957 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 18:52:13 visual_prompt]: 	Test 400/407. loss: 2.293, 0.1892 s / batch. (data: 2.79e-05)max mem: 17.22449 GB 
[09/16 18:52:16 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1938, average loss: 2.3438
[09/16 18:52:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 62.23	
[09/16 18:52:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 18:52:27 visual_prompt]: Epoch 4 / 100: avg data time: 1.49e-01, avg batch time: 0.5516, average train loss: 2.3622
[09/16 18:52:32 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1426, average loss: 2.4732
[09/16 18:52:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 49.00	
[09/16 18:52:54 visual_prompt]: 	Test 100/407. loss: 2.380, 0.1830 s / batch. (data: 1.63e-04)max mem: 17.22449 GB 
[09/16 18:53:14 visual_prompt]: 	Test 200/407. loss: 2.452, 0.2142 s / batch. (data: 3.26e-02)max mem: 17.22449 GB 
[09/16 18:53:33 visual_prompt]: 	Test 300/407. loss: 2.456, 0.1918 s / batch. (data: 6.77e-05)max mem: 17.22449 GB 
[09/16 18:53:53 visual_prompt]: 	Test 400/407. loss: 2.422, 0.1826 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 18:53:56 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1942, average loss: 2.4542
[09/16 18:53:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 49.49	
[09/16 18:53:56 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 18:54:07 visual_prompt]: Epoch 5 / 100: avg data time: 1.44e-01, avg batch time: 0.5467, average train loss: 2.5292
[09/16 18:54:11 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1424, average loss: 2.5351
[09/16 18:54:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/16 18:54:33 visual_prompt]: 	Test 100/407. loss: 2.884, 0.2037 s / batch. (data: 1.11e-02)max mem: 17.22449 GB 
[09/16 18:54:53 visual_prompt]: 	Test 200/407. loss: 2.911, 0.1914 s / batch. (data: 9.50e-03)max mem: 17.22449 GB 
[09/16 18:55:12 visual_prompt]: 	Test 300/407. loss: 2.442, 0.1915 s / batch. (data: 1.50e-04)max mem: 17.22449 GB 
[09/16 18:55:31 visual_prompt]: 	Test 400/407. loss: 2.549, 0.1829 s / batch. (data: 2.48e-05)max mem: 17.22449 GB 
[09/16 18:55:35 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1937, average loss: 2.5700
[09/16 18:55:35 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 63.51	
[09/16 18:55:35 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 18:55:45 visual_prompt]: Epoch 6 / 100: avg data time: 1.65e-01, avg batch time: 0.5639, average train loss: 2.8409
[09/16 18:55:50 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1425, average loss: 2.5586
[09/16 18:55:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/16 18:56:12 visual_prompt]: 	Test 100/407. loss: 3.006, 0.1820 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 18:56:32 visual_prompt]: 	Test 200/407. loss: 2.977, 0.2067 s / batch. (data: 2.54e-02)max mem: 17.22449 GB 
[09/16 18:56:51 visual_prompt]: 	Test 300/407. loss: 2.486, 0.1821 s / batch. (data: 1.11e-04)max mem: 17.22449 GB 
[09/16 18:57:11 visual_prompt]: 	Test 400/407. loss: 2.652, 0.1827 s / batch. (data: 3.19e-05)max mem: 17.22449 GB 
[09/16 18:57:14 visual_prompt]: Inference (test):avg data time: 8.86e-03, avg batch time: 0.1959, average loss: 2.6478
[09/16 18:57:14 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.11	top5: 61.05	
[09/16 18:57:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 18:57:25 visual_prompt]: Epoch 7 / 100: avg data time: 1.59e-01, avg batch time: 0.5632, average train loss: 3.5154
[09/16 18:57:30 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1425, average loss: 3.7652
[09/16 18:57:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/16 18:57:52 visual_prompt]: 	Test 100/407. loss: 3.453, 0.1824 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 18:58:11 visual_prompt]: 	Test 200/407. loss: 3.958, 0.1966 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 18:58:31 visual_prompt]: 	Test 300/407. loss: 2.964, 0.2127 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 18:58:50 visual_prompt]: 	Test 400/407. loss: 3.404, 0.1821 s / batch. (data: 4.46e-05)max mem: 17.22449 GB 
[09/16 18:58:53 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1942, average loss: 3.6445
[09/16 18:58:53 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 48.56	
[09/16 18:58:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 18:59:04 visual_prompt]: Epoch 8 / 100: avg data time: 1.49e-01, avg batch time: 0.5508, average train loss: 3.9284
[09/16 18:59:08 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1423, average loss: 6.4322
[09/16 18:59:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.00	
[09/16 18:59:30 visual_prompt]: 	Test 100/407. loss: 7.304, 0.1822 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 18:59:50 visual_prompt]: 	Test 200/407. loss: 6.618, 0.1819 s / batch. (data: 1.15e-04)max mem: 17.22449 GB 
[09/16 19:00:09 visual_prompt]: 	Test 300/407. loss: 7.100, 0.1833 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 19:00:29 visual_prompt]: 	Test 400/407. loss: 6.859, 0.1819 s / batch. (data: 3.74e-05)max mem: 17.22449 GB 
[09/16 19:00:32 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1938, average loss: 6.7342
[09/16 19:00:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 51.33	
[09/16 19:00:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 19:00:42 visual_prompt]: Epoch 9 / 100: avg data time: 1.51e-01, avg batch time: 0.5517, average train loss: 11.3125
[09/16 19:00:47 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 13.0491
[09/16 19:00:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/16 19:01:09 visual_prompt]: 	Test 100/407. loss: 12.798, 0.1824 s / batch. (data: 1.55e-04)max mem: 17.22449 GB 
[09/16 19:01:28 visual_prompt]: 	Test 200/407. loss: 13.404, 0.1825 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 19:01:48 visual_prompt]: 	Test 300/407. loss: 11.983, 0.1877 s / batch. (data: 5.20e-03)max mem: 17.22449 GB 
[09/16 19:02:07 visual_prompt]: 	Test 400/407. loss: 11.605, 0.1827 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 19:02:10 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1934, average loss: 12.4620
[09/16 19:02:11 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 64.04	
[09/16 19:02:11 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 19:02:21 visual_prompt]: Epoch 10 / 100: avg data time: 1.53e-01, avg batch time: 0.5555, average train loss: 24.6459
[09/16 19:02:26 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1422, average loss: 21.6044
[09/16 19:02:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/16 19:02:48 visual_prompt]: 	Test 100/407. loss: 26.380, 0.1823 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 19:03:07 visual_prompt]: 	Test 200/407. loss: 27.289, 0.1822 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 19:03:27 visual_prompt]: 	Test 300/407. loss: 20.679, 0.1851 s / batch. (data: 1.43e-04)max mem: 17.22449 GB 
[09/16 19:03:46 visual_prompt]: 	Test 400/407. loss: 23.122, 0.1829 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 19:03:50 visual_prompt]: Inference (test):avg data time: 6.59e-03, avg batch time: 0.1949, average loss: 22.9406
[09/16 19:03:50 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 63.51	
[09/16 19:03:50 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 19:04:00 visual_prompt]: Epoch 11 / 100: avg data time: 1.58e-01, avg batch time: 0.5581, average train loss: 18.4852
[09/16 19:04:05 visual_prompt]: Inference (val):avg data time: 4.58e-05, avg batch time: 0.1425, average loss: 12.5475
[09/16 19:04:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 49.00	
[09/16 19:04:27 visual_prompt]: 	Test 100/407. loss: 13.451, 0.2085 s / batch. (data: 2.69e-02)max mem: 17.22449 GB 
[09/16 19:04:46 visual_prompt]: 	Test 200/407. loss: 13.293, 0.1825 s / batch. (data: 8.27e-05)max mem: 17.22449 GB 
[09/16 19:05:05 visual_prompt]: 	Test 300/407. loss: 11.276, 0.1965 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 19:05:24 visual_prompt]: 	Test 400/407. loss: 10.242, 0.1831 s / batch. (data: 3.36e-05)max mem: 17.22449 GB 
[09/16 19:05:28 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1915, average loss: 12.4611
[09/16 19:05:28 visual_prompt]: Classification results with test_vtab-svhn: top1: 7.59	top5: 52.05	
[09/16 19:05:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 19:05:39 visual_prompt]: Epoch 12 / 100: avg data time: 1.61e-01, avg batch time: 0.5901, average train loss: 21.8724
[09/16 19:05:43 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1424, average loss: 12.2111
[09/16 19:05:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/16 19:06:05 visual_prompt]: 	Test 100/407. loss: 17.213, 0.1981 s / batch. (data: 1.15e-04)max mem: 17.22449 GB 
[09/16 19:06:25 visual_prompt]: 	Test 200/407. loss: 17.794, 0.1952 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 19:06:45 visual_prompt]: 	Test 300/407. loss: 9.556, 0.1863 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 19:07:04 visual_prompt]: 	Test 400/407. loss: 14.435, 0.1829 s / batch. (data: 3.48e-05)max mem: 17.22449 GB 
[09/16 19:07:07 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1944, average loss: 13.3470
[09/16 19:07:07 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 51.24	
[09/16 19:07:07 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 19:07:18 visual_prompt]: Epoch 13 / 100: avg data time: 1.61e-01, avg batch time: 0.5639, average train loss: 21.5759
[09/16 19:07:22 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1424, average loss: 16.4765
[09/16 19:07:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/16 19:07:44 visual_prompt]: 	Test 100/407. loss: 19.303, 0.2195 s / batch. (data: 2.51e-02)max mem: 17.22449 GB 
[09/16 19:08:03 visual_prompt]: 	Test 200/407. loss: 18.197, 0.1945 s / batch. (data: 1.30e-02)max mem: 17.22449 GB 
[09/16 19:08:23 visual_prompt]: 	Test 300/407. loss: 17.585, 0.1836 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 19:08:42 visual_prompt]: 	Test 400/407. loss: 16.899, 0.1824 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 19:08:45 visual_prompt]: Inference (test):avg data time: 6.66e-03, avg batch time: 0.1918, average loss: 16.5578
[09/16 19:08:45 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 58.04	
[09/16 19:08:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 19:08:56 visual_prompt]: Epoch 14 / 100: avg data time: 1.54e-01, avg batch time: 0.5543, average train loss: 18.2841
[09/16 19:09:01 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1446, average loss: 21.4449
[09/16 19:09:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/16 19:09:22 visual_prompt]: 	Test 100/407. loss: 22.166, 0.2037 s / batch. (data: 1.92e-02)max mem: 17.22449 GB 
[09/16 19:09:42 visual_prompt]: 	Test 200/407. loss: 25.188, 0.1828 s / batch. (data: 9.06e-05)max mem: 17.22449 GB 
[09/16 19:10:01 visual_prompt]: 	Test 300/407. loss: 18.686, 0.1969 s / batch. (data: 1.48e-02)max mem: 17.22449 GB 
[09/16 19:10:21 visual_prompt]: 	Test 400/407. loss: 20.557, 0.1823 s / batch. (data: 4.24e-05)max mem: 17.22449 GB 
[09/16 19:10:24 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1945, average loss: 21.4891
[09/16 19:10:24 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 62.13	
[09/16 19:10:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 19:10:35 visual_prompt]: Epoch 15 / 100: avg data time: 1.44e-01, avg batch time: 0.5450, average train loss: 20.0974
[09/16 19:10:40 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1424, average loss: 20.1185
[09/16 19:10:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 51.50	
[09/16 19:11:02 visual_prompt]: 	Test 100/407. loss: 22.374, 0.2008 s / batch. (data: 1.94e-02)max mem: 17.22449 GB 
[09/16 19:11:21 visual_prompt]: 	Test 200/407. loss: 22.239, 0.1996 s / batch. (data: 1.44e-02)max mem: 17.22449 GB 
[09/16 19:11:40 visual_prompt]: 	Test 300/407. loss: 20.423, 0.1966 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 19:12:00 visual_prompt]: 	Test 400/407. loss: 22.250, 0.1824 s / batch. (data: 4.74e-05)max mem: 17.22449 GB 
[09/16 19:12:03 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1933, average loss: 21.1774
[09/16 19:12:03 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 47.44	
[09/16 19:12:03 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 19:12:14 visual_prompt]: Epoch 16 / 100: avg data time: 1.56e-01, avg batch time: 0.5564, average train loss: 18.8822
[09/16 19:12:18 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1461, average loss: 16.9706
[09/16 19:12:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.50	
[09/16 19:12:40 visual_prompt]: 	Test 100/407. loss: 17.120, 0.1826 s / batch. (data: 1.38e-04)max mem: 17.22449 GB 
[09/16 19:13:00 visual_prompt]: 	Test 200/407. loss: 21.362, 0.1918 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 19:13:19 visual_prompt]: 	Test 300/407. loss: 14.702, 0.1892 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 19:13:38 visual_prompt]: 	Test 400/407. loss: 17.465, 0.1824 s / batch. (data: 3.00e-05)max mem: 17.22449 GB 
[09/16 19:13:42 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1930, average loss: 16.8413
[09/16 19:13:42 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 63.51	
[09/16 19:13:42 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 19:13:52 visual_prompt]: Epoch 17 / 100: avg data time: 1.53e-01, avg batch time: 0.5591, average train loss: 16.1904
[09/16 19:13:57 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1426, average loss: 9.8088
[09/16 19:13:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 60.50	
[09/16 19:14:19 visual_prompt]: 	Test 100/407. loss: 10.997, 0.2067 s / batch. (data: 1.60e-02)max mem: 17.22449 GB 
[09/16 19:14:38 visual_prompt]: 	Test 200/407. loss: 12.404, 0.2157 s / batch. (data: 1.92e-02)max mem: 17.22449 GB 
[09/16 19:14:58 visual_prompt]: 	Test 300/407. loss: 7.606, 0.1959 s / batch. (data: 1.22e-02)max mem: 17.22449 GB 
[09/16 19:15:17 visual_prompt]: 	Test 400/407. loss: 8.952, 0.1826 s / batch. (data: 3.43e-05)max mem: 17.22449 GB 
[09/16 19:15:21 visual_prompt]: Inference (test):avg data time: 7.09e-03, avg batch time: 0.1934, average loss: 9.7777
[09/16 19:15:21 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.16	top5: 57.10	
[09/16 19:15:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 19:15:31 visual_prompt]: Epoch 18 / 100: avg data time: 1.57e-01, avg batch time: 0.5558, average train loss: 14.4130
[09/16 19:15:36 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1475, average loss: 13.5248
[09/16 19:15:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/16 19:15:58 visual_prompt]: 	Test 100/407. loss: 14.783, 0.1955 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 19:16:17 visual_prompt]: 	Test 200/407. loss: 15.813, 0.1847 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 19:16:37 visual_prompt]: 	Test 300/407. loss: 11.150, 0.1944 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 19:16:56 visual_prompt]: 	Test 400/407. loss: 12.799, 0.1821 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 19:16:59 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1937, average loss: 13.3371
[09/16 19:17:00 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 60.48	
[09/16 19:17:00 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 19:17:10 visual_prompt]: Epoch 19 / 100: avg data time: 1.56e-01, avg batch time: 0.5578, average train loss: 12.4957
[09/16 19:17:15 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1425, average loss: 9.9539
[09/16 19:17:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/16 19:17:37 visual_prompt]: 	Test 100/407. loss: 13.572, 0.1867 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 19:17:56 visual_prompt]: 	Test 200/407. loss: 12.480, 0.2245 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 19:18:15 visual_prompt]: 	Test 300/407. loss: 8.814, 0.1831 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 19:18:35 visual_prompt]: 	Test 400/407. loss: 9.779, 0.1822 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 19:18:38 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1928, average loss: 10.1353
[09/16 19:18:38 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 60.32	
[09/16 19:18:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 19:18:49 visual_prompt]: Epoch 20 / 100: avg data time: 1.62e-01, avg batch time: 0.5628, average train loss: 8.5313
[09/16 19:18:53 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1425, average loss: 8.6166
[09/16 19:18:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/16 19:19:15 visual_prompt]: 	Test 100/407. loss: 9.974, 0.1825 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 19:19:34 visual_prompt]: 	Test 200/407. loss: 10.182, 0.1981 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 19:19:54 visual_prompt]: 	Test 300/407. loss: 8.859, 0.1832 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 19:20:14 visual_prompt]: 	Test 400/407. loss: 8.609, 0.1826 s / batch. (data: 2.86e-05)max mem: 17.22449 GB 
[09/16 19:20:17 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1939, average loss: 8.7647
[09/16 19:20:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 62.13	
[09/16 19:20:17 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 19:20:27 visual_prompt]: Epoch 21 / 100: avg data time: 1.53e-01, avg batch time: 0.5549, average train loss: 7.7299
[09/16 19:20:32 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1426, average loss: 6.4770
[09/16 19:20:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/16 19:20:54 visual_prompt]: 	Test 100/407. loss: 7.496, 0.1851 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 19:21:13 visual_prompt]: 	Test 200/407. loss: 7.382, 0.1959 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 19:21:33 visual_prompt]: 	Test 300/407. loss: 7.053, 0.1828 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 19:21:52 visual_prompt]: 	Test 400/407. loss: 6.823, 0.1825 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 19:21:55 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1930, average loss: 6.7575
[09/16 19:21:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 54.23	
[09/16 19:21:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 19:22:06 visual_prompt]: Epoch 22 / 100: avg data time: 1.56e-01, avg batch time: 0.5556, average train loss: 6.0891
[09/16 19:22:11 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1426, average loss: 6.0550
[09/16 19:22:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/16 19:22:32 visual_prompt]: 	Test 100/407. loss: 7.128, 0.2011 s / batch. (data: 1.25e-02)max mem: 17.22449 GB 
[09/16 19:22:52 visual_prompt]: 	Test 200/407. loss: 6.822, 0.1923 s / batch. (data: 1.03e-02)max mem: 17.22449 GB 
[09/16 19:23:11 visual_prompt]: 	Test 300/407. loss: 5.227, 0.1919 s / batch. (data: 1.15e-04)max mem: 17.22449 GB 
[09/16 19:23:30 visual_prompt]: 	Test 400/407. loss: 5.626, 0.1827 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 19:23:34 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1928, average loss: 6.1566
[09/16 19:23:34 visual_prompt]: Classification results with test_vtab-svhn: top1: 11.07	top5: 61.95	
[09/16 19:23:34 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 19:23:44 visual_prompt]: Epoch 23 / 100: avg data time: 1.59e-01, avg batch time: 0.5599, average train loss: 5.7555
[09/16 19:23:49 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1425, average loss: 3.2392
[09/16 19:23:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/16 19:24:11 visual_prompt]: 	Test 100/407. loss: 3.317, 0.1827 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 19:24:31 visual_prompt]: 	Test 200/407. loss: 3.456, 0.1976 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 19:24:50 visual_prompt]: 	Test 300/407. loss: 3.370, 0.1828 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 19:25:10 visual_prompt]: 	Test 400/407. loss: 3.533, 0.1826 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 19:25:13 visual_prompt]: Inference (test):avg data time: 8.75e-03, avg batch time: 0.1948, average loss: 3.3429
[09/16 19:25:13 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 52.41	
[09/16 19:25:13 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 19:25:24 visual_prompt]: Epoch 24 / 100: avg data time: 1.60e-01, avg batch time: 0.5618, average train loss: 2.6478
[09/16 19:25:28 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1424, average loss: 2.3804
[09/16 19:25:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.00	top5: 49.50	
[09/16 19:25:50 visual_prompt]: 	Test 100/407. loss: 2.399, 0.1895 s / batch. (data: 1.32e-04)max mem: 17.22449 GB 
[09/16 19:26:10 visual_prompt]: 	Test 200/407. loss: 2.382, 0.2277 s / batch. (data: 4.63e-02)max mem: 17.22449 GB 
[09/16 19:26:29 visual_prompt]: 	Test 300/407. loss: 2.340, 0.1970 s / batch. (data: 1.45e-02)max mem: 17.22449 GB 
[09/16 19:26:48 visual_prompt]: 	Test 400/407. loss: 2.284, 0.1820 s / batch. (data: 2.88e-05)max mem: 17.22449 GB 
[09/16 19:26:52 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1929, average loss: 2.3817
[09/16 19:26:52 visual_prompt]: Classification results with test_vtab-svhn: top1: 10.46	top5: 52.08	
[09/16 19:26:52 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 19:27:03 visual_prompt]: Epoch 25 / 100: avg data time: 1.65e-01, avg batch time: 0.5648, average train loss: 2.5245
[09/16 19:27:07 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1423, average loss: 2.6493
[09/16 19:27:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/16 19:27:30 visual_prompt]: 	Test 100/407. loss: 2.998, 0.1933 s / batch. (data: 1.49e-04)max mem: 17.22449 GB 
[09/16 19:27:49 visual_prompt]: 	Test 200/407. loss: 2.860, 0.1827 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 19:28:09 visual_prompt]: 	Test 300/407. loss: 2.838, 0.2178 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 19:28:28 visual_prompt]: 	Test 400/407. loss: 2.907, 0.1823 s / batch. (data: 2.31e-05)max mem: 17.22449 GB 
[09/16 19:28:32 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1948, average loss: 2.7566
[09/16 19:28:32 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 52.74	
[09/16 19:28:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 19:28:42 visual_prompt]: Epoch 26 / 100: avg data time: 1.51e-01, avg batch time: 0.5519, average train loss: 2.7049
[09/16 19:28:47 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1425, average loss: 2.5122
[09/16 19:28:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.00	
[09/16 19:29:09 visual_prompt]: 	Test 100/407. loss: 2.427, 0.5925 s / batch. (data: 1.59e-02)max mem: 17.22449 GB 
[09/16 19:29:29 visual_prompt]: 	Test 200/407. loss: 2.427, 0.2079 s / batch. (data: 1.91e-02)max mem: 17.22449 GB 
[09/16 19:29:48 visual_prompt]: 	Test 300/407. loss: 2.425, 0.1829 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 19:30:08 visual_prompt]: 	Test 400/407. loss: 2.344, 0.1827 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 19:30:11 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1950, average loss: 2.4783
[09/16 19:30:11 visual_prompt]: Classification results with test_vtab-svhn: top1: 15.94	top5: 47.19	
[09/16 19:30:11 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 19:30:22 visual_prompt]: Epoch 27 / 100: avg data time: 1.46e-01, avg batch time: 0.5479, average train loss: 2.6415
[09/16 19:30:26 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1424, average loss: 2.5330
[09/16 19:30:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/16 19:30:48 visual_prompt]: 	Test 100/407. loss: 2.902, 0.1826 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 19:31:08 visual_prompt]: 	Test 200/407. loss: 2.881, 0.1827 s / batch. (data: 1.15e-04)max mem: 17.22449 GB 
[09/16 19:31:27 visual_prompt]: 	Test 300/407. loss: 2.515, 0.1962 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 19:31:47 visual_prompt]: 	Test 400/407. loss: 2.685, 0.1829 s / batch. (data: 3.84e-05)max mem: 17.22449 GB 
[09/16 19:31:50 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1939, average loss: 2.6023
[09/16 19:31:50 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 62.67	
[09/16 19:31:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 19:32:01 visual_prompt]: Epoch 28 / 100: avg data time: 1.58e-01, avg batch time: 0.5589, average train loss: 2.5957
[09/16 19:32:05 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1425, average loss: 3.5700
[09/16 19:32:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/16 19:32:27 visual_prompt]: 	Test 100/407. loss: 4.239, 0.1936 s / batch. (data: 1.23e-02)max mem: 17.22449 GB 
[09/16 19:32:47 visual_prompt]: 	Test 200/407. loss: 3.944, 0.2102 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 19:33:07 visual_prompt]: 	Test 300/407. loss: 3.853, 0.2028 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 19:33:26 visual_prompt]: 	Test 400/407. loss: 3.736, 0.1823 s / batch. (data: 3.91e-05)max mem: 17.22449 GB 
[09/16 19:33:29 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1953, average loss: 3.7418
[09/16 19:33:30 visual_prompt]: Classification results with test_vtab-svhn: top1: 19.59	top5: 56.14	
[09/16 19:33:30 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 19:33:40 visual_prompt]: Epoch 29 / 100: avg data time: 1.59e-01, avg batch time: 0.5597, average train loss: 3.1754
[09/16 19:33:45 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.1432, average loss: 3.4217
[09/16 19:33:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/16 19:34:07 visual_prompt]: 	Test 100/407. loss: 3.559, 0.1833 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 19:34:27 visual_prompt]: 	Test 200/407. loss: 3.472, 0.1829 s / batch. (data: 8.70e-05)max mem: 17.22449 GB 
[09/16 19:34:46 visual_prompt]: 	Test 300/407. loss: 3.616, 0.1954 s / batch. (data: 1.06e-04)max mem: 17.22449 GB 
[09/16 19:35:06 visual_prompt]: 	Test 400/407. loss: 3.559, 0.1822 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 19:35:09 visual_prompt]: Inference (test):avg data time: 8.47e-03, avg batch time: 0.1938, average loss: 3.5100
[09/16 19:35:09 visual_prompt]: Classification results with test_vtab-svhn: top1: 9.69	top5: 54.56	
[09/16 19:35:09 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 19:35:20 visual_prompt]: Epoch 30 / 100: avg data time: 1.54e-01, avg batch time: 0.5533, average train loss: 2.8575
[09/16 19:35:24 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1426, average loss: 2.3713
[09/16 19:35:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.00	top5: 66.50	
[09/16 19:35:46 visual_prompt]: 	Test 100/407. loss: 2.397, 0.1827 s / batch. (data: 1.44e-04)max mem: 17.22449 GB 
[09/16 19:36:06 visual_prompt]: 	Test 200/407. loss: 2.549, 0.1824 s / batch. (data: 9.37e-05)max mem: 17.22449 GB 
[09/16 19:36:25 visual_prompt]: 	Test 300/407. loss: 2.278, 0.1820 s / batch. (data: 1.01e-04)max mem: 17.22449 GB 
[09/16 19:36:44 visual_prompt]: 	Test 400/407. loss: 2.365, 0.1826 s / batch. (data: 4.10e-05)max mem: 17.22449 GB 
[09/16 19:36:47 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1926, average loss: 2.4293
[09/16 19:36:48 visual_prompt]: Classification results with test_vtab-svhn: top1: 31.44	top5: 67.44	
[09/16 19:36:48 visual_prompt]: Best epoch 30: best metric: 0.320
[09/16 19:36:48 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 19:36:58 visual_prompt]: Epoch 31 / 100: avg data time: 1.53e-01, avg batch time: 0.5546, average train loss: 2.6576
[09/16 19:37:03 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1425, average loss: 2.5072
[09/16 19:37:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 63.00	
[09/16 19:37:25 visual_prompt]: 	Test 100/407. loss: 2.978, 0.1830 s / batch. (data: 9.66e-05)max mem: 17.22449 GB 
[09/16 19:37:44 visual_prompt]: 	Test 200/407. loss: 2.658, 0.1960 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 19:38:04 visual_prompt]: 	Test 300/407. loss: 2.959, 0.1964 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 19:38:23 visual_prompt]: 	Test 400/407. loss: 2.724, 0.1827 s / batch. (data: 3.81e-05)max mem: 17.22449 GB 
[09/16 19:38:26 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1935, average loss: 2.6657
[09/16 19:38:26 visual_prompt]: Classification results with test_vtab-svhn: top1: 20.12	top5: 55.88	
[09/16 19:38:26 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 19:38:37 visual_prompt]: Epoch 32 / 100: avg data time: 1.57e-01, avg batch time: 0.5566, average train loss: 2.8470
[09/16 19:38:42 visual_prompt]: Inference (val):avg data time: 3.21e-04, avg batch time: 0.2330, average loss: 2.3454
[09/16 19:38:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.50	top5: 67.00	
[09/16 19:39:04 visual_prompt]: 	Test 100/407. loss: 2.950, 0.1959 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 19:39:23 visual_prompt]: 	Test 200/407. loss: 2.693, 0.2090 s / batch. (data: 2.74e-02)max mem: 17.22449 GB 
[09/16 19:39:42 visual_prompt]: 	Test 300/407. loss: 2.475, 0.1976 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 19:40:02 visual_prompt]: 	Test 400/407. loss: 2.540, 0.1971 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 19:40:05 visual_prompt]: Inference (test):avg data time: 6.62e-03, avg batch time: 0.1928, average loss: 2.4547
[09/16 19:40:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 21.82	top5: 63.35	
[09/16 19:40:05 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 19:40:16 visual_prompt]: Epoch 33 / 100: avg data time: 1.50e-01, avg batch time: 0.5508, average train loss: 2.1806
[09/16 19:40:20 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1425, average loss: 1.7552
[09/16 19:40:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.50	top5: 82.00	
[09/16 19:40:42 visual_prompt]: 	Test 100/407. loss: 2.220, 0.1937 s / batch. (data: 1.65e-04)max mem: 17.22449 GB 
[09/16 19:41:02 visual_prompt]: 	Test 200/407. loss: 2.160, 0.1858 s / batch. (data: 1.26e-04)max mem: 17.22449 GB 
[09/16 19:41:21 visual_prompt]: 	Test 300/407. loss: 1.807, 0.1964 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 19:41:41 visual_prompt]: 	Test 400/407. loss: 1.890, 0.1834 s / batch. (data: 2.91e-05)max mem: 17.22449 GB 
[09/16 19:41:44 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1939, average loss: 1.9122
[09/16 19:41:44 visual_prompt]: Classification results with test_vtab-svhn: top1: 38.53	top5: 73.64	
[09/16 19:41:44 visual_prompt]: Best epoch 33: best metric: 0.435
[09/16 19:41:44 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 19:41:55 visual_prompt]: Epoch 34 / 100: avg data time: 1.52e-01, avg batch time: 0.5541, average train loss: 1.8856
[09/16 19:41:59 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1424, average loss: 2.1389
[09/16 19:41:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.50	top5: 69.50	
[09/16 19:42:21 visual_prompt]: 	Test 100/407. loss: 2.400, 0.2077 s / batch. (data: 2.59e-02)max mem: 17.22449 GB 
[09/16 19:42:40 visual_prompt]: 	Test 200/407. loss: 2.237, 0.1909 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 19:43:00 visual_prompt]: 	Test 300/407. loss: 2.262, 0.1960 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 19:43:19 visual_prompt]: 	Test 400/407. loss: 2.060, 0.1828 s / batch. (data: 3.34e-05)max mem: 17.22449 GB 
[09/16 19:43:23 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1933, average loss: 2.3232
[09/16 19:43:23 visual_prompt]: Classification results with test_vtab-svhn: top1: 25.41	top5: 67.84	
[09/16 19:43:23 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 19:43:33 visual_prompt]: Epoch 35 / 100: avg data time: 1.53e-01, avg batch time: 0.5537, average train loss: 1.8900
[09/16 19:43:38 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1425, average loss: 1.7525
[09/16 19:43:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 40.00	top5: 86.50	
[09/16 19:44:00 visual_prompt]: 	Test 100/407. loss: 2.356, 0.1819 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 19:44:19 visual_prompt]: 	Test 200/407. loss: 2.061, 0.1958 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 19:44:38 visual_prompt]: 	Test 300/407. loss: 2.066, 0.1974 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 19:44:58 visual_prompt]: 	Test 400/407. loss: 2.217, 0.1828 s / batch. (data: 2.72e-05)max mem: 17.22449 GB 
[09/16 19:45:01 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1926, average loss: 2.0328
[09/16 19:45:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 33.26	top5: 80.64	
[09/16 19:45:01 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 19:45:12 visual_prompt]: Epoch 36 / 100: avg data time: 1.55e-01, avg batch time: 0.5554, average train loss: 3.3360
[09/16 19:45:16 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1425, average loss: 3.8184
[09/16 19:45:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.00	top5: 67.00	
[09/16 19:45:38 visual_prompt]: 	Test 100/407. loss: 3.564, 0.1955 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 19:45:57 visual_prompt]: 	Test 200/407. loss: 3.752, 0.1828 s / batch. (data: 1.06e-04)max mem: 17.22449 GB 
[09/16 19:46:17 visual_prompt]: 	Test 300/407. loss: 4.282, 0.1830 s / batch. (data: 9.32e-05)max mem: 17.22449 GB 
[09/16 19:46:36 visual_prompt]: 	Test 400/407. loss: 4.329, 0.1828 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 19:46:39 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1924, average loss: 4.0538
[09/16 19:46:39 visual_prompt]: Classification results with test_vtab-svhn: top1: 26.20	top5: 62.35	
[09/16 19:46:39 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 19:46:50 visual_prompt]: Epoch 37 / 100: avg data time: 1.48e-01, avg batch time: 0.5475, average train loss: 2.8794
[09/16 19:46:55 visual_prompt]: Inference (val):avg data time: 3.45e-04, avg batch time: 0.2144, average loss: 2.2933
[09/16 19:46:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.00	top5: 76.50	
[09/16 19:47:17 visual_prompt]: 	Test 100/407. loss: 2.864, 0.2265 s / batch. (data: 1.25e-02)max mem: 17.22449 GB 
[09/16 19:47:36 visual_prompt]: 	Test 200/407. loss: 2.931, 0.2115 s / batch. (data: 2.97e-02)max mem: 17.22449 GB 
[09/16 19:47:56 visual_prompt]: 	Test 300/407. loss: 2.159, 0.1918 s / batch. (data: 1.07e-04)max mem: 17.22449 GB 
[09/16 19:48:16 visual_prompt]: 	Test 400/407. loss: 2.301, 0.1827 s / batch. (data: 3.29e-05)max mem: 17.22449 GB 
[09/16 19:48:19 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1958, average loss: 2.4030
[09/16 19:48:19 visual_prompt]: Classification results with test_vtab-svhn: top1: 35.09	top5: 73.99	
[09/16 19:48:19 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 19:48:30 visual_prompt]: Epoch 38 / 100: avg data time: 1.47e-01, avg batch time: 0.5504, average train loss: 1.9991
[09/16 19:48:35 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1425, average loss: 1.5338
[09/16 19:48:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.00	top5: 85.50	
[09/16 19:48:56 visual_prompt]: 	Test 100/407. loss: 1.897, 0.1822 s / batch. (data: 1.46e-04)max mem: 17.22449 GB 
[09/16 19:49:16 visual_prompt]: 	Test 200/407. loss: 1.873, 0.1978 s / batch. (data: 1.59e-02)max mem: 17.22449 GB 
[09/16 19:49:35 visual_prompt]: 	Test 300/407. loss: 1.551, 0.1961 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 19:49:55 visual_prompt]: 	Test 400/407. loss: 1.696, 0.1818 s / batch. (data: 3.29e-05)max mem: 17.22449 GB 
[09/16 19:49:58 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1938, average loss: 1.6714
[09/16 19:49:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 42.52	top5: 84.62	
[09/16 19:49:58 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 19:50:09 visual_prompt]: Epoch 39 / 100: avg data time: 1.53e-01, avg batch time: 0.5550, average train loss: 1.7989
[09/16 19:50:13 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1463, average loss: 1.6788
[09/16 19:50:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 38.00	top5: 85.50	
[09/16 19:50:35 visual_prompt]: 	Test 100/407. loss: 1.866, 0.1825 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 19:50:55 visual_prompt]: 	Test 200/407. loss: 1.845, 0.2088 s / batch. (data: 2.72e-02)max mem: 17.22449 GB 
[09/16 19:51:14 visual_prompt]: 	Test 300/407. loss: 1.881, 0.1826 s / batch. (data: 1.49e-04)max mem: 17.22449 GB 
[09/16 19:51:34 visual_prompt]: 	Test 400/407. loss: 2.035, 0.1823 s / batch. (data: 2.88e-05)max mem: 17.22449 GB 
[09/16 19:51:37 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1935, average loss: 1.8831
[09/16 19:51:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 32.79	top5: 83.62	
[09/16 19:51:37 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 19:51:48 visual_prompt]: Epoch 40 / 100: avg data time: 1.59e-01, avg batch time: 0.5598, average train loss: 1.7003
[09/16 19:51:52 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1425, average loss: 1.3770
[09/16 19:51:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 48.50	top5: 88.50	
[09/16 19:52:14 visual_prompt]: 	Test 100/407. loss: 1.835, 0.1882 s / batch. (data: 1.22e-04)max mem: 17.22449 GB 
[09/16 19:52:34 visual_prompt]: 	Test 200/407. loss: 1.861, 0.2119 s / batch. (data: 1.63e-02)max mem: 17.22449 GB 
[09/16 19:52:53 visual_prompt]: 	Test 300/407. loss: 1.403, 0.1827 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 19:53:13 visual_prompt]: 	Test 400/407. loss: 1.778, 0.1826 s / batch. (data: 3.41e-05)max mem: 17.22449 GB 
[09/16 19:53:16 visual_prompt]: Inference (test):avg data time: 8.06e-03, avg batch time: 0.1938, average loss: 1.6090
[09/16 19:53:16 visual_prompt]: Classification results with test_vtab-svhn: top1: 44.00	top5: 85.71	
[09/16 19:53:16 visual_prompt]: Best epoch 40: best metric: 0.485
[09/16 19:53:16 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 19:53:27 visual_prompt]: Epoch 41 / 100: avg data time: 1.53e-01, avg batch time: 0.5726, average train loss: 1.4142
[09/16 19:53:32 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1424, average loss: 1.3811
[09/16 19:53:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.50	top5: 92.00	
[09/16 19:53:53 visual_prompt]: 	Test 100/407. loss: 1.990, 0.1823 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 19:54:13 visual_prompt]: 	Test 200/407. loss: 1.730, 0.1824 s / batch. (data: 1.53e-04)max mem: 17.22449 GB 
[09/16 19:54:32 visual_prompt]: 	Test 300/407. loss: 1.619, 0.1824 s / batch. (data: 1.26e-04)max mem: 17.22449 GB 
[09/16 19:54:52 visual_prompt]: 	Test 400/407. loss: 1.732, 0.1825 s / batch. (data: 3.24e-05)max mem: 17.22449 GB 
[09/16 19:54:55 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1937, average loss: 1.6486
[09/16 19:54:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 49.45	top5: 86.29	
[09/16 19:54:55 visual_prompt]: Best epoch 41: best metric: 0.525
[09/16 19:54:55 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 19:55:06 visual_prompt]: Epoch 42 / 100: avg data time: 1.57e-01, avg batch time: 0.5614, average train loss: 1.2554
[09/16 19:55:10 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1424, average loss: 1.1154
[09/16 19:55:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 59.50	top5: 94.00	
[09/16 19:55:32 visual_prompt]: 	Test 100/407. loss: 1.752, 0.1829 s / batch. (data: 1.69e-04)max mem: 17.22449 GB 
[09/16 19:55:51 visual_prompt]: 	Test 200/407. loss: 1.466, 0.2098 s / batch. (data: 1.54e-02)max mem: 17.22449 GB 
[09/16 19:56:11 visual_prompt]: 	Test 300/407. loss: 1.388, 0.1968 s / batch. (data: 1.07e-04)max mem: 17.22449 GB 
[09/16 19:56:31 visual_prompt]: 	Test 400/407. loss: 1.621, 0.1832 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 19:56:34 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1939, average loss: 1.5067
[09/16 19:56:34 visual_prompt]: Classification results with test_vtab-svhn: top1: 52.07	top5: 89.46	
[09/16 19:56:34 visual_prompt]: Best epoch 42: best metric: 0.595
[09/16 19:56:34 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 19:56:44 visual_prompt]: Epoch 43 / 100: avg data time: 1.54e-01, avg batch time: 0.5526, average train loss: 1.0744
[09/16 19:56:49 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1427, average loss: 0.9259
[09/16 19:56:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 98.00	
[09/16 19:57:11 visual_prompt]: 	Test 100/407. loss: 1.462, 0.2050 s / batch. (data: 2.33e-02)max mem: 17.22449 GB 
[09/16 19:57:31 visual_prompt]: 	Test 200/407. loss: 1.253, 0.2122 s / batch. (data: 3.09e-02)max mem: 17.22449 GB 
[09/16 19:57:50 visual_prompt]: 	Test 300/407. loss: 1.152, 0.1950 s / batch. (data: 1.01e-04)max mem: 17.22449 GB 
[09/16 19:58:10 visual_prompt]: 	Test 400/407. loss: 1.293, 0.1839 s / batch. (data: 4.94e-05)max mem: 17.22449 GB 
[09/16 19:58:13 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1949, average loss: 1.3409
[09/16 19:58:13 visual_prompt]: Classification results with test_vtab-svhn: top1: 56.88	top5: 92.90	
[09/16 19:58:13 visual_prompt]: Best epoch 43: best metric: 0.720
[09/16 19:58:13 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 19:58:24 visual_prompt]: Epoch 44 / 100: avg data time: 1.57e-01, avg batch time: 0.5583, average train loss: 0.8649
[09/16 19:58:28 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1426, average loss: 0.6868
[09/16 19:58:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 98.50	
[09/16 19:58:50 visual_prompt]: 	Test 100/407. loss: 1.570, 0.1978 s / batch. (data: 1.08e-04)max mem: 17.22449 GB 
[09/16 19:59:10 visual_prompt]: 	Test 200/407. loss: 1.120, 0.1926 s / batch. (data: 1.03e-02)max mem: 17.22449 GB 
[09/16 19:59:29 visual_prompt]: 	Test 300/407. loss: 1.333, 0.2045 s / batch. (data: 2.28e-02)max mem: 17.22449 GB 
[09/16 19:59:48 visual_prompt]: 	Test 400/407. loss: 1.414, 0.1825 s / batch. (data: 3.60e-05)max mem: 17.22449 GB 
[09/16 19:59:52 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1937, average loss: 1.3052
[09/16 19:59:52 visual_prompt]: Classification results with test_vtab-svhn: top1: 58.58	top5: 92.06	
[09/16 19:59:52 visual_prompt]: Best epoch 44: best metric: 0.775
[09/16 19:59:52 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 20:00:02 visual_prompt]: Epoch 45 / 100: avg data time: 1.44e-01, avg batch time: 0.5475, average train loss: 0.9301
[09/16 20:00:07 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1427, average loss: 1.0891
[09/16 20:00:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.50	top5: 96.00	
[09/16 20:00:29 visual_prompt]: 	Test 100/407. loss: 2.167, 0.1824 s / batch. (data: 1.36e-04)max mem: 17.22449 GB 
[09/16 20:00:48 visual_prompt]: 	Test 200/407. loss: 1.683, 0.1954 s / batch. (data: 1.30e-02)max mem: 17.22449 GB 
[09/16 20:01:08 visual_prompt]: 	Test 300/407. loss: 1.547, 0.1975 s / batch. (data: 1.62e-02)max mem: 17.22449 GB 
[09/16 20:01:27 visual_prompt]: 	Test 400/407. loss: 1.830, 0.1829 s / batch. (data: 2.86e-05)max mem: 17.22449 GB 
[09/16 20:01:30 visual_prompt]: Inference (test):avg data time: 8.47e-03, avg batch time: 0.1937, average loss: 1.6799
[09/16 20:01:30 visual_prompt]: Classification results with test_vtab-svhn: top1: 47.84	top5: 91.29	
[09/16 20:01:30 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 20:01:41 visual_prompt]: Epoch 46 / 100: avg data time: 1.50e-01, avg batch time: 0.5511, average train loss: 0.8331
[09/16 20:01:45 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1424, average loss: 1.1147
[09/16 20:01:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.50	top5: 98.50	
[09/16 20:02:07 visual_prompt]: 	Test 100/407. loss: 2.374, 0.1828 s / batch. (data: 1.21e-04)max mem: 17.22449 GB 
[09/16 20:02:27 visual_prompt]: 	Test 200/407. loss: 1.911, 0.1863 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 20:02:46 visual_prompt]: 	Test 300/407. loss: 2.133, 0.1830 s / batch. (data: 5.13e-05)max mem: 17.22449 GB 
[09/16 20:03:05 visual_prompt]: 	Test 400/407. loss: 1.870, 0.1859 s / batch. (data: 4.53e-05)max mem: 17.22449 GB 
[09/16 20:03:09 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1926, average loss: 1.9018
[09/16 20:03:09 visual_prompt]: Classification results with test_vtab-svhn: top1: 48.52	top5: 90.29	
[09/16 20:03:09 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 20:03:19 visual_prompt]: Epoch 47 / 100: avg data time: 1.57e-01, avg batch time: 0.5649, average train loss: 0.7507
[09/16 20:03:24 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1426, average loss: 0.4204
[09/16 20:03:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 99.50	
[09/16 20:03:46 visual_prompt]: 	Test 100/407. loss: 1.138, 0.1823 s / batch. (data: 1.04e-04)max mem: 17.22449 GB 
[09/16 20:04:05 visual_prompt]: 	Test 200/407. loss: 0.960, 0.1822 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 20:04:24 visual_prompt]: 	Test 300/407. loss: 0.791, 0.1826 s / batch. (data: 1.30e-04)max mem: 17.22449 GB 
[09/16 20:04:43 visual_prompt]: 	Test 400/407. loss: 0.902, 0.1830 s / batch. (data: 4.43e-05)max mem: 17.22449 GB 
[09/16 20:04:47 visual_prompt]: Inference (test):avg data time: 6.89e-03, avg batch time: 0.1917, average loss: 0.8861
[09/16 20:04:47 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.59	top5: 95.77	
[09/16 20:04:47 visual_prompt]: Best epoch 47: best metric: 0.875
[09/16 20:04:47 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 20:04:58 visual_prompt]: Epoch 48 / 100: avg data time: 1.48e-01, avg batch time: 0.5690, average train loss: 0.5995
[09/16 20:05:02 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1444, average loss: 0.6671
[09/16 20:05:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 95.50	
[09/16 20:05:24 visual_prompt]: 	Test 100/407. loss: 1.556, 0.2162 s / batch. (data: 3.49e-02)max mem: 17.22449 GB 
[09/16 20:05:43 visual_prompt]: 	Test 200/407. loss: 1.242, 0.2092 s / batch. (data: 2.75e-02)max mem: 17.22449 GB 
[09/16 20:06:03 visual_prompt]: 	Test 300/407. loss: 1.171, 0.1959 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 20:06:22 visual_prompt]: 	Test 400/407. loss: 1.387, 0.1826 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 20:06:25 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1929, average loss: 1.2745
[09/16 20:06:26 visual_prompt]: Classification results with test_vtab-svhn: top1: 65.57	top5: 93.12	
[09/16 20:06:26 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 20:06:36 visual_prompt]: Epoch 49 / 100: avg data time: 1.59e-01, avg batch time: 0.5625, average train loss: 0.5431
[09/16 20:06:41 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1426, average loss: 0.3655
[09/16 20:06:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 100.00	
[09/16 20:07:02 visual_prompt]: 	Test 100/407. loss: 1.150, 0.1982 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 20:07:22 visual_prompt]: 	Test 200/407. loss: 1.151, 0.1947 s / batch. (data: 1.33e-02)max mem: 17.22449 GB 
[09/16 20:07:41 visual_prompt]: 	Test 300/407. loss: 1.143, 0.1843 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 20:08:01 visual_prompt]: 	Test 400/407. loss: 1.287, 0.1827 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 20:08:04 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1928, average loss: 1.0770
[09/16 20:08:04 visual_prompt]: Classification results with test_vtab-svhn: top1: 70.45	top5: 95.18	
[09/16 20:08:04 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 20:08:15 visual_prompt]: Epoch 50 / 100: avg data time: 1.64e-01, avg batch time: 0.5649, average train loss: 0.4585
[09/16 20:08:19 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1423, average loss: 0.4601
[09/16 20:08:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 99.50	
[09/16 20:08:41 visual_prompt]: 	Test 100/407. loss: 1.208, 0.2003 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 20:09:01 visual_prompt]: 	Test 200/407. loss: 1.090, 0.1974 s / batch. (data: 1.56e-02)max mem: 17.22449 GB 
[09/16 20:09:20 visual_prompt]: 	Test 300/407. loss: 0.911, 0.1880 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 20:09:39 visual_prompt]: 	Test 400/407. loss: 1.215, 0.1823 s / batch. (data: 3.70e-05)max mem: 17.22449 GB 
[09/16 20:09:43 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1931, average loss: 1.0417
[09/16 20:09:43 visual_prompt]: Classification results with test_vtab-svhn: top1: 71.17	top5: 96.44	
[09/16 20:09:43 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 20:09:53 visual_prompt]: Epoch 51 / 100: avg data time: 1.60e-01, avg batch time: 0.5612, average train loss: 0.3389
[09/16 20:09:58 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1427, average loss: 0.6124
[09/16 20:09:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 100.00	
[09/16 20:10:20 visual_prompt]: 	Test 100/407. loss: 1.889, 0.1961 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 20:10:39 visual_prompt]: 	Test 200/407. loss: 1.587, 0.1831 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 20:10:59 visual_prompt]: 	Test 300/407. loss: 1.537, 0.1958 s / batch. (data: 1.43e-04)max mem: 17.22449 GB 
[09/16 20:11:18 visual_prompt]: 	Test 400/407. loss: 1.419, 0.1819 s / batch. (data: 3.91e-05)max mem: 17.22449 GB 
[09/16 20:11:22 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1932, average loss: 1.4774
[09/16 20:11:22 visual_prompt]: Classification results with test_vtab-svhn: top1: 62.39	top5: 94.56	
[09/16 20:11:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 20:11:32 visual_prompt]: Epoch 52 / 100: avg data time: 1.59e-01, avg batch time: 0.5612, average train loss: 0.4077
[09/16 20:11:37 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1424, average loss: 0.4046
[09/16 20:11:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 99.50	
[09/16 20:11:59 visual_prompt]: 	Test 100/407. loss: 1.293, 0.2293 s / batch. (data: 1.72e-02)max mem: 17.22449 GB 
[09/16 20:12:18 visual_prompt]: 	Test 200/407. loss: 1.151, 0.2133 s / batch. (data: 3.19e-02)max mem: 17.22449 GB 
[09/16 20:12:37 visual_prompt]: 	Test 300/407. loss: 1.010, 0.1865 s / batch. (data: 1.45e-04)max mem: 17.22449 GB 
[09/16 20:12:57 visual_prompt]: 	Test 400/407. loss: 0.985, 0.1828 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 20:13:00 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1928, average loss: 1.1451
[09/16 20:13:00 visual_prompt]: Classification results with test_vtab-svhn: top1: 70.66	top5: 94.86	
[09/16 20:13:00 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 20:13:11 visual_prompt]: Epoch 53 / 100: avg data time: 1.46e-01, avg batch time: 0.5480, average train loss: 0.3606
[09/16 20:13:15 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1424, average loss: 0.1951
[09/16 20:13:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 92.00	top5: 100.00	
[09/16 20:13:37 visual_prompt]: 	Test 100/407. loss: 0.998, 0.1955 s / batch. (data: 1.03e-04)max mem: 17.22449 GB 
[09/16 20:13:56 visual_prompt]: 	Test 200/407. loss: 0.928, 0.1824 s / batch. (data: 1.43e-04)max mem: 17.22449 GB 
[09/16 20:14:16 visual_prompt]: 	Test 300/407. loss: 0.803, 0.1829 s / batch. (data: 1.15e-04)max mem: 17.22449 GB 
[09/16 20:14:35 visual_prompt]: 	Test 400/407. loss: 0.965, 0.1825 s / batch. (data: 2.81e-05)max mem: 17.22449 GB 
[09/16 20:14:38 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1930, average loss: 0.8754
[09/16 20:14:38 visual_prompt]: Classification results with test_vtab-svhn: top1: 75.53	top5: 96.85	
[09/16 20:14:38 visual_prompt]: Best epoch 53: best metric: 0.920
[09/16 20:14:38 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 20:14:49 visual_prompt]: Epoch 54 / 100: avg data time: 1.51e-01, avg batch time: 0.5545, average train loss: 0.2243
[09/16 20:14:54 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1425, average loss: 0.2667
[09/16 20:14:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 89.00	top5: 100.00	
[09/16 20:15:16 visual_prompt]: 	Test 100/407. loss: 1.508, 0.1838 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 20:15:35 visual_prompt]: 	Test 200/407. loss: 1.013, 0.1828 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 20:15:55 visual_prompt]: 	Test 300/407. loss: 1.296, 0.1972 s / batch. (data: 1.49e-02)max mem: 17.22449 GB 
[09/16 20:16:14 visual_prompt]: 	Test 400/407. loss: 1.120, 0.1828 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 20:16:18 visual_prompt]: Inference (test):avg data time: 8.66e-03, avg batch time: 0.1942, average loss: 1.1784
[09/16 20:16:18 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.15	top5: 97.27	
[09/16 20:16:18 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 20:16:28 visual_prompt]: Epoch 55 / 100: avg data time: 1.62e-01, avg batch time: 0.5632, average train loss: 0.2608
[09/16 20:16:33 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1425, average loss: 0.3246
[09/16 20:16:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 90.00	top5: 99.50	
[09/16 20:16:55 visual_prompt]: 	Test 100/407. loss: 1.124, 0.1962 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 20:17:14 visual_prompt]: 	Test 200/407. loss: 0.921, 0.1837 s / batch. (data: 2.93e-05)max mem: 17.22449 GB 
[09/16 20:17:34 visual_prompt]: 	Test 300/407. loss: 0.994, 0.1935 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 20:17:53 visual_prompt]: 	Test 400/407. loss: 1.274, 0.1832 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 20:17:56 visual_prompt]: Inference (test):avg data time: 8.10e-03, avg batch time: 0.1935, average loss: 0.9214
[09/16 20:17:56 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.32	top5: 95.32	
[09/16 20:17:56 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 20:18:07 visual_prompt]: Epoch 56 / 100: avg data time: 1.58e-01, avg batch time: 0.5816, average train loss: 0.2279
[09/16 20:18:12 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1424, average loss: 0.1208
[09/16 20:18:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 95.00	top5: 100.00	
[09/16 20:18:34 visual_prompt]: 	Test 100/407. loss: 1.395, 0.1826 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 20:18:53 visual_prompt]: 	Test 200/407. loss: 0.806, 0.1974 s / batch. (data: 1.56e-02)max mem: 17.22449 GB 
[09/16 20:19:13 visual_prompt]: 	Test 300/407. loss: 1.036, 0.1971 s / batch. (data: 1.50e-02)max mem: 17.22449 GB 
[09/16 20:19:32 visual_prompt]: 	Test 400/407. loss: 1.444, 0.1920 s / batch. (data: 2.88e-05)max mem: 17.22449 GB 
[09/16 20:19:35 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1939, average loss: 0.9844
[09/16 20:19:36 visual_prompt]: Classification results with test_vtab-svhn: top1: 75.78	top5: 97.33	
[09/16 20:19:36 visual_prompt]: Best epoch 56: best metric: 0.950
[09/16 20:19:36 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 20:19:46 visual_prompt]: Epoch 57 / 100: avg data time: 1.61e-01, avg batch time: 0.5605, average train loss: 0.1384
[09/16 20:19:51 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1425, average loss: 0.2669
[09/16 20:19:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 90.50	top5: 99.50	
[09/16 20:20:13 visual_prompt]: 	Test 100/407. loss: 2.227, 0.1962 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 20:20:32 visual_prompt]: 	Test 200/407. loss: 1.296, 0.1934 s / batch. (data: 1.13e-02)max mem: 17.22449 GB 
[09/16 20:20:52 visual_prompt]: 	Test 300/407. loss: 1.500, 0.2076 s / batch. (data: 2.53e-02)max mem: 17.22449 GB 
[09/16 20:21:11 visual_prompt]: 	Test 400/407. loss: 2.077, 0.1830 s / batch. (data: 3.79e-05)max mem: 17.22449 GB 
[09/16 20:21:14 visual_prompt]: Inference (test):avg data time: 8.50e-03, avg batch time: 0.1935, average loss: 1.4320
[09/16 20:21:14 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.47	top5: 95.19	
[09/16 20:21:14 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 20:21:25 visual_prompt]: Epoch 58 / 100: avg data time: 1.54e-01, avg batch time: 0.5568, average train loss: 0.2494
[09/16 20:21:29 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1425, average loss: 0.1453
[09/16 20:21:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 95.50	top5: 100.00	
[09/16 20:21:51 visual_prompt]: 	Test 100/407. loss: 1.284, 0.1882 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 20:22:10 visual_prompt]: 	Test 200/407. loss: 0.914, 0.1960 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 20:22:30 visual_prompt]: 	Test 300/407. loss: 1.004, 0.1958 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 20:22:50 visual_prompt]: 	Test 400/407. loss: 1.123, 0.1916 s / batch. (data: 4.03e-05)max mem: 17.22449 GB 
[09/16 20:22:53 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1937, average loss: 0.9379
[09/16 20:22:53 visual_prompt]: Classification results with test_vtab-svhn: top1: 75.80	top5: 96.89	
[09/16 20:22:53 visual_prompt]: Best epoch 58: best metric: 0.955
[09/16 20:22:53 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 20:23:03 visual_prompt]: Epoch 59 / 100: avg data time: 1.54e-01, avg batch time: 0.5556, average train loss: 0.2734
[09/16 20:23:08 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1426, average loss: 0.1617
[09/16 20:23:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 94.00	top5: 100.00	
[09/16 20:23:30 visual_prompt]: 	Test 100/407. loss: 1.655, 0.1955 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 20:23:49 visual_prompt]: 	Test 200/407. loss: 0.860, 0.1986 s / batch. (data: 1.03e-02)max mem: 17.22449 GB 
[09/16 20:24:08 visual_prompt]: 	Test 300/407. loss: 1.339, 0.1828 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 20:24:28 visual_prompt]: 	Test 400/407. loss: 1.414, 0.1825 s / batch. (data: 3.53e-05)max mem: 17.22449 GB 
[09/16 20:24:31 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1932, average loss: 1.1789
[09/16 20:24:31 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.42	top5: 97.17	
[09/16 20:24:31 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 20:24:42 visual_prompt]: Epoch 60 / 100: avg data time: 1.46e-01, avg batch time: 0.5488, average train loss: 0.2450
[09/16 20:24:46 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1423, average loss: 0.1313
[09/16 20:24:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 95.50	top5: 100.00	
[09/16 20:25:08 visual_prompt]: 	Test 100/407. loss: 1.299, 0.1937 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 20:25:28 visual_prompt]: 	Test 200/407. loss: 0.981, 0.1954 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 20:25:47 visual_prompt]: 	Test 300/407. loss: 0.998, 0.2178 s / batch. (data: 1.42e-02)max mem: 17.22449 GB 
[09/16 20:26:07 visual_prompt]: 	Test 400/407. loss: 1.363, 0.1824 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 20:26:10 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1942, average loss: 0.8982
[09/16 20:26:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.14	top5: 96.99	
[09/16 20:26:10 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 20:26:21 visual_prompt]: Epoch 61 / 100: avg data time: 1.54e-01, avg batch time: 0.5548, average train loss: 0.1419
[09/16 20:26:25 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1425, average loss: 0.0973
[09/16 20:26:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 96.50	top5: 100.00	
[09/16 20:26:47 visual_prompt]: 	Test 100/407. loss: 1.734, 0.1828 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 20:27:06 visual_prompt]: 	Test 200/407. loss: 0.949, 0.2362 s / batch. (data: 5.41e-02)max mem: 17.22449 GB 
[09/16 20:27:26 visual_prompt]: 	Test 300/407. loss: 1.144, 0.1829 s / batch. (data: 1.67e-04)max mem: 17.22449 GB 
[09/16 20:27:46 visual_prompt]: 	Test 400/407. loss: 1.266, 0.1826 s / batch. (data: 3.19e-05)max mem: 17.22449 GB 
[09/16 20:27:49 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1935, average loss: 1.0786
[09/16 20:27:49 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.12	top5: 97.23	
[09/16 20:27:49 visual_prompt]: Best epoch 61: best metric: 0.965
[09/16 20:27:49 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 20:27:59 visual_prompt]: Epoch 62 / 100: avg data time: 1.51e-01, avg batch time: 0.5536, average train loss: 0.1715
[09/16 20:28:04 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1423, average loss: 0.2751
[09/16 20:28:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 92.00	top5: 100.00	
[09/16 20:28:26 visual_prompt]: 	Test 100/407. loss: 2.130, 0.1826 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 20:28:45 visual_prompt]: 	Test 200/407. loss: 1.668, 0.1820 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 20:29:05 visual_prompt]: 	Test 300/407. loss: 1.506, 0.2080 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 20:29:25 visual_prompt]: 	Test 400/407. loss: 1.321, 0.1835 s / batch. (data: 2.74e-05)max mem: 17.22449 GB 
[09/16 20:29:28 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1947, average loss: 1.4784
[09/16 20:29:28 visual_prompt]: Classification results with test_vtab-svhn: top1: 72.89	top5: 95.83	
[09/16 20:29:28 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 20:29:38 visual_prompt]: Epoch 63 / 100: avg data time: 1.56e-01, avg batch time: 0.5565, average train loss: 0.1312
[09/16 20:29:43 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1423, average loss: 0.1495
[09/16 20:29:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 95.50	top5: 100.00	
[09/16 20:30:05 visual_prompt]: 	Test 100/407. loss: 1.572, 0.1827 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 20:30:24 visual_prompt]: 	Test 200/407. loss: 1.248, 0.2008 s / batch. (data: 1.23e-02)max mem: 17.22449 GB 
[09/16 20:30:44 visual_prompt]: 	Test 300/407. loss: 1.173, 0.1960 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 20:31:03 visual_prompt]: 	Test 400/407. loss: 1.238, 0.1827 s / batch. (data: 2.84e-05)max mem: 17.22449 GB 
[09/16 20:31:06 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1936, average loss: 1.1493
[09/16 20:31:07 visual_prompt]: Classification results with test_vtab-svhn: top1: 75.40	top5: 96.83	
[09/16 20:31:07 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 20:31:17 visual_prompt]: Epoch 64 / 100: avg data time: 1.58e-01, avg batch time: 0.5584, average train loss: 0.0955
[09/16 20:31:22 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1426, average loss: 0.0877
[09/16 20:31:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 95.50	top5: 100.00	
[09/16 20:31:44 visual_prompt]: 	Test 100/407. loss: 1.143, 0.1842 s / batch. (data: 1.10e-04)max mem: 17.22449 GB 
[09/16 20:32:03 visual_prompt]: 	Test 200/407. loss: 1.785, 0.1841 s / batch. (data: 1.40e-04)max mem: 17.22449 GB 
[09/16 20:32:23 visual_prompt]: 	Test 300/407. loss: 0.853, 0.2081 s / batch. (data: 2.59e-02)max mem: 17.22449 GB 
[09/16 20:32:42 visual_prompt]: 	Test 400/407. loss: 1.678, 0.1825 s / batch. (data: 3.12e-05)max mem: 17.22449 GB 
[09/16 20:32:45 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1939, average loss: 1.1914
[09/16 20:32:45 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.57	top5: 97.24	
[09/16 20:32:45 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 20:32:56 visual_prompt]: Epoch 65 / 100: avg data time: 1.54e-01, avg batch time: 0.5546, average train loss: 0.1348
[09/16 20:33:01 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1426, average loss: 0.0939
[09/16 20:33:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 96.50	top5: 100.00	
[09/16 20:33:22 visual_prompt]: 	Test 100/407. loss: 1.968, 0.1959 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 20:33:42 visual_prompt]: 	Test 200/407. loss: 1.405, 0.2023 s / batch. (data: 2.04e-02)max mem: 17.22449 GB 
[09/16 20:34:01 visual_prompt]: 	Test 300/407. loss: 1.435, 0.1835 s / batch. (data: 1.65e-04)max mem: 17.22449 GB 
[09/16 20:34:21 visual_prompt]: 	Test 400/407. loss: 1.734, 0.1825 s / batch. (data: 3.55e-05)max mem: 17.22449 GB 
[09/16 20:34:24 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1937, average loss: 1.3118
[09/16 20:34:24 visual_prompt]: Classification results with test_vtab-svhn: top1: 73.66	top5: 96.35	
[09/16 20:34:24 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 20:34:35 visual_prompt]: Epoch 66 / 100: avg data time: 1.50e-01, avg batch time: 0.5533, average train loss: 0.1133
[09/16 20:34:40 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1430, average loss: 0.0530
[09/16 20:34:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 98.50	top5: 100.00	
[09/16 20:35:01 visual_prompt]: 	Test 100/407. loss: 1.905, 0.1963 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 20:35:21 visual_prompt]: 	Test 200/407. loss: 1.230, 0.1815 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 20:35:40 visual_prompt]: 	Test 300/407. loss: 1.189, 0.1869 s / batch. (data: 8.58e-05)max mem: 17.22449 GB 
[09/16 20:36:00 visual_prompt]: 	Test 400/407. loss: 1.713, 0.1828 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 20:36:03 visual_prompt]: Inference (test):avg data time: 8.39e-03, avg batch time: 0.1937, average loss: 1.1995
[09/16 20:36:03 visual_prompt]: Classification results with test_vtab-svhn: top1: 76.99	top5: 97.03	
[09/16 20:36:03 visual_prompt]: Best epoch 66: best metric: 0.985
[09/16 20:36:03 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 20:36:14 visual_prompt]: Epoch 67 / 100: avg data time: 1.49e-01, avg batch time: 0.5521, average train loss: 0.0567
[09/16 20:36:18 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1425, average loss: 0.1281
[09/16 20:36:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 97.00	top5: 100.00	
[09/16 20:36:40 visual_prompt]: 	Test 100/407. loss: 2.472, 0.1876 s / batch. (data: 5.23e-03)max mem: 17.22449 GB 
[09/16 20:37:00 visual_prompt]: 	Test 200/407. loss: 2.103, 0.1825 s / batch. (data: 1.11e-04)max mem: 17.22449 GB 
[09/16 20:37:19 visual_prompt]: 	Test 300/407. loss: 1.236, 0.2152 s / batch. (data: 1.86e-02)max mem: 17.22449 GB 
[09/16 20:37:38 visual_prompt]: 	Test 400/407. loss: 1.382, 0.1821 s / batch. (data: 3.53e-05)max mem: 17.22449 GB 
[09/16 20:37:42 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1927, average loss: 1.6695
[09/16 20:37:42 visual_prompt]: Classification results with test_vtab-svhn: top1: 74.58	top5: 96.37	
[09/16 20:37:42 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 20:37:52 visual_prompt]: Epoch 68 / 100: avg data time: 1.63e-01, avg batch time: 0.5623, average train loss: 0.0914
[09/16 20:37:57 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1463, average loss: 0.0732
[09/16 20:37:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 98.00	top5: 100.00	
[09/16 20:38:19 visual_prompt]: 	Test 100/407. loss: 2.392, 0.2175 s / batch. (data: 3.45e-04)max mem: 17.22449 GB 
[09/16 20:38:38 visual_prompt]: 	Test 200/407. loss: 1.752, 0.1826 s / batch. (data: 1.05e-04)max mem: 17.22449 GB 
[09/16 20:38:58 visual_prompt]: 	Test 300/407. loss: 1.352, 0.1822 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 20:39:17 visual_prompt]: 	Test 400/407. loss: 1.268, 0.1819 s / batch. (data: 3.84e-05)max mem: 17.22449 GB 
[09/16 20:39:21 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1939, average loss: 1.4941
[09/16 20:39:21 visual_prompt]: Classification results with test_vtab-svhn: top1: 74.75	top5: 96.67	
[09/16 20:39:21 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 20:39:31 visual_prompt]: Epoch 69 / 100: avg data time: 1.50e-01, avg batch time: 0.5543, average train loss: 0.0498
[09/16 20:39:36 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1423, average loss: 0.0066
[09/16 20:39:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 20:39:58 visual_prompt]: 	Test 100/407. loss: 1.507, 0.2119 s / batch. (data: 3.02e-02)max mem: 17.22449 GB 
[09/16 20:40:17 visual_prompt]: 	Test 200/407. loss: 1.104, 0.1958 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 20:40:37 visual_prompt]: 	Test 300/407. loss: 1.072, 0.2171 s / batch. (data: 2.98e-02)max mem: 17.22449 GB 
[09/16 20:40:57 visual_prompt]: 	Test 400/407. loss: 1.460, 0.1822 s / batch. (data: 3.27e-05)max mem: 17.22449 GB 
[09/16 20:41:00 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1934, average loss: 1.0489
[09/16 20:41:00 visual_prompt]: Classification results with test_vtab-svhn: top1: 80.53	top5: 97.02	
[09/16 20:41:00 visual_prompt]: Best epoch 69: best metric: 1.000
[09/16 20:41:00 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 20:41:11 visual_prompt]: Epoch 70 / 100: avg data time: 1.64e-01, avg batch time: 0.5662, average train loss: 0.0736
[09/16 20:41:15 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1425, average loss: 0.2213
[09/16 20:41:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 93.50	top5: 100.00	
[09/16 20:41:37 visual_prompt]: 	Test 100/407. loss: 2.608, 0.1872 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 20:41:56 visual_prompt]: 	Test 200/407. loss: 2.196, 0.1820 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 20:42:16 visual_prompt]: 	Test 300/407. loss: 2.236, 0.1973 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 20:42:35 visual_prompt]: 	Test 400/407. loss: 2.905, 0.1828 s / batch. (data: 2.96e-05)max mem: 17.22449 GB 
[09/16 20:42:38 visual_prompt]: Inference (test):avg data time: 6.92e-03, avg batch time: 0.1922, average loss: 1.8980
[09/16 20:42:38 visual_prompt]: Classification results with test_vtab-svhn: top1: 74.12	top5: 96.07	
[09/16 20:42:38 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 20:42:49 visual_prompt]: Epoch 71 / 100: avg data time: 1.54e-01, avg batch time: 0.5564, average train loss: 0.1311
[09/16 20:42:54 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1426, average loss: 0.0394
[09/16 20:42:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 20:43:16 visual_prompt]: 	Test 100/407. loss: 1.473, 0.2063 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 20:43:35 visual_prompt]: 	Test 200/407. loss: 0.634, 0.1975 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 20:43:55 visual_prompt]: 	Test 300/407. loss: 1.187, 0.1954 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 20:44:15 visual_prompt]: 	Test 400/407. loss: 1.304, 0.1830 s / batch. (data: 2.81e-05)max mem: 17.22449 GB 
[09/16 20:44:18 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1955, average loss: 0.9585
[09/16 20:44:18 visual_prompt]: Classification results with test_vtab-svhn: top1: 79.58	top5: 98.10	
[09/16 20:44:18 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 20:44:29 visual_prompt]: Epoch 72 / 100: avg data time: 1.60e-01, avg batch time: 0.5596, average train loss: 0.0552
[09/16 20:44:33 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1425, average loss: 0.0092
[09/16 20:44:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 20:44:55 visual_prompt]: 	Test 100/407. loss: 1.391, 0.2123 s / batch. (data: 2.58e-02)max mem: 17.22449 GB 
[09/16 20:45:15 visual_prompt]: 	Test 200/407. loss: 1.082, 0.1835 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 20:45:34 visual_prompt]: 	Test 300/407. loss: 1.049, 0.1992 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 20:45:54 visual_prompt]: 	Test 400/407. loss: 0.983, 0.1838 s / batch. (data: 3.39e-05)max mem: 17.22449 GB 
[09/16 20:45:57 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1937, average loss: 0.9897
[09/16 20:45:57 visual_prompt]: Classification results with test_vtab-svhn: top1: 79.89	top5: 97.59	
[09/16 20:45:57 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 20:46:08 visual_prompt]: Epoch 73 / 100: avg data time: 1.62e-01, avg batch time: 0.5617, average train loss: 0.0168
[09/16 20:46:12 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1426, average loss: 0.0239
[09/16 20:46:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 20:46:34 visual_prompt]: 	Test 100/407. loss: 2.103, 0.2042 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 20:46:53 visual_prompt]: 	Test 200/407. loss: 1.627, 0.2010 s / batch. (data: 1.23e-02)max mem: 17.22449 GB 
[09/16 20:47:12 visual_prompt]: 	Test 300/407. loss: 1.591, 0.2108 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 20:47:32 visual_prompt]: 	Test 400/407. loss: 1.874, 0.1832 s / batch. (data: 3.55e-05)max mem: 17.22449 GB 
[09/16 20:47:35 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1932, average loss: 1.3971
[09/16 20:47:36 visual_prompt]: Classification results with test_vtab-svhn: top1: 77.81	top5: 97.10	
[09/16 20:47:36 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 20:47:46 visual_prompt]: Epoch 74 / 100: avg data time: 1.55e-01, avg batch time: 0.5554, average train loss: 0.0167
[09/16 20:47:51 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1427, average loss: 0.0292
[09/16 20:47:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.50	top5: 100.00	
[09/16 20:48:13 visual_prompt]: 	Test 100/407. loss: 1.926, 0.1916 s / batch. (data: 1.05e-04)max mem: 17.22449 GB 
[09/16 20:48:32 visual_prompt]: 	Test 200/407. loss: 1.566, 0.2094 s / batch. (data: 2.38e-02)max mem: 17.22449 GB 
[09/16 20:48:51 visual_prompt]: 	Test 300/407. loss: 1.662, 0.1826 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 20:49:11 visual_prompt]: 	Test 400/407. loss: 1.653, 0.1825 s / batch. (data: 3.15e-05)max mem: 17.22449 GB 
[09/16 20:49:15 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1936, average loss: 1.4087
[09/16 20:49:15 visual_prompt]: Classification results with test_vtab-svhn: top1: 79.52	top5: 97.28	
[09/16 20:49:15 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 20:49:25 visual_prompt]: Epoch 75 / 100: avg data time: 1.51e-01, avg batch time: 0.5525, average train loss: 0.0223
[09/16 20:49:30 visual_prompt]: Inference (val):avg data time: 4.82e-05, avg batch time: 0.1463, average loss: 0.0026
[09/16 20:49:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 20:49:52 visual_prompt]: 	Test 100/407. loss: 2.679, 0.1830 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 20:50:11 visual_prompt]: 	Test 200/407. loss: 1.682, 0.1836 s / batch. (data: 1.42e-04)max mem: 17.22449 GB 
[09/16 20:50:31 visual_prompt]: 	Test 300/407. loss: 1.920, 0.2097 s / batch. (data: 1.29e-02)max mem: 17.22449 GB 
[09/16 20:50:50 visual_prompt]: 	Test 400/407. loss: 2.425, 0.1854 s / batch. (data: 2.91e-05)max mem: 17.22449 GB 
[09/16 20:50:54 visual_prompt]: Inference (test):avg data time: 6.68e-03, avg batch time: 0.1927, average loss: 1.5958
[09/16 20:50:54 visual_prompt]: Classification results with test_vtab-svhn: top1: 78.73	top5: 97.35	
[09/16 20:50:54 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 20:51:05 visual_prompt]: Epoch 76 / 100: avg data time: 1.51e-01, avg batch time: 0.5551, average train loss: 0.0053
[09/16 20:51:09 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1426, average loss: 0.0162
[09/16 20:51:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 20:51:31 visual_prompt]: 	Test 100/407. loss: 2.153, 0.1823 s / batch. (data: 3.58e-04)max mem: 17.22449 GB 
[09/16 20:51:50 visual_prompt]: 	Test 200/407. loss: 1.419, 0.1834 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 20:52:10 visual_prompt]: 	Test 300/407. loss: 1.398, 0.1844 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 20:52:29 visual_prompt]: 	Test 400/407. loss: 2.038, 0.1820 s / batch. (data: 3.10e-05)max mem: 17.22449 GB 
[09/16 20:52:33 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1926, average loss: 1.3360
[09/16 20:52:33 visual_prompt]: Classification results with test_vtab-svhn: top1: 80.47	top5: 97.40	
[09/16 20:52:33 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 20:52:43 visual_prompt]: Epoch 77 / 100: avg data time: 1.62e-01, avg batch time: 0.5621, average train loss: 0.0099
[09/16 20:52:48 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1423, average loss: 0.0573
[09/16 20:52:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 99.00	top5: 100.00	
[09/16 20:53:10 visual_prompt]: 	Test 100/407. loss: 2.419, 0.2117 s / batch. (data: 3.05e-02)max mem: 17.22449 GB 
[09/16 20:53:29 visual_prompt]: 	Test 200/407. loss: 1.677, 0.1821 s / batch. (data: 3.62e-05)max mem: 17.22449 GB 
[09/16 20:53:49 visual_prompt]: 	Test 300/407. loss: 2.032, 0.2066 s / batch. (data: 6.34e-05)max mem: 17.22449 GB 
[09/16 20:54:08 visual_prompt]: 	Test 400/407. loss: 2.300, 0.1833 s / batch. (data: 2.96e-05)max mem: 17.22449 GB 
[09/16 20:54:11 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1938, average loss: 1.6399
[09/16 20:54:12 visual_prompt]: Classification results with test_vtab-svhn: top1: 78.43	top5: 97.31	
[09/16 20:54:12 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 20:54:22 visual_prompt]: Epoch 78 / 100: avg data time: 1.47e-01, avg batch time: 0.5478, average train loss: 0.0061
[09/16 20:54:26 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1437, average loss: 0.0011
[09/16 20:54:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 20:54:48 visual_prompt]: 	Test 100/407. loss: 1.872, 0.1828 s / batch. (data: 1.23e-04)max mem: 17.22449 GB 
[09/16 20:55:08 visual_prompt]: 	Test 200/407. loss: 1.146, 0.1968 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 20:55:28 visual_prompt]: 	Test 300/407. loss: 1.384, 0.1827 s / batch. (data: 1.35e-04)max mem: 17.22449 GB 
[09/16 20:55:47 visual_prompt]: 	Test 400/407. loss: 2.148, 0.1834 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 20:55:51 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1954, average loss: 1.2441
[09/16 20:55:51 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.50	top5: 97.94	
[09/16 20:55:51 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 20:56:01 visual_prompt]: Epoch 79 / 100: avg data time: 1.49e-01, avg batch time: 0.5520, average train loss: 0.0028
[09/16 20:56:06 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1426, average loss: 0.0005
[09/16 20:56:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 20:56:28 visual_prompt]: 	Test 100/407. loss: 2.141, 0.1841 s / batch. (data: 1.39e-04)max mem: 17.22449 GB 
[09/16 20:56:47 visual_prompt]: 	Test 200/407. loss: 1.413, 0.1820 s / batch. (data: 1.37e-04)max mem: 17.22449 GB 
[09/16 20:57:06 visual_prompt]: 	Test 300/407. loss: 1.480, 0.1827 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 20:57:26 visual_prompt]: 	Test 400/407. loss: 2.171, 0.1820 s / batch. (data: 3.96e-05)max mem: 17.22449 GB 
[09/16 20:57:29 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1929, average loss: 1.3316
[09/16 20:57:29 visual_prompt]: Classification results with test_vtab-svhn: top1: 80.97	top5: 97.65	
[09/16 20:57:29 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 20:57:40 visual_prompt]: Epoch 80 / 100: avg data time: 1.59e-01, avg batch time: 0.5626, average train loss: 0.0018
[09/16 20:57:44 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1458, average loss: 0.0015
[09/16 20:57:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 20:58:06 visual_prompt]: 	Test 100/407. loss: 2.313, 0.2034 s / batch. (data: 1.48e-02)max mem: 17.22449 GB 
[09/16 20:58:26 visual_prompt]: 	Test 200/407. loss: 1.492, 0.2098 s / batch. (data: 2.78e-02)max mem: 17.22449 GB 
[09/16 20:58:45 visual_prompt]: 	Test 300/407. loss: 1.645, 0.1957 s / batch. (data: 1.31e-02)max mem: 17.22449 GB 
[09/16 20:59:05 visual_prompt]: 	Test 400/407. loss: 2.225, 0.1837 s / batch. (data: 3.05e-05)max mem: 17.22449 GB 
[09/16 20:59:08 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1938, average loss: 1.3887
[09/16 20:59:08 visual_prompt]: Classification results with test_vtab-svhn: top1: 80.21	top5: 97.29	
[09/16 20:59:08 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 20:59:19 visual_prompt]: Epoch 81 / 100: avg data time: 1.59e-01, avg batch time: 0.5593, average train loss: 0.0010
[09/16 20:59:23 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1472, average loss: 0.0002
[09/16 20:59:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 20:59:45 visual_prompt]: 	Test 100/407. loss: 2.002, 0.1832 s / batch. (data: 1.24e-04)max mem: 17.22449 GB 
[09/16 21:00:05 visual_prompt]: 	Test 200/407. loss: 1.343, 0.1916 s / batch. (data: 1.09e-04)max mem: 17.22449 GB 
[09/16 21:00:24 visual_prompt]: 	Test 300/407. loss: 1.524, 0.1962 s / batch. (data: 1.36e-02)max mem: 17.22449 GB 
[09/16 21:00:44 visual_prompt]: 	Test 400/407. loss: 2.218, 0.1832 s / batch. (data: 2.88e-05)max mem: 17.22449 GB 
[09/16 21:00:47 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1955, average loss: 1.2773
[09/16 21:00:47 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.68	top5: 97.79	
[09/16 21:00:47 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 21:00:58 visual_prompt]: Epoch 82 / 100: avg data time: 1.61e-01, avg batch time: 0.5627, average train loss: 0.0004
[09/16 21:01:03 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1425, average loss: 0.0002
[09/16 21:01:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:01:24 visual_prompt]: 	Test 100/407. loss: 1.924, 0.2066 s / batch. (data: 1.40e-02)max mem: 17.22449 GB 
[09/16 21:01:44 visual_prompt]: 	Test 200/407. loss: 1.312, 0.1962 s / batch. (data: 1.37e-02)max mem: 17.22449 GB 
[09/16 21:02:04 visual_prompt]: 	Test 300/407. loss: 1.501, 0.2355 s / batch. (data: 2.54e-02)max mem: 17.22449 GB 
[09/16 21:02:24 visual_prompt]: 	Test 400/407. loss: 2.208, 0.1829 s / batch. (data: 3.03e-05)max mem: 17.22449 GB 
[09/16 21:02:27 visual_prompt]: Inference (test):avg data time: 8.53e-03, avg batch time: 0.1965, average loss: 1.2703
[09/16 21:02:27 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.71	top5: 97.82	
[09/16 21:02:27 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 21:02:38 visual_prompt]: Epoch 83 / 100: avg data time: 1.50e-01, avg batch time: 0.5520, average train loss: 0.0005
[09/16 21:02:43 visual_prompt]: Inference (val):avg data time: 7.26e-05, avg batch time: 0.1484, average loss: 0.0002
[09/16 21:02:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:03:04 visual_prompt]: 	Test 100/407. loss: 1.858, 0.1948 s / batch. (data: 1.30e-02)max mem: 17.22449 GB 
[09/16 21:03:24 visual_prompt]: 	Test 200/407. loss: 1.317, 0.2174 s / batch. (data: 1.52e-02)max mem: 17.22449 GB 
[09/16 21:03:43 visual_prompt]: 	Test 300/407. loss: 1.504, 0.1825 s / batch. (data: 1.17e-04)max mem: 17.22449 GB 
[09/16 21:04:04 visual_prompt]: 	Test 400/407. loss: 2.212, 0.1829 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 21:04:07 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1952, average loss: 1.2577
[09/16 21:04:07 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.83	top5: 97.82	
[09/16 21:04:07 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 21:04:17 visual_prompt]: Epoch 84 / 100: avg data time: 1.37e-01, avg batch time: 0.5372, average train loss: 0.0004
[09/16 21:04:22 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1427, average loss: 0.0002
[09/16 21:04:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:04:44 visual_prompt]: 	Test 100/407. loss: 1.851, 0.2149 s / batch. (data: 1.35e-02)max mem: 17.22449 GB 
[09/16 21:05:03 visual_prompt]: 	Test 200/407. loss: 1.324, 0.1959 s / batch. (data: 1.41e-02)max mem: 17.22449 GB 
[09/16 21:05:23 visual_prompt]: 	Test 300/407. loss: 1.503, 0.1946 s / batch. (data: 1.29e-02)max mem: 17.22449 GB 
[09/16 21:05:42 visual_prompt]: 	Test 400/407. loss: 2.173, 0.1829 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 21:05:46 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1937, average loss: 1.2523
[09/16 21:05:46 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.81	top5: 97.83	
[09/16 21:05:46 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 21:05:56 visual_prompt]: Epoch 85 / 100: avg data time: 1.59e-01, avg batch time: 0.5597, average train loss: 0.0003
[09/16 21:06:01 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1426, average loss: 0.0002
[09/16 21:06:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:06:23 visual_prompt]: 	Test 100/407. loss: 1.853, 0.1819 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 21:06:42 visual_prompt]: 	Test 200/407. loss: 1.317, 0.1828 s / batch. (data: 1.53e-04)max mem: 17.22449 GB 
[09/16 21:07:03 visual_prompt]: 	Test 300/407. loss: 1.503, 0.1829 s / batch. (data: 1.29e-04)max mem: 17.22449 GB 
[09/16 21:07:22 visual_prompt]: 	Test 400/407. loss: 2.145, 0.1832 s / batch. (data: 3.50e-05)max mem: 17.22449 GB 
[09/16 21:07:26 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1965, average loss: 1.2497
[09/16 21:07:26 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.73	top5: 97.84	
[09/16 21:07:26 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 21:07:36 visual_prompt]: Epoch 86 / 100: avg data time: 1.57e-01, avg batch time: 0.5579, average train loss: 0.0004
[09/16 21:07:41 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1441, average loss: 0.0002
[09/16 21:07:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:08:03 visual_prompt]: 	Test 100/407. loss: 1.846, 0.1972 s / batch. (data: 1.53e-02)max mem: 17.22449 GB 
[09/16 21:08:23 visual_prompt]: 	Test 200/407. loss: 1.311, 0.1972 s / batch. (data: 1.51e-02)max mem: 17.22449 GB 
[09/16 21:08:42 visual_prompt]: 	Test 300/407. loss: 1.493, 0.1920 s / batch. (data: 9.49e-03)max mem: 17.22449 GB 
[09/16 21:09:01 visual_prompt]: 	Test 400/407. loss: 2.150, 0.1821 s / batch. (data: 2.43e-05)max mem: 17.22449 GB 
[09/16 21:09:05 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1934, average loss: 1.2447
[09/16 21:09:05 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.74	top5: 97.85	
[09/16 21:09:05 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 21:09:15 visual_prompt]: Epoch 87 / 100: avg data time: 1.58e-01, avg batch time: 0.5578, average train loss: 0.0004
[09/16 21:09:20 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1423, average loss: 0.0002
[09/16 21:09:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:09:42 visual_prompt]: 	Test 100/407. loss: 1.851, 0.1826 s / batch. (data: 3.58e-05)max mem: 17.22449 GB 
[09/16 21:10:01 visual_prompt]: 	Test 200/407. loss: 1.291, 0.1982 s / batch. (data: 1.09e-04)max mem: 17.22449 GB 
[09/16 21:10:20 visual_prompt]: 	Test 300/407. loss: 1.497, 0.1911 s / batch. (data: 1.58e-04)max mem: 17.22449 GB 
[09/16 21:10:40 visual_prompt]: 	Test 400/407. loss: 2.136, 0.1829 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 21:10:43 visual_prompt]: Inference (test):avg data time: 6.96e-03, avg batch time: 0.1920, average loss: 1.2407
[09/16 21:10:43 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.78	top5: 97.88	
[09/16 21:10:43 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 21:10:54 visual_prompt]: Epoch 88 / 100: avg data time: 1.54e-01, avg batch time: 0.5533, average train loss: 0.0004
[09/16 21:10:58 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1424, average loss: 0.0002
[09/16 21:10:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:11:20 visual_prompt]: 	Test 100/407. loss: 1.848, 0.1949 s / batch. (data: 1.27e-04)max mem: 17.22449 GB 
[09/16 21:11:39 visual_prompt]: 	Test 200/407. loss: 1.270, 0.1891 s / batch. (data: 1.13e-04)max mem: 17.22449 GB 
[09/16 21:11:59 visual_prompt]: 	Test 300/407. loss: 1.495, 0.1956 s / batch. (data: 1.34e-02)max mem: 17.22449 GB 
[09/16 21:12:18 visual_prompt]: 	Test 400/407. loss: 2.123, 0.1823 s / batch. (data: 3.22e-05)max mem: 17.22449 GB 
[09/16 21:12:21 visual_prompt]: Inference (test):avg data time: 6.93e-03, avg batch time: 0.1930, average loss: 1.2367
[09/16 21:12:22 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.89	top5: 97.91	
[09/16 21:12:22 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 21:12:32 visual_prompt]: Epoch 89 / 100: avg data time: 1.41e-01, avg batch time: 0.5470, average train loss: 0.0005
[09/16 21:12:36 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1425, average loss: 0.0002
[09/16 21:12:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:12:59 visual_prompt]: 	Test 100/407. loss: 1.847, 0.2754 s / batch. (data: 1.46e-02)max mem: 17.22449 GB 
[09/16 21:13:18 visual_prompt]: 	Test 200/407. loss: 1.270, 0.1819 s / batch. (data: 3.43e-05)max mem: 17.22449 GB 
[09/16 21:13:38 visual_prompt]: 	Test 300/407. loss: 1.501, 0.1821 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 21:13:57 visual_prompt]: 	Test 400/407. loss: 2.118, 0.1825 s / batch. (data: 2.98e-05)max mem: 17.22449 GB 
[09/16 21:14:00 visual_prompt]: Inference (test):avg data time: 8.21e-03, avg batch time: 0.1949, average loss: 1.2362
[09/16 21:14:01 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.82	top5: 97.90	
[09/16 21:14:01 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 21:14:11 visual_prompt]: Epoch 90 / 100: avg data time: 1.44e-01, avg batch time: 0.5469, average train loss: 0.0002
[09/16 21:14:16 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1425, average loss: 0.0002
[09/16 21:14:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:14:38 visual_prompt]: 	Test 100/407. loss: 1.846, 0.1949 s / batch. (data: 1.19e-04)max mem: 17.22449 GB 
[09/16 21:14:57 visual_prompt]: 	Test 200/407. loss: 1.270, 0.1918 s / batch. (data: 1.12e-04)max mem: 17.22449 GB 
[09/16 21:15:17 visual_prompt]: 	Test 300/407. loss: 1.507, 0.1824 s / batch. (data: 1.15e-04)max mem: 17.22449 GB 
[09/16 21:15:37 visual_prompt]: 	Test 400/407. loss: 2.115, 0.1823 s / batch. (data: 3.19e-05)max mem: 17.22449 GB 
[09/16 21:15:40 visual_prompt]: Inference (test):avg data time: 8.31e-03, avg batch time: 0.1956, average loss: 1.2363
[09/16 21:15:40 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.82	top5: 97.88	
[09/16 21:15:40 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 21:15:51 visual_prompt]: Epoch 91 / 100: avg data time: 1.64e-01, avg batch time: 0.5653, average train loss: 0.0003
[09/16 21:15:56 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1424, average loss: 0.0002
[09/16 21:15:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:16:17 visual_prompt]: 	Test 100/407. loss: 1.844, 0.1819 s / batch. (data: 7.58e-05)max mem: 17.22449 GB 
[09/16 21:16:37 visual_prompt]: 	Test 200/407. loss: 1.268, 0.1820 s / batch. (data: 1.33e-04)max mem: 17.22449 GB 
[09/16 21:16:56 visual_prompt]: 	Test 300/407. loss: 1.506, 0.1982 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 21:17:16 visual_prompt]: 	Test 400/407. loss: 2.110, 0.1821 s / batch. (data: 3.39e-05)max mem: 17.22449 GB 
[09/16 21:17:19 visual_prompt]: Inference (test):avg data time: 6.68e-03, avg batch time: 0.1940, average loss: 1.2349
[09/16 21:17:19 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.80	top5: 97.87	
[09/16 21:17:19 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 21:17:30 visual_prompt]: Epoch 92 / 100: avg data time: 1.58e-01, avg batch time: 0.5576, average train loss: 0.0004
[09/16 21:17:35 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1426, average loss: 0.0002
[09/16 21:17:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:17:57 visual_prompt]: 	Test 100/407. loss: 1.841, 0.1947 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 21:18:16 visual_prompt]: 	Test 200/407. loss: 1.264, 0.1979 s / batch. (data: 1.56e-02)max mem: 17.22449 GB 
[09/16 21:18:36 visual_prompt]: 	Test 300/407. loss: 1.504, 0.1919 s / batch. (data: 9.76e-03)max mem: 17.22449 GB 
[09/16 21:18:55 visual_prompt]: 	Test 400/407. loss: 2.110, 0.1832 s / batch. (data: 3.08e-05)max mem: 17.22449 GB 
[09/16 21:18:58 visual_prompt]: Inference (test):avg data time: 8.72e-03, avg batch time: 0.1940, average loss: 1.2335
[09/16 21:18:58 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.78	top5: 97.87	
[09/16 21:18:58 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 21:19:09 visual_prompt]: Epoch 93 / 100: avg data time: 1.54e-01, avg batch time: 0.5561, average train loss: 0.0005
[09/16 21:19:13 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1425, average loss: 0.0002
[09/16 21:19:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:19:35 visual_prompt]: 	Test 100/407. loss: 1.839, 0.1993 s / batch. (data: 1.14e-04)max mem: 17.22449 GB 
[09/16 21:19:55 visual_prompt]: 	Test 200/407. loss: 1.265, 0.2188 s / batch. (data: 3.69e-02)max mem: 17.22449 GB 
[09/16 21:20:14 visual_prompt]: 	Test 300/407. loss: 1.503, 0.1948 s / batch. (data: 1.32e-02)max mem: 17.22449 GB 
[09/16 21:20:33 visual_prompt]: 	Test 400/407. loss: 2.112, 0.1827 s / batch. (data: 3.58e-05)max mem: 17.22449 GB 
[09/16 21:20:37 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1932, average loss: 1.2334
[09/16 21:20:37 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.77	top5: 97.87	
[09/16 21:20:37 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 21:20:48 visual_prompt]: Epoch 94 / 100: avg data time: 1.57e-01, avg batch time: 0.5616, average train loss: 0.0003
[09/16 21:20:52 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1426, average loss: 0.0002
[09/16 21:20:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:21:14 visual_prompt]: 	Test 100/407. loss: 1.835, 0.2005 s / batch. (data: 1.86e-02)max mem: 17.22449 GB 
[09/16 21:21:34 visual_prompt]: 	Test 200/407. loss: 1.267, 0.1959 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 21:21:54 visual_prompt]: 	Test 300/407. loss: 1.503, 0.1900 s / batch. (data: 1.16e-04)max mem: 17.22449 GB 
[09/16 21:22:13 visual_prompt]: 	Test 400/407. loss: 2.118, 0.1820 s / batch. (data: 3.86e-05)max mem: 17.22449 GB 
[09/16 21:22:17 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1949, average loss: 1.2331
[09/16 21:22:17 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.77	top5: 97.87	
[09/16 21:22:17 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 21:22:27 visual_prompt]: Epoch 95 / 100: avg data time: 1.59e-01, avg batch time: 0.5619, average train loss: 0.0004
[09/16 21:22:32 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1425, average loss: 0.0002
[09/16 21:22:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:22:54 visual_prompt]: 	Test 100/407. loss: 1.834, 0.1951 s / batch. (data: 3.60e-05)max mem: 17.22449 GB 
[09/16 21:23:13 visual_prompt]: 	Test 200/407. loss: 1.268, 0.1829 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 21:23:32 visual_prompt]: 	Test 300/407. loss: 1.503, 0.1831 s / batch. (data: 1.28e-04)max mem: 17.22449 GB 
[09/16 21:23:52 visual_prompt]: 	Test 400/407. loss: 2.117, 0.1832 s / batch. (data: 3.29e-05)max mem: 17.22449 GB 
[09/16 21:23:55 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1929, average loss: 1.2326
[09/16 21:23:55 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.78	top5: 97.87	
[09/16 21:23:55 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 21:24:06 visual_prompt]: Epoch 96 / 100: avg data time: 1.66e-01, avg batch time: 0.5683, average train loss: 0.0004
[09/16 21:24:11 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1425, average loss: 0.0002
[09/16 21:24:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:24:33 visual_prompt]: 	Test 100/407. loss: 1.834, 0.1974 s / batch. (data: 1.59e-02)max mem: 17.22449 GB 
[09/16 21:24:52 visual_prompt]: 	Test 200/407. loss: 1.267, 0.2070 s / batch. (data: 2.58e-02)max mem: 17.22449 GB 
[09/16 21:25:11 visual_prompt]: 	Test 300/407. loss: 1.502, 0.1871 s / batch. (data: 1.38e-04)max mem: 17.22449 GB 
[09/16 21:25:31 visual_prompt]: 	Test 400/407. loss: 2.116, 0.1822 s / batch. (data: 2.81e-05)max mem: 17.22449 GB 
[09/16 21:25:34 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1937, average loss: 1.2326
[09/16 21:25:34 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.78	top5: 97.87	
[09/16 21:25:34 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 21:25:45 visual_prompt]: Epoch 97 / 100: avg data time: 1.57e-01, avg batch time: 0.5571, average train loss: 0.0003
[09/16 21:25:50 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1423, average loss: 0.0002
[09/16 21:25:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:26:11 visual_prompt]: 	Test 100/407. loss: 1.834, 0.1822 s / batch. (data: 1.18e-04)max mem: 17.22449 GB 
[09/16 21:26:31 visual_prompt]: 	Test 200/407. loss: 1.266, 0.1962 s / batch. (data: 1.43e-02)max mem: 17.22449 GB 
[09/16 21:26:50 visual_prompt]: 	Test 300/407. loss: 1.502, 0.1957 s / batch. (data: 1.38e-02)max mem: 17.22449 GB 
[09/16 21:27:10 visual_prompt]: 	Test 400/407. loss: 2.115, 0.1820 s / batch. (data: 3.31e-05)max mem: 17.22449 GB 
[09/16 21:27:13 visual_prompt]: Inference (test):avg data time: 8.45e-03, avg batch time: 0.1947, average loss: 1.2326
[09/16 21:27:13 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.79	top5: 97.87	
[09/16 21:27:13 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 21:27:24 visual_prompt]: Epoch 98 / 100: avg data time: 1.60e-01, avg batch time: 0.5600, average train loss: 0.0005
[09/16 21:27:29 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1425, average loss: 0.0002
[09/16 21:27:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:27:50 visual_prompt]: 	Test 100/407. loss: 1.834, 0.1957 s / batch. (data: 1.39e-02)max mem: 17.22449 GB 
[09/16 21:28:10 visual_prompt]: 	Test 200/407. loss: 1.267, 0.2024 s / batch. (data: 2.06e-02)max mem: 17.22449 GB 
[09/16 21:28:29 visual_prompt]: 	Test 300/407. loss: 1.502, 0.1827 s / batch. (data: 1.25e-04)max mem: 17.22449 GB 
[09/16 21:28:49 visual_prompt]: 	Test 400/407. loss: 2.115, 0.1822 s / batch. (data: 3.91e-05)max mem: 17.22449 GB 
[09/16 21:28:52 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1939, average loss: 1.2327
[09/16 21:28:52 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.76	top5: 97.87	
[09/16 21:28:52 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 21:29:03 visual_prompt]: Epoch 99 / 100: avg data time: 1.60e-01, avg batch time: 0.5606, average train loss: 0.0003
[09/16 21:29:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1426, average loss: 0.0002
[09/16 21:29:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:29:29 visual_prompt]: 	Test 100/407. loss: 1.834, 0.1814 s / batch. (data: 1.34e-04)max mem: 17.22449 GB 
[09/16 21:29:49 visual_prompt]: 	Test 200/407. loss: 1.267, 0.1824 s / batch. (data: 1.50e-04)max mem: 17.22449 GB 
[09/16 21:30:08 visual_prompt]: 	Test 300/407. loss: 1.502, 0.2286 s / batch. (data: 1.29e-02)max mem: 17.22449 GB 
[09/16 21:30:28 visual_prompt]: 	Test 400/407. loss: 2.115, 0.1825 s / batch. (data: 3.17e-05)max mem: 17.22449 GB 
[09/16 21:30:31 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1933, average loss: 1.2326
[09/16 21:30:31 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.76	top5: 97.87	
[09/16 21:30:31 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 21:30:42 visual_prompt]: Epoch 100 / 100: avg data time: 1.52e-01, avg batch time: 0.5512, average train loss: 0.0003
[09/16 21:30:46 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1425, average loss: 0.0002
[09/16 21:30:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 100.00	top5: 100.00	
[09/16 21:31:08 visual_prompt]: 	Test 100/407. loss: 1.834, 0.1962 s / batch. (data: 1.47e-02)max mem: 17.22449 GB 
[09/16 21:31:28 visual_prompt]: 	Test 200/407. loss: 1.267, 0.2009 s / batch. (data: 1.23e-02)max mem: 17.22449 GB 
[09/16 21:31:47 visual_prompt]: 	Test 300/407. loss: 1.502, 0.1965 s / batch. (data: 1.20e-04)max mem: 17.22449 GB 
[09/16 21:32:06 visual_prompt]: 	Test 400/407. loss: 2.115, 0.1822 s / batch. (data: 3.00e-05)max mem: 17.22449 GB 
[09/16 21:32:10 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1933, average loss: 1.2326
[09/16 21:32:10 visual_prompt]: Classification results with test_vtab-svhn: top1: 81.76	top5: 97.87	
